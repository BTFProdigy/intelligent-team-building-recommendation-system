Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081?1088
Manchester, August 2008
Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627, USA
David Chiang
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292, USA
Abstract
We generalize Uno and Yagiura?s algo-
rithm for finding all common intervals of
two permutations to the setting of two
sequences with many-to-many alignment
links across the two sides. We show how
to maximally decompose a word-aligned
sentence pair in linear time, which can be
used to generate all possible phrase pairs
or a Synchronous Context-Free Grammar
(SCFG) with the simplest rules possible.
We also use the algorithm to precisely
analyze the maximum SCFG rule length
needed to cover hand-aligned data from
various language pairs.
1 Introduction
Many recent syntax-based statistical machine
translation systems fall into the general formalism
of Synchronous Context-Free Grammars (SCFG),
where the grammar rules are found by first align-
ing parallel text at the word level. From word-
level alignments, such systems extract the gram-
mar rules consistent either with the alignments
and parse trees for one of languages (Galley et
al., 2004), or with the the word-level alignments
alone without reference to external syntactic anal-
ysis (Chiang, 2005), which is the scenario we ad-
dress here.
In this paper, we derive an optimal, linear-time
algorithm for the problem of decomposing an ar-
bitrary word-level alignment into SCFG rules such
that each rule has at least one aligned word and is
minimal in the sense that it cannot be further de-
composed into smaller rules. Extracting minimal
rules is of interest both because rules with fewer
words are more likely to generalize to new data,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
and because rules with lower rank (the number of
nonterminals on the right-hand side) can be parsed
more efficiently.
This algorithm extends previous work on factor-
ing permutations to the general case of factoring
many-to-many alignments. Given two permuta-
tions of n, a common interval is a set of numbers
that are consecutive in both. The breakthrough
algorithm of Uno and Yagiura (2000) computes
all K common intervals of two length n permu-
tations in O(n + K) time. This is achieved by
designing data structures to index possible bound-
aries of common intervals as the computation pro-
ceeds, so that not all possible pairs of beginning
and end points need to be considered. Landau et
al. (2005) and Bui-Xuan et al (2005) show that all
common intervals can be encoded in O(n) space,
and adapt Uno and Yagiura?s algorithm to produce
this compact representation in O(n) time. Zhang
and Gildea (2007) use similar techniques to factor-
ize Synchronous Context Free Grammars in linear
time.
These previous algorithms assume that the input
is a permutation, but in machine translation it is
common to work with word-level alignments that
are many-to-many; in general any set of pairs of
words, one from each language, is a valid align-
ment for a given bilingual sentence pair. In this
paper, we consider a generalized concept of com-
mon intervals given such an alignment: a common
interval is a pair of phrases such that no word pair
in the alignment links a word inside the phrase
to a word outside the phrase. Extraction of such
phrases is a common feature of state-of-the-art
phrase-based and syntax-based machine transla-
tion systems (Och and Ney, 2004a; Chiang, 2005).
We generalize Uno and Yagiura?s algorithm to this
setting, and demonstrate a linear time algorithm
for a pair of aligned sequences. The output is a tree
representation of possible phrases, which directly
provides a set of minimal synchronous grammar
1081
rules for an SCFG-based machine translation sys-
tem. For phrase-based machine translation, one
can also read all phrase pairs consistent with the
original alignment off of the tree in time linear in
the number of such phrases.
2 Alignments and Phrase Pairs
Let [x, y] denote the sequence of integers between
x and y inclusive, and [x, y) the integers between
x and y ? 1 inclusive. An aligned sequence pair
or simply an alignment is a tuple (E,F,A), where
E = e
1
? ? ? e
n
and F = f
1
? ? ? f
m
are strings, and
A is a set of links (x, y), where 1 ? x ? n and
1 ? y ? m, connecting E and F . For most of this
paper, since we are not concerned with the identity
of the symbols in E and F , we will assume for
simplicity that e
i
= i and f
j
= j, so that E =
[1, n] and F = [1,m].
In the context of statistical machine translation
(Brown et al, 1993), we may interpretE as an En-
glish sentence, F its translation in French, and A
a representation of how the words correspond to
each other in the two sentences. A pair of sub-
strings [s, t] ? E and [u, v] ? F is a phrase pair
(Och and Ney, 2004b) if and only if the subset of
links emitted from [s, t] in E is equal to the sub-
set of links emitted from [u, v] in F , and both are
nonempty.
Figure 1a shows an example of a many-to-
many alignment, where E = [1, 6], F =
[1, 7], and A = {(1, 6), (2, 5), (2, 7), (3, 4),
(4, 1), (4, 3), (5, 2), (6, 1), (6, 3)}. The eight
phrase pairs in this alignment are:
([1, 1], [6, 6]), ([1, 2], [5, 7]),
([3, 3], [4, 4]), ([1, 3], [4, 7]),
([5, 5], [2, 2]), ([4, 6], [1, 3]),
([3, 6], [1, 4]), ([1, 6], [1, 7]).
In Figure 1b, we show the alignment matrix rep-
resentation of the given alignment. By default, the
columns correspond to the tokens in E, the rows
correspond to the tokens in F , and the black cells
in the matrix are the alignment links in A. Using
the matrix representation, the phrase pairs can be
viewed as submatrices as shown with the black-
lined boundary boxes. Visually, a submatrix rep-
resents a phrase pair when it contains at least one
alignment link and there are no alignment links di-
rectly above, below, or to the right or left of it.
e
1
e
2
e
3
e
4
e
5
e
6
f
1
f
2
f
3
f
4
f
5
f
6
f
7
1
1
2
2
3
3
4
4
5
5
6
6
7
(a) (b)
Figure 1: An example of (a) a many-to-many
alignment and (b) the same alignment as a matrix,
with its phrase pairs marked.
2.1 Number of Phrase Pairs
In this section, we refine our definition of phrase
pairs with the concept of tightness and give an
asymptotic upper bound on the total number of
such phrase pairs as the two sequences? lengths
grow. In the original definition, the permissive
many-to-many constraint allows for unaligned to-
kens in both sequences E and F . If there is an un-
aligned token adjacent to a phrase pair, then there
is also a phrase pair that includes the unaligned
token. We say that a phrase pair ([s, t], [u, v]) is
tight if none of e
s
, e
t
, f
u
and f
v
is unaligned. By
focusing on tight phrase pairs, we eliminate the
non-tight ones that share the same set of alignment
links with their tight counterpart.
Given [s, t] in E, let l be the first member of
F that any position in [s, t] links to, and let u be
the last. According to the definition of tight phrase
pair, [l, u] is the only candidate phrase in F to pair
up with [s, t] in E. So, the total number of tight
phrase pairs is upper-bounded by the total number
of intervals in each sequence, which is O(n
2
).
If we do not enforce the tightness constraint, the
total number of phrase pairs can grow much faster.
For example, if a sentence contains only a single
alignment link between the midpoint of F and the
midpoint of E, then there will be O(n
2
m
2
) possi-
ble phrase pairs, but only a single tight phrase pair.
From now on, term phrase pair always refers to a
tight phrase pair.
2.2 Hierarchical Decomposition of Phrase
Pairs
In this section, we show how to encode all the tight
phrase pairs of an alignment in a tree of sizeO(n).
Lemma 2.1. When two phrase pairs overlap, the
intersection, the differences, and the union of the
two are also phrase pairs.
The following picture graphically represents the
two possible overlapping structures of two phrase
1082
([1, 6], [1, 7])
([1, 3], [4, 7])
([1, 2], [5, 7])
([1, 1], [6, 6])
([3, 3], [4, 4])
([4, 6], [1, 3])
([5, 5], [2, 2])
Figure 2: The normalized decomposition tree of
the alignment in Figure 1.
pairs: ([s, t], [u, v]) and ([s
?
, t
?
], [u
?
, v
?
]).
s s? t t?
u
u?
v
v?
s s? t t?
u?
u
v?
v
Let AB and BC be two overlapping English
phrases, with B being their overlap. There are six
possible phrases, A, B, C, AB, BC, and ABC,
but if we omit BC, the remainder are nested and
can be represented compactly by ((AB)C), from
which BC can easily be recovered. If we system-
atically apply this to the whole sentence, we obtain
a hierarchical representation of all the phrase pairs,
which we call the normalized decomposition tree.
The normalized decomposition tree for the exam-
ple is shown in Figure 2.
Bui-Xuan et al (2005) show that the family of
common intervals is weakly partitive, i.e. closed
under intersection, difference and union. This al-
lows the family to be represented as a hierarchi-
cal decomposition. The normalized decomposi-
tion focuses on the right strong intervals, those
that do not overlap with any others on the right.
Lemma 2.1 shows that the family of phrase pairs
is also a weakly partitive family and can be hierar-
chically decomposed after normalization. A minor
difference is we prefer left strong intervals since
our algorithms scan F from left to right. Another
difference is that we binarize a linearly-arranged
sequence of non-overlapping phrase pairs instead
of grouping them together.
In the following sections, we show how to pro-
duce the normalized hierarchical analysis of a
given alignment.
3 Shift-Reduce Algorithm
In this section, we present anO(n
2
+m+|A|) algo-
rithm that is similar in spirit to a shift-reduce algo-
rithm for parsing context-free languages. This al-
gorithm is not optimal, but its left-to-right bottom-
up control will form the basis for the improved al-
gorithm in the next section.
First, we can efficiently test whether a span
[x, y] is a phrase as follows. Define a pair of func-
tions l(x, y) and u(x, y) that record the minimum
and maximum, respectively, of the positions on the
French side that are linked to the positions [x, y]:
l(x, y) = min{j | (i, j) ? A, i ? [x, y]}
u(x, y) = max{j | (i, j) ? A, i ? [x, y]}
Note that l(?, y) is monotone increasing and u(?, y)
is monotone decreasing. Define a step of l(?, y)
(or u(?, y)) to be a maximal interval over which
l(?, y) (resp., u(?, y)) is constant. We can compute
u(x, y) in constant time from its value on smaller
spans:
u(x, y) = max{u(x, z), u(z + 1, y)}
and similarly for l(x, y).
We define the following functions to count the
number of links emitted from prefixes of F and E:
F
c
(j) = |{(i
?
, j
?
) ? A | j
?
? j}|
E
c
(i) = |{(i
?
, j
?
) ? A | i
?
? i}|
Then the difference F
c
(u) ? F
c
(l ? 1) counts the
total number of links to positions in [l, u], and
E
c
(y)?E
c
(x?1) counts the total number of links
to positions in [x, y]. E
c
and F
c
can be precom-
puted in O(n + m + |A|) time.
Finally, let
f(x, y) = F
c
(u(x, y))? F
c
(l(x, y)? 1)
? (E
c
(y)? E
c
(x? 1))
Note that f is non-negative, but not monotonic in
general. Figure 4 provides a visualization of u, l,
and f for the example alignment from Section 2.
This gives us our phrase-pair test:
Lemma 3.1. [x, y] and [l(x, y), u(x, y)] are a
phrase pair if and only if f(x, y) = 0.
This test is used in the following shift-reduce-
style algorithm:
X ? {1}
for y ? [2, n] from left to right do
append y to X
for x ? X from right to left do
compute u(x, y) from u(x + 1, y)
compute l(x, y) from l(x + 1, y)
if f(x, y) = 0 then
[x, y] is a phrase
1083
remove [x+ 1, y] from X
end if
end for
end for
In the worst case, at each iteration we traverse
the entire stack X without a successful reduction,
indicating that the worst case time complexity is
O(n
2
).
4 A Linear Algorithm
In this section, we modify the shift-reduce algo-
rithm into a linear-time algorithm that avoids un-
necessary reduction attempts. It is a generalization
of Uno and Yagiura?s algorithm.
4.1 Motivation
The reason that our previous algorithm is quadratic
is that for each y, we try every possible combina-
tion with the values in X . Uno and Yagiura (2000)
point out that in the case of permutations, it is not
necessary to examine all spans, because it is pos-
sible to delete elements from X so that f(?, y) is
monotone decreasing on X . This means that all
the x ? X such that f(x, y) = 0 can always be
conveniently found at the end of X . That this can
be done safely is guaranteed by the following:
Lemma 4.1. If x
1
< x
2
< y and f(x
1
, y) <
f(x
2
, y), then for all y
?
? y, f(x
2
, y
?
) > 0 (i.e.,
[x
2
, y
?
] is not a phrase).
Let us say that x
2
violates monotonicity if x
1
is the predecessor of x
2
in X and f(x
1
, y) <
f(x
2
, y). Then by Lemma 4.1, we can safely re-
move x
2
from X .
Furthermore, Uno and Yagiura (2000) show that
we can enforce monotonicity at all times in such a
way that the whole algorithm runs in linear time.
This is made possible with a shortcut based on the
following:
Lemma 4.2. If x
1
< x
2
< y and u(x
1
, y ? 1) >
u(x
2
, y ? 1) but u(x
1
, y) = u(x
2
, y), then for all
y
?
? y, f(x
2
, y
?
) > 0 (i.e., [x
2
, y
?
] is not a phrase).
The same holds mutatis mutandis for l.
Let us say that y updates a step [x
?
, y
?
] of u (or
l) if u(x
?
, y) > u(x
?
, y ? 1) (resp., l(x
?
, y) <
l(x
?
, y?1)). By Lemma 4.2, if [x
1
, y
1
] and [x
2
, y
2
]
are different steps of u(?, y ? 1) (resp., l(?, y ? 1))
and y updates both of them, then we can remove
from X all x
?
such that x
2
? x
?
< y.
u(?, y ? 1)
l(?, y ? 1)
u(?, y)
l(?, y)
x
?
1
y
?
2
y
x
?
2
y
?
1
Figure 3: Illustration of step (3) of the algorithm.
The letters indicate substeps of (3).
4.2 Generalized algorithm
These results generalize to the many-to-many
alignment case, although we must introduce a few
nuances. The new algorithm proceeds as follows:
Initialize X = {1}. For y ? [2, n] from left to
right:
1. Append y to X .
2. Update u and l:
(a) Traverse the steps of u(?, y ? 1) from
right to left and compute u(?, y) until we
have found the leftmost step [x
?
, y
?
] of
u(?, y ? 1) that gets updated by y.
(b) Do the same for l.
We have computed two values for x
?
; let x
?
1
be the smaller and x
?
2
be the larger. Similarly,
let y
?
1
be the smaller y
?
.
3. Enforce monotonicity of f(?, y) (see Fig-
ure 3):
(a) The positions left of the smaller x
?
al-
ways satisfy monotonicity, so do noth-
ing.
(b) For x ? [x
?
1
, x
?
2
) ? X while x violates
monotonicity, remove x from X .
(c) For x ? [x
?
2
, y
?
1
] ? X while x violates
monotonicity, remove x from X .
(d) The steps right of y
?
1
may or may not
violate monotonicity, but we use the
stronger Lemma 4.2 to delete all of them
(excluding y).
1
1
In the special case where [x
?
, y
?
] is updated by y to the
1084
y = 1 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 2 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 3 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 4 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 5 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 6 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
Figure 4: The evolution of u(x, y) , l(x, y), and f(x, y) as y goes from 1 to 6 for the example alignment.
Each pair of diagrams shows the state of affairs between steps (3) and (4) of the algorithm. Light grey
boxes are the steps of u, and darker grey boxes are the steps of l. We use solid boxes to plot the values
of remaining x?s on the list but also show the other values in empty boxes for completeness.
(e) Finally, if y violates monotonicity, re-
move it from X .
4. For x ? X from right to left until f(x, y) >
0, output [x, y] and remove x?s successor in
X .
2
An example of the algorithm?s execution is
shown in Figure 4. The evolution of u(x, y),
l(x, y), and f(x, y) is displayed for increasing y
(from 1 to 6). We point out the interesting steps.
When y = 2, position 2 is eliminated due to step
(3e) of our algorithm to ensure monotonicity of
f at the right end, and [1, 2] is reduced. When
same value as the step to its left, we can use Lemma 4.2 to
delete [x
?
, y
?
] and y as well, bypassing steps (3b),(3c), and
(3e).
2
If there are any such x, they must lie to the left of x
?
1
.
Therefore a further optimization would be to perform step (4)
before step (3), starting with the predecessor of x
?
1
. If a re-
duction is made, we can jump to step (3e).
y = 3, two reductions are made: one on [3, 3] and
the other on [1, 3]. Because of leftmost normaliza-
tion, position 3 is deleted. When y = 6, we have
x
?
1
= x
?
2
= y
?
1
= 5, so that position 5 is deleted by
step (3c) and position 6 is deleted by step (3e).
4.3 Correctness
We have already argued in Section 4.1 that the
deletion of elements fromX does not alter the out-
put of the algorithm. It remains to show that step
(3) guarantees monotonicity:
Claim 4.3. For all y, at the end of step (3), f(?, y)
is monotone decreasing.
Proof. By induction on y. For y = 1, the claim
is trivially true. For y > 1, we want to show
that for x
1
, x
2
adjacent in X such that x
1
< x
2
,
f(x
1
, y) ? f(x
2
, y). We consider the five regions
of X covered by step (3) (cf. Figure 3), and then
1085
the boundaries between them.
Region (a): x
1
, x
2
? [1, x
?
1
]. Since u(x
i
, y) =
u(x
i
, y ? 1) and l(x
i
, y) = l(x
i
, y ? 1), we have:
f(x
i
, y)?f(x
i
, y?1) = 0? (E
c
(y)?E
c
(y?1))
i.e., in this region, f shifts down uniformly from
iteration y ? 1 to iteration y. Hence, if f(?, y ?
1) was monotonic, then f(?, y) is also monotonic
within this region.
Region (b): x
1
, x
2
? [x
?
1
, x
?
2
). Since u(x
1
, y ?
1) = u(x
2
, y ? 1) and u(x
1
, y) = u(x
2
, y) and
similarly for l, we have:
f(x
1
, y)? f(x
1
, y? 1) = f(x
2
, y)? f(x
2
, y? 1)
i.e., in this region, f shifts up or down uniformly.
3
Hence, if f(?, y ? 1) was monotonic, then f(?, y)
is also monotonic within this region.
Region (c): x
1
, x
2
? [x
?
2
, y
?
1
]. Same as Case 2.
Region (d) and (e): Vacuous (these regions have at
most one element).
The remaining values of x
1
, x
2
are those that
straddle the boundaries between regions. But
step (3) of the algorithm deals with each of
these boundaries explicitly, deleting elements until
f(x
1
) ? f(x
2
). Thus f(?, y) is monotonic every-
where.
4.4 Implementation and running time
X should be implemented in a way that allows
linear-time traversal and constant-time deletion;
also, u and l must be implemented in a way that
allows linear-time traversal of their steps. Doubly-
linked lists are appropriate for all three functions.
Claim 4.4. The above algorithm runs in O(n +
m + |A|) time.
We can see that the algorithm runs in linear time
if we observe that whenever we traverse a part of
X , we delete it, except for a constant amount of
work per iteration (that is, per value of y): the steps
traversed in (2) are all deleted in (3d) except four
(two for u and two for l); the positions traversed in
(3b), (3c), and (4) are all deleted except one.
4.5 SCFG Rule extraction
The algorithm of the previous section outputs the
normalized decomposition tree depicted in Fig-
ure 2. From this tree, it is straightforward to obtain
3
It can be shown further that in this region, f shifts up or
is unchanged. Therefore any reductions in step (4) must be in
region (a).
A? B
(1)
C
(2)
, C
(2)
B
(1)
B ? D
(1)
E
(2)
, E
(2)
D
(1)
D ? G
(1)
e
2
, f
5
G
(1)
f
6
G? e
1
, f
6
E ? e
3
, f
4
C ? e
4
F
(1)
e
6
, f
1
F
(1)
f
3
F ? e
5
, f
2
Figure 5: Each node from the normalized decom-
position tree of Figure 2 is converted into an SCFG
rule.
a set of maximally-decomposed SCFG rules. As
an example, the tree of Figure 2 produces the rules
shown in Figure 5.
We adopt the SCFG notation of Satta and Pe-
serico (2005). Each rule has a right-hand side se-
quence for both languages, separated by a comma.
Superscript indices in the right-hand side of gram-
mar rules such as:
A? B
(1)
C
(2)
, C
(2)
B
(1)
indicate that the nonterminals with the same index
are linked across the two languages, and will even-
tually be rewritten by the same rule application.
The example above inverts the order of B and C
when translating from the source language to the
target language.
The SCFG rule extraction proceeds as follows.
Assign a nonterminal label to each node in the tree.
Then for each node (S, T ) in the tree top-down,
where S and T are sequences of positions,
1. For each child (S
?
, T
?
), S
?
and T
?
must be
subsequences of S and T , respectively. Re-
place their occurrences in S and T with a pair
of coindexed nonterminals X
?
, where X
?
is
the nonterminal assigned to the child.
2. For each remaining position i in S, replace i
with e
i
.
3. For each remaining position j in T , replace j
with f
j
.
4. Output the rule X ? S, T , where X is the
nonterminal assigned to the parent.
As an example, consider the node ([4, 6], [1, 3])
in Figure 2. After step 1, it becomes
(4F
(1)
6, 1F
(1)
3)
and after steps 2 and 3, it becomes
(e
4
F
(1)
e
6
, f
1
F
(1)
f
3
)
1086
0 1 2 3 4 5 6
Hindi/English 52.8 53.5 99.9 99.9 100.0
Chinese/English 51.0 52.4 99.7 99.8 100.0 100.0 100.0
French/English 52.1 53.5 99.9 100.0 100.0 100.0
Romanian/English 50.8 52.6 99.9 99.9 100.0 100.0
Spanish/English 50.7 51.8 99.9 100.0 100.0 100.0
Table 1: Cumulative percentages of rule tokens by number of nonterminals in right-hand side. A blank
indicates that no rules were found with that number of nonterminals.
Finally, step 4 outputs
C ? e
4
F
(1)
e
6
, f
1
F
(1)
f
3
A few choices are available to the user depend-
ing on the application intended for the SCFG ex-
traction. The above algorithm starts by assigning
a nonterminal to each node in the decomposition
tree; one could assign a unique nonterminal to each
node, so that the resulting grammar produces ex-
actly the set of sentences given as input. But for
machine translation, one may wish to use a single
nonterminal, such that the extracted rules can re-
combine freely, as in Chiang (2005).
Unaligned words in either language (an empty
row or column in the alignment matrix, not present
in our example) will be attached as high as possi-
ble in our tree. However, other ways of handling
unaligned words are possible given the decompo-
sition tree. One can produce all SCFG rules con-
sistent with the alignment by, for each unaligned
word, looping through possible attachment points
in the decomposition tree. In this case, the num-
ber of SCFG rules produced may be exponential
in the size of the original input sentence; however,
even in this case, the decomposition tree enables a
rule extraction algorithm that is linear in the output
length (the number of SCFG rules).
4.6 Phrase extraction
We briefly discuss the process of extracting all
phrase pairs consistent with the original alignment
from the normalized decomposition tree. First of
all, every node in the tree gives a valid phrase
pair. Then, in the case of overlapping phrase pairs
such as the example in Section 2.1, the decom-
position tree will contain a left-branching chain
of binary nodes all performing the same permuta-
tion. While traversing the tree, whenever we iden-
tify such a chain, let ?
1
, . . . , ?
k
be the sequence of
all the children of the nodes in the chain. Then,
each of the subsequences {?
i
, . . . , ?
j
| 1 < i <
j ? k} yields a valid phrase pair. In our exam-
ple, the root of the tree of Figure 2 and its left
child form such a chain, with three children; the
subsequence {([3, 3], [4, 4]), ([4, 6], [1, 3])} yields
the phrase ([3, 6], [1, 4]). In the case of unaligned
words, we can also consider all combinations of
their attachments, as discussed for SCFG rule ex-
traction.
5 Experiments on Analyzing Word
Alignments
One application of our factorization algorithm
is analyzing human-annotated word alignments.
Wellington et al (2006) argue for the necessity
of discontinuous spans (i.e., for a formalism be-
yond Synchronous CFG) in order for synchronous
parsing to cover human-annotated word alignment
data under the constraint that rules have a rank
of no more than two. In a related study, Zhang
and Gildea (2007) analyze the rank of the Syn-
chronous CFG derivation trees needed to parse the
same data. The number of discontinuous spans
and the rank determine the complexity of dynamic
programming algorithms for synchronous parsing
(alignment) or machine translation decoding.
Both studies make simplifying assumptions on
the alignment data to avoid dealing with many-to-
many word links. Here, we apply our alignment
factorization algorithm directly to the alignments
to produce a normalized decomposition tree for
each alignment and collect statistics on the branch-
ing factors of the trees.
We use the same alignment data for the
five language pairs Chinese-English, Romanian-
English, Hindi-English, Spanish-English, and
French-English as Wellington et al (2006). Ta-
ble 1 reports the number of rules extracted by the
rank, or number of nonterminals on the right-hand
side. Almost all rules are binary, implying both
that binary synchronous grammars are adequate
for MT, and that our algorithm can find such gram-
mars. Table 2 gives similar statistics for the num-
ber of terminals in each rule. The phrases we ex-
tract are short enough that they are likely to gener-
alize to new sentences. The apparent difficulty of
1087
0 1 2 3 4 5 6 7 8 9 ?10 max
Hindi/English 39.6 92.2 97.7 99.5 99.7 99.9 99.9 100.0 7
Chinese/English 39.8 87.2 96.2 99.0 99.7 99.9 100.0 100.0 100.0 100.0 100.0 12
French/English 44.5 89.0 93.4 95.8 97.5 98.4 99.0 99.3 99.6 99.8 100.0 18
Romanian/English 42.9 89.8 96.9 98.9 99.5 99.8 99.9 100.0 100.0 9
Spanish/English 47.5 91.8 97.7 99.4 99.9 99.9 100.0 100.0 100.0 9
Table 2: Cumulative percentages of rule tokens by number of terminals in right-hand side. A blank
indicates that no rules were found with that number of terminals.
the French-English pair is due to the large number
of ?possible? alignments in this dataset.
6 Conclusion
By extending the algorithm of Uno and Yagiura
(2000) from one-to-one mappings to many-to-
many mappings, we have shown how to construct a
hierarchical representation of all the phrase pairs in
a given aligned sentence pair in linear time, which
yields a set of minimal SCFG rules. We have also
illustrated how to apply the algorithm as an analyt-
ical tool for aligned bilingual data.
Acknowledgments Thanks to Bob Moore for
suggesting the extension to phrase extraction at
SSST 2007. This work was supported in part
by NSF grants IIS-0546554 and ITR-0428020,
and DARPA grant HR0011-06-C-0022 under BBN
Technologies subcontract 9500008412.
References
Brown, Peter F., Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Bui-Xuan, Binh Minh, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s al-
gorithm. In The 16th Annual International Sympo-
sium on Algorithms and Computation (ISAAC ?05),
pages 146?155.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005, pages 263?270.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL 2004.
Landau, Gad M., Laxmi Parida, and Oren Weimann.
2005. Gene proximity analysis across whole
genomes via PQ trees. Journal of Computational Bi-
ology, 12(10):1289?1306.
Och, Franz Josef and Hermann Ney. 2004a. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Och, Franz Josef and Hermann Ney. 2004b. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30:417?449.
Satta, Giorgio and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In Proceedings of EMNLP
2005, pages 803?810, Vancouver, Canada, October.
Uno, Takeaki and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of two
permutations. Algorithmica, 26(2):290?309.
Wellington, Benjamin, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of COLING-ACL 2006.
Zhang, Hao and Daniel Gildea. 2007. Factorization
of synchronous context-free grammars in linear time.
In Proceedings of the NAACL Workshop on Syntax
and Structure in Statistical Translation (SSST).
1088
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224?233,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Large-Margin Training of
Syntactic and Structural Translation Features
David Chiang
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
Yuval Marton and Philip Resnik
Department of Linguistics and the UMIACS
Laboratory for Computational Linguistics
and Information Processing
University of Maryland
College Park, MD 20742, USA
{ymarton,resnik}@umiacs.umd.edu
Abstract
Minimum-error-rate training (MERT) is a bot-
tleneck for current development in statistical
machine translation because it is limited in
the number of weights it can reliably opti-
mize. Building on the work of Watanabe et
al., we explore the use of the MIRA algorithm
of Crammer et al as an alternative to MERT.
We first show that by parallel processing and
exploiting more of the parse forest, we can
obtain results using MIRA that match or sur-
pass MERT in terms of both translation qual-
ity and computational cost. We then test the
method on two classes of features that address
deficiencies in the Hiero hierarchical phrase-
based model: first, we simultaneously train a
large number of Marton and Resnik?s soft syn-
tactic constraints, and, second, we introduce
a novel structural distortion model. In both
cases we obtain significant improvements in
translation performance. Optimizing them in
combination, for a total of 56 feature weights,
we improve performance by 2.6 B??? on a
subset of the NIST 2006 Arabic-English eval-
uation data.
1 Introduction
Since its introduction by Och (2003), minimum er-
ror rate training (MERT) has been widely adopted
for training statistical machine translation (MT) sys-
tems. However, MERT is limited in the number of
feature weights that it can optimize reliably, with
folk estimates of the limit ranging from 15 to 30 fea-
tures.
One recent example of this limitation is a series
of experiments by Marton and Resnik (2008), in
which they added syntactic features to Hiero (Chi-
ang, 2005; Chiang, 2007), which ordinarily uses no
linguistically motivated syntactic information. Each
of their new features rewards or punishes a deriva-
tion depending on how similar or dissimilar it is
to a syntactic parse of the input sentence. They
found that in order to obtain the greatest improve-
ment, these features had to be specialized for par-
ticular syntactic categories and weighted indepen-
dently. Not being able to optimize them all at once
using MERT, they resorted to running MERT many
times in order to test different combinations of fea-
tures. But it would have been preferable to use a
training method that can optimize the features all at
once.
There has been much work on improving MERT?s
performance (Duh and Kirchoff, 2008; Smith and
Eisner, 2006; Cer et al, 2008), or on replacing
MERT wholesale (Turian et al, 2007; Blunsom et
al., 2008). This paper continues a line of research on
online discriminative training (Tillmann and Zhang,
2006; Liang et al, 2006; Arun and Koehn, 2007),
extending that of Watanabe et al (2007), who use
the Margin Infused Relaxed Algorithm (MIRA) due
to Crammer et al (2003; 2006). Our guiding princi-
ple is practicality: like Watanabe et al, we train on
a small tuning set comparable in size to that used
by MERT, but by parallel processing and exploit-
ing more of the parse forest, we obtain results us-
ing MIRA that match or surpass MERT in terms of
both translation quality and computational cost on a
large-scale translation task.
Taking this further, we test MIRA on two classes
of features that make use of syntactic information
and hierarchical structure. First, we generalize Mar-
ton and Resnik?s (2008) soft syntactic constraints by
224
training all of them simultaneously; and, second, we
introduce a novel structural distortion model. We ob-
tain significant improvements in both cases, and fur-
ther large improvements when the two feature sets
are combined.
The paper proceeds as follows. We describe our
training algorithm in section 2; our generalization
of Marton and Resnik?s soft syntactic constraints in
section 3; our novel structural distortion features in
section 4; and experimental results in section 5.
2 Learning algorithm
The translation model is a standard linear model
(Och and Ney, 2002), which we train using MIRA
(Crammer and Singer, 2003; Crammer et al, 2006),
following Watanabe et al (2007). We describe the
basic algorithm first and then progressively refine it.
2.1 Basic algorithm
Let e, by abuse of notation, stand for both output
strings and their derivations. We represent the fea-
ture vector for derivation e as h(e). Initialize the fea-
ture weights w. Then, repeatedly:
? Select a batch of input sentences f1, . . . , fm.
? Decode each fi to obtain a set of hypothesis
translations ei1, . . . , ein.
? For each i, select one of the ei j to be the oracle
translation e?i , by a criterion described below.
Let ?hi j = h(e?i ) ? h(ei j).
? For each ei j, compute the loss `i j, which is
some measure of how bad it would be to guess
ei j instead of e?i .
? Update w to the value of w? that minimizes:
1
2
?w? ? w?2 + C
m?
i=1
max
1? j?n
(`i j ? ?hi j ? w?) (1)
where we set C = 0.01. The first term means
that we want w? to be close to w, and second
term (the generalized hinge loss) means that we
want w? to score e?i higher than each ei j by a
margin at least as wide as the loss `i j.
When training is finished, the weight vectors from
all iterations are averaged together. (If multiple
passes through the training data are made, we only
average the weight vectors from the last pass.) The
technique of averaging was introduced in the con-
text of perceptrons as an approximation to taking a
vote among all the models traversed during training,
and has been shown to work well in practice (Fre-
und and Schapire, 1999; Collins, 2002). We follow
McDonald et al (2005) in applying this technique to
MIRA.
Note that the objective (1) is not the same as that
used by Watanabe et al; rather, it is the same as
that used by Crammer and Singer (2003) and related
to that of Taskar et al (2005). We solve this opti-
mization problem using a variant of sequential min-
imal optimization (Platt, 1998): for each i, initialize
?i j = C for a single value of j such that ei j = e?i ,
and initialize ?i j = 0 for all other values of j. Then,
repeatedly choose a sentence i and a pair of hypothe-
ses j, j?, and let
w? ? w? + ?(?hi j ? ?hi j?) (2)
?i j ? ?i j + ? (3)
?i j? ? ?i j? ? ? (4)
where
? = clip
[??i j,?i j? ]
(`i j ? `i j?) ? (?hi j ? ?hi j?) ? w?
??hi j ? ?hi j??2
(5)
where the function clip[x,y](z) gives the closest num-
ber to z in the interval [x, y].
2.2 Loss function
Assuming B??? as the evaluation criterion, the loss
`i j of ei j relative to e?i should be related somehow
to the difference between their B??? scores. How-
ever, B??? was not designed to be used on individ-
ual sentences; in general, the highest-B??? transla-
tion of a sentence depends on what the other sen-
tences in the test set are. Sentence-level approxi-
mations to B??? exist (Lin and Och, 2004; Liang
et al, 2006), but we found it most effective to per-
form B??? computations in the context of a set O of
previously-translated sentences, following Watan-
abe et al (2007). However, we don?t try to accu-
mulate translations for the entire dataset, but simply
maintain an exponentially-weighted moving average
of previous translations.
225
More precisely: For an input sentence f, let e be
some hypothesis translation and let {rk} be the set of
reference translations for f. Let c(e; {rk}), or simply
c(e) for short, be the vector of the following counts:
|e|, the effective reference length mink |rk|, and, for
1 ? n ? 4, the number of n-grams in e, and the num-
ber of n-gram matches between e and {rk}. These
counts are sufficient to calculate a B??? score, which
we write as B???(c(e)). The pseudo-document O is
an exponentially-weighted moving average of these
vectors. That is, for each training sentence, let e? be
the 1-best translation; after processing the sentence,
we update O, and its input length O f :
O ? 0.9(O + c(e?)) (6)
O f ? 0.9(O f + |f|) (7)
We can then calculate the B??? score of hypothe-
ses e in the context of O. But the larger O is, the
smaller the impact the current sentence will have on
the B??? score. To correct for this, and to bring the
loss function roughly into the same range as typical
margins, we scale the B??? score by the size of the
input:
B(e; f, {rk}) = (O f + |f|) ? B???(O + c(e; {rk})) (8)
which we also simply write as B(e). Finally, the loss
function is defined to be:
`i j = B(e?i ) ? B(ei j) (9)
2.3 Oracle translations
We now describe the selection of e?. We know of
three approaches in previous work. The first is to
force the decoder to output the reference sentence
exactly, and select the derivation with the highest
model score, which Liang et al (2006) call bold up-
dating. The second uses the decoder to search for
the highest-B??? translation (Tillmann and Zhang,
2006), which Arun and Koehn (2007) call max-B???
updating. Liang et al and Arun and Koehn experi-
ment with these methods and both opt for a third
method, which Liang et al call local updating: gen-
erate an n-best list of translations and select the
highest-B??? translation from it. The intuition is that
due to noise in the training data or reference transla-
tions, a high-B??? translation may actually use pe-
culiar rules which it would be undesirable to en-
courage the model to use. Hence, in local updating,
Model score
B
??
?
sc
or
e
0.4
0.5
0.6
0.7
0.8
0.9
1
-90 -85 -80 -75 -70 -65 -60
? = 0
? = 0.5
? = 1
? = ?
Figure 1: Scatter plot of 10-best unique translations of a
single sentence obtained by forest rescoring using various
values of ? in equation (11).
the search for the highest-B??? translation is limited
to the n translations with the highest model score,
where n must be determined experimentally.
Here, we introduce a new oracle-translation selec-
tion method, formulating the intuition behind local
updating as an optimization problem:
e? = arg max
e
(B(e) + h(e) ? w) (10)
Instead of choosing the highest-B??? translation
from an n-best list, we choose the translation that
maximizes a combination of (approximate) B???
and the model.
We can also interpret (10) in the following way:
we want e? to be the max-B??? translation, but we
also want to minimize (1). So we balance these two
criteria against each other:
e? = arg max
e
(B(e) ? ?(B(e) ? h(e) ? w)) (11)
where (B(e) ? h(e) ? w) is that part of (1) that de-
pends on e?, and ? is a parameter that controls how
much we are willing to allow some translations to
have higher B??? than e? if we can better minimize
(1). Setting ? = 0 would reduce to max-B??? up-
dating; setting ? = ? would never update w at all.
Setting ? = 0.5 reduces to equation (10).
Figure 1 shows the 10-best unique translations for
a single input sentence according to equation (11)
under various settings of ?. The points at far right are
the translations that are scored highest according to
226
the model. The ? = 0 points in the upper-left corner
are typical of oracle translations that would be se-
lected under the max-B??? policy: they indeed have
a very high B??? score, but are far removed from the
translations preferred by the model; thus they would
cause violent updates to w. Local updating would
select the topmost point labeled ? = 1. Our scheme
would select one of the ? = 0.5 points, which have
B??? scores almost as high as the max-B??? transla-
tions, yet are not very far from the translations pre-
ferred by the model.
2.4 Selecting hypothesis translations
What is the set {ei j} of translation hypotheses? Ide-
ally we would let it be the set of all possible transla-
tions, and let the objective function (1) take all of
them into account. This is the approach taken by
Taskar et al (2004), but their approach assumes that
the loss function can be decomposed into local loss
functions. Since our loss function cannot be so de-
composed, we select:
? the 10-best translations according to the model;
we then rescore the forest to obtain
? the 10-best translations according to equation
(11) with ? = 0.5, the first of which is the oracle
translation, and
? the 10-best translations with ? = ?, to serve as
negative examples.
The last case is what Crammer et al (2006) call
max-loss updating (where ?loss? refers to the gener-
alized hinge loss) and Taskar et al (2005) call loss-
augmented inference. The rationale here is that since
the objective (1) tries to minimize max j(`i j ? ?hi j ?
w?), we should include the translations that have the
highest (`i j ? ?hi j ? w) in order to approximate the
effect of using the whole forest.
See Figure 1 again for an illustration of the hy-
potheses selected for a single sentence. The max-
B??? points in the upper left are not included (and
would have no effect even if they were included).
The ? = ? points in the lower-right are the negative
examples: they are poor translations that are scored
too high by the model, and the learning algorithm
attempts to shift them to the left.
To perform the forest rescoring, we need to use
several approximations, since an exact search for
B???-optimal translations is NP-hard (Leusch et al,
2008). For every derivation e in the forest, we calcu-
late a vector c(e) of counts as in Section 2.2 except
using unclipped counts of n-gram matches (Dreyer
et al, 2007), that is, the number of matches for an n-
gram can be greater than the number of occurrences
of the n-gram in any reference translation. This can
be done efficiently by calculating c for every hyper-
edge (rule application) in the forest:
? the number of output words generated by the
rule
? the effective reference length scaled by the frac-
tion of the input sentence consumed by the rule
? the number of n-grams formed by the applica-
tion of the rule (1 ? n ? 4)
? the (unclipped) number of n-gram matches
formed by the application of the rule (1 ? n ?
4)
We keep track of n-grams using the same scheme
used to incorporate an n-gram language model into
the decoder (Wu, 1996; Chiang, 2007).
To find the best derivation in the forest, we tra-
verse it bottom-up as usual, and for every set of al-
ternative subtranslations, we select the one with the
highest score. But here a rough approximation lurks,
because we need to calculate B on the nodes of the
forest, but B does not have the optimal substructure
property, i.e., the optimal score of a parent node can-
not necessarily be calculated from the optimal scores
of its children. Nevertheless, we find that this rescor-
ing method is good enough for generating high-B???
oracle translations and low-B??? negative examples.
2.5 Parallelization
One convenient property of MERT is that it is em-
barrassingly parallel: we decode the entire tuning set
sending different sentences to different processors,
and during optimization of feature weights, differ-
ent random restarts can be sent to different proces-
sors. In order to make MIRA comparable in effi-
ciency to MERT, we must parallelize it. But with
an online learning algorithm, parallelization requires
a little more coordination. We run MIRA on each
227
processor simultaneously, with each maintaining its
own weight vector. A master process distributes dif-
ferent sentences from the tuning set to each of the
processors; when each processor finishes decoding
a sentence, it transmits the resulting hypotheses,
with their losses, to all the other processors and re-
ceives any hypotheses waiting from other proces-
sors. Those hypotheses were generated from differ-
ent weight vectors, but can still provide useful in-
formation. The sets of hypotheses thus collected are
then processed as one batch. When the whole train-
ing process is finished, we simply average all the
weight vectors from all the processors.
Having described our training algorithm, which
includes several practical improvements to Watan-
abe et al?s usage of MIRA, we proceed in the re-
mainder of the paper to demonstrate the utility of the
our training algorithm on models with large numbers
of structurally sensitive features.
3 Soft syntactic constraints
The first features we explore are based on a line
of research introduced by Chiang (2005) and im-
proved on by Marton and Resnik (2008). A hi-
erarchical phrase-based translation model is based
on synchronous context-free grammar, but does not
normally use any syntactic information derived from
linguistic knowledge or treebank data: it uses trans-
lation rules that span any string of words in the input
sentence, without regard for parser-defined syntac-
tic constituency boundaries. Chiang (2005) exper-
imented with a constituency feature that rewarded
rules whose source language side exactly spans a
syntactic constituent according to the output of an
external source-language parser. This feature can
be viewed as a soft syntactic constraint: it biases
the model toward translations that respect syntactic
structure, but does not force it to use them. However,
this more syntactically aware model, when tested in
Chinese-English translation, did not improve trans-
lation performance.
Recently, Marton and Resnik (2008) revisited
the idea of constituency features, and succeeded in
showing that finer-grained soft syntactic constraints
yield substantial improvements in B??? score for
both Chinese-English and Arabic-English transla-
tion. In addition to adding separate features for dif-
ferent syntactic nonterminals, they introduced a new
type of constraint that penalizes rules when the
source language side crosses the boundaries of a
source syntactic constituent, as opposed to simply
rewarding rules when they are consistent with the
source-language parse tree.
Marton and Resnik optimized their features?
weights using MERT. But since MERT does not
scale well to large numbers of feature weights, they
were forced to test individual features and manu-
ally selected feature combinations each in a sepa-
rate model. Although they showed gains in trans-
lation performance for several such models, many
larger, potentially better feature combinations re-
mained unexplored. Moreover, the best-performing
feature subset was different for the two language
pairs, suggesting that this labor-intensive feature se-
lection process would have to be repeated for each
new language pair.
Here, we use MIRA to optimize Marton and
Resnik?s finer-grained single-category features all at
once. We define below two sets of features, a coarse-
grained class that combines several constituency cat-
egories, and a fine-grained class that puts different
categories into different features. Both kinds of fea-
tures were used by Marton and Resnik, but only a
few at a time. Crucially, our training algorithm pro-
vides the ability to train all the fine-grained features,
a total of 34 feature weights, simultaneously.
Coarse-grained features As the basis for coarse-
grained syntactic features, we selected the following
nonterminal labels based on their frequency in the
tuning data, whether they frequently cover a span
of more than one word, and whether they repre-
sent linguistically relevant constituents: NP, PP, S,
VP, SBAR, ADJP, ADVP, and QP. We define two
new features, one which fires when a rule?s source
side span in the input sentence matches any of the
above-mentioned labels in the input parse, and an-
other which fires when a rule?s source side span
crosses a boundary of one of these labels (e.g., its
source side span only partially covers the words in
a VP subtree, and it also covers some or all or the
words outside the VP subtree). These two features
are equivalent to Marton and Resnik?s XP= and XP+
feature combinations, respectively.
228
Fine-grained features We selected the following
nonterminal labels that appear more than 100 times
in the tuning data: NP, PP, S, VP, SBAR, ADJP,
WHNP, PRT, ADVP, PRN, and QP. The labels that
were excluded were parts of speech, nonconstituent
labels like FRAG, or labels that occurred only two
or three times. For each of these labels X, we added
a separate feature that fires when a rule?s source side
span in the input sentence matches X, and a second
feature that fires when a span crosses a boundary of
X. These features are similar to Marton and Resnik?s
X= and X+, except that our set includes features for
WHNP, PRT, and PRN.
4 Structural distortion features
In addition to parser-based syntactic constraints,
which were introduced in prior work, we introduce
a completely new set of features aimed at improv-
ing the modeling of reordering within Hiero. Again,
the feature definition gives rise to a larger number of
features than one would expect to train successfully
using MERT.
In a phrase-based model, reordering is per-
formed both within phrase pairs and by the phrase-
reordering model. Both mechanisms are able to
learn that longer-distance reorderings are more
costly than shorter-distance reorderings: phrase
pairs, because phrases that involve more extreme re-
orderings will (presumably) have a lower count in
the data, and phrase reordering, because models are
usually explicitly dependent on distance.
By contrast, in a hierarchical model, all reordering
is performed by a single mechanism, the rules of the
grammar. In some cases, the model will be able to
learn a preference for shorter-distance reorderings,
as in a phrase-based system, but in the case of a word
being reordered across a nonterminal, or two non-
terminals being reordered, there is no dependence in
the model on the size of the nonterminal or nonter-
minals involved in reordering.
So, for example, if we have rules
X? (il dit X1, he said X1) (12)
X? (il dit X1,X1 he said) (13)
we might expect that rule (12) is more common in
general, but that rule (13) becomes more and more
?
?
?
?
?
?
?
?
Figure 2: Classifying nonterminal occurrences for the
structural distortion model.
rare as X1 gets larger. The default Hiero features
have no way to learn this.
To address this defect, we can classify every
nonterminal pair occurring on the right-hand side
of each grammar rule as ?reordered? or ?not re-
ordered?, that is, whether it intersects any other word
alignment link or nonterminal pair (see Figure 2).
We then define coarse- and fine-grained versions of
the structural distortion model.
Coarse-grained features Let R be a binary-
valued random variable that indicates whether a non-
terminal occurrence is reordered, and let S be an
integer-valued random variable that indicates how
many source words are spanned by the nonterminal
occurrence. We can estimate P(R | S ) via relative-
frequency estimation from the rules as they are ex-
tracted from the parallel text, and incorporate this
probability as a new feature of the model.
Fine-grained features A difficulty with the
coarse-grained reordering features is that the gram-
mar extraction process finds overlapping rules in the
training data and might not give a sensible proba-
bility estimate; moreover, reordering statistics from
the training data might not carry over perfectly into
the translation task (in particular, the training data
may have some very freely-reordering translations
that one might want to avoid replicating in transla-
tion). As an alternative, we introduce a fine-grained
version of our distortion model that can be trained
directly in the translation task as follows: define
229
a separate binary feature for each value of (R, S ),
where R is as above and S ? {?, 1, . . . , 9,?10} and ?
means any size. For example, if a nonterminal with
span 11 has its contents reordered, then the features
(true,?10) and (true, ?) would both fire. Grouping
all sizes of 10 or more into a single feature is de-
signed to avoid overfitting.
Again, using MIRA makes it practical to train
with the full fine-grained feature set?coincidentally
also a total of 34 features.
5 Experiment and results
We now describe our experiments to test MIRA and
our features, the soft-syntactic constraints and the
structural distortion features, on an Arabic-English
translation task. It is worth noting that this exper-
imentation is on a larger scale than Watanabe et
al.?s (2007), and considerably larger than Marton
and Resnik?s (2008).
5.1 Experimental setup
The baseline model was Hiero with the following
baseline features (Chiang, 2005; Chiang, 2007):
? two language models
? phrase translation probabilities p( f | e) and
p(e | f )
? lexical weighting in both directions (Koehn et
al., 2003)
? word penalty
? penalties for:
? automatically extracted rules
? identity rules (translating a word into it-
self)
? two classes of number/name translation
rules
? glue rules
The probability features are base-100 log-
probabilities.
The rules were extracted from all the allow-
able parallel text from the NIST 2008 evalua-
tion (152+175 million words of Arabic+English),
aligned by IBM Model 4 using GIZA++ (union of
both directions). Hierarchical rules were extracted
from the most in-domain corpora (4.2+5.4 million
words) and phrases were extracted from the remain-
der. We trained the coarse-grained distortion model
on 10,000 sentences of the training data.
Two language models were trained, one on data
similar to the English side of the parallel text and
one on 2 billion words of English. Both were 5-
gram models with modified Kneser-Ney smoothing,
lossily compressed using a perfect-hashing scheme
similar to that of Talbot and Brants (2008) but using
minimal perfect hashing (Botelho et al, 2005).
We partitioned the documents of the NIST 2004
(newswire) and 2005 Arabic-English evaluation data
into a tuning set (1178 sentences) and a develop-
ment set (1298 sentences). The test data was the
NIST 2006 Arabic-English evaluation data (NIST
part, newswire and newsgroups, 1529 sentences).
To obtain syntactic parses for this data, we tok-
enized it according to the Arabic Treebank standard
using AMIRA (Diab et al, 2004), parsed it with
the Stanford parser (Klein and Manning, 2003), and
then forced the trees back into the MT system?s tok-
enization.1
We ran both MERT and MIRA on the tuning
set using 20 parallel processors. We stopped MERT
when the score on the tuning set stopped increas-
ing, as is common practice, and for MIRA, we used
the development set to decide when to stop train-
ing.2 In our runs, MERT took an average of 9 passes
through the tuning set and MIRA took an average of
8 passes. (For comparison, Watanabe et al report de-
coding their tuning data of 663 sentences 80 times.)
5.2 Results
Table 1 shows the results of our experiments with
the training methods and features described above.
All significance testing was performed against the
first line (MERT baseline) using paired bootstrap re-
sampling (Koehn, 2004).
First of all, we find that MIRA is competitive with
MERT when both use the baseline feature set. In-
1The only notable consequence this had for our experimen-
tation is that proclitic Arabic prepositions were fused onto the
first word of their NP object, so that the PP and NP brackets
were coextensive.
2We chose this policy for MIRA to avoid overfitting. How-
ever, we could have used the tuning set for this purpose, just as
with MERT: in none of our runs would this change have made
more than a 0.2 B??? difference on the development set.
230
Dev NIST 06 (NIST part)
Train Features # nw nw ng nw+ng
MERT baseline 12 52.0 50.5 32.4 44.6
syntax (coarse) 14 52.2 50.9 33.0+ 45.0+
syntax (fine) 34 52.1 50.4 33.5++ 44.8
distortion (coarse) 13 52.3 51.3+ 34.3++ 45.8++
distortion (fine) 34 52.0 50.9 34.5++ 45.5++
MIRA baseline 12 52.0 49.8? 34.2++ 45.3++
syntax (fine) 34 53.1++ 51.3+ 34.5++ 46.4++
distortion (fine) 34 53.3++ 51.5++ 34.7++ 46.7++
distortion+syntax (fine) 56 53.6++ 52.0++ 35.0++ 47.2++
Table 1: Comparison of MERT and MIRA on various feature sets. Key: # = number of features; nw = newswire, ng =
newsgroups; + or ++ = significantly better than MERT baseline (p < 0.05 or p < 0.01, respectively), ? = significantly
worse than MERT baseline (p < 0.05).
deed, the MIRA system scores significantly higher
on the test set; but if we break the test set down by
genre, we see that the MIRA system does slightly
worse on newswire and better on newsgroups. (This
is largely attributable to the fact that the MIRA trans-
lations tend to be longer than the MERT transla-
tions, and the newsgroup references are also rela-
tively longer than the newswire references.)
When we add more features to the model, the two
training methods diverge more sharply. When train-
ing with MERT, the coarse-grained pair of syntax
features yields a small improvement, but the fine-
grained syntax features do not yield any further im-
provement. By contrast, when the fine-grained fea-
tures are trained using MIRA, they yield substan-
tial improvements. We observe similar behavior for
the structural distortion features: MERT is not able
to take advantage of the finer-grained features, but
MIRA is. Finally, using MIRA to combine both
classes of features, 56 in all, produces the largest im-
provement, 2.6 B??? points over the MERT baseline
on the full test set.
We also tested some of the differences between
our training method and Watanabe et al?s (2007); the
results are shown in Table 2. Compared with local
updating (line 2), our method of selecting the ora-
cle translation and negative examples does better by
0.5 B??? points on the development data. Using loss-
augmented inference to add negative examples to lo-
cal updating (line 3) does not appear to help. Never-
theless, the negative examples are important: for if
Setting Dev
full 53.6
local updating, no LAI 53.1?
local updating, LAI 53.0??
? = 0.5 oracle, no LAI failed
no sharing of updates 53.1??
Table 2: Effect of removing various improvements in
learning method. Key: ? or ?? = significantly worse than
full system (p < 0.05 or p < 0.01, respectively); LAI =
loss-augmented inference for additional negative exam-
ples.
we use our method for selecting the oracle transla-
tion without the additional negative examples (line
4), the algorithm fails, generating very long transla-
tions and unable to find a weight setting to shorten
them. It appears, then, that the additional negative
examples enable the algorithm to reliably learn from
the enhanced oracle translations.
Finally, we compared our parallelization method
against a simpler method in which all processors
learn independently and their weight vectors are all
averaged together (line 5). We see that sharing in-
formation among the processors makes a significant
difference.
6 Conclusions
In this paper, we have brought together two existing
lines of work: the training method of Watanabe et al
(2007), and the models of Chiang (2005) and Marton
231
and Resnik (2008). Watanabe et al?s work showed
that large-margin training with MIRA can be made
feasible for state-of-the-art MT systems by using a
manageable tuning set; we have demonstrated that
parallel processing and exploiting more of the parse
forest improves MIRA?s performance and that, even
using the same set of features, MIRA?s performance
compares favorably to MERT in terms of both trans-
lation quality and computational cost.
Marton and Resnik (2008) showed that it is pos-
sible to improve translation in a data-driven frame-
work by incorporating source-side syntactic analy-
sis in the form of soft syntactic constraints. This
work joins a growing body of work demonstrating
the utility of syntactic information in statistical MT.
In the area of source-side syntax, recent research
has continued to improve tree-to-string translation
models, soften the constraints of the input tree in
various ways (Mi et al, 2008; Zhang et al, 2008),
and extend phrase-based translation with source-
side soft syntactic constraints (Cherry, 2008). All
this work shows strong promise, but Marton and
Resnik?s soft syntactic constraint approach is par-
ticularly appealing because it can be used unobtru-
sively with any hierarchically-structured translation
model. Here, we have shown that using MIRA to
weight all the constraints at once removes the cru-
cial drawback of the approach, the problem of fea-
ture selection.
Finally, we have introduced novel structural dis-
tortion features to fill a notable gap in the hierar-
chical phrase-based approach. By capturing how re-
ordering depends on constituent length, these fea-
tures improve translation quality significantly. In
sum, we have shown that removing the bottleneck
of MERT opens the door to many possibilities for
better translation.
Acknowledgments
Thanks to Michael Bloodgood for performing ini-
tial simulations of parallelized perceptron training.
Thanks also to John DeNero, Kevin Knight, Daniel
Marcu, and Fei Sha for valuable discussions and
suggestions. This research was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies and HR0011-06-02-001
under subcontract to IBM.
References
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In Proc. MT
Summit XI.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical ma-
chine translation. In Proc. ACL-08: HLT.
Fabiano C. Botelho, Yoshiharu Kohayakawa, and Nivio
Ziviani. 2005. A practical minimal perfect hashing
method. In 4th International Workshop on Efficient
and Experimental Algorithms (WEA05).
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proc. Third Workshop on Statis-
tical Machine Translation.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. ACL-08: HLT.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP 2002.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proc. HLT/NAACL 2004.
Companion volume.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT us-
ing efficient B??? oracle computation. In Proc. 2007
Workshop on Syntax and Structure in Statistical Trans-
lation.
Kevin Duh and Katrin Kirchoff. 2008. Beyond log-linear
models: Boosted minimum error rate training for n-
best re-ranking. In Proc. ACL-08: HLT, Short Papers.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Dan Klein and Chris D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. In Advances in Neural Information Processing
Systems 15 (NIPS 2002).
232
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proc. EMNLP
2008. This volume.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. COLING-ACL
2006.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proc. COLING 2004.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ACL-08: HLT.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc. ACL 2005.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL-08: HLT.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ACL 2002.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
Bernhard Scho?lkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 195?208. MIT
Press.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In
Proc. COLING/ACL 2006, Poster Sessions.
David Talbot and Thorsten Brants. 2008. Random-
ized language models via perfect hash functions. In
Proc. ACL-08: HLT.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. EMNLP 2004, pages 1?8.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proc. ICML
2005.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. COLING-ACL 2006.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learning for
natural language parsing and translation. In Advances
in Neural Information Processing Systems 19 (NIPS
2006).
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP 2007.
Dekai Wu. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. 34th Annual
Meeting of the Association for Computational Linguis-
tics, pages 152?158.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT.
233
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610?619,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Decomposability of Translation Metrics
for Improved Evaluation and Efficient Algorithms
David Chiang and Steve DeNeefe
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
{chiang,sdeneefe}@isi.edu
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
Law Link
Singapore 117590
{chanys,nght}@comp.nus.edu.sg
Abstract
B??? is the de facto standard for evaluation
and development of statistical machine trans-
lation systems. We describe three real-world
situations involving comparisons between dif-
ferent versions of the same systems where one
can obtain improvements in B??? scores that
are questionable or even absurd. These situ-
ations arise because B??? lacks the property
of decomposability, a property which is also
computationally convenient for various appli-
cations. We propose a very conservative modi-
fication to B??? and a cross between B??? and
word error rate that address these issues while
improving correlation with human judgments.
1 Introduction
B??? (Papineni et al, 2002) was one of the first au-
tomatic evaluation metrics for machine translation
(MT), and despite being challenged by a number
of alternative metrics (Melamed et al, 2003; Baner-
jee and Lavie, 2005; Snover et al, 2006; Chan and
Ng, 2008), it remains the standard in the statistical
MT literature. Callison-Burch et al (2006) have sub-
jected B??? to a searching criticism, with two real-
world case studies of significant failures of corre-
lation between B??? and human adequacy/fluency
judgments. Both cases involve comparisons between
statistical MT systems and other translation meth-
ods (human post-editing and a rule-based MT sys-
tem), and they recommend that the use of B??? be
restricted to comparisons between related systems or
different versions of the same systems. In B????s de-
fense, comparisons between different versions of the
same system were exactly what B??? was designed
for.
However, we show that even in such situations,
difficulties with B??? can arise. We illustrate three
ways that properties of B??? can be exploited to
yield improvements that are questionable or even
absurd. All of these scenarios arose in actual prac-
tice and involve comparisons between different ver-
sions of the same statistical MT systems. They can
be traced to the fact that B??? is not decomposable
at the sentence level: that is, it lacks the property
that improving a sentence in a test set leads to an
increase in overall score, and degrading a sentence
leads to a decrease in the overall score. This prop-
erty is not only intuitive, but also computationally
convenient for various applications such as transla-
tion reranking and discriminative training. We pro-
pose a minimal modification to B??? that reduces
its nondecomposability, as well as a cross between
B??? and word error rate (WER) that is decompos-
able down to the subsentential level (in a sense to be
made more precise below). Both metrics correct the
observed problems and correlate with human judg-
ments better than B???.
2 The B??? metric
Let gk(w) be the multiset of all k-grams of a sentence
w. We are given a sequence of candidate translations
c to be scored against a set of sequences of reference
translations, {r j} = r1, . . . , rR:
c = c1, c2, c3, . . . , cN
r1 = r11, r
1
2, r
1
3, . . . , r
1
N
...
rR = rR1 , r
R
2 , r
R
3 , . . . , r
R
N
610
Then the B??? score of c is defined to be
B???(c, {r j}) =
4?
k=1
prk(c, {r
j})
1
4 ? bp(c, {r j}) (1)
where1
prk(c, {r
j}) =
?
i
????gk(ci) ?
?
j gk(r
j
i )
????
?
i |gk(ci)|
(2)
is the k-gram precision of c with respect to {r j}, and
bp(c, r), known as the brevity penalty, is defined as
follows. Let ?(x) = exp(1 ? 1/x). In the case of a
single reference r,
bp(c, r) = ?
(
min
{
1,
?
i |ci|
?
i |ri|
})
(3)
In the multiple-reference case, the length |ri| is re-
placed with an effective reference length, which can
be calculated in several ways.
? In the original definition (Papineni et al, 2002),
it is the length of the reference sentence whose
length is closest to the test sentence.
? In the NIST definition, it is the length of the
shortest reference sentence.
? A third possibility would be to take the average
length of the reference sentences.
The purpose of the brevity penalty is to prevent
a system from generating very short but precise
translations, and the definition of effective reference
length impacts how strong the penalty is. The NIST
definition is the most tolerant of short translations
and becomes more tolerant with more reference sen-
tences. The original definition is less tolerant but
has the counterintuitive property that decreasing the
length of a test sentence can eliminate the brevity
penalty. Using the average reference length seems
attractive but has the counterintuitive property that
1We use the following definitions about multisets: if X is a
multiset, let #X(a) be the number of times a occurs in X. Then:
|X| ?
?
a
#X(a)
#X?Y (a) ? min{#X(a), #Y (a)}
#X?Y (a) ? max{#X(a), #Y (a)}
an exact match with one of the references may not
get a 100% score. Throughout this paper we use the
NIST definition, as it is currently the definition most
used in the literature and in evaluations.
The brevity penalty can also be seen as a stand-
in for recall. The fraction
?
i |ci |?
i |ri |
in the definition of
the brevity penalty (3) indeed resembles a weak re-
call score in which every guessed item counts as a
match. However, with recall, the per-sentence score
|ci |
|ri |
would never exceed unity, but with the brevity
penalty, it can. This means that if a system generates
a long translation for one sentence, it can generate
a short translation for another sentence without fac-
ing a penalty. This is a serious weakness in the B???
metric, as we demonstrate below using three scenar-
ios, encountered in actual practice.
3 Exploiting the B??? metric
3.1 The sign test
We are aware of two methods that have been pro-
posed for significance testing with B???: bootstrap
resampling (Koehn, 2004b; Zhang et al, 2004) and
the sign test (Collins et al, 2005). In bootstrap re-
sampling, we sample with replacement from the test
set to synthesize a large number of test sets, and
then we compare the performance of two systems on
those synthetic test sets to see whether one is better
95% (or 99%) of the time. But Collins et al (2005)
note that it is not clear whether the conditions re-
quired by bootstrap resampling are met in the case of
B???, and recommend the sign test instead. Suppose
we want to determine whether a set of outputs c from
a test system is better or worse than a set of baseline
outputs b. The sign test requires a function f (bi, ci)
that indicates whether ci is a better, worse, or same-
quality translation relative to bi. However, because
B??? is not defined on single sentences, Collins et
al. use an approximation: for each i, form a compos-
ite set of outputs b? = {b1, . . . , bi?1, ci, bi+1, . . . , bN},
and compare the B??? scores of b and b?.
The goodness of this approximation depends on
to what extent the comparison between b and b? is
dependent only on bi and ci, and independent of the
other sentences. However, B??? scores are highly
context-dependent: for example, if the sentences in
b are on average  words longer than the reference
sentences, then ci can be as short as (N ? 1) words
611
shorter than ri without incurring the brevity penalty.
Moreover, since the ci are substituted in one at a
time, we can do this for all of the ci. Hence, c could
have a disastrously low B??? score (because of the
brevity penalty) yet be found by the sign test to be
significantly better than the baseline.
We have encountered this situation in practice:
two versions of the same system with B??? scores of
29.6 (length ratio 1.02) and 29.3 (length ratio 0.97),
where the sign test finds the second system to be sig-
nificantly better than the first (and the first system
significantly better than the second). Clearly, in or-
der for a significance test to be sensible, it should not
contradict the observed scores, and should certainly
not contradict itself. In the rest of this paper, except
where indicated, all significance tests are performed
using bootstrap resampling.
3.2 Genre-specific training
For several years, much statistical MT research has
focused on translating newswire documents. One
likely reason is that the DARPA TIDES program
used newswire documents for evaluation for several
years. But more recent evaluations have included
other genres such as weblogs and conversation. The
conventional wisdom has been that if one uses a
single statistical translation system to translate text
from several different genres, it may perform poorly,
and it is better to use several systems optimized sep-
arately for each genre.
However, if our task is to translate documents
from multiple known genres, but they are evaluated
together, the B??? metric allows us to use that fact
to our advantage. To understand how, notice that
our system has an optimal number of words that it
should generate for the entire corpus: too few and it
will be penalized by B????s brevity penalty, and too
many increases the risk of additional non-matching
k-grams. But these words can be distributed among
the sentences (and genres) in any way we like. In-
stead of translating sentences from each genre with
the best genre-specific systems possible, we can
generate longer outputs for the genre we have more
confidence in, while generating shorter outputs for
the harder genre. This strategy will have mediocre
performance on each individual genre (according to
both intuition and B???), yet will receive a higher
B??? score on the combined test set than the com-
bined systems optimized for each genre.
In fact, knowing which sentence is in which genre
is not even always necessary. In one recent task,
we translated documents from two different genres,
without knowing the genre of any given sentence.
The easier genre, newswire, also tended to have
shorter reference sentences (relative to the source
sentences) than the harder genre, weblogs. For ex-
ample, in one dataset, the newswire reference sets
had between 1.3 and 1.37 English words per Ara-
bic word, but the weblog reference set had 1.52 En-
glish words per Arabic word. Thus, a system that
is uniformly verbose across both genres will appor-
tion more of its output to newswire than to weblogs,
serendipitously leading to a higher score. This phe-
nomenon has subsequently been observed by Och
(2008) as well.
We trained three Arabic-English syntax-based
statistical MT systems (Galley et al, 2004; Galley
et al, 2006) using max-B??? training (Och, 2003):
one on a newswire development set, one on a we-
blog development set, and one on a combined devel-
opment set containing documents from both genres.
We then translated a new mixed-genre test set in two
ways: (1) each document with its appropriate genre-
specific system, and (2) all documents with the sys-
tem trained on the combined (mixed-genre) devel-
opment set. In Table 3, we report the results of both
approaches on the entire test dataset as well as the
portion of the test dataset in each genre, for both the
genre-specific and mixed-genre trainings.
The genre-specific systems each outperform the
mixed system on their own genre as expected, but
when the same results are combined, the mixed sys-
tem?s output is a full B??? point higher than the com-
bination of the genre-specific systems. This is be-
cause the mixed system produces outputs that have
about 1.35 English words per Arabic word on av-
erage: longer than the shortest newswire references,
but shorter than the weblog references. The mixed
system does worse on each genre but better on the
combined test set, whereas, according to intuition,
a system that does worse on the two subsets should
also do worse on the combined test set.
3.3 Word deletion
A third way to take advantage of the B??? metric
is to permit an MT system to delete arbitrary words
612
in the input sentence. We can do this by introduc-
ing new phrases or rules into the system that match
words in the input sentence but generate no output;
to these rules we attach a feature whose weight is
tuned during max-B??? training. Such rules have
been in use for some time but were only recently
discussed by Li et al (2008).
When we add word-deletion rules to our MT sys-
tem, we find that the B??? increases significantly
(Table 6, line 2). Figure 1 shows some examples
of deletion in Chinese-English translation. The first
sentence has a proper name,?<[[/maigesaisai
?Magsaysay?, which has been mistokenized into four
tokens. The baseline system attempts to translate the
first two phonetic characters as ?wheat Georgia,?
whereas the other system simply deletes them. On
the other hand, the second sentence shows how word
deletion can sacrifice adequacy for the sake of flu-
ency, and the third sentence shows that sometimes
word deletion removes words that could have been
translated well (as seen in the baseline translation).
Does B??? reward word deletion fairly? We note
two reasons why word deletion might be desirable.
First, some function words should truly be deleted:
for example, the Chinese particle?/de and Chinese
measure words often have no counterpart in English
(Li et al, 2008). Second, even content word deletion
might be helpful if it allows a more fluent translation
to be assembled from the remnants. We observe that
in the above experiment, word deletion caused the
absolute number of k-gram matches, and not just k-
gram precision, to increase for all 1 ? k ? 4.
Human evaluation is needed to conclusively de-
termine whether B??? rewards deletion fairly. But to
control for these potentially positive effects of dele-
tion, we tested a sentence-deletion system, which
is the same as the word-deletion system but con-
strained to delete all of the words in a sentence or
none of them. This system (Table 6, line 3) deleted
8?10% of its input and yielded a B??? score with
no significant decrease (p ? 0.05) from the base-
line system?s. Given that our model treats sentences
independently, so that it cannot move information
from one sentence to another, we claim that dele-
tion of nearly 10% of the input is a grave translation
deficiency, yet B??? is insensitive to it.
What does this tell us about word deletion? While
acknowledging that some word deletions can im-
prove translation quality, we suggest in addition that
because word deletion provides a way for the system
to translate the test set selectively, a behavior which
we have shown that B??? is insensitive to, part of
the score increase due to word deletion is likely an
artifact of B???.
4 Other metrics
Are other metrics susceptible to the same problems
as the B??? metric? In this section we examine sev-
eral other popular metrics for these problems, pro-
pose two of our own, and discuss some desirable
characteristics for any new MT evaluation metric.
4.1 Previous metrics
We ran a suite of other metrics on the above problem
cases to see whether they were affected. In none of
these cases did we repeat minimum-error-rate train-
ing; all these systems were trained using max-B???.
The metrics we tested were:
? METEOR (Banerjee and Lavie, 2005), version
0.6, using the exact, Porter-stemmer, andWord-
Net synonmy stages, and the optimized param-
eters ? = 0.81, ? = 0.83, ? = 0.28 as reported
in (Lavie and Agarwal, 2007).
? GTM (Melamed et al, 2003), version 1.4, with
default settings, except e = 1.2, following the
WMT 2007 shared task (Callison-Burch et al,
2007).
? M??S?? (Chan and Ng, 2008), more specifi-
cally M??S??n, which skips the dependency re-
lations.
On the sign test (Table 2), all metrics found sig-
nificant differences consistent with the difference in
score between the two systems. The problem related
to genre-specific training does not seem to affect the
other metrics (see Table 4), but they still manifest
the unintuitive result that genre-specific training is
sometimes worse than mixed-genre training. Finally,
all metrics but GTM disfavored both word deletion
and sentence deletion (Table 7).
4.2 Strict brevity penalty
A very conservative way of modifying the B??? met-
ric to combat the effects described above is to im-
613
(a) source 9]Parsing Arabic Dialects
David Chiang?, Mona Diab?, Nizar Habash?, Owen Rambow?, Safiullah Shareef?
? ISI, University of Southern California
? CCLS, Columbia University
? The Johns Hopkins University
chiang@isi.edu, {mdiab,habash,rambow}@cs.columbia.edu, safi@jhu.edu
Abstract
The Arabic language is a collection of
spoken dialects with important phonolog-
ical, morphological, lexical, and syntac-
tic differences, along with a standard writ-
ten language, Modern Standard Arabic
(MSA). Since the spoken dialects are not
officially written, it is very costly to obtain
adequate corpora to use for training dialect
NLP tools such as parsers. In this paper,
we address the problem of parsing tran-
scribed spoken Levantine Arabic (LA).We
do not assume the existence of any anno-
tated LA corpus (except for development
and testing), nor of a parallel corpus LA-
MSA. Instead, we use explicit knowledge
about the relation between LA and MSA.
1 Introduction: Arabic Dialects
The Arabic language is a collection of spoken
dialects and a standard written language.1 The
dialects show phonological, morphological, lexi-
cal, and syntactic differences comparable to those
among the Romance languages. The standard
written language is the same throughout the Arab
world: Modern Standard Arabic (MSA). MSA is
also used in some scripted spoken communica-
tion (news casts, parliamentary debates). MSA is
based on Classical Arabic and is not a native lan-
guage of any Arabic speaking people, i.e., children
do not learn it from their parents but in school.
1This paper is based on work done at the 2005 Johns Hop-
kins Summer Workshop, which was partially supported by
the National Science Foundation under Grant No. 0121285.
Diab, Habash, and Rambow were supported for additional
work by DARPA contract HR0011-06-C-0023 under the
GALE program. We wish to thank audiences at JHU for their
useful feedback. The authors are listed in alphabetical order.
Most native speakers of Arabic are unable to pro-
duce sustained spontaneous MSA. Dialects vary
not only along a geographical continuum but also
with other sociolinguistic variables such as the ur-
ban/rural/Bedouin dimension.
The multidialectal situation has important neg-
ative consequences for Arabic natural language
processing (NLP): since the spoken dialects are
not officially written and do not have standard or-
thography, it is very costly to obtain adequate cor-
pora, even unannotated corpora, to use for train-
ing NLP tools such as parsers. Furthermore, there
are almost no parallel corpora involving one di-
alect and MSA.
In this paper, we address the problem of parsing
transcribed spoken Levantine Arabic (LA), which
we use as a representative example of the Arabic
dialects.2 Our work is based on the assumption
that it is easier to manually create new resources
that relate LA to MSA than it is to manually cre-
ate syntactically annotated corpora in LA. Our ap-
proaches do not assume the existence of any anno-
tated LA corpus (except for development and test-
ing), nor of a parallel LA-MSA corpus. Instead,
we assume we have at our disposal a lexicon that
relates LA lexemes to MSA lexemes, and knowl-
edge about the morphological and syntactic differ-
ences between LA and MSA. For a single dialect,
it may be argued that it is easier to create corpora
than to encode all this knowledge explicitly. In
response, we claim that because the dialects show
important similarities, it will be easier to reuse and
modify explicit linguistic resources for a new di-
alect, than to create a new corpus for it. The goal
of this paper is to show that leveraging LA/MSA
2We exclude from this study part-of-speech (POS) tag-
ging and LA/MSA lexicon induction. See (Rambow et al,
2005) for these issues, as well as for more details on parsing.
369
resources is feasible; we do not provide a demon-
stration of cost-effectiveness.
The paper is organized as follows. After dis-
cussing related work and available corpora, we
present linguistic issues in LA and MSA (Sec-
tion 4). We then proceed to discuss three ap-
proaches: sentence transduction, in which the LA
sentence to be parsed is turned into an MSA sen-
tence and then parsed with an MSA parser (Sec-
tion 5); treebank transduction, in which the MSA
treebank is turned into an LA treebank (Section 6);
and grammar transduction, in which an MSA
grammar is turned into an LA grammar which is
then used for parsing LA (Section 7). We summa-
rize and discuss the results in Section 8.
2 Related Work
There has been a fair amount of interest in parsing
one language using another language, see for ex-
ample (Smith and Smith, 2004; Hwa et al, 2004)
for recent work. Much of this work uses synchro-
nized formalisms as do we in the grammar trans-
duction approach. However, these approaches rely
on parallel corpora. For MSA and its dialects,
there are no naturally occurring parallel corpora. It
is this fact that has led us to investigate the use of
explicit linguistic knowledge to complement ma-
chine learning. We refer to additional relevant
work in the appropriate sections.
3 Linguistic Resources
We use the MSA treebanks 1, 2 and 3 (ATB) from
the LDC (Maamouri et al, 2004). We split the cor-
pus into 10% development data, 80% training data
and 10% test data all respecting document bound-
aries. The training data (ATB-Train) comprises
17,617 sentences and 588,244 tokens.
The Levantine treebank LATB (Maamouri et
al., 2006) comprises 33,000 words of treebanked
conversational telephone transcripts collected as
part of the LDC CALL HOME project. The tree-
banked section is primarily in the Jordanian sub-
dialect of LA. The data is annotated by the LDC
for speech effects such as disfluencies and repairs.
We removed the speech effects, rendering the data
more text-like. The orthography and syntactic
analysis chosen by the LDC for LA closely fol-
low previous choices for MSA, see Figure 1 for
two examples. The LATB is used exclusively for
development and testing, not for training. We
split the data in half respecting document bound-
aries. The resulting development data comprises
1928 sentences and 11151 tokens (DEV). The
test data comprises 2051 sentences and 10,644 to-
kens (TEST). For all the experiments, we use the
non-vocalized (undiacritized) version of both tree-
banks, as well as the collapsed POS tag set pro-
vided by the LDC for MSA and LA.
Two lexicons were created: a small lexicon
comprising 321 LA/MSA word form pairs cov-
ering LA closed-class words and a few frequent
open-class words; and a big lexicon which con-
tains the small lexicon and an additional 1,560
LA/MSA word form pairs. We assign to the map-
pings in the two lexicons both uniform probabil-
ities and biased probabilities using Expectation
Maximization (EM; see (Rambow et al, 2005)
for details of the use of EM). We thus have four
different lexicons: Small lexicon with uniform
probabilities (SLXUN); Small Lexicon with EM-
based probabilities (SLXEM); Big Lexicon with
uniform probabilities (BLXUN); and Big Lexicon
with EM-based probabilities (BLXEM).
4 Linguistic Facts
We illustrate the differences between LA and
MSA using an example3:
(1) a.   	
   ffHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218?226,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
11,001 New Features for Statistical Machine Translation?
David Chiang and Kevin Knight
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
Wei Wang
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292 USA
Abstract
We use the Margin Infused Relaxed Algo-
rithm of Crammer et al to add a large num-
ber of new features to two machine transla-
tion systems: the Hiero hierarchical phrase-
based translation system and our syntax-based
translation system. On a large-scale Chinese-
English translation task, we obtain statistically
significant improvements of +1.5 B??? and
+1.1 B???, respectively. We analyze the im-
pact of the new features and the performance
of the learning algorithm.
1 Introduction
What linguistic features can improve statistical ma-
chine translation (MT)? This is a fundamental ques-
tion for the discipline, particularly as it pertains to
improving the best systems we have. Further:
? Do syntax-based translation systems have
unique and effective levers to pull when design-
ing new features?
? Can large numbers of feature weights be
learned efficiently and stably on modest
amounts of data?
In this paper, we address these questions by exper-
imenting with a large number of new features. We
add more than 250 features to improve a syntax-
based MT system?already the highest-scoring sin-
gle system in the NIST 2008 Chinese-English
common-data track?by +1.1 B???. We also add
more than 10,000 features to Hiero (Chiang, 2005)
and obtain a +1.5 B??? improvement.
?This research was supported in part by DARPA contract
HR0011-06-C-0022 under subcontract to BBN Technologies.
Many of the new features use syntactic informa-
tion, and in particular depend on information that
is available only inside a syntax-based translation
model. Thus they widen the advantage that syntax-
based models have over other types of models.
The models are trained using the Margin Infused
Relaxed Algorithm or MIRA (Crammer et al, 2006)
instead of the standard minimum-error-rate training
or MERT algorithm (Och, 2003). Our results add
to a growing body of evidence (Watanabe et al,
2007; Chiang et al, 2008) that MIRA is preferable to
MERT across languages and systems, even for very
large-scale tasks.
2 Related Work
The work of Och et al(2004) is perhaps the best-
known study of new features and their impact on
translation quality. However, it had a few shortcom-
ings. First, it used the features for reranking n-best
lists of translations, rather than for decoding or for-
est reranking (Huang, 2008). Second, it attempted to
incorporate syntax by applying off-the-shelf part-of-
speech taggers and parsers to MT output, a task these
tools were never designed for. By contrast, we incor-
porate features directly into hierarchical and syntax-
based decoders.
A third difficulty with Och et al?s study was that
it used MERT, which is not an ideal vehicle for fea-
ture exploration because it is observed not to per-
form well with large feature sets. Others have in-
troduced alternative discriminative training meth-
ods (Tillmann and Zhang, 2006; Liang et al, 2006;
Turian et al, 2007; Blunsom et al, 2008; Macherey
et al, 2008), in which a recurring challenge is scal-
ability: to train many features, we need many train-
218
ing examples, and to train discriminatively, we need
to search through all possible translations of each
training example. Another line of research (Watan-
abe et al, 2007; Chiang et al, 2008) tries to squeeze
as many features as possible from a relatively small
dataset. We follow this approach here.
3 Systems Used
3.1 Hiero
Hiero (Chiang, 2005) is a hierarchical, string-to-
string translation system. Its rules, which are ex-
tracted from unparsed, word-aligned parallel text,
are synchronous CFG productions, for example:
X? X1 de X2,X2 of X1
As the number of nonterminals is limited to two, the
grammar is equivalent to an inversion transduction
grammar (Wu, 1997).
The baseline model includes 12 features whose
weights are optimized using MERT. Two of the fea-
tures are n-gram language models, which require
intersecting the synchronous CFG with finite-state
automata representing the language models. This
grammar can be parsed efficiently using cube prun-
ing (Chiang, 2007).
3.2 Syntax-based system
Our syntax-based system transforms source Chinese
strings into target English syntax trees. Following
previous work in statistical MT (Brown et al, 1993),
we envision a noisy-channel model in which a lan-
guage model generates English, and then a transla-
tion model transforms English trees into Chinese.
We represent the translation model as a tree trans-
ducer (Knight and Graehl, 2005). It is obtained from
bilingual text that has been word-aligned and whose
English side has been syntactically parsed. From this
data, we use the the GHKM minimal-rule extraction
algorithm of (Galley et al, 2004) to yield rules like:
NP-C(x0:NPB PP(IN(of x1:NPB))? x1 de x0
Though this rule can be used in either direction,
here we use it right-to-left (Chinese to English). We
follow Galley et al (2006) in allowing unaligned
Chinese words to participate in multiple translation
rules, and in collecting larger rules composed of
minimal rules. These larger rules have been shown
to substantially improve translation accuracy (Gal-
ley et al, 2006; DeNeefe et al, 2007).
We apply Good-Turing discounting to the trans-
ducer rule counts and obtain probability estimates:
P(rule) = count(rule)count(LHS-root(rule))
When we apply these probabilities to derive an En-
glish sentence e and a corresponding Chinese sen-
tence c, we wind up with the joint probability P(e, c).
The baseline model includes log P(e, c), the two
n-gram language models log P(e), and other features
for a total of 25. For example, there is a pair of
features to punish rules that drop Chinese content
words or introduce spurious English content words.
All features are linearly combined and their weights
are optimized using MERT.
For efficient decoding with integrated n-gram lan-
guage models, all transducer rules must be binarized
into rules that contain at most two variables and
can be incrementally scored by the language model
(Zhang et al, 2006). Then we use a CKY-style parser
(Yamada and Knight, 2002; Galley et al, 2006) with
cube pruning to decode new sentences.
We include two other techniques in our baseline.
To get more general translation rules, we restruc-
ture our English training trees using expectation-
maximization (Wang et al, 2007), and to get more
specific translation rules, we relabel the trees with up
to 4 specialized versions of each nonterminal sym-
bol, again using expectation-maximization and the
split/merge technique of Petrov et al (2006).
3.3 MIRA training
We incorporate all our new features into a linear
model (Och and Ney, 2002) and train them using
MIRA (Crammer et al, 2006), following previous
work (Watanabe et al, 2007; Chiang et al, 2008).
Let e stand for output strings or their derivations,
and let h(e) stand for the feature vector for e. Initial-
ize the feature weights w. Then, repeatedly:
? Select a batch of input sentences f1, . . . , fm and
decode each fi to obtain a forest of translations.
? For each i, select from the forest a set of hy-
pothesis translations ei1, . . . , ein, which are the
219
10-best translations according to each of:
h(e) ? w
B???(e) + h(e) ? w
?B???(e) + h(e) ? w
(1)
? For each i, select an oracle translation:
e? = arg max
e
(B???(e) + h(e) ? w) (2)
Let ?hi j = h(e?i ) ? h(ei j).
? For each ei j, compute the loss
`i j = B???(e?i ) ? B???(ei j) (3)
? Update w to the value of w? that minimizes:
1
2?w
? ? w?2 + C
m?
i=1
max1? j?n(`i j ? ?hi j ? w
?) (4)
where C = 0.01. This minimization is per-
formed by a variant of sequential minimal opti-
mization (Platt, 1998).
Following Chiang et al (2008), we calculate the sen-
tence B??? scores in (1), (2), and (3) in the context
of some previous 1-best translations. We run 20 of
these learners in parallel, and when training is fin-
ished, the weight vectors from all iterations of all
learners are averaged together.
Since the interface between the trainer and the de-
coder is fairly simple?for each sentence, the de-
coder sends the trainer a forest, and the trainer re-
turns a weight update?it is easy to use this algo-
rithm with a variety of CKY-based decoders: here,
we are using it in conjunction with both the Hiero
decoder and our syntax-based decoder.
4 Features
In this section, we describe the new features intro-
duced on top of our baseline systems.
Discount features Both of our systems calculate
several features based on observed counts of rules in
the training data. Though the syntax-based system
uses Good-Turing discounting when computing the
P(e, c) feature, we find, as noted above, that it uses
quite a few one-count rules, suggesting that their
probabilities have been overestimated. We can di-
rectly attack this problem by adding features counti
that reward or punish rules seen i times, or features
count[i, j] for rules seen between i and j times.
4.1 Target-side features
String-to-tree MT offers some unique levers to pull,
in terms of target-side features. Because the system
outputs English trees, we can analyze output trees on
the tuning set and design new features to encourage
the decoder to produce more grammatical trees.
Rule overlap features While individual rules ob-
served in decoder output are often quite reasonable,
two adjacent rules can create problems. For exam-
ple, a rule that has a variable of type IN (preposi-
tion) needs another rule rooted with IN to fill the po-
sition. If the second rule supplies the wrong prepo-
sition, a bad translation results. The IN node here
is an overlap point between rules. Considering that
certain nonterminal symbols may be more reliable
overlap points than others, we create a binary fea-
ture for each nonterminal. A rule like:
IN(at)? zai
will have feature rule-root-IN set to 1 and all
other rule-root features set to 0. Our rule root fea-
tures range over the original (non-split) nontermi-
nal set; we have 105 in total. Even though the
rule root features are locally attached to individual
rules?and therefore cause no additional problems
for the decoder search?they are aimed at problem-
atic rule/rule interactions.
Bad single-level rewrites Sometimes the decoder
uses questionable rules, for example:
PP(x0:VBN x1:NP-C)? x0 x1
This rule is learned from 62 cases in our training
data, where the VBN is almost always the word
given. However, the decoder misuses this rule with
other VBNs. So we can add a feature that penalizes
any rule in which a PP dominates a VBN and NP-C.
The feature class bad-rewrite comprises penalties
for the following configurations based on our analy-
sis of the tuning set:
PP? VBN NP-C
PP-BAR? NP-C IN
VP? NP-C PP
CONJP? RB IN
220
Node count features It is possible that the de-
coder creates English trees with too many or too few
nodes of a particular syntactic category. For exam-
ple, there may be an tendency to generate too many
determiners or past-tense verbs. We therefore add a
count feature for each of the 109 (non-split) English
nonterminal symbols. For a rule like
NPB(NNP(us) NNP(president) x0:NNP)
? meiguo zongtong x0
the feature node-count-NPB gets value 1, node-
count-NNP gets value 2, and all others get 0.
Insertion features Among the rules we extract
from bilingual corpora are target-language insertion
rules, which have a word on the English side, but no
words on the source Chinese side. Sample syntax-
based insertion rules are:
NPB(DT(the) x0:NN)? x0
S(x0:NP-C VP(VBZ(is) x1:VP-C))? x0 x1
We notice that our decoder, however, frequently fails
to insert words like is and are, which often have no
equivalent in the Chinese source. We also notice that
the-insertion rules sometimes have a good effect, as
in the translation ?in the bloom of youth,? but other
times have a bad effect, as in ?people seek areas of
the conspiracy.?
Each time the decoder uses (or fails to use) an in-
sertion rule, it incurs some risk. There is no guaran-
tee that the interaction of the rule probabilities and
the language model provides the best way to manage
this risk. We therefore provide MIRA with a feature
for each of the most common English words appear-
ing in insertion rules, e.g., insert-the and insert-is.
There are 35 such features.
4.2 Source-side features
We now turn to features that make use of source-side
context. Although these features capture dependen-
cies that cross boundaries between rules, they are
still local in the sense that no new states need to
be added to the decoder. This is because the entire
source sentence, being fixed, is always available to
every feature.
Soft syntactic constraints Neither of our systems
uses source-side syntactic information; hence, both
could potentially benefit from soft syntactic con-
straints as described by Marton and Resnik (2008).
In brief, these features use the output of an in-
dependent syntactic parser on the source sentence,
rewarding decoder constituents that match syntac-
tic constituents and punishing decoder constituents
that cross syntactic constituents. We use separately-
tunable features for each syntactic category.
Structural distortion features Both of our sys-
tems have rules with variables that generalize over
possible fillers, but neither system?s basic model
conditions a rule application on the size of a filler,
making it difficult to distinguish long-distance re-
orderings from short-distance reorderings. To rem-
edy this problem, Chiang et al (2008) introduce a
structural distortion model, which we include in our
experiment. Our syntax-based baseline includes the
generative version of this model already.
Word context During rule extraction, we retain
word alignments from the training data in the ex-
tracted rules. (If a rule is observed with more than
one set of word alignments, we keep only the
most frequent one.) We then define, for each triple
( f , e, f+1), a feature that counts the number of times
that f is aligned to e and f+1 occurs to the right of
f ; and similarly for triples ( f , e, f?1) with f?1 occur-
ring to the left of f . In order to limit the size of the
model, we restrict words to be among the 100 most
frequently occurring words from the training data;
all other words are replaced with a token <unk>.
These features are somewhat similar to features
used by Watanabe et al (2007), but more in the spirit
of features used in the word sense disambiguation
model introduced by Lee and Ng (2002) and incor-
porated as a submodel of a translation system by
Chan et al (2007); here, we are incorporating some
of its features directly into the translation model.
5 Experiments
For our experiments, we used a 260 million word
Chinese/English bitext. We ran GIZA++ on the en-
tire bitext to produce IBM Model 4 word align-
ments, and then the link deletion algorithm (Fossum
et al, 2008) to yield better-quality alignments. For
221
System Training Features # Tune Test
Hiero MERT baseline 11 35.4 36.1
MIRA syntax, distortion 56 35.9 36.9?
syntax, distortion, discount 61 36.6 37.3??
all source-side, discount 10990 38.4 37.6??
Syntax MERT baseline 25 38.6 39.5
MIRA baseline 25 38.5 39.8?
overlap 132 38.7 39.9?
node count 136 38.7 40.0??
all target-side, discount 283 39.6 40.6??
Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM
B??? scores. ? or ?? = significantly better than MERT baseline (p < 0.05 or 0.01, respectively).
the syntax-based system, we ran a reimplementation
of the Collins parser (Collins, 1997) on the English
half of the bitext to produce parse trees, then restruc-
tured and relabeled them as described in Section 3.2.
Syntax-based rule extraction was performed on a 65
million word subset of the training data. For Hiero,
rules with up to two nonterminals were extracted
from a 38 million word subset and phrasal rules were
extracted from the remainder of the training data.
We trained three 5-gram language models: one on
the English half of the bitext, used by both systems,
one on one billion words of English, used by the
syntax-based system, and one on two billion words
of English, used by Hiero. Modified Kneser-Ney
smoothing (Chen and Goodman, 1998) was applied
to all language models. The language models are
represented using randomized data structures simi-
lar to those of Talbot et al (2007).
Our tuning set (2010 sentences) and test set (1994
sentences) were drawn from newswire data from the
NIST 2004 and 2005 evaluations and the GALE pro-
gram (with no overlap at either the segment or doc-
ument level). For the source-side syntax features,
we used the Berkeley parser (Petrov et al, 2006) to
parse the Chinese side of both sets.
We implemented the source-side context features
for Hiero and the target-side syntax features for the
syntax-based system, and the discount features for
both. We then ran MIRA on the tuning set with 20
parallel learners for Hiero and 73 parallel learners
for the syntax-based system. We chose a stopping it-
eration based on the B??? score on the tuning set,
and used the averaged feature weights from all iter-
Syntax-based Hiero
count weight count weight
1 +1.28 1 +2.23
2 +0.35 2 +0.77
3?5 ?0.73 3 +0.54
6?10 ?0.64 4 +0.29
5+ ?0.02
Table 2: Weights learned for discount features. Nega-
tive weights indicate bonuses; positive weights indicate
penalties.
ations of all learners to decode the test set.
The results (Table 1) show significant improve-
ments in both systems (p < 0.01) over already very
strong MERT baselines. Adding the source-side and
discount features to Hiero yields a +1.5 B??? im-
provement, and adding the target-side syntax and
discount features to the syntax-based system yields a
+1.1 B??? improvement. The results also show that
for Hiero, the various classes of features contributed
roughly equally; for the syntax-based system, we see
that two of the feature classes make small contribu-
tions but time constraints unfortunately did not per-
mit isolated testing of all feature classes.
6 Analysis
How did the various new features improve the trans-
lation quality of our two systems? We begin by ex-
amining the discount features. For these features,
we used slightly different schemes for the two sys-
tems, shown in Table 2 with their learned feature
weights. We see in both cases that one-count rules
are strongly penalized, as expected.
222
Reward
?0.42 a
?0.13 are
?0.09 at
?0.09 on
?0.05 was
?0.05 from
?0.04 ?s
?0.04 by
?0.04 is
?0.03 it
?0.03 its
...
Penalty
+0.67 of
+0.56 the
+0.47 comma
+0.13 period
+0.11 in
+0.08 for
+0.06 to
+0.05 will
+0.04 and
+0.02 as
+0.02 have
...
Table 3: Weights learned for inserting target English
words with rules that lack Chinese words.
6.1 Syntax features
Table 3 shows word-insertion feature weights. The
system rewards insertion of forms of be; examples
1?3 in Figure 1 show typical improved translations
that result. Among determiners, inserting a is re-
warded, while inserting the is punished. This seems
to be because the is often part of a fixed phrase, such
as the White House, and therefore comes naturally
as part of larger phrasal rules. Inserting the outside
these fixed phrases is a risk that the generative model
is too inclined to take. We also note that the system
learns to punish unmotivated insertions of commas
and periods, which get into our grammar via quirks
in the MT training data.
Table 4 shows weights for rule-overlap features.
MIRA punishes the case where rules overlap with
an IN (preposition) node. This makes sense: if a
rule has a variable that can be filled by any English
preposition, there is a risk that an incorrect preposi-
tion will fill it. On the other hand, splitting at a pe-
riod is a safe bet, and frees the model to use rules that
dig deeper into NP and VP trees when constructing
a top-level S. Table 5 shows weights for generated
English nonterminals: SBAR-C nodes are rewarded
and commas are punished.
The combined effect of all weights is subtle.
To interpret them further, it helps to look at gross
changes in the system?s behavior. For example, a
major error in the baseline system is to move ?X
said? or ?X asked? from the beginning of the Chi-
nese input to the middle or end of the English trans-
Bonus
?0.50 period
?0.39 VP-C
?0.36 VB
?0.31 SG-C
?0.30 MD
?0.26 VBG
?0.25 ADJP
?0.22 -LRB-
?0.21 VP-BAR
?0.20 NPB-BAR
?0.16 FRAG
?0.16 PRN
?0.15 NPB
?0.13 RB
?0.12 SBAR-C
?0.12 VP-C-BAR
?0.11 -RRB-
...
Penalty
+0.93 IN
+0.57 NNP
+0.44 NN
+0.41 DT
+0.34 JJ
+0.24 right double quote
+0.20 VBZ
+0.19 NP
+0.16 TO
+0.15 ADJP-BAR
+0.14 PRN-BAR
+0.14 NML
+0.13 comma
+0.12 VBD
+0.12 NNPS
+0.12 PRP
+0.11 SG
...
Table 4: Weights learned for employing rules whose En-
glish sides are rooted at particular syntactic categories.
Bonus
?0.73 SBAR-C
?0.54 VBZ
?0.54 IN
?0.52 NN
?0.51 PP-C
?0.47 right double quote
?0.39 ADJP
?0.34 POS
?0.31 ADVP
?0.30 RP
?0.29 PRT
?0.27 SG-C
?0.22 S-C
?0.21 NNPS
?0.21 VP-BAR
?0.20 PRP
?0.20 NPB-BAR
...
Penalty
+1.30 comma
+0.80 DT
+0.58 PP
+0.44 TO
+0.33 NNP
+0.30 NNS
+0.30 NML
+0.22 CD
+0.18 PRN
+0.16 SYM
+0.15 ADJP-BAR
+0.15 NP
+0.15 MD
+0.15 HYPH
+0.14 PRN-BAR
+0.14 NP-C
+0.11 ADJP-C
...
Table 5: Weights learned for generating syntactic nodes
of various types anywhere in the English translation.
223
lation. The error occurs with many speaking verbs,
and each time, we trace it to a different rule. The
problematic rules can even be non-lexical, e.g.:
S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)
? x3 x4 x2 x0 x1 x5
It is therefore difficult to come up with a straightfor-
ward feature to address the problem. However, when
we apply MIRA with the features already listed,
these translation errors all disappear, as demon-
strated by examples 4?5 in Figure 1. Why does this
happen? It turns out that in translation hypotheses
that move ?X said? or ?X asked? away from the be-
ginning of the sentence, more commas appear, and
fewer S-C and SBAR-C nodes appear. Therefore, the
new features work to discourage these hypotheses.
Example 6 shows additionally that commas next to
speaking verbs are now correctly deleted.
Examples 7?8 in Figure 1 show other kinds of
unanticipated improvements. We do not have space
for a fuller analysis, but we note that the specific ef-
fects we describe above account for only part of the
overall B??? improvement.
6.2 Word context features
In Table 6 are shown feature weights learned for the
word-context features. A surprising number of the
highest-weighted features have to do with transla-
tions of dates and bylines. Many of the penalties
seem to discourage spurious insertion or deletion
of frequent words (for, ?s, said, parentheses, and
quotes). Finally, we note that several of the features
(the third- and eighth-ranked reward and twelfth-
ranked penalty) shape the translation of shuo ?said?,
preferring translations with an overt complementizer
that and without a comma. Thus these features work
together to attack a frequent problem that our target-
syntax features also addressed.
Figure 2 shows the performance of Hiero with all
of its features on the tuning and test sets over time.
The scores on the tuning set rise rapidly, and the
scores on the test set alo rise, but much more slowly,
and there appears to be slight degradation after the
18th pass through the tuning data. This seems in line
with the finding of Watanabe et al (2007) that with
on the order of 10,000 features, overfitting is possi-
ble, but we can still improve accuracy on new data.
 35
 35.5
 36
 36.5
 37
 37.5
 38
 38.5
 0  5  10  15  20  25
B
L
E
U
Epoch
Tune
Test
Figure 2: Using over 10,000 word-context features leads
to overfitting, but its detrimental effects are modest.
Scores on the tuning set were obtained from the 1-best
output of the online learning algorithm, whereas scores
on the test set were obtained using averaged weights.
Early stopping would have given +0.2 B??? over the
results reported in Table 1.1
7 Conclusion
We have described a variety of features for statisti-
cal machine translation and applied them to syntax-
based and hierarchical systems. We saw that these
features, discriminatively trained using MIRA, led
to significant improvements, and took a closer look
at the results to see how the new features qualita-
tively improved translation quality. We draw three
conclusions from this study.
First, we have shown that these new features can
improve the performance even of top-scoring MT
systems. Second, these results add to a growing body
of evidence that MIRA is preferable to MERT for
discriminative training. When training over 10,000
features on a modest amount of data, we, like Watan-
abe et al (2007), did observe overfitting, yet saw
improvements on new data. Third, we have shown
that syntax-based machine translation offers possi-
bilities for features not available in other models,
making syntax-based MT and MIRA an especially
strong combination for future work.
1It was this iteration, in fact, which was used to derive the
combined feature count used in the title of this paper.
224
1 MERT: the united states pending israeli clarification on golan settlement plan
MIRA: the united states is waiting for israeli clarification on golan settlement plan
2 MERT: . . . the average life expectancy of only 18 months , canada ?s minority goverment will . . .
MIRA: . . . the average life expectancy of canada?s previous minority government is only 18 months . . .
3 MERT: . . . since un inspectors expelled by north korea . . .
MIRA: . . . since un inspectors were expelled by north korea . . .
4 MERT: another thing is . . . , " he said , " obviously , the first thing we need to do . . . .
MIRA: he said : " obviously , the first thing we need to do . . . , and another thing is . . . . "
5 MERT: the actual timing . . . reopened in january , yoon said .
MIRA: yoon said the issue of the timing . . .
6 MERT: . . . us - led coalition forces , said today that the crash . . .
MIRA: . . . us - led coalition forces said today that a us military . . .
7 MERT: . . . and others will feel the danger .
MIRA: . . . and others will not feel the danger .
8 MERT: in residential or public activities within 200 meters of the region , . . .
MIRA: within 200 m of residential or public activities area , . . .
Figure 1: Improved syntax-based translations due to MIRA-trained weights.
Bonus
f e context
?1.19 <unk> <unk> f?1 = ri ?day?
?1.01 <unk> <unk> f?1 = (
?0.84 , that f?1 = shuo ?say?
?0.82 yue ?month? <unk> f+1 = <unk>
?0.78 " " f?1 = <unk>
?0.76 " " f+1 = <unk>
?0.66 <unk> <unk> f+1 = nian ?year?
?0.65 , that f+1 = <unk>
...
Penalty
f e context
+1.12 <unk> ) f+1 = <unk>
+0.83 jiang ?shall? be f+1 = <unk>
+0.83 zhengfu ?government? the f?1 = <unk>
+0.73 <unk> ) f?1 = <unk>
+0.73 <unk> ( f+1 = <unk>
+0.72 <unk> ) f?1 = ri ?day?
+0.70 <unk> ( f?1 = ri ?day?
+0.69 <unk> ( f?1 = <unk>
+0.66 <unk> for f?1 = <unk>
+0.66 <unk> ?s f?1 = ,
+0.65 <unk> said f?1 = <unk>
+0.60 , , f?1 = shuo ?say?
...
Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese
word f , with Chinese word f?1 to the left or f+1 to the right. Glosses for Chinese words are not part of features.
225
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical ma-
chine translation. In Proc. ACL-08: HLT.
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?312.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. ACL 2007.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proc. ACL 1997.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP-CoNLL-2007.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In Proc. Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proc. HLT-NAACL 2004, Boston, Massachusetts.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proc. ACL 2006.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of the Sixth International
Conference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing).
Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation. In
Proc. EMNLP 2002, pages 41?48.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. COLING-ACL
2006.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uskoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. EMNLP 2008.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ACL-08: HLT.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ACL 2002.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proc. HLT-NAACL 2004, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL 2006.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing, pages 195?208. MIT Press.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proc. ACL 2007, pages 512?519.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. COLING-ACL 2006.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learn-
ing for natural language parsing and translation. In
Proc. NIPS 2006.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP-CoNLL 2007.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP 2007.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. ACL 2002.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. HLT-NAACL 2006.
226
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Word Sense Disambiguation Improves Statistical Machine Translation
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2
Singapore 117543
{chanys, nght}@comp.nus.edu.sg
David Chiang
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
Abstract
Recent research presents conflicting evi-
dence on whether word sense disambigua-
tion (WSD) systems can help to improve the
performance of statistical machine transla-
tion (MT) systems. In this paper, we suc-
cessfully integrate a state-of-the-art WSD
system into a state-of-the-art hierarchical
phrase-based MT system, Hiero. We show
for the first time that integrating a WSD sys-
tem improves the performance of a state-of-
the-art statistical MT system on an actual
translation task. Furthermore, the improve-
ment is statistically significant.
1 Introduction
Many words have multiple meanings, depending on
the context in which they are used. Word sense dis-
ambiguation (WSD) is the task of determining the
correct meaning or sense of a word in context. WSD
is regarded as an important research problem and is
assumed to be helpful for applications such as ma-
chine translation (MT) and information retrieval.
In translation, different senses of a word w in a
source language may have different translations in a
target language, depending on the particular mean-
ing of w in context. Hence, the assumption is that
in resolving sense ambiguity, a WSD system will be
able to help an MT system to determine the correct
translation for an ambiguous word. To determine the
correct sense of a word, WSD systems typically use
a wide array of features that are not limited to the lo-
cal context of w, and some of these features may not
be used by state-of-the-art statistical MT systems.
To perform translation, state-of-the-art MT sys-
tems use a statistical phrase-based approach (Marcu
and Wong, 2002; Koehn et al, 2003; Och and
Ney, 2004) by treating phrases as the basic units
of translation. In this approach, a phrase can be
any sequence of consecutive words and is not nec-
essarily linguistically meaningful. Capitalizing on
the strength of the phrase-based approach, Chiang
(2005) introduced a hierarchical phrase-based sta-
tistical MT system, Hiero, which achieves signifi-
cantly better translation performance than Pharaoh
(Koehn, 2004a), which is a state-of-the-art phrase-
based statistical MT system.
Recently, some researchers investigated whether
performing WSD will help to improve the perfor-
mance of an MT system. Carpuat and Wu (2005)
integrated the translation predictions from a Chinese
WSD system (Carpuat et al, 2004) into a Chinese-
English word-based statistical MT system using the
ISI ReWrite decoder (Germann, 2003). Though they
acknowledged that directly using English transla-
tions as word senses would be ideal, they instead
predicted the HowNet sense of a word and then used
the English gloss of the HowNet sense as the WSD
model?s predicted translation. They did not incor-
porate their WSD model or its predictions into their
translation model; rather, they used the WSD pre-
dictions either to constrain the options available to
their decoder, or to postedit the output of their de-
coder. They reported the negative result that WSD
decreased the performance of MT based on their ex-
periments.
In another work (Vickrey et al, 2005), the WSD
problem was recast as a word translation task. The
33
translation choices for a word w were defined as the
set of words or phrases aligned to w, as gathered
from a word-aligned parallel corpus. The authors
showed that they were able to improve their model?s
accuracy on two simplified translation tasks: word
translation and blank-filling.
Recently, Cabezas and Resnik (2005) experi-
mented with incorporating WSD translations into
Pharaoh, a state-of-the-art phrase-based MT sys-
tem (Koehn et al, 2003). Their WSD system pro-
vided additional translations to the phrase table of
Pharaoh, which fired a new model feature, so that
the decoder could weigh the additional alternative
translations against its own. However, they could
not automatically tune the weight of this feature in
the same way as the others. They obtained a rela-
tively small improvement, and no statistical signifi-
cance test was reported to determine if the improve-
ment was statistically significant.
Note that the experiments in (Carpuat and Wu,
2005) did not use a state-of-the-art MT system,
while the experiments in (Vickrey et al, 2005) were
not done using a full-fledged MT system and the
evaluation was not on how well each source sentence
was translated as a whole. The relatively small im-
provement reported by Cabezas and Resnik (2005)
without a statistical significance test appears to be
inconclusive. Considering the conflicting results re-
ported by prior work, it is not clear whether a WSD
system can help to improve the performance of a
state-of-the-art statistical MT system.
In this paper, we successfully integrate a state-
of-the-art WSD system into the state-of-the-art hi-
erarchical phrase-based MT system, Hiero (Chiang,
2005). The integration is accomplished by introduc-
ing two additional features into the MT model which
operate on the existing rules of the grammar, with-
out introducing competing rules. These features are
treated, both in feature-weight tuning and in decod-
ing, on the same footing as the rest of the model,
allowing it to weigh the WSD model predictions
against other pieces of evidence so as to optimize
translation accuracy (as measured by BLEU). The
contribution of our work lies in showing for the first
time that integrating a WSD system significantly im-
proves the performance of a state-of-the-art statisti-
cal MT system on an actual translation task.
In the next section, we describe our WSD system.
Then, in Section 3, we describe the Hiero MT sys-
tem and introduce the two new features used to inte-
grate the WSD system into Hiero. In Section 4, we
describe the training data used by the WSD system.
In Section 5, we describe how the WSD translations
provided are used by the decoder of the MT system.
In Section 6 and 7, we present and analyze our ex-
perimental results, before concluding in Section 8.
2 Word Sense Disambiguation
Prior research has shown that using Support Vector
Machines (SVM) as the learning algorithm for WSD
achieves good results (Lee and Ng, 2002). For our
experiments, we use the SVM implementation of
(Chang and Lin, 2001) as it is able to work on multi-
class problems to output the classification probabil-
ity for each class.
Our implemented WSD classifier uses the knowl-
edge sources of local collocations, parts-of-speech
(POS), and surrounding words, following the suc-
cessful approach of (Lee and Ng, 2002). For local
collocations, we use 3 features, w?1w+1, w?1, and
w+1, where w?1 (w+1) is the token immediately to
the left (right) of the current ambiguous word oc-
currence w. For parts-of-speech, we use 3 features,
P?1, P0, and P+1, where P0 is the POS of w, and
P?1 (P+1) is the POS of w?1 (w+1). For surround-
ing words, we consider all unigrams (single words)
in the surrounding context of w. These unigrams can
be in a different sentence from w. We perform fea-
ture selection on surrounding words by including a
unigram only if it occurs 3 or more times in some
sense of w in the training data.
To measure the accuracy of our WSD classifier,
we evaluate it on the test data of SENSEVAL-3 Chi-
nese lexical-sample task. We obtain accuracy that
compares favorably to the best participating system
in the task (Carpuat et al, 2004).
3 Hiero
Hiero (Chiang, 2005) is a hierarchical phrase-based
model for statistical machine translation, based on
weighted synchronous context-free grammar (CFG)
(Lewis and Stearns, 1968). A synchronous CFG
consists of rewrite rules such as the following:
X ? ??, ?? (1)
34
where X is a non-terminal symbol, ? (?) is a string
of terminal and non-terminal symbols in the source
(target) language, and there is a one-to-one corre-
spondence between the non-terminals in ? and ? in-
dicated by co-indexation. Hence, ? and ? always
have the same number of non-terminal symbols. For
instance, we could have the following grammar rule:
X ? ???t X 1 , go to X 1 every month to? (2)
where boxed indices represent the correspondences
between non-terminal symbols.
Hiero extracts the synchronous CFG rules auto-
matically from a word-aligned parallel corpus. To
translate a source sentence, the goal is to find its
most probable derivation using the extracted gram-
mar rules. Hiero uses a general log-linear model
(Och and Ney, 2002) where the weight of a deriva-
tion D for a particular source sentence and its trans-
lation is
w(D) =
?
i
?i(D)?i (3)
where ?i is a feature function and ?i is the weight for
feature ?i. To ensure efficient decoding, the ?i are
subject to certain locality restrictions. Essentially,
they should be defined as products of functions de-
fined on isolated synchronous CGF rules; however,
it is possible to extend the domain of locality of
the features somewhat. A n-gram language model
adds a dependence on (n?1) neighboring target-side
words (Wu, 1996; Chiang, 2007), making decoding
much more difficult but still polynomial; in this pa-
per, we add features that depend on the neighboring
source-side words, which does not affect decoding
complexity at all because the source string is fixed.
In principle we could add features that depend on
arbitrary source-side context.
3.1 New Features in Hiero for WSD
To incorporate WSD into Hiero, we use the trans-
lations proposed by the WSD system to help Hiero
obtain a better or more probable derivation during
the translation of each source sentence. To achieve
this, when a grammar rule R is considered during
decoding, and we recognize that some of the ter-
minal symbols (words) in ? are also chosen by the
WSD system as translations for some terminal sym-
bols (words) in ?, we compute the following fea-
tures:
? Pwsd(t | s) gives the contextual probability of
the WSD classifier choosing t as a translation
for s, where t (s) is some substring of terminal
symbols in ? (?). Because this probability only
applies to some rules, and we don?t want to pe-
nalize those rules, we must add another feature,
? Ptywsd = exp(?|t|), where t is the translation
chosen by the WSD system. This feature, with
a negative weight, rewards rules that use trans-
lations suggested by the WSD module.
Note that we can take the negative logarithm of
the rule/derivation weights and think of them as
costs rather than probabilities.
4 Gathering Training Examples for WSD
Our experiments were for Chinese to English trans-
lation. Hence, in the context of our work, a syn-
chronous CFG grammar rule X ? ??, ?? gathered
by Hiero consists of a Chinese portion ? and a cor-
responding English portion ?, where each portion is
a sequence of words and non-terminal symbols.
Our WSD classifier suggests a list of English
phrases (where each phrase consists of one or more
English words) with associated contextual probabil-
ities as possible translations for each particular Chi-
nese phrase. In general, the Chinese phrase may
consist of k Chinese words, where k = 1, 2, 3, . . ..
However, we limit k to 1 or 2 for experiments re-
ported in this paper. Future work can explore en-
larging k.
Whenever Hiero is about to extract a grammar
rule where its Chinese portion is a phrase of one or
two Chinese words with no non-terminal symbols,
we note the location (sentence and token offset) in
the Chinese half of the parallel corpus from which
the Chinese portion of the rule is extracted. The ac-
tual sentence in the corpus containing the Chinese
phrase, and the one sentence before and the one sen-
tence after that actual sentence, will serve as the con-
text for one training example for the Chinese phrase,
with the corresponding English phrase of the gram-
mar rule as its translation. Hence, unlike traditional
WSD where the sense classes are tied to a specific
sense inventory, our ?senses? here consist of the En-
glish phrases extracted as translations for each Chi-
nese phrase. Since the extracted training data may
35
be noisy, for each Chinese phrase, we remove En-
glish translations that occur only once. Furthermore,
we only attempt WSD classification for those Chi-
nese phrases with at least 10 training examples.
Using the WSD classifier described in Section 2,
we classified the words in each Chinese source sen-
tence to be translated. We first performed WSD on
all single Chinese words which are either noun, verb,
or adjective. Next, we classified the Chinese phrases
consisting of 2 consecutive Chinese words by simply
treating the phrase as a single unit. When perform-
ing classification, we give as output the set of En-
glish translations with associated context-dependent
probabilities, which are the probabilities of a Chi-
nese word (phrase) translating into each English
phrase, depending on the context of the Chinese
word (phrase). After WSD, the ith word ci in every
Chinese sentence may have up to 3 sets of associ-
ated translations provided by the WSD system: a set
of translations for ci as a single word, a second set
of translations for ci?1ci considered as a single unit,
and a third set of translations for cici+1 considered
as a single unit.
5 Incorporating WSD during Decoding
The following tasks are done for each rule that is
considered during decoding:
? identify Chinese words to suggest translations
for
? match suggested translations against the En-
glish side of the rule
? compute features for the rule
The WSD system is able to predict translations
only for a subset of Chinese words or phrases.
Hence, we must first identify which parts of the
Chinese side of the rule have suggested translations
available. Here, we consider substrings of length up
to two, and we give priority to longer substrings.
Next, we want to know, for each Chinese sub-
string considered, whether the WSD system sup-
ports the Chinese-English translation represented by
the rule. If the rule is finally chosen as part of the
best derivation for translating the Chinese sentence,
then all the words in the English side of the rule will
appear in the translated English sentence. Hence,
we need to match the translations suggested by the
WSD system against the English side of the rule. It
is for these matching rules that the WSD features
will apply.
The translations proposed by the WSD system
may be more than one word long. In order for a
proposed translation to match the rule, we require
two conditions. First, the proposed translation must
be a substring of the English side of the rule. For
example, the proposed translation ?every to? would
not match the chunk ?every month to?. Second, the
match must contain at least one aligned Chinese-
English word pair, but we do not make any other
requirements about the alignment of the other Chi-
nese or English words.1 If there are multiple possi-
ble matches, we choose the longest proposed trans-
lation; in the case of a tie, we choose the proposed
translation with the highest score according to the
WSD model.
Define a chunk of a rule to be a maximal sub-
string of terminal symbols on the English side of the
rule. For example, in Rule (2), the chunks would be
?go to? and ?every month to?. Whenever we find
a matching WSD translation, we mark the whole
chunk on the English side as consumed.
Finally, we compute the feature values for the
rule. The feature Pwsd(t | s) is the sum of the costs
(according to the WSD model) of all the matched
translations, and the feature Ptywsd is the sum of
the lengths of all the matched translations.
Figure 1 shows the pseudocode for the rule scor-
ing algorithm in more detail, particularly with re-
gards to resolving conflicts between overlapping
matches. To illustrate the algorithm given in Figure
1, consider Rule (2). Hereafter, we will use symbols
to represent the Chinese and English words in the
rule: c1, c2, and c3 will represent the Chinese words
???, ???, and ?t? respectively. Similarly, e1, e2,
e3, e4, and e5 will represent the English words go,
to, every, month, and to respectively. Hence, Rule
(2) has two chunks: e1e2 and e3e4e5. When the rule
is extracted from the parallel corpus, it has these
alignments between the words of its Chinese and
English portion: {c1?e3,c2?e4,c3?e1,c3?e2,c3?e5},
which means that c1 is aligned to e3, c2 is aligned to
1In order to check this requirement, we extended Hiero to
make word alignment information available to the decoder.
36
Input: rule R considered during decoding with its own associated costR
Lc = list of symbols in Chinese portion of R
WSDcost = 0
i = 1
while i ? len(Lc):
ci = ith symbol in Lc
if ci is a Chinese word (i.e., not a non-terminal symbol):
seenChunk = ? // seenChunk is a global variable and is passed by reference to matchWSD
if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL
if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations:
WSDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities
WSDcost = WSDcost + matchWSD(ci, WSDc, seenChunk)
WSDcost = WSDcost + matchWSD(ci+1, WSDc, seenChunk)
i = i + 1
else:
WSDc = set of WSD translations for ci with context-dependent probabilities
WSDcost = WSDcost + matchWSD(ci, WSDc, seenChunk)
i = i + 1
costR = costR + WSDcost
matchWSD(c, WSDc, seenChunk):
// seenChunk is the set of chunks of R already examined for possible matching WSD translations
cost = 0
ChunkSet = set of chunks in R aligned to c
for chunkj in ChunkSet:
if chunkj not in seenChunk:
seenChunk = seenChunk ? { chunkj }
Echunkj = set of English words in chunkj aligned to c
Candidatewsd = ?
for wsdk in WSDc:
if (wsdk is sub-sequence of chunkj) and (wsdk contains at least one word in Echunkj )
Candidatewsd = Candidatewsd ? { wsdk }
wsdbest = best matching translation in Candidatewsd against chunkj
cost = cost + costByWSDfeatures(wsdbest) // costByWSDfeatures sums up the cost of the two WSD features
return cost
Figure 1: WSD translations affecting the cost of a rule R considered during decoding.
e4, and c3 is aligned to e1, e2, and e5. Although all
words are aligned here, in general for a rule, some of
its Chinese or English words may not be associated
with any alignments.
In our experiment, c1c2 as a phrase has a list of
translations proposed by the WSD system, includ-
ing the English phrase ?every month?. matchWSD
will first be invoked for c1, which is aligned to only
one chunk e3e4e5 via its alignment with e3. Since
?every month? is a sub-sequence of the chunk and
also contains the word e3 (?every?), it is noted as
a candidate translation. Later, it is determined that
the most number of words any candidate translation
has is two words. Since among all the 2-word candi-
date translations, the translation ?every month? has
the highest translation probability as assigned by the
WSD classifier, it is chosen as the best matching
translation for the chunk. matchWSD is then invoked
for c2, which is aligned to only one chunk e3e4e5.
However, since this chunk has already been exam-
ined by c1 with which it is considered as a phrase, no
further matching is done for c2. Next, matchWSD is
invoked for c3, which is aligned to both chunks of R.
The English phrases ?go to? and ?to? are among the
list of translations proposed by the WSD system for
c3, and they are eventually chosen as the best match-
ing translations for the chunks e1e2 and e3e4e5, re-
spectively.
6 Experiments
As mentioned, our experiments were on Chinese to
English translation. Similar to (Chiang, 2005), we
trained the Hiero system on the FBIS corpus, used
the NIST MT 2002 evaluation test set as our devel-
opment set to tune the feature weights, and the NIST
MT 2003 evaluation test set as our test data. Using
37
System BLEU-4 Individual n-gram precisions
1 2 3 4
Hiero 29.73 74.73 40.14 21.83 11.93
Hiero+WSD 30.30 74.82 40.40 22.45 12.42
Table 1: BLEU scores
Features
System Plm(e) P (?|?) P (?|?) Pw(?|?) Pw(?|?) Ptyphr Glue Ptyword Pwsd(t|s) Ptywsd
Hiero 0.2337 0.0882 0.1666 0.0393 0.1357 0.0665 ?0.0582 ?0.4806 - -
Hiero+WSD 0.1937 0.0770 0.1124 0.0487 0.0380 0.0988 ?0.0305 ?0.1747 0.1051 ?0.1611
Table 2: Weights for each feature obtained by MERT training. The first eight features are those used by
Hiero in (Chiang, 2005).
the English portion of the FBIS corpus and the Xin-
hua portion of the Gigaword corpus, we trained a tri-
gram language model using the SRI Language Mod-
elling Toolkit (Stolcke, 2002). Following (Chiang,
2005), we used the version 11a NIST BLEU script
with its default settings to calculate the BLEU scores
(Papineni et al, 2002) based on case-insensitive n-
gram matching, where n is up to 4.
First, we performed word alignment on the FBIS
parallel corpus using GIZA++ (Och and Ney, 2000)
in both directions. The word alignments of both
directions are then combined into a single set of
alignments using the ?diag-and? method of Koehn
et al (2003). Based on these alignments, syn-
chronous CFG rules are then extracted from the cor-
pus. While Hiero is extracting grammar rules, we
gathered WSD training data by following the proce-
dure described in section 4.
6.1 Hiero Results
Using the MT 2002 test set, we ran the minimum-
error rate training (MERT) (Och, 2003) with the
decoder to tune the weights for each feature. The
weights obtained are shown in the row Hiero of
Table 2. Using these weights, we run Hiero?s de-
coder to perform the actual translation of the MT
2003 test sentences and obtained a BLEU score of
29.73, as shown in the row Hiero of Table 1. This is
higher than the score of 28.77 reported in (Chiang,
2005), perhaps due to differences in word segmenta-
tion, etc. Note that comparing with the MT systems
used in (Carpuat and Wu, 2005) and (Cabezas and
Resnik, 2005), the Hiero system we are using rep-
resents a much stronger baseline MT system upon
which the WSD system must improve.
6.2 Hiero+WSD Results
We then added the WSD features of Section 3.1 into
Hiero and reran the experiment. The weights ob-
tained by MERT are shown in the row Hiero+WSD
of Table 2. We note that a negative weight is learnt
for Ptywsd. This means that in general, the model
prefers grammar rules having chunks that matches
WSD translations. This matches our intuition. Us-
ing the weights obtained, we translated the test sen-
tences and obtained a BLEU score of 30.30, as
shown in the row Hiero+WSD of Table 1. The im-
provement of 0.57 is statistically significant at p <
0.05 using the sign-test as described by Collins et al
(2005), with 374 (+1), 318 (?1) and 227 (0). Us-
ing the bootstrap-sampling test described in (Koehn,
2004b), the improvement is statistically significant
at p < 0.05. Though the improvement is modest, it is
statistically significant and this positive result is im-
portant in view of the negative findings in (Carpuat
and Wu, 2005) that WSD does not help MT. Fur-
thermore, note that Hiero+WSD has higher n-gram
precisions than Hiero.
7 Analysis
Ideally, the WSD system should be suggesting high-
quality translations which are frequently part of the
reference sentences. To determine this, we note the
set of grammar rules used in the best derivation for
translating each test sentence. From the rules of each
test sentence, we tabulated the set of translations
proposed by the WSD system and check whether
they are found in the associated reference sentences.
On the entire set of NIST MT 2003 evaluation test
sentences, an average of 10.36 translations proposed
38
No. of All test sentences +1 from Collins sign-test
words in No. of % match No. of % match
WSD translations WSD translations used reference WSD translations used reference
1 7087 77.31 3078 77.68
2 1930 66.11 861 64.92
3 371 43.13 171 48.54
4 124 26.61 52 28.85
Table 3: Number of WSD translations used and proportion that matches against respective reference sen-
tences. WSD translations longer than 4 words are very sparse (less than 10 occurrences) and thus they are
not shown.
by the WSD system were used for each sentence.
When limited to the set of 374 sentences which
were judged by the Collins sign-test to have better
translations from Hiero+WSD than from Hiero, a
higher number (11.14) of proposed translations were
used on average. Further, for the entire set of test
sentences, 73.01% of the proposed translations are
found in the reference sentences. This increased to
a proportion of 73.22% when limited to the set of
374 sentences. These figures show that having more,
and higher-quality proposed translations contributed
to the set of 374 sentences being better translations
than their respective original translations from Hi-
ero. Table 3 gives a detailed breakdown of these
figures according to the number of words in each
proposed translation. For instance, over all the test
sentences, the WSD module gave 7087 translations
of single-word length, and 77.31% of these trans-
lations match their respective reference sentences.
We note that although the proportion of matching 2-
word translations is slightly lower for the set of 374
sentences, the proportion increases for translations
having more words.
After the experiments in Section 6 were com-
pleted, we visually inspected the translation output
of Hiero and Hiero+WSD to categorize the ways in
which integrating WSD contributes to better trans-
lations. The first way in which WSD helps is when
it enables the integrated Hiero+WSD system to out-
put extra appropriate English words. For example,
the translations for the Chinese sentence ?. . .? ?
??q??R?Rz?????
??tZ? are as follows.
? Hiero: . . . or other bad behavior ?, will be more
aid and other concessions.
? Hiero+WSD: . . . or other bad behavior ?, will
be unable to obtain more aid and other conces-
sions.
Here, the Chinese words ??Rz? are not trans-
lated by Hiero at all. By providing the correct trans-
lation of ?unable to obtain? for ?? Rz?, the
translation output of Hiero+WSD is more complete.
A second way in which WSD helps is by correct-
ing a previously incorrect translation. For example,
for the Chinese sentence ?. . .? ? \ ) ? |
??. . . ?, the WSD system helps to correct Hiero?s
original translation by providing the correct transla-
tion of ?all ethnic groups? for the Chinese phrase
???:
? Hiero: . . . , and people of all nationalities
across the country, . . .
? Hiero+WSD: . . . , and people of
all ethnic groups across the country, . . .
We also looked at the set of 318 sentences that
were judged by the Collins sign-test to be worse
translations. We found that in some situations,
Hiero+WSD has provided extra appropriate English
words, but those particular words are not used in the
reference sentences. An interesting example is the
translation of the Chinese sentence ??? i? ?
?8q??R?Rz?????.
? Hiero: Australian foreign minister said that
North Korea bad behavior will be more aid
? Hiero+WSD: Australian foreign minister said
that North Korea bad behavior will be
unable to obtain more aid
This is similar to the example mentioned earlier. In
this case however, those extra English words pro-
vided by Hiero+WSD, though appropriate, do not
39
result in more n-gram matches as the reference sen-
tences used phrases such as ?will not gain?, ?will not
get?, etc. Since the BLEU metric is precision based,
the longer sentence translation by Hiero+WSD gets
a lower BLEU score instead.
8 Conclusion
We have shown that WSD improves the transla-
tion performance of a state-of-the-art hierarchical
phrase-based statistical MT system and this im-
provement is statistically significant. We have also
demonstrated one way to integrate a WSD system
into an MT system without introducing any rules
that compete against existing rules, and where the
feature-weight tuning and decoding place the WSD
system on an equal footing with the other model
components. For future work, an immediate step
would be for the WSD classifier to provide trans-
lations for longer Chinese phrases. Also, different
alternatives could be tried to match the translations
provided by the WSD classifier against the chunks
of rules. Finally, besides our proposed approach of
integrating WSD into statistical MT via the intro-
duction of two new features, we could explore other
alternative ways of integration.
Acknowledgements
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076). David Chiang was partially supported un-
der the GALE program of the Defense Advanced
Research Projects Agency, contract HR0011-06-C-
0022.
References
C. Cabezas and P. Resnik. 2005. Using WSD techniques for
lexical selection in statistical machine translation. Technical
report, University of Maryland.
M. Carpuat and D. Wu. 2005. Word sense disambiguation
vs. statistical machine translation. In Proc. of ACL05, pages
387?394.
M. Carpuat, W. Su, and D. Wu. 2004. Augmenting ensemble
classification for word sense disambiguation with a kernel
PCA model. In Proc. of SENSEVAL-3, pages 88?92.
C. C. Chang and C. J. Lin, 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
D. Chiang. 2005. A hierarchical phrase-based model for sta-
tistical machine translation. In Proc. of ACL05, pages 263?
270.
D. Chiang. 2007. Hierarchical phrase-based translation. To
appear in Computational Linguistics, 33(2).
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restruc-
turing for statistical machine translation. In Proc. of ACL05,
pages 531?540.
U. Germann. 2003. Greedy decoding for statistical machine
translation in almost linear time. In Proc. of HLT-NAACL03,
pages 72?79.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL03, pages 48?54.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, Uni-
versity of Southern California.
P. Koehn. 2004a. Pharaoh: A beam search decoder for phrase-
based statistical machine translation models. In Proc. of
AMTA04, pages 115?124.
P. Koehn. 2004b. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP04, pages 388?
395.
Y. K. Lee and H. T. Ng. 2002. An empirical evaluation of
knowledge sources and learning algorithms for word sense
disambiguation. In Proc. of EMNLP02, pages 41?48.
P. M. II Lewis and R. E. Stearns. 1968. Syntax-directed trans-
duction. Journal of the ACM, 15(3):465?488.
D. Marcu and W. Wong. 2002. A phrase-based, joint proba-
bility model for statistical machine translation. In Proc. of
EMNLP02, pages 133?139.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proc. of ACL00, pages 440?447.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of ACL02, pages 295?302.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL03, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU:
A method for automatic evaluation of machine translation.
In Proc. of ACL02, pages 311?318.
A. Stolcke. 2002. SRILM - an extensible language modeling
toolkit. In Proc. of ICSLP02, pages 901?904.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation. In
Proc. of EMNLP05, pages 771?778.
D. Wu. 1996. A polynomial-time algorithm for statistical ma-
chine translation. In Proc. of ACL96, pages 152?158.
40
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Forest Rescoring: Faster Decoding with Integrated Language Models ?
Liang Huang
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
David Chiang
USC Information Sciences Institute
Marina del Rey, CA 90292
chiang@isi.edu
Abstract
Efficient decoding has been a fundamental
problem in machine translation, especially
with an integrated language model which
is essential for achieving good translation
quality. We develop faster approaches for
this problem based on k-best parsing algo-
rithms and demonstrate their effectiveness
on both phrase-based and syntax-based MT
systems. In both cases, our methods achieve
significant speed improvements, often by
more than a factor of ten, over the conven-
tional beam-search method at the same lev-
els of search error and translation accuracy.
1 Introduction
Recent efforts in statistical machine translation
(MT) have seen promising improvements in out-
put quality, especially the phrase-based models (Och
and Ney, 2004) and syntax-based models (Chiang,
2005; Galley et al, 2006). However, efficient de-
coding under these paradigms, especially with inte-
grated language models (LMs), remains a difficult
problem. Part of the complexity arises from the ex-
pressive power of the translation model: for exam-
ple, a phrase- or word-based model with full reorder-
ing has exponential complexity (Knight, 1999). The
language model also, if fully integrated into the de-
coder, introduces an expensive overhead for main-
taining target-language boundary words for dynamic
? The authors would like to thank Dan Gildea, Jonathan
Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob
Moore and Hao Zhang. L. H. was partially supported by
NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA-
0205456 at UPenn. D. C. was partially supported under the
GALE/DARPA program, contract HR0011-06-C-0022.
programming (Wu, 1996; Och and Ney, 2004). In
practice, one must prune the search space aggres-
sively to reduce it to a reasonable size.
A much simpler alternative method to incorporate
the LM is rescoring: we first decode without the LM
(henceforth ?LM decoding) to produce a k-best list
of candidate translations, and then rerank the k-best
list using the LM. This method runs much faster in
practice but often produces a considerable number
of search errors since the true best translation (taking
LM into account) is often outside of the k-best list.
Cube pruning (Chiang, 2007) is a compromise be-
tween rescoring and full-integration: it rescores k
subtranslations at each node of the forest, rather than
only at the root node as in pure rescoring. By adapt-
ing the k-best parsing Algorithm 2 of Huang and
Chiang (2005), it achieves significant speed-up over
full-integration on Chiang?s Hiero system.
We push the idea behind this method further and
make the following contributions in this paper:
? We generalize cube pruning and adapt it to two
systems very different from Hiero: a phrase-
based system similar to Pharaoh (Koehn, 2004)
and a tree-to-string system (Huang et al, 2006).
? We also devise a faster variant of cube pruning,
called cube growing, which uses a lazy version
of k-best parsing (Huang and Chiang, 2005)
that tries to reduce k to the minimum needed
at each node to obtain the desired number of
hypotheses at the root.
Cube pruning and cube growing are collectively
called forest rescoring since they both approxi-
mately rescore the packed forest of derivations from
?LM decoding. In practice they run an order of
144
magnitude faster than full-integration with beam
search, at the same level of search errors and trans-
lation accuracy as measured by BLEU.
2 Preliminaries
We establish in this section a unified framework
for translation with an integrated n-gram language
model in both phrase-based systems and syntax-
based systems based on synchronous context-free
grammars (SCFGs). An SCFG (Lewis and Stearns,
1968) is a context-free rewriting system for generat-
ing string pairs. Each rule A ? ?, ? rewrites a pair
of nonterminals in both languages, where ? and ?
are the source and target side components, and there
is a one-to-one correspondence between the nonter-
minal occurrences in ? and the nonterminal occur-
rences in ?. For example, the following rule
VP ? PP (1) VP (2), VP (2) PP (1)
captures the swapping of VP and PP between Chi-
nese (source) and English (target).
2.1 Translation as Deduction
We will use the following example from Chinese to
English for both systems described in this section:
yu?
with
Sha?lo?ng
Sharon
ju?x??ng
hold
le
[past]
hu?`ta?n
meeting
?held a meeting with Sharon?
A typical phrase-based decoder generates partial
target-language outputs in left-to-right order in the
form of hypotheses (Koehn, 2004). Each hypothesis
has a coverage vector capturing the source-language
words translated so far, and can be extended into a
longer hypothesis by a phrase-pair translating an un-
covered segment.
This process can be formalized as a deduc-
tive system. For example, the following deduc-
tion step grows a hypothesis by the phrase-pair
?yu? Sha?lo?ng, with Sharon?:
( ???) : (w, ?held a talk?)
(?????) : (w + c, ?held a talk with Sharon?) (1)
where a ? in the coverage vector indicates the source
word at this position is ?covered? (for simplicity
we omit here the ending position of the last phrase
which is needed for distortion costs), and where w
and w + c are the weights of the two hypotheses,
respectively, with c being the cost of the phrase-pair.
Similarly, the decoding problem with SCFGs can
also be cast as a deductive (parsing) system (Shieber
et al, 1995). Basically, we parse the input string us-
ing the source projection of the SCFG while build-
ing the corresponding subtranslations in parallel. A
possible deduction of the above example is notated:
(PP1,3) : (w1, t1) (VP3,6) : (w2, t2)
(VP1,6) : (w1 + w2 + c?, t2t1) (2)
where the subscripts denote indices in the input sen-
tence just as in CKY parsing, w1, w2 are the scores
of the two antecedent items, and t1 and t2 are the
corresponding subtranslations. The resulting trans-
lation t2t1 is the inverted concatenation as specified
by the target-side of the SCFG rule with the addi-
tional cost c? being the cost of this rule.
These two deductive systems represent the search
space of decoding without a language model. When
one is instantiated for a particular input string, it de-
fines a set of derivations, called a forest, represented
in a compact structure that has a structure of a graph
in the phrase-based case, or more generally, a hyper-
graph in both cases. Accordingly we call items like
(?????) and (VP1,6) nodes in the forest, and instan-
tiated deductions like
(?????) ? ( ???) with Sharon,
(VP1,6) ? (VP3,6) (PP1,3)
we call hyperedges that connect one or more an-
tecedent nodes to a consequent node.
2.2 Adding a Language Model
To integrate with a bigram language model, we can
use the dynamic-programming algorithms of Och
and Ney (2004) and Wu (1996) for phrase-based
and SCFG-based systems, respectively, which we
may think of as doing a finer-grained version of the
deductions above. Each node v in the forest will
be split into a set of augmented items, which we
call +LM items. For phrase-based decoding, a +LM
item has the form (v a) where a is the last word
of the hypothesis. Thus a +LM version of Deduc-
tion (1) might be:
( ??? talk) : (w, ?held a talk?)
(????? Sharon) : (w?, ?held a talk with Sharon?)
145
1.0
1.1
3.5
1.0 4.0 7.0
2.5 8.3 8.5
2.4 9.5 8.4
9.2 17.0 15.2
(VP held ? meeting3,6 )
(VP held ? talk3,6 )
(VP hold ? conference3,6 )
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
1.0 4.0 7.0
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
2.5
2.4
8.3
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
1.0 4.0 7.0
2.5
2.4
8.3
9.5
9.2
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
1.0 4.0 7.0
2.5
2.4
8.3
9.2
9.5
8.5
(a) (b) (c) (d)
Figure 1: Cube pruning along one hyperedge. (a): the numbers in the grid denote the score of the resulting
+LM item, including the combination cost; (b)-(d): the best-first enumeration of the top three items. Notice
that the items popped in (b) and (c) are out of order due to the non-monotonicity of the combination cost.
where the score of the resulting +LM item
w? = w + c? logPlm(with | talk)
now includes a combination cost due to the bigrams
formed when applying the phrase-pair.
Similarly, a +LM item in SCFG-based models
has the form (va?b), where a and b are boundary
words of the hypothesis string, and ? is a placeholder
symbol for an elided part of that string, indicating
that a possible translation of the part of the input
spanned by v starts with a and ends with b. An ex-
ample +LM version of Deduction (2) is:
(PP with ? Sharon1,3 ): (w1, t1) (VP held ? talk3,6 ): (w2, t2)
(VP held ? Sharon1,6 ): (w, t2t1)
where w = w1 +w2 +c?? logPlm(with | talk) with
a similar combination cost formed in combining ad-
jacent boundary words of antecedents. This scheme
can be easily extended to work with a general n-
gram model (Chiang, 2007). The experiments in this
paper use trigram models.
The conventional full-integration approach tra-
verses the forest bottom-up and explores all pos-
sible +LM deductions along each hyperedge.
The theoretical running time of this algorithm
is O(|F ||T |(m?1)) for phrase-based models, and
O(|F ||T |4(m?1)) for binary-branching SCFG-based
models, where |F | is the size of the forest, and |T |
is the number of possible target-side words. Even
if we assume a constant number of translations for
each word in the input, with a trigram model, this
still amounts to O(n11) for SCFG-based models and
O(2nn2) for phrase-based models.
3 Cube Pruning
Cube pruning (Chiang, 2007) reduces the search
space significantly based on the observation that
when the above method is combined with beam
search, only a small fraction of the possible +LM
items at a node will escape being pruned, and more-
over we can select with reasonable accuracy those
top-k items without computing all possible items
first. In a nutshell, cube pruning works on the ?LM
forest, keeping at most k +LM items at each node,
and uses the k-best parsing Algorithm 2 of Huang
and Chiang (2005) to speed up the computation.
For simplicity of presentation, we will use concrete
SCFG-based examples, but the method applies to the
general hypergraph framework in Section 2.
Consider Figure 1(a). Here k = 3 and we use
D(v) to denote the top-k +LM items (in sorted or-
der) of node v. Suppose we have computed D(u1)
and D(u2) for the two antecedent nodes u1 =
(VP3,6) and u2 = (PP1,3) respectively. Then for
the consequent node v = (VP1,6) we just need
to derive the top-3 from the 9 combinations of
(Di(u1), Dj(u2)) with i, j ? [1, 3]. Since the an-
tecedent items are sorted, it is very likely that the
best consequent items in this grid lie towards the
upper-left corner. This situation is very similar to k-
best parsing and we can adapt the Algorithm 2 of
Huang and Chiang (2005) here to explore this grid
in a best-first order.
Suppose that the combination costs are negligible,
and therefore the weight of a consequent item is just
the product of the weights of the antecedent items.
146
1: function CUBE(F ) ? the input is a forest F
2: for v ? F in (bottom-up) topological order do
3: KBEST(v)
4: return D1(TOP)
5: procedure KBEST(v)
6: cand ? {?e,1? | e ? IN (v)} ? for each incoming e
7: HEAPIFY(cand) ? a priority queue of candidates
8: buf ? ?
9: while |cand | > 0 and |buf | < k do
10: item? POP-MIN(cand)
11: append item to buf
12: PUSHSUCC(item, cand)
13: sort buf to D(v)
14: procedure PUSHSUCC(?e, j?, cand )
15: e is v ? u1 . . . u|e|
16: for i in 1 . . . |e| do
17: j? ? j + bi
18: if |D(ui)| ? j?i then
19: PUSH(?e, j??, cand)
Figure 2: Pseudocode for cube pruning.
Then we know that D1(v) = (D1(u1), D1(u2)),
the upper-left corner of the grid. Moreover, we
know that D2(v) is the better of (D1(u1), D2(u2))
and (D2(u1), D1(u2)), the two neighbors of the
upper-left corner. We continue in this way (see Fig-
ure 1(b)?(d)), enumerating the consequent items
best-first while keeping track of a relatively small
number of candidates (shaded cells in Figure 1(b),
cand in Figure 2) for the next-best item.
However, when we take into account the combi-
nation costs, this grid is no longer monotonic in gen-
eral, and the above algorithm will not always enu-
merate items in best-first order. We can see this in
the first iteration in Figure 1(b), where an item with
score 2.5 has been enumerated even though there is
an item with score 2.4 still to come. Thus we risk
making more search errors than the full-integration
method, but in practice the loss is much less signif-
icant than the speedup. Because of this disordering,
we do not put the enumerated items directly into
D(v); instead, we collect items in a buffer (buf in
Figure 2) and re-sort the buffer into D(v) after it has
accumulated k items.1
In general the grammar may have multiple rules
that share the same source side but have different
target sides, which we have treated here as separate
1Notice that different combinations might have the same re-
sulting item, in which case we only keep the one with the better
score (sometimes called hypothesis recombination in MT liter-
ature), so the number of items in D(v) might be less than k.
method k-best +LM rescoring. . .
rescoring Alg. 3 only at the root node
cube pruning Alg. 2 on-the-fly at each node
cube growing Alg. 3 on-the-fly at each node
Table 1: Comparison of the three methods.
hyperedges in the ?LM forest. In Hiero, these hy-
peredges are processed as a single unit which we
call a hyperedge bundle. The different target sides
then constitute a third dimension of the grid, form-
ing a cube of possible combinations (Chiang, 2007).
Now consider that there are many hyperedges that
derive v, and we are only interested the top +LM
items of v over all incoming hyperedges. Following
Algorithm 2, we initialize the priority queue cand
with the upper-left corner item from each hyper-
edge, and proceed as above. See Figure 2 for the
pseudocode for cube pruning. We use the notation
?e, j? to identify the derivation of v via the hyper-
edge e and the jith best subderivation of antecedent
ui (1 ? i ? |j|). Also, we let 1 stand for a vec-
tor whose elements are all 1, and bi for the vector
whose members are all 0 except for the ith whose
value is 1 (the dimensionality of either should be ev-
ident from the context). The heart of the algorithm
is lines 10?12. Lines 10?11 move the best deriva-
tion ?e, j? from cand to buf , and then line 12 pushes
its successors {?e, j + bi? | i ? 1 . . . |e|} into cand .
4 Cube Growing
Although much faster than full-integration, cube
pruning still computes a fixed amount of +LM items
at each node, many of which will not be useful for
arriving at the 1-best hypothesis at the root. It would
be more efficient to compute as few +LM items at
each node as are needed to obtain the 1-best hypoth-
esis at the root. This new method, called cube grow-
ing, is a lazy version of cube pruning just as Algo-
rithm 3 of Huang and Chiang (2005), is a lazy ver-
sion of Algorithm 2 (see Table 1).
Instead of traversing the forest bottom-up, cube
growing visits nodes recursively in depth-first or-
der from the root node (Figure 4). First we call
LAZYJTHBEST(TOP, 1), which uses the same al-
gorithm as cube pruning to find the 1-best +LM
item of the root node using the best +LM items of
147
1.0
1.1
3.5
1.0 4.0 7.0
2.1 5.1 8.1
2.2 5.2 8.2
4.6 7.6 10.6
1.0 4.0 7.0
2.5
2.4
8.3
(a) h-values (b) true costs
Figure 3: Example of cube growing along one hyper-
edge. (a): the h(x) scores for the grid in Figure 1(a),
assuming hcombo(e) = 0.1 for this hyperedge; (b)
cube growing prevents early ranking of the top-left
cell (2.5) as the best item in this grid.
the antecedent nodes. However, in this case the best
+LM items of the antecedent nodes are not known,
because we have not visited them yet. So we re-
cursively invoke LAZYJTHBEST on the antecedent
nodes to obtain them as needed. Each invocation of
LAZYJTHBEST(v, j) will recursively call itself on
the antecedents of v until it is confident that the jth
best +LM item for node v has been found.
Consider again the case of one hyperedge e. Be-
cause of the nonmonotonicity caused by combina-
tion costs, the first +LM item (?e,1?) popped from
cand is not guaranteed to be the best of all combina-
tions along this hyperedge (for example, the top-left
cell of 2.5 in Figure 1 is not the best in the grid). So
we cannot simply enumerate items just as they come
off of cand .2 Instead, we need to store up popped
items in a buffer buf , just as in cube pruning, and
enumerate an item only when we are confident that it
will never be surpassed in the future. In other words,
we would like to have an estimate of the best item
not explored yet (analogous to the heuristic func-
tion in A* search). If we can establish a lower bound
hcombo(e) on the combination cost of any +LM de-
duction via hyperedge e, then we can form a mono-
tonic grid (see Figure 3(a)) of lower bounds on the
grid of combinations, by using hcombo(e) in place of
the true combination cost for each +LM item x in
the grid; call this lower bound h(x).
Now suppose that the gray-shaded cells in Fig-
ure 3(a) are the members of cand . Then the min-
imum of h(x) over the items in cand , in this ex-
2If we did, then the out-of-order enumeration of +LM items
at an antecedent node would cause an entire row or column in
the grid to be disordered at the consequent node, potentially
leading to a multiplication of search errors.
1: procedure LAZYJTHBEST(v, j)
2: if cand [v] is undefined then
3: cand [v]? ?
4: FIRE(e,1, cand) foreach e ? IN (v)
5: buf [v]? ?
6: while |D(v)| < j and |buf [v]| + |D(v)| < k and
|cand [v]| > 0 do
7: item? POP-MIN(cand [v])
8: PUSH(item, buf [v])
9: PUSHSUCC(item, cand [v])
10: bound ? min{h(x) | x ? cand [v]}
11: ENUM(buf [v],D(v), bound)
12: ENUM(buf [v],D(v), +?)
13: procedure FIRE(e, j, cand )
14: e is v ? u1 . . . u|e|
15: for i in 1 . . . |e| do
16: LAZYJTHBEST(ui, ji)
17: if |D(ui)| < ji then return
18: PUSH(?e, j?, cand)
19: procedure PUSHSUCC(?e, j?, cand )
20: FIRE(e, j + bi, cand) foreach i in 1 . . . |e|
21: procedure ENUM(buf ,D, bound )
22: while |buf | > 0 and MIN(buf ) < bound do
23: append POP-MIN(buf ) to D
Figure 4: Pseudocode of cube growing.
ample, min{2.2, 5.1} = 2.2 is a lower bound on
the cost of any item in the future for the hyperedge
e. Indeed, if cand contains items from multiple hy-
peredges for a single consequent node, this is still a
valid lower bound. More formally:
Lemma 1. For each node v in the forest, the term
bound = min
x?cand [v]
h(x) (3)
is a lower bound on the true cost of any future item
that is yet to be explored for v.
Proof. For any item x that is not explored yet, the
true cost c(x) ? h(x), by the definition of h. And
there exists an item y ? cand[v] along the same hy-
peredge such that h(x) ? h(y), due to the mono-
tonicity of h within the grid along one hyperedge.
We also have h(y) ? bound by the definition of
bound. Therefore c(x) ? bound .
Now we can safely pop the best item from buf if
its true cost MIN(buf ) is better than bound and pass
it up to the consequent node (lines 21?23); but other-
wise, we have to wait for more items to accumulate
in buf to prevent a potential search error, for exam-
ple, in the case of Figure 3(b), where the top-left cell
148
(a)
1 2 3 4 5
(b)
1 2 3 4 5
Figure 5: (a) Pharaoh expands the hypotheses in the
current bin (#2) into longer ones. (b) In Cubit, hy-
potheses in previous bins are fed via hyperedge bun-
dles (solid arrows) into a priority queue (shaded tri-
angle), which empties into the current bin (#5).
(2.5) is worse than the current bound of 2.2. The up-
date of bound in each iteration (line 10) can be effi-
ciently implemented by using another heap with the
same contents as cand but prioritized by h instead.
In practice this is a negligible overhead on top of
cube pruning.
We now turn to the problem of estimating the
heuristic function hcombo . In practice, computing
true lower bounds of the combination costs is too
slow and would compromise the speed up gained
from cube growing. So we instead use a much sim-
pler method that just calculates the minimum com-
bination cost of each hyperedge in the top-i deriva-
tions of the root node in ?LM decoding. This is
just an approximation of the true lower bound, and
bad estimates can lead to search errors. However, the
hope is that by choosing the right value of i, these es-
timates will be accurate enough to affect the search
quality only slightly, which is analogous to ?almost
admissible? heuristics in A* search (Soricut, 2006).
5 Experiments
We test our methods on two large-scale English-to-
Chinese translation systems: a phrase-based system
and our tree-to-string system (Huang et al, 2006).
1.0
1.1
3.5
1.0 4.0 7.0
2.5 8.3 8.5
2.4 9.5 8.4
9.2 17.0 15.2
( ??? meeting)
( ??? talk)
( ??? conference)
with
Sha
ron
an
d Sh
aro
n
with
Arie
l Sh
aro
n
.
.
.
Figure 6: A hyperedge bundle represents all +LM
deductions that derives an item in the current bin
from the same coverage vector (see Figure 5). The
phrases on the top denote the target-sides of appli-
cable phrase-pairs sharing the same source-side.
5.1 Phrase-based Decoding
We implemented Cubit, a Python clone of the
Pharaoh decoder (Koehn, 2004),3 and adapted cube
pruning to it as follows. As in Pharaoh, each bin
i contains hypotheses (i.e., +LM items) covering i
words on the source-side. But at each bin (see Fig-
ure 5), all +LM items from previous bins are first
partitioned into ?LM items; then the hyperedges
leading from those ?LM items are further grouped
into hyperedge bundles (Figure 6), which are placed
into the priority queue of the current bin.
Our data preparation follows Huang et al (2006):
the training data is a parallel corpus of 28.3M words
on the English side, and a trigram language model is
trained on the Chinese side. We use the same test set
as (Huang et al, 2006), which is a 140-sentence sub-
set of the NIST 2003 test set with 9?36 words on the
English side. The weights for the log-linear model
are tuned on a separate development set. We set the
decoder phrase-table limit to 100 as suggested in
(Koehn, 2004) and the distortion limit to 4.
Figure 7(a) compares cube pruning against full-
integration in terms of search quality vs. search ef-
ficiency, under various pruning settings (threshold
beam set to 0.0001, stack size varying from 1 to
200). Search quality is measured by average model
cost per sentence (lower is better), and search effi-
ciency is measured by the average number of hy-
potheses generated (smaller is faster). At each level
3In our tests, Cubit always obtains a BLEU score within
0.004 of Pharaoh?s (Figure 7(b)). Source code available at
http://www.cis.upenn.edu/
?
lhuang3/cubit/
149
76
80
84
88
92
102 103 104 105 106
a
ve
ra
ge
 m
od
el
 c
os
t
average number of hypotheses per sentence
full-integration (Cubit)
cube pruning (Cubit)
0.200
0.205
0.210
0.215
0.220
0.225
0.230
0.235
0.240
0.245
102 103 104 105 106
BL
EU
 s
co
re
average number of hypotheses per sentence
Pharaoh
full-integration (Cubit)
cube pruning (Cubit)
(a) (b)
Figure 7: Cube pruning vs. full-integration (with beam search) on phrase-based decoding.
of search quality, the speed-up is always better than
a factor of 10. The speed-up at the lowest search-
error level is a factor of 32. Figure 7(b) makes a
similar comparison but measures search quality by
BLEU, which shows an even larger relative speed-up
for a given BLEU score, because translations with
very different model costs might have similar BLEU
scores. It also shows that our full-integration imple-
mentation in Cubit faithfully reproduces Pharaoh?s
performance. Fixing the stack size to 100 and vary-
ing the threshold yielded a similar result.
5.2 Tree-to-string Decoding
In tree-to-string (also called syntax-directed) decod-
ing (Huang et al, 2006; Liu et al, 2006), the source
string is first parsed into a tree, which is then re-
cursively converted into a target string according to
transfer rules in a synchronous grammar (Galley et
al., 2006). For instance, the following rule translates
an English passive construction into Chinese:
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? be`i x2 x1
Our tree-to-string system performs slightly bet-
ter than the state-of-the-art phrase-based system
Pharaoh on the above data set. Although differ-
ent from the SCFG-based systems in Section 2, its
derivation trees remain context-free and the search
space is still a hypergraph, where we can adapt the
methods presented in Sections 3 and 4.
The data set is same as in Section 5.1, except that
we also parsed the English-side using a variant of
the Collins (1997) parser, and then extracted 24.7M
tree-to-string rules using the algorithm of (Galley et
al., 2006). Since our tree-to-string rules may have
many variables, we first binarize each hyperedge in
the forest on the target projection (Huang, 2007).
All the three +LM decoding methods to be com-
pared below take these binarized forests as input. For
cube growing, we use a non-duplicate k-best method
(Huang et al, 2006) to get 100-best unique transla-
tions according to ?LM to estimate the lower-bound
heuristics.4 This preprocessing step takes on aver-
age 0.12 seconds per sentence, which is negligible
in comparison to the +LM decoding time.
Figure 8(a) compares cube growing and cube
pruning against full-integration under various beam
settings in the same fashion of Figure 7(a). At the
lowest level of search error, the relative speed-up
from cube growing and cube pruning compared with
full-integration is by a factor of 9.8 and 4.1, respec-
tively. Figure 8(b) is a similar comparison in terms
of BLEU scores and shows an even bigger advantage
of cube growing and cube pruning over the baseline.
4If a hyperedge is not represented at all in the 100-best?LM
derivations at the root node, we use the 1-best ?LM derivation
of this hyperedge instead. Here, rules that share the same source
side but have different target sides are treated as separate hy-
peredges, not collected into hyperedge bundles, since grouping
becomes difficult after binarization.
150
218.2
218.4
218.6
218.8
219.0
103 104 105
a
ve
ra
ge
 m
od
el
 c
os
t
average number of +LM items explored per sentence
full-integration
cube pruning
cube growing
0.254
0.256
0.258
0.260
0.262
103 104 105
BL
EU
 s
co
re
average number of +LM items explored per sentence
full-integration
cube pruning
cube growing
(a) (b)
Figure 8: Cube growing vs. cube pruning vs. full-integration (with beam search) on tree-to-string decoding.
6 Conclusions and Future Work
We have presented a novel extension of cube prun-
ing called cube growing, and shown how both can be
seen as general forest rescoring techniques applica-
ble to both phrase-based and syntax-based decoding.
We evaluated these methods on large-scale transla-
tion tasks and observed considerable speed improve-
ments, often by more than a factor of ten. We plan
to investigate how to adapt cube growing to phrase-
based and hierarchical phrase-based systems.
These forest rescoring algorithms have potential
applications to other computationally intensive tasks
involving combinations of different models, for
example, head-lexicalized parsing (Collins, 1997);
joint parsing and semantic role labeling (Sutton and
McCallum, 2005); or tagging and parsing with non-
local features. Thus we envision forest rescoring as
being of general applicability for reducing compli-
cated search spaces, as an alternative to simulated
annealing methods (Kirkpatrick et al, 1983).
References
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2). To appear.
Michael Collins. 1997. Three generative lexicalised models for
statistical parsing. In Proc. ACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models. In
Proc. COLING-ACL.
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proc. IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Sta-
tistical syntax-directed translation with extended domain of
locality. In Proc. AMTA.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proc. NAACL Workshop on
Syntax and Structure in Statistical Translation.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220(4598):671?680.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proc. AMTA, pages 115?124.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduc-
tion. J. ACM, 15:465?488.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proc. COLING-ACL, pages 609?616.
Franz Joseph Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417?449.
Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995.
Principles and implementation of deductive parsing. J. Logic
Programming, 24:3?36.
Radu Soricut. 2006. Natural Language Generation using an
Information-Slim Representation. Ph.D. thesis, University
of Southern California.
Charles Sutton and Andrew McCallum. 2005. Joint parsing
and semantic role labeling. In Proc. CoNLL 2005.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. In Proc. ACL.
151
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567?575,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Fast Consensus Decoding over Translation Forests
John DeNero David Chiang and Kevin Knight
Computer Science Division Information Sciences Institute
University of California, Berkeley University of Southern California
denero@cs.berkeley.edu {chiang, knight}@isi.edu
Abstract
The minimum Bayes risk (MBR) decoding ob-
jective improves BLEU scores for machine trans-
lation output relative to the standard Viterbi ob-
jective of maximizing model score. However,
MBR targeting BLEU is prohibitively slow to op-
timize over k-best lists for large k. In this pa-
per, we introduce and analyze an alternative to
MBR that is equally effective at improving per-
formance, yet is asymptotically faster ? running
80 times faster than MBR in experiments with
1000-best lists. Furthermore, our fast decoding
procedure can select output sentences based on
distributions over entire forests of translations, in
addition to k-best lists. We evaluate our proce-
dure on translation forests from two large-scale,
state-of-the-art hierarchical machine translation
systems. Our forest-based decoding objective
consistently outperforms k-best list MBR, giving
improvements of up to 1.0 BLEU.
1 Introduction
In statistical machine translation, output transla-
tions are evaluated by their similarity to human
reference translations, where similarity is most of-
ten measured by BLEU (Papineni et al, 2002).
A decoding objective specifies how to derive final
translations from a system?s underlying statistical
model. The Bayes optimal decoding objective is
to minimize risk based on the similarity measure
used for evaluation. The corresponding minimum
Bayes risk (MBR) procedure maximizes the ex-
pected similarity score of a system?s translations
relative to the model?s distribution over possible
translations (Kumar and Byrne, 2004). Unfortu-
nately, with a non-linear similarity measure like
BLEU, we must resort to approximating the ex-
pected loss using a k-best list, which accounts for
only a tiny fraction of a model?s full posterior dis-
tribution. In this paper, we introduce a variant
of the MBR decoding procedure that applies effi-
ciently to translation forests. Instead of maximiz-
ing expected similarity, we express similarity in
terms of features of sentences, and choose transla-
tions that are similar to expected feature values.
Our exposition begins with algorithms over k-
best lists. A na??ve algorithm for finding MBR
translations computes the similarity between every
pair of k sentences, entailing O(k2) comparisons.
We show that if the similarity measure is linear in
features of a sentence, then computing expected
similarity for all k sentences requires only k sim-
ilarity evaluations. Specific instances of this gen-
eral algorithm have recently been proposed for two
linear similarity measures (Tromble et al, 2008;
Zhang and Gildea, 2008).
However, the sentence similarity measures we
want to optimize in MT are not linear functions,
and so this fast algorithm for MBR does not ap-
ply. For this reason, we propose a new objective
that retains the benefits of MBR, but can be op-
timized efficiently, even for non-linear similarity
measures. In experiments using BLEU over 1000-
best lists, we found that our objective provided
benefits very similar to MBR, only much faster.
This same decoding objective can also be com-
puted efficiently from forest-based expectations.
Translation forests compactly encode distributions
over much larger sets of derivations and arise nat-
urally in chart-based decoding for a wide variety
of hierarchical translation systems (Chiang, 2007;
Galley et al, 2006; Mi et al, 2008; Venugopal
et al, 2007). The resulting forest-based decoding
procedure compares favorably in both complexity
and performance to the recently proposed lattice-
based MBR (Tromble et al, 2008).
The contributions of this paper include a linear-
time algorithm for MBR using linear similarities,
a linear-time alternative to MBR using non-linear
similarity measures, and a forest-based extension
to this procedure for similarities based on n-gram
counts. In experiments, we show that our fast pro-
cedure is on average 80 times faster than MBR
using 1000-best lists. We also show that using
forests outperforms using k-best lists consistently
across language pairs. Finally, in the first pub-
lished multi-system experiments on consensus de-
567
coding for translation, we demonstrate that bene-
fits can differ substantially across systems. In all,
we show improvements of up to 1.0 BLEU from
consensus approaches for state-of-the-art large-
scale hierarchical translation systems.
2 Consensus Decoding Algorithms
Let e be a candidate translation for a sentence f ,
where e may stand for a sentence or its derivation
as appropriate. Modern statistical machine trans-
lation systems take as input some f and score each
derivation e according to a linear model of fea-
tures:
?
i ?i ??i(f, e). The standard Viterbi decod-
ing objective is to find e? = arg maxe ? ? ?(f, e).
For MBR decoding, we instead leverage a sim-
ilarity measure S(e; e?) to choose a translation us-
ing the model?s probability distribution P(e|f),
which has support over a set of possible transla-
tions E. The Viterbi derivation e? is the mode of
this distribution. MBR is meant to choose a trans-
lation that will be similar, on expectation, to any
possible reference translation. To this end, MBR
chooses e? that maximizes expected similarity to
the sentences in E under P(e|f):1
e? = arg maxe EP(e?|f)
[
S(e; e?)
]
= arg maxe
?
e??E
P(e?|f) ? S(e; e?)
MBR can also be interpreted as a consensus de-
coding procedure: it chooses a translation similar
to other high-posterior translations. Minimizing
risk has been shown to improve performance for
MT (Kumar and Byrne, 2004), as well as other
language processing tasks (Goodman, 1996; Goel
and Byrne, 2000; Kumar and Byrne, 2002; Titov
and Henderson, 2006; Smith and Smith, 2007).
The distribution P(e|f) can be induced from a
translation system?s features and weights by expo-
nentiating with base b to form a log-linear model:
P (e|f) =
b???(f,e)
?
e??E b
???(f,e?)
We follow Ehling et al (2007) in choosing b using
a held-out tuning set. For algorithms in this sec-
tion, we assume that E is a k-best list and b has
been chosen already, so P(e|f) is fully specified.
1Typically, MBR is defined as arg mine?EE[L(e; e
?)] for
some loss function L, for example 1 ? BLEU(e; e?). These
definitions are equivalent.
2.1 Minimum Bayes Risk over Sentence Pairs
Given any similarity measure S and a k-best
list E, the minimum Bayes risk translation can
be found by computing the similarity between all
pairs of sentences in E, as in Algorithm 1.
Algorithm 1 MBR over Sentence Pairs
1: A? ??
2: for e ? E do
3: Ae ? 0
4: for e? ? E do
5: Ae ? Ae + P (e?|f) ? S(e; e?)
6: if Ae > A then A, e?? Ae, e
7: return e?
We can sometimes exit the inner for loop early,
whenever Ae can never become larger than A
(Ehling et al, 2007). Even with this shortcut, the
running time of Algorithm 1 is O(k2 ? n), where
n is the maximum sentence length, assuming that
S(e; e?) can be computed in O(n) time.
2.2 Minimum Bayes Risk over Features
We now consider the case when S(e; e?) is a lin-
ear function of sentence features. Let S(e; e?) be
a function of the form
?
j ?j(e) ? ?j(e
?), where
?j(e?) are real-valued features of e?, and ?j(e) are
sentence-specific weights on those features. Then,
the MBR objective can be re-written as
arg maxe?E EP(e?|f)
[
S(e; e?)
]
= arg maxe
?
e??E
P (e?|f) ?
?
j
?j(e) ? ?j(e
?)
= arg maxe
?
j
?j(e)
[
?
e??E
P (e?|f) ? ?j(e
?)
]
= arg maxe
?
j
?j(e) ? EP(e?|f)
[
?j(e
?)
]
. (1)
Equation 1 implies that we can find MBR trans-
lations by first computing all feature expectations,
then applying S only once for each e. Algorithm 2
proceduralizes this idea: lines 1-4 compute feature
expectations, and lines 5-11 find the translation
with highest S relative to those expectations. The
time complexity is O(k ?n), assuming the number
of non-zero features ?(e?) and weights ?(e) grow
linearly in sentence length n and all features and
weights can be computed in constant time.
568
Algorithm 2 MBR over Features
1: ??? [0 for j ? J ]
2: for e? ? E do
3: for j ? J such that ?j(e?) 6= 0 do
4: ??j ? ??j + P (e?|f) ? ?j(e?)
5: A? ??
6: for e ? E do
7: Ae ? 0
8: for j ? J such that ?j(e) 6= 0 do
9: Ae ? Ae + ?j(e) ? ??j
10: if Ae > A then A, e?? Ae, e
11: return e?
An example of a linear similarity measure is
bag-of-words precision, which can be written as:
U(e; e?) =
?
t?T1
?(e, t)
|e|
? ?(e?, t)
where T1 is the set of unigrams in the language,
and ?(e, t) is an indicator function that equals 1
if t appears in e and 0 otherwise. Figure 1 com-
pares Algorithms 1 and 2 using U(e; e?). Other
linear functions have been explored for MBR, in-
cluding Taylor approximations to the logarithm of
BLEU (Tromble et al, 2008) and counts of match-
ing constituents (Zhang and Gildea, 2008), which
are discussed further in Section 3.3.
2.3 Fast Consensus Decoding using
Non-Linear Similarity Measures
Most similarity measures of interest for machine
translation are not linear, and so Algorithm 2 does
not apply. Computing MBR even with simple
non-linear measures such as BLEU, NIST or bag-
of-words F1 seems to require O(k2) computation
time. However, these measures are all functions
of features of e?. That is, they can be expressed as
S(e;?(e?)) for a feature mapping ? : E ? Rn.
For example, we can express BLEU(e; e?) =
exp
"?
1 ?
|e?|
|e|
?
?
+
1
4
4X
n=1
ln
P
t?Tn
min(c(e, t), c(e?, t))
P
t?Tn
c(e, t)
#
In this expression, BLEU(e; e?) references e? only
via its n-gram count features c(e?, t).2
2The length penalty
?
1 ? |e
?|
|e|
?
?
is also a function of n-
gram counts: |e?| =
P
t?T1
c(e?, t). The negative part oper-
ator (?)? is equivalent to min(?, 0).
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [r(man with)] = 0.4 + 0.6 ? 1.0
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
22.5
45.0
67.5
90.0
Hiero SBMT
70.2
84.6
56.6
61.4
51.1
50.5
Viterbi n-gram precision
Forest n-gram precision at Viterbi recall
Forest n-gram precision for Er(t) ? 1
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+ 13)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) = 0.6+0.7+0.73
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
Figure 1: For the linear similarity measure U(e; e?), which
computes unigram precision, the MBR translation can be
found by iterating either over s ntence pairs (Algorithm 1) or
over features (Algorithm 2). These two algorithms take the
same input (step 1), but diverge in their consensus computa-
tions (steps 2 & 3). However, they produce identical results
for U and any other linear similarity measure.
Following the structure of Equation 1, we can
choose a translation e based on the feature expec-
tations of e?. In particular, we can choose
e? = arg maxe?ES(e;EP(e?|f)
[
?(e?)
]
). (2)
This objective differs from MBR, but has a simi-
lar consensus-building structure. We have simply
moved the expectation inside the similarity func-
tion, just as we did in Equation 1. This new ob-
jective can be optimized by Algorithm 3, a pro-
cedure that runs in O(k ? n) time if the count of
non-zero features in e? and the computation time
of S(e;?(e?)) are both linear in sentence length n.
This fast consensus decoding procedure shares
the same structure as linear MBR: first we com-
pute feature expectations, then we choose the sen-
tence that is most similar to those expectations. In
fact, Algorithm 2 is a special case of Algorithm 3.
Lines 7-9 of the former and line 7 of the latter are
equivalent for linear S(e; e?). Thus, for any linear
similarity measure, Algorithm 3 is an algorithm
for minimum Bayes risk decoding.
569
Algorithm 3 Fast Consensus Decoding
1: ??? [0 for j ? J ]
2: for e? ? E do
3: for j ? J such that ?j(e?) 6= 0 do
4: ??j ? ??j + P (e?|f) ? ?j(e?)
5: A? ??
6: for e ? E do
7: Ae ? S(e; ??)
8: if Ae > A then A, e?? Ae, e
9: return e?
As described, Algorithm 3 can use any sim-
ilarity measure that is defined in terms of real-
valued features of e?. There are some nuances
of this procedure, however. First, the precise
form of S(e;?(e?)) will affect the output, but
S(e;E[?(e?)]) is often an input point for which a
sentence similarity measure S was not originally
defined. For example, our definition of BLEU
above will have integer valued ?(e?) for any real
sentence e?, butE[?(e?)]will not be integer valued.
As a result, we are extending the domain of BLEU
beyond its original intent. One could imagine dif-
ferent feature-based expressions that also produce
BLEU scores for real sentences, but produce dif-
ferent values for fractional features. Some care
must be taken to define S(e;?(e?)) to extend nat-
urally from integer-valued to real-valued features.
Second, while any similarity measure can in
principle be expressed as S(e;?(e?)) for a suffi-
ciently rich feature space, fast consensus decoding
will not apply effectively to all functions. For in-
stance, we cannot naturally use functions that in-
clude alignments or matchings between e and e?,
such as METEOR (Agarwal and Lavie, 2007) and
TER (Snover et al, 2006). Though these functions
can in principle be expressed in terms of features
of e? (for instance with indicator features for whole
sentences), fast consensus decoding will only be
effective if different sentences share many fea-
tures, so that the feature expectations effectively
capture trends in the underlying distribution.
3 Computing Feature Expectations
We now turn our focus to efficiently comput-
ing feature expectations, in service of our fast
consensus decoding procedure. Computing fea-
ture expectations from k-best lists is trivial, but
k-best lists capture very little of the underlying
model?s posterior distribution. In place of k-best
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
20
40
60
80
Hiero SBMT
56.6
61.4
51.1
50.5
N-grams from baseline translation
N-grams with high expected count
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+ 13)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) = 0.6+0.7+0.73
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [c(e, ?man with?)] =
?
h
P (h|f) ? c(h, ?man with?)
= 0.4 ? 1 + (0.6 ? 1.0) ? 1
Figure 2: This translation forest for a Spanish sentence en-
codes two English parse trees. Hyper-edges (boxes) are an-
notated with normalized transition probabilities, as well as
the bigrams produced by each rule application. The expected
count of the bigram ?man with? is the sum of posterior prob-
abilities of the two hyper-edges that produce it. In this exam-
ple, we normalized inside scores at all nodes to 1 for clarity.
lists, compact encodings of translation distribu-
tions have proven effective for MBR (Zhang and
Gildea, 2008; Tromble et al, 2008). In this sec-
tion, we consider BLEU in particular, for which
the relevant features ?(e) are n-gram counts up to
length n = 4. We show how to compute expec-
tations of these counts efficiently from translation
forests.
3.1 Translation Forests
Translation forests compactly encode an exponen-
tial number of output translations for an input
sentence, along with their model scores. Forests
arise naturally in chart-based decoding procedures
for many hierarchical translation systems (Chiang,
2007). Exploiting forests has proven a fruitful av-
enue of research in both parsing (Huang, 2008)
and machine translation (Mi et al, 2008).
Formally, translation forests are weighted
acyclic hyper-graphs. The nodes are states in the
decoding process that include the span (i, j) of the
sentence to be translated, the grammar symbol s
over that span, and the left and right context words
of the translation relevant for computing n-gram
language model scores.3 Each hyper-edge h rep-
resents the application of a synchronous rule r that
combines nodes corresponding to non-terminals in
3Decoder states can include additional information as
well, such as local configurations for dependency language
model scoring.
570
r into a node spanning the union of the child spans
and perhaps some additional portion of the input
sentence covered directly by r?s lexical items. The
weight of h is the incremental score contributed
to all translations containing the rule application,
including translation model features on r and lan-
guage model features that depend on both r and
the English contexts of the child nodes. Figure 2
depicts a forest.
Each n-gram that appears in a translation e is as-
sociated with some h in its derivation: the h corre-
sponding to the rule that produces the n-gram. Un-
igrams are produced by lexical rules, while higher-
order n-grams can be produced either directly by
lexical rules, or by combining constituents. The
n-gram language model score of e similarly de-
composes over the h in e that produce n-grams.
3.2 Computing Expected N-Gram Counts
We can compute expected n-gram counts effi-
ciently from a translation forest by appealing to
the linearity of expectations. Let ?(e) be a vector
of n-gram counts for a sentence e. Then, ?(e) is
the sum of hyper-edge-specific n-gram count vec-
tors ?(h) for all h in e. Therefore, E[?(e)] =
?
h?e E[?(h)].
To compute n-gram expectations for a hyper-
edge, we first compute the posterior probability of
each h, conditioned on the input sentence f :
P(h|f) =
(
?
e:h?e
b???(f,e)
)(
?
e
b???(f,e)
)?1
,
where e iterates over translations in the forest. We
compute the numerator using the inside-outside al-
gorithm, while the denominator is the inside score
of the root node. Note that many possible deriva-
tions of f are pruned from the forest during decod-
ing, and so this posterior is approximate.
The expected n-gram count vector for a hyper-
edge is E[?(h)] = P(h|f) ? ?(h). Hence, after
computing P (h|f) for every h, we need only sum
P(h|f) ? ?(h) for all h to compute E[?(e)]. This
entire procedure is a linear-time computation in
the number of hyper-edges in the forest.
To complete forest-based fast consensus de-
coding, we then extract a k-best list of unique
translations from the forest (Huang et al, 2006)
and continue Algorithm 3 from line 5, which
chooses the e? from the k-best list that maximizes
BLEU(e;E[?(e?)]).
3.3 Comparison to Related Work
Zhang and Gildea (2008) embed a consensus de-
coding procedure into a larger multi-pass decoding
framework. They focus on inversion transduction
grammars, but their ideas apply to richer models as
well. They propose an MBR decoding objective
of maximizing the expected number of matching
constituent counts relative to the model?s distri-
bution. The corresponding constituent-matching
similarity measure can be expressed as a linear
function of features of e?, which are indicators of
constituents. Expectations of constituent indicator
features are the same as posterior constituent prob-
abilities, which can be computed from a transla-
tion forest using the inside-outside algorithm. This
forest-based MBR approach improved translation
output relative to Viterbi translations.
Tromble et al (2008) describe a similar ap-
proach using MBR with a linear similarity mea-
sure. They derive a first-order Taylor approxima-
tion to the logarithm of a slightly modified defini-
tion of corpus BLEU4, which is linear in n-gram
indicator features ?(e?, t) of e?. These features are
weighted by n-gram counts c(e, t) and constants
? that are estimated from held-out data. The lin-
ear similarity measure takes the following form,
where Tn is the set of n-grams:
G(e; e?) = ?0|e|+
4?
n=1
?
t?Tn
?t ? c(e, t) ? ?(e
?, t).
Using G, Tromble et al (2008) extend MBR to
word lattices, which improves performance over
k-best list MBR.
Our approach differs from Tromble et al (2008)
primarily in that we propose decoding with an al-
ternative to MBR using BLEU, while they propose
decoding with MBR using a linear alternative to
BLEU. The specifics of our approaches also differ
in important ways.
First, word lattices are a subclass of forests that
have only one source node for each edge (i.e., a
graph, rather than a hyper-graph). While forests
are more general, the techniques for computing
posterior edge probabilities in lattices and forests
are similar. One practical difference is that the
forests needed for fast consensus decoding are
4The log-BLEU function must be modified slightly to
yield a linear Taylor approximation: Tromble et al (2008)
replace the clipped n-gram count with the product of an n-
gram count and an n-gram indicator function.
571
generated already by the decoder of a syntactic
translation system.
Second, rather than use BLEU as a sentence-
level similarity measure directly, Tromble et al
(2008) approximate corpus BLEU with G above.
The parameters ? of the approximation must be es-
timated on a held-out data set, while our approach
requires no such estimation step.
Third, our approach is also simpler computa-
tionally. The features required to compute G are
indicators ?(e?, t); the features relevant to us are
counts c(e?, t). Tromble et al (2008) compute ex-
pected feature values by intersecting the transla-
tion lattice with a lattices for each n-gram t. By
contrast, expectations of c(e?, t) can all be com-
puted with a single pass over the forest. This con-
trast implies a complexity difference. LetH be the
number of hyper-edges in the forest or lattice, and
T the number of n-grams that can potentially ap-
pear in a translation. Computing indicator expec-
tations seems to require O(H ? T ) time because of
automata intersections. Computing count expec-
tations requires O(H) time, because only a con-
stant number of n-grams can be produced by each
hyper-edge.
Our approaches also differ in the space of trans-
lations from which e? is chosen. A linear similar-
ity measure like G allows for efficient search over
the lattice or forest, whereas fast consensus decod-
ing restricts this search to a k-best list. However,
Tromble et al (2008) showed that most of the im-
provement from lattice-based consensus decoding
comes from lattice-based expectations, not search:
searching over lattices instead of k-best lists did
not change results for two language pairs, and im-
proved a third language pair by 0.3 BLEU. Thus,
we do not consider our use of k-best lists to be a
substantial liability of our approach.
Fast consensus decoding is also similar in char-
acter to the concurrently developed variational de-
coding approach of Li et al (2009). Using BLEU,
both approaches choose outputs that match ex-
pected n-gram counts from forests, though differ
in the details. It is possible to define a similar-
ity measure under which the two approaches are
equivalent.5
5For example, decoding under a variational approxima-
tion to the model?s posterior that decomposes over bigram
probabilities is equivalent to fast consensus decoding with
the similarity measure B(e; e?) =
Q
t?T2
h
c(e?,t)
c(e?,h(t))
ic(e,t)
,
where h(t) is the unigram prefix of bigram t.
4 Experimental Results
We evaluate these consensus decoding techniques
on two different full-scale state-of-the-art hierar-
chical machine translation systems. Both systems
were trained for 2008 GALE evaluations, in which
they outperformed a phrase-based system trained
on identical data.
4.1 Hiero: a Hierarchical MT Pipeline
Hiero is a hierarchical system that expresses its
translation model as a synchronous context-free
grammar (Chiang, 2007). No explicit syntactic in-
formation appears in the core model. A phrase
discovery procedure over word-aligned sentence
pairs provides rule frequency counts, which are
normalized to estimate features on rules.
The grammar rules of Hiero all share a single
non-terminal symbol X , and have at most two
non-terminals and six total items (non-terminals
and lexical items), for example:
my X2 ?s X1 ? X1 de mi X2
We extracted the grammar from training data using
standard parameters. Rules were allowed to span
at most 15 words in the training data.
The log-linear model weights were trained us-
ing MIRA, a margin-based optimization proce-
dure that accommodates many features (Crammer
and Singer, 2003; Chiang et al, 2008). In addition
to standard rule frequency features, we included
the distortion and syntactic features described in
Chiang et al (2008).
4.2 SBMT: a Syntax-Based MT Pipeline
SBMT is a string-to-tree translation system with
rich target-side syntactic information encoded in
the translation model. The synchronous grammar
rules are extracted from word aligned sentence
pairs where the target sentence is annotated with
a syntactic parse (Galley et al, 2004). Rules map
source-side strings to target-side parse tree frag-
ments, and non-terminal symbols correspond to
target-side grammatical categories:
(NP (NP (PRP$ my) NN2 (POS ?s)) NNS1)?
NNS1 de mi NN2
We extracted the grammar via an array of criteria
(Galley et al, 2006; DeNeefe et al, 2007; Marcu
et al, 2006). The model was trained using min-
imum error rate training for Arabic (Och, 2003)
and MIRA for Chinese (Chiang et al, 2008).
572
Arabic-English
Objective Hiero SBMT
Min. Bayes Risk (Alg 1) 2h 47m 12h 42m
Fast Consensus (Alg 3) 5m 49s 5m 22s
Speed Ratio 29 142
Chinese-English
Objective Hiero SBMT
Min. Bayes Risk (Alg 1) 10h 24m 3h 52m
Fast Consensus (Alg 3) 4m 52s 6m 32s
Speed Ratio 128 36
Table 1: Fast consensus decoding is orders of magnitude
faster than MBR when using BLEU as a similarity measure.
Times only include reranking, not k-best list extraction.
4.3 Data Conditions
We evaluated on both Chinese-English and
Arabic-English translation tasks. Both Arabic-
English systems were trained on 220 million
words of word-aligned parallel text. For the
Chinese-English experiments, we used 260 mil-
lion words of word-aligned parallel text; the hi-
erarchical system used all of this data, and the
syntax-based system used a 65-million word sub-
set. All four systems used two language models:
one trained from the combined English sides of
both parallel texts, and another, larger, language
model trained on 2 billion words of English text
(1 billion for Chinese-English SBMT).
All systems were tuned on held-out data (1994
sentences for Arabic-English, 2010 sentences for
Chinese-English) and tested on another dataset
(2118 sentences for Arabic-English, 1994 sen-
tences for Chinese-English). These datasets were
drawn from the NIST 2004 and 2005 evaluation
data, plus some additional data from the GALE
program. There was no overlap at the segment or
document level between the tuning and test sets.
We tuned b, the base of the log-linear model,
to optimize consensus decoding performance. In-
terestingly, we found that tuning b on the same
dataset used for tuning ?was as effective as tuning
b on an additional held-out dataset.
4.4 Results over K-Best Lists
Taking expectations over 1000-best lists6 and us-
ing BLEU7 as a similarity measure, both MBR
6We ensured that k-best lists contained no duplicates.
7To prevent zero similarity scores, we also used a standard
smoothed version of BLEU that added 1 to the numerator and
denominator of all n-gram precisions. Performance results
Arabic-English
Expectations Similarity Hiero SBMT
Baseline - 52.0 53.9
104-best BLEU 52.2 53.9
Forest BLEU 53.0 54.0
Forest Linear G 52.3 54.0
Chinese-English
Expectations Similarity Hiero SBMT
Baseline - 37.8 40.6
104-best BLEU 38.0 40.7
Forest BLEU 38.2 40.8
Forest Linear G 38.1 40.8
Table 2: Translation performance improves when computing
expected sentences from translation forests rather than 104-
best lists, which in turn improve over Viterbi translations. We
also contrasted forest-based consensus decoding with BLEU
and its linear approximation, G. Both similarity measures are
effective, but BLEU outperforms G.
and our variant provided consistent small gains of
0.0?0.2 BLEU. Algorithms 1 and 3 gave the same
small BLEU improvements in each data condition
up to three significant figures.
The two algorithms differed greatly in speed,
as shown in Table 1. For Algorithm 1, we ter-
minated the computation of E[BLEU(e; e?)] for
each e whenever e could not become the maxi-
mal hypothesis. MBR speed depended on how
often this shortcut applied, which varied by lan-
guage and system. Despite this optimization, our
new Algorithm 3 was an average of 80 times faster
across systems and language pairs.
4.5 Results for Forest-Based Decoding
Table 2 contrasts Algorithm 3 over 104-best lists
and forests. Computing E[?(e?)] from a transla-
tion forest rather than a 104-best list improved Hi-
ero by an additional 0.8 BLEU (1.0 over the base-
line). Forest-based expectations always outper-
formed k-best lists, but curiously the magnitude
of benefit was not consistent across systems. We
believe the difference is in part due to more ag-
gressive forest pruning within the SBMT decoder.
For forest-based decoding, we compared two
similarity measures: BLEU and its linear Taylor
approximationG from section 3.3.8 Table 2 shows
were identical to standard BLEU.
8We did not estimate the ? parameters of G ourselves;
instead we used the parameters listed in Tromble et al
(2008), which were also estimated for GALE data. We
also approximated E[?(e?, t)] with a clipped expected count
573
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
20
40
60
80
Hiero SBMT
56.6
61.4
51.1
50.5
N-grams from baseline translations
N-grams with high expected count
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+
1
3)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) =
0.6+0.7+0.7
3
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [c(e, ?man with?)] =
?
h
P (h|f) ? c(h, ?man with?)
= 0.4 ? 1 + (0.6 ? 1.0) ? 1
N
-
g
r
a
m
 
P
r
e
c
i
s
i
o
n
Figure 3: N -grams with high expected count are more likely
to appear in the reference translation that n-grams in the
translation model?s Viterbi translation, e?. Above, we com-
pare the precision, relative to reference translations, of sets of
n-grams chosen in two ways. The left bar is the precision of
the n-grams in e?. The right bar is the precision of n-grams
with E[c(e, t)] > ?. To justify this comparison, we chose ?
so that both methods of choosing n-grams gave the same n-
gram recall: the fraction of n-grams in reference translations
that also appeared in e? or had E[c(e, t)] > ?.
that both similarities were effective, but BLEU
outperformed its linear approximation.
4.6 Analysis
Forest-based consensus decoding leverages infor-
mation about the correct translation from the en-
tire forest. In particular, consensus decoding
with BLEU chooses translations using n-gram
count expectations E[c(e, t)]. Improvements in
translation quality should therefore be directly at-
tributable to information in these expected counts.
We endeavored to test the hypothesis that ex-
pected n-gram counts under the forest distribution
carry more predictive information than the base-
line Viterbi derivation e?, which is the mode of the
distribution. To this end, we first tested the pre-
dictive accuracy of the n-grams proposed by e?:
the fraction of the n-grams in e? that appear in a
reference translation. We compared this n-gram
precision to a similar measure of predictive accu-
racy for expected n-gram counts: the fraction of
the n-grams t with E[c(e, t)] ? ? that appear in
a reference. To make these two precisions com-
parable, we chose ? such that the recall of ref-
erence n-grams was equal. Figure 3 shows that
computing n-gram expectations?which sum over
translations?improves the model?s ability to pre-
dict which n-grams will appear in the reference.
min(1,E[c(e?, t)]). Assuming an n-gram appears at most
once per sentence, these expressions are equivalent, and this
assumption holds for most n-grams.
Reference translation:
Mubarak said that he received a telephone call from
Sharon in which he said he was ?ready (to resume ne-
gotiations) but the Palestinians are hesitant.?
Baseline translation:
Mubarak said he had received a telephone call from
Sharon told him he was ready to resume talks with the
Palestinians.
Fast forest-based consensus translation:
Mubarak said that he had received a telephone call from
Sharon told him that he ?was ready to resume the nego-
tiations) , but the Palestinians are hesitant.?
Figure 4: Three translations of an example Arabic sentence:
its human-generated reference, the translation with the high-
est model score under Hiero (Viterbi), and the translation
chosen by forest-based consensus decoding. The consensus
translation reconstructs content lost in the Viterbi translation.
We attribute gains from fast consensus decoding
to this increased predictive accuracy.
Examining the translations chosen by fast con-
sensus decoding, we found that gains in BLEU of-
ten arose from improved lexical choice. However,
in our hierarchical systems, consensus decoding
did occasionally trigger large reordering. We also
found examples where the translation quality im-
proved by recovering content that was missing
from the baseline translation, as in Figure 4.
5 Conclusion
We have demonstrated substantial speed increases
in k-best consensus decoding through a new pro-
cedure inspired by MBR under linear similarity
measures. To further improve this approach, we
computed expected n-gram counts from transla-
tion forests instead of k-best lists. Fast consensus
decoding using forest-based n-gram expectations
and BLEU as a similarity measure yielded con-
sistent improvements over MBR with k-best lists,
yet required only simple computations that scale
linearly with the size of the translation forest.
The space of similarity measures is large and
relatively unexplored, and the feature expectations
that can be computed from forests extend beyond
n-gram counts. Therefore, future work may show
additional benefits from fast consensus decoding.
Acknowledgements
This work was supported under DARPA GALE,
Contract No. HR0011-06-C-0022.
574
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Workshop on Statistical Machine
Translation for the Association of Computational
Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and CoNLL.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum Bayes risk decoding for BLEU. In Pro-
ceedings of the Association for Computational Lin-
guistics: Short Paper Track.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT: the North American Chapter
of the Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Vaibhava Goel and William Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. In Com-
puter, Speech and Language.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the Association for Compu-
tational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the Associa-
tion for Machine Translation in the Americas.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2002. Minimum
Bayes-risk word alignments of bilingual texts. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics.
David Smith and Noah Smith. 2007. Probabilistic
models of nonprojective dependency trees. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and CoNLL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Roy Tromble, Shankar Kumar, Franz Josef Och, and
Wolfgang Macherey. 2008. Lattice minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan
Vogel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT: the North American Association
for Computational Linguistics Conference.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the Association for Compu-
tational Linguistics.
575
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Hidden TAG Model: Synchronous Grammars for Parsing
Resource-Poor Languages
David Chiang?
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
Owen Rambow
Center for Computational Learning Systems
Columbia University
475 Riverside Dr., Suite 850
New York, NY, USA
rambow@cs.columbia.edu
Abstract
This paper discusses a novel probabilis-
tic synchronous TAG formalism, syn-
chronous Tree Substitution Grammar with
sister adjunction (TSG+SA). We use it
to parse a language for which there is
no training data, by leveraging off a sec-
ond, related language for which there is
abundant training data. The grammar for
the resource-rich side is automatically ex-
tracted from a treebank; the grammar on
the resource-poor side and the synchro-
nization are created by handwritten rules.
Our approach thus represents a combina-
tion of grammar-based and empirical nat-
ural language processing. We discuss the
approach using the example of Levantine
Arabic and Standard Arabic.
1 Parsing Arabic Dialects and Tree
Adjoining Grammar
The Arabic language is a collection of spoken
dialects and a standard written language. The
standard written language is the same throughout
the Arab world, Modern Standard Arabic (MSA),
which is also used in some scripted spoken com-
munication (news casts, parliamentary debates).
It is based on Classical Arabic and is not a na-
tive language of any Arabic speaking people, i.e.,
children do not learn it from their parents but in
school. Thus most native speakers of Arabic are
unable to produce sustained spontaneous MSA.
The dialects show phonological, morphological,
lexical, and syntactic differences comparable to
?This work was primarily carried out while the first au-
thor was at the University of Maryland Institute for Advanced
Computer Studies.
those among the Romance languages. They vary
not only along a geographical continuum but also
with other sociolinguistic variables such as the ur-
ban/rural/Bedouin dimension.
The multidialectal situation has important neg-
ative consequences for Arabic natural language
processing (NLP): since the spoken dialects are
not officially written and do not have standard or-
thography, it is very costly to obtain adequate cor-
pora, even unannotated corpora, to use for train-
ing NLP tools such as parsers. Furthermore, there
are almost no parallel corpora involving one di-
alect and MSA.
The question thus arises how to create a statisti-
cal parser for an Arabic dialect, when statistical
parsers are typically trained on large corpora of
parse trees. We present one solution to this prob-
lem, based on the assumption that it is easier to
manually create new resources that relate a dialect
to MSA (lexicon and grammar) than it is to man-
ually create syntactically annotated corpora in the
dialect. In this paper, we deal with Levantine Ara-
bic (LA). Our approach does not assume the exis-
tence of any annotated LA corpus (except for de-
velopment and testing), nor of a parallel LA-MSA
corpus.
The approach described in this paper uses a spe-
cial parameterization of stochastic synchronous
TAG (Shieber, 1994) which we call a ?hidden TAG
model.? This model couples a model of MSA
trees, learned from the Arabic Treebank, with a
model of MSA-LA translation, which is initial-
ized by hand and then trained in an unsupervised
fashion. Parsing new LA sentences then entails si-
multaneously building a forest of MSA trees and
the corresponding forest of LA trees. Our imple-
mentation uses an extension of our monolingual
parser (Chiang, 2000) based on tree-substitution
1
grammar with sister adjunction (TSG+SA).
The main contributions of this paper are as fol-
lows:
1. We introduce the novel concept of a hidden
TAG model.
2. We use this model to combine statistical ap-
proaches with grammar engineering (specif-
ically motivated from the linguistic facts).
Our approach thus exemplifies the specific
strength of a grammar-based approach.
3. We present an implementation of stochas-
tic synchronous TAG that incorporates vari-
ous facilities useful for training on real-world
data: sister-adjunction (needed for generating
the flat structures found in most treebanks),
smoothing, and Inside-Outside reestimation.
This paper is structured as follows. We first
briefly discuss related work (Section 2) and some
of the linguistic facts that motivate this work (Sec-
tion 3). We then present the formalism, probabilis-
tic model, and parsing algorithm (Section 4). Fi-
nally, we discuss the manual grammar engineering
(Section 5) and evaluation (Section 6).
2 Related Work
This paper is part of a larger investigation into
parsing Arabic dialects (Rambow et al, 2005; Chi-
ang et al, 2006). In that investigation, we exam-
ined three different approaches:
? Sentence transduction, in which a dialect sen-
tence is roughly translated into one or more
MSA sentences and then parsed by an MSA
parser.
? Treebank transduction, in which the MSA
treebank is transduced into an approximation
of a LA treebank, on which a LA parer is then
trained.
? Grammar transduction, which is the name
given in the overview papers to the approach
discussed in this paper. The present paper
provides for the first time a complete tech-
nical presentation of this approach.
Overall, grammar transduction outperformed
the other two approaches.
In other work, there has been a fair amount of
interest in parsing one language using another lan-
guage, see for example (Smith and Smith, 2004;
Hwa et al, 2004). Much of this work, like ours,
relies on synchronous grammars (CFGs). How-
ever, these approaches rely on parallel corpora.
For MSA and its dialects, there are no naturally
occurring parallel corpora. It is this fact that has
led us to investigate the use of explicit linguistic
knowledge to complement machine learning.
3 Linguistic Facts
We illustrate the differences between LA and
MSA using an example:
(1) a.   	
   ffProceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 25?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The weak generative capacity of linear tree-adjoining grammars
David Chiang?
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
1 Introduction
Linear tree-adjoining grammars (TAGs), by anal-
ogy with linear context-free grammars, are tree-
adjoining grammars in which at most one sym-
bol in each elementary tree can be rewritten (ad-
joined or substituted at). Uemura et al (1999),
calling these grammars simple linear TAGs (SL-
TAGs), show that they generate a class of lan-
guages incommensurate with the context-free lan-
guages, and can be recognized in O(n4) time.
Working within the application domain of mod-
eling of RNA secondary structures, they find
that SL-TAGs are too restrictive?they can model
RNA pseudoknots but because they cannot gen-
erate all the context-free languages, they cannot
model even some very simple RNA secondary
structures. Therefore they propose a more power-
ful version of linear TAGs, extended simple linear
TAGs (ESL-TAGs), which generate a class of lan-
guages that include the context-free languages and
can be recognized in O(n5) time.
Satta and Schuler (1998), working within the
application domain of natural language syntax, de-
fine another restriction on TAG which is also rec-
ognizable in O(n5) time. Despite being less pow-
erful than full TAG, it is still able to generate lan-
guages like the copy language {ww} and Dutch
cross-serial dependencies (Joshi, 1985). Kato et
al. (2004) conjecture that this restricted TAG is in
fact equivalent to ESL-TAG.
In this paper we prove their conjecture, and also
prove that adding substitution to ESL-TAG does
not increase its weak generative capacity, whereas
adding substitution to SL-TAG makes it weakly
equivalent to ESL-TAG. Thus these four for-
?This research was primarily carried out while the author
was at the University of Pennsylvania.
malisms converge to the same weak-equivalence
class, the intuition being that the ?hardest? oper-
ation in TAG, namely, adjunction of a wrapping
auxiliary tree in the middle of the spine of an-
other wrapping auxiliary tree, is subjected to the
linearity constraint, but most other operations are
unrestricted.1 Kato et al (2004) show that these
formalisms are more powerful than SL-TAG or
general CFG or their union and conjecture, on the
other hand, that they are less powerful than TAG.
We prove this conjecture as well.
2 Definitions
We assume a standard definition of TAG, with or
without substitution, in which adjunction is not al-
lowed at foot nodes, and other nodes can have no-
adjunction (NA) constraints, obligatory-adjunction
(OA), or selective-adjunction constraints. We use
the symbols ?, ?1, ?2, etc. to range over nodes ofelementary trees or derived trees, although some-
times we use the label of a node to refer to the
node itself. The spine of an auxiliary tree is the
path from its root node to its foot node, inclusive.
The subtree of a node ? is the set of all nodes
dominated by ?, including ? itself. The segment
of a tree from ?1 to ?2 (where ?1 dominates ?2)is the set of all nodes in the subtree of ?1 but notin the subtree of ?2. A segment can be excised,which means removing the nodes of the segment
and making ?2 replace ?1 as the child of its parent.
We also assume a standard definition of TAG
derivation trees. We use the symbols h, h1, h2, etc.to range over nodes of derivation trees. The sub-
1Adjunction at root and foot nodes is another operation
that by itself will not take a formalism beyond context-free
power, a fact which is exploited in Rogers? regular-form TAG
(Rogers, 1994). But allowing this in a linear TAG would cir-
cumvent the linearity constraint.
25
derivation of h is the subtree of h in the deriva-
tion tree. When we cut up derivations into sub-
derivations or segments and recombine them, the
edge labels (indicating addresses of adjunctions
and substitutions) stay with the node above, not
the node below.
Now we define various versions of linear TAG.
Definition 1. A right (left) auxiliary tree is one in
which the leftmost (rightmost) frontier node is the
foot node, and the spine contains only the root and
foot nodes. A wrapping auxiliary tree is one which
is neither a left or a right auxiliary tree.
Definition 2. We say that a node of an elementary
tree is active if adjunction is allowed to occur at
it, and that a node is w-active if adjunction of a
wrapping auxiliary tree is allowed to occur at it.
Definition 3. A Satta-Schuler linear tree-
adjoining grammar (SSL-TAG) is a TAG with
substitution in which:
1. In the spine of each wrapping auxiliary tree,
there is at most one w-active node.
2. In the spine of each left or right auxiliary tree,
there are no w-active nodes, nor are there any
other adjoining constraints.
Definition 4. A simple linear tree-adjoining
grammar (SL-TAG), with or without substitution,
is a TAG, with or without substitution, respec-
tively, in which every initial tree has exactly one
active node, and every auxiliary tree has exactly
one active node on its spine and no active nodes
elsewhere.
Definition 5. An extended simple linear tree-
adjoining grammar (ESL-TAG), with or without
substitution, is a TAG, with or without substitu-
tion, respectively, in which every initial tree has
exactly one active node, and every auxiliary tree
has exactly one active node on its spine and at
most one active node elsewhere.
3 Properties
We now review several old results and prove a few
new results relating the weak generative capacity
of these formalisms to one another and to (linear)
CFG and TAG. These results are summarized in
Figure 1.
3.1 Previous results
Proposition 1 (Uemura et al 1999).
Linear CFL ( SL-TAL
Linear CFL
SL-TAL CFL
SL-TAL ? CFL
SSL-TAL = ESL-TAL = (E)SL-TAL + subst
TAL
Figure 1: Summary of results: an edge indicates
that the higher formalism has strictly greater weak
generative capacity than the lower.
Proposition 2 (Uemura et al 1999).
CFL ( ESL-TAL
Proposition 3 (Kato et al 2004).
CFL ? SL-TAL ( ESL-TAL
Proposition 4 (Satta and Schuler 1998; Ue-
mura et al 1999). SSL-TAG and ESL-TAG can
be parsed in O(n5) time.
3.2 Weak equivalence
Proposition 5. The following formalisms are
weakly equivalent:
(i) ESL-TAG
(ii) SL-TAG with substitution
(iii) ESL-TAG with substitution
(iv) SSL-TAG
Proof. We prove this by proving four inclusions.
L(ESL-TAG) ? L(ESL-TAG + substitution):
Trivial.
L(ESL-TAG + substitution) ? L(SSL-TAG):
Trivial.
L(SSL-TAG) ? L(SL-TAG + substitution): We
deal first with the left and right auxiliary trees, and
then with off-spine adjunction.
First, we eliminate the left and right auxiliary
trees. Since these only insert material to the left or
right of a node, just as in tree-insertion grammars
(TIGs), we may apply the conversion from TIGs to
tree-substitution grammars (Schabes and Waters,
1995), used in the proof of the context-freeness of
26
(Step 1a)
...
X...
?
...
X...
...
XNA
LX? XNA...
...
XNA
XNA...
RX?
...
XNA
LX? XNA...
RX?
(Step 1b)
X
X? Y ?
RX
Y
RX
Y RX?
X
Y X? ?
LX
Y
LX
LX? Y
Figure 2: Elimination of left/right auxiliary trees.
TIG.2 (Step 1a) For each active node X that is not
the root of a left or right auxiliary tree, we create
four copies of the containing elementary tree with
X altered in the following ways: first, leave X un-
changed; then, add a copy of X above it, making
both nodes no-adjunction nodes, and add a new
left sister substitution node labeled LX or a newright sister substitution node labeled RX , or both.See Figure 2. (Step 1b) For each ? that was origi-
nally a left (right) auxiliary tree with root/foot la-
bel X , relabel the root node as LX (RX ) and deletethe foot node, and create two copies of the contain-
ing elementary tree, one unchanged, and one with
a new left (right) sister substitution node. See Fig-
ure 2. When the modified ? substitutes at one of
the new children of an ?, the substitution clearly
results in the same string that would have resulted
from adjoining the original ? to ?.
This construction might appear incorrect in two
ways. First, the new grammar has trees with both
an LX and an RX node corresponding to the sameoriginal node, which would correspond to adjunc-
tion of two auxiliary trees ?L and ?R at the samenode X in the original grammar. But this new
derivation generates a string that was generable in
the original grammar, namely by adjoining ?L at
2This corresponds to Steps 1?4 of that proof (Schabes and
Waters, 1995, p. 486). Since that proof uses a more relaxed
definition of left and right auxiliary trees, it is probable that
SSL-TAG could also be relaxed in the same way.
X , then adjoining ?R at the root of ?L, which isallowed because the definition of SSL-TAG pro-
hibits adjunction constraints at the root of ?L.
Thus the first apparent problem is really the so-
lution to the second problem: in the original gram-
mar, a left auxiliary tree ?L could adjoin at the rootof a right auxiliary tree ?R, which in turn adjoinedat a node ?, whereas in the new grammar, ?R doesnot have an LX substitution node to allow this pos-sibility. But the same string can be generated by
substituting both trees under ? in the new gram-
mar. In the case of a whole chain of adjunctions
of left/right auxiliary trees at the root of left/right
auxiliary trees, we can generate the same string by
rearranging the chain into a chain of left auxiliary
trees and a chain of right auxiliary trees (which is
allowed because adjunction constraints are prohib-
ited at all the roots), and substituting both at ?.
(Step 2) Next, we eliminate the case of a wrap-
ping auxiliary tree ? that can adjoin at an off-spine
node ?. (Step 2a) For each active off-spine node ?,
we relabel ? with a unique identifier ?? and split the
containing elementary tree at ?:
...
??...
?
...
T???
B??...
27
(Step 2b) After step 2a has been completed for all
nodes ?, we revisit each ?, and for every wrapping
? that could adjoin at ?, create a copy of ? with
root relabeled to T?? and foot relabeled to B?? .
X
X?
?
T??
B???
Then the original ? is discarded. Substituting one
of these copies of ? at a T?? node and then sub-stituting a B?? tree at the former foot node has thesame effect as adjoining ? at ?. Finally, unless ?
had an obligatory-adjunction constraint, simulate
the lack of adjunction at ? by adding the initial
tree
T??
B???
L(SL-TAG + substitution) ? L(ESL-TAG): This
construction is related to Lang?s normal form
which ensures binary-branching derivation trees
(Lang, 1994), but guarantees that one adjunction
site is on the spine and one is off the spine.
(Step 0a) Ensure that the elementary trees are
binary-branching. (Step 0b) Add a new root and
foot node to every elementary tree:
X
?
XNA
X
X
X?
?
XNA
X
XNA
X?
(Step 1) We transform the grammar so that no
auxiliary tree has more than one substitution node.
For any auxiliary tree with spine longer than four
nodes, we apply the following transformation: tar-
get either the active node or its parent, and call
it Y . Let Z1 be the child that dominates the footnode; let V1 be a fresh nonterminal symbol andinsert V1 nodes above Y and below Z1, and ex-cise the segment between the two V nodes, leav-
ing behind an active obligatory-adjunction node.
If Y has another child, call it Z2; let V2 be a freshnonterminal symbol and insert a V2 node above
Z2, and break off the subtree rooted in V2, leav-ing behind a substitution node. See Figure 3. This
transformation reduces the spine of the auxiliary
tree by one node, and creates two new trees that
satisfy the desired form. We repeat this until the
entire grammar is in the desired form.
(Step 2) Next, we transform the grammar so
that no initial tree has more than one substitution
node, while maintaining the form acquired in step
1. For any initial tree with height greater than three
nodes, we apply the same transformation as in step
1, except that Y is the child of the root node, Z1is its left child, and Z2 is its other child if it ex-ists and is not already a substitution node. See Fig-
ure 3. This transformation replaces an initial tree
with at most two shorter initial trees, and one aux-
iliary tree in the desired form. Again we repeat this
until the entire grammar is in the desired form.
(Step 3) Finally, we convert each substitution
node into an adjunction node (Schabes, 1990). For
each substitution node ?, let X be the label of ?.
Relabel ? to SX with obligatory adjunction andplace an empty terminal beneath ?.
...
X?
?
...
SX OA
?
For each initial tree with root label X , convert it
into an auxiliary tree by adding a new root node
labeled SX whose children are the old root nodeand a new foot node.
X
?
SX NA
X SX?
3.3 Relation to tree-adjoining languages
Our second result, also conjectured by Kato et
al., is that the weak equivalence class established
above is a proper subset of TAL.
Proposition 6. The language
L = {ar1bp1b
p
2c
q
1c
q
2ar2ar3c
q
3c
q
4b
p
3b
p
4ar4}
is in TAL but not ESL-TAL.
28
(Step 1)
X...
Y
Z1...
X?
Z2NA...
?
X...
V1
Y
Z1
V1...
X?
V2
Z2NA...
?
X...
V1OA...
X?
V1NA
Y
Z1
V1?
V2?
V2
Z2NA...
(Step 2)
X
Y
Z1...
Z2...
?
X
V1
Y
Z1
V1...
V2
Z2...
?
X
V1OA...
V1NA
Y
Z1
V1?
V2?
V2
Z2...
Figure 3: Separation of substitution nodes. Some adjunction constraints are omitted to avoid clutter.
Proof (L ? TAL). The language is generated by
the following TAG:
X
?
XNA
a1 X
a2 X? a3
a4
XNA
Y
Z
X?
YNA
b1 Y
b2 Y? b3
b4
ZNA
c1 Z
c2 Z? c3
c4
Before proceeding to the other half of the proof,
we define a few useful notions. A marked string
(as in Ogden?s Lemma) over an alphabet ? is a
string over ? ? {0, 1}, where a symbol ??, 1? is
marked and a symbol ??, 0? is not. Marked strings
over ? can be projected into ?? in the obvious way
and we will talk about marked strings and their
projections interchangeably.
A decomposed string over ? is a sequence
of strings over ?, which can be projected into
?? by concatenating their members in order, and
again we will talk about decomposed strings and
their projections interchangeably. In particular,
we will often simply write a decomposed string
?w1, . . . , wn? as w1 ? ? ?wn. Moreover, we may usethe symbol wi to refer to the occurrence of the ithmember of the decomposition in w; for example, if
w is a marked string, we may say that a symbol in
wi is marked, or if w is generated by a TAG deriva-tion, we may say that wi is generated by some setof nodes in the derivation tree.
The second half of the proof requires a double-
decker pumping lemma.
Condition 1 (cf. Vijay-Shanker (1987), Theo-
rem 4.7). Given a language L and a decom-
posed string x1zx2 ? L with some symbols in
z marked, there exists a decomposition of z into
u1v1w1v2u2v3w2v4u3 such that one of the vi con-tains a mark, and L contains, for all k ? 1,
x1(u1vk1w1vk2u2vk3w2vk4u3)x2
Condition 2 (cf. Uemura et al (1999), Lemma
29
1). Given a language L and a decomposed string
x1z1z2x2z3z4x3 ? L with some symbols in one ofthe zi marked, there exist decompositions of the ziinto uiviwi such that one of the vi contains a mark,and L contains, for all k ? 1,
x1(u1vk1w1)(u2vk2w2)x2(u3vk3w3)(u4vk4w4)x3
Lemma 7. If L is an ESL-TAL, then there exists
a constant n such that for any z ? L with n sym-
bols marked, Condition 1 holds of ? ? z ? ?. More-
over, it holds such that the w1 and w2 it provides
can be further decomposed into z1z2 and z3z4, re-
spectively, such that for any marking of n sym-
bols of any of the zj , either Condition 1 holds
of z = x1zjx2 (where x1 and x2 are the sur-
rounding context of zj) or Condition 2 holds of
z = x1z1z2x2z3z4x3 (where x1, x2, and x3 are
the surrounding context of z1z2 and z3z4).
Proof. Since L is an ESL-TAL, it is generated by
some ESL-TAG G. Let k be the number of ele-
mentary trees in G and t be the maximum number
of terminal symbols in any elementary tree of G.
Then set n = 2k+1t.
The first invocation of Condition 1 is the TAG
version of Ogden?s lemma (Hopcroft and Ullman,
1979). To show that it holds, we need to find a
path P in the derivation tree of z that has a cy-
cle that generates at least one marked symbol. De-
fine a branch point to be a node h in the derivation
tree such that the marked nodes generated by the
subderivation of h are not all generated by the sub-
derivation of a single child of h. We seek a P that
has at least k + 1 branch points. Start by adding
the root of the derivation tree to P . Thereafter let
h be the last node in P . If h is a leaf, stop; other-
wise, add to P the child of h whose subderivation
generates the most marked symbols. Note that if
a branch point in P generates m marked symbols,
the next branch point generates at least m?t2 . Ourchoice of n then guarantees that P has at least k+1
branch points, at least two of which must corre-
spond to the same auxiliary tree. Call these nodes
h1 and h2.These two nodes divide the derivation up into
three phases: first, the derivation segment from the
root to h1, which we call ? (because it can bethought of as the derived initial tree it generates);
then the segment from h1 to h2, which we call ?1(because it can be thought of as the derived aux-
iliary tree it generates); then subderivation of h2,
which we call ?2. Note that we can form new validderivations of G by repeating ?2: that is, in termsof derivation trees, stacking ? on top of one or
more copies of ?1, on top of ?2?or in terms ofderived trees, repeatedly adjoining ?1 into ? andthen adjoining ?2.
If ?2 adjoins into the spine of ?1, then let
?u1, u2, u3? be the parts of z generated by ?,
?v1, v2, v3, v4? the parts generated by ?1, and
?w1, w2? the parts generated by ?2 (see Figure 4a).Then these new derivations generate the strings
u1vk1w1vk2u2vk3w2vk4u3.
But if ?2 adjoins at a node to the left of the spineof ?1, then let ?u1, v42, u3? be the parts of the zgenerated by ?, ?v1, u2, v41, v43? the parts gener-ated by ?1, and ?w1, w2? the parts generated by
?2 (see Figure 4b). Then let v2 = v3 = ? and
v4 = v41v42v43; the new derivations will gener-ate the strings u1vk1w1vk2u2vk3w2vk4u3. The casewhere ?2 adjoins to the right of the spine.
Now we focus attention on ?2. Let S be thelongest path of the derivation of ?2 containingthe root of the derivation and auxiliary trees ad-
joined at spine nodes. This S is unique because
each spine can only have one active node. Let h3be the last node in S, which divides the deriva-
tion of ?2 into two phases: the segment from theroot to h3, which we call ?21, and the subderiva-tion of h3, which we call ?22. This gives a decom-position ?w1, w2? = ?z1z21z22, z31z32z4?, where
?22 generates z21 and z32 (see Figure 5). Notethat the derivation nodes in S are the only ones
that can generate symbols in z1, z22, z31, and z4at once; the other derivation nodes only gener-
ate symbols in a single zi. We let z2 = z21z22and z3 = z31z32 and hand off the decomposition
?w1, w2? = ?z1z2, z3z4? to our adversary, whomay choose a zj and mark n symbols in it.
Then we recapitulate the reasoning above to get
a path P ? starting from the root of the deriva-
tion of ?2 and containing at least k + 1 branchpoints, two of which correspond to the same aux-
iliary tree. Call these nodes h4 and h5 and the seg-ment between them ?3, and let ?v1, v2, v3, v4? nowstand for the parts of ?w1, w2? generated by ?3.Once again, we are going to repeat ?3 to gener-ate new derivations, pumping copies of the vi into
?w1, w2?. But the location of the vi depends on h5:if h5 is in S, then the vi will appear inside each ofthe zi, satisfying Condition 2. Otherwise, they willall appear inside zj .
30
(a)
?
?1
?2
?1
?
u1 v1 w1 v2 u2 v3 w2 v4 u3
(b)
?
?1
?2
??1
u1 v1 w1 v2 w2 v41 v42 v43 u3
Figure 4: Anatomy of derived tree in proof of Lemma 7.
?21
?22
?21
?
z1 z21 z22 z31 z32 z4
Figure 5: Anatomy of ?2 in proof of Lemma 7.
31
Finally we complete the proof of Proposition 6.
Proof of Proposition 6 (L /? ESL-TAL). Suppose
L is an ESL-TAL. Let z be the string obtained by
setting p = q = r = n, and mark the a1s. ThenLemma 7 must hold. The first invocation of Con-
dition 1 must give a w1 of the form a?1bn1 bn2 cn1 cn2a?2and a w2 of the form a?3cn3 cn4 bn3 bn4a?4. Lemma 7must further decompose w1 into z1z2. Obviously,either z1 contains all the bjs or z2 contains allthe cjs. Supposing the former, we can obtain acontradiction by marking the b1s: Condition 2is impossible because it would give unequal
numbers of b1s and b2s; Condition 1 is impossiblebecause it would give unequal numbers of b1s and
b3s. On the other hand, if z2 contains all the cjs,we mark the c1s, and both Conditions are againrendered impossible.
4 Conclusion
The weak equivalence of the previously proposed
ESL-TAG and SSL-TAG, along with the fact that
SL-TAG with substitution and ESL-TAG with
substitution belong to the same class, suggests
that they represent a useful compromise between
CFGs and TAGs. In the two-dimensional language
hierarchy of Rambow and Satta (1999), where the
two dimensions are rank (how many substructures
does a rule combine) and fanout (how many dis-
continuous spans of the input does a substructure
cover), CFGs comprise the fanout-1 grammars and
TAGs are a subset of the the fanout-2 grammars;
both have arbitrary rank, whereas linear CFGs
and linear TAGs are rank-1. The grammars dis-
cussed here are mixed: a rule can combine one
fanout-2 substructure and an arbitrary number of
fanout-1 substructures. A related example would
be a version of synchronous CFG that allows only
one pair of linked nonterminals and any number
of unlinked nonterminals, which could be bitext-
parsed in O(n5) time, whereas inversion transduc-
tion grammar (Wu, 1997) takes O(n6). It may be
of interest to make a more general exploration of
other formalisms that are mixed in this sense.
Acknowledgements
Thanks to Hiroyuki Seki for discussions that led to
this paper, and to Anoop Sarkar, Giorgio Satta, and
William Schuler. This research was partially sup-
ported by NSF grant ITR EIA-02-05456. S. D. G.
References
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and Com-
putation. Addison-Wesley, Reading, MA.
Aravind K. Joshi. 1985. Tree adjoining grammars:How much context-sensitivity is necessary for as-signing structural descriptions? In David Dowty,
Lauri Karttunen, and Arnold Zwicky, editors, Nat-
ural Language Parsing, pages 206?250. CambridgeUniversity Press, Cambridge.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2004.
Subclasses of tree adjoining grammar for RNA sec-ondary structure. In Proc. Seventh International
Workshop on TAG and Related Formalisms (TAG+),
pages 48?55.
Bernard Lang. 1994. Recognition can be harder thanparsing. Computational Intelligence, 10(4):484?494. Special Issue on Tree Adjoining Grammars.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-tems. Theoretical Computer Science, 223:87?120.
James Rogers. 1994. Capturing CFLs with tree adjoin-ing grammars. In Proc. 32nd Annual Meeting of the
ACL, pages 155?162.
Giorgio Satta and William Schuler. 1998. Restrictionson tree adjoining languages. In Proc. COLING-
ACL, pages 1176?1182.
Yves Schabes and Richard C. Waters. 1995. Treeinsertion grammar: a cubic-time parsable formal-ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, 21:479?513.
Yves Schabes. 1990. Mathematical and Computa-
tional Aspects of Lexicalized Grammars. Ph.D. the-
sis, University of Pennsylvania. Available as techni-cal report MS-CIS-90-48.
Yasuo Uemura, Aki Hasegawa, Satoshi Kobayashi, andTakashi Yokomori. 1999. Tree adjoining grammars
for RNA structure prediction. Theoretical Computer
Science, 210:277?303.
K Vijayashanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis, University of Pennsylva-
nia. Available as technical report MS-CIS-88-03.
Dekai Wu. 1997. Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
32
Recovering latent information in treebanks
David Chiang and Daniel M. Bikel
University of Pennsylvania
Dept of Computer and Information Science
200 S 33rd Street
Philadelphia PA 19104 USA
{dchiang,dbikel}@cis.upenn.edu
Abstract
Many recent statistical parsers rely on a preprocess-
ing step which uses hand-written, corpus-specific
rules to augment the training data with extra infor-
mation. For example, head-finding rules are used
to augment node labels with lexical heads. In this
paper, we provide machinery to reduce the amount
of human effort needed to adapt existing models to
new corpora: first, we propose a flexible notation for
specifying these rules that would allow them to be
shared by different models; second, we report on an
experiment to see whether we can use Expectation-
Maximization to automatically fine-tune a set of
hand-written rules to a particular corpus.
1 Introduction
Most work in statistical parsing does not operate
in the realm of parse trees as they appear in many
treebanks, but rather on trees transformed via aug-
mentation of their node labels, or some other trans-
formation (Johnson, 1998). This methodology is il-
lustrated in Figure 1. The information included in
the node labels? augmentations may include lexical
items, or a node label suffix to indicate the node is an
argument and not an adjunct; such extra information
may be viewed as latent information, in that it is not
directly present in the treebank parse trees, but may
be recovered by some means. The process of recov-
ering this latent information has largely been limited
to the hand-construction of heuristics. However, as
is often the case, hand-constructed heuristics may
not be optimal or very robust. Also, the effort re-
quired to construct such rules can be considerable.
In both respects, the use of such rules runs counter
to the data-driven approach to statistical parsing.
In this paper, we propose two steps to address
this problem. First, we define a new, fairly simple
syntax for the identification and transformation of
node labels that accommodates a wide variety of
node-label augmentations, including all those that
Model
parsed data +annotated data +
annotated data Training Decoding parsed data
Figure 1: Methodology for the development of a sta-
tistical parser. A + indicates augmentation.
are performed by existing statistical parsers that we
have examined. Second, we explore a novel use of
Expectation-Maximization (Dempster et al, 1977)
that iteratively reestimates a parsing model using
the augmenting heuristics as a starting point. Specif-
ically, the EM algorithm we use is a variant of
the Inside-Outside algorithm (Baker, 1979; Lari and
Young, 1990; Hwa, 1998). The reestimation adjusts
the model?s parameters in the augmented parse-tree
space to maximize the likelihood of the observed
(incomplete) data, in the hopes of finding a better
distribution over augmented parse trees (the com-
plete data). The ultimate goal of this work is to mini-
mize the human effort needed when adapting a pars-
ing model to a new domain.
2 Background
2.1 Head-lexicalization
Many of the recent, successful statistical parsers
have made use of lexical information or an im-
plicit lexicalized grammar, both for English and,
more recently, for other languages. All of these
parsers recover the ?hidden? lexicalizations in a
treebank and find the most probable lexicalized tree
when parsing, only to strip out this hidden infor-
mation prior to evaluation. Also, in all these pars-
ing efforts lexicalization has meant finding heads
of constituents and then propagating those lexical
heads to their respective parents. In fact, nearly
identical head-lexicalizations were used in the dis-
S(caught?VBD)
NP(boy?NN)
DET
The
NN
boy
ADVP(also?RB)
RB
also
VP(caught?VBD)
VBD
caught
NP(ball?NN)
DET
the
NN
ball
Figure 2: A simple lexicalized parse tree.
criminative models described in (Magerman, 1995;
Ratnaparkhi, 1997), the lexicalized PCFG models
in (Collins, 1999), the generative model in (Char-
niak, 2000), the lexicalized TAG extractor in (Xia,
1999) and the stochastic lexicalized TAG models
in (Chiang, 2000; Sarkar, 2001; Chen and Vijay-
Shanker, 2000). Inducing a lexicalized structure
based on heads has a two-pronged effect: it not
only allows statistical parsers to be sensitive to lex-
ical information by including this information in
the probability model?s dependencies, but it also
determines which of all possible dependencies?
both syntactic and lexical?will be included in the
model itself. For example, in Figure 2, the nontermi-
nal NP(boy?NN) is dependent on VP(caught?VBD)
and not the other way around.
2.2 Other tree transformations
Lexicalization via head-finding is but one of many
possible tree transformations that might be use-
ful for parsing. As explored thoroughly by John-
son (1998), even simple, local syntactic trans-
formations on training trees for an unlexicalized
PCFG model can have a significant impact on pars-
ing performance. Having picked up on this idea,
Collins (1999) devises rules to identify arguments,
i.e., constituents that are required to exist on a par-
ticular side of a head child constituent dominated
by a particular parent. The parsing model can then
probabilistically predict sets of requirements on ei-
ther side of a head constituent, thereby incorporat-
ing a type of subcategorization information. While
the model is augmented to include this subcat-
prediction feature, the actual identification of argu-
ments is performed as one of many preprocessing
steps on training trees, using a set of rules sim-
ilar to those used for the identification of heads.
Also, (Collins, 1999) makes use of several other
transformations, such as the identification of sub-
jectless sentences (augmenting S nodes to become
SG) and the augmentation of nonterminals for gap
threading. Xia (1999) combines head-finding with
argument identification to extract elementary trees
for use in the lexicalized TAG formalism. Other re-
searchers investigated this type of extraction to con-
struct stochastic TAG parsers (Chiang, 2000; Chen
and Vijay-Shanker, 2000; Sarkar, 2001).
2.3 Problems with heuristics
While head-lexicalization and other tree transfor-
mations allow the construction of parsing models
with more data-sensitivity and richer representa-
tions, crafting rules for these transformations has
been largely an art, with heuristics handed down
from researcher to researcher. What?s more, on
top of the large undertaking of designing and im-
plementing a statistical parsing model, the use of
heuristics has required a further effort, forcing the
researcher to bring both linguistic intuition and,
more often, engineering savvy to bear whenever
moving to a new treebank. For example, in the rule
sets used by the parsers described in (Magerman,
1995; Ratnaparkhi, 1997; Collins, 1999), the sets of
rules for finding the heads of ADJP, ADVP, NAC,
PP and WHPP include rules for picking either the
rightmost or leftmost FW (foreign word). The ap-
parently haphazard placement of these rules that
pick out FW and the rarity of FW nodes in the data
strongly suggest these rules are the result of engi-
neering effort. Furthermore, it is not at all apparent
that tree-transforming heuristics that are useful for
one parsing model will be useful for another. Fi-
nally, as is often the case with heuristics, those used
in statistical parsers tend not to be data-sensitive,
and ironically do not rely on the words themselves.
3 Rule-based augmentation
In the interest of reducing the effort required to con-
struct augmentation heuristics, we would like a no-
tation for specifying rules for selecting nodes in
bracketed data that is both flexible enough to encode
the kinds of rule sets used by existing parsers, and
intuitive enough that a rule set for a new language
can be written easily without knowledge of com-
puter programming. Such a notation would simplify
the task of writing new rule sets, and facilitate ex-
perimentation with different rules. Moreover, rules
written in this notation would be interchangeable
between different models, so that, ideally, adapta-
tion of a model to a new corpus would be trivial.
We define our notation in two parts: a structure
pattern language, whose basic patterns are speci-
fications of single nodes written in a label pattern
language.
3.1 Structure patterns
Most existing head-finding rules and argument-
finding rules work by specifying parent-child rela-
tions (e.g., NN is the head of NP, or NP is an argu-
ment of VP). A generalization of this scheme that
is familiar to linguists and computer scientists alike
would be a context-free grammar with rules of the
form
A? A1 ? ? ? (Ai)l ? ? ? An,
where the superscript l specifies that if this rule gets
used, the ith child of A should be marked with the
label l.
However, there are two problems with such an ap-
proach. First, writing down such a grammar would
be tedious to say the least, and impossible if we
want to handle trees with arbitrary branching fac-
tors. So we can use an extended CFG (Thatcher,
1967), a CFG whose right-hand sides are regular ex-
pressions. Thus we introduce a union operator (?)
and a Kleene star (?) into the syntax for right-hand
sides.
The second problem that our grammar may be
ambiguous. For example, the grammar
X? YhY ? YYh
could mark with an h either the first or second sym-
bol of YY. So we impose an ordering on the rules of
the grammar: if two rules match, the first one wins.
In addition, we make the ? operator noncommuta-
tive: ?? ? tries to match ? first, and ? only if it does
not match ?, as in Perl. (Thus the above grammar
would mark the first Y.) Similarly, ?? tries to match
as many times as possible, also as in Perl.
But this creates a third and final problem: in the
grammar
X? (YYh ? Yh)(YY ? Y),
it is not defined which symbol of YYY should be
marked, that is, which union operator takes priority
over the other. Perl circumvents this problem by al-
ways giving priority to the left. In algebraic terms,
concatenation left-distributes over union but does
not right-distribute over union in general.
However, our solution is to provide a pair of con-
catenation operators: , which gives priority to the
left, and ?, which gives priority to the right:
X ? (YYh ? Yh)  (YY ? Y) (1)
X ? (YYh ? Yh) ? (YY ? Y) (2)
Rule (1) marks the second Y in YYY, but rule (2)
marks the first Y. More formally,
? ? (? ? ?) = (? ? ?) ? (? ? ?)
(? ? ?)  ? = (?  ?) ? (?  ?)
But if ? contains no unions or Kleene stars, then
?  ? = ? ? ? (? ??)
?  ? = ? ? ? (? ??)
So then, consider the following rules:
VP ? ??  VBh  ??, (3)
VP ? ?? ? VBh ? ??. (4)
where ? is a wildcard pattern which matches any
single label (see below). Rule (3) mark with an h
the rightmost VB child of a VP, whereas rule (4)
marks the leftmost VB. This is because the Kleene
star always prefers to match as many times as possi-
ble, but in rule (3) the first Kleene star?s preference
takes priority over the last?s, whereas in rule (4) the
last Kleene star?s preference takes priority over the
first?s.
Consider the slightly more complicated exam-
ples:
VP ? ?? ? (VBh ?MDh) ? ?? (5)
VP ? ?? ? ((VBh ?MDh)  ??) (6)
Rule (5) marks the leftmost child which is either a
VB or a MD, whereas rule (6) marks the leftmost
VB if any, or else the leftmost MD. To see why this
so, consider the string MD VB X. Rule (5) would
mark the MD as h, whereas rule (6) would mark
the VB. In both rules VB is preferred over MD, and
symbols to the left over symbols to the right, but in
rule (5) the leftmost preference (that is, the prefer-
ence of the last Kleene star to match as many times
as possible) takes priority, whereas in rule (6) the
preference for VB takes priority.
3.2 Label patterns
Since nearly all treebanks have complex nontermi-
nal alphabets, we need a way of concisely specify-
ing classes of labels. Unfortunately, this will neces-
sarily vary somewhat across treebanks: all we can
define that is truly treebank-independent is the ?
pattern, which matches any label. For Penn Tree-
bank II style annotation (Marcus et al, 1993), in
which a nonterminal symbol is a category together
with zero or more functional tags, we adopt the fol-
lowing scheme: the atomic pattern a matches any
label with category a or functional tag a; more-
over, we define Boolean operators ?, ?, and ?. Thus
NP ? ?ADV matches NP?SBJ but not NP?ADV.1
3.3 Summary
Using the structure pattern language and the la-
bel pattern language together, one can fully encode
the head/argument rules used by Xia (which resem-
ble (5) above), and the family of rule sets used by
Black, Magerman, Collins, Ratnaparkhi, and others
(which resemble (6) above). In Collins? version of
the head rules, NP and PP require special treatment,
but these can be encoded in our notation as well.
4 Unsupervised learning of augmentations
In the type of approach we have been discussing
so far, hand-written rules are used to augment the
training data, and this augmented training data is
then used to train a statistical model. However, if we
train the model by maximum-likelihood estimation,
the estimate we get will indeed maximize the likeli-
hood of the training data as augmented by the hand-
written rules, but not necessarily that of the training
data itself. In this section we explore the possibility
of training a model directly on unaugmented data.
A generative model that estimates P(S ,T,T +)
(where T+ is an augmented tree) is normally used
for parsing, by computing the most likely (T,T +)
for a given S . But we may also use it for augment-
ing trees, by computing the most likely T + for a
given sentence-tree pair (S ,T ). From the latter per-
spective, because its trees are unaugmented, a tree-
bank is a corpus of incomplete data, warranting the
use of unsupervised learning methods to reestimate
a model that includes hidden parameters. The ap-
proach we take below is to seed a parsing model
using hand-written rules, and then use the Inside-
Outside algorithm to reestimate its parameters. The
resulting model, which locally maximizes the likeli-
hood of the unaugmented training data, can then be
used in two ways: one might hope that as a parser,
it would parse more accurately than a model which
only maximizes the likelihood of training data aug-
mented by hand-written rules; and that as a tree-
augmenter, it would augment trees in a more data-
sensitive way than hand-written rules.
4.1 Background: tree adjoining grammar
The parsing model we use is based on the stochas-
tic tree-insertion grammar (TIG) model described
1Note that unlike the noncommutative union operator ?, the
disjunction operator ? has no preference for its first argument.
by Chiang (2000). TIG (Schabes and Waters, 1995)
is a weakly-context free restriction of tree adjoin-
ing grammar (Joshi and Schabes, 1997), in which
tree fragments called elementary trees are com-
bined by two composition operations, substitution
and adjunction (see Figure 3). In TIG there are
certain restrictions on the adjunction operation.
Chiang?s model adds a third composition operation
called sister-adjunction (see Figure 3), borrowed
from D-tree substitution grammar (Rambow et al,
1995).2
There is an important distinction between derived
trees and derivation trees (see Figure 3). A deriva-
tion tree records the operations that are used to com-
bine elementary trees into a derived tree. Thus there
is a many-to-one relationship between derivation
trees and derived trees: every derivation tree speci-
fies a derived tree, but a derived tree can be the result
of several different derivations.
The model can be trained directly on TIG deriva-
tions if they are available, but corpora like the
Penn Treebank have only derived trees. Just as
Collins uses rules to identify heads and arguments
and thereby lexicalize trees, Chiang uses nearly the
same rules to reconstruct derivations: each training
example is broken into elementary trees, with each
head child remaining attached to its parent, each ar-
gument broken into a substitution node and an ini-
tial root, and each adjunct broken off as a modifier
auxiliary tree.
However, in this experiment we view the derived
trees in the Treebank as incomplete data, and try to
reconstruct the derivations (the complete data) using
the Inside-Outside algorithm.
4.2 Implementation
The expectation step (E-step) of the Inside-Outside
algorithm is performed by a parser that computes all
possible derivations for each parse tree in the train-
ing data. It then computes inside and outside prob-
abilities as in Hwa?s experiment (1998), and uses
these to compute the expected number of times each
event occurred. For the maximization step (M-step),
we obtain a maximum-likelihood estimate of the pa-
rameters of the model using relative-frequency es-
2The parameters for sister-adjunction in the present model
differ slightly from the original. In the original model, all the
modifier auxiliary trees that sister-adjoined at a particular po-
sition were generated independently, except that each sister-
adjunction was conditioned on whether it was the first at that
position. In the present model, each sister-adjunction is condi-
tioned on the root label of the previous modifier tree.
NP
NNP
John
S
NP? VP
VB
leave
VP
MD
should
VP?
NP
NN
tomorrow
(?1)
(?2)
(?) (?)
?
?2
?1
1
?
2
?
2,1
S
NP
NNP
John
VP
MD
should
VP
VB
leave
NP
NN
tomorrow
Derivation tree Derived tree
Figure 3: Grammar and derivation for ?John should leave tomorrow.? In this derivation, ?1 gets substituted,
? gets adjoined, and ? gets sister-adjoined.
timation, just as in the original experiment, as if
the expected values for the complete data were the
training data.
Smoothing presents a special problem. There are
several several backoff levels for each parameter
class that are combined by deleted interpolation. Let
?1, ?2 and ?3 be functions from full history con-
texts Y to less specific contexts at levels 1, 2 and
3, respectively, for some parameter class with three
backoff levels (with level 1 using the most specific
contexts). Smoothed estimates for parameters in this
class are computed as follows:
e = ?1e1 + (1 ? ?1)(?2e2 + (1 ? ?2)e3)
where ei is the estimate of p(X | ?i(Y)) for some
future context X, and the ?i are computed by the
formula found in (Bikel et al, 1997), modified to
use the multiplicative constant 5 found in the similar
formula of (Collins, 1999):
?i =
(
1 ?
di?1
di
) (
1
1 + 5ui/di
)
(7)
where di is the number of occurrences in training of
the context ?i(Y) (and d0 = 0), and ui is the number
of unique outcomes for that context seen in training.
There are several ways one might incorporate this
smoothing into the reestimation process, and we
chose to depart as little as possible from the orig-
inal smoothing method: in the E-step, we use the
smoothed model, and after the M-step, we use the
original formula (7) to recompute the smoothing
weights based on the new counts computed from
the E-step. While simple, this approach has two im-
portant consequences. First, since the formula for
the smoothing weights intentionally does not maxi-
mize the likelihood of the training data, each itera-
tion of reestimation is not guaranteed to increase the
87.3
87.35
87.4
87.45
87.5
87.55
87.6
0 5 10 15 20
F-
m
ea
su
re
Iteration
Figure 4: English, starting with full rule set
likelihood of the training data. Second, reestimation
tends to increase the size of the model in memory,
since smoothing gives nonzero expected counts to
many events which were unseen in training. There-
fore, since the resulting model is quite large, if an
event at a particular point in the derivation forest
has an expected count below 10?15, we throw it out.
4.3 Experiment
We first trained the initial model on sections 02?21
of the WSJ corpus using the original head rules, and
then ran the Inside-Outside algorithm on the same
data. We tested each successive model on some
held-out data (section 00), using a beam width of
10?4, to determine at which iteration to stop. The
F-measure (harmonic mean of labeled precision and
recall) for sentences of length ? 100 for each itera-
tion is shown in Figure 4. We then selected the ninth
reestimated model and compared it with the initial
model on section 23 (see Figure 7). This model did
only marginally better than the initial model on sec-
tion 00, but it actually performs worse than the ini-
tial model on section 23. One explanation is that the
84.5
84.55
84.6
84.65
84.7
84.75
84.8
84.85
84.9
84.95
85
85.05
0 5 10 15 20 25 30 35 40
F-
m
ea
su
re
Iteration
Figure 5: English, starting with simplified rule set
73
73.05
73.1
73.15
73.2
73.25
73.3
73.35
73.4
73.45
73.5
0 5 10 15 20 25 30 35 40
F-
m
ea
su
re
Iteration
Figure 6: Chinese, starting with full rule set
head rules, since they have been extensively fine-
tuned, do not leave much room for improvement.
To test this, we ran two more experiments.
The second experiment started with a simplified
rule set, which simply chooses either the leftmost or
rightmost child of each node as the head, depend-
ing on the label of the parent: e.g., for VP, the left-
most child is chosen; for NP, the rightmost child
is chosen. The argument rules, however, were not
changed. This rule set is supposed to represent the
kind of rule set that someone with basic familiarity
with English syntax might write down in a few min-
utes. The reestimated models seemed to improve on
this simplified rule set when parsing section 00 (see
Figure 5); however, when we compared the 30th
reestimated model with the initial model on section
23 (see Figure 7), there was no improvement.
The third experiment was on the Chinese Tree-
bank, starting with the same head rules used in
(Bikel and Chiang, 2000). These rules were origi-
nally written by Xia for grammar development, and
although we have modified them for parsing, they
have not received as much fine-tuning as the English
rules have. We trained the model on sections 001?
270 of the Penn Chinese Treebank, and reestimated
it on the same data, testing it at each iteration on
sections 301?325 (Figure 6). We selected the 38th
reestimated model for comparison with the initial
model on sections 271?300 (Figure 7). Here we did
observe a small improvement: an error reduction of
3.4% in the F-measure for sentences of length ? 40.
4.4 Discussion
Our hypothesis that reestimation does not improve
on the original rule set for English because that
rule set is already fine-tuned was partially borne
out by the second and third experiments. The model
trained with a simplified rule set for English showed
improvement on held-out data during reestimation,
but showed no improvement in the final evaluation;
however, the model trained on Chinese did show a
small improvement in both. We are uncertain as to
why the gains observed during the second experi-
ment were not reflected in the final evaluation, but
based on the graph of Figure 5 and the results on
Chinese, we believe that reestimation by EM can
be used to facilitate adaptation of parsing models
to new languages or corpora.
It is possible that our method for choosing
smoothing weights at each iteration (see ?4.2) is
causing some interference. For future work, more
careful methods should be explored. We would
also like to experiment on the parsing model of
Collins (1999), which, because it can recombine
smaller structures and reorder subcategorization
frames, might open up the search space for better
reestimation.
5 Conclusion
Even though researchers designing and implement-
ing statistical parsing models have worked in the
methodology shown in Figure 1 for several years
now, most of the work has focused on finding ef-
fective features for the model component of the
methodology, and on finding effective statistical
techniques for parameter estimation. However, there
has been much behind-the-scenes work on the ac-
tual transformations, such as head finding, and most
of this work has consisted of hand-tweaking exist-
ing heuristics. It is our hope that by introducing this
new syntax, less toil will be needed to write non-
terminal augmentation rules, and that human effort
will be lessened further by the use of unsupervised
methods such as the one presented here to produce
better models for parsing and tree augmentation.
? 100 words ? 40 words
Model Step LR LP CB 0CB ? 2 CB LR LP CB 0CB ? 2 CB
Original initial 86.95 87.02 1.21 62.38 82.33 87.68 87.76 1.02 65.30 84.86
Original 9 86.37 86.71 1.26 61.42 81.79 87.18 87.48 1.06 64.41 84.23
Simple initial 84.50 84.18 1.54 57.57 78.35 85.46 85.17 1.29 60.71 81.11
Simple 30 84.21 84.50 1.53 57.95 77.77 85.12 85.35 1.30 60.94 80.62
Chinese initial 75.30 76.77 2.72 45.95 67.05 78.37 80.03 1.79 52.82 74.75
Chinese 38 75.20 77.99 2.66 47.69 67.63 78.79 81.06 1.69 54.15 75.08
Figure 7: Results on test sets. Original = trained on English with original rule set; Simple = English, sim-
plified rule set. LR = labeled recall, LP = labeled precision; CB = average crossing brackets, 0 CB = no
crossing brackets, ? 2 CB = two or fewer crossing brackets. All figures except CB are percentages.
Acknowledgments
This research was supported in part by NSF grant
SBR-89-20230. We would like to thank Anoop
Sarkar, Dan Gildea, Rebecca Hwa, Aravind Joshi,
and Mitch Marcus for their valuable help.
References
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Conference
of the Acoustical Society of America, pages 547?550.
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 1?6.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing (ANLP 1997), pages 194?201.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of ANLP-NAACL2000, pages
132?139.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of TAGs from the Penn Treebank. In Pro-
ceedings of the Sixth International Workshop on Pars-
ing Technologies (IWPT 2000), pages 65?76, Trento.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of ACL-2000, pages 456?463.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. J. Roy. Stat. Soc. B, 39:1?38.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of COLING-ACL ?98, pages 557?563.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613?
632.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rosenberg and Arto
Salomaa, editors, Handbook of Formal Languages
and Automata, volume 3, pages 69?124. Springer-
Verlag, Heidelberg.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35?56.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL ?95, pages
276?283.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-tree grammars. In Proceedings of ACL ?95,
pages 151?158.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2).
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL-2001,
pages 175?182.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: a cubic-time parsable formalism
that lexicalizes context-free grammar without chang-
ing the trees produced. Computational Linguistics,
21:479?513.
J. W. Thatcher. 1967. Characterizing derivation trees of
context-free grammars through a generalization of fi-
nite automata theory. J. Comp. Sys. Sci., 1:317?322.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the 5th Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403.
Facilitating Treebank Annotation Using a Statistical Parser
Fu-Dong Chiou, David Chiang, and Martha Palmer
Dept of Computer and Information Science
University of Pennsylvania
200 S 33rd Street, Philadelphia, PA 19104-6389
{chioufd,dchiang,mpalmer}@linc.cis.upenn.edu
1. INTRODUCTION
Corpora of phrase-structure-annotated text, or treebanks, are use-
ful for supervised training of statistical models for natural language
processing, as well as for corpus linguistics. Their primary draw-
back, however, is that they are very time-consuming to produce. To
alleviate this problem, the standard approach is to make two passes
over the text: first, parse the text automatically, then correct the
parser output by hand.
In this paper we explore three questions:
? How much does an automatic first pass speed up annotation?
? Does this automatic first pass affect the reliability of the final
product?
? What kind of parser is best suited for such an automatic first
pass?
We investigate these questions by an experiment to augment the
Penn Chinese Treebank [15] using a statistical parser developed
by Chiang [3] for English. This experiment differs from previous
efforts in two ways: first, we quantify the increase in annotation
speed provided by the automatic first pass (70?100%); second, we
use a parser developed on one language to augment a corpus in an
unrelated language.
2. THE PARSER
The parsing model described by Chiang [3] is based on stochas-
tic TAG [13, 14]. In this model a parse tree is built up out of tree
fragments (called elementary trees), each of which contains exactly
one lexical item (its anchor).
In the variant of TAG used here, there are three kinds of el-
ementary trees: initial, (predicative) auxiliary, and modifier, and
three corresponding composition operations: substitution, adjunc-
tion, and sister-adjunction. Figure 1 illustrates all three of these op-
erations. The first two come from standard TAG [8]; the third is
borrowed from D-tree grammar [11].
In a stochastic TAG derivation, each elementary tree is gener-
ated with a certain probability which depends on the elementary
tree itself as well as the node it gets attached to. Since every tree is
.
lexicalized, each of these probabilities involves a bilexical depen-
dency, as in many recent statistical parsing models [9, 2, 4].
Since the number of parameters of a stochastic TAG is quite high,
we do two things to make parameter estimation easier. First, we
generate an elementary tree in two steps: the unlexicalized tree,
then a lexical anchor. Second, we smooth the probability estimates
of these two steps by backing off to reduced contexts.
When trained on about 80,000 words of the Penn Chinese Tree-
bank and tested on about 10,000 words of unseen text, this model
obtains 73.9% labeled precision and 72.2% labeled recall [1].
3. METHODOLOGY
For the present experiment the parsing model was trained on
the entire treebank (99,720 words). We then prepared a new set
of 20,202 segmented, POS-tagged words of Xinhua newswire text,
which was blindly divided into 3 sets of equal size (?10 words).
Each set was then annotated in two or three passes, as summa-
rized by the following table:
Set Pass 1 Pass 2 Pass 3
1 ? Annotator A Annotators A&B
2 parser Annotator A Annotators A&B
3 revised parser Annotator A Annotators A&B
Here ?Annotators A&B? means that Annotator B checked the
work of Annotator A, then for each point of disagreement, both an-
notators worked together to arrive at a consensus structure. ?Parser?
is Chiang?s parser, adapted to parse Chinese text as described by
Bikel and Chiang [1].
?Revised parser? is the same parser with additional modifications
suggested by Annotator A after correcting Set 2. These revisions
primarily resulted from a difference between the artificial evalua-
tion metric used by Bikel and Chiang [1] and this real-world task.
The metric used earlier, following common practice, did not take
punctuation or empty elements into account, whereas the present
task ideally requires that they be present and correctly placed. Thus
following changes were made:
? The parser was originally trained on data with the punctua-
tion marks moved, and did not bother to move the punctua-
tion marks back. For Set 3 we simply removed the prepro-
cessing phase which moved the punctuation marks.
? Similarly, the parser was trained on data which had all empty
elements removed. In this case we simply applied a rule-
based postprocessor which inserted null relative pronouns.
? Finally, the parser often produced an NP (or VP) which dom-
inated only a single NP (respectively, VP), whereas such a
NP
NNP
John
S
NP? VP
VB
leaveVP
MD
should
VP?
NP
NN
tomorrow
(
1
)
(
2
)
() (?)
?

2

1
1

2
?
2,1
derivation tree
S
NP
NNP
John
VP
MD
should
VP
VB
leave
NP
NN
tomorrow
derived tree
Figure 1: Grammar and derivation for ?John should leave tomorrow.? 
1
and 
2
are initial trees,  is a (predicative) auxiliary tree,
? is a modifier tree.
structure is not specified by the bracketing guidelines. There-
fore we applied another rule-based postprocessor to remove
these nodes. (This modification would have helped the orig-
inal evaluation as well.)
In short, none of the modifications required major changes to the
parser, but they did improve annotation speed significantly, as we
will see below.
4. RESULTS
The annotation times and rates for Pass 2 are as follows:
Set Pass 1 Time (Pass 2) Rate (Pass 2)
(hours:min) (words/hour)
1 ? 28:01 240
2 parser 16:21 412
3 revised parser 14:06 478
The rate increase for Set 2 over Set 1 was about 70%; for Set 3 over
Set 1, about double. Thus the time saved by the use of an automatic
first pass is substantial.
Assessing the reliability of the final product is somewhat trickier.
Set Pass 1 Accuracy (Pass 1) Accuracy (Pass 2)
LP LR LP LR
1 ? ? ? 99.84 99.76
2 parser 76.73 75.36 99.76 99.65
3 revised parser 82.87 81.42 99.81 99.26
where LP stands for labeled precision and LR stands for labeled
recall. The third column reports the accuracy of Pass 1 (the parser)
using the results of Pass 2 (Annotator A) as a gold standard. The
fourth column reports the accuracy of Pass 2 (Annotator A) using
the results of Pass 3 (Annotators A&B) as a gold standard.
We note several points:
? There is no indication that the addition of an automatic first
pass affected the accuracy of Pass 2. On the other hand, the
near-perfect reported accuracy of Pass 2 suggests that in fact
each pass biased subsequent passes substantially. We need
a more objective measure of reliability, which we leave for
future experiments.
? The parser revisions significantly improved the accuracy of
the parser with respect to the present metric (which is sensi-
tive to punctuation and empty elements). On Set 2 the revised
parser obtained 78.98/77.39% labeled precision/recall, an er-
ror reduction of about 9%.
? Not surprisingly, errors due to large-scale structural ambi-
guities were the most time-consuming to correct by hand. To
take an extreme example, one parse produced by the parser is
shown in Figure 2. It often matches the correct parse (shown
in Figure 3) at the lowest levels but the large-scale errors re-
quire the annotator to make many corrections.
5. DISCUSSION
In summary, although Chiang?s parser was not specifically de-
signed for Chinese, and trained on a moderate amount of data (less
than 100,000 words), the parses it provided were reliable enough
that the annotation rate was effectively doubled.
Now we turn to our third question: what kind of parser is most
suitable for an automatic first pass? Marcus et al [10] describe the
use of the deterministic parser Fidditch [6] as an automatic first
pass for the Penn (English) Treebank. They cite two features of this
parser as strengths:
1. It only produces a single parse per sentence, so that the an-
notator does not have to search through many parses.
2. It produces reliable partial parses, and leaves uncertain struc-
tures unspecified.
The Penn-Helsinki Parsed Corpus of Middle English was con-
structed using a statistical parser developed by Collins [4] as an
automatic first pass. This parser, as well as Chiang?s, retains the
first advantage but not the second. However, we suggest two ways
a statistical parser might be used to speed annotation further:
First, the parser can be made more useful to the annotator. A
statistical parser typically produces a single parse, but can also
(with little additional computation) produce multiple parses. Rat-
naparkhi [12] has found that choosing (by oracle) the best parse out
of the 20 highest-ranked parses boosts labeled recall and precision
(IP (NP (DP (DTYJ)) these
(NP (NN?))) businesses
(VP (VP (ADVP (AD?)) also
(VP (BA?) BA
(IP (NP (QP (CD??y) 36,000
(CLP (M1))) item
(CP (WHNP (-NONE- *OP*))
(CP (IP (VP (VVp?) possess
(NP (NN?) to be one?s own master
(NN#) knowledge
(NN?Y)))) property rights
(DEC{))) DE
(NP (NNb))) technologies
(VP (PP (P5) toward
(NP (DP (DT ??)) other
(NP (NN ?) businesses
(PU)
(NN ??)))) organizations
(VP (VV?#)))))) transfer
(CCZ) and
(VP (VVj?) spread
(IP (VP (PU?)
(VP (VV) create
(NP (NNB?)) income
(QP (CD????7) 4.43 billion
(CLP (M?)))))))) RMB
(PU ))
Figure 2: Parser output. Translation: ?These businesses also transfer and spread the intellectual property rights of 36,000 technolo-
gies to other businesses and organizations, creating an income of 4.43 billion RMB.?
(IP (NP-SBJ (DP (DTYJ)) these
(NP (NN?))) businesses
(VP (ADVP (AD ?)) also
(VP (VP (BA?) BA
(IP-OBJ (NP-SBJ (QP (CD??y) 36,000
(CLP (M1))) item
(CP (WHNP-1 (-NONE- *OP*))
(CP (IP (NP-SBJ (-NONE- *T*-1))
(VP (VVp?) possess
(NP-OBJ (NN ?) to be one?s own master
(NN #) knowledge
(NN ?Y)))) property rights
(DEC{))) DE
(NP (NNb))) technologies
(VP (PP-DIR (P5) toward
(NP (DP (DT??)) other
(NP (NN?) businesses
(PU)
(NN??)))) organizations
(VP (VP (VV ?#)) transfer
(CC Z) and
(VP (VV j?)))))) spread
(PU?)
(VP (VV) create
(NP-OBJ (NN B?)) income
(QP-EXT (CD????7) 4.43 billion
(CLP (M?)))))) RMB
(PU ))
Figure 3: Corrected parse for sentence of Figure 2.
from about 87% to about 93%. This suggests that if the annotator
had access to several of the highest-ranked parses, he or she could
save time by choosing the parse with the best gross structure and
making small-scale corrections.
Would such a change defeat the first advantage above by forcing
the annotator to search through multiple parses? No, because the
parses produced by a statistical parser are ranked. The additional
lower-ranked parses can only be of benefit to the annotator. Indeed,
because the chart contains information about the certainty of each
subparse, a statistical parser might regain the second advantage as
well, provided this information can be suitably presented.
Second, the annotator can be made more useful to the parser by
means of active learning or sample selection [5, 7]. (We are as-
suming now that the parser and annotator will take turns in a train-
parse-correct cycle, as opposed to a simple two-pass scheme.) The
idea behind sample selection is that some sentences are more in-
formative for training a statistical model than others; therefore, if
we have some way of automatically guessing which sentences are
more informative, these sentences are the ones we should hand-
correct first. Thus the parser?s accuracy will increase more quickly,
potentially requiring the annotator to make fewer corrections over-
all.
6. ACKNOWLEDGMENTS
We would like to thank Fei Xia, Mitch Marcus, Aravind Joshi,
Mary Ellen Okurowski and John Kovarik for their helpful com-
ments on the design of the evaluation, Beth Randall for her postpro-
cessing and error-checking code, and Nianwen Xue for serving as
?Annotator B.? This research was funded by DARPA N66001-00-
1-8915, DOD MDA904-97-C-0307, and NSF SBR-89-20230-15.
7. REFERENCES
[1] Daniel M. Bikel and David Chiang. Two statistical parsing
models applied to the Chinese Treebank. In Proceedings of
the Second Chinese Language Processing Workshop, pages
1?6, 2000.
[2] Eugene Charniak. Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the
Fourteenth National Conference on Artificial Intelligence
(AAAI-97), pages 598?603. AAAI Press/MIT Press, 1997.
[3] David Chiang. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the Assocation
for Computational Linguistics, pages 456?463, Hong Kong,
2000.
[4] Michael Collins. Three generative lexicalised models for
statistical parsing. In Proceedings of the 35th Annual
Meeting of the Assocation for Computational Linguistics
(ACL-EACL ?97), pages 16?23, Madrid, 1997.
[5] Ido Dagan and Sean P. Engelson. Committee-based sampling
for training probabilistic classifiers. In Proceedings of the
Twelfth International Conference on Machine Learning,
pages 150?157. Morgan Kaufmann, 1995.
[6] Donald Hindle. Acquiring disambiguation rules from text. In
Proceedings of the 27th Annual Meeting of the Association
for Computational Linguistics, 1989.
[7] Rebecca Hwa. Sample selection for statistical grammar
induction. In Proceedings of EMNLP/VLC-2000, pages
45?52, Hong Kong, 2000.
[8] Aravind K. Joshi and Yves Schabes. Tree-adjoining
grammars. In Grzegorz Rosenberg and Arto Salomaa,
editors, Handbook of Formal Languages and Automata,
volume 3, pages 69?124. Springer-Verlag, Heidelberg, 1997.
[9] David M. Magerman. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
Assocation for Computational Linguistics, pages 276?283,
Cambridge, MA, 1995.
[10] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics,
19:313?330, 1993.
[11] Owen Rambow, K. Vijay-Shanker, and David Weir. D-tree
grammars. In Proceedings of the 33rd Annual Meeting of the
Assocation for Computational Linguistics, pages 151?158,
Cambridge, MA, 1995.
[12] Adwait Ratnaparkhi. Maximum entropy models for natural
language ambiguity resolution. PhD thesis, University of
Pennsylvania, 1998.
[13] Philip Resnik. Probabilistic tree-adjoining grammar as a
framework for statistical natural language processing. In
Proceedings of the Fourteenth International Conference on
Computational Linguistics (COLING-92), pages 418?424,
Nantes, 1992.
[14] Yves Schabes. Stochastic lexicalized tree-adjoining
grammars. In Proceedings of the Fourteenth International
Conference on Computational Linguistics (COLING-92),
pages 426?432, Nantes, 1992.
[15] Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang,
Tony Kroch, and Mitch Marcus. Developing guidelines and
ensuring consistency for Chinese text annotation. In
Proceedings of the Second International Conference on
Language Resources and Evaluation (LREC-2000), Athens,
Greece, 2000.
Multi-Component TAG and Notions of Formal Power
William Schuler, David Chiang
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
fschuler,dchiangg@linc.cis.upenn.edu
Mark Dras
Inst. for Research in Cognitive Science
University of Pennsylvania
Suite 400A, 3401 Walnut Street
Philadelphia, PA 19104-6228
madras@linc.cis.upenn.edu
Abstract
This paper presents a restricted version
of Set-Local Multi-Component TAGs
(Weir, 1988) which retains the strong
generative capacity of Tree-Local Multi-
Component TAG (i.e. produces the
same derived structures) but has a
greater derivational generative capacity
(i.e. can derive those structures in more
ways). This formalism is then applied as
a framework for integrating dependency
and constituency based linguistic repre-
sentations.
1 Introduction
An aim of one strand of research in gener-
ative grammar is to nd a formalism that
has a restricted descriptive capacity sucient
to describe natural language, but no more
powerful than necessary, so that the reasons
some constructions are not legal in any nat-
ural language is explained by the formalism
rather than stipulations in the linguistic the-
ory. Several mildly context-sensitive grammar
formalisms, all characterizing the same string
languages, are currently possible candidates
for adequately describing natural language;
however, they dier in their capacities to as-
sign appropriate linguistic structural descrip-
tions to these string languages. The work in
this paper is in the vein of other work (Joshi,
2000) in extracting as much structural de-
scriptive power given a xed ability to de-
scribe strings, and uses this to model depen-
dency as well as constituency correctly.
One way to characterize a formalism's de-
scriptive power is by the the set of string lan-
guages it can generate, called its weak gener-
ative capacity. For example, Tree Adjoining
Grammars (TAGs) (Joshi et al, 1975) can
generate the language a
n
b
n
c
n
d
n
and Context-
Free Grammars (CFGs) cannot (Joshi, 1985).
S
a  b
S
a S
a  b
b
S
a S
a S
a  b
b
b
: : :
Figure 1: CFG-generable tree set for a
n
b
n
.
S
a S
b 
S
a S
a S
b S
b 
S
a S
a S
a S
b S
b S
b 
: : :
Figure 2: TAG-generable tree set for a
n
b
n
.
However, weak generative capacity ignores
the capacity of a grammar formalism to gener-
ate derived trees. This is known as its strong
generative capacity. For example, CFGs and
TAGs can both generate the language a
n
b
n
,
but CFGs can only associate the a's and b's
by making them siblings in the derived tree,
as shown in Figure 1, whereas a TAG can gen-
erate the innite set of trees for the language
a
n
b
n
that have a's and b's as siblings, as well
as the innite set of trees where the a's dom-
inate the b's in each tree, shown in Figure 2
(Joshi, 1985); thus TAGs have more strong
generative capacity than CFGs.
In addition to the tree sets and string lan-
guages a formalism can generate, there may
also be linguistic reasons to care about how
these structures are derived. For this reason,
multi-component TAGs (MCTAGs) (Weir,
1988) have been adopted to model some
linguistic phenomena. In multi-component
TAG, elementary trees are grouped into tree
sets, and at each step of the derivation all the
trees of a set adjoin simultaneously. In tree-
local MCTAG (TL-MCTAG) all the trees of
a set are required to adjoin into the same
elementary tree; in set-local MCTAG (SL-
MCTAG) all the trees of a set are required
to adjoin into the same elementary tree set.
TL-MCTAGs can generate the same string
languages and derived tree sets as ordinary
TAGs, so they have the same weak and strong
generative capacities, but TL-MCTAGs can
derive these same strings and trees in more
than TAGs can. One motivation for TL-
MCTAG as a linguistic formalism (Frank,
1992) is that it can generate a functional head
(such as does) in the same derivational step
as the lexical head with which it is associated
(see Figure 3) without violating any assump-
tions about the derived phrase structure tree
{ something TAGs cannot do in every case.

seem
:
S
does
S
.
.
.
VP
seem VP

sleep
:
S
John
VP
to sleep

sleep

seem
S
does S
John VP
seem VP
to sleep
Figure 3: TL-MCTAG generable derivation
This notion of the derivations of a gram-
mar formalism as they relate to the struc-
tures they derive has been called the deriva-
tional generative capacity (1992). Somewhat
more formally (for a precise denition, see
Becker et al (1992)): we annotate each ele-
ment of a derived structure with a code indi-
cating which step of the derivation produced
that element. This code is simply the address
of the corresponding node in the derivation
tree.
1
Then a formalism's derivational gener-
ative capacity is the sets of derived structures,
thus annotated, that it can generate.
1
In Becker et al (1992) the derived structures were
always strings, and the codes were not addresses but
unordered identiers. We trust that our denition is
in the spirit of theirs.
The derivational generative capacity of a
formalism also describes what parts of a de-
rived structure combine with each other. Thus
if we consider each derivation step to corre-
spond to a semantic dependency, then deriva-
tional generative capacity describes what
other elements a semantic element may de-
pend on. That is, if we interpret the derivation
trees of TAG as dependency structures and
the derived trees as phrase structures, then
the derivational generative capacity of TAG
limits the possible dependency structures that
can be assigned to a given phrase structure.
1.1 Dependency and Constituency
We have seen that TL-MCTAGs can gener-
ate some derivations for \Does John seem
to sleep" that TAG cannot, but even TL-
MCTAG cannot generate the string, \Does
John seem likely to sleep" with a derived tree
that matches some linguistic notion of correct
constituency and a derivation that matches
some notion of correct dependency. This is
because the components for `does' and `seem'
would have to adjoin into dierent compo-
nents of the elementary tree set for `likely'
(see Figure 4), which would require a set-local
multi-component TAG instead of tree-local.

seem
:
S
does
S
.
.
.
VP
seem VP

likely
:
S
.
.
.
VP
likely VP

sleep
:
S
John VP
to sleep

sleep

likely

seem
Figure 4: SL-MCTAG generable derivation
Unfortunately, unrestricted set-local multi-
component TAGs not only have more deriva-
tional generative capacity than TAGs, but
they also have more weak generative capac-
ity: SL-MCTAGs can generate the quadru-
ple copy language wwww, for example, which
does not correspond to any known linguis-
tic phenomenon. Other formalisms aiming to
model dependency correctly similarly expand
weak generative capacity, notably D-tree Sub-
stitution Grammar (Rambow et al, 1995),
and consequently end up with much greater
parsing complexity.
The work in this paper follows another
Figure 5: Set-local adjunction.
line of research which has focused on squeez-
ing as much strong generative capacity as
possible out of weakly TAG-equivalent for-
malisms. Tree-local multicomponent TAG
(Weir, 1988), nondirectional composition
(Joshi and Vijay-Shanker, 1999), and seg-
mented adjunction (Kulick, 2000) are exam-
ples of this approach, wherein the constraint
on weak generative capacity naturally limits
the expressivity of these systems. We discuss
the relation of the formalism of this paper,
Restricted MCTAG (R-MCTAG) with some
of these in Section 5.
2 Formalism
2.1 Restricting set-local MCTAG
The way we propose to deal with multi-
component adjunction is rst to limit the
number of components to two, and then,
roughly speaking, to treat two-component
adjunction as one-component adjunction by
temporarily removing the material between
the two adjunction sites. The reasons behind
this scheme will be explained in subsequent
sections, but we mention it now because it
motivates the somewhat complicated restric-
tions on possible adjunction sites:
 One adjunction site must dominate the
other. If the two sites are 
h
and 
l
, call
the set of nodes dominated by one node
but not strictly dominated by the other
the site-segment h
h
; 
l
i.
 Removing a site-segment must not de-
prive a tree of its foot node. That is, no
site-segment h
h
; 
l
i may contain a foot
node unless 
l
is itself the foot node.
 If two tree sets adjoin into the same tree,
the two site-segments must be simulta-
neously removable. That is, the two site-
segments must be disjoint, or one must
contain the other.
Because of the rst restriction, we depict
tree sets with the components connected by
a dominance link (dotted line), in the man-
ner of (Becker et al, 1991). As written, the
above rules only allow tree-local adjunction;
we can generalize them to allow set-local ad-
junction by treating this dominance link like
an ordinary arc. But this would increase the
weak generative capacity of the system. For
present purposes it is sucient just to allow
one type of set-local adjunction: adjoin the
upper tree to the upper foot, and the lower
tree to the lower root (see Figure 5).
This does not increase the weak generative
capacity, as will be shown in Section 2.3. Ob-
serve that the set-local TAG given in Figure 5
obeys the above restrictions.
2.2 2LTAG
For the following section, it is useful to think
of TAG in a manner other than the usual.
Instead of it being a tree-rewriting system
whose derivation history is recorded in a
derivation tree, it can be thought of as a set
of trees (the `derivation' trees) with a yield
function (here, reading o the node labels of
derivation trees, and composing correspond-
ing elementary trees by adjunction or sub-
stitution as appropriate) applied to get the
TAG trees. Weir (1988) observed that several
TAGs could be daisy-chained into a multi-
level TAG whose yield function is the com-
position of the individual yield functions.
More precisely: a 2LTAG is a pair of
TAGs hG;G
0
i = hh;NT ; I; A; Si; hI [ A; I [
A; I
0
; A
0
; S
0
ii.
We call G the object-level grammar, and
G
0
the meta-level grammar. The object-level
grammar is a standard TAG:  and NT are
its terminal and nonterminal alphabets, I and
A are its initial and auxiliary trees, and S 2 I
contains the trees which derivations may start
with.
The meta-level grammar G
0
is dened so
that it derives trees that look like derivation
trees of G:
 Nodes are labeled with (the names of)
elementary trees of G.
 Foot nodes have no labels.
 Arcs are labeled with Gorn addresses.
2
2
The Gorn address of a root node is ; if a node has
Gorn address , then its ith child has Gorn address


Figure 6: Adjoining into  by removing 

.
 An auxiliary tree may adjoin anywhere.
 When a tree  is adjoined at a node ,  is
rewritten as , and the foot of  inherits
the label of .
The tree set of hG;G
0
i, T (hG;G
0
i), is
f
G
[T (G
0
)], where f
G
is the yield function of
G and T (G
0
) is the tree set of G
0
. Thus, the
elementary trees of G
0
are combined to form
a derived tree, which is then interpreted as a
derivation tree for G, which gives instructions
for combining elementary trees of G into the
nal derived tree.
It was shown in Dras (1999) that when the
meta-level grammar is in the regular form of
Rogers (1994) the formalism is weakly equiv-
alent to TAG.
2.3 Reducing restricted R-MCTAG
to RF-2LTAG
Consider the case of a multicomponent tree
set f
1
; 
2
g adjoining into an initial tree 
(Figure 6). Recall that we dened a site-
segment of a pair of adjunction sites to be all
the nodes which are dominated by the upper
site but not the lower site. Imagine that the
site-segment 

is excised from , and that 
1
and 
2
are fused into a single elementary tree.
Now we can simulate the multi-component
adjunction by ordinary adjunction: adjoin the
fused 
1
and 
2
into what is left of ; then
replace 

by adjoining it between 
1
and 
2
.
The replacement of 

can be postponed
indenitely: some other (fused) tree set
f
1
0
; 
2
0
g can adjoin between 
1
and 
2
, and
so on, and then 

adjoins between the last
pair of trees. This will produce the same re-
sult as a series of set-local adjunctions.
More formally:
1. Fuse all the elementary tree sets of the
grammar by identifying the upper foot
  i.
with the lower root. Designate this fused
node the meta-foot.
2. For each tree, and for every possible com-
bination of site-segments, excise all the
site-segments and add all the trees thus
produced (the excised auxiliary trees and
the remainders) to the grammar.
Now that our grammar has been smashed
to pieces, we must make sure that the right
pieces go back in the right places. We could do
this using features, but the resulting grammar
would only be strongly equivalent, not deriva-
tionally equivalent, to the original. Therefore
we use a meta-level grammar instead:
1. For each initial tree, and for every pos-
sible combination of site-segments, con-
struct the derivation tree that will re-
assemble the pieces created in step (2)
above and add it to the meta-level gram-
mar.
2. For each auxiliary tree, and for every pos-
sible combination of site-segments, con-
struct a derivation tree as above, and for
the node which corresponds to the piece
containing the meta-foot, add a child, la-
bel its arc with the meta-foot's address
(within the piece), and mark it a foot
node. Add the resulting (meta-level) aux-
iliary tree to the meta-level grammar.
Observe that set-local adjunction corre-
sponds to meta-level adjunction along the
(meta-level) spine. Recall that we restricted
set-local adjunction so that a tree set can
only adjoin at the foot of the upper tree and
the root of the lower tree. Since this pair of
nodes corresponds to the meta-foot, we can
restate our restriction in terms of the con-
verted grammar: no meta-level adjunction is
allowed along the spine of a (meta-level) aux-
iliary tree except at the (meta-level) foot.
Then all meta-level adjunction is regular
adjunction in the sense of (Rogers, 1994).
Therefore this converted 2LTAG produces
derivation tree sets which are recognizable,
and therefore our formalism is strongly equiv-
alent to TAG.
Note that this restriction is much stronger
than Rogers' regular form restriction. This
was done for two reasons. First, the deni-
tion of our restriction would have been more
complicated otherwise; second, this restric-
tion overcomes some computational dicul-
ties with RF-TAG which we discuss below.
3 Linguistic Applications
In cases where TAG models dependencies cor-
rectly, the use of R-MCTAG is straightfor-
ward: when an auxiliary tree adjoins at a
site pair which is just a single node, it looks
just like conventional adjunction. However, in
problematic cases we can use the extra expres-
sive power of R-MCTAG to model dependen-
cies correctly. Two such cases are discussed
below.
3.1 Bridge and Raising Verbs
S
NP
John
VP
V
thinks
S
.
.
.
S
S
C
that
S
.
.
.
VP
V
seems
VP
S
NP
Mary
VP
V
to sleep
Figure 7: Trees for (1)
Consider the case of sentences which con-
tain both bridge and raising verbs, noted
by Rambow et al (1995). In most TAG-based
analyses, bridge verbs adjoin at S (or C
0
), and
raising verbs adjoin at VP (or I
0
). Thus the
derivation for a sentence like
(1) John thinks that Mary seems to
sleep.
will have the trees for thinks and seems si-
multaneously adjoining into the tree for like,
which, when interpreted, gives an incorrect
dependency structure.
But under the present view we can ana-
lyze sentences like (1) with derivations mir-
roring dependencies. The desired trees for (1)
are shown in Figure 7. Since the tree for that
seems can meta-adjoin around the subject,
the tree for thinks correctly adjoins into the
tree for seems rather than eat.
Also, although the above analysis produces
the correct dependency links, the directions
are inverted in some cases. This is a disad-
vantage compared to, for example, DSG; but
since the directions are consistently inverted,
for applications like translation or statistical
modeling, the particular choice of direction is
usually immaterial.
3.2 More on Raising Verbs
Tree-local MCTAG is able to derive (2a), but
unable to derive (2b) except by adjoining the
auxiliary tree for to be likely at the foot of the
auxiliary tree for seem (Frank et al, 1999).
(2) a. Does John seem to sleep?
b. Does John seem to be likely to
sleep?
The derivation structure of this analysis does
not match the dependencies, however|seem
adjoins into to sleep.
DSG can derive this sentence with a deriva-
tion matching the dependencies, but it loses
some of the advantage of TAG in that, for
example, cases of super-raising (where the
verb is raised out of two clauses) must be ex-
plicitly ruled out by subsertion-insertion con-
straints. Frank et al (1999) and Kulick (2000)
give analyses of raising which assign the de-
sired derivation structures without running
into this problem. It turns out that the anal-
ysis of raising from the previous section, de-
signed for a translation problem, has both
of these properties as well. The grammar is
shown back in Figure 4.
4 A Parser
Figure 8 shows a CKY-style parser for our
restriction of MCTAG as a system of inference
rules. It is limited to grammars whose trees
are at most binary-branching.
The parser consists of rules over items of
one of the following forms, where w
1
  w
n
is
the input; , 
h
, and 
l
specify nodes of the
grammar; i, j, k, and l are integers between 0
and n inclusive; and code is either + or  :
 [; code ; i; ; ; l; ; ] and
[; code ; i; j; k; l; ; ] function as in
a CKY-style parser for standard TAG
(Vijay-Shanker, 1987): the subtree
rooted by  2 T derives a tree whose
fringe is w
i
  w
l
if T is initial, or
w
i
  w
j
Fw
k
  w
l
if T is the lower
auxiliary tree of a set and F is the label
of its foot node. In all four item forms,
code = + i adjunction has taken place
at .
 [; code ; i; j; k; l; ; 
l
] species that the
segment h; 
l
i derives a tree whose
fringe is w
i
  w
j
Lw
k
  w
l
, where L is
the label of 
l
. Intuitively, it means that
a potential site-segment has been recog-
nized.
 [; code ; i; j; k; l; 
h
; 
l
] species, if  be-
longs to the upper tree of a set, that
the subtree rooted by , the segment
h
h
; 
l
i, and the lower tree concatenated
together derive a tree whose fringe is
w
i
  w
j
Fw
k
  w
l
, where F is the la-
bel of the lower foot node. Intuitively, it
means that a tree set has been partially
recognized, with a site-segment inserted
between the two components.
The rules which require dier from a TAG
parser and hence explanation are Pseudopod,
Push, Pop, and Pop-push. Pseudopod applies
to any potential lower adjunction site and is
so called because the parser essentially views
every potential site-segment as an auxiliary
tree (see Section 2.3), and the Pseudopod ax-
iom recognizes the feet of these false auxiliary
trees.
The Push rule performs the adjunction of
one of these false auxiliary trees|that is, it
places a site-segment between the two trees of
an elementary tree set. It is so called because
the site-segment is saved in a \stack" so that
the rest of its elementary tree can be recog-
nized later. Of course, in our case the \stack"
has at most one element.
The Pop rule does the reverse: every com-
pleted elementary tree set must contain a
site-segment, and the Pop rule places it back
where the site-segment came from, emptying
the \stack." The Pop-push rule performs set-
local adjunction: a completed elementary tree
set is placed between the two trees of yet an-
other elementary tree set, and the \stack" is
unchanged.
Pop-push is computationally the most ex-
pensive rule; since it involves six indices and
three dierent elementary trees, its running
time is O(n
6
G
3
).
It was noted in (Chiang et al, 2000) that
for synchronous RF-2LTAG, parse forests
could not be transferred in time O(n
6
). This
fact turns out to be connected to several prop-
erties of RF-TAG (Rogers, 1994).
3
3
Thanks to Anoop Sarkar for pointing out the rst
The CKY-style parser for regular form
TAG described in (Rogers, 1994) essentially
keeps track of adjunctions using stacks, and
the regular form constraint ensures that the
stack depth is bounded. The only kinds of ad-
junction that can occur to arbitrary depth are
root and foot adjunction, which are treated
similarly to substitution and do not aect the
stacks. The reader will note that our parser
works in exactly the same way.
A problem arises if we allow both root
and foot adjunction, however. It is well-known
that allowing both types of adjunction creates
derivational ambiguity (Vijay-Shanker, 1987):
adjoining 
1
at the foot of 
2
produces the
same derived tree that adjoining 
1
at the
root of 
2
would. The problem is not the am-
biguity per se, but that the regular form TAG
parser, unlike a standard TAG parser, does
not always distinguish these multiple deriva-
tions, because root and foot adjunction are
both performed by the same rule (analogous
to our Pop-push). Thus for a given application
of this rule, it is not possible to say which tree
is adjoining into which without examining the
rest of the derivation.
But this knowledge is necessary to per-
form certain tasks online: for example, enforc-
ing adjoining constraints, computing proba-
bilities (and pruning based on them), or per-
forming synchronous mappings. Therefore we
arbitrarily forbid one of the two possibilities.
4
The parser given in Section 4 already takes
this into account.
5 Discussion
Our version of MCTAG follows other
work in incorporating dependency into a
constituency-based approach to modeling
natural language. One such early integra-
tion involved work by Gaifman (1965), which
showed that projective dependency grammars
could be represented by CFGs. However, it
is known that there are common phenom-
ena which require non-projective dependency
grammars, so looking only at projective de-
such connection.
4
Against tradition, we forbid root adjunction, be-
cause adjunction at the foot ensures that a bottom-up
traversal of the derived tree will encounter elementary
trees in the same order as they appear in a bottom-up
traversal of the derivation tree, simplifying the calcu-
lation of derivations.
Goal: [
r
; ; 0; ; ; n; ; ] 
r
an initial root
(Leaf) [;+; i; ; ; j; ; ]  a leaf
(Foot) [;+; i; i; j; j; ; ]  a lower foot
(Pseudopod) [;+; i; i; j; j; ; ]
(Unary)
[
1
;+; i; p; q; j; 
h
; 
l
]
[; ; i; p; q; j; 
h
; 
l
]


1
(Binary 1)
[
1
;+; i; p; q; j; 
h
; 
l
] [
2
;+; j; ; ; k; ; ]
[; ; i; p; q; k; 
h
; 
l
]


1

2
(Binary 2)
[
1
;+; i; ; ; j; ; ] [
2
;+; j; p; q; k; 
h
; 
l
]
[; ; i; p; q; k; 
h
; 
l
]


1

2
(No adjunction)
[; ; i; p; q; j; 
h
; 
l
]
[;+; i; p; q; j; 
h
; 
l
]
(Push)
[
1
;+; j; p; q; k; ; ] [
h
; ; i; j; k; l; ; 
l
]
[;+; i; p; q; l; 
h
; 
l
]

.
.
.

1
(i.e.  is an upper foot
and 
1
is a lower root)
(Pop)
[
l
; ; j; p; q; k; 
h
0
; 
l
0
] [
r
;+; i; j; k; l; 
h
; 
l
]
[
h
;+; i; p; q; l; 
h
0
; 
l
0
]

r
a root of an upper tree
adjoinable at h
h
; 
l
i
(Pop-push)
[
1
;+; j; p; q; k; ; ] [
r
;+; i; j; k; l; 
h
; 
l
]
[;+; i; p; q; l; 
h
; 
l
]

.
.
.

1
, 
r
a root of an upper
tree adjoinable at
h; 
1
i
Figure 8: Parser
pendency grammars is inadequate. Follow-
ing the observation of TAG derivations' sim-
ilarity to dependency relations, other for-
malisms have also looked at relating depen-
dency and constituency approaches to gram-
mar formalisms.
A more recent instance is D-Tree Substi-
tution Grammars (DSG) (Rambow et al,
1995), where the derivations are also inter-
preted as dependency relations. Thought of
in the terms of this paper, there is a clear
parallel with R-MCTAG, with a local set
ultimately representing dependencies having
some yield function applied to it; the idea
of non-immediate dominance also appears in
both formalisms. The dierence between the
two is in the kinds of languages that they are
able to describe: DSG is both less and more
restrictive than R-MCTAG. DSG can gener-
ate the language count-k for some arbitrary
k (that is, fa
1
n
a
2
n
: : : a
k
n
g), which makes
it extremely powerful, whereas R-MCTAG
can only generate count-4. However, DSG
cannot generate the copy language (that is,
fww j w 2 

g with  some terminal al-
phabet), whereas R-MCTAG can; this may
be problematic for a formalism modeling nat-
ural language, given the key role of the copy
language in demonstrating that natural lan-
guage is not context-free (Shieber, 1985). R-
MCTAG is thus a more constrained relaxation
of the notion of immediate dominance in fa-
vor of non-immediate dominance than is the
case for DSG.
Another formalism of particular interest
here is the Segmented Adjoining Grammar of
(Kulick, 2000). This generalization of TAG is
characterized by an extension of the adjoining
operation, motivated by evidence in scram-
bling, clitic climbing and subject-to-subject
raising. Most interestingly, this extension to
TAG, proposed on empirical grounds, is de-
ned by a composition operation with con-
strained non-immediate dominance links that
looks quite similar to the formalism described
in this paper, which began from formal con-
siderations and was then applied to data. This
conuence suggests that the ideas described
here concerning combining dependency and
constituency might be reaching towards some
deeper connection.
6 Conclusion
From a theoretical perspective, extracting
more derivational generative capacity and
thereby integrating dependency and con-
stituency into a common framework is an in-
teresting exercise. It also, however, proves to
be useful in modeling otherwise problematic
constructions, such as subject-auxiliary inver-
sion and bridge and raising verb interleaving.
Moreover, the formalism developed from the-
oretical considerations, presented in this pa-
per, has similar properties to work developed
on empirical grounds, suggesting that this is
worth further exploration.
References
Tilman Becker, Aravind Joshi, and Owen Ram-
bow. 1991. Long distance scrambling and tree
adjoining grammars. In Fifth Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL'91), pages 21{26.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The derivational generative power of for-
mal systems, or, Scrambling is beyond LCFRS.
Technical Report IRCS-92-38, Institute for Re-
search in Cognitive Science, University of Penn-
sylvania.
David Chiang, William Schuler, and Mark Dras.
2000. Some Remarks on an Extension of Syn-
chronous TAG. In Proceedings of TAG+5,
Paris, France.
Mark Dras. 1999. A meta-level grammar: re-
dening synchronous TAG for translation and
paraphrase. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL '99).
Robert Frank, Seth Kulick, and K. Vijay-Shanker.
1999. C-command and extraction in tree-
adjoining grammar. Proceedings of the Sixth
Meeting on the Mathematics of Language
(MOL6).
Robert Frank. 1992. Syntactic locality and
tree adjoining grammar: grammatical acquisi-
tion and processing perspectives. Ph.D. the-
sis, Computer Science Department, University
of Pennsylvania.
Haim Gaifman. 1965. Dependency Systems and
Phrase-Structure Systems. Information and
Control, 8:304{337.
Gerald Gazdar. 1988. Applicability of indexed
grammars to natural languages. In Uwe Reyle
and Christian Rohrer, editors, Natural Lan-
guage Parsin and Linguistic Theories. D. Reidel
Publishing Company, Dordrecht, Holland.
Aravind Joshi and K. Vijay-Shanker. 1999. Com-
positional Semantics with Lexicalized Tree-
Adjoining Grammar (LTAG): How Much Un-
derspecication is Necessary? In Proceedings of
the 2nd International Workshop on Computa-
tional Semantics.
Aravind K. Joshi, Leon S. Levy, and M. Taka-
hashi. 1975. Tree adjunct grammars. Journal
of computer and system sciences, 10:136{163.
Aravind K. Joshi. 1985. How much context sen-
sitivity is necessary for characterizing struc-
tural descriptions: Tree adjoining grammars. In
L. Karttunen D. Dowty and A. Zwicky, editors,
Natural language parsing: Psychological, com-
putational and theoretical perspectives, pages
206{250. Cambridge University Press, Cam-
bridge, U.K.
Aravind Joshi. 2000. Relationship between strong
and weak generative power of formal systems.
In Proceedings of TAG+5, pages 107{114, Paris,
France.
Seth Kulick. 2000. A uniform account of locality
constraints for clitic climbing and long scram-
bling. In Proceedings of the Penn Linguistics
Colloquium.
Owen Rambow, David Weir, and K. Vijay-
Shanker. 1995. D-tree grammars. In Proceed-
ings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL '95).
James Rogers. 1994. Capturing CFLs with tree
adjoining grammars. In Proceedings of the 32nd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL '94).
Stuart Shieber. 1985. Evidence against the
context-freeness of natural language. Linguis-
tics and Philosophy, 8:333{343.
K. Vijay-Shanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis, Department of Com-
puter and Information Science, University of
Pennsylvania.
David Weir. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Department of Computer and In-
formation Science, University of Pennsylvania.
Statistical parsing with an automatically-extracted
tree adjoining grammar
David Chiang
Department of Computer and Information Science
University of Pennsylvania
200 S 33rd St
Philadelphia PA 19104
dchiang@linc.cis.upenn.edu
Abstract
We discuss the advantages of lexical-
ized tree-adjoining grammar as an al-
ternative to lexicalized PCFG for sta-
tistical parsing, describing the induction
of a probabilistic LTAG model from the
Penn Treebank and evaluating its pars-
ing performance. We nd that this in-
duction method is an improvement over
the EM-based method of (Hwa, 1998),
and that the induced model yields re-
sults comparable to lexicalized PCFG.
1 Introduction
Why use tree-adjoining grammar for statisti-
cal parsing? Given that statistical natural lan-
guage processing is concerned with the proba-
ble rather than the possible, it is not because
TAG can describe constructions like arbitrar-
ily large Dutch verb clusters. Rather, what
makes TAG useful for statistical parsing are
the structural descriptions it assigns to bread-
and-butter sentences.
The approach of Chelba and Jelinek (1998)
to language modeling is illustrative: even
though the probability estimate of w appear-
ing as the kth word can be conditioned on the
entire history w
1
; : : : ; w
k 1
, the quantity of
available training data limits the usable con-
text to about two words|but which two? A
trigram model chooses w
k 1
and w
k 2
and
works quite well; a model which chose w
k 7
and w
k 11
would probably work less well. But
(Chelba and Jelinek, 1998) chooses the lexical
heads of the two previous constituents as de-
termined by a shift-reduce parser, and works
better than a trigram model. Thus the (vir-
tual) grammar serves to structure the history
so that the two most useful words can be cho-
sen, even though the structure of the problem
itself is entirely linear.
Similarly, nothing about the parsing prob-
lem requires that we construct any struc-
ture other than phrase structure. But be-
ginning with (Magerman, 1995) statistical
parsers have used bilexical dependencies with
great success. Since these dependencies are
not encoded in plain phrase-structure trees,
the standard approach has been to let the lex-
ical heads percolate up the tree, so that when
one lexical head is immediately dominated by
another, it is understood to be dependent on
it. Eectively, a dependency structure is made
parasitic on the phrase structure so that they
can be generated together by a context-free
model.
However, this solution is not ideal. Aside
from cases where context-free derivations are
incapable of encoding both constituency and
dependency (which are somewhat isolated
and not of great interest for statistical pars-
ing) there are common cases where percola-
tion of single heads is not sucient to encode
dependencies correctly|for example, relative
clause attachment or raising/auxiliary verbs
(see Section 3). More complicated grammar
transformations are necessary.
A more suitable approach is to employ
a grammar formalism which produces struc-
tural descriptions that can encode both con-
stituency and dependency. Lexicalized TAG
is such a formalism, because it assigns to
each sentence not only a parse tree, which
is built out of elementary trees and is inter-
preted as encoding constituency, but a deriva-
tion tree, which records how the various el-
ementary trees were combined together and
is commonly intepreted as encoding depen-
dency. The ability of probabilistic LTAG to
NP
NNP
John
S
NP# VP
VB
leave
VP
MD
should
VP
NP
NN
tomorrow
(
1
)
(
2
)
() ()
)

2

1
1

2

2,1
S
NP
NNP
John
VP
MD
should
VP
VB
leave
NP
NN
tomorrow
Figure 1: Grammar and derivation for \John should leave tomorrow."
model bilexical dependencies was noted early
on by (Resnik, 1992).
It turns out that there are other pieces of
contextual information that need to be ex-
plicitly accounted for in a CFG by gram-
mar transformations but come for free in a
TAG. We discuss a few such cases in Sec-
tion 3. In Sections 4 and 5 we describe
an experiment to test the parsing accuracy
of a probabilistic TAG extracted automati-
cally from the Penn Treebank. We nd that
the automatically-extracted grammar gives
an improvement over the EM-based induction
method of (Hwa, 1998), and that the parser
performs comparably to lexicalized PCFG
parsers, though certainly with room for im-
provement.
We emphasize that TAG is attractive not
because it can do things that CFG cannot,
but because it does everything that CFG can,
only more cleanly. (This is where the anal-
ogy with (Chelba and Jelinek, 1998) breaks
down.) Thus certain possibilities which were
not apparent in a PCFG framework or pro-
hibitively complicated might become simple
to implement in a PTAG framework; we con-
clude by oering two such possibilities.
2 The formalism
The formalism we use is a variant of lexical-
ized tree-insertion grammar (LTIG), which is
in turn a restriction of LTAG (Schabes and
Waters, 1995). In this variant there are three
kinds of elementary tree: initial, (predicative)
auxiliary, and modier, and three composi-
tion operations: substitution, adjunction, and
sister-adjunction.
Auxiliary trees and adjunction are re-
stricted as in TIG: essentially, no wrapping
adjunction or anything equivalent to wrap-
ping adjunction is allowed. Sister-adjunction
is not an operation found in standard deni-
tions of TAG, but is borrowed from D-Tree
Grammar (Rambow et al, 1995). In sister-
adjunction the root of a modier tree is added
as a new daughter to any other node. (Note
that as it stands sister-adjunction is com-
pletely unconstrained; it will be constrained
by the probability model.) We introduce this
operation simply so we can derive the at
structures found in the Penn Treebank. Fol-
lowing (Schabes and Shieber, 1994), multiple
modier trees can be sister-adjoined at a sin-
gle site, but only one auxiliary tree may be
adjoined at a single node.
Figure 1 shows an example grammar and
the derivation of the sentence \John should
leave tomorrow." The derivation tree encodes
this process, with each arc corresponding to a
composition operation. Arcs corresponding to
substitution and adjunction are labeled with
the Gorn address
1
of the substitution or ad-
1
A Gorn address is a list of integers: the root of a
tree has address , and the jth child of the node with
junction site. An arc corresponding to the
sister-adjunction of a tree between the ith and
i + 1th children of  (allowing for two imagi-
nary children beyond the leftmost and right-
most children) is labeled ; i.
This grammar, as well as the grammar used
by the parser, is lexicalized in the sense that
every elementary tree has exactly one termi-
nal node, its lexical anchor.
Since sister-adjunction can be simulated
by ordinary adjunction, this variant is, like
TIG (and CFG), weakly context-free and
O(n
3
)-time parsable. Rather than coin a new
acronym for this particular variant, we will
simply refer to it as \TAG" and trust that no
confusion will arise.
The parameters of a probabilistic TAG
(Resnik, 1992; Schabes, 1992) are:
X

P
i
() = 1
X

P
s
( j ) = 1
X

P
a
( j ) + P
a
(NONE j ) = 1
where  ranges over initial trees,  over aux-
iliary trees,  over modier trees, and  over
nodes. P
i
() is the probability of beginning
a derivation with ; P
s
( j ) is the prob-
ability of substituting  at ; P
a
( j ) is
the probability of adjoining  at ; nally,
P
a
(NONE j ) is the probability of nothing
adjoining at . (Carroll and Weir, 1997) sug-
gest other parameterizations worth exploring
as well.
Our variant adds another set of parameters:
X

P
sa
( j ; i; f) + P
sa
(STOP j ; i; f) = 1
This is the probability of sister-adjoining 
between the ith and i + 1th children of  (as
before, allowing for two imaginary children
beyond the leftmost and rightmost children).
Since multiple modier trees can adjoin at the
same location, P
sa
() is also conditioned on a
ag f which indicates whether  is the rst
modier tree (i.e., the one closest to the head)
to adjoin at that location.
The probability of a derivation can then be
expressed as a product of the probabilities of
address i has address i  j.
the individual operations of the derivation.
Thus the probability of the example deriva-
tion of Figure 1 would be
P
i
(
2
)  P
a
(NONE j 
2
()) 
P
s
(
1
j 
2
(1))  P
a
( j 
2
(2)) 
P
sa
( j 
2
(2); 1; true) 
P
sa
(STOP j 
2
(2); 1; false) 
P
sa
(STOP j 
2
(); 0; true)  : : :
where (i) is the node of  with address i.
We want to obtain a maximum-likelihood
estimate of these parameters, but cannot es-
timate them directly from the Treebank, be-
cause the sample space of PTAG is the space
of TAG derivations, not the derived trees that
are found in the Treebank. One approach,
taken in (Hwa, 1998), is to choose some gram-
mar general enough to parse the whole corpus
and obtain a maximum-likelihood estimate by
EM. Another approach, taken in (Magerman,
1995) and others for lexicalized PCFGs and
(Neumann, 1998; Xia, 1999; Chen and Vijay-
Shanker, 2000) for LTAGs, is to use heuristics
to reconstruct the derivations, and directly es-
timate the PTAG parameters from the recon-
structed derivations. We take this approach
as well. (One could imagine combining the
two approaches, using heuristics to extract a
grammar but EM to estimate its parameters.)
3 Some properties of probabilistic
TAG
In a lexicalized TAG, because each compo-
sition brings together two lexical items, ev-
ery composition probability involves a bilex-
ical dependency. Given a CFG and head-
percolation scheme, an equivalent TAG can
be constructed whose derivations mirror the
dependency analysis implicit in the head-
percolation scheme.
Furthermore, there are some dependency
analyses encodable by TAGs that are not en-
codable by a simple head-percolation scheme.
For example, for the sentence \John should
have left," Magerman's rules make should and
have the heads of their respective VPs, so that
there is no dependency between left and its
subject John (see Figure 2a). Since nearly a
quarter of nonempty subjects appear in such
a conguration, this is not a small problem.
left
have
should
John
left
have
should
John
(a) (b)
Figure 2: Bilexical dependencies for \John
should have left."
(We could make VP the head of VP instead,
but this would generate auxiliaries indepen-
dently of each other, so that, for example,
P (John leave) > 0.)
TAG can produce the desired dependencies
(b) easily, using the grammar of Figure 1. A
more complex lexicalization scheme for CFG
could as well (one which kept track of two
heads at a time, for example), but the TAG
account is simpler and cleaner.
Bilexical dependencies are not the only
nonlocal dependencies that can be used to
improve parsing accuracy. For example, the
attachment of an S depends on the presence
or absence of the embedded subject (Collins,
1999); Treebank-style two-level NPs are mis-
modeled by PCFG (Collins, 1999; Johnson,
1998); the generation of a node depends on
the label of its grandparent (Charniak, 2000;
Johnson, 1998). In order to capture such
dependencies in a PCFG-based model, they
must be localized either by transforming the
data or modifying the parser. Such changes
are not always obvious a priori and often
must be devised anew for each language or
each corpus.
But none of these cases really requires
special treatment in a PTAG model, be-
cause each composition probability involves
not only a bilexical dependency but a \biarbo-
real" (tree-tree) dependency. That is, PTAG
generates an entire elementary tree at once,
conditioned on the entire elementary tree be-
ing modied. Thus dependencies that have to
be stipulated in a PCFG by tree transforma-
tions or parser modications are captured for
free in a PTAG model. Of course, the price
that the PTAG model pays is sparser data;
the backo model must therefore be chosen
carefully.
4 Inducing a stochastic grammar
from the Treebank
4.1 Reconstructing derivations
We want to extract from the Penn Tree-
bank an LTAG whose derivations mirror
the dependency analysis implicit in the
head-percolation rules of (Magerman, 1995;
Collins, 1997). For each node , these rules
classify exactly one child of  as a head and
the rest as either arguments or adjuncts. Us-
ing this classication we can construct a TAG
derivation (including elementary trees) from a
derived tree as follows:
1. If  is an adjunct, excise the subtree
rooted at  to form a modier tree.
2. If  is an argument, excise the subtree
rooted at  to form an initial tree, leaving
behind a substitution node.
3. If  has a right corner  which is an ar-
gument with the same label as  (and all
intervening nodes are heads), excise the
segment from  down to  to form an
auxiliary tree.
Rules (1) and (2) produce the desired re-
sult; rule (3) changes the analysis somewhat
by making subtrees with recursive arguments
into predicative auxiliary trees. It produces,
among other things, the analysis of auxiliary
verbs described in the previous section. It is
applied in a greedy fashion, with potential s
considered top-down and potential s bottom-
up. The complicated restrictions on  are sim-
ply to ensure that a well-formed TIG deriva-
tion is produced.
4.2 Parameter estimation and
smoothing
Now that we have augmented the training
data to include TAG derivations, we could
try to directly estimate the parameters of the
model from Section 2. But since the number of
(tree, site) pairs is very high, the data would
be too sparse. We therefore generate an ele-
mentary tree in two steps: rst the tree tem-
plate (that is, the elementary tree minus its
modier trees auxiliary trees
PP
IN

NP#
JJ

,

ADVP
RB

VP
TO

VP
VP
MD

VP
NP
NNS

NP
NP
NNS

S
NP# VP
VBD

NP#
S
NP# VP
VBD

S
VP
VB

NP#
initial trees
Figure 3: A few of the more frequently-occurring tree templates.  marks where the lexical
anchor is inserted.
anchor), then the anchor. The probabilities
are decomposed as follows:
P
i
() = P
i
1
(

)P
i
2
(w

j 

)
P
s
( j ) = P
s
1
(

j )
P
s
2
(w

j 

; t

; w

)
P
a
( j ) = P
a
1
(

j )
P
a
2
(w

j 

; t

; w

)
P
sa
( j ; i; f) = P
sa
1
(

j ; i; f)
P
sa
2
(w

j 

; t

; w

; f)
where 

is the tree template of , t

is the
part-of-speech tag of the anchor, and w

is
the anchor itself.
The generation of the tree template has two
backo levels: at the rst level, the anchor
of  is ignored, and at the second level, the
POS tag of the anchor as well as the ag f
are ignored. The generation of the anchor has
three backo levels: the rst two are as before,
and the third just conditions the anchor on its
POS tag. The backed-o models are combined
by linear interpolation, with the weights cho-
sen as in (Bikel et al, 1997).
5 The experiment
5.1 Extracting the grammar
We ran the algorithm given in Section 4.1 on
sections 02{21 of the Penn Treebank. The ex-
tracted grammar is large (about 73,000 trees,
with words seen fewer than four times re-
placed with the symbol *UNKNOWN*), but if we
1
10
100
1000
10000
100000
1 10 100 1000 10000
Fr
eq
ue
nc
y
Rank
Figure 4: Frequency of tree templates versus
rank (log-log)
consider elementary tree templates, the gram-
mar is quite manageable: 3626 tree templates,
of which 2039 occur more than once (see Fig-
ure 4).
The 616 most frequent tree-template types
account for 99% of tree-template tokens in the
training data. Removing all but these trees
from the grammar increased the error rate by
about 5% (testing on a subset of section 00).
A few of the most frequent tree-templates are
shown in Figure 3.
So the extracted grammar is fairly com-
pact, but how complete is it? If we plot the
growth of the grammar during training (Fig-
ure 5), it's not clear the grammar will ever
converge, even though the very idea of a
110
100
1000
10000
1 10 100 1000 10000 100000 1e+06
Ty
pe
s
Tokens
Figure 5: Growth of grammar during training
(log-log)
grammar requires it. Three possible explana-
tions are:
 New constructions continue to appear.
 Old constructions continue to be (erro-
neously) annotated in new ways.
 Old constructions continue to be com-
bined in new ways, and the extraction
heuristics fail to factor this variation out.
In a random sample of 100 once-seen ele-
mentary tree templates, we found (by casual
inspection) that 34 resulted from annotation
errors, 50 from deciencies in the heuristics,
and four apparently from performance errors.
Only twelve appeared to be genuine.
Therefore the continued growth of the
grammar is not as rapid as Figure 5 might
indicate. Moreover, our extraction heuristics
evidently have room to improve. The major-
ity of trees resulting from deciencies in the
heuristics involved complicated coordination
structures, which is not surprising, since co-
ordination has always been problematic for
TAG.
To see what the impact of this failure to
converge is, we ran the grammar extractor on
some held-out data (section 00). Out of 45082
tree tokens, 107 tree templates, or 0.2%, had
not been seen in training. This amounts to
about one unseen tree template every 20 sen-
tences. When we consider lexicalized trees,
this gure of course rises: out of the same
45082 tree tokens, 1828 lexicalized trees, or
4%, had not been seen in training.
So the coverage of the grammar is quite
good. Note that even in cases where the parser
encounters a sentence for which the (fallible)
extraction heuristics would have produced an
unseen tree template, it is possible that the
parser will use other trees to produce the cor-
rect bracketing.
5.2 Parsing with the grammar
We used a CKY-style parser similar to the one
described in (Schabes and Waters, 1996), with
a modication to ensure completeness (be-
cause foot nodes are treated as empty, which
CKY prohibits) and another to reduce useless
substitutions. We also extended the parser
to simulate sister-adjunction as regular ad-
junction and compute the ag f which dis-
tinguishes the rst modier from subsequent
modiers.
We use a beam search, computing the score
of an item [; i; j] by multiplying it by the
prior probability P () (Goodman, 1997); any
item with score less than 10
 5
times that of
the best item in a cell is pruned.
Following (Collins, 1997), words occur-
ring fewer than four times in training were
replaced with the symbol *UNKNOWN* and
tagged with the output of the part-of-speech
tagger described in (Ratnaparkhi, 1996). Tree
templates occurring only once in training
were ignored entirely.
We rst compared the parser with (Hwa,
1998): we trained the model on sentences of
length 40 or less in sections 02{09 of the Penn
Treebank, down to parts of speech only, and
then tested on sentences of length 40 or less in
section 23, parsing from part-of-speech tag se-
quences to fully bracketed parses. The metric
used was the percentage of guessed brackets
which did not cross any correct brackets. Our
parser scored 84.4% compared with 82.4% for
(Hwa, 1998), an error reduction of 11%.
Next we compared our parser against lex-
icalized PCFG parsers, training on sections
02{21 and testing on section 23. The results
are shown in Figure 6.
These results place our parser roughly in
the middle of the lexicalized PCFG parsers.
While the results are not state-of-the-art,
they do demonstrate the viability of TAG
as a framework for statistical parsing. With
 40 words  100 words
LR LP CB 0 CB  2 CB LR LP CB 0 CB  2 CB
(Magerman, 1995) 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8
(Collins, 1996) 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8
present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8
(Collins, 1997) 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6
(Charniak, 2000) 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7
Figure 6: Parsing results. LR = labeled recall, LP = labeled precision; CB = average crossing
brackets, 0 CB = no crossing brackets,  2 CB = two or fewer crossing brackets. All gures
except CB are percentages.
improvements in smoothing and cleaner han-
dling of punctuation and coordination, per-
haps these results can be brought more up-
to-date.
6 Conclusion: related and future
work
(Neumann, 1998) describes an experiment
similar to ours, although the grammar he ex-
tracts only arrives at a complete parse for 10%
of unseen sentences. (Xia, 1999) describes a
grammar extraction process similar to ours,
and describes some techniques for automati-
cally ltering out invalid elementary trees.
Our work has a great deal in common
with independent work by Chen and Vijay-
Shanker (2000). They present a more detailed
discussion of various grammar extraction pro-
cesses and the performance of supertagging
models (B. Srinivas, 1997) based on the ex-
tracted grammars. They do not report parsing
results, though their intention is to evaluate
how the various grammars aect parsing ac-
curacy and how k-best supertagging afects
parsing speed.
Srinivas's work on supertags (B. Srinivas,
1997) also uses TAG for statistical parsing,
but with a rather dierent strategy: tree tem-
plates are thought of as extended parts-of-
speech, and these are assigned to words based
on local (e.g., n-gram) context.
As for future work, there are still possibili-
ties made available by TAG which remain to
be explored. One, also suggested by (Chen
and Vijay-Shanker, 2000), is to group elemen-
tary trees into families and relate the trees of
a family by transformations. For example, one
would imagine that the distribution of active
verbs and their subjects would be similar to
the distribution of passive verbs and their no-
tional subjects, yet they are treated as inde-
pendent in the current model. If the two con-
gurations could be related, then the sparse-
ness of verb-argument dependencies would be
reduced.
Another possibility is the use of multiply-
anchored trees. Nothing about PTAG requires
that elementary trees have only a single an-
chor (or any anchor at all), so multiply-
anchored trees could be used to make, for
example, the attachment of a PP dependent
not only on the preposition (as in the cur-
rent model) but the lexical head of the prepo-
sitional object as well, or the attachment of
a relative clause dependent on the embed-
ded verb as well as the relative pronoun. The
smoothing method described above would
have to be modied to account for multiple
anchors.
In summary, we have argued that TAG pro-
vides a cleaner way of looking at statisti-
cal parsing than lexicalized PCFG does, and
demonstrated that in practice it performs in
the same range. Moreover, the greater ex-
ibility of TAG suggests some potential im-
provements which would be cumbersome to
implement using a lexicalized CFG. Further
research will show whether these advantages
turn out to be signicant in practice.
Acknowledgements
This research is supported in part by ARO
grant DAAG55971-0228 and NSF grant SBR-
89-20230-15. Thanks to Mike Collins, Aravind
Joshi, and the anonymous reviewers for their
valuable help. S. D. G.
References
B. Srinivas. 1997. Complexity of lexical descrip-
tions: relevance to partial parsing. Ph.D. thesis,
Univ. of Pennsylvania.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-nder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing (ANLP 1997), pages 194{
201.
John Carroll and David Weir. 1997. Encoding
frequency information in lexicalized grammars.
In Proceedings of the Fifth International Work-
shop on Parsing Technologies (IWPT '97),
pages 8{17.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First
Meeting of the North American Chapter of
the Association for Computational Linguistics
(ANLP-NAACL2000), pages 132{139.
Ciprian Chelba and Frederick Jelinek. 1998. Ex-
ploiting syntactic structure for language model-
ing. In Proceedings of COLING-ACL '98, pages
225{231.
John Chen and K. Vijay-Shanker. 2000. Au-
tomated extraction of TAGs from the Penn
Treebank. In Proceedings of the Sixth In-
ternational Workshop on Parsing Technologies
(IWPT 2000), pages 65{76.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Pro-
ceedings of the 34th Annual Meeting of the As-
socation for Computational Linguistics, pages
184{191.
Michael Collins. 1997. Three generative lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the As-
socation for Computational Linguistics, pages
16{23.
Michael Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. the-
sis, Univ. of Pennsylvania.
Joshua Goodman. 1997. Global thresholding
and multiple-pass parsing. In Proceedings of
the Second Conference on Empirical Methods
in Natural Language Processing (EMNLP-2),
pages 11{25.
Rebecca Hwa. 1998. An empirical evaluation
of probabilistic lexicalized tree insertion gram-
mars. In Proceedings of COLING-ACL '98,
pages 557{563.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguis-
tics, 24:613{632.
David M. Magerman. 1995. Statistical decision-
tree models for parsing. In Proceedings of
the 33rd Annual Meeting of the Assocation for
Computational Linguistics, pages 276{283.
Gunter Neumann. 1998. Automatic extraction
of stochastic lexicalized tree grammars from
treebanks. In Proceedings of the 4th Inter-
national Workshop on TAG and Related For-
malisms (TAG+4), pages 120{123.
Owen Rambow, K. Vijay-Shanker, and David
Weir. 1995. D-tree grammars. In Proceedings
of the 33rd Annual Meeting of the Assocation
for Computational Linguistics, pages 151{158.
Adwait Ratnaparkhi. 1996. A maximum-entropy
model for part-of-speech tagging. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, pages 1{10.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical natu-
ral language processing. In Proceedings of the
Fourteenth International Conference on Com-
putational Linguistics (COLING-92), pages
418{424.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining deriva-
tion. Computational Linguistics, 20(1):91{124.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: a cubic-time parsable for-
malism that lexicalizes context-free grammar
without changing the trees produced. Compu-
tational Linguistics, 21:479{513.
Yves Schabes and RichardWaters. 1996. Stochas-
tic lexicalized tree-insertion grammar. In
H. Bunt and M. Tomita, editors, Recent Ad-
vances in Parsing Technology, pages 281{294.
Kluwer Academic Press, London.
Yves Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proceedings of the
Fourteenth International Conference on Com-
putational Linguistics (COLING-92), pages
426{432.
Fei Xia. 1999. Extracting tree adjoining gram-
mars from bracketed corpora. In Proceedings
of the 5th Natural Language Processing Pacic
Rim Symposium (NLPRS-99), pages 398{403.
Constraints on strong generative power
David Chiang
University of Pennsylvania
Dept of Computer and Information Science
200 S 33rd St
Philadelphia, PA 19104 USA
dchiang@cis.upenn.edu
Abstract
We consider the question ?How
much strong generative power can
be squeezed out of a formal system
without increasing its weak generative
power?? and propose some theoret-
ical and practical constraints on this
problem. We then introduce a formal-
ism which, under these constraints,
maximally squeezes strong generative
power out of context-free grammar.
Finally, we generalize this result to
formalisms beyond CFG.
1 Introduction
?How much strong generative power can be
squeezed out of a formal system without increas-
ing its weak generative power?? This question,
posed by Joshi (2000), is important for both lin-
guistic description and natural language process-
ing. The extension of tree adjoining grammar
(TAG) to tree-local multicomponent TAG (Joshi,
1987), or the extension of context free gram-
mar (CFG) to tree insertion grammar (Schabes
and Waters, 1993) or regular form TAG (Rogers,
1994) can be seen as steps toward answering this
question. But this question is difficult to answer
with much finality unless we pin its terms down
more precisely.
First, what is meant by strong generative
power? In the standard definition (Chomsky,
1965) a grammar G weakly generates a set of
sentences L(G) and strongly generates a set of
structural descriptions ?(G); the strong genera-
tive capacity of a formalism F is then f?(G) j
F provides Gg. There is some vagueness in the
literature, however, over what structural descrip-
tions are and how they can reasonably be com-
pared across theories (Miller (1999) gives a good
synopsis).
(a) X

X
X a
X
X b
(b) XNA
a X
b X a
b
Figure 1: Example of weakly context-free TAG.
The approach that Vijay-Shanker et al (1987)
and Weir (1988) take, elaborated on by Becker
et al (1992), is to identify a very general class
of formalisms, which they call linear context-
free rewriting systems (CFRSs), and define for
this class a large space of structural descriptions
which serves as a common ground in which the
strong generative capacities of these formalisms
can be compared. Similarly, if we want to talk
about squeezing strong generative power out of
a formal system, we need to do so in the context
of some larger space of structural descriptions.
Second, why is preservation of weak generative
power important? If we interpret this constraint to
the letter, it is almost vacuous. For example, the
class of all tree adjoining grammars which gen-
erate context-free languages includes the gram-
mar shown in Figure 1a (which generates the lan-
guage fa, bg). We can also add the tree shown in
Figure 1b without increasing the grammar?s weak
generative capacity; indeed, we can add any trees
we please, provided they yield only as and bs. In-
tuitively, the constraint of weak context-freeness
has little force.
This intuition is verified if we consider that
weak context-freeness is desirable for computa-
tional efficiency. Though a weakly context-free
TAG might be recognizable in cubic time (if we
know the equivalent CFG), it need not be parsable
in cubic time?that is, given a string, to compute
all its possible structural descriptions will take
O(n6) time in general. If we are interested in com-
puting structural descriptions from strings, then
Derivations Structuraldescriptions
Sentences
Figure 2: Simulation: structural descriptions as
derived structures.
we need a tighter constraint than preservation of
weak generative power.
In Section 3 below we examine some restric-
tions on tree adjoining grammar which are weakly
context-free, and observe that their parsers all
work in the same way: though given a TAG G,
they implicitly parse using a CFG G0 which de-
rives the same strings as G, but also their corre-
sponding structural descriptions under G, in such
a way that preserves the dynamic-programming
structure of the parsing algorithm.
Based on this observation, we replace the con-
straint of preservation of weak generative power
with a constraint of simulability: essentially, a
grammar G0 simulates another grammar G if it
generates the same strings that G does, as well as
their corresponding structural descriptions under
G (see Figure 2).
So then, within the class of context-free rewrit-
ing systems, how does this constraint of simu-
lability limit strong generative power? In Sec-
tion 4.1 we define a formalism called multicom-
ponent multifoot TAG (MMTAG) which, when
restricted to a regular form, characterizes pre-
cisely those CFRSs which are simulable by a
CFG. Thus, in the sense we have set forth, this
formalism can be said to squeeze as much strong
generative power out of CFG as is possible. Fi-
nally, we generalize this result to formalisms be-
yond CFG.
2 Characterizing structural descriptions
First we define context-free rewriting systems.
What these formalisms have in common is that
their derivation sets are all local sets (that is, gen-
erable by a CFG). These derivations are taken as
structural descriptions. The following definitions
are adapted from Weir (1988).
Definition 1 A generalized context-free gram-
mar G is a tuple hV, S , F, Pi, where
1. V is a finite set of variables,
2. S 2 V is a distinguished start symbol,
3. F is a finite set of function symbols, and
X
Y

XNA
a X d
YNA
b Y c
S ! ?(X, Y) ?(hx1, x2i, hy1, y2i) = x1y1y2x2
X ! ?1(X) ?1(hx1, x2i) = hax1, x2di
X ! () () = h, i
Y ! ?2(Y) ?2(hy1, y2i) = hby1, y2ci
Y ! () () = h, i
Figure 3: Example of TAG with corresponding
GCFG and interpretation. Here adjunction at foot
nodes is allowed.
4. P is a finite set of productions of the form
A! f (A1, . . . , An)
where n  0, f 2 F, and A, Ai 2 V .
A generalized CFG G generates a set T (G) of
terms, which are interpreted as derivations under
some formalism. In this paper we require that G
be free of spurious ambiguity, that is, that each
term be uniquely generated.
Definition 2 We say that a formalism F is a
context-free rewriting system (CFRS) if its deriva-
tion sets can be characterized by generalized
CFGs, and its derived structures are produced by
a function ~F from terms to strings such that for
each function symbol f , there is a yield function
fF such that
~ f (t1, . . . , tn)F = fF (~t1F , . . . , ~tnF )
(A linear CFRS is subject to further restrictions,
which we do not make use of.)
As an example, Figure 3 shows a simple TAG
with a corresponding GCFG and interpretation.
A nice property of CFRS is that any formal-
ism which can be defined as a CFRS immedi-
ately lends itself to several extensions, which arise
when we give additional interpretations to the
function symbols. For example, we can interpret
the functions as ranging over probabilities, cre-
ating a stochastic grammar; or we can interpret
them as yield functions of another grammar, cre-
ating a synchronous grammar.
Now we define strong generative capacity as
the relationship between strings and structural de-
scriptions.1
1This is similar in spirit, but not the same as, the notion
of derivational generative capacity (Becker et al, 1992).
Definition 3 The strong generative capacity of a
grammar G a CFRS F is the relation
fh~tF , ti j t 2 T (G)g.
For example, the strong generative capacity of the
grammar of Figure 3 is
fhambncndm, ?(?m1 (()), ?n2(()))ig
whereas any equivalent CFG must have a strong
generative capacity of the form
fhambncndm, f m(gn(e()))ig
That is, in a CFG the n bs and cs must appear later
in the derivation than the m as and ds, whereas in
our example they appear in parallel.
3 Simulating structural descriptions
We now take a closer look at some examples of
?squeezed? context-free formalisms to illustrate
how a CFG can be used to simulate formalisms
with greater strong generative power than CFG.
3.1 Motivation
Tree substitution grammar (TSG), tree insertion
grammar (TIG), and regular-form TAG (RF-TAG)
are all weakly context free formalisms which can
additionally be parsed in cubic time (with a caveat
for RF-TAG below). For each of these formalisms
a CKY-style parser can be written whose items are
of the form [X, i, j] and are combined in various
ways, but always according to the schema
[X, i, j] [Y, j, k]
[Z, i, k]
just as in the CKY parser for CFG. In effect the
parser dynamically converts the TSG, TIG, or RF-
TAG into an equivalent CFG?each parser rule of
the above form corresponds to the rule schema
Z ! XY .
More importantly, given a grammar G and a
string w, a parser can reconstruct all possible
derivations of w under G by storing inside each
chart item how that item was inferred. If we think
of the parser as dynamically converting G into a
CFG G0, then this CFG is likewise able to com-
positionally reconstruct TSG, TIG, or RF-TAG
derivations?we say that G0 simulates G.
Note that the parser specifies how to convert G
into G0, but G0 is not itself a parser. Thus these
three formalisms have a special relationship to
CFG that is independent of any particular pars-
ing algorithm: for any TSG, TIG, or RF-TAG G,
there is a CFG that simulates G. We make this no-
tion more precise below.
3.2 Excursus: regular form TAG
Strictly speaking, the recognition algorithm
Rogers gives cannot be extended to parsing; that
is, it generates all possible derived trees for a
given string, but not all possible derivations. It
is correct, however, as a parser for a further re-
stricted subclass of TAGs:
Definition 4 We say that a TAG is in strict reg-
ular form if there exists some partial ordering 
over the nonterminal alphabet such that for ev-
ery auxiliary tree ?, if the root and foot of ? are
labeled X, then for every node ? along ??s spine
where adjunction is allowed, X  label(?), and
X = label(?) only if ? is a foot node. (In this vari-
ant adjunction at foot nodes is permitted.)
Thus the only kinds of adjunction which can oc-
cur to unbounded depth are off-spine adjunction
and adjunction at foot nodes.
This stricter definition still has greater strong
generative capacity than CFG. For example, the
TAG in Figure 3 is in strict regular form, because
the only nodes along spines where adjunction is
allowed are foot nodes.
3.3 Simulability
So far we have not placed any restrictions on
how these structural descriptions are computed.
Even though we might imagine attaching arbi-
trary functions to the rules of a parser, an algo-
rithm like CKY is only really capable of com-
puting values of bounded size, or else structure-
sharing in the chart will be lost, increasing the
complexity of the algorithm possibly to exponen-
tial complexity.
For a parser to compute arbitrary-sized objects,
such as the derivations themselves, it must use
back-pointers, references to the values of sub-
computations but not the values themselves. The
only functions on a back-pointer the parser can
compute online are the identity function (by copy-
ing the back-pointer) and constant functions (by
replacing the back-pointer); any other function
would have to dereference the back-pointer and
destroy the structure of the algorithm. Therefore
such functions must be computed offline.
Definition 5 A simulating interpretation ~ is a
bijection between two recognizable sets of terms
such that
1. For each function symbol ?, there is a func-
tion ?? such that
~?(t1, . . . , tn) = ??(~t1, . . . , ~tn)
2. Each ?? is definable as:
?
?(hx11, . . . , x1m1)i), . . . , hxn1, . . . , xmnmi) =
hw1, . . . ,wmi
where each wi can take one of the following
forms:
(a) a variable xi j, or
(b) a function application f (xi1 j1 , . . . xin jn),
n  0
3. Furthermore, we require that for any recog-
nizable set T , ~T is also a recognizable set.
We say that ~ is trivial if every ?? is definable as
?
?(x1, . . . xn) = f (xpi(1), . . . xpi(n))
where pi is a permutation of f1, . . . , ng.2
The rationale for requirement (3) is that it
should not be possible, simply by imposing local
constraints on the simulating grammar, to produce
a simulated grammar which does not even come
from a CFRS.3
Definition 6 We say that a grammar G from a
CFRS F is (trivially) simulable by a grammar G?
from another CFRS F if there is a (trivial) simu-
lating interpretation ~s : T (G0) ! T (G) which
satisfies ~tF 0 = ~~tsF for all t 2 T (G0).
As an example, a CFG which simulates the
TAG of Figure 3 is shown in Figure 4. Note that
if we give additional interpretations to the simu-
lated yield functions ?, ?1, and ?2, this CFG can
compute any probabilities, translations, etc., that
the original TAG can.
Note that if G0 trivially simulates G, they are
very nearly strongly equivalent, except that the
yield functions of G0 might take their arguments
in a different order thanG, and there might be sev-
eral yield functions of G0 which correspond to a
single yield function ofG used in several different
contexts. In fact, for technical reasons we will use
this notion instead of strong equivalence for test-
ing the strong generative power of a formal sys-
tem.
Thus the original problem, which was, given
a formalism F , to find a formalism that has as
much strong generative power as possible but re-
mains weakly equivalent to F , is now recast as
2Simulating interpretations and trivial simulating inter-
pretations are similar to the generalized and ?ungeneralized?
syntax-directed translations, respectively, of Aho and Ull-
man (1969; 1971).
3Without this requirement, there are certain pathological
cases that cause the construction of Section 4.2 to produce
infinite MM-TAGs.
S ! ?0 ?(x1, x2)   hx1, x2i
?
0 ! ?0 h(), x2i    h?, x2i
?
0
 ! ?1
 h?, x2i    h?, x2i
?
1 ! ?1 h?, ()i    h?,?i
?
1
 !  h?,?i    h?,?i
?
0 ! ?01[?0] h?1(x1), x2i    hx1, x2i
?
0
1[?0]! a ?21[?0] d hx1, x2i    hx1, x2i
?
2
1[?0]! ?01[?0] h?1(x1), x2i    hx1, x2i
?
2
1[?0]! ?0 h(), x2i    h?, x2i
?
1 ! ?02[?1] h?, ?2(x2)i    h?, x2i
?
0
2[?1]! b ?22[?1] c h?, x2i    h?, x2i
?
2
2[?1]! ?12[?1] h?, ?2(x2)i    h?, x2i
?
2
2[?1]! ?1 h?, ()i    h?,?i
Figure 4: CFG which simulates the grammar
of Figure 3. Here we leave the yield functions
anonymous; y    x denotes the function which
maps x to y.
the following problem: find a formalism that triv-
ially simulates as many grammars as possible but
remains simulable by F .
3.4 Results
The following is easy to show:
Proposition 1 Simulability is reflexive and tran-
sitive.
Because of transitivity, it is impossible that a for-
malism which is simulable by F could simulate
a grammar that is not simulable by F . So we are
looking for a formalism that can trivially simulate
exactly those grammars that F can.
In Section 4.1 we define a formalism called
multicomponent multifoot TAG (MMTAG), and
then in Section 4.2 we prove the following result:
Proposition 2 A grammar G from a CFRS is
simulable by a CFG if and only if it is trivially
simulable by an MMTAG in regular form.
The ?if? direction (() implies (because simu-
lability is reflexive) that RF-MMTAG is simula-
ble by a CFG, and therefore cubic-time parsable.
(The proof below does give an effective proce-
dure for constructing a simulating CFG for any
RF-MMTAG.) The ?only if? direction ()) shows
that, in the sense we have defined, RF-MMTAG
is the most powerful such formalism.
We can generalize this result using the notion
of a meta-level grammar (Dras, 1999).
Definition 7 If F1 and F2 are two CFRSs, F2 
F1 is the CFRS characterized by the interpretation
function ~F2F1 = ~F2  ~F1 .
F1 is the meta-level formalism, which generates
derivations for F2. Obviously F1 must be a tree-
rewriting system.
Proposition 3 For any CFRS F 0, a grammar G
from a (possibly different) CFRS is simulable by
a grammar in F 0 if and only if it is trivially simu-
lable by a grammar in F 0  RF-MMTAG.
The ?only if? direction ()) follows from the
fact that the MMTAG constructed in the proof of
Proposition 2 generates the same derived trees as
the CFG. The ?if? direction (() is a little trickier
because the constructed CFG inserts and relabels
nodes.
4 Multicomponent multifoot TAG
4.1 Definitions
MMTAG resembles a cross between set-local
multicomponent TAG (Joshi, 1987) and ranked
node rewriting grammar (Abe, 1988), a variant of
TAG in which auxiliary trees may have multiple
foot nodes. It also has much in common with d-
tree substitution grammar (Rambow et al, 1995).
Definition 8 An elementary tree set ~? is a finite
set of trees (called the components of ~?) with the
following properties:
1. Zero or more frontier nodes are designated
foot nodes, which lack labels (following
Abe), but are marked with the diacritic ;
2. Zero or more (non-foot) nodes are desig-
nated adjunction nodes, which are parti-
tioned into one or more disjoint sets called
adjunction sites. We notate this by assigning
an index i to each adjunction site and mark-
ing each node of site i with the diacritic i .
3. Each component is associated with a sym-
bol called its type. This is analogous to the
left-hand side of a CFG rule (again, follow-
ing Abe).
4. The components of ~? are connected by d-
edges from foot nodes to root nodes (notated
by dotted lines) to form a single tree struc-
ture. A single foot node may have multiple
d-children, and their order is significant. (See
Figure 5 for an example.)
A multicomponent multifoot tree adjoining gram-
mar is a tuple h?, P, S i, where:
A
 X 1
Y 2 
X 1

X 1

A
 X 1
Y 3 
X 1

X 1

{
A
 A
Y 3 X 1
Y 2 
X 1

X 1

Figure 5: Example of MMTAG adjunction. The
types of the components, not shown in the figure,
are all X.
1. ? is a finite alphabet;
2. P is a finite set of tree sets; and
3. S 2 ? is a distinguished start symbol.
Definition 9 A component ? is adjoinable at a
node ? if ? is an adjunction node and the type of
? equals the label of ?.
The result of adjoining a component ? at a node
? is the tree set formed by separating ? from its
children, replacing ? with the root of ?, and re-
placing the ith foot node of ? with the ith child
of ?. (Thus adjunction of a one-foot component
is analogous to TAG adjunction, and adjunction
of a zero-foot component is analogous to substi-
tution.)
A tree set ~? is adjoinable at an adjunction site
~? if there is a way to adjoin each component of ~?
at a different node of ~? (with no nodes left over)
such that the dominance and precedence relations
within ~? are preserved. (See Figure 5 for an ex-
ample.)
We now define a regular form for MMTAG that
is analogous to strict regular form for TAG. A
spine is the path from the root to a foot of a sin-
gle component. Whenever adjunction takes place,
several spines are inserted inside or concatenated
with other spines. To ensure that unbounded in-
sertion does not take place, we impose an order-
ing on spines, by means of functions ?i that map
the type of a component to the rank of that com-
ponent?s ith spine.
Definition 10 We say that an adjunction node ? 2
~? is safe in a spine if it is the lowest node (except
the foot) in that spine, and if each component un-
der that spine consists only of a member of ~? and
zero or more foot nodes.
We say that an MMTAGG is in regular form if
there are functions ?i from ? into the domain of
some partial ordering  such that for each com-
ponent ? of type X, for each adjunction node
? 2 ?, if the jth child of ? dominates the ith foot
node of ? (that is, another component?s jth spine
would adjoin into the ith spine), then ?i(X) 
? j(label(?)), and ?i(X) = ? j(label(?)) only if ?
is safe in the ith spine.
Thus the only kinds of adjunction which can oc-
cur to unbounded depth are off-spine adjunction
and safe adjunction. The adjunction shown in Fig-
ure 5 is an example of safe adjunction.
4.2 Proof of Proposition 2
(() First we describe how to construct a simu-
lating CFG for any RF-MMTAG; then this direc-
tion of the proof follows from the transitivity of
simulability.
When a CFG simulates a regular form TAG,
each nonterminal must encapsulate a stack (of
bounded depth) to keep track of adjunctions. In
the multicomponent case, these stacks must be
generalized to trees (again, of bounded size).
So the nonterminals ofG0 are of the form [?, t],
where t is a derivation fragment of G with a dot
() at exactly one node ~?, and ? is a node of ~?. Let
?? be the node in the derived tree where ? ends up.
A fragment t can be put into a normal form as
follows:
1. For every ~? above the dot, if ?? does not lie
along a spine of ~?, delete everything above
~?.
2. For every ~? not above or at the dot, if ?? does
not lie along a d-edge of ~?, delete ~? and
everything below and replace it with > if ??
dominates ~?; otherwise replace it with ?.
3. If there are two nodes ~?1 and ~?2 along a
path which name the same tree set and ?? lies
along the same spine or same d-edge in both
of them, collapse ~?1 and ~?2, deleting every-
thing in between.
Basically this process removes all unboundedly
long paths, so that the set of normal forms is finite.
In the rule schemata below, the terms in the left-
hand sides range over normalized terms, and their
corresponding right-hand sides are renormalized.
Let up(t) denote the tree that results from moving
the dot in t up one step.
The value of a subderivation t0 of G0 under ~s
is a tuple of partial derivations of G, one for each
> symbol in the root label of t0, in order. Where
we do not define a yield function for a production
below, the identity function is understood.
For every set ~? with a single, S -type compo-
nent rooted by ?, add the rule
S ! [?, ~?(>, . . . ,>)]
~?(x1, . . . , xn)   hx1, . . . , xni
For every non-adjunction, non-foot node ? with
children ?1, . . . , ?n (n  0),
[?, t]! [?1, t]    [?n, t]
For every component with root ?0 that is adjoin-
able at ?,
[?, up(t)]! [?0, t]
If ?0 is the root of the whole set ~?0, this rule
rewrites a > to several > symbols; the corre-
sponding yield function is then
h. . . , ~?0(x1, . . . , xn), . . .i    h. . . , x1, . . . , xn, . . .i
For every component with ith foot ?0i that is ad-joinable at a node with ith child ?i,
[?0i , t]! [?i, up(t)]
This last rule skips over deleted parts of the
derivation tree, but this is harmless in a regular
form MMTAG, because all the skipped adjunc-
tions are safe.
()) First we describe how to decompose any
given derivation t0 of G0 into a set of elementary
tree sets.
Let t = ~t0s. (Note the convention that primed
variables always pertain to the simulating gram-
mar, unprimed variables to the simulated gram-
mar.) If, during the computation of t, a node ?0
creates the node ?, we say that ?0 is productive
and produces ?. Without loss of generality, let us
assume that there is a one-to-one correspondence
between productive nodes and nodes of t.4
To start, let ? be the root of t, and ?1, . . . , ?n its
children.
Define the domain of ?i as follows: any node
in t0 that produces ?i or any of its descendants is
in the domain of ?i, and any non-productive node
whose parent is in the domain of ?i is also in the
domain of ?i.
For each ?i, excise each connected component
of the domain of ?i. This operation is the reverse
of adjunction (see Figure 6): each component gets
4If G0 does not have this property, it can be modified
so that it does. This may change the derived trees slightly,
which makes the proof of Proposition 3 trickier.
 ?
 ?1

a  
 

d
{
Q1 :  ?1

a  

d
 ?
Q1 1
 

Figure 6: Example derivation (left) of the gram-
mar of Figure 4, and first step of decomposition.
Non-adjunction nodes are shown with the place-
holder  (because the yield functions in the origi-
nal grammar were anonymous), the Greek letters
indicating what is produced by each node. Ad-
junction nodes are shown with labels Qi in place
of the (very long) true labels.
S : 
Q1 1
Q2 2

Q1 : 

a Q1 1

d
Q1 : 

Q2 : 

b Q2 2 c
Q2 : 

Figure 7: MMTAG converted from CFG of Fig-
ure 4 (cf. the original TAG in Figure 3). Each
components? type is written to its left.
foot nodes to replace its lost children, and the
components are connected by d-edges according
to their original configuration.
Meanwhile an adjunction node is created in
place of each component. This node is given a la-
bel (which also becomes the type of the excised
component) whose job is to make sure the final
grammar does not overgenerate; we describe how
the label is chosen below. The adjunction nodes
are partitioned such that the ith site contains all
the adjunction nodes created when removing ?i.
The tree set that is left behind is the elementary
tree set corresponding to ? (rather, the function
symbol that labels ?); this process is repeated re-
cursively on the children of ?, if any.
Thus any derivation of G0 can be decomposed
into elementary tree sets. Let ?G be the union of
the decompositions of all possible derivations of
G0 (see Figure 7 for an example).
Labeling adjunction nodes For any node ?0,
and any list of nodes h?01, . . . , ?
0
ni, let the sig-
nature of ?0 with respect to h?01, . . . , ?
0
ni be
hA, a1, . . . , ami, where A is the left-hand side of
the GCFG production that generated ?0, and ai =
h j, ki if ?0 gets its ith field from the kth field of
?
0
j, or  if ?
0 produces a function symbol in its ith
field.
So when we excise the domain of ?i, the la-
bel of the node left behind by a component ? is
hs, s1, . . . , sni, where s is the signature of the root
of ? with respect to the foot nodes and s1, . . . , sn
are the signatures of the foot nodes with respect to
their d-children. Note that the number of possible
adjunction labels is finite, though large.
?G trivially simulates G. Since each tree of ?G
corresponds to a function symbol (though not
necessarily one-to-one), it is easy to write a triv-
ial simulating interpretation ~ : T ( ?G) ! T (G).
To see that ?G does not overgenerate, observe that
the nonterminal labels inside the signatures en-
sure that every derivation of ?G corresponds to a
valid derivation ofG0, and therefore G. To see that
~ is one-to-one, observe that the adjunction la-
bels keep track of how G0 constructed its simu-
lated derivations, ensuring that for any derivation
t? of ?G, the decomposition of the derived tree of t?
is t? itself. Therefore two derivations of ?G cannot
correspond to the same derivation of G0, nor of G.
?G is finite. Briefly, suppose that the number of
components per tree set is unbounded. Then it is
possible, by intersecting G0 with a recognizable
set, to obtain a grammar whose simulated deriva-
tion set is non-recognizable. The idea is that mul-
ticomponent tree sets give rise to dependent paths
in the derivation set, so if there is no bound on
the number of components in a tree set, neither is
there a bound on the length of dependent paths.
This contradicts the requirement that a simulating
interpretation map recognizable sets to recogniz-
able sets.
Suppose that the number of nodes per compo-
nent is unbounded. If the number of components
per tree set is bounded, so must the number of ad-
junction nodes per component; then it is possible,
again by intersecting G0 with a recognizable set,
to obtain a grammar which is infinitely ambigu-
ous with respect to simulated derivations, which
contradicts the requirement that simulating inter-
pretations be bijective.
?G is in regular form. A component of ?G corre-
sponds to a derivation fragment ofG0 which takes
fields from several subderivations and processes
them, combining some into a larger structure and
copying some straight through to the root. Let
?i(X) be the number of fields that a component
of type X copies from its ith foot up to its root.
This information is encoded in X, in the signa-
ture of the root. Then ?G satisfies the regular form
constraint, because when adjunction inserts one
spine into another spine, the the inserted spine
must copy at least as many fields as the outer
one. Furthermore, if the adjunction site is not safe,
then the inserted spine must additionally copy the
value produced by some lower node.
5 Discussion
We have proposed a more constrained version of
Joshi?s question, ?How much strong generative
power can be squeezed out of a formal system
without increasing its weak generative power,?
and shown that within these constraints, a vari-
ant of TAG called MMTAG characterizes the limit
of how much strong generative power can be
squeezed out of CFG. Moreover, using the notion
of a meta-level grammar, this result is extended to
formalisms beyond CFG.
It remains to be seen whether RF-MMTAG,
whether used directly or for specifying meta-level
grammars, provides further practical benefits on
top of existing ?squeezed? grammar formalisms
like tree-local MCTAG, tree insertion grammar,
or regular form TAG.
This way of approaching Joshi?s question is by
no means the only way, but we hope that this work
will contribute to a better understanding of the
strong generative capacity of constrained gram-
mar formalisms as well as reveal more powerful
formalisms for linguistic analysis and natural lan-
guage processing.
Acknowledgments
This research is supported in part by NSF
grant SBR-89-20230-15. Thanks to Mark Dras,
William Schuler, Anoop Sarkar, Aravind Joshi,
and the anonymous reviewers for their valuable
help. S. D. G.
References
Naoki Abe. 1988. Feasible learnability of formal
grammars and the theory of natural language ac-
quisition. In Proceedings of the Twelfth Inter-
national Conference on Computational Linguistics
(COLING-88), pages 1?6, Budapest.
A. V. Aho and J. D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comp.
Sys. Sci, 3:37?56.
A. V. Aho and J. D. Ullman. 1971. Translations on
a context free grammar. Information and Control,
19:439?475.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The derivational generative power of formal
systems, or, Scrambling is beyond LCFRS. Tech-
nical Report IRCS-92-38, Institute for Research in
Cognitive Science, University of Pennsylvania.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge, MA.
Mark Dras. 1999. A meta-level grammar: redefining
synchronous TAG for translation and paraphrase.
In Proceedings of the 37th Annual Meeting of the
Assocation for Computational Linguistics, pages
80?87, College Park, MD.
Aravind K. Joshi. 1987. An introduction to tree ad-
joining grammars. In Alexis Manaster-Ramer, ed-
itor, Mathematics of Language. John Benjamins,
Amsterdam.
Aravind K. Joshi. 2000. Relationship between strong
and weak generative power of formal systems. In
Proceedings of the Fifth International Workshop on
TAG and Related Formalisms (TAG+5), pages 107?
113.
Philip H. Miller. 1999. Strong Generative Capacity:
The Semantics of Linguistic Formalism. Number
103 in CSLI lecture notes. CSLI Publications, Stan-
ford.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-tree grammars. In Proceedings of the
33rd Annual Meeting of the Assocation for Com-
putational Linguistics, pages 151?158, Cambridge,
MA.
James Rogers. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd An-
nual Meeting of the Assocation for Computational
Linguistics, pages 155?162, Las Cruces, NM.
Yves Schabes and Richard C. Waters. 1993. Lexical-
ized context-free grammars. In Proceedings of the
31st Annual Meeting of the Association for Com-
putational Linguistics, pages 121?129, Columbus,
OH.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th AnnualMeeting of the Associa-
tion for Computational Linguistics, pages 104?111,
Stanford, CA.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univ.
of Pennsylvania.
Two Statistical Parsing Models Applied to the Chinese Treebank 
Daniel M. Bikel David Chiang 
Department of Computer & Information Science 
University of Pennsylvania 
200 South 33rd Street 
Philadelphia, PA 19104-6389 
(dbikel, dchiang)?cis, upenn, edu 
Abst ract  
This paper presents the first-ever 
results of applying statistical pars- 
ing models to the newly-available 
Chinese Treebank. We have em- 
ployed two models, one extracted 
and adapted from BBN's SIFT Sys- 
tem (Miller et al, 1998) and a TAG- 
based parsing model, adapted from 
(Chiang, 2000). On sentences with 
<40 words, the former model per- 
forms at 69% precision, 75% recall, 
and the latter at 77% precision and 
78% recall. 
1 In t roduct ion  
Ever since the success of HMMs' applica- 
tion to part-of-speech tagging in (Church, 
1988), machine learning approaches to nat- 
ural language processing have steadily be- 
come more widespread. This increase has of 
course been due to their proven efficacy in 
many tasks, but also to their engineering effi- 
Cacy. Many machine learning approaches let 
the data speak for itself (data ipsa loquun- 
tur), as it were, allowing the modeler to focus 
on what features of the data are important, 
rather than on the complicated interaction 
of such features, as had often been the case 
with hand-crafted NLP systems. The success 
of statistical methods in particular has been 
quite evident in the area of syntactic pars- 
ing, most recently with the outstanding re- 
sults of (Charniak, 2000) and (Colhns, 2000) 
on the now-standard English test set of the 
Penn Treebank (Marcus et al, 1993). A sig- 
nificant trend in parsing models has been the 
incorporation of linguistically-motivated f a- 
tures; however, it is important o note that 
"linguistically-motivated" does not necessarily 
mean "language-dependent"---often, it means 
just the opposite. For example, almost all sta- 
tistical parsers make use of lexicalized non- 
terminals in some way, which allows lexical 
items' indiosyncratic parsing preferences to 
be modeled, but the paring between head 
words and their parent nonterminals i  deter- 
mined almost entirely by the training data, 
thereby making this feature--which models 
preferences of particular words of a par- 
ticular language---almost entirely language- 
independent. In this paper, we will explore the 
use of two parsing models, which were origi- 
nally designed for English parsing, on parsing 
Chinese, using the newly-available Chinese 
Treebank. We will show that the language- 
dependent components of these parsers are 
quite compact, and that with little effort they 
can be adapted to produce promising results 
for Chinese parsing. We also discuss directions 
for future work. 
2 Mode ls  and  Mod i f i ca t ions  
We will briefly describe the two parsing mod- 
els employed (for a full description of the BBN 
model, see (Miller et al, 1998) and also (Bikel, 
2000); for a full description Of the TAG model, 
see (Chiang, 2000)). 
2.1 Model  2 of (Collins, 1997) 
Both parsing models discussed in this paper 
inherit a great deal from this model, so we 
briefly describe its "progenitive" features here, 
describing only how each of the two models of 
this paper differ in the subsequent two sec- 
tions. 
The lexicalized PCFG that sits behind 
Model 2 of (Collins, 1997) has rules of the 
form 
P ~ LnLn-I"'" L IHR I " "  .Rn-IRn (1) 
S(will-MD) 
NP(AppI,~NNP) VP(wilI-MD) 
NNP 
I Apple MD VP (buy-VB) 
VB PRT(out-RP) NP(Microsoft--NNP) 
I \[ I 
buy RP NNP 
I I 
out Microsoft 
Figure 1: A sample sentence with parse tree. 
where P, Li, R/ and H are all lexicalized 
nonterminals, and P inherits its lexical head 
from its distinguished head child, H. In this 
generative model, first P is generated, then 
its head-child H, then each of the left- and 
right-modifying nonterminals are generated 
from the head outward. The modifying non- 
terminals Li and R/are  generated condition- 
ing on P and H, as well as a distance met- 
ric (based on what material intervenes be- 
tween the currently-generated modifying non- 
terminal and H) and an incremental subcat 
frame feature (a multiset containing the com- 
plements of H that have yet to be gener- 
ated on the side of H in which the currently- 
generated nonterminal falls). Note that if the 
modifying nonterminals were generated com- 
pletely independently, the model would be 
very impoverished, but in actuality, by includ- 
ing the distance and subcat frame features, 
the model captures a crucial bit of linguis- 
tic reality, viz., that words often have well- 
defined sets of complements and adjuncts, dis- 
persed with some well-defined istribution in 
the right hand sides of a (context-free) rewrit- 
ing system. 
2.2 BBN Mode l  
2.2.1 Overv iew 
The BBN model is also of the lexicalized 
PCFG variety. In the BB.N model, as with 
Model 2 of (Collins, 1997), modifying non- 
terminals are generated conditioning both on 
the parent P and its head child H. Unlike 
Model 2 of (Collins, 1997), they are also gen- 
erated conditioning on the previously gener- 
ated modifying nonterminal, L/-1 or Pq-1, 
and there is no subcat frame or distance fea- 
ture. While the BBN model does not per- 
form at the level of Model 2 of (Collins, 1997) 
on Wall Street Journal text, it is also less 
language-dependent, eschewing the distance 
metric (which relied on specific features of the 
English Treebank) in favor of the "bigrams on 
nonterminals" model. 
2.2.2 Mode l  Parameters  
This section briefly describes the top-level 
parameters used in the BBN parsing model. 
We use p to denote the unlexicalized nonter- 
minal corresponding to P in (1), and simi- 
larly for li, ri and h. We now present he top- 
level generation probabilities, along with ex- 
amples from Figure 1. For brevity, we omit the 
smoothing details of BBN's model (see (Miller 
et al, 1998) for a complete description); we 
note that all smoothing weights are computed 
via the technique described in (Bikel et al, 
1997). 
The probability of generating p as the 
root label is predicted conditioning on only 
+TOP+,  which is the hidden root of all parse 
trees: 
P (Pl + TOP+) ,  e.g., P(S I + TOP+).  (2) 
The probability of generating a head node h 
with a parent p is 
P(h I P), e.g., P(VP \] S). (3) 
The probability of generating a left-modifier 
li is 
PL(li I Z -i,p, h, wh),, e.g., (4) 
PL(NP \] + BEGIN+, S, VP, will) 
2 
when generating the NP for NP(Apple-NNP), 
and the probability of generating a right mod- 
ifier ri is 
PR(ri i ri-i,p, h, Wh), e.g., (5) 
Pn(NP I PRT, VP, VB, buy) 
when generating the NP for NP(Microsoft- 
NNP). 1 
The probabilities for generating lexical ele- 
ments (part-of-speech tags and words) are as 
follows. The part of speech tag of the head of 
the entire sentence, th, is computed condition- 
ing only on the top-most symbol p:2 
P(th I P)- (6) 
Part of speech tags of modifier constituents, 
tli and tri, are predicted conditioning on the 
modifier constituent li or r/, the tag of the 
head constituent, h, and the word of the head 
constituent, Wh 
P(tl, \[li, th, Wh) and P(tr~ \[ ri, th, Wh). (7) 
The head word of the entire sentence, Wh, is 
predicted conditioning only on the top-most 
symbol p and th: 
P(whlth,p). (8) 
Head words of modifier constituents, w h and 
Wry, are predicted conditioning on all the con- 
text used for predicting parts of speech in (7), 
as well as the parts of speech themsleves 
P(wt, \[ tl,, li, th, Wh) 
and P(wri \[ try, ri, th, Wh). (9) 
The original English model also included a 
word feature to heIp reduce part-of-speech 
ambiguity for unknown words, but this com- 
ponent of the model was removed for Chinese, 
as it was language-dependent. 
The probability of an entire parse tree is 
the product of the probabilities of generat- 
ing all of the elements of that parse tree, 
1The hidden nonterminal +BEGIN+ is used to 
provide a convenient mechanism for determining the 
initial probability of the underlying Markov process 
generating the modifying nonterminals; the hidden 
nonterminal +END+ is used to provide consistency to
the underlying Markov process, i.e., so that he proba- 
bilities of all possible nonterminal sequences sum to 1. 
2This is the one place where we altered the original 
model, as the lexical components of the head of the 
entire sentence were all being estimated incorrectly, 
causing an inconsistency in the model. We corrected 
the estimation of th and Wh in our implementation. 
where an element is either a constituent la- 
bel, a part of speech tag or a word. We obtain 
maximum-likelihood estimates of the param- 
eters of this model using frequencies gathered 
from the training data. 
2.3 TAG Mode l  
The model of (Chiang, 2000) is based 
on stochastic TAG (Resnik, 1992; Schabes, 
1992). In this model a parse tree is built up not 
out of lexicalized phrase-structure rules but by 
tree fragments (called elementary trees) which 
are texicalized in the sense that each fragment 
contains exactly one lexical item (its anchor). 
In the variant of TAG we use, there are 
three kinds of elementary tree: initial, (pred- 
icative) auxiliary, and modifier, and three 
composition operations: substitution, adjunc- 
tion, and sister-adjunction. Figure 2 illus- 
trates all three of these operations, c~i is an 
initial tree which substitutes at the leftmost 
node labeled NP$;/~ is an auxiliary tree which 
adjoins at the node labeled VP. See (Joshi and 
Schabes, 1997) for a more detailed explana- 
tion. 
Sister-adjunction is not a standard TAG op- 
eration, but borrowed from D-Tree Grammar 
(Rainbow et al, 1995). In Figure 2 the modi- 
fier tree V is sister adjoined between the nodes 
labeled VB and NP$. Multiple modifier trees 
can adjoin at the same place, in the spirit of 
(Schabes and Shieber, 1994). 
In stochastic TAG, the probability of gen- 
erating an elementary tree depends on the el- 
ementary tree itself and the elementary tree 
it attaches to. The parameters are as follows: 
= i 
I + P (NONE I = i 
# 
where c~ ranges over initial trees,/~ over aux- 
iliary trees, 3' over modifier trees, and T/over 
nodes. Pi(c~) is the probability of beginning 
a derivation with c~; Ps(o~ I 77) is the prob- 
ability of substituting o~ at 7; Pa(/~ I r/) is 
the probability of adjoining ~ at 7/; finally, 
Pa(NONE I 7) is the probability of nothing 
adjoining at ~/. 
Our variant adds another set of parameters: 
3 
~2 
(O~2) S l ~. 2 
NPJ~ ..... VP S 
, . "  gap 
...... ! ~ 4 
.... ?....... ! i ~.. . ............ NP VP .,._.re d
{ \[ buy \ ": I 
NP  VP  PRT  NP ~ NNP 
I ~ .  I I I MD VP 
NNP MD VP* RP NNP Apple I 
I I I I will 
Apple will out Microsoft VB PitT NP 
I I I 
(oq) (fl) (7) (a3) buy RP NNP 
I I 
out Microsoft 
de.vat .n  tree 
tree 
Figure 2: Grammar and derivation for "Apple will buy out Microsoft." 
~ Psa(T I ~7, i , f )  + Psa(STOP I ~l,i,f) = 1 
This is the probability of sister-adjoining 7 
between the ith and i + lth children of ~ (al- 
lowing for two imaginary children beyond the 
leftmost and rightmost children). Since multi- 
ple modifier trees can adjoin at the same lo- 
cation, Psa(7) is also conditioned on a flag f 
which indicates whether '7 is the first modi- 
fier tree (i.e., the one closest o the head) to 
adjoin at that location. 
For our model we break down these prob- 
abilities further: first the elementary tree is 
generated without its anchor, and then its an- 
chor is generated. See (Chiang, 2000) for more 
details. 
During training each example is broken 
into elementary trees using head rules and 
argument/adjunct rules similar to those of 
(Collins, 1997). The rules are interpreted as 
follows: a head is kept in the same elemen- 
tary tree in its parent, an argument is broken 
off into a separate initial tree, leaving a sub- 
stitution node, and an adjunct is broken off 
into a separate modifier tree. A different rule 
is used for extracting auxiliary trees; see (Chi- 
ang, 2000) for details. Xia (1999) describes a
similar process, and in fact our rules for the 
Xinhua corpus are based on hers. 
2.4 Modif icat ions 
The primary language-dependent component 
that had to be changed in both models was 
the head table, used to determine heads when 
training. We modified the head rules described 
in (Xia, 1999) for the Xinhua corpus and sub- 
stituted these new rules into both models. 
The (Chiang, 2000) model had the following 
additional modifications. 
? The new corpus had to be prepared for 
use with the trainer and parser. Aside 
from technicalities, this involved retrain- 
ing the part-of-speech tagger described in 
(Ratnaparkhi, 1997), which was used for 
tagging unknown words. We also lowered 
the unknown word threshold from 4 to 2 
because the Xinhua corpus was smaller 
than the WSJ corpus. 
? In addition to the change to the head- 
finding rules, we also changed the rules 
for classifying modifiers as arguments or 
adjuncts. In both cases the new rules were 
adapted from (Xia, 1999). 
? For the tests done in this paper, a beam 
width of 10 -4 was used. 
The BBN model had the following additional 
modifications: 
? As with the (Chiang, 2000) model, we 
similarly lowered the unknown word 
threshold of the BBN model from its de- 
fault 5 to 2. 
? The language-dependent word-feature 
was eliminated, causing parts of speech 
for unknown words to be predicted solely 
on the head relations in the model. 
? The default beam size in the probabilis- 
tic CKY parsing algorithm was widened. 
The default beam pruned away chart en- 
tries whose scores were not within a fac- 
tor of e -5 of the top-ranked subtree; this 
4 
Model, test set 
BBN-allt, WSJ-all 
BBN-small-h WSJ-small* 
BBN,  Xinhua:~ 
Chiang-all, WSJ-all 
Chiang-small, WSJ-small 
Ch iang,  X inhua  
BBN-allt, WSJ-all 
BBN-smallI, WSJ-small* 
BBN,  Xinhua:~ 
Chiang-all, WSJ-all 
Chiang-small, WSJ-small 
Ch iang,  X inhua  
<40 words 
LR LP CB 0CB <2CB 
84.7 86.5 1.12 60.6 83.2 
79.0 80.7 1.66 47.0 74.6 
69.0 74.8 2.05 45.0 68.5 
86.9 86.6 1.09 63.2 84.3 
78.9 79.6 1.75 44.8 72.4 
76.8 77.8 1.99 50.8 74.1 
-< 100 words 
LR LP CB 0CB _<2CB 
83.9 85.7 1.31 57.8 80.8 
78.4 80.0 1.92 44.3 71.3 
67.5 73.5 2.87 39.9 61.8 
86.2 85.8 1.29 60.4 81.8 
77.1 78.8 2.00 43.25 70.5 
73.3 74.6 3.03 44.8 66.8 
Table 1: Results for both parsing models on all test sets. Key: LR = labeled recall, LP = labeled 
precision, CB = avg. crossing brackets, 0CB = zero crossing brackets, <2CB = <2 crossing 
brackets. All results are percentages, except for those in the CB column, tUsed larger beam 
settings and lower unknown word threshold than the defaults. *3 of the 400 sentences were not 
parsed due to timeouts and/or pruning problems. :~3 of the 348 sentences did not get parsed due 
to pruning problems, and 2 other sentences had length mismatches ( coring program errors). 
tight limit was changed to e -9. Also, the 
default decoder pruned away all but the 
top 25-ranked chart entries in each cell; 
this limit was expanded to 50. 
3 Exper iments  and  Resu l ts  
The Chinese Treebank consists of 4185 sen- 
tences of Xinhua newswire text. We blindly 
separated this into training, devtest and test 
sets, with a roughly 80/10/10 split, putting 
files 001-270 (3484 sentences, 84,873 words) 
into the training set, 301-325 (353 sentences, 
6776 words) into the development test set and 
reserving 271-300 (348 sentences, 7980 words) 
for testing. See Table 1 for results. 
In order to put the new Chinese Treebank 
results into context with the unmodified (En- 
glish) parsing models, we present results on 
two test sets from the Wall Street Journal: 
WSJ-all, which is the complete Section 23 (the 
de facto standard test set for English pars- 
ing), and WSJ-small, which is the first 400 
sentences of Section 23 and which is roughly 
comparable in size to the Chinese test set. 
Furthermore, when testing on WSJ-small, we 
trained on a subset of our English training 
data roughly equivalent in size to our Chinese 
training set (Sections 02 and 03 of the Penn 
Treebank); we have indicated models trained 
on all English training with "-all", and mod- 
els trained with the reduced English train- 
ing set with "-small". Therefore, by compar- 
ing the WSJ-small results with the Chinese 
results, one can reasonably gauge the perfor- 
mance gap between English parsing on the 
Penn Treebank and Chinese parsing on the 
Chinese Treebank. 
The reader will note that the modified BBN 
model does significantly poorer than (Chiang, 
2000) on Chinese. While more investigation is 
required, we suspect part of the difference may 
be due to the fact that currently, the BBN 
model uses language-specific rules to guess 
part of speech tags for unknown words. 
4 Conc lus ions  and  Future  Work  
There is no question that a great deal of care 
and expertise went into creating the Chinese 
Treebank, and that it is a source of important 
grammatical information that is ufiique to the 
Chinese language. However, there are definite 
similarities between the grammars of English 
and Chinese, especially when viewed through 
the lens of the statistical models we employed 
here. In both languages, the nouns, adjec- 
tives, adverbs, and verbs have preferences for 
certain arguments and adjuncts, and these 
preferences--in spite of the potentially vastly- 
different configurations of these items--are f- 
fectively modeled. As discussed in the intro- 
duction, lexica! items' idiosyncratic parsing 
preferences are modeled by lexicalizing the 
grammar formalism, using a lexicalized PCFG 
in one case and a lexicalized stochastic TAG 
in the other. Linguistically-reasonable inde- 
pendence assumptions are made, such as the 
independence of grammar productions in the 
case of the PCFG model, or the independence 
of the composition operations in the case of 
the LTAG model, and we would argue that 
these assumptions are no less reasonable for 
the Chinese grammar than they are for that 
of English. While results for the two languages 
are far from equal, we believe that further tun- 
ing of the head rules, and analysis of develop- 
ment test set errors will yield significant per- 
formance gains on Chinese to close the gap. 
Finally, we fully expect hat absolute perfor- 
mance will increase greatly as additional high- 
quality Chinese parse data becomes available. 
5 Acknowledgements  
This research Was funded in part by NSF 
grant SBR-89-20230-15. We would greatly 
like to acknowledge the researchers at BBN 
who allowed us to use their model: Ralph 
Weischedel, Scott Miller, Lance Rarnshaw, 
Heidi Fox and Sean Boisen. We would also 
like to thank Mike Collins and our advisors 
Aravind Joshi and Mitch Marcus. 
Re ferences  
Daniel M. Bikel, Richard Schwartz, Ralph 
Weischedel, and Scott Miller. 1997. Nymble: 
A high-performance learning name-finder. In 
Fifth Conference on Applied Natural Language 
Processing, pages 194-201,, Washington, D.C. 
Daniel M. Bikel. 2000. A statistical model for 
parsing and word-sense disarnbiguation. In 
Joint SIGDAT Conference on Empirical Meth- 
ods in Natural Language Processing and Very 
Large Corpora, Hong Kong, October. 
Eugene Charniak. 2000. A maximum entropy- 
inspired parser. In Proceedings of the 1st Meet- 
ing of the North American Chapter of the As- 
sociation for Computational Linguistics, pages 
132-139, Seattle, Washington, April 29 to May 
4. 
David Chiang. 2000. Statistical parsing with an 
automatically-extracted tr eadjoining gram- 
mar. In Proceedings of the 38th Annual Meeting 
of the Association for Computational Linguis- 
tics. 
Kenneth Church. 1988. A stochastic parts pro- 
gram and noun phrase parser for unrestricted 
text. In Second Conference on Applied Natural 
Language Processing, pages 136-143, Austin, 
Texas. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
ceedings of ACL-EACL '97, pages 16-23. 
Michael Collins. 2000. Discriminative r ranking 
for natural language parsing. In International 
Conference on Machine Learning. (to appear). 
Aravind K. Joshi and Yves Schabes. 1997. Tree- 
adjoining grammars. In A. Salomma and 
G. Rosenberg, editors, Handbook of Formal 
Languages and Automata, volume 3, pages 69- 
124. Springer-Verlag, Heidelberg. 
Mitchell P. Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building a 
large annotated corpus of English: The Penn 
Treebank. Computational Linguistics, 19:313- 
330. 
Scott Miller, Heidi Fox, Lance Ramshaw, and 
Ralph Weischedel. 1998. SIFT - Statistically- 
derived Information From Text. In Seventh 
Message Understanding Conference (MUC-7), 
Washington, D.C. 
Owen Rainbow, K. Vijay-Shanker, and David 
Weir. 1995. D-tree grammars. In Proceedings 
of the 33rd Annual Meeting of the Assocation 
for Computational Linguistics, pages 151-158. 
Adwait Ratnaparkhi. 1997. A simple introduc- 
tion to maximum entropy models for natural 
language processing. Technical Report 1RCS 
Report 97-08, Institute for Research in Cogni- 
tive Science, May. 
Philip Resnik. 1992. Probabilistic tree-adjoining 
grammar as a framework for statistical nat- 
ural language processing. In Proceedings of 
COLING-92, pages 418-424. 
Yves Schabes and Stuart M. Shieber. 1994. An 
alternative conception of tree-adjoining deriva- 
tion. Computational Linguistics, 20(1):91-124. 
Yves Schabes. 1992. Stochastic lexicalized 
tree-adjoining grammars. In Proceedings of 
COLING-92, pages 426-432. 
Fei Xia. 1999. Extracting tree adjoining ram- 
mars from bracketed corpora. In Proceedings 
of the 5th Natural Language Processing Pacific 
Rim Symposium (NLPRS-99). 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 779?786, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Hiero Machine Translation System:
Extensions, Evaluation, and Analysis
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
{dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu
Abstract
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has been largely absent from the best
performing machine translation systems in recent
community-wide evaluations. In this paper, we dis-
cuss a new hierarchical phrase-based statistical ma-
chine translation system (Chiang, 2005), present-
ing recent extensions to the original proposal, new
evaluation results in a community-wide evaluation,
and a novel technique for fine-grained comparative
analysis of MT systems.
1 Introduction
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has, for the last several years, been
absent from the best performing machine transla-
tion systems in community-wide evaluations. Statis-
tical phrase-based models (e.g. (Och and Ney, 2004;
Koehn et al, 2003; Marcu andWong, 2002)) charac-
terize a source sentence f as a flat partition of non-
overlapping subsequences, or ?phrases?, f?1 ? ? ? f?J ,
and the process of translation involves selecting tar-
get phrases e?i corresponding to the f? j and modify-
ing their sequential order. The need for some way
to model aspects of syntactic behavior, such as the
tendency of constituents to move together as a unit,
is widely recognized?the role of syntactic units is
well attested in recent systematic studies of trans-
lation (Fox, 2002; Hwa et al, 2002; Koehn and
Knight, 2003), and their absence in phrase-based
models is quite evident when looking at MT system
output. Nonetheless, attempts to incorporate richer
linguistic features have generally met with little suc-
cess (Och et al, 2004a).
Chiang (2005) introduces Hiero, a hierarchical
phrase-based model for statistical machine transla-
tion. Hiero extends the standard, non-hierarchical
notion of ?phrases? to include nonterminal sym-
bols, which permits it to capture both word-level and
phrase-level reorderings within the same framework.
The model has the formal structure of a synchronous
CFG, but it does not make any commitment to a
linguistically relevant analysis, and it does not re-
quire syntactically annotated training data. Chiang
(2005) reported significant performance improve-
ments in Chinese-English translation as compared
with Pharaoh, a state-of-the-art phrase-based system
(Koehn, 2004).
In Section 2, we review the essential elements
of Hiero. In Section 3 we describe extensions to
this system, including new features involving named
entities and numbers and support for a fourfold
scale-up in training set size. Section 4 presents new
evaluation results for Chinese-English as well as
Arabic-English translation, obtained in the context
of the 2005 NISTMT Eval exercise. In Section 5, we
introduce a novel technique for fine-grained com-
parative analysis of MT systems, which we em-
ploy in analyzing differences between Hiero?s and
Pharaoh?s translations.
2 Hiero
Hiero is a stochastic synchronous CFG, whose pro-
ductions are extracted automatically from unanno-
tated parallel texts, and whose rule probabilities
form a log-linear model learned by minimum-error-
rate training; together with a modified CKY beam-
search decoder (similar to that of Wu (1996)). We
describe these components in brief below.
779
S ? ?S 1 X 2 ,S 1 X 2 ?
S ? ?X 1 ,X 1 ?
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?
X ? ?X 1 de X 2 , the X 2 that X 1 ?
X ? ?X 1 zhiyi, one of X 1 ?
X ? ?Aozhou,Australia?
X ? ?shi, is?
X ? ?shaoshu guojia, few countries?
X ? ?bangjiao, diplomatic relations?
X ? ?Bei Han,North Korea?
Figure 1: Example synchronous CFG
2.1 Grammar
A synchronous CFG or syntax-directed transduction
grammar (Lewis and Stearns, 1968) consists of pairs
of CFG rules with aligned nonterminal symbols. We
denote this alignment by coindexation with boxed
numbers (Figure 1). A derivation starts with a pair
of aligned start symbols, and proceeds by rewrit-
ing pairs of aligned nonterminal symbols using the
paired rules (Figure 2).
Training begins with phrase pairs, obtained as by
Och, Koehn, and others: GIZA++ (Och and Ney,
2000) is used to obtain one-to-many word align-
ments in both directions, which are combined into a
single set of refined alignments using the ?final-and?
method of Koehn et al (2003); then those pairs of
substrings that are exclusively aligned to each other
are extracted as phrase pairs.
Then, synchronous CFG rules are constructed
out of the initial phrase pairs by subtraction: ev-
ery phrase pair ? f? , e?? becomes a rule X ? ? f? , e??,
and a phrase pair ? f? , e?? can be subtracted from a
rule X ? ??1 f??2, ?1e??2? to form a new rule X ?
??1X i ?2, ?1X i ?2?, where i is an index not already
used. Various filters are also applied to reduce the
number of extracted rules. Since one of these filters
restricts the number of nonterminal symbols to two,
our extracted grammar is equivalent to an inversion
transduction grammar (Wu, 1997).
2.2 Model
The model is a log-linear model (Och and Ney,
2002) over synchronous CFG derivations. The
weight of a derivation is PLM(e)?LM , the weighted
language model probability, multiplied by the prod-
uct of the weights of the rules used in the derivation.
The weight of each rule is, in turn:
(1) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. The ba-
sic model uses the following features, analogous to
Pharaoh?s default feature set:
? P(? | ?) and P(? | ?)
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003);1
? a phrase penalty exp(1);
? a word penalty exp(l), where l is the number of
terminals in ?.
The exceptions to the above are the two ?glue?
rules, which are the rules with left-hand side S in
Figure 1. The second has weight one, and the first
has weight w(S ? ?S 1 X 2 ,S 1 X 2 ?) = exp(??g),
the idea being that parameter ?g controls the model?s
preference for hierarchical phrases over serial com-
bination of phrases.
Phrase translation probabilities are estimated by
relative-frequency estimation. Since the extraction
process does not generate a unique derivation for
each training sentence pair, a distribution over pos-
sible derivations is hypothesized, which gives uni-
form weight to all initial phrases extracted from a
sentence pair and uniform weight to all rules formed
out of an initial phrase. This distribution is then used
to estimate the phrase translation probabilities.
The lexical-weighting features are estimated us-
ing a method similar to that of Koehn et al (2003).
The language model is a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998), trained using the SRI-LM toolkit (Stolcke,
2002).
1This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; Hiero uses a weighted average.
780
?S 1 ,S 1 ? ? ?S 2 X 3 ,S 2 X 3 ?
? ?S 4 X 5 X 3 ,S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 ,X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 ,Australia X 5 X 3 ?
? ?Aozhou shi X 3 ,Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi,Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi,Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi,Australia is one of the X 9 that have X 2 with X 1 ?
Figure 2: Example partial derivation of a synchronous CFG.
The feature weights are learned by maximizing
the BLEU score (Papineni et al, 2002) on held-out
data, using minimum-error-rate training (Och, 2003)
as implemented by Koehn. The implementation was
slightly modified to ensure that the BLEU scoring
matches NIST?s definition and that hypotheses in
the n-best lists are merged when they have the same
translation and the same feature vector.
3 Extensions
In this section we describe our extensions to the base
Hiero system that improve its performance signif-
icantly. First, we describe the addition of two new
features to the Chinese model, in a manner similar
to that of Och et al (2004b); then we describe how
we scaled the system up to a much larger training
set.
3.1 New features
The LDC Chinese-English named entity lists (900k
entries) are a potentially valuable resource, but
previous experiments have suggested that simply
adding them to the training data does not help
(Vogel et al, 2003). Instead, we placed them in
a supplementary phrase-translation table, giving
greater weight to phrases that occurred less fre-
quently in the primary training data. For each en-
try ? f , {e1, . . . , en}?, we counted the number of times
c( f ) that f appeared in the primary training data,
and assigned the entry the weight 1c( f )+1 , which
was then distributed evenly among the supplemen-
tary phrase pairs {? f , ei?}. We then created a new
model feature for named entities. When one of these
supplementary phrase pairs was used in transla-
tion, its feature value for the named-entity feature
was the weight defined above, and its value in the
other phrase-translation and lexical-weighting fea-
tures was zero. Since these scores belonged to a sep-
arate feature from the primary translation probabili-
ties, they could be reweighted independently during
minimum-error-rate training.
Similarly, to process Chinese numbers and dates,
we wrote a rule-based Chinese number/date transla-
tor, and created a new model feature for it. Again,
the weight given to this module was optimized
during minimum-error-rate training. In some cases
we wrote the rules to provide multiple uniformly-
weighted English translations for a Chinese phrase
(for example,k? (bari) could become ?the 8th? or
?on the 8th?), allowing the language model to decide
between the options.
3.2 Scaling up training
Chiang (2005) reports on experiments in Chinese-
English translation using a model trained on
7.2M+9.2M words of parallel data.2 For the NIST
MT Eval 2005 large training condition, consider-
ably more data than this is allowable. We chose
to use only newswire data, plus data from Sino-
rama, a Taiwanese news magazine.3 This amounts
to almost 30M+30M words. Scaling to this set re-
quired reducing the initial limit on phrase lengths,
previously fixed at 10, to avoid explosive growth of
2Here and below, the notation ?X + Y words? denotes X
words of foreign text and Y words of English text.
3From Sinorama, only data from 1991 and later were used,
as articles prior to that were translated quite loosely.
781
the extracted grammar. However, since longer initial
phrases can be beneficial for translation accuracy,
we adopted a variable length limit: 10 for the FBIS
corpus and other mainland newswire sources, and 7
for the HK News corpus and Sinorama. (During de-
coding, limits of up to 15 were sometimes used; in
principle these limits should all be the same, but in
practice it is preferable to tune them separately.)
For Arabic-English translation, we used the ba-
sic Hiero model, without special features for named
entities or numbers/dates. We again used only the
newswire portions of the allowable training data; we
also excluded the Ummah data, as the translations
were found to be quite loose. Since this amounted
to only about 1.5M+1.5M words, we used a higher
initial phrase limit of 15 during both training and de-
coding.
4 Evaluation
Figure 1 shows the performance of several systems
on NIST MT Eval 2003 Chinese test data: Pharaoh
(2004 version), trained only on the FBIS data; Hi-
ero, with various combinations of the new features
and the larger training data.4 This table also shows
Hiero?s performance on the NIST 2005 MT evalua-
tion task.5 The metric here is case-sensitive BLEU.6
Figure 2 shows the performance of two systems
on Arabic in the NIST 2005 MT Evaluation task:
DC, a phrase-based decoder for a model trained by
Pharaoh, and Hiero.
5 Analysis
Over the last few years, several automatic metrics
for machine translation evaluation have been intro-
duced, largely to reduce the human cost of itera-
tive system evaluation during the development cy-
cle (Lin and Och, 2004; Melamed et al, 2003; Pap-
ineni et al, 2002). All are predicated on the concept
4The third line, corresponding to the model without new fea-
tures trained on the larger data, may be slightly depressed be-
cause the feature weights from the fourth line were used instead
of doing minimum-error-rate training specially for this model.
5Full results are available at http://www.nist.gov/
speech/tests/summaries/2005/mt05.htm. For this test, a
phrase length limit of 15 was used during decoding.
6For this task, the translation output was uppercased using
the SRI-LM toolkit: essentially, it was decoded again using
an HMM whose states and transitions are a trigram language
model of cased English, and whose emission probabilities are
reversed, i.e., probability of cased word given lowercased word.
System Features Train MT03 MT05
Pharaoh standard FBIS 0.268
Hiero standard FBIS 0.288
Hiero standard full 0.329
Hiero +nums, names full 0.339 0.300
Table 1: Chinese results. (BLEU-4; MT03 case-
insensitive, MT05 case-sensitive)
System Train MT05
DC full 0.399
Hiero full 0.450
Table 2: Arabic results. (BLEU-4; MT03 case-
insensitive, MT05 scores case-sensitive.
of n-gram matching between the sentence hypothe-
sized by the translation system and one or more ref-
erence translations?that is, human translations for
the test sentence. Although the motivations and for-
mulae underlying these metrics are all different, ul-
timately they all produce a single number represent-
ing the ?goodness? of the MT system output over a
set of reference documents. This facility is valuable
in determining whether a given system modification
has a positive impact on overall translation perfor-
mance. However, the metrics are all holistic. They
provide no insight into the specific competencies or
weaknesses of one system relative to another.
Ideally, we would like to use automatic methods
to provide immediate diagnostic information about
the translation output?what the system does well,
and what it does poorly. At the most general level,
we want to know how our system performs on the
two most basic problems in translation?word trans-
lation and reordering. Unigram precision and recall
statistics tell us something about the performance of
an MT system?s internal translation dictionaries, but
nothing about reordering. It is thought that higher or-
der n-grams correlate with the reordering accuracy
of MT systems, but this is again a holistic metric.
What we would really like to know is howwell the
system is able to capture systematic reordering pat-
terns in the input, which ones it is successful with,
and which ones it has difficulty with. Word n-grams
are little help here: they are too many, too sparse, and
it is difficult to discern general patterns from them.
782
5.1 A New Analysis Method
In developing a new analysis method, we are moti-
vated in part by recent studies suggesting that word
reorderings follow general patterns with respect to
syntax, although there remains a high degree of flex-
ibility (Fox, 2002; Hwa et al, 2002). This suggests
that in a comparative analysis of two MT systems, it
may be useful to look for syntactic patterns that one
system captures well in the target language and the
other does not, using a syntax based metric.
We propose to summarize reordering patterns us-
ing part-of-speech sequences. Unfortunately, recent
work has shown that applying statistical parsers to
ungrammatical MT output is unreliable at best, with
the parser often assigning unreasonable probabili-
ties and incongruent structure (Yamada and Knight,
2002; Och et al, 2004a). Anticipating that this
would be equally problematic for part-of-speech
tagging, we make the conservative choice to apply
annotation only to the reference corpus. Word n-
gram correspondences with a reference translation
are used to infer the part-of-speech tags for words in
the system output.
First, we tagged the reference corpus with parts
of speech. We used MXPOST (Ratnaparkhi, 1996),
and in order to discover more general patterns, we
map the tag set down after tagging, e.g. NN, NNP,
NNPS and NNS all map to NN. Second, we com-
puted the frequency freq(ti . . . t j) of every possible
tag sequence ti . . . t j in the reference corpus. Third,
we computed the correspondence between each hy-
pothesis sentence and each of its corresponding ref-
erence sentences using an approximation to max-
imum matching (Melamed et al, 2003). This al-
gorithm provides a list of runs or contiguous se-
quences of words ei . . . e j in the reference that are
also present in the hypothesis. (Note that runs are
order-sensitive.) Fourth, for each recalled n-gram
ei . . . e j, we looked up the associated tag sequence
ti . . . t j and incremented a counter recalled(ti . . . t j).
Finally, we computed the recall of tag patterns,
R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
By examining examples of these tag sequences in
the reference corpus and their hypothesized trans-
lations, we expect to gain some insight into the
comparative strengths and weaknesses of the MT
systems? reordering models. (An interactive plat-
form for this analysis is demonstrated by Lopez and
Resnik (2005).)
5.2 Chinese
We performed tag sequence analysis on the Hiero
and Pharaoh systems trained on the FBIS data only.
Table 3 shows those n-grams for which Hiero and
Pharaoh?s recall differed significantly (p < 0.01).
The numbers shown are the ratio of Hiero?s recall
to Pharaoh?s. Note that the n-grams on which Hi-
ero had better recall are dominated by fragments of
prepositional phrases (in the Penn Treebank tagset,
prepositions are tagged IN or TO).
Our hypothesis is that Hiero produces English PPs
better because many of them are translated from
Chinese phrases which have an NP modifying an NP
to its right, often connected with the particle? (de).
These are often translated into English as PPs, which
modify the NP to the left. A correct translation, then,
would have to reorder the two NPs. Notice in the ta-
ble that Hiero recalls proportionally more n-grams
as n increases, corroborating the intuition that Hiero
should be better at longer-distance reorderings.
Investigating this hypothesis qualitatively, we in-
spected the first five occurrences of the n-grams of
the first type on the list (JJ NN IN DT NN). Of
these, we omit one example because both systems
recalled the n-gram correctly, and one because they
differed only in lexical choice (Hiero matched the
5-gram with one reference sentence, Pharaoh with
zero). The other three examples are shown below (H
= Hiero, P = Pharaoh):
(2) T?
UN
?h
security
?Proceedings of the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Hierarchical Phrase-Based Model for Statistical Machine Translation
David Chiang
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
dchiang@umiacs.umd.edu
Abstract
We present a statistical phrase-based transla-
tion model that uses hierarchical phrases?
phrases that contain subphrases. The model
is formally a synchronous context-free gram-
mar but is learned from a bitext without any
syntactic information. Thus it can be seen as
a shift to the formal machinery of syntax-
based translation systems without any lin-
guistic commitment. In our experiments us-
ing BLEU as a metric, the hierarchical phrase-
based model achieves a relative improve-
ment of 7.5% over Pharaoh, a state-of-the-art
phrase-based system.
1 Introduction
The alignment template translation model (Och and
Ney, 2004) and related phrase-based models ad-
vanced the previous state of the art by moving
from words to phrases as the basic unit of transla-
tion. Phrases, which can be any substring and not
necessarily phrases in any syntactic theory, allow
these models to learn local reorderings, translation
of short idioms, or insertions and deletions that are
sensitive to local context. They are thus a simple and
powerful mechanism for machine translation.
The basic phrase-based model is an instance of
the noisy-channel approach (Brown et al, 1993),1 in
which the translation of a French sentence f into an
1Throughout this paper, we follow the convention of Brown
et al of designating the source and target languages as ?French?
and ?English,? respectively. The variables f and e stand for
source and target sentences; f ji stands for the substring of f
from position i to position j inclusive, and similarly for e ji .
English sentence e is modeled as:
arg max
e
P(e | f ) = arg max
e
P(e, f )(1)
= arg max
e
(P(e) ? P( f | e))(2)
The translation model P( f | e) ?encodes? e into f by
the following steps:
1. segment e into phrases e?1 ? ? ? e?I , typically with
a uniform distribution over segmentations;
2. reorder the e?i according to some distortion
model;
3. translate each of the e?i into French phrases ac-
cording to a model P( ?f | e?) estimated from the
training data.
Other phrase-based models model the joint distribu-
tion P(e, f ) (Marcu and Wong, 2002) or made P(e)
and P( f | e) into features of a log-linear model (Och
and Ney, 2002). But the basic architecture of phrase
segmentation (or generation), phrase reordering, and
phrase translation remains the same.
Phrase-based models can robustly perform trans-
lations that are localized to substrings that are com-
mon enough to have been observed in training. But
Koehn et al (2003) find that phrases longer than
three words improve performance little, suggesting
that data sparseness takes over for longer phrases.
Above the phrase level, these models typically have
a simple distortion model that reorders phrases in-
dependently of their content (Och and Ney, 2004;
Koehn et al, 2003), or not at all (Zens and Ney,
2004; Kumar et al, 2005).
But it is often desirable to capture translations
whose scope is larger than a few consecutive words.
263
Consider the following Mandarin example and its
English translation:
(3) ?2
Aozhou
Australia
/
shi
is

yu
with

Bei
North
?
Han
Korea
	
you
have
??
bangjiao
dipl. rels.
?
de
that
p
shaoshu
few
??
guojia
countries
K 
zhiyi
one of
?Australia is one of the few countries that have
diplomatic relations with North Korea?
If we count zhiyi, lit. ?of-one,? as a single token, then
translating this sentence correctly into English re-
quires reversing a sequence of five elements. When
we run a phrase-based system, Pharaoh (Koehn et
al., 2003; Koehn, 2004a), on this sentence (using the
experimental setup described below), we get the fol-
lowing phrases with translations:
(4) [Aozhou] [shi] [yu] [Bei Han] [you]
[bangjiao]1 [de shaoshu guojia zhiyi]
[Australia] [is] [dipl. rels.]1 [with] [North
Korea] [is] [one of the few countries]
where we have used subscripts to indicate the re-
ordering of phrases. The phrase-based model is
able to order ?diplomatic. . .Korea? correctly (using
phrase reordering) and ?one. . .countries? correctly
(using a phrase translation), but does not accom-
plish the necessary inversion of those two groups.
A lexicalized phrase-reordering model like that in
use in ISI?s system (Och et al, 2004) might be able
to learn a better reordering, but simpler distortion
models will probably not.
We propose a solution to these problems that
does not interfere with the strengths of the phrase-
based approach, but rather capitalizes on them: since
phrases are good for learning reorderings of words,
we can use them to learn reorderings of phrases
as well. In order to do this we need hierarchical
phrases that consist of both words and subphrases.
For example, a hierarchical phrase pair that might
help with the above example is:
(5) ?yu 1 you 2 , have 2 with 1 ?
where 1 and 2 are placeholders for subphrases. This
would capture the fact that Chinese PPs almost al-
ways modify VP on the left, whereas English PPs
usually modify VP on the right. Because it gener-
alizes over possible prepositional objects and direct
objects, it acts both as a discontinuous phrase pair
and as a phrase-reordering rule. Thus it is consider-
ably more powerful than a conventional phrase pair.
Similarly,
(6) ? 1 de 2 , the 2 that 1 ?
would capture the fact that Chinese relative clauses
modify NPs on the left, whereas English relative
clauses modify on the right; and
(7) ? 1 zhiyi, one of 1 ?
would render the construction zhiyi in English word
order. These three rules, along with some conven-
tional phrase pairs, suffice to translate the sentence
correctly:
(8) [Aozhou] [shi] [[[yu [Bei Han]1 you
[bangjiao]2] de [shaoshu guojia]3] zhiyi]
[Australia] [is] [one of [the [few countries]3
that [have [dipl. rels.]2 with [North Korea]1]]]
The system we describe below uses rules like this,
and in fact is able to learn them automatically from
a bitext without syntactic annotation. It translates the
above example almost exactly as we have shown, the
only error being that it omits the word ?that? from (6)
and therefore (8).
These hierarchical phrase pairs are formally pro-
ductions of a synchronous context-free grammar
(defined below). A move to synchronous CFG can
be seen as a move towards syntax-based MT; how-
ever, we make a distinction here between formally
syntax-based and linguistically syntax-based MT. A
system like that of Yamada and Knight (2001) is
both formally and linguistically syntax-based: for-
mally because it uses synchronous CFG, linguisti-
cally because the structures it is defined over are (on
the English side) informed by syntactic theory (via
the Penn Treebank). Our system is formally syntax-
based in that it uses synchronous CFG, but not nec-
essarily linguistically syntax-based, because it in-
duces a grammar from a parallel text without relying
on any linguistic annotations or assumptions; the re-
sult sometimes resembles a syntactician?s grammar
but often does not. In this respect it resembles Wu?s
264
bilingual bracketer (Wu, 1997), but ours uses a dif-
ferent extraction method that allows more than one
lexical item in a rule, in keeping with the phrase-
based philosophy. Our extraction method is basi-
cally the same as that of Block (2000), except we
allow more than one nonterminal symbol in a rule,
and use a more sophisticated probability model.
In this paper we describe the design and imple-
mentation of our hierarchical phrase-based model,
and report on experiments that demonstrate that hi-
erarchical phrases indeed improve translation.
2 The model
Our model is based on a weighted synchronous CFG
(Aho and Ullman, 1969). In a synchronous CFG the
elementary structures are rewrite rules with aligned
pairs of right-hand sides:
(9) X ? ??, ?,??
where X is a nonterminal, ? and ? are both strings
of terminals and nonterminals, and ? is a one-to-one
correspondence between nonterminal occurrences
in ? and nonterminal occurrences in ?. Rewriting
begins with a pair of linked start symbols. At each
step, two coindexed nonterminals are rewritten us-
ing the two components of a single rule, such that
none of the newly introduced symbols is linked to
any symbols already present.
Thus the hierarchical phrase pairs from our above
example could be formalized in a synchronous CFG
as:
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?(10)
X ? ?X 1 de X 2 , the X 2 that X 1 ?(11)
X ? ?X 1 zhiyi, one of X 1 ?(12)
where we have used boxed indices to indicate which
occurrences of X are linked by ?.
Note that we have used only a single nonterminal
symbol X instead of assigning syntactic categories
to phrases. In the grammar we extract from a bitext
(described below), all of our rules use only X, ex-
cept for two special ?glue? rules, which combine a
sequence of Xs to form an S:
S ? ?S 1 X 2 , S 1 X 2 ?(13)
S ? ?X 1 , X 1 ?(14)
These give the model the option to build only par-
tial translations using hierarchical phrases, and then
combine them serially as in a standard phrase-based
model. For a partial example of a synchronous CFG
derivation, see Figure 1.
Following Och and Ney (2002), we depart from
the traditional noisy-channel approach and use a
more general log-linear model. The weight of each
rule is:
(15) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. For our
experiments we used the following features, analo-
gous to Pharaoh?s default feature set:
? P(? | ?) and P(? | ?), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003), which estimate how well
the words in ? translate the words in ?;2
? a phrase penalty exp(1), which allows the
model to learn a preference for longer or
shorter derivations, analogous to Koehn?s
phrase penalty (Koehn, 2003).
The exceptions to the above are the two glue rules,
(13), which has weight one, and (14), which has
weight
(16) w(S ? ?S 1 X 2 , S 1 X 2 ?) = exp(??g)
the idea being that ?g controls the model?s prefer-
ence for hierarchical phrases over serial combination
of phrases.
Let D be a derivation of the grammar, and let f (D)
and e(D) be the French and English strings gener-
ated by D. Let us represent D as a set of triples
?r, i, j?, each of which stands for an application of
a grammar rule r to rewrite a nonterminal that spans
f (D) ji on the French side.3 Then the weight of D
2This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; we take a weighted average.
3This representation is not completely unambiguous, but is
sufficient for defining the model.
265
?S 1 , S 1 ? ? ?S 2 X 3 , S 2 X 3 ?
? ?S 4 X 5 X 3 , S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 , X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 , Australia X 5 X 3 ?
? ?Aozhou shi X 3 , Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi, Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 ?
Figure 1: Example partial derivation of a synchronous CFG.
is the product of the weights of the rules used in the
translation, multiplied by the following extra factors:
(17) w(D) =
?
?r,i, j??D
w(r)? plm(e)?lm ? exp(??wp|e|)
where plm is the language model, and exp(??wp|e|),
the word penalty, gives some control over the length
of the English output.
We have separated these factors out from the rule
weights for notational convenience, but it is concep-
tually cleaner (and necessary for polynomial-time
decoding) to integrate them into the rule weights,
so that the whole model is a weighted synchronous
CFG. The word penalty is easy; the language model
is integrated by intersecting the English-side CFG
with the language model, which is a weighted finite-
state automaton.
3 Training
The training process begins with a word-aligned cor-
pus: a set of triples ? f , e,??, where f is a French
sentence, e is an English sentence, and ? is a (many-
to-many) binary relation between positions of f and
positions of e. We obtain the word alignments using
the method of Koehn et al (2003), which is based
on that of Och and Ney (2004). This involves run-
ning GIZA++ (Och and Ney, 2000) on the corpus in
both directions, and applying refinement rules (the
variant they designate ?final-and?) to obtain a single
many-to-many word alignment for each sentence.
Then, following Och and others, we use heuris-
tics to hypothesize a distribution of possible deriva-
tions of each training example, and then estimate
the phrase translation parameters from the hypoth-
esized distribution. To do this, we first identify ini-
tial phrase pairs using the same criterion as previous
systems (Och and Ney, 2004; Koehn et al, 2003):
Definition 1. Given a word-aligned sentence pair
? f , e,??, a rule ? f ji , e j
?
i? ? is an initial phrase pair of
? f , e,?? iff:
1. fk ? ek? for some k ? [i, j] and k? ? [i?, j?];
2. fk / ek? for all k ? [i, j] and k? < [i?, j?];
3. fk / ek? for all k < [i, j] and k? ? [i?, j?].
Next, we form all possible differences of phrase
pairs:
Definition 2. The set of rules of ? f , e,?? is the
smallest set satisfying the following:
1. If ? f ji , e j
?
i? ? is an initial phrase pair, then
X ? ? f ji , e j
?
i? ?
is a rule.
2. If r = X ? ??, ?? is a rule and ? f ji , e j
?
i? ? is an
initial phrase pair such that ? = ?1 f ji ?2 and ? =
?1e
j?
i? ?2, then
X ? ??1X k ?2, ?1X k ?2?
is a rule, where k is an index not used in r.
The above scheme generates a very large num-
ber of rules, which is undesirable not only because
it makes training and decoding very slow, but also
266
because it creates spurious ambiguity?a situation
where the decoder produces many derivations that
are distinct yet have the same model feature vectors
and give the same translation. This can result in n-
best lists with very few different translations or fea-
ture vectors, which is problematic for the algorithm
we use to tune the feature weights. Therefore we
filter our grammar according to the following prin-
ciples, chosen to balance grammar size and perfor-
mance on our development set:
1. If there are multiple initial phrase pairs contain-
ing the same set of alignment points, we keep
only the smallest.
2. Initial phrases are limited to a length of 10 on
the French side, and rule to five (nonterminals
plus terminals) on the French right-hand side.
3. In the subtraction step, f ji must have length
greater than one. The rationale is that little
would be gained by creating a new rule that is
no shorter than the original.
4. Rules can have at most two nonterminals,
which simplifies the decoder implementation.
Moreover, we prohibit nonterminals that are
adjacent on the French side, a major cause of
spurious ambiguity.
5. A rule must have at least one pair of aligned
words, making translation decisions always
based on some lexical evidence.
Now we must hypothesize weights for all the deriva-
tions. Och?s method gives equal weight to all the
extracted phrase occurences. However, our method
may extract many rules from a single initial phrase
pair; therefore we distribute weight equally among
initial phrase pairs, but distribute that weight equally
among the rules extracted from each. Treating this
distribution as our observed data, we use relative-
frequency estimation to obtain P(? | ?) and P(? | ?).
4 Decoding
Our decoder is a CKY parser with beam search
together with a postprocessor for mapping French
derivations to English derivations. Given a French
sentence f , it finds the best derivation (or n best
derivations, with little overhead) that generates ? f , e?
for some e. Note that we find the English yield of the
highest-probability single derivation
(18) e
?
????? arg max
D s.t. f (D) = f
w(D)
?
?????
and not necessarily the highest-probability e, which
would require a more expensive summation over
derivations.
We prune the search space in several ways. First,
an item that has a score worse than ? times the best
score in the same cell is discarded; second, an item
that is worse than the bth best item in the same cell is
discarded. Each cell contains all the items standing
for X spanning f ji . We choose b and ? to balance
speed and performance on our development set. For
our experiments, we set b = 40, ? = 10?1 for X cells,
and b = 15, ? = 10?1 for S cells. We also prune rules
that have the same French side (b = 100).
The parser only operates on the French-side gram-
mar; the English-side grammar affects parsing only
by increasing the effective grammar size, because
there may be multiple rules with the same French
side but different English sides, and also because in-
tersecting the language model with the English-side
grammar introduces many states into the nontermi-
nal alphabet, which are projected over to the French
side. Thus, our decoder?s search space is many times
larger than a monolingual parser?s would be. To re-
duce this effect, we apply the following heuristic
when filling a cell: if an item falls outside the beam,
then any item that would be generated using a lower-
scoring rule or a lower-scoring antecedent item is
also assumed to fall outside the beam. This heuristic
greatly increases decoding speed, at the cost of some
search errors.
Finally, the decoder has a constraint that pro-
hibits any X from spanning a substring longer than
10 on the French side, corresponding to the maxi-
mum length constraint on initial rules during train-
ing. This makes the decoding algorithm asymptoti-
cally linear-time.
The decoder is implemented in Python, an inter-
preted language, with C++ code from the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002). Using the
settings described above, on a 2.4 GHz Pentium IV,
it takes about 20 seconds to translate each sentence
(average length about 30). This is faster than our
267
Python implementation of a standard phrase-based
decoder, so we expect that a future optimized imple-
mentation of the hierarchical decoder will run at a
speed competitive with other phrase-based systems.
5 Experiments
Our experiments were on Mandarin-to-English
translation. We compared a baseline system,
the state-of-the-art phrase-based system Pharaoh
(Koehn et al, 2003; Koehn, 2004a), against our sys-
tem. For all three systems we trained the transla-
tion model on the FBIS corpus (7.2M+9.2M words);
for the language model, we used the SRI Language
Modeling Toolkit to train a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998) on 155M words of English newswire text,
mostly from the Xinhua portion of the Gigaword
corpus. We used the 2002 NIST MT evaluation test
set as our development set, and the 2003 test set as
our test set. Our evaluation metric was BLEU (Pap-
ineni et al, 2002), as calculated by the NIST script
(version 11a) with its default settings, which is to
perform case-insensitive matching of n-grams up to
n = 4, and to use the shortest (as opposed to nearest)
reference sentence for the brevity penalty. The re-
sults of the experiments are summarized in Table 1.
5.1 Baseline
The baseline system we used for comparison was
Pharaoh (Koehn et al, 2003; Koehn, 2004a), as pub-
licly distributed. We used the default feature set: lan-
guage model (same as above), p( ?f | e?), p(e? | ?f ), lex-
ical weighting (both directions), distortion model,
word penalty, and phrase penalty. We ran the trainer
with its default settings (maximum phrase length 7),
and then used Koehn?s implementation of minimum-
error-rate training (Och, 2003) to tune the feature
weights to maximize the system?s BLEU score on
our development set, yielding the values shown in
Table 2. Finally, we ran the decoder on the test set,
pruning the phrase table with b = 100, pruning the
chart with b = 100, ? = 10?5, and limiting distor-
tions to 4. These are the default settings, except for
the phrase table?s b, which was raised from 20, and
the distortion limit. Both of these changes, made by
Koehn?s minimum-error-rate trainer by default, im-
prove performance on the development set.
Rank Chinese English
1  .
3 ? the
14 ( in
23 ? ?s
577 X 1 ? X 2 the X 2 of X 1
735 X 1 ? X 2 the X 2 X 1
763 X 1 K  one of X 1
1201 X 1 ;? president X 1
1240 X 1 ?C $ X 1
2091 ?t X 1 X 1 this year
3253 ~K X 1 X 1 percent
10508 ( X 1  under X 1
28426 ( X 1 M before X 1
47015 X 1 ? X 2 the X 2 that X 1
1752457  X 1 	 X 2 have X 2 with X 1
Figure 2: A selection of extracted rules, with ranks
after filtering for the development set. All have X for
their left-hand sides.
5.2 Hierarchical model
We ran the training process of Section 3 on the same
data, obtaining a grammar of 24M rules. When fil-
tered for the development set, the grammar has 2.2M
rules (see Figure 2 for examples). We then ran the
minimum-error rate trainer with our decoder to tune
the feature weights, yielding the values shown in Ta-
ble 2. Note that ?g penalizes the glue rule much less
than ?pp does ordinary rules. This suggests that the
model will prefer serial combination of phrases, un-
less some other factor supports the use of hierarchi-
cal phrases (e.g., a better language model score).
We then tested our system, using the settings de-
scribed above.4 Our system achieves an absolute im-
provement of 0.02 over the baseline (7.5% relative),
without using any additional training data. This dif-
ference is statistically significant (p < 0.01).5 See
Table 1, which also shows that the relative gain is
higher for higher n-grams.
4Note that we gave Pharaoh wider beam settings than we
used on our own decoder; on the other hand, since our decoder?s
chart has more cells, its b limits do not need to be as high.
5We used Zhang?s significance tester (Zhang et al, 2004),
which uses bootstrap resampling (Koehn, 2004b); it was mod-
ified to conform to NIST?s current definition of the BLEU
brevity penalty.
268
BLEU-n n-gram precisions
System 4 1 2 3 4 5 6 7 8
Pharaoh 0.2676 0.72 0.37 0.19 0.10 0.052 0.027 0.014 0.0075
hierarchical 0.2877 0.74 0.39 0.21 0.11 0.060 0.032 0.017 0.0084
+constituent 0.2881 0.73 0.39 0.21 0.11 0.062 0.032 0.017 0.0088
Table 1: Results on baseline system and hierarchical system, with and without constituent feature.
Features
System Plm(e) P(?|?) P(?|?) Pw(?|?) Pw(?|?) Word Phr ?d ?g ?c
Pharaoh 0.19 0.095 0.030 0.14 0.029 ?0.20 0.22 0.11 ? ?
hierarchical 0.15 0.036 0.074 0.037 0.076 ?0.32 0.22 ? 0.09 ?
+constituent 0.11 0.026 0.062 0.025 0.029 ?0.23 0.21 ? 0.11 0.20
Table 2: Feature weights obtained by minimum-error-rate training (normalized so that absolute values sum
to one). Word = word penalty; Phr = phrase penalty. Note that we have inverted the sense of Pharaoh?s
phrase penalty so that a positive weight indicates a penalty.
5.3 Adding a constituent feature
The use of hierarchical structures opens the pos-
sibility of making the model sensitive to syntac-
tic structure. Koehn et al (2003) mention German
?es gibt, there is? as an example of a good phrase
pair which is not a syntactic phrase pair, and report
that favoring syntactic phrases does not improve ac-
curacy. But in our model, the rule
(19) X ? ?es gibt X 1 , there is X 1 ?
would indeed respect syntactic phrases, because it
builds a pair of Ss out of a pair of NPs. Thus, favor-
ing subtrees in our model that are syntactic phrases
might provide a fairer way of testing the hypothesis
that syntactic phrases are better phrases.
This feature adds a factor to (17),
(20) c(i, j) =
?
???
???
1 if f ji is a constituent
0 otherwise
as determined by a statistical tree-substitution-
grammar parser (Bikel and Chiang, 2000), trained
on the Penn Chinese Treebank, version 3 (250k
words). Note that the parser was run only on the
test data and not the (much larger) training data. Re-
running the minimum-error-rate trainer with the new
feature yielded the feature weights shown in Table 2.
Although the feature improved accuracy on the de-
velopment set (from 0.314 to 0.322), it gave no sta-
tistically significant improvement on the test set.
6 Conclusion
Hierarchical phrase pairs, which can be learned
without any syntactically-annotated training data,
improve translation accuracy significantly compared
with a state-of-the-art phrase-based system. They
also facilitate the incorporation of syntactic informa-
tion, which, however, did not provide a statistically
significant gain.
Our primary goal for the future is to move towards
a more syntactically-motivated grammar, whether
by automatic methods to induce syntactic categories,
or by better integration of parsers trained on an-
notated data. This would potentially improve both
accuracy and efficiency. Moreover, reducing the
grammar size would allow more ambitious train-
ing settings. The maximum initial phrase length
is currently 10; preliminary experiments show that
increasing this limit to as high as 15 does im-
prove accuracy, but requires more memory. On the
other hand, we have successfully trained on almost
30M+30M words by tightening the initial phrase
length limit for part of the data. Streamlining the
grammar would allow further experimentation in
these directions.
In any case, future improvements to this system
will maintain the design philosophy proven here,
that ideas from syntax should be incorporated into
statistical translation, but not in exchange for the
strengths of the phrase-based approach.
269
Acknowledgements
I would like to thank Philipp Koehn for the use of the
Pharaoh software; and Adam Lopez, Michael Sub-
otin, Nitin Madnani, Christof Monz, Liang Huang,
and Philip Resnik. This work was partially sup-
ported by ONR MURI contract FCPO.810548265
and Department of Defense contract RD-02-5700.
S. D. G.
References
A. V. Aho and J. D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3:37?56.
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 1?6.
Hans Ulrich Block. 2000. Example-based incremen-
tal synchronous interpretation. In Wolfgang Wahlster,
editor, Verbmobil: Foundations of Speech-to-Speech
Translation, pages 411?417. Springer-Verlag, Berlin.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19:263?311.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
Philipp Koehn. 2004a. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388?395.
Shankar Kumar, Yonggang Deng, and William Byrne.
2005. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering. To appear.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 133?139.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin
Knight, Dragos Stefan Munteanu, Quamrul Tipu,
Michel Galley, and Mark Hopkins. 2004. Arabic and
Chinese MT at USC/ISI. Presentation given at NIST
Machine Translation Evaluation Workshop.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. B???: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 311?318.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the ACL, pages 523?530.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 257?264.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of the Fourth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 2051?
2054.
270
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 53?64,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Better k-best Parsing
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
David Chiang
Inst. for Advanced Computer Studies
University of Maryland
3161 AV Williams
College Park, MD 20742
dchiang@umiacs.umd.edu
Abstract
We discuss the relevance of k-best parsing to
recent applications in natural language pro-
cessing, and develop efficient algorithms for
k-best trees in the framework of hypergraph
parsing. To demonstrate the efficiency, scal-
ability and accuracy of these algorithms, we
present experiments on Bikel?s implementation
of Collins? lexicalized PCFG model, and on
Chiang?s CFG-based decoder for hierarchical
phrase-based translation. We show in particu-
lar how the improved output of our algorithms
has the potential to improve results from parse
reranking systems and other applications.
1 Introduction
Many problems in natural language processing (NLP) in-
volve optimizing some objective function over a set of
possible analyses of an input string. This set is often
exponential-sized but can be compactly represented by
merging equivalent subanalyses. If the objective function
is compatible with a packed representation, then it can be
optimized efficiently by dynamic programming. For ex-
ample, the distribution of parse trees for a given sentence
under a PCFG can be represented as a packed forest from
which the highest-probability tree can be easily extracted.
However, when the objective function f has no com-
patible packed representation, exact inference would be
intractable. To alleviate this problem, one common ap-
proach from machine learning is loopy belief propaga-
tion (Pearl, 1988). Another solution (which is popular
in NLP) is to split the computation into two phases: in
the first phase, use some compatible objective function
f ? to produce a k-best list (the top k candidates under
f ?), which serves as an approximation to the full set.
Then, in the second phase, optimize f over all the anal-
yses in the k-best list. A typical example is discrimina-
tive reranking on k-best lists from a generative module,
such as (Collins, 2000) for parsing and (Shen et al, 2004)
for translation, where the reranking model has nonlocal
features that cannot be computed during parsing proper.
Another example is minimum-Bayes-risk decoding (Ku-
mar and Byrne, 2004; Goodman, 1998),where, assum-
ing f ? defines a probability distribution over all candi-
dates, one seeks the candidate with the highest expected
score according to an arbitrary metric (e.g., PARSEVAL
or BLEU); since in general the metric will not be com-
patible with the parsing algorithm, the k-best lists can
be used to approximate the full distribution f ?. A simi-
lar situation occurs when the parser can produce multiple
derivations that are regarded as equivalent (e.g., multiple
lexicalized parse trees corresponding to the same unlexi-
calized parse tree); if we want the maximum a posteriori
parse, we have to sum over equivalent derivations. Again,
the equivalence relation will in general not be compati-
ble with the parsing algorithm, so the k-best lists can be
used to approximate f ?, as in Data Oriented Parsing (Bod,
2000) and in speech recognition (Mohri and Riley, 2002).
Another instance of this k-best approach is cascaded
optimization. NLP systems are often cascades of mod-
ules, where we want to optimize the modules? objective
functions jointly. However, often a module is incompati-
ble with the packed representation of the previous module
due to factors like non-local dependencies. So we might
want to postpone some disambiguation by propagating
k-best lists to subsequent phases, as in joint parsing and
semantic role labeling (Gildea and Jurafsky, 2002; Sutton
and McCallum, 2005), information extraction and coref-
erence resolution (Wellner et al, 2004), and formal se-
mantics of TAG (Joshi and Vijay-Shanker, 1999).
Moreover, much recent work on discriminative train-
ing uses k-best lists; they are sometimes used to ap-
proximate the normalization constant or partition func-
tion (which would otherwise be intractable), or to train a
model by optimizing some metric incompatible with the
packed representation. For example, Och (2003) shows
how to train a log-linear translation model not by max-
imizing the likelihood of training data, but maximizing
the BLEU score (among other metrics) of the model on
53
the data. Similarly, Chiang (2005) uses the k-best pars-
ing algorithm described below in a CFG-based log-linear
translation model in order to learn feature weights which
maximize BLEU.
For algorithms whose packed representations are
graphs, such as Hidden Markov Models and other finite-
state methods, Ratnaparkhi?s MXPARSE parser (Ratna-
parkhi, 1997), and many stack-based machine transla-
tion decoders (Brown et al, 1995; Och and Ney, 2004),
the k-best paths problem is well-studied in both pure
algorithmic context (see (Eppstein, 2001) and (Brander
and Sinclair, 1995) for surveys) and NLP/Speech com-
munity (Mohri, 2002; Mohri and Riley, 2002). This pa-
per, however, aims at the k-best tree algorithms whose
packed representations are hypergraphs (Gallo et al,
1993; Klein and Manning, 2001) (equivalently, and/or
graphs or packed forests), which includes most parsers
and parsing-based MT decoders. Any algorithm express-
ible as a weighted deductive system (Shieber et al, 1995;
Goodman, 1999; Nederhof, 2003) falls into this class. In
our experiments, we apply the algorithms to the lexical-
ized PCFG parser of Bikel (2004), which is very similar
to Collins? Model 2 (Collins, 2003), and to a synchronous
CFG based machine translation system (Chiang, 2005).
2 Previous Work
As pointed out by Charniak and Johnson (2005), the ma-
jor difficulty in k-best parsing is dynamic programming.
The simplest method is to abandon dynamic program-
ming and rely on aggressive pruning to maintain tractabil-
ity, as is used in (Collins, 2000; Bikel, 2004). But this
approach is prohibitively slow, and produces rather low-
quality k-best lists (see Sec. 5.1.2). Gildea and Juraf-
sky (2002) described an O(k2)-overhead extension for the
CKY algorithm and reimplemented Collins? Model 1 to
obtain k-best parses with an average of 14.9 parses per
sentence. Their algorithm turns out to be a special case
of our Algorithm 0 (Sec. 4.1), and is reported to also be
prohibitively slow.
Since the original design of the algorithm described
below, we have become aware of two efforts that are
very closely related to ours, one by Jime?nez and Marzal
(2000) and another done in parallel to ours by Charniak
and Johnson (2005). Jime?nez and Marzal present an al-
gorithm very similar to our Algorithm 3 (Sec. 4.4) while
Charniak and Johnson propose using an algorithm similar
to our Algorithm 0, but with multiple passes to improve
efficiency. They apply this method to the Charniak (2000)
parser to get 50-best lists for reranking, yielding an im-
provement in parsing accuracy.
Our work differs from Jime?nez and Marzal?s in the
following three respects. First, we formulate the pars-
ing problem in the more general framework of hyper-
graphs (Klein and Manning, 2001), making it applica-
ble to a very wide variety of parsing algorithms, whereas
Jime?nez and Marzal define their algorithm as an exten-
sion of CKY, for CFGs in Chomsky Normal Form (CNF)
only. This generalization is not only of theoretical impor-
tance, but also critical in the application to state-of-the-
art parsers such as (Collins, 2003) and (Charniak, 2000).
In Collins? parsing model, for instance, the rules are dy-
namically generated and include unary productions, mak-
ing it very hard to convert to CNF by preprocessing,
whereas our algorithms can be applied directly to these
parsers. Second, our Algorithm 3 has an improvement
over Jime?nez and Marzal which leads to a slight theoret-
ical and empirical speedup. Third, we have implemented
our algorithms on top of state-of-the-art, large-scale sta-
tistical parser/decoders and report extensive experimental
results while Jime?nez and Marzal?s was tested on rela-
tively small grammars.
On the other hand, our algorithms are more scalable
and much more general than the coarse-to-fine approach
of Charniak and Johnson. In our experiments, we can ob-
tain 10000-best lists nearly as fast as 1-best parsing, with
very modest use of memory. Indeed, Charniak (p.c.) has
adopted our Algorithm 3 into his own parser implemen-
tation and confirmed our findings.
In the literature of k shortest-path problems, Minieka
(1974) generalized the Floyd algorithm in a way very
similar to our Algorithm 0 and Lawler (1977) improved
it using an idea similar to but a little slower than the bi-
nary branching case of our Algorithm 1. For hypergraphs,
Gallo et al (1993) study the shortest hyperpath problem
and Nielsen et al (2005) extend it to k shortest hyper-
path. Our work differes from (Nielsen et al, 2005) in two
aspects. First, we solve the problem of k-best derivations
(i.e., trees), not the k-best hyperpaths, although in many
cases they coincide (see Sec. 3 for further discussions).
Second, their work assumes non-negative costs (or prob-
abilities ? 1) so that they can apply Dijkstra-like algo-
rithms. Although generative models, being probability-
based, do not suffer from this problem, more general
models (e.g., log-linear models) may require negative
edge costs (McDonald et al, 2005; Taskar et al, 2004).
Our work, based on the Viterbi algorithm, is still appli-
cable as long as the hypergraph is acyclic, and is used by
McDonald et al (2005) to get the k-best parses.
3 Formulation
Following Klein and Manning (2001), we use weighted
directed hypergraphs (Gallo et al, 1993) as an abstraction
of the probabilistic parsing problem.
Definition 1. An ordered hypergraph (henceforth hy-
pergraph) H is a tuple ?V, E, t,R?, where V is a finite
set of vertices, E is a finite set of hyperarcs, and R
is the set of weights. Each hyperarc e ? E is a triple
54
e = ?T (e), h(e), f (e)?, where h(e) ? V is its head and
T (e) ? V? is a vector of tail nodes. f (e) is a weight func-
tion from R|T (e)| to R. t ? V is a distinguished vertex
called target vertex.
Note that our definition is different from those in previ-
ous work in the sense that the tails are now vectors rather
than sets, so that we can allow multiple occurrences of
the same vertex in a tail and there is an ordering among
the components of a tail.
Definition 2. A hypergraph H is said to be monotonic if
there is a total ordering ? on R such that every weight
function f in H is monotonic in each of its arguments ac-
cording to ?, i.e., if f : Rm 7? R, then ?1 ? i ? m, if ai ?
a?i , then f (a1, ? ? ? , ai, ? ? ? , am) ? f (a1, ? ? ? , a?i , ? ? ? , am).We also define the comparison function min?(a, b) to out-
put a if a ? b, or b if otherwise.
In this paper we will assume this monotonicity, which
corresponds to the optimal substructure property in dy-
namic programming (Cormen et al, 2001).
Definition 3. We denote |e| = |T (e)| to be the arity of the
hyperarc. If |e| = 0, then f (e) ? R is a constant and we
call h(e) a source vertex. We define the arity of a hyper-
graph to be the maximum arity of its hyperarcs.
Definition 4. The backward-star BS(v) of a vertex v is
the set of incoming hyperarcs {e ? E | h(e) = v}. The
in-degree of v is |BS (v)|.
Definition 5. A derivation D of a vertex v in a hyper-
graph H, its size |D| and its weight w(D) are recursively
defined as follows:
? If e ? BS (v) with |e| = 0, then D = ?e, ?? is
a derivation of v, its size |D| = 1, and its weight
w(D) = f (e)().
? If e ? BS (v) where |e| > 0 and Di is a derivation
of Ti(e) for 1 ? i ? |e|, then D = ?e,D1 ? ? ?D|e|? is
a derivation of v, its size |D| = 1 + ?|e|i=1 |Di| and itsweight w(D) = f (e)(w(D1), . . . ,w(D|e|)).
The ordering on weights in R induces an ordering on
derivations: D ? D? iff w(D) ? w(D?).
Definition 6. Define Di(v) to be the ith-best derivation of
v. We can think of D1(v), . . . ,Dk(v) as the components of
a vector we shall denote by D(v). The k-best derivations
problem for hypergraphs, then, is to find D(t) given a hy-
pergraph ?V, E, t,R?.
With the derivations thus ranked, we can introduce a
nonrecursive representation for derivations that is analo-
gous to the use of back-pointers in parser implementa-
tion.
Definition 7. A derivation with back-pointers (dbp) D?
of v is a tuple ?e, j? such that e ? BS(v), and j ?
{1, 2, . . . , k}|e|. There is a one-to-one correspondence ?
between dbps of v and derivations of v:
?e, ( j1 ? ? ? j|e|)? ? ?e,D j1 (T1(e)) ? ? ?D j|e| (T |e|(e))?
Accordingly, we extend the weight function w to dbps:
w(D?) = w(D) if D? ? D. This in turn induces an ordering
on dbps: D? ? D?? iff w(D?) ? w(D??). Let D?i(v) denote the
ith-best dbp of v.
Where no confusion will arise, we use the terms ?deriva-
tion? and ?dbp? interchangeably.
Computationally, then, the k-best problem can be
stated as follows: given a hypergraph H with arity a, com-
pute D?1(t), . . . , D?k(t).1
As shown by Klein and Manning (2001), hypergraphs
can be used to represent the search space of most parsers
(just as graphs, also known as trellises or lattices, can
represent the search space of finite-state automata or
HMMs). More generally, hypergraphs can be used to rep-
resent the search space of most weighted deductive sys-
tem (Nederhof, 2003). For example, the weighted CKY
algorithm given a context-free grammar G = ?N,T, P, S ?
in Chomsky Normal Form (CNF) and an input string w
can be represented as a hypergraph of arity 2 as follows.
Each item [X, i, j] is represented as a vertex v, corre-
sponding to the recognition of nonterminal X spanning
w from positions i+1 through j. For each production rule
X ? YZ in P and three free indices i < j < k, we have a
hyperarc ?((Y, i, k), (Z, k, j)), (X, i, k), f ? corresponding to
the instantiation of the inference rule C??????? in the de-
ductive system of (Shieber et al, 1995), and the weight
function f is defined as f (a, b) = ab ?Pr(X ? YZ), which
is the same as in (Nederhof, 2003). In this sense, hyper-
graphs can be thought of as compiled or instantiated ver-
sions of weighted deductive systems.
A parser does nothing more than traverse this hyper-
graph. In order that derivation values be computed cor-
rectly, however, we need to traverse the hypergraph in a
particular order:
Definition 8. The graph projection of a hypergraph H =
?V, E, t,R? is a directed graph G = ?V, E?? where E? =
{(u, v) | ?e ? BS (v), u ? T (e)}. A hypergraph H is said to
be acyclic if its graph projection G is a directed acyclic
graph; then a topological ordering of H is an ordering
of V that is a topological ordering in G (from sources to
target).
We assume the input hypergraph is acyclic so that we
can use its topological ordering to traverse it. In practice
the hypergraph is typically not known in advance, but the
1Note that although we have defined the weight of a deriva-
tion as a function on derivations, in practice one would store a
derivation?s weight inside the dbp itself, to avoid recomputing
it over and over.
55
p v
u t
q w
(a)
p v
u t
w
(b)
p u v
t
q u w
(c)
Figure 1: Examples of hypergraph, hyperpath, and derivation: (a) a hypergraph H, with t as the target vertex and p, q as
source vertices, (b) a hyperpath pit in H, and (c) a derivation of t in H, where vertex u appears twice with two different
(sub-)derivations. This would be impossible in a hyperpath.
topological ordering often is, so that the (dynamic) hy-
pergraph can be generated in that order. For example, for
CKY it is sufficient to generate all items [X, i, j] before all
items [Y, i?, j?] when j? ? i? > j ? i (X and Y are arbitrary
nonterminals).
Excursus: Derivations and Hyperpaths
The work of Klein and Manning (2001) introduces a cor-
respondence between hyperpaths and derivations. When
extended to the k-best case, however, that correspondence
no longer holds.
Definition 9. (Nielsen et al, 2005) Given a hypergraph
H = ?V, E, t,R?, a hyperpath piv of destination v ? V is an
acyclic minimal hypergraph Hpi = ?Vpi, Epi, v,R? such that
1. Epi ? E
2. v ? Vpi = ?e?Epi (T (e) ? {h(e)})
3. ?u ? Vpi, u is either a source vertex or connected to
a source vertex in Hpi.
As illustrated by Figure 1, derivations (as trees) are dif-
ferent from hyperpaths (as minimal hypergraphs) in the
sense that in a derivation the same vertex can appear more
than once with possibly different sub-derivations while it
is represented at most once in a hyperpath. Thus, the k-
best derivations problem we solve in this paper is very
different in nature from the k-shortest hyperpaths prob-
lem in (Nielsen et al, 2005).
However, the two problems do coincide when k = 1
(since all the sub-derivations must be optimal) and for
this reason the 1-best hyperpath algorithm in (Klein and
Manning, 2001) is very similar to the 1-best tree algo-
rithm in (Knuth, 1977). For k-best case (k > 1), they also
coincide when the hypergraph is isomorphic to a Case-
Factor Diagram (CFD) (McAllester et al, 2004) (proof
omitted). The derivation forest of CFG parsing under the
CKY algorithm, for instance, can be represented as a
CFD while the forest of Earley algorithm can not. An
(A? ?.B?, i, j)
(A? ?.B?, i, j)
(B? .?, j, j)
? ? ? ? ? ?
(B? ?., j, k)
(A? ?B.?, i, k)
Figure 2: An Earley derivation. Note that item (A ?
?.B?, i, j) appears twice (predict and complete).
1: procedure V??????(k)
2: for v ? V in topological order do
3: for e ? BS(v) do . for all incoming hyperarcs
4: D?1(v)? min?(D?1(v), ?e, 1?) . update
Figure 3: The generic 1-best Viterbi algorithm
item (or equivalently, a vertex in hypergraph) can appear
twice in an Earley derivation because of the prediction
rule (see Figure 2 for an example).
The k-best derivations problem has potentially more
applications in tree generation (Knight and Graehl,
2005), which can not be modeled by hyperpaths. But de-
tailed discussions along this line are out of the scope of
this paper.
4 Algorithms
The traditional 1-best Viterbi algorithm traverses the hy-
pergraph in topological order and for each vertex v, cal-
culates its 1-best derivation D1(v) using all incoming hy-
perarcs e ? BS(v) (see Figure 3). If we take the arity of
the hypergraph to be constant, then the overall time com-
plexity of this algorithm is O(|E|).
4.1 Algorithm 0: na??ve
Following (Goodman, 1999; Mohri, 2002), we isolate
two basic operations in line 4 of the 1-best algorithm that
56
can be generalized in order to extend the algorithm: first,
the formation of the derivation ?e, 1? out of |e| best sub-
derivations (this is a generalization of the binary operator
? in a semiring); second, min?, which chooses the better
of two derivations (same as the ? operator in an idem-
potent semiring (Mohri, 2002)). We now generalize these
two operations to operate on k-best lists.
Let r = |e|. The new multiplication operation,
mult?k(e), is performed in three steps:
1. enumerate the kr derivations {?e, j1 ? ? ? jr? | ?i, 1 ?
ji ? k}. Time: O(kr).
2. sort these kr derivations (according to weight).
Time: O(kr log(kr)) = O(rkr log k).
3. select the first k elements from the sorted list of kr
elements. Time: O(k).
So the overall time complexity of mult?k is O(rkr log k).
We also have to extend min? to merge?k, which takes
two vectors of length k (or fewer) as input and outputs the
top k (in sorted order) of the 2k elements. This is similar
to merge-sort (Cormen et al, 2001) and can be done in
linear time O(k). Then, we only need to rewrite line 4 of
the Viterbi algorithm (Figure 3) to extend it to the k-best
case:
4: D?(v) ? merge?k(D?(v),mult?k(e))
and the time complexity for this line is O(|e|k|e| log k),
making the overall complexity O(|E|ka log k) if we con-
sider the arity a of the hypergraph to be constant.2 The
overall space complexity is O(|V |k) since for each vertex
we need to store a vector of length k.
In the context of CKY parsing for CFG, the 1-best
Viterbi algorithm has complexity O(n3|P|) while the k-
best version is O(n3|P|k2 log k), which is slower by a fac-
tor of O(k2 log k).
4.2 Algorithm 1: speed up mult?k
First we seek to exploit the fact that input vectors are all
sorted and the function f is monotonic; moreover, we are
only interested in the top k elements of the k|e| possibili-
ties.
Define 1 to be the vector whose elements are all 1; de-
fine bi to be the vector whose elements are all 0 except
bii = 1.As we compute pe = mult?k(e), we maintain a candi-
date set C of derivations that have the potential to be the
next best derivation in the list. If we picture the input as an
|e|-dimensional space, C contains those derivations that
2Actually, we do not need to sort all k|e| elements in order
to extract the top k among them; there is an efficient algorithm
(Cormen et al, 2001) that can select the kth best element from
the k|e| elements in time O(k|e|). So we can improve the overhead
to O(ka).
have not yet been included in pe, but are on the bound-
ary with those which have. It is initialized to {?e, 1?}. At
each step, we extract the best derivation from C?call it
?e, j??and append it to pe. Then ?e, j? must be replaced
in C by its neighbors,
{?e, j + bl? | 1 ? l ? |e|}
(see Figure 4.2 for an illustration). We implement C as a
priority queue (Cormen et al, 2001) to make the extrac-
tion of its best derivation efficient. At each iteration, there
are one E??????-M?? and |e| I????? operations. If we use
a binary-heap implementation for priority queues, we get
O(|e| log k|e|) time complexity for each iteration.3 Since
we are only interested in the top k elements, there are
k iterations and the time complexity for a single mult?k
is O(k|e| log k|e|), yielding an overall time complexity of
O(|E|k log k) and reducing the multiplicative overhead by
a factor of O(ka?1) (again, assuming a is constant). In
the context of CKY parsing, this reduces the overhead
to O(k log k). Figure 5 shows the additional pseudocode
needed for this algorithm. It is integrated into the Viterbi
algorithm (Figure 3) simply by rewriting line 4 of to in-
voke the function M???(e, k):
4: D?(v) ? merge?k(D?(v),M???(e, k))
4.3 Algorithm 2: combine merge?k into mult?k
We can further speed up both merge?k and mult?k by a
similar idea. Instead of letting each mult?k generate a full
k derivations for each hyperarc e and only then applying
merge?k to the results, we can combine the candidate sets
for all the hyperarcs into a single candidate set. That is,
we initialize C to {?e, 1? | e ? BS (v)}, the set of all the
top parses from each incoming hyperarc (cf. Algorithm
1). Indeed, it suffices to keep only the top k out of the
|BS (v)| candidates in C, which would lead to a significant
speedup in the case where |BS (v)| ? k. 4 Now the top
derivation in C is the top derivation for v. Then, whenever
we remove an element ?e, j? from C, we replace it with
the |e| elements {?e, j + bl? | 1 ? l ? |e|} (again, as in
Algorithm 1). The full pseudocode for this algorithm is
shown in Figure 6.
4.4 Algorithm 3: compute mult?k lazily
Algorithm 2 exploited the idea of lazy computation: per-
forming mult?k only as many times as necessary. But this
algorithm still calculates a full k-best list for every ver-
tex in the hypergraph, whereas we are only interested in
3If we maintain a Min-Heap along with the Min-Heap, we
can reduce the per-iteration cost to O(|e| log k), and with Fi-
bonacci heap we can further improve it to be O(|e| + log k). But
these techniques do not change the overall complexity when a
is constant, as we will see.
4This can be implemented by a linear-time randomized-
selection algorithm (a.k.a. quick-select) (Cormen et al, 2001).
57
2
2 ?
0
?
?
?
?
1 ?
1 2 4
(a)
2
2 ?
?
?
?
3 ?
0 1
?
?
?
?
2 ?
1 2 4
(b)
2
2
?
?
?
?
3 ?
?
?
?
4
0 1 2 ?
?
?
?
4
1 2 4
(c)
Figure 4: An illustration of Algorithm 1 in |e| = 2 dimensions. Here k = 3, ? is the numerical ?, and the monotonic
function f is defined as f (a, b) = a + b. Italic numbers on the x and y axes are ai?s and b j?s, respectively. We want
to compute the top 3 results from f (ai, b j) with 1 ? i, j ? 3. In each iteration the current frontier is shown in oval
boxes, with the bold-face denoting the best element among them. That element will be extracted and replaced by its
two neighbors (? and?) in the next iteration.
1: function M???(e, k)
2: cand ? {?e, 1?} . initialize the heap
3: p? empty list . the result of mult?k
4: while |p| < k and |cand| > 0 do
5: A?????N???(cand,p, k)
6: return p
7:
8: procedure A?????N???(cand, p)
9: ?e, j? ? E??????-M??(cand)
10: append ?e, j? to p
11: for i? 1 . . . |e| do . add the |e| neighbors
12: j? ? j + bi
13: if j?i ? |D?(Ti(e))| and ?e, j?? < cand then
14: I?????(cand, ?e, j??) . add to heap
Figure 5: Part of Algorithm 1.
1: procedure F???A??KB???(k)
2: for v ? V in topological order do
3: F???KB???(v, k)
4:
5: procedure F???KB???(v, k)
6: G??C?????????(v, k) . initialize the heap
7: while |D?(v)| < k and |cand[v]| > 0 do
8: A?????N???(cand[v], D?(v))
9:
10: procedure G??C?????????(v, k)
11: temp? {?e, 1? | e ? BS (v)}
12: cand[v]? the top k elements in temp . prune
away useless candidates
13: H??????(cand[v])
Figure 6: Algorithm 2
1: procedure L???K??B???(v, k, k?) . k? is the global k
2: if |D?(v)| ? k then . kth derivation already computed?
3: return
4: if cand[v] is not defined then . first visit of vertex v?
5: G??C?????????(v, k?) . initialize the heap
6: append E??????-M??(cand[v]) to D?(v) . 1-best
7: while |D?(v)| < k and |cand[v]| > 0 do
8: ?e, j? ? D?|D?(v)|(v) . last derivation
9: L???N???(cand[v], e, j, k?) . update the heap, adding the successors of last derivation
10: append E??????-M??(cand[v]) to D?(v) . get the next best derivation and delete it from the heap
11:
12: procedure L???N???(cand, e, j, k?)
13: for i? 1 . . . |e| do . add the |e| neighbors
14: j? ? j + bi
15: L???K??B???(Ti(e), j?i , k?) . recursively solve a sub-problem
16: if j?i ? |D?(Ti(e))| and ?e, j?? < cand then . if it exists and is not in heap yet
17: I?????(cand, ?e, j??) . add to heap
Figure 7: Algorithm 3
58
Algorithm Time Complexity
1-best Viterbi O(E)
Algorithm 0 O(Eka log k)
Algorithm 1 O(Ek log k)
Algorithm 2 O(E + Vk log k)
Algorithm 3 O(E + |Dmax|k log k)
generalized J&M O(E + |Dmax|k log(d + k))
Table 1: Summary of Algorithms.
the k-best derivations of the target vertex (goal item). We
can therefore take laziness to an extreme by delaying the
whole k-best calculation until after parsing. Algorithm 3
assumes an initial parsing phase that generates the hyper-
graph and finds the 1-best derivation of each item; then
in the second phase, it proceeds as in Algorithm 2, but
starts at the goal item and calls itself recursively only as
necessary. The pseudocode for this algorithm is shown in
Figure 7. As a side note, this second phase should be ap-
plicable also to a cyclic hypergraph as long as its deriva-
tion weights are bounded.
Algorithm 2 has an overall complexity of O(|E| +
|V |k log k) and Algorithm 3 is O(|E|+ |Dmax|k log k) where
|Dmax| is the size of the longest among all top k deriva-
tions (for CFG in CNF, |D| = 2n?1 for all D, so |Dmax| is
O(n)). These are significant improvements against Algo-
rithms 0 and 1 since it turns the multiplicative overhead
into an additive overhead. In practice, |E| usually dom-
inates, as in CKY parsing of CFG. So theoretically the
running times grow very slowly as k increases, which is
exactly demonstrated by our experiments below.
4.5 Summary and Discussion of Algorithms
The four algorithms, along with the 1-best Viterbi algo-
rithm and the generalized Jime?nez and Marzal algorithm,
are compared in Table 1.
The key difference between our Algorithm 3 and
Jime?nez and Marzal?s algorithm is the restriction of top
k candidates before making heaps (line 11 in Figure 6,
see also Sec. 4.3). Without this line Algorithm 3 could
be considered as a generalization of the Jime?nez and
Marzal algorithm to the case of acyclic monotonic hy-
pergraphs. This line is also responsible for improving
the time complexity from O(|E| + |Dmax|k log(d + k))
(generalized Jime?nez and Marzal algorithm) to O(|E| +
|Dmax|k log k), where d = maxv |BS (v)| is the maximum
in-degree among all vertices. So in case k < d, our algo-
rithm outperforms Jime?nez and Marzal?s.
5 Experiments
We report results from two sets of experiments. For prob-
abilistic parsing, we implemented Algorithms 0, 1, and
3 on top of a widely-used parser (Bikel, 2004) and con-
ducted experiments on parsing efficiency and the qual-
ity of the k-best-lists. We also implemented Algorithms 2
and 3 in a parsing-based MT decoder (Chiang, 2005) and
report results on decoding speed.
5.1 Experiment 1: Bikel Parser
Bikel?s parser (2004) is a state-of-the-art multilingual
parser based on lexicalized context-free models (Collins,
2003; Eisner, 2000). It does support k-best parsing, but,
following Collins? parse-reranking work (Collins, 2000)
(see also Section 5.1.2), it accomplishes this by sim-
ply abandoning dynamic programming, i.e., no items
are considered equivalent (Charniak and Johnson, 2005).
Theoretically, the time complexity is exponential in n (the
input sentence length) and constant in k, since, without
merging of equivalent items, there is no limit on the num-
ber of items in the chart. In practice, beam search is used
to reduce the observed time.5 But with the standard beam
width of 10?4, this method becomes prohibitively expen-
sive for n ? 25 on Bikel?s parser. Collins (2000) used
a narrower 10?3 beam and further applied a cell limit of
100,6 but, as we will show below, this has a detrimental
effect on the quality of the output. We therefore omit this
method from our speed comparisons, and use our imple-
mentation of Algorithm 0 (na??ve) as the baseline.
We implemented our k-best Algorithms 0, 1, and 3 on
top of Bikel?s parser and conducted experiments on a 2.4
GHz 64-bit AMD Opteron with 32 GB memory. The pro-
gram is written in Java 1.5 running on the Sun JVM in
server mode with a maximum heap size of 5 GB. For this
experiment, we used sections 02?21 of the Penn Tree-
bank (PTB) (Marcus et al, 1993) as the training data and
section 23 (2416 sentences) for evaluation, as is now stan-
dard. We ran Bikel?s parser using its settings to emulate
Model 2 of (Collins, 2003).
5.1.1 Efficiency
We tested our algorithms under various conditions. We
first did a comparison of the average parsing time per
sentence of Algorithms 0, 1, and 3 on section 23, with
k ? 10000 for the standard beam of width 10?4. Fig-
ure 8(a) shows that the parsing speed of Algorithm 3 im-
proved dramatically against the other algorithms and is
nearly constant in k, which exactly matches the complex-
ity analysis. Algorithm 1 (k log k) also significantly out-
performs the baseline na??ve algorithm (k2 log k).
We also did a comparison between our Algorithm 3
and the Jime?nez and Marzal algorithm in terms of average
5In beam search, or threshold pruning, each cell in the chart
(typically containing all the items corresponding to a span [i, j])
is reduced by discarding all items that are worse than ? times the
score of the best item in the cell. This ? is known as the beam
width.
6In this type of pruning, also known as histogram pruning,
only the ? best items are kept in each cell. This ? is called the
cell limit.
59
 1.5
 2.5
 3.5
 4.5
 5.5
 6.5
 7.5
 1  10  100  1000  10000
Av
er
ag
e 
Pa
rs
in
g 
Ti
m
e 
(se
co
nd
s)
k
Algorithm 0
Algorithm 1
Algorithm 3
(a) Average parsing speed (Algs. 0 vs. 1 vs. 3, log-log)
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2  4  8  16  32  64
Av
er
ag
e 
He
ap
 S
ize
k
JM Algorithm with 10-5 beam
Algorithm 3 with 10-5 beam 
JM Algorithm with 10-4 beam
Algorithm 3 with 10-4 beam 
(b) Average heap size (Alg. 3 vs. Jime?nez and Marzal)
Figure 8: Efficiency results of the k-best Algorithms, compared to Jime?nez and Marzal?s algorithm
heap size. Figure 8(b) shows that for larger k, the two al-
gorithms have the same average heap size, but for smaller
k, our Algorithm 3 has a considerably smaller average
heap size. This difference is useful in applications where
only short k-best lists are needed. For example, McDon-
ald et al (2005) find that k = 5 gives optimal parsing
accuracy.
5.1.2 Accuracy
Our efficient k-best algorithms enable us to search over
a larger portion of the whole search space (e.g. by less
aggressive pruning), thus producing k-best lists with bet-
ter quality than previous methods. We demonstrate this
by comparing our k-best lists to those in (Ratnaparkhi,
1997), (Collins, 2000) and the parallel work by Char-
niak and Johnson (2005) in several ways, including oracle
reranking and average number of found parses.
Ratnaparkhi (1997) introduced the idea of oracle
reranking: suppose there exists a perfect reranking
scheme that magically picks the best parse that has the
highest F-score among the top k parses for each sentence.
Then the performance of this oracle reranking scheme
is the upper bound of any actual reranking system like
(Collins, 2000).As k increases, the F-score is nondecreas-
ing, and there is some k (which might be very large) at
which the F-score converges.
Ratnaparkhi reports experiments using oracle rerank-
ing with his statistical parser MXPARSE, which can
compute its k-best parses (in his experiments, k = 20).
Collins (2000), in his parse-reranking experiments, used
his Model 2 parser (Collins, 2003) with a beam width of
10?3 together with a cell limit of 100 to obtain k-best lists;
the average number of parses obtained per sentence was
29.2, the maximum, 101.7 Charniak and Johnson (2005)
use coarse-to-fine parsing on top of the Charniak (2000)
parser and get 50-best lists for section 23.
Figure 9(a) compares the results of oracle reranking.
Collins? curve converges at around k = 50 while ours
continues to increase. With a beam width of 10?4 and
k = 100, our parser plus oracle reaches an F-score of
96.4%, compared to Collins? 94.9%. Charniak and John-
son?s work, however, is based on a completely different
parser whose 1-best F-score is 1.5 points higher than the
1-bests of ours and Collins?, making it difficult to com-
pare in absolute numbers. So we instead compared the
relative improvement over 1-best. Figure 9(b) shows that
our work has the largest percentage of improvement in
terms of F-score when k > 20.
To further explore the impact of Collins? cell limit on
the quality of k-best lists, we plotted average number of
parses for a given sentence length (Figure 10). Generally
speaking, as input sentences get longer, the number of
parses grows (exponentially). But we see that the curve
for Collins? k-best list goes down for large k (> 40). We
suspect this is due to the cell limit of 100 pruning away
potentially good parses too early in the chart. As sen-
tences get longer, it is more likely that a lower-probability
parse might contribute eventually to the k-best parses. So
we infer that Collins? k-best lists have limited quality for
large k, and this is demonstrated by the early convergence
of its oracle-reranking score. By comparison, our curves
of both beam widths continue to grow with k = 100.
All these experiments suggest that our k-best parses are
of better quality than those from previous k-best parsers,
7The reason the maximum is 101 and not 100 is that Collins
merged the 100-best list using a beam of 10?3 with the 1-best
list using a beam of 10?4 (Collins, p.c.).
60
 86
 88
 90
 92
 94
 96
 98
 1  2  5  10  20  30  50  70  100
O
ra
cle
 F
-s
co
re
k
(Charniak and Johnson, 2005)
This work with beam width 10-4(Collins, 2000)
(Ratnaparkhi, 1997)
(a) Oracle Reranking
 0
 2
 4
 6
 8
 10
 1  2  5  10  20  30  50  70  100
Pe
rc
en
ta
ge
 o
f I
m
pr
ov
em
en
t o
ve
r 1
-b
es
t
k
(Charniak and Johnson, 2005)
This work with beam width 10-4(Collins, 2000)
(Ratnaparkhi, 1997)
(b) Relative Improvement
Figure 9: Absolutive and Relative F-scores of oracle reranking for the top k (? 100) parses for section 23, compared
to (Charniak and Johnson, 2005), (Collins, 2000) and (Ratnaparkhi, 1997).
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70
Av
er
ag
e 
Nu
m
be
r o
f P
ar
se
s
Sentence Length
This work with beam width 10-4
This work with beam width 10-3
(Collins, 2000) with beam width 10-3
Figure 10: Average number of parses for each sentence length in section 23, using k=100, with beam width 10?4 and
10?3, compared to (Collins, 2000).
61
 0.001
 0.01
 0.1
 1
 10
 10  100  1000  10000  100000  1e+06
s
e
c
o
n
ds
k
Algorithm 2
Algorithm 3
Figure 11: Algorithm 2 compared with Algorithm 3 (of-
fline) on MT decoding task. Average time (both exclud-
ing initial 1-best phase) vs. k (log-log).
and similar quality to those from (Charniak and Johnson,
2005) which has so far the highest F-score after rerank-
ing, and this might lead to better results in real parse
reranking.
5.2 Experiment 2: MT decoder
Our second experiment was on a CKY-based decoder
for a machine translation system (Chiang, 2005), imple-
mented in Python 2.4 accelerated with Psyco 1.3 (Rigo,
2004). We implemented Algorithms 2 and 3 to compute
k-best English translations of Mandarin sentences. Be-
cause the CFG used in this system is large to begin with
(millions of rules), and then effectively intersected with
a finite-state machine on the English side (the language
model), the grammar constant for this system is quite
large. The decoder uses a relatively narrow beam search
for efficiency.
We ran the decoder on a 2.8 GHz Xeon with 4 GB of
memory, on 331 sentences from the 2002 NIST MTEval
test set. We tested Algorithm 2 for k = 2i, 3 ? i ? 10, and
Algorithm 3 (offline algorithm) for k = 2i, 3 ? i ? 20.
For each sentence, we measured the time to calculate the
k-best list, not including the initial 1-best parsing phase.
We then averaged the times over our test set to produce
the graph of Figure 11, which shows that Algorithm 3
runs an average of about 300 times faster than Algorithm
2. Furthermore, we were able to test Algorithm 3 up to
k = 106 in a reasonable amount of time.8
8The curvature in the plot for Algorithm 3 for k < 1000
may be due to lack of resolution in the timing function for short
times.
6 Conclusion
The problem of k-best parsing and the effect of k-best list
size and quality on applications are subjects of increas-
ing interest for NLP research. We have presented here
a general-purpose algorithm for k-best parsing and ap-
plied it to two state-of-the-art, large-scale NLP systems:
Bikel?s implementation of Collins? lexicalized PCFG
model (Bikel, 2004; Collins, 2003) and Chiang?s syn-
chronous CFG based decoder (Chiang, 2005) for machine
translation. We hope that this work will encourage further
investigation into whether larger and better k-best lists
will improve performance in NLP applications, questions
which we ourselves intend to pursue as well.
Acknowledgements
We would like to thank one of the anonymous reviewers
of a previous version of this paper for pointing out the
work by Jime?nez and Marzal, and Eugene Charniak and
Mark Johnson for providing an early draft of their paper
and very useful comments. We are also extremely grate-
ful to Dan Bikel for the help in experiments, and Michael
Collins for providing the data in his paper. Our thanks
also go to Dan Gildea, Jonathan Graehl, Julia Hock-
enmaier, Aravind Joshi, Kevin Knight, Daniel Marcu,
Mitch Marcus, Ryan McDonald, Fernando Pereira, Gior-
gio Satta, Libin Shen, and Hao Zhang.
References
Bikel, D. M. (2004). Intricacies of Collins? parsing
model. Computational Linguistics, 30, 479?511.
Bod, R. (2000). Parsing with the shortest derivation. In
Proc. Eighteenth International Conference on Compu-
tational Linguistics (COLING), pages 69?75.
Brander, A. and Sinclair, M. (1995). A comparative
study of k-shortest path algorithms. In Proc. 11th UK
Performance Engineering Workshop for Computer and
Telecommunications Systems.
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra,
V. J., Jelinek, F., Lai, J. C., and Mercer, R. L. (1995).
Method and system for natural language translation.
U. S. Patent 5,477,451.
Charniak, E. (2000). A maximum-entropy-inspired
parser. In Proc. First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL), pages 132?139.
Charniak, E. and Johnson, M. (2005). Coarse-to-fine-
grained n-best parsing and discriminative reranking. In
Proc. ACL 2005.
Chiang, D. (2005). A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
62
Collins, M. (2000). Discriminative reranking for natural
language parsing. In Proc. Seventeenth International
Conference on Machine Learning (ICML), pages 175?
182. Morgan Kaufmann.
Collins, M. (2003). Head-driven statistical models for
natural language parsing. Computational Linguistics,
29, 589?637.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein,
C. (2001). Introduction to Algorithms. MIT Press, sec-
ond edition.
Eisner, J. (2000). Bilexical grammars and their cubic-
time parsing algorithms. In H. Bunt and A. Nijholt,
editors, Advances in Probabilistic and Other Parsing
Technologies, pages 29?62. Kluwer Academic Pub-
lishers.
Eppstein, D. (2001). Bibliography on k short-
est paths and other ?k best solutions? problems.
http://www.ics.uci.edu/?eppstein/bibs/kpath.bib.
Gallo, G., Longo, G., and Pallottino, S. (1993). Directed
hypergraphs and applications. Discrete Applied Math-
ematics, 42(2), 177?201.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28(3),
245?288.
Goodman, J. (1998). Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Goodman, J. (1999). Semiring parsing. Computational
Linguistics, 25, 573?605.
Jime?nez, V. M. and Marzal, A. (2000). Computation
of the n best parse trees for weighted and stochastic
context-free grammars. In Proc. of the Joint IAPR In-
ternational Workshops on Advances in Pattern Recog-
nition.
Joshi, A. K. and Vijay-Shanker, K. (1999). Composi-
tional semantics with lexicalized tree-adjoining gram-
mar (LTAG): How much underspecification is neces-
sary? In H. C. Bunt and E. G. C. Thijsse, editors, Proc.
IWCS-3, pages 131?145.
Klein, D. and Manning, C. D. (2001). Parsing and hy-
pergraphs. In Proceedings of the Seventh International
Workshop on Parsing Technologies (IWPT-2001), 17-
19 October 2001, Beijing, China. Tsinghua University
Press.
Knight, K. and Graehl, J. (2005). An overview of proba-
bilistic tree transducers for natural language process-
ing. In Proc. of the Sixth International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), LNCS.
Knuth, D. (1977). A generalization of Dijkstra?s algo-
rithm. Information Processing Letters, 6(1).
Kumar, S. and Byrne, W. (2004). Minimum bayes-risk
decoding for statistical machine translation. In HLT-
NAACL.
Lawler, E. L. (1977). Comment on computing the k short-
est paths in a graph. Comm. of the ACM, 20(8), 603?
604.
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A.
(1993). Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19,
313?330.
McAllester, D., Collins, M., and Pereira, F. (2004). Case-
factor diagrams for structured probabilistic modeling.
In Proc. UAI 2004.
McDonald, R., Crammer, K., and Pereira, F. (2005). On-
line large-margin training of dependency parsers. In
Proc. ACL 2005.
Minieka, E. (1974). On computing sets of shortest paths
in a graph. Comm. of the ACM, 17(6), 351?353.
Mohri, M. (2002). Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3), 321?350.
Mohri, M. and Riley, M. (2002). An efficient algorithm
for the n-best-strings problem. In Proceedings of the
International Conference on Spoken Language Pro-
cessing 2002 (ICSLP ?02), Denver, Colorado.
Nederhof, M.-J. (2003). Weighted deductive parsing and
Knuth?s algorithm. Computational Linguistics, pages
135?143.
Nielsen, L. R., Andersen, K. A., and Pretolani, D. (2005).
Finding the k shortest hyperpaths. Computers and Op-
erations Research.
Och, F. J. (2003). Minimum error rate training in statis-
tical machine translation. In Proc. ACL 2003, pages
160?167.
Och, F. J. and Ney, H. (2004). The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30, 417?449.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
Ratnaparkhi, A. (1997). A linear observed time statistical
parser based on maximum entropy models. In Proc.
EMNLP 1997, pages 1?10.
Rigo, A. (2004). Representation-based just-in-time spe-
cialization and the Psyco prototype for Python. In
N. Heintze and P. Sestoft, editors, Proceedings of the
2004 ACM SIGPLAN Workshop on Partial Evaluation
and Semantics-based Program Manipulation, pages
15?26.
63
Shen, L., Sarkar, A., and Och, F. J. (2004). Discrimina-
tive reranking for machine translation. In Proc. HLT-
NAACL 2004.
Shieber, S., Schabes, Y., and Pereira, F. (1995). Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24, 3?36.
Sutton, C. and McCallum, A. (2005). Joint parsing and
semantic role labeling. In Proc. CoNLL 2005.
Taskar, B., Klein, D., Collins, M., Koller, D., and Man-
ning, C. (2004). Max-margin parsing. In Proc. EMNLP
2004.
Wellner, B., McCallum, A., Peng, F., and Hay, M. (2004).
An integrated, conditional model of information ex-
traction and coreference with application to citation
matching. In Proc. UAI 2004.
64
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 940?948,
Beijing, August 2010
Fast, Greedy Model Minimization for Unsupervised Tagging
Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang
University of Southern California
Information Sciences Institute
{sravi,avaswani,knight,chiang}@isi.edu
Abstract
Model minimization has been shown to
work well for the task of unsupervised
part-of-speech tagging with a dictionary.
In (Ravi and Knight, 2009), the authors in-
voke an integer programming (IP) solver
to do model minimization. However,
solving this problem exactly using an
integer programming formulation is in-
tractable for practical purposes. We pro-
pose a novel two-stage greedy approxima-
tion scheme to replace the IP. Our method
runs fast, while yielding highly accurate
tagging results. We also compare our
method against standard EM training, and
show that we consistently obtain better
tagging accuracies on test data of varying
sizes for English and Italian.
1 Introduction
The task of unsupervised part-of-speech (POS)
tagging with a dictionary as formulated by Meri-
aldo (1994) is: given a raw word sequence and a
dictionary of legal POS tags for each word type,
tag each word token in the text. A common ap-
proach to modeling such sequence labeling prob-
lems is to build a bigram Hidden Markov Model
(HMM) parameterized by tag-bigram transition
probabilities P (ti|ti?1) and word-tag emission
probabilities P (wi|ti). Given a word sequence w
and a tag sequence t, of length N , the joint prob-
ability P (w, t) is given by:
P (w, t) =
N?
i=1
P (wi|ti) ? P (ti|ti?1) (1)
We can train this model using the Expectation
Maximization (EM) algorithm (Dempster and Ru-
bin, 1977) which learns P (wi|ti) and P (ti|ti?1)
that maximize the likelihood of the observed data.
Once the parameters are learnt, we can find the
best tagging using the Viterbi algorithm.
t? = arg max
t
P (w, t) (2)
Ravi and Knight (2009) attack the Merialdo
task in two stages. In the first stage, they search
for a minimized transition model (i.e., the small-
est set of tag bigrams) that can explain the data
using an integer programming (IP) formulation.
In the second stage, they build a smaller HMM
by restricting the transition parameters to only
those tag bigrams selected in the minimization
step. They employ the EM algorithm to train this
model, which prunes away some of the emission
parameters. Next, they use the pruned emission
model along with the original transition model
(which uses the full set of tag bigrams) and re-
train using EM. This alternating EM training pro-
cedure is repeated until the number of tag bigrams
in the Viterbi tagging output does not change be-
tween subsequent iterations. The final Viterbi tag-
ging output from their method achieves state-of-
the-art accuracy for this task. However, their mini-
mization step involves solving an integer program,
which can be very slow, especially when scal-
ing to large-scale data and more complex tagging
problems which use bigger tagsets. In this pa-
per, we present a novel method that optimizes the
same objective function using a fast greedy model
selection strategy. Our contributions are summa-
rized below:
940
? We present an efficient two-phase greedy-
selection method for solving the minimiza-
tion objective from Ravi and Knight (2009),
which runs much faster than their IP.
? Our method easily scales to large data
sizes (and big tagsets), unlike the previ-
ous minimization-based approaches and we
show runtime comparisons for different data
sizes.
? We achieve very high tagging accuracies
comparable to state-of-the-art results for un-
supervised POS tagging for English.
? Unlike previous approaches, we also show
results obtained when testing on the entire
Penn Treebank data (973k word tokens) in
addition to the standard 24k test data used for
this task. We also show the effectiveness of
this approach for Italian POS tagging.
2 Previous work
There has been much work on the unsupervised
part-of-speech tagging problem. Goldwater and
Griffiths (2007) also learn small models employ-
ing a fully Bayesian approach with sparse pri-
ors. They report 86.8% tagging accuracy with
manual hyperparameter selection. Smith and Eis-
ner (2005) design a contrastive estimation tech-
nique which yields a higher accuracy of 88.6%.
Goldberg et al (2008) use linguistic knowledge to
initialize the the parameters of the HMM model
prior to EM training. They achieve 91.4% ac-
curacy. Ravi and Knight (2009) use a Minimum
Description Length (MDL) method and achieve
the best results on this task thus far (91.6% word
token accuracy, 91.8% with random restarts for
EM). Our work follows a similar approach using a
model minimization component and alternate EM
training.
Recently, the integer programming framework
has been widely adopted by researchers to solve
other NLP tasks besides POS tagging such as se-
mantic role labeling (Punyakanok et al, 2004),
sentence compression (Clarke and Lapata, 2008),
decipherment (Ravi and Knight, 2008) and depen-
dency parsing (Martins et al, 2009).
3 Model minimization formulated as a
Path Problem
The complexity of the model minimization step
in (Ravi and Knight, 2009) and its proposed ap-
proximate solution can be best understood if we
formulate it as a path problem in a graph.
Let w = w0, w1, . . . , wN , wN+1 be a word se-
quence where w1, . . . , wN are the input word to-
kens and {w0, wN+1} are the start/end tokens.
Let T = {T1, . . . , TK}?{T0, TK+1} be the fixed
set of all possible tags. T0 and TK+1 are special
tags that we add for convenience. These would be
the start and end tags that one typically adds to
the HMM lattice. The tag dictionary D contains
entries of the form (wi, Tj) for all the possible
tags Tj that word token wi can have. We add en-
tries (w0, T0) and (wK+1, TK+1) to D. Given this
input, we now create a directed graph G(V,E).
Let C0, C1 . . . , CK+1 be columns of nodes in G,
where column Ci corresponds to word token wi.
For all i = 0, . . . , N+1 and j = 0, . . . ,K+1, we
add node Ci,j in column Ci if (wi, Tj) ? D. Now,
?i = 0, . . . , N , we create directed edges from ev-
ery node in Ci to every node in Ci+1. Each of
these edges e = (Ci,j , Ci+1,k) is given the label
(Tj , Tk) which corresponds to a tag bigram. This
creates our directed graph. Let l(e) be the tag bi-
gram label of edges e ? E. For every path P from
C0,0 to CN+1,K+1, we say that P uses an edge la-
bel or tag bigram (Tj , Tk) if there exists an edge
e in P such that l(e) = (Tj , Tk). We can now
formulate the the optimization problem as: Find
the smallest set S of tag bigrams such that there
exists at least one path from C0,0 to CN+1,K+1 us-
ing only the tag bigrams in S. Let us call this the
Minimal Tag Bigram Path (MinTagPath) problem.
Figure 1 shows an example graph where the
input word sequence is w1, . . . , w4 and T =
{T1, . . . , T3} is the input tagset. We add the
start/end word tokens {w0, w5} and correspond-
ing tags {T0, T4}. The edges in the graph are in-
stantiated according to the word/tag dictionary D
provided as input. The node and edge labels are
also illustrated in the graph. Our goal is to find a
path from C0,0 to C5,4 using the smallest set of tag
bigrams.
941
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
0
,T
3
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
2
T
3
,T
2
T
3
,T
4
T
2
,T
4
T
2
,T
3
T
2
,T
2
T
1
,T
3
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
Initial graph: G (V, E)
Figure 1: Graph instantiation for the MinTagPath problem.
4 Problem complexity
Having defined the problem, we now show that
it can be solved in polynomial time even though
the number of paths from C0,0 to CN+1,K+1 is
exponential in N , the input size. This relies on the
assumption that the tagset T is fixed in advance,
which is the case for most tagging tasks.1 Let B
be the set of all the tag bigram labels in the graph,
B = {l(e), ?e ? E}. Now, the size of B would
be at most K2 + 2K where every word could be
tagged with every possible tag. For m = 1 . . . |B|,
let Bm be the set of subsets of B each of which
have size m. Algorithm 1 optimally solves the
MinTagPath problem.
Algorithm 1 basically enumerates all the possi-
ble subsets of B, from the smallest to the largest,
and checks if there is a path. It exits the first time a
path is found and therefore finds the smallest pos-
sible set si of size m such that a path exists that
uses only the tag bigrams in si. This implies the
correctness of the algorithm. To check for path ex-
istence, we could either throw away all the edges
from E not having a label in si, and then execute
a Breadth-First-Search (BFS) or we could traverse
1If K, the size of the tagset, is a variable as well, then we
suspect the problem is NP-hard.
Algorithm 1 Brute Force solution to MinTagPath
for m = 1 to |B| do
for si ? Bm do
Use Breadth First Search (BFS) to check
if ? path P from C0,0 to CN+1,K+1 using
only the tag bigrams in si.
if P exists then
return si,m
end if
end for
end for
only the edges with labels in si during BFS. The
running time of Algorithm 1 is easy to calculate.
Since, in the worst case we go over all the sub-
sets of size m = 1, . . . , |B| of B, the number of
iterations we can perform is at most 2|B|, the size
of the powerset P of B. In each iteration, we do
a BFS through the lattice, which has O(N) time
complexity2 since the lattice size is linear in N
and BFS is linear in the lattice size. Hence the run-
ning time is? 2|B| ?O(N) = O(N). Even though
this shows that MinTagPath can be solved in poly-
nomial time, the time complexity is prohibitively
large. For the Penn Treebank, K = 45 and the
2Including throwing away edges or not.
942
worst case running time would be ? 1013.55 ? N .
Clearly, for all practical purposes, this approach is
intractable.
5 Greedy Model Minimization
We do not know of an efficient, exact algorithm
to solve the MinTagPath problem. Therefore, we
present a simple and fast two-stage greedy ap-
proximation scheme. Notice that an optimal path
P (or any path) covers all the input words i.e., ev-
ery word token wi has one of its possible taggings
in P . Exploiting this property, in the first phase,
we set our goal to cover all the word tokens using
the least possible number of tag bigrams. This can
be cast as a set cover problem (Garey and John-
son, 1979) and we use the set cover greedy ap-
proximation algorithm in this stage. The output
tag bigrams from this phase might still not allow
any path from C0,0 to CN+1,K+1. So we carry out
a second phase, where we greedily add a few tag
bigrams until a path is created.
5.1 Phase 1: Greedy Set Cover
In this phase, our goal is to cover all the word to-
kens using the least number of tag bigrams. The
covering problem is exactly that of set cover. Let
U = {w0, . . . , wN +1} be the set of elements that
needs to be covered (in this case, the word tokens).
For each tag bigram (Ti, Tj) ? B, we define its
corresponding covering set STi,Tj as follows:
STi,Tj = {wn : ((wn, Ti) ? D
? (Cn,i, Cn+1,j) ? E
? l(Cn,i, Cn+1,j) = (Ti, Tj))?
((wn, Tj) ? D
? (Cn?1,i, Cn,j) ? E
? l(Cn?1,i, Cn,j) = (Ti, Tj))}
Let the set of covering sets be X . We assign
a cost of 1 to each covering set in X . The goal
is to select a set CHOSEN ? X such that?
STi,Tj?CHOSEN
= U , minimizing the total cost
of CHOSEN . This corresponds to covering all
the words with the least possible number of tag
bigrams. We now use the greedy approximation
algorithm for set cover to solve this problem. The
pseudo code is shown in Algorithm 2.
Algorithm 2 Set Cover : Phase 1
Definitions
Define CAND : Set of candidate covering sets
in the current iteration
Define Urem : Number of elements in U re-
maining to be covered
Define ESTi,Tj : Current effective cost of a setDefine Itr : Iteration number
Initializations
LET CAND = X
LET CHOSEN = ?
LET Urem = U
LET Itr = 0
LET ESTi,Tj = 1|STi,Tj | , ? STi,Tj ? CAND
while Urem 6= ? do
Itr ? Itr + 1
Define S?Itr = argmin
STi,Tj?CAND
ESTi,Tj
CHOSEN = CHOSEN
?
S?Itr
Remove S?Itr from CAND
Remove all the current elements in S?Itr from
Urem
Remove all the current elements in S?Itr from
every STi,Tj ? CAND
Update effective costs, ? STi,Tj ? CAND,
ESTi,Tj =
1
|STi,Tj |end while
return CHOSEN
For the graph shown in Figure 1, here are a few
possible covering sets STi,Tj and their initial ef-
fective costs ESTi,Tj .
? ST0,T1 = {w0, w1}, EST0,T1 = 1/2
? ST1,T2 = {w1, w2, w3, w4}, EST1,T2 = 1/4
? ST2,T2 = {w2, w3, w4}, EST2,T2 = 1/3
In every iteration Itr of Algorithm 2, we pick a
set S?Itr that is most cost effective. The elements
that S?Itr covers are then removed from all the re-
maining candidate sets and Urem and the effec-
tiveness of the candidate sets is recalculated for
the next iteration. The algorithm stops when all
elements of U i.e., all the word tokens are cov-
ered. Let, BCHOSEN = {(Ti, Tj) : STi,Tj ?
943
CHOSEN}, be the set of tag bigrams that have
been chosen by set cover. Now, we check, using
BFS, if there exists a path from C0,0 to CN+1,K+1
using only the tag bigrams in BCHOSEN . If not,
then we have to add tag bigrams to BCHOSEN to
enable a path. To accomplish this, we carry out the
second phase of this scheme with another greedy
strategy (described in the next section).
For the example graph in Figure 1,
one possible solution BCHOSEN =
{(T0, T1), (T1, T2), (T2, T4)}.
5.2 Phase 2: Greedy Path Completion
We define a graph GCHOSEN (V ?, E?) ?
G(V,E) that contains the edges e ? E such
l(e) ? BCHOSEN .
Let BCAND = B \ BCHOSEN , be the current
set of candidate tag bigrams that can be added to
the final solution which would create a path. We
would like to know how many holes a particular
tag bigram (Ti, Tj) can fill. We define a hole as an
edge e such that e ? G \ GCHOSEN and there
exists e?, e?? ? GCHOSEN such that tail(e?) =
head(e) ? tail(e) = head(e??).
Figure 2 illustrates the graph GCHOSEN using
tag bigrams from the example solution to Phase 1
(Section 5.1). The dotted edge (C2,2, C3,1) rep-
resents a hole, which has to be filled in the cur-
rent phase in order to complete a path from C0,0
to C5,4.
In Algorithm 3, we define the effectiveness of a
candidate tag bigram H(Ti, Tj) to be the number
of holes it covers. In every iteration, we pick the
most effective tag bigram, fill the holes and recal-
culate the effectiveness of the remaining candidate
tag bigrams.
Algorithm 3 returns BFINAL, the final set of
chosen tag bigrams. It terminates when a path has
been found.
5.3 Fitting the Model
Once the greedy algorithm terminates and returns
a minimized grammar of tag bigrams, we follow
the approach of Ravi and Knight (2009) and fit
the minimized model to the data using the alter-
nating EM strategy. The alternating EM iterations
are terminated when the change in the size of the
observed grammar (i.e., the number of unique tag
Algorithm 3 Greedy Path Complete : Phase 2
Define BFINAL : Final set of tag bigrams se-
lected by the two-phase greedy approach
LET BFINAL = BCHOSEN
LET H(Ti, Tj) = |{e}| such that l(e) =
(Ti, Tj) and e is a hole, ? (Ti, Tj) ? BCAND
while @ path P from C0,0 to CN+1,K+1 using
only (Ti, Tj) ? BCHOSEN do
Define (T?i, T?j) = argmax
(Ti,Tj)?BCAND
H(Ti, Tj)
BFINAL = BFINAL
?
(T?i, T?j)
Remove (T?i, T?j) from BCAND
GCHOSEN = GCHOSEN
?{e} such that
l(e) = (Ti, Tj)
? (Ti, Tj) ? BCAND, Recalculate H(Ti, Tj)
end while
return BFINAL
bigrams in the tagging output) is ? 5%. We refer
to our entire approach using greedy minimization
followed by EM training as MIN-GREEDY.
6 Experiments and Results
6.1 English POS Tagging
Data: We use a standard test set (consisting of
24,115 word tokens from the Penn Treebank) for
the POS tagging task (described in Section 1). The
tagset consists of 45 distinct tag labels and the
dictionary contains 57,388 word/tag pairs derived
from the entire Penn Treebank. Per-token ambi-
guity for the test data is about 1.5 tags/token. In
addition to the standard 24k dataset, we also train
and test on larger data sets of 48k, 96k, 193k, and
the entire Penn Treebank (973k).
Methods: We perform comparative evaluations
for POS tagging using three different methods:
1. EM: Training a bigram HMM model using
EM algorithm.
2. IP: Minimizing grammar size using inte-
ger programming, followed by EM training
(Ravi and Knight, 2009).
3. MIN-GREEDY: Minimizing grammar size
using the Greedy method described in Sec-
944
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
4
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
 T
0
,T
1
 T
1
,T
2
 T
2
,T
4
Tag bigrams chosen after Phase 1 
(B
CHOSEN
)
Hole in graph: Edge e = (C
2,2
, C
3,1
)
Graph after Phase 1: G
CHOSEN 
(V?, E?)
Figure 2: Graph constructed with tag bigrams chosen in Phase 1 of the MIN-GREEDY method.
tion 5, followed by EM training.
Results: Figure 3 shows the tagging perfor-
mance (word token accuracy %) achieved by the
three methods on the standard test (24k tokens) as
well as Penn Treebank test (PTB = 973k tokens).
On the 24k test data, the MIN-GREEDY method
achieves a high tagging accuracy comparable to
the previous best from the IP method. However,
the IP method does not scale well which makes
it infeasible to run this method in a much larger
data setting (the entire Penn Treebank). MIN-
GREEDY on the other hand, faces no such prob-
lem and in fact it achieves high tagging accuracies
on all four datasets, consistently beating EM by
significant margins. When tagging all the 973k
word tokens in the Penn Treebank data, it pro-
duces an accuracy of 87.1% which is much better
than EM (82.3%) run on the same data.
Ravi and Knight (2009) mention that it is pos-
sible to interrupt the IP solver and obtain a sub-
optimal solution faster. However, the IP solver did
not return any solution when provided the same
amount of time as taken by MIN-GREEDY for
any of the data settings. Also, our algorithms
were implemented in Python while the IP method
employs the best available commercial software
package (CPLEX) for solving integer programs.
Figure 4 compares the running time efficiency
for the IP method versus MIN-GREEDY method
Test set Efficiency
(average running time in secs.)
IP MIN-GREEDY
24k test 93.0 34.3
48k test 111.7 64.3
96k test 397.8 93.3
193k test 2347.0 331.0
PTB (973k) test ? 1485.0
Figure 4: Comparison of MIN-GREEDY versus
MIN-GREEDY approach in terms of efficiency
(average running time in seconds) for different
data sizes. All the experiments were run on a sin-
gle machine with a 64-bit, 2.4 GHz AMD Opteron
850 processor.
as we scale to larger datasets. Since the IP solver
shows variations in running times for different
datasets of the same size, we show the average
running times for both methods (for each row in
the figure, we run a particular method on three
different datasets with similar sizes and average
the running times). The figure shows that the
greedy approach can scale comfortably to large
data sizes, and a complete run on the entire Penn
Treebank data finishes in just 1485 seconds. In
contrast, the IP method does not scale well?on
average, it takes 93 seconds to finish on the 24k
test (versus 34 seconds for MIN-GREEDY) and
on the larger PTB test data, the IP solver runs for
945
Method Tagging accuracy (%)
when training & testing on:
24k 48k 96k 193k PTB (973k)
EM 81.7 81.4 82.8 82.0 82.3
IP 91.6 89.3 89.5 91.6 ?
MIN-GREEDY 91.6 88.9 89.4 89.1 87.1
Figure 3: Comparison of tagging accuracies on test data of varying sizes for the task of unsupervised
English POS tagging with a dictionary using a 45-tagset. (? IP method does not scale to large data).
 400
 600
 800
 1000
 1200
 1400
 1600
Ob
se
rv
ed
 g
ra
mm
ar
 s
iz
e 
(#
 o
f 
ta
g 
bi
gr
am
s)
 
 i
n 
fi
na
l 
ta
gg
in
g 
ou
tp
ut
Size of test data (# of word tokens)
24k 48k 96k 193k PTB (973k)
EM
IP
Greedy
Figure 5: Comparison of observed grammar size
(# of tag bigram types) in the final tagging output
from EM, IP and MIN-GREEDY.
more than 3 hours without returning a solution.
It is interesting to see that for the 24k dataset,
the greedy strategy finds a grammar set (contain-
ing only 478 tag bigrams). We observe that MIN-
GREEDY produces 452 tag bigrams in the first
minimization step (phase 1), and phase 2 adds an-
other 26 entries, yielding a total of 478 tag bi-
grams in the final minimized grammar set. That
is almost as good as the optimal solution (459
tag bigrams from IP) for the same problem. But
MIN-GREEDY clearly has an advantage since it
runs much faster than IP (as shown in Figure 4).
Figure 5 shows a plot with the size of the ob-
served grammar (i.e., number of tag bigram types
in the final tagging output) versus the size of the
test data for EM, IP and MIN-GREEDY methods.
The figure shows that unlike EM, the other two
approaches reduce the grammar size considerably
and we observe the same trend even when scaling
Test set Average Speedup Optimality Ratio
24k test 2.7 0.96
48k test 1.7 0.98
96k test 4.3 0.98
193k test 7.1 0.93
Figure 6: Average speedup versus Optimality ra-
tio computed for the model minimization step
(when using MIN-GREEDY over IP) on different
datasets.
to larger data. Minimizing the grammar size helps
remove many spurious tag combinations from the
grammar set, thereby yielding huge improvements
in tagging accuracy over the EM method (Fig-
ure 3). We observe that for the 193k dataset, the
final observed grammar size is greater for IP than
MIN-GREEDY. This is because the alternating
EM steps following the model minimization step
add more tag bigrams to the grammar.
We compute the optimality ratio of the MIN-
GREEDY approach with respect to the grammar
size as follows:
Optimality ratio = Size of IP grammarSize of MIN-GREEDY grammar
A value of 1 for this ratio implies that the solu-
tion found by MIN-GREEDY algorithm is exact.
Figure 6 compares the optimality ratio versus av-
erage speedup (in terms of running time) achieved
in the minimization step for the two approaches.
The figure illustrates that our solution is nearly op-
timal for all data settings with significant speedup.
The MIN-GREEDY algorithm presented here
can also be applied to scenarios where the dictio-
nary is incomplete (i.e., entries for all word types
are not present in the dictionary) and rare words
946
Method Tagging accuracy (%) Number of unique tag bigrams in final tagging output
EM 83.4 1195
IP 88.0 875
MIN-GREEDY 88.0 880
Figure 7: Results for unsupervised Italian POS tagging with a dictionary using a set of 90 tags.
take on all tag labels. In such cases, we can fol-
low a similar approach as Ravi and Knight (2009)
to assign tag possibilities to every unknown word
using information from the known word/tag pairs
present in the dictionary. Once the completed dic-
tionary is available, we can use the procedure de-
scribed in Section 5 to minimize the size of the
grammar, followed by EM training.
6.2 Italian POS Tagging
We also compare the three approaches for Italian
POS tagging and show results.
Data: We use the Italian CCG-TUT corpus (Bos
et al, 2009), which contains 1837 sentences. It
has three sections: newspaper texts, civil code
texts and European law texts from the JRC-Acquis
Multilingual Parallel Corpus. For our experi-
ments, we use the POS-tagged data from the
CCG-TUT corpus, which uses a set of 90 tags.
We created a tag dictionary consisting of 8,733
word/tag pairs derived from the entire corpus
(42,100 word tokens). We then created a test set
consisting of 926 sentences (21,878 word tokens)
from the original corpus. The per-token ambiguity
for the test data is about 1.6 tags/token.
Results: Figure 7 shows the results on Italian
POS tagging. We observe that MIN-GREEDY
achieves significant improvements in tagging ac-
curacy over the EM method and comparable to IP
method. This also shows that the idea of model
minimization is a general-purpose technique for
such applications and provides good tagging ac-
curacies on other languages as well.
7 Conclusion
We present a fast and efficient two-stage greedy
minimization approach that can replace the inte-
ger programming step in (Ravi and Knight, 2009).
The greedy approach finds close-to-optimal solu-
tions for the minimization problem. Our algo-
rithm runs much faster and achieves accuracies
close to state-of-the-art. We also evaluate our
method on test sets of varying sizes and show that
our approach outperforms standard EM by a sig-
nificant margin. For future work, we would like
to incorporate some linguistic constraints within
the greedy method. For example, we can assign
higher costs to unlikely tag combinations (such as
?SYM SYM?, etc.).
Our greedy method can also be used for solving
other unsupervised tasks where model minimiza-
tion using integer programming has proven suc-
cessful, such as word alignment (Bodrumlu et al,
2009).
Acknowledgments
The authors would like to thank Shang-Hua Teng
and Anup Rao for their helpful comments and
also the anonymous reviewers. This work was
jointly supported by NSF grant IIS-0904684,
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
Bodrumlu, T., K. Knight, and S. Ravi. 2009. A new
objective function for word alignment. In Proceed-
ings of the NAACL/HLT Workshop on Integer Pro-
gramming for Natural Language Processing.
Bos, J., C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8).
Clarke, J. and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence
Research (JAIR), 31(4):399?429.
Dempster, A.P., N.M. Laird and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
947
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
Garey, M. R. and D. S. Johnson. 1979. Computers
and Intractability: A Guide to the Theory of NP-
Completeness. John Wiley & Sons.
Goldberg, Y., M. Adler, and M. Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL/HLT).
Goldwater, Sharon and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Martins, A., N. A. Smith, and E. P. Xing. 2009. Con-
cise integer linear programming formulations for
dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics (ACL) and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP.
Merialdo, B. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
Punyakanok, V., D. Roth, W. Yih, and D. Zimak.
2004. Semantic role labeling via integer linear pro-
gramming inference. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
Ravi, S. and K. Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Ravi, S. and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics (ACL) and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Smith, N. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL).
948
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387?1392,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Decoding with Large-Scale Neural Language Models
Improves Translation
Ashish Vaswani
University of Southern California
Department of Computer Science
avaswani@isi.edu
Yinggong Zhao
Nanjing University, State Key Laboratory
for Novel Software Technology
zhaoyg@nlp.nju.edu.cn
Victoria Fossum and David Chiang
University of Southern California
Information Sciences Institute
{vfossum,chiang}@isi.edu
Abstract
We explore the application of neural language
models to machine translation. We develop a
new model that combines the neural proba-
bilistic language model of Bengio et al, rec-
tified linear units, and noise-contrastive esti-
mation, and we incorporate it into a machine
translation system both by reranking k-best
lists and by direct integration into the decoder.
Our large-scale, large-vocabulary experiments
across four language pairs show that our neu-
ral language model improves translation qual-
ity by up to 1.1 Bleu.
1 Introduction
Machine translation (MT) systems rely upon lan-
guage models (LMs) during decoding to ensure flu-
ent output in the target language. Typically, these
LMs are n-gram models over discrete representa-
tions of words. Such models are susceptible to data
sparsity?that is, the probability of an n-gram ob-
served only few times is difficult to estimate reli-
ably, because these models do not use any informa-
tion about similarities between words.
To address this issue, Bengio et al (2003) pro-
pose distributed word representations, in which each
word is represented as a real-valued vector in a
high-dimensional feature space. Bengio et al (2003)
introduce a feed-forward neural probabilistic LM
(NPLM) that operates over these distributed repre-
sentations. During training, the NPLM learns both a
distributed representation for each word in the vo-
cabulary and an n-gram probability distribution over
words in terms of these distributed representations.
Although neural LMs have begun to rival or even
surpass traditional n-gram LMs (Mnih and Hin-
ton, 2009; Mikolov et al, 2011), they have not yet
been widely adopted in large-vocabulary applica-
tions such as MT, because standard maximum like-
lihood estimation (MLE) requires repeated summa-
tions over all words in the vocabulary. A variety of
strategies have been proposed to combat this issue,
many of which require severe restrictions on the size
of the network or the size of the data.
In this work, we extend the NPLM of Bengio et
al. (2003) in two ways. First, we use rectified lin-
ear units (Nair and Hinton, 2010), whose activa-
tions are cheaper to compute than sigmoid or tanh
units. There is also evidence that deep neural net-
works with rectified linear units can be trained suc-
cessfully without pre-training (Zeiler et al, 2013).
Second, we train using noise-contrastive estimation
or NCE (Gutmann and Hyva?rinen, 2010; Mnih and
Teh, 2012), which does not require repeated summa-
tions over the whole vocabulary. This enables us to
efficiently build NPLMs on a larger scale than would
be possible otherwise.
We then apply this LM to MT in two ways. First,
we use it to rerank the k-best output of a hierarchi-
cal phrase-based decoder (Chiang, 2007). Second,
we integrate it directly into the decoder, allowing the
neural LM to more strongly influence the model. We
achieve gains of up to 0.6 Bleu translating French,
German, and Spanish to English, and up to 1.1 Bleu
on Chinese-English translation.
1387
u1 u2
input
words
input
embeddings
hidden
h1
hidden
h2
output
P(w | u)
D?
M
C1 C2
D
Figure 1: Neural probabilistic language model (Bengio et
al., 2003).
2 Neural Language Models
Let V be the vocabulary, and n be the order of
the language model; let u range over contexts, i.e.,
strings of length (n?1), and w range over words. For
simplicity, we assume that the training data is a sin-
gle very long string, w1 ? ? ?wN , where wN is a special
stop symbol, </s>. We write ui for wi?n+1 ? ? ?wi?1,
where, for i ? 0, wi is a special start symbol, <s>.
2.1 Model
We use a feedforward neural network as shown in
Figure 1, following Bengio et al (2003). The input
to the network is a sequence of one-hot represen-
tations of the words in context u, which we write
u j (1 ? j ? n ? 1). The output is the probability
P(w | u) for each word w, which the network com-
putes as follows.
The hidden layers consist of rec-
tified linear units (Nair and Hinton,
2010), which use the activation func-
tion ?(x) = max(0, x) (see graph at
right).
The output of the first hidden layer h1 is
h1 = ?
?
????????
n?1?
j=1
C jDu j
?
????????
(1)
where D is a matrix of input word embeddings
which is shared across all positions, the C j are the
context matrices for each word in u, and ? is applied
elementwise. The output of the second layer h2 is
h2 = ? (Mh1) ,
where M is the matrix of connection weights be-
tween h1 and h2. Finally, the output layer is a soft-
max layer,
P(w | u) ? exp
(
D?h2 + b
)
(2)
where D? is the output word embedding matrix and b
is a vector of biases for every word in the vocabulary.
2.2 Training
The typical way to train neural LMs is to maximize
the likelihood of the training data by gradient ascent.
But the softmax layer requires, at each iteration, a
summation over all the units in the output layer, that
is, all words in the whole vocabulary. If the vocabu-
lary is large, this can be prohibitively expensive.
Noise-contrastive estimation or NCE (Gutmann
and Hyva?rinen, 2010) is an alternative estimation
principle that allows one to avoid these repeated
summations. It has been applied previously to log-
bilinear LMs (Mnih and Teh, 2012), and we apply it
here to the NPLM described above.
We can write the probability of a word w given a
context u under the NPLM as
P(w | u) =
1
Z(u)
p(w | u)
p(w | u) = exp
(
D?h2 + b
)
Z(u) =
?
w?
p(w? | u) (3)
where p(w | u) is the unnormalized output of the unit
corresponding to w, and Z(u) is the normalization
factor. Let ? stand for the parameters of the model.
One possibility would be to treat Z(u), instead of
being defined by (3), as an additional set of model
parameters which are learned along with ?. But it is
easy to see that we can make the likelihood arbitrar-
ily large by making the Z(u) arbitrarily small.
In NCE, we create a noise distribution q(w).
For each example uiwi, we add k noise samples
w?i1, . . . , w?ik into the data, and extend the model to
account for noise samples by introducing a random
1388
variable C which is 1 for training examples and 0 for
noise samples:
P(C = 1,w | u) =
1
1 + k
?
1
Z(u)
p(w | u)
P(C = 0,w | u) =
k
1 + k
? q(w).
We then train the model to classify examples as
training data or noise, that is, to maximize the con-
ditional likelihood,
L =
N?
i=1
(
log P(C = 1 | uiwi) +
k?
j=1
log P(C = 0 | uiw?i j)
)
with respect to both ? and Z(u).
We do this by stochastic gradient ascent. The gra-
dient with respect to ? turns out to be
?L
??
=
N?
i=1
(
P(C = 0 | uiwi)
?
??
log p(wi | ui) ?
k?
j=1
P(C = 1 | uiw?i j)
?
??
log p(w?i j | ui)
)
and similarly for the gradient with respect to Z(u).
These can be computed by backpropagation. Unlike
before, the Z(u) will converge to a value that normal-
izes the model, satisfying (3), and, under appropriate
conditions, the parameters will converge to a value
that maximizes the likelihood of the data.
3 Implementation
Both training and scoring of neural LMs are compu-
tationally expensive at the scale needed for machine
translation. In this section, we describe some of the
techniques used to make them practical for transla-
tion.
3.1 Training
During training, we compute gradients on an en-
tire minibatch at a time, allowing the use of matrix-
matrix multiplications instead of matrix-vector mul-
tiplications (Bengio, 2012). We represent the inputs
as a sparse matrix, allowing the computation of the
input layer (1) to use sparse matrix-matrix multi-
plications. The output activations (2) are computed
only for the word types that occur as the positive ex-
ample or one of the noise samples, yielding a sparse
matrix of outputs. Similarly, during backpropaga-
tion, sparse matrix multiplications are used at both
the output and input layer.
In most of these operations, the examples in a
minibatch can be processed in parallel. However, in
the sparse-dense products used when updating the
parameters D and D?, we found it was best to di-
vide the vocabulary into blocks (16 per thread) and
to process the blocks in parallel.
3.2 Translation
To incorporate this neural LM into a MT system, we
can use the LM to rerank k-best lists, as has been
done in previous work. But since the NPLM scores
n-grams, it can also be integrated into a phrase-based
or hierarchical phrase-based decoder just as a con-
ventional n-gram model can, unlike a RNN.
The most time-consuming step in computing n-
gram probabilities is the computation of the nor-
malization constants Z(u). Following Mnih and Teh
(2012), we set al the normalization constants to one
during training, so that the model learns to produce
approximately normalized probabilities. Then, when
applying the LM, we can simply ignore normaliza-
tion. A similar strategy was taken by Niehues and
Waibel (2012). We find that a single n-gram lookup
takes about 40 ?s.
The technique, described above, of grouping ex-
amples into minibatches works for scoring of k-best
lists, but not while decoding. But caching n-gram
probabilities helps to reduce the cost of the many
lookups required during decoding.
A final issue when decoding with a neural LM
is that, in order to estimate future costs, we need
to be able to estimate probabilities of n?-grams for
n? < n. In conventional LMs, this information is
readily available,1 but not in NPLMs. Therefore, we
defined a special word <null> whose embedding is
the weighted average of the (input) embeddings of
all the other words in the vocabulary. Then, to esti-
mate the probability of an n?-gram u?w, we used the
probability of P(w | <null>n?n
?
u?).
1However, in Kneser-Ney smoothed LMs, this information
is also incorrect (Heafield et al, 2012).
1389
setting dev 2004 2005 2006
baseline 38.2 38.4 37.7 34.3
reranking 38.5 38.6 37.8 34.7
decoding 39.1 39.5 38.8 34.9
Table 1: Results for Chinese-English experiments, with-
out neural LM (baseline) and with neural LM for rerank-
ing and integrated decoding. Reranking with the neural
LM improves translation quality, while integrating it into
the decoder improves even more.
4 Experiments
We ran experiments on four language pairs ? Chi-
nese to English and French, German, and Spanish
to English ? using a hierarchical phrase-based MT
system (Chiang, 2007) and GIZA++ (Och and Ney,
2003) for word alignments.
For all experiments, we used four LMs. The base-
lines used conventional 5-gram LMs, estimated with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the English side of the bitext and the
329M-word Xinhua portion of English Gigaword
(LDC2011T07). Against these baselines, we tested
systems that included the two conventional LMs as
well as two 5-gram NPLMs trained on the same
datasets. The Europarl bitext NPLMs had a vocab-
ulary size of 50k, while the other NPLMs had a vo-
cabulary size of 100k. We used 150 dimensions for
word embeddings, 750 units in hidden layer h1, and
150 units in hidden layer h2. We initialized the net-
work parameters uniformly from (?0.01, 0.01) and
the output biases to ? log |V |, and optimized them by
10 epochs of stochastic gradient ascent, using mini-
batches of size 1000 and a learning rate of 1. We
drew 100 noise samples per training example from
the unigram distribution, using the alias method for
efficiency (Kronmal and Peterson, 1979).
We trained the discriminative models with MERT
(Och, 2003) and the discriminative rerankers on
1000-best lists with MERT. Except where noted, we
ran MERT three times and report the average score.
We evaluated using case-insensitive NIST Bleu.
4.1 NIST Chinese-English
For the Chinese-English task (Table 1), the training
data came from the NIST 2012 constrained track,
excluding sentences longer than 60 words. Rules
Fr-En De-En Es-En
setting dev test dev test dev test
baseline 33.5 25.5 28.8 21.5 33.5 32.0
reranking 33.9 26.0 29.1 21.5 34.1 32.2
decoding 34.12 26.12 29.3 21.9 34.22 32.12
Table 2: Results for Europarl MT experiments, without
neural LM (baseline) and with neural LM for reranking
and integrated decoding. The neural LM gives improve-
ments across three different language pairs. Superscript 2
indicates a score averaged between two runs; all other
scores were averaged over three runs.
without nonterminals were extracted from all train-
ing data, while rules with nonterminals were ex-
tracted from the FBIS corpus (LDC2003E14). We
ran MERT on the development data, which was the
NIST 2003 test data, and tested on the NIST 2004?
2006 test data.
Reranking using the neural LM yielded improve-
ments of 0.2?0.4 Bleu, while integrating the neural
LM yielded larger improvements, between 0.6 and
1.1 Bleu.
4.2 Europarl
For French, German, and Spanish translation, we
used a parallel text of about 50M words from Eu-
roparl v7. Rules without nonterminals were ex-
tracted from all the data, while rules with nonter-
minals were extracted from the first 200k words. We
ran MERT on the development data, which was the
WMT 2005 test data, and tested on the WMT 2006
news commentary test data (nc-test2006).
The improvements, shown in Table 2, were more
modest than on Chinese-English. Reranking with
the neural LM yielded improvements of up to 0.5
Bleu, and integrating the neural LM into the decoder
yielded improvements of up to 0.6 Bleu. In one
case (Spanish-English), integrated decoding scored
higher than reranking on the development data but
lower on the test data ? perhaps due to the differ-
ence in domain between the two. On the other tasks,
integrated decoding outperformed reranking.
4.3 Speed comparison
We measured the speed of training a NPLM by NCE,
compared with MLE as implemented by the CSLM
toolkit (Schwenk, 2013). We used the first 200k
1390
10 20 30 40 50 60 70
0
1,
00
0
2,
00
0
3,
00
0
4,
00
0
Vocabulary size (?1000)
T
ra
in
in
g
tim
e
(s
)
CSLM
NCE k = 1000
NCE k = 100
NCE k = 10
Figure 2: Noise contrastive estimation (NCE) is much
faster, and much less dependent on vocabulary size, than
MLE as implemented by the CSLM toolkit (Schwenk,
2013).
lines (5.2M words) of the Xinhua portion of Giga-
word and timed one epoch of training, for various
values of k and |V |, on a dual hex-core 2.67 GHz
Xeon X5650 machine. For these experiments, we
used minibatches of 128 examples. The timings are
plotted in Figure 2. We see that NCE is considerably
faster than MLE; moreover, as expected, the MLE
training time is roughly linear in |V |, whereas the
NCE training time is basically constant.
5 Related Work
The problem of training with large vocabularies in
NPLMs has received much attention. One strategy
has been to restructure the network to be more hi-
erarchical (Morin and Bengio, 2005; Mnih and Hin-
ton, 2009) or to group words into classes (Le et al,
2011). Other strategies include restricting the vocab-
ulary of the NPLM to a shortlist and reverting to a
traditional n-gram LM for other words (Schwenk,
2004), and limiting the number of training examples
using resampling (Schwenk and Gauvain, 2005) or
selecting a subset of the training data (Schwenk et
al., 2012). Our approach can be efficiently applied
to large-scale tasks without limiting either the model
or the data.
NPLMs have previously been applied to MT, most
notably feed-forward NPLMs (Schwenk, 2007;
Schwenk, 2010) and RNN-LMs (Mikolov, 2012).
However, their use in MT has largely been limited
to reranking k-best lists for MT tasks with restricted
vocabularies. Niehues and Waibel (2012) integrate a
RBM-based language model directly into a decoder,
but they only train the RBM LM on a small amount
of data. To our knowledge, our approach is the first
to integrate a large-vocabulary NPLM directly into a
decoder for a large-scale MT task.
6 Conclusion
We introduced a new variant of NPLMs that com-
bines the network architecture of Bengio et al
(2003), rectified linear units (Nair and Hinton,
2010), and noise-contrastive estimation (Gutmann
and Hyva?rinen, 2010). This model is dramatically
faster to train than previous neural LMs, and can be
trained on a large corpus with a large vocabulary and
directly integrated into the decoder of a MT system.
Our experiments across four language pairs demon-
strated improvements of up to 1.1 Bleu. Code for
training and using our NPLMs is available for down-
load.2
Acknowledgements
We would like to thank the anonymous reviewers for
their very helpful comments. This research was sup-
ported in part by DOI IBC grant D12AP00225. This
work was done while the second author was visit-
ing USC/ISI supported by China Scholarship Coun-
cil. He was also supported by the Research Fund for
the Doctoral Program of Higher Education of China
(No. 20110091110003) and the National Fundamen-
tal Research Program of China (2010CB327903).
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research.
Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. CoRR,
abs/1206.5533.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
2http://nlg.isi.edu/software/nplm
1391
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Gutmann and Aapo Hyva?rinen. 2010. Noise-
contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of
AISTATS.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of EMNLP-CoNLL, pages 1169?1178.
Richard Kronmal and Arthur Peterson. 1979. On the
alias method for generating random variables from
a discrete distribution. The American Statistician,
33(4):214?218.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?
Burget, and Jan ?Honza? C?ernocky?. 2011. Em-
pirical evaluation and combination of advanced lan-
guage modeling techniques. In Proceedings of IN-
TERSPEECH, pages 605?608.
Toma?s? Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In Advances
in Neural Information Processing Systems.
Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical
probabilistic neural network language model. In Pro-
ceedings of AISTATS, pages 246?252.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified lin-
ear units improve restricted Boltzmann machines. In
Proceedings of ICML, pages 807?814.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using Restricted Boltzmann
Machines. In Proceedings of IWSLT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Holger Schwenk and Jean-Luc Gauvain. 2005. Training
neural network language models on very large corpora.
In Proceedings of EMNLP.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine trans-
lation. In Proceedings of the NAACL-HLT 2012 Work-
shop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling for HLT, pages
11?19.
Holger Schwenk. 2004. Efficient training of large neural
networks for language modeling. In Proceedings of
IJCNN, pages 3059?3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague Bul-
letin of Mathematical Linguistics, 93:137?146.
Holger Schwenk. 2013. CSLM - a modular open-source
continuous space language modeling toolkit. In Pro-
ceedings of Interspeech.
M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,
Q.V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean,
and G.E. Hinton. 2013. On rectified linear units for
speech processing. In Proceedings of ICASSP.
1392
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1840?1845,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Improving Word Alignment using Word Similarity
Theerawat Songyot
Dept of Computer Science
University of Southern California
songyot@usc.edu
David Chiang
?
Dept of Computer Science and Engineering
University of Notre Dame
dchiang@nd.edu
Abstract
We show that semantic relationships can
be used to improve word alignment, in ad-
dition to the lexical and syntactic features
that are typically used. In this paper, we
present a method based on a neural net-
work to automatically derive word simi-
larity from monolingual data. We present
an extension to word alignment models
that exploits word similarity. Our exper-
iments, in both large-scale and resource-
limited settings, show improvements in
word alignment tasks as well as translation
tasks.
1 Introduction
Word alignment is an essential step for learn-
ing translation rules in statistical machine trans-
lation. The task is to find word-level transla-
tion correspondences in parallel text. Formally,
given a source sentence e consisting of words
e
1
, e
2
, . . . , e
l
and a target sentence f consisting
of words f
1
, f
2
, . . . , f
m
, we want to infer an
alignment a, a sequence of indices a
1
, a
2
, . . . , a
m
which indicates, for each target word f
i
, the corre-
sponding source word e
a
i
or a null word. Machine
translation systems, including state-of-the-art sys-
tems, then use the word-aligned corpus to extract
translation rules.
The most widely used methods, the IBM mod-
els (Brown et al., 1993) and HMM (Vogel et al.,
1996), define a probability distribution p(f ,a | e)
that models how each target word f
i
is gener-
ated from a source word e
a
i
with respect to an
alignment a. The models, however, tend to mis-
align low-frequency words as they have insuffi-
cient training samples. The problem can get worse
in low-resource languages. Two branches of re-
search have tried to alleviate the problem. The
?
Most of the work reported here was performed while the
second author was at the University of Southern California.
first branch relies solely on the parallel data; how-
ever, additional assumptions about the data are re-
quired. This includes, but is not limited to, ap-
plying prior distributions (Mermer and Sarac?lar,
2011; Vaswani et al., 2012) or smoothing tech-
niques (Zhang and Chiang, 2014). The other
branch uses information learned from monolin-
gual data, which is generally easier to acquire than
parallel data. Previous work in this branch mostly
involves applying syntactic constraints (Yamada
and Knight, 2001; Cherry and Lin, 2006; Wang
and Zong, 2013) and syntactic features (Toutanova
et al., 2002) into the models. The use of syntac-
tic relationships can, however, be limited between
historically unrelated language pairs.
Our motivation lies in the fact that a meaningful
sentence is not merely a grammatically structured
sentence; its semantics can provide insightful in-
formation for the task. For example, suppose that
the models are uncertain about aligning e to f . If
the models are informed that e is semantically re-
lated to e
?
, f is semantically related to f
?
, and f
?
is
a translation of e
?
, it should intuitively increase the
probability that f is a translation of e. Our work
focuses on using such a semantic relationship, in
particular, word similarity, to improve word align-
ments.
In this paper, we propose a method to learn sim-
ilar words from monolingual data (Section 2) and
an extension to word alignment models in which
word similarity can be incorporated (Section 3).
We demonstrate its application in word alignment
and translation (Section 4) and then briefly discuss
the novelty of our work in comparison to other
methods (Section 5).
2 Learning word similarity
Given a word w, we want to learn a word simi-
larity model p(w
?
| w) of what words w
?
might
be used in place of w. Word similarity can be
used to improve word alignment, as in this pa-
1840
per, but can potentially be useful for other nat-
ural language processing tasks as well. Such a
model might be obtained from a monolingual the-
saurus, in which humans manually provide sub-
jective evaluation for word similarity probabilities,
but an automatic method would be preferable. In
this section, we present a direct formulation of the
word similarity model, which can automatically be
trained from monolingual data, and then consider
a more practical variant, which we adopt in our
experiments.
2.1 Model
Given an arbitrary word type w, we define a word
similarity model p(w
?
| w) for all word types w
?
in the vocabulary V as
p(w
?
| w) =
?
c
p(c | w) p(w
?
| c)
where c is a word context represented by a se-
quence w
1
, w
2
, . . . , w
2n
consisting of n word to-
kens on the left and n word tokens on the right
of w, excluding w. The submodel p(c | w) can
be a categorical distribution. However, modeling
the word context model, p(w
?
| c), as a categori-
cal distribution would cause severe overfitting, be-
cause the number of all possible contexts is |V |
2n
,
which is exponential in the length of the context.
We therefore parameterize it using a feedforward
neural network as shown in Figure 1, since the
structure has been shown to be effective for lan-
guage modeling (Bengio et al., 2006; Vaswani et
al., 2013). The input to the network is a one-hot
representation of each word in c, where the spe-
cial symbols <s>, </s>, <unk> are reserved for
sentence beginning, sentence ending, and words
not in the vocabulary. There is an output node
for each w
?
? V , whose activation is p(w
?
| c).
Following Bengio et al. (2006), the network uses
a shared linear projection matrix to the input em-
bedding layer, which allows information sharing
among the context words and also substantially
reduces the number of parameters. The input em-
bedding layer has a dimensionality of 150 for each
input word. The network uses two hidden layers
with 1,000 and 150 rectified linear units, respec-
tively, and a softmax output layer. We arbitrarily
use n = 5 throughout this paper.
2.2 Training
We extract training data by either collecting or
sampling the target words w ? V and their word
input
word
. . . . . .
w
1
w
2n
. . .
input
embeddings
. . . . . .
hidden layer 1
. . .
hidden layer 2
. . .
output
layer
. . .
Figure 1: The structure of the word context model
contexts from monolingual data. The submodel
p(c | w) can be independently trained easily by
maximum likelihood estimation, while the word
context model p(w
?
| c) may be difficult to train at
scale. We follow previous work (Mnih and Teh,
2012; Vaswani et al., 2013) in adopting noise-
contrastive estimation (Gutmann and Hyv?arinen,
2010), a fast and simple training algorithm that
scales independently of the vocabulary size.
2.3 Model variants
The above formulation of the word similarity
model can be interpreted as a mixture model in
which w
?
is similar to w if any of the context prob-
abilities agrees. However, to guard against false
positives, we can alternatively reformulate it as a
product of experts (Hinton, 1999),
p(w
?
| w) =
1
Z(w)
exp
?
c
p(c | w) log p(w
?
| c)
where Z(w) is a normalization constant. Under
this model, w
?
is similar to w if all of the context
probabilities agree. Both methods produce reason-
ably good word similarity; however, in practice,
the latter performs better.
Since most of the p(w
?
| w) will be close
to zero, for computational efficiency, we can se-
lect the k most similar words and renormalize
the probabilities. Table 1 shows some examples
learned from the 402M-word Xinhua portion of
the English Gigaword corpus (LDC2007T07), us-
ing a vocabulary V of the 30,000 most frequent
words. We set k = 5 for illustration purposes.
3 Word alignment model
In this section, we present our word alignment
models by extending the standard IBM models.
1841
p(w
?
| country) p(w
?
| region) p(w
?
| area)
country 0.8363 region 0.8338 area 0.8551
region 0.0558 area 0.0760 region 0.0524
nation 0.0522 country 0.0524 zone 0.0338
world 0.0282 province 0.0195 city 0.0326
city 0.0273 city 0.0181 areas 0.0258
Table 1: Examples of word similarity
The method can easily be applied to other related
models, for example, the log-linear reparameteri-
zation of Model 2 by Dyer et al. (2013). Basically,
all the IBM models involve modeling lexical trans-
lation probabilities p(f | e) which are parameter-
ized as categorical distributions. IBM Model 1, for
instance, is defined as
p(f ,a | e) ?
m
?
i=1
p(f
i
| e
a
i
) =
m
?
i=1
t(f
i
| e
a
i
)
where each t(f | e) denotes the model parameters
directly corresponding to p(f | e). Models 2?5
and the HMM-based model introduce additional
components in order to capture word ordering and
word fertility. However, they have p(f | e) in
common.
3.1 Model
To incorporate word similarity in word alignment
models, we redefine the lexical translation proba-
bilities as
p(f | e) =
?
e
?
,f
?
p(e
?
| e) t(f
?
| e
?
) p(f | f
?
)
for all f, e, including words not in the vocabulary.
While the factor p(e
?
| e) can be directly computed
by the word similarity model, the factor p(f | f
?
)
can be problematic because it vanishes for f out
of vocabulary. One possible solution would be to
use Bayes? rule
p(f | f
?
) =
p(f
?
| f) p(f)
p(f
?
)
where p(f
?
| f) is computed by the word similar-
ity model. However, we find that this is prone to
numerical instability and other complications. In
our experiments, we tried the simpler assumption
that p(f | f
?
) ? p(f
?
| f), with the rationale that
both probabilities are measures of word similarity,
which is intuitively a symmetric relation. We also
compared the performance of both methods. Ta-
ble 2 shows that this simple solution works as well
as the more exact method of using Bayes? rule. We
describe the experiment details in Section 4.
Model F1
BLEU
Test 1 Test 2
Chinese-English
Bayes? rule 75.7 30.0 27.0
Symmetry assumption 75.3 29.9 27.0
Arabic-English
Bayes? rule 70.4 37.9 36.7
Symmetry assumption 69.5 38.2 36.8
Table 2: Assuming that word similarity is sym-
metric, i.e. p(f | f
?
) ? p(f
?
| f), works as well
as computing p(f | f
?
) using Bayes? rule.
3.2 Re-estimating word similarity
Depending on the quality of word similarity and
the distribution of words in the parallel data, ap-
plying word similarity directly to the model could
lead to an undesirable effect where similar but not
interchangeable words rank in the top of the trans-
lation probabilities. On the other hand, if we set
p(e
?
| e) = 1[e
?
= e]
p(f
?
| f) = 1[f
?
= f ]
where 1 denotes the indicator function, the model
reduces to the standard IBM models. To get the
best of both worlds, we smooth the two models
together so that we rely more on word similarity
for rare words and less for frequent words
p?(w
?
| w) =
count(w)1[w
?
= w] + ?p(w
?
| w)
count(w) + ?
This can be thought of as similar to Witten-Bell
smoothing, or adding ? pseudocounts distributed
according to our p(w
?
| w). The hyperparame-
ter ? controls how much influence our word sim-
ilarity model has. We investigated the effect of ?
by varying this hyperparameter in our word align-
ment experiments whose details are described in
Section 4. Figure 2 shows that performance of the
model, as measured by F1 score, is rather insensi-
tive to the choice of ?. We used a value of 40 in
our experiments.
3.3 Training
Our word alignment models can be trained in the
same way as the IBM models using the Expec-
tation Maximization (EM) algorithm to maximize
the likelihood of the parallel data. Our extension
only introduces an additional time complexity on
the order of O(k
2
) on top of the base models,
where k is the number of word types used to es-
timate the full-vocabulary word similarity models.
1842
0 10 20 30 40 50 60
66
68
70
72
74
76
Value of ?
F
1
(
%
)
Chinese-English
Arabic-English
Figure 2: Alignment F1 is fairly insensitive to ?
over a large range of values
The larger the value of k is, the closer to the full-
vocabulary models our estimations are. In prac-
tice, a small value of k seems to be effective since
p(w
?
| w) is negligibly small for most w
?
.
4 Experiments
4.1 Alignment experiments
We conducted word alignment experiments
on 2 language pairs: Chinese-English and
Arabic-English. For Chinese-English, we used
9.5M+12.3M words of parallel text from the
NIST 2009 constrained task
1
and evaluated
on 39.6k+50.9k words of hand-aligned data
(LDC2010E63, LDC2010E37). For Arabic-
English, we used 4.2M+5.4M words of parallel
text from the NIST 2009 constrained task
2
and evaluated on 10.7k+15.1k words of hand-
aligned data (LDC2006E86). To demonstrate
performance under resource-limited settings,
we additionally experimented on only the first
eighth of the full data, specifically, 1.2M+1.6M
words for Chinese-English and 1.0M+1.4M
words for Arabic-English. We trained word
similarity models on the Xinhua portions of
English Gigaword (LDC2007T07), Chinese
Gigaword (LDC2007T38), and Arabic Gigaword
(LDC2011T1), which are 402M, 323M, and
125M words, respectively. The vocabulary V was
the 30,000 most frequent words from each corpus
1
Catalog numbers: LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,
LDC2006E85, LDC2006E86, LDC2006E92, and
LDC2006E93.
2
Excluding: United Nations proceedings (LDC2004E13),
ISI Automatically Extracted Parallel Text (LDC2007E08),
and Ummah newswire text (LDC2004T18)
1-10 11-20 21-30 31-40 41-50 51-60
30
40
50
60
70
80
38.1
52.4
56.9
55.4
59.5
60.1
58.5
63.7
66.3
66.4
70.5
71.5
Source word frequency
F
1
(
%
)
Baseline
Our model
Figure 3: F1 scores for words binned by fre-
quency. Our model gives the largest improvements
for the lowest-frequency words.
and the k = 10 most similar words were used.
We modified GIZA++ (Och and Ney, 2003) to
incorporate word similarity. For all experiments,
we used the default configuration of GIZA++: 5
iterations each of IBM Model 1, 2, HMM, 3 and
4. We aligned the parallel texts in both forward
and backward directions and symmetrized them
using grow-diag-final-and (Koehn et al., 2005).
We evaluated alignment quality using precision,
recall, and F1.
The results in Table 3 suggest that our modeling
approach produces better word alignments. We
found that our models not only learned smoother
translation models for low frequency words but
also ranked the conditional probabilities more ac-
curately with respect to the correct translations.
To illustrate this, we categorized the alignment
links from the Chinese-English low-resource ex-
periment into bins with respect to the English
source word frequency and individually evaluated
them. As shown in Figure 3, the gain for low fre-
quency words is particularly large.
4.2 Translation experiments
We also ran end-to-end translation experiments.
For both languages, we used subsets of the NIST
2004 and 2006 test sets as development data. We
used two different data sets as test data: different
subsets of the NIST 2004 and 2006 test sets (called
Test 1) and the NIST 2008 test sets (called Test 2).
We trained a 5-gram language model on the Xin-
hua portion of English Gigaword (LDC2007T07).
We used the Moses toolkit (Koehn et al., 2007) to
1843
Model Precision Recall F1
BLEU METEOR
Test 1 Test 2 Test 1 Test 2
Chinese-English
Baseline 65.2 76.9 70.6 29.4 26.7 29.7 28.5
Our model 71.4 79.7 75.3 29.9 27.0 30.0 28.8
Baseline (resource-limited) 56.1 68.1 61.5 23.6 20.3 26.0 24.4
Our model (resource-limited) 66.5 74.4 70.2 24.7 21.6 26.8 25.6
Arabic-English
Baseline 56.1 79.0 65.6 37.7 36.2 31.1 30.9
Our model 60.0 82.4 69.5 38.2 36.8 31.6 31.4
Baseline (resource-limited) 56.7 76.1 65.0 34.1 33.0 27.9 27.7
Our model (resource-limited) 59.4 80.7 68.4 35.0 33.8 28.7 28.6
Table 3: Experimental results. Our model improves alignments and translations on both language pairs.
build a hierarchical phrase-based translation sys-
tem (Chiang, 2007) trained using MIRA (Chiang,
2012). Then, we evaluated the translation qual-
ity using BLEU (Papineni et al., 2002) and ME-
TEOR (Denkowski and Lavie, 2014), and per-
formed significance testing using bootstrap resam-
pling (Koehn, 2004) with 1,000 samples.
Under the resource-limited settings, our meth-
ods consistently show 1.1?1.3 BLEU (0.8?1.2
METEOR) improvements on Chinese-English and
0.8?0.9 BLEU (0.8?0.9 METEOR) improvements
on Arabic-English, as shown in Table 3. These im-
provements are statistically significant (p < 0.01).
On the full data, our method improves Chinese-
English translation by 0.3?0.5 BLEU (0.3 ME-
TEOR), which is unfortunately not statistically
significant, and Arabic-English translation by 0.5?
0.6 BLEU (0.5 METEOR), which is statistically
significant (p < 0.01).
5 Related work
Most previous work on word alignment problems
uses morphosyntactic-semantic features, for ex-
ample, word stems, content words, orthography
(De Gispert et al., 2006; Hermjakob, 2009). A
variety of log-linear models have been proposed to
incorporate these features (Dyer et al., 2011; Berg-
Kirkpatrick et al., 2010). These approaches usu-
ally require numerical optimization for discrimi-
native training as well as language-specific engi-
neering and may limit their applications to mor-
phologically rich languages.
A more semantic approach resorts to training
word alignments on semantic word classes (Ma
et al., 2011). However, the resulting alignments
are only used to supplement the word alignments
learned on lexical words. To our knowledge, our
work, which directly incorporates semantic rela-
tionships in word alignment models, is novel.
6 Conclusion
We have presented methods to extract word simi-
larity from monolingual data and apply it to word
alignment models. Our method can learn simi-
lar words and word similarity probabilities, which
can be used inside any probability model and in
many natural language processing tasks. We have
demonstrated its effectiveness in statistical ma-
chine translation. The enhanced models can sig-
nificantly improve alignment quality as well as
translation quality.
Acknowledgments
We express our appreciation to Ashish Vaswani
for his advice and assistance. We also thank Hui
Zhang, Tomer Levinboim, Qing Dou, Aliya Deri
for helpful discussions and the anonymous review-
ers for their insightful critiques. This research was
supported in part by DOI/IBC grant D12AP00225
and a Google Research Award to Chiang.
References
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
HLT NAACL, pages 582?590.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
1844
Parameter estimation. Computational Linguistics,
19(2):263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntac-
tic constraints for word alignment through discrim-
inative training. In Proceedings of COLING/ACL,
pages 105?112.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 13(1):1159?1187.
Adri`a De Gispert, Deepa Gupta, Maja Popovi?c, Patrik
Lambert, Jose B. Mari?no, Marcello Federico, Her-
mann Ney, and Rafael Banchs. 2006. Improving
statistical word alignments with morpho-syntactic
transformations. In Advances in Natural Language
Processing, pages 368?379. Springer.
Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the EACL
2014 Workshop on Statistical Machine Translation.
Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised word alignment with
arbitrary features. In Proceedings of ACL: HLT, vol-
ume 1, pages 409?419.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proceedings of NAACL-
HLT, pages 644?648.
Michael Gutmann and Aapo Hyv?arinen. 2010. Noise-
contrastive estimation: A new estimation princi-
ple for unnormalized statistical models. In Inter-
national Conference on Artificial Intelligence and
Statistics (AI-STATS), pages 297?304.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In Proceedings of
EMNLP, volume 1, pages 229?237.
Geoffrey E. Hinton. 1999. Products of experts. In
International Conference on Artificial Neural Net-
works, volume 1, pages 1?6.
Philipp Koehn, Amittai Axelrod, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 IWSLT
speech translation evaluation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL: Interactive Poster and Demon-
stration Sessions, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Jeff Ma, Spyros Matsoukas, and Richard Schwartz.
2011. Improving low-resource statistical machine
translation with a novel semantic word clustering al-
gorithm. In Proceedings of MT Summit.
Cos?kun Mermer and Murat Sarac?lar. 2011. Bayesian
word alignment for statistical machine translation.
In Proceedings of ACL: HLT, volume 2, pages 182?
187.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of ICML.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to HMM-based sta-
tistical word alignment models. In Proceedings of
EMNLP, pages 87?94.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller alignment models for better trans-
lations: unsupervised word alignment with the `
0
-
norm. In Proceedings of ACL, volume 1, pages 311?
319.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proceedings of EMNLP.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of COLING, volume 2,
pages 836?841.
Zhiguo Wang and Chengqing Zong. 2013. Large-
scale word alignment using soft dependency cohe-
sion constraints. Transactions of the Association for
Computational Linguistics, 1(6):291?300.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of ACL, pages 523?530.
Hui Zhang and David Chiang. 2014. Kneser-Ney
smoothing on expected counts. In Proceedings of
ACL.
1845
Hierarchical Phrase-Based Translation
David Chiang?
Information Sciences Institute
University of Southern California
We present a statistical machine translation model that uses hierarchical phrases?phrases
that contain subphrases. The model is formally a synchronous context-free grammar but is
learned from a parallel text without any syntactic annotations. Thus it can be seen as combining
fundamental ideas from both syntax-based translation and phrase-based translation. We describe
our system?s training and decoding methods in detail, and evaluate it for translation speed and
translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system
performs significantly better than the Alignment Template System, a state-of-the-art phrase-
based system.
1. Introduction
The alignment template translation model (Och and Ney 2004) and related phrase-based
models advanced the state of the art in machine translation by expanding the basic
unit of translation from words to phrases, that is, substrings of potentially unlimited
size (but not necessarily phrases in any syntactic theory). These phrases allow a model
to learn local reorderings, translations of multiword expressions, or insertions and
deletions that are sensitive to local context. This makes them a simple and powerful
mechanism for translation.
The basic phrase-based model is an instance of the noisy-channel approach (Brown
et al 1993). Following convention, we call the source language ?French? and the target
language ?English?; the translation of a French sentence f into an English sentence e is
modeled as:
arg max
e
P(e | f ) = arg max
e
P(e, f ) (1)
= arg max
e
(P(e) ? P( f | e)) (2)
The phrase-based translation model P( f | e) ?encodes? e into f by the following steps:
1. segment e into phrases e?1 ? ? ? e?I, typically with a uniform distribution over
segmentations;
? 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292, USA. E-mail: chiang@isi.edu. Much of the
research presented here was carried out while the author was at the University of Maryland Institute for
Advanced Computer Studies.
Submission received: 1 May 2006; accepted for publication: 3 October 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 2
2. reorder the e?i according to some distortion model;
3. translate each of the e?i into French phrases according to a model P( f? | e?)
estimated from the training data.
Other phrase-based models model the joint distribution P(e, f ) (Marcu and Wong 2002)
or make P(e) and P( f | e) into features of a log-linear model (Och and Ney 2002). But
the basic architecture of phrase segmentation (or generation), phrase reordering, and
phrase translation remains the same.
Phrase-based models can robustly perform translations that are localized to sub-
strings that are common enough to have been observed in training. But Koehn, Och, and
Marcu (2003) find that phrases longer than three words improve performance little for
training corpora of up to 20 million words, suggesting that the data may be too sparse
to learn longer phrases. Above the phrase level, some models perform no reordering
(Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion
model that reorders phrases independently of their content (Koehn, Och, and Marcu
2003; Och and Ney 2004), and some, for example, the Alignment Template System
(Och et al 2004; Thayer et al 2004), hereafter ATS, and the IBM phrase-based system
(Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add
some lexical sensitivity. But, as an illustration of the limitations of phrase reordering,
consider the following Mandarin example and its English translation:
?2
Aozhou
Australia
/
shi
is
?
yu
with
?
Beihan
North Korea
	
you
have
??
bangjiao
dipl. rels.
?
de
that
p
shaoshu
few
??
guojia
countries
K 
zhiyi
one of

.
.
Australia is one of the few countries that have diplomatic relations with North Korea.
If we count zhiyi (literally, ?of-one?) as a single token, then translating this sentence
correctly into English requires identifying a sequence of five word groups that need
to be reversed. When we run a phrase-based system, ATS, on this sentence (using the
experimental setup described herein), we get the following phrases with translations:
[Aozhou] [shi]1 [yu Beihan]2 [you] [bangjiao] [de shaoshu guojia zhiyi] [.]
[Australia] [has] [dipl. rels.] [with North Korea]2 [is]1 [one of the few countries] [.]
where we have used subscripts to indicate the reordering of phrases. The phrase-based
model is able to order ?has diplomatic relations with North Korea? correctly (using
phrase reordering) and ?is one of the few countries? correctly (using a combination of
phrase translation and phrase reordering), but does not invert these two groups as it
should.
We propose a solution to these problems that does not interfere with the strengths
of the phrase-based approach, but rather capitalizes on them: Because phrases are good
for learning reorderings of words, we can use them to learn reorderings of phrases as
well. In order to do this we need hierarchical phrases that can contain other phrases.
For example, a hierarchical phrase pair that might help with the above example is
?yu 1 you 2 , have 2 with 1 ? (3)
where 1 and 2 are placeholders for subphrases (Chiang 2005). This would capture
the fact that Chinese prepositional phrases almost always modify verb phrases on the
202
Chiang Hierarchical Phrase-Based Translation
left, whereas English prepositional phrases usually modify verb phrases on the right.
Because it generalizes over possible prepositional objects and direct objects, it acts both
as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably
more powerful than a conventional phrase pair.
Similarly, the hierarchical phrase pair
? 1 de 2 , the 2 that 1 ? (4)
would capture the fact that Chinese relative clauses modify NPs on the left, whereas
English relative clauses modify on the right; and the pair
? 1 zhiyi, one of 1 ? (5)
would render the construction zhiyi in English word order. These three rules, along with
some conventional phrase pairs, suffice to translate the sentence correctly:
[Aozhou] [shi] [[[yu [Beihan]1 you [bangjiao]2] de [shaoshu guojia]3] zhiyi]
[Australia] [is] [one of [the [few countries]3 that [have [dipl. rels.]2 with [N. Korea]1]]]
The system we describe in this article uses rules like (3), (4), and (5), which we formalize
in the next section as rules of a synchronous context-free grammar (CFG).1 Moreover,
the system is able to learn them automatically from a parallel text without syntactic
annotation.
Because our system uses a synchronous CFG, it could be thought of as an example
of syntax-based statistical machine translation (MT), joining a line of research (Wu 1997;
Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful
but has not previously produced systems that can compete with phrase-based systems
in large-scale translation tasks such as the evaluations held by NIST. Our approach
differs from early syntax-based statistical translation models in combining the idea of
hierarchical structure with key insights from phrase-based MT: Crucially, by incorpo-
rating the use of elementary structures with possibly many words, we hope to inherit
phrase-based MT?s capacity for memorizing translations from parallel data. Other in-
sights borrowed from the current state of the art include minimum-error-rate training of
log-linear models (Och and Ney 2002; Och 2003) and use of an m-gram language model.
The conjunction of these various elements presents a considerable challenge for
implementation, which we discuss in detail in this article. The result is the first system
employing a grammar (to our knowledge) to perform better than phrase-based systems
in large-scale evaluations.2
1 The actual derivation used varies in practice. A previous version of the model selected precisely the
derivation shown in the text, although the version described in this article happens to select a less
intuitive one:
[Aozhou shi] [[[yu]1 Beihan [you [bangjiao]2 de [shaoshu]3 guojia]] zhiyi .]
[Australia is] [one of the [[[few]3 countries having [diplomatic relations]2] [with]1 North Korea] .]
2 An earlier version of the system described in this article was entered by the University of Maryland
as its primary system in the 2005 NIST MT Evaluation. The results can be found at
http://www.nist.gov/speech/tests/mt/mt05eval official results release 20050801 v3.html.
203
Computational Linguistics Volume 33, Number 2
2. Related Work
Approaches to syntax-based statistical MT have varied in their reliance on syntactic
theories, or annotations made according to syntactic theories. At one extreme are
those, exemplified by that of Wu (1997), that have no dependence on syntactic the-
ory beyond the idea that natural language is hierarchical. If these methods distin-
guish between different categories, they typically do not distinguish very many. Our
approach, as presented here, falls squarely into this family. By contrast, other ap-
proaches, exemplified by that of Yamada and Knight (2001), do make use of parallel
data with syntactic annotations, either in the form of phrase-structure trees or de-
pendency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because
syntactically annotated corpora are comparatively small, obtaining parsed parallel text
in quantity usually entails running an automatic parser on a parallel corpus to produce
noisy annotations.
Both of these strands of research have recently begun to explore extraction of
larger rules, guided by word alignments. The extraction method we use, which is a
straightforward generalization of phrase extraction from word-aligned parallel text, has
been independently proposed before in various settings. The method of Block (2000) is
the earliest instance we are aware of, though it is restricted to rules with one variable.
The same method has also been used by Probst et al (2002) and Xia and McCord (2004)
in conjunction with syntactic annotations to extract rules that are used for reordering
prior to translation. Finally, Galley et al (2004) use the same method to extract a very
large grammar from syntactically annotated data. The discontinuous phrases used by
Simard et al (2005) have a similar purpose to synchronous grammar rules; but they have
variables that stand for single words rather than subderivations, and they can interleave
in non-hierarchical ways.
3. Grammar
The model is based on a synchronous CFG, elsewhere known as a syntax-directed
transduction grammar (Lewis and Stearns 1968). We give here an informal definition
and then describe in detail how we build a synchronous CFG for our model.
3.1 Synchronous CFG
In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of
right-hand sides:
X ? ??,?,??
where X is a nonterminal, ? and ? are both strings of terminals and nonterminals,
and ? is a one-to-one correspondence between nonterminal occurrences in ? and
nonterminal occurrences in ?. For example, the hierarchical phrase pairs (3), (4), and
(5) previously presented could be formalized in a synchronous CFG as:
X ?
?
yu X 1 you X 2 , have X 2 with X 1
?
(6)
X ?
?
X 1 de X 2 , the X 2 that X 1
?
(7)
204
Chiang Hierarchical Phrase-Based Translation
X ?
?
X 1 zhiyi, one of X 1
?
(8)
where we have used boxed indices to indicate which nonterminal occurrences are linked
by ?. The conventional phrase pairs would be formalized as:
X ? ?Aozhou, Australia? (9)
X ? ?Beihan, North Korea? (10)
X ? ?shi, is? (11)
X ? ?bangjiao, diplomatic relations? (12)
X ? ?shaoshu guojia, few countries? (13)
Two more rules complete our example:
S ?
?
S 1 X 2 , S 1 X 2
?
(14)
S ?
?
X 1 , X 1
?
(15)
A synchronous CFG derivation begins with a pair of linked start symbols. At each step,
two linked nonterminals are rewritten using the two components of a single rule. When
denoting links with boxed indices, we must consistently reindex the newly introduced
symbols apart from the symbols already present. For an example using these rules, see
Figure 1.
3.2 Rule Extraction
The bulk of the grammar consists of automatically extracted rules. The extraction
process begins with a word-aligned corpus: a set of triples ? f, e,??, where f is a French
sentence, e is an English sentence, and ? is a (many-to-many) binary relation between
positions of f and positions of e. The word alignments are obtained by running GIZA++
(Och and Ney 2000) on the corpus in both directions, and forming the union of the two
sets of word alignments.
We then extract from each word-aligned sentence pair a set of rules that are
consistent with the word alignments. This can be thought of in two steps. First, we
identify initial phrase pairs using the same criterion as most phrase-based systems
(Och and Ney 2004), namely, there must be at least one word inside one phrase
aligned to a word inside the other, but no word inside one phrase can be aligned to
a word outside the other phrase. For example, suppose our training data contained the
fragment
30
30
30
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118?126,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Syntactic Alignment with Inversion Transduction Grammars
Adam Pauls Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
David Chiang Kevin Knight
Information Sciences Institute
University of Southern California
{chiang,knight}@isi.edu
Abstract
Syntactic machine translation systems cur-
rently use word alignments to infer syntactic
correspondences between the source and tar-
get languages. Instead, we propose an un-
supervised ITG alignment model that directly
aligns syntactic structures. Our model aligns
spans in a source sentence to nodes in a target
parse tree. We show that our model produces
syntactically consistent analyses where possi-
ble, while being robust in the face of syntactic
divergence. Alignment quality and end-to-end
translation experiments demonstrate that this
consistency yields higher quality alignments
than our baseline.
1 Introduction
Syntactic machine translation has advanced signif-
icantly in recent years, and multiple variants cur-
rently achieve state-of-the-art translation quality.
Many of these systems exploit linguistically-derived
syntactic information either on the target side (Gal-
ley et al, 2006), the source side (Huang et al, 2006),
or both (Liu et al, 2009). Still others induce their
syntax from the data (Chiang, 2005). Despite differ-
ences in detail, the vast majority of syntactic meth-
ods share a critical dependence on word alignments.
In particular, they infer syntactic correspondences
between the source and target languages through
word alignment patterns, sometimes in combination
with constraints from parser outputs.
However, word alignments are not perfect indi-
cators of syntactic alignment, and syntactic systems
are very sensitive to word alignment behavior. Even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, while
unaligned words can result in an exponentially large
set of extractable rules to choose from. Researchers
have worked to incorporate syntactic information
into word alignments, resulting in improvements to
both alignment quality (Cherry and Lin, 2006; DeN-
ero and Klein, 2007), and translation quality (May
and Knight, 2007; Fossum et al, 2008).
In this paper, we remove the dependence on word
alignments and instead directly model the syntactic
correspondences in the data, in a manner broadly
similar to Yamada and Knight (2001). In particu-
lar, we propose an unsupervised model that aligns
nodes of a parse tree (or forest) in one language to
spans of a sentence in another. Our model is an in-
stance of the inversion transduction grammar (ITG)
formalism (Wu, 1997), constrained in such a way
that one side of the synchronous derivation respects
a syntactic parse. Our model is best suited to sys-
tems which use source- or target-side trees only.
The design of our model is such that, for divergent
structures, a structurally integrated backoff to flatter
word-level (or null) analyses is available. There-
fore, our model is empirically robust to the case
where syntactic divergence between languages pre-
vents syntactically accurate ITG derivations.
We show that, with appropriate pruning, our
model can be efficiently trained on large parallel cor-
pora. When compared to standard word-alignment-
backed baselines, our model produces more con-
sistent analyses of parallel sentences, leading to
high-count, high-quality transfer rules. End-to-
end translation experiments demonstrate that these
higher quality rules improve translation quality by
1.0 BLEU over a word-alignment-backed baseline.
2 Syntactic Rule Extraction
Our model is intended for use in syntactic transla-
tion models which make use of syntactic parses on
either the target (Galley et al, 2006) or source side
(Huang et al, 2006; Liu et al, 2006). Our model?s
118
SNP
DT* NN NN
VP
VBZ
ADVP
RB VBN
the trade surplus has drastically fallen
??
??
???
 ??
 ?
trade
surplus
drastically
fall
(past)
Figure 1: A single incorrect alignment removes an ex-
tractable node, and hence several desirable rules. We
represent correct extractable nodes in bold, spurious ex-
tractable nodes with a *, and incorrectly blocked ex-
tractable nodes in bold strikethrough.
chief purpose is to align nodes in the syntactic parse
in one language to spans in the other ? an alignment
we will refer to as a ?syntactic? alignment. These
alignments are employed by standard syntactic rule
extraction algorithms, for example, the GHKM al-
gorithm of Galley et al (2004). Following that work,
we will assume parses are present in the target lan-
guage, though our model applies in either direction.
Currently, although syntactic systems make use of
syntactic alignments, these alignments must be in-
duced indirectly from word-level alignments. Pre-
vious work has discussed at length the poor interac-
tion of word-alignments with syntactic rule extrac-
tion (DeNero and Klein, 2007; Fossum et al, 2008).
For completeness, we provide a brief example of this
interaction, but for a more detailed discussion we re-
fer the reader to these presentations.
2.1 Interaction with Word Alignments
Syntactic systems begin rule extraction by first iden-
tifying, for each node in the target parse tree, a
span of the foreign sentence which (1) contains ev-
ery source word that aligns to a target word in the
yield of the node and (2) contains no source words
that align outside that yield. Only nodes for which
a non-empty span satisfying (1) and (2) exists may
form the root or leaf of a translation rule; for that
reason, we will refer to these nodes as extractable
nodes.
Since extractable nodes are inferred based on
word alignments, spurious word alignments can rule
out otherwise desirable extraction points. For exam-
ple, consider the alignment in Figure 1. This align-
ment, produced by GIZA++ (Och and Ney, 2003),
contains 4 correct alignments (the filled circles),
but incorrectly aligns the to the Chinese past tense
marker ? (the hollow circle). This mistaken align-
ment produces the incorrect rule (DT ? the ; ?),
and also blocks the extraction of (VBN ? fallen ;
???).
More high-level syntactic transfer rules are also
ruled out, for example, the ?the insertion rule? (NP
? the NN1 NN2 ; NN1 NN2) and the high-level (S
? NP1 VP2 ; NP1 VP2).
3 A Syntactic Alignment Model
The most common approach to avoiding these prob-
lems is to inject knowledge about syntactic con-
straints into a word alignment model (Cherry and
Lin, 2006; DeNero and Klein, 2007; Fossum et al,
2008).1 While syntactically aware, these models re-
main limited by the word alignment models that un-
derly them.
Here, we describe a model which directly infers
alignments of nodes in the target-language parse tree
to spans of the source sentence. Formally, our model
is an instance of a Synchronous Context-Free Gram-
mar (see Chiang (2004) for a review), or SCFG,
which generates an English (target) parse tree T and
foreign (source) sentence f given a target sentence e.
The generative process underlying this model pro-
duces a derivation d of SCFG rules, from which T
and f can be read off; because we condition on e,
the derivations produce e with probability 1. This
model places a distribution over T and f given by
p(T, f | e) =
?
d
p(d | e) =
?
d
?
r?d
p(r | e)
where the sum is over derivations d which yield T
and f . The SCFG rules r come from one of 4 types,
pictured in Table 1. In general, because our model
can generate English trees, it permits inference over
forests. Although we will restrict ourselves to a sin-
gle parse tree for our experiments, in this section, we
discuss the more general case.
1One notable exception is May and Knight (2007), who pro-
duces syntactic alignments using syntactic rules derived from
word-aligned data.
119
Rule Type Root English Foreign Example Instantiation
TERMINAL E e ft FOUR ? four ;?
UNARY A B fl B fr CD ? FOUR ;  FOUR ?
BINARYMONO A B C fl B fm C fr NP ? NN NN ;  NN ? NN 
BINARYINV A B C fl C fm B fr PP ? IN NP ;? NP  IN 
Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type.
Empty word sequences f have been explicitly marked with an .
The first rule type is the TERMINAL production,
which rewrites a terminal symbol2 E as its En-
glish word e and a (possibly empty) sequence of
foreign words ft. Generally speaking, the majority
of foreign words are generated using this rule. It
is only when a straightforward word-to-word corre-
spondence cannot be found that our model resorts to
generating foreign words elsewhere.
We can also rewrite a non-terminal symbol A us-
ing a UNARY production, which on the English side
produces a single symbol B, and on the foreign side
produces the symbol B, with sequences of words fl
to its left and fr to its right.
Finally, there are two binary productions: BINA-
RYMONO rewrites A with two non-terminals B and
C on the English side, and the same non-terminals
B and C in monotonic order on the foreign side,
with sequences of words fl, fr, and fm to the left,
right, and the middle. BINARYINV inverts the or-
der in which the non-terminals B and C are written
on the source side, allowing our model to capture a
large subset of possible reorderings (Wu, 1997).
Derivations from this model have two key prop-
erties: first, the English side of a derivation is con-
strained to form a valid constituency parse, as is re-
quired in a syntax system with target-side syntax;
and second, for each parse node in the English pro-
jection, there is exactly one (possibly empty) con-
tiguous span of the foreign side which was gener-
ated from that non-terminal or one of its descen-
dants. Identifying extractable nodes from a deriva-
tion is thus trivial: any node aligned to a non-empty
foreign span is extractable.
In Figure 2, we show a sample sentence pair frag-
2For notational convenience, we imagine that for each par-
ticular English word e, there is a special preterminal symbol E
which produces it. These symbols E act like any other non-
terminal in the grammar with respect to the parameterization in
Section 3.1. To denote standard non-terminals, we will use A,
B, and C.
PP[0,4]
IN[3,4]
NP[1,3]
DT[1,1]
NNS[1,3]
the[1,1]
elections[1,3]
? ??
?
??
at parliament election
before
before[3,4]
PP
NP IN
NNSDT
0 1 2 3 4
?
PP ? IN NP ; ? NP IN
NP ? DT NNS ; DT NNS
IN ? before ; before
before ? before ; ??
DT ? the ; the
the ? the ; !
NNS ? elections ; elections
elections ? elections ; ?? ??
Figure 2: Top: A synchronous derivation of a small sen-
tence pair fragment under our model. The English pro-
jection of the derivation represents a valid constituency
parse, while the foreign projection is less constrained.
We connect each foreign terminal with a dashed line to
the node in the English side of the synchronous deriva-
tion at which it is generated. The foreign span assigned
to each English node is indicated with indices. All nodes
with non-empty spans, shown in boldface, are extractable
nodes. Bottom: The SCFG rules used in the derivation.
ment as generated by our model. Our model cor-
rectly identifies that the English the aligns to nothing
on the foreign side. Our model also effectively cap-
tures the one-to-many alignment3 of elections to ?
3While our model does not explicitly produce many-to-one
alignments, many-to-one rules can be discovered via rule com-
position (Galley et al, 2006).
120
? ??. Finally, our model correctly analyzes the
Chinese circumposition ? . . .?? (before . . . ). In
this construction, ?? carries the meaning of ?be-
fore?, and thus correctly aligns to before, while ?
functions as a generic preposition, which our model
handles by attaching it to the PP. This analysis per-
mits the extraction of the general rule (PP ? IN1
NP2 ;? NP2 IN1), and the more lexicalized (PP?
before NP ;? NP??).
3.1 Parameterization
In principle, our model could have one parameter for
each instantiation r of a rule type. This model would
have an unmanageable number of parameters, pro-
ducing both computational and modeling issues ? it
is well known that unsupervised models with large
numbers of parameters are prone to degenerate anal-
yses of the data (DeNero et al, 2006). One solution
might be to apply an informed prior with a compu-
tationally tractable inference procedure (e.g. Cohn
and Blunsom (2009) or Liu and Gildea (2009)). We
opt here for the simpler, statistically more robust so-
lution of making independence assumptions to keep
the number of parameters at a reasonable level.
Concretely, we define the probability of the BI-
NARYMONO rule,4
p(r = A? B C; fl B fm C fr|A, eA)
which conditions on the root of the rule A and the
English yield eA, as
pg(A? B C | A, eA) ? pinv(I | B,C)?
pleft(fl | A, eA)?pmid(fm | A, eA)?pright(fr | A, eA)
In words, we assume that the rule probability de-
composes into a monolingual PCFG grammar prob-
ability pg, an inversion probability pinv, and a proba-
bility of left, middle, and right word sequences pleft,
pmid, and pright.5 Because we condition on e, the
monolingual grammar probability pg must form a
distribution which produces e with probability 1.6
4In the text, we only describe the factorization for the BI-
NARYMONO rule. For a parameterization of all rules, we refer
the reader to Table 2.
5All parameters in our model are multinomial distributions.
6A simple case of such a distribution is one which places all
of its mass on a single tree. More complex distributions can be
obtained by conditioning an arbitrary PCFG on e (Goodman,
1998).
We further assume that the probability of produc-
ing a foreign word sequence fl decomposes as:
pleft(fl | A, eA) = pl(|fl| = m | A)
m?
j=1
p(fj | A, eA)
where m is the length of the sequence fl. The pa-
rameter pl is a left length distribution. The prob-
abilities pmid, pright, decompose in the same way,
except substituting a separate length distribution pm
and pr for pl. For the TERMINAL rule, we emit ft
with a similarly decomposed distribution pterm us-
ing length distribution pw.
We define the probability of generating a foreign
word fj as
p(fj | A, eA) =
?
i?eA
1
| eA |
pt(fj | ei)
with i ? eA denoting an index ranging over the in-
dices of the English words contained in eA. The
reader may recognize the above expressions as the
probability assigned by IBM Model 1 (Brown et al,
1993) of generating the words fl given the words eA,
with one important difference ? the length m of the
foreign sentence is often not modeled, so the term
pl(|fl| = m | A) is set to a constant and ignored.
Parameterizing this length allows our model to ef-
fectively control the number of words produced at
different levels of the derivation.
It is worth noting how each parameter affects the
model?s behavior. The pt distribution is a standard
?translation? table, familiar from the IBM Models.
The pinv distribution is a ?distortion? parameter, and
models the likelihood of inverting non-terminals B
and C. This parameter can capture, for example,
the high likelihood that prepositions IN and noun
phrases NP often invert in Chinese due to its use
of postpositions. The non-terminal length distribu-
tions pl, pm, and pr model the probability of ?back-
ing off? and emitting foreign words at non-terminals
when a more refined analysis cannot be found. If
these parameters place high mass on 0 length word
sequences, this heavily penalizes this backoff be-
haviour. For the TERMINAL rule, the length distri-
bution pw parameterizes the number of words pro-
duced for a particular English word e, functioning
similarly to the ?fertilities? employed by IBM Mod-
els 3 and 4 (Brown et al, 1993). This allows us
121
to model, for example, the tendency of English de-
terminers the and a translate to nothing in the Chi-
nese, and of English names to align to multiple Chi-
nese words. In general, we expect an English word
to usually align to one Chinese word, and so we
place a weak Dirichlet prior on on the pe distribution
which puts extra mass on 1-length word sequences.
This is helpful for avoiding the ?garbage collection?
(Moore, 2004) problem for rare words.
3.2 Exploiting Non-Terminal Labels
There are often foreign words that do not correspond
well to any English word, which our model must
also handle. We elected for a simple augmentation
to our model to account for these words. When gen-
erating foreign word sequences f at a non-terminal
(i.e. via the UNARY or BINARY productions), we
also allow for the production of foreign words from
the non-terminal symbol A. We modify p(fj | eA)
from the previous section to allow production of fj
directly from the non-terminal7 A:
p(fj | eA) = pnt ? p(fj | A)
+ (1? pnt) ?
?
i?eA
1
|eA|
pt(fj | ei)
where pnt is a global binomial parameter which con-
trols how often such alignments are made.
This necessitates the inclusion of parameters like
pt(? | NP) into our translation table. Generally,
these parameters do not contain much information,
but rather function like a traditional NULL rooted
at some position in the tree. However, in some
cases, the particular annotation used by the Penn
Treebank (Marcus et al, 1993) (and hence most
parsers) allows for some interesting parameters to
be learned. For example, we found that our aligner
often matched the Chinese word ?, which marks
the past tense (among other things), to the preter-
minals VBD and VBN, which denote the English
simple past and perfect tense. Additionally, Chinese
measure words like ? and ? often align to the CD
(numeral) preterminal. These generalizations can be
quite useful ? where a particular number might pre-
dict a measure word quite poorly, the generalization
that measure words co-occur with the CD tag is very
robust.
7For terminal symbols E, this production is not possible.
3.3 Membership in ITG
The generative process which describes our model
contains a class of grammars larger than the com-
putationally efficient class of ITG grammars. For-
tunately, the parameterization described above not
only reduces the number of parameters to a man-
ageable level, but also introduces independence as-
sumptions which permit synchronous binarization
(Zhang et al, 2006) of our grammar. Any SCFG that
can be synchronously binarized is an ITG, meaning
that our parameterization permits efficient inference
algorithms which we will make use of in the next
section. Although several binarizations are possi-
ble, we give one such binarization and its associated
probabilities in Table 2.
3.4 Robustness to Syntactic Divergence
Generally speaking, ITG grammars have proven
more useful without the monolingual syntactic con-
straints imposed by a target parse tree. When deriva-
tions are restricted to respect a target-side parse tree,
many desirable alignments are ruled out when the
syntax of the two languages diverges, and align-
ment quality drops precipitously (Zhang and Gildea,
2004), though attempts have been made to address
this issue (Gildea, 2003).
Our model is designed to degrade gracefully in
the case of syntactic divergence. Because it can pro-
duce foreign words at any level of the derivation,
our model can effectively back off to a variant of
Model 1 in the case where an ITG derivation that
both respects the target parse tree and the desired
word-level alignments cannot be found.
For example, consider the sentence pair fragment
in Figure 3. It is not possible to produce an ITG
derivation of this fragment that both respects the
English tree and also aligns all foreign words to
their obvious English counterparts. Our model han-
dles this case by attaching the troublesome ?? at
the uppermost VP. This analysis captures 3 of the
4 word-level correspondences, and also permits ex-
traction of abstract rules like (S? NP VP ; NP VP)
and (NP? the NN ; NN).
Unfortunately, this analysis leaves the English
word tomorrow with an empty foreign span, permit-
ting extraction of the incorrect translation (VP ?
announced tomorrow ; ??), among others. Our
122
Rule Type Root English side Foreign side Probability
TERMINAL E e wt pterm(wt | E)
UNARY A Bu wl Bu pg(A ? B | A)pleft(wl | A, eA)
Bu B B wr pright(wr | A, eA)
BINARY A A1 wl A1 pleft(wl | A, eA)
A1 B C1 B C1 pg(A ? B C | A)pinv(I=false | B,C)
A1 B C1 C1 B pg(A ? B C | A)pinv(I=true | B,C)
C1 C2 fm C2 pmid(fm | A, eA)
C2 C C fr pright(fr | A, eA)
Table 2: A synchronous binarization of the SCFG describing our model.
S[0,4]
NP[3,4]
DT[3,3] NN[3,4]
VP[0,3]
VB[2,2]
VP[2,3]
VBN[2,3]
NN[3,3]
VP[2,3]
MD[1,2]
?? ? ?? ??
listannouncewilltomorrow0 1 2 3 4
the[3,3] list[3,4]
be[2,2]
announced[2,3] tomorrow[3,3]
will[1,2]
(a)
Figure 3: The graceful degradation of our model in the
face of syntactic divergence. It is not possible to align
all foreign words with their obvious English counterparts
with an ITG derivation. Instead, our model analyzes as
much as possible, but must resort to emitting ?? high
in the tree.
point here is not that our model?s analysis is ?cor-
rect?, but ?good enough? without resorting to more
computationally complicated models. In general,
our model follows an ?extract as much as possi-
ble? approach. We hypothesize that this approach
will capture important syntactic generalizations, but
it also risks including low-quality rules. It is an em-
pirical question whether this approach is effective,
and we investigate this issue further in Section 5.3.
There are possibilities for improving our model?s
treatment of syntactic divergence. One option is
to allow the model to select trees which are more
consistent with the alignment (Burkett et al, 2010),
which our model can do since it permits efficient in-
ference over forests. The second is to modify the
generative process slightly, perhaps by including the
?clone? operator of Gildea (2003).
4 Learning and Inference
4.1 Parameter Estimation
The parameters of our model can be efficiently
estimated in an unsupervised fashion using the
Expectation-Maximization (EM) algorithm. The E-
step requires the computation of expected counts un-
der our model for each multinomial parameter. We
omit the details of obtaining expected counts for
each distribution, since they can be obtained using
simple arithmetic from a single quantity, namely, the
expected count of a particular instantiation of a syn-
chronous rule r. This expectation is a standard quan-
tity that can be computed in O(n6) time using the
bitext Inside-Outside dynamic program (Wu, 1997).
4.2 Dynamic Program Pruning
While our model permits O(n6) inference over a
forest of English trees, inference over a full forest
would be very slow, and so we fix a single n-ary En-
glish tree obtained from a monolingual parser. How-
ever, it is worth noting that the English side of the
ITG derivation is not completely fixed. Where our
English trees are more than binary branching, we
permit any binarization in our dynamic program.
For efficiency, we also ruled out span alignments
that are extremely lopsided, for example, a 1-word
English span aligned to a 20-word foreign span.
Specifically, we pruned any span alignment in which
one side is more than 5 times larger than the other.
Finally, we employ pruning based on high-
precision alignments from simpler models (Cherry
and Lin, 2007; Haghighi et al, 2009). We com-
pute word-to-word alignments by finding all word
pairs which have a posterior of at least 0.7 according
to both forward and reverse IBM Model 1 parame-
ters, and prune any span pairs which invalidate more
than 3 of these alignments. In total, this pruning re-
123
Span P R F1
Syntactic Alignment 50.9 83.0 63.1
GIZA++ 56.1 67.3 61.2
Rule P R F1
Syntactic Alignment 39.6 40.3 39.9
GIZA++ 46.2 34.7 39.6
Table 3: Alignment quality results for our syntactic
aligner and our GIZA++ baseline.
duced computation from approximately 1.5 seconds
per sentence to about 0.3 seconds per sentence, a
speed-up of a factor of 5.
4.3 Decoding
Given a trained model, we extract a tree-to-string
alignment as follows: we compute, for each node
in the English tree, the posterior probability of a
particular foreign span assignment using the same
dynamic program needed for EM. We then com-
pute the set of span assignments which maximizes
the sum of these posteriors, constrained such that
the foreign span assignments nest in the obvious
way. This algorithm is a natural synchronous gener-
alization of the monolingual Maximum Constituents
Parse algorithm of Goodman (1996).
5 Experiments
5.1 Alignment Quality
We first evaluated our alignments against gold stan-
dard annotations. Our training data consisted of the
2261 manually aligned and translated sentences of
the Chinese Treebank (Bies et al, 2007) and approx-
imately half a million unlabeled sentences of parallel
Chinese-English newswire. The unlabeled data was
subsampled (Li et al, 2009) from a larger corpus by
selecting sentences which have good tune and test
set coverage, and limited to sentences of length at
most 40. We parsed the English side of the train-
ing data with the Berkeley parser.8 For our baseline
alignments, we used GIZA++, trained in the stan-
dard way.9 We used the grow-diag-final alignment
heuristic, as we found it outperformed union in early
experiments.
We trained our unsupervised syntactic aligner on
the concatenation of the labelled and unlabelled
8http://code.google.com/p/berkeleyparser/
95 iterations of model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4.
data. As is standard in unsupervised alignment mod-
els, we initialized the translation parameters pt by
first training 5 iterations of IBM Model 1 using the
joint training algorithm of Liang et al (2006), and
then trained our model for 5 EM iterations. We
extracted syntactic rules using a re-implementation
of the Galley et al (2006) algorithm from both our
syntactic alignments and the GIZA++ alignments.
We handle null-aligned words by extracting every
consistent derivation, and extracted composed rules
consisting of at most 3 minimal rules.
We evaluate our alignments against the gold stan-
dard in two ways. We calculated Span F-score,
which compares the set of extractable nodes paired
with a foreign span, and Rule F-score (Fossum et al,
2008) over minimal rules. The results are shown in
Table 3. By both measures, our syntactic aligner ef-
fectively trades recall for precision when compared
to our baseline, slightly increasing overall F-score.
5.2 Translation Quality
For our translation system, we used a re-
implementation of the syntactic system of Galley et
al. (2006). For the translation rules extracted from
our data, we computed standard features based on
relative frequency counts, and tuned their weights
using MERT (Och, 2003). We also included a
language model feature, using a 5-gram language
model trained on 220 million words of English text
using the SRILM Toolkit (Stolcke, 2002).
For tuning and test data, we used a subset of the
NIST MT04 and MT05 with sentences of length at
most 40. We used the first 1000 sentences of this set
for tuning and the remaining 642 sentences as test
data. We used the decoder described in DeNero et
al. (2009) during both tuning and testing.
We provide final tune and test set results in Ta-
ble 4. Our alignments produce a 1.0 BLEU improve-
ment over the baseline. Our reported syntactic re-
sults were obtained when rules were thresholded by
count; we discuss this in the next section.
5.3 Analysis
As discussed in Section 3.4, our aligner is designed
to extract many rules, which risks inadvertently ex-
tracting low-quality rules. To quantify this, we
first examined the number of rules extracted by our
aligner as compared with GIZA++. After relativiz-
124
Tune Test
Syntactic Alignment 29.78 29.83
GIZA++ 28.76 28.84
GIZA++ high count 25.51 25.38
Table 4: Final tune and test set results for our grammars
extracted using the baseline GIZA++ alignments and our
syntactic aligner. When we filter the GIZA++ grammars
with the same count thresholds used for our aligner (?high
count?), BLEU score drops substantially.
ing to the tune and test set, we extracted approx-
imately 32 million unique rules using our aligner,
but only 3 million with GIZA++. To check that
we were not just extracting extra low-count, low-
quality rules, we plotted the number of rules with
a particular count in Figure 4. We found that while
our aligner certainly extracts many more low-count
rules, it also extracts many more high-count rules.
Of course, high-count rules are not guaranteed
to be high quality. To verify that frequent rules
were better for translation, we experimented with
various methods of thresholding to remove rules
with low count extracted from using aligner. We
found in early development found that removing
low-count rules improved translation performance
substantially. In particular, we settled on the follow-
ing scheme: we kept all rules with a single foreign
terminal on the right-hand side. For entirely lexical
(gapless) rules, we kept all rules occurring at least
3 times. For unlexicalized rules, we kept all rules
occurring at least 20 times per gap. For rules which
mixed gaps and lexical items, we kept all rules oc-
curring at least 10 times per gap. This left us with
a grammar about 600 000 rules, the same grammar
which gave us our final results reported in Table 4.
In contrast to our syntactic aligner, rules extracted
using GIZA++ could not be so aggressively pruned.
When pruned using the same count thresholds, ac-
curacy dropped by more than 3.0 BLEU on the tune
set, and similarly on the test set (see Table 4). To
obtain the accuracy shown in our final results (our
best results with GIZA++), we had to adjust the
count threshold to include all lexicalized rules, all
unlexicalized rules, and mixed rules occurring at
least twice per gap. With these count thresholds, the
GIZA++ grammar contained about 580 000 rules,
roughly the same number as our syntactic grammar.
We also manually searched the grammars for
rules that had high count in the syntactically-
0 200 400 600 800 1000
1e+00
1e+02
1e+04
1e+06
Count
Numbe
r of rul
es with
 count SyntacticGIZA++
Figure 4: Number of extracted translation rules with a
particular count. Grammars extracted from our syntactic
aligner produce not only more low-count rules, but also
more high-count rules than GIZA++.
extracted grammar and low (or 0) count in the
GIZA++ grammar. Of course, we can always
cherry-pick such examples, but a few rules were il-
luminating. For example, for the ? . . .?? con-
struction discussed earlier, our aligner permits ex-
traction of the general rule (PP? IN1 NP2 ;? NP2
IN1) 3087 times, and the lexicalized rule (PP? be-
fore NP ; ? NP ??) 118 times. In constrast, the
GIZA++ grammar extracts the latter only 23 times
and the former not at all. The more complex rule
(NP? NP2 , who S1 , ; S1 ? NP2), which captures
a common appositive construction, was absent from
the GIZA++ grammar but occurred 63 in ours.
6 Conclusion
We have described a syntactic alignment model
which explicitly aligns nodes of a syntactic parse in
one language to spans in another, making it suitable
for use in many syntactic translation systems. Our
model is unsupervised and can be efficiently trained
with a straightforward application of EM. We have
demonstrated that our model can accurately capture
many syntactic correspondences, and is robust in the
face of syntactic divergence between language pairs.
Our aligner permits the extraction of more reliable,
high-count rules when compared to a standard word-
alignment baseline. These high-count rules also pro-
duce improvements in BLEU score.
Acknowledgements
This project is funded in part by the NSF under grant 0643742;
by BBN under DARPA contract HR0011-06-C-0022; and an
NSERC Postgraduate Fellowship. The authors would like to
thank Michael Auli for his input.
125
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007.
English chinese translation treebank v 1.0. web download.
In LDC2007T02.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammar. In
Proceedings of the North American Association for Compu-
tational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In Pro-
ceedings of the Association of Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion transduction
grammar for joint phrasal translation modeling. In Workshop
on Syntax and Structure in Statistical Translation.
David Chiang. 2004. Evaluating grammar formalisms for ap-
plications to natural language processing and biological se-
quence analysis. Ph.D. thesis, University of Pennsylvania.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In The Annual Conference of
the Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of
syntax-directed tree to string grammar induction. In Pro-
ceedings of the Conference on Emprical Methods for Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring word alignments
to syntactic machine translation. In The Annual Conference
of the Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Workshop on Statistical Machine Translation at
NAACL.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of NAACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Us-
ing syntax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proceed-
ings of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proceedings of the Association for Compu-
tational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Association for Computational Linguis-
tics.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, John Blitzer, John Denero, and Dan Klein.
2009. Better word alignments with supervised itg models.
In Proceedings of the Association for Computational Lin-
guistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of CHSLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch,
Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,
Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an
open source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2009. Bayesian learning of
phrasal tree-to-string templates. In Proceedings of EMNLP.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-
tree translation with packed forests. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
Jonathan May and Kevin Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of the Con-
ference on Emprical Methods for Natural Language Pro-
cessing.
Robert C. Moore. 2004. Improving ibm word alignment model
1. In The Annual Conference of the Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the Association
for Computational Linguistics.
Andreas Stolcke. 2002. SRILM: An extensible language mod-
eling toolkit. In ICSLP 2002.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the Association of
Computational Linguistics.
Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment:
supervised or unsupervised? In Proceedings of the Confer-
ence on Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of the North American Chapter of the Associa-
tion for Computational Linguistics.
126
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447?455,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Bayesian Inference for Finite-State Transducers?
David Chiang1 Jonathan Graehl1 Kevin Knight1 Adam Pauls2 Sujith Ravi1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
Abstract
We describe a Bayesian inference algorithm
that can be used to train any cascade of
weighted finite-state transducers on end-to-
end data. We also investigate the problem
of automatically selecting from among mul-
tiple training runs. Our experiments on four
different tasks demonstrate the genericity of
this framework, and, where applicable, large
improvements in performance over EM. We
also show, for unsupervised part-of-speech
tagging, that automatic run selection gives a
large improvement over previous Bayesian ap-
proaches.
1 Introduction
In this paper, we investigate Bayesian infer-
ence for weighted finite-state transducers (WFSTs).
Many natural language models can be captured
by weighted finite-state transducers (Pereira et al,
1994; Sproat et al, 1996; Knight and Al-Onaizan,
1998; Clark, 2002; Kolak et al, 2003; Mathias and
Byrne, 2006), which offer several benefits:
? WFSTs provide a uniform knowledge represen-
tation.
? Complex problems can be broken down into a
cascade of simple WFSTs.
? Input- and output-epsilon transitions allow
compact designs.
? Generic algorithms exist for doing inferences
with WFSTs. These include best-path de-
coding, k-best path extraction, composition,
?The authors are listed in alphabetical order. Please direct
correspondence to Sujith Ravi (sravi@isi.edu). This work
was supported by NSF grant IIS-0904684 and DARPA contract
HR0011-06-C0022.
intersection, minimization, determinization,
forward-backward training, forward-backward
pruning, stochastic generation, and projection.
? Software toolkits implement these generic al-
gorithms, allowing designers to concentrate on
novel models rather than problem-specific in-
ference code. This leads to faster scientific ex-
perimentation with fewer bugs.
Weighted tree transducers play the same role for
problems that involve the creation and transforma-
tion of tree structures (Knight and Graehl, 2005). Of
course, many problems do not fit either the finite-
state string or tree transducer framework, but in this
paper, we concentrate on those that do.
Bayesian inference schemes have become popu-
lar recently in natural language processing for their
ability to manage uncertainty about model param-
eters and to allow designers to incorporate prior
knowledge flexibly. Task-accuracy results have gen-
erally been favorable. However, it can be time-
consuming to apply Bayesian inference methods to
each new problem. Designers typically build cus-
tom, problem-specific sampling operators for ex-
ploring the derivation space. They may factor their
programs to get some code re-use from one problem
to the next, but highly generic tools for string and
tree processing are not available.
In this paper, we marry the world of finite-state
machines with the world of Bayesian inference, and
we test our methods across a range of natural lan-
guage problems. Our contributions are:
? We describe a Bayesian inference algorithm
that can be used to train any cascade of WFSTs
on end-to-end data.
? We propose a method for automatic run selec-
447
tion, i.e., how to automatically select among
multiple training runs in order to achieve the
best possible task accuracy.
The natural language applications we consider
in this paper are: (1) unsupervised part-of-speech
(POS) tagging (Merialdo, 1994; Goldwater and
Griffiths, 2007), (2) letter substitution decipher-
ment (Peleg and Rosenfeld, 1979; Knight et al,
2006; Ravi and Knight, 2008), (3) segmentation of
space-free English (Goldwater et al, 2009), and (4)
Japanese/English phoneme alignment (Knight and
Graehl, 1998; Ravi and Knight, 2009a). Figure 1
shows how each of these problems can be repre-
sented as a cascade of finite-state acceptors (FSAs)
and finite-state transducers (FSTs).
2 Generic EM Training
We first describe forward-backward EM training for
a single FST M. Given a string pair (v,w) from our
training data, we transform v into an FST Mv that
just maps v to itself, and likewise transform w into
an FST Mw. Then we compose Mv with M, and com-
pose the result with Mw. This composition follows
Pereira and Riley (1996), treating epsilon input and
output transitions correctly, especially with regards
to their weighted interleaving. This yields a deriva-
tion lattice D, each of whose paths transforms v into
w.1 Each transition in D corresponds to some tran-
sition in the FST M. We run the forward-backward
algorithm over D to collect fractional counts for the
transitions in M. After we sum fractional counts for
all examples, we normalize with respect to com-
peting transitions in M, assign new probabilities to
M, and iterate. Transitions in M compete with each
other if they leave the same state with the same input
symbol, which may be empty ().
In order to train an FSA on observed string data,
we convert the FSA into an FST by adding an input-
epsilon to every transition. We then convert each
training string v into the string pair (, v). After run-
ning the above FST training algorithm, we can re-
move all input- from the trained machine.
It is straightforward to modify generic training to
support the following controls:
1Throughout this paper, we do not assume that lattices are
acyclic; the algorithms described work on general graphs.
B:E
a:A b:B A:D
A:C
=
a: 
 :D
 :E b: 
a:  :C
Figure 2: Composition of two FSTs maintaining separate
transitions.
Maximum iterations and early stopping. We spec-
ify a maximum number of iterations, and we halt
early if the ratio of log P(data) from one iteration
to the next exceeds a threshold (such as 0.99999).
Initial point. Any probabilities supplied on the pre-
trained FST are interpreted as a starting point for
EM?s search. If no probabilities are supplied, EM
begins with uniform probabilities.
Random restarts. We can request n random restarts,
each from a different, randomly-selected initial
point.
Locking and tying. Transitions on the pre-trained
FST can be marked as locked, in which case EM
will not modify their supplied probabilities. Groups
of transitions can be tied together so that their frac-
tional counts are pooled, and when normalization
occurs, they all receive the same probability.
Derivation lattice caching. If memory is available,
training can cache the derivation lattices computed
in the first EM iteration for all training pairs. Subse-
quent iterations then run much faster. In our experi-
ments, we observe an average 10-fold speedup with
caching.
Next we turn to training a cascade of FSTs on
end-to-end data. The algorithm takes as input: (1) a
sequence of FSTs, and (2) pairs of training strings
(v,w), such that v is accepted by the first FST in
the cascade, and w is produced by the last FST. The
algorithm outputs the same sequence of FSTs, but
with trained probabilities.
To accomplish this, we first compose the supplied
FSTs, taking care to keep the transitions from differ-
ent machines separate. Figure 2 illustrates this with a
small example. It may thus happen that a single tran-
sition in an input FST is represented multiple times
in the composed device, in which case their prob-
448
ABCD:a 
REY:r 
?:c 
1.  Unsupervised part-of-speech tagging with constrained dictionary 
POS Tag 
sequence 
Observed 
word 
sequence 
2.  Decipherment of letter-substitution cipher 
English 
letter 
sequence 
Observed 
enciphered 
text 
3.  Re-Spacing of English text written without spaces 
Word 
sequence 
Observed 
letter 
sequence 
w/o spaces 
4.  Alignment of Japanese/English phoneme sequences 
English 
phoneme 
sequence 
Japanese 
katakana 
phoneme 
sequence 
26 x 26 table 
letter bigram model, 
learned separately 
constrained tag?word 
substitution model tag bigram model 
unigram model over 
words and non-words deterministic spell-out 
mapping from each English  
phoneme to each Japanese  
phoneme sequence of length 1 to 3 
NN 
JJ 
JJ 
JJ 
NN 
VB ? 
? 
? 
NN:fish 
IN:at 
VB:fish 
SYM:a DT:a 
a 
b 
b 
b 
a 
c ? 
? 
? 
a:A 
a:B 
a:C 
b:A b:B b:C 
A AR 
ARE AREY 
AREYO 
?:? 
AREY:a 
?:b 
?:d 
?:r ?:e 
?:y 
AE:? 
?:S 
?:S 
?:U 
Figure 1: Finite-state cascades for five natural language problems.
449
abilities are tied together. Next, we run FST train-
ing on the end-to-end data. This involves creating
derivation lattices and running forward-backward on
them. After FST training, we de-compose the trained
device back into a cascade of trained machines.
When the cascade?s first machine is an FSA,
rather than an FST, then the entire cascade is viewed
as a generator of strings rather than a transformer of
strings. Such a cascade is trained on observed strings
rather than string pairs. By again treating the first
FSA as an FST with empty input, we can train using
the FST-cascade training algorithm described in the
previous paragraph.
Once we have our trained cascade, we can apply it
to new data, obtaining (for example) the k-best out-
put strings for an input string.
3 Generic Bayesian Training
Bayesian learning is a wide-ranging field. We focus
on training using Gibbs sampling (Geman and Ge-
man, 1984), because it has been popularly applied
in the natural language literature, e.g., (Finkel et al,
2005; DeNero et al, 2008; Blunsom et al, 2009).
Our overall plan is to give a generic algorithm
for Bayesian training that is a ?drop-in replacement?
for EM training. That is, we input an FST cas-
cade and data and output the same FST cascade
with trained weights. This is an approximation to a
purely Bayesian setup (where one would always in-
tegrate over all possible weightings), but one which
makes it easy to deploy FSTs to efficiently decode
new data. Likewise, we do not yet support non-
parametric approaches?to create a drop-in replace-
ment for EM, we require that all parameters be spec-
ified in the initial FST cascade. We return to this is-
sue in Section 5.
3.1 Particular Case
We start with a well-known application of Bayesian
inference, unsupervised POS tagging (Goldwater
and Griffiths, 2007). Raw training text is provided,
and each potential corpus tagging corresponds to a
hidden derivation of that data. Derivations are cre-
ated and probabilistically scored as follows:
1. i? 1
2. Choose tag t1 according to P0(t1)
3. Choose word w1 according to P0(w1 | t1)
4. i? i + 1
5. Choose tag ti according to
?P0(ti | ti?1) + ci?11 (ti?1, ti)
? + ci?11 (ti?1)
(1)
6. Choose word wi according to
?P0(wi | ti) + ci?11 (ti,wi)
? + ci?11 (ti)
(2)
7. With probability Pquit, quit; else go to 4.
This defines the probability of any given derivation.
The base distribution P0 represents prior knowl-
edge about the distribution of tags and words, given
the relevant conditioning context. The ci?11 are the
counts of events occurring before word i in the
derivation (the ?cache?).
When ? and ? are large, tags and words are essen-
tially generated according to P0. When ? and ? are
small, tags and words are generated with reference
to previous decisions inside the cache.
We use Gibbs sampling to estimate the distribu-
tion of tags given words. The key to efficient sam-
pling is to define a sampling operator that makes
some small change to the overall corpus derivation.
With such an operator, we derive an incremental
formula for re-scoring the probability of an entire
new derivation based on the probability of the old
derivation. Exchangeability makes this efficient?
we pretend like the area around the small change oc-
curs at the end of the corpus, so that both old and
new derivations share the same cache. Goldwater
and Griffiths (2007) choose the re-sampling operator
?change the tag of a single word,? and they derive
the corresponding incremental scoring formula for
unsupervised tagging. For other problems, design-
ers develop different sampling operators and derive
different incremental scoring formulas.
3.2 Generic Case
In order to develop a generic algorithm, we need
to abstract away from these problem-specific de-
sign choices. In general, hidden derivations corre-
spond to paths through derivation lattices, so we first
450
Figure 3: Changing a decision in the derivation lattice.
All paths generate the observed data. The bold path rep-
resents the current sample, and the dotted path represents
a sidetrack in which one decision is changed.
compute derivation lattices for our observed training
data through our cascade of FSTs. A random path
through these lattices constitutes the initial sample,
and we calculate its derivation probability directly.
One way to think about a generic small change
operator is to consider a single transition in the cur-
rent sample. This transition will generally compete
with other transitions. One possible small change is
to ?sidetrack? the derivation to a competing deriva-
tion. Figure 3 shows how this works. If the sidetrack
path quickly re-joins the old derivation path, then an
incremental score can be computed. However, side-
tracking raises knotty questions. First, what is the
proper path continuation after the sidetracking tran-
sition is selected? Should the path attempt to re-join
the old derivation as soon as possible, and if so, how
is this efficiently done? Then, how can we compute
new derivation scores for all possible sidetracks, so
that we can choose a new sample by an appropriate
weighted coin flip? Finally, would such a sampler be
reversible? In order to satisfy theoretical conditions
for Gibbs sampling, if we move from sample A to
sample B, we must be able to immediately get back
to sample A.
We take a different tack here, moving from point-
wise sampling to blocked sampling. Gao and John-
son (2008) employed blocked sampling for POS tag-
ging, and the approach works nicely for arbitrary
derivation lattices. We again start with a random
derivation for each example in the corpus. We then
choose a training example and exchange its entire
derivation lattice to the end of the corpus. We cre-
ate a weighted version of this lattice, called the pro-
posal lattice, such that we can approximately sample
whole paths by stochastic generation. The probabil-
ities are based on the event counts from the rest of
the sample (the cache), and on the base distribution,
and are computed in this way:
P(r | q) =
?P0(r | q) + c(q, r)
? + c(q)
(3)
where q and r are states of the derivation lattice, and
the c(?) are counts collected from the corpus minus
the entire training example being resampled. This is
an approximation because we are ignoring the fact
that P(r | q) in general depends on choices made
earlier in the lattice. The approximation can be cor-
rected using the Metropolis-Hastings algorithm, in
which the sample drawn from the proposal lattice is
accepted only with a certain probability ?; but Gao
and Johnson (2008) report that ? > 0.99, so we skip
this step.
3.3 Choosing the best derivations
After the sampling run has finished, we can choose
the best derivations using two different methods.
First, if we want to find the MAP derivations of the
training strings, then following Goldwater and Grif-
fiths (2007), we can use annealing: raise the proba-
bilities in the sampling distribution to the 1T power,
where T is a temperature parameter, decrease T to-
wards zero, and take a single sample.
But in practice one often wants to predict the
MAP derivation for a new string w? not contained
in the training data. To approximate the distribution
of derivations of w? given the training data, we aver-
age the transition counts from all the samples (after
burn-in) and plug the averaged counts into (3) to ob-
tain a single proposal lattice.2 The predicted deriva-
tion is the Viterbi path through this lattice. Call this
method averaging. An advantage of this approach is
that the trainer, taking a cascade of FSAs as input,
outputs a weighted version of the same cascade, and
this trained cascade can be used on unseen examples
without having to rerun training.
3.4 Implementation
That concludes the generic Bayesian training algo-
rithm, to which we add the following controls:
2A better approximation might have been to build a proposal
lattice for each sample (after burn-in), and then construct a sin-
gle FSA that computes the average of the probability distribu-
tions computed by all the proposal lattices. But this FSA would
be rather large.
451
Number of Gibbs sampling iterations. We execute
the full number specified.
Base distribution. Any probabilities supplied on the
pre-trained FST are interpreted as base distribution
probabilities. If no probabilities are supplied, then
the base distribution is taken to be uniform.
Hyperparameters. We supply a distinct ? for each
machine in the FST cascade. We do not yet support
different ? values for different states within a single
FST.
Random restarts. We can request multiple runs
from different, randomly-selected initial samples.
EM-based initial point. If random initial samples
are undesirable, we can request that the Gibbs sam-
pler be initialized with the Viterbi path using param-
eter values obtained by n iterations of EM.
Annealing schedule. If annealing is used, it follows
a linear annealing schedule with starting and stop-
ping temperature specified by the user.
EM and Bayesian training for arbitrary FST
cascades are both implemented in the finite-state
toolkit Carmel, which is distributed with source
code.3 All controls are implemented as command-
line switches. We use Carmel to carry out the exper-
iments in the next section.
4 Run Selection
For both EM and Bayesian methods, different train-
ing runs yield different results. EM?s objective func-
tion (probability of observed data) is very bumpy for
the unsupervised problems we work on?different
initial points yield different trained WFST cascades,
with different task accuracies. Averaging task accu-
racies across runs is undesirable, because we want to
deploy a particular trained cascade in the real world,
and we want an estimate of its performance. Select-
ing the run with the best task accuracy is illegal in an
unsupervised setting. With EM, we have a good al-
ternative: select the run that maximizes the objective
function, i.e., the likelihood of the observed training
data. We find a decent correlation between this value
and task accuracy, and we are generally able to im-
prove accuracy using this run selection method. Fig-
ure 4 shows a scatterplot of 1000 runs for POS tag-
ging. A single run with a uniform start yields 81.8%
3http://www.isi.edu/licensed-sw/carmel
 0.75
 0.8
 0.85
 0.9
 211200
 211300
 211400
 211500
 211600
 211700
 211800
 211900
 212000
 212100
 212200
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(data)
EM (random start)EM (uniform start)
Figure 4: Multiple EM restarts for POS tagging. Each
point represents one random restart; the y-axis is tag-
ging accuracy and the x-axis is EM?s objective function,
? log P(data).
accuracy, while automatic selection from 1000 runs
yields 82.4% accuracy.
Gibbs sampling runs also yield WFST cascades
with varying task accuracies, due to random initial
samples and sampling decisions. In fact, the varia-
tion is even larger than what we find with EM. It is
natural to ask whether we can do automatic run se-
lection for Gibbs sampling. If we are using anneal-
ing, it makes sense to use the probability of the fi-
nal sample, which is supposed to approximate the
MAP derivation. When using averaging, however,
choosing the final sample would be quite arbitrary.
Instead, we propose choosing the run that has the
highest average log-probability (that is, the lowest
entropy) after burn-in. The rationale is that the runs
that have found their way to high-probability peaks
are probably more representative of the true distri-
bution, or at least capture a part of the distribution
that is of greater interest to us.
We find that this method works quite well in prac-
tice. Figure 5 illustrates 1000 POS tagging runs
for annealing with automatic run selection, yield-
ing 84.7% accuracy. When using averaging, how-
ever, automatic selection from 1000 runs (Figure 6)
produces a much higher accuracy of 90.7%. This
is better than accuracies reported previously using
452
 0.75
 0.8
 0.85
 0.9
 235100
 235150
 235200
 235250
 235300
 235350
 235400
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) for final sample
Bayesian run (with annealing)
Figure 5: Multiple Bayesian learning runs (using anneal-
ing with temperature decreasing from 2 to 0.08) for POS
tagging. Each point represents one run; the y-axis is tag-
ging accuracy and the x-axis is the ? log P(derivation) of
the final sample.
 0.75
 0.8
 0.85
 0.9
 236800
 236900
 237000
 237100
 237200
 237300
 237400
 237500
 237600
 237700
 237800
 237900
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) averaged over all post-burnin samples
Bayesian run (using averaging)
Figure 6: Multiple Bayesian learning runs (using averag-
ing) for POS tagging. Each point represents one run; the
y-axis is tagging accuracy and the x-axis is the average
? log P(derivation) over all samples after burn-in.
Bayesian methods (85.2% from Goldwater and Grif-
fiths (2007), who use a trigram model) and close to
the best accuracy reported on this task (91.8% from
Ravi and Knight (2009b), who use an integer linear
program to minimize the model directly).
5 Experiments and Results
We run experiments for various natural language ap-
plications and compare the task accuracies achieved
by the EM and Bayesian learning methods. The
tasks we consider are:
Unsupervised POS tagging. We adopt the com-
mon problem formulation for this task described
by Merialdo (1994), in which we are given a raw
24,115-word sequence and a dictionary of legal tags
for each word type. The tagset consists of 45 dis-
tinct grammatical tags. We use the same modeling
approach as as Goldwater and Griffiths (2007), us-
ing a probabilistic tag bigram model in conjunction
with a tag-to-word model.
Letter substitution decipherment. Here, the task
is to decipher a 414-letter substitution cipher and un-
cover the original English letter sequence. The task
accuracy is defined as the percent of ciphertext to-
kens that are deciphered correctly. We work on the
same standard cipher described in previous litera-
ture (Ravi and Knight, 2008). The model consists
of an English letter bigram model, whose probabil-
ities are fixed and an English-to-ciphertext channel
model, which is learnt during training.
Segmentation of space-free English. Given
a space-free English text corpus (e.g.,
iwalkedtothe...), the task is to segment the
text into words (e.g., i walked to the ...).
Our input text corpus consists of 11,378 words,
with spaces removed. As illustrated in Figure 1,
our method uses a unigram FSA that models every
letter sequence seen in the data, which includes
both words and non-words (at most 10 letters long)
composed with a deterministic spell-out model.
In order to evaluate the quality of our segmented
output, we compare it against the gold segmentation
and compute the word token f-measure.
Japanese/English phoneme alignment. We
use the problem formulation of Knight and
Graehl (1998). Given an input English/Japanese
katakana phoneme sequence pair, the task is to
produce an alignment that connects each English
453
MLE Bayesian
EM prior VB-EM Gibbs
POS tagging 82.4 ? = 10?2, ? = 10?1 84.1 90.7
Letter decipherment 83.6 ? = 106, ? = 10?2 83.6 88.9
Re-spacing English 0.9 ? = 10?8, ? = 104 0.8 42.8
Aligning phoneme strings? 100 ? = 10?2 99.9 99.1
Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. ?The output of
EM alignment was used as the gold standard.
phoneme to its corresponding Japanese sounds (a
sequence of one or more Japanese phonemes). For
example, given a phoneme sequence pair ((AH B
AW T) ? (a b a u t o)), we have to produce
the alignments ((AH ? a), (B ? b), (AW ?
a u), (T ? t o)). The input data consists of
2,684 English/Japanese phoneme sequence pairs.
We use a model that consists of mappings from each
English phoneme to Japanese phoneme sequences
(of length up to 3), and the mapping probabilities
are learnt during training. We manually analyzed
the alignments produced by the EM method for
this task and found them to be nearly perfect.
Hence, for the purpose of this task we treat the EM
alignments as our gold standard, since there are no
gold alignments available for this data.
In all the experiments reported here, we run EM
for 200 iterations and Bayesian for 5000 iterations
(the first 2000 for burn-in). We apply automatic run
selection using the objective function value for EM
and the averaging method for Bayesian.
Table 1 shows accuracy results for our four tasks,
using run selection for both EM and Bayesian learn-
ing. For the Bayesian runs, we compared two infer-
ence methods: Gibbs sampling, as described above,
and Variational Bayesian EM (Beal and Ghahra-
mani, 2003), both of which are implemented in
Carmel. We used the hyperparameters (?, ?) as
shown in the table. Setting a high value yields a fi-
nal distribution that is close to the original one (P0).
For example, in letter decipherment we want to keep
the language model probabilities fixed during train-
ing, and hence we set the prior on that model to
be very strong (? = 106). Table 1 shows that the
Bayesian methods consistently outperform EM for
all the tasks (except phoneme alignment, where EM
was taken as the gold standard). Each iteration of
Gibbs sampling was 2.3 times slower than EM for
POS tagging, and in general about twice as slow.
6 Discussion
We have described general training algorithms for
FST cascades and their implementation, and exam-
ined the problem of run selection for both EM and
Bayesian training. This work raises several interest-
ing points for future study.
First, is there an efficient method for perform-
ing pointwise sampling on general FSTs, and would
pointwise sampling deliver better empirical results
than blocked sampling across a range of tasks?
Second, can generic methods similar to the ones
described here be developed for cascades of tree
transducers? It is straightforward to adapt our meth-
ods to train a single tree transducer (Graehl et al,
2008), but as most types of tree transducers are
not closed under composition (Ge?cseg and Steinby,
1984), the compose/de-compose method cannot be
directly applied to train cascades.
Third, what is the best way to extend the FST for-
malism to represent non-parametric Bayesian mod-
els? Consider the English re-spacing application. We
currently take observed (un-spaced) data and build
a giant unigram FSA that models every letter se-
quence seen in the data of up to 10 letters, both
words and non-words. This FSA has 207,253 tran-
sitions. We also define P0 for each individual transi-
tion, which allows a preference for short words. This
set-up works fine, but in a nonparametric approach,
P0 is defined more compactly and without a word-
length limit. An extension of FSTs along the lines
of recursive transition networks may be appropriate,
but we leave details for future work.
454
References
Matthew J. Beal and Zoubin Ghahramani. 2003. The
Variational Bayesian EM algorithm for incomplete
data: with application to scoring graphical model
structures. Bayesian Statistics, 7:453?464.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL-
IJCNLP 2009.
Alexander Clark. 2002. Memory-based learning of mor-
phology with stochastic transducers. In Proceedings
of ACL 2002.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of ACL 2005.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised Hidden Markov
Model POS taggers. In Proceedings of EMNLP 2008.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21 ? 54.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Proceedings of AMTA
1998.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Knight Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of CICLing-2005.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of COLING-ACL 2006.
Okan Kolak, Willian Byrne, and Philip Resnik. 2003. A
generative probabilistic OCR model for NLP applica-
tions. In Proceedings of HLT-NAACL 2003.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP 2006.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Communications of the ACM, 22(11):598?605.
Fernando C. N. Pereira and Michael D. Riley. 1996.
Speech recognition by composition of weighted finite
automata. Finite-State Language Processing, pages
431?453.
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their appli-
cations to human language processing. In ARPA Hu-
man Language Technology Workshop.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of EMNLP 2008.
Sujith Ravi and Kevin Knight. 2009a. Learning
phoneme mappings for transliteration without parallel
data. In Proceedings of NAACL HLT 2009.
Sujith Ravi and Kevin Knight. 2009b. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009.
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
455
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443?1452,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Translate with Source and Target Syntax
David Chiang
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
chiang@isi.edu
Abstract
Statistical  translation  models  that  try  to
capture the recursive structure of language
have been widely adopted over the last few
years. These  models  make  use  of  vary-
ing amounts of information from linguis-
tic theory: some use none at all, some use
information about the grammar of the tar-
get language, some use information about
the grammar of the source language. But
progress  has  been  slower  on  translation
models  that  are  able  to  learn  the  rela-
tionship  between  the  grammars  of  both
the source and target  language. We dis-
cuss the reasons why this has been a chal-
lenge, review existing attempts to meet this
challenge, and show how some old and
new ideas can be combined into a  sim-
ple approach that uses both source and tar-
get syntax for significant improvements in
translation accuracy.
1 Introduction
Statistical translation models that use synchronous
context-free  grammars  (SCFGs)  or  related  for-
malisms to try to capture the recursive structure of
language have been widely adopted over the last
few years. The simplest of these (Chiang, 2005)
make no use of information from syntactic theo-
ries or syntactic annotations, whereas others have
successfully incorporated syntactic information on
the target side (Galley et al, 2004; Galley et al,
2006) or the source side (Liu et al, 2006; Huang
et al, 2006). The next obvious step is toward mod-
els that make full use of syntactic information on
both sides. But the natural generalization to this
setting has been found to underperform phrase-
based models (Liu et al, 2009; Ambati and Lavie,
2008), and researchers have begun to explore so-
lutions (Zhang et al, 2008; Liu et al, 2009).
In this paper, we explore the reasons why tree-
to-tree translation has been challenging, and how
source syntax and target syntax might be used to-
gether. Drawing on previous successful attempts to
relax syntactic constraints during grammar extrac-
tion in various ways (Zhang et al, 2008; Liu et al,
2009; Zollmann and Venugopal, 2006), we com-
pare several methods for extracting a synchronous
grammar from tree-to-tree data. One confounding
factor in such a comparison is that some methods
generate many new syntactic categories, making it
more difficult to satisfy syntactic constraints at de-
coding time. We therefore propose to move these
constraints from the formalism into the model, im-
plemented as features in the hierarchical phrase-
based  model  Hiero  (Chiang, 2005). This  aug-
mented model is able to learn from data whether
to rely on syntax or not, or to revert back to mono-
tone phrase-based translation.
In experiments on Chinese-English and Arabic-
English translation, we find that when both source
and target syntax are made available to the model
in an unobtrusive way, the model chooses to build
structures that are more syntactically well-formed
and yield significantly better translations than a
nonsyntactic hierarchical phrase-based model.
2 Grammar extraction
A synchronous tree-substitution grammar (STSG)
is a set of rules or elementary tree pairs (?, ?),
where:
? ? is a tree whose interior labels are source-
language  nonterminal  symbols  and  whose
frontier labels are source-language nontermi-
nal symbols or terminal symbols (words). The
nonterminal-labeled frontier nodes are called
substitution  nodes, conventionally  marked
with an arrow (?).
? ? is  a  tree  of  the  same  form except  with
1443
..
..PP
.
.
..LCP
.
.
..LC
.. ?
zh?ng
.
.NP?
.
.P
..?
z?i
.
..PP
.
.
.
..NP?
.
.IN
.
..in
.
.
..NP
.
.
..NP
..NN
..??
m?oy?
.NP
.
.
..NP
..NN
.
..?
?n
.QP
..CD
..?
li?ng
.
..NP
.
.
.
..PP
.
.
.
..NP
.
.
.
..NNS
.
..shores
.
..CD
.
..two
.DT
.
..the
.IN
.
..between
.NP
.
..NN
.
..trade
..
..PP
.
.
..LCP
.
.
..LC
.. ?
zh?ng
.NP
.
.
..NP
..NN
..??
m?oy?
.NP
. ..NP
..NN
.
..?
?n
.QP
..CD
..?
li?ng
.
.P
..?
z?i
.
..PP
.
.
.
..NP
.
.
.
..PP
.
.
.
..NP
.
.
.
..NNS
.
..shores
.
..CD
.
..two
.DT
.
..the
.IN
.
..between
.NP
.
..NN
.
..trade
.IN
.
..in
(?
1
, ?
1
) (?
2
, ?
2
) (?
3
, ?
3
)
Figure 1: Synchronous tree substitution. Rule (?
2
, ?
2
) is substituted into rule (?
1
, ?
1
) to yield (?
3
, ?
3
).
target-language  instead  of  source-language
symbols.
? The substitution nodes of ? are aligned bijec-
tively with those of ?.
? The terminal-labeled frontier nodes of ? are
aligned (many-to-many) with those of ?.
In the substitution operation, an aligned pair  of
substitution nodes is rewritten with an elementary
tree pair. The labels of the substitution nodes must
match the root labels of the elementary trees with
which they are rewritten (but we will relax this
constraint below). See Figure 1 for examples of el-
ementary tree pairs and substitution.
2.1 Exact tree-to-tree extraction
The use of STSGs for translation was proposed
in the Data-Oriented Parsing literature (Poutsma,
2000; Hearne  and  Way, 2003)  and  by  Eis-
ner (2003). Both of these proposals are more am-
bitious  about  handling  spurious  ambiguity  than
approaches derived from phrase-based translation
usually have been (the former uses random sam-
pling to sum over equivalent derivations during de-
coding, and the latter uses dynamic programming
human automatic
string-to-string 198,445 142,820
max nested 78,361 64,578
tree-to-string 60,939 (78%) 48,235 (75%)
string-to-tree 59,274 (76%) 46,548 (72%)
tree-to-tree 53,084 (68%) 39,049 (60%)
Table 1: Analysis  of  phrases  extracted  from
Chinese-English newswire data with human and
automatic  word  alignments  and  parses. As  tree
constraints are added, the number of phrase pairs
drops. Errors  in  automatic  annotations  also  de-
crease the number of phrase pairs. Percentages are
relative to the maximum number of nested phrase
pairs.
to sum over equivalent derivations during train-
ing). If we take a more typical approach, which
generalizes that of Galley et al (2004; 2006) and
is similar to Stat-XFER (Lavie et al, 2008), we
obtain the following grammar extraction method,
which we call exact tree-to-tree extraction.
Given  a  pair  of  source-  and  target-language
parse trees with a word alignment between their
leaves, identify  all  the phrase pairs ( f? , e?), i.e.,
those substring pairs that respect the word align-
1444
..
.IP
.
.
..VP
..??????
y?b?is?sh?q?
??
m?iyu?n
.
..NP
..NN
.. ??
sh?nch?
.
..PP
.
.
..LCP
.
.
..LC
.. ?
zh?ng
.NP
.
.
..NP
..NN
..??
m?oy?
.NP
.
.
..NP
..NN
.
..?
?n
.QP
..CD
..?
li?ng
.
.P
..?
z?i
.
.NP
..NR
..??
T?iw?n
.
..S
.
.
.
..VP
.
..is 14.7 billion US dollars
.NP
.
.
.
..PP
.
.
.
..NP
.
.
.
..PP
.
.
.
..NP
.
.
.
..NNS
.
..shores
.
..CD
.
..two
.DT
.
..the
.IN
.
..between
.NP
.
..NN
.
..trade
.IN
.
..in
.NP
.
.
.
..NN
.
..surplus
.NP
.
.
.
..POS
.
..?s
.NNP
.
..Taiwan
Figure 2: Example Chinese-English sentence pair with human-annotated parse trees and word alignments.
ment in the sense that at least one word in f? is
aligned to a word in e?, and no word in f? is aligned
to a word outside of e?, or vice versa. Then the ex-
tracted grammar is the smallest STSGG satisfying:
? If (?, ?) is a pair of subtrees of a training ex-
ample and the frontiers of ? and ? form a
phrase pair, then (?, ?) is a rule in G.
? If (?
2
, ?
2
) ? G, (?
3
, ?
3
) ? G, and (?
1
, ?
1
) is
an elementary tree pair such that substituting
(?
2
, ?
2
) into (?
1
, ?
1
) results in (?
3
, ?
3
), then
(?
1
, ?
1
) is a rule in G.
For example, consider the training example in Fig-
ure 2, from which the elementary tree pairs shown
in Figure 1 can be extracted. The elementary tree
pairs (?
2
, ?
2
) and (?
3
, ?
3
) are rules in G because
their yields are phrase pairs, and (?
1
, ?
1
) results
from subtracting (?
2
, ?
2
) from (?
3
, ?
3
).
2.2 Fuzzy tree-to-tree extraction
Exact tree-to-tree translation requires that transla-
tion rules deal with syntactic constituents on both
the source and target side, which reduces the num-
ber of eligible phrases. Table 1 shows an analy-
sis of phrases extracted from human word-aligned
and parsed data and automatically word-aligned
and parsed data.
1
The first line shows the num-
ber of phrase-pair occurrences that are extracted
in the absence of syntactic constraints,
2
and the
second line shows the maximum number of nested
phrase-pair occurrences, which is the most that ex-
act syntax-based extraction can achieve. Whereas
tree-to-string extraction and string-to-tree extrac-
tion  permit  70?80%  of  the  maximum  possible
number of phrase pairs, tree-to-tree extraction only
permits 60?70%.
Why does this happen? We can see that moving
from human annotations to automatic annotations
decreases not only the absolute number of phrase
pairs, but the percentage of phrases that pass the
syntactic filters. Wellington et al (2006), in a more
systematic study, find that, of sentences where the
tree-to-tree constraint blocks rule extraction, the
majority are due to parser errors. To address this
problem, Liu et al (2009) extract rules from pairs
1
The  first  2000  sentences  from  the  GALE Phase  4
Chinese  Parallel  Word  Alignment  and  Tagging  Part 1
(LDC2009E83) and the Chinese News Translation Text Part 1
(LDC2005T06), respectively.
2
Only counting phrases that have no unaligned words at
their endpoints.
1445
of packed forests instead of pairs of trees. Since a
packed forest is much more likely to include the
correct tree, it is less likely that parser errors will
cause good rules to be filtered out.
However, even on human-annotated data, tree-
to-tree extraction misses many rules, and many
such  rules  would  seem  to  be  useful. For  ex-
ample, in  Figure 2, the  whole  English  phrase
?Taiwan?s. . .shores?  is  an  NP,  but  its  Chinese
counterpart is not a constituent. Furthermore, nei-
ther ?surplus. . .shores? nor its Chinese counterpart
are constituents. But both rules are arguably use-
ful for translation. Wellington et al therefore ar-
gue that in order to extract as many rules as possi-
ble, a more powerful formalism than synchronous
CFG/TSG is required: for  example, generalized
multitext grammar (Melamed et al, 2004), which
is equivalent to synchronous set-local multicom-
ponent CFG/TSG (Weir, 1988).
But  the  problem illustrated  in  Figure 2 does
not reflect a very deep fact about syntax or cross-
lingual divergences, but rather choices in annota-
tion style that interact badly with the exact tree-
to-tree extraction heuristic. On the Chinese side,
the IP is too flat (because ??/T?iw?n has been
analyzed as a topic), whereas the more articulated
structure
(1) [
NP
T?iw?n [
NP
[
PP
za? . . .] sh?nch?]]
would also be quite reasonable. On the English
side, the high attachment of the PP disagrees with
the corresponding Chinese structure, but low at-
tachment also seems reasonable:
(2) [
NP
[
NP
Taiwan?s] [
NP
surplus in trade. . .]]
Thus even in the gold-standard parse trees, phrase
structure  can be underspecified (like the flat  IP
above) or uncertain (like the PP attachment above).
For this reason, some approaches work with a
more flexible notion of constituency. Synchronous
tree-sequence?substitution grammar (STSSG) al-
lows either side of a rule to comprise a sequence of
trees instead of a single tree (Zhang et al, 2008). In
the substitution operation, a sequence of sister sub-
stitution nodes is rewritten with a tree sequence of
equal length (see Figure 3a). This extra flexibility
effectively makes the analysis (1) available to us.
Any STSSG can be converted into an equivalent
STSG via the creation of virtual nodes (see Fig-
ure 3b): for every elementary tree sequence with
roots X
1
, . . . ,X
n
, create a new root node with a
.
.
.NP
.
.
..NNP?
.
..NNP?
.
..NN
..Minister
.NN
..Prime
.
?
?
?
. .
.
.NNP
..Ariel
. .
.
.NNP
..Sharon
.
?
?
?
(a)
.
.
.NP
.
.
..NNP
?
NNP?
.
..NN
..Minister
.NN
..Prime
. .
.
.NNP
?
NNP
.
.
..NNP
..Sharon
.NNP
..Ariel
(b)
Figure 3: (a) Example tree-sequence substitution
grammar and (b) its equivalent SAMT-style tree-
substitution grammar.
complex label X
1
? ? ? ? ?X
n
immediately dominat-
ing the old roots, and replace every sequence of
substitution sites X
1
, . . . ,X
n
with a single substi-
tution site X
1
? ? ? ? ?X
n
. This is essentially what
syntax-augmented MT (SAMT) does, in the string-
to-tree setting (Zollmann and Venugopal, 2006). In
addition, SAMT drops the requirement that the X
i
are sisters, and uses categories X / Y (an Xmissing
a Y on the right) and Y \X (an Xmissing a Y on the
left) in the style of categorial grammar (Bar-Hillel,
1953). Under this flexible notion of constituency,
both (1) and (2) become available, albeit with more
complicated categories.
Both STSSG and SAMT are examples of what
we might call fuzzy tree-to-tree extraction. We fol-
low this approach here as well: as in STSSG, we
work on tree-to-tree data, and we use the com-
plex categories of SAMT. Moreover, we allow the
product categoriesX
1
? ? ? ? ?X
n
to be of any length
n, and we allow the slash categories to take any
number of arguments on either side. Thus every
phrase can be assigned a (possibly very complex)
syntactic category, so that fuzzy tree-to-tree ex-
traction does not lose any rules relative to string-
to-string extraction.
On the other hand, if several rules are extracted
1446
that differ only in their nonterminal labels, only the
most-frequent rule is kept, and its count is the to-
tal count of all the rules. This means that there is a
one-to-one correspondence between the rules ex-
tracted by fuzzy tree-to-tree extraction and hierar-
chical string-to-string extraction.
2.3 Nesting phrases
Fuzzy tree-to-tree extraction (like string-to-string
extraction) generates many times more rules than
exact tree-to-tree extraction does. In Figure 2, we
observed that the flat structure of the Chinese IP
prevented  exact  tree-to-tree  extraction  from ex-
tracting a rule containing just part of the IP, for
example:
(3) [
PP
za? . . .] [
NP
sh?nch?]
(4) [
NP
T?iw?n] [
PP
za? . . .] [
NP
sh?nch?]
(5) [
PP
za? . . .] [
NP
sh?nch?] [
VP
. . . m?iyu?n]
Fuzzy tree-to-tree extraction allows any of these
to be the source side of a rule. We might think of
it as effectively restructuring the trees by insert-
ing nodes with complex labels. However, it is not
possible to represent this restructuring with a sin-
gle tree (see Figure 4). More formally, let us say
that two phrases w
i
? ? ?w
j?1 and w
i
? ? ? ?w
j
??1 nest
if i ? i? < j? ? j or i? ? i < j < j?; otherwise,
they cross. The two Chinese phrases (4) and (5)
cross, and therefore cannot both be constituents in
the same tree. In other words, exact tree-to-tree ex-
traction commits to a single structural analysis but
fuzzy tree-to-tree extraction pursues many restruc-
tured analyses at once.
We can strike a compromise by continuing to al-
low SAMT-style complex categories, but commit-
ting to a single analysis by requiring all phrases to
nest. To do this, we use a simple heuristic. Iterate
through all the phrase pairs ( f? , e?) in the following
order:
1. sort by whether f? and e? can be assigned a sim-
ple syntactic category (both, then one, then
neither); if there is a tie,
2. sort by how many syntactic constituents f? and
e? cross (low to high); if there is a tie,
3. give priority to ( f? , e?) if neither f? nor e? be-
gins or ends with punctuation; if there is a tie,
finally
4. sort by the position of f? in the source-side
string (right to left).
For each phrase pair, accept it if it does not cross
any previously accepted phrase pair; otherwise, re-
ject it.
Because this heuristic produces a set of nesting
phrases, we can represent them all in a single re-
structured tree. In Figure 4, this heuristic chooses
structure (a) because the English-side counterpart
of IP/VP has the simple category NP.
3 Decoding
In  decoding, the  rules  extracted during training
must be reassembled to form a derivation whose
source side matches the input sentence. In the ex-
act  tree-to-tree  approach, whenever  substitution
is  performed, the  root  labels  of  the  substituted
trees  must  match  the  labels  of  the  substitution
nodes?call this the matching constraint. Because
this constraint must be satisfied on both the source
and target side, it can become difficult to general-
ize well from training examples to new input sen-
tences.
Venugopal et al (2009), in the string-to-tree set-
ting, attempt to soften the data-fragmentation ef-
fect of the matching constraint: instead of trying
to find the single derivation with the highest prob-
ability, they sum over derivations that differ only
in their nonterminal labels and try to find the sin-
gle derivation-class with the highest probability.
Still, only  derivations  that  satisfy  the  matching
constraint are included in the summation.
But in some cases we may want to soften the
matching  constraint  itself. Some syntactic  cate-
gories are similar enough to be considered com-
patible: for example, if a rule rooted in VBD (past-
tense verb) could substitute into a site labeled VBZ
(present-tense verb), it might still generate correct
output. This is all the more true with the addition
of SAMT-style categories: for example, if a rule
rooted in ADVP
?
VP could substitute into a site
labeled VP, it would very likely generate correct
output.
Since we want syntactic information to help the
model make good translation choices, not to rule
out potentially correct choices, we can change the
way the information is used during decoding: we
allow any rule to substitute into any site, but let
the model learn which substitutions are better than
others. To do this, we add the following features to
the model:
1447
..
.IP
.
.
..VP
..??????
y?b?is?sh?q?
??
m?iyu?n
.IP/VP
.
.
..PP
?
NP
.
.
..NP
..NN
.. ??
sh?nch?
.
.PP
..?
z?i
?
li?ng
?
?n
??
m?oy?
?
zh?ng
.
.NP
..NR
..??
T?iw?n
.
.
.IP
.
.
..IP\NP
.
.
..VP
..??????
y?b?is?sh?q?
??
m?iyu?n
.PP
?
NP
.
.
..NP
..NN
.. ??
sh?nch?
.
.PP
..?
z?i
?
li?ng
?
?n
??
m?oy?
?
zh?ng
.
.NP
..NR
..??
T?iw?n
(a) (b)
Figure 4: Fuzzy tree-to-tree extraction effectively restructures the Chinese tree from Figure 2 in two ways
but does not commit to either one.
? match f counts  the  number  of  substitutions
where the label of the source side of the sub-
stitution  site  matches  the  root  label  of  the
source side of the rule, and ?match f counts
those where the labels do not match.
? subst f
X?Y counts the number of substitutions
where the label of the source side of the sub-
stitution site is X and the root label of the
source side of the rule is Y.
? matche, ?matche, and subste
X?Y do the same
for the target side.
? root
X,X? counts  the  number  of  rules  whose
root label on the source side is X and whose
root label on the target side is X
?
.
3
For example, in the derivation of Figure 1, the fol-
lowing features would fire:
match
f = 1
subst
f
NP?NP = 1
match
e = 1
subst
e
NP?NP = 1
root
NP,NP = 1
The decoding algorithm then operates as in hier-
archical phrase-based translation. The decoder has
to store in each hypothesis the source and target
root labels of the partial derivation, but these la-
bels are used for calculating feature vectors only
and not for checking well-formedness of deriva-
tions. This additional state does increase the search
space of the decoder, but we did not change any
pruning settings.
3
Thanks to Adam Pauls for suggesting this feature class.
4 Experiments
To compare the methods described above with hi-
erarchical string-to-string translation, we ran ex-
periments  on both Chinese-English and Arabic-
English translation.
4.1 Setup
The sizes of the parallel texts used are shown in Ta-
ble 2. We word-aligned the Chinese-English par-
allel  text using GIZA++ followed by link dele-
tion (Fossum et al, 2008), and the Arabic-English
parallel text using a combination of GIZA++ and
LEAF (Fraser and Marcu, 2007). We parsed the
source sides of both parallel texts using the Berke-
ley parser (Petrov et al, 2006), trained on the Chi-
nese Treebank 6 and Arabic Treebank parts 1?3,
and the English sides using a reimplementation of
the Collins parser (Collins, 1997).
For string-to-string extraction, we used the same
constraints as in previous work (Chiang, 2007),
with differences shown in Table 2. Rules with non-
terminals were extracted from a subset of the data
(labeled ?Core? in Table 2), and rules without non-
terminals were extracted from the full parallel text.
Fuzzy tree-to-tree extraction was performed using
analogous constraints. For exact tree-to-tree ex-
traction, we used simpler settings: no limit on ini-
tial phrase size or unaligned words, and a maxi-
mum of 7 frontier nodes on the source side.
All systems used the glue rule (Chiang, 2005),
which allows the decoder, working bottom-up, to
stop  building  hierarchical  structure  and  instead
concatenate  partial  translations  without  any  re-
ordering. The model attaches a weight to the glue
rule so that it can learn from data whether to build
shallow or rich structures, but for efficiency?s sake
the decoder has a hard limit, called the distortion
1448
Chi-Eng Ara-Eng
Core training words 32+38M 28+34M
initial phrase size 10 15
final rule size 6 6
nonterminals 2 2
loose source 0 ?
loose target 0 2
Full training words 240+260M 190+220M
final rule size 6 6
nonterminals 0 0
loose source ? ?
loose target 1 2
Table 2: Rule extraction settings used for exper-
iments. ?Loose  source/target?  is  the  maximum
number of  unaligned source/target  words at  the
endpoints of a phrase.
limit, above which the glue rule must be used.
We trained two 5-gram language models: one
on the combined English halves of the bitexts, and
one on two billion words of English. These were
smoothed using modified Kneser-Ney (Chen and
Goodman, 1998) and stored using randomized data
structures similar  to those of Talbot and Brants
(2008).
The base feature set for all systems was similar
to the expanded set recently used for Hiero (Chiang
et al, 2009), but with bigram features (source and
target word) instead of trigram features (source and
target word and neighboring source word). For all
systems but the baselines, the features described
in Section 3 were added. The systems were trained
using MIRA (Crammer and Singer, 2003; Chiang
et al, 2009) on a tuning set of about 3000 sentences
of newswire from NIST MT evaluation data and
GALE development data, disjoint from the train-
ing data. We optimized feature weights on 90% of
this and held out the other 10% to determine when
to stop.
4.2 Results
Table 3 shows the scores on our development sets
and  test  sets, which  are  about  3000  and  2000
sentences, respectively, of newswire drawn from
NISTMT evaluation data and GALE development
data and disjoint from the tuning data.
For Chinese, we first tried increasing the distor-
tion limit from 10 words to 20. This limit controls
how deeply nested the tree structures built by the
decoder are, and we want to see whether adding
syntactic information leads to more complex struc-
tures. This change by itself led to an increase in
the BLEU score. We then compared against two
systems using  tree-to-tree  grammars. Using  ex-
act tree-to-tree extraction, we got a much smaller
grammar, but decreased accuracy on all  but the
Chinese-English test set, where there was no sig-
nificant change. But with fuzzy tree-to-tree extrac-
tion, we obtained an improvement of +0.6 on both
Chinese-English sets, and +0.7/+0.8 on the Arabic-
English sets.
Applying the heuristic for nesting phrases re-
duced the grammar sizes dramatically (by a factor
of 2.4 for Chinese and 4.2 for Arabic) but, interest-
ingly, had almost no effect on translation quality: a
slight decrease in BLEU on the Arabic-English de-
velopment set and no significant difference on the
other sets. This suggests that the strength of fuzzy
tree-to-tree extraction lies in its ability to break up
flat structures and to reconcile the source and target
trees with each other, rather than multiple restruc-
turings of the training trees.
4.3 Rule usage
We then  took  a  closer  look  at  the  behavior  of
the  string-to-string  and fuzzy tree-to-tree  gram-
mars (without the nesting heuristic). Because the
rules of these grammars are in one-to-one corre-
spondence, we can analyze the string-to-string sys-
tem?s derivations as though they had syntactic cat-
egories. First, Table 4 shows that the system using
the tree-to-tree grammar used the glue rule much
less and performed more matching substitutions.
That is, in order to minimize errors on the tuning
set, the model learned to build syntactically richer
and more well-formed derivations.
Tables 5 and 6 show how the new syntax fea-
tures affected particular substitutions. In general
we see a shift  towards more matching substitu-
tions; correct placement of punctuation is particu-
larly emphasized. Several changes appear to have
to  do  with  definiteness  of  NPs: on the  English
side, adding the syntax features encourages match-
ing substitutions of type DT \NP-C (anarthrous
NP),  but  discourages  DT \NP-C and  NN from
substituting  into  NP-C and  vice  versa. For  ex-
ample, a translation with the rewriting NP-C ?
DT \NP-C begins  with  ?24th  meeting  of  the
Standing Committee. . .,? but the system using the
fuzzy tree-to-tree grammar changes this to ?The
24th meeting of the Standing Committee. . . .?
The root features had a less noticeable effect on
1449
BLEU
task extraction dist. lim. rules features dev test
Chi-Eng string-to-string 10 440M 1k 32.7 23.4
string-to-string 20 440M 1k 33.3 23.7
]
tree-to-tree exact 20 50M 5k 32.8 23.9
tree-to-tree fuzzy 20 440M 160k 33.9
]
24.3
]
+ nesting 20 180M 79k 33.9 24.3
Ara-Eng string-to-string 10 790M 1k 48.7 48.9
tree-to-tree exact 10 38M 5k 46.6 47.5
tree-to-tree fuzzy 10 790M 130k 49.4 49.7
]
+ nesting 10 190M 66k 49.2 49.8
Table 3: On both the Chinese-English and Arabic-English translation tasks, fuzzy tree-to-tree extraction
outperforms exact tree-to-tree extraction and string-to-string extraction. Brackets indicate statistically
insignificant differences (p ? 0.05).
rule choice; one interesting change was that the fre-
quency of rules with Chinese root VP / IP and En-
glish root VP / S-C increased from 0.2% to 0.7%:
apparently the model learned that it is good to use
rules that pair Chinese and English verbs that sub-
categorize for sentential complements.
5 Conclusion
Though exact tree-to-tree translation tends to ham-
per translation quality by imposing too many con-
straints during both grammar extraction and de-
coding, we have shown that using both source and
target syntax improves translation accuracy when
the model is given the opportunity to learn from
data how strongly to apply syntactic constraints.
Indeed, we have found that the model learns on its
own to choose syntactically richer and more well-
formed structures, demonstrating that source- and
target-side syntax can be used together profitably
as long as they are not allowed to overconstrain the
translation model.
Acknowledgements
Thanks to Steve DeNeefe, Adam Lopez, Jonathan
May, Miles  Osborne, Adam  Pauls, Richard
Schwartz, and the anonymous reviewers for their
valuable help. This research was supported in part
by  DARPA contract  HR0011-06-C-0022  under
subcontract  to  BBN Technologies  and  DARPA
contract HR0011-09-1-0028. S. D. G.
frequency (%)
task side kind s-to-s t-to-t
Chi-Eng source glue 25 18
match 17 30
mismatch 58 52
target glue 25 18
match 9 23
mismatch 66 58
Ara-Eng source glue 36 19
match 17 34
mismatch 48 47
target glue 36 19
match 11 29
mismatch 53 52
Table 4: Moving from string-to-string (s-to-s) ex-
traction to fuzzy tree-to-tree (t-to-t) extraction de-
creases glue rule usage and increases the frequency
of matching substitutions.
1450
frequency (%)
kind s-to-s t-to-t
NP ? NP 16.0 20.7
VP ? VP 3.3 5.9
NN ? NP 3.1 1.3
NP ? VP 2.5 0.8
NP ? NN 2.0 1.4
NP ? entity 1.4 1.6
NN ? NN 1.1 1.0
QP ? entity 1.0 1.3
VV ? VP 1.0 0.7
PU ? NP 0.8 1.1
VV ? VP ? PU 0.2 1.2
PU ? PU 0.1 3.8
Table 5: Comparison of frequency of source-side
rewrites  in  Chinese-English  translation  between
string-to-string (s-to-s) and fuzzy tree-to-tree (t-to-
t) grammars. All rewrites occurring more than 1%
of the time in either system are shown. The label
?entity? stands for handwritten rules for named en-
tities and numbers.
frequency (%)
kind s-to-s t-to-t
NP-C ? NP-C 5.3 8.7
NN ? NN 1.7 3.0
NP-C ? entity 1.1 1.4
DT \NP-C ? DT \NP-C 1.1 2.6
NN ? NP-C 0.8 0.4
NP-C ? VP 0.8 1.1
DT \NP-C ? NP-C 0.8 0.5
NP-C ? DT \NP-C 0.6 0.4
JJ ? JJ 0.5 1.8
NP-C ? NN 0.5 0.3
PP ? PP 0.4 1.7
VP-C ? VP-C 0.4 1.2
VP ? VP 0.4 1.4
IN ? IN 0.1 1.8
, ? , 0.1 1.7
Table 6: Comparison of frequency of target-side
rewrites  in  Chinese-English  translation  between
string-to-string (s-to-s) and fuzzy tree-to-tree (t-
to-t) grammars. All rewrites occurring more than
1% of the time in either system are shown, plus a
few more of interest. The label ?entity? stands for
handwritten rules for named entities and numbers.
References
Vamshi Ambati  and Alon Lavie. 2008. Improving
syntax driven translation models by re-structuring
divergent and non-isomorphic parse tree structures.
In Proc. AMTA-2008 Student Research Workshop,
pages 235?244.
Yehoshua  Bar-Hillel. 1953. A quasi-arithmetical
notation  for  syntactic  description. Language,
29(1):47?58.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical  study  of  smoothing  techniques  for  lan-
guage modeling. Technical Report TR-10-98, Har-
vard University Center for Research in Computing
Technology.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. NAACL HLT 2009, pages 218?226.
David  Chiang. 2005. A hierarchical  phrase-
based model for statistical machine translation. In
Proc. ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins. 1997. Three generative lexicalised
models for statistical parsing. In Proc. ACL-EACL,
pages 16?23.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. ACL
2003 Companion Volume, pages 205?208.
Victoria  Fossum, Kevin  Knight, and  Steven  Abney.
2008. Using syntax  to  improve word alignment
for syntax-based statistical machine translation. In
Proc. Third Workshop on Statistical Machine Trans-
lation, pages 44?52.
Alexander Fraser and Daniel Marcu. 2007. Getting
the structure right for word alignment: LEAF. In
Proc. EMNLP 2007, pages 51?60.
Michel  Galley, Mark  Hopkins, Kevin  Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL 2004, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve  DeNeefe, Wei  Wang, and  Ignacio
Thayer. 2006. Scalable  inference  and  training
of  context-rich  syntactic  translation  models. In
Proc. COLING-ACL 2006, pages 961?968.
Mary Hearne and Andy Way. 2003. Seeing the wood
for the trees: Data-Oriented Translation. InProc.MT
Summit IX, pages 165?172.
1451
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of  locality. In Proc. AMTA 2006, pages
65?73.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. SSST-2, pages 87?95.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. COLING-ACL 2006, pages
609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improv-
ing tree-to-tree translation with packed forests. In
Proc. ACL 2009, pages 558?566.
I. Dan  Melamed, Giorgio  Satta, and  Ben  Welling-
ton. 2004. Generalized multitext grammars. In
Proc. ACL 2004, pages 661?668.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. COLING-ACL
2006, pages 433?440.
Arjen Poutsma. 2000. Data-Oriented Translation. In
Proc. COLING 2000, pages 635?641.
David Talbot and Thorsten Brants. 2008. Random-
ized language models via perfect hash functions. In
Proc. ACL-08: HLT, pages 505?513.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statisti-
cal machine translation. In Proc. NAACL HLT 2009,
pages 236?244.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical  lower  bounds  on
the  complexity  of  translational  equivalence. In
Proc. COLING-ACL 2006, pages 977?984.
Min  Zhang, Hongfei  Jiang, Aiti  Aw, Haizhou  Li,
ChewLim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proc. ACL-08: HLT, pages 559?567.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. Workshop on Statistical Machine Transla-
tion, pages 138?141.
1452
Proceedings of the ACL 2010 Conference Short Papers, pages 209?214,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Optimization of an MDL-Inspired Objective Function for
Unsupervised Part-of-Speech Tagging
Ashish Vaswani1 Adam Pauls2 David Chiang1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{avaswani,chiang}@isi.edu
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
adpauls@eecs.berkeley.edu
Abstract
The Minimum Description Length (MDL)
principle is a method for model selection
that trades off between the explanation of
the data by the model and the complexity
of the model itself. Inspired by the MDL
principle, we develop an objective func-
tion for generative models that captures
the description of the data by the model
(log-likelihood) and the description of the
model (model size). We also develop a ef-
ficient general search algorithm based on
the MAP-EM framework to optimize this
function. Since recent work has shown that
minimizing the model size in a Hidden
Markov Model for part-of-speech (POS)
tagging leads to higher accuracies, we test
our approach by applying it to this prob-
lem. The search algorithm involves a sim-
ple change to EM and achieves high POS
tagging accuracies on both English and
Italian data sets.
1 Introduction
The Minimum Description Length (MDL) princi-
ple is a method for model selection that provides a
generic solution to the overfitting problem (Barron
et al, 1998). A formalization of Ockham?s Razor,
it says that the parameters are to be chosen that
minimize the description length of the data given
the model plus the description length of the model
itself.
It has been successfully shown that minimizing
the model size in a Hidden Markov Model (HMM)
for part-of-speech (POS) tagging leads to higher
accuracies than simply running the Expectation-
Maximization (EM) algorithm (Dempster et al,
1977). Goldwater and Griffiths (2007) employ a
Bayesian approach to POS tagging and use sparse
Dirichlet priors to minimize model size. More re-
cently, Ravi and Knight (2009) alternately mini-
mize the model using an integer linear program
and maximize likelihood using EM to achieve the
highest accuracies on the task so far. However, in
the latter approach, because there is no single ob-
jective function to optimize, it is not entirely clear
how to generalize this technique to other prob-
lems. In this paper, inspired by the MDL princi-
ple, we develop an objective function for genera-
tive models that captures both the description of
the data by the model (log-likelihood) and the de-
scription of the model (model size). By using a
simple prior that encourages sparsity, we cast our
problem as a search for the maximum a poste-
riori (MAP) hypothesis and present a variant of
EM to approximately search for the minimum-
description-length model. Applying our approach
to the POS tagging problem, we obtain higher ac-
curacies than both EM and Bayesian inference as
reported by Goldwater and Griffiths (2007). On a
Italian POS tagging task, we obtain even larger
improvements. We find that our objective function
correlates well with accuracy, suggesting that this
technique might be useful for other problems.
2 MAP EM with Sparse Priors
2.1 Objective function
In the unsupervised POS tagging task, we are
given a word sequence w = w1, . . . ,wN and want
to find the best tagging t = t1, . . . , tN , where
ti ? T , the tag vocabulary. We adopt the problem
formulation of Merialdo (1994), in which we are
given a dictionary of possible tags for each word
type.
We define a bigram HMM
P(w, t | ?) =
N?
i=1
P(w, t | ?) ? P(ti | ti?1) (1)
In maximum likelihood estimation, the goal is to
209
find parameter estimates
?? = arg max
?
log P(w | ?) (2)
= arg max
?
log
?
t
P(w, t | ?) (3)
The EM algorithm can be used to find a solution.
However, we would like to maximize likelihood
and minimize the size of the model simultane-
ously. We define the size of a model as the number
of non-zero probabilities in its parameter vector.
Let ?1, . . . , ?n be the components of ?. We would
like to find
?? = arg min
?
(
? log P(w | ?) + ????0
)
(4)
where ???0, called the L0 norm of ?, simply counts
the number of non-zero parameters in ?. The
hyperparameter ? controls the tradeoff between
likelihood maximization and model minimization.
Note the similarity of this objective function with
MDL?s, where ? would be the space (measured
in nats) needed to describe one parameter of the
model.
Unfortunately, minimization of the L0 norm
is known to be NP-hard (Hyder and Mahata,
2009). It is not smooth, making it unamenable
to gradient-based optimization algorithms. There-
fore, we use a smoothed approximation,
???0 ?
?
i
(
1 ? e
??i
?
)
(5)
where 0 < ? ? 1 (Mohimani et al, 2007). For
smaller values of ?, this closely approximates the
desired function (Figure 1). Inverting signs and ig-
noring constant terms, our objective function is
now:
?? = arg max
?
?
??????log P(w | ?) + ?
?
i
e
??i
?
?
?????? (6)
We can think of the approximate model size as
a kind of prior:
P(?) =
exp?
?
i e
??i
?
Z
(7)
log P(?) = ? ?
?
i
e
??i
? ? log Z (8)
where Z =
?
d?
exp?
?
i e
??i
? is a normalization
constant. Then our goal is to find the maximum
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1  0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Function Values
? i
?=0.005 ?=0.05 ?=0.5 1-||? i|| 0
Figure 1: Ideal model-size term and its approxima-
tions.
a posterior parameter estimate, which we find us-
ing MAP-EM (Bishop, 2006):
?? = arg max
?
log P(w, ?) (9)
= arg max
?
(
log P(w | ?) + log P(?)
)
(10)
Substituting (8) into (10) and ignoring the constant
term log Z, we get our objective function (6) again.
We can exercise finer control over the sparsity
of the tag-bigram and channel probability distri-
butions by using a different ? for each:
arg max
?
(
log P(w | ?) +
?c
?
w,t
e
?P(w|t)
? + ?t
?
t,t?
e
?P(t? |t)
?
)
(11)
In our experiments, we set ?c = 0 since previ-
ous work has shown that minimizing the number
of tag n-gram parameters is more important (Ravi
and Knight, 2009; Goldwater and Griffiths, 2007).
A common method for preferring smaller mod-
els is minimizing the L1 norm,
?
i |?i|. However,
for a model which is a product of multinomial dis-
tributions, the L1 norm is a constant.
?
i
|?i| =
?
i
?i
=
?
t
?
??????
?
w
P(w | t) +
?
t?
P(t? | t)
?
??????
= 2|T |
Therefore, we cannot use the L1 norm as part of
the size term as the result will be the same as the
EM algorithm.
210
2.2 Parameter optimization
To optimize (11), we use MAP EM, which is an it-
erative search procedure. The E step is the same as
in standard EM, which is to calculate P(t | w, ?t),
where the ?t are the parameters in the current iter-
ation t. The M step in iteration (t + 1) looks like
?t+1 = arg max
?
(
EP(t|w,?t)
[
log P(w, t | ?)
]
+
?t
?
t,t?
e
?P(t? |t)
?
) (12)
Let C(t,w; t,w) count the number of times the
word w is tagged as t in t, and C(t, t?; t) the number
of times the tag bigram (t, t?) appears in t. We can
rewrite the M step as
?t+1 = arg max
?
(?
t
?
w
E[C(t,w)] log P(w | t)+
?
t
?
t?
(
E[C(t, t?)] log P(t? | t) + ?te
?P(t? |t)
?
)??????? (13)
subject to the constraints
?
w P(w | t) = 1 and?
t? P(t
? | t) = 1. Note that we can optimize each
term of both summations over t separately. For
each t, the term
?
w
E[C(t,w)] log P(w | t) (14)
is easily optimized as in EM: just let P(w | t) ?
E[C(t,w)]. But the term
?
t?
(
E[C(t, t?)] log P(t? | t) + ?te
?P(t? |t)
?
)
(15)
is trickier. This is a non-convex optimization prob-
lem for which we invoke a publicly available
constrained optimization tool, ALGENCAN (An-
dreani et al, 2007). To carry out its optimization,
ALGENCAN requires computation of the follow-
ing in every iteration:
? Objective function, defined in equation (15).
This is calculated in polynomial time using
dynamic programming.
? Constraints: gt =
?
t? P(t
? | t) ? 1 = 0 for
each tag t ? T . Also, we constrain P(t? | t) to
the interval [, 1].1
1We must have  > 0 because of the log P(t? | t) term
in equation (15). It seems reasonable to set   1N ; in our
experiments, we set  = 10?7.
? Gradient of objective function:
?F
?P(t? | t)
=
E[C(t, t?)]
P(t? | t)
?
?t
?
e
?P(t? |t)
? (16)
? Gradient of equality constraints:
?gt
?P(t?? | t?)
=
?
???
???
1 if t = t?
0 otherwise
(17)
? Hessian of objective function, which is not
required but greatly speeds up the optimiza-
tion:
?2F
?P(t? | t)?P(t? | t)
= ?
E[C(t, t?)]
P(t? | t)2
+ ?t
e
?P(t? |t)
?
?2
(18)
The other second-order partial derivatives are
all zero, as are those of the equality con-
straints.
We perform this optimization for each instance
of (15). These optimizations could easily be per-
formed in parallel for greater scalability.
3 Experiments
We carried out POS tagging experiments on En-
glish and Italian.
3.1 English POS tagging
To set the hyperparameters ?t and ?, we prepared
three held-out sets H1,H2, and H3 from the Penn
Treebank. Each Hi comprised about 24, 000 words
annotated with POS tags. We ran MAP-EM for
100 iterations, with uniform probability initializa-
tion, for a suite of hyperparameters and averaged
their tagging accuracies over the three held-out
sets. The results are presented in Table 2. We then
picked the hyperparameter setting with the highest
average accuracy. These were ?t = 80, ? = 0.05.
We then ran MAP-EM again on the test data with
these hyperparameters and achieved a tagging ac-
curacy of 87.4% (see Table 1). This is higher than
the 85.2% that Goldwater and Griffiths (2007) ob-
tain using Bayesian methods for inferring both
POS tags and hyperparameters. It is much higher
than the 82.4% that standard EM achieves on the
test set when run for 100 iterations.
Using ?t = 80, ? = 0.05, we ran multiple ran-
dom restarts on the test set (see Figure 2). We find
that the objective function correlates well with ac-
curacy, and picking the point with the highest ob-
jective function value achieves 87.1% accuracy.
211
?t
?
0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.0025
10 82.81 82.78 83.10 83.50 83.76 83.70 84.07 83.95 83.75
20 82.78 82.82 83.26 83.60 83.89 84.88 83.74 84.12 83.46
30 82.78 83.06 83.26 83.29 84.50 84.82 84.54 83.93 83.47
40 82.81 83.13 83.50 83.98 84.23 85.31 85.05 83.84 83.46
50 82.84 83.24 83.15 84.08 82.53 84.90 84.73 83.69 82.70
60 83.05 83.14 83.26 83.30 82.08 85.23 85.06 83.26 82.96
70 83.09 83.10 82.97 82.37 83.30 86.32 83.98 83.55 82.97
80 83.13 83.15 82.71 83.00 86.47 86.24 83.94 83.26 82.93
90 83.20 83.18 82.53 84.20 86.32 84.87 83.49 83.62 82.03
100 83.19 83.51 82.84 84.60 86.13 85.94 83.26 83.67 82.06
110 83.18 83.53 83.29 84.40 86.19 85.18 80.76 83.32 82.05
120 83.08 83.65 83.71 84.11 86.03 85.39 80.66 82.98 82.20
130 83.10 83.19 83.52 84.02 85.79 85.65 80.08 82.04 81.76
140 83.11 83.17 83.34 85.26 85.86 85.84 79.09 82.51 81.64
150 83.14 83.20 83.40 85.33 85.54 85.18 78.90 81.99 81.88
Table 2: Average accuracies over three held-out sets for English.
system accuracy (%)
Standard EM 82.4
+ random restarts 84.5
(Goldwater and Griffiths, 2007) 85.2
our approach 87.4
+ random restarts 87.1
Table 1: MAP-EM with a L0 norm achieves higher
tagging accuracy on English than (2007) and much
higher than standard EM.
system zero parameters bigram types
maximum possible 1389 ?
EM, 100 iterations 444 924
MAP-EM, 100 iterations 695 648
Table 3: MAP-EM with a smoothed L0 norm
yields much smaller models than standard EM.
We also carried out the same experiment with stan-
dard EM (Figure 3), where picking the point with
the highest corpus probability achieves 84.5% ac-
curacy.
We also measured the minimization effect of the
sparse prior against that of standard EM. Since our
method lower-bounds all the parameters by , we
consider a parameter ?i as a zero if ?i ? . We
also measured the number of unique tag bigram
types in the Viterbi tagging of the word sequence.
Table 3 shows that our method produces much
smaller models than EM, and produces Viterbi
taggings with many fewer tag-bigram types.
3.2 Italian POS tagging
We also carried out POS tagging experiments on
an Italian corpus from the Italian Turin Univer-
 
0.78
 
0.79 0.8
 
0.81
 
0.82
 
0.83
 
0.84
 
0.85
 
0.86
 
0.87
 
0.88
 
0.89 -532
00-53
000-5
2800-
52600
-
52400
-
52200
-
52000
-
51800
-
51600
-
51400
Tagging accuracy
objective
 function
 value
? t=80,
?=0.05,T
est Set
 24115
 Words
Figure 2: Tagging accuracy vs. objective func-
tion for 1152 random restarts of MAP-EM with
smoothed L0 norm.
sity Treebank (Bos et al, 2009). This test set com-
prises 21, 878 words annotated with POS tags and
a dictionary for each word type. Since this is all
the available data, we could not tune the hyperpa-
rameters on a held-out data set. Using the hyper-
parameters tuned on English (?t = 80, ? = 0.05),
we obtained 89.7% tagging accuracy (see Table 4),
which was a large improvement over 81.2% that
standard EM achieved. When we tuned the hyper-
parameters on the test set, the best setting (?t =
120, ? = 0.05 gave an accuracy of 90.28%.
4 Conclusion
A variety of other techniques in the literature have
been applied to this unsupervised POS tagging
task. Smith and Eisner (2005) use conditional ran-
dom fields with contrastive estimation to achieve
212
?t
?
0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.0025
10 81.62 81.67 81.63 82.47 82.70 84.64 84.82 84.96 84.90
20 81.67 81.63 81.76 82.75 84.28 84.79 85.85 88.49 85.30
30 81.66 81.63 82.29 83.43 85.08 88.10 86.16 88.70 88.34
40 81.64 81.79 82.30 85.00 86.10 88.86 89.28 88.76 88.80
50 81.71 81.71 78.86 85.93 86.16 88.98 88.98 89.11 88.01
60 81.65 82.22 78.95 86.11 87.16 89.35 88.97 88.59 88.00
70 81.69 82.25 79.55 86.32 89.79 89.37 88.91 85.63 87.89
80 81.74 82.23 80.78 86.34 89.70 89.58 88.87 88.32 88.56
90 81.70 81.85 81.00 86.35 90.08 89.40 89.09 88.09 88.50
100 81.70 82.27 82.24 86.53 90.07 88.93 89.09 88.30 88.72
110 82.19 82.49 82.22 86.77 90.12 89.22 88.87 88.48 87.91
120 82.23 78.60 82.76 86.77 90.28 89.05 88.75 88.83 88.53
130 82.20 78.60 83.33 87.48 90.12 89.15 89.30 87.81 88.66
140 82.24 78.64 83.34 87.48 90.12 89.01 88.87 88.99 88.85
150 82.28 78.69 83.32 87.75 90.25 87.81 88.50 89.07 88.41
Table 4: Accuracies on test set for Italian.
 
0.76
 
0.78 0.8
 
0.82
 
0.84
 
0.86
 
0.88 0.9 -1475
00-14
7400-
147300
-
147200
-
147100
-
147000
-
146900
-
146800
-
146700
-
146600
-
146500
-
146400
Tagging accuracy
objective
 function
 value
EM, T
est Set
 24115
 Words
Figure 3: Tagging accuracy vs. likelihood for 1152
random restarts of standard EM.
88.6% accuracy. Goldberg et al (2008) provide
a linguistically-informed starting point for EM to
achieve 91.4% accuracy. More recently, Chiang et
al. (2010) use GIbbs sampling for Bayesian in-
ference along with automatic run selection and
achieve 90.7%.
In this paper, our goal has been to investi-
gate whether EM can be extended in a generic
way to use an MDL-like objective function that
simultaneously maximizes likelihood and mini-
mizes model size. We have presented an efficient
search procedure that optimizes this function for
generative models and demonstrated that maxi-
mizing this function leads to improvement in tag-
ging accuracy over standard EM. We infer the hy-
perparameters of our model using held out data
and achieve better accuracies than (Goldwater and
Griffiths, 2007). We have also shown that the ob-
jective function correlates well with tagging accu-
racy supporting the MDL principle. Our approach
performs quite well on POS tagging for both En-
glish and Italian. We believe that, like EM, our
method can benefit from more unlabeled data, and
there is reason to hope that the success of these
experiments will carry over to other tasks as well.
Acknowledgements
We would like to thank Sujith Ravi, Kevin Knight
and Steve DeNeefe for their valuable input, and
Jason Baldridge for directing us to the Italian
POS data. This research was supported in part by
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
R. Andreani, E. G. Birgin, J. M. Martnez, and M. L.
Schuverdt. 2007. On Augmented Lagrangian meth-
ods with general lower-level constraints. SIAM
Journal on Optimization, 18:1286?1309.
A. Barron, J. Rissanen, and B. Yu. 1998. The min-
imum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorical grammar tree-
bank for italian. In Eighth International Workshop
on Treebanks and Linguistic Theories (TLT8).
D. Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi.
2010. Bayesian inference for Finite-State transduc-
ers. In Proceedings of the North American Associa-
tion of Computational Linguistics.
213
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1?
38.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Proceedings of the 2009 IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
H. Mohimani, M. Babaie-Zadeh, and C. Jutten. 2007.
Fast sparse representation based on smoothed L0
norm. In Proceedings of the 7th International Con-
ference on Independent Component Analysis and
Signal Separation (ICA2007).
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
N. Smith. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the ACL.
214
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856?864,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Rule Markov Models for Fast Tree-to-String Translation
Ashish Vaswani
Information Sciences Institute
University of Southern California
avaswani@isi.edu
Haitao Mi
Institute of Computing Technology
Chinese Academy of Sciences
htmi@ict.ac.cn
Liang Huang and David Chiang
Information Sciences Institute
University of Southern California
{lhuang,chiang}@isi.edu
Abstract
Most statistical machine translation systems
rely on composed rules (rules that can be
formed out of smaller rules in the grammar).
Though this practice improves translation by
weakening independence assumptions in the
translation model, it nevertheless results in
huge, redundant grammars, making both train-
ing and decoding inefficient. Here, we take the
opposite approach, where we only use min-
imal rules (those that cannot be formed out
of other rules), and instead rely on a rule
Markov model of the derivation history to
capture dependencies between minimal rules.
Large-scale experiments on a state-of-the-art
tree-to-string translation system show that our
approach leads to a slimmer model, a faster
decoder, yet the same translation quality (mea-
sured using Bleu) as composed rules.
1 Introduction
Statistical machine translation systems typically
model the translation process as a sequence of trans-
lation steps, each of which uses a translation rule,
for example, a phrase pair in phrase-based transla-
tion or a tree-to-string rule in tree-to-string transla-
tion. These rules are usually applied independently
of each other, which violates the conventional wis-
dom that translation should be done in context.
To alleviate this problem, most state-of-the-art sys-
tems rely on composed rules, which are larger rules
that can be formed out of smaller rules (includ-
ing larger phrase pairs that can be formerd out of
smaller phrase pairs), as opposed to minimal rules,
which are rules that cannot be formed out of other
rules. Although this approach does improve trans-
lation quality dramatically by weakening the inde-
pendence assumptions in the translation model, they
suffer from two main problems. First, composition
can cause a combinatorial explosion in the number
of rules. To avoid this, ad-hoc limits are placed dur-
ing composition, like upper bounds on the number
of nodes in the composed rule, or the height of the
rule. Under such limits, the grammar size is man-
ageable, but still much larger than the minimal-rule
grammar. Second, due to large grammars, the de-
coder has to consider many more hypothesis transla-
tions, which slows it down. Nevertheless, the advan-
tages outweigh the disadvantages, and to our knowl-
edge, all top-performing systems, both phrase-based
and syntax-based, use composed rules. For exam-
ple, Galley et al (2004) initially built a syntax-based
system using only minimal rules, and subsequently
reported (Galley et al, 2006) that composing rules
improves Bleu by 3.6 points, while increasing gram-
mar size 60-fold and decoding time 15-fold.
The alternative we propose is to replace composed
rules with a rule Markov model that generates rules
conditioned on their context. In this work, we re-
strict a rule?s context to the vertical chain of ances-
tors of the rule. This ancestral context would play
the same role as the context formerly provided by
rule composition. The dependency treelet model de-
veloped by Quirk and Menezes (2006) takes such
an approach within the framework of dependency
translation. However, their study leaves unanswered
whether a rule Markov model can take the place
of composed rules. In this work, we investigate the
use of rule Markov models in the context of tree-
856
to-string translation (Liu et al, 2006; Huang et al,
2006). We make three new contributions.
First, we carry out a detailed comparison of rule
Markov models with composed rules. Our experi-
ments show that, using trigram rule Markov mod-
els, we achieve an improvement of 2.2 Bleu over
a baseline of minimal rules. When we compare
against vertically composed rules, we find that our
rule Markov model has the same accuracy, but our
model is much smaller and decoding with our model
is 30% faster. When we compare against full com-
posed rules, we find that our rule Markov model still
often reaches the same level of accuracy, again with
savings in space and time.
Second, we investigate methods for pruning rule
Markov models, finding that even very simple prun-
ing criteria actually improve the accuracy of the
model, while of course decreasing its size.
Third, we present a very fast decoder for tree-to-
string grammars with rule Markov models. Huang
and Mi (2010) have recently introduced an efficient
incremental decoding algorithm for tree-to-string
translation, which operates top-down and maintains
a derivation history of translation rules encountered.
This history is exactly the vertical chain of ancestors
corresponding to the contexts in our rule Markov
model, which makes it an ideal decoder for our
model.
We start by describing our rule Markov model
(Section 2) and then how to decode using the rule
Markov model (Section 3).
2 Rule Markov models
Our model which conditions the generation of a rule
on the vertical chain of its ancestors, which allows it
to capture interactions between rules.
Consider the example Chinese-English tree-to-
string grammar in Figure 1 and the example deriva-
tion in Figure 2. Each row is a derivation step; the
tree on the left is the derivation tree (in which each
node is a rule and its children are the rules that sub-
stitute into it) and the tree pair on the right is the
source and target derived tree. For any derivation
node r, let anc1(r) be the parent of r (or  if it has no
parent), anc2(r) be the grandparent of node r (or  if
it has no grandparent), and so on. Let ancn1(r) be the
chain of ancestors anc1(r) ? ? ? ancn(r).
The derivation tree is generated as follows. With
probability P(r1 | ), we generate the rule at the root
node, r1. We then generate rule r2 with probability
P(r2 | r1), and so on, always taking the leftmost open
substitution site on the English derived tree, and gen-
erating a rule ri conditioned on its chain of ancestors
with probability P(ri | ancn1(ri)). We carry on until
no more children can be generated. Thus the proba-
bility of a derivation tree T is
P(T ) =
?
r?T
P(r | ancn1(r)) (1)
For the minimal rule derivation tree in Figure 2, the
probability is:
P(T ) = P(r1 | ) ? P(r2 | r1) ? P(r3 | r1)
? P(r4 | r1, r3) ? P(r6 | r1, r3, r4)
? P(r7 | r1, r3, r4) ? P(r5 | r1, r3) (2)
Training We run the algorithm of Galley et al
(2004) on word-aligned parallel text to obtain a sin-
gle derivation of minimal rules for each sentence
pair. (Unaligned words are handled by attaching
them to the highest node possible in the parse tree.)
The rule Markov model
can then be trained on the path set of these deriva-
tion trees.
Smoothing We use interpolation with absolute
discounting (Ney et al, 1994):
Pabs(r | ancn1(r)) =
max
{
c(r | ancn1(r)) ? Dn, 0
}
?
r? c(r? | ancn1(r?))
+ (1 ? ?n)Pabs(r | ancn?11 (r)), (3)
where c(r | ancn1(r)) is the number of times we have
seen rule r after the vertical context ancn1(r), Dn is
the discount for a context of length n, and (1 ? ?n) is
set to the value that makes the smoothed probability
distribution sum to one.
We experiment with bigram and trigram rule
Markov models. For each, we try different values of
D1 and D2, the discount for bigrams and trigrams,
respectively. Ney et al (1994) suggest using the fol-
lowing value for the discount Dn:
Dn =
n1
n1 + n2
(4)
857
rule id translation rule
r1 IP(x1:NP x2:VP) ? x1 x2
r2 NP(Bu`sh??) ? Bush
r3 VP(x1:PP x2:VP) ? x2 x1
r4 PP(x1:P x2:NP) ? x1 x2
r5 VP(VV(ju?x??ng) AS(le) NPB(hu?`ta?n)) ? held talks
r6 P(yu?) ? with
r?6 P(yu?) ? and
r7 NP(Sha?lo?ng) ? Sharon
Figure 1: Example tree-to-string grammar.
derivation tree derived tree pair
 IP@ : IP@
r1
IP@
NP@1 VP@2 : NP
@1 VP@2
r1
r2 r3
IP@
NP@1
Bu`sh??
VP@2
PP@2.1 VP@2.2
: Bush VP@2.2 PP@2.1
r1
r2 r3
r4 r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1 NP@2.1.2
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks P@2.1.1 NP@2.1.2
r1
r2 r3
r4
r6 r7
r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks with Sharon
Figure 2: Example tree-to-string derivation. Each row shows a rewriting step; at each step, the leftmost nonterminal
symbol is rewritten using one of the rules in Figure 1.
858
Here, n1 and n2 are the total number of n-grams with
exactly one and two counts, respectively. For our
corpus, D1 = 0.871 and D2 = 0.902. Additionally,
we experiment with 0.4 and 0.5 for Dn.
Pruning In addition to full n-gram Markov mod-
els, we experiment with three approaches to build
smaller models to investigate if pruning helps. Our
results will show that smaller models indeed give a
higher Bleu score than the full bigram and trigram
models. The approaches we use are:
? RM-A: We keep only those contexts in which
more than P unique rules were observed. By
optimizing on the development set, we set P =
12.
? RM-B: We keep only those contexts that were
observed more than P times. Note that this is a
superset of RM-A. Again, by optimizing on the
development set, we set P = 12.
? RM-C: We try a more principled approach
for learning variable-length Markov models in-
spired by that of Bejerano and Yona (1999),
who learn a Prediction Suffix Tree (PST). They
grow the PST in an iterative manner by start-
ing from the root node (no context), and then
add contexts to the tree. A context is added if
the KL divergence between its predictive distri-
bution and that of its parent is above a certain
threshold and the probability of observing the
context is above another threshold.
3 Tree-to-string decoding with rule
Markov models
In this paper, we use our rule Markov model frame-
work in the context of tree-to-string translation.
Tree-to-string translation systems (Liu et al, 2006;
Huang et al, 2006) have gained popularity in recent
years due to their speed and simplicity. The input to
the translation system is a source parse tree and the
output is the target string. Huang and Mi (2010) have
recently introduced an efficient incremental decod-
ing algorithm for tree-to-string translation. The de-
coder operates top-down and maintains a derivation
history of translation rules encountered. The history
is exactly the vertical chain of ancestors correspond-
ing to the contexts in our rule Markov model. This
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV@2.2.1
ju?x??ng
AS@2.2.2
le
NP@2.2.3
hu?`ta?n
Figure 3: Example input parse tree with tree addresses.
makes incremental decoding a natural fit with our
generative story. In this section, we describe how
to integrate our rule Markov model into this in-
cremental decoding algorithm. Note that it is also
possible to integrate our rule Markov model with
other decoding algorithms, for example, the more
common non-incremental top-down/bottom-up ap-
proach (Huang et al, 2006), but it would involve
a non-trivial change to the decoding algorithms to
keep track of the vertical derivation history, which
would result in significant overhead.
Algorithm Given the input parse tree in Figure 3,
Figure 4 illustrates the search process of the incre-
mental decoder with the grammar of Figure 1. We
write X@? for a tree node with label X at tree address
? (Shieber et al, 1995). The root node has address ,
and the ith child of node ? has address ?.i. At each
step, the decoder maintains a stack of active rules,
which are rules that have not been completed yet,
and the rightmost (n ? 1) English words translated
thus far (the hypothesis), where n is the order of the
word language model (in Figure 4, n = 2). The stack
together with the translated English words comprise
a state of the decoder. The last column in the fig-
ure shows the rule Markov model probabilities with
the conditioning context. In this example, we use a
trigram rule Markov model.
After initialization, the process starts at step 1,
where we predict rule r1 (the shaded rule) with prob-
ability P(r1 | ) and push its English side onto the
stack, with variables replaced by the correspond-
ing tree nodes: x1 becomes NP@1 and x2 becomes
VP@2. This gives us the following stack:
s = [ NP@1 VP@2]
The dot () indicates the next symbol to process in
859
stack hyp. MR prob.
0 [<s>  IP@ </s>] <s>
1 [<s>  IP@ </s>] [ NP@1 VP@2] <s> P(r1 | )
2 [<s>  IP@ </s>] [ NP@1 VP@2] [ Bush] <s> P(r2 | r1)
3 [<s>  IP@ </s>] [ NP@1 VP@2] [Bush  ] . . . Bush
4 [<s>  IP@ </s>] [NP@1  VP@2] . . . Bush
5 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] . . . Bush P(r3 | r1)
6 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks] . . . Bush P(r5 | r1, r3)
7 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held  talks] . . . held
8 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks  ] . . . talks
9 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] . . . talks
10 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] . . . talks P(r4 | r1, r3)
11 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ with] . . . with P(r6 | r3, r4)
12 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [with  ] . . . with
13 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . with
14 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . with P(r7 | r3, r4)
11? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ and] . . . and P(r?6 | r3, r4)
12? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [and  ] . . . and
13? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . and
14? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . and P(r7 | r3, r4)
15 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [Sharon  ] . . . Sharon
16 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1 NP@2.1.2  ] . . . Sharon
17 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2 PP@2.1  ] . . . Sharon
18 [<s>  IP@ </s>] [NP@1 VP@2  ] . . . Sharon
19 [<s> IP@  </s>] . . . Sharon
20 [<s> IP@ </s>  ] . . . </s>
Figure 4: Simulation of incremental decoding with rule Markov model. The solid arrows indicate one path and the
dashed arrows indicate an alternate path.
860
VP@2
VP@2.2 PP@2.1
P@2.1.1
yu?
NP@2.1.2
Figure 5: Vertical context r3 r4 which allows the model
to correctly translate yu? as with.
the English word order. We expand node NP@1 first
with English word order. We then predict lexical rule
r2 with probability P(r2 | r1) and push rule r2 onto
the stack:
[ NP@1 VP@2 ] [ Bush]
In step 3, we perform a scan operation, in which
we append the English word just after the dot to the
current hypothesis and move the dot after the word.
Since the dot is at the end of the top rule in the stack,
we perform a complete operation in step 4 where we
pop the finished rule at the top of the stack. In the
scan and complete steps, we don?t need to compute
rule probabilities.
An interesting branch occurs after step 10 with
two competing lexical rules, r6 and r?6. The Chinese
word yu? can be translated as either a preposition with
(leading to step 11) or a conjunction and (leading
to step 11?). The word n-gram model does not have
enough information to make the correct choice, with.
As a result, good translations might be pruned be-
cause of the beam. However, our rule Markov model
has the correct preference because of the condition-
ing ancestral sequence (r3, r4), shown in Figure 5.
Since VP@2.2 has a preference for yu? translating to
with, our corpus statistics will give a higher proba-
bility to P(r6 | r3, r4) than P(r?6 | r3, r4). This helps
the decoder to score the correct translation higher.
Complexity analysis With the incremental decod-
ing algorithm, adding rule Markov models does not
change the time complexity, which is O(nc|V |g?1),
where n is the sentence length, c is the maximum
number of incoming hyperedges for each node in the
translation forest, V is the target-language vocabu-
lary, and g is the order of the n-gram language model
(Huang and Mi, 2010). However, if one were to use
rule Markov models with a conventional CKY-style
bottom-up decoder (Liu et al, 2006), the complexity
would increase to O(nCm?1|V |4(g?1)), where C is the
maximum number of outgoing hyperedges for each
node in the translation forest, and m is the order of
the rule Markov model.
4 Experiments and results
4.1 Setup
The training corpus consists of 1.5M sentence pairs
with 38M/32M words of Chinese/English, respec-
tively. Our development set is the newswire portion
of the 2006 NIST MT Evaluation test set (616 sen-
tences), and our test set is the newswire portion of
the 2008 NIST MT Evaluation test set (691 sen-
tences).
We word-aligned the training data using GIZA++
followed by link deletion (Fossum et al, 2008),
and then parsed the Chinese sentences using the
Berkeley parser (Petrov and Klein, 2007). To extract
tree-to-string translation rules, we applied the algo-
rithm of Galley et al (2004). We trained our rule
Markov model on derivations of minimal rules as
described above. Our trigram word language model
was trained on the target side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing. The base feature
set for all systems is similar to the set used in Mi et
al. (2008). The features are combined into a standard
log-linear model, which we trained using minimum
error-rate training (Och, 2003) to maximize the Bleu
score on the development set.
At decoding time, we again parse the input
sentences using the Berkeley parser, and convert
them into translation forests using rule pattern-
matching (Mi et al, 2008). We evaluate translation
quality using case-insensitive IBM Bleu-4, calcu-
lated by the script mteval-v13a.pl.
4.2 Results
Table 1 presents the main results of our paper. We
used grammars of minimal rules and composed rules
of maximum height 3 as our baselines. For decod-
ing, we used a beam size of 50. Using the best
bigram rule Markov models and the minimal rule
grammar gives us an improvement of 1.5 Bleu over
the minimal rule baseline. Using the best trigram
rule Markov model brings our gain up to 2.3 Bleu.
861
grammar rule Markov max parameters (?10
6) Bleu time
model rule height full dev+test test (sec/sent)
minimal None 3 4.9 0.3 24.2 1.2
RM-B bigram 3 4.9+4.7 0.3+0.5 25.7 1.8
RM-A trigram 3 4.9+7.6 0.3+0.6 26.5 2.0
vertical composed None 7 176.8 1.3 26.5 2.9
composed None 3 17.5 1.6 26.4 2.2
None 7 448.7 3.3 27.5 6.8
RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2
Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same
level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for
both the full model and the model filtered for the concatenation of the development and test sets (dev+test).
These gains are statistically significant with p <
0.01, using bootstrap resampling with 1000 samples
(Koehn, 2004). We find that by just using bigram
context, we are able to get at least 1 Bleu point
higher than the minimal rule grammar. It is interest-
ing to see that using just bigram rule interactions can
give us a reasonable boost. We get our highest gains
from using trigram context where our best perform-
ing rule Markov model gives us 2.3 Bleu points over
minimal rules. This suggests that using longer con-
texts helps the decoder to find better translations.
We also compared rule Markov models against
composed rules. Since our models are currently lim-
ited to conditioning on vertical context, the closest
comparison is against vertically composed rules. We
find that our approach performs equally well using
much less time and space.
Comparing against full composed rules, we find
that our system matches the score of the base-
line composed rule grammar of maximum height 3,
while using many fewer parameters. (It should be
noted that a parameter in the rule Markov model is
just a floating-point number, whereas a parameter in
the composed-rule system is an entire rule; there-
fore the difference in memory usage would be even
greater.) Decoding with our model is 0.2 seconds
faster per sentence than with composed rules.
These experiments clearly show that rule Markov
models with minimal rules increase translation qual-
ity significantly and with lower memory require-
ments than composed rules. One might wonder if
the best performance can be obtained by combin-
ing composed rules with a rule Markov model. This
rule Markov D1
Bleu time
model dev (sec/sent)
RM-A 0.871 29.2 1.8
RM-B 0.4 29.9 1.8
RM-C 0.871 29.8 1.8
RM-Full 0.4 29.7 1.9
Table 2: For rule bigrams, RM-B with D1 = 0.4 gives the
best results on the development set.
rule Markov D1 D2
Bleu time
model dev (sec/sent)
RM-A 0.5 0.5 30.3 2.0
RM-B 0.5 0.5 29.9 2.0
RM-C 0.5 0.5 30.1 2.0
RM-Full 0.4 0.5 30.1 2.2
Table 3: For rule bigrams, RM-A with D1, D2 = 0.5 gives
the best results on the development set.
is straightforward to implement: the rule Markov
model is still defined over derivations of minimal
rules, but in the decoder?s prediction step, the rule
Markov model?s value on a composed rule is cal-
culated by decomposing it into minimal rules and
computing the product of their probabilities. We find
that using our best trigram rule Markov model with
composed rules gives us a 0.5 Bleu gain on top of
the composed rule grammar, statistically significant
with p < 0.05, achieving our highest score of 28.0.1
4.3 Analysis
Tables 2 and 3 show how the various types of rule
Markov models compare, for bigrams and trigrams,
1For this experiment, a beam size of 100 was used.
862
parameters (?106) Bleu dev/test time (sec/sent)
dev/test without RMM with RMM without/with RMM
2.6 31.0/27.0 31.1/27.4 4.5/7.0
2.9 31.5/27.7 31.4/27.3 5.6/8.1
3.3 31.4/27.5 31.4/28.0 6.8/9.2
Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.
D2
D1
0.4 0.5 0.871
0.4 30.0 30.0
0.5 29.3 30.3
0.902 30.0
Table 4: RM-A is robust to different settings of Dn on the
development set.
parameters (?106) Bleu time
dev+test dev test (sec/sent)
1.2 30.2 26.1 2.8
1.3 30.1 26.5 2.9
1.3 30.1 26.2 3.2
Table 5: Comparison of vertically composed rules using
various settings (maximum rule height 7).
respectively. It is interesting that the full bigram and
trigram rule Markov models do not give our high-
est Bleu scores; pruning the models not only saves
space but improves their performance. We think that
this is probably due to overfitting.
Table 4 shows that the RM-A trigram model does
fairly well under all the settings of Dn we tried. Ta-
ble 5 shows the performance of vertically composed
rules at various settings. Here we have chosen the
setting that gives the best performance on the test
set for inclusion in Table 1.
Table 6 shows the performance of fully composed
rules and fully composed rules with a rule Markov
Model at various settings.2 In the second line (2.9
million rules), the drop in Bleu score resulting from
adding the rule Markov model is not statistically sig-
nificant.
5 Related Work
Besides the Quirk and Menezes (2006) work dis-
cussed in Section 1, there are two other previous
2For these experiments, a beam size of 100 was used.
efforts both using a rule bigram model in machine
translation, that is, the probability of the current rule
only depends on the immediate previous rule in the
vertical context, whereas our rule Markov model
can condition on longer and sparser derivation his-
tories. Among them, Ding and Palmer (2005) also
use a dependency treelet model similar to Quirk and
Menezes (2006), and Liu and Gildea (2008) use a
tree-to-string model more like ours. Neither com-
pared to the scenario with composed rules.
Outside of machine translation, the idea of weak-
ening independence assumptions by modeling the
derivation history is also found in parsing (Johnson,
1998), where rule probabilities are conditioned on
parent and grand-parent nonterminals. However, be-
sides the difference between parsing and translation,
there are still two major differences. First, our work
conditions rule probabilities on parent and grandpar-
ent rules, not just nonterminals. Second, we com-
pare against a composed-rule system, which is anal-
ogous to the Data Oriented Parsing (DOP) approach
in parsing (Bod, 2003). To our knowledge, there has
been no direct comparison between a history-based
PCFG approach and DOP approach in the parsing
literature.
6 Conclusion
In this paper, we have investigated whether we can
eliminate composed rules without any loss in trans-
lation quality. We have developed a rule Markov
model that captures vertical bigrams and trigrams of
minimal rules, and tested it in the framework of tree-
to-string translation. We draw three main conclu-
sions from our experiments. First, our rule Markov
models dramatically improve a grammar of minimal
rules, giving an improvement of 2.3 Bleu. Second,
when we compare against vertically composed rules
we are able to get about the same Bleu score, but
our model is much smaller and decoding with our
863
model is faster. Finally, when we compare against
full composed rules, we find that we can reach the
same level of performance under some conditions,
but in order to do so consistently, we believe we
need to extend our model to condition on horizon-
tal context in addition to vertical context. We hope
that by modeling context in both axes, we will be
able to completely replace composed-rule grammars
with smaller minimal-rule grammars.
Acknowledgments
We would like to thank Fernando Pereira, Yoav
Goldberg, Michael Pust, Steve DeNeefe, Daniel
Marcu and Kevin Knight for their comments.
Mi?s contribution was made while he was vis-
iting USC/ISI. This work was supported in part
by DARPA under contracts HR0011-06-C-0022
(subcontract to BBN Technologies), HR0011-09-1-
0028, and DOI-NBC N10AP20031, by a Google
Faculty Research Award to Huang, and by the Na-
tional Natural Science Foundation of China under
contracts 60736014 and 90920004.
References
Gill Bejerano and Golan Yona. 1999. Modeling pro-
tein families using probabilistic suffix trees. In Proc.
RECOMB, pages 15?24. ACM Press.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of EACL, pages 19?26.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of ACL, pages 541?
548.
Victoria Fossum, Kevin Knight, and Steve Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613?
632.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Ding Liu and Daniel Gildea. 2008. Improved tree-to-
string transducer for machine translation. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 62?69.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192?199.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1?38.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404?411.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of NAACL
HLT, pages 9?16.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
864
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212?216,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-Independent Parsing with Empty Elements
Shu Cai and David Chiang
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{shucai,chiang}@isi.edu
Yoav Goldberg
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg@cs.bgu.ac.il
Abstract
We  present  a  simple, language-independent
method for integrating recovery of empty ele-
ments into syntactic parsing. This method out-
performs  the  best  published  method  we  are
aware of on English and a recently published
method on Chinese.
1 Introduction
Empty elements in the syntactic analysis of a sen-
tence are markers that show where a word or phrase
might otherwise be expected to appear, but does not.
They play an important role in understanding the
grammatical relations in the sentence. For example,
in the tree of Figure 2a, the first empty element (*)
marks where John would be if believed were in the
active voice (someone believed. . .), and the second
empty element (*T*) marks where the manwould be
ifwhowere not fronted (John was believed to admire
who?).
Empty elements exist in many languages and serve
different purposes. In languages such as Chinese and
Korean, where subjects and objects can be dropped
to avoid duplication, empty elements are particularly
important, as they indicate the position of dropped
arguments. Figure 1 gives an example of a Chinese
parse tree with empty elements. The first empty el-
ement (*pro*) marks the subject of the whole sen-
tence, a pronoun inferable from context. The second
empty element (*PRO*) marks the subject of the de-
pendent VP (sh?sh? f?l? ti?ow?n).
The Penn Treebanks (Marcus et  al., 1993; Xue
et al, 2005) contain detailed annotations of empty
elements. Yet  most  parsing  work  based  on  these
resources has ignored empty elements, with some
.IP
. .VP
. .VP
. .IP
. .VP
. .NP
. .NN
.??
ti?ow?n
clause
.
NN
.??
f?l?
law
.
VV
.??
sh?sh?
implement
.
NP
.-NONE-
.*PRO*
.
VV
.??
zh?ngzh?
suspend
.
ADVP
.AD
.??
z?nsh?
for now
.
NP
.-NONE-
.*pro*
Figure 1: Chinese parse tree with empty elements marked.
The meaning of the sentence is, ?Implementation of the
law is temporarily suspended.?
notable exceptions. Johnson (2002) studied empty-
element  recovery in English, followed by several
others (Dienes and Dubey, 2003; Campbell, 2004;
Gabbard et al, 2006); the best results we are aware of
are due to Schmid (2006). Recently, empty-element
recovery for Chinese has begun to receive attention:
Yang and Xue (2010) treat it as classification prob-
lem, while Chung and Gildea (2010) pursue several
approaches for both Korean and Chinese, and ex-
plore applications to machine translation.
Our intuition motivating this work is that empty
elements are an integral part of syntactic structure,
and should be constructed jointly with it, not added
in afterwards. Moreover, we expect empty-element
recovery to improve as the parsing quality improves.
Our method makes use of a strong syntactic model,
the PCFGs with latent annotation of Petrov et al
(2006), which  we  extend  to  predict  empty  cate-
212
gories  by the  use  of lattice  parsing. The method
is language-independent and performs very well on
both languages we tested it on: for English, it out-
performs the best published method we are aware of
(Schmid, 2006), and for Chinese, it outperforms the
method of Yang and Xue (2010).
1
2 Method
Our method is fairly simple. We take a state-of-the-
art parsing model, the Berkeley parser (Petrov et al,
2006), train it on data with explicit empty elements,
and test it on word lattices that can nondeterminis-
tically insert empty elements anywhere. The idea is
that the state-splitting of the parsing model will en-
able it to learn where to expect empty elements to be
inserted into the test sentences.
Tree transformations Prior to training, we alter
the annotation of empty elements so that the termi-
nal label is a consistent symbol (?), the preterminal
label is the type of the empty element, and -NONE-
is deleted (see Figure 2b). This simplifies the lat-
tices because there is only one empty symbol, and
helps the parsing model to learn dependencies be-
tween nonterminal labels and empty-category types
because there is no intervening -NONE-.
Then, following Schmid (2006), if a constituent
contains an empty element that is linked to another
node with label X, then we append /X to its label.
If there is more than one empty element, we pro-
cess them bottom-up (see Figure 2b). This helps the
parser learn to expect where to find empty elements.
In our experiments, we did this only for elements of
type *T*. Finally, we train the Berkeley parser on the
preprocessed training data.
Lattice parsing Unlike the training data, the test
data does not mark any empty elements. We allow
the parser to produce empty elements by means of
lattice-parsing (Chappelier et al, 1999), a general-
ization of CKY parsing allowing it to parse a word-
lattice instead of a predetermined list of terminals.
Lattice parsing adds a layer of flexibility to exist-
ing parsing technology, and allows parsing in sit-
uations where the yield of  the tree  is  not  known
in advance. Lattice parsing originated in the speech
1
Unfortunately, not  enough  information  was  available  to
carry out comparison with the method of Chung and Gildea
(2010).
processing community  (Hall, 2005; Chappelier  et
al., 1999), and  was  recently  applied  to  the  task
of joint clitic-segmentation and syntactic-parsing in
Hebrew  (Goldberg  and  Tsarfaty, 2008; Goldberg
and Elhadad, 2011) and Arabic (Green and Man-
ning, 2010). Here, we use lattice parsing for empty-
element recovery.
We use a modified version of the Berkeley parser
which allows handling lattices as input.
2
The modifi-
cation is fairly straightforward: Each lattice arc cor-
respond to a lexical item. Lexical items are now in-
dexed by their start and end states rather than by
their sentence position, and the initialization proce-
dure of the CKY chart is changed to allow lexical
items of spans greater than 1. We then make the nec-
essary adjustments to the parsing algorithm to sup-
port this change: trying rules involving preterminals
even when the span is greater than 1, and not relying
on span size for identifying lexical items.
At test time, we first construct a lattice for each
test sentence that allows 0, 1, or 2 empty symbols
(?) between each pair of words or at the start/end of
the sentence. Then we feed these lattices through our
lattice parser to produce trees with empty elements.
Finally, we reverse the transformations that had been
applied to the training data.
3 Evaluation Measures
Evaluation metrics for empty-element recovery are
not well established, and previous studies use a vari-
ety of metrics. We review several of these here and
additionally propose a unified evaluation of parsing
and empty-element recovery.
3
If A and B are multisets, let A(x) be the number
of occurrences of x in A, let |A| = ?x A(x), and
let A ? B be the multiset such that (A ? B)(x) =
min(A(x), B(x)). If T is the multiset of ?items? in the
trees being tested andG is the multiset of ?items? in
the gold-standard trees, then
precision =
|G ? T |
|T | recall =
|G ? T |
|G|
F1 =
2
1
precision
+
1
recall
2
The modified parser is available at http://www.cs.bgu.
ac.il/~yoavg/software/blatt/
3
We provide a scoring script which supports all of these eval-
uation metrics. The code is available at http://www.isi.edu/
~chiang/software/eevalb.py .
213
.SBARQ
. .SQ
. .VP
. .S
. .VP
. .VP
. .NP
.-NONE-
.*T*
.
VB
.admire
.
TO
.to
.
NP
.-NONE-
.*
.
VBN
.believed
.
.NP
.NNP
.John
.
VBZ
.is
.
WHNP
.WP
.who
.SBARQ
. .SQ/WHNP
. .VP/WHNP/NP
. .S/WHNP/NP
. .VP/WHNP
. .VP/WHNP
. .NP/WHNP
.*T*
.?
.
VB
.admire
.
TO
.to
.
NP
.*
.?
.
VBN
.believed
.
.NP
.NNP
.John
.
VBZ
.is
.
WHNP
.WP
.who
(a) (b)
Figure 2: English parse tree with empty elements marked. (a) As annotated in the Penn Treebank. (b) With empty
elements reconfigured and slash categories added.
where ?items? are defined differently for each met-
ric, as  follows. Define  a nonterminal node, for
present purposes, to be a node which is neither a ter-
minal nor preterminal node.
The  standard  PARSEVAL metric  (Black  et  al.,
1991) counts labeled nonempty brackets: items are
(X, i, j) for each nonempty nonterminal node, where
X is its label and i, j are the start and end positions
of its span.
Yang  and  Xue  (2010)  simply  count unlabeled
empty elements: items are (i, i) for each empty ele-
ment, where i is its position. If multiple empty ele-
ments occur at the same position, they only count the
last one.
The metric originally proposed by Johnson (2002)
counts labeled empty brackets: items are (X/t, i, i) for
each empty nonterminal node, where X is its label
and t is the type of the empty element it dominates,
but also (t, i, i) for each empty element not domi-
nated by an empty nonterminal node.
4
The following
structure has an empty nonterminal dominating two
empty elements:
.SBAR
. .S
.-NONE-
.*T*
.
-NONE-
.0
Johnson  counts  this  as (SBAR, i, i), (S/*T*, i, i);
Schmid  (2006)  counts  it  as  a  single
4
This happens in the Penn Treebank for types *U* and 0, but
never in the Penn Chinese Treebank except by mistake.
(SBAR-S/*T*, i, i).5 We  tried  to  follow  Schmid
in a generic way: we collapse any vertical chain of
empty nonterminals into a single nonterminal.
In order to avoid problems associated with cases
like this, we suggest a pair of simpler metrics. The
first is to count labeled empty elements, i.e., items
are (t, i, i) for each empty element, and the second,
similar in spirit to SParseval (Roark et al, 2006), is
to count all labeled brackets, i.e., items are (X, i, j)
for  each nonterminal  node (whether  nonempty or
empty). These two metrics, together with part-of-
speech accuracy, cover all possible nodes in the tree.
4 Experiments and Results
English As is standard, we trained the parser on
sections 02?21 of  the Penn Treebank Wall  Street
Journal corpus, used section 00 for development, and
section 23 for testing. We ran 6 cycles of training;
then, because we were unable to complete the 7th
split-merge cycle with the default setting of merg-
ing 50% of splits, we tried increasing merges to 75%
and ran 7 cycles of training. Table 1 presents our
results. We chose the parser settings that gave the
best labeled empty elements F1 on the dev set, and
used these settings for the test set. We outperform the
state of the art at recovering empty elements, as well
as achieving state of the art accuracy at recovering
phrase structure.
5
This difference is not small; scores using Schmid?s metric
are lower by roughly 1%. There are other minor differences in
Schmid?s metric which we do not detail here.
214
Labeled Labeled All Labeled
Empty Brackets Empty Elements Brackets
Section System P R F1 P R F1 P R F1
00 Schmid (2006) 88.3 82.9 85.5 89.4 83.8 86.5 87.1 85.6 86.3
split 5? merge 50% 91.0 79.8 85.0 93.1 81.8 87.1 90.4 88.7 89.5
split 6? merge 50% 91.9 81.1 86.1 93.6 82.4 87.6 90.4 89.1 89.7
split 6? merge 75% 92.7 80.7 86.3 94.6 82.0 87.9 90.3 88.5 89.3
split 7? merge 75% 91.0 80.4 85.4 93.2 82.1 87.3 90.5 88.9 89.7
23 Schmid (2006) 86.1 81.7 83.8 87.9 83.0 85.4 86.8 85.9 86.4
split 6? merge 75% 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3
Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.
Unlabeled Labeled All Labeled
Empty Elements Empty Elements Brackets
Task System P R F1 P R F1 P R F1
Dev split 5? merge 50% 82.5 58.0 68.1 72.6 51.8 60.5 84.6 80.7 82.6
split 6? merge 50% 76.4 60.5 67.5 68.2 55.1 60.9 83.2 81.3 82.2
split 7? merge 50% 74.9 58.7 65.8 65.9 52.5 58.5 82.7 81.1 81.9
Test Yang and Xue (2010) 80.3 57.9 63.2
split 6? merge 50% 74.0 61.3 67.0 66.0 54.5 58.6 82.7 80.8 81.7
Table 2: Results on Penn (Chinese) Treebank.
Chinese We  also  experimented  on  a  subset  of
the  Penn  Chinese  Treebank  6.0. For  comparabil-
ity  with  previous  work  (Yang  and  Xue, 2010),
we trained the parser on sections 0081?0900, used
sections 0041?0080 for development, and sections
0001?0040 and 0901?0931 for testing. The results
are shown in Table 2.We selected the 6th split-merge
cycle based on the labeled empty elements F1 mea-
sure. The unlabeled empty elements column shows
that our system outperforms the baseline system of
Yang and Xue (2010). We also analyzed the empty-
element recall by type (Table 3). Our system outper-
formed that of Yang and Xue (2010) especially on
*pro*, used for dropped arguments, and *T*, used
for relative clauses and topicalization.
5 Discussion and Future Work
The  empty-element  recovery  method  we  have
presented  is  simple, highly  effective, and  fully
integrated with  state  of  the  art  parsing. We hope
to  exploit  cross-lingual  information  about  empty
elements  in  machine  translation. Chung  and
Gildea (2010)  have  shown that  such  information
indeed helps translation, and we plan to extend this
work  by  handling  more  empty  categories  (rather
Total Correct Recall
Type Gold YX Ours YX Ours
*pro* 290 125 159 43.1 54.8
*PRO* 299 196 199 65.6 66.6
*T* 578 338 388 58.5 67.1
*RNR* 32 20 15 62.5 46.9
*OP* 134 20 65 14.9 48.5
* 19 5 3 26.3 15.8
Table 3: Recall on different types of empty categories.
YX = (Yang and Xue, 2010), Ours = split 6?.
than just *pro* and *PRO*), and to incorporate them
into a syntax-based translation model instead of a
phrase-based model.
We also plan to extend our work here to recover
coindexation information (links between a moved el-
ement and the trace which marks the position it was
moved from). As a step towards shallow semantic
analysis, this may further benefit other natural lan-
guage processing tasks such as machine translation
and summary generation.
Acknowledgements
We would like to thank Slav Petrov for his help in
running the Berkeley parser, and Yaqin Yang, Bert
215
Xue, Tagyoung Chung, and Dan Gildea for their an-
swering our  many questions. We would also like
to  thank  our  colleagues  in  the  Natural  Language
Group  at  ISI for  meaningful  discussions  and  the
anonymous reviewers for their thoughtful sugges-
tions. This work was supported in part by DARPA
under contracts HR0011-06-C-0022 (subcontract to
BBN Technologies) and DOI-NBC N10AP20031,
and by NSF under contract IIS-0908532.
References
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proc. DARPA Speech and Natu-
ral Language Workshop.
Richard Campbell. 2004. Using linguistic principles to
recover empty categories. In Proc. ACL.
J.-C. Chappelier, M. Rajman, R. Aragu?es, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Proc. Traitement Automatique du Langage Naturel
(TALN).
Tagyoung Chung and Daniel  Gildea. 2010. Effects
of empty categories on machine translation. In Proc.
EMNLP.
Pe?ter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proc. EMNLP.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006.
Fully parsing the Penn Treebank. In Proc. NAACL
HLT.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFG-LA lat-
tice parser. In Proc. of ACL.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis. In
Proc of COLING-2010.
Keith B. Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University, Providence, RI, USA.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm  for  recovering  empty  nodes  and  their  an-
tecedents. In Proc. ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Slav  Petrov, Leon Barrett, Romain  Thibaux, and  Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL.
Brian  Roark, Mary  Harper, Eugene  Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart,
and Lisa Yung. 2006. SParseval: Evaluation metrics
for parsing speech. In Proc. LREC.
Helmut Schmid. 2006. Trace prediction and recovery
with unlexicalized PCFGs and slash features. In Proc.
COLING-ACL.
Nianwen  Xue, Fei  Xia, Fu-dong  Chiou, and  Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proc. COLING.
216
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323?328,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Models and Training for Unsupervised Preposition Sense Disambiguation
Dirk Hovy and Ashish Vaswani and Stephen Tratz and
David Chiang and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh,avaswani,stratz,chiang,hovy}@isi.edu
Abstract
We present a preliminary study on unsu-
pervised preposition sense disambiguation
(PSD), comparing different models and train-
ing techniques (EM, MAP-EM with L0 norm,
Bayesian inference using Gibbs sampling). To
our knowledge, this is the first attempt at un-
supervised preposition sense disambiguation.
Our best accuracy reaches 56%, a significant
improvement (at p <.001) of 16% over the
most-frequent-sense baseline.
1 Introduction
Reliable disambiguation of words plays an impor-
tant role in many NLP applications. Prepositions
are ubiquitous?they account for more than 10% of
the 1.16m words in the Brown corpus?and highly
ambiguous. The Preposition Project (Litkowski and
Hargraves, 2005) lists an average of 9.76 senses
for each of the 34 most frequent English preposi-
tions, while nouns usually have around two (Word-
Net nouns average about 1.2 senses, 2.7 if monose-
mous nouns are excluded (Fellbaum, 1998)). Dis-
ambiguating prepositions is thus a challenging and
interesting task in itself (as exemplified by the Sem-
Eval 2007 task, (Litkowski and Hargraves, 2007)),
and holds promise for NLP applications such as
Information Extraction or Machine Translation.1
Given a sentence such as the following:
In the morning, he shopped in Rome
we ultimately want to be able to annotate it as
1See (Chan et al, 2007) for how using WSD can help MT.
in/TEMPORAL the morning/TIME he/PERSON
shopped/SOCIAL in/LOCATIVE
Rome/LOCATION
Here, the preposition in has two distinct meanings,
namely a temporal and a locative one. These mean-
ings are context-dependent. Ultimately, we want
to disambiguate prepositions not by and for them-
selves, but in the context of sequential semantic la-
beling. This should also improve disambiguation of
the words linked by the prepositions (here, morn-
ing, shopped, and Rome). We propose using un-
supervised methods in order to leverage unlabeled
data, since, to our knowledge, there are no annotated
data sets that include both preposition and argument
senses. In this paper, we present our unsupervised
framework and show results for preposition disam-
biguation. We hope to present results for the joint
disambiguation of preposition and arguments in a
future paper.
The results from this work can be incorporated
into a number of NLP problems, such as seman-
tic tagging, which tries to assign not only syntac-
tic, but also semantic categories to unlabeled text.
Knowledge about semantic constraints of preposi-
tional constructions would not only provide better
label accuracy, but also aid in resolving preposi-
tional attachment problems. Learning by Reading
approaches (Mulkar-Mehta et al, 2010) also cru-
cially depend on unsupervised techniques as the
ones described here for textual enrichment.
Our contributions are:
? we present the first unsupervised preposition
sense disambiguation (PSD) system
323
? we compare the effectiveness of various models
and unsupervised training methods
? we present ways to extend this work to prepo-
sitional arguments
2 Preliminaries
A preposition p acts as a link between two words, h
and o. The head word h (a noun, adjective, or verb)
governs the preposition. In our example above, the
head word is shopped. The object of the preposi-
tional phrase (usually a noun) is denoted o, in our
example morning and Rome. We will refer to h and
o collectively as the prepositional arguments. The
triple h, p, o forms a syntactically and semantically
constrained structure. This structure is reflected in
dependency parses as a common construction. In
our example sentence above, the respective struc-
tures would be shopped in morning and shopped in
Rome. The senses of each element are denoted by a
barred letter, i.e., p? denotes the preposition sense, h?
denotes the sense of the head word, and o? the sense
of the object.
3 Data
We use the data set for the SemEval 2007 PSD
task, which consists of a training (16k) and a test
set (8k) of sentences with sense-annotated preposi-
tions following the sense inventory of The Preposi-
tion Project, TPP (Litkowski and Hargraves, 2005).
It defines senses for each of the 34 most frequent
prepositions. There are on average 9.76 senses per
preposition. This corpus was chosen as a starting
point for our study since it allows a comparison with
the original SemEval task. We plan to use larger
amounts of additional training data.
We used an in-house dependency parser to extract
the prepositional constructions from the data (e.g.,
?shop/VB in/IN Rome/NNP?). Pronouns and num-
bers are collapsed into ?PRO? and ?NUM?, respec-
tively.
In order to constrain the argument senses, we con-
struct a dictionary that lists for each word all the
possible lexicographer senses according to Word-
Net. The set of lexicographer senses (45) is a higher
level abstraction which is sufficiently coarse to allow
for a good generalization. Unknown words are as-
sumed to have all possible senses applicable to their
respective word class (i.e. all noun senses for words
labeled as nouns, etc).
4 Graphical Model
ph o
p?h? o?
h o
p?h? o?
h o
p?h? o?
a)
b)
c)
Figure 1: Graphical Models. a) 1st order HMM. b)
variant used in experiments (one model/preposition,
thus no conditioning on p). c) incorporates further
constraints on variables
As shown by Hovy et al (2010), preposition
senses can be accurately disambiguated using only
the head word and object of the PP. We exploit this
property of prepositional constructions to represent
the constraints between h, p, and o in a graphical
model. We define a good model as one that reason-
ably constrains the choices, but is still tractable in
terms of the number of parameters being estimated.
As a starting point, we choose the standard first-
order Hidden Markov Model as depicted in Figure
1a. Since we train a separate model for each preposi-
tion, we can omit all arcs to p. This results in model
1b. The joint distribution over the network can thus
be written as
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (1)
P (p?|h?) ? P (o?|p?) ? P (o|o?)
We want to incorporate as much information as
possible into the model to constrain the choices. In
Figure 1c, we condition p? on both h? and o?, to reflect
the fact that prepositions act as links and determine
324
their sense mainly through context. In order to con-
strain the object sense o?, we condition on h?, similar
to a second-order HMM. The actual object o is con-
ditioned on both p? and o?. The joint distribution is
equal to
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (2)
P (o?|h?) ? P (p?|h?, o?) ? P (o|o?, p?)
Though we would like to also condition the prepo-
sition sense p? on the head word h (i.e., an arc be-
tween them in 1c) in order to capture idioms and
fixed phrases, this would increase the number of pa-
rameters prohibitively.
5 Training
The training method largely determines how well the
resulting model explains the data. Ideally, the sense
distribution found by the model matches the real
one. Since most linguistic distributions are Zipfian,
we want a training method that encourages sparsity
in the model.
We briefly introduce different unsupervised train-
ing methods and discuss their respective advantages
and disadvantages. Unless specified otherwise, we
initialized all models uniformly, and trained until the
perplexity rate stopped increasing or a predefined
number of iterations was reached. Note that MAP-
EM and Bayesian Inference require tuning of some
hyper-parameters on held-out data, and are thus not
fully unsupervised.
5.1 EM
We use the EM algorithm (Dempster et al, 1977) as
a baseline. It is relatively easy to implement with ex-
isting toolkits like Carmel (Graehl, 1997). However,
EM has a tendency to assume equal importance for
each parameter. It thus prefers ?general? solutions,
assigning part of the probability mass to unlikely
states (Johnson, 2007). We ran EM on each model
for 100 iterations, or until the perplexity stopped de-
creasing below a threshold of 10?6.
5.2 EM with Smoothing and Restarts
In addition to the baseline, we ran 100 restarts with
random initialization and smoothed the fractional
counts by adding 0.1 before normalizing (Eisner,
2002). Smoothing helps to prevent overfitting. Re-
peated random restarts help escape unfavorable ini-
tializations that lead to local maxima. Carmel pro-
vides options for both smoothing and restarts.
5.3 MAP-EM with L0 Norm
Since we want to encourage sparsity in our mod-
els, we use the MDL-inspired technique intro-
duced by Vaswani et al (2010). Here, the goal
is to increase the data likelihood while keeping
the number of parameters small. The authors use
a smoothed L0 prior, which encourages probabil-
ities to go down to 0. The prior involves hyper-
parameters ?, which rewards sparsity, and ?, which
controls how close the approximation is to the true
L0 norm.2 We perform a grid search to tune the
hyper-parameters of the smoothed L0 prior for ac-
curacy on the preposition against, since it has a
medium number of senses and instances. For HMM,
we set ?trans =100.0, ?trans =0.005, ?emit =1.0,
?emit =0.75. The subscripts trans and emit de-
note the transition and emission parameters. For
our model, we set ?trans =70.0, ?trans =0.05,
?emit =110.0, ?emit =0.0025. The latter resulted
in the best accuracy we achieved.
5.4 Bayesian Inference
Instead of EM, we can use Bayesian inference with
Gibbs sampling and Dirichlet priors (also known as
the Chinese Restaurant Process, CRP). We follow
the approach of Chiang et al (2010), running Gibbs
sampling for 10,000 iterations, with a burn-in pe-
riod of 5,000, and carry out automatic run selec-
tion over 10 random restarts.3 Again, we tuned the
hyper-parameters of our Dirichlet priors for accu-
racy via a grid search over the model for the prepo-
sition against. For both models, we set the concen-
tration parameter ?trans to 0.001, and ?emit to 0.1.
This encourages sparsity in the model and allows for
a more nuanced explanation of the data by shifting
probability mass to the few prominent classes.
2For more details, the reader is referred to Vaswani et al
(2010).
3Due to time and space constraints, we did not run the 1000
restarts used in Chiang et al (2010).
325
result table
Page 1
HMM
0.40 (0.40)
0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)
0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)
baseline Vanilla EM
EM, smoothed, 
100 random 
restarts
MAP-EM + 
smoothed L0 
norm
CRP, 10 random 
restarts
our model
Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAP-
EM+smoothed L0 norm on our model. Italics denote significant improvement over baseline at p <.001.
Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)
6 Results
Given a sequence h, p, o, we want to find the se-
quence of senses h?, p?, o? that maximizes the joint
probability. Since unsupervised methods use the
provided labels indiscriminately, we have to map the
resulting predictions to the gold labels. The pre-
dicted label sequence h?, p?, o? generated by the model
via Viterbi decoding can then be compared to the
true key. We use many-to-1 mapping as described
by Johnson (2007) and used in other unsupervised
tasks (Berg-Kirkpatrick et al, 2010), where each
predicted sense is mapped to the gold label it most
frequently occurs with in the test data. Success is
measured by the percentage of accurate predictions.
Here, we only evaluate p?.
The results presented in Table 1 were obtained
on the SemEval test set. We report results both
with and without against, since we tuned the hyper-
parameters of two training methods on this preposi-
tion. To test for significance, we use a two-tailed
t-test, comparing the number of correctly labeled
prepositions. As a baseline, we simply label all word
types with the same sense, i.e., each preposition to-
ken is labeled with its respective name. When using
many-to-1 accuracy, this technique is equivalent to a
most-frequent-sense baseline.
Vanilla EM does not improve significantly over
the baseline with either model, all other methods
do. Adding smoothing and random restarts increases
the gain considerably, illustrating how important
these techniques are for unsupervised training. We
note that EM performs better with the less complex
HMM.
CRP is somewhat surprisingly roughly equivalent
to EM with smoothing and random restarts. Accu-
racy might improve with more restarts.
MAP-EM with L0 normalization produces the
best result (56%), significantly outperforming the
baseline at p < .001. With more parameters (9.7k
vs. 3.7k), which allow for a better modeling of
the data, L0 normalization helps by zeroing out in-
frequent ones. However, the difference between
our complex model and the best HMM (EM with
smoothing and random restarts, 55%) is not signifi-
cant.
The best (supervised) system in the SemEval task
(Ye and Baldwin, 2007) reached 69% accuracy. The
best current supervised system we are aware of
(Hovy et al, 2010) reaches 84.8%.
7 Related Work
The semantics of prepositions were topic of a special
issue of Computational Linguistics (Baldwin et al,
2009). Preposition sense disambiguation was one of
the SemEval 2007 tasks (Litkowski and Hargraves,
2007), and was subsequently explored in a number
of papers using supervised approaches: O?Hara and
Wiebe (2009) present a supervised preposition sense
disambiguation approach which explores different
settings; Tratz and Hovy (2009), Hovy et al (2010)
make explicit use of the arguments for preposition
sense disambiguation, using various features. We
differ from these approaches by using unsupervised
methods and including argument labeling.
The constraints of prepositional constructions
have been explored by Rudzicz and Mokhov (2003)
and O?Hara and Wiebe (2003) to annotate the se-
mantic role of complete PPs with FrameNet and
Penn Treebank categories. Ye and Baldwin (2006)
explore the constraints of prepositional phrases for
326
semantic role labeling. We plan to use the con-
straints for argument disambiguation.
8 Conclusion and Future Work
We evaluate the influence of two different models (to
represent constraints) and three unsupervised train-
ing methods (to achieve sparse sense distributions)
on PSD. Using MAP-EM with L0 norm on our
model, we achieve an accuracy of 56%. This is a
significant improvement (at p <.001) over the base-
line and vanilla EM. We hope to shorten the gap to
supervised systems with more unlabeled data. We
also plan on training our models with EM with fea-
tures (Berg-Kirkpatrick et al, 2010).
The advantage of our approach is that the models
can be used to infer the senses of the prepositional
arguments as well as the preposition. We are cur-
rently annotating the data to produce a test set with
Amazon?s Mechanical Turk, in order to measure la-
bel accuracy for the preposition arguments.
Acknowledgements
We would like to thank Steve DeNeefe, Jonathan
Graehl, Victoria Fossum, and Kevin Knight, as well
as the anonymous reviewers for helpful comments
on how to improve the paper. We would also like
to thank Morgan from Curious Palate for letting us
write there. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program and by DARPA under con-
tract DOI-NBC N10AP20031.
References
Tim Baldwin, Valia Kordoni, and Aline Villavicencio.
2009. Prepositions in applications: A survey and in-
troduction to the special issue. Computational Lin-
guistics, 35(2):119?149.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Unsu-
pervised Learning with Features. In North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Annual Meeting ? Association
For Computational Linguistics, volume 45, pages 33?
40.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for Finite-State transducers. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 447?455. Association for
Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
ISI/USC.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion semantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL, pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting se-
mantic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
327
Frank Rudzicz and Serguei A. Mokhov. 2003. Towards
a heuristic categorization of prepositional phrases in
english with wordnet. Technical report, Cornell
University, arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Patrick Ye and Tim Baldwin. 2006. Semantic role la-
beling of prepositional phrases. ACM Transactions
on Asian Language Information Processing (TALIP),
5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
328
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 455?460,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Two Easy Improvements to Lexical Weighting
David Chiang and Steve DeNeefe and Michael Pust
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{chiang,sdeneefe,pust}@isi.edu
Abstract
We introduce two simple improvements to the
lexical weighting features of Koehn, Och, and
Marcu  (2003)  for  machine  translation: one
which  smooths  the  probability  of  translating
word f to word e by simplifying English mor-
phology, and one which conditions it  on the
kind of training data that f and e co-occurred
in. These new variations lead to improvements
of up to +0.8 BLEU, with an average improve-
ment of +0.6 BLEU across two language pairs,
two genres, and two translation systems.
1 Introduction
Lexical weighting features (Koehn et al, 2003) es-
timate the probability of a phrase pair or translation
rule word-by-word. In this paper, we introduce two
simple improvements to these features: one which
smooths  the  probability  of  translating  word f to
word e using English morphology, and one which
conditions it on the kind of training data that f and
e co-occurred in. These new variations lead to im-
provements of up to+0.8BLEU, with an average im-
provement of +0.6BLEU across two language pairs,
two genres, and two translation systems.
2 Background
Since there  are  slight  variations  in  how the  lexi-
cal weighting features are computed, we begin by
defining the baseline lexical weighting features. If
f = f1 ? ? ? fn and e = e1 ? ? ? em are a training sentence
pair, let ai (1 ? i ? n) be the (possibly empty) set of
positions in f that ei is aligned to.
First, compute a word translation table from the
word-aligned parallel text: for each sentence pair and
each i, let
c( f j, ei)? c( f j, ei) +
1
|ai|
for j ? ai (1)
c(NULL, ei)? c(NULL, ei) + 1 if |ai| = 0 (2)
Then
t(e | f ) = c( f , e)?
e c( f , e)
(3)
where f can be NULL.
Second, during phrase-pair extraction, store with
each phrase pair the alignments between the words
in the phrase pair. If it is observed with more than
one word alignment pattern, store the most frequent
pattern.
Third, for each phrase pair ( f? , e?, a), compute
t(e? | f? ) =
|e?|
?
i=1
?
?
?
?
?
?
?
?
?
?
?
1
|ai|
?
j?ai
t(e?i | f? j) if |ai| > 0
t(e?i | NULL) otherwise
(4)
This generalizes to synchronous CFG rules in the ob-
vious way.
Similarly, compute the reverse probability t( f? | e?).
Then add two new model features
? log t(e? | f? ) and ? log t( f? | e?)
455
translation
feature (7) (8)
small LM 26.7 24.3
large LM 31.4 28.2
? log t(e? | f? ) 9.3 9.9
? log t( f? | e?) 5.8 6.3
Table 1: Although the language models prefer translation
(8), which translates ?? and ?? as singular nouns, the
lexical weighting features prefer translation (7), which in-
correctly generates plural nouns. All features are negative
log-probabilities, so lower numbers indicate preference.
3 Morphological smoothing
Consider the following example Chinese sentence:
(5) ???
W?n Ji?b?o
Wen Jiabao
??
bi?osh?
said
,
,
,
????
K?t?d?w?
C?te d?Ivoire
?
sh?
is
??
Zh?nggu?
China
?
z?i
in
??
F?izh?u
Africa
?
de
?s
?
h?o
good
??
p?ngy?u
friend
,
,
,
?
h?o
good
??
hu?b?n
partner
.
.
.
(6) Human: Wen Jiabao said that C?te d?Ivoire is
a good friend and a good partner of China?s in
Africa.
(7) MT (baseline): Wen  Jiabao  said  that  Cote
d?Ivoire  is  China?s  good friends, and  good
partners in Africa.
(8) MT (better):Wen Jiabao said that Cote d?Ivoire
is  China?s  good friend and  good partner in
Africa.
The baseline machine translation (7) incorrectly gen-
erates plural nouns. Even though the language mod-
els (LMs) prefer singular nouns, the lexical weight-
ing features prefer plural nouns (Table 1).
1
The reason for this is that the Chinese words do not
have any marking for number. Therefore the infor-
mation needed to mark friend and partner for num-
ber must come from the context. The LMs are able
to capture this context: the 5-gram is China?s good
1
The presence of an extra comma in translation (7) affects
the LM scores only slightly; removing the comma would make
them 26.4 and 32.0.
f e t(e | f ) t( f | e) tm(e | f ) tm( f | e)
?? friends 0.44 0.44 0.47 0.48
?? friend 0.21 0.58 0.19 0.48
?? partners 0.44 0.60 0.40 0.53
?? partner 0.13 0.40 0.17 0.53
Table 2: The morphologically-smoothed lexical weight-
ing features weaken the preference for singular or plural
translations, with the exception of t(friends | ??).
friend is observed in our large LM, and the 4-gram
China?s good friend in our small LM, but China?s
good friends is not observed in either LM. Likewise,
the 5-grams good friend and good partner and good
friends and good partners are both observed in our
LMs, but neither good friend and good partners nor
good friends and good partner is.
By contrast, the lexical weighting tables (Table 2,
columns 3?4), which ignore context, have a strong
preference for plural translations, except in the case
of t(?? | friend). Therefore  we hypothesize  that,
for Chinese-English translation, we should weaken
the lexical weighting features? morphological pref-
erences so that more contextual features can do their
work.
Running a morphological stemmer (Porter, 1980)
on  the  English  side  of  the  parallel  data  gives  a
three-way parallel text: for each sentence, we have
French f, English e, and stemmed English e?. We can
then build two word translation tables, t(e? | f ) and
t(e | e?), and form their product
tm(e | f ) =
?
e?
t(e? | f )t(e | e?) (9)
Similarly, we can compute tm( f | e) in the opposite
direction.
2
(See Table 2, columns 5?6.) These tables
can then be extended to phrase pairs or synchronous
CFG rules as before and added as two new features
of the model:
? log tm(e? | f? ) and ? log tm( f? | e?)
The feature tm(e? | f? ) does still prefer certain word-
forms, as can be seen in Table 2. But because e is
generated from e? and not from f , we are protected
from the situation where a rare f leads to poor esti-
mates for the e.
2
Since the Porter stemmer is deterministic, we always have
t(e? | e) = 1.0, so that tm( f | e) = t( f | e?), as seen in the last
column of Table 2.
456
When  we  applied  an  analogous  approach  to
Arabic-English translation, stemming  both  Arabic
and English, we generated very large lexicon tables,
but saw no statistically significant change in BLEU.
Perhaps this is  not surprising, because  in  Arabic-
English translation (unlike Chinese-English transla-
tion), the source language is morphologically richer
than the target language. So we may benefit from fea-
tures that preserve this information, while smoothing
over morphological differences blurs important dis-
tinctions.
4 Conditioning on provenance
Typical machine translation systems are trained on
a fixed set of training data ranging over a variety of
genres, and if the genre of an input sentence is known
in advance, it is usually advantageous to use model
parameters tuned for that genre.
Consider the following Arabic sentence, from a
weblog (words written left-to-right):
(10) ????
wlEl
perhaps
???
h*A
this
???
AHd
one
???
Ahm
main
??????
Alfrwq
differences
???
byn
between
???
Swr
images
?????
AnZmp
systems
?????
AlHkm
ruling
????????
AlmqtrHp
proposed
.
.
.
(11) Human: Perhaps this is one of the most impor-
tant differences between the images of the pro-
posed ruling systems.
(12) MT (baseline): This may be one of the most
important differences between pictures of the
proposed ruling regimes.
(13) MT (better): Perhaps this is one of the most im-
portant differences between the images of the
proposed regimes.
The Arabic word ???? can be translated asmay or per-
haps (among others), with the latter more common
according to t(e | f ), as shown in Table 3. But some
genres favor perhaps more or less strongly. Thus,
both translations (12) and (13) are good, but the lat-
ter uses a slightly more informal register appropriate
to the genre.
Following Matsoukas et al (2009), we assign each
training sentence pair a set of binary features which
we call s-features:
t(e | f ) ts(e | f )
f e ? nw web bn un
???? may 0.13 0.12 0.16 0.09 0.13
???? perhaps 0.20 0.23 0.32 0.42 0.19
Table 3: Different genres have different preferences for
word translations. Key: nw = newswire, web = Web, bn =
broadcast news, un = United Nations proceedings.
? Whether the sentence pair came from a particu-
lar genre, for example, newswire or web
? Whether the sentence pair came from a particu-
lar collection, for example, FBIS or UN
Matsoukas et  al. (2009)  use these s-features  to
compute  weights  for  each  training  sentence  pair,
which are in turn used for computing various model
features. They found that the sentence-level weights
were most helpful for computing the lexical weight-
ing  features  (p.c.). The  mapping  from  s-features
to  sentence  weights  was  chosen  to  optimize  ex-
pected TER on held-out data. A drawback of this
method is that we must now learn the mapping from
s-features to sentence-weights and then the model
feature weights. Therefore, we tried an alternative
that incorporates s-features into the model itself.
For each s-feature s, we compute new word trans-
lation tables ts(e | f ) and ts( f | e) estimated from
only those sentence pairsf on which s fires, and ex-
tend them to phrases/rules as before. The idea is to
use these probabilities as new features in the model.
However, two  challenges  arise: first, many  word
pairs are unseen for a given s, resulting in zero or
undefined probabilities; second, this adds many new
features for each rule, which requires a lot of space.
To address the problem of unseen word pairs, we
use Witten-Bell smoothing (Witten and Bell, 1991):
t?s(e | f ) = ? f sts(e | f ) + (1 ? ? f s)t(e | f ) (14)
? f s =
c( f , s)
c( f , s) + d( f , s)
(15)
where c( f , s) is the number of times f has been ob-
served in sentences with s-feature s, and d( f , s) is the
number of e types observed aligned to f in sentences
with s-feature s.
For each s-feature s, we add two model features
? log t?s(e? | f? )
t(e? | f? )
and ? log t?s( f? | e?)
t( f? | e?)
457
Arabic-English Chinese-English
newswire web newswire web
system features Dev Test Dev Test Dev Test Dev Test
string-to-string baseline 47.1 43.8 37.1 38.4 28.7 26.0 23.2 25.9
full
2
47.7 44.2
?
37.4 39.0 29.5 26.8 23.8 26.3
string-to-tree baseline 47.3 43.6 37.7 39.6 29.2 26.4 23.0 26.0
full 47.7 44.3 38.3 40.2 29.8 27.1 23.4 26.6
Table 4: Our variations on lexical weighting improve translation quality significantly across 16 different test conditions.
All improvements are significant at the p < 0.01 level, except where marked with an asterisk (?), indicating p < 0.05.
In order to address the space problem, we use the
following heuristic: for any given rule, if the absolute
value of one of these features is less than log 2, we
discard it for that rule.
5 Experiments
Setup We  tested  these  features  on  two  ma-
chine  translation  systems: a  hierarchical  phrase-
based (string-to-string) system (Chiang, 2005) and
a syntax-based (string-to-tree) system (Galley et al,
2004; Galley et al, 2006). For Arabic-English trans-
lation, both systems were trained on 190+220 mil-
lion words of parallel data; for Chinese-English, the
string-to-string system was trained on 240+260 mil-
lion words of parallel data, and the string-to-tree sys-
tem, 58+65 million words. Both used two language
models, one trained on the combined English sides
of the Arabic-English and Chinese-English data, and
one trained on 4 billion words of English data.
The baseline string-to-string system already incor-
porates some simple provenance features: for each
s-feature s, there is a feature P(s | rule). Both base-
line also include a variety of other features (Chiang
et al, 2008; Chiang et al, 2009; Chiang, 2010).
Both systems were trained using MIRA (Cram-
mer et al, 2006; Watanabe et al, 2007; Chiang et al,
2008) on a held-out set, then tested on two more sets
(Dev and Test) disjoint from the data used for rule
extraction and for  MIRA training. These datasets
have roughly 1000?3000 sentences (30,000?70,000
words) and are drawn from test sets from the NIST
MT evaluation and development sets from the GALE
program.
Individual  tests We  first  tested  morphological
smoothing  using  the  string-to-string  system  on
Chinese-English  translation. The  morphologically
smoothed system generated the improved translation
(8) above, and generally gave a small improvement:
task features Dev
Chi-Eng nw baseline 28.7
morph 29.1
We then tested the provenance-conditioned fea-
tures on both Arabic-English and Chinese-English,
again using the string-to-string system:
task features Dev
Ara-Eng nw baseline 47.1
(Matsoukas et al, 2009) 47.3
provenance
2
47.7
Chi-Eng nw baseline 28.7
provenance
2
29.4
The  translations  (12)  and  (13)  come  from  the
Arabic-English baseline and provenance systems.
For Arabic-English, we also compared against lex-
ical  weighting  features  that  use  sentence  weights
kindly provided to us by Matsoukas et al Our fea-
tures performed better, although it should be noted
that those sentence weights had been optimized for
a different translation model.
Combined  tests Finally, we  tested  the  features
across a wider range of tasks. For Chinese-English
translation, we  combined  the  morphologically-
smoothed  and  provenance-conditioned  lexical
weighting  features; for  Arabic-English, we  con-
tinued  to  use  only  the  provenance-conditioned
features. We  tested  using  both  systems, and  on
both  newswire  and  web  genres. The  results  are
shown in Table 4. The features produce statistically
significant improvements across all 16 conditions.
2
In these systems, an error crippled the t( f | e), tm( f | e), and
ts( f | e) features. Time did not permit rerunning all of these sys-
tems with the error fixed, but partial results suggest that it did
not have a significant impact.
458
-0.4-0.3-0.2-0.1 0
 0.1 0.2 0.3 0.4 0.5
-0.8 -0.6 -0.4 -0.2  0  0.2  0.4  0.6  0.8
Web
Newswire
bc bn LDC2005T06 NameEntityLDC2006E24LDC2006E92LDC2006G05LDC2007E08
LDC2007E101LDC2007E103 LDC2008G05lexiconng nwNewsExplorer UNweb
wl
Figure 1: Feature  weights  for  provenance-conditioned  features: string-to-string, Chinese-English, web  versus
newswire. A higher weight indicates a more useful source of information, while a negative weight indicates a less
useful or possibly problematic source. For clarity, only selected points are labeled. The diagonal line indicates where
the two weights would be equal relative to the original t(e | f ) feature weight.
Figure 1 shows the feature weights obtained for
the provenance-conditioned features ts( f | e) in the
string-to-string Chinese-English system, trained on
newswire and web data. On the diagonal are cor-
pora that were equally useful in either genre. Surpris-
ingly, the UN data received strong positive weights,
indicating usefulness in both genres. Two lists  of
named entities received large weights: the LDC list
(LDC2005T34)  in  the  positive  direction  and  the
NewsExplorer  list  in  the  negative  direction, sug-
gesting  that  there  are  noisy  entries  in  the  latter.
The corpus LDC2007E08, which contains parallel
data mined from comparable corpora (Munteanu and
Marcu, 2005), received strong negative weights.
Off the diagonal are corpora favored in only one
genre or the other: above, we see that the wl (we-
blog)  and ng (newsgroup)  genres  are  more help-
ful for web translation, as expected (although web
oddly seems less helpful), as well as LDC2006G05
(LDC/FBIS/NVTC Parallel Text V2.0). Below are
corpora  more  helpful  for  newswire  translation,
like LDC2005T06 (Chinese News Translation Text
Part 1).
6 Conclusion
Many  different  approaches  to  morphology  and
provenance in machine translation are possible. We
have chosen to implement our approach as exten-
sions  to  lexical  weighting  (Koehn  et  al., 2003),
which is nearly ubiquitous, because it is defined at
the level of word alignments. For this reason, the
features we have introduced should be easily ap-
plicable to a wide range of phrase-based, hierarchi-
cal phrase-based, and syntax-based systems. While
the improvements obtained using them are not enor-
mous, we have demonstrated that they help signif-
icantly across many different conditions, and over
very strong baselines. We therefore fully expect that
these  new  features  would  yield  similar  improve-
ments in other systems as well.
Acknowledgements
We would like to thank Spyros Matsoukas and col-
leagues at BBN for providing their sentence-level
weights  and  important  insights  into  their  corpus-
weighting work. This work was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies.
459
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of  syntactic  and struc-
tural translation features. In Proc. EMNLP 2008, pages
224?233.
David  Chiang, Kevin  Knight, and  Wei  Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL HLT, pages 218?226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005,
pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, pages 1443?1452.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
HLT-NAACL 2004, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In Proc. COLING-ACL
2006, pages 961?968.
Philipp  Koehn, Franz Josef  Och, and  Daniel  Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Spyros  Matsoukas, Antti-Veikko I.  Rosti, and  Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proc. EMNLP 2009,
pages 708?717.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007, pages 764?773.
Ian H.  Witten  and  Timothy C.  Bell. 1991. The
zero-frequency problem: Estimating the probabilities
of novel events in adaptive text compression. IEEE
Trans. Information Theory, 37(4):1085?1094.
460
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?319,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the `0-norm
Ashish Vaswani Liang Huang David Chiang
University of Southern California
Information Sciences Institute
{avaswani,lhuang,chiang}@isi.edu
Abstract
Two decades after their invention, the IBM
word-based translation models, widely avail-
able in the GIZA++ toolkit, remain the dom-
inant approach to word alignment and an in-
tegral part of many statistical translation sys-
tems. Although many models have surpassed
them in accuracy, none have supplanted them
in practice. In this paper, we propose a simple
extension to the IBM models: an `0 prior to en-
courage sparsity in the word-to-word transla-
tion model. We explain how to implement this
extension efficiently for large-scale data (also
released as a modification to GIZA++) and
demonstrate, in experiments on Czech, Ara-
bic, Chinese, and Urdu to English translation,
significant improvements over IBM Model 4
in both word alignment (up to +6.7 F1) and
translation quality (up to +1.4 Bleu).
1 Introduction
Automatic word alignment is a vital component of
nearly all current statistical translation pipelines. Al-
though state-of-the-art translation models use rules
that operate on units bigger than words (like phrases
or tree fragments), they nearly always use word
alignments to drive extraction of those translation
rules. The dominant approach to word alignment has
been the IBM models (Brown et al, 1993) together
with the HMM model (Vogel et al, 1996). These
models are unsupervised, making them applicable
to any language pair for which parallel text is avail-
able. Moreover, they are widely disseminated in the
open-source GIZA++ toolkit (Och and Ney, 2004).
These properties make them the default choice for
most statistical MT systems.
In the decades since their invention, many mod-
els have surpassed them in accuracy, but none has
supplanted them in practice. Some of these models
are partially supervised, combining unlabeled paral-
lel text with manually-aligned parallel text (Moore,
2005; Taskar et al, 2005; Riesa and Marcu, 2010).
Although manually-aligned data is very valuable, it
is only available for a small number of language
pairs. Other models are unsupervised like the IBM
models (Liang et al, 2006; Grac?a et al, 2010; Dyer
et al, 2011), but have not been as widely adopted as
GIZA++ has.
In this paper, we propose a simple extension to
the IBM/HMM models that is unsupervised like the
IBM models, is as scalable as GIZA++ because it is
implemented on top of GIZA++, and provides sig-
nificant improvements in both alignment and trans-
lation quality. It extends the IBM/HMM models by
incorporating an `0 prior, inspired by the princi-
ple of minimum description length (Barron et al,
1998), to encourage sparsity in the word-to-word
translation model (Section 2.2). This extension fol-
lows our previous work on unsupervised part-of-
speech tagging (Vaswani et al, 2010), but enables
it to scale to the large datasets typical in word
alignment, using an efficient training method based
on projected gradient descent (Section 2.3). Ex-
periments on Czech-, Arabic-, Chinese- and Urdu-
English translation (Section 3) demonstrate consis-
tent significant improvements over IBM Model 4 in
both word alignment (up to +6.7 F1) and transla-
tion quality (up to +1.4 Bleu). Our implementation
has been released as a simple modification to the
GIZA++ toolkit that can be used as a drop-in re-
placement for GIZA++ in any existing MT pipeline.
311
2 Method
We start with a brief review of the IBM and HMM
word alignment models, then describe how to extend
them with a smoothed `0 prior and how to efficiently
train them.
2.1 IBM Models and HMM
Given a French string f = f1 ? ? ? f j ? ? ? fm and an
English string e = e1 ? ? ? ei ? ? ? e`, these models de-
scribe the process by which the French string is
generated by the English string via the alignment
a = a1, . . . , a j, . . . , am. Each a j is a hidden vari-
ables, indicating which English word ea j the French
word f j is aligned to.
In IBM Model 1?2 and the HMM model, the joint
probability of the French sentence and alignment
given the English sentence is
P(f, a | e) =
m?
j=1
d(a j | a j?1, j)t( f j | ea j). (1)
The parameters of these models are the distortion
probabilities d(a j | a j?1, j) and the translation prob-
abilities t( f j | ea j). The three models differ in their
estimation of d, but the differences do not concern us
here. All three models, as well as IBM Models 3?5,
share the same t. For further details of these models,
the reader is referred to the original papers describ-
ing them (Brown et al, 1993; Vogel et al, 1996).
Let ? stand for all the parameters of the model.
The standard training procedure is to find the param-
eter values that maximize the likelihood, or, equiv-
alently, minimize the negative log-likelihood of the
observed data:
?? = arg min
?
(
? log P(f | e, ?)
)
(2)
= arg min
?
?
??????? log
?
a
P(f, a | e, ?)
?
?????? (3)
This is done using the Expectation-Maximization
(EM) algorithm (Dempster et al, 1977).
2.2 MAP-EM with the `0-norm
Maximum likelihood training is prone to overfitting,
especially in models with many parameters. In word
alignment, one well-known manifestation of overfit-
ting is that rare words can act as ?garbage collectors?
(Moore, 2004), aligning to many unrelated words.
This hurts alignment precision and rule-extraction
recall. Previous attempted remedies include early
stopping, smoothing (Moore, 2004), and posterior
regularization (Grac?a et al, 2010).
We have previously proposed another simple
remedy to overfitting in the context of unsuper-
vised part-of-speech tagging (Vaswani et al, 2010),
which is to minimize the size of the model using a
smoothed `0 prior. Applying this prior to an HMM
improves tagging accuracy for both Italian and En-
glish.
Here, our goal is to apply a similar prior in a
word-alignment model to the word-to-word transla-
tion probabilities t( f | e). We leave the distortion
models alone, since they are not very large, and there
is not much reason to believe that we can profit from
compacting them.
With the addition of the `0 prior, the MAP (maxi-
mum a posteriori) objective function is
?? = arg min
?
(
? log P(f | e, ?)P(?)
)
(4)
where
P(?) ? exp
(
??????0
)
(5)
and
????0 =
?
e, f
(
1 ? exp
?t( f | e)
?
)
(6)
is a smoothed approximation of the `0-norm. The
hyperparameter ? controls the tightness of the ap-
proximation, as illustrated in Figure 1. Substituting
back into (4) and dropping constant terms, we get
the following optimization problem: minimize
? log P(f | e, ?) ? ?
?
e, f
exp
?t( f | e)
?
(7)
subject to the constraints
?
f
t( f | e) = 1 for all e. (8)
We can carry out the optimization in (7) with the
MAP-EM algorithm (Bishop, 2006). EM and MAP-
EM share the same E-step; the difference lies in the
312
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Figure 1: The `0-norm (top curve) and smoothed approx-
imations (below) for ? = 0.05, 0.1, 0.2.
M-step. For vanilla EM, the M-step is:
?? = arg min
?
?
????????
?
?
e, f
E[C(e, f )] log t( f | e)
?
????????
(9)
again subject to the constraints (8). The count
C(e, f ) is the number of times that f occurs aligned
to e. For MAP-EM, it is:
?? = arg min
?
(
?
?
e, f
E[C(e, f )] log t( f | e) ?
?
?
e, f
exp
?t( f | e)
?
) (10)
This optimization problem is non-convex, and we
do not know of a closed-form solution. Previously
(Vaswani et al, 2010), we used ALGENCAN, a non-
linear optimization toolkit, but this solution does not
scale well to the number of parameters involved in
word alignment models. Instead, we use a simpler
and more scalable method which we describe in the
next section.
2.3 Projected gradient descent
Following Schoenemann (2011b), we use projected
gradient descent (PGD) to solve the M-step (but
with the `0-norm instead of the `1-norm). Gradient
projection methods are attractive solutions to con-
strained optimization problems, particularly when
the constraints on the parameters are simple (Bert-
sekas, 1999). Let F(?) be the objective function in
(10); we seek to minimize this function. As in pre-
vious work (Vaswani et al, 2010), we optimize each
set of parameters {t(? | e)} separately for each En-
glish word type e. The inputs to the PGD are the
expected counts E[C(e, f )] and the current word-to-
word conditional probabilities ?. We run PGD for K
iterations, producing a sequence of intermediate pa-
rameter vectors ?1, . . . , ?k, . . . , ?K . Each iteration has
two steps, a projection step and a line search.
Projection step In this step, we compute:
?
k
=
[
?k ? s?F(?k)
]?
(11)
This moves ? in the direction of steepest descent
(?F) with step size s, and then the function [?]?
projects the resulting point onto the simplex; that
is, it finds the nearest point that satisfies the con-
straints (8).
The gradient ?F(?k) is
?F
?t( f | e)
= ?
E[C( f , e)]
t( f | e)
+
?
?
exp
?t( f | e)
?
(12)
In contrast to Schoenemann (2011b), we use an
O(n log n) algorithm for the projection step due to
Duchi et. al. (2008), shown in Pseudocode 1.
Pseudocode 1 Project input vector u ? Rn onto the
probability simplex.
v = u sorted in non-increasing order
? = 0
for i = 1 to n do
if vi ? 1i
(?i
r=1 vr ? 1
)
> 0 then
? = i
end if
end for
? = 1
?
(??
r=1 vr ? 1
)
wr = max{vr ? ?, 0} for 1 ? r ? n
return w
Line search Next, we move to a point between ?k
and ?
k
that satisfies the Armijo condition,
F(?k + ?m) ? F(?k) + ?
(
?F(?k) ? ?m
)
(13)
where ?m = ?m(?
k
? ?k) and ? and ? are both con-
stants in (0, 1). We try values m = 1, 2, . . . until the
Armijo condition (13) is satisfied or the limit m = 20
313
Pseudocode 2 Find a point between ?k and ?
k
that
satisfies the Armijo condition.
Fmin = F(?k)
?min = ?
k
for m = 1 to 20 do
?m = ?
m
(
?
k
? ?k
)
if F(?k + ?m) < Fmin then
Fmin = F(?k + ?m)
?min = ?
k + ?m
end if
if F(?k + ?m) ? F(?k) + ?
(
?F(?k) ? ?m
)
then
break
end if
end for
?k+1 = ?min
return ?k+1
is reached. (Note that we don?t allow m = 0 because
this can cause ?k + ?m to land on the boundary of
the probability simplex, where the objective func-
tion is undefined.) Then we set ?k+1 to the point in
{?k} ? {?k + ?m | 1 ? m ? 20} that minimizes F.
The line search algorithm is summarized in Pseu-
docode 2.
In our implementation, we set ? = 0.5 and ? =
0.5. We keep s fixed for all PGD iterations; we ex-
perimented with s ? {0.1, 0.5} and did not observe
significant changes in F-score. We run the projection
step and line search alternately for at most K itera-
tions, terminating early if there is no change in ?k
from one iteration to the next. We set K = 35 for the
large Arabic-English experiment; for all other con-
ditions, we set K = 50. These choices were made to
balance efficiency and accuracy. We found that val-
ues of K between 30 and 75 were generally reason-
able.
3 Experiments
To demonstrate the effect of the `0-norm on the IBM
models, we performed experiments on four trans-
lation tasks: Arabic-English, Chinese-English, and
Urdu-English from the NIST Open MT Evaluation,
and the Czech-English translation from the Work-
shop on Machine Translation (WMT) shared task.
We measured the accuracy of word alignments gen-
erated by GIZA++ with and without the `0-norm,
and also translation accuracy of systems trained us-
ing the word alignments. Across all tests, we found
strong improvements from adding the `0-norm.
3.1 Training
We have implemented our algorithm as an open-
source extension to GIZA++.1 Usage of the exten-
sion is identical to standard GIZA++, except that the
user can switch the `0 prior on or off, and adjust the
hyperparameters ? and ?.
For vanilla EM, we ran five iterations of Model 1,
five iterations of HMM, and ten iterations of
Model 4. For our approach, we first ran one iter-
ation of Model 1, followed by four iterations of
Model 1 with smoothed `0, followed by five itera-
tions of HMM with smoothed `0. Finally, we ran ten
iterations of Model 4.2
We used the following parallel data:
? Chinese-English: selected data from the con-
strained task of the NIST 2009 Open MT Eval-
uation.3
? Arabic-English: all available data for the
constrained track of NIST 2009, excluding
United Nations proceedings (LDC2004E13),
ISI Automatically Extracted Parallel Text
(LDC2007E08), and Ummah newswire text
(LDC2004T18), for a total of 5.4+4.3 mil-
lion words. We also experimented on a larger
Arabic-English parallel text of 44+37 million
words from the DARPA GALE program.
? Urdu-English: all available data for the con-
strained track of NIST 2009.
1The code can be downloaded from the first author?s website
at http://www.isi.edu/?avaswani/giza-pp-l0.html.
2GIZA++ allows changing some heuristic parameters for
efficient training. Currently, we set two of these to zero:
mincountincrease and probcutoff. In the default setting,
both are set to 10?7. We set probcutoff to 0 because we would
like the optimization to learn the parameter values. For a fair
comparison, we applied the same setting to our vanilla EM
training as well. To test, we ran GIZA++ with the default set-
ting on the smaller of our two Arabic-English datasets with the
same number of iterations and found no change in F-score.
3LDC catalog numbers LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,
LDC2006E85, LDC2006E86, LDC2006E92, and
LDC2006E93.
314
pr
es
id
en
t
of th
e
fo
re
ig
n
af
fa
ir
s
in
st
it
ut
e
sh
uq
in
li
u
wa
s
al
so
pr
es
en
t
at th
e
me
et
in
g
.
   u u           
wa`ijia?o
     u          
xue?hu?`
u               
hu?`zha?ng
       u        
liu?
      u u        
shu?q??ng
             u  
hu?`jia`n
               
sh??
         u u u  u  
za`izuo`
              u 
.
ov
er
40
00
gu
es
ts
fr
om
ho
me
an
d
ab
ro
ad
at
te
nd
ed
th
e
op
en
in
g
ce
re
mo
ny
.
    u  u      
zho?ngwa`i
  u          
la?ib??n
 u           
s?`qia?n
u u           
duo?
            
re?n
       u     
chu?x??
       u     
le
         u u  
ka?imu`sh?`
           u 
.
(a) (b)
it ?s ex
tr
em
el
y
tr
ou
bl
es
om
e
to ge
t
th
er
e
vi
a
la
nd
.
u          
ru?guo?
    u      
ya`o
       u u  
lu`lu`
          
zhua?n
     u u    
qu`
          
dehua`
          
ne
         u 
,
  u        
he?n
  u        
he?n
  u        
he?n
  u        
he?n
   u       
ma?fan
          
de
         u 
,
af
te
r
th
is
wa
s
ta
ke
n
ca
re
of , fo
ur
bl
oc
kh
ou
se
s
we
re
bl
ow
n
up .
 u            
zhe`ge
    u         
chu`l??
             
wa?n
u             
y??ho`u
             
ne
      u       
,
             
ha?i
          u   
zha`
           u  
le
       u      
s?`ge
        u     
dia?oba?o
            u 
.
(c) (d)
Figure 2: Smoothed-`0 alignments (red circles) correct many errors in the baseline GIZA++ alignments (black
squares), as shown in four Chinese-English examples (the red circles are almost perfect for these examples, except
for minor mistakes such as liu-shu?q??ng and meeting-za`izuo` in (a) and .-, in (c)). In particular, the baseline system
demonstrates typical ?garbage-collection? phenomena in proper name ?shuqing? in both languages in (a), number
?4000? and word ?la?ib??n? (lit. ?guest?) in (b), word ?troublesome? and ?lu`lu`? (lit. ?land-route?) in (c), and ?block-
houses? and ?dia?oba?o? (lit. ?bunker?) in (d). We found this garbage-collection behavior to be especially common with
proper names, numbers, and uncommon words in both languages. Most interestingly, in (c), our smoothed-`0 system
correctly aligns ?extremely? to ?he?n he?n he?n he?n? (lit. ?very very very very?) which is rare in the bitext.
315
task data (M) system align F1 (%) word trans (M) ??sing. Bleu (%)
2008 2009 2010
Chi-Eng 9.6+12
baseline 73.2 3.5 6.2 28.7
`0-norm 76.5 2.0 3.3 29.5
difference +3.3 ?43% ?47% +0.8
Ara-Eng 5.4+4.3
baseline 65.0 3.1 4.5 39.8 42.5
`0-norm 70.8 1.8 1.8 41.1 43.7
difference +5.9 ?39% ?60% +1.3 +1.2
Ara-Eng 44+37
baseline 66.2 15 5.0 41.6 44.9
`0-norm 71.8 7.9 1.8 42.5 45.3
difference +5.6 ?47% ?64% +0.9 +0.4
Urd-Eng 1.7+1.5
baseline 1.7 4.5 25.3? 29.8
`0-norm 1.2 2.2 25.9? 31.2
difference ?29% ?51% +0.6? +1.4
Cze-Eng 2.1+2.3
baseline 65.6 1.5 3.0 17.3 18.0
`0-norm 72.3 1.0 1.4 17.9 18.4
difference +6.7 ?33% ?53% +0.6 +0.4
Table 1: Adding the `0-norm to the IBM models improves both alignment and translation accuracy across four different
language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the
lexical weighting table) is reduced. The ??sing. column shows the average fertility of once-seen source words. For
Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open
MT Evaluation. ?Half of this test set was also used for tuning feature weights.
? Czech-English: A corpus of 4 million words of
Czech-English data from the News Commen-
tary corpus.4
We set the hyperparameters ? and ? by tuning
on gold-standard word alignments (to maximize F1)
when possible. For Arabic-English and Chinese-
English, we used 346 and 184 hand-aligned sen-
tences from LDC2006E86 and LDC2006E93. Sim-
ilarly, for Czech-English, 515 hand-aligned sen-
tences were available (Bojar and Prokopova?, 2006).
But for Urdu-English, since we did not have any
gold alignments, we used ? = 10 and ? = 0.05. We
did not choose a large ?, as the dataset was small,
and we chose a conservative value for ?.
We ran word alignment in both directions and
symmetrized using grow-diag-final (Koehn et al,
2003). For models with the smoothed `0 prior, we
tuned ? and ? separately in each direction.
3.2 Alignment
First, we evaluated alignment accuracy directly by
comparing against gold-standard word alignments.
4This data is available at http://statmt.org/wmt10.
The results are shown in the alignment F1 col-
umn of Table 1. We used balanced F-measure rather
than alignment error rate as our metric (Fraser and
Marcu, 2007).
Following Dyer et al (2011), we also measured
the average fertility, ??sing., of once-seen source
words in the symmetrized alignments. Our align-
ments show smaller fertility for once-seen words,
suggesting that they suffer from ?garbage collec-
tion? effects less than the baseline alignments do.
The fact that we had to use hand-aligned data to
tune the hyperparameters ? and ? means that our
method is no longer completely unsupervised. How-
ever, our observation is that alignment accuracy is
actually fairly robust to the choice of these hyperpa-
rameters, as shown in Table 2. As we will see below,
we still obtained strong improvements in translation
quality when hand-aligned data was unavailable.
We also tried generating 50 word classes using
the tool provided in GIZA++. We found that adding
word classes improved alignment quality a little, but
more so for the baseline system (see Table 3). We
used the alignments generated by training with word
classes for our translation experiments.
316
? model
?
0 10 25 50 75 100 250 500 750
?
HMM 47.5
M4 52.1
0.5
HMM 46.3 48.4 52.8 55.7 57.5 61.5 62.6 62.7
M4 51.7 53.7 56.4 58.6 59.8 63.3 64.4 64.8
0.1
HMM 55.6 60.4 61.6 62.1 61.9 61.8 60.2 60.1
M4 58.2 62.4 64.0 64.4 64.8 65.5 65.6 65.9
0.05
HMM 59.1 61.4 62.4 62.5 62.3 60.8 58.7 57.7
M4 61.0 63.5 64.6 65.3 65.3 65.4 65.7 65.7
0.01
HMM 59.7 61.6 60.0 59.5 58.7 56.9 55.7 54.7
M4 62.9 65.0 65.1 65.2 65.1 65.4 65.3 65.4
0.005
HMM 58.1 59.0 58.3 57.6 57.0 55.9 53.9 51.7
M4 62.0 64.1 64.5 64.5 64.5 65.0 64.8 64.6
0.001
HMM 51.7 52.1 51.4 49.3 50.4 46.8 45.4 44.0
M4 59.8 61.3 61.5 61.0 61.8 61.2 61.0 61.2
Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model
for Arabic-English alignment (? = 0).
word classes?
direction system no yes
P( f | e)
baseline 49.0 52.1
`0-norm 63.9 65.9
difference +14.9 +13.8
P(e | f )
baseline 64.3 65.2
`0-norm 69.2 70.3
difference +4.9 +5.1
Table 3: Adding word classes improves the F-score in
both directions for Arabic-English alignment by a little,
for the baseline system more so than ours.
Figure 2 shows four examples of Chinese-
English alignment, comparing the baseline with our
smoothed-`0 method. In all four cases, the base-
line produces incorrect extra alignments that prevent
good translation rules from being extracted while
the smoothed-`0 results are correct. In particular, the
baseline system demonstrates typical ?garbage col-
lection? behavior (Moore, 2004) in all four exam-
ples.
3.3 Translation
We then tested the effect of word alignments on
translation quality using the hierarchical phrase-
based translation system Hiero (Chiang, 2007). We
used a fairly standard set of features: seven in-
herited from Pharaoh (Koehn et al, 2003), a sec-
setting align F1 (%) Bleu (%)
t( f | e) t(e | f ) 2008 2009
1st 1st 70.8 41.1 43.7
1st 2nd 70.7 41.1 43.8
2nd 1st 70.7 40.7 44.1
2nd 2nd 70.9 41.1 44.2
Table 4: Optimizing hyperparameters on alignment F1
score does not necessarily lead to optimal Bleu. The
first two columns indicate whether we used the first- or
second-best alignments in each direction (according to
F1); the third column shows the F1 of the symmetrized
alignments, whose corresponding Bleu scores are shown
in the last two columns.
ond language model, and penalties for the glue
rule, identity rules, unknown-word rules, and two
kinds of number/name rules. The feature weights
were discriminatively trained using MIRA (Chi-
ang et al, 2008). We used two 5-gram language
models, one on the combined English sides of
the NIST 2009 Arabic-English and Chinese-English
constrained tracks (385M words), and another on
2 billion words of English.
For each language pair, we extracted grammar
rules from the same data that were used for word
alignment. The development data that were used for
discriminative training were: for Chinese-English
and Arabic-English, data from the NIST 2004 and
NIST 2006 test sets, plus newsgroup data from the
317
GALE program (LDC2006E92); for Urdu-English,
half of the NIST 2008 test set; for Czech-English,
a training set of 2051 sentences provided by the
WMT10 translation workshop.
The results are shown in the Bleu column of Ta-
ble 1. We used case-insensitive IBM Bleu (closest
reference length) as our metric. Significance test-
ing was carried out using bootstrap resampling with
1000 samples (Koehn, 2004; Zhang et al, 2004).
All of the tests showed significant improvements
(p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu.
For Urdu, even though we didn?t have manual align-
ments to tune hyperparameters, we got significant
gains over a good baseline. This is promising for lan-
guages that do not have any manually aligned data.
Ideally, one would want to tune ? and ? to max-
imize Bleu. However, this is prohibitively expen-
sive, especially if we must tune them separately
in each alignment direction before symmetrization.
We ran some contrastive experiments to investi-
gate the impact of hyperparameter tuning on trans-
lation quality. For the smaller Arabic-English cor-
pus, we symmetrized all combinations of the two
top-scoring alignments (according to F1) in each di-
rection, yielding four sets of alignments. Table 4
shows Bleu scores for translation models learned
from these alignments. Unfortunately, we find that
optimizing F1 is not optimal for Bleu?using the
second-best alignments yields a further improve-
ment of 0.5 Bleu on the NIST 2009 data, which is
statistically significant (p < 0.05).
4 Related Work
Schoenemann (2011a), taking inspiration from Bo-
drumlu et al (2009), uses integer linear program-
ming to optimize IBM Model 1?2 and the HMM
with the `0-norm. This method, however, does not
outperform GIZA++. In later work, Schoenemann
(2011b) used projected gradient descent for the `1-
norm. Here, we have adopted his use of projected
gradient descent, but using a smoothed `0-norm.
Liang et al (2006) show how to train IBM mod-
els in both directions simultaneously by adding a
term to the log-likelihood that measures the agree-
ment between the two directions. Grac?a et al (2010)
explore modifications to the HMM model that en-
courage bijectivity and symmetry. The modifications
take the form of constraints on the posterior dis-
tribution over alignments that is computed during
the E-step. Mermer and Sarac?lar (2011) explore a
Bayesian version of IBM Model 1, applying sparse
Dirichlet priors to t. However, because this method
requires the use of Monte Carlo methods, it is not
clear how well it can scale to larger datasets.
5 Conclusion
We have extended the IBM models and HMM model
by the addition of an `0 prior to the word-to-word
translation model, which compacts the word-to-
word translation table, reducing overfitting, and, in
particular, the ?garbage collection? effect. We have
shown how to perform MAP-EM with this prior
efficiently, even for large datasets. The method is
implemented as a modification to the open-source
toolkit GIZA++, and we have shown that it signif-
icantly improves translation quality across four dif-
ferent language pairs. Even though we have used a
small set of gold-standard alignments to tune our
hyperparameters, we found that performance was
fairly robust to variation in the hyperparameters, and
translation performance was good even when gold-
standard alignments were unavailable. We hope that
our method, due to its simplicity, generality, and ef-
fectiveness, will find wide application for training
better statistical translation systems.
Acknowledgments
We are indebted to Thomas Schoenemann for ini-
tial discussions and pilot experiments that led to
this work, and to the anonymous reviewers for
their valuable comments. We thank Jason Riesa for
providing the Arabic-English and Chinese-English
hand-aligned data and the alignment visualization
tool, and Chris Dyer for the Czech-English hand-
aligned data. This research was supported in part
by DARPA under contract DOI-NBC D11AP00244
and a Google Faculty Research Award to L. H.
318
References
Andrew Barron, Jorma Rissanen, and Bin Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information Theory,
44(6):2743?2760.
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing.
Ondr?ej Bojar and Magdalena Prokopova?. 2006. Czech-
English word alignment. In Proceedings of LREC.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19:263?311.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?208.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Computational Linguistics, 39(4):1?38.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
`1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Chris Dyer, Jonathan H. Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised word alignment with ar-
bitrary features. In Proceedings of ACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar.
2010. Learning tractable word alignment models
with complex constraints. Computational Linguistics,
36(3):481?504.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Cos?kun Mermer and Murat Sarac?lar. 2011. Bayesian
word alignment for statistical machine translation. In
Proceedings of ACL HLT.
Robert C. Moore. 2004. Improving IBM word-
alignment Model 1. In Proceedings of ACL.
Robert Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of HLT-
EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of ACL.
Thomas Schoenemann. 2011a. Probabilistic word align-
ment under the L0-norm. In Proceedings of CoNLL.
Thomas Schoenemann. 2011b. Regularizing mono- and
bi-word models for word alignment. In Proceedings
of IJCNLP.
Ben Taskar, Lacoste-Julien Simon, and Klein Dan. 2005.
A discriminative matching approach to word align-
ment. In Proceedings of HLT-EMNLP.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC.
319
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 317?321,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
An Exploration of Forest-to-String Translation:
Does Translation Help or Hurt Parsing?
Hui Zhang
University of Southern California
Department of Computer Science
hzhang@isi.edu
David Chiang
University of Southern California
Information Sciences Institute
chiang@isi.edu
Abstract
Syntax-based translation models that operate
on the output of a source-language parser have
been shown to perform better if allowed to
choose from a set of possible parses. In this
paper, we investigate whether this is because it
allows the translation stage to overcome parser
errors or to override the syntactic structure it-
self. We find that it is primarily the latter, but
that under the right conditions, the transla-
tion stage does correct parser errors, improv-
ing parsing accuracy on the Chinese Treebank.
1 Introduction
Tree-to-string translation systems (Liu et al, 2006;
Huang et al, 2006) typically employ a pipeline of
two stages: a syntactic parser for the source lan-
guage, and a decoder that translates source-language
trees into target-language strings. Originally, the
output of the parser stage was a single parse tree, and
this type of system has been shown to outperform
phrase-based translation on, for instance, Chinese-
to-English translation (Liu et al, 2006). More recent
work has shown that translation quality is improved
further if the parser outputs a weighted parse forest,
that is, a representation of a whole distribution over
possible parse trees (Mi et al, 2008). In this paper,
we investigate two hypotheses to explain why.
One hypothesis is that forest-to-string translation
selects worse parses. Although syntax often helps
translation, there may be situations where syntax, or
at least syntax in the way that our models use it, can
impose constraints that are too rigid for good-quality
translation (Liu et al, 2007; Zhang et al, 2008).
For example, suppose that a tree-to-string system
encounters the following correct tree (only partial
bracketing shown):
(1) [NP j??ngj?`
economy
ze?ngzha?ng]
growth
de
DE
su`du`
rate
?economic growth rate?
Suppose further that the model has never seen this
phrase before, although it has seen the subphrase
ze?ngzha?ng de su`du` ?growth rate?. Because this sub-
phrase is not a syntactic unit in sentence (1), the sys-
tem will be unable to translate it. But a forest-to-
string system would be free to choose another (in-
correct but plausible) bracketing:
(2) j??ngj?`
economy
[NP ze?ngzha?ng
growth
de
DE
su`du`]
rate
and successfully translate it using rules learned from
observed data.
The other hypothesis is that forest-to-string trans-
lation selects better parses. For example, if a Chi-
nese parser is given the input ca?njia? bia?ojie? de hu?nl??,
it might consider two structures:
(3) [VP ca?njia?
attend
bia?ojie?]
cousin
de
DE
hu?nl??
wedding
?wedding that attends a cousin?
(4) ca?njia?
attend
[NP bia?ojie?
cousin
de
DE
hu?nl??]
wedding
?attend a cousin?s wedding?
The two structures have two different translations
into English, shown above. While the parser prefers
structure (3), an n-gram language model would eas-
ily prefer translation (4) and, therefore, its corre-
sponding Chinese parse.
317
(a) f f f
parser
????? .
f f f
decoder?????? e e e e
source source target
string tree string
(b) f f f
parser
????? .
f f f
decoder?????? e e e e
source source target
string forest string
Figure 1: (a) In tree-to-string translation, the parser gen-
erates a single tree which the decoder must use to gen-
erate a translation. (b) In forest-to-string translation, the
parser generates a forest of possible trees, any of which
the decoder can use to generate a translation.
Previous work has shown that an observed target-
language translation can improve parsing of source-
language text (Burkett and Klein, 2008; Huang et al,
2009), but to our knowledge, only Chen et al (2011)
have explored the case where the target-language
translation is unobserved.
Below, we carry out experiments to test these
two hypotheses. We measure the accuracy (using
labeled-bracket F1) of the parses that the translation
model selects, and find that they are worse than the
parses selected by the parser. Our basic conclusion,
then, is that the parses that help translation (accord-
ing to Bleu) are, on average, worse parses. That is,
forest-to-string translation hurts parsing.
But there is a twist. Neither labeled-bracket F1
nor Bleu is a perfect metric of the phenomena it is
meant to measure, and our translation system is op-
timized to maximize Bleu. If we optimize our sys-
tem to maximize labeled-bracket F1 instead, we find
that our translation system selects parses that score
higher than the baseline parser?s. That is, forest-to-
string translation can help parsing.
2 Background
We provide here only a cursory overview of tree-
to-string and forest-to-string translation. For more
details, the reader is referred to the original papers
describing them (Liu et al, 2006; Mi et al, 2008).
Figure 1a illustrates the tree-to-string transla-
tion pipeline. The parser stage can be any phrase-
structure parser; it computes a parse for each source-
language string. The decoder stage translates the
source-language tree into a target-language string,
using a synchronous tree-substitution grammar.
In forest-to-string translation (Figure 1b), the
parser outputs a forest of possible parses of each
source-language string. The decoder uses the same
rules as in tree-to-string translation, but is free to se-
lect any of the trees contained in the parse forest.
3 Translation hurts parsing
The simplest experiment to carry out is to exam-
ine the parses actually selected by the decoder, and
see whether they are better or worse than the parses
selected by the parser. If they are worse, this sup-
ports the hypothesis that syntax can hurt translation.
If they are better, we can conclude that translation
can help parsing. In this initial experiment, we find
that the former is the case.
3.1 Setup
The baseline parser is the Charniak parser (Char-
niak, 2000). We trained it on the Chinese Treebank
(CTB) 5.1, split as shown in Table 1, following
Duan et al (2007).1 The parser outputs a parse forest
annotated with head words and other information.
Since the decoder does not use these annotations,
we use the max-rule algorithm (Petrov et al, 2006)
to (approximately) sum them out. As a side bene-
fit, this improves parsing accuracy from 77.76% to
78.42% F1. The weight of a hyperedge in this for-
est is its posterior probability, given the input string.
We retain these weights as a feature in the translation
model.
The decoder stage is a forest-to-string system (Liu
et al, 2006; Mi et al, 2008) for Chinese-to-English
translation. The datasets used are listed in Ta-
ble 1. We generated word alignments with GIZA++
and symmetrized them using the grow-diag-final-
and heuristic. We parsed the Chinese side using
the Charniak parser as described above, and per-
formed forest-based rule extraction (Mi and Huang,
2008) with a maximum height of 3 nodes. We used
the same features as Mi and Huang (2008). The
language model was a trigram model with modi-
fied Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998), trained on the target
1The more common split, used by Bikel and Chiang (2000),
has flaws that are described by Levy and Manning (2003).
318
Parsing Translation
Train CTB 1?815 FBIS
CTB 1101?1136
Dev CTB 900?931 NIST 2002
CTB 1148?1151
Test CTB 816?885 NIST 2003
CTB 1137?1147
Table 1: Data used for training and testing the parsing and
translation models.
Parsing Translation
System Objective F1% Bleu%
Charniak n/a 78.42 n/a
tree-to-string max-Bleu 78.42 23.07
forest-to-string max-Bleu 77.75 24.60
forest-to-string max-F1 78.81 19.18
Table 2: Forest-to-string translation outperforms tree-to-
string translation according to Bleu, but the decreases
parsing accuracy according to labeled-bracket F1. How-
ever, when we train to maximize labeled-bracket F1,
forest-to-string translation yields better parses than both
tree-to-string translation and the original parser.
side of the training data. We used minimum-error-
rate (MER) training to optimize the feature weights
(Och, 2003) to maximize Bleu.
At decoding time, we select the best derivation
and extract its source tree. In principle, we ought
to sum over all derivations for each source tree; but
the approximations that we tried (n-best list crunch-
ing, max-rule decoding, minimum Bayes risk) did
not appear to help.
3.2 Results
Table 2 shows the main results of our experiments.
In the second and third line, we see that the forest-
to-string system outperforms the tree-to-string sys-
tem by 1.53 Bleu, consistent with previously pub-
lished results (Mi et al, 2008; Zhang et al, 2009).
However, we also find that the trees selected by the
forest-to-string system score much lower according
to labeled-bracket F1. This suggests that the reason
the forest-to-string system is able to generate better
translations is that it can soften the constraints im-
posed by the syntax of the source language.
4 Translation helps parsing
We have found that better translations can be ob-
tained by settling for worse parses. However, trans-
lation accuracy is measured using Bleu and pars-
ing accuracy is measured using labeled-bracket F1,
and neither of these is a perfect metric of the phe-
nomenon it is meant to measure. Moreover, we op-
timized the translation model in order to maximize
Bleu. It is known that when MER training is used
to optimize one translation metric, other translation
metrics suffer (Och, 2003); much more, then, can
we expect that optimizing Bleu will cause labeled-
bracket F1 to suffer. In this section, we try optimiz-
ing labeled-bracket F1, and find that, in this case, the
translation model does indeed select parses that are
better on average.
4.1 Setup
MER training with labeled-bracket F1 as an objec-
tive function is straightforward. At each iteration of
MER training, we run the parser and decoder over
the CTB dev set to generate an n-best list of possible
translation derivations (Huang and Chiang, 2005).
For each derivation, we extract its Chinese parse tree
and compute the number of brackets guessed and
the number matched against the gold-standard parse
tree. A trivial modification of the MER trainer then
optimizes the feature weights to maximize labeled-
bracket F1.
A technical challenge that arises is ensuring di-
versity in the n-best lists. The MER trainer re-
quires that each list contain enough unique transla-
tions (when maximizing Bleu) or source trees (when
maximizing labeled-bracket F1). However, because
one source tree may lead to many translation deriva-
tions, the n-best list may contain only a few unique
source trees, or in the extreme case, the derivations
may all have the same source tree. We use a variant
of the n-best algorithm that allows efficient genera-
tion of equivalence classes of derivations (Huang et
al., 2006). The standard algorithm works by gener-
ating, at each node of the forest, a list of the best
subderivations at that node; the variant drops a sub-
derivation if it has the same source tree as a higher-
scoring subderivation.
319
Maximum
rule height F1%
3 78.81
4 78.93
5 79.14
LM data
(lines) F1%
none 78.78
100 78.79
30k 78.67
300k 79.14
13M 79.24
Features F1%
monolingual 78.89
+ bilingual 79.24
Parallel data
(lines) F1%
60k 78.00
120k 78.16
300k 79.24
(a) (b) (c) (d)
Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases
parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.
(c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing
accuracy, but only slightly.
4.2 Results
The last line of Table 2 shows the results of this
second experiment. The system trained to opti-
mize labeled-bracket F1 (max-F1) obtains a much
lower Bleu score than the one trained to maximize
Bleu (max-Bleu)?unsurprisingly, because a single
source-side parse can yield many different transla-
tions, but the objective function scores them equally.
What is more interesting is that the max-F1 system
obtains a higher F1 score, not only compared with
the max-Bleu system but also the original parser.
We then tried various settings to investigate what
factors affect parsing performance. First, we found
that increasing the maximum rule height increases
F1 further (Table 3a).
One of the motivations of our method is that bilin-
gual information (especially the language model)
can help disambiguate the source side structures. To
test this, we varied the size of the corpus used to train
the language model (keeping a maximum rule height
of 5 from the previous experiment). The 13M-line
language model adds the Xinhua portion of Giga-
word 3. In Table 3b we see that the parsing perfor-
mance does increase with the language model size,
with the largest language model yielding a net im-
provement of 0.82 over the baseline parser.
To test further the importance of bilingual infor-
mation, we compared against a system built only
from the Chinese side of the parallel text (with each
word aligned to itself). We removed all features that
use bilingual information, retaining only the parser
probability and the phrase penalty. In their place
we added a new feature, the probability of a rule?s
source side tree given its root label, which is essen-
tially the same model used in Data-Oriented Parsing
(Bod, 1992). Table 3c shows that this system still
outperforms the original parser. In other words, part
of the gain is not attributable to translation, but ad-
ditional source-side context and data that the trans-
lation model happens to capture.
Finally, we varied the size of the parallel text
(keeping a maximum rule height of 5 and the largest
language model) and found that, as expected, pars-
ing performance correlates with parallel data size
(Table 3d).
5 Conclusion
We set out to investigate why forest-to-string trans-
lation outperforms tree-to-string translation. By
comparing their performance as Chinese parsers, we
found that forest-to-string translation sacrifices pars-
ing accuracy, suggesting that forest-to-string trans-
lation works by overriding constraints imposed by
syntax. But when we optimized the system to max-
imize labeled-bracket F1, we found that, in fact,
forest-to-string translation is able to achieve higher
accuracy, by 0.82 F1%, than the baseline Chinese
parser, demonstrating that, to a certain extent, forest-
to-string translation is able to correct parsing errors.
Acknowledgements
We are grateful to the anonymous reviewers for
their helpful comments. This research was sup-
ported in part by DARPA under contract DOI-NBC
D11AP00244.
320
References
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the Chinese Treebank.
In Proc. Second Chinese Language Processing Work-
shop, pages 1?6.
Rens Bod. 1992. A computational model of language
performance: Data Oriented Parsing. In Proc. COL-
ING 1992, pages 855?859.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP 2008, pages 877?886.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL, pages 132?139.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-
masa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro
Torisawa, and Haizhou Li. 2011. SMT helps bitext
dependency parsing. In Proc. EMNLP 2011, pages
73?83.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based Chinese dependency pars-
ing. In Proc. ECML 2007, pages 559?566.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT 2005, pages 53?64.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. AMTA 2006, pages 65?
73.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP 2009, pages 1222?1231.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Proc.
ICASSP 1995, pages 181?184.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank? In
Proc. ACL 2003, pages 439?446.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. COLING-ACL 2006, pages 609?616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Proc.
ACL 2007, pages 704?711.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. EMNLP 2008, pages
206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL-08: HLT, pages 192?
199.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. ACL 2003,
pages 160?167.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL 2006,
pages 433?440.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT, pages 559?567.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proc. ACL-IJCNLP 2009,
pages 172?180.
321
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing Graphs with Hyperedge Replacement Grammars
David Chiang
Information Sciences Institute
University of Southern California
Jacob Andreas
Columbia University
University of Cambridge
Daniel Bauer
Department of Computer Science
Columbia University
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Bevan Jones
University of Edinburgh
Macquarie University
Kevin Knight
Information Sciences Institute
University of Southern California
Abstract
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm?s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
1 Introduction
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al, 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al, 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann?s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: ?As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.? Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
2 Hyperedge replacement grammars
We give a short example of how HRG works, fol-
lowed by formal definitions.
2.1 Example
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S . The first rewriting step replaces this edge
with a subgraph, which we might read as ?The
924
boy?girl?
want? arg0
arg1
boy?
believe? arg1
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 1: Sample members of a graph language,
representing the meanings of (clockwise from up-
per left): ?The girl wants the boy,? ?The boy is
believed,? and ?The boy wants the girl to believe
that he wants her.?
boy wants something (X) involving himself.? The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as ?The boy
wants the girl to believe something (Y) involving
both of them.? The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
2.2 Definitions
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = ?V, E, ??, where
? V is a finite set of nodes
? E ? V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
? ? : E ? C assigns a label (drawn from the
finite set C) to each edge.
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
Definition 2. A hypergraph fragment is a tuple
?V, E, ?, X?, where ?V, E, ?? is a hypergraph and
X ? V+ is a list of distinct nodes called the ex-
ternal nodes.
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = ?N,T, P, S ? where
? N and T are finite disjoint sets of nonterminal
and terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
A ? R, where A ? N and R is a graph frag-
ment over N ? T .
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ?G H? (or, H? is derived from H in one
step) as follows. Let e = (v1 ? ? ? vk) be an edge in
H with label A. Let (A? R) be a production ofG,
where R has external nodes XR = (u1 ? ? ? uk). Then
we write H ?G H? if H? is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1, . . . , k.
Let H ??G H? (or, H? is derived from H) be thereflexive, transitive closure of?G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
S ??G H.
When a HRG rule (A ? R) is applied to an
edge e, the mapping of external nodes in R to the
925
1
X ?
believe? arg1
girl?
arg0
1
Y
1 2
Y
?
12
want?
arg0
arg1
S
1
boy?
X
want? arg1
arg0
2 believe? arg1
want? arg1
girl?
arg0
boy?
arg0
Y
3
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?The
boy wants the girl to believe that he wants her.?
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = ?N,T, P, S , ??, where
?N, T, P, S ? is a HRG and ? : P ? K assigns a
weight in K to each production. The weight of a
derivation ofG is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
3 Parsing
Lautemann?s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
3.1 Representing subgraphs
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = ?V, E, ?? is, for some
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
926
subset E? ? E, the smallest subgraph containing
all edges in E?. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 ? ? ?w j. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple ?bn(I), be(I, v),m ? I?, where
? bn(I) is the set of boundary nodes of I
? be(I, v) be the set of boundary edges of v in I
? (m ? I) is a flag indicating whether the
marker node is in I.
The boundary representation of I suffices to
specify I compactly.
Proposition 1. If I and I? are two subgraphs of H
with the same boundary representation, then I =
I?.
Proof. Case 1: bn(I) is empty. If m ? I and m ? I?,
then all edges of H must belong to both I and I?,
that is, I = I? = H. Otherwise, if m < I and m < I?,
then no edges can belong to either I or I?, that is,
I = I? = ?.
Case 2: bn(I) is nonempty. Suppose I , I?;
without loss of generality, suppose that there is an
edge e that is in I \ I?. Let ? be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along ?
must be in I \ I?, or else there would be a boundary
node in the middle of ?, and ? would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of ?must be in I \ I?. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I?, which is a contradiction. 
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I ? J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I ? J iff it has a
boundary node of I ? J as an endpoint and is a
boundary edge of I or J.
Proof. (?) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(?) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I ? J, and an edge e? outside I, which
must also be outside J. For e < J (because I and
J are disjoint), and if e? ? J, then v would be a
boundary node of J. Therefore, e? < I ? J, so v is
a boundary node of I ? J. Case (ii): v has an edge
in I and therefore I ? J, and an edge not in either
I or J. 
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v ? bn(I) do
E ? be(I, v) ? be(J, v)
if v < bn(J) or v has an edge not in E then
add v to bn(I ? J)
be(I ? J, v)? E
for all v ? bn(J) do
if v < bn(I) then
add v to bn(I ? J)
be(I ? J, v)? be(I, v) ? be(J, v)
(m ? I ? J)? (m ? I) ? (m ? J)
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
3.2 Treewidth
Lautemann?s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
927
the notion of treewidth (Gildea, 2011), which we
review briefly here.
Definition 7. A tree decomposition of a graph
H = ?V, E? is a tree T , each of whose nodes ?
is associated with sets V? ? V and E? ? E, with
the following properties:
1. Vertex cover: For each v ? V , there is a node
? ? T such that v ? V?.
2. Edge cover: For each e = (v1 ? ? ? vk) ? E,
there is exactly one node ? ? T such that e ?
E?. We say that ? introduces e. Moreover,
v1, . . . , vk ? V?.
3. Running intersection: For each v ? V , the set
{? ? T | v ? V?} is connected.
The width of T is max |V?| ? 1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
?V, E, X? is a tree decomposition of ?V, E? that has
the additional property that all the external nodes
belong to V? for some ?. (Without loss of general-
ity, we assume that ? is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
K4 =
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al, 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a na??ve algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T . Then,
augment the V? as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SynSem algorithm (Jones et al, 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
method mean max
exact 1.491 2
approximate 1.494 3
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
(a) 0
a
believe? arg1
b
girl?
arg0
1
Y
(b) 0
1
0
b 1
0
a
b 1
arg1
a
b 1
Y
?
0
b
arg0
b
girl?
?
0believe?
?
Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al (2011)). Each tree node ?
must be one of:
? A leaf node, such that V? = ?.
? A unary node, which introduces exactly one
edge e.
? A binary node, which introduces no edges.
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A ?
R) ? G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
928
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let RD? be the subgraph of R in-
duced by the union of E?? for all ?? equal to or
dominated by ?. Then we can show the following.
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of RD? belong to V? ? Vparent(?).
Proof. Let v be a boundary node of RD?. Node v
must have an edge in RD? and therefore in R?? for
some ?? dominated by or equal to ?.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both V? and Vparent(?)
must contain v as well.
Case 2: v has an edge not in RD?. Therefore
there must be a tree node not dominated by or
equal to ? that contains this edge, and therefore
v. So by the running intersection property, ? and
its parent must contain v as well. 
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
3.3 Inference rules
We present the parsing algorithm as a deductive
system (Shieber et al, 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X ? V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A ??G I. Thus, the goalitem is [S ,H, ?]. An active item has the form
[A? R, ?, I, ?], where
? (A? R) is a production of G
? ? is a node of TR
? I is a subgraph of H
? ? is a bijection between the boundary nodes
of RD? and those of I.
The parser must ensure that ? is a bijection when
it creates a new item. Below, we use the notation
{e 7? e?} or {e 7? X} for the mapping that sends
each node of e to the corresponding node of e?
or X.
Passive items are generated by the following
rule:
? Root [B? Q, ?, J, ?]
[B, J, X]
where ? is the root of TQ, and X j = ?(XQ, j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
? Leaf
[A? R, ?, ?, ?]
where ? is a leaf node of TR.
? (Unary) Nonterminal
[A? R, ?1, I, ?] [B, J, X]
[A? R, ?, I ? J, ? ? {e 7? X}]
where ?1 is the only child of ?, and e is intro-
duced by ? and is labeled with nonterminal B.
? (Unary) Terminal
[A? R, ?1, I, ?]
[A? R, ?, I ? {e?}, ? ? {e 7? e?}]
where ?1 is the only child of ?, e is introduced
by ?, and e and e? are both labeled with ter-
minal a.
? Binary
[A? R, ?1, I, ?1] [A? R, ?2, J, ?2]
[A? R, ?, I ? J, ?1 ? ?2]
where ?1 and ?2 are the two children of ?.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key ?B, |bn(J)|?, so that when the next item on the
agenda is an active item [A ? R, ?1, I, ?], we
know that all possible matching passive items are
929
S ?
X
X
X
X ?
a
a a
a
a
(a) (b)
a
a a
a aa
(c)
Figure 4: Illustration of unsoundness in the recog-
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key ??(e), |e|?. Similarly, active items should
be indexed by key ??(e), |e|? so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (?1
or ?2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
3.4 The disjointness check
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H?, as well as a graph isomorphism
? : H? ? H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H? is isomorphic to the input graph H? as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
Condition (1) is enough to guarantee that ? is lo-
cally one-to-one in the sense that for all v ? H?, ?
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if ?I : I? ? H and
?J : J? ? H are locally one-to-one, then ?I ? ?J
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where ? changes (say) from being
one-to-one to two-to-one. So if ? is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that ? maps only
one node to the marker m. We can show this again
by induction: if ?I and ?J each map only one node
to m, then ?I??J must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that ?I , ?J , and ?I ? ?J
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H? are connected graphs,
? : H? ? H is locally one-to-one, and ??1 is de-
fined for some node of H, then ? is a bijection.
Proof. Suppose that ? is not a bijection. Then
there must be two nodes v?1, v?2 ? H? such that
?(v?1) = ?(v?2) = v ? H. We also know that thereis a node, namely, m, such that m? = ??1(m) is de-
fined.2 Choose a path ? (ignoring edge direction)
from v to m. Because ? is a local isomorphism,
we can construct a path from v?1 to m? that mapsto ?. Similarly, we can construct a path from v?2to m? that maps to ?. Let u? be the first node that
these two paths have in common. But u? must have
two edges that map to the same edge, which is a
contradiction. 
2If H were not connected, we would choose the marker in
the same connected component as v.
930
3.5 Complexity
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of RD? or RD?i , all
of which belong to V? (by Proposition 3), so there
are at most |V?| ? k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A? R, ?, I, ?], every boundary node of RD? must
belong to V??Vparent(?) (by Proposition 3). There-
fore the number of boundary nodes is at most k+1
(but typically less), and the number of possible
items is in O((2dn)k+1).
4 Synchronous Parsing
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
?N, T, T ?, P, S ?, where
? N is a finite set of nonterminal symbols
? T and T ? are finite sets of terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
(A? ?R,R?,??), where R is a graph fragment
over N ? T and R? is a graph fragment over
N ? T ?. The relation ? is a bijection linking
nonterminal mentions in R and R?, such that
if e ? e?, then they have the same label. We
call R the source side and R? the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A? ?R,R?,??), we con-
struct a nice tree decomposition of R?R? such that:
? All the external nodes of both R and R? be-
long to V? for some ?. (Without loss of gen-
erality, assume that ? is the root.)
? If e ? e?, then e and e? are introduced by the
same tree node.
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I?, X?] and active
items have the form [A? R : R?, ?, I, ?, I?, ??].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
[A? R : R?, ?1, I, ?, I?, ??] [B, J, X, J?, X?]
[A? R : R?, ?, I ? J, ? ? {e j 7? X j},
I? ? J?, ?? ? {e?j 7? X?j}]
where ?1 is the only child of ?, e and e? are both
introduced by ? and e ? e?, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
5 Conclusion
Although Lautemann?s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann?s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
3The Bolinas toolkit can be downloaded from
?http://www.isi.edu/licensed-sw/bolinas/?.
931
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
References
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ?97), pages 29?36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Micha? Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95?162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231?248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69?124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399?421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65?72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars?basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136?167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
932
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 765?774,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Kneser-Ney Smoothing on Expected Counts
Hui Zhang
Department of Computer Science
University of Southern California
hzhang@isi.edu
David Chiang
Information Sciences Institute
University of Southern California
chiang@isi.edu
Abstract
Widely used in speech and language pro-
cessing, Kneser-Ney (KN) smoothing has
consistently been shown to be one of
the best-performing smoothing methods.
However, KN smoothing assumes integer
counts, limiting its potential uses?for ex-
ample, inside Expectation-Maximization.
In this paper, we propose a generaliza-
tion of KN smoothing that operates on
fractional counts, or, more precisely, on
distributions over counts. We rederive all
the steps of KN smoothing to operate
on count distributions instead of integral
counts, and apply it to two tasks where
KN smoothing was not applicable before:
one in language model adaptation, and the
other in word alignment. In both cases,
our method improves performance signifi-
cantly.
1 Introduction
In speech and language processing, smoothing is
essential to reduce overfitting, and Kneser-Ney
(KN) smoothing (Kneser and Ney, 1995; Chen
and Goodman, 1999) has consistently proven to be
among the best-performing and most widely used
methods. However, KN smoothing assumes inte-
ger counts, whereas in many NLP tasks, training
instances appear with possibly fractional weights.
Such cases have been noted for language model-
ing (Goodman, 2001; Goodman, 2004), domain
adaptation (Tam and Schultz, 2008), grapheme-to-
phoneme conversion (Bisani and Ney, 2008), and
phrase-based translation (Andr?es-Ferrer, 2010;
Wuebker et al, 2012).
For example, in Expectation-Maximization
(Dempster et al, 1977), the Expectation (E) step
computes the posterior distribution over possi-
ble completions of the data, and the Maximiza-
tion (M) step reestimates the model parameters as
if that distribution had actually been observed. In
most cases, the M step is identical to estimating
the model from complete data, except that counts
of observations from the E step are fractional. It
is common to apply add-one smoothing to the
M step, but we cannot apply KN smoothing.
Another example is instance weighting. If we
assign a weight to each training instance to indi-
cate how important it is (say, its relevance to a par-
ticular domain), and the counts are not integral,
then we again cannot train the model using KN
smoothing.
In this paper, we propose a generalization of KN
smoothing (called expected KN smoothing) that
operates on fractional counts, or, more precisely,
on distributions over counts. We rederive all the
steps of KN smoothing to operate on count distri-
butions instead of integral counts. We demonstrate
how to apply expected KN to two tasks where KN
smoothing was not applicable before. One is lan-
guage model domain adaptation, and the other is
word alignment using the IBM models (Brown et
al., 1993). In both tasks, expected KN smoothing
improves performance significantly.
2 Smoothing on integral counts
Before presenting our method, we review KN
smoothing on integer counts as applied to lan-
guage models, although, as we will demonstrate
in Section 7, KN smoothing is applicable to other
tasks as well.
2.1 Maximum likelihood estimation
Let uw stand for an n-gram, where u stands for
the (n ? 1) context words and w, the predicted
word. Let c(uw) be the number of occurrences
of uw. We use a bullet (?) to indicate summa-
tion over words, that is, c(u?) =
?
w
c(uw). Under
maximum-likelihood estimation (MLE), we max-
765
imize
L =
?
uw
c(uw) log p(w | u),
obtaining the solution
p
mle
(w | u) =
c(uw)
c(u?)
. (1)
2.2 Absolute discounting
Absolute discounting (Ney et al, 1994) ? on which
KN smoothing is based ? tries to generalize bet-
ter to unseen data by subtracting a discount from
each seen n-gram?s count and distributing the sub-
tracted discounts to unseen n-grams. For now, we
assume that the discount is a constant D, so that
the smoothed counts are
c?(uw) =
?
?
?
?
?
?
?
c(uw) ? D if c(uw) > 0
n
1+
(u?)Dq
u
(w) otherwise
where n
1+
(u?) = |{w | c(uw) > 0}| is the number
of word types observed after context u, and q
u
(w)
specifies how to distribute the subtracted discounts
among unseen n-gram types. Maximizing the like-
lihood of the smoothed counts c?, we get
p(w | u) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
c(uw) ? D
c(u?)
if c(uw) > 0
n
1+
(u?)Dq
u
(w)
c(u?)
otherwise.
(2)
How to choose D and q
u
(w) are described in the
next two sections.
2.3 Estimating D by leaving-one-out
The discount D can be chosen by various means;
in absolute discounting, it is chosen by the method
of leaving one out. Given N training instances, we
form the probability of each instance under the
MLE using the other (N ? 1) instances as train-
ing data; then we maximize the log-likelihood of
all those instances. The probability of an n-gram
token uw using the other tokens as training data is
p
loo
(w | u) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
c(uw) ? 1 ? D
c(u?) ? 1
c(uw) > 1
(n
1+
(u?) ? 1)Dq
u
(w)
c(u?) ? 1
c(uw) = 1.
We want to find the D that maximizes the
leaving-one-out log-likelihood
L
loo
=
?
uw
c(uw) log p
loo
(w | u)
=
?
uw|c(uw)>1
c(uw) log
c(uw) ? 1 ? D
c(u?) ? 1
+
?
uw|c(uw)=1
log
(n
1+
(u?) ? 1)Dq
u
(w)
c(u?) ? 1
=
?
r>1
rn
r
log(r ? 1 ? D) + n
1
logD +C, (3)
where n
r
= |{uw | c(uw) = r}| is the number of n-
gram types appearing r times, and C is a constant
not depending on D. Setting the partial derivative
with respect to D to zero, we have
?L
loo
?D
= ?
?
r>1
rn
r
r ? 1 ? D
+
n
1
D
n
1
D
=
?
r>1
rn
r
r ? 1 ? D
?
2n
2
1 ? D
.
Solving for D, we have
D ?
n
1
n
1
+ 2n
2
. (4)
Theoretically, we can use iterative methods to op-
timize D. But in practice, setting D to this upper
bound is effective and simple (Ney et al, 1994;
Chen and Goodman, 1999).
2.4 Estimating the lower-order distribution
Finally, q
u
(w) is defined to be proportional to an
(n ? 1)-gram model p
?
(w | u
?
), where u
?
is the
(n ? 2)-gram suffix of u. That is,
q
u
(w) = ?(u)p
?
(w | u
?
),
where ?(u) is an auxiliary function chosen to make
the distribution p(w | u) in (2) sum to one.
Absolute discounting chooses p
?
(w | u
?
) to be
the maximum-likelihood unigram distribution; un-
der KN smoothing (Kneser and Ney, 1995), it is
chosen to make p in (2) satisfy the following con-
straint for all (n ? 1)-grams u
?
w:
p
mle
(u
?
w) =
?
v
p(w | vu
?
)p
mle
(vu
?
). (5)
Substituting in the definition of p
mle
from (1) and
p from (2) and canceling terms, we get
c(u
?
w) =
?
v|c(vu
?
w)>0
(c(vu
?
w) ? D)
+
?
v|c(vu
?
w)=0
n
1+
(vu
?
?)D?(vu
?
)p
?
(w | u
?
).
766
Solving for p
?
(w | u
?
), we have
p
?
(w | u
?
) =
?
v|c(vu
?
w)>0
1
?
v|c(vu
?
w)=0
n
1+
(vu
?
?)?(vu
?
)
.
Kneser and Ney assume the denominator is con-
stant in w and renormalize to get an approximation
p
?
(w | u
?
) ?
n
1+
(?u
?
w)
n
1+
(?u
?
?)
, (6)
where
n
1+
(?u
?
w) = |{v | c(vu
?
w) > 0}|
n
1+
(?u
?
?) =
?
w
n
1+
(?u
?
w).
3 Count distributions
The computation of D and p
?
above made use of
n
r
and n
r+
, which presupposes integer counts. But
in many applications, the counts are not integral,
but fractional. How do we apply KN smoothing in
such cases? In this section, we introduce count dis-
tributions as a way of circumventing this problem.
3.1 Definition
In the E step of EM, we compute a probability dis-
tribution (according to the current model) over all
possible completions of the observed data, and the
expected counts of all types, which may be frac-
tional. However, note that in each completion of
the data, the counts are integral. Although it does
not make sense to compute n
r
or n
r+
on fractional
counts, it does make sense to compute them on
possible completions.
In other situations where fractional counts arise,
we can still think of the counts as expectations un-
der some distribution over possible ?realizations?
of the data. For example, if we assign a weight
between zero and one to every instance in a cor-
pus, we can interpret each instance?s weight as the
probability of that instance occurring or not, yield-
ing a distribution over possible subsets of the data.
Let X be a random variable ranging over pos-
sible realizations of the data, and let c
X
(uw) be
the count of uw in realization X. The expecta-
tion E[c
X
(uw)] is the familiar fractional expected
count of uw, but we can also compute the proba-
bilities p(c
X
(uw) = r) for any r. From now on, for
brevity, we drop the subscript X and understand
c(uw) to be a random variable depending on X.
The n
r
(u?) and n
r+
(u?) and related quantities also
become random variables depending on X.
For example, suppose that our data consists of
the following bigrams, with their weights:
(a) fat cat 0.3
(b) fat cat 0.8
(c) big dog 0.9
We can interpret this as a distribution over eight
subsets (not all distinct), with probabilities:
? 0.7 ? 0.2 ? 0.1 = 0.014
{a} 0.3 ? 0.2 ? 0.1 = 0.006
{b} 0.7 ? 0.8 ? 0.1 = 0.056
{a, b} 0.3 ? 0.8 ? 0.1 = 0.024
{c} 0.7 ? 0.2 ? 0.9 = 0.126
{a, c} 0.3 ? 0.2 ? 0.9 = 0.054
{b, c} 0.7 ? 0.8 ? 0.9 = 0.504
{a, b, c} 0.3 ? 0.8 ? 0.9 = 0.216
Then the count distributions and the E[n
r
] are:
r = 1 r = 2 r > 0
p(c(fat cat) = r) 0.62 0.24 0.86
p(c(big dog) = r) 0.9 0 0.9
E[n
r
] 1.52 0.24
3.2 Efficient computation
How to compute these probabilities and expecta-
tions depends in general on the structure of the
model. If we assume that all occurrences of uw
are independent (although in fact they are not al-
ways), the computation is very easy. If there are
k occurrences of uw, each occurring with proba-
bility p
i
, the count c(uw) is distributed according
to the Poisson-binomial distribution (Hong, 2013).
The expected count E[c(uw)] is just
?
i
p
i
, and the
distribution of c(uw) can be computed as follows:
p(c(uw) = r) = s(k, r)
where s(k, r) is defined by the recurrence
s(k, r) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
s(k ? 1, r)(1 ? p
k
)
+ s(k ? 1, r ? 1)p
k
if 0 ? r ? k
1 if k = r = 0
0 otherwise.
We can also compute
p(c(uw) ? r) = max
{
s(m, r), 1 ?
?
r
?
<r
s(m, r
?
)
}
,
the floor operation being needed to protect against
rounding errors, and we can compute
E[n
r
(u?)] =
?
w
p(c(uw) = r)
E[n
r+
(u?)] =
?
w
p(c(uw) ? r).
Since, as we shall see, we only need to compute
these quantities up to a small value of r (2 or 4),
this takes time linear in k.
767
4 Smoothing on count distributions
We are now ready to describe how to apply KN
smoothing to count distributions. Below, we reca-
pitulate the derivation of KN smoothing presented
in Section 2, using the expected log-likelihood
in place of the log-likelihood and applying KN
smoothing to each possible realization of the data.
4.1 Maximum likelihood estimation
The MLE objective function is the expected log-
likelihood,
E[L] = E
?
?
?
?
?
?
?
?
uw
c(uw) log p(w | u)
?
?
?
?
?
?
?
=
?
uw
E[c(uw)] log p(w | u)
whose maximum is
p
mle
(w | u) =
E[c(uw)]
E[c(u?)]
. (7)
4.2 Absolute discounting
If we apply absolute discounting to every realiza-
tion of the data, the expected smoothed counts are
E[c?(uw)] =
?
r>0
p(c(uw) = r)(r ? D)
+ p(c(uw) = 0)E[n
1+
(u?)]Dq
u
(w)
= E[c(uw)] ? p(c(uw) > 0)D
+ p(c(uw) = 0)E[n
1+
(u?)]Dq
u
(w) (8)
where, to be precise, the expectation E[n
1+
(u?)]
should be conditioned on c(uw) = 0; in practice, it
seems safe to ignore this. The MLE is then
p(w | u) =
E[c?(uw)]
E[c?(u?)]
. (9)
4.3 Estimating D by leaving-one-out
It would not be clear how to perform leaving-
one-out estimation on fractional counts, but here
we have a distribution over realizations of the
data, each with integral counts, and we can
perform leaving-one-out estimation on each of
these. In other words, our goal is to find the D
that maximizes the expected leaving-one-out log-
likelihood, which is just the expected value of (3):
E[L
loo
] = E
[
n
1
logD +
?
r>1
rn
r
log(r ? 1 ? D) +C
]
= E[n
1
] logD
+
?
r>1
rE[n
r
] log(r ? 1 ? D) +C,
where C is a constant not depending on D. We
have made the assumption that the n
r
are indepen-
dent.
By exactly the same reasoning as before, we ob-
tain an upper bound for D:
D ?
E[n
1
]
E[n
1
] + 2E[n
2
]
. (10)
In our example above, D =
1.52
1.52+2?0.24
= 0.76.
4.4 Estimating the lower-order distribution
We again require p
?
to satisfy the marginal con-
straint (5). Substituting in (7) and solving for p
?
as
in Section 2.4, we obtain the solution
p
?
(w | u
?
) =
E[n
1+
(?u
?
w)]
E[n
1+
(?u
?
?)]
. (11)
For the example above, the estimates for the un-
igram model p
?
(w) are
p
?
(cat) =
0.86
0.86+0.9
? 0.489
p
?
(dog) =
0.9
0.86+0.9
? 0.511.
4.5 Extensions
Chen and Goodman (1999) introduce three exten-
sions to Kneser-Ney smoothing which are now
standard. For our experiments, we used all three,
for both integral counts and count distributions.
4.5.1 Interpolation
In interpolated KN smoothing, the subtracted dis-
counts are redistributed not only among unseen
events but also seen events. That is,
c?(uw) = max{0, c(uw) ? D} + n
1+
(u?)Dp
?
(w | u
?
).
In this case, ?(u) is always equal to one, so that
q
u
(w) = p
?
(w | u
?
). (Also note that (6) becomes
an exact solution to the marginal constraint.) The-
oretically, this requires us to derive a new estimate
for D. However, as this is not trivial, nearly all im-
plementations simply use the original estimate (4).
On count distributions, the smoothed counts be-
come
E[c?(uw)] = E[c(uw)] ? p(c(uw) > 0)D
+ E[n
1+
(u?)]Dp
?
(w | u
?
). (12)
In our example, the smoothed counts are:
uw E[c?]
fat cat 1.1 ? 0.86 ? 0.76 + 0.86 ? 0.76 ? 0.489 ? 0.766
fat dog 0 ? 0 ? 0.76 + 0.86 ? 0.76 ? 0.511 ? 0.334
big cat 0 ? 0 ? 0.76 + 0.9 ? 0.76 ? 0.489 ? 0.334
big dog 0.9 ? 0.9 ? 0.76 + 0.9 ? 0.76 ? 0.511 ? 0.566
768
which give the smoothed probability estimates:
p(cat | fat) =
0.766
0.766+0.334
= 0.696
p(dog | fat) =
0.334
0.766+0.334
= 0.304
p(dog | big) =
0.334
0.334+0.556
= 0.371
p(cat | big) =
0.556
0.334+0.556
= 0.629.
4.5.2 Modified discounts
Modified KN smoothing uses a different discount
D
r
for each count r < 3, and a discount D
3+
for
counts r ? 3. On count distributions, a similar ar-
gument to the above leads to the estimates:
D
1
? 1 ? 2Y
E[n
2
]
E[n
1
]
D
2
? 2 ? 3Y
E[n
3
]
E[n
2
]
D
3+
? 3 ? 4Y
E[n
4
]
E[n
3
]
Y =
E[n
1
]
E[n
1
] + 2E[n
2
]
.
(13)
One side-effect of this change is that (6) is no
longer the correct solution to the marginal con-
straint (Teh, 2006; Sundermeyer et al, 2011). Al-
though this problem can be fixed, standard imple-
mentations simply use (6).
4.5.3 Recursive smoothing
In the original KN method, the lower-order
model p
?
was estimated using (6); recursive KN
smoothing applies KN smoothing to p
?
. To do this,
we need to reconstruct counts whose MLE is (6).
On integral counts, this is simple: we generate, for
each n-gram type vu
?
w, an (n?1)-gram token u
?
w,
for a total of n
1+
(?u
?
w) tokens. We then apply KN
smoothing to these counts.
Analogously, on count distributions, for each n-
gram type vu
?
w, we generate an (n ? 1)-gram to-
ken u
?
w with probability p(c(vu
?
w) > 0). Since
E[c(u
?
w)] =
?
v
p(c(vu
?
w) > 0) = E[n
1+
(?u
?
w)],
this has (11) as its MLE and therefore satisfies the
marginal constraint. We then apply expected KN
smoothing to these count distributions.
For the example above, the count distributions
used for the unigram distribution would be:
r = 0 r = 1
p(c(cat) = r) 0.14 0.86
p(c(dog) = r) 0.1 0.9
4.6 Summary
In summary, to perform expected KN smoothing
(either the original version or Chen and Good-
man?s modified version), we perform the steps
listed below:
orig. mod.
compute count distributions ?3.2
estimate discount D (10) (13)
estimate lower-order model p
?
(11) ?4.5.3
compute smoothed counts c? (8) (12)
compute probabilities p (9)
The computational complexity of expected KN
is almost identical to KN on integral counts. The
main addition is computing and storing the count
distributions. Using the dynamic program in Sec-
tion 3.2, computing the distributions for each r is
linear in the number of n-gram types, and we only
need to compute the distributions up to r = 2 (or
r = 4 for modified KN), and store them for r = 0
(or up to r = 2 for modified KN).
5 Related Work
Witten-Bell (WB) smoothing is somewhat easier
than KN to adapt to fractional counts. The SRI-
LM toolkit (Stolcke, 2002) implements a method
which we call fractional WB:
p(w | u) = ?(u)p
mle
(w | u) + (1 ? ?(u))p
?
(w | u
?
)
?(u) =
E[c(u)]
E[c(u)] + n
1+
(u?)
,
where n
1+
(u?) is the number of word types ob-
served after context u, computed by ignoring all
weights. This method, although simple, inconsis-
tently uses weights for counting tokens but not
types. Moreover, as we will see below, it does not
perform as well as expected KN.
The only previous adaptation of KN smoothing
to fractional counts that we are aware of is that
of Tam and Schultz (2008) and Bisani and Ney
(2008), called fractional KN. This method sub-
tracts D directly from the fractional counts, zero-
ing out counts that are smaller than D. The dis-
count D must be set by minimizing an error metric
on held-out data using a line search (Tam, p. c.) or
Powell?s method (Bisani and Ney, 2008), requiring
repeated estimation and evaluation of the language
model. By contrast, we choose D by leaving-one-
out. Like KN on integral counts, our method has
a closed-form approximation and requires neither
held-out data nor trial and error.
769
6 Language model adaptation
N-gram language models are widely used in appli-
cations like machine translation and speech recog-
nition to select fluent output sentences. Although
they can easily be trained on large amounts of data,
in order to perform well, they should be trained on
data containing the right kind of language. For ex-
ample, if we want to model spoken language, then
we should train on spoken language data. If we
train on newswire, then a spoken sentence might
be regarded as ill-formed, because the distribution
of sentences in these two domains are very differ-
ent. In practice, we often have limited-size training
data from a specific domain, and large amounts
of data consisting of language from a variety of
domains (we call this general-domain data). How
can we utilize the large general-domain dataset to
help us train a model on a specific domain?
Many methods (Lin et al, 1997; Gao et al,
2002; Klakow, 2000; Moore and Lewis, 2010; Ax-
elrod et al, 2011) rank sentences in the general-
domain data according to their similarity to the
in-domain data and select only those with score
higher than some threshold. Such methods are ef-
fective and widely used. However, sometimes it is
hard to say whether a sentence is totally in-domain
or out-of-domain; for example, quoted speech in a
news report might be partly in-domain if the do-
main of interest is broadcast conversation. Here,
we propose to assign each sentence a probability
to indicate how likely it is to belong to the domain
of interest, and train a language model using ex-
pected KN smoothing. We show that this approach
yields models with much better perplexity than the
original sentence-selection approach.
6.1 Method
One of the most widely used sentence-selection
approaches is that of Moore and Lewis (2010).
They first train two language models, p
in
on a set
of in-domain data, and p
out
on a set of general-
domain data. Then each sentence w is assigned a
score
H(w) =
log(p
in
(w)) ? log(p
out
(w))
|w|
.
They set a threshold on the score to select a subset.
We adapt this approach as follows. After selec-
tion, for each sentence in the subset, we use a sig-
moid function to map the scores into probabilities:
p(w is in-domain) =
1
1 + exp(?H(w))
.
.
.
0 0.2 0.4
0.6
0.8 1 1.2 1.4
140
160
180
200
220
240
260
sentences selected (?10
7
)
p
e
r
p
l
e
x
i
t
y
. .
fractional KN
. .
fractional WB
. .
integral KN
. .
expected KN
Figure 1: On the language model adaptation task,
expected KN outperforms all other methods across
all sizes of selected subsets. Integral KN is ap-
plied to unweighted instances, while fractional
WB, fractional KN and expected KN are applied
to weighted instances.
Then we use the weighted subset to train a lan-
guage model with expected KN smoothing.
6.2 Experiments
Moore and Lewis (2010) test their method by
partitioning the in-domain data into training data
and test data, both of which are disjoint from
the general-domain data. They use the in-domain
training data to select a subset of the general-
domain data, build a language model on the se-
lected subset, and evaluate its perplexity on the in-
domain test data. Here, we follow this experimen-
tal framework and compare Moore and Lewis?s
unweighted method to our weighted method.
For our experiments, we used all the English
data allowed for the BOLT Phase 1 Chinese-
English evaluation. We took 60k sentences (1.7M
words) of web forum data as in-domain data,
further subdividing it into 54k sentences (1.5M
words) for training, 3k sentences (100k words)
for testing, and 3k sentences (100k words) for fu-
ture use. The remaining 12.7M sentences (268M
words) we treated as general-domain data.
We trained trigram language models and com-
pared expected KN smoothing against integral KN
smoothing, fractional WB smoothing, and frac-
tional KN smoothing, measuring perplexity across
various subset sizes (Figure 1). For fractional KN,
for each subset size, we optimized D to mini-
770
mize perplexity on the test set to give it the great-
est possible advantage; nevertheless, it is clearly
the worst performer. Expected KN consistently
gives the best perplexity, and, at the optimal sub-
set size, obtains better perplexity (148) than the
other methods (156 for integral KN, 162 for frac-
tional WB and 197 for fractional KN). Finally, we
note that integral KN is very sensitive to the subset
size, whereas expected KN and the other methods
are more robust.
7 Word Alignment
In this section, we show how to apply expected KN
to the IBM word alignment models (Brown et al,
1993). This illustrates both how to use expected
KN inside EM and how to use it beyond language
modeling. Of course, expected KN can be applied
to other instances of EM besides word alignment.
7.1 Problem
Given a French sentence f = f
1
f
2
? ? ? f
m
and its
English translation e = e
1
e
2
? ? ? e
n
, an alignment a
is a sequence a
1
, a
2
, . . . , a
m
, where a
i
is the index
of the English word which generates the French
word f
i
, or NULL. As is common, we assume that
each French word can only be generated from one
English word or from NULL (Brown et al, 1993;
Och and Ney, 2003; Vogel et al, 1996).
The IBM models and related models define
probability distributions p(a, f | e, ?), which model
how likely a French sentence f is to be generated
from an English sentence ewith word alignment a.
Different models parameterize this probability dis-
tribution in different ways. For example, Model 1
only models the lexical translation probabilities:
p(a, f | e, ?) ?
m
?
j=1
p( f
j
| e
a
j
).
Models 2?5 and the HMM model introduce addi-
tional components to model word order and fer-
tility. All, however, have the lexical translation
model p( f
j
| e
i
) in common. It also contains most
of the model?s parameters and is where overfit-
ting occurs most. Thus, here we only apply KN
smoothing to the lexical translation probabilities,
leaving the other model components for future
work.
7.2 Method
The f and e are observed, while a is a latent vari-
able. Normally, in the E step, we collect expected
counts E[c(e, f )] for each e and f . Then, in the M
step, we find the parameter values that maximize
their likelihood. However, MLE is prone to over-
fitting, one symptom of which is the ?garbage col-
lection? phenomenon where a rare English word is
wrongly aligned to many French words.
To reduce overfitting, we use expected KN
smoothing during the M step. That is, during the
E step, we calculate the distribution of c(e, f ) for
each e and f , and during the M step, we train a
language model on bigrams e f using expected KN
smoothing (that is, with u = e and w = f ). This
gives a smoothed probability estimate for p( f | e).
One question that arises is: what distribution to
use as the lower-order distribution p
?
? Following
common practice in language modeling, we use
the unigram distribution p( f ) as the lower-order
distribution. We could also use the uniform distri-
bution over word types, or a distribution that as-
signs zero probability to all known word types.
(The latter case is equivalent to a backoff language
model, where, since all bigrams are known, the
lower-order model is never used.) Below, we com-
pare the performance of all three choices.
7.3 Alignment experiments
We modified GIZA++ (Och and Ney, 2003) to
perform expected KN smoothing as described
above. Smoothing is enabled or disabled with a
command-line switch, making direct comparisons
simple. Our implementation is publicly available
as open-source software.
1
We carried out experiments on two language
pairs: Arabic to English and Czech to English.
For Arabic-English, we used 5.4+4.3 million
words of parallel text from the NIST 2009 con-
strained task,
2
and 346 word-aligned sentence
pairs (LDC2006E86) for evaluation. For Czech-
English, we used all 2.0+2.2 million words of
training data from the WMT 2009 shared task,
and 515 word-aligned sentence pairs (Bojar and
Prokopov?a, 2006) for evaluation.
For all methods, we used five iterations of IBM
Models 1, 2, and HMM, followed by three iter-
ations of IBM Models 3 and 4. We applied ex-
pected KN smoothing to all iterations of all mod-
els. We aligned in both the foreign-to-English
1
https://github.com/hznlp/giza-kn
2
All data was used except for: United Nations pro-
ceedings (LDC2004E13), ISI Automatically Extracted Par-
allel Text (LDC2007E08), and Ummah newswire text
(LDC2004T18).
771
Alignment F1 Bleu
Smoothing p
?
Ara-Eng Cze-Eng Ara-Eng Cze-Eng
none (baseline) ? 66.5 67.2 37.0 16.6
variational Bayes uniform 65.7 65.5 36.5 16.6
fractional WB
unigram 60.1 63.7 ? ?
uniform 60.8 66.5 37.8 16.9
zero 60.8 65.2 ? ?
fractional KN unigram 67.7 70.2 37.2 16.5
expected KN
unigram 69.7 71.9 38.2 17.0
uniform 69.4 71.3 ? ?
zero 69.2 71.9 ? ?
Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other
methods. For variational Bayes, we followed Riley and Gildea (2012) in setting ? to zero (so that the
choice of p
?
is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2).
and English-to-foreign directions and then used
the grow-diag-final method to symmetrize them
(Koehn et al, 2003), and evaluated the alignments
using F-measure against gold word alignments.
As shown in Table 1, for KN smoothing, in-
terpolation with the unigram distribution performs
the best, while for WB smoothing, interestingly,
interpolation with the uniform distribution per-
forms the best. The difference can be explained by
the way the two smoothing methods estimate p
?
.
Consider again a training example with a word e
that occurs nowhere else in the training data. In
WB smoothing, p
?
( f ) is the empirical unigram
distribution. If f contains a word that is much
more frequent than the correct translation of e,
then smoothing may actually encourage the model
to wrongly align e with the frequent word. This
is much less of a problem in KN smoothing,
where p
?
is estimated from bigram types rather
than bigram tokens.
We also compared with variational Bayes (Ri-
ley and Gildea, 2012) and fractional KN. Overall,
expected KN performs the best. Variational Bayes
is not consistent across different language pairs.
While fractional KN does beat the baseline for
both language pairs, the value of D, which we op-
timized D to maximize F1, is not consistent across
language pairs: as shown in Figure 2, on Arabic-
English, a smaller D is better, while for Czech-
English, a larger D is better. By contrast, expected
KN uses a closed-form expression for D that out-
performs the best performance of fractional KN.
Table 2 shows that, if we apply expected KN
smoothing to only selected stages of training,
adding smoothing always brings an improvement,
.
0 0.2 0.4
0.6
0.8 1
64
66
68
70
72
D
a
l
i
g
n
m
e
n
t
F
1
. .
Cze-Eng
. .
Ara-Eng
Figure 2: Alignment F1 vs. D of fractional KN
smoothing for word alignment.
Smoothed models Alignment F1
1 2 H 3 4 Ara-Eng Cze-Eng
? ? ? ? ? 66.5 67.2
? ? ? ? ? 67.3 67.9
? ? ? ? ? 68.0 68.7
? ? ? ? ? 68.6 70.0
? ? ? ? ? 66.9 68.4
? ? ? ? ? 67.0 68.6
? ? ? ? ? 69.7 71.9
Table 2: Smoothing more stages of training makes
alignment accuracy go up. For each row, we
smoothed all iterations of the models indicated.
Key: H = HMM model; ? = smoothing enabled;
? = smoothing disabled.
772
with the best setting being to smooth all stages.
This shows that expected KN smoothing is consis-
tently effective. It is also interesting to note that
smoothing is less helpful for the fertility-based
Models 3 and 4. Whether this is because modeling
fertility makes them less susceptible to ?garbage
collection,? or the way they approximate the E step
makes them less amenable to smoothing, or an-
other reason, would require further investigation.
7.4 Translation experiments
Finally, we ran MT experiments to see whether the
improved alignments also lead to improved trans-
lations. We used the same training data as before.
For the Arabic-English tasks, we used the NIST
2008 test set as development data and the NIST
2009 test set as test data; for the Czech-English
tasks, we used the WMT 2008 test set as develop-
ment data and the WMT 2009 test set as test data.
We used the Moses toolkit (Koehn et al, 2007)
to build MT systems using various alignments
(for expected KN, we used the one interpolated
with the unigram distribution, and for fractional
WB, we used the one interpolated with the uni-
form distribution). We used a trigram language
model trained on Gigaword (AFP, AP World-
stream, CNA, and Xinhua portions), and minimum
error-rate training (Och, 2003) to tune the feature
weights.
Table 1 shows that, although the relationship
between alignment F1 and Bleu is not very con-
sistent, expected KN smoothing achieves the best
Bleu among all these methods and is significantly
better than the baseline (p < 0.01).
8 Conclusion
For a long time, and as noted by many authors,
the usage of KN smoothing has been limited by its
restriction to integer counts. In this paper, we ad-
dressed this issue by treating fractional counts as
distributions over integer counts and generalizing
KN smoothing to operate on these distributions.
This generalization makes KN smoothing, widely
considered to be the best-performing smoothing
method, applicable to many new areas. We have
demonstrated the effectiveness of our method in
two such areas and showed significant improve-
ments in both.
Acknowledgements
We thank Qing Dou, Ashish Vaswani, Wilson Yik-
Cheung Tam, and the anonymous reviewers for
their input to this work. This research was sup-
ported in part by DOI IBC grant D12AP00225.
References
Jes?us Andr?es-Ferrer. 2010. Statistical approaches for
natural language modelling and monotone statisti-
cal machine translation. Ph.D. thesis, Universidad
Polit?ecnica de Valencia.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proc. EMNLP, pages 355?362.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Ondr?ej Bojar and Magdalena Prokopov?a. 2006.
Czech-English word alignment. In Proc. LREC,
pages 1236?1239.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19:263?311.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
13:359?394.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39:1?38.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statisti-
cal language modeling for Chinese. ACM Transac-
tions on Asian Language Information, 1:3?33.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling: Extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. HLT-NAACL, pages
305?312.
Yili Hong. 2013. On computing the distribution func-
tion for the Poisson binomial distribution. Compu-
tational Statistics and Data Analysis, 59:41?51.
Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proc. ICASSP,
pages 1695?1698.
773
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In
Proc. ICASSP 1995, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, Companion Volume, pages 177?180.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Proc. Eurospeech, pages 1463?1466.
Robert Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proc.
ACL, pages 220?224.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 8:1?38, 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Darcey Riley and Daniel Gildea. 2012. Improving
the IBM alignment models using variational Bayes.
In Proc. ACL (Volume 2: Short Papers), pages 306?
310.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. International Con-
ference on Spoken Language Processing, volume 2,
pages 901?904.
Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.
2011. On the estimation of discount parameters for
language model smoothing. In Proc. Interspeech,
pages 1433?1436.
Yik-Cheung Tam and Tanja Schultz. 2008. Correlated
bigram LSA for unsupervised language model adap-
tation. In Proc. NIPS, pages 1633?1640.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. COLING-ACL, pages 985?992.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. COLING, pages 836?841.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012. Leave-one-out phrase model training for
large-scale deployment. In Proc. WMT, pages 460?
467.
774
