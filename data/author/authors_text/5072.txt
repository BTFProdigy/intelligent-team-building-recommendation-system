Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 408?414,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Arabic OCR Error Correction Using Character Segment Correction, 
Language Modeling, and Shallow Morphology 
 
Walid Magdy and Kareem Darwish 
IBM Technology Development Center 
P.O. Box 166 El-Ahram, Giza, Egypt 
{wmagdy,darwishk}@eg.ibm.com  
 
  
 
Abstract 
This paper explores the use of a character 
segment based character correction 
model, language modeling, and shallow 
morphology for Arabic OCR error cor-
rection.  Experimentation shows that 
character segment based correction is su-
perior to single character correction and 
that language modeling boosts correction, 
by improving the ranking of candidate 
corrections, while shallow morphology 
had a small adverse effect.  Further, 
given sufficiently large corpus to extract 
a dictionary and to train a language 
model, word based correction works well 
for a morphologically rich language such 
as Arabic.   
1 Introduction 
Recent advances in printed document digitization 
and processing led to large scale digitization ef-
forts of legacy printed documents producing 
document images.  To enable subsequent proc-
essing and retrieval, the document images are 
often transformed to character-coded text using 
Optical Character Recognition (OCR).  Although 
OCR is fast, OCR output typically contains er-
rors.  The errors are even more pronounced in 
OCR?ed Arabic text due to Arabic?s orthographic 
and morphological properties.  The introduced 
errors adversely affect linguistic processing and 
retrieval of OCR?ed documents.  This paper ex-
plores the effectiveness post-OCR error correc-
tion.  The correction uses an improved character 
segment based noisy channel model, language 
modeling, and shallow morphological processing 
to correct OCR errors.  The paper will be organ-
ized as follows:  Section 2 provides background 
information on Arabic OCR and OCR error cor-
rection; Section 3 presents the error correction 
methodology; Section 4 reports and discusses 
experimental results; and Section 5 concludes the 
paper and provides possible future directions. 
2 Background 
This section reviews prior work on Arabic OCR 
for Arabic and OCR error correction. 
 
2.1 Arabic OCR 
The goal of OCR is to transform a document im-
age into character-coded text. The usual process 
is to automatically segment a document image 
into character images in the proper reading order 
using image analysis heuristics, apply an auto-
matic classifier to determine the character codes 
that most likely correspond to each character im-
age, and then exploit sequential context (e.g., 
preceding and following characters and a list of 
possible words) to select the most likely charac-
ter in each position. The character error rate can 
be influenced by reproduction quality (e.g., 
original documents are typically better than pho-
tocopies), the resolution at which a document 
was scanned, and any mismatch between the in-
stances on which the character image classifier 
was trained and the rendering of the characters in 
the printed document.  Arabic OCR presents sev-
eral challenges, including: 
? Arabic?s cursive script in which most charac-
ters are connected and their shape vary with posi-
tion in the word.  
? The optional use of word elongations and liga-
tures, which are special forms of certain letter 
sequences. 
? The presence of dots in 15 of the 28 letters to 
distinguish between different letters and the op-
tional use of diacritic which can be confused 
with dirt, dust, and speckle (Darwish and Oard, 
2002). 
? The morphological complexity of Arabic, 
which results in an estimated 60 billion possible 
408
surface forms, complicates dictionary-based er-
ror correction.  Arabic words are built from a 
closed set of about 10,000 root forms that typi-
cally contain 3 characters, although 4-character 
roots are not uncommon, and some 5-character 
roots do exist.  Arabic stems are derived from 
these root forms by fitting the root letters into a 
small set of regular patterns, which sometimes 
includes addition of ?infix? characters between 
two letters of the root (Ahmed, 2000). 
There is a number of commercial Arabic OCR 
systems, with Sakhr?s Automatic Reader and 
Shonut?s Omni Page being perhaps the most 
widely used. Retrieval of OCR degraded text 
documents has been reported for many lan-
guages, including English (Harding et al, 1997), 
Chinese (Tseng and Oard, 2001), and Arabic 
(Darwish and Oard, 2002). 
 
2.2 OCR Error Correction 
Much research has been done to correct 
recognition errors in OCR-degraded collections.  
There are two main categories of determining 
how to correct these errors. They are word-level 
and passage-level post-OCR processing. Some of 
the kinds of word level post-processing include 
the use of dictionary lookup, probabilistic 
relaxation, character and word n-gram frequency 
analysis (Hong, 1995), and morphological 
analysis (Oflazer, 1996). Passage-level post-
processing techniques include the use of word n-
grams, word collocations, grammar, conceptual 
closeness, passage level word clustering, 
linguistic context, and visual context. The 
following introduces some of the error correction 
techniques. 
? Dictionary Lookup:  Dictionary Lookup, which 
is the basis for the correction reported in this 
paper, is used to compare recognized words with 
words in a term list (Church and Gale, 1991; 
Hong, 1995; Jurafsky and Martin, 2000). If a 
word is found in the dictionary, then it is 
considered correct. Otherwise, a checker 
attempts to find a dictionary word that might be 
the correct spelling of the misrecognized word. 
Jurafsky and Martin (2000) illustrate the use of a 
noisy channel model to find the correct spelling 
of misspelled or misrecognized words. The 
model assumes that text errors are due to edit 
operations namely insertions, deletions, and 
substitutions. Given two words, the number of 
edit operations required to transform one of the 
words to the other is called the Levenshtein edit 
distance (Baeza-Yates and Navarro, 1996). To 
capture the probabilities associated with different 
edit operations, confusion matrices are 
employed. Another source of evidence is the 
relative probabilities that candidate word 
corrections would be observed. These 
probabilities can be obtained using word 
frequency in text corpus (Jurafsky and Martin, 
2000).  However, the dictionary lookup approach 
has the following problems (Hong, 1995):  
a) A correctly recognized word might not be in 
the dictionary. This problem could surface if the 
dictionary is small, if the correct word is an 
acronym or a named entity that would not 
normally appear in a dictionary, or if the 
language being recognized is morphologically 
complex. In a morphological complex language 
such as Arabic, German, and Turkish the number 
of valid word surface forms is arbitrarily large 
which complicates building dictionaries for spell 
checking.  
b) A word that is misrecognized is in the 
dictionary. An example of that is the recognition 
of the word ?tear? instead of ?fear?. This 
problem is particularly acute in a language such 
as Arabic where a large fraction of three letters 
sequences are valid words.   
? Character N-Grams:  Character n-grams maybe 
used alone or in combination with dictionary 
lookup (Lu et al, 1999; Taghva et al, 1994).  
The premise for using n-grams is that some letter 
sequences are more common than others and 
other letter sequences are rare or impossible. For 
example, the trigram ?xzx? is rare in the English 
language, while the trigram ?ies? is common. 
Using this method, an unusual sequence of letters 
can point to the position of an error in a 
misrecognized word.  This technique is 
employed by BBN?s Arabic OCR system (Lu et 
al., 1999). 
? Using Morphology:  Many morphologically 
complex languages, such as Arabic, Swedish, 
Finnish, Turkish, and German, have enormous 
numbers of possible words. Accounting for and 
listing all the possible words is not feasible for 
purposes of error correction. Domeij proposed a 
method to build a spell checker that utilizes a 
stem lists and orthographic rules, which govern 
how a word is written, and morphotactic rules, 
which govern how morphemes (building blocks 
of meanings) are allowed to combine, to accept 
legal combinations of stems (Domeij et al, 
1994). By breaking up compound words, 
dictionary lookup can be applied to individual 
constituent stems.  Similar work was done for 
Turkish in which an error tolerant finite state 
409
recognizer was employed (Oflazer, 1996). The 
finite state recognizer tolerated a maximum 
number of edit operations away from correctly 
spelled candidate words. This approach was 
initially developed to perform morphological 
analysis for Turkish and was extended to 
perform spelling correction.  The techniques 
used for Swedish and Turkish can potentially be 
applied to Arabic. Much work has been done on 
Arabic morphology and can be potentially 
extended for spelling correction. 
? Word Clustering:  Another approach tries to 
cluster different spellings of a word based on a 
weighted Levenshtein edit distance. The insight 
is that an important word, specially acronyms 
and named-entities, are likely to appear more 
than once in a passage. Taghva described an 
English recognizer that identifies acronyms and 
named-entities, clusters them, and then treats the 
words in each cluster as one word (Taghva, 
1994).  Applying this technique for Arabic 
requires accounting for morphology, because 
prefixes or suffixes might be affixed to instances 
of named entities. DeRoeck introduced a 
clustering technique tolerant of Arabic?s 
complex morphology (De Roeck and Al-Fares, 
2000). Perhaps the technique can be modified to 
make it tolerant of errors. 
? Using Grammar:  In this approach, a passage 
containing spelling errors is parsed based on a 
language specific grammar. In a system 
described by Agirre (1998), an English grammar 
was used to parse sentences with spelling 
mistakes.  Parsing such sentences gives clues to 
the expected part of speech of the word that 
should replace the misspelled word. Thus 
candidates produced by the spell checker can be 
filtered.  Applying this technique to Arabic might 
prove challenging because the work on Arabic 
parsing has been very limited (Moussa et al, 
2003). 
? Word N-Grams (Language Modeling):  A 
Word n-gram is a sequence of n consecutive 
words in text. The word n-gram technique is a 
flexible method that can be used to calculate the 
likelihood that a word sequence would appear 
(Tillenius, 1996). Using this method, the 
candidate correction of a misspelled word might 
be successfully picked. For example, in the 
sentence ?I bought a peece of land,? the possible 
corrections for the word peece might be ?piece? 
and ?peace?. However, using the n-gram method 
will likely indicate that the word trigram ?piece 
of land? is much more likely than the trigram 
?peace of land.? Thus the word ?piece? is a more 
likely correction than ?peace?. 
3 Error Correction Methodology 
This section describes the character level model-
ing, the language modeling, and shallow mor-
phological analysis. 
 
3.1 OCR Character Level Model 
A noisy channel model was used to learn how 
OCR corrupts single characters or character 
segments, producing a character level confusion 
model.   To train the model, 6,000 OCR cor-
rupted words were obtained from a modern print-
ing of a medieval religious Arabic book (called 
?The Provisions of the Return? or ?Provisions? 
for short by Ibn Al-Qayim).  The words were 
then manually corrected, and the corrupted and 
manually corrected versions were aligned. The 
Provisions book was scanned at 300x300 dots 
per inch (dpi), and Sakhr?s Automatic Reader 
was used to OCR the scanned pages.  From the 
6,000 words, 4,000 were used for training and 
the remaining words were set aside for later test-
ing.  The Word Error Rate (WER) for the 2,000 
testing words was 39%.  For all words (in train-
ing and testing), the different forms of alef 
(hamza, alef, alef maad, alef with hamza on top, 
hamza on wa, alef with hamza on the bottom, and 
hamza on ya) were normalized to alef, and ya 
and alef maqsoura were normalized to ya.  Sub-
sequently, the characters in the aligned words 
can aligned in two different ways, namely:  1:1 
(one-to-one) character alignment, where each 
character is mapped to no more than one charac-
ter (Church and Gale, 1991); or using m:n align-
ment, where a character segment of length m is 
aligned to a character segment of length n (Brill 
and Moore, 2000).  The second method is more 
general and potentially more accurate especially 
for Arabic where a character can be confused 
with as many as three or four characters.  The 
following example highlights the difference be-
tween the 1:1 and the m:n alignment approaches.  
Given the training pair (rnacle, made): 
 
1:1 alignment :    
 
r     n     a     c     l     e 
 
 
 
m    ?     a     d    ?     e 
410
m:n alignment: 
 
 
For alignment, Levenstein dynamic program-
ming minimum edit distance algorithm was used 
to produce 1:1 alignments.  The algorithm com-
putes the minimum number of edit operations 
required to transform one string into another.  
Given the output alignments of the algorithm, 
properly aligned characters (such as a  a and e 
 e) are used as anchors, ??s (null characters) 
are combined to misaligned adjacent characters 
producing m:n alignments, and ??s between cor-
rectly aligned characters are counted as deletions 
or insertions. 
To formalize the error model, given a clean word 
? = #C1..Ck.. Cl..Cn# and the resulting OCR de-
graded word ? = #D1..Dx.. Dy..Dm#, where Dx.. Dy 
resulted from Ck.. Cl, ? representing the null 
character, and # marking word boundaries, the 
probability estimates for the three edit operations 
for the models are: 
 
Psubstitution (Ck..Cl ?> Dx.. Dy) = 
)..(
)....(
lk
yxlk
CCcount
DDCCcount ?  
 
Pdeletion (Ck..Cl ?> ?) = 
)..(
)..(
lk
lk
CCcount
CCcount ??  
 
Pinsertion (? ?>  Dx.. Dy) = 
)(
)..(
Ccount
DDcount yx??  
 
When decoding a corrupted string ? composed of 
the characters D1..Dx.. Dy..Dm, the goal is to find 
a string ? composed of the characters C1..Ck.. 
Cl..Cn such that P(?|?)?P(?) is maximum.  P(?) is 
the prior probability of observing ? in text and 
P(?|?) is the probability of producing ? from ?.   
P(?) was computed from a web-mined collection 
of religious text by Ibn Taymiya, the main 
teacher of the medieval author of the ?Provi-
sions? book.  The collection contained approxi-
mately 16 million words, with 278,877 unique 
surface forms.  
P(?|?) is calculated using the trained model, as 
follows: 
?=
yx DDall
lkyx CCDDPP
..:
)..|..()|( ??
 
The segments Dx.. Dy are generated by finding all 
possible 2n-1 segmentations of the word ?.  For 
example, given ?macle? then all possible seg-
mentations are (m,a,c,l,e), (ma,c,l,e), (m,ac,l,e), 
(mac,l,e), (m,a,cl,e), (ma,cl,e), (m,acl,e), 
(macl,e), (m,a,c,le), (ma,c,le), (m,ac,le), (mac,le), 
(m,a,cle), (ma,cle), (m,acle), (macle). 
All segment sequences Ck.. Cl known to produce 
Dx.. Dy for each of the possible segmentations are 
produced.  If a sequence of C1.. Cn segments 
generates a valid word ? which exists in the web-
mined collection, then argmax? P(?|?)?P(?) is 
computed, otherwise the sequence is discarded.  
Possible corrections are subsequently ranked.  
For all the experiments reported in this paper, the 
top 10 corrections are generated.  Note that error 
correction reported in this paper does not assume 
that a word is correct because it exists in the 
web-mined collection and assumes that all words 
are possibly incorrect. 
The effect of two modifications to the m:n char-
acter model mentioned above were examined.   
The first modification involved making the char-
acter model account for the position of letters in 
a word.  The intuition for this model is that since 
Arabic letters change their shape based on their 
positions in words and would hence affect the 
letters with which they would be confused.  
Formally, given L denoting the positions of the 
letter at the boundaries of character segments, 
whether start, middle, end, or isolated, the char-
acter model would be: 
 
Psubstitution (Ck..Cl ?> Dx.. Dy | L) = 
)|..(
)|....(
LCCcount
LDDCCcount
lk
yxlk ?  
 
Pdeletion (Ck..Cl ?> ? | L) = 
)|..(
)|..(
LCCcount
LCCcount
lk
lk ??  
 
Pinsertion (? ?>  Dx.. Dy) = 
)|(
)|..(
LCcount
LDDcount yx??  
 
The second modification involved giving a small 
uniform probability to single character substiu-
tions that are unseen in the training data.  This 
was done in accordance to Lidstone?s law to 
smooth probabilities.  The probability was set to 
be 100 times smaller than the probability of the 
smallest seen single character substitution*.   
                                                 
* Other uniform probability estimates were examined for the 
training data and the one reported here seemed to work best 
r     n      a     c     l     e 
 
 
            
         m       a     d      e 
411
3.2 Language Modeling  
For language modeling, a trigram language 
model was trained on the same web-mined col-
lection that was mentioned in the previous sub-
section without any kind of morphological proc-
essing.  Like the text extracted from the ?Provi-
sions? book, alef and ya letter normalizations 
were performed.  The language model was built 
using SRILM toolkit with Good-Turing smooth-
ing and default backoff.   
Given a corrupted word sequence ? = {?1 .. ?i .. 
?n} and ? = {?1 .. ?i .. ?n}, where ?i ={?i0 .. ?im} 
are possible corrections of ?i (m = 10 for all the 
experiments reported in the paper), the aim was 
to find a sequence ? = {?1 .. ?i .. ?n}, where 
?i ? ?i, that maximizes: 
( )
4342144444 344444 21 odelCharacterM
iji
delLanguageMo
jijiijmjni
PP )|(,| ,2,1..1,..1 ????? ???
???
? ? ??==  
 
3.3 Language Modeling and Shallow Mor-
phological Analysis 
Two paths were pursued to explore the combined 
effect of language modeling and shallow mor-
phological analysis.   
In the first, a 6-gram language model was trained 
on the same web-mined collection after each of 
the words in the collection was segmented into 
its constituent prefix, stem, and suffix (in this 
order) using language model based stemmer (Lee 
et al, 2003).  For example, ? ?PQR?TU  ? wktAbhm? 
was replaced by ?w# ktAb +hm? where # and + 
were used to mark prefixes and suffixes respec-
tively and to distinguish them from stems.  Like 
before, alef and ya letter normalizations were 
performed and the language model was built us-
ing SRILM toolkit with the same parameters.   
Formally, the only difference between this 
model and the one before is that ?i ={?i0 .. ?im} 
are the {prefix, stem, suffix} tuples of the possi-
ble corrections of ?i (a tuple is treated as a block).  
Otherwise everything else is identical. 
In the second, a trigram language model was 
trained on the same collection after the language 
modeling based stemming was used on all the 
tokens in the collection (Lee et al, 2003).  The 
top n generated corrections were subsequently 
stemmed and the stems were reranked using the 
language model.  The top resulting stem was 
compared to the condition in which language 
modeling was used without morphological 
analysis (as in the previous subsection) and then 
the top resulting correction were stemmed.  This 
path was pursued to examine the effect of correc-
tion on applications where stems are more useful 
than words such as Arabic information retrieval 
(Darwish et al, 2005; Larkey et al, 2002).   
 
3.4 Testing the Models 
The 1:1 and m:n character mapping models were 
tested while enabling or disabling character posi-
tion training (CP), smoothing by the assignment 
of small probabilities to unseen single character 
substitutions (UP), language modeling (LM), and 
shallow morphological processing (SM) using 
the 6-gram model.  
As mentioned earlier, all models were tested us-
ing sentences containing 2,000 words in total.  
4 Experimental Results 
Table 1 reports on the percentage of words for 
which a proper correction was found in the top n 
generated corrections using different models.   
The percentage of words for which a proper cor-
rection exists in the top 10 proposed correction is 
the upper limit accuracy we can achieve given 
than we can rerank the correction using language 
modeling.  Table 2 reports the word error rate for 
the 1:1 and m:n models with and without CP, 
UP, LM, and SM.  Further, the before and after 
stemming error rates are reported for setups that 
use language modeling.  Table 3 reports on the 
stem error rate when using the stem trigram lan-
guage model. 
The best model was able to find the proper cor-
rection within the top 10 proposed correction for 
90% of the words.  The failure to find a proper 
correction within the proposed corrections was 
generally due to grossly misrecognized words 
and was rarely due to words that do not exist in 
web-mined collection.  Perhaps, more training 
examples for the character based models would 
improve correction. 
 
 
Corrections 1 2 3 4 5 10 
1:1 75.3 80.3 83.1 84.5 85 86.5 
1:1 + CP 76.9 82.1 83.5 83.2 85 86 
1:1 + UP 76 81 83.6 84.6 85.2 86.7 
m:n 78.3 83.5 85.4 86.7 87.1 88.5 
m:n + CP 79.9 83.9 84.0 85.5 85.9 86.8 
m:n + UP 78.4 83.7 85.6 84.1 87.0 90.0 
Table 1:  Percentage of words for which a proper cor-
rection was found in the top n generated corrections 
 
412
Model 1:1 m:n 
 Word Stem Word Stem 
No Correction 39.0% - 39.0% - 
Base Model 24.7% - 21.8% - 
+ CP 23.1% - 21.5% - 
+ UP 24% - 21.6% - 
+ LM 15.8% 14.6% 13.3% 12.1% 
+ LM + CP 16.5% 15.1% 15.5% 14.7% 
+ LM + UP 15.4% 14.3% 11.7% 10.8% 
+ SM + UP 27.8% 26.5% 24.5% 23.0% 
Table 2:  Word/stem error rate for correction with the 
different models 
 
Model 1:1 m:n 
Stem 3-gram 16.1% 12.9% 
Table 3:  Stem error rate for top correction using stem 
trigram language model 
 
The results indicate that the m:n character model 
is better than the 1:1 model in two ways.  The 
first is that the m:n model yielded a greater per-
centage of proper corrections in the top 10 gen-
erated corrections, and the second is that the 
scores of the top 10 corrections were better 
which led to better results compared to the 1:1 
model when used in combination with language 
modeling.  For the m:n model with language 
modeling, the language model properly picked 
the proper correction from the proposed correc-
tion 98% of the time (for the cases where a 
proper correction was within the proposed cor-
rections). 
Also the use of smoothing, UP, produced better 
corrections, while accounting for character posi-
tions had an adverse effect on correction.  This 
might be an indication that the character segment 
correction training data was sparse.  Using the 6-
gram language model on the segmented words 
had a severely negative impact on correction ac-
curacy.  Perhaps is due to insufficient training 
data for the model.  This situation lends itself to 
using a factored language model using the sur-
face form of words as well as other linguistic 
features of the word such as part of speech tags, 
prefixes, and suffixes. 
As for training a language model on words ver-
sus stems, the results suggest that word based 
correction is slightly better than stem based cor-
rection.  The authors? intuition is that this re-
sulted from having a sufficiently large corpus to 
train the language model and that this might have 
been reversed if the training corpus for the lan-
guage model was smaller.  Perhaps further inves-
tigation would prove or disprove the authors? 
intuition. 
5 Conclusion and Future Work 
The paper examined the use of single character 
and character segment models based correction 
of Arabic OCR text combined with language 
modeling and shallow morphological analysis.  
Further, character position and smoothing issues 
were also examined.  The results show the supe-
riority of the character segment based model 
compared to the single character based model.  
Further, the use of language modeling yielded 
improved error correction particularly for the 
character segment based model.  Accounting for 
character position and shallow morphological 
analysis had a negative impact on correction, 
while smoothing had a positive impact.  Lastly, 
given a large in-domain corpus to extract a cor-
rection dictionary and to train a language model 
is a sufficient strategy for correcting a morpho-
logically rich language such as Arabic with a 
70% reduction in word error rate.  
For future work, a factor language model 
might prove beneficial to incorporate morpho-
logical information and other factors such as part 
of speech tags while overcoming training data 
sparseness problems.  Also, determining the size 
of a sufficiently large corpus to generate a cor-
rection dictionary and to train a language model 
is desirable.  Finally, word prediction might 
prove useful for cases where OCR grossly mis-
recognized words. 
 
Reference 
Agirre, E., K. Gojenola, K. Sarasola, and A. Vouti-
lainen. Towards a Single Proposal in Spelling Cor-
rection. In COLING-ACL'98 (1998). 
Ahmed, M. A Large-Scale Computational Processor 
of Arabic Morphology and Applications. MSc. The-
sis, in Faculty of Engineering Cairo University: 
Cairo, Egypt. (2000). 
Baeza-Yates, R. and G. Navarro. A Faster Algorithm 
for Approximate String Matching. In Combinato-
rial Pattern Matching (CPM'96), Springer-Verlag 
LNCS (1996). 
Brill, E. and R. Moore.  An improved error model for 
noisy channel spelling correction.  In the proceed-
ings of the 38th Annual Meeting on Association for 
Computational Linguistics, pages 286 ? 293 (2000). 
Church, K. and W. Gale. ?Probability Scoringfor 
Spelling Correction.? Statistics and Computing, 1: 
93-103 (1991). 
Darwish, K. and D. Oard. Term Selection for Search-
ing Printed Arabic. In SIGIR-2002 (2002). 
413
Darwish, K., H. Hassan, and O. Emam. Examining 
the Effect of Improved Context Sensitive Morphol-
ogy on Arabic Information Retrieval. In ACL Work-
shop on Computation Approaches to Semitic Lan-
guages, Ann Arbor, (2005). 
De Roeck, A. and W. Al-Fares. A Morphologically 
Sensitive Clustering Algorithm for Identifying Ara-
bic Roots. In the 38th Annual Meeting of the ACL, 
Hong Kong, (2000). 
Domeij, R., J. Hollman, V. Kann.  Detection of spell-
ing errors in Swedish not using a word list en clair. 
Journal of Quantitative Linguistics (1994) 195-201. 
Harding, S., W. Croft, and C. Weir. Probabilistic Re-
trieval of OCR-degraded Text Using N-Grams. In 
European Conference on Digital Libraries (1997). 
Hong, T. Degraded Text Recognition Using Visual 
and Linguistic Context. Ph.D. Thesis, Computer 
Science Department, SUNY Buffalo: Buffalo (1995). 
Jurafsky, D. and J. Martin.  Speech and Language 
Processing.  Chapter 5: pages 141-163. Prentice 
Hall (2000). 
Larkey, L., L. Ballesteros, and M. Connell. Improving 
stemming for Arabic information retrieval: light 
stemming and cooccurrence analysis. In proceed-
ings of the 25th annual international ACM SIGIR 
conference, pages 275-282 (2002). 
Lee, Y., K. Papineni, S. Roukos, O. Emam, and H. 
Hassan. Language Model Based Arabic Word Seg-
mentation. In the Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, pages 399 - 406 (2003). 
Lu, Z., I. Bazzi, A. Kornai, J. Makhoul, P. Natarajan, 
and R. Schwartz. A Robust, Language-Independent 
OCR System. In the 27th AIPR Workshop: Ad-
vances in Computer Assisted Recognition, SPIE 
(1999). 
Moussa B., M. Maamouri, H. Jin, A. Bies, X. Ma. 
Arabic Treebank: Part 1 - 10Kword English Trans-
lation. Linguistic Data Consortium (2003). 
Oflazer, K. Error-Tolerant Finite State Recognition 
with Applications to Morphological Analysis and 
Spelling Correction. Computational Linguistics 
22(1), 73-90 (1996). 
Taghva, K., J. Borsack, and A. Condit. An Expert 
System for Automatically Correcting OCR Output. 
In SPIE - Document Recognition (1994). 
Tillenius, M., Efficient generation and ranking of 
spelling error corrections. NADA (1996). 
Tseng, Y. and D. Oard. Document Image Retrieval 
Techniques for Chinese. In Symposium on Docu-
ment Image Understanding Technology, Columbia, 
MD (2001).  
 
414
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Arabic Cross-Document Person Name Normalization 
Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan 
Human Language Technologies Group 
IBM Cairo Technology Development Center 
P.O. Box 166 El-Ahram, Giza, Egypt 
{wmagdy, darwishk, emam, hanyh}@eg.ibm.com 
 
Abstract 
This paper presents a machine learning 
approach based on an SVM classifier 
coupled with preprocessing rules for cross-
document named entity normalization.  The 
classifier uses lexical, orthographic, 
phonetic, and morphological features.  The 
process involves disambiguating different 
entities with shared name mentions and 
normalizing identical entities with different 
name mentions.  In evaluating the quality of 
the clusters, the reported approach achieves 
a cluster F-measure of 0.93.  The approach 
is significantly better than the two baseline 
approaches in which none of the entities are 
normalized or entities with exact name 
mentions are normalized.  The two baseline 
approaches achieve cluster F-measures of 
0.62 and 0.74 respectively.  The classifier 
properly normalizes the vast majority of 
entities that are misnormalized by the 
baseline system. 
1. Introduction: 
Much recent attention has focused on the 
extraction of salient information from unstructured 
text.  One of the enabling technologies for 
information extraction is Named Entity 
Recognition (NER), which is concerned with 
identifying the names of persons, organizations, 
locations, expressions of times, quantities, ... etc. 
(Chinchor, 1999; Maynard et al, 2001;  Sekine, 
2004; Joachims, 2002).  The NER task is 
challenging due to the ambiguity of natural 
language and to the lack of uniformity in writing 
styles and vocabulary used across documents 
(Solorio, 2004). 
Beyond NER, considerable work has focused 
on the tracking and normalization of entities that 
could be mentioned using different names (e.g. 
George Bush, Bush) or nominals (e.g. the 
president, Mr., the son) (Florian et al, 2004).  
Most of the named entity tracking work has 
focused on intra-document normalization with 
very limited work on cross-documents 
normalization. 
Recognizing and tracking entities of type 
?Person Name? are particularly important for 
information extraction.  Yet they pose interesting 
challenges that require special attention.  The 
problems can result from: 
1. A Person?s name having many variant spellings 
(especially when it is transliterated into a 
foreign language).  These variations are 
typically limited in the same document, but are 
very common across different documents from 
different sources (e.g. Mahmoud Abbas = 
Mahmod Abas, Mohamed El-Baradei = 
Muhammad AlBaradey ? etc). 
2. A person having more than one name (e.g. 
Mahmoud Abbas = Abu Mazen). 
3. Some names having very similar or identical 
names but refer to completely different persons 
(George H. W. Bush ? George W. Bush). 
4. Single token names (e.g. Bill Clinton = Clinton 
? Hillary Clinton). 
This paper will focus on Arabic cross-document 
normalization of named entities of type ?person 
name,? which would involve resolving the 
aforementioned problems.  As illustrated in Figure 
1, the task involves normalizing a set of person 
entities into a set of classes each of which is 
25
formed of at least one entity.  For N input entities, 
the output of normalization process will be M 
classes, where M ? N.  Each class would refer to 
only one person and each class would contain all 
entities referring to that person. 
For this work, intra-document normalization is 
assumed and an entity refers to a normalized set of 
name mentions and nominals referring to a single 
person in a single document.  Florian et al (2004) 
were kind enough to provide the authors access to 
an updated version of their state-of-the-art Named 
Entity Recognition and Tracking (NERT) system, 
which achieves an F-measure of 0.77 for NER, 
and an F-measure of 0.88 for intra-document 
normalization assuming perfect NER.  Although 
the NERT systems is efficient for relatively short 
documents, it is computational impractical for 
large documents, which precludes using the NERT 
system for cross-document normalization through 
combining the documents into one large 
document.  The main challenges of this work stem 
from large variations in the spelling of 
transliterated foreign names and the presence of 
many common Arabic names (such as 
Muhammad, Abdullah, Ahmed ?etc.), which 
increases the ambiguity in identifying the person 
referred to by the mentioned name.  Further, the 
NERT system output system contains many NER 
errors and intra-document normalization errors. 
In this paper, cross-document normalization 
system employs a two-step approach.  In the first 
step, preprocessing rules are used to remove errant 
named entities.  In the second step, a support 
vector machine (SVM) classifier is used to 
determine if two entities from two different 
documents need to be normalized.  The classifier 
is trained on lexical, orthographic, phonetic, and 
morphological features. 
The paper is organized as follows: Section 2 
provides a background on cross-document NE 
normalization; Section 3 describes the 
preprocessing steps and data used for training and 
testing;  Section 4 describes the normalization 
methodology; Section 5 describes the 
experimental setup;  Section 6 reports and 
discusses experimental results;  and Section 7 
concludes the paper and provides possible future 
directions. 
2. Background 
While considerable work has focused on named 
entity normalization within a single document, 
little work has focused on the challenges 
associated with resolving person name references 
across multiple documents.  Most of the work 
done in cross-document normalization focused on 
the problem of determining if two instances with 
the same name from different documents referring 
to the same person (Fleischman and Hovy, 2004).  
Fleischman and Hovy (2004) focused on 
distinguishing between individuals having 
identical names, but they did not extend 
normalization to different names referring to the 
same individual.  Their task is a subtask of what is 
examined in this paper.  They used a large number 
of features to accomplish their work, depending 
mostly on language specific dictionaries and 
wordnet.  Some these resources are not available 
for Arabic and many other languages.  Mann and 
Yarowsky (Mann and Yarowsky, 2003) examined 
the same problem but they treated it as a clustering 
task.  They focused on information extraction to 
build biographical profiles (date of birth, place of 
birth, etc.), and they wanted to disambiguate 
biographies belonging to different authors with 
identical names. 
Dozier and Zielund (Dozier and Zielund, 2004) 
reported on cross-document person name 
normalization in the legal domain.  They used a 
 
Figure 1 Normalization Model 
E1 
E3 
E7 E5 
E2 
E4 E6 
E8 
 
Normalization 
E1 
E4 E8 
E2 
E3 E7 
E5 
E6 
26
finite state machine that identifies paragraphs in a 
document containing the names of attorneys, 
judges, or experts and a semantic parser that 
extracts from the paragraphs template information 
about each named individual.  They relied on 
reliable biographies for each individual.  A 
biography would typically contain a person?s first 
name, middle name, last name, firm, city, state, 
court, and other information.  They used a 
Bayesian network to match the name mentions to 
the biographical records. 
Bhattacharya and Getoor (Bhattacharya and 
Getoor, 2006) introduced a collective decision 
algorithm for author name entity resolution, where 
decisions are not considered on an independent 
pairwise basis.  They focused on using relational 
links among the references and co-author 
relationships to infer collaboration groups, which 
would disambiguate entity names.  Such explicit 
links between co-authors can be extracted directly.  
However, implicit links can be useful when 
looking at completely unstructured text.  Other 
work has extended beyond entities of type ?person 
name? to include the normalization of location 
names (Li et al, 2002) and organizations (Ji and 
Grishman. 2004). 
3.  Preprocessing  and the Data Set 
For this work, a set of 7,184 person name entities 
was constructed.  Building new training and test 
sets is warranted, because the task at hand is 
sufficiently different from previously reported 
tasks in the literature.  The entities were 
recognized from 2,931 topically related documents 
(relating to the situation in the Gaza and Lebanon 
during July of 2006) from different Arabic news 
sources (obtained from searching the Arabic 
version of news.google.com).  The entities were 
recognized and normalized (within document) 
using the NERT system of Florian et al(2004).  
As shown in Figure 2, each entity is composed of 
a set of name mentions (one or more) and a set of 
nominal mentions (zero or more).  
The NERT system achieves an F-score of 0.77 
with precision of 0.82 and recall of 0.73 for person 
name mention and nominal recognition and an F-
score of 0.88 for tracking (assuming 100% 
recognition accuracy).  The produced entities may 
suffer from the following: 
1. Errant name mentions: Two name mentions 
referring to two different entities are 
concatenated into an errant name mention (e.g. 
?Bush Blair?, ?Ahmadinejad Bush?).  These 
types of errors stem from phrases such as ?The 
meeting of Bush Blair? and generally due to 
lack of sufficient punctuation marks. 
2. NE misrecognitions: Regular words are 
recognized as person name mentions and are 
embedded into person entities (e.g. Bush = 
George Bush = said). 
3. Errant entity tracking: name mentions of 
different entities are recognized as different 
mentions of the same entity (e.g. Bush = 
Clinton = Ahmadinejad). 
4. Lack of nominal mentions: Many entities do 
not contain any nominal mentions, which 
increases the entity ambiguity (especially 
when there is only one name mention 
composed of a single token).  
To overcome these problems, entities were 
preprocessed as follows: 
1. Errant name mentions such as ?Bush Blair? 
were automatically removed.  In this step, a 
dictionary of person name mentions was built 
from the 2,931 documents collection from 
which the entities were recognized and 
normalized along with the frequency of 
appearance in the collection.  For each entity, 
all its name mentions are checked in the 
dictionary and their frequencies are compared 
to each other.  Any name mention with a 
frequency less than 1/30 of the frequency of 
the name mention with the highest frequency 
is automatically removed (1/30 was picked 
based on manual examination of the training 
set).   Figure 2 Entity Description  
27
2. Name mentions formed of a single token 
consisting of less than 3 characters are 
removed.  Such names are almost always 
misrecognized name entities. 
3. Name entities with 10 or more different name 
mentions are automatically removed.  The 
NERT system often produces entities that 
include many different name mentions 
referring to different persons as one.  Such 
entities are errant because they over normalize 
name mentions.  Persons are referred to using 
a limited number of name mentions. 
4. Nominal mentions are stemmed using a 
context sensitive Arabic stemmer (Lee et al 
2003) to overcome the morphological 
complexity of Arabic.  For example, ?JKL?? = 
?president?, ? O?JKLQ ? = ?the president?, 
? O??JKLQ ? = ?and the president?, ? SKL?TU ? = ?its 
presidents? ? etc are stemmed to ?JKL?? = 
?president?.  
 
Cross-document entities are compared in a 
pairwise manner and binary decision is taken on 
whether they are the same.  Therefore, the 
available 7,184 entities lead to nearly 26 million 
pairwise comparisons (For N entities, the number 
of pair wise comparisons = 2
)1( ?NN ). 
Entity pairs were chosen to be included in the 
training set if they match any of the following 
criteria: 
1. Both entities have one shared name mention. 
2. Both entities have shared nominal mentions. 
3. A name mention in one of the entities is a 
substring of a name mention in the other 
entity. 
4. Both entities have nearly identical name 
mentions (small edit distance between both 
mentions). 
The resulting set was composed of 19,825 
pairs, which were manually judged to determine if 
they should be normalized or not.  These criteria 
skew the selection of pairs towards more 
ambiguous cases, which would be better 
candidates to train the intended SVM classifier, 
where the items near the boundary dividing the 
hyperplane are the most important.  For the 
training set, 18,503 pairs were normalized, and 
1,322 pairs were judged as different.  
Unfortunately, the training set selection criteria 
skewed the distribution of training examples 
heavily in favor of positive examples.  It would 
interesting to examine other training sets where 
the distribution of positives and negatives is 
balanced or skewed in favor of negatives. 
The test set was composed of 470 entities that 
were manually normalized into 253 classes, of 
which 304 entities were normalized to 87 classes 
and 166 entities remained unnormalized (forming 
single-entity classes).  Using 470 entities leads to 
110,215 pairwise comparisons.  The test set, which 
was distinct from the training set, was chosen 
using the same criteria as the training set.  Further, 
all duplicate (identical) entities were removed 
from the test set.  The selection criteria insure that 
the test set is skewed more towards ambiguous 
cases.  Randomly choosing entities would have 
made the normalization too easy.   
4. Normalization Methodology 
SVMLight, an SVM classifier (Joachims, 2002), 
was used for classification with a linear kernel and 
default parameters.  The following training 
features were employed: 
1. The percentage of shared name mentions 
between two entities calculated as: 
Name Commonality = 
? ??>< ???
?
???
?
namescommon j
i
j
i
f
f
f
f
2
2
1
1 ,min  
 where f1i is the frequency of the shared name 
mention in first entity, and f2i is the frequency 
of the shared name mention in the second 
entity.  ? f1i is the number of name mentions 
appearing in the entity. 
2. The maximum number of tokens in the shared 
name mentions, i.e. if there exists more than 
one shared name mention then this feature is 
the number of tokens in the longest shared 
name mention. 
3. The percentage of shared nominal mentions 
between two entities, and it is calculated as the 
name commonality but for nominal mentions.  
4. The smallest minimum edit distance 
(Levenshtein distance with uniform weights) 
between any two name mentions in both 
entities (Cohen et al, 2003) and this feature is 
only enabled when name commonality 
between both entities equals to zero. 
28
5. Phonetic edit distance, which is similar to edit 
distance except that phonetically similar 
characters, namely {(? ? t, ? ? T), (? ? k, ? ? 
q),(? ? d, ? ? D),(? ? v, ? ? s, ? ? S), (? ? *, 
? ? z, ? ? Z),(? ? j, ? ? g),(i?  ? p, k? ? h),(? ? <, 
n ? |, ? ,< ? ? ? A)1}, are normalized, vowels are 
removed, and spaces between tokens are 
removed. 
6. The number of tokens in the pair of name 
mentions that lead to the minimum edit 
distance. 
Some of the features might seem duplicative.  
However, the edit distance and phonetic edit 
distance are often necessary when names are 
transliterated into Arabic and hence may have 
different spellings and consequently no shared 
name mentions.  Conversely, given a shared name 
mention between a pair of entities will lead to zero 
edit distance, but the name commonality may also 
be very low indicating two different persons may 
have a shared name mention.  For example 
?Abdullah the second? and ?Abdullah bin 
Hussein? have the shared name mention 
?Abdullah? that leads to zero edit distance, but 
they are in fact two different persons.  In this case, 
the name commonality feature can be indicative of 
the difference.  Further, nominals are important in 
differentiating between identical name mentions 
that in fact refer to different persons (Fleischman 
and Hovy, 2004).  The number of tokens feature 
indicates the importance of the presence of 
similarity between two name mentions, as the 
similarity between name mentions formed of one 
token cannot be indicative for similarity when the 
number of tokens is more than one. 
Further, it is assumed that entities are transitive 
and are not available all at once, but rather the 
system has to normalize entities incrementally as 
they appear.  Therefore, for a given set of entity 
pairs, if the classifier deems that Entityi = Entityj 
and Entityj = Entityk, then Entityi is set to equal 
Entityk even if the classifier indicates that Entityi ? 
Entityk, and all entities (i, j, and k) are merged into 
one class.   
                                                 
1 Buckwalter transliteration scheme is used throughout 
the paper 
5. Experimental Setup 
Two baselines were established for the 
normalization process.  In the first, no entities are 
normalized, which produces single entity classes 
(?no normalization? condition).  In the second, any 
two entities having two identical name mentions in 
common are normalized (?surface normalization? 
condition).  For the rest of the experiments, focus 
was given to two main issues: 
1. Determining the effect of the different features 
used for classification. 
2. Determining the effect of varying the number 
of training examples.  
To determine the effect of different features, 
multiple classifiers were trained using different 
features, namely: 
? All features: all the features mentioned above 
are used,  
? Edit distance removed:  edit distance features 
(features 4, 5, and 6) are removed,  
? Number of tokens per name mention removed:  
the number of shared tokens and the number 
of tokens leading to the least edit distance 
(features 2 and 6) are removed.   
To determine the effect of training examples, 
the classifier was trained using all features but 
with a varying number of training example pairs, 
namely all 19,825 pairs, a set of randomly picked 
5,000 pairs, and a set of randomly picked 2,000 
pairs.   
For evaluation, 470 entities in test set were 
normalized into set of classes with different 
thresholds for the SVM classifier.  The quality of 
the clusters was evaluated using purity, entropy, 
and Cluster F-measure (CF-measure) in the 
manner suggested by Rosell et al (2004).  For the 
cluster quality measures, given cluster i (formed 
using automatic normalization) and each cluster j 
(reference normalization formed manually), cluster 
precision (p) and recall (r) are computed as 
follows: 
i
ij
ij n
n
p = , and 
j
ij
ij n
n
r = , where ni number of 
entities in cluster i, nj number of entities in cluster 
j, and nij number of shared entities between cluster 
i and j.  
The CF-measure for an automatic cluster i 
against a manually formed reference cluster j is:  
29
ijij
ijij
ij pr
pr
CF +
??= 2 , and the CF-measure for a 
reference cluster j is: 
}{max ijij CFCF = .  
The final CF-measure is computed over all the 
reference clusters as follows: ?= j jij CFn
n
CF . 
Purity of (?i) of an automatically produced 
cluster i is the maximum cluster precision obtained 
when comparing it with all the reference clusters 
as follows: }{max ijji p=? , and the weighted 
average purity over all clusters is: 
?= i i
i
ij
n
n ?? , where n is the total number of 
entities in the set to be normalized (470 in this 
case). 
As for entropy of a cluster, it is calculated as:  
??= j ijiji ppE log , and the average entropy 
as: 
?= i i
i
i E
n
nE . 
The CF-measure captures both precision and 
recall while purity and entropy are precision 
oriented measures (Rosell et al, 2004). 
6. Results and Discussion 
Figure 3 shows the purity and CF-measure for the 
two baseline conditions (no normalization, and 
surface normalization) and for the normalization 
system with different SVM thresholds.  Since 
purity is a precision measure, purity is 100% when 
no normalization is done.  The CF-measure is 62% 
and 74% for baseline runs with no normalization 
and surface normalization respectively.  As can be 
seen from the results, the baseline run based on 
exact matching of name mentions in entities 
achieves low CF-measure and low purity.  Low 
CF-measure values stem from the inability to 
match identical entities with different name 
mentions, and the low purity value stems from not 
disambiguating different entities with shared name 
mentions.  Some notable examples where the 
surface normalization baseline failed include:  
1. The normalization of the different entities 
referring to the Israeli soldier who is 
imprisoned in Gaza with different Arabic 
spellings for his name, namely ?tKuv ?Twux? 
(jlEAd $lyT), ?tKOTv ?Twux? (jlEAd $AlyT), 
?zKuv ?|}~O?? (the soldier $lyt), and so forth.  
2. The separation between ??T?O?? ? |?? ?u?O?? 
(King Abdullah the Second) and ? ?? ?? |?? ?u?O?
???wO? |??? (King Abdullah ibn Abdul-Aziz) 
that have a shared name mention ???|?? ?u?O?? 
(King Abdullah). 
3. The normalization of the different entities 
representing the president of Palestinian 
Authority with different name mentions, 
namely ???T? ???? (Abu Mazen) and ? ?????
?T??? (Mahmoud Abbas).   
 
The proposed normalization technique 
properly normalized the aforementioned examples.  
Given different SVM thresholds, Figure 3 shows 
that the purity of resultant classes increases as the 
SVM threshold increases since the number of 
normalized entities decreases as the threshold 
increases.  The best CF-measure of 93.1% is 
obtained at a threshold of 1.4 and as show in Table 
1 the corresponding purity and entropy are 97.2% 
and 0.056 respectively.  The results confirm the 
success of the approach. 
Table 1 highlights the effect of removing 
different training feature and the highest CF-
measures (at different SVM thresholds) as a result.  
The table shows that using all 6 features produced 
the best results and the removal of the shared 
names and tokens (features 2 and 6) had the most 
adverse effect on normalization effectiveness.  The 
adverse effect is reasonable especially given that 
some single token names such as ?Muhammad? 
and ?Abdullah? are very common and matching 
one of these names across entities is an insufficient 
indicator that they are the same.  Meanwhile, the 
exclusion of edit distance features (features 4, 5, 
and 6) had a lesser but significant adverse impact 
on normalization effectiveness.  Table 1 reports 
the best results obtained using different thresholds.  
Perhaps, a separate development set should be 
used for ascertaining the best threshold. 
Table 2 shows that decreasing the number of 
training examples (all six features are used) has a 
noticeable but less pronounced effect on 
normalization effectiveness compared to removing 
training features. 
30
 
Table 1 Quality of clusters as measured by purity (higher values are better), entropy (lower values are 
better), and CF-measure (higher values are better) for different feature sets.  Values are shown for max 
CF-measure.  Thresholds were tuned for max CF-measure for each feature configuration separately 
Training Data Purity Maximum CF-Measure Entropy Threshold 
No Normalization 100.0% 62.6% 0.000 - 
Baseline 83.4% 74.7% 0.151 - 
All Features 97.2% 93.1% 0.056 1.4 
Edit Distance removed 99.4% 85.5% 0.010 1.0 
# of tokens/name removed 96.6% 77.8% 0.071 1.5 
 
Normalization Evaluation
60%
65%
70%
75%
80%
85%
90%
95%
100%
No
No
rm
al
iza
tio
n
Ba
se
lin
e
1.
0
1.
1
1.
2
1.
3
1.
4
1.
5
1.
6
1.
7
1.
8
1.
9
2.
0
SVM Threshold
Purity
CF-Measure
 
Figure 3 Purity and cluster F-measure versus SVM Threshold 
 
Table 2 Effect of number of training examples on normalization effectiveness 
Training Data Purity Maximum CF-Measure Entropy Threshold 
20k training pairs 97.2% 93.1% 0.056 1.4 
5k training pairs 97.4% 90.5% 0.053 1.5 
2k training pairs 98.5% 90.3% 0.031 1.6 
 
7. Conclusion: 
This paper presented a two-step approach to cross-
document named entity normalization.  In the first 
step, preprocessing rules are used to remove errant 
named entities.  In the second step, a machine 
learning approach based on an SVM classifier to 
disambiguate different entities with matching 
name mentions and to normalize identical entities 
with different name mentions.  The classifier was 
trained on features that capture name mentions and 
nominals overlap between entities, edit distance, 
and phonetic similarity.  In evaluating the quality 
of the clusters, the reported approach achieved a 
cluster F-measure of 0.93.  The approach 
outperformed that two baseline approaches in 
which no normalization was done or normalization 
was done when two entities had matching name 
31
mentions.  The two approaches achieved cluster F-
measures of 0.62 and 0.74 respectively. 
For future work, implicit links between entities 
in the text can serve as the relational links that 
would enable the use of entity attributes in 
conjunction with relationships between entities.  
An important problem that has not been 
sufficiently explored is cross-lingual cross-
document normalization.  This problem would 
pose unique and interesting challenges.  The 
described approach could be generalized to 
perform normalization of entities of different types 
across multilingual documents.  Also, the 
normalization problem was treated as a 
classification problem.  Examining the problem as 
a clustering (or alternatively an incremental 
clustering) problem might prove useful.  Lastly, 
the effect of cross-document normalization should 
be examined on applications such as information 
extraction, information retrieval, and relationship 
and social network visualization.   
References: 
Bhattacharya I. and Getoor L. ?A Latent Dirichlet 
Allocation Model for Entity Resolution.? 6th SIAM 
Conference on Data Mining (SDM), Bethesda, USA, 
April 2006. 
Chinchor N., Brown E., Ferro L., and Robinson P.  
?Named Entity Recognition Task Definition.? 
MITRE, 1999. 
Cohen W., Ravikumar P., and Fienberg S. E. ?A 
Comparison of String Distance Metrics for Name-
Matching Tasks.?  In Proceedings of the 
International Joint Conference on Artificial 
Intelligence, 2003. 
Dozier C. and Zielund T. ?Cross-document Co-
Reference Resolution Applications for People in the 
Legal Domain.? In 42nd Annual Meeting of the 
Association for Computational Linguistics, 
Reference Resolution Workshop, Barcelona, Spain. 
July 2004. 
Fleischman M. B. and Hovy E. ?Multi-Document 
Person Name Resolution.?  In 42nd Annual Meeting 
of the Association for Computational Linguistics, 
Reference Resolution Workshop, Barcelona, Spain. 
July 2004. 
Ji H. and Grishman R. ?Applying Coreference to 
Improve Name Recognition?. In 42nd Annual 
Meeting of the Association for Computational 
Linguistics, Reference Resolution Workshop, 
Barcelona, Spain. July (2004). 
Ji H. and Grishman R. "Improving Name Tagging by 
Reference Resolution and Relation Detection." ACL 
2005 
Joachims T. ?Learning to Classify Text Using Support 
Vector Machines.? Ph.D. Dissertation, Kluwer, 
(2002). 
Joachims T. ?Optimizing Search Engines Using Click-
through Data.?  Proceedings of the ACM Conference 
on Knowledge Discovery and Data Mining (KDD), 
(2002).  
Lee Y. S., Papineni K., Roukos S., Emam O., Hassan 
H. ?Language Model Based Arabic Word 
Segmentation.?  In ACL 2003, pp. 399-406, (2003). 
Li H., Srihari R. K., Niu C., and Li W. ?Location 
Normalization for Information Extraction.?  
Proceedings of the 19th international conference on 
Computational linguistics, pp. 1-7, 2002 
Li H., Srihari R. K., Niu C., and Li W. ?Location 
Normalization for Information Extraction.?  
Proceedings of the sixth conference on applied 
natural language processing, 2000. pp. 247 ? 254. 
Mann G. S. and Yarowsky D. ?Unsupervised Personal 
Name Disambiguation.? Proceedings of the seventh 
conference on Natural language learning at HLT-
NAACL 2003.  pp. 33-40. 
Maynard D., Tablan V., Ursu C., Cunningham H., and 
Wilks Y. ?Named Entity Recognition from Diverse 
Text Types.? Recent Advances in Natural Language 
Processing Conference, (2001). 
Palmer D. D. and Day D. S. ?A statistical Profile of the 
Named Entity Task?.  Proceedings of the fifth 
conference on Applied natural language processing, 
pp. 190-193, (1997). 
R. Florian R., Hassan H., Ittycheriah A., Jing H., 
Kambhatla N., Luo X., Nicolov N., and Roukos S. 
?A Statistical Model for Multilingual Entity 
Detection and Tracking.?  In HLT-NAACL, 2004. 
Rosell M., Kann V., and Litton J. E.  ?Comparing 
Comparisons: Document Clustering Evaluation 
Using Two Manual Classifications.?  In ICON 2004 
Sekine S. ?Named Entity: History and Future?.  Project 
notes, New York University, (2004). 
Solorio T. ?Improvement of Named Entity Tagging by 
Machine Learning.?  Ph.D. thesis, National Institute 
of Astrophysics, Optics and Electronics, Puebla, 
Mexico, September 2005. 
 
32
