Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 803?812,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Relational Model of Semantic Similarity between Words using
Automatically Extracted Lexical Pattern Clusters from the Web
Danushka Bollegala
?
danushka@mi.ci.i.
u-tokyo.ac.jp
Yutaka Matsuo
matsuo@biz-model.
t.u-tokyo.ac.jp
The University of Tokyo
7-3-1, Hongo, Tokyo, 113-8656, Japan
Mitsuru Ishizuka
ishizuka@i.
u-tokyo.ac.jp
Abstract
Semantic similarity is a central concept
that extends across numerous fields such
as artificial intelligence, natural language
processing, cognitive science and psychol-
ogy. Accurate measurement of semantic
similarity between words is essential for
various tasks such as, document cluster-
ing, information retrieval, and synonym
extraction. We propose a novel model
of semantic similarity using the semantic
relations that exist among words. Given
two words, first, we represent the seman-
tic relations that hold between those words
using automatically extracted lexical pat-
tern clusters. Next, the semantic similar-
ity between the two words is computed
using a Mahalanobis distance measure.
We compare the proposed similarity mea-
sure against previously proposed seman-
tic similarity measures on Miller-Charles
benchmark dataset and WordSimilarity-
353 collection. The proposed method out-
performs all existing web-based seman-
tic similarity measures, achieving a Pear-
son correlation coefficient of 0.867 on the
Millet-Charles dataset.
1 Introduction
Similarity is a fundamental concept in theories
of knowledge and behavior. Psychological ex-
periments have shown that similarity acts as an
organizing principle by which individuals clas-
sify objects, and make generalizations (Goldstone,
1994). For example, a biologist would classify
a newly found animal specimen based upon the
properties that it shares with existing categories
of animals. We can then make additional infer-
ences on the new specimen using the properties
?
Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
known for the existing category. As the simi-
larity between two objects X and Y increases,
so does the probability of correctly inferring that
Y has the property T upon knowing that X has
T (Tenenbaum, 1999). Accurate measurement of
semantic similarity between lexical units such as
words or phrases is important for numerous tasks
in natural language processing such as word sense
disambiguation (Resnik, 1995), synonym extrac-
tion (Lin, 1998a), and automatic thesauri gener-
ation (Curran, 2002). In information retrieval,
similar or related words are used to expand user
queries to improve recall (Sahami and Heilman,
2006).
Semantic similarity is a context dependent and
dynamic phenomenon. New words are constantly
being created and existing words are assigned with
new senses on the Web. To decide whether two
words are semantically similar, it is important to
know the semantic relations that hold between the
words. For example, the words horse and cow can
be considered semantically similar because both
horses and cows are useful animals in agriculture.
Similarly, a horse and a car can be considered se-
mantically similar because cars, and historically
horses, are used for transportation. Semantic re-
lations such as X and Y are used in agriculture,
or X and Y are used for transportation, exist be-
tween two words X and Y in these examples. We
use bold-italics, X, to denote the slot of a word X
in a lexical pattern.
We propose a relational model to compute the
semantic similarity between two words. First, us-
ing snippets retrieved from a web search engine,
we present an automatic lexical pattern extraction
algorithm to represent the semantic relations that
exist between two words. For example, given two
words ostrich and bird, we extract X is a Y, X is
a large Y, and X is a flightless Y from the Web.
Using a set of semantically related words as train-
ing data, we evaluate the confidence of a lexical
803
pattern as an indicator of semantic similarity. For
example, the pattern X is a Y is a better indica-
tor of semantic similarity between X and Y than
the pattern X and Y. Consequently, we would like
to emphasize the former pattern by assigning it a
higher confidence score. It is noteworthy that all
lexical patterns are not independent ? multiple lex-
ical patterns can express the same semantic rela-
tion. For example, the pattern X is a large Y sub-
sumes the more general pattern X is a Y and they
both indicate a hypernymic relationship between
X and Y. By clustering the semantically related
patterns into groups, we can both overcome the
data sparseness problem, and reduce the number
of parameters during training. To identify seman-
tically related patterns, we use a sequential pattern
clustering algorithm that is based on the distribu-
tional hypothesis (Harris, 1954). We represent two
words by a feature vector defined over the clus-
ters of patterns. Finally, the semantic similarity
is computed as the Mahalanobis distance between
points corresponding to the feature vectors. By
using Mahalanobis distance instead of Euclidean
distance, we can account for the inter-dependence
between semantic relations.
2 Related Work
Geometric models, such as multi-dimensional
scaling has been used in psychological ex-
periments analyzing the properties of similar-
ity (Krumhansl, 1978). These models represent
objects as points in some coordinate space such
that the observed dissimilarities between objects
correspond to the metric distances between the re-
spective points. Geometric models assume that
objects can be adequately represented as points in
some coordinate space and that dissimilarity be-
haves like a metric distance function satisfying
minimality, symmetry, and triangle inequality as-
sumptions. However, both dimensional and metric
assumptions are open to question.
Tversky (1977) proposed the contrast model of
similarity to overcome the problems in geometric
models. The contrast model relies on featural rep-
resentation of objects, and it is used to compute the
similarity between the representations of two ob-
jects. Similarity is defined as an increasing func-
tion of common features (i.e. features in common
to the two objects), and as a decreasing function of
distinctive features (i.e. features that apply to one
object but not the other). The attributes of objects
are primal to contrast model and it does not ex-
plicitly incorporate the relations between objects
when measuring similarity.
Hahn et al (2003) define similarity between
two representations as the complexity required to
transform one representation into the other. Their
model of similarity is based on the Representa-
tional Distortion theory, which aims to provide
a theoretical framework of similarity judgments.
Their experiments using pattern sequences and ge-
ometric shapes show an inverse correlation be-
tween the number of transformations required to
convert one pattern (or shape) to another, and the
perceived similarity ratings by human subjects.
How to represent an object, which transformations
are allowed on a representation, and how to mea-
sure the complexity of a transformation, are all
important decisions in the transformational model
of similarity. Although distance measures such as
edit distance have been used to find approximate
matches in a dictionary, it is not obvious how to
compute semantic similarity between words using
representational distortion theory.
Given a taxonomy of concepts, a straightfor-
ward method to calculate similarity between two
words (or concepts) is to find the length of the
shortest path connecting the two words in the tax-
onomy (Rada et al, 1989). If a word is polyse-
mous (i.e. has more than one sense) then multi-
ple paths might exist between the two words. In
such cases, only the shortest path between any two
senses of the words is considered for calculating
similarity. A problem that is frequently acknowl-
edged with this approach is that it relies on the
notion that all links in the taxonomy represent a
uniform distance. As a solution to this problem,
Schickel-Zuber and Faltings (2007) propose ontol-
ogy structure based similarity (OSS) between two
concepts in an ontology, which is an asymmetric
distance function.
Resnik (1995) proposed a similarity measure
using information content. He defined the similar-
ity between two concepts C
1
and C
2
in the taxon-
omy as the maximum of the information content of
all concepts C that subsume both C
1
and C
2
. Then
the similarity between two words is defined as the
maximum of the similarity between any concepts
that the words belong to. He used WordNet as the
taxonomy; information content is calculated using
the Brown corpus.
Li et al, (2003) combined structural seman-
804
tic information from a lexical taxonomy, and in-
formation content from a corpus, in a nonlinear
model. They proposed a similarity measure that
uses shortest path length, depth and local density
in a taxonomy. Their experiments reported a Pear-
son correlation coefficient of 0.8914 on the Miller-
Charles benchmark dataset (Miller and Charles,
1998). Lin (1998b) defined the similarity between
two concepts as the information that is in common
to both concepts and the information contained in
each individual concept.
Cilibrasi and Vitanyi (2007) proposed a distance
metric between words using page-counts retrieved
from a web search engine. The proposed metric is
named Normalized Google Distance (NGD) and is
defined as the normalized information distance (Li
et al, 2004) between two strings. They evaluate
NGD in a word classification task. Unfortunately
NGD only uses page-counts of words and ignores
the context in which the words appear. Therefore,
it produces inaccurate similarity scores when one
or both words between which similarity is com-
puted are polysemous.
Sahami and Heilman (2006) measured semantic
similarity between two queries using snippets re-
turned for those queries by a search engine. For
each query, they collect snippets from a search
engine and represent each snippet as a TF-IDF-
weighted term vector. Each vector is L
2
normal-
ized and the centroid of the set of vectors is com-
puted. Semantic similarity between two queries
is then defined as the inner product between the
corresponding centroid vectors. They did not
compare their similarity measure with taxonomy-
based similarity measures.
Chen et al, (2006) propose a web-based double-
checking model to compute the semantic similar-
ity between words. For two words X and Y , they
collect snippets for each word from a web search
engine. Then they count the number of occur-
rences of X in the snippets for Y , and Y in the
snippets for X . The two values are combined non-
linearly to compute the similarity between X and
Y . This method heavily depends on the search en-
gine?s ranking algorithm. Although two words X
and Y may be very similar, there is no reason to
believe that one can find Y in the snippets for X ,
or vice versa. This observation is confirmed by the
experimental results in their paper which reports 0
similarity scores for many pairs of words in the
Miller-Charles dataset.
In our previous work (Bollegala et al, 2007),
we proposed a semantic similarity measure using
page counts and snippets retrieved from a Web
search engine. To compute the similarity between
two words X and Y , we queried a web search en-
gine using the query X AND Y and extract lex-
ical patterns that combine X and Y from snip-
pets. A feature vector is formed using frequen-
cies of 200 lexical patterns in snippets and four
co-occurrence measures: Dice coefficient, overlap
coefficient, Jaccard coefficient and pointwise mu-
tual information. We trained a two-class support
vector machine using automatically selected syn-
onymous and non-synonymous word pairs from
WordNet. This method reports a Pearson corre-
lation coefficient of 0.837 with Miller-Charles rat-
ings. However, it does not consider the relatedness
between patterns.
Gabrilovich and Markovitch (2007) represent
words using weighted vectors of Wikipedia-based
concepts, and define the similarity between words
as the cosine of the angle between the correspond-
ing vectors. Their method can be used to com-
pute similarity between words as well as between
texts. Although Wikipedia is growing in popular-
ity, not all concepts found on the Web have arti-
cles in Wikipedia. Specially, novel or not very
popular concepts are not adequately covered by
Wikipedia. Moreover, their method requires the
concepts to be independent. For non-independent,
hierarchical taxonomies such as open directory
project (ODP)
1
, their method produces suboptimal
results.
3 Relational Model of Similarity
We propose a model to compute the semantic sim-
ilarity between two words a and b using the set
of semantic relations R(a, b) that hold between a
and b. We call the proposed model the relational
model of semantic similarity and it is defined by
the following equation,
sim(a, b) = ?(R(a, b)). (1)
Here, sim(a, b) is the semantic similarity between
the two words a and b, and ? is a weighting
function defined over the set of semantic relations
R(a, b). Given that a particular set of semantic
relations are known to hold between two words,
the function ? expresses our confidence on those
words being semantically similar.
1
http://www.dmoz.org
805
A semantic relation can be expressed in a num-
ber of ways. For example, given a taxonomy of
words such as the WordNet, semantic relations
(i.e. hypernymy, meronymy, synonymy etc.) be-
tween words can be directly looked up in the tax-
onomy. Alternatively, the labels of the edges in
the path connecting two words can be used as
semantic relations. However, in this paper we
do not assume the availability of manually cre-
ated resources such as dictionaries or taxonomies.
We represent semantic relations using automati-
cally extracted lexical patterns. Lexical patterns
have been successfully used to represent various
semantic relations between words such as hyper-
nymy (Hearst, 1992), and meronymy (Berland and
Charniak, 1999). Following these previous ap-
proaches, we represent R(a, b) as a set of lexical
patterns. Moreover, we denote the frequency of a
lexical pattern r for a word pair (a, b) by f(r, a, b).
So far we have not defined the functional form
of ?. A straightforward approach is to use a lin-
early weighted combination of relations as shown
below,
?(R(a, b)) =
?
r
i
?R(a,b)
w
i
? f(r
i
, a, b). (2)
Here, w
i
is the weight associated with the lexical
pattern r
i
and can be determined using training
data. However, this formulation has two funda-
mental drawbacks. First, the number of weight
parameters w
i
is equal to the number of lexical
patterns. Typically two words can co-occur in nu-
merous patterns. Consequently, we end up with a
large number of parameters in the model. Com-
plex models with a large number of parameters
are difficult to train because they tend to overfit to
the training data. Second, the linear combination
given in Equation 2 assumes the lexical patterns
to be mutually independent. However, in practice
this is not true. For example, both patterns X is a
Y and Y such as X indicate a hypernymic relation
between X and Y.
To overcome the above mentioned limitations,
we first cluster the lexical patterns to identify the
semantically related patterns. Our clustering algo-
rithm is detailed in section 3.2. Next, we define ?
using the formed clusters as follows,
?(R(a, b)) = x
T
ab
??. (3)
Here, x
ab
is a feature vector representing the
words a and b. Each formed cluster contributes
a feature in vector x
ab
as described later in Sec-
tion 5. The vector ? is a prototypical vector rep-
resenting synonymous word pairs. We compute
? as the centroid of feature vectors representing
synonymous word pairs. ? is the inter-cluster cor-
relation matrix. The (i, j)-th element of matrix ?
denotes the correlation between the two clusters c
i
and c
j
. Matrix ? is expected to capture the de-
pendence between semantic relations. Intuitively,
if two clusters i and j are highly correlated, then
the (i, j)-th element of ? will be closer to 1. Equa-
tion 3 computes the similarity between a word pair
(a, b) and a set of synonymous word pairs. In-
tuitively, if the relations that exist between a and
b are typical relations that hold between synony-
mous word pairs, then Equation 3 returns a high
similarity score for a and b.
The proposed relational model of semantic sim-
ilarity differs from feature models of similarity,
such as the contrast model (Tversky, 1977), in
that it is defined over the set of semantic relations
that exist between two words instead of the set of
features for each word. Specifically, in contrast
model, the similarity S(a, b) between two objects
a and b is defined in terms of the features common
to a and b, A ? B, the features that are distinctive
to a, A?B, and the features that are distinctive to
b, B ?A. The contrast model is formalized in the
following equation,
S(a, b) = ?f(A ?B)? ?f(A?B)? ?f(B ?A). (4)
Here, the function f measures the salience of a
particular set of features, and non-negative param-
eters ?, ?, and ? determine the relative weights
assigned to the different components. However, in
the relational model of similarity we do not focus
on features of individual words but on relations be-
tween two words.
Modeling similarity as a phenomenon of rela-
tions between objects rather than features of indi-
vidual objects is central to computational models
of analogy-making such as the structure mapping
theory (SMT) (Falkenhainer et al, 1989). SMT
claims that an analogy is a mapping of knowl-
edge from one domain (base) into another (target)
which conveys that a system of relations known
to hold in the base also holds in the target. The
target objects do not have to resemble their corre-
sponding base objects. During the mapping pro-
cess, features of individual objects are dropped
and only relations are mapped. The proposed rela-
tional model of similarity uses this relational view
806
Ostrich, a large, flightless bird that lives in the dry grass-
lands of Africa.
Figure 1: A snippet returned for the query ?ostrich
* * * * * bird?.
of similarity to compute semantic similarity be-
tween words.
3.1 Extracting Lexical Patterns
To compute semantic similarity between two
words using the relational model (Equation 3),
we must first extract the numerous lexical pat-
terns from contexts in which those two words ap-
pear. For this purpose, we propose a pattern ex-
traction algorithm using snippets retrieved from
a web search engine. The proposed method re-
quires no language-dependent preprocessing such
as part-of-speech tagging or dependency parsing,
which can be both time consuming at Web scale,
and likely to produce incorrect results because of
the fragmented and ill-formed snippets.
Given two words a and b, we query a web search
engine using the wildcard query ?a * * * * * b?
and download snippets. The ?*? operator matches
one word or none in a web page. Therefore, our
wildcard query retrieves snippets in which a and
b appear within a window of seven words. We
attempt to approximate the local context of two
words using wildcard queries. For example, Fig-
ure 1 shows a snippet retrieved for the query ?os-
trich * * * * * bird?.
For a snippet S, retrieved for a word pair (a, b),
first, we replace the two words a and b, respec-
tively, with two variables X and Y. We replace all
numeric values by D, a marker for digits. Next,
we generate all subsequences of words from S that
satisfy all of the following conditions.
(i). A subsequence must contain exactly one oc-
currence of each X and Y
(ii). The maximum length of a subsequence is L
words.
(iii). A subsequence is allowed to have gaps. How-
ever, we do not allow gaps of more than g
number of words. Moreover, the total length
of all gaps in a subsequence should not ex-
ceed G words.
(iv). We expand all negation contractions in a con-
text. For example, didn?t is expanded to did
not. We do not skip the word not when gen-
erating subsequences. For example, this con-
dition ensures that from the snippet X is not a
Y, we do not produce the subsequence X is a
Y.
Finally, we count the frequency of all generated
subsequences and only use subsequences that oc-
cur more than N times as lexical patterns.
The parameters L, g, G and N are set exper-
imentally, as explained later in Section 6. It is
noteworthy that the proposed pattern extraction al-
gorithm considers all the words in a snippet, and
is not limited to extracting patterns only from the
mid-fix (i.e., the portion of text in a snippet that
appears between the queried words). Moreover,
the consideration of gaps enables us to capture re-
lations between distant words in a snippet. We use
a modified version of the prefixspan algorithm (Pei
et al, 2004) to generate subsequences from a text
snippet. Specifically, we use the constraints (ii)-
(iv) to prune the search space of candidate sub-
sequences. For example, if a subsequence has
reached the maximum length L, or contains the
maximum number of gaps G, then we will not ex-
tend it further. By pruning the search space, we
can speed up the pattern generation process. How-
ever, none of these modifications affect the accu-
racy of the proposed semantic similarity measure
because the modified version of the prefixspan al-
gorithm still generates the exact set of patterns that
we would obtain if we used the original prefixspan
algorithm (i.e. without pruning) and subsequently
remove patterns that violate the above mentioned
constraints. For example, some patterns extracted
form the snippet shown in Figure 1 are: X, a large
Y, X a flightless Y, and X, large Y lives.
3.2 Clustering Lexical Patterns
A semantic relation can be expressed using more
than one pattern. By grouping the semantically
related patterns, we can both reduce the model
complexity in Equation 2, and consider the depen-
dence among semantic relations in Equation 3. We
use the distributional hypothesis (Harris, 1954) to
find semantically related lexical patterns. The dis-
tributional hypothesis states that words that occur
in the same context have similar meanings. If two
lexical patterns are similarly distributed over a set
of word pairs, then from the distributional hypoth-
esis it follows that the two patterns must be similar.
We represent a pattern p by a vector p in which
807
the i-th element is the frequency f(a
i
, b
i
, p) of p in
a word pair (a
i
, b
i
). Given a set P of patterns and
a similarity threshold ?, Algorithm 1 returns clus-
ters of similar patterns. First, the function SORT
sorts the patterns in the descending order of their
total occurrences in all word pairs. The total oc-
currences of a pattern p is defined as ?(p), and is
given by,
?(p) =
?
(a,b)?W
f(a, b, p). (5)
Here, W is the set of word pairs. Then the outer
for-loop (starting at line 3), repeatedly takes a pat-
tern p
i
from the ordered set P , and in the inner for-
loop (starting at line 6), finds the cluster, c
?
(? C)
that is most similar to p
i
. Similarity between p
i
and the cluster centroid c
j
is computed using co-
sine similarity. The centroid vector c
j
of cluster c
j
is defined as the vector sum of all pattern vectors
for patterns in that cluster (i.e. c
j
=
?
p?c
j
p).
If the maximum similarity exceeds the threshold
?, we append p
i
to c
?
(line 14). Here, the op-
erator ? denotes vector addition. Otherwise, we
form a new cluster {p
i
} and append it to C, the
set of clusters. After all patterns are clustered,
we compute the (i, j) element of the inter-cluster
correlation matrix ? (Equation 3) as the inner-
product between the centroid vectors c
i
and c
j
of
the corresponding clusters i and j. The parame-
ter ? (? [0, 1]) determines the purity of the formed
clusters and is set experimentally in Section 5. Al-
gorithm 1 scales linearly with the number of pat-
terns. Moreover, sorting the patterns by their to-
tal word pair frequency prior to clustering ensures
that the final set of clusters contains the most com-
mon relations in the dataset.
4 Evaluation Procedure
Evaluating a semantic similarity measure is diffi-
cult because the notion of semantic similarity is
subjective. Miller-Charles (1998) dataset has been
frequently used to benchmark semantic similar-
ity measures. Miller-Charles dataset contains 30
word pairs rated by a group of 38 human subjects.
The word pairs are rated on a scale from 0 (no sim-
ilarity) to 4 (perfect synonymy). Because of the
omission of two word pairs in earlier versions of
WordNet, most researchers had used only 28 pairs
for evaluations. The degree of correlation between
the human ratings in the benchmark dataset and
the similarity scores produced by an automatic se-
mantic similarity measure, can be considered as a
Algorithm 1 Sequential pattern clustering algo-
rithm.
Input: patterns P = {p
1
, . . . ,p
n
}, threshold ?
Output: clusters C
1: SORT(P )
2: C ? {}
3: for pattern p
i
? P do
4: max ? ??
5: c
?
? null
6: for cluster c
j
? C do
7: sim ? cosine(p
i
, c
j
)
8: if sim > max then
9: max ? sim
10: c
?
? c
j
11: end if
12: end for
13: if max ? ? then
14: c
?
? c
?
? p
i
15: else
16: C ? C ? {p
i
}
17: end if
18: end for
19: return C
measurement of how well the semantic similarity
measure captures the notion of semantic similar-
ity held by humans. In addition to Miller-Charles
dataset we also evaluate on the WordSimilarity-
353 (Finkelstein et al, 2002) dataset. In con-
trast to Miller-Charles dataset which has only 30
word pairs, WordSimilarity-353 dataset contains
353 word pairs. Each pair has 13-16 human judg-
ments, which were averaged for each pair to pro-
duce a single relatedness score. Following the pre-
vious work, we use both Miller-Charles dataset
and WordSimilarity-353 dataset to evaluate the
proposed semantic similarity measure.
5 Computing Semantic Similarity
To extract lexical patterns that express numer-
ous semantic relations, we first select synonymous
words from WordNet synsets. A synset is a set
of synonymous words assigned for a particular
sense of a word in WordNet. We randomly select
2000 synsets of nouns from WordNet. From each
synset, a pair of synonymous words is selected.
For polysemous nouns, we selected synonyms
from the dominant sense. To perform a fair evalu-
ation, we do not select any words that appear in the
Miller-Charles dataset or the WordSimilarity-353
808
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
 1.3
 1.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
era
ge 
Sim
ilar
ity
Clustering Threshold
Figure 2: Average similarity vs. clustering thresh-
old ?
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Clu
ste
r S
par
sity
Clustering Threshold
Figure 3: Sparsity vs. clustering threshold ?
dataset, which are used later for evaluation pur-
poses. As we describe later, the clustering thresh-
old ? is tuned using this set of 2000 word pairs
selected from the WordNet.
We use the YahooBOSS API
2
and download
1000 snippets for each of those word pairs. Ex-
perimentally, we set the values for the parameters
in the pattern extraction algorithm (Section 3.1):
L = 5, g = 2, G = 4, and extract 5, 238, 637
unique patterns. However, only 1, 680, 914 of
those patterns occur more than twice. Low fre-
quency patterns often contain misspellings and are
not suitable for training. Therefore, we selected
patterns that occur at least 10 times in the snip-
pet collection. Moreover, we remove very long
patterns (ca. over 20 characters). The final set
contains 140, 691 unique lexical patterns. The re-
mainder of the experiments described in the paper
use those patterns.
2
http://developer.yahoo.com/search/boss/
We use the clustering Algorithm 1 to cluster the
extracted patterns. The only parameter in Algo-
rithm 1, the clustering threshold ?, is set as fol-
lows. We vary the value of theta ? from 0 to 1,
and use Algorithm 1 to cluster the extracted set
of patterns. We use the resultant set of clusters to
represent a word pair by a feature vector. We com-
pute a feature from each cluster as follows. First,
we assign a weight w
ij
to a pattern p
i
that is in a
cluster c
j
as follows,
w
ij
=
?(p
i
)
?
q?c
j
?(q)
. (6)
Here, ?(q) is the total frequency of a pattern, and it
is given by Equation 5. Because we perform a hard
clustering on patterns, a pattern can belong to only
one cluster (i.e. w
ij
= 0 for p
i
/? c
j
). Finally, we
compute the value of the j-th feature in the feature
vector for word pair (a, b) as follows,
?
p
i
?c
j
w
ij
f(a, b, p
i
). (7)
For each set of clusters, we compute the element
?
ij
of the corresponding inter-cluster correlation
matrix ? by the cosine similarity between the cen-
troid vectors for clusters c
i
and c
j
. The prototype
vector ? in Equation 3 is computed as the vector
sum of individual feature vectors for the synony-
mous word pairs selected from the WordNet as de-
scribed above. We then use Equation 3 to compute
the average of similarity scores for synonymous
word pairs we selected from WordNet.
We select the ? that maximizes the average
similarity score between those synonymous word
pairs. Formally, the optimal value of ?,
?
? is given
by the following Equation,
?
? = argmax
??[0,1]
(
1
|W |
?
(a,b)?W
sim(a, b)
)
. (8)
Here, W is the set of synonymous word pairs
(a, b), |W | is the total number of synonymous
word pairs (i.e. 2000 in our experiments), and
sim(a, b) is given by Equation 3. Because the av-
erages are taken over 2000 word pairs this proce-
dure gives a reliable estimate for ?. Moreover,
this method does not require negative training
instances such as, non-synonymous word pairs,
which are difficult to create manually. Average
similarity scores for various ? values are shown
in Figure 2. From Figure 2, we see that initially
average similarity increases when ? is increased.
809
This is because clustering of semantically related
patterns reduces the sparseness in feature vectors.
Average similarity is stable within a range of ? val-
ues between 0.5 and 0.7. However, increasing ?
beyond 0.7 results in a rapid drop of average sim-
ilarity. To explain this behavior consider Figure
3 where we plot the sparsity of the set of clusters
(i.e. the ratio between singletons to total clusters)
against threshold ?. As seen from Figure 3, high ?
values result in a high percentage of singletons be-
cause only highly similar patterns will form clus-
ters. Consequently, feature vectors for different
word pairs do not have many features in common.
The maximum average similarity score of 1.303 is
obtained with ? = 0.7, corresponding to 17, 015
total clusters out of which 12, 476 are singletons
with exactly one pattern (sparsity = 0.733). For
the remainder of the experiments in this paper we
set ? to this optimal value and use the correspond-
ing set of clusters to compute semantic similarity
by Equation 3. Similarity scores computed us-
ing Equation 3 can be greater than 1 (see Figure
2) because of the terms corresponding to the non-
diagonal elements in ?. We do not normalize the
similarity scores to [0, 1] range in our experiments
because the evaluation metrics we use are insensi-
tive to linear transformations of similarity scores.
6 Experiments
Table 1 compares the proposed method against
Miller-Charles ratings (MC), and previously pro-
posed web-based semantic similarity measures:
Jaccard, Dice, Overlap, PMI (Bollegala et al,
2007), Normalized Google Distance (NGD) (Cili-
brasi and Vitanyi, 2007), Sahami and Heil-
man (SH) (2006), co-occurrence double checking
model (CODC) (Chen et al, 2006), and support
vector machine-based (SVM) approach (Bollegala
et al, 2007). The bottom row of Table 1 shows the
Pearson correlation coefficient of similarity scores
produced by each algorithm with MC. All similar-
ity scores, except for the human-ratings in Miller-
Charles dataset, are normalized to [0, 1] range for
the ease of comparison. It is noteworthy that the
Pearson correlation coefficient is invariant under a
linear transformation. All similarity scores shown
in Table 1 except for the proposed method are
taken from the original published papers.
The highest correlation is reported by the pro-
posed semantic similarity measure. The improve-
ment of the proposed method is statistically sig-
nificant (confidence interval [0.73, 0.93]) against
all the similarity measures compared in Table 1
except against the SVM approach. From Table 1
we see that measures that use contextual informa-
tion from snippets (e.g. SH, CODC, SVM, and
proposed) outperform the ones that use only co-
occurrence statistics (e.g. Jaccard, overlap, Dice,
PMI, and NGD) such as page-counts. This is be-
cause similarity measures that use contextual in-
formation are better equipped to compute the sim-
ilarity between polysemous words. Although both
SVM and proposed methods use lexical patterns,
unlike the proposed method, the SVM method
does not consider the relatedness between pat-
terns. The superior performance of the proposed
method is attributable to its consideration of relat-
edness of patterns.
Table 2 summarizes the previously proposed
WordNet-based semantic similarity measures. De-
spite the fact that the proposed method does not
use manually compiled resources such as Word-
Net for computing similarity, its performance is
comparable to similarity measures that use Word-
Net. We believe that the proposed method will
be useful to compute the semantic similarity be-
tween named-entities for which manually created
resources are either incomplete or do not exist.
We evaluate the proposed method using the
WordSimilarity-353 dataset. Experimental re-
sults are presented in Table 3. Following pre-
vious work, we use Spearman rank correlation
coefficient, which does not require ratings to be
linearly dependent, for the evaluations on this
dataset. Likewise with the Miller-Charles ratings,
we measure the correlation between the similar-
ity scores produced by the proposed method for
word pairs in the WordSimilarity-353 dataset and
the human ratings. A higher Spearman correla-
tion coefficient (value=0.504, confidence interval
[0.422, 0.578]) indicates a better agreement with
the human notion of semantic similarity. From Ta-
ble 3 we can see that the proposed method outper-
forms a wide variety of semantic similarity mea-
sures developed using numerous resources includ-
ing lexical resources such as WordNet and knowl-
edge sources such as Wikipedia (i.e. WikiRe-
late!). In contrast to the Miller-Charles dataset
which only contains common English words se-
lected from the WordNet, the WordSimilarity-353
dataset contains word pairs where one or both
words are named entities (e.g. (Maradona, foot-
810
Table 1: Semantic similarity scores on Miller-Charles dataset
Word Pair MC Jaccrad Dice Overlap PMI NGD SH CODC SVM Proposed
automobile-car 3.920 0.650 0.664 0.831 0.427 0.466 0.225 0.008 0.980 0.918
journey-voyage 3.840 0.408 0.424 0.164 0.468 0.556 0.121 0.005 0.996 1.000
gem-jewel 3.840 0.287 0.300 0.075 0.688 0.566 0.052 0.012 0.686 0.817
boy-lad 3.760 0.177 0.186 0.593 0.632 0.456 0.109 0.000 0.974 0.958
coast-shore 3.700 0.783 0.794 0.510 0.561 0.603 0.089 0.006 0.945 0.975
asylum-madhouse 3.610 0.013 0.014 0.082 0.813 0.782 0.052 0.000 0.773 0.794
magician-wizard 3.500 0.287 0.301 0.370 0.863 0.572 0.057 0.008 1.000 0.997
midday-noon 3.420 0.096 0.101 0.116 0.586 0.687 0.069 0.010 0.819 0.987
furnace-stove 3.110 0.395 0.410 0.099 1.000 0.638 0.074 0.011 0.889 0.878
food-fruit 3.080 0.751 0.763 1.000 0.449 0.616 0.045 0.004 0.998 0.940
bird-cock 3.050 0.143 0.151 0.144 0.428 0.562 0.018 0.006 0.593 0.867
bird-crane 2.970 0.227 0.238 0.209 0.516 0.563 0.055 0.000 0.879 0.846
implement-tool 2.950 1.000 1.000 0.507 0.297 0.750 0.098 0.005 0.684 0.496
brother-monk 2.820 0.253 0.265 0.326 0.623 0.495 0.064 0.007 0.377 0.265
crane-implement 1.680 0.061 0.065 0.100 0.194 0.559 0.039 0.000 0.133 0.056
brother-lad 1.660 0.179 0.189 0.356 0.645 0.505 0.058 0.005 0.344 0.132
car-journey 1.160 0.438 0.454 0.365 0.205 0.410 0.047 0.004 0.286 0.165
monk-oracle 1.100 0.004 0.005 0.002 0.000 0.579 0.015 0.000 0.328 0.798
food-rooster 0.890 0.001 0.001 0.412 0.207 0.568 0.022 0.000 0.060 0.018
coast-hill 0.870 0.963 0.965 0.263 0.350 0.669 0.070 0.000 0.874 0.356
forest-graveyard 0.840 0.057 0.061 0.230 0.495 0.612 0.006 0.000 0.547 0.442
monk-slave 0.550 0.172 0.181 0.047 0.611 0.698 0.026 0.000 0.375 0.243
coast-forest 0.420 0.861 0.869 0.295 0.417 0.545 0.060 0.000 0.405 0.150
lad-wizard 0.420 0.062 0.065 0.050 0.426 0.657 0.038 0.000 0.220 0.231
cord-smile 0.130 0.092 0.097 0.015 0.208 0.460 0.025 0.000 0 0.006
glass-magician 0.110 0.107 0.113 0.396 0.598 0.488 0.037 0.000 0.180 0.050
rooster-voyage 0.080 0.000 0.000 0.000 0.228 0.487 0.049 0.000 0.017 0.052
noon-string 0.080 0.116 0.123 0.040 0.102 0.488 0.024 0.000 0.018 0.000
Correlation - 0.260 0.267 0.382 0.549 0.205 0.580 0.694 0.834 0.867
Table 2: Comparison with WordNet-based simi-
larity measures.
Method Correlation
Edge-counting 0.664
Jiang & Conrath (1998) 0.848
Lin (1998a) 0.822
Resnik (1995) 0.745
Li et al (2003) 0.891
ball) and (Jerusalem, Israel)). Because the pro-
posed method use snippets retrieved from a web
search engine, it is capable of extracting expres-
sive lexical patterns that can explicitly state the re-
lationship between two entities.
If we must compare n objects using a feature
model of similarity, then we only need to define
features for each of those n objects. However, in
the proposed relational model we must define re-
lations between all pairs of objects. In the case
where all n objects are different, this requires us to
define relations for n(n?1)/2 object pairs. Defin-
ing relations for all pairs can be computationally
costly for large n values. Efficiently comparing n
objects using a relational model is an interesting
future research direction of the current work.
Table 3: Results on WordSimilarity-353 dataset.
Method Correlation
WordNet Edges (Jarmasz, 1993) 0.27
Hirst & St-Onge (1997) 0.34
Jiang & Conrath (1998) 0.34
WikiRelate! (Strube and Ponzetto, 2006) 0.19-0.48
Leacock & Chodrow (1998) 0.36
Lin (1998b) 0.36
Resnik (1995) 0.37
Proposed 0.504
7 Conclusion
We proposed a relational model to measure the
semantic similarity between two words. First, to
represent the numerous semantic relations that ex-
ist between two words, we extract lexical patterns
from snippets retrieved from a web search engine.
Second, we cluster the extracted patterns to iden-
tify the semantically related patterns. Third, us-
ing the pattern clusters we define a feature vector
to represent two words and compute the semantic
similarity by taking into account the inter-cluster
correlation. The proposed method outperformed
all existing web-based semantic similarity mea-
sures on two benchmark datasets.
811
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of ACL?99, pages 57?
64.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of WWW?07, pages 757?
766.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL ?06, pages 1009?1016.
R.L. Cilibrasi and P.M.B. Vitanyi. 2007. The google
similarity distance. IEEE Transactions on Knowl-
edge and Data Engineering, 19(3):370?383.
J. Curran. 2002. Ensemble menthods for automatic
thesaurus extraction. In Proc. of EMNLP.
B. Falkenhainer, K.D. Forbus, and D. Gentner. 1989.
Structure mapping engine: Algorithm and examples.
Artificial Intelligence, 41:1?63.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
TOIS, 20:116?131.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proc. of IJCAI?07, pages
1606?1611.
R. L. Goldstone. 1994. The role of similarity in cat-
egorization: providing a groundwork. Cognition,
52:125?157.
U. Hahn, N. Chater, and L. B. Richardson. 2003. Sim-
ilarity as transformation. Cognition, 87:1?32.
Z. Harris. 1954. Distributional structure. Word,
10:146?162.
M.A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of 14th
COLING, pages 539?545.
G. Hirst and D. St-Onge. 1997. Lexical chains as rep-
resentations of context for the detection and correc-
tion of malapropisms.
M. Jarmasz. 1993. Roget?s thesaurus as a lexical re-
source for natural language processing. Master?s
thesis, University of Ottawa.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of ROCLING?98.
C. L. Krumhansl. 1978. Concerning the applicability
of geometric models to similarity data: The inter-
relationship between similarity and spatial density.
Psychological Review, 85:445?463.
C. Leacock and M. Chodorow. 1998. Combining Lo-
cal Context and WordNet Similarity for Word Sense
Identification. MIT.
M. Li, X. Chen, X. Li, B. Ma, and P.M.B. Vitanyi.
2004. The similarity metric. IEEE Transactions on
Information Theory, 50(12):3250?3264.
D. Lin. 1998a. Automatic retreival and clustering of
similar words. In Proc. of the 17th COLING, pages
768?774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296?
304.
G. Miller and W. Charles. 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
J. Pei, J. Han, B. Mortazavi-Asi, J. Wang, H. Pinto,
Q. Chen, U. Dayal, and M. Hsu. 2004. Mining se-
quential patterns by pattern-growth: the prefixspan
approach. IEEE Transactions on Knowledge and
Data Engineering, 16(11):1424?1440.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on seman-
tic nets. IEEE Transactions on Systems, Man and
Cybernetics, 9(1):17?30.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
IJCAI?95.
M. Sahami and T. Heilman. 2006. A web-based kernel
function for measuring the similarity of short text
snippets. In Proc. of WWW?06.
V. Schickel-Zuber and B. Faltings. 2007. Oss: A se-
mantic similarity function based on hierarchical on-
tologies. In Proc. of IJCAI?07, pages 551?556.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using wikipedia. In
Proc. of AAAI? 06.
J. B. Tenenbaum. 1999. Bayesian modeling of human
concept learning. In NIPS?99.
A. Tversky. 1977. Features of similarity. Psychologi-
cal Review, 84:327?652.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An ap-
proch for measuring semantic similarity between
words using multiple information sources. IEEE
Transactions on Knowledge and Data Engineering,
15(4):871?882.
812
A Machine Learning Approach to Sentence
Ordering for Multidocument Summarization
and Its Evaluation
Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka
University of Tokyo, Japan
Abstract. Ordering information is a difficult but a important task for
natural language generation applications. A wrong order of information
not only makes it difficult to understand, but also conveys an entirely
different idea to the reader. This paper proposes an algorithm that learns
orderings from a set of human ordered texts. Our model consists of a set
of ordering experts. Each expert gives its precedence preference between
two sentences. We combine these preferences and order sentences. We
also propose two new metrics for the evaluation of sentence orderings.
Our experimental results show that the proposed algorithm outperforms
the existing methods in all evaluation metrics.
1 Introduction
The task of ordering sentences arises in many fields. Multidocument Summa-
rization (MDS) [5], Question and Answer (QA) systems and concept to text
generation systems are some of them. These systems extract information from
different sources and combine them to produce a coherent text. Proper ordering
of sentences improves readability of a summary [1]. In most cases it is a trivial
task for a human to read a set of sentences and order them coherently. Hu-
mans use their wide background knowledge and experience to decide the order
among sentences. However, it is not an easy task for computers. This paper pro-
poses a sentence ordering algorithm and evaluate its performance with regard
to MDS.
MDS is the task of generating a human readable summary from a given set of
documents. With the increasing amount of texts available in electronic format,
automatic text summarization has become necessary. It can be considered as a
two-stage process. In the first stage the source documments are analyzed and a
set of sentences are extracted. However, the document set may contain repeating
information as well as contradictory information and these challenges should
be considered when extracting sentences for the summary. Researchers have
already investigated this problem and various algorithms exist. The second stage
of MDS creates a coherent summary from this extract. When summarizing a
single document, a naive strategy that arranges extracted sentences according
to the appearance order may yield a coherent summary. However, in MDS the
extracted sentences belong to different source documents. The source documents
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 624?635, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
A Machine Learning Approach to Sentence Ordering 625
may have been written by various authors and on various dates. Therefore we
cannot simply order the sentences according to the position of the sentences in
the original document to get a comprehensible summary.
This second stage of MDS has received lesser attention compared to the
first stage. Chronological ordering; ordering sentences according to the pub-
lished date of the documents they belong to [6], is one solution to this problem.
However, showing that this approach is insufficient, Barzilay [1] proposed an
refined algorithm which integrates chronology ordering with topical relatedness
of documents. Okazaki [7] proposes a improved chronological ordering algorithm
using precedence relations among sentences. His algorithm searches for an order
which satisfies the precedence relations among sentences. In addition to these
studies which make use of chronological ordering, Lapata [3] proposes a prob-
abilistic model of text structuring and its application to the sentence ordering.
Her system calculates the conditional probabilities between sentences from a
corpus and uses a greedy ordering algorithm to arrange sentences according to
the conditional probabilities.
Even though these previous studies proposed different strategies to decide the
sentence ordering, the appropriate way to combine these different methods to
obtain more robust and coherent text remains unknown. In addition to these ex-
isting sentence ordering heuristics, we propose a new method which we shall call
succession in this paper. We then learn the optimum linear combination of these
heuristics that maximises readability of a summary using a set of human-made
orderings. We then propose two new metrics for evaluating sentence orderings;
Weighted Kendall Coefficient and Average Continuity. Comparing with an in-
trinsic evaluation made by human subjects, we perform a quantitative evaluation
using a number of metrics and discuss the possiblity of the automatic evaluation
of sentence orderings.
2 Method
For sentences taken from the same document we keep the order in that docu-
ment as done in single document summarization. However, we have to be careful
when ordering sentences which belong to different documents. To decide the or-
der among such sentences, we implement five ranking experts: Chronological,
Probabilistic, Topical relevance, Precedent and Succedent. These experts return
precedence preference between two sentences. Cohen [2] proposes an elegant
learning model that works with preference functions and we adopt this learn-
ing model to our task. Each expert e generates a pair-wise preference function
defined as following:
PREFe(u, v, Q) ? [0, 1]. (1)
Where, u, v are two sentences that we want to order; Q is the set of sentences
which has been already ordered. The expert returns its preference of u to v. If
the expert prefers u to v then it returns a value greater than 0.5. In the extreme
case where the expert is absolutely sure of preferring u to v it will return 1.0.
On the other hand, if the expert prefers v to u it will return a value lesser than
626 D. Bollegala, N. Okazaki, and M. Ishizuka
0.5. In the extreme case where the expert is absolutely sure of preferring v to u
it will return 0. When the expert is undecided of its preference between u and v
it will return 0.5.
The linear weighted sum of these individual preference functions is taken as
the total preference by the set of experts as follows:
PREFtotal(u, v, Q) =
?
e?E
wePREFe(u, v, Q). (2)
Therein: E is the set of experts and we is the weight associated to expert e ? E.
These weights are normalized so that the sum of them is 1. We use the Hedge
learning algorithm to learn the weights associated with each expert?s preference
function. Then we use the greedy algorithm proposed by Cohen [2] to get an
ordering that approximates the total preference.
2.1 Chronological Expert
Chronological expert emulates conventional chronological ordering [4,6] which
arranges sentences according to the dates on which the documents were published
and preserves the appearance order for sentences in the same document. We
define a preference function for the expert as follows:
PREFchro(u, v, Q) =
?
?
?
?
?
1 T (u) < T (v)
1 [D(u) = D(v)] ? [N(u) < N(v)]
0.5 [T (u) = T (v)] ? [D(u) = D(v)]
0 otherwise
. (3)
Therein: T (u) is the publication date of sentence u; D(u) presents the unique
identifier of the document to which sentence u belongs; N(u) denotes the line
number of sentence u in the original document. Chronological expert gives 1
(preference) to the newly published sentence over the old and to the prior over
the posterior in the same article. Chronological expert returns 0.5 (undecided)
when comparing two sentences which are not in the same article but have the
same publication date.
2.2 Probabilistic Expert
Lapata [3] proposes a probabilistic model to predict sentence order. Her model
assumes that the position of a sentence in the summary depends only upon the
sentences preceding it. For example let us consider a summary T which has
sentences S1, . . . , Sn in that order. The probability P (T ) of getting this order is
given by:
P (T ) =
n
?
i=1
P (Sn|S1, . . . , Sn?i). (4)
She further reduces this probability using bi-gram approximation as follows.
P (T ) =
n
?
i=1
P (Si|Si?1) (5)
A Machine Learning Approach to Sentence Ordering 627
She breaks each sentence into features and takes the vector product of features
as follows:
P (Si|Si?1) =
?
(a<i,j>,a<i?1,k>)?Si?Si?1
P (a<i,j>, a<i?1,k>). (6)
Feature conditional probabilities can be calculated using frequency counts of
features as follows:
P (a<i,j>|a<i?1,k>) =
f(a<i,j>, a<i?1,k>)
?
a<i,j>
f(a<i,j>, a<i?1,k>)
. (7)
Lapata [3] uses nouns,verbs and dependency structures as features. Where as
in our expert we implemented only nouns and verbs as features. We performed
back-off smoothing on the frequency counts in equation 7 as these values were
sparse. Once these conditional probabilities are calculated, for two sentences u,v
we can define the preference function for the probabilistic expert as follows:
PREFprob(u, v, Q) =
{
1+P (u|r)?P (v|r)
2 Q = 
1+P (u)?P (v)
2 Q = 
. (8)
Where, Q is the set of sentences ordered so far and r ? Q is the lastly ordered
sentence in Q. Initially, Q is null and we prefer the sentence with higher absolute
probability. When Q is not null and u is preferred to v, i.e. P (u|r) > P (v|r),
according to definition 8 a preference value greater than 0.5 is returned. If v is
preferred to u, i.e. P (u|r) < P (v|r), we have a preference value smaller than 0.5.
When P (u|r) = P (v|r), the expert is undecided and it gives the value 0.5.
2.3 Topical Relevance Expert
In MDS, the source documents could contain multiple topics. Therefore, the
extracted sentences could be covering different topics. Grouping the extracted
sentences which belong to the same topic, improves readability of the summary.
Motivated by this fact, we designed an expert which groups the sentences which
belong to the same topic. This expert prefers sentences which are more similar
to the ones that have been already ordered. For each sentence l in the extract
we define its topical relevance, topic(l) as follows:
topic(l) = max
q?Q
sim(l, q). (9)
We use cosine similarity to calculate sim(l, q). The preference function of this
expert is defined as follows:
PREFtopic(u, v, Q) =
{
0.5 [Q = ] ? [topic(u) = topic(v)]
1 [Q = ] ? [topic(u) > topic(v)]
0 otherwise
. (10)
Where,  represents the null set, u,v are the two sentences under considera-
tion and Q is the block of sentences that has been already ordered so far in
the summary.
628 D. Bollegala, N. Okazaki, and M. Ishizuka
3
7PQTFGTGF
5GPVGPEGU

.5WOOCT[
Fig. 1. Topical relevance expert
2.4 Precedent Expert
When placing a sentence in the summary it is important to check whether the
preceding sentences convey the necessary background information for this sen-
tence to be clearly understood. Placing a sentence without its context being
stated in advanced, makes an unintelligible summary. As shown in figure 2, for
each extracted sentence l, we can compare the block of text that appears before
it in its source document (P ) with the block of sentences which we have ordered
so far in the summary (Q). If P and Q matches well, then we can safely as-
sume that Q contains the necessary background information required by l. We
can then place l after Q. Such relations among sentences are called precedence
relations. Okazaki [7] proposes precedence relations as a method to improve the
chronological ordering of sentences. He considers the information stated in the
documents preceding the extracted sentences to judge the order. Based on this
idea, we define precedence pre(l) of the extracted sentence l as follows:
pre(l) = max
p?P,q?Q
sim(p, q). (11)
l
2
&QEWOGPV
&5WOOCT[
3
Fig. 2. Precedent expert
Here, P is the set of sentences preceding the extract sentence l in the original
document. We calculate sim(p, q) using cosine similarity. The preference function
for this expert can be written as follows:
PREFpre(u, v, Q) =
{
0.5 [Q = ] ? [pre(u) = pre(v)]
1 [Q = ] ? [pre(u) > pre(v)]
0 otherwise
. (12)
A Machine Learning Approach to Sentence Ordering 629
&QEWOGPV

&
5WOOCT[
3
r
-
7PQTFGTGF
5GPVGPEGU

.
l
Fig. 3. Succedent expert
2.5 Succedent Expert
When extracting sentences from source documents, sentences which are similar to
the ones that are already extracted, are usually ignored to prevent repetition of
information. However, this information is valuable when ordering sentences. For
example, a sentence that was ignored by the sentence extraction algorithm might
turn out to be more suitable when ordering the extracted sentences. However, we
assume that the sentence ordering algorithm is independent from the sentence ex-
traction algorithmand therefore does not possess this knowledge regarding the left
out candidates. This assumption improves the compatibility of our algorithm as it
can be used to order sentences extracted by any sentence extraction algorithm. We
design an expert which uses this information to order sentences.
Let us consider the siuation depicted in Figure 3 where a block Q of text is
orderd in the summary so far. The lastly ordered setence r belongs to document
D in which a block K of sentences follows r. The author of this document assumes
that K is a natural consequence of r. However, the sentence selection algorithm
might not have selected any sentences from K because it already selected some
sentences with this information from some other document. Therefore, we search
the extract L for a sentence that best matches with a sentence in K. We define
succession as a measure of this agreement(13) as follows:
succ(l) = max
k?K
sim(l, k). (13)
Here, we calculate sim(l, k) using cosine similarity. Sentences with higher succes-
sion values are preferred by the expert. The preference function for this expert
can be written as follows:
PREFsucc(u, v, Q) =
{
0.5 [Q = ] ? [succ(u) = succ(v)]
1 [Q = ] ? [succ(u) > succ(v)]
0 otherwise
. (14)
2.6 Ordering Algorithm
Using the five preference functions described in the previous sections, we compute
the total preference function of the set of experts as defined by equation 2. Sec-
tion 2.7 explains the method that we use to calculate the weights assigned to each
expert?s preference. In this section we will consider the problem of finding an order
that satisfies the total preference function. Finding the optimal order for a given
630 D. Bollegala, N. Okazaki, and M. Ishizuka
total preference function is NP-complete [2]. However, Cohen [2] proposes a greedy
algorithm that approximates the optimal ordering. Once the unordered extract X
and total preference (equation 2) are given, this greedy algorithm can be used to
generate an approximately optimal ordering function ??.
let V = X
for each v ? V do
?(v) =
?
u?V
PREF(v, u, Q) ?
?
u?V
PREF(u, v, Q)
while V is non-empty do
let t = arg maxu?V ?(u)
let ??(t) = |V |
V = V ? {t}
for each v ? V do
?(v) = ?(v) + PREF(t, u) ? PREF(v, t)
endwhile
2.7 Learning Algorithm
Cohen [2] proposes a weight allocation algorithm that learns the weights associ-
ated with each expert in equation 2. We shall explain this algorithm in regard
to our model of five experts.
Rate of learning ? ? [0, 1], initial weight vector w1 ? [0, 1]5, s.t.
?
e?E w
1
e = 1.
Do for t = 1, 2, . . . , T where T is the number of training examples.
1. Get Xt; the set of sentences to be ordered.
2. Compute a total order ??t which approximates,
PREFttotal(u, v, Q) =
?
e?E
PREFte(u, v, Q).
We used the greedy ordering algorithm described in section 2.6 to get ??t.
3. Order Xt using ??t.
4. Get the human ordered set F t of Xt. Calculate the loss for each expert.
Loss(PREFte, F
t) = 1 ? 1|F |
?
(u,v)?F
PREFte(u, v, Q) (15)
5. Set the new weight vector,
wt+1e =
wte?
Loss(PREFte,F
t)
Zt
(16)
where, Zt is a normalization constant, chosen so that,
?
e?E w
t+1
e = 1.
A Machine Learning Approach to Sentence Ordering 631
In our experiments we set ? = 0.5 and w1i = 0.2. To explain equation 15 let us
assume that sentence u comes before sentence v in the human ordered summary.
Then the expert must return the value 1 for PREF(u,v,Q). However,if the expert
returns any value less than 1, then the difference is taken as the loss. We do this
for all such sentence pairs in F . For a summary of length N we have N(N ?1)/2
such pairs. Since this loss is taken to the power of ?, a value smaller than 1, the
new weight of the expert gets changed according to the loss as in equation 16.
3 Evaluation
In addition to Kendall?s ? coefficient and Spearman?s rank correlation coefficient
which are widely used for comparing two ranks, we use sentence continuity [7]
as well as two metrics we propose; Weighted Kendall and Average Continuity.
3.1 Weighted Kendall Coefficient
The Kendall?s ? coefficient is defined as following:
? = 1 ? 2Q
nC2
. (17)
Where, Q is the number of discordant pairs and nC2 is the number of combi-
nations that can be generated from a set of n distinct elements by taking two
elements at a time with replacement. However, one major drawback of this met-
ric when evaluating sentence orderings is that, it does not take into consideration
the relative distance d between the discordant pairs. However, when reading a
text a human reader is likely to be more sensitive to a closer discordant pair than
a discordant pair far apart. Therefore, a closer discordant pair is more likely to
harm the readability of the summary compared to a far apart discordant pair. In
order to reflect these differences in our metric, we use an exponentially decreasing
weight function as follows:
h(d) =
{
exp(1 ? d) d ? 1
0 else
. (18)
Here, d is the number of sentences that lie between the two sentences of the
discordant pair. Going by the traditional Kendall?s ? coefficient we defined our
weighted Kendall coefficient as following, so that it becomes a metric in [1, ?1]
range.
?w = 1 ?
2
?
d h(d)
?n
i=1 h(i)
(19)
3.2 Average Continuity
Both Kendall?s ? coefficient and the Weighted Kendall coefficient measure dis-
cordants between ranks. However, in the case of summaries, we need a metric
which expresses the continuity of the sentences. A summary which can be read
632 D. Bollegala, N. Okazaki, and M. Ishizuka
continuously is better compared to a one that cannot. If the ordered extract
contains most of the sentence blocks of the reference summary then we can
safely assume that it is far more readable and coherent to a one that is not.
Sentence n-gram counts of continuous sentences give a rough idea of this kind
of continuity.
For a summary of length N there are N ? n + 1 possible sentence n-grams
of length n. Therefore, we can define a precision Pn of continuity length n as:
Pn =
number of matched n-grams
N ? n + 1 . (20)
Due to sparseness of higher order n-grams Pn decreases in an exponential-like
curve with n. Therefore, we define Average Continuity as the logrithmic average
of Pn as follows:
Average Continuity = exp(
1
3
4
?
n=2
log(Pn)) (21)
We add a small quantity ? to numerator and denominator of Pn in equation
20 so that the logarithm will not diverge when n-grams count is zero. We used
? = 0.01 in our evaluations. Experimental results showed that taking n-grams up
to four gave contrasting results because the n-grams tend to be sparse for larger
n values. BLEU(BiLingual Evaluation Understudy) proposed by Papineni [8]
for the task of evaluating machine translations has an analogical form to our
average continuity. In BLEU, a machine translation is compared against multiple
reference translations and precision values are calculated using word n-grams.
BLEU is then defined as the logarithmic average of these precision values.
4 Results
We used the 3rd Text Summarization Challenge (TSC) corpus for our exper-
iments. TSC1 corpus contains news articles taken from two leading Japanese
newspapers; Mainichi and Yomiuri. TSC-3 corpus contains human selected ex-
tracts for 30 different topics. However, in the TSC corpus the extracted sentences
are not ordered to make a readable summary. Therefore, we first prepared 30
summaries by ordering the extraction data of TSC-3 corpus by hand. We then
compared the orderings by the proposed algorithm against these human ordered
summaries. We used 10-fold cross validation to learn the weights assigned to
each expert in our proposed algorithm. These weights are shown in table 1.
According to table 1, succedent, chronology and precedent experts have the
highest weights among the five experts and therefore almost entirely control the
process of ordering. Whereas probabilistic and topical relevance experts have
almost no influence on their decisions. However, we cannot directly compare La-
pata?s [3] approach with our probabilistic expert as we do not use dependency
1 http://lr-www.pi.titech.ac.jp/tsc/index-en.html
A Machine Learning Approach to Sentence Ordering 633
Table 1. Weights learned
Expert Chronological Probabilistic Topical Relevance Precedent Succedent
Weights 0.327947 0.000039 0.016287 0.196562 0.444102
Table 2. Comparison with Human Ordering
Spearman Kendall Continuity Weighted Kendall Average Continuity
RO -0.267 -0.160 -0.118 -0.003 0.024
PO 0.062 0.040 0.187 0.013 0.029
CO 0.774 0.735 0.629 0.688 0.511
LO 0.783 0.746 0.706 0.717 0.546
HO 1.000 1.000 1.000 1.000 1.000








A Co-occurrence Graph-based Approach for
Personal Name Alias Extraction from Anchor Texts
Danushka Bollegala ?
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@mi.ci.i.u-
tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and
Technology
1-18-13, Sotokanda, Tokyo,
101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
ishizuka@i.u-
tokyo.ac.jp
Abstract
A person may have multiple name aliases
on the Web. Identifying aliases of a name
is important for various tasks such as in-
formation retrieval, sentiment analysis and
name disambiguation. We introduce the no-
tion of a word co-occurrence graph to rep-
resent the mutual relations between words
that appear in anchor texts. Words in an-
chor texts are represented as nodes in the
co-occurrence graph and an edge is formed
between nodes which link to the same url.
For a given personal name, its neighboring
nodes in the graph are considered as can-
didates of its aliases. We formalize alias
identification as a problem of ranking nodes
in this graph with respect to a given name.
We integrate various ranking scores through
support vector machines to leverage a robust
ranking function and use it to extract aliases
for a given name. Experimental results on a
dataset of Japanese celebrities show that the
proposed method outperforms all baselines,
displaying a MRR score of 0.562.
1 Introduction
Searching for information about people in the Web is
one of the most common activities of Internet users.
Around 30% of search engine queries include person
names (Guha and Garg, 2004). However, an indi-
vidual might have multiple nicknames or aliases on
?Research Fellow of the Japan Society for the Promotion of
Science (JSPS)
the Web. For example, the famous Japanese major
league baseball player Hideki Matsui is often called
as Godzilla in web contents. Identifying aliases of
a name is important in various tasks such as infor-
mation retrieval (Salton and McGill, 1986), senti-
ment analysis (Turney, 2002) and name disambigua-
tion (Bekkerman and McCallum, 2005).
In information retrieval, to improve recall of a
web search on a person name, a search engine can
automatically expand the query using aliases of the
name. In our previous example, a user who searches
for Hideki Matsui might also be interested in re-
trieving documents in which Matsui is referred to
as Godzilla. People use different aliases when ex-
pressing their opinions about an entity. By aggre-
gating texts written on an individual that use various
aliases, a sentiment analysis system can make an in-
formed judgment on the sentiment. Name disam-
biguation focuses on identifying different individu-
als with the same name. For example, for the name
Jim Clark, aside from the two most popular name-
sakes - the formula-one racing champion and the
founder of Netscape - at least ten different people are
listed among the top 100 results returned by Google
for the name. Although namesakes have identical
names, their nicknames usually differ. Therefore, a
name disambiguation algorithm can benefit from the
knowledge related to name aliases.
We propose an alias extraction method that ex-
ploits anchor texts and the links indicated by the
anchor texts. Link structure has been studied
extensively in information retrieval and has been
found to be useful in various tasks such as rank-
ing of web pages, identification of hub-authority
865
sites, text categorization and social network extrac-
tion (Chakrabarti, 2003). Anchor texts pointing to
an url provide useful semantic clues regarding the
resource represented by the url.
If the majority of inbound anchor texts of an
url contain a person name, then it is likely that
the remainder of the anchor texts contain informa-
tion about aliases of the name. For example, an
image of Hideki Matsui on a web page might be
linked using the real name, Hideki Matsui, as well
as aliases Godzilla and Matsu Hide. However, ex-
tracting aliases from anchor texts is a challenging
problem due to the noise in both link structure and
anchor texts. For example, web pages of extremely
diverse topics link to yahoo.com using various an-
chor texts. Moreover, given the scale of the Web,
broken links and incorrectly linked anchor texts are
abundant. Naive heuristics are insufficient to extract
aliases from anchor texts.
Our main contributions are summarized as fol-
lows:
? We introduce word co-occurrence graphs to
represents words that appear in anchor texts
and formalize the problem of alias extraction
as a one of ranking nodes in the graph with re-
spect to a given name.
? We define various ranking scores to evaluate
the appropriateness of a word as an alias of a
name. Moreover, the ranking scores are inte-
grated using support vector machines to lever-
age a robust alias detection method.
2 Related Work
Hokama and Kitagawa (2006) propose an alias ex-
traction method that is specific to Japanese lan-
guage. For a given name p, they search for the query
* koto p 1 and extract the words that match the aster-
isk. However, koto is highly ambiguous and extracts
lots of incorrect aliases. Moreover, the method can-
not extract aliases when a name and its aliases ap-
pear in separate documents.
Anchor texts and link structure have been em-
ployed in synonym extraction (Chen et al, 2003)
and translations extraction (Lu et al, 2004). Chen
et al (2003) propose the use of hyperlink structure
1koto is written in hiragana and and means also known as
Hideki MatsuiGodzilla Matsu Hide
????
Yankees baseball
sportsNew York
Figure 1: Co-occurrence graph for Hideki Matsui
within a particular domain to generate a domain-
specific thesaurus. First, a set of high quality web-
sites from a given domain is selected. Second, sev-
eral link analysis techniques are used to remove
noisy links and the navigational structure of the web-
site is converted into a content structure. Third,
pointwise mutual information is applied to identify
phrases within content structures to create a domain
specific thesaurus. They evaluate the thesaurus in a
query expansion task. Anchor texts written in differ-
ent languages that point the same object have been
used in cross-language information retrieval (CLIR)
to translate user queries. Lu et al (2004) extend this
idea by associating anchor texts written using a piv-
otal third language to find translations of queries.
3 Method
3.1 Outline
We introduce word co-occurrence graph, an undi-
rected graph, to represent words that appear in an-
chor texts. For each word that appears in the vocabu-
lary of words in anchor texts, we create a node in the
graph. Two words are considered as co-occurring if
two anchor texts containing these words link to the
same url. An edge is formed between two nodes if
the words represented by those nodes co-occur. Fig-
ure 1 illustrates a portion of the co-occurrence graph
in the proximity of Hideki Matsui as extracted by
this method from anchor texts.
Representing words that appear in anchor texts
as a graph enables us to capture the complex inter-
relations between the words. Words in inbound an-
chor texts of an url contain important semantic clues
866
regarding the resource represented by the url. Such
words form a clique in the co-occurrence graph,
indicating their close connectivity. Moreover, co-
occurrence graphs represent indirect relationships
between words. For example, in Figure 1 Hideki
Matsui is connected to New York via Yankees.
We model the problem of extracting aliases as
a one of ranking nodes in the co-occurrence graph
with respect to a real name. Usually, an individual
has just one or two aliases. A name alias extraction
algorithm must identify the correct aliases among a
vast number of related terms for an individual.
3.2 Word Co-occurrence Graph
Let V be the vocabulary of words wi that appear
in anchor texts. The boolean function A(ai, wi) re-
turns true if the anchor text ai contains the word wi.
Moreover, let the boolean function L(ai, ui) to be
true if the anchor text ai points to url ui. Then two
words wi, wj are defined to be co-occurring in a url
u, if A(ai, wi) ? A(aj , wj) ? L(ai, u) ? L(aj , u) is
true for at least one pair of anchor texts (ai, aj). In
other words, two words are said to co-occur in an url
if at least one inbound pair of anchor texts contains
the two words. Moreover, we define the number of
co-occurrences of wi and wj to be the number of
different urls they co-occur.
We define word co-occurrence graph, G(V,E)
(V is the set of nodes and E is the set of edges) as an
undirected graph where each word wi in vocabulary
V is represented by a node in the graph. Because
one-to-one mapping pertains between a word and a
node, for simplicity we use wi to represent both the
word and the corresponding node in the graph. An
edge eij ? E is created between two nodes wi, wj if
they co-occur. Given a personal name p, represented
by a node p in the co-occurrence graph, our objec-
tive is to identify the nodes that represent aliases of
p. We rank the nodes in the graph with respect to
p such that more likely a node is an alias of p, the
higher the rank it is assigned. According to our def-
inition, a node that lies n hops away from p has an
n-order co-occurrence with p. Considering the fact
that a single web page might link to many pages with
diverse topics, higher order co-occurrences with p
(i.e. nodes that appear further from p) are unreliable
as aliases of p. Consequently, we limit C(p), the set
of candidate aliases of p, to nodes which are directly
Table 1: Contingency table for a candidate alias x
x C(p)? {x}
p k n? k n
V ? {p} K ? k N ? n?K + k N ? n
V K N ?K N
connected to p in the graph. In Figure 1 candidates
of Hideki Matsui fall inside the dotted ellipse.
3.3 Ranking of Candidates
To evaluate the strength of co-occurrence between
a candidate alias and the real name, for each candi-
date alias x in C(p) we create a contingency table
as shown in Table 1. In Table 1, the first row repre-
sents candidates of p and the first column represents
nodes in the graph. Therein, k is the number of urls
in which p and x co-occur, K is the number of urls
in which at least one inbound anchor text contains
the candidate x, n is the number of urls in which
at least one inbound anchor text contains p and N is
the total number of urls in the crawl. Next, we define
various ranking scores based on Table 1.
Simplest of all ranking scores is the link frequency
(lf ). We define link frequency of an candidate x as
the number of different urls in which x and p co-
occur. This is exactly the value of k in Table 1.
Link frequency is biased towards highly frequent
words. A word that has a high frequency in anchor
texts can also report a high co-occurrence with p.
tfidf measure which is popularly used in information
retrieval can be used to normalize this bias. tfidf is
computed from Table 1 as follows,
tfidf(nj) = k log NK + 1 .
From Table 1 we compute co-occurrence mea-
sures; log likelihood ratio LLR (Dunning, 1993),
chi-squared measure CS, point-wise mutual infor-
mation PMI (Church and Hanks, 1991) and hyper
geometric distribution HG (Hisamitsu and Niwa,
2001). Each of these measures is used to rank candi-
date aliases of a given name. Because of the limited
availability of space, we omit the definitions of these
measures.
Furthermore, we define popular set overlap mea-
sures; cosine measure, overlap coefficient and Dice
coefficient from Table 1 as follows,
867
cosine(p, x) = k?n+?K ,
overlap(p, x) = kmin(n,K) ,
Dice(p, x) = 2kn+K .
3.4 Hub weighting
A frequently observed phenomenon on the Web is
that many web pages with diverse topics link to so
called hubs such as Google, Yahoo or Amazon. Be-
cause two anchor texts might link to a hub for en-
tirely different reasons, co-occurrences coming from
hubs are prone to noise. To overcome the adverse ef-
fects of a hub h when computing the ranking scores
described in section 3.3, we multiply the number
of co-occurrences of words linked to h by a factor
?(h, p) where,
?(h, p) = td? 1 . (1)
Here, t is the number of inbound anchor texts of
h that contain the real name p, d is the total num-
ber of inbound anchor texts of h. If many anchor
texts that link to h contain p (i.e., larger t value)
then the reliability of h as a source of information
about p increases. On the other hand, if h has many
inbound links (i.e., larger d value) then it is likely
to be a noisy hub and gets discounted when mul-
tiplied by ?(<< 1). Intuitively, Formula 1 boosts
hubs that are likely to be containing information re-
garding p, while penalizing those that contain vari-
ous other topics.
3.5 Training
In section 3.3 we introduced 9 ranking scores to
evaluate the appropriateness of a candidate alias for
a given name. Each of the scores is computed
with and without weighting for hubs, resulting in
2? 9 = 18 ranking scores. The ranking scores cap-
ture different statistical properties of candidates; it is
not readily apparent which ranking scores best con-
vey aliases of a name. We use real world name-alias
data to learn the proper combination of the ranking
scores.
We represent each candidate alias as a vector of
the ranking scores. Because we use the 18 rank-
ing scores described above, each candidate is repre-
sented by an 18-dimensional vector. Given a set of
personal names and their aliases, we model the train-
ing process as a preference learning task. For each
name, we impose a binary preference constraint be-
tween the correct alias and each candidate.
For example, let us assume for a name wp we
selected the four candidates a1, a2, a3, a4. With-
out loss of generality, let us further assume that a1
and a2 are the correct aliases of p. Therefore, we
form four partial preferences: a1 ? a3, a1 ? a4,
a2 ? a3 and a2 ? a4. Here, x ? y denotes
the fact that x is preferred to y. We use ranking
SVMs (Joachims, 2002) to learn a ranking function
from preference constraints. Ranking SVMs attempt
to minimize the number of discordant pairs during
training, thereby improving average precision. The
trained SVM model is used to rank a set of candi-
dates extracted for a name. Then the highest ranking
candidate is selected as the alias of the name.
4 Experiments
We crawled Japanese web sites and extracted anchor
texts and urls linked by the anchor texts. A web
site might use links for purely navigational purposes,
which convey no semantic clue. To remove naviga-
tional links in our dataset, we prepare a list of words
that are commonly used in navigational menus, such
as top, last, next, previous, links, etc and remove
anchor texts that contain those words. In addition
we remove any links that point to pages within the
same site. All urls with only one inbound anchor text
are removed from the dataset. After the above men-
tioned processing, the dataset contains 24, 456, 871
anchor texts pointing to 8, 023, 364 urls. The aver-
age number of inbound anchor texts per url is 3.05
and its standard deviation is 54.02. We tokenize
anchor texts using the Japanese morphological an-
alyzer MeCab (Kudo et al, 2004) and select nouns
as nodes in the co-occurrence graph.
For training and evaluation purposes we manually
assigned aliases for 441 Japanese celebrities. The
name-alias dataset covers people from various fields
868
Table 2: Mean Reciprocal Rank
Method MRR Method MRR
SVM (RBF) 0.5625 lf 0.0839
SVM (Linear) 0.5186 cosine 0.0761
SVM (Quad) 0.4898 tfidf 0.0757
SVM (Cubic) 0.4087 Dice 0.0751
tfidf(h) 0.3957 overlap(h) 0.0750
LLR(h) 0.3879 PMI(h) 0.0624
cosine(h) 0.3701 LLR 0.0604
lf(h) 0.3677 HG 0.0399
HG(h) 0.3297 CS 0.0079
Dice(h) 0.2905 PMI 0.0072
CS(h) 0.1186 overlap 0.0056
of cinema, sports, politics and mass-media. The ma-
jority of people in the dataset have only one alias
assigned. For each real name in the dataset we ex-
tract a set of candidates using the proposed method.
We then sort the real names in the dataset accord-
ing to the number of candidates extracted for them.
We select the top 50 real names with the greatest
number of candidate aliases for evaluation purposes
because recognizing the correct alias from numerous
candidates is a more challenging task that enables us
to perform a strict evaluation. On average a name in
our evaluation dataset has 6500 candidates, of which
only one is correct. The rest of the 391 (441 ? 50)
names are used for training.
We compare the proposed method (SVM) against
various baseline ranking scores using mean recip-
rocal rank (MRR) (Baeza-Yates and Ribeiro-Neto,
1999). The MRR is defined as follows;
MRR = 1n
n?
i=1
1
Ri . (2)
Therein, Ri is the rank assigned to a correct alias and
n is the total number of aliases. The MRR is widely
used in information retrieval to evaluate the rank-
ing of search results. Formula 2 gives high MRR to
ranking scores which assign higher ranks to correct
aliases.
Our experimental results are summarized in Ta-
ble 2. The hub weighted versions of ranking scores
are denoted by (h). We trained rank SVMs with
linear SVM (Linear), quadratic SVM (Quad), cubic
SVM (Cubic) and radial basis functions (RBF) SVM
(RBF) kernels. As shown in Table 2, the proposed
SVM-based method has the highest MRR values
among all methods compared. The best results are
obtained with the RBF kernel (SVM RBF). In fact
for 21 out of 50 names in our dataset, SVM (RBF)
correctly ranks their aliases at the first rank. Con-
sidering the fact that each name has more than 6000
candidate aliases, this is a marked improvement over
the baselines. It is noteworthy in Table 2 that the
hub-weighted versions of ranking scores outperform
the corresponding non-weighted version. This jus-
tifies the hub weighting method proposed in sec-
tion 3.4. The hub-weighted tfidf score (tfidf(h)) has
the best MRR among the baseline ranking scores.
For polynomial kernels, we observe a drop of preci-
sion concomitant with the complexity of the kernel,
which occurs as a result of over-fitting.
Table 3 shows the top-three ranked aliases ex-
tracted for Hideki Matsui by various methods. En-
glish translation of words are given within brackets.
The correct alias, Godzilla, is ranked first by SVM
(RBF). Moreover, the correct alias is followed by
the last name Matsui and his team, New York Yan-
kees. In fact, tfidf(h), LLR(h) and lf(h) all have the
exact ranking for the top three candidates. Hide,
which is an abbreviated form of Hideki, is ranked
second by these measures. However, none con-
tains the alias Godzilla among the top three candi-
dates. The non-hub weighted measures tend to in-
clude general terms such as Tokyo, Yomiuri (a pop-
ular Japanese newspaper), Nikkei (a Japanese busi-
ness newspaper), and Tokyo stock exchange. A close
analysis revealed that such general terms frequently
co-occur with a name in hubs. Without adjusting
the co-occurrences coming from hubs, such terms
invariably receive high ranking scores, as shown in
Table 3.
Incorrect tokenization of Japanese names is a
main source of error. Many aliases are out-of-
dictionary (unknown) words, which are known to
produce incorrect tokenizations in Japanese mor-
phological analyzers. Moreover, a name and its
aliases can be written in various scripts: Hiragana,
Katanaka, Kanji, Roman and even combinations of
multiple scripts. Some foreign names such as David
even have orthographic variants in Japanese: da-
bid-do or de-bid-do. Failing to recognize the differ-
ent ways in which a name can be written engenders
wrong preference constraints during training.
869
Table 3: Top ranking candidate aliases for Hideki Matsui
Method First Second Third
SVM (RBF) (Godzilla) (Matsui) (Yankees)
tfidf(h) (Matsui) (Hide) (Yankees)
LLR(h) (Matsui) (Hide) (Yankees)
cosine(h) (Matsui) (Yankees) (Hide)
lf(h) (Matsui) (Hide) (Yankees)
HG(h) (Matsui) (Yankees) (Hide)
Dice(h) (Matsui) (Yankees) (Hide)
CS(h) (Matsui) (Major league) (player)
lf (Tokyo) (Yomiuri) (Nikkei)
cosine (Yomiuri) (Tokyo stock exchange) (Matsui)
tfidf (Yomiuri) (Tokyo) (Tokyo stock exchange)
Dice (Yomiuri) (Tokyo stock exchange) (Matsui)
overlap(h) (play) (Godzilla) (Steinbrenner)
PMI(h) (play) (Godzilla) (Steinbrenner)
LLR (Yomiuri) (Tokyo stock exchange) (jiji.com)
HG (Yomiuri) (Tokyo stock exchange) (Matsui)
CS (jiji.com) (Tokyo stock exchange) (Yomiuri)
PMI (Komdatzien) (picture) (contents)
overlap (Komdatzien) (picture) (contents)
5 Conclusion
We proposed a method to extract aliases of a given
name using anchor texts and link structure. We cre-
ated a co-occurrence graph to represent words in an-
chor texts and modeled the problem of alias extrac-
tion as a one of ranking nodes in this graph with re-
spect to a given name. In future, we intend to apply
the proposed method to extract aliases for other en-
tity types such as products, organizations and loca-
tions.
References
R.A. Baeza-Yates and B.A. Ribeiro-Neto. 1999. Modern
Information Retrieval. ACM Press / Addison-Wesley.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proc. of the World Wide Web Conference (WWW? 05),
pages 463?470.
S. Chakrabarti. 2003. Mining the Web: Discovering
Knowledge from Hypertext Data. Morgan Kaufmann.
Z. Chen, S. Liu, L. Wenyin, Ge. Pu, and W. Ma. 2003.
Building a web thesaurus from web link structure.
In Proc. of the 26th annual international ACM SI-
GIR conference on Research and development in in-
formaion retrieval, pages 48?55.
K. Church and P. Hanks. 1991. Word association norms,
mutual information and lexicography. Computational
Linguistics, 16:22?29.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19:61?74.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
T. Hisamitsu and Y. Niwa. 2001. Topic-word selection
based on combinatorial probability. In Proc. of NL-
PRS?01, pages 289?296.
T. Hokama and H. Kitagawa. 2006. Extracting
mnemonic names of people from the web. In Proc.
of 9th International Conference on Asian Digital Li-
braries (ICADL?06), pages 121?130.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of the ACM conference on
Knowledge Discovery and Data Minning (KDD).
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to japanese morpho-
logical analysis. In Proc. of EMNLP?04.
W. Lu, L. Chien, and H. Lee. 2004. Anchor text mining
for translation of web queries: A transitive translation
approach. ACM Transactions on Information Systems,
22(2):242?269.
G. Salton and M.J. McGill. 1986. Introduction to Mod-
ern Information Retreival. McGraw-Hill Inc., New
York, NY.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of the ACL, pages 417?424.
870
Proceedings of NAACL HLT 2007, pages 340?347,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Integrated Approach to Measuring Semantic Similarity between Words
Using Information available on the Web
Danushka Bollegala
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@mi.ci.i.u-
tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and
Technology
1-18-13, Sotokanda, Tokyo,
101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
ishizuka@i.u-
tokyo.ac.jp
Abstract
Measuring semantic similarity between
words is vital for various applications
in natural language processing, such as
language modeling, information retrieval,
and document clustering. We propose a
method that utilizes the information avail-
able on the Web to measure semantic sim-
ilarity between a pair of words or entities.
We integrate page counts for each word in
the pair and lexico-syntactic patterns that
occur among the top ranking snippets for
the AND query using support vector ma-
chines. Experimental results on Miller-
Charles? benchmark data set show that the
proposed measure outperforms all the ex-
isting web based semantic similarity mea-
sures by a wide margin, achieving a cor-
relation coefficient of 0.834. Moreover,
the proposed semantic similarity measure
significantly improves the accuracy (F -
measure of 0.78) in a named entity cluster-
ing task, proving the capability of the pro-
posed measure to capture semantic simi-
larity using web content.
1 Introduction
The study of semantic similarity between words has
been an integral part of natural language processing
and information retrieval for many years. Semantic
similarity measures are vital for various applications
in natural language processing such as word sense
disambiguation (Resnik, 1999), language model-
ing (Rosenfield, 1996), synonym extraction (Lin,
1998a) and automatic thesaurus extraction (Curran,
2002).
Pre-compiled taxonomies such as WordNet 1 and
text corpora have been used in previous work on se-
mantic similarity (Lin, 1998a; Resnik, 1995; Jiang
and Conrath, 1998; Lin, 1998b). However, seman-
tic similarity between words change over time as
new senses and associations of words are constantly
created. One major issue behind taxonomies and
corpora oriented approaches is that they might not
necessarily capture similarity between proper names
such as named entities (e.g., personal names, loca-
tion names, product names) and the new uses of ex-
isting words. For example, apple is frequently asso-
ciated with computers on the Web but this sense of
apple is not listed in the WordNet. Maintaining an
up-to-date taxonomy of all the new words and new
usages of existing words is costly if not impossible.
The Web can be regarded as a large-scale, dy-
namic corpus of text. Regarding the Web as a live
corpus has become an active research topic recently.
Simple, unsupervised models have shown to per-
form better when n-gram counts are obtained from
the Web rather than from a large corpus (Keller and
Lapata, 2003; Lapata and Keller, 2005). Resnik and
Smith (2003) extract bilingual sentences from the
Web to create parallel corpora for machine trans-
lation. Turney (2001) defines a point wise mutual
information (PMI-IR) measure using the number of
hits returned by a Web search engine to recognize
synonyms. Matsuo et. al, (2006b) follows a similar
1http://wordnet.princeton.edu/
340
approach to measure the similarity between words
and apply their method in a graph-based word clus-
tering algorithm.
Due to the huge number of documents and the
high growth rate of the Web, it is difficult to di-
rectly analyze each individual document separately.
Search engines provide an efficient interface to this
vast information. Page counts and snippets are two
useful information sources provided by most Web
search engines. Page count of a query is the number
of pages that contain the query words 2. A snippet is
a brief window of text extracted by a search engine
around the query term in a document. Snippets pro-
vide useful information about the immediate context
of the query term.
This paper proposes a Web-based semantic simi-
larity metric which combines page counts and snip-
pets using support vector machines. We extract
lexico-syntactic patterns from snippets. For exam-
ple, X is a Y indicates there is a high semantic sim-
ilarity between X and Y. Automatically extracted
lexico-syntactic patterns have been successfully em-
ployed in various term extraction tasks (Hearst,
1992).
Our contributions are summarized as follows:
? We propose a lexico-syntactic patterns-based
approach to compute semantic similarity using
snippets obtained from a Web search engine.
? We integrate different Web-based similarity
scores using WordNet synsets and support vec-
tor machines to create a robust semantic sim-
ilarity measure. The integrated measure out-
performs all existing Web-based semantic sim-
ilarity measures in a benchmark dataset and a
named entity clustering task. To the best of
our knowledge, this is the first attempt to com-
bine both WordNet synsets and Web content to
leverage a robust semantic similarity measure.
2 Previous Work
Given a taxonomy of concepts, a straightforward
method for calculating similarity between two words
(concepts) is to find the length of the shortest path
2page count may not necessarily be equal to the word fre-
quency because the queried word may appear many times in a
page
connecting the two words in the taxonomy (Rada
et al, 1989). If a word is polysemous (i.e., having
more than one sense) then multiple paths may ex-
ist between the two words. In such cases only the
shortest path between any two senses of the words is
considered for the calculation of similarity. A prob-
lem frequently acknowledged with this approach is
that it relies on the notion that all links in the taxon-
omy represent uniform distances.
Resnik (1995) proposes a similarity measure
based on information content. He defines the sim-
ilarity between two concepts C1 and C2 in the tax-
onomy as the maximum of the information content
of all concepts C that subsume both C1 and C2.
Then the similarity between two words are defined
as the maximum of the similarity between any con-
cepts that the words belong to. He uses WordNet as
the taxonomy and information content is calculated
using the Brown corpus.
Li et al, (2003) combines structural semantic in-
formation from a lexical taxonomy and informa-
tion content from a corpus in a non-linear model.
They propose a similarity measure that uses shortest
path length, depth and local density in a taxonomy.
Their experiments using WordNet and the Brown
corpus reports a Pearson correlation coefficient of
0.8914 on the Miller and Charles? (1998) bench-
mark dataset. They do not evaluate their method on
similarities between named entities. Recently, some
work has been carried out on measuring semantic
similarity using web content. Matsuo et al, (2006a)
propose the use of Web hits for the extraction of
communities on the Web. They measure the associ-
ation between two personal names using the overlap
coefficient, calculated based on the number of Web
hits for each individual name and their conjunction.
Sahami et al, (2006) measure semantic similarity
between two queries using the snippets returned for
those queries by a search engine. For each query,
they collect snippets from a search engine and rep-
resent each snippet as a TF-IDF weighted term vec-
tor. Each vector is L2 normalized and the centroid
of the set of vectors is computed. Semantic similar-
ity between two queries is then defined as the inner
product between the corresponding centroid vectors.
They do not compare their similarity measure with
taxonomy based similarity measures.
Chen et al, (2006) propose a web-based double-
341
checking model to compute semantic similarity be-
tween words. For two words P and Q, they col-
lect snippets for each word from a web search en-
gine. Then they count the number of occurrences of
word P in the snippets for word Q and the number
of occurrences of word Q in the snippets for word
P . These values are combined non-linearly to com-
pute the similarity between P and Q. This method
heavily depends on the search engine?s ranking al-
gorithm. Although two words P and Q may be very
similar, there is no reason to believe that one can find
Q in the snippets for P , or vice versa. This observa-
tion is confirmed by the experimental results in their
paper which reports 0 similarity scores for many
pairs of words in the Miller and Charles (1998) data
set.
3 Method
In this section we will describe the various similarity
features we use in our model. We utilize page counts
and snippets returned by the Google 3 search engine
for simple text queries to define various similarity
scores.
3.1 Page Counts-based Similarity Scores
For the rest of this paper we use the notation H(P )
to denote the page count for the query P in a search
engine. Terra and Clarke (2003) compare various
similarity scores for measuring similarity between
words in a corpus. We modify the traditional Jac-
card, overlap (Simpson), Dice and PMI measures
for the purpose of measuring similarity using page
counts. WebJaccard coefficient between words (or
phrases) P and Q, WebJaccard(P,Q), is defined
by,
WebJaccard(P,Q)
=
{ 0 if H(P ?Q) ? c
H(P?Q)
H(P )+H(Q)?H(P?Q) otherwise
.(1)
Here, P ? Q denotes the conjunction query P AND
Q. Given the scale and noise in the Web, some words
might occur arbitrarily, i.e. by random chance, on
some pages. Given the scale and noise in web data, it
is a possible that two words man order to reduce the
adverse effect due to random co-occurrences, we set
3http://www.google.com
the WebJaccard coefficient to zero if the page counts
for the query P ?Q is less than a threshold c. 4
Likewise, we define WebOverlap coefficient,
WebOverlap(P,Q), as,
WebOverlap(P,Q)
=
{ 0 if H(P ?Q) ? c
H(P?Q)
min(H(P ),H(Q)) otherwise
.(2)
We define WebDice as a variant of Dice coeffi-
cient. WebDice(P,Q) is defined as,
WebDice(P,Q)
=
{ 0 if H(P ?Q) ? c
2H(P?Q)
H(P )+H(Q) otherwise
. (3)
We define WebPMI as a variant form of PMI using
page counts by,
WebPMI(P,Q)
=
?
?
?
0 if H(P ?Q) ? c
log2(
H(P?Q)
N
H(P )
N
H(Q)
N
) otherwise .(4)
Here, N is the number of documents indexed by the
search engine. Probabilities in Formula 4 are esti-
mated according to the maximum likelihood princi-
ple. In order to accurately calculate PMI using For-
mula 4, we must know N , the number of documents
indexed by the search engine. Although estimating
the number of documents indexed by a search en-
gine (Bar-Yossef and Gurevich, 2006) is an interest-
ing task itself, it is beyond the scope of this work. In
this work, we set N = 1010 according to the number
of indexed pages reported by Google.
3.2 Snippets-based Synonymous Word
Patterns
Page counts-based similarity measures do not con-
sider the relative distance between P and Q in a page
or the length of the page. Although P and Q occur
in a page they might not be related at all. Therefore,
page counts-based similarity measures are prone to
noise and are not reliable when H(P ?Q) is low. On
the other hand snippets capture the local context of
query words. We propose lexico-syntactic patterns
extracted from snippets as a solution to the problems
with page counts-based similarity measures.
4we set c = 5 in our experiments
342
To illustrate our pattern extraction algorithm con-
sider the following snippet from Google for the
query jaguar AND cat.
?The Jaguar is the largest cat in Western Hemi-
sphere and can subdue a larger prey than can the
puma?
Here, the phrase is the largest indicates a hy-
pernymic relationship between Jaguar and the cat.
Phrases such as also known as, is a, part of, is an ex-
ample of all indicate various of semantic relations.
Such indicative phrases have been successfully ap-
plied in various tasks such as synonym extraction,
hyponym extraction (Hearst, 1992) and fact extrac-
tion (Pasca et al, 2006).
We describe our pattern extraction algorithm in
three steps.
Step 1
We replace the two query terms in a snippet by two
wildcards X and Y. We extract all word n-grams that
contain both X and Y. In our experiments we ex-
tracted n-grams for n = 2 to 5. For example, from
the previous snippet we extract the pattern, X is the
largest X. In order to leverage the pattern extraction
process, we randomly select 5000 pairs of synony-
mous nouns from WordNet synsets. We ignore the
nouns which do not have synonyms in the WordNet.
For nouns with more than one sense, we select syn-
onyms from its dominant sense. For each pair of
synonyms (P,Q), we query Google for ?P? AND
?Q? and download the snippets. Let us call this col-
lection of snippets as the positive corpus. We apply
the above mentioned n-gram based pattern extrac-
tion procedure and count the frequency of each valid
pattern in the positive corpus.
Step 2
Pattern extraction algorithm described in step 1
yields 4, 562, 471 unique patterns. 80%of these pat-
terns occur less than 10 times in the positive corpus.
It is impossible to learn with such a large number of
sparse patterns. Moreover, some patterns might oc-
cur purely randomly in a snippet and are not good
indicators of semantic similarity. To measure the
reliability of a pattern as an indicator of semantic
similarity we employ the following procedure. We
create a set of non-synonymous word-pairs by ran-
domly shuffling the words in our data set of synony-
Table 1: Contingency table
v other than v All
Freq. in positive corpus pv P ? pv P
Freq. in negative corpus nv N ? nv N
mous word-pairs. We check each pair of words in
this newly created data set against WordNet and con-
firm that they do not belong to any of the synsets
in the WordNet. From this procedure we created
5000 non-synonymous pairs of words. For each
non-synonymous word-pair, we query Google for
the conjunction of its words and download snippets.
Let us call this collection of snippets as the nega-
tive corpus. For each pattern generated in step 1, we
count its frequency in the negative corpus.
Step 3
We create a contingency table as shown in Table 1
for each pattern v extracted in step 1 using its fre-
quency pv in positive corpus and nv in negative cor-
pus. In Table 1, P denotes the total frequency of all
patterns in the positive corpus and N denotes that in
the negative corpus.
Using the information in Table 1, we calculate
?2 (Manning and Schu?tze, 2002) value for each pat-
tern as,
?2 = (P +N)(pv(N ? nv)? nv(P ? pv))
2
PN(pv + nv)(P +N ? pv ? nv) .
(5)
We selected the top ranking 200 patterns experimen-
tally as described in section 4.2 according to their ?2
values. Some of the selected patterns are shown in
Table 2.
3.3 Training
For each pair of synonymous and non-synonymous
words in our datasets, we count the frequency of
occurrence of the patterns selected in Step 3. We
normalize the frequency count of each pattern by
dividing from the total frequency of all patterns.
Moreover, we compute the page counts-based fea-
tures as given by formulae (1-4). Using the 200
pattern features and the 4 page counts-based fea-
tures we create 204 dimensional feature vectors for
each training instance in our synonymous and non-
synonymous datasets. We train a two class support
vector machine (SVM) (Vapnik, 1998), where class
343
+1 represents synonymous word-pairs and class
?1 represents non-synonymous word-pairs. Finally,
SVM outputs are converted to posterior probabilities
(Platt, 2000). We consider the posterior probability
of a given pair of words belonging to class +1 as the
semantic similarity between the two words.
4 Experiments
To evaluate the performance of the proposed se-
mantic similarity measure, we conduct two sets of
experiments. Firstly, we compare the similarity
scores produced by the proposed measure against
the Miller-Charles? benchmark dataset. We analyze
the performance of the proposed measure with the
number of snippets and the size of the training data
set. Secondly, we apply the proposed measure in a
real-world named entity clustering task and measure
its performance.
4.1 The Benchmark Dataset
We evaluated the proposed method against Miller-
Charles (1998) dataset, a dataset of 30 5 word-pairs
rated by a group of 38 human subjects. Word-
pairs are rated on a scale from 0 (no similarity) to
4 (perfect synonymy). Miller-Charles? dataset is
a subset of Rubenstein-Goodenough?s (1965) orig-
inal dataset of 65 word-pairs. Although Miller-
Charles? experiment was carried out 25 years
later than Rubenstein-Goodenough?s, two sets of
ratings are highly correlated (Pearson correlation
coefficient=0.97). Therefore, Miller-Charles ratings
can be considered as a reliable benchmark for eval-
uating semantic similarity measures.
4.2 Pattern Selection
We trained a linear kernel SVM with top N pattern
features (ranked according to their ?2 values) and
calculated the Pearson correlation coefficient against
the Miller-Charles? benchmark dataset. Experimen-
tal results are shown in Figure 1. From Figure 1
we select N = 200, where correlation maximizes.
Features with the highest linear kernel weights are
shown in Table 2 alongside with their ?2 values. The
weight of a feature in the linear kernel can be consid-
ered as a rough estimate of the influence it has on the
5Due to the omission of two word-pairs in earlier versions
of WordNet most researchers had used only 28 pairs for evalu-
ations
0 200 400 600 800 1000120014001600180020000.780
0.782
0.784
0.786
0.788
0.790
0.792
0.794
0.796
0.798
0.800
C
or
re
la
tio
n 
C
oe
ffi
ci
en
t (
r)
Number of pattern features (N)
Figure 1: Correlation vs No of pattern features
Table 2: Features with the highest SVM linear ker-
nel weights
feature ?2 SVM weight
WebDice N/A 8.19
X/Y 33459 7.53
X, Y : 4089 6.00
X or Y 3574 5.83
X Y for 1089 4.49
X . the Y 1784 2.99
with X ( Y 1819 2.85
X=Y 2215 2.74
X and Y are 1343 2.67
X of Y 2472 2.56
final SVM output. WebDice has the highest linear
kernel weight followed by a series of patterns-based
features. WebOverlap (rank=18, weight=2.45), We-
bJaccard (rank=66, weight=0.618) and WebPMI
(rank=138, weight=0.0001) are not shown in Table 2
due to space limitations. It is noteworthy that the
pattern features in Table 2 agree with the intuition.
Lexical patterns (e.g., X or Y, X and Y are, X of Y) as
well as syntactic patterns (e.g., bracketing, comma
usage) are extracted by our method.
4.3 Semantic Similarity
We score the word-pairs in Miller-Charles dataset
using the page counts-based similarity measures,
previous work on web-based semantic similarity
measures (Sahami (2006), Chen (2006)) and the
proposed method (SVM). Results are shown in Ta-
ble 4.3. All figures except for the Miller-Charles
ratings are normalized into [0, 1] range for the ease
of comparison 6. Proposed method (SVM) re-
6Pearson correlation coefficient is invariant against a linear
transformation
344
Table 3: Semantic Similarity of Human Ratings and baselines on Miller-Charles dataset
Word Pair Miller- Web Web Web Web Sahami Chen (CODC) Proposed
Charles Jaccard Dice Overlap PMI (2006) (2006) (SVM)
cord-smile 0.13 0.102 0.108 0.036 0.207 0.090 0 0
rooster-voyage 0.08 0.011 0.012 0.021 0.228 0.197 0 0.017
noon-string 0.08 0.126 0.133 0.060 0.101 0.082 0 0.018
glass-magician 0.11 0.117 0.124 0.408 0.598 0.143 0 0.180
monk-slave 0.55 0.181 0.191 0.067 0.610 0.095 0 0.375
coast-forest 0.42 0.862 0.870 0.310 0.417 0.248 0 0.405
monk-oracle 1.1 0.016 0.017 0.023 0 0.045 0 0.328
lad-wizard 0.42 0.072 0.077 0.070 0.426 0.149 0 0.220
forest-graveyard 0.84 0.068 0.072 0.246 0.494 0 0 0.547
food-rooster 0.89 0.012 0.013 0.425 0.207 0.075 0 0.060
coast-hill 0.87 0.963 0.965 0.279 0.350 0.293 0 0.874
car-journey 1.16 0.444 0.460 0.378 0.204 0.189 0.290 0.286
crane-implement 1.68 0.071 0.076 0.119 0.193 0.152 0 0.133
brother-lad 1.66 0.189 0.199 0.369 0.644 0.236 0.379 0.344
bird-crane 2.97 0.235 0.247 0.226 0.515 0.223 0 0.879
bird-cock 3.05 0.153 0.162 0.162 0.428 0.058 0.502 0.593
food-fruit 3.08 0.753 0.765 1 0.448 0.181 0.338 0.998
brother-monk 2.82 0.261 0.274 0.340 0.622 0.267 0.547 0.377
asylum-madhouse 3.61 0.024 0.025 0.102 0.813 0.212 0 0.773
furnace-stove 3.11 0.401 0.417 0.118 1 0.310 0.928 0.889
magician-wizard 3.5 0.295 0.309 0.383 0.863 0.233 0.671 1
journey-voyage 3.84 0.415 0.431 0.182 0.467 0.524 0.417 0.996
coast-shore 3.7 0.786 0.796 0.521 0.561 0.381 0.518 0.945
implement-tool 2.95 1 1 0.517 0.296 0.419 0.419 0.684
boy-lad 3.76 0.186 0.196 0.601 0.631 0.471 0 0.974
automobile-car 3.92 0.654 0.668 0.834 0.427 1 0.686 0.980
midday-noon 3.42 0.106 0.112 0.135 0.586 0.289 0.856 0.819
gem-jewel 3.84 0.295 0.309 0.094 0.687 0.211 1 0.686
Correlation 1 0.259 0.267 0.382 0.548 0.579 0.693 0.834
ports the highest correlation of 0.8129 in our ex-
periments. Our implementation of Co-occurrence
Double Checking (CODC) measure (Chen et al,
2006) reports the second best correlation of 0.6936.
However, CODC measure reports zero similarity for
many word-pairs. This is because for a word-pair
(P,Q), we might not necessarily find Q among the
top snippets for P (and vice versa). CODC mea-
sure returns zero under these conditions. Sahami
et al (2006) is ranked third with a correlation of
0.5797. Among the four page counts based mea-
sures WebPMI reports the highest correlation (r =
0.5489). Overall, the results in Table 4.3 suggest
that snippet-based measures are more accurate than
page counts-based measures in capturing semantic
similarity. This is evident for word-pairs where at
least one of the words is a polysemous word (e.g.,
pairs that include cock, brother). Page counts-based
measures do not consider the context in which the
words appear in a page, thus cannot disambiguate
Table 4: Comparison with taxonomy based methods
Method correlation
Human replication 0.901
Resnik (1995) 0.745
Lin (1998) 0.822
Li et al(2003) 0.891
Edge-counting 0.664
Information content 0.745
Jiang & Conrath (1998) 0.848
proposed (SVM) 0.834
the multiple senses.
As summarized in Table 4.3, proposed method
is comparable with the WordNet based methods.
In fact, the proposed method outperforms simple
WordNet based approaches such as Edge-Counting
and Information Content measures. However, con-
sidering the high correlation between human sub-
jects (0.9), there is still room for improvement.
Figure 2 illustrates the effect of the number
of snippets on the performance of the proposed
345
0 100 200 300 400 500 600 700 800 900 10000.70
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Number of snippets
Figure 2: Correlation vs No of snippets
 500  1000  1500  2000  2500  3000  3500  4000negative examples  500
 1000 1500
 2000 2500
 3000 3500
 4000
positive examples
 0.45 0.5
 0.55 0.6
 0.65 0.7
 0.75 0.8
 0.85
correlation
Figure 3: Correlation vs No of positive and negative
training instances
method. Correlation coefficient steadily improves
with the number of snippets used for extracting pat-
terns. When few snippets are processed only a few
patterns are found, thus the feature vector becomes
sparse, resulting in poor performance. Figure 3 de-
picts the correlation with human ratings for various
combinations of positive and negative training in-
stances. Maximum correlation coefficient of 0.834
is achieved with 1900 positive training examples and
2400 negative training examples. Moreover, Fig-
ure 3 reveals that correlation does not improve be-
yond 2500 positive and negative training examples.
Therefore, we can conclude that 2500 examples are
sufficient to leverage the proposed semantic similar-
ity measure.
4.4 Named Entity Clustering
Measuring semantic similarity between named en-
tities is vital in many applications such as query
expansion (Sahami and Heilman, 2006) and com-
munity mining (Matsuo et al, 2006a). Since most
named entities are not covered by WordNet, simi-
larity measures based on WordNet alne cannot be
Table 5: Performance of named entity clustering
Method Precision Recall F Measure
WebJaccard 0.5926 0.712 0.6147
WebOverlap 0.5976 0.68 0.5965
WebDice 0.5895 0.716 0.6179
WebPMI 0.2649 0.428 0.2916
Sahami (2006) 0.6384 0.668 0.6426
Chen (2006) 0.4763 0.624 0.4984
Proposed 0.7958 0.804 0.7897
used in such tasks. Unlike common English words,
named entities are constantly being created. Manu-
ally maintaining an up-to-date taxonomy of named
entities is costly, if not impossible. The proposed
semantic similarity measure is appealing as it does
not require pre-compiled taxonomies. In order to
evaluate the performance of the proposed measure
in capturing the semantic similarity between named
entities, we set up a named entity clustering task.
We selected 50 person names from 5 categories :
tennis players, golfers, actors, politicians and scien-
tists, (10 names from each category) from the dmoz
directory 7. For each pair of names in our dataset,
we measure the association between the two names
using the proposed method and baselines. We use
group-average agglomerative hierarchical clustering
to cluster the names in our dataset into five clusters.
We employed the B-CUBED metric (Bagga and
Baldwin, 1998) to evaluate the clustering results. As
summarized in Table 5 the proposed method outper-
forms all the baselines with a statistically significant
(p ? 0.01 Tukey HSD) F score of 0.7897.
5 Conclusion
We propose an SVM-based approach to combine
page counts and lexico-syntactic patterns extracted
from snippets to leverage a robust web-based seman-
tic similarity measure. The proposed similarity mea-
sure outperforms existing web-based similarity mea-
sures and competes with models trained on Word-
Net. It requires just 2500 synonymous word-pairs,
automatically extracted from WordNet synsets, for
training. Moreover, the proposed method proves
useful in a named entity clustering task. In future,
we intend to apply the proposed method to automat-
ically extract synonyms from the web.
7http://dmoz.org
346
References
A. Bagga and B. Baldwin. 1998. Entity-based cross doc-
ument coreferencing using the vector space model. In
Proc. of 36th COLING-ACL, pages 79?85.
Z. Bar-Yossef and M. Gurevich. 2006. Random sam-
pling from a search engine?s index. In Proceedings of
15th International World Wide Web Conference.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL 2006, pages 1009?1016.
J. Curran. 2002. Ensemble menthods for automatic the-
saurus extraction. In Proc. of EMNLP.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of 14th COLING,
pages 539?545.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the International Conference on Research in
Computational Linguistics ROCLING X.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
M. Lapata and F. Keller. 2005. Web-based models ofr
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
D. Lin. 1998a. Automatic retreival and clustering of sim-
ilar words. In Proc. of the 17th COLING, pages 768?
774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296?304.
C. D. Manning and H. Schu?tze. 2002. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Y. Matsuo, J. Mori, M. Hamasaki, K. Ishida,
T. Nishimura, H. Takeda, K. Hasida, and M. Ishizuka.
2006a. Polyphonet: An advanced social network ex-
traction system. In Proc. of 15th International World
Wide Web Conference.
Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006b. Graph-based word clustering using web search
engine. In Proc. of EMNLP 2006.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts - step one: the one-million fact extraction chal-
lenge. In Proc. of AAAI-2006.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. Advances in Large Margin Classifiers, pages
61?74.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man and Cyber-
netics, 9(1):17?30.
P. Resnik and N. A. Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
14th International Joint Conference on Aritificial In-
telligence.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information based measure and its application to prob-
lems of ambiguity in natural language. Journal of Ar-
itificial Intelligence Research, 11:95?130.
R. Rosenfield. 1996. A maximum entropy approach to
adaptive statistical modelling. Computer Speech and
Language, 10:187?228.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8:627?633.
M. Sahami and T. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proc. of 15th International World Wide
Web Conference.
E. Terra and C.L.A. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc. of the
NAACL/HLT, pages 165?172.
P. D. Turney. 2001. Minning the web for synonyms:
Pmi-ir versus lsa on toefl. In Proc. of ECML-2001,
pages 491?502.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An approch
for measuring semantic similarity between words us-
ing multiple information sources. IEEE Transactions
on Knowledge and Data Engineering, 15(4):871?882.
347
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 385?392,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Bottom-up Approach to Sentence Ordering
for Multi-document Summarization
Danushka Bollegala Naoaki Okazaki ?
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
{danushka,okazaki}@mi.ci.i.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Mitsuru Ishizuka
Abstract
Ordering information is a difficult but
important task for applications generat-
ing natural-language text. We present
a bottom-up approach to arranging sen-
tences extracted for multi-document sum-
marization. To capture the association and
order of two textual segments (eg, sen-
tences), we define four criteria, chronol-
ogy, topical-closeness, precedence, and
succession. These criteria are integrated
into a criterion by a supervised learning
approach. We repeatedly concatenate two
textual segments into one segment based
on the criterion until we obtain the overall
segment with all sentences arranged. Our
experimental results show a significant im-
provement over existing sentence ordering
strategies.
1 Introduction
Multi-document summarization (MDS) (Radev
and McKeown, 1999) tackles the information
overload problem by providing a condensed ver-
sion of a set of documents. Among a number
of sub-tasks involved in MDS, eg, sentence ex-
traction, topic detection, sentence ordering, infor-
mation extraction, sentence generation, etc., most
MDS systems have been based on an extraction
method, which identifies important textual seg-
ments (eg, sentences or paragraphs) in source doc-
uments. It is important for such MDS systems
to determine a coherent arrangement of the tex-
tual segments extracted from multi-documents in
order to reconstruct the text structure for summa-
rization. Ordering information is also essential for
?Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
other text-generation applications such as Ques-
tion Answering.
A summary with improperly ordered sen-
tences confuses the reader and degrades the qual-
ity/reliability of the summary itself. Barzi-
lay (2002) has provided empirical evidence that
proper order of extracted sentences improves their
readability significantly. However, ordering a
set of sentences into a coherent text is a non-
trivial task. For example, identifying rhetorical
relations (Mann and Thompson, 1988) in an or-
dered text has been a difficult task for computers,
whereas our task is even more complicated: to
reconstruct such relations from unordered sets of
sentences. Source documents for a summary may
have been written by different authors, by different
writing styles, on different dates, and based on dif-
ferent background knowledge. We cannot expect
that a set of extracted sentences from such diverse
documents will be coherent on their own.
Several strategies to determine sentence order-
ing have been proposed as described in section 2.
However, the appropriate way to combine these
strategies to achieve more coherent summaries re-
mains unsolved. In this paper, we propose four
criteria to capture the association of sentences in
the context of multi-document summarization for
newspaper articles. These criteria are integrated
into one criterion by a supervised learning ap-
proach. We also propose a bottom-up approach
in arranging sentences, which repeatedly concate-
nates textual segments until the overall segment
with all sentences arranged, is achieved.
2 Related Work
Existing methods for sentence ordering are di-
vided into two approaches: making use of chrono-
logical information (McKeown et al, 1999; Lin
385
and Hovy, 2001; Barzilay et al, 2002; Okazaki
et al, 2004); and learning the natural order of sen-
tences from large corpora not necessarily based on
chronological information (Lapata, 2003; Barzi-
lay and Lee, 2004). A newspaper usually dissem-
inates descriptions of novel events that have oc-
curred since the last publication. For this reason,
ordering sentences according to their publication
date is an effective heuristic for multidocument
summarization (Lin and Hovy, 2001; McKeown
et al, 1999). Barzilay et al (2002) have proposed
an improved version of chronological ordering by
first grouping sentences into sub-topics discussed
in the source documents and then arranging the
sentences in each group chronologically.
Okazaki et al (2004) have proposed an algo-
rithm to improve chronological ordering by re-
solving the presuppositional information of ex-
tracted sentences. They assume that each sen-
tence in newspaper articles is written on the basis
that presuppositional information should be trans-
ferred to the reader before the sentence is inter-
preted. The proposed algorithm first arranges sen-
tences in a chronological order and then estimates
the presuppositional information for each sentence
by using the content of the sentences placed before
each sentence in its original article. The evaluation
results show that the proposed algorithm improves
the chronological ordering significantly.
Lapata (2003) has suggested a probabilistic
model of text structuring and its application to the
sentence ordering. Her method calculates the tran-
sition probability from one sentence to the next
from a corpus based on the Cartesian product be-
tween two sentences defined using the following
features: verbs (precedent relationships of verbs
in the corpus); nouns (entity-based coherence by
keeping track of the nouns); and dependencies
(structure of sentences). Although she has not
compared her method with chronological order-
ing, it could be applied to generic domains, not re-
lying on the chronological clue provided by news-
paper articles.
Barzilay and Lee (2004) have proposed con-
tent models to deal with topic transition in do-
main specific text. The content models are formal-
ized by Hidden Markov Models (HMMs) in which
the hidden state corresponds to a topic in the do-
main of interest (eg, earthquake magnitude or pre-
vious earthquake occurrences), and the state tran-
sitions capture possible information-presentation
orderings. The evaluation results showed that
their method outperformed Lapata?s approach by a
wide margin. They did not compare their method
with chronological ordering as an application of
multi-document summarization.
As described above, several good strate-
gies/heuristics to deal with the sentence ordering
problem have been proposed. In order to integrate
multiple strategies/heuristics, we have formalized
them in a machine learning framework and have
considered an algorithm to arrange sentences us-
ing the integrated strategy.
3 Method
We define notation a ? b to represent that sen-
tence a precedes sentence b. We use the term seg-
ment to describe a sequence of ordered sentences.
When segment A consists of sentences a1, a2, ...,
am in this order, we denote as:
A = (a1 ? a2 ? ... ? am). (1)
The two segments A and B can be ordered either
B after A or A after B. We define the notation
A ? B to show that segment A precedes segment
B.
Let us consider a bottom-up approach in arrang-
ing sentences. Starting with a set of segments ini-
tialized with a sentence for each, we concatenate
two segments, with the strongest association (dis-
cussed later) of all possible segment pairs, into
one segment. Repeating the concatenating will
eventually yield a segment with all sentences ar-
ranged. The algorithm is considered as a variation
of agglomerative hierarchical clustering with the
ordering information retained at each concatenat-
ing process.
The underlying idea of the algorithm, a bottom-
up approach to text planning, was proposed by
Marcu (1997). Assuming that the semantic units
(sentences) and their rhetorical relations (eg, sen-
tence a is an elaboration of sentence d) are given,
he transcribed a text structuring task into the prob-
lem of finding the best discourse tree that satisfied
the set of rhetorical relations. He stated that global
coherence could be achieved by satisfying local
coherence constraints in ordering and clustering,
thereby ensuring that the resultant discourse tree
was well-formed.
Unfortunately, identifying the rhetorical rela-
tion between two sentences has been a difficult
386
a
A B C D
b c d
E = (b a)
G = (b a c d)
F = (c d)
Segments
Sentences
f (as
soci
ation
 stre
ngth
)
Figure 1: Arranging four sentences A, B, C, and
D with a bottom-up approach.
task for computers. However, the bottom-up algo-
rithm for arranging sentences can still be applied
only if the direction and strength of the associa-
tion of the two segments (sentences) are defined.
Hence, we introduce a function f(A ? B) to rep-
resent the direction and strength of the association
of two segments A and B,
f(A ? B) =
{ p (if A precedes B)
0 (if B precedes A) , (2)
where p (0 ? p ? 1) denotes the association
strength of the segments A and B. The associa-
tion strengths of the two segments with different
directions, eg, f(A ? B) and f(B ? A), are not
always identical in our definition,
f(A ? B) 6= f(B ? A). (3)
Figure 1 shows the process of arranging four
sentences a, b, c, and d. Firstly, we initialize four
segments with a sentence for each,
A = (a), B = (b), C = (c), D = (d). (4)
Suppose that f(B ? A) has the highest value of
all possible pairs, eg, f(A ? B), f(C ? D), etc,
we concatenate B and A to obtain a new segment,
E = (b ? a). (5)
Then we search for the segment pair with the
strongest association. Supposing that f(C ? D)
has the highest value, we concatenate C and D to
obtain a new segment,
F = (c ? d). (6)
Finally, comparing f(E ? F ) and f(F ? E), we
obtain the global sentence ordering,
G = (b ? a ? c ? d). (7)
In the above description, we have not defined
the association of the two segments. The previ-
ous work described in Section 2 has addressed the
association of textual segments (sentences) to ob-
tain coherent orderings. We define four criteria to
capture the association of two segments: chronol-
ogy; topical-closeness; precedence; and succes-
sion. These criteria are integrated into a function
f(A ? B) by using a machine learning approach.
The rest of this section explains the four criteria
and an integration method with a Support Vector
Machine (SVM) (Vapnik, 1998) classifier.
3.1 Chronology criterion
Chronology criterion reflects the chronological or-
dering (Lin and Hovy, 2001; McKeown et al,
1999), which arranges sentences in a chronologi-
cal order of the publication date. We define the as-
sociation strength of arranging segments B after A
measured by a chronology criterion fchro(A ? B)
in the following formula,
fchro(A ? B)
=
?
???
???
1 T(am) < T(b1)
1 [D(am) = D(b1)] ? [N(am) < N(b1)]
0.5 [T(am) = T(b1)] ? [D(am) 6= D(b1)]
0 otherwise
.
(8)
Here, am represents the last sentence in segment
A; b1 represents the first sentence in segment B;
T (s) is the publication date of the sentence s;
D(s) is the unique identifier of the document to
which sentence s belongs: and N(s) denotes the
line number of sentence s in the original docu-
ment. The chronological order of arranging seg-
ment B after A is determined by the comparison
between the last sentence in the segment A and the
first sentence in the segment B.
The chronology criterion assesses the appropri-
ateness of arranging segment B after A if: sen-
tence am is published earlier than b1; or sentence
am appears before b1 in the same article. If sen-
tence am and b1 are published on the same day but
appear in different articles, the criterion assumes
the order to be undefined. If none of the above
conditions are satisfied, the criterion estimates that
segment B will precede A.
3.2 Topical-closeness criterion
The topical-closeness criterion deals with the as-
sociation, based on the topical similarity, of two
387
a1a2
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....a3a4
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
b1
b2
b3
b3
b2
b1 Pb1 Pb2 Pb3
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
Segment A
?
Segment B
Original articlefor sentence b Original articlefor sentence b2 Original articlefor sentence b31
max
average
maxmax
Figure 2: Precedence criterion
segments. The criterion reflects the ordering strat-
egy proposed by Barzilay et al(2002), which
groups sentences referring to the same topic. To
measure the topical closeness of two sentences, we
represent each sentence with a vector whose ele-
ments correspond to the occurrence1 of the nouns
and verbs in the sentence. We define the topical
closeness of two segments A and B as follows,
ftopic(A ? B) = 1|B|
?
b?B
max
a?A
sim(a, b). (9)
Here, sim(a, b) denotes the similarity of sentences
a and b, which is calculated by the cosine similar-
ity of two vectors corresponding to the sentences.
For sentence b ? B, maxa?A sim(a, b) chooses
the sentence a ? A most similar to sentence b and
yields the similarity. The topical-closeness crite-
rion ftopic(A ? B) assigns a higher value when
the topic referred by segment B is the same as seg-
ment A.
3.3 Precedence criterion
Let us think of the case where we arrange seg-
ment A before B. Each sentence in segment B
has the presuppositional information that should
be conveyed to a reader in advance. Given sen-
tence b ? B, such presuppositional information
may be presented by the sentences appearing be-
fore the sentence b in the original article. How-
ever, we cannot guarantee whether a sentence-
extraction method for multi-document summa-
rization chooses any sentences before b for a sum-
mary because the extraction method usually deter-
1The vector values are represented by boolean values, i.e.,
1 if the sentence contains a word, otherwise 0.
a1a2
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....a3 .... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
b
b2
b3
a3
a2
a1 S a1 S a2 S a3
. ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
Segment A
?
Segment B
Original articlefor sentence a1 Original articlefor sentence a2 Original articlefor sentence a3
max
average
maxmax
.... .. .. .... .. ....... ......1
Figure 3: Succession criterion
mines a set of sentences, within the constraint of
summary length, that maximizes information cov-
erage and excludes redundant information. Prece-
dence criterion measures the substitutability of the
presuppositional information of segment B (eg,
the sentences appearing before sentence b) as seg-
ment A. This criterion is a formalization of the
sentence-ordering algorithm proposed by Okazaki
et al (2004).
We define the precedence criterion in the fol-
lowing formula,
fpre(A ? B) = 1|B|
?
b?B
max
a?A,p?Pb
sim(a, p).
(10)
Here, Pb is a set of sentences appearing before sen-
tence b in the original article; and sim(a, b) de-
notes the cosine similarity of sentences a and b
(defined as in the topical-closeness criterion). Fig-
ure 2 shows an example of calculating the prece-
dence criterion for arranging segment B after A.
We approximate the presuppositional information
for sentence b by sentences Pb, ie, sentences ap-
pearing before the sentence b in the original arti-
cle. Calculating the similarity among sentences in
Pb and A by the maximum similarity of the pos-
sible sentence combinations, Formula 10 is inter-
preted as the average similarity of the precedent
sentences ?Pb(b ? B) to the segment A.
3.4 Succession criterion
The idea of succession criterion is the exact op-
posite of the precedence criterion. The succession
criterion assesses the coverage of the succedent in-
formation for segment A by arranging segment B
388
ab
c
d
Partitioning point
 segment before the
 partitioning point
segment after the
partitioning point
Partitioning 
window
Figure 4: Partitioning a human-ordered extract
into pairs of segments
after A:
fsucc(A ? B) = 1|A|
?
a?A
max
s?Sa,b?B
sim(s, b).
(11)
Here, Sa is a set of sentences appearing after sen-
tence a in the original article; and sim(a, b) de-
notes the cosine similarity of sentences a and b
(defined as in the topical-closeness criterion). Fig-
ure 3 shows an example of calculating the succes-
sion criterion to arrange segments B after A. The
succession criterion measures the substitutability
of the succedent information (eg, the sentences ap-
pearing after the sentence a ? A) as segment B.
3.5 SVM classifier to assess the integrated
criterion
We integrate the four criteria described above
to define the function f(A ? B) to represent
the association direction and strength of the two
segments A and B (Formula 2). More specifi-
cally, given the two segments A and B, function
f(A ? B) is defined to yield the integrated asso-
ciation strength from four values, fchro(A ? B),
ftopic(A ? B), fpre(A ? B), and fsucc(A ? B).
We formalize the integration task as a binary clas-
sification problem and employ a Support Vector
Machine (SVM) as the classifier. We conducted a
supervised learning as follows.
We partition a human-ordered extract into pairs
each of which consists of two non-overlapping
segments. Let us explain the partitioning process
taking four human-ordered sentences, a ? b ?
c ? d shown in Figure 4. Firstly, we place the
partitioning point just after the first sentence a.
Focusing on sentence a arranged just before the
partition point and sentence b arranged just after
we identify the pair {(a), (b)} of two segments
(a) and (b). Enumerating all possible pairs of two
segments facing just before/after the partitioning
point, we obtain the following pairs, {(a), (b ?
c)} and {(a), (b ? c ? d)}. Similarly, segment
+1 : [fchro(A ? B), ftopic(A ? B), fpre(A ? B), fsucc(A ? B)]
?1 : [fchro(B ? A), ftopic(B ? A), fpre(B ? A), fsucc(B ? A)]
Figure 5: Two vectors in a training data generated
from two ordered segments A ? B
pairs, {(b), (c)}, {(a ? b), (c)}, {(b), (c ? d)},
{(a ? b), (c ? d)}, are obtained from the parti-
tioning point between sentence b and c. Collect-
ing the segment pairs from the partitioning point
between sentences c and d (i.e., {(c), (d)}, {(b ?
c), (d)} and {(a ? b ? c), (d)}), we identify ten
pairs in total form the four ordered sentences. In
general, this process yields n(n2?1)/6 pairs from
ordered n sentences. From each pair of segments,
we generate one positive and one negative training
instance as follows.
Given a pair of two segments A and B arranged
in an order A ? B, we calculate four values,
fchro(A ? B), ftopic(A ? B), fpre(A ? B),
and fsucc(A ? B) to obtain the instance with
the four-dimensional vector (Figure 5). We label
the instance (corresponding to A ? B) as a posi-
tive class (ie, +1). Simultaneously, we obtain an-
other instance with a four-dimensional vector cor-
responding to B ? A. We label it as a negative
class (ie, ?1). Accumulating these instances as
training data, we obtain a binary classifier by using
a Support Vector Machine with a quadratic kernel.
The SVM classifier yields the association direc-
tion of two segments (eg, A ? B or B ? A) with
the class information (ie, +1 or ?1). We assign
the association strength of two segments by using
the class probability estimate that the instance be-
longs to a positive (+1) class. When an instance
is classified into a negative (?1) class, we set the
association strength as zero (see the definition of
Formula 2).
4 Evaluation
We evaluated the proposed method by using the
3rd Text Summarization Challenge (TSC-3) cor-
pus2. The TSC-3 corpus contains 30 sets of ex-
tracts, each of which consists of unordered sen-
tences3 extracted from Japanese newspaper arti-
cles relevant to a topic (query). We arrange the
extracts by using different algorithms and evaluate
2http://lr-www.pi.titech.ac.jp/tsc/tsc3-en.html
3Each extract consists of ca. 15 sentences on average.
389
Table 1: Correlation between two sets of human-
ordered extracts
Metric Mean Std. Dev Min Max
Spearman 0.739 0.304 -0.2 1
Kendall 0.694 0.290 0 1
Average Continuity 0.401 0.404 0.001 1
the readability of the ordered extracts by a subjec-
tive grading and several metrics.
In order to construct training data applica-
ble to the proposed method, we asked two hu-
man subjects to arrange the extracts and obtained
30(topics) ? 2(humans) = 60 sets of ordered
extracts. Table 1 shows the agreement of the or-
dered extracts between the two subjects. The cor-
relation is measured by three metrics, Spearman?s
rank correlation, Kendall?s rank correlation, and
average continuity (described later). The mean
correlation values (0.74 for Spearman?s rank cor-
relation and 0.69 for Kendall?s rank correlation)
indicate a certain level of agreement in sentence
orderings made by the two subjects. 8 out of 30
extracts were actually identical.
We applied the leave-one-out method to the pro-
posed method to produce a set of sentence or-
derings. In this experiment, the leave-out-out
method arranges an extract by using an SVM
model trained from the rest of the 29 extracts. Re-
peating this process 30 times with a different topic
for each iteration, we generated a set of 30 ex-
tracts for evaluation. In addition to the proposed
method, we prepared six sets of sentence orderings
produced by different algorithms for comparison.
We describe briefly the seven algorithms (includ-
ing the proposed method):
Agglomerative ordering (AGL) is an ordering
arranged by the proposed method;
Random ordering (RND) is the lowest anchor,
in which sentences are arranged randomly;
Human-made ordering (HUM) is the highest
anchor, in which sentences are arranged by
a human subject;
Chronological ordering (CHR) arranges sen-
tences with the chronology criterion defined
in Formula 8. Sentences are arranged in
chronological order of their publication date;
Topical-closeness ordering (TOP) arranges sen-
tences with the topical-closeness criterion de-
fined in Formula 9;
0 20 40 60 80 100
UnacceptablePoorAcceptablePerfect
HUM
AGL
CHR
RND
%
Figure 6: Subjective grading
Precedence ordering (PRE) arranges sentences
with the precedence criterion defined in For-
mula 10;
Suceedence ordering (SUC) arranges sentences
with the succession criterion defined in For-
mula 11.
The last four algorithms (CHR, TOP, PRE, and
SUC) arrange sentences by the corresponding cri-
terion alone, each of which uses the association
strength directly to arrange sentences without the
integration of other criteria. These orderings are
expected to show the performance of each expert
independently and their contribution to solving the
sentence ordering problem.
4.1 Subjective grading
Evaluating a sentence ordering is a challenging
task. Intrinsic evaluation that involves human
judges to rank a set of sentence orderings is a nec-
essary approach to this task (Barzilay et al, 2002;
Okazaki et al, 2004). We asked two human judges
to rate sentence orderings according to the follow-
ing criteria. A perfect summary is a text that we
cannot improve any further by re-ordering. An ac-
ceptable summary is one that makes sense and is
unnecessary to revise even though there is some
room for improvement in terms of readability. A
poor summary is one that loses a thread of the
story at some places and requires minor amend-
ment to bring it up to an acceptable level. An un-
acceptable summary is one that leaves much to be
improved and requires overall restructuring rather
than partial revision. To avoid any disturbance in
rating, we inform the judges that the summaries
were made from a same set of extracted sentences
and only the ordering of sentences is different.
Figure 6 shows the distribution of the subjective
grading made by two judges to four sets of order-
ings, RND, CHR, AGL and HUM. Each set of or-
390
Teval = (e ? a ? b ? c ? d)
Tref = (a ? b ? c ? d ? e)
Figure 7: An example of an ordering under evalu-
ation Teval and its reference Tref .
derings has 30(topics) ? 2(judges) = 60 ratings.
Most RND orderings are rated as unacceptable.
Although CHR and AGL orderings have roughly
the same number of perfect orderings (ca. 25%),
the AGL algorithm gained more acceptable order-
ings (47%) than the CHR alghrotihm (30%). This
fact shows that integration of CHR experts with
other experts worked well by pushing poor order-
ing to an acceptable level. However, a huge gap
between AGL and HUM orderings was also found.
The judges rated 28% AGL orderings as perfect
while the figure rose as high as 82% for HUM
orderings. Kendall?s coefficient of concordance
(Kendall?s W ), which asses the inter-judge agree-
ment of overall ratings, reported a higher agree-
ment between the two judges (W = 0.939).
4.2 Metrics for semi-automatic evaluation
We also evaluated sentence orderings by reusing
two sets of gold-standard orderings made for the
training data. In general, subjective grading con-
sumes much time and effort, even though we
cannot reproduce the evaluation afterwards. The
previous studies (Barzilay et al, 2002; Lapata,
2003) employ rank correlation coefficients such
as Spearman?s rank correlation and Kendall?s rank
correlation, assuming a sentence ordering to be
a rank. Okazaki et al (2004) propose a metric
that assess continuity of pairwise sentences com-
pared with the gold standard. In addition to Spear-
man?s and Kendall?s rank correlation coefficients,
we propose an average continuity metric, which
extends the idea of the continuity metric to contin-
uous k sentences.
A text with sentences arranged in proper order
does not interrupt a human?s reading while moving
from one sentence to the next. Hence, the qual-
ity of a sentence ordering can be estimated by the
number of continuous sentences that are also re-
produced in the reference sentence ordering. This
is equivalent to measuring a precision of continu-
ous sentences in an ordering against the reference
ordering. We define Pn to measure the precision of
Table 2: Comparison with human-made ordering
Method Spearman Kendall Average
coefficient coefficient Continuity
RND -0.127 -0.069 0.011
TOP 0.414 0.400 0.197
PRE 0.415 0.428 0.293
SUC 0.473 0.476 0.291
CHR 0.583 0.587 0.356
AGL 0.603 0.612 0.459
n continuous sentences in an ordering to be evalu-
ated as,
Pn = mN ? n+ 1 . (12)
Here, N is the number of sentences in the refer-
ence ordering; n is the length of continuous sen-
tences on which we are evaluating; m is the num-
ber of continuous sentences that appear in both the
evaluation and reference orderings. In Figure 7,
the precision of 3 continuous sentences P3 is cal-
culated as:
P3 = 25? 3 + 1 = 0.67. (13)
The Average Continuity (AC) is defined as the
logarithmic average of Pn over 2 to k:
AC = exp
(
1
k ? 1
k?
n=2
log(Pn + ?)
)
. (14)
Here, k is a parameter to control the range of the
logarithmic average; and ? is a small value in case
if Pn is zero. We set k = 4 (ie, more than five
continuous sentences are not included for evalua-
tion) and ? = 0.01. Average Continuity becomes
0 when evaluation and reference orderings share
no continuous sentences and 1 when the two or-
derings are identical. In Figure 7, Average Conti-
nuity is calculated as 0.63. The underlying idea of
Formula 14 was proposed by Papineni et al (2002)
as the BLEU metric for the semi-automatic evalu-
ation of machine-translation systems. The origi-
nal definition of the BLEU metric is to compare a
machine-translated text with its reference transla-
tion by using the word n-grams.
4.3 Results of semi-automatic evaluation
Table 2 reports the resemblance of orderings pro-
duced by six algorithms to the human-made ones
with three metrics, Spearman?s rank correlation,
Kendall?s rank correlation, and Average Continu-
ity. The proposed method (AGL) outperforms the
391
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
AGLCHRSUCPRETOPRND
8765432
P
re
ci s i o
n P
n
Length n
Figure 8: Precision vs unit of measuring continu-
ity.
rest in all evaluation metrics, although the chrono-
logical ordering (CHR) appeared to play the major
role. The one-way analysis of variance (ANOVA)
verified the effects of different algorithms for sen-
tence orderings with all metrics (p < 0.01). We
performed Tukey Honest Significant Differences
(HSD) test to compare differences among these al-
gorithms. The Tukey test revealed that AGL was
significantly better than the rest. Even though we
could not compare our experiment with the prob-
abilistic approach (Lapata, 2003) directly due to
the difference of the text corpora, the Kendall co-
efficient reported higher agreement than Lapata?s
experiment (Kendall=0.48 with lemmatized nouns
and Kendall=0.56 with verb-noun dependencies).
Figure 8 shows precision Pn with different
length values of continuous sentence n for the six
methods compared in Table 2. The number of
continuous sentences becomes sparse for a higher
value of length n. Therefore, the precision values
decrease as the length n increases. Although RND
ordering reported some continuous sentences for
lower n values, no continuous sentences could be
observed for the higher n values. Four criteria de-
scribed in Section 3 (ie, CHR, TOP, PRE, SUC)
produce segments of continuous sentences at all
values of n.
5 Conclusion
We present a bottom-up approach to arrange sen-
tences extracted for multi-document summariza-
tion. Our experimental results showed a signif-
icant improvement over existing sentence order-
ing strategies. However, the results also implied
that chronological ordering played the major role
in arranging sentences. A future direction of this
study would be to explore the application of the
proposed framework to more generic texts, such
as documents without chronological information.
Acknowledgment
We used Mainichi Shinbun and Yomiuri Shinbun
newspaper articles, and the TSC-3 test collection.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence order-
ing in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. Proceedings of
the annual meeting of ACL, 2003., pages 545?552.
C.Y. Lin and E. Hovy. 2001. Neats:a multidocument
summarizer. Proceedings of the Document Under-
standing Workshop(DUC).
W. Mann and S. Thompson. 1988. Rhetorical structure
theory: Toward a functional theory of text organiza-
tion. Text, 8:243?281.
Daniel Marcu. 1997. From local to global coherence:
A bottom-up approach to text planning. In Proceed-
ings of the 14th National Conference on Artificial
Intelligence, pages 629?635, Providence, Rhode Is-
land.
Kathleen McKeown, Judith Klavans, Vasileios Hatzi-
vassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by
reformulation: Progress and prospects. AAAI/IAAI,
pages 453?460.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2004. Improving chronological sentence
ordering by precedence relation. In Proceedings
of 20th International Conference on Computational
Linguistics (COLING 04), pages 750?756.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu:a method for automatic eval-
uation of machine translation. Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311?318.
Dragomir R. Radev and Kathy McKeown. 1999.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24:469?500.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
392
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extracting Key Phrases to Disambiguate
Personal Name Queries in Web Search
Danushka Bollegala Yutaka Matsuo ?
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
danushka@mi.ci.i.u-tokyo.ac.jp
y.matsuo@aist.go.jp
ishizuka@i.u-tokyo.ac.jp
Mitsuru Ishizuka
Abstract
Assume that you are looking for informa-
tion about a particular person. A search
engine returns many pages for that per-
son?s name. Some of these pages may
be on other people with the same name.
One method to reduce the ambiguity in the
query and filter out the irrelevant pages, is
by adding a phrase that uniquely identi-
fies the person we are interested in from
his/her namesakes. We propose an un-
supervised algorithm that extracts such
phrases from the Web. We represent each
document by a term-entity model and clus-
ter the documents using a contextual sim-
ilarity metric. We evaluate the algorithm
on a dataset of ambiguous names. Our
method outperforms baselines, achieving
over 80% accuracy and significantly re-
duces the ambiguity in a web search task.
1 Introduction
The Internet has grown into a collection of bil-
lions of web pages. Web search engines are im-
portant interfaces to this vast information. We
send simple text queries to search engines and re-
trieve web pages. However, due to the ambigu-
ities in the queries, a search engine may return
a lot of irrelevant pages. In the case of personal
name queries, we may receive web pages for other
people with the same name (namesakes). For ex-
ample, if we search Google 1 for Jim Clark, even
among the top 100 results we find at least eight
different Jim Clarks. The two popular namesakes;
?National Institute of Advanced Industrial Science and
Technology
1www.google.com
Jim Clark the Formula one world champion (46
pages), and Jim Clark the founder of Netscape (26
pages), cover the majority of the pages. What if
we are interested only in the Formula one world
champion and want to filter out the pages for the
other Jim Clarks? One solution is to modify our
query by including a phrase such as Formula one
or racing driver with the name, Jim Clark.
This paper presents an automatic method to ex-
tract such phrases from the Web. We follow a
three-stage approach. In the first stage we rep-
resent each document containing the ambiguous
name by a term-entity model, as described in sec-
tion 5.2. We define a contextual similarity metric
based on snippets returned by a search engine, to
calculate the similarity between term-entity mod-
els. In the second stage, we cluster the documents
using the similarity metric. In the final stage, we
select key phrases from the clusters that uniquely
identify each namesake.
2 Applications
Two tasks that can readily benefit from automat-
ically extracted key phrases to disambiguate per-
sonal names are query suggestion and social net-
work extraction. In query suggestion (Gauch and
Smith, 1991), the search engine returns a set of
phrases to the user alongside with the search re-
sults. The user can then modify the original query
using these phrases to narrow down the search.
Query suggestion helps the users to easily navigate
through the result set. For personal name queries,
the key phrases extracted by our algorithm can be
used as suggestions to reduce the ambiguity and
narrow down the search on a particular namesake.
Social networking services (SNSs) have been
given much attention on the Web recently. As
a kind of online applications, SNSs can be used
17
to register and share personal information among
friends and communities. There have been recent
attempts to extract social networks using the infor-
mation available on the Web 2(Mika, 2004; Mat-
suo et al, 2006). In both Matsuo?s (2006) and
Mika?s (2004) algorithms, each person is repre-
sented by a node in the social network and the
strength of the relationship between two people
is represented by the length of the edge between
the corresponding two nodes. As a measure of the
strength of the relationship between two people A
and B, these algorithms use the number of hits ob-
tained for the query A AND B. However, this ap-
proach fails when A or B has namesakes because
the number of hits in these cases includes the hits
for the namesakes. To overcome this problem, we
could include phrases in the query that uniquely
identify A and B from their namesakes.
3 Related Work
Person name disambiguation can be seen as
a special case of word sense disambiguation
(WSD) (Schutze, 1998; McCarthy et al, 2004)
problem which has been studied extensively in
Natural Language Understanding. However, there
are several fundamental differences between WSD
and person name disambiguation. WSD typically
concentrates on disambiguating between 2-4 pos-
sible meanings of the word, all of which are a
priori known. However, in person name disam-
biguation in Web, the number of different name-
sakes can be much larger and unknown. From a
resource point of view, WSD utilizes sense tagged
dictionaries such as WordNet, whereas no dictio-
nary can provide information regarding different
namesakes for a particular name.
The problem of person name disambiguation
has been addressed in the domain of research pa-
per citations (Han et al, 2005), with various super-
vised methods proposed for its solution. However,
citations have a fixed format compared to free text
on the Web. Fields such as co-authors, title, jour-
nal name, conference name, year of publication
can be easily extracted from a citation and provide
vital information to the disambiguation process.
Research on multi-document person name res-
olution (Bagga and Baldwin, 1998; Mann and
Yarowsky, 2003; Fleischman and Hovy, 2004) fo-
cuses on the related problem of determining if
2http://flink.sematicweb.org/. The system won the 1st
place at the Semantic Web Challenge in ISWC2004.
two instances with the same name and from dif-
ferent documents refer to the same individual.
Bagga and Baldwin (1998) first perform within-
document coreference resolution to form coref-
erence chains for each entity in each document.
They then use the text surrounding each reference
chain to create summaries about each entity in
each document. These summaries are then con-
verted to a bag of words feature vector and are
clustered using standard vector space model of-
ten employed in IR. The use of simplistic bag of
words clustering is an inherently limiting aspect of
their methodology. On the other hand, Mann and
Yarowsky (2003) proposes a richer document rep-
resentation involving automatically extracted fea-
tures. However, their clustering technique can be
basically used only for separating two people with
the same name. Fleischman and Hovy (2004) con-
structs a maximum entropy classifier to learn dis-
tances between documents that are then clustered.
Their method requires a large training set.
Pedersen et al (2005) propose an unsupervised
approach to resolve name ambiguity by represent-
ing the context of an ambiguous name using sec-
ond order context vectors derived using singular
value decomposition (SVD) on a co-occurrence
matrix. They agglomeratively cluster the vec-
tors using cosine similarity. They evaluate their
method only on a conflated dataset of pseudo-
names, which begs the question of how well such
a technique would fair on a more real-world chal-
lenge. Li et al (2005) propose two approaches to
disambiguate entities in a set of documents: a su-
pervisedly trained pairwise classifier and an unsu-
pervised generative model. However, they do not
evaluate the effectiveness of their method in Web
search.
Bekkerman and McCallum (2005) present two
unsupervised methods for finding web pages re-
ferring to a particular person: one based on
link structure and another using Agglomera-
tive/Conglomerative Double Clustering (A/CDC).
Their scenario focuses on simultaneously disam-
biguating an existing social network of people,
who are closely related. Therefore, their method
cannot be applied to disambiguate an individual
whose social network (for example, friends, col-
leagues) is not known. Guha and Grag (2004)
present a re-ranking algorithm to disambiguate
people. The algorithm requires a user to select one
of the returned pages as a starting point. Then,
18
Table 1: Data set for experiments
Collection No of namesakes
person-X 4
Michael Jackson 3
Jim Clark 8
William Cohen 10
through comparing the person descriptions, the al-
gorithm re-ranks the entire search results in such
a way that pages referring to the same person de-
scribed in the user-selected page are ranked higher.
A user needs to browse the documents in order to
find which matches the user?s intended referent,
which puts an extra burden on the user.
None of the above mentioned works attempt to
extract key phrases to disambiguate person name
queries, a contrasting feature in our work.
4 Data Set
We select three ambiguous names (Micheal Jack-
son, William Cohen and Jim Clark) that appear in
previous work in name resolution. For each name
we query Google with the name and download top
100 pages. We manually classify each page ac-
cording to the namesakes discussed in the page.
We ignore pages which we could not decide the
namesake from the content. We also remove pages
with images that do not contain any text. No pages
were found where more than one namesakes of a
name appear. For automated pseudo-name evalua-
tion purposes, we select four names (Bill Clinton,
Bill Gates, Tom Cruise and Tiger Woods) for con-
flation, who we presumed had one vastly predom-
inant sense. We download 100 pages from Google
for each person. We replace the name of the per-
son by ?person-X? in the collection, thereby intro-
ducing ambiguity. The structure of our dataset is
shown in Table 1.
5 Method
5.1 Problem Statement
Given a collection of documents relevant to an am-
biguous name, we assume that each document in
the collection contains exactly one namesake of
the ambiguous name. This is a fair assumption
considering the fact that although namesakes share
a common name, they specializes in different
fields and have different Web appearances. More-
over, the one-to-one association between docu-
ments and people formed by this assumption, let
us model the person name disambiguation prob-
lem as a one of hard-clustering of documents.
The outline of our method is as following;
Given a set of documents representing a group of
people with the same name, we represent each
document in the collection using a Term-Entity
model (section 5.2). We define a contextual sim-
ilarity metric (section 5.4) and then cluster (sec-
tion 5.5) the term-entity models using the contex-
tual similarity between them. Each cluster is con-
sidered to be representing a different namesake.
Finally, key phrases that uniquely identify each
namesake are selected from the clusters. We per-
form experiments at each step of our method to
evaluate its performance.
5.2 Term-Entity Model
The first step toward disambiguating a personal
name is to identify the discriminating features of
one person from another. In this paper we propose
Term-Entity models to represent a person in a doc-
ument.
Definition. A term-entity model T (A), represent-
ing a person A in a document D, is a boolean
expression of n literals a1, a2, . . . , an. Here, a
boolean literal ai is a multi-word term or a named
entity extracted from the document D.
For simplicity, we only consider boolean ex-
pressions that combine the literals through AND
operator.
The reasons for using terms as well as named
entities in our model are two fold. Firstly, there are
multi-word phrases such as secretary of state, rac-
ing car driver which enable us to describe a person
uniquely but not recognized by named entity tag-
gers. Secondly, automatic term extraction (Frantzi
and Ananiadou, 1999) can be done using statistical
methods and does not require extensive linguistic
resources such as named entity dictionaries, which
may not be available for some domains.
5.3 Creating Term-Entity Models
We extract terms and named entities from each
document to build the term-entity model for that
document. For automatic multi-word term ex-
traction, we use the C-value metric proposed by
Frantzi et al (1999). Firstly, the text from which
we need to extract terms is tagged using a part
of speech tagger. Then a linguistic filter and a
stop words list constrain the word sequences that
19
020
40
60
80
100
120 President of the United States
George Bush
pr
es
id
en
tia
l
ge
or
ge
pr
es
id
en
t
ne
ws
bi
og
ra
ph
y
ga
m
es
bu
sh
bu
sh
s
lib
ra
ry
fa
th
er
vic
e
go
ve
rn
m
en
t
pr
es
id
en
ts
sh
al
l
un
ite
d
st
at
es
ex
ec
ut
ive
Figure 1: Distribution of words in snippets for
?George Bush? and ?President of the United
States?
are allowed as genuine multi-word terms. The
linguistic filter contains a predefined set of pat-
terns of nouns, adjectives and prepositions that are
likely to be terms. The sequences of words that re-
main after this initial filtering process (candidate
terms) are evaluated for their termhood (likeliness
of a candidate to be a term) using C-value. C-
value is built using statistical characteristics of the
candidate string, such as, total frequency of oc-
currence of the candidate string in the document,
the frequency of the candidate string as part of
other longer candidate strings, the number of these
longer candidate terms and the length of candidate
string (in number of words). We select the candi-
dates with higher C-values as terms (see (Frantzi
and Ananiadou, 1999) for more details on C-value
based term extraction).
To extract entities for the term-entity model, the
documents were annotated by a named entity tag-
ger 3. We select personal names, organization
names and location names to be included in the
term-entity model.
5.4 Contextual Similarity
We need to calculate the similarity between term-
entity models derived from different documents,
in order to decide whether they belong to the
same namesake or not. WordNet 4 based similar-
ity metrics have been widely used to compute the
semantic similarity between words in sense dis-
3The named entity tagger was developed by the Cognitive
Computation Group at UIUC. http://L2R.cs.uiuc.edu/ cog-
comp/eoh/ne.html
4http://wordnet.princeton.edu/perl/webwn
0
30
60
90
120
150
President of the United States
Tiger Woods
st
at
es
ge
or
ge
go
lfe
r
bu
sh
w
oo
ds
to
ur
vi
ce
tig
er
ch
ea
ts
sh
al
l
go
ve
rn
m
en
t
pr
es
id
en
ts
pg
a
go
lf
un
ite
d
pr
es
id
en
t
re
vi
ew
s
ex
ec
ut
iv
e
Figure 2: Distribution of words in snippets for
?Tiger Woods? and ?President of the United
States?
ambiguation tasks (Banerjee and Pedersen, 2002;
McCarthy et al, 2004). However, most of the
terms and entities in our term-entity models are
proper names or multi-word expressions which are
not listed in WordNet.
Sahami et al (2005) proposed the use of snip-
pets returned by a Web search engine to calculate
the semantic similarity between words. A snippet
is a brief text extracted from a document around
the query term. Many search engines provide snip-
pets alongside with the link to the original docu-
ment. Since snippets capture the immediate sur-
rounding of the query term in the document, we
can consider a snippet as the context of a query
term. Using snippets is also efficient because we
do not need to download the source documents.
To calculate the contextual similarity between two
terms (or entities), we first collect snippets for
each term (or entity) and pool the snippets into
a combined ?bag of words?. Each collection of
snippets is represented by a word vector, weighted
by the normalized frequency (i.e., frequency of a
word in the collection is divided by the total num-
ber of words in the collection). Then, the contex-
tual similarity between two phrases is defined as
the inner product of their snippet-word vectors.
Figures 1 and 2 show the distribution of most
frequent words in snippets for the queries ?George
Bush?, ?Tiger Woods? and ?President of the
United States?. In Figure 1 we observe the words
?george? and ?bush? appear in snippets for the
query ?President of the United States?, whereas
in Figure 2 none of the high frequent words ap-
pears in snippets for both queries. Contextual
20
similarity calculated as the inner product between
word vectors is 0.2014 for ?George Bush? and
?President of the United States?, whereas the
same is 0.0691 for ?Tiger Woods? and ?Presi-
dent of the United States?. We define the simi-
larity sim(T (A), T (B)), between two term-entity
models T (A) = {a1, . . . , an} and T (B) =
{b1, . . . , bm} of documents A and B as follows,
sim(T (A), T (B)) = 1n
n?
i=1
max
1?j?m
|ai| ? |bj |. (1)
Here, |ai| represents the vector that contains the
frequency of words that appear in the snippets
for term/entity ai. Contextual similarity between
terms/entities ai and bj , is defined as the inner
product |ai| ? |bj |. Without a loss of generality we
assume n ? m in formula 1.
5.5 Clustering
We use Group-average agglomerative clustering
(GAAC) (Cutting et al, 1992), a hybrid of single-
link and complete-link clustering, to group the
documents that belong to a particular namesake.
Initially, we assign a separate cluster for each of
the documents in the collection. Then, GAAC in
each iteration executes the merger that gives rise
to the cluster ? with the largest average correla-
tion C(?) where,
C(?) = 12
1
|?|(|?| ? 1)
X
u??
X
v??
sim(T (u), T (v)) (2)
Here, |?| denotes the number of documents in
the merged cluster ?; u and v are two documents
in ? and sim(T (u), T (v)) is given by equation 1.
Determining the total number of clusters is an im-
portant issue that directly affects the accuracy of
disambiguation. We will discuss an automatic
method to determine the number of clusters in sec-
tion 6.3.
5.6 Key phrases Selection
GAAC process yields a set of clusters representing
each of the different namesakes of the ambiguous
name. To select key phrases that uniquely iden-
tify each namesake, we first pool all the terms and
entities in all term-entity models in each cluster.
For each cluster we select the most discrimina-
tive terms/entities as the key phrases that uniquely
identify the namesake represented by that cluster
from the other namesakes. We achieve this in
two steps. In the first step, we reduce the num-
ber of terms/entities in each cluster by removing
terms/entities that also appear in other clusters.
In the second step, we select the terms/entities
in each cluster according to their relevance to
the ambiguous name. We compute the con-
textual similarity between the ambiguous name
and each term/entity and select the top ranking
terms/entities from each cluster.
6 Experiments and Results
6.1 Evaluating Contextual Similarity
In section 5.4, we defined the similarity between
documents (i.e., term-entity models created from
the documents) using a web snippets based con-
textual similarity (Formula 1). However, how well
such a metric represents the similarity between
documents, remains unknown. Therefore, to eval-
uate the contextual similarity among documents,
we group the documents in ?person-X? dataset
into four classes (each class representing a differ-
ent person) and use Formula 1 to compute within-
class and cross-class similarity histograms, as il-
lustrated in Figure 3.
Ideally, within-class similarity distribution
should have a peak around 1 and cross-class sim-
ilarity distribution around 0, whereas both his-
tograms in Figure 3(a) and 3(b) have their peaks
around 0.2. However, within-class similarity dis-
tribution is heavily biased toward to the right of
this peak and cross-class similarity distribution to
the left. Moreover, there are no document pairs
with more than 0.5 cross-class similarity. The ex-
perimental results guarantees the validity of the
contextual similarity metric.
6.2 Evaluation Metric
We evaluate experimental results based on the
confusion matrix, where A[i.j] represents the
number of documents of ?person i? predicted as
?person j? in matrix A. A[i, i] represents the num-
ber of correctly predicted documents for ?person
i?. We define the disambiguation accuracy as the
sum of diagonal elements divided by the sum of
all elements in the matrix.
6.3 Cluster Quality
Each cluster formed by the GAAC process is sup-
posed to be representing a different namesake.
Ideally, the number of clusters formed should be
equal to the number of different namesakes for
21
0300
600
900
1200
1500
1.11.00.90.80.70.60.50.40.30.20.1
(a) Within-class similarity distribution in
?person-X? dataset
0
1000
2000
3000
4000
5000
1.11.00.90.80.70.60.50.40.30.20.1
(b) Cross-class similarity distribution in
?person-X? dataset
Figure 3: The histogram of within-class and cross-class similarity distributions in ?person-X? dataset. X
axis represents the similarity value. Y axis represents the number of document pairs from the same class
(within-class) or from different classes (cross-class) that have the corresponding similarity value.
the ambiguous name. However, in reality it is
impossible to exactly know the number of name-
sakes that appear on the Web for a particular name.
Moreover, the distribution of pages among name-
sakes is not even. For example, in the ?Jim Clark?
dataset 78% of documents belong to the two fa-
mous namesakes (CEO Nestscape and Formula
one world champion). The rest of the documents
are distributed among the other six namesakes. If
these outliers get attached to the otherwise pure
clusters, both disambiguation accuracy and key
phrase selection deteriorate. Therefore, we moni-
tor the quality of clustering and terminate further
agglomeration when the cluster quality drops be-
low a pre-set threshold. Numerous metrics have
been proposed for evaluating quality of cluster-
ing (Kannan et al, 2000). We use normalized
cuts (Shi and Malik, 2000) as a measure of cluster-
quality.
Let, V denote the set of documents for a name.
Consider, A ? V to be a cluster of documents
taken from V . For two documents x,y in V ,
sim(x, y) represents the contextual similarity be-
tween the documents (Formula 1). Then, the nor-
malized cut Ncut(A) of cluster A is defined as,
Ncut(A) =
?
x?A y?(V?A) sim(x, y)?
x?A y?V sim(x, y)
. (3)
For a set, {A1, . . . , An} of non-overlapping n
clusters Ai, we define the quality of clustering,
Ac
cu
ra
cy
Quality
0
0.2
0.4
0.6
0.8
1
1.2
0.8 0.85 0.9 0.95 1 1.05
Figure 4: Accuracy Vs Cluster Quality for person-
X data set.
Quality({A1, . . . , An}), as follows,
Quality({A1, . . . , An}) = 1n
n?
i=1
Ncut(Ai). (4)
To explore the faithfulness of cluster quality
in approximating accuracy, we compare accuracy
(calculated using human-annotated data) and clus-
ter quality (automatically calculated using For-
mula 4) for person-X data set. Figure 4 shows
cluster quality in x-axis and accuracy in y-axis.
We observe a high correlation (Pearson coefficient
of 0.865) between these two measures, which en-
ables us to guide the clustering process through
cluster quality.
When cluster quality drops below a pre-defined
22
Threshold
Ac
cu
ra
cy
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.6 0.7 0.8 0.9 1
Figure 5: Accuracy Vs Threshold value for
person-X data set.
threshold, we terminate further clustering. We
assign the remaining documents to the already
formed clusters based on the correlation (For-
mula 2) between the document and the cluster. To
determine the threshold of cluster quality, we use
person-X collection as training data. Figure 5 il-
lustrates the variation of accuracy with threshold.
We select threshold at 0.935 where accuracy max-
imizes in Figure 5. Threshold was fixed at 0.935
for the rest of the experiments.
6.4 Disambiguation Accuracy
Table 2 summarizes the experimental results. The
baseline, majority sense , assigns all the doc-
uments in a collection to the person that have
most documents in the collection. Proposed
method outperforms the baseline in all data sets.
Moreover, the accuracy values for the proposed
method in Table 2 are statistically significant (t-
test: P(T?t)=0.0087, ? = 0.05) compared to the
baseline. To identify each cluster with a name-
sake, we chose the person that has most num-
ber of documents in the cluster. ?Found? column
shows the number of correctly identified name-
sakes as a fraction of total namesakes. Although
the proposed method correctly identifies the pop-
ular namesakes, it fails to identify the namesakes
who have just one or two documents in the collec-
tion.
6.5 Web Search Task
Key phrases extracted by the proposed method are
listed in Figure 6 (Due to space limitations, we
show only the top ranking key phrases for two col-
lections). To evaluate key phrases in disambiguat-
Table 2: Disambiguation accuracy for each collec-
tion.
Collection Majority Proposed Found
Sense Method Correct
person-X 0.3676 0.7794 4/4
Michael Jackson 0.6470 0.9706 2/3
Jim Clark 0.4407 0.7627 3/8
William Cohen 0.7614 0.8068 3/10
Michael Jackson
Jim Clark
fan club
trial
world network
superstar 
new charity song
neverland ranch
beer hunter
ultimate beer FAQ
christmas beer
great beer
pilsener beer
barvaria
CLUSTER #1 CLUSTER #2
CLUSTER #1 CLUSTER #2
racing driver
rally
scotsman
driving genius
scottish automobile racer
british rally news
entrepreneur
story
silicon valley
CEO
silicon graphics 
SGI/ Netscape
Figure 6: Top ranking key phrases in clusters for
Michael Jackson and Jim Clark datasets.
ing namesakes, we set up a web search experiment
as follows. We search for the ambiguous name and
the key phrase (for example, ?Jim Clark? AND
?racing driver?) and classify the top 100 results
according to their relevance to each namesake. Re-
sults of our experiment on Jim Clark dataset for
the top ranking key phrases are shown in Table 3.
In Table 3 we classified Google search results
into three categories. ?person-1? is the formula
one racing world champion, ?person -2? is the
founder of Netscape and ?other? category contains
rest of the pages that we could not classify to pre-
vious two groups 5. We first searched Google
without adding any key phrases to the name. In-
cluding terms racing diver, rally and scotsman,
Table 3: Effectiveness of key phrases in disam-
biguating namesakes.
Phrase person-1 person-2 others Hits
NONE 41 26 33 1,080,000
racing driver 81 1 18 22,500
rally 42 0 58 82,200
scotsman 67 0 33 16,500
entrepreneur 1 74 25 28,000
story 17 53 30 186,000
silicon valley 0 81 19 46,800
5some of these pages were on other namesakes and some
were not sufficiently detailed to properly classify
23
which were the top ranking terms for Jim Clark
the formula one champion, yields no results for the
other popular namesake. Likewise, the key words
entrepreneur and silicon valley yield results fort
he founder of Netscape. However, the key word
story appears for both namesakes. A close investi-
gation revealed that, the keyword story is extracted
from the title of the book ?The New New Thing:
A Silicon Valley Story?, a book on the founder of
Netscape.
7 Conclusion
We proposed and evaluated a key phrase extraction
algorithm to disambiguate people with the same
name on the Web. We represented each document
with a term-entity model and used a contextual
similarity metric to cluster the documents. We also
proposed a novel approach to determine the num-
ber of namesakes. Our experiments with pseudo
and naturally ambiguous names show a statisti-
cally significant improvement over the baseline
method. We evaluated the key phrases extracted
by the algorithm in a web search task. The web
search task reveals that including the key phrases
in the query considerably reduces ambiguity. In
future, we plan to extend the proposed method
to disambiguate other types of entities such as
location names, product names and organization
names.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proceedings of COLING, pages 79?85.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using word net. In Proceedings of the third in-
ternational conference on computational linguistics
and intelligent text processing, pages 136?145.
Ron Bekkerman and Andrew McCallum. 2005. Dis-
ambiguating web appearances of people in a social
network. In Proceedings of the 14th international
conference on World Wide Web, pages 463?470.
Douglass R. Cutting, Jan O. Pedersen, David Karger,
and John W. Tukey. 1992. Scatter/gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings SIGIR ?92, pages 318?329.
M.B. Fleischman and E. Hovy. 2004. Multi-document
person name resolution. In Proceedings of 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Reference Resolution Workshop.
K.T. Frantzi and S. Ananiadou. 1999. The c-value/nc-
value domain independent method for multi-word
term extraction. Journal of Natural Language Pro-
cessing, 6(3):145?179.
S. Gauch and J. B. Smith. 1991. Search improvement
via automatic query reformulation. ACM Trans. on
Information Systems, 9(3):249?280.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
Hui Han, Hongyuan Zha, and C. Lee Giles. 2005.
Name disambiguation in author citations using a k-
way spectral clustering method. In Proceedings of
the International Conference on Digital Libraries.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2000. On clusterings: Good, bad, and spectral. In
Proceedings of the 41st Annual Symposium on the
Foundation of Computer Science, pages 367?380.
Xin Li, Paul Morie, and Dan Roth. 2005. Semantic
integration in text, from ambiguous names to identi-
fiable entities. AI Magazine, American Association
for Artificial Intelligence, Spring:45?58.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceed-
ings of CoNLL-2003, pages 33?40.
Y. Matsuo, J. Mori, and M. Hamasaki. 2006. Poly-
phonet: An advanced social network extraction sys-
tem. In to appear in World Wide Web Conference
(WWW).
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of the As-
sociation for Computational Linguistics (ACL?04),
pages 279?286.
P. Mika. 2004. Bootstrapping the foaf-web: and ex-
periment in social networking network minning. In
Proceedings of 1st Workshop on Friend of a Friend,
Social Networking and the Semantic Web.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering sim-
ilar contexts. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Mehran Sahami and Tim Heilman. 2005. A web-based
kernel function for matching short text snippets. In
International Workshop located at the 22nd Inter-
national Conference on Machine Learning (ICML
2005).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
24
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399?409,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Approach to Improve Classification of
Infrequent Discourse Relations using Feature Vector Extension
Hugo Hernault
hugo@mi.ci.i.
u-tokyo.ac.jp
Danushka Bollegala
danushka@iba.t.
u-tokyo.ac.jp
Graduate School of Information Science & Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
Mitsuru Ishizuka
ishizuka@i.
u-tokyo.ac.jp
Abstract
Several recent discourse parsers have em-
ployed fully-supervised machine learning ap-
proaches. These methods require human an-
notators to beforehand create an extensive
training corpus, which is a time-consuming
and costly process. On the other hand, un-
labeled data is abundant and cheap to col-
lect. In this paper, we propose a novel
semi-supervised method for discourse rela-
tion classification based on the analysis of co-
occurring features in unlabeled data, which is
then taken into account for extending the fea-
ture vectors given to a classifier. Our exper-
imental results on the RST Discourse Tree-
bank corpus and Penn Discourse Treebank in-
dicate that the proposed method brings a sig-
nificant improvement in classification accu-
racy and macro-average F-score when small
training datasets are used. For instance, with
training sets of c.a. 1000 labeled instances, the
proposed method brings improvements in ac-
curacy and macro-average F-score up to 50%
compared to a baseline classifier. We believe
that the proposed method is a first step towards
detecting low-occurrence relations, which is
useful for domains with a lack of annotated
data.
1 Introduction
Automatic detection of discourse relations in natu-
ral language text is important for numerous tasks in
NLP, such as sentiment analysis (Somasundaran et
al., 2009), text summarization (Marcu, 2000) and di-
alogue generation (Piwek et al, 2007). However,
most of the recent work employing discourse re-
lation classifiers are based on fully-supervised ma-
chine learning approaches (duVerle and Prendinger,
2009; Pitler et al, 2009; Lin et al, 2009). Two
of the main corpora with discourse annotations are
the RST Discourse Treebank (RSTDT) (Carlson et
al., 2001) and the Penn Discourse Treebank (PDTB)
(Prasad et al, 2008a), which are both based on the
Wall Street Journal (WSJ) corpus.
In the RSTDT, annotation is done using 78
fine-grained discourse relations, which are usually
grouped into 18 coarser-grained relations. Each of
these relations has furthermore several possible con-
figurations for its arguments?its ?nuclearity? (Mann
and Thompson, 1988). In practice, a classifier
trained on these coarse-grained relations must solve
a 41-class classification problem. Some of the re-
lations corresponding to these classes are relatively
more frequent in the corpus, such as the ELAB-
ORATION[N][S] relation (4441 instances), or the
ATTRIBUTION[S][N] relation (1612 instances).1
However, other relation types occur very rarely,
such as TOPIC-COMMENT[S][N] (2 instances), or
EVALUATION[N][N] (3 instances). A similar phe-
nomenon can be observed in PDTB, in which 15
level-two relations are employed: Some, such as
EXPANSION.CONJUNCTION, occur as often as 8759
times throughout the corpus, whereas the remainder
of the relations, such as EXPANSION.EXCEPTION
and COMPARISON.PRAGMATIC CONCESSION, can
appear as rarely as 17 and 12 times respectively. Al-
though supervised approaches to discourse relation
learning achieve good results on frequent relations,
performance is poor on rare relation types (duVerle
and Prendinger, 2009).
Nonetheless, certain infrequent relation types
might be important for specific tasks. For instance,
1We use the notation [N] and [S] respectively to denote the
nucleus and satellite in a RST discourse relation.
399
capturing the RST TOPIC-COMMENT[S][N] and
EVALUATION[N][N] relations can be useful for
sentiment analysis (Pang and Lee, 2008).
Another situation where detection of low-
occurring relations is desirable is the case where we
have only a small training set at our disposal, for in-
stance when there is not enough annotated data for
all the relation types described in a discourse the-
ory. In this case, all the dataset?s relations can be
considered rare, and being able to build an efficient
classifier depends on the capacity to deal with this
lack of annotated data.
Our contributions in this paper are summarized as
follows.
? We propose a semi-supervised method that
exploits the abundant, freely-available unla-
beled data, which is harvested for feature co-
occurrence information, and used as a basis to
extend feature vectors to help classification for
cases where unknown features are found in test
vectors.
? The proposed method is evaluated on the
RSTDT and PDTB corpus, where it signifi-
cantly improves accuracy and macro-average
F-score when small training sets are used. For
instance, when trained on moderately small
datasets with ca. 1000 instances, the proposed
method increases the macro-average F-score
and accuracy up to 50%, compared to a base-
line classifier.
2 Related Work
Since the release in 2001 of the RSTDT corpus,
several fully-supervised discourse parsers have been
built in the RST framework. In the recent work of
duVerle and Prendinger (2009), a discourse parser
based on Support Vector Machines (SVM) (Vapnik,
1995) is proposed. SVMs are employed to train two
classifiers: One, binary, for determining the pres-
ence of a relation, and another, multi-class, for deter-
mining the relation label between related text spans.
For the discourse relation classifier, shallow lexical,
syntactic and structural features, including ?domi-
nance sets? (Soricut and Marcu, 2003) are used. For
relation classification, they report an accuracy of
0.668, and an F-score of 0.509 for the creation of
the full discourse tree.
The unsupervised method of Marcu and Echihabi
(2002) was the first that tried to detect implicit rela-
tions (i.e. relations not accompanied by a cue phrase,
such as ?however?, ?but?), using word pairs extracted
from two spans of text. Their method attempts to
capture the difference of polarity in words. For ex-
ample, the word pair (sell, hold) indicates a CON-
TRAST relation.
Discourse relation classifiers have also been
trained using PDTB. Pitler et al (2008) performed a
corpus study of the PDTB, and found that ?explicit?
relations can be most of the times distinguished by
their discourse connectives. Their discourse relation
classifier reported an accuracy of 0.93 for explicit
relations and in overall an accuracy of 0.744 for all
relations in PDTB.
Lin et al (2009) studied the problem of detecting
implicit relations in PDTB. Their relational classi-
fier is trained using features extracted from depen-
dency paths, contextual information, word pairs and
production rules in parse trees. They reported for
their classifier an accuracy of 0.402, which is an im-
provement of 14.1% over the previous state-of-the-
art for implicit relation classification in PDTB. For
the same task, Pitler et al (2009) also used word
pairs, as well as several other types of features such
as verb classes, modality, context, and lexical fea-
tures.
In text classification, similarity measures have
been employed in kernel methods, where they have
been shown to improve accuracy over ?bag-of-
words? approaches. In Siolas and d?Alche?-Buc
(2000), a semantic proximity measure based on
WordNet (Fellbaum, 1998) is defined, as a basis to
create a proximity matrix for all terms of the prob-
lem. This matrix is then used to smooth the vectorial
data, and the resulting ?semantic? metric is incorpo-
rated into a SVM kernel, resulting in a significant
increase of accuracy and F-score over a baseline.
Cristianini et al (2002) have used a lexical sim-
ilarity measure derived from Latent Semantic In-
dexing (Deerwester et al, 1990), where the seman-
tic similarity between two terms is inferred from
the analysis of their co-occurrence patterns: Terms
that co-occur often in the same documents are con-
sidered as related. In this work, the statistical co-
occurrence information is extracted by the means of
singular value decomposition. The authors observe
400
substantial improvements in performance for some
datasets, while little effect is obtained for others.
Semantic kernels have also been shown to be effi-
cient for text classification tasks, in the case in of un-
balanced and sparse datasets. In Basili et al (2006),
a ?conceptual density? metric based on WordNet is
introduced, and employed in a SVM kernel. Using
this metric results in improved accuracy of 10% for
text classification in poor training conditions. How-
ever, the authors observe that when the number of
training documents is increased, the improvement
produced by the semantic kernel is lower.
Bloehdorn et al (2006) compare the performance
of different semantic kernels, based on several mea-
sures of semantic relatedness in WordNet. For each
measure, the authors note a performance increase
when little training data is available, or when the
feature representations are very sparse. However,
for our task, classification of discourse relations, we
employ not only words but also other types of fea-
tures such as parse tree production rules, and thus
cannot compute semantic kernels using WordNet.
In this paper, we are not aiming at defining
novel features for improving performance in RST or
PDTB relation classification. Instead we incorporate
numerous features that have been shown to be useful
for discourse relation learning and explore the pos-
sibilities of using unlabeled data for this task. One
of our goals is to improve classification accuracy for
rare discourse relations.
3 Method
Given a set of unlabeled instances U and labeled in-
stances L, our objective is to learn an n-class rela-
tion classifier H such that for a given test instance
x return its correct relation type H(x). In the case
of discourse relation learning we are interested in
the situation where |U | >> |L|. Here, we use the
notation |A| to denote the number of elements in a
set A. A fundamental problem that one encounters
when trying to learn a classifier for a large number
of relations with small training dataset is that most
of the features that appear in the test instances ei-
ther never occur in training instances or appear a
small number of times. Therefore, the classifica-
tion algorithm does not have sufficient information
to correctly predict the relation type of the given test
instance. We propose a method that first computes
the co-occurrence between features using unlabeled
data and use that information to extend the feature
vectors during training and testing, thereby reducing
the sparseness in test feature vectors. In Section 3.1,
we introduce the concept of feature co-occurrence
matrix and describe how it is computed using unla-
beled data. A method to extend feature vectors dur-
ing training and testing is presented in Section 3.2.
We defer the details on exact features used in the
method to Section 3.3. It is noteworthy that the
proposed method does not depend or assume a par-
ticular multi-class classification algorithm. Conse-
quently, it can be used with any multi-class classifi-
cation algorithm to learn a discourse relation classi-
fier.
3.1 Feature Co-occurrence Matrix
We represent an instance using a d dimensional fea-
ture vector f = [f1, . . . , fd]T, where fi ? R. We
define a feature co-occurrence matrix, C such that
the (i, j)-th element of C, C(i,j) ? [0, 1] denotes
the degree of co-occurrence between the two fea-
tures fi and fj . If both fi and fj appear in a fea-
ture vector then we define them to be co-occurring.
The number of different feature vectors in which fi
and fj co-occur is denoted by the function h(fi, fj).
From our definition of co-occurrence it follows that
h(fi, fj) = h(fj , fi). Importantly, feature co-
occurrences can be calculated only using unlabeled
data.
Feature co-occurrence matrices can be computed
using any co-occurrence measure. For the current
task we use the ?2-measure (Plackett, 1983) as the
preferred co-occurrence measure because of its sim-
plicity. ?2-measure between two features fi and fj
is defined as follows,
?2i,j =
2?
k=1
2?
l=1
(Oi,jk,l ? E
i,j
k,l)
2
Ei,jk,l
. (1)
Therein,Oi,j andEi,j are the 2?2 matrices contain-
ing respectively observed frequencies and expected
frequencies, which are respectively computed using
C as,
Oi,j =
(
h(fi, fj) Zi ? h(fi, fj)
Zj ? h(fi, fj) Zs ? Zi ? Zj
)
, (2)
401
and
Ei,j =
(
Zi?Zj
Zs
Zi?(Zs?Zj)
Zs
Zj ?(Zs?Zi)
Zs
(Zs?Zi)?(Zs?Zj)
Zs
)
. (3)
Here, Zi =
?
k 6=i h(fi, fk), and Zs =
?n
i=1 Zi.
Finally, we create the feature co-occurrence ma-
trix C, such that, for all pairs of features (fi, fj),
C(i,j) =
{
??2i,j if ?
2
i,j > c
0 otherwise
. (4)
Here ??2i,j =
?2i,j??
2
min
?2max??
2
min
? [0, 1], and c is the critical
value, which, for a confidence level of 0.05 and one
degree of freedom, can be set to 3.84. KeepingC(i,j)
in the range [0, 1] makes it convenient to filter out
low-relevance co-occurrences at the feature vector
extension step of Section 3.2.
In discourse relation learning, the feature space
can be extremely large. For example, with word
pair features (discussed later in Section 3.3), any
two words that appear in two adjoining discourse
units can form a feature. Because the number of
elements in the feature co-occurrence matrix is pro-
portional to the square of the feature space?s dimen-
sion, computing co-occurrences for all pairs of fea-
tures can be computationally costly. Moreover, stor-
ing a large matrix in memory for further computa-
tions can be problematic. To reduce the dimension-
ality and improve the sparseness in the feature co-
occurrence matrix, we use entropy-based feature se-
lection (Manning and Schu?tze, 1999). The negative
entropy, E(fi), of a feature fi is defined as follows,
E(fi) = ?
?
j 6=i
p(i, j) ? log (p (i, j)) . (5)
Here, p(i, j) is the probability that feature fi co-
occurs with feature fj , and is given by p(i, j) =
h(fi, fj)/Zi.
If a particular feature fi co-occurs with many
other features, then its negative entropy E(fi) de-
creases. Because we are interested in identifying
salient co-occurrences between features, we can ig-
nore the features that tend to co-occur with many
other features. Consequently, we sort the features in
the descending order of their entropy, and select the
top rankedN number of features to build the feature
co-occurrence matrix. This feature selection proce-
dure can efficiently reduce the dimensions of the fea-
ture co-occurrence matrix to N ? N . Because the
feature co-occurrence matrix is symmetric, we must
only store the elements for the upper (or lower) tri-
angular portion of it.
3.2 Feature Vector Extension
Once the feature co-occurrence matrix is computed
using unlabeled data as described in Section 3.1, we
can use it to extend a feature vector during train-
ing and testing. The proposed feature vector exten-
sion method is inspired by query expansion in the
field of Information Retrieval (Salton and Buckley,
1983; Fang, 2008). One of the reasons that a clas-
sifier might perform poorly on a test instance is that
there are features in the test instance that were not
observed during training. We call FU = {fi} the
set of features that were not observed by the clas-
sifier during training (i.e. occurring in test data but
not in training data). For each of those features, we
use the feature co-occurrence matrix to find the set
of co-occurring features, Fc(fi).
Let us denote the feature vector corresponding to
a training or test instance x by fx. We use the su-
perscript notation, f ix to denote the i-th feature in fx.
Moreover, the total number of features of fx is indi-
cated by d(x). For a feature f ix in fx, we define n(i)
number of expansion features, f (i,1)x , . . . , f
(i,n(i))
x as
follows. First, we require that each expansion fea-
ture f (i,j)x belongs to Fc(fi). Second, the value of
f (i,j)x is set to f ix ? C(i,j). The expansion features
for each feature f ix are then appended to the orig-
inal feature vector fx to create an extended feature
vector, f ?x, where,
f ?x = (f
1
x , . . . , f
d(x)
x , (6)
f (i,1)x , . . . , f
(i,n(i))
x , . . . ,
f (d(x),1)x , . . . , f
(d(x),n(d(x))
x ).
In total, doing so augments the original vector?s size
by
?
fi?U
|Fc(fi)|. All training and test instances
are extended in this fashion.
Note that because this process can potentially in-
crease the dimension too much, it is possible to re-
tain only candidate co-occurring features of Fc(fi)
possessing a co-occurrence value C(i,j) above a cer-
tain threshold. In the experiments of Section 4 how-
402
ever, we experienced dimension increase of 10000 at
most, which did not require us to use thresholding.
3.3 Features
We use three types of features: Word pairs, produc-
tion rules from the parse tree, as well as features en-
coding the lexico-syntactic context at the border be-
tween two units of text (Soricut and Marcu, 2003).
Our word pairs are lemmatized using the Wordnet-
based lemmatizer of NLTK (Loper and Bird, 2002).
Figure 1 shows the parse tree for a sentence com-
posed of two discourse units, which serve as argu-
ments of a discourse relation we want to generate a
feature vector from. Lexical heads have been calcu-
lated using the projection rules of Magerman (1995),
and annotated between brackets. Surrounded by
dots is, for each argument, the minimal set of sub-
parse trees containing strictly all the words of the
argument.
We first extract all possible lemmatized word-
pairs from the two arguments, such as (Mr., when),
(decline, ask) or (comment, sale). Next, we extract
from left and right argument separately, all produc-
tion rules from the sub-parse trees, such as NP 7?
NNP NNP, NNP 7? ?Sherry? or TO 7? ?to?.
Finally, we encode in our features three nodes of
the parse tree, which capture the local context at the
connection point between the two arguments: The
first node, which we call Nw, is the highest ances-
tor of the first argument?s last word w, and is such
that Nw?s right-sibling is the ancestor of the second
argument?s first word. Nw?s right-sibling node is
called Nr. Finally, we call Np the parent of Nw and
Nr. For each node, we encode in the feature vec-
tor its part-of-speech (POS) and lexical head. For
instance, in Figure 1, we have Nw = S(comment),
Nr = SBAR(when), and Np = VP(declined). In the
PDTB, certain discourse relations have disjoint ar-
guments. In this case, as well as in the case where
the two arguments belong to different sentences, the
nodes Nw, Nr, Np cannot be defined, and their cor-
responding features are given the value zero.
4 Experiments
The proposed method is independent of any partic-
ular classification algorithm. Because our goal is
strictly to evaluate the relative benefit of employing
the proposed method, and not the absolute perfor-
mance when used with a specific classification algo-
rithm, we select a logistic regression classifier, for its
simplicity. We use the multi-class logistic regression
(maximum entropy model) implemented in the Clas-
sias toolkit (Okazaki, 2009). Regularization param-
eters are set to their default value of one and are fixed
throughout the experiments described in the paper.
To create our unlabeled dataset, we use sentences
extracted from the English Wikipedia2, as they are
freely available and relatively easy to collect. For
further extraction of syntactic features, these sen-
tences are automatically parsed using the Stanford
parser (Klein and Manning, 2003). Then, they are
segmented into elementary discourse units (EDUs)
using our sequential discourse segmenter (Hernault
et al, 2010). The relatively high performance of
this RST segmenter, which has an F-score of 0.95
compared to that of 0.98 between human annota-
tors (Soricut and Marcu, 2003), is acceptable for this
task. We collect and parse 100000 sentences from
random Wikipedia articles. As there is no segmen-
tation tool for the PDTB framework, we assume that
co-occurrence information taken from EDUs created
using a RST segmenter is also useful for extending
feature vectors of PDTB relations. Unless other-
wise noted, the experiments presented in the rest of
this paper are done using those 100000 unlabeled in-
stances.
In the unlabeled data, any two consecutive dis-
course units might not always be connected by a dis-
course relation. Therefore, we introduce an artificial
NONE relation in the training set, in order to facil-
itate this. Instances of the NONE relation are gen-
erated randomly by pairing consecutive discourse
units which are not connected by a discourse relation
in the training data. NONE is also learnt as a separate
discourse relation class by the multi-class classifica-
tion algorithm. This enables us to detect discourse
units between which there exist no discourse rela-
tion, thereby improving the classification accuracy
for other relation types.
We follow the common practice in discourse re-
search for partitioning the discourse corpora into
training and test set. For the RST classifier, the
dedicated training and test sets of the RSTDT are
2http://en.wikipedia.org
403
NP (Sherry)
S (declined)
VP (declined)
NNP NNP
declined
VBD (declined)
Mr. Sherry to
VP (comment)
comment when asked about the sales
TO VP
SBAR (when)
WHADVP (when)
WRB
S (asked)
VP (asked)
VBN
PP (about)
IN NP (sales)
DT NNS
.
. (.)
Argument 1 Argument 2
VB
S (comment)
Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them?lexical heads
are indicated between brackets.
employed. For the PDTB classifier, we conform to
the guidelines of Prasad et al (2008b, 5): The por-
tion of the corpus corresponding to sections 2?21
of the WSJ is used for training the classifier, while
the portion corresponding to WSJ section 23 is used
for testing. In order to extract syntactic features, all
training and test data are furthermore aligned with
their corresponding parse trees in the Penn Treebank
(Marcus et al, 1993).
Because in the PDTB an instance can be
annotated with several discourse relations
simultaneously?called ?senses? in Prasad et
al. (2008b)?for each instance with n senses in
the corpus, we create n identical feature vectors,
each being labeled by one of the instance?s senses.
However, in the RST framework, only one relation
is allowed to hold between two EDUs. Conse-
quently, each instance from the RSTDT is labeled
with a single discourse relation, from which a
single feature vector is created. For RSTDT, we
extract 25078 training vectors and 1633 test vectors.
For PDTB we extract 49748 training vectors and
1688 test vectors. There are 41 classes (relation
types) in the RSTDT relation classification task,
and 29 classes in the PDTB task. For the PDTB,
we selected level-two relations, because they have
better expressivity and are not too fine-grained.
We experimentally set the entropy-based feature
selection parameter to N = 5000. With large N
values, we must store and process large feature
co-occurrence matrices. For example, doubling
the number of selected features, N to 10000 did
not improve the classification accuracy, although
it required 4GB of memory to store the feature
co-occurrence matrix.
Figure 2 shows the number of features that occur
in test data but not in labeled training data, against
the number of training instances. It can be seen from
Figure 2 that, with less training data available to the
classifier, we can potentially obtain more informa-
tion regarding features by looking at unlabeled data.
However, when the training dataset?s size increases,
the number of features that only appear in test data
decreases rapidly. This inverse relation between the
training dataset size and the number of features that
only appear in test data can be observed in both
RSTDT and PDTB datasets. For a training set of
100 instances, there are 23580 unseen features in
the case of RSTDT, and 27757 in the case of PDTB.
The number of unseen features is halved for a train-
ing set of 1800 instances in the case of RSTDT, and
for a training set of 1300 instances in the case of
PDTB. Finally, when selecting all available training
data, we count only 1365 unseen test features in the
case of RSTDT, and 87 in the case of PDTB.
In the following experiments, we use macro-
averaged F-scores to evaluate the performance of the
proposed discourse relation classifier on test data.
Macro-averaged F-score is not influenced by the
number of instances that exist in each relation type.
It equally weights the performance on both frequent
relation types and infrequent relation types. Because
we are interested in measuring the overall perfor-
mance of a discourse relation classifier across all re-
404
0 5000 10000 15000 20000 25000Number of training instances0
5000
10000
15000
20000
25000
30000
Num
ber
 of u
nse
en t
est 
feat
ure
s RSTDTPDTB
Figure 2: Number of features seen only in the test set, as
a function of the number of training instances used.
lation types we use macro-averaged F-score as the
preferred evaluation metric for this task.
We train a multi-class logistic regression model
without extending the feature vectors as a baseline
method. This baseline is expected to show the ef-
fect of using the proposed feature vector extension
approach for the task of discourse relation learn-
ing. Experimental results on RSTDT and PDTB
datasets are depicted in Figures 3 and 4. From
these figures, we see that the proposed feature ex-
tension method outperforms the baseline for both
RSTDT and PDTB datasets for the full range of
training dataset sizes. However, whereas the differ-
ence of scores between the two methods is obvious
for small amounts of training data, this difference
progressively decreases as we increase the amount
of training data. Specifically, with 100 training in-
stances, the difference between baseline and pro-
posed method is the largest: For RSTDT, the base-
line has a macro-averaged F-score of 0.084, whereas
the the proposed method has a macro-averaged F-
score of 0.189 (ca. 119% increase in F-score). For
PDTB, the baseline has an F-score of 0.016, while
the proposed method has an F-score of 0.089 (459%
increase). The difference of scores between the two
methods then progressively diminishes as the num-
ber of training instances is increased, and fades be-
yond 10000 training instances. The reason for this
behavior is given by Figure 2: For a small number
of training instances, the number of unseen features
in training data is large. In this case, the feature vec-
tor extension process is comprehensive, and score
can be increased by the use of unlabeled data. When
more training data is progressively used, the num-
ber of unseen test features sharply diminishes, which
means feature vector extension becomes more lim-
ited, and the performance of the proposed method
gets progressively closer to the baseline. Note that
we plotted PDTB performance up to 25000 train-
ing instances, as the number of unseen test features
becomes so small past this point that the perfor-
mances of the proposed method and baseline are
identical. Using all PDTB training data (49748 in-
stances), both baseline and proposed method reach a
macro-average F-score of 0.308.
0 5000 10000 15000 20000 25000Number of training instances0.05
0.10
0.15
0.20
0.25
0.30
Mac
ro-a
vera
ge F
-sco
re
Baseline RSTDTProposed method
Figure 3: Macro-average F-score (RSTDT) as a function
of the number of training instances used.
0 5000 10000 15000 20000 25000Number of training instances0.00
0.05
0.10
0.15
0.20
0.25
0.30
Mac
ro-a
vera
ge F
-sco
re
Baseline PDTBProposed method
Figure 4: Macro-average F-score (PDTB) as a function
of the number of training instances used.
405
#Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7
Relation name B. P.M. B. P.M. B. P.M. B. P.M. B. P.M.
Attribution[N][S] ? 0.127 ? 0.237 ? 0.458 0.038 0.290 0.724 0.773
Attribution[S][N] ? 0.597 ? 0.449 0.009 0.639 0.250 0.721 0.579 0.623
Background[N][S] ? 0.113 ? ? ? 0.036 ? 0.095 ? 0.089
Cause[N][S] ? ? ? 0.128 ? ? ? 0.034 0.057 0.187
Comparison[N][S] ? 0.118 ? 0.037 ? ? 0.133 0.130 0.143 0.031
Condition[N][S] ? 0.041 ? 0.136 ? 0.113 ? 0.154 0.242 0.152
Condition[S][N] ? ? ? 0.122 0.133 0.148 0.214 0.233 0.390 0.308
Contrast[N][N] ? ? ? 0.086 ? 0.073 0.050 0.111 ? 0.109
Contrast[N][S] ? 0.071 ? ? ? 0.188 ? 0.087 ? 0.136
Elaboration[N][S] ? 0.134 ? 0.126 0.004 0.067 0.004 0.340 ? 0.165
Enablement[N][S] ? ? ? 0.462 ? 0.579 0.115 0.423 0.419 0.438
Joint[N][N] ? 0.030 ? 0.015 ? ? 0.016 0.059 0.015 0.155
Manner-Means[N][S] ? ? ? 0.056 ? 0.103 0.345 0.372 0.412 0.383
Summary[N][S] ? 0.429 ? 0.453 0.080 0.358 ? 0.349 0.154 0.471
Temporal[N][S] ? 0.158 ? ? ? 0.091 ? 0.052 0.204 0.101
Accuracy 0.000 0.110 0.000 0.105 0.004 0.146 0.034 0.222 0.122 0.213
Macro-average F-score 0.000 0.060 0.000 0.069 0.008 0.101 0.038 0.118 0.107 0.134
Table 1: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation. B. indicates
F-score for baseline, P.M. for the proposed method. A boldface indicates the best classifier for each relation.
Although the distribution of discourse relations
in RSTDT and PDTB is not uniform, it is possi-
ble to study the performance of the proposed method
when all relations are made equally rare. We evalu-
ate performance on artificially-created training sets
containing an equal amount of each discourse rela-
tion. Table 1 contains the F-score for each RSTDT
relation, using training sets containing respectively
one, two, three, five and seven instances of each
relation. For space considerations, only relations
with significant results are shown. We observe that,
when using respectively one and two instances of
each relation, the baseline classifier is unable to de-
tect any relation, and has a macro-average F-score
of zero. Contrastingly, the classifier built with fea-
ture vector extension reaches in those cases an F-
score of 0.06. Furthermore, when employing the
proposed method, certain relations have relatively
high F-scores even with very little labeled data: With
one training instance, ATTRIBUTION[S][N] has an
F-score of 0.597, while SUMMARY[N][S] has an F-
score of 0.429. With three training instances, EN-
ABLEMENT[N][S] has an F-score of 0.579. When
the amount of each relation is increased, the baseline
classifier starts detecting more relations. In all cases,
the proposed method performs better in terms of ac-
curacy and macro-average F-score. With a train-
ing set containing seven instances of each relation,
the baseline?s macro-average F-score is starting to
get closer to the extended classifier?s, with superior
performances for several relations, such as COM-
PARISON[N][S], CONDITION[N][S], and TEMPO-
RAL[N][S]. Still, in this case, the extended classi-
fier?s accuracy is higher than the baseline (0.213 ver-
sus 0.122). Table 2 summarizes the outcome of the
same experiments performed on the PDTB dataset.
The results exhibit a similar trend, despite the base-
line classifier having a relatively high accuracy for
each case.
Using the data from Figures 2, 3 and 4, it is pos-
sible to calculate the relative score change occur-
ring when using the proposed method, as a func-
tion of the number of unseen features found in test
data. This graph is plotted in Figure 5. Besides
macro-average F-score, we additionally plot accu-
racy change. In the top subfigure, representing the
case of RSTDT, we see that, for the lowest amount
of unseen test features, the proposed method does
406
#Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7
Relation name B. P.M. B. P.M. B. P.M. B. P.M. B. P.M.
Comparison.Concession[2][1] ? 0.056 ? ? ? 0.133 ? ? ? 0.154
Comparison.Contrast[2][1] ? ? ? 0.333 ? ? ? 0.190 0.105 0.368
Contingency.Cause[1][2] ? 0.013 ? 0.007 ? ? ? 0.026 ? 0.013
Contingency.Condition[1][2] ? 0.082 ? 0.160 ? 0.127 0.250 0.253 0.214 0.171
Contingency.Condition[2][1] ? ? ? ? ? 0.074 ? 0.143 0.250 0.296
Contingency.Prag. cond.[1][2] ? ? ? 0.133 ? 0.034 ? ? 0.133 0.043
Contingency.Prag. cond.[2][1] ? ? ? ? ? ? 0.133 0.087 0.154 0.087
Expansion.Conjunction[1][2] 0.326 0.352 0.326 0.351 0.326 0.368 0.332 0.371 0.335 0.384
Expansion.Instantiation[1][2] ? ? ? ? ? 0.042 ? 0.057 ? 0.131
Temporal.Asynchronous[1][2] ? 0.204 ? ? ? 0.142 0.039 0.148 ? 0.035
Temporal.Asynchronous[2][1] ? ? ? ? ? 0.316 ? 0.483 0.143 ?
Temporal.Synchrony[1][2] ? ? ? 0.032 ? 0.162 0.032 0.103 0.032 0.157
Temporal.Synchrony[2][1] ? ? ? 0.083 ? 0.143 0.200 0.308 0.211 0.174
Accuracy 0.195 0.201 0.195 0.202 0.195 0.212 0.202 0.214 0.204 0.213
Macro-average F-score 0.015 0.033 0.015 0.054 0.015 0.084 0.045 0.108 0.072 0.100
Table 2: F-scores for PDTB relations.
not bring any change in F-score or accuracy. In-
deed, as the number of unknown features is low,
feature vector extension is very limited, and does
not improve the performance compared to the base-
line. Then, a progressive increase of both accuracy
and macro-average F-score is observed, as the num-
ber of unseen test features is incremented. For in-
stance, for 8500 unseen test features, the macro-
average F-score increase (resp. accuracy increase)
is 25% (resp. 2.5%), while it is 20% (resp. 1%) for
11000 unseen test instances. These values reach a
maximum of 119% macro-average F-score increase,
and 66% accuracy increase, when 23500 features
unseen during training are present in test data. This
situation corresponds in Figures 3 and 4 to the case
of very small training sets. The bottom subfigure
of Figure 2, for the case of PDTB, reveals a sim-
ilar tendency. The macro-average F-score increase
(resp. accuracy increase) is negligible for 1000 un-
seen test features, while this increase is 21% for both
macro-average F-score and accuracy in the case of
9700 unseen test features, and 459% (resp. 630% for
accuracy) when 28000 unseen features are found in
test data. This shows that the proposed method is
useful when large numbers of features are missing
from the training set, which corresponds in practice
to small training sets, with few training instances for
each relation type. For large training sets, most fea-
tures are encountered by the classifier during train-
ing, and feature vector extension does not bring use-
ful information.
We empirically evaluate the effect of using differ-
ent amounts of unlabeled data on the performance of
the proposed method. We use respectively 100 and
10000 labeled training instances, create feature co-
occurrence matrices with different amounts of unla-
beled data, and evaluate the performance in relation
classification. Experimental results for RSTDT are
illustrated in Figure 6 (top). From Figure 6 it appears
clearly that macro-average F-scores improve with
increased number of unlabeled instances. However,
the benefit of using larger amounts of unlabeled data
is more pronounced when only a small number of la-
beled training instances are employed (ca. 100). In
fact, with 100 labeled training instances, the maxi-
mum improvement in F-score is 119% (corresponds
to using all our 100000 unlabeled instances). How-
ever, the maximum improvement in F-score with
10000 labeled training instances is small, only 2.5%
(corresponds to 10000 unlabeled instances).
The effect of using unlabeled data on PDTB rela-
tion classification is illustrated in Figure 6 (bottom).
Similarly, we consecutively set the labeled training
dataset size to 100 and 10000 instances, and plot the
macro-average F-score against the unlabeled dataset
size. As in the RSTDT experiment, the benefit of us-
407
0 5000 10000 15000 20000 25000Number of unseen features in test data
0
20
40
60
80
100
120
Sco
re c
han
ge o
ver 
bas
elin
e (%
) Accuracy change RSTDTMacro-av. F-score change RSTDT
0 5000 10000 15000 20000 25000 30000Number of unseen features in test data0
100
200
300
400
500
600
700
Sco
re c
han
ge o
ver 
bas
elin
e (%
) Accuracy change PDTBMacro-av. F-score change PDTB
Figure 5: Score change as a function of unseen test fea-
tures for RSTDT (top) and PDTB (bottom).
ing unlabeled data is more obvious when the num-
ber of labeled training instances is small. In par-
ticular, with 100 training instances, the maximum
improvement in F-score is 459% (corresponds to
100000 unlabeled instances). However, with 10000
labeled training instances the maximum improve-
ment in F-score is 15% (corresponds to 100 unla-
beled instances). These results confirm that, on the
one hand performance improvement is more promi-
nent for smaller training sets, and that on the other
hand, performance is increased when using larger
amounts of unlabeled data.
5 Conclusion
We presented a semi-supervised method which ex-
ploits the co-occurrence of features in unlabeled
data, to extend feature vectors during training and
testing in a discourse relation classifier. Despite the
0.00
0.05
0.10
0.15
0.20
0.25
Mac
ro-a
vera
ge F
-sco
re
101 102 103 104 105Number of unlabeled instances
RSTDT (100)Baseline RSTDT (100)RSTDT (10000)Baseline RSTDT (10000)
0.10
0.05
0.00
0.05
0.10
0.15
0.20
0.25
Mac
ro-a
vera
ge F
-sco
re
101 102 103 104 105Number of unlabeled instances
PDTB (100)Baseline PDTB (100)PDTB (10000)Baseline PDTB (10000)
Figure 6: Macro-average F-score for RSTDT (top) and
PDTB (bottom), for 100 and 10000 training instances,
against the number of unlabeled instances.
simplicity of the proposed method, it significantly
improved the macro-average F-score in discourse re-
lation classification for small training datasets, con-
taining low-occurrence relations. We performed an
evaluation on two popular datasets, the RSTDT and
PDTB. We empirically evaluated the benefit of using
a variable amount of unlabeled data for the proposed
method. Although the macro-average F-scores of
the classifiers described are too low to be used di-
rectly as discourse analyzers, the gain in F-score and
accuracy for small labeled datasets are a promising
perspective for improving classification accuracy for
infrequent relation types. In particular, the proposed
method can be employed in existing discourse clas-
sifiers that work well on popular relations, and be
expected to improve the overall accuracy.
408
References
R. Basili, M. Cammisa, and A. Moschitti. 2006. A se-
mantic kernel to classify texts with very few training
examples. Informatica (Slovenia), 30(2):163?172.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. In Proc. of
ICDM?06, pages 808?812.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of Rhetorical Structure Theory. Proc. of Second SIG-
dial Workshop on Discourse and Dialogue-Volume 16,
pages 1?10.
N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. La-
tent semantic kernels. Journal of Intelligent Informa-
tion Systems, 18:127?152.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
D. A. duVerle and H. Prendinger. 2009. A novel dis-
course parser based on Support Vector Machine clas-
sification. In Proc. of ACL?09, pages 665?673.
H. Fang. 2008. A re-examination of query expansion us-
ing lexical resources. In Proc. of ACL?08, pages 139?
147.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A
sequential model for discourse segmentation. In Proc.
of CICLing?10, pages 315?326.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in Neural Information Processing Systems,
volume 15. MIT Press.
Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In Proc. of EMNLP?09, pages 343?351.
E. Loper and S. Bird. 2002. NLTK: The natural lan-
guage toolkit. In Proc. of ACL?02 Workshop on Effec-
tive tools and methodologies for teaching natural lan-
guage processing and computational linguistics, pages
63?70.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. Proc. of ACL?95, pages 276?283.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language processing. MIT Press.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc. of
ACL?02, pages 368?375.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
N. Okazaki. 2009. Classias: A collection of machine-
learning algorithms for classification. http://
www.chokkan.org/software/classias/.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse rela-
tions. In Proc. of COLING?08 (Posters), pages 87?90.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of ACL?09, pages 683?691.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. Generating dialogues between virtual agents au-
tomatically from text. In Proc. of IVA?07, pages 161?
174.
R. L. Plackett. 1983. Karl Pearson and the chi-squared
test. International Statistical Review / Revue Interna-
tionale de Statistique, 51(1):59?72.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The Penn Discourse
TreeBank 2.0. In Proc. of LREC?08.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber. 2008b. The Penn Dis-
course Treebank 2.0 annotation manual. Technical re-
port, University of Pennsylvania Institute for Research
in Cognitive Science.
G. Salton and C. Buckley. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill Book Company.
G. Siolas and F. d?Alche?-Buc. 2000. Support Vector Ma-
chines based on a semantic kernel for text categoriza-
tion. In Proc. of IJCNN?00, volume 5, page 5205.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proc. of EMNLP?09, pages
170?179.
R. Soricut and D. Marcu. 2003. Sentence level discourse
parsing using syntactic and lexical information. Proc.
of NA-ACL?03, 1:149?156.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.
409
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 132?141,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus
for Cross-Domain Sentiment Classification
Danushka Bollegala
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@
iba.t.u-tokyo.ac.jp
David Weir
School of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
sussex.ac.uk
John Carroll
School of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
j.a.carroll@
sussex.ac.uk
Abstract
We describe a sentiment classification method
that is applicable when we do not have any la-
beled data for a target domain but have some
labeled data for multiple other domains, des-
ignated as the source domains. We automat-
ically create a sentiment sensitive thesaurus
using both labeled and unlabeled data from
multiple source domains to find the associa-
tion between words that express similar senti-
ments in different domains. The created the-
saurus is then used to expand feature vectors
to train a binary classifier. Unlike previous
cross-domain sentiment classification meth-
ods, our method can efficiently learn from
multiple source domains. Our method signif-
icantly outperforms numerous baselines and
returns results that are better than or com-
parable to previous cross-domain sentiment
classification methods on a benchmark dataset
containing Amazon user reviews for different
types of products.
1 Introduction
Users express opinions about products or services
they consume in blog posts, shopping sites, or re-
view sites. It is useful for both consumers as well
as for producers to know what general public think
about a particular product or service. Automatic
document level sentiment classification (Pang et al,
2002; Turney, 2002) is the task of classifying a given
review with respect to the sentiment expressed by
the author of the review. For example, a sentiment
classifier might classify a user review about a movie
as positive or negative depending on the sentiment
expressed in the review. Sentiment classification
has been applied in numerous tasks such as opinion
mining (Pang and Lee, 2008), opinion summariza-
tion (Lu et al, 2009), contextual advertising (Fan
and Chang, 2010), and market analysis (Hu and Liu,
2004).
Supervised learning algorithms that require la-
beled data have been successfully used to build sen-
timent classifiers for a specific domain (Pang et al,
2002). However, sentiment is expressed differently
in different domains, and it is costly to annotate
data for each new domain in which we would like
to apply a sentiment classifier. For example, in the
domain of reviews about electronics products, the
words ?durable? and ?light? are used to express pos-
itive sentiment, whereas ?expensive? and ?short bat-
tery life? often indicate negative sentiment. On the
other hand, if we consider the books domain the
words ?exciting? and ?thriller? express positive sen-
timent, whereas the words ?boring? and ?lengthy?
usually express negative sentiment. A classifier
trained on one domain might not perform well on
a different domain because it would fail to learn the
sentiment of the unseen words.
Work in cross-domain sentiment classification
(Blitzer et al, 2007) focuses on the challenge of
training a classifier from one or more domains
(source domains) and applying the trained classi-
fier in a different domain (target domain). A cross-
domain sentiment classification system must over-
come two main challenges. First, it must identify
which source domain features are related to which
target domain features. Second, it requires a learn-
ing framework to incorporate the information re-
132
garding the relatedness of source and target domain
features. Following previous work, we define cross-
domain sentiment classification as the problem of
learning a binary classifier (i.e. positive or negative
sentiment) given a small set of labeled data for the
source domain, and unlabeled data for both source
and target domains. In particular, no labeled data is
provided for the target domain.
In this paper, we describe a cross-domain senti-
ment classification method using an automatically
created sentiment sensitive thesaurus. We use la-
beled data from multiple source domains and unla-
beled data from source and target domains to rep-
resent the distribution of features. We represent a
lexical element (i.e. a unigram or a bigram of word
lemma) in a review using a feature vector. Next, for
each lexical element we measure its relatedness to
other lexical elements and group related lexical ele-
ments to create a thesaurus. The thesaurus captures
the relatedness among lexical elements that appear
in source and target domains based on the contexts
in which the lexical elements appear (their distribu-
tional context). A distinctive aspect of our approach
is that, in addition to the usual co-occurrence fea-
tures typically used in characterizing a word?s dis-
tributional context, we make use, where possible, of
the sentiment label of a document: i.e. sentiment la-
bels form part of our context features. This is what
makes the distributional thesaurus sensitive to senti-
ment. Unlabeled data is cheaper to collect compared
to labeled data and is often available in large quan-
tities. The use of unlabeled data enables us to ac-
curately estimate the distribution of words in source
and target domains. Our method can learn from a
large amount of unlabeled data to leverage a robust
cross-domain sentiment classifier.
We model the cross-domain sentiment classifica-
tion problem as one of feature expansion, where we
append additional related features to feature vectors
that represent source and target domain reviews in
order to reduce the mismatch of features between the
two domains. Methods that use related features have
been successfully used in numerous tasks such as
query expansion (Fang, 2008), and document classi-
fication (Shen et al, 2009). However, feature expan-
sion techniques have not previously been applied to
the task of cross-domain sentiment classification.
In our method, we use the automatically created
thesaurus to expand feature vectors in a binary clas-
sifier at train and test times by introducing related
lexical elements from the thesaurus. We use L1 reg-
ularized logistic regression as the classification al-
gorithm. (However, the method is agnostic to the
properties of the classifier and can be used to expand
feature vectors for any binary classifier). L1 regular-
ization enables us to select a small subset of features
for the classifier. Unlike previous work which at-
tempts to learn a cross-domain classifier using a sin-
gle source domain, we leverage data from multiple
source domains to learn a robust classifier that gen-
eralizes across multiple domains. Our contributions
can be summarized as follows.
? We describe a fully automatic method to create
a thesaurus that is sensitive to the sentiment of
words expressed in different domains.
? We describe a method to use the created the-
saurus to expand feature vectors at train and test
times in a binary classifier.
2 A Motivating Example
To explain the problem of cross-domain sentiment
classification, consider the reviews shown in Ta-
ble 1 for the domains books and kitchen appliances.
Table 1 shows two positive and one negative re-
view from each domain. We have emphasized in
boldface the words that express the sentiment of
the authors of the reviews. We see that the words
excellent, broad, high quality, interesting, and
well researched are used to express positive senti-
ment in the books domain, whereas the word disap-
pointed indicates negative sentiment. On the other
hand, in the kitchen appliances domain the words
thrilled, high quality, professional, energy sav-
ing, lean, and delicious express positive sentiment,
whereas the words rust and disappointed express
negative sentiment. Although high quality would
express positive sentiment in both domains, and dis-
appointed negative sentiment, it is unlikely that we
would encounter well researched in kitchen appli-
ances reviews, or rust or delicious in book reviews.
Therefore, a model that is trained only using book
reviews might not have any weights learnt for deli-
cious or rust, which would make it difficult for this
model to accurately classify reviews of kitchen ap-
pliances.
133
books kitchen appliances
+ Excellent and broad survey of the development of
civilization with all the punch of high quality fiction.
I was so thrilled when I unpack my processor. It is
so high quality and professional in both looks and
performance.
+ This is an interesting and well researched book. Energy saving grill. My husband loves the burgers
that I make from this grill. They are lean and deli-
cious.
- Whenever a new book by Philippa Gregory comes
out, I buy it hoping to have the same experience, and
lately have been sorely disappointed.
These knives are already showing spots of rust de-
spite washing by hand and drying. Very disap-
pointed.
Table 1: Positive (+) and negative (-) sentiment reviews in two different domains.
sentence Excellent and broad survey of
the development of civilization.
POS tags Excellent/JJ and/CC broad/JJ
survey/NN1 of/IO the/AT
development/NN1 of/IO civi-
lization/NN1
lexical elements
(unigrams)
excellent, broad, survey, devel-
opment, civilization
lexical elements
(bigrams)
excellent+broad, broad+survey,
survey+development, develop-
ment+civilization
sentiment fea-
tures (lemma)
excellent*P, broad*P, sur-
vey*P, excellent+broad*P,
broad+survey*P
sentiment fea-
tures (POS)
JJ*P, NN1*P, JJ+NN1*P
Table 2: Generating lexical elements and sentiment fea-
tures from a positive review sentence.
3 Sentiment Sensitive Thesaurus
One solution to the feature mismatch problem out-
lined above is to use a thesaurus that groups differ-
ent words that express the same sentiment. For ex-
ample, if we know that both excellent and delicious
are positive sentiment words, then we can use this
knowledge to expand a feature vector that contains
the word delicious using the word excellent, thereby
reducing the mismatch between features in a test in-
stance and a trained model. Below we describe a
method to construct a sentiment sensitive thesaurus
for feature expansion.
Given a labeled or an unlabeled review, we first
split the review into individual sentences. We carry
out part-of-speech (POS) tagging and lemmatiza-
tion on each review sentence using the RASP sys-
tem (Briscoe et al, 2006). Lemmatization reduces
the data sparseness and has been shown to be effec-
tive in text classification tasks (Joachims, 1998). We
then apply a simple word filter based on POS tags to
select content words (nouns, verbs, adjectives, and
adverbs). In particular, previous work has identified
adjectives as good indicators of sentiment (Hatzi-
vassiloglou and McKeown, 1997; Wiebe, 2000).
Following previous work in cross-domain sentiment
classification, we model a review as a bag of words.
We select unigrams and bigrams from each sentence.
For the remainder of this paper, we will refer to un-
igrams and bigrams collectively as lexical elements.
Previous work on sentiment classification has shown
that both unigrams and bigrams are useful for train-
ing a sentiment classifier (Blitzer et al, 2007). We
note that it is possible to create lexical elements both
from source domain labeled reviews as well as from
unlabeled reviews in source and target domains.
Next, we represent each lexical element u using a
set of features as follows. First, we select other lex-
ical elements that co-occur with u in a review sen-
tence as features. Second, from each source domain
labeled review sentence in which u occurs, we cre-
ate sentiment features by appending the label of the
review to each lexical element we generate from that
review. For example, consider the sentence selected
from a positive review of a book shown in Table 2.
In Table 2, we use the notation ?*P? to indicate posi-
tive sentiment features and ?*N? to indicate negative
sentiment features. The example sentence shown in
Table 2 is selected from a positively labeled review,
and generates positive sentiment features as shown
in Table 2. In addition to word-level sentiment fea-
tures, we replace words with their POS tags to create
134
POS-level sentiment features. POS tags generalize
the word-level sentiment features, thereby reducing
feature sparseness.
Let us denote the value of a feature w in the fea-
ture vector u representing a lexical element u by
f(u, w). The vector u can be seen as a compact rep-
resentation of the distribution of a lexical element u
over the set of features that co-occur with u in the re-
views. From the construction of the feature vector u
described in the previous paragraph, it follows that
w can be either a sentiment feature or another lexical
element that co-occurs with u in some review sen-
tence. The distributional hypothesis (Harris, 1954)
states that words that have similar distributions are
semantically similar. We compute f(u, w) as the
pointwise mutual information between a lexical ele-
ment u and a feature w as follows:
f(u, w) = log
(
c(u,w)
N
?n
i=1 c(i,w)
N ?
?m
j=1 c(u,j)
N
)
(1)
Here, c(u,w) denotes the number of review sen-
tences in which a lexical element u and a feature
w co-occur, n and m respectively denote the total
number of lexical elements and the total number of
features, and N =
?n
i=1
?m
j=1 c(i, j). Pointwise
mutual information is known to be biased towards
infrequent elements and features. We follow the dis-
counting approach of Pantel & Ravichandran (2004)
to overcome this bias.
Next, for two lexical elements u and v (repre-
sented by feature vectors u and v, respectively), we
compute the relatedness ?(v, u) of the feature v to
the feature u as follows,
?(v, u) =
?
w?{x|f(v,x)>0} f(u, w)
?
w?{x|f(u,x)>0} f(u, w)
. (2)
Here, we use the set notation {x|f(v, x) > 0} to
denote the set of features that co-occur with v. Re-
latedness of a lexical element u to another lexical
element v is the fraction of feature weights in the
feature vector for the element u that also co-occur
with the features in the feature vector for the ele-
ment v. If there are no features that co-occur with
both u and v, then the relatedness reaches its min-
imum value of 0. On the other hand if all features
that co-occur with u also co-occur with v, then the
relatedness , ?(v, u), reaches its maximum value of
1. Note that relatedness is an asymmetric measure
by the definition given in Equation 2, and the relat-
edness ?(v, u) of an element v to another element u
is not necessarily equal to ?(u, v), the relatedness of
u to v.
We use the relatedness measure defined in Equa-
tion 2 to construct a sentiment sensitive thesaurus in
which, for each lexical element u we list lexical el-
ements v that co-occur with u (i.e. f(u, v) > 0) in
descending order of relatedness values ?(v, u). In
the remainder of the paper, we use the term base en-
try to refer to a lexical element u for which its related
lexical elements v (referred to as the neighbors of u)
are listed in the thesaurus. Note that relatedness val-
ues computed according to Equation 2 are sensitive
to sentiment labels assigned to reviews in the source
domain, because co-occurrences are computed over
both lexical and sentiment elements extracted from
reviews. In other words, the relatedness of an ele-
ment u to another element v depends upon the sen-
timent labels assigned to the reviews that generate u
and v. This is an important fact that differentiates
our sentiment-sensitive thesaurus from other distri-
butional thesauri which do not consider sentiment
information.
Moreover, we only need to retain lexical elements
in the sentiment sensitive thesaurus because when
predicting the sentiment label for target reviews (at
test time) we cannot generate sentiment elements
from those (unlabeled) reviews, therefore we are
not required to find expansion candidates for senti-
ment elements. However, we emphasize the fact that
the relatedness values between the lexical elements
listed in the sentiment-sensitive thesaurus are com-
puted using co-occurrences with both lexical and
sentiment features, and therefore the expansion can-
didates selected for the lexical elements in the tar-
get domain reviews are sensitive to sentiment labels
assigned to reviews in the source domain. Using
a sparse matrix format and approximate similarity
matching techniques (Sarawagi and Kirpal, 2004),
we can efficiently create a thesaurus from a large set
of reviews.
4 Feature Expansion
Our feature expansion phase augments a feature vec-
tor with additional related features selected from the
135
sentiment-sensitive thesaurus created in Section 3 to
overcome the feature mismatch problem. First, fol-
lowing the bag-of-words model, we model a review
d using the set {w1, . . . , wN}, where the elements
wi are either unigrams or bigrams that appear in the
review d. We then represent a review d by a real-
valued term-frequency vector d ? RN , where the
value of the j-th element dj is set to the total number
of occurrences of the unigram or bigram wj in the
review d. To find the suitable candidates to expand a
vector d for the review d, we define a ranking score
score(ui,d) for each base entry in the thesaurus as
follows:
score(ui,d) =
?N
j=1 dj?(wj , ui)
?N
l=1 dl
(3)
According to this definition, given a review d, a base
entry ui will have a high ranking score if there are
many words wj in the review d that are also listed
as neighbors for the base entry ui in the sentiment-
sensitive thesaurus. Moreover, we weight the re-
latedness scores for each word wj by its normal-
ized term-frequency to emphasize the salient uni-
grams and bigrams in a review. Recall that related-
ness is defined as an asymmetric measure in Equa-
tion 2, and we use ?(wj , ui) in the computation of
score(ui,d) in Equation 3. This is particularly im-
portant because we would like to score base entries
ui considering all the unigrams and bigrams that ap-
pear in a review d, instead of considering each uni-
gram or bigram individually.
To expand a vector, d, for a review d, we first
rank the base entries, ui using the ranking score
in Equation 3 and select the top k ranked base en-
tries. Let us denote the r-th ranked (1 ? r ? k)
base entry for a review d by vrd. We then extend the
original set of unigrams and bigrams {w1, . . . , wN}
by the base entries v1d, . . . , v
k
d to create a new vec-
tor d? ? R(N+k) with dimensions corresponding to
w1, . . . , wN , v1d, . . . , v
k
d for a review d. The values
of the extended vector d? are set as follows. The
values of the first N dimensions that correspond to
unigrams and bigrams wi that occur in the review d
are set to di, their frequency in d. The subsequent k
dimensions that correspond to the top ranked based
entries for the review d are weighted according to
their ranking score. Specifically, we set the value of
the r-th ranked base entry vrd to 1/r. Alternatively,
one could use the ranking score, score(vrd, d), itself
as the value of the appended base entries. However,
both relatedness scores as well as normalized term-
frequencies can be small in practice, which leads to
very small absolute ranking scores. By using the
inverse rank, we only take into account the rela-
tive ranking of base entries and ignore their absolute
scores.
Note that the score of a base entry depends on a
review d. Therefore, we select different base en-
tries as additional features for expanding different
reviews. Furthermore, we do not expand each wi
individually when expanding a vector d for a re-
view. Instead, we consider all unigrams and bi-
grams in d when selecting the base entries for ex-
pansion. One can think of the feature expansion pro-
cess as a lower dimensional latent mapping of fea-
tures onto the space spanned by the base entries in
the sentiment-sensitive thesaurus. The asymmetric
property of the relatedness (Equation 2) implicitly
prefers common words that co-occur with numerous
other words as expansion candidates. Such words
act as domain independent pivots and enable us to
transfer the information regarding sentiment from
one domain to another.
Using the extended vectors d? to represent re-
views, we train a binary classifier from the source
domain labeled reviews to predict positive and neg-
ative sentiment in reviews. We differentiate the ap-
pended base entries vrd from wi that existed in the
original vector d (prior to expansion) by assigning
different feature identifiers to the appended base en-
tries. For example, a unigram excellent in a feature
vector is differentiated from the base entry excellent
by assigning the feature id, ?BASE=excellent? to the
latter. This enables us to learn different weights for
base entries depending on whether they are useful
for expanding a feature vector. We use L1 regu-
larized logistic regression as the classification algo-
rithm (Ng, 2004), which produces a sparse model in
which most irrelevant features are assigned a zero
weight. This enables us to select useful features for
classification in a systematic way without having to
preselect features using heuristic approaches. The
regularization parameter is set to its default value
of 1 for all the experiments described in this paper.
136
5 Experiments
5.1 Dataset
To evaluate our method we use the cross-domain
sentiment classification dataset prepared by Blitzer
et al (2007). This dataset consists of Amazon prod-
uct reviews for four different product types: books
(B), DVDs (D), electronics (E) and kitchen appli-
ances (K). There are 1000 positive and 1000 neg-
ative labeled reviews for each domain. Moreover,
the dataset contains some unlabeled reviews (on av-
erage 17, 547) for each domain. This benchmark
dataset has been used in much previous work on
cross-domain sentiment classification and by eval-
uating on it we can directly compare our method
against existing approaches.
Following previous work, we randomly select 800
positive and 800 negative labeled reviews from each
domain as training instances (i.e. 1600?4 = 6400);
the remainder is used for testing (i.e. 400 ? 4 =
1600). In our experiments, we select each domain in
turn as the target domain, with one or more other do-
mains as sources. Note that when we combine more
than one source domain we limit the total number
of source domain labeled reviews to 1600, balanced
between the domains. For example, if we combine
two source domains, then we select 400 positive and
400 negative labeled reviews from each domain giv-
ing (400 + 400) ? 2 = 1600. This enables us to
perform a fair evaluation when combining multiple
source domains. The evaluation metric is classifica-
tion accuracy on a target domain, computed as the
percentage of correctly classified target domain re-
views out of the total number of reviews in the target
domain.
5.2 Effect of Feature Expansion
To study the effect of feature expansion at train time
compared to test time, we used Amazon reviews for
two further domains, music and video, which were
also collected by Blitzer et al (2007) but are not
part of the benchmark dataset. Each validation do-
main has 1000 positive and 1000 negative labeled
reviews, and 15000 unlabeled reviews. Using the
validation domains as targets, we vary the number
of top k ranked base entries (Equation 3) used for
feature expansion during training (Traink) and test-
ing (Testk), and measure the average classification
0 200 400 600 800 10000
200
400
600
800
1000  
Traink 
Test k
0.776
0.778
0.78
0.782
0.784
0.786
Figure 1: Feature expansion at train vs. test times.
B D K B+D B+K D+K B+D+K50
55
60
65
70
75
80
85
Source Domains
Accu
racy 
on el
ectro
nics d
omai
n
Figure 2: Effect of using multiple source domains.
accuracy. Figure 1 illustrates the results using a heat
map, where dark colors indicate low accuracy val-
ues and light colors indicate high accuracy values.
We see that expanding features only at test time (the
left-most column) does not work well because we
have not learned proper weights for the additional
features. Similarly, expanding features only at train
time (the bottom-most row) also does not perform
well because the expanded features are not used dur-
ing testing. The maximum classification accuracy is
obtained when Testk = 400 and Traink = 800, and
we use these values for the remainder of the experi-
ments described in the paper.
5.3 Combining Multiple Sources
Figure 2 shows the effect of combining multiple
source domains to build a sentiment classifier for
the electronics domain. We see that the kitchen do-
main is the single best source domain when adapt-
ing to the electronics target domain. This behavior
137
0 200 400 600 80040
4550
5560
6570
7580
85
Positive/Negative instances
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 3: Effect of source domain labeled data.
0 0.2 0.4 0.6 0.8 150
55
60
65
70
Source unlabeled dataset size
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 4: Effect of source domain unlabeled data.
is explained by the fact that in general kitchen appli-
ances and electronic items have similar aspects. But
a more interesting observation is that the accuracy
that we obtain when we use two source domains is
always greater than the accuracy if we use those do-
mains individually. The highest accuracy is achieved
when we use all three source domains. Although
not shown here for space limitations, we observed
similar trends with other domains in the benchmark
dataset.
To investigate the impact of the quantity of source
domain labeled data on our method, we vary the
amount of data from zero to 800 reviews, with equal
amounts of positive and negative labeled data. Fig-
ure 3 shows the accuracy with the DVD domain as
the target. Note that source domain labeled data is
used both to create the sentiment sensitive thesaurus
as well as to train the sentiment classifier. When
there are multiple source domains we limit and bal-
ance the number of labeled instances as outlined in
Section 5.1. The amount of unlabeled data is held
constant, so that any change in classification accu-
0 0.2 0.4 0.6 0.8 150
55
60
65
70
Target unlabeled dataset size
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 5: Effect of target domain unlabeled data.
racy is directly attributable to the source domain la-
beled instances. Because this is a binary classifica-
tion task (i.e. positive vs. negative sentiment), a ran-
dom classifier that does not utilize any labeled data
would report a 50% classification accuracy. From
Figure 3, we see that when we increase the amount
of source domain labeled data the accuracy increases
quickly. In fact, by selecting only 400 (i.e. 50% of
the total 800) labeled instances per class, we achieve
the maximum performance in most of the cases.
To study the effect of source and target domain
unlabeled data on the performance of our method,
we create sentiment sensitive thesauri using differ-
ent proportions of unlabeled data. The amount of
labeled data is held constant and is balanced across
multiple domains as outlined in Section 5.1, so any
changes in classification accuracy can be directly at-
tributed to the contribution of unlabeled data. Figure
4 shows classification accuracy on the DVD target
domain when we vary the proportion of source do-
main unlabeled data (target domain?s unlabeled data
is fixed).
Likewise, Figure 5 shows the classification ac-
curacy on the DVD target domain when we vary
the proportion of the target domain?s unlabeled data
(source domains? unlabeled data is fixed). From Fig-
ures 4 and 5, we see that irrespective of the amount
being used, there is a clear performance gain when
we use unlabeled data from multiple source domains
compared to using a single source domain. How-
ever, we could not observe a clear gain in perfor-
mance when we increase the amount of the unla-
beled data used to create the sentiment sensitive the-
saurus.
138
Method K D E B
No Thesaurus 72.61 68.97 70.53 62.72
SCL 80.83 74.56 78.43 72.76
SCL-MI 82.06 76.30 78.93 74.56
SFA 81.48 76.31 75.30 77.73
LSA 79.00 73.50 77.66 70.83
FALSA 80.83 76.33 77.33 73.33
NSS 77.50 73.50 75.50 71.46
Proposed 85.18 78.77 83.63 76.32
Within-Domain 87.70 82.40 84.40 80.40
Table 3: Cross-domain sentiment classification accuracy.
5.4 Cross-Domain Sentiment Classification
Table 3 compares our method against a number of
baselines and previous cross-domain sentiment clas-
sification techniques using the benchmark dataset.
For all previous techniques we give the results re-
ported in the original papers. The No Thesaurus
baseline simulates the effect of not performing any
feature expansion. We simply train a binary clas-
sifier using unigrams and bigrams as features from
the labeled reviews in the source domains and ap-
ply the trained classifier on the target domain. This
can be considered to be a lower bound that does
not perform domain adaptation. SCL is the struc-
tural correspondence learning technique of Blitzer
et al (2006). In SCL-MI, features are selected us-
ing the mutual information between a feature (uni-
gram or bigram) and a domain label. After selecting
salient features, the SCL algorithm is used to train a
binary classifier. SFA is the spectral feature align-
ment technique of Pan et al (2010). Both the LSA
and FALSA techniques are based on latent semantic
analysis (Pan et al, 2010). For the Within-Domain
baseline, we train a binary classifier using the la-
beled data from the target domain. This upper base-
line represents the classification accuracy we could
hope to obtain if we were to have labeled data for the
target domain. Note that this is not a cross-domain
classification setting. To evaluate the benefit of us-
ing sentiment features on our method, we give a NSS
(non-sentiment sensitive) baseline in which we cre-
ate a thesaurus without using any sentiment features.
Proposed is our method.
From Table 3, we see that our proposed method
returns the best cross-domain sentiment classifica-
tion accuracy (shown in boldface) for the three do-
mains kitchen appliances, DVDs, and electronics.
For the books domain, the best results are returned
by SFA. The books domain has the lowest number
of unlabeled reviews (around 5000) in the dataset.
Because our method relies upon the availability of
unlabeled data for the construction of a sentiment
sensitive thesaurus, we believe that this accounts for
our lack of performance on the books domain. How-
ever, given that it is much cheaper to obtain unla-
beled than labeled data for a target domain, there is
strong potential for improving the performance of
our method in this domain. The analysis of vari-
ance (ANOVA) and Tukey?s honestly significant dif-
ferences (HSD) tests on the classification accuracies
for the four domains show that our method is sta-
tistically significantly better than both the No The-
saurus and NSS baselines, at confidence level 0.05.
We therefore conclude that using the sentiment sen-
sitive thesaurus for feature expansion is useful for
cross-domain sentiment classification. The results
returned by our method are comparable to state-of-
the-art techniques such as SCL-MI and SFA. In par-
ticular, the differences between those techniques and
our method are not statistically significant.
6 Related Work
Compared to single-domain sentiment classifica-
tion, which has been studied extensively in previous
work (Pang and Lee, 2008; Turney, 2002), cross-
domain sentiment classification has only recently re-
ceived attention in response to advances in the area
of domain adaptation. Aue and Gammon (2005) re-
port a number of empirical tests into domain adap-
tation of sentiment classifiers using an ensemble of
classifiers. However, most of these tests were un-
able to outperform a simple baseline classifier that
is trained using all labeled data for all domains.
Blitzer et al (2007) apply the structural corre-
spondence learning (SCL) algorithm to train a cross-
domain sentiment classifier. They first chooses a set
of pivot features using pointwise mutual informa-
tion between a feature and a domain label. Next,
linear predictors are learnt to predict the occur-
rences of those pivots. Finally, they use singular
value decomposition (SVD) to construct a lower-
dimensional feature space in which a binary classi-
139
fier is trained. The selection of pivots is vital to the
performance of SCL and heuristically selected pivot
features might not guarantee the best performance
on target domains. In contrast, our method uses all
features when creating the thesaurus and selects a
subset of features during training using L1 regular-
ization. Moreover, we do not require SVD, which
has cubic time complexity so can be computation-
ally expensive for large datasets.
Pan et al (2010) use structural feature alignment
(SFA) to find an alignment between domain spe-
cific and domain independent features. The mu-
tual information of a feature with domain labels is
used to classify domain specific and domain inde-
pendent features. Next, spectral clustering is per-
formed on a bipartite graph that represents the re-
lationship between the two sets of features. Fi-
nally, the top eigenvectors are selected to construct
a lower-dimensional projection. However, not all
words can be cleanly classified into domain spe-
cific or domain independent, and this process is con-
ducted prior to training a classifier. In contrast, our
method lets a particular lexical entry to be listed as
a neighour for multiple base entries. Moreover, we
expand each feature vector individually and do not
require any clustering. Furthermore, unlike SCL and
SFA, which consider a single source domain, our
method can efficiently adapt from multiple source
domains.
7 Conclusions
We have described and evaluated a method to
construct a sentiment-sensitive thesaurus to bridge
the gap between source and target domains in
cross-domain sentiment classification using multi-
ple source domains. Experimental results using a
benchmark dataset for cross-domain sentiment clas-
sification show that our proposed method can im-
prove classification accuracy in a sentiment classi-
fier. In future, we intend to apply the proposed
method to other domain adaptation tasks.
Acknowledgements
This research was conducted while the first author
was a visiting research fellow at Sussex university
under the postdoctoral fellowship of the Japan Soci-
ety for the Promotion of Science (JSPS).
References
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
Technical report, Microsoft Research.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP 2006.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
ACL 2007, pages 440?447.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In COL-
ING/ACL 2006 Interactive Presentation Sessions.
Teng-Kai Fan and Chia-Hui Chang. 2010. Sentiment-
oriented contextual advertising. Knowledge and Infor-
mation Systems, 23(3):321?344.
Hui Fang. 2008. A re-examination of query expansion
using lexical resources. In ACL 2008, pages 139?147.
Z. Harris. 1954. Distributional structure. Word, 10:146?
162.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In ACL 1997, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD 2004, pages 168?177.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In ECML 1998, pages 137?142.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009.
Rated aspect summarization of short comments. In
WWW 2009, pages 131?140.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In ICML 2004.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
WWW 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In EMNLP 2002, pages 79?
86.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In NAACL-
HLT?04, pages 321 ? 328.
Sunita Sarawagi and Alok Kirpal. 2004. Efficient set
joins on similarity predicates. In SIGMOD ?04, pages
743?754.
140
Dou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, Qiang
Yang, Zheng Chen, and Ying Li. 2009. Exploit-
ing term relationship to boost text classification. In
CIKM?09, pages 1637 ? 1640.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL 2002, pages 417?424.
Janyce M. Wiebe. 2000. Learning subjective adjective
from corpora. In AAAI 2000, pages 735?740.
141
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613?623,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning to Predict Distributions of Words Across Domains
Danushka Bollegala
Department of Computer Science
University of Liverpool
Liverpool,
L69 3BX, UK
danushka.bollegala@
liverpool.ac.uk
David Weir
Department of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
sussex.ac.uk
John Carroll
Department of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
j.a.carroll@
sussex.ac.uk
Abstract
Although the distributional hypothesis has
been applied successfully in many natural
language processing tasks, systems using
distributional information have been lim-
ited to a single domain because the dis-
tribution of a word can vary between do-
mains as the word?s predominant mean-
ing changes. However, if it were pos-
sible to predict how the distribution of
a word changes from one domain to an-
other, the predictions could be used to
adapt a system trained in one domain to
work in another. We propose an unsuper-
vised method to predict the distribution of
a word in one domain, given its distribu-
tion in another domain. We evaluate our
method on two tasks: cross-domain part-
of-speech tagging and cross-domain sen-
timent classification. In both tasks, our
method significantly outperforms compet-
itive baselines and returns results that are
statistically comparable to current state-
of-the-art methods, while requiring no
task-specific customisations.
1 Introduction
The Distributional Hypothesis, summarised by the
memorable line of Firth (1957) ? You shall know
a word by the company it keeps ? has inspired a
diverse range of research in natural language pro-
cessing. In such work, a word is represented by
the distribution of other words that co-occur with
it. Distributional representations of words have
been successfully used in many language process-
ing tasks such as entity set expansion (Pantel et al,
2009), part-of-speech (POS) tagging and chunk-
ing (Huang and Yates, 2009), ontology learning
(Curran, 2005), computing semantic textual sim-
ilarity (Besanc?on et al, 1999), and lexical infer-
ence (Kotlerman et al, 2012).
However, the distribution of a word often varies
from one domain
1
to another. For example, in
the domain of portable computer reviews the word
lightweight is often associated with positive sen-
timent bearing words such as sleek or compact,
whereas in the movie review domain the same
word is often associated with negative sentiment-
bearing words such as superficial or formulaic.
Consequently, the distributional representations of
the word lightweight will differ considerably be-
tween the two domains. In this paper, given the
distribution w
S
of a word w in the source domain
S, we propose an unsupervised method for pre-
dicting its distributionw
T
in a different target do-
main T .
The ability to predict how the distribution of a
word varies from one domain to another is vital
for numerous adaptation tasks. For example, un-
supervised cross-domain sentiment classification
(Blitzer et al, 2007; Aue and Gamon, 2005) in-
volves using sentiment-labeled user reviews from
the source domain, and unlabeled reviews from
both the source and the target domains to learn
a sentiment classifier for the target domain. Do-
main adaptation (DA) of sentiment classification
becomes extremely challenging when the distribu-
tions of words in the source and the target domains
are very different, because the features learnt from
the source domain labeled reviews might not ap-
pear in the target domain reviews that must be
classified. By predicting the distribution of a word
across different domains, we can find source do-
main features that are similar to the features in
target domain reviews, thereby reducing the mis-
match of features between the two domains.
We propose a two-step unsupervised approach
to predict the distribution of a word across do-
mains. First, we create two lower dimensional la-
1
In this paper, we use the term domain to refer to a col-
lection of documents about a particular topic, for example
reviews of a particular kind of product.
613
tent feature spaces separately for the source and
the target domains using Singular Value Decom-
position (SVD). Second, we learn a mapping from
the source domain latent feature space to the tar-
get domain latent feature space using Partial Least
Square Regression (PLSR). The SVD smoothing
in the first step both reduces the data sparseness in
distributional representations of individual words,
as well as the dimensionality of the feature space,
thereby enabling us to efficiently and accurately
learn a prediction model using PLSR in the sec-
ond step. Our proposed cross-domain word dis-
tribution prediction method is unsupervised in the
sense that it does not require any labeled data in
either of the two steps.
Using two popular multi-domain datasets, we
evaluate the proposed method in two prediction
tasks: (a) predicting the POS of a word in a tar-
get domain, and (b) predicting the sentiment of a
review in a target domain. Without requiring any
task specific customisations, systems based on our
distribution prediction method significantly out-
perform competitive baselines in both tasks. Be-
cause our proposed distribution prediction method
is unsupervised and task independent, it is poten-
tially useful for a wide range of DA tasks such en-
tity extraction (Guo et al, 2009) or dependency
parsing (McClosky et al, 2010). Our contribu-
tions are summarised as follows:
? Given the distribution w
S
of a word w in a
source domain S, we propose a method for
learning its distribution w
T
in a target do-
main T .
? Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain POS tagger.
? Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain sentiment classifier.
To our knowledge, ours is the first successful at-
tempt to learn a model that predicts the distribu-
tion of a word across different domains.
2 Related Work
Learning semantic representations for words us-
ing documents from a single domain has received
much attention lately (Vincent et al, 2010; Socher
et al, 2013; Baroni and Lenci, 2010). As we have
already discussed, the semantics of a word varies
across different domains, and such variations are
not captured by models that only learn a single se-
mantic representation for a word using documents
from a single domain.
The POS of a word is influenced both by its
context (contextual bias), and the domain of the
document in which it appears (lexical bias). For
example, the word signal is predominately used
as a noun in MEDLINE, whereas it appears pre-
dominantly as an adjective in the Wall Street Jour-
nal (WSJ) (Blitzer et al, 2006). Consequently, a
tagger trained on WSJ would incorrectly tag sig-
nal in MEDLINE. Blitzer et al (2006) append
the source domain labeled data with predicted piv-
ots (i.e. words that appear in both the source and
target domains) to adapt a POS tagger to a tar-
get domain. Choi and Palmer (2012) propose
a cross-domain POS tagging method by training
two separate models: a generalised model and a
domain-specific model. At tagging time, a sen-
tence is tagged by the model that is most similar
to that sentence. Huang and Yates (2009) train a
Conditional Random Field (CRF) tagger with fea-
tures retrieved from a smoothing model trained us-
ing both source and target domain unlabeled data.
Adding latent states to the smoothing model fur-
ther improves the POS tagging accuracy (Huang
and Yates, 2012). Schnabel and Sch?utze (2013)
propose a training set filtering method where they
eliminate shorter words from the training data
based on the intuition that longer words are more
likely to be examples of productive linguistic pro-
cesses than shorter words.
The sentiment of a word can vary from one do-
main to another. In Structural Correspondence
Learning (SCL) (Blitzer et al, 2006; Blitzer et
al., 2007), a set of pivots are chosen using point-
wise mutual information. Linear predictors are
then learnt to predict the occurrence of those piv-
ots, and SVD is used to construct a lower dimen-
sional representation in which a binary classifier
is trained. Spectral Feature Alignment (SFA) (Pan
et al, 2010) also uses pivots to compute an align-
ment between domain specific and domain inde-
pendent features. Spectral clustering is performed
on a bipartite graph representing domain specific
and domain independent features to find a lower-
dimensional projection between the two sets of
features. The cross-domain sentiment-sensitive
thesaurus (SST) (Bollegala et al, 2011) groups
together words that express similar sentiments in
614
different domains. The created thesaurus is used to
expand feature vectors during train and test stages
in a binary classifier. However, unlike our method,
SCL, SFA, or SST do not learn a prediction model
between word distributions across domains.
Prior knowledge of the sentiment of words, such
as sentiment lexicons, has been incorporated into
cross-domain sentiment classification. He et al
(2011) propose a joint sentiment-topic model that
imposes a sentiment-prior depending on the oc-
currence of a word in a sentiment lexicon. Pono-
mareva and Thelwall (2012) represent source and
target domain reviews as nodes in a graph and ap-
ply a label propagation algorithm to predict the
sentiment labels for target domain reviews from
the sentiment labels in source domain reviews. A
sentiment lexicon is used to create features for a
document. Although incorporation of prior senti-
ment knowledge is a promising technique to im-
prove accuracy in cross-domain sentiment classi-
fication, it is complementary to our task of distri-
bution prediction across domains.
The unsupervised DA setting that we consider
does not assume the availability of labeled data for
the target domain. However, if a small amount of
labeled data is available for the target domain, it
can be used to further improve the performance of
DA tasks (Xiao et al, 2013; Daum?e III, 2007).
3 Distribution Prediction
3.1 In-domain Feature Vector Construction
Before we tackle the problem of learning a model
to predict the distribution of a word across do-
mains, we must first compute the distribution of
a word from a single domain. For this purpose, we
represent a word w using unigrams and bigrams
that co-occur with w in a sentence as follows.
Given a document H, such as a user-review of
a product, we split H into sentences, and lemma-
tize each word in a sentence using the RASP sys-
tem (Briscoe et al, 2006). Using a standard stop
word list, we filter out frequent non-content un-
igrams and select the remainder as unigram fea-
tures to represent a sentence. Next, we generate
bigrams of word lemmas and remove any bigrams
that consists only of stop words. Bigram features
capture negations more accurately than unigrams,
and have been found to be useful for sentiment
classification tasks. Table 1 shows the unigram
and bigram features we extract for a sentence us-
ing this procedure. Using data from a single do-
sentence This is an interesting and well researched book
unigrams this, is, an, interesting, and, well, researched,
(surface) book
unigrams this, be, an, interest, and, well, research, book
(lemma)
unigrams interest, well, research, book
(features)
bigrams this+be, be+an, an+interest, interest+and,
(lemma) and+well, well+research, research+book
bigrams an+interest, interest+and, and+well,
(features) well+research, research+book
Table 1: Extracting unigram and bigram features.
main, we construct a feature co-occurrence ma-
trix A in which columns correspond to unigram
features and rows correspond to either unigram or
bigram features. The value of the element a
ij
in
the co-occurrence matrix A is set to the number of
sentences in which the i-th and j-th features co-
occur.
Typically, the number of unique bigrams is
much larger than that of unigrams. Moreover, co-
occurrences of bigrams are rare compared to co-
occurrences of unigrams, and co-occurrences in-
volving a unigram and a bigram. Consequently,
in matrix A, we consider co-occurrences only be-
tween unigrams vs. unigrams, and bigrams vs.
unigrams. We consider each row in A as repre-
senting the distribution of a feature (i.e. unigrams
or bigrams) in a particular domain over the uni-
gram features extracted from that domain (repre-
sented by the columns of A). We apply Positive
Pointwise Mutual Information (PPMI) to the co-
occurrence matrix A. This is a variation of the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), in which all PMI values that are less
than zero are replaced with zero (Lin, 1998; Bul-
linaria and Levy, 2007). Let F be the matrix that
results when PPMI is applied to A. Matrix F has
the same number of rows, n
r
, and columns, n
c
, as
the raw co-occurrence matrix A.
Note that in addition to the above-mentioned
representation, there are many other ways to rep-
resent the distribution of a word in a particular do-
main (Turney and Pantel, 2010). For example,
one can limit the definition of co-occurrence to
words that are linked by some dependency relation
(Pado and Lapata, 2007), or extend the window
of co-occurrence to the entire document (Baroni
and Lenci, 2010). Since the method we propose
in Section 3.2 to predict the distribution of a word
across domains does not depend on the particular
615
feature representation method, any of these alter-
native methods could be used.
To reduce the dimensionality of the feature
space, and create dense representations for words,
we perform SVD on F. We use the left singu-
lar vectors corresponding to the k largest singular
values to compute a rank k approximation
?
F, of
F. We perform truncated SVD using SVDLIBC
2
.
Each row in
?
F is considered as representing a word
in a lower k (n
c
) dimensional feature space cor-
responding to a particular domain. Distribution
prediction in this lower dimensional feature space
is preferrable to prediction over the original fea-
ture space because there are reductions in overfit-
ting, feature sparseness, and the learning time. We
created two matrices,
?
F
S
and
?
F
T
from the source
and target domains, respectively, using the above
mentioned procedure.
3.2 Cross-Domain Feature Vector Prediction
We propose a method to learn a model that can
predict the distribution w
T
of a word w in the
target domain T , given its distribution w
S
in
the source domain S. We denote the set of
features that occur in both domains by W =
{w
(1)
, . . . , w
(n)
}. In the literature, such features
are often referred to as pivots, and they have been
shown to be useful for DA, allowing the weights
learnt to be transferred from one domain to an-
other. Various criteria have been proposed for se-
lecting a small set of pivots for DA, such as the
mutual information of a word with the two do-
mains (Blitzer et al, 2007). However, we do not
impose any further restrictions on the set of pivots
W other than that they occur in both domains.
For each word w
(i)
? W , we denote the cor-
responding rows in
?
F
S
and
?
F
T
by column vec-
tors w
(i)
S
and w
(i)
T
. Note that the dimensional-
ity of w
(i)
S
and w
(i)
T
need not be equal, and we
may select different numbers of singular vectors
to approximate
?
F
S
and
?
F
T
. We model distribu-
tion prediction as a multivariate regression prob-
lem where, given a set {(w
(i)
S
,w
(i)
T
)}
n
i=1
consist-
ing of pairs of feature vectors selected from each
domain for the pivots in W , we learn a mapping
from the inputs (w
(i)
S
) to the outputs (w
(i)
T
).
We use Partial Least Squares Regression
(PLSR) (Wold, 1985) to learn a regression model
using pairs of vectors. PLSR has been applied in
2
http://tedlab.mit.edu/
?
dr/SVDLIBC/
Algorithm 1 Learning a prediction model.
Input: X, Y, L.
Output: Prediction matrix M.
1: Randomly select ?
l
from columns in Y
l
.
2: v
l
= X
l
>?
l
/
?
?
?
?
X
l
>?
l
?
?
?
?
3: ?
l
= X
l
v
l
4: q
l
= Y
l
>?
l
/
?
?
?
?
Y
l
>?
l
?
?
?
?
5: ?
l
= Y
l
q
l
6: If ?
l
is unchanged go to Line 7; otherwise go to Line 2
7: c
l
= ?
l
>?
l
/
?
?
?
??
l
>?
l
?
?
?
?
8: p
l
= X
l
>?
l
/?
l
>?
l
9: X
l+1
= X
l
? ?
l
p
l
>
and Y
l+1
= Y
l
? c
l
?
l
q
l
>
.
10: Stop if l = L; otherwise l = l + 1 and return to Line 1.
11: Let C = diag(c
1
, . . . , c
L
), and V = [v
1
. . .v
L
]
12: M = V(P
>
V)
?1
CQ
>
13: return M
Chemometrics (Geladi and Kowalski, 1986), pro-
ducing stable prediction models even when the
number of samples is considerably smaller than
the dimensionality of the feature space. In particu-
lar, PLSR fits a smaller number of latent variables
(10? 100 in practice) such that the correlation be-
tween the feature vectors for pivots in the two do-
mains are maximised in this latent space.
Let X and Y denote matrices formed by ar-
ranging respectively the vectors w
(i)
S
s and w
(i)
T
in
rows. PLSR decomposes X and Y into a series of
products between rank 1 matrices as follows:
X ?
L
?
l=1
?
l
p
l
>
= ?P
>
(1)
Y ?
L
?
l=1
?
l
q
l
>
= ?Q
>
. (2)
Here, ?
l
, ?
l
, p
l
, and q
l
are column vectors, and
the summation is taken over the rank 1 matrices
that result from the outer product of those vectors.
The matrices, ?, ?, P, and Q are constructed re-
spectively by arranging ?
l
, ?
l
, p
l
, and q
l
vectors
as columns.
Our method for learning a distribution predic-
tion model is shown in Algorithm 1. It is based on
the two block NIPALS routine (Wold, 1975; Rosi-
pal and Kramer, 2006) and iteratively discovers L
pairs of vectors (?
l
,?
l
) such that the covariances,
Cov(?
l
,?
l
), are maximised under the constraint
||p
l
|| = ||q
l
|| = 1. Finally, the prediction matrix,
M is computed using ?
l
,?
l
,p
l
, q
l
. The predicted
distribution
?
w
T
of a word w in T is given by
?
w
T
= Mw
S
. (3)
616
Our distribution prediction learning method is un-
supervised in the sense that it does not require
manually labeled data for a particular task from
any of the domains. This is an important point,
and means that the distribution prediction method
is independent of the task to which it may subse-
quently be applied. As we go on to show in Sec-
tion 6, this enables us to use the same distribution
prediction method for both POS tagging and sen-
timent classification.
4 Domain Adaptation
The main reason that a model trained only on the
source domain labeled data performs poorly in
the target domain is the feature mismatch ? few
features in target domain test instances appear in
source domain training instances. To overcome
this problem, we use the proposed distribution pre-
diction method to find those related features in the
source domain that correspond to the features ap-
pearing in the target domain test instances.
We consider two DA tasks: (a) cross-domain
POS tagging (Section 4.1), and (b) cross-domain
sentiment classification (Section 4.2). Note that
our proposed distribution prediction method can
be applied to numerous other NLP tasks that in-
volve sequence labelling and document classifica-
tion.
4.1 Cross-Domain POS Tagging
We represent each word using a set of features
such as capitalisation (whether the first letter of the
word is capitalised), numeric (whether the word
contains digits), prefixes up to four letters, and
suffixes up to four letters (Miller et al, 2011).
Next, for each word w in a source domain labeled
(i.e. manually POS tagged) sentence, we select its
neighbours u
(i)
in the source domain as additional
features. Specifically, we measure the similarity,
sim(u
(i)
S
,w
S
), between the source domain distri-
butions of u
(i)
and w, and select the top r simi-
lar neighbours u
(i)
for each word w as additional
features for w. We refer to such features as dis-
tributional features in this work. The value of a
neighbour u
(i)
selected as a distributional feature
is set to its similarity score sim(u
(i)
S
,w
S
). Next,
we train a CRF model using all features (i.e. cap-
italisation, numeric, prefixes, suffixes, and distri-
butional features) on source domain labeled sen-
tences.
We train a PLSR model, M, that predicts the
target domain distribution Mu
(i)
S
of a word u
(i)
in
the source domain labeled sentences, given its dis-
tribution, u
(i)
S
. At test time, for each word w that
appears in a target domain test sentence, we mea-
sure the similarity, sim(Mu
(i)
S
,w
T
), and select
the most similar r words u
(i)
in the source domain
labeled sentences as the distributional features for
w, with their values set to sim(Mu
(i)
S
,w
T
). Fi-
nally, the trained CRF model is applied to a target
domain test sentence.
Note that distributional features are always se-
lected from the source domain during both train
and test times, thereby increasing the number of
overlapping features between the trained model
and test sentences. To make the inference tractable
and efficient, we use a first-order Markov factori-
sation, in which we consider all pairwise combi-
nations between the features for the current word
and its immediate predecessor.
4.2 Cross-Domain Sentiment Classification
Unlike in POS tagging, where we must individ-
ually tag each word in a target domain test sen-
tence, in sentiment classification we must classify
the sentiment for the entire review. We modify the
DA method presented in Section 4.1 to satisfy this
requirement as follows.
Let us assume that we are given a set
{(x
(i)
S
, y
(i)
)}
n
i=1
of n labeled reviews x
(i)
S
for the
source domain S. For simplicity, let us consider
binary sentiment classification where each review
x
(i)
is labeled either as positive (i.e. y
(i)
= 1) or
negative (i.e. y
(i)
= ?1). Our cross-domain bi-
nary sentiment classification method can be easily
extended to the multi-class setting as well. First,
we lemmatise each word in a source domain la-
beled review x
(i)
S
, and extract both unigrams and
bigrams as features to represent x
(i)
S
by a binary-
valued feature vector. Next, we train a binary clas-
sification model, ?, using those feature vectors.
Any binary classification algorithm can be used
to learn ?. In our experiments, we used L2 reg-
ularised logistic regression.
Next, we train a PLSR model, M, as described
in Section 3.2 using unlabeled reviews in the
source and target domains. At test time, we rep-
resent a test target review H using a binary-valued
feature vector h of unigrams and bigrams of lem-
mas of the words in H, as we did for source do-
main labeled train reviews. Next, for each feature
w
(j)
extracted from H, we measure the similarity,
617
sim(Mu
(i)
S
,w
(j)
T
), between the target domain dis-
tribution of w
(j)
, and each feature (unigram or bi-
gram) u
(i)
in the source domain labeled reviews.
We score each source domain feature u
(i)
for its
relatedness to H using the formula:
score(u
(i)
,H) =
1
|H|
|H|
?
j=1
sim(Mu
(i)
S
,w
(j)
T
) (4)
where |H| denotes the total number of features ex-
tracted from the test review H. We select the top
scoring r features u
(i)
as distributional features for
H, and append those to h. The corresponding val-
ues of those distributional features are set to the
scores given by Equation 4. Finally, we classify
h using the trained binary classifier ?. Note that
given a test review, we find the distributional fea-
tures that are similar to all the words in the test re-
view from the source domain. In particular, we do
not find distributional features independently for
each word in the test review. This enables us to
find distributional features that are consistent with
all the features in a test review.
4.3 Model Choices
For both POS tagging and sentiment classifica-
tion, we experimented with several alternative
approaches for feature weighting, representation,
and similarity measures using development data,
which we randomly selected from the training in-
stances from the datasets described in Section 5.
For feature weighting for sentiment classifica-
tion, we considered using the number of occur-
rences of a feature in a review and tf-idf weight-
ing (Salton and Buckley, 1983). For representa-
tion, we considered distributional features u
(i)
in
descending order of their scores given by Equa-
tion 4, and then taking the inverse-rank as the val-
ues for the distributional features (Bollegala et al,
2011). However, none of these alternatives re-
sulted in performance gains. With respect to simi-
larity measures, we experimented with cosine sim-
ilarity and the similarity measure proposed by Lin
(1998); cosine similarity performed consistently
well over all the experimental settings. The feature
representation was held fixed during these similar-
ity measure comparisons.
For POS tagging, we measured the effect of
varying r, the number of distributional features,
using a development dataset. We observed that
setting r larger than 10 did not result in signifi-
cant improvements in tagging accuracy, but only
increased the train time due to the larger feature
space. Consequently, we set r = 10 in POS tag-
ging. For sentiment analysis, we used all features
in the source domain labeled reviews as distri-
butional features, weighted by their scores given
by Equation 4, taking the inverse-rank. In both
tasks, we parallelised similarity computations us-
ing BLAS
3
level-3 routines to speed up the com-
putations. The source code of our implementation
is publicly available
4
.
5 Datasets
To evaluate DA for POS tagging, following Blitzer
et al (2006), we use sections 2 ? 21 from Wall
Street Journal (WSJ) as the source domain labeled
data. An additional 100, 000 WSJ sentences from
the 1988 release of the WSJ corpus are used as the
source domain unlabeled data. Following Schn-
abel and Sch?utze (2013), we use the POS labeled
sentences in the SACNL dataset (Petrov and Mc-
Donald, 2012) for the five target domains: QA fo-
rums, Emails, Newsgroups, Reviews, and Blogs.
Each target domain contains around 1000 POS
labeled test sentences and around 100, 000 unla-
beled sentences.
To evaluate DA for sentiment classification,
we use the Amazon product reviews collected by
Blitzer et al (2007) for four different product cat-
egories: books (B), DVDs (D), electronic items
(E), and kitchen appliances (K). There are 1000
positive and 1000 negative sentiment labeled re-
views for each domain. Moreover, each domain
has on average 17, 547 unlabeled reviews. We use
the standard split of 800 positive and 800 negative
labeled reviews from each domain as training data,
and the remainder for testing.
6 Experiments and Results
For each domain D in the SANCL (POS tag-
ging) and Amazon review (sentiment classifica-
tion) datasets, we create a PPMI weighted co-
occurrence matrix F
D
. On average, F
D
created
for a target domain in the SANCL dataset con-
tains 104, 598 rows and 65, 528 columns, whereas
those numbers in the Amazon dataset are 27, 397
and 35, 200 respectively. In cross-domain senti-
ment classification, we measure the binary senti-
ment classification accuracy for the target domain
3
http://www.openblas.net/
4
http://www.csc.liv.ac.uk/
?
danushka/
software.html
618
test reviews for each pair of domains (12 pairs in
total for 4 domains). On average, we have 40, 176
pivots for a pair of domains in the Amazon dataset.
In cross-domain POS tagging, WSJ is always
the source domain, whereas the five domains in
SANCL dataset are considered as the target do-
mains. For this setting we have 9822 pivots on
average. The number of singular vectors k se-
lected in SVD, and the number of PLSR dimen-
sions L are set respectively to 1000 and 50 for the
remainder of the experiments described in the pa-
per. Later we study the effect of those two param-
eters on the performance of the proposed method.
The L-BFGS (Liu and Nocedal, 1989) method is
used to train the CRF and logistic regression mod-
els.
6.1 POS Tagging Results
Table 2 shows the token-level POS tagging accu-
racy for unseen words (i.e. words that appear in the
target domain test sentences but not in the source
domain labeled train sentences). By limiting the
evaluation to unseen words instead of all words,
we can evaluate the gain in POS tagging accuracy
solely due to DA. The NA (no-adapt) baseline sim-
ulates the effect of not performing any DA. Specif-
ically, in POS tagging, a CRF trained on source
domain labeled sentences is applied to target do-
main test sentences, whereas in sentiment classi-
fication, a logistic regression classifier trained us-
ing source domain labeled reviews is applied to the
target domain test reviews. The S
pred
baseline di-
rectly uses the source domain distributions for the
words instead of projecting them to the target do-
main. This is equivalent to setting the prediction
matrix M to the unit matrix. The T
pred
baseline
uses the target domain distribution w
T
for a word
w instead of Mw
S
. If w does not appear in the
target domain, then w
T
is set to the zero vector.
The S
pred
and T
pred
baselines simulate the two al-
ternatives of using source and target domain dis-
tributions instead of learning a PLSR model. The
DA method proposed in Section 4.1 is shown as
the Proposed method. Filter denotes the train-
ing set filtering method proposed by Schnabel and
Sch?utze (2013) for the DA of POS taggers.
From Table 2, we see that the Proposed method
achieves the best performance in all five domains,
followed by the T
pred
baseline. Recall that the
T
pred
baseline cannot find source domain words
that do not appear in the target domain as distri-
Target NA S
pred
T
pred
Filter Proposed
QA 67.34 68.18 68.75 57.08 69.28
?
Emails 65.62 66.62 67.07 65.61 67.09
Newsgroups 75.71 75.09 75.57 70.37 75.85
?
Reviews 56.36 54.60 56.68 47.91 56.93
?
Blogs 76.64 54.78 76.90 74.56 76.97
?
Table 2: POS tagging accuracies on SANCL.
butional features for the words in the target do-
main test reviews. Therefore, when the overlap be-
tween the vocabularies used in the source and the
target domains is small, T
pred
cannot reduce the
mismatch between the feature spaces. Poor perfor-
mance of the S
pred
baseline shows that the distri-
butions of a word in the source and target domains
are different to the extent that the distributional
features found using source domain distributions
are inadequate. The two baselines S
pred
and T
pred
collectively motivate our proposal to learn a distri-
bution prediction model from the source domain
to the target. The improvements of Proposed over
the previously proposed Filter are statistically sig-
nificant in all domains except the Emails domain
(denoted by ? in Table 2 according to the Bino-
mial exact test at 95% confidence). However, the
differences between the T
pred
and Proposed meth-
ods are not statistically significant.
6.2 Sentiment Classification Results
In Figure 1, we compare the Proposed cross-
domain sentiment classification method (Section
4.2) against several baselines and the current state-
of-the-art methods. The baselines NA, S
pred
, and
T
pred
are defined similarly as in Section 6.1. SST
is the Sentiment Sensitive Thesaurus proposed by
Bollegala et al (2011). SST creates a single distri-
bution for a word using both source and target do-
main reviews, instead of two separate distributions
as done by the Proposed method. SCL denotes
the Structural Correspondence Learning method
proposed by Blitzer et al (2006). SFA denotes
the Spectral Feature Alignment method proposed
by Pan et al (2010). SFA and SCL represent the
current state-of-the-art methods for cross-domain
sentiment classification. All methods are evalu-
ated under the same settings, including train/test
split, feature spaces, pivots, and classification al-
gorithms so that any differences in performance
can be directly attributable to their domain adapt-
ability. For each domain, the accuracy obtained
by a classifier trained using labeled data from that
619
E?>B D?>B K?>B5055
6065
7075
8085
Accura
cy
B?>E D?>E K?>E50
60
70
80
90
Accura
cy
B?>D E?>D K?>D5055
6065
7075
8085
Accura
cy
 
 
NA SFA SST SCL Spred Tpred ProposedB?>K E?>K D?>K
50
60
70
80
90
Accura
cy
Figure 1: Cross-Domain sentiment classification.
domain is indicated by a solid horizontal line in
each sub-figure. This upper baseline represents
the classification accuracy we could hope to obtain
if we were to have labeled data for the target do-
main. Clopper-Pearson 95% binomial confidence
intervals are superimposed on each vertical bar.
From Figure 1 we see that the Proposed method
reports the best results in 8 out of the 12 domain
pairs, whereas SCL, SFA, and S
pred
report the
best results in other cases. Except for the D-E set-
ting in which Proposed method significantly out-
performs both SFA and SCL, the performance of
the Proposed method is not statistically signifi-
cantly different to that of SFA or SCL.
The selection of pivots is vital to the perfor-
mance of SFA. However, unlike SFA, which re-
quires us to carefully select a small subset of pivots
(ca. less than 500) using some heuristic approach,
our Proposed method does not require any pivot
selection. Moreover, SFA projects source domain
reviews to a lower-dimensional latent space, in
which a binary sentiment classifier is subsequently
trained. At test time SFA projects a target review
into this lower-dimensional latent space and ap-
plies the trained classifier. In contrast, our Pro-
posed method predicts the distribution of a word
in the target domain, given its distribution in the
source domain, thereby explicitly translating the
source domain reviews to the target. This property
enables us to apply the proposed distribution pre-
diction method to tasks other than sentiment anal-
ysis such as POS tagging where we must identify
distributional features for individual words.
10 100 200 300 400 500 600 700 8000.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
PLSR dimensions
Accu
racy
 
 
E??>BD??>B
Figure 2: The effect of PLSR dimensions.
Unlike our distribution prediction method,
which is unsupervised, SST requires labeled data
for the source domain to learn a feature mapping
between a source and a target domain in the form
of a thesaurus. However, from Figure 1 we see
that in 10 out of the 12 domain-pairs the Proposed
method returns higher accuracies than SST.
To evaluate the overall effect of the number of
singular vectors k used in the SVD step, and the
number of PLSR components L used in Algorithm
1, we conduct two experiments. To evaluate the ef-
fect of the PLSR dimensions, we fixed k = 1000
and measured the cross-domain sentiment classi-
fication accuracy over a range of L values. As
shown in Figure 2, accuracy remains stable across
a wide range of PLSR dimensions. Because the
time complexity of Algorithm 1 increases linearly
with L, it is desirable that we select smaller L val-
620
1000 1500 2000 2500 3000
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
SVD dimensions
Accu
racy
 
 E??>BD??>B
Figure 3: The effect of SVD dimensions.
Measure Distributional features
sim(u
S
, w
S
) thin (0.1733), digestible (0.1728),
small+print (0.1722)
sim(u
T
, w
T
) travel+companion (0.6018), snap-in
(0.6010), touchpad (0.6016)
sim(u
S
, w
T
) segregation (0.1538), participation
(0.1512), depression+era (0.1508)
sim(Mu
S
, w
T
) small (0.2794), compact (0.2641),
sturdy (0.2561)
Table 3: Top 3 distributional features u ? S for
the word lightweight (w).
ues in practice.
To evaluate the effect of the SVD dimensions,
we fixed L = 100 and measured the cross-domain
sentiment classification accuracy for different k
values as shown in Figure 3. We see an overall
decrease in classification accuracy when k is in-
creased. Because the dimensionality of the source
and target domain feature spaces is equal to k, the
complexity of the least square regression problem
increases with k. Therefore, larger k values result
in overfitting to the train data and classification ac-
curacy is reduced on the target test data.
As an example of the distribution prediction
method, in Table 3 we show the top 3 similar
distributional features u in the books (source) do-
main, predicted for the electronics (target) domain
word w = lightweight, by different similarity
measures. Bigrams are indicted by a + sign and
the similarity scores of the distributional features
are shown within brackets.
Using the source domain distributions for both
u and w (i.e. sim(u
S
, w
S
)) produces distribu-
tional features that are specific to the books do-
main, or to the dominant adjectival sense of hav-
ing no importance or influence. On the other
hand, using target domain distributions for u and
w (i.e. sim(u
T
, w
T
)) returns distributional fea-
tures of the dominant nominal sense of lower in
weight frequently associated with electronic de-
vices. Simply using source domain distributions
u
S
(i.e. sim(u
S
, w
T
)) returns totally unrelated dis-
tributional features. This shows that word distribu-
tions in source and target domains are very differ-
ent and some adaptation is required prior to com-
puting distributional features.
Interestingly, we see that by using the dis-
tributions predicted by the proposed method
(i.e. sim(Mu
S
, w
T
)) we overcome this problem
and find relevant distributional features from the
source domain. Although for illustrative purposes
we used the word lightweight, which occurs in
both the source and the target domains, our pro-
posed method does not require the source domain
distribution w
S
for a word w in a target domain
document. Therefore, it can find distributional fea-
tures even for words occurring only in the target
domain, thereby reducing the feature mismatch
between the two domains.
7 Conclusion
We proposed a method to predict the distribution
of a word across domains. We first create a distri-
butional representation for a word using the data
from a single domain, and then learn a Partial
Least Square Regression (PLSR) model to pre-
dict the distribution of a word in a target domain
given its distribution in a source domain. We eval-
uated the proposed method in two domain adapta-
tion tasks: cross-domain POS tagging and cross-
domain sentiment classification. Our experiments
show that without requiring any task-specific cus-
tomisations to our distribution prediction method,
it outperforms competitive baselines and achieves
comparable results to the current state-of-the-art
domain adaptation methods.
References
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. Technical report, Microsoft Research.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673 ? 721.
Romaric Besanc?on, Martin Rajman, and Jean-C?edric
Chappelier. 1999. Textual similarities based on a
621
distributional approach. In Proc. of DEXA, pages
180 ? 184.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120 ?
128.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proc. of ACL, pages 440 ? 447.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a senti-
ment sensitive thesaurus for cross-domain sentiment
classification. In Proc. of ACL/HLT, pages 132 ?
141.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
COLING/ACL Interactive Presentation Sessions.
John A. Bullinaria and Jospeh P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510 ? 526.
Jinho D. Choi and Martha Palmer. 2012. Fast and
robust part-of-speech tagging using dynamic model
selection. In Proc. of ACL Short Papers, volume 2,
pages 363 ? 367.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22 ? 29,
March.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 26 ? 33.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256 ? 263.
John R. Firth. 1957. A synopsis of linguistic theory
1930-55. Studies in Linguistic Analysis, pages 1 ?
32.
Paul Geladi and Bruce R. Kowalski. 1986. Partial
least-squares regression: a tutorial. Analytica Chim-
ica Acta, 185(0):1 ? 17.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named en-
tity recognition. In Proc. of NAACL, pages 281 ?
289.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proc. of
ACL/HLT, pages 123 ? 131.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL-IJCNLP?09, pages 495
? 503.
Fei Huang and Alexander Yates. 2012. Biased repre-
sentation learning for domain adaptation. In Proc.
of EMNLP/CoNLL, pages 1313 ? 1323.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2012. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359 ? 389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL, pages 768 ? 774.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503 ? 528.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL/HLT, pages 28 ? 36.
John E. Miller, Manabu Torii, and K. Vijay-Shanker.
2011. Building domain-specific taggers without an-
notated (domain) data. In Proc. of EMNLP/CoNLL,
pages 1103 ? 1111.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161 ?
199.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proc. of WWW, pages 751 ? 760.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proc. of EMNLP, pages 938 ? 947.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes of
the 1st SANCL Workshop.
Natalia Ponomareva and Mike Thelwall. 2012. Do
neighbours help? an exploration of graph-based al-
gorithms for cross-domain sentiment classification.
In Proc. of EMNLP, pages 655 ? 665.
Roman Rosipal and Nicole Kramer. 2006. Overview
and recent advances in partial least squares. In
C. Saunders et al, editor, SLSFS?05, volume 3940 of
LNCS, pages 34 ? 51, Berlin Heidelberg. Springer-
Verlag.
G. Salton and C. Buckley. 1983. Introduction to
Modern Information Retreival. McGraw-Hill Book
Company.
Tobias Schnabel and Hinrich Sch?utze. 2013. Towards
robust cross-domain domain adaptation for part-of-
speech tagging. In Proc. of IJCNLP, pages 198 ?
206.
622
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proc. of EMNLP, pages 1631 ? 1642.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Aritificial Intelligence Research,
37:141 ? 188.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antonie Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. Journal of Machine Learning
Research, 11:3371 ? 3408.
Herman Wold. 1975. Path models with latent vari-
ables: the NIPALS approach. In H. M. Blalock
et al, editor, Quantitative socialogy: international
perspective on mathematical and statistical model-
ing, pages 307 ? 357. Academic.
Herman Wold. 1985. Partial least squares. In Samel
Kotz and Norman L. Johnson, editors, Encyclopedia
of the Statistical Sciences, pages 581 ? 591. Wiley.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In
Proc. of EMNLP, pages 152 ? 162.
623
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 55?58,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Semi-Supervised Classification of Discourse Relations using
Feature Correlations
Hugo Hernault and Danushka Bollegala and Mitsuru Ishizuka
Graduate School of Information Science & Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
hugo@mi.ci.i.u-tokyo.ac.jp
danushka@iba.t.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Abstract
Two of the main corpora available for
training discourse relation classifiers are
the RST Discourse Treebank (RST-DT)
and the Penn Discourse Treebank (PDTB),
which are both based on the Wall Street
Journal corpus. Most recent work us-
ing discourse relation classifiers have em-
ployed fully-supervised methods on these
corpora. However, certain discourse rela-
tions have little labeled data, causing low
classification performance for their asso-
ciated classes. In this paper, we attempt
to tackle this problem by employing a
semi-supervised method for discourse re-
lation classification. The proposed method
is based on the analysis of feature co-
occurrences in unlabeled data. This in-
formation is then used as a basis to ex-
tend the feature vectors during training.
The proposed method is evaluated on both
RST-DT and PDTB, where it significantly
outperformed baseline classifiers. We be-
lieve that the proposed method is a first
step towards improving classification per-
formance, particularly for discourse rela-
tions lacking annotated data.
1 Introduction
The RST Discourse Treebank (RST-DT) (Carl-
son et al, 2001), based on the Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
framework, and the Penn Discourse Treebank
(PDTB) (Prasad et al, 2008), are two of the most
widely-used corpora for training discourse rela-
tion classifiers. They are both based on the Wall
Street Journal (WSJ) corpus, although there are
substantial differences in the relation taxonomy
used to annotate the corpus. These corpora have
been used in most of the recent work employ-
ing discourse relation classifiers, which are based
on fully-supervised machine learning approaches
(duVerle and Prendinger, 2009; Pitler et al, 2009;
Lin et al, 2009).
Still, when building a discourse relation clas-
sifier on either corpus, one is faced with the
same practical issue: Certain relations are very
prevalent, such as ELABORATION[N][S] (RST-
DT), with more than 4000 instances, whereas
other occur rarely, such as EVALUATION[N][N]1
(RST-DT), with three instances, or COMPARI-
SON.PRAGMATIC CONCESSION (PDTB), with 12
instances. This lack of training data causes poor
classification performance on the classes associ-
ated to these relations.
In this paper, we try to tackle this problem by
using feature co-occurrence information, extracted
from unlabeled data, as a way to inform the classi-
fier when unseen features are found in test vectors.
The advantage of the method is that it relies solely
on unlabeled data, which is abundant, and cheap
to collect.
The contributions of this paper are the follow-
ing: First, we propose a semi-supervised method
that exploits the abundant, freely-available un-
labeled data, which is harvested for feature co-
occurrence information, and used as a basis to ex-
tend feature vectors to help classification for cases
where unknown features are found in test vec-
tors. Second, the proposed method is evaluated
on the RST-DT and PDTB corpus, where it signif-
icantly improves F-score when trained on moder-
ately small datasets. For instance, when trained on
a dataset with around 1000 instances, the proposed
method increases the macro-average F-score up to
30%, compared to a baseline classifier.
2 Related Work
Since the release in 2002 of the RST-DT corpus,
several fully-supervised discourse parsers have
1We use the notation [N] and [S] respectively to denote
the nucleus and satellite in a RST discourse relation.
55
been built in the RST framework. In duVerle and
Prendinger (2009), a discourse parser based on
Support Vector Machines (SVM) (Vapnik, 1995)
is proposed. Shallow lexical, syntactic and struc-
tural features, including ?dominance sets? (Soricut
and Marcu, 2003) are used.
The unsupervised method of Marcu and Echi-
habi (2002) was the first to try to detect ?implicit?
relations (i.e. relations not accompanied by a cue
phrase, such as ?however?, ?but?), using word pairs
extracted from two spans of text. Their method
attempts to capture the difference of polarity in
words.
Discourse relation classifiers have also been
trained using PDTB. Pitler et al (2008) performed
a corpus study of the PDTB, and found that ?ex-
plicit? relations can be most of the times distin-
guished by their discourse connectives.
Lin et al (2009) studied the problem of detect-
ing implicit relations in PDTB. Their relational
classifier is trained using features extracted from
dependency paths, contextual information, word
pairs and production rules in parse trees. For the
same task, Pitler et al (2009) also use word pairs,
as well as several other types of features such as
verb classes, modality, context, and lexical fea-
tures.
In this paper, we are not aiming at defining
novel features for improving performance in RST
or PDTB relation classification. Instead we incor-
porate features that have already shown to be use-
ful for discourse relation learning and explore the
possibilities of using unlabeled data for this task.
3 Method
In this section, we describe a semi-supervised
method for relation classification, based on feature
vector extension. The extension process employs
feature co-occurrence information. Co-occurrence
information is useful in this context as, for in-
stance, we might know that the word pair (for,
when) is a good indicator of a TEMPORAL rela-
tion. Or, after analyzing a large body of unlabeled
data, we might also notice that this word pair co-
occurs often with the word ?run-up? placed at the
end of a span of text. Suppose now that we have to
classify a test instance containing the feature ?run-
up?, but not the word pair (for, when). In this case,
by using the co-occurrence information, we know
that the instance has a chance of being a TEM-
PORAL relation. We first explain how to compute
a feature correlation matrix, using unlabeled data.
In a second section, we show how to extend fea-
ture vectors in order to include co-occurrence in-
formation. Finally, we describe the features used
in the discourse relation classifiers.
3.1 Feature Correlation Matrix
A training/test instance is represented using a d-
dimensional feature vector f = [f1, . . . , fd]T,
where fi ? {0, 1}. We define a feature correla-
tion matrix, C such that the (i, j)-th element of
C, C(i,j) ? {0, 1} denotes the correlation between
the two features fi and fj . If both fi and fj appear
in a feature vector then we define them to be co-
occurring. The number of different feature vectors
in which fi and fj co-occur is used as a basis to
compute C(i,j). Importantly, feature correlations
can be calculated using only unlabeled data.
It is noteworthy that feature correlation matri-
ces can be computed using any correlation mea-
sure. For the current task we use the ?2-measure
(Plackett, 1983) as the preferred correlation mea-
sure because of its simplicity. We create the fea-
ture correlation matrix C, such that, for all pairs of
features (fi, fj),
C(i,j) =
{
1 if ?2i,j > c
0 otherwise
. (1)
Here c is the critical value, which, for a confi-
dence level of 0.05 and one degree of freedom, can
be set to 3.84.
3.2 Feature Vector Extension
Once the feature correlation matrix is computed
using unlabeled data as described in Section 3.1,
we can use it to extend a feature vector during
testing. One of the reasons explaining why a clas-
sifier might perform poorly on a test instance, is
that there are features in the test instance that were
not observed during training. Let us represent the
feature vector corresponding to a test instance x
by fx. Then, we use the feature correlation ma-
trix to find the set of correlated features Fc(fi) of
a particular feature fi that occur in fx.
Specifically, for a feature fi ? fx, F ?(fi) con-
sists of features fj , where C(i,j) = 1. We define
the extended feature vector f ?x of fx as the union of
all the features that appear in fx and Fc(fx). Since
a discourse relation is defined between two spans
of short texts (elementary discourse units), which
are typically two clauses or sentences, a particu-
lar feature does not usually occur more than once
56
in a feature vector. Therefore, we introduced the
proposed method in the context of binary valued
features. However, the above mentioned discus-
sion can be naturally extended to cover real-valued
features.
3.3 Features
Figure 1 shows the parse tree for a sentence com-
posed of two discourse units, which serve as argu-
ments of a discourse relation we want to generate
a feature vector from. Lexical heads have been
calculated using the projection rules of Magerman
(1995), and indicated between brackets. For each
argument, surrounded by dots, is the minimal set
of sub-parse trees containing strictly all the words
of the argument.
We extract all possible lemmatized word pairs
from the two arguments. Next, we extract from
left and right argument separately, all production
rules from the sub-parse trees. Finally, we encode
in our features three nodes of the parse tree, which
capture the local context at the connection point
between the two arguments (Soricut and Marcu,
2003): The first node, which we call Nw, is the
highest ancestor of the first argument?s last word
w, and is such that Nw?s right-sibling is the an-
cestor of the second argument?s first word. Nw?s
right-sibling node is calledNr. Finally, we callNp
the parent of Nw and Nr. For each node, we en-
code in the feature vector its part-of-speech (POS)
and lexical head. For instance, in Figure 1, we
have Nw = S(comment), Nr = SBAR(when), and
Np = VP(declined).
4 Experiments
It is worth noting that the proposed method is inde-
pendent of any particular classification algorithm.
As our goal is strictly to evaluate the relative ben-
efit of employing the proposed method, we se-
lect a logistic regression classifier, for its simplic-
ity. We used the multi-class logistic regression
(maximum entropy model) implemented in Clas-
sias (Okazaki, 2009). Regularization parameters
are set to their default value of one.
Unlabeled instances are created by selecting
texts of the WSJ, and segmenting them into ele-
mentary discourse units (EDUs) using our sequen-
tial discourse segmenter (Hernault et al, 2010).
As there is no segmentation tool for the PDTB
framework, we assumed that feature correlation
information taken from EDUs created using a RST
segmenter is also useful for extending feature vec-
tors of PDTB relations.
Since we are interested in measuring the over-
all performance of a discourse relation classifier
across all relation types, we use macro-averaged
F-score as the preferred evaluation metric for this
task. We train a multi-class logistic regression
model without extending the feature vectors as
a baseline method. This baseline is expected to
show the effect of using the proposed feature ex-
tension approach for the task of discourse relation
learning.
Experimental results on RST-DT and PDTB
datasets are depicted in Figures 2 and 3. We ob-
serve that the proposed feature extension method
outperforms the baseline for both RST-DT and
PDTB datasets for the full range of training dataset
sizes. However, the difference between the two
methods decreases as we increase the amount of
training data. Specifically, with 200 training in-
stances, for RST-DT, the baseline method has a
macro-averaged F-score of 0.079, whereas the the
proposed method has a macro-averaged F-score
of 0.159 (around 101% increase in F-score). For
1000 training instances, the F-score for RST-DT
increases by 29.2%, from 0.143 to 0.185, while
the F-score for PDTB increases by 27.9%, from
0.109 to 0.139. However, the difference between
the two methods diminishes beyond 10000 train-
ing instances.
0 5000 10000 15000 20000Number of training instances0.05
0.10
0.15
0.20
0.25
0.30
Mac
ro-a
vera
ge F
-sco
re
Proposed methodBaseline RST-DT
Figure 2: Macro-average F-score (RST-DT) as a
function of the number of training instances used.
5 Conclusion
We presented a semi-supervised method for im-
proving the performance of discourse relation
classifiers. The proposed method is based on
the analysis of co-occurrence information har-
vested from unlabeled data only. We evaluated
57
NP (Sherry)
S (declined)
VP (declined)
NNP NNP
declined
VBD (declined)
Mr. Sherry to
VP (comment)
comment when asked about the sales
TO VP
SBAR (when)
WHADVP (when)
WRB
S (asked)
VP (asked)
VBN
PP (about)
IN NP (sales)
DT NNS
.
. (.)
Argument 1 Argument 2
VB
S (comment)
Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them?
lexical heads are indicated between brackets.
0 2000 4000 6000 8000 10000Number of training instances0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Mac
ro-a
vera
ge F
-sco
re
Proposed methodBaseline PDTB
Figure 3: Macro-average F-score (PDTB) as a
function of the number of training instances used.
the method on two of the most widely-used dis-
course corpora, RST-DT and PDTB. The method
performs significantly better than a baseline classi-
fier trained on the same features, especially when
the number of labeled instances used for training is
small. For instance, using 1000 training instances,
we observed an increase of nearly 30% in macro-
average F-score. This is an interesting perspective
for improving classification performance of rela-
tions with little training data. In the future, we
plan to improve the method by employing ranked
co-occurrences. This way, only the most relevant
correlated features can be selected during feature
vector extension. Finally, we plan to investigate
using larger amounts of unlabeled training data.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. Proc. of Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1?10.
D. A. duVerle and H. Prendinger. 2009. A novel
discourse parser based on Support Vector Machine
classification. In Proc. of ACL?09, pages 665?673.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010.
A sequential model for discourse segmentation. In
Proc. of CICLing?10, pages 315?326.
Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing
implicit discourse relations in the Penn Discourse
Treebank. In Proc. of EMNLP?09, pages 343?351.
D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. Proc. of ACL?95, pages 276?283.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc.
of ACL?02, pages 368?375.
N. Okazaki. 2009. Classias: A collection of machine-
learning algorithms for classification.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proc. of COLING?08 (Posters),
pages 87?90.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proc. of ACL?09, pages 683?691.
R. L. Plackett. 1983. Karl Pearson and the chi-squared
test. International Statistical Review, 51(1):59?72.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn Discourse Treebank 2.0. In Proc. of LREC?08.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. Proc. of NA-ACL?03, 1:149?156.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.
58
