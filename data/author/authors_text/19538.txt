Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 213?223, Dublin, Ireland, August 23-29 2014.
Sarcasm Detection on Czech and English Twitter
Tom
?
a
?
s Pt
?
a
?
cek
??
, Ivan Habernal
?
and Jun Hong
?
?
Department of Computer Science and Engineering, Faculty of Applied Sciences,
University of West Bohemia, Univerzitn?? 8, 306 14 Plze?n, Czech Republic
tigi@kiv.zcu.cz habernal@kiv.zcu.cz
?
School of Electronics, Electrical Engineering and Computer Science,
Queen?s University Belfast, Belfast BT7 1NN, UK
j.hong@qub.ac.uk
Abstract
This paper presents a machine learning approach to sarcasm detection on Twitter in two lan-
guages ? English and Czech. Although there has been some research in sarcasm detection in
languages other than English (e.g., Dutch, Italian, and Brazilian Portuguese), our work is the
first attempt at sarcasm detection in the Czech language. We created a large Czech Twitter cor-
pus consisting of 7,000 manually-labeled tweets and provide it to the community. We evaluate
two classifiers with various combinations of features on both the Czech and English datasets.
Furthermore, we tackle the issues of rich Czech morphology by examining different preprocess-
ing techniques. Experiments show that our language-independent approach significantly outper-
forms adapted state-of-the-art methods in English (F-measure 0.947) and also represents a strong
baseline for further research in Czech (F-measure 0.582).
1 Introduction
Sentiment analysis on social media has been one of the most targeted research topics in NLP in the past
decade, as shown in several recent surveys (Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Since
the goal of sentiment analysis is to automatically detect the polarity of a document, misinterpreting irony
and sarcasm represents a big challenge (Davidov et al., 2010).
As there is only a weak boundary in meaning between irony, sarcasm and satire (Reyes et al., 2012),
we will use only the term sarcasm in this paper. Bosco et al. (2013) claim that ?even if there is no agree-
ment on a formal definition of irony, psychological experiments have delivered evidence that humans can
reliably identify ironic text utterances from an early age in life.? We have thus decided to rely on the abil-
ity of our human annotators to manually label sarcastic tweets to train our classifiers. Sarcasm generally
reverses the polarity of an utterance from positive or negative into its opposite, which deteriorates the
results of a given NLP task. Therefore, correct identification of sarcasm can improve the performance.
The issue of automatic sarcasm detection has been addressed mostly in English, although there has
been some research in other languages, such as Dutch (Liebrecht et al., 2013), Italian (Bosco et al.,
2013), or Brazilian Portuguese (Vanin et al., 2013). To the best of our knowledge, no research has been
conducted in Czech or other Slavic languages. These languages are challenging for many NLP tasks
because of their rich morphology and syntax. This has motivated us to focus our current research on both
English and Czech.
Majority of the existing state-of-the-art techniques are language dependent, which rely on language-
specific lexical resources. Since no such resources are available for Czech, we adapt some language-
independent methods and also apply various preprocessing steps for sentiment analysis proposed by
Habernal et al. (2013).
This paper focuses on document-level sarcasm detection on Czech and English Twitter datasets using
supervised machine learning methods. The Czech dataset consists of 7,000 manually labeled tweets,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
213
the English dataset consists of a balanced distribution and an imbalanced distribution, each contain-
ing 100,000 tweets, where hashtag #sarcasm was used as an indicator of sarcastic tweets. We pro-
vide both datasets under Creative Commons BY-NC-SA licence
1
at http://liks.fav.zcu.cz/
sarcasm/.
Our research questions were the following: (1) To what extent can the language-independent approach
compete with methods based on lexical language-dependent resources? (2) Is it possible to reach good
agreement on annotating sarcasm and what typical text properties on Twitter are important for sarcasm
detection? (3) What is the best preprocessing pipeline that can boost performance on highly-flective
Czech language and what types of features and classifiers yield the best results?
The rest of this article is organized as follows. Section 2 describes the related work. In section 3, we
outline our approach to sarcasm detection and describe the selection of features in our approach. Section
4 thoroughly describes the datasets and the annotation process. Section 5 describes and discusses the
experimental results. Finally we conclude in Section 6.
2 Related Work
Experiments with semi-supervised sarcasm identification on a Twitter dataset (5.9 million tweets) and on
66,000 product reviews from Amazon were conducted in (Davidov et al., 2010) and (Tsur et al., 2010).
They used 5-fold cross validation on their kNN-like classifier and obtained an F-measure of 0.83 on
the product reviews dataset and 0.55 on the Twitter dataset. For acquiring the Twitter dataset they used
hashtag #sarcasm as an indicator of sarcastic tweets. They further created a balanced evaluation set of
180 tweets using 15 human annotators via Amazon Mechanical Turk
2
and achieved an inter-annotator
agreement 0.41 (Fleiss? ?).
Gonz?alez-Ib?a?nez et al. (2011) experimented with Twitter data divided into three categories (sarcas-
tic, positive sentiment and negative sentiment), each containing 900 tweets. They used the #sarcasm
and #sarcastic hashtags to identify sarcastic tweets. They used two classifiers ? support vector
machine (SVM) with sequential minimal optimization (SMO) and logistic regression. They tried var-
ious combinations of unigrams, dictionary-based features and pragmatic factors (positive and negative
emoticons and user references), achieving the best result (accuracy 0.65) for sarcastic and non-sarcastic
classification with the combination of SVM with SMO and unigrams. They employed 3 human judges to
annotate 180 tweets (90 sarcastic and 90 non-sarcastic). The human judges achieved Fleiss? ? = 0.586,
demonstrating the difficulty of sarcasm classification. Another experiment included 50 sarcastic and 50
non-sarcastic (25 positive, 25 negative) tweets with emoticons annotated by two judges. The automatic
classification and human judges achieved the accuracy of 0.71 and 0.89 respectively. The inter-annotator
agreement (Cohen?s ?) was 0.74.
Reyes et al. (2012) proposed features to capture properties of a figurative language such as ambiguity,
polarity, unexpectedness and emotional scenarios. Their corpus consists of five categories (humor, irony,
politics, technology and general), each containing 10,000 tweets. The best result in the classification of
irony and general tweets was F-measure 0.65.
In (Reyes et al., 2013) they explored the representativeness and relevance of conceptual features (sig-
natures, unexpectedness, style and emotional scenarios). These features include punctuation marks,
emoticons, quotes, capitalized words, lexicon-based features, character n-grams, skip-grams, (Guthrie
et al., 2006), and polarity skip-grams. Their corpus consists of four categories (irony, humor, education
and politics), each containing 10,000 tweets. Their evaluation was performed on two distributional sce-
narios, balanced distribution and imbalanced distribution (25% ironic tweets and 75% tweets from all
three non-ironic categories) using the Naive Bayes and decision trees algorithms from the Weka toolkit
(Witten and Frank, 2005). The classification by the decision trees achieved an F-measure of 0.72 on the
balanced distribution and an F-measure of 0.53 on the imbalanced distribution.
The work of Riloff et al. (2013) identifies one type of sarcasm: contrast between a positive sentiment
and negative situation. They used a bootstrapping algorithm to acquire lists of positive sentiment phrases
1
http://creativecommons.org/licenses/by-nc-sa/3.0/
2
http://www.mturk.com
214
and negative situation phrases from sarcastic tweets. They proposed a method which classifies tweets as
sarcastic if it contains a positive predicative that precedes a negative situation phrase in close proximity.
Their evaluation on a human-annotated dataset
3
of 3000 tweets (23% sarcastic) was done using the SVM
classifier with unigrams and bigrams as features, achieving an F-measure of 0.48. The hybrid approach
that combines the results of the SVM classifier and their contrast method achieved an F-measure of 0.51.
Sarcasm and nastiness classification in online dialogues was also explored in (Lukin and Walker, 2013)
using bootstrapping, syntactic patterns and a high precision classifier. They achieved an F-measure of
0.57 on their sarcasm dataset.
3 Our Approach
This paper presents the first attempt at sarcasm detection in the Czech language, in which we focus on
supervised machine learning approaches and evaluate their performance. We selected various n-grams,
including unigrams, bigrams, trigrams with frequency greater than three (Liebrecht et al., 2013), and a
set of language-independent features, including punctuation marks, emoticons, quotes, capitalized words,
character n-grams and skip-grams (Reyes et al., 2013) as our baselines.
3.1 Classification
Our evaluation was performed using the Maximum Entropy (MaxEnt) and Support Vector Machine
(SVM) classifiers. We used Brainy ? a Java framework for machine learning (Konkol, 2014) ? with
default settings (the linear kernel for SVM). All experiments were conducted in the 5-fold cross vali-
dation manner similar to (Davidov et al., 2010; Gonz?alez-Ib?a?nez et al., 2011). Our motivation to test
multiple classifiers stemmed also from related works which mostly test more than one classifier. On the
other hand, the choice between state-of-the-art linear classifiers might not be much of importance, as the
most important is the feature engineering.
3.2 Features
For our evaluation we used the most promising language-independent features from the related work and
POS related features. Feature sets used in our evaluation are described in Table 1.
Group Features
Description
N-gram
Character n-gram
We used character n-gram features (Blamey et al., 2012). We set the minimum
occurrence of a particular character n-gram to either 5 or 50, in order to prune the
feature space. Our character feature set contains 3-grams to 6-grams.
N-gram
We used word unigrams, bigrams and trigrams as binary features. The feature space
is pruned by the minimum n-gram occurrence set to 3 (Liebrecht et al., 2013).
Skip-bigram
Instead of using sequences of adjacent words (n-grams) we used skip-grams
(Guthrie et al., 2006), which skip over arbitrary gaps. Reyes et al. (2013) consider
skip-bigrams with 2 or 3 word skips and remove skip-grams with a frequency? 20.
Pattern
Pattern
Patterns composed of high frequency words (HFWs)
4
and content words (CWs)
5
used by (Davidov et al., 2010). Pattern must contain at least one high frequency
word. The patterns contain 2-6 HFWs and 1-6 CWs. We set the minimum occur-
rence of a particular pattern to 5.
Word-shape pattern
We tried to improve pattern features by using word-shape classes for content words.
We assign words into one of 24 classes
6
similar to the function specified in (Bikel
et al., 1997).
POS
POS characteristics
We implemented various POS features, e.g., the number of nouns, verbs, and adjec-
tives (Ahkter and Soria, 2010), the ratio of nouns to adjectives and verbs to adverbs
(Kouloumpis et al., 2011), and number of negative verbs obtained from POS tags.
3
They used three annotators. Each annotator was given the same 100 tweets with the sarcasm hashtag and 100 tweets
without the sarcasm hashtag (the hashtags were removed). On these tweets the pairwise inter-annotator scores were computed
(Cohen?s Kappa ?
1
= 0.80, ?
2
= 0.81 and ?
3
= 0.82). Then each annotator labeled additional 1000 tweets.
4
A word whose corpus frequency is more than 1000 words per million plus all punctuation characters.
5
A word whose corpus frequency is less than 1000 words per million.
6
We use edu.stanford.nlp.process.WordShapeClassifier with the WORDSHAPECHRIS1 setting.
215
POS word-shape
Unigram feature consisting of POS and word-shape (see Word-shape pattern). The
feature space is pruned by the minimum occurrence set to 5.
POS n-gram
Direct use of POS n-grams has not shown any significant improvement in sentiment
analysis but it may improve the results of sarcasm detection. We experimented with
3-grams to 6-grams with the minimum n-gram occurrence set to 5.
Others
Emoticons
We used two lists of positive and negative emoticons (Montejo-R?aez et al., 2012).
The feature captures the number of occurrences of each class of emoticons within
the text.
Punctuation-based
We adapted punctuation-based features proposed by (Davidov et al., 2010). This
feature set consists of number of words, exclamation marks, question marks, quota-
tion marks and capitalized words normalized by dividing them by the maximal ob-
served value multiplied by the averaged maximal value of the other feature groups.
Pointedness
Pointedness was used by (Reyes et al., 2013) to distinguish irony. It focuses on
explicit marks which should reflect a sharp distinction in the information that is
transmitted. The presence of punctuation marks, emoticons, quotes and capitalized
words has been considered.
Extended Pointedness
This feature captures the number of occurrences of punctuation marks and emoti-
cons as well as the number of words, exclamation marks, question marks, quotation
marks and capitalized words normalized by maximal observed value.
Word-case
We implemented various word-case features that include, e.g., the number of upper
cased words, number of words with first letter capital normalized by number of
words and number of upper cased characters normalized by number of words.
Table 1: Descriptions of used feature sets.
4 Evaluation Datasets
We collected datasets using Twitter Search API and Java Language Detector
7
. We collected 140,000
Czech and 780,000 English tweets, respectively. Due to lack of support for the Czech language on
Twitter, we used the Twitter Search API parameter geocode to acquire tweets posted near Prague. For
the English dataset we also collected tweets with the #sarcasm hashtag. Czech users generally don?t
use the sarcasm (?#sarkasmus?) or irony (?#ironie?) hashtag variants
8
thus we had to annotate the
Czech dataset manually. The final label distribution in datasets is shown in Table 4.
4.1 Filtering and Normalization
All user, URL and hashtag references in tweets have been replaced by ?user?, ?link? and ?hashtag?
respectively. We also removed all tweets starting with ?RT? because they refer to previous tweets and
tweets containing just combinations of user, link, ?RT? and hashtags without any additional words.
Tokenization of tweets requires proper handling of emoticons and other special character sequences
typical on Twitter. The Ark-tweet-nlp tool (Gimpel et al., 2011) offers precisely that and although it was
developed and tested in English, it yields satisfactory results in Czech as well.
Czech is a highly flective language and uses a lot of diacritics. However some Czech users type
only the unaccented characters.
9
Preliminary experiments showed that removing diacritics yields better
results, thus we removed diacritics from all tweets.
4.2 Czech Dataset Annotation
Firstly we conducted an experiment to determine whether to annotate the original data or the normalized
data. We selected two sample sets of 50 tweets containing Czech sarcasm (#sarkasmus) and irony
(#ironie) hashtags and other tweets. One annotator obtained the original data while the other got the
normalized data from the first sample set. We then tried to give both annotators the original data from the
first sample set and finally we gave them both the normalized data from the second sample set. Table 2
shows the difficulty of sarcasm identification without the knowledge hidden in hashtags, user and links.
7
http://code.google.com/p/jlangdetect/
8
We found only 10 tweets with sarcasm hashtag (?#sarkasmus?) and 100 tweets with irony hashtag (?#ironie?) in
140,000 collected tweets.
9
Approximately 10% of collected tweets were without any diacritics.
216
N
o
r
m
a
l
i
z
e
d
Normalized
Tag n s
n 35 10
s 0 5
Cohen?s ?: 0.412
O
r
i
g
i
n
a
l
Normalized
Tag n s
n 19 10
s 5 16
Cohen?s ?: 0.404
O
r
i
g
i
n
a
l
Original
Tag n s
n 25 4
s 3 18
Cohen?s ?: 0.715
Table 2: Confusion matrices and annotation agreement (Cohen?s ?) between two annotators using orig-
inal or normalized data.
?Basic? pipe Pipe 2 Pipe 3
Tokenizing: ArkTweetNLP
POS tagging: PDT
? Stem: no (Sn) / light (Sl) / HPS (Sh)
?
Stopwords removal
? ?
Phonetic: eSpeak (Pe)
Table 3: The preprocessing pipes for Czech (top-down). Combinations of methods are denoted using
the appropriate labels, e.g. ?Sn? means 1. tokenizing, 2. POS-tagging, 3. no stemming and 4. removing
stopwords. eSpeak stands for a phonetic transcription to International Phonetic Alphabet, which should
reduce the effects of grammar mistakes and misspellings.
The most promising results come from the annotation of the original data, thus the rest of the data are
annotated in this manner.
We randomly selected 7,000 tweets from the collected data for annotation. The annotators were given
just simple instructions without an explicit sarcasm definition (see Section 1): ?A tweet is considered
sarcastic when its content is intended ironically / sarcastically without anticipating further information.
Offensive utterances, jokes and ironic situations are not considered ironic / sarcastic.?
The complete dataset of 7,000 tweets was independently annotated by two annotators. The inter-
annotator agreement (Cohen?s ?) between the two annotators is 0.54. They disagreed on 403 tweets. To
resolve these conflicts we used a third annotator.
The third annotator has been instructed the same way as the other two. The final ? agreement was mea-
sured between the first two annotators, thus it was not affected by the third annotator. Kappa agreements
measured on the conflicted states (403 tweets) were 0.4 (annotator 1 vs. annotator 3) and 0.6 (annotator
2 vs. annotator 3).
Preprocessing
Preprocessing steps for handling social media texts in Czech were explored in (Habernal et al., 2013).
The preprocessing diagram and its variants is depicted in Table 3. Overall, there are various possible pre-
processing ?pipe? configurations including ?Basic? pipeline consisting of tokenizing and POS-tagging
only. We adapted all their preprocessing pipelines. However, as the number of combinations would be
too large, we report only the settings with better performance.
4.3 English Dataset
We collected 780,000 (130,000 sarcastic and 650,000 non-sarcastic) tweets in English. The #sarcasm
hashtag was used as an indicator of sarcastic tweets. From this corpus we created two distributional
scenarios based on the work of (Reyes et al., 2013). Refer to Table 4 for the final statistics of the dataset.
Part of speech tagging was done using the Ark-tweet-nlp tool (Gimpel et al., 2011).
5 Results
For each preprocessing pipeline (refer to table 3) we assembled various sets of features and employed
two classifiers. Accuracy (micro F-measure) tends to prefer performance on dominant classes in highly
217
Dataset \ Tweets Sarcastic Non-sarcastic
Czech 325 6,675
English Balanced 50,000 50,000
English Imbalanced 25,000 75,000
Table 4: The tweet distributions in datasets.
Feature Set \ Pipeline Basic Sh ShPe Sl SlPe Sn SnPe
Baseline 1 (B1): n-gram 54.8 55.3 55.2 55.0 55.0 54.4 55.3
B1 + pattern 55.1 54.4 54.7 55.1 54.8 54.2 54.5
B1 + word-shape pattern 54.6 54.8 55.2 54.4 55.0 54.8 55.1
B1 + punctuation-based 54.7 48.8 48.8 48.8 48.8 53.8 55.5
B1 + pointedness 55.0 54.7 54.7 55.0 55.9 54.8 54.9
B1 + extended pointedness 54.5 48.8 48.8 48.8 48.8 54.7 54.6
B1 + POS n-gram 53.4 54.1 54.2 55.3 55.1 54.2 53.9
B1 + POS word-shape 55.0 55.6 55.2 54.8 54.6 55.8 54.4
B1 + skip-bigram 54.2 54.8 54.2 54.7 56.0 54.6 54.4
B1 + POS characteristics + emoticons 55.5 54.7 55.6 55.2 55.4 55.2 53.9
B1 + POS characteristics + emoticons + word-case 53.8 56.4 55.5 54.6 55.3 55.9 55.3
Character n-gram (3-6, min. occurrence > 5) 53.0 52.7 53.2 53.9 54.7 52.0 53.2
Baseline 2 (B2) 55.0 55.2 55.4 56.8 56.2 54.7 54.0
B2 + FS1 52.3 48.8 48.8 48.8 48.8 52.0 52.9
B2 + FS1 + FS2 53.0 48.8 48.8 48.8 48.8 52.2 53.6
B2 + pattern 55.3 55.4 55.7 56.9 56.6 54.4 53.6
B2 + POS word-shape 55.5 55.8 55.4 57.0 56.3 55.3 54.7
B2 + POS characteristics + emoticons + word-case 56.1 55.7 55.7 56.9 56.1 55.0 54.3
Table 5: Results on the Czech dataset with the MaxEnt classifier. Macro F-measure, 95% confidence
interval ? ?1.2. Best results are in bold. B2: character n-gram (3-5, min. occurrence > 50) + skip-
bigram + pointedness; FS1: character n-gram (3-6, min. occurrence > 5) + extended pointedness; FS2:
POS word-shape + pattern + POS characteristics + emoticons + word-case.
unbalanced datasets (Manning et al., 2008), thus we chose macro F-measure as the evaluation metric
(Forman and Scholz, 2010), as it allows us to compare classification results on different datasets. For
statistical significance testing, we report confidence intervals at ? 0.05. Another applicable methods
would be, i.e., two-matched-samples t Test or McNemar?s test (Japkowicz and Shah, 2011).
5.1 Czech
Tables 5 and 6 show the results on the Czech dataset. The best result (F-measure 0.582) was achieved
by the SVM classifier and a feature set enriched with patterns, utilizing stopwords removal and phonetic
transcription in the preprocessing step.
The importance of the appropriate preprocessing techniques for Czech is evident from the improve-
ment of results for various feature sets, e.g., the best result for ?Basic? pipeline (see line ?B2 + pattern?).
Both baselines show improvement on most preprocessing pipelines. The most significant difference is
visible on the second baseline with the MaxEnt classifier and the ?Sl? pipeline where the F-measure is
0.018 higher than the ?Basic? pipeline with no additional preprocessing. The n-gram baseline was sig-
nificantly outperformed by the SVM classifier with feature sets ?B1 + POS characteristics + Emoticons
+ Word-case? and ?B1 + extended pointedness? on the ?SnPe? pipeline.
Error Analysis
To get a better understanding of the limitations of our approach, we inspected 100 random tweets from the
Czech dataset, which were wrongly classified by the SVM classifier with the best feature combination.
218
Feature Set \ Pipeline Basic Sh ShPe Sl SlPe Sn SnPe
Baseline 1 (B1): n-gram 55.8 54.6 54.5 54.6 55.5 56.0 53.9
B1 + pattern 55.6 54.0 54.3 54.6 55.7 55.4 55.6
B1 + word-shape pattern 54.9 55.0 53.8 55.2 55.1 55.4 55.3
B1 + punctuation-based 55.8 48.8 48.8 48.8 48.8 55.7 53.7
B1 + pointedness 55.9 54.5 53.1 54.6 54.3 55.4 54.6
B1 + extended pointedness 56.5 48.8 48.8 48.8 48.8 55.8 56.9
B1 + POS n-gram 54.0 54.1 54.0 54.7 53.4 54.5 53.9
B1 + POS word-shape 55.2 56.4 55.9 55.1 56.0 56.1 55.0
B1 + skip-bigram 55.9 55.3 54.8 55.4 55.0 56.1 55.2
B1 + POS characteristics + emoticons 55.9 54.5 54.1 54.6 54.2 56.7 55.8
B1 + POS characteristics + emoticons + word-case 55.6 54.5 54.3 55.1 55.5 56.3 56.4
Character n-gram (3-6, min. occurrence > 5) 54.6 53.6 53.3 55.2 53.6 53.4 54.9
Baseline 2 (B2) 55.9 56.4 56.3 57.0 56.2 57.1 55.8
B2 + FS1 52.2 48.8 48.8 48.8 48.8 53.1 52.7
B2 + FS1 + FS2 54.0 48.8 48.8 48.8 48.8 54.4 54.3
B2 + pattern 56.8 57.0 56.7 56.5 57.5 57.1 58.2
B2 + POS word-shape 56.5 56.3 57.2 56.4 56.1 56.3 57.8
B2 + POS characteristics + emoticons + word-case 56.2 55.7 55.8 56.0 56.0 57.0 56.0
Table 6: Results on the Czech dataset with the SVM classifier. Macro F-measure, 95% confidence
interval ? ?1.2. Best results are in bold. B2: character n-gram (3-5, min. occurrence > 50) + skip-
bigram + pointedness; FS1: character n-gram (3-6, min. occurrence > 5) + extended pointedness; FS2:
POS word-shape + pattern + POS characteristics + emoticons + word-case.
We found 48 false positives and 52 false negatives. The annotators disagreed upon 10% of these tweets.
Non-sarcastic tweets were often about news, reviews, general information and user status updates. In
most of the difficult cases of true negatives, the tweet contains a question, insult, opinion or wordplay.
Understanding sarcasm in some tweets was often bound with broader common knowledge (e.g., about
news or celebrities), the context known only to the author or authors opinion. Another difficulty poses
subtle or sophisticated expression of sarcasm such as ?I?m not sure whether you didn?t overdo a bit the
first part of the renovation - the demolition. :)?
10
or ?Conservatism, once something is in the school
rules, it must be followed, forever, otherwise anarchy will break out and traditional values will die.?
11
5.2 English
The results on both balanced and imbalanced English datasets are presented in Table 7. In most cases the
MaxEnt classifier significantly outperforms the SVM classifier. The combination of majority of features
(?B2 + FS1 + FS2?) with the MaxEnt classifier yields the best results for both balanced and imbalanced
dataset distributions. This suggests that these features are coherent. While no single feature captures the
essence of sarcasm, all features together provide useful linguistic information for detecting sarcasm at
textual level.
Balanced distribution Both baselines were surpassed by various combinations of feature sets with
the MaxEnt classifier, although in some cases very narrowly (?B1 + punctuation-based? and ?B1 +
pointedness? feature sets). Although the SVM classifier has slightly worse results, it still performs
reasonably, and we even recorded significant improvement over the baseline for ?B1 + POS word-shape?.
The best results were achieved using the MaxEnt classifier with ?B2 + FS1 + FS2? (F-measure 0.947)
and ?B1 + word-shape pattern ? (F-measure 0.943) feature sets.
10
?Jestli jste tu prvn?? ?c?ast rekonstrukce - demolici - trochu nep?rehnali . :)?
11
?Konzervatismus , kdy?z je to jednou ve ?skoln??m ?r?adu , tak se to mus?? dodr?zovat , a to nav?zdy , jinak vypukne anarchie a
tradi?cn?? hodnoty zem?rou .?
219
Dataset Balanced Imbalanced
Classifier MaxEnt SVM MaxEnt SVM
Feature set \ Results Fm CI Fm CI Fm CI Fm CI
Baseline 1 (B1): n-gram 93.28 0.16 92.86 0.16 90.76 0.18 90.44 0.18
B1 + pattern 94.25 0.14 93.13 0.16 91.86 0.17 90.22 0.18
B1 + word-shape pattern 94.33 0.14 93.17 0.16 92.01 0.17 90.35 0.18
B1 + punctuation-based 93.32 0.15 92.84 0.16 90.72 0.18 90.43 0.18
B1 + pointedness 93.29 0.16 92.99 0.16 91.00 0.18 90.07 0.19
B1 + extended pointedness 93.68 0.15 92.61 0.16 91.07 0.18 89.89 0.19
B1 + POS n-gram 93.66 0.15 92.64 0.16 91.20 0.18 89.85 0.19
B1 + POS word-shape 93.96 0.15 93.19 0.16 91.41 0.17 90.51 0.18
B1 + skip-bigram 93.63 0.15 93.17 0.16 90.99 0.18 90.48 0.18
B1 + POS characteristics + emoticons 93.97 0.15 91.66 0.17 91.69 0.17 89.39 0.19
B1 + POS characteristics + emoticons + word-case 93.96 0.15 91.54 0.17 91.61 0.17 88.89 0.19
Character n-gram: (3-6, min. occurrence > 5) 93.01 0.16 91.73 0.17 90.36 0.18 88.81 0.20
Baseline 2 (B2) 92.81 0.16 91.67 0.17 90.65 0.18 88.70 0.20
B2 + FS1 93.82 0.15 91.56 0.17 91.21 0.18 88.73 0.20
B2 + FS1 + FS2 94.66 0.14 91.39 0.17 92.37 0.16 88.62 0.20
B2 + pattern 93.60 0.15 91.66 0.17 90.86 0.18 88.82 0.20
B2 + POS word-shape 93.20 0.16 91.65 0.17 90.82 0.18 88.74 0.20
B2 + POS characteristics + emoticons + word-case 93.21 0.16 91.07 0.18 89.98 0.19 88.40 0.20
Table 7: Results on the English dataset with the MaxEnt and SVM classifiers. Macro F-measure (Fm)
and 95% confidence interval (CI) are in %. Best results are in bold. B2: character n-gram (3-5, min.
occurrence > 50) + skip-bigram + pointedness; FS1: character n-gram (3-6, min. occurrence > 5) +
extended pointedness; FS2: POS word-shape + pattern + POS characteristics + emoticons + word-case.
Imbalanced distribution However, data in the real world do not necessarily resemble the balanced
distribution. Therefore we have also performed the evaluation on an imbalanced distribution. The Max-
Ent classifier clearly achieves the best results. This experiment indicates that combinations of features
?B2 + FS1 + FS2? (F-measure 0.924) and ?B1, word-shape pattern? (F-measure 0.920) yields the best
results for both balanced and imbalanced dataset distribution.
5.3 Discussion
To explain the huge difference in the performance between English and Czech, we conducted an addi-
tional experiment in English. We sampled the ?big-data? English corpus (100k Tweets) to obtain the
same distribution as on the ?small-data? Czech corpus (325 sarcastic and 6,675 non-sarcastic Tweets).
Feature combination ?B2 + FS1 + FS2? achieves an F-measure of 0.734? 0.01 (MaxEnt classifier) and
0.729 ? 0.01 (SVM). This performance drop shows that the amount of training data plays a key role
(? 0.92 on ?big-data? vs. ? 0.73 on ?small-data?). However, these results are still significantly better
than in Czech (? 0.58). This demonstrates that Czech is a challenging language in sarcasm detection, as
in other NLP tasks.
In addition, we also experimented with the Naive Bayes classifier and with delta TF-IDF feature
variants (Martineau and Finin, 2009; Paltoglou and Thelwall, 2010) in both languages. However, the
performance was not satisfactory in comparison with the reported results.
6 Conclusions
We investigated supervised machine learning methods for sarcasm detection on Twitter. As a pilot study
for sarcasm detection in the Czech language, we provide a large human-annotated Czech Twitter dataset
containing 7,000 tweets with inter-annotator agreement ? = 0.54. The novel contributions of our work
include the extensive evaluation of two classifiers with various combinations of feature sets on both
the Czech and English datasets as well as a comparison of different preprocessing techniques for the
220
Czech dataset. Our approaches significantly outperformed both baselines adapted from related work
12
in English and achieved F-measure of 0.947 and 0.924 on the balanced and imbalanced datasets, re-
spectively.
13
The best result on the Czech dataset was achieved by the SVM classifier with the feature
set enriched with patterns yielding F-measure 0.582. The whole project is available to the community
under GPL license at http://liks.fav.zcu.cz/sarcasm/. We believe that our findings will
contribute to the research outside the mainstream languages and may be applied to sarcasm detection in
other Slavic languages, such as Slovak or Polish.
6.1 Future work
We approached the problem mainly from the data-driven perspective (annotation, feature engineering,
error analysis). However, we feel that elaborating deep linguistic insights would be helpful to better
understand the phenomena of sarcasm on social media (Averbeck, 2013; Averbeck and Hample, 2008;
Ivanko et al., 2004; Jorgensen, 1996).
There are also possible extensions to the lexical/morphological features ? either in the direction of
semi-supervised learning and adding for example features based on latent semantics, topic models, or
graphical models popular in the sentiment analysis field (Habernal and Brychc??n, 2013; Brychc??n and
Habernal, 2013), or the direction of deeper linguistic processing in terms of, e.g., syntax/dependecy
parsing (but this has limitation given the nature of Twitter data as well as unavailability of such tools for
Czech). These deserve further investigation and are planned in future work.
Acknowledgements
Access to computing and storage facilities owned by parties and projects contributing to the National Grid
Infrastructure MetaCentrum, provided under the programme ?Projects of Large Infrastructure for Re-
search, Development, and Innovations? (LM2010005), is greatly appreciated. Access to the CERIT-SC
computing and storage facilities provided under the programme Center CERIT Scientific Cloud, part of
the Operational Program Research and Development for Innovations, reg. no. CZ. 1.05/3.2.00/08.0144,
is greatly appreciated.
References
Julie Kane Ahkter and Steven Soria. 2010. Sentiment analysis: Facebook status messages. Technical report,
Stanford University. Final Project CS224N.
Joshua M Averbeck and Dale Hample. 2008. Ironic message production: How and why we produce ironic
messages. Communication Monographs, 75(4):396?410.
Joshua M Averbeck. 2013. Comparisons of ironic and sarcastic arguments in terms of appropriateness and effec-
tiveness in personal relationships. Argumentation & Advocacy, 50(1).
Daniel M Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a high-performance
learning name-finder. In Proceedings of the fifth conference on Applied natural language processing, pages
194?201. Association for Computational Linguistics.
Ben Blamey, Tom Crick, and Giles Oatley. 2012. R U : -) or : -( ? character- vs. word-gram feature selection
for sentiment classification of OSN corpora. In Proceedings of AI-2012, The Thirty-second SGAI International
Conference on Innovative Techniques and Applications of Artificial Intelligence, pages 207?212. Springer.
Cristina Bosco, Viviana Patti, and Andrea Bolioli. 2013. Developing corpora for sentiment analysis: The case of
irony and senti-tut. IEEE Intelligent Systems, 28(2):55?63.
Tom?a?s Brychc??n and Ivan Habernal. 2013. Unsupervised improving of sentiment analysis using global target
context. In Proceedings of the International Conference Recent Advances in Natural Language Processing
RANLP 2013, pages 122?128, Hissar, Bulgaria, September. INCOMA Ltd. Shoumen, BULGARIA.
12
Word unigrams, bigrams, trigrams (Liebrecht et al., 2013) and a set of language-independent features (punctuation marks,
emoticons, quotes, capitalized words, character n-grams and skip-grams.) (Reyes et al., 2013)
13
Note that the best result (F-measure 0.715 on the balanced distribution and F-measure 0.533 on the imbalanced distribution)
from the related work was achieved by (Reyes et al., 2013) using decision trees classifier.
221
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twit-
ter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,
CoNLL ?10, pages 107?116, Stroudsburg, PA, USA. Association for Computational Linguistics.
George Forman and Martin Scholz. 2010. Apples-to-apples in cross-validation studies: Pitfalls in classifier per-
formance measurement. SIGKDD Explor. Newsl., 12(1):49?57, November.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heil-
man, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: annota-
tion, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short papers - Volume 2, HLT ?11, pages 42?47, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies: Short Papers - Volume 2, HLT ?11, pages 581?586, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. A closer look at skip-gram
modelling. In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-
2006), pages 1?4.
Ivan Habernal and Tom?a?s Brychc??n. 2013. Semantic spaces for sentiment analysis. In Ivan Habernal and V?aclav
Matou?sek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture Notes in Computer Science, pages
484?491. Springer Berlin Heidelberg.
Ivan Habernal, Tom?a?s Pt?a?cek, and Josef Steinberger. 2013. Sentiment analysis in czech social media using
supervised machine learning. In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 65?74, Atlanta, Georgia, June. Association for Computational
Linguistics.
Stacey L Ivanko, Penny M Pexman, and Kara M Olineck. 2004. How sarcastic are you? individual differences
and verbal irony. Journal of language and social psychology, 23(3):244?271.
Nathalie Japkowicz and Mohak Shah. 2011. Evaluating Learning Algorithms: A Classification Perspective.
Cambridge University Press.
Julia Jorgensen. 1996. The functions of sarcastic irony in speech. Journal of Pragmatics, 26(5):613 ? 634.
Michal Konkol. 2014. Brainy: A machine learning library. In Leszek Rutkowski, Marcin Korytkowski, Rafal
Scherer, Ryszard Tadeusiewicz, Lotfi Zadeh, and Jacek Zurada, editors, Artificial Intelligence and Soft Comput-
ing, volume 8468 of Lecture Notes in Computer Science, pages 490?499. Springer International Publishing.
Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad
and the OMG! In Proceedings of the Fifth International Conference on Weblogs and Social Media, Barcelona,
Catalonia, Spain, July 17-21, 2011. The AAAI Press.
Christine Liebrecht, Florian Kunneman, and Antal Van den Bosch. 2013. The perfect solution for detecting
sarcasm in tweets #not. In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 29?37, Atlanta, Georgia, June. Association for Computational
Linguistics.
Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. In Mining Text Data, pages
415?463. Springer.
Stephanie Lukin and Marilyn Walker. 2013. Really? Well. Apparently bootstrapping improves the performance
of sarcasm and nastiness classifiers for online dialogue. In Proceedings of the Workshop on Language Analysis
in Social Media, pages 30?40, Atlanta, Georgia, June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
Justin Martineau and Tim Finin. 2009. Delta TFIDF: An improved feature space for sentiment analysis. In
Proceedings of the Third International Conference on Weblogs and Social Media, ICWSM 2009, San Jose,
California, USA. The AAAI Press.
222
A. Montejo-R?aez, E. Mart??nez-C?amara, M. T. Mart??n-Valdivia, and L. A. Ure?na L?opez. 2012. Random walk
weighting over sentiwordnet for sentiment polarity detection on twitter. In Proceedings of the 3rd Workshop in
Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ?12, pages 3?10, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL
?10, pages 1386?1395, Stroudsburg, PA, USA. Association for Computational Linguistics.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi. 2012. From humor recognition to irony detection: The
figurative language of social media. Data Knowl. Eng., 74:1?12, April.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony in twitter.
Language Resources and Evaluation, 47(1):239?268.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sar-
casm as contrast between a positive sentiment and negative situation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Processing, pages 704?714, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010. ICWSM - a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product reviews. In William W. Cohen and Samuel Gosling, editors,
Proceedings of the Fourth International Conference on Weblogs and Social Media, ICWSM 2010, Washington,
DC, USA, May 23-26, 2010. The AAAI Press.
Mikalai Tsytsarau and Themis Palpanas. 2012. Survey on mining subjective data on the web. Data Mining and
Knowledge Discovery, 24(3):478?514, May.
Aline A Vanin, Larissa A Freitas, Renata Vieira, and Marco Bochernitsan. 2013. Some clues on irony detection
in tweets. In Proceedings of the 22nd international conference on World Wide Web companion, pages 635?636.
International World Wide Web Conferences Steering Committee.
Ian H Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan
Kaufmann.
223
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 65?74,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis in Czech Social Media Using Supervised Machine
Learning
Ivan Habernal
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
habernal@kiv.zcu.cz
Toma?s? Pta?c?ek
Department of Computer
Science and Engineering,
Faculty of Applied Sciences
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
tigi@kiv.zcu.cz
Josef Steinberger
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
jstein@kiv.zcu.cz
Abstract
This article provides an in-depth research of
machine learning methods for sentiment ana-
lysis of Czech social media. Whereas in En-
glish, Chinese, or Spanish this field has a
long history and evaluation datasets for vari-
ous domains are widely available, in case of
Czech language there has not yet been any
systematical research conducted. We tackle
this issue and establish a common ground for
further research by providing a large human-
annotated Czech social media corpus. Fur-
thermore, we evaluate state-of-the-art super-
vised machine learning methods for sentiment
analysis. We explore different pre-processing
techniques and employ various features and
classifiers. Moreover, in addition to our newly
created social media dataset, we also report re-
sults on other widely popular domains, such
as movie and product reviews. We believe
that this article will not only extend the current
sentiment analysis research to another family
of languages, but will also encourage competi-
tion which potentially leads to the production
of high-end commercial solutions.
1 Introduction
Sentiment analysis has become a mainstream re-
search field in the past decade. Its impact can be
seen in many practical applications, ranging from
analyzing product reviews (Stepanov and Riccardi,
2011) to predicting sales and stock markets using so-
cial media monitoring (Yu et al, 2013). The users?
opinions are mostly extracted either on a certain po-
larity scale, or binary (positive, negative); various
levels of granularity are also taken into account, e.g.,
document-level, sentence-level, or aspect-based sen-
timent (Hajmohammadi et al, 2012).
Most of the research in automatic sentiment ana-
lysis of social media has been performed in English
and Chinese, as shown by several recent surveys,
i.e., (Liu and Zhang, 2012; Tsytsarau and Palpanas,
2012). For Czech language, there have been very
few attempts, although the importance of sentiment
analysis of social media became apparent, i.e., dur-
ing the recent presidential elections 1. Many Czech
companies also discovered a huge potential in social
media marketing and started launching campaigns,
contests, and even customer support on Facebook?
the dominant social network of the Czech online
community with approximately 3.5 million users.2
However, one aspect still eludes many of them: au-
tomatic analysis of customer sentiment of products,
services, or even a brand or a company name. In
many cases, sentiment is still labeled manually, ac-
cording to our information from one of the leading
Czech companies for social media monitoring.
Automatic sentiment analysis in the Czech envi-
ronment has not yet been thoroughly targeted by the
research community. Therefore it is necessary to
create a publicly available labeled dataset as well as
to evaluate the current state of the art for two rea-
sons. First, many NLP methods must deal with high
flection and rich syntax when processing the Czech
language. Facing these issues may lead to novel
1http://www.mediaguru.cz/2013/01/
analyza-facebook-rozhodne-o-volbe-prezidenta/ [in
Czech]
2http://www.czso.cz/csu/redakce.nsf/i/
uzivatele facebooku [in Czech]
65
approaches to sentiment analysis as well. Second,
freely accessible and well-documented datasets, as
known from many shared NLP tasks, may stimulate
competition which usually leads to the production of
cutting-edge solutions.3
This article focuses on document-level sentiment
analysis performed on three different Czech datasets
using supervised machine learning. As the first
dataset, we created a Facebook corpus consisting
of 10,000 posts. The dataset was manually la-
beled by two annotators. The other two datasets
come from online databases of movie and prod-
uct reviews, whose sentiment labels were derived
from the accompanying star ratings from users of
the databases. We provide all these labeled datasets
under Creative Commons BY-NC-SA licence4
at http://liks.fav.zcu.cz/sentiment ,
together with the sources for all the presented exper-
iments.
The rest of this article is organized as follows.
Section 2 examines the related work with a focus
on the Czech research and social media. Section 3
thoroughly describes the datasets and the annotation
process. In section 4, we list the employed features
and describe our approach to classification. Finally,
section 5 contains the results with a thorough discus-
sion.
2 Related work
There are two basic approaches to sentiment ana-
lysis: dictionary-based and machine learning-based.
While dictionary-based methods usually depend on
a sentiment dictionary (or a polarity lexicon) and a
set of handcrafted rules (Taboada et al, 2011), ma-
chine learning-based methods require labeled train-
ing data that are later represented as features and
fed into a classifier. Recent attempts have also in-
vestigated semi-supervised methods that incorporate
auxiliary unlabeled data (Zhang et al, 2012).
3E.g., named entity recognition based on Conditional Ran-
dom Fields emerged from CoNLL-2003 named entity recogni-
tion shared task.
4http://creativecommons.org/licenses/
by-nc-sa/3.0/
2.1 Supervised machine learning for sentiment
analysis
The key point of using machine learning for senti-
ment analysis lies in engineering a representative set
of features. Pang et al (2002) experimented with
unigrams (presence of a certain word, frequencies of
words), bigrams, part-of-speech (POS) tags, and ad-
jectives on a Movie Review dataset. Martineau and
Finin (2009) tested various weighting schemes for
unigrams based on TFIDF model (Manning et al,
2008) and proposed delta weighting for a binary sce-
nario (positive, negative). Their approach was later
extended by Paltoglou and Thelwall (2010) who pro-
posed further improvement in delta TFIDF weight-
ing.
The focus of the current sentiment analysis re-
search is shifting towards social media, mainly tar-
geting Twitter (Kouloumpis et al, 2011; Pak and
Paroubek, 2010) and Facebook (Go et al, 2009;
Ahkter and Soria, 2010; Zhang et al, 2011; Lo?pez et
al., 2012). Analyzing media with very informal lan-
guage benefits from involving novel features, such
as emoticons (Pak and Paroubek, 2010; Montejo-
Ra?ez et al, 2012), character n-grams (Blamey et al,
2012), POS and POS ratio (Ahkter and Soria, 2010;
Kouloumpis et al, 2011), or word shape (Go et al,
2009; Agarwal et al, 2011).
In many cases, the gold data for training and test-
ing the classifiers are created semi-automatically, as
in, e.g., (Kouloumpis et al, 2011; Go et al, 2009;
Pak and Paroubek, 2010). In the first step, random
samples from a large dataset are drawn according to
presence of emoticons (usually positive and nega-
tive) and are then filtered manually. Although large
high-quality collections can be created very quickly
using this approach, it makes a strong assumption
that every positive or negative post must contain an
emoticon.
Balahur and Tanev (2012) performed experiments
with Twitter posts as part of the CLEF 2012 Re-
pLab5. They classified English and Spanish tweets
by a small but precise lexicon, which contained also
slang, combined with a set of rules that capture the
manner in which sentiment is expressed in social
media.
5http://www.limosine-project.eu/events/
replab2012
66
Since the limited space of this paper does not al-
low us to present detailed evaluation from the related
work, we recommend an in-depth survey by Tsytsa-
rau and Palpanas (2012) for actual results obtained
from the abovementioned methods.
2.2 Sentiment analysis in Czech environment
Veselovska? et al (2012) presented an initial research
on Czech sentiment analysis. They created a corpus
which contains polarity categories of 410 news sen-
tences. They used the Naive Bayes classifier and
a classifier based on a lexicon generated from an-
notated data. The corpus is not publicly available,
moreover, due to the small size of the corpus no
strong conclusions can be drawn.
Steinberger et al (2012) proposed a semi-
automatic ?triangulation? approach to creating sen-
timent dictionaries in many languages, including
Czech. They first produced high-level gold-standard
sentiment dictionaries for two languages and then
translated them automatically into the third lan-
guage by a state-of-the-art machine translation ser-
vice. Finally, the resulting sentiment dictionaries
were merged by taking overlap from the two auto-
matic translations.
A multilingual parallel news corpus annotated
with opinions towards entities was presented in
(Steinberger et al, 2011). Sentiment annotations
were projected from one language to several others,
which saved annotation time and guaranteed compa-
rability of opinion mining evaluation results across
languages. The corpus contains 1,274 news sen-
tences where an entity (the target of the sentiment
analysis) occurs. It contains 7 languages including
Czech. Their research targets fundamentally differ-
ent objectives from our research as they focus on
news media and aspect-based sentiment analysis.
3 Datasets
3.1 Social media dataset
The initial selection of Facebook brand pages for our
dataset was based on the ?top? Czech pages, accord-
ing to the statistics from SocialBakers.6 We focused
on pages with a large Czech fan base and a sufficient
number of Czech posts. Using Facebook Graph API
6http://www.socialbakers.com/facebook-pages/
brands/czech-republic/
and Java Language Detector7 we acquired 10,000
random posts in the Czech language from nine dif-
ferent Facebook pages. The posts were then com-
pletely anonymized as we kept only their textual
contents.
Sentiment analysis of posts at Facebook brand
pages usually serves as a marketing feedback of user
opinions about brands, services, products, or current
campaigns. Thus we consider the sentiment target
to be the given product, brand, etc. Typically, users?
complaints hold negative sentiment, whereas joy or
happiness about the brand is taken as positive. We
also added another class called bipolar which rep-
resents both positive and negative sentiment in one
post.8 In some cases, the user?s opinion, although
being somehow positive, does not relate to the given
page.9 Therefore the sentiment is treated as neutral
in these cases, according to our above-mentioned as-
sumption.
The complete 10k dataset was independently an-
notated by two annotators. The inter-annotator
agreement (Cohen?s ?) between these two anno-
tators reaches 0.66 which represents a substantial
agreement level (Pustejovsky and Stubbs, 2013),
therefore the task can be considered as well-defined.
The gold data were created based on the agree-
ment of the two annotators. They disagreed in
2,216 cases. To solve these conflicts, we involved
a third super-annotator to assign the final sentiment
label. However, even after the third annotator?s la-
beling, there was still no agreement for 308 labels.
These cases were later solved by a fourth annotator.
We discovered that most of these conflicting cases
were classified as either neutral or bipolar. These
posts were often difficult to label because the author
used irony, sarcasm or the context or previous posts.
These issues remain open.
The Facebook dataset contains of 2,587 positive,
5,174 neutral, 1,991 negative, and 248 bipolar posts,
respectively. We ignore the bipolar class later in all
experiments. The sentiment distribution among the
7http://code.google.com/p/jlangdetect/
8For example ?to bylo moc dobry ,fakt jsem se nadlabla :-D
skoda ze uz neni v nabidce???It was very tasty, I really stuffed
myself :-D sad it?s not on the menu anymore?.
9Certain campaigns ask the fans for, i.e., writing a poem?
these posts are mostly positive (or funny, at least) but are irrele-
vant for the desired task.
67
source pages is shown in Figure 1. The statistics
reveal negative opinions towards cell phone oper-
ators and positive opinions towards, e.g., perfumes
and ZOO.
Figure 1: Social media dataset statistics
3.2 Movie review dataset
Movie reviews as a corpus for sentiment analysis
has been used in research since the pioneering re-
search conducted by Pang et al (2002). Therefore
we covered the same domain in our experiments as
well. We downloaded 91,381 movie reviews from
the Czech Movie Database10 and split them into 3
categories according to their star rating (0?2 stars as
negative, 3?4 stars as neutral, 5?6 stars as positive).
The dataset contains of 30,897 positive, 30,768 neu-
tral, and 29,716 negative reviews, respectively.
3.3 Product review dataset
Another very popular domain for sentiment analy-
sis deals with product reviews (Hu and Liu, 2004).
We crawled all user reviews from a large Czech e-
shop Mall.cz11 which offers a wide range of prod-
ucts. The product reviews are accompanied with star
ratings on the scale 0?5. We took a different strat-
egy for assigning sentiment labels. Whereas in the
movie dataset the distribution of stars was rather uni-
form, in the product review domain the ratings were
skewed towards the higher values. After a manual
inspection we discovered that 4-star ratings mostly
correspond to neutral opinions and 3 or less stars de-
note mostly negative comments. Thus we split the
10http://www.csfd.cz/
11http://www.mall.cz
dataset into three categories according to this obser-
vation. The final dataset consists of 145,307 posts
(102,977 positive, 31,943 neutral, and 10,387 nega-
tive).
4 Classification
4.1 Preprocessing
As pointed out by Laboreiro et al (2010), tokeniza-
tion significantly affects sentiment analysis, espe-
cially in case of social media. Although Ark-tweet-
nlp tool (Gimpel et al, 2011) was developed and
tested in English, it yields satisfactory results in
Czech as well, according to our initial experiments
on the Facebook corpus. Its significant feature is
proper handling of emoticons and other special char-
acter sequences that are typical for social media.
Furthermore, we remove stopwords using the stop-
word list from Apache Lucene project.12
In many NLP applications, a very popular pre-
processing technique is stemming. We tested Czech
light stemmer (Dolamic and Savoy, 2009) and High
Precision Stemmer13. Another widely-used method
for reducing the vocabulary size, and thus the feature
space, is lemmatization. For Czech language the
only currently available lemmatizer is shipped with
Prague Dependency Treebank (PDT) toolkit (Hajic?
et al, 2006). However, we use our in-house Java
HMM-based implementation using the PDT train-
ing data as we need a better control over each pre-
processing step.
Part-of-speech tagging is done using our in-house
Java solution that exploits Prague Dependency Tree-
bank (PDT) data as well. However, since PDT is
trained on news corpora, we doubt it is suitable for
tagging social media that are written in very infor-
mal language (consult, i.e., (Gimpel et al, 2011)
where similar issues were tackled in English).
Since the Facebook dataset contains a huge num-
ber of grammar mistakes and misspellings (typ-
ically ?i/y?,?e?/je/ie?, and others), we incorporated
phonetic transcription to International Phonetic Al-
phabet (IPA) in order to reduce the effect of these
mistakes. We rely on eSpeak14 implementation. An-
12http://lucene.apache.org/core/
13Publication pending; please visit
http://liks.fav.zcu.cz/HPS/.
14http://espeak.sourceforge.net
68
Pipe 1 Pipe 2 Pipe 3
Tokenizing
ArkTweetNLP
POS tagging
PDT
Stem (S) Lemma (L)
none (n) PDT (p)
light (l)
HPS (h)
Stopwords
remove
Casing (C) Phonetic (P) ?
keep (k) eSpeak (e)
lower (l)
Table 1: The preprocessing pipes (top-down). Various
combinations of methods can be denoted using the ap-
propriate labels, e.g. ?SnCk? means 1. tokenizing, 2.
POS-tagging, 3. no stemming, 4. removing stopwords,
and 5. no casing, or ?Lp? means 1. tokenizing, 2. POS-
tagging, 3. lemmatization using PDT, and 4. removing
stopwords.
other preprocessing step might involve removing di-
acritics, as many Czech users type only using unac-
cented characters. However, posts without diacritics
represent only about 8% of our datasets, thus we de-
cided to keep diacritics unaffected.
The complete preprocessing diagram and its vari-
ants is depicted in Table 1. Overall, there are 10
possible preprocessing ?pipe? configurations.
4.2 Features
N-gram features We use presence of unigrams
and bigrams as binary features. The feature space is
pruned by minimum n-gram occurrence which was
empirically set to 5. Note that this is the baseline
feature in most of the related work.
Character n-gram features Similarly to the word
n-gram features, we added character n-gram fea-
tures, as proposed by, e.g., (Blamey et al, 2012). We
set the minimum occurrence of a particular charac-
ter n-gram to 5, in order to prune the feature space.
Our feature set contains 3-grams to 6-grams.
POS-related features Direct usage of part-of-
speech n-grams that would cover sentiment patterns
has not shown any significant improvement in the re-
lated work. Still, POS tags provide certain character-
istics of a particular post. We implemented various
POS features that include, e.g., the number of nouns,
verbs, and adjectives (Ahkter and Soria, 2010), the
ratio of nouns to adjectives and verbs to adverbs
(Kouloumpis et al, 2011), and number of negative
verbs.
Emoticons We adapted the two lists of emoticons
that were considered as positive and negative from
(Montejo-Ra?ez et al, 2012). The feature captures
number of occurrences of each class of emoticons
within the text.
Delta TFIDF variants for binary scenarios Al-
though simple binary word features (presence of a
certain word) reach surprisingly good performance,
they have been surpassed by various TFIDF-based
weighting, such as Delta TFIDF (Martineau and
Finin, 2009), or Delta BM25 TFIDF (Paltoglou and
Thelwall, 2010). Delta-TFIDF still uses traditional
TFIDF word weighting but treats positive and nega-
tive documents differently. However, all the exist-
ing related works which use this kind of features
deal only with binary decisions (positive/negative),
thus we filtered out neutral documents from the
datasets.15 We implemented the most promising
weighting schemes from (Paltoglou and Thelwall,
2010), namely Augmented TF, LogAve TF, BM25
TF, Delta Smoothed IDF, Delta Prob. IDF, Delta
Smoothed Prob. IDF, and Delta BM25 IDF.
4.3 Classifiers
All evaluation tests were performed using two clas-
sifiers, Maximum Entropy (MaxEnt) and Support
Vector Machines (SVM). Although Naive Bayes
classifier is also widely used in the related work, we
did not include it as it usually performs worse than
SVM or MaxEnt. We used a pure Java framework
for machine learning16 with default settings (linear
kernel for SVM).
5 Results
For each combination from the preprocessing
pipeline (refer to Table 1) we assembled various sets
of features and employed two classifiers. In the first
15Opposite to leave-one-out cross validation in (Paltoglou
and Thelwall, 2010), we still use 10-fold cross validation in all
experiments.
16http://liks.fav.zcu.cz/ml
69
scenario, we classify into all three classes (positive,
negative, and neutral).17 In the second scenario,
we follow a strand of related research, e.g., (Mar-
tineau and Finin, 2009; Celikyilmaz et al, 2010),
that deals only with positive and negative classes.
For these purposes we filtered out all the neutral doc-
uments from the datasets. Furthermore, in this sce-
nario we evaluate only features based on weighted
delta-TFIDF, as, e.g., in (Paltoglou and Thelwall,
2010). We also involved only MaxEnt classifier into
the second scenario.
All tests were conducted in the 10-fold cross val-
idation manner. We report macro F-measure, as
it allows comparing classifier results on different
datasets. Moreover, we do not report micro F-
measure (accuracy) as it tends to prefer performance
on dominant classes in highly unbalanced datasets
(Manning et al, 2008), which is, e.g., the case of
our Product Review dataset where most of the labels
are positive.
5.1 Social media
Table 2 shows the results for the 3-class classifica-
tion scenario on the Facebook dataset. The row la-
bels denote the preprocessing configuration accord-
ing to Table 1. In most cases, maximum entropy
classifier significantly outperforms SVM. The com-
bination of all features (the last column) yields the
best results regardless to the preprocessing steps.
The reason might be that the involved character n-
gram feature captures subtle sequences which repre-
sent subjective punctuation or emoticons, that were
not covered by the emoticon feature. On average,
the best results were obtained when HPS stemmer
and lowercasing or phonetic transcription were in-
volved (lines ShCl and ShPe). This configuration
significantly outperforms other preprocessing tech-
niques for token-based features (see column Unigr
+ bigr + POS + emot.).
In the second scenario we evaluated various
TFIDF weighting schemes for binary sentiment
classification. The results are shown in Table 3.
The three-character notation consists of term fre-
quency, inverse document frequency, and normal-
ization. Due to a large number of possible combi-
nations, we report only the most successful ones,
17We ignore the bipolar posts in the current research.
namely Augmented?a and LogAve?L term fre-
quency, followed by Delta Smoothed??(t?), Delta
Smoothed Prob.??(p?), and Delta BM25??(k)
inverse document frequency; normalization was not
involved. We can see that the baseline (the first col-
umn bnn) is usually outperformed by any weighted
TFIDF technique. Moreover, using any kind of
stemming (the row entitled various*) significantly
improves the results. For the exact formulas of the
delta TFIDF variants please refer to (Paltoglou and
Thelwall, 2010).
We also tested the impact of TFIDF word fea-
tures when added to other features from the first sce-
nario (refer to Table 2). Column FS1 in Table 3 dis-
plays results for a feature set with the simple binary
presence-of-the-word feature (binary unigrams). In
the last column FS2 we replaced this binary feature
with TFIDF weighted feature a?(t?)n. It turned out
that the weighed form of word feature does not im-
prove the performance, when compared with sim-
ple binary unigram feature. Furthermore, a set of
different features (words, bigrams, POS, emoticons,
character n-grams) significantly outperforms a sin-
gle TFIDF weighted feature.
We also report the effect of the dataset size on
the performance. We randomly sampled 10 subsets
from the dataset (1k, 2k, etc.) and tested the per-
formance; still using 10-fold cross validation. We
took the most promising preprocessing configura-
tion (ShCl) and MaxEnt classifier. As can be seen in
Figure 2, while the dataset grows to approx 6k?7k
items, the performance rises for most combinations
of features. At 7k-items dataset, the performance
begins to reach its limits for most combinations of
features and hence adding more data does not lead
to a significant improvement.
5.1.1 Upper limits of automatic sentiment
analysis
To see the upper limits of the task itself, we also
evaluate the annotator?s judgments. Although the
gold labels were chosen after a consensus of at least
two people, there were many conflicting cases that
must have been solved by a third or even a fourth
person. Thus even the original annotators do not
achieve 1.00 F-measure on the gold data.
We present ?performance? results of both annota-
tors and of the best system as well (MaxEnt classi-
70
Facebook dataset, 3 classes
Unigrams Unigr + bigrams Unigr + bigr + Unigr + bigr + Unigr + bigr + POS +
POS features POS + emot. emot. + char n-grams
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
SnCk 0.63 0.64 0.63 0.64 0.66 0.64 0.66 0.64 0.69 0.67
SnCl 0.63 0.64 0.63 0.64 0.66 0.63 0.66 0.63 0.69 0.68
SlCk 0.65 0.67 0.66 0.67 0.68 0.66 0.67 0.66 0.69 0.67
SlCl 0.65 0.67 0.65 0.67 0.68 0.66 0.69 0.66 0.69 0.67
ShCk 0.66 0.67 0.66 0.67 0.68 0.67 0.67 0.67 0.69 0.67
ShCl 0.66 0.66 0.66 0.67 0.69 0.67 0.69 0.67 0.69 0.67
SnPe 0.64 0.65 0.64 0.65 0.67 0.65 0.67 0.65 0.68 0.68
SlPe 0.65 0.67 0.65 0.67 0.68 0.67 0.67 0.66 0.68 0.67
ShPe 0.66 0.67 0.66 0.67 0.69 0.66 0.69 0.66 0.68 0.67
Lp 0.64 0.65 0.63 0.65 0.67 0.64 0.67 0.65 0.68 0.67
Table 2: Results on the Facebook dataset, classification into 3 classes. Macro F-measure, 95% confidence interval
= ?0.01. Bold numbers denote the best results.
Facebook dataset, positive and negative classes only
bnn a?(t?)n a?(p?)n a?(k)n L?(t?)n L?(p?)n L?(k)n FS1 FS2
SnCk 0.83 0.86 0.86 0.86 0.85 0.86 0.86 0.90 0.89
SnCl 0.84 0.86 0.86 0.86 0.86 0.86 0.86 0.90 0.90
various* 0.85 0.88 0.88 0.88 0.88 0.88 0.88 0.90 0.90
SnPe 0.84 0.86 0.86 0.86 0.86 0.86 0.86 0.90 0.90
Lp 0.84 0.86 0.85 0.85 0.86 0.86 0.86 0.88 0.88
* same results for ShCk, ShCl, SlCl, SlPe, SlCk, and ShPe
FS1: Unigr + bigr + POS + emot. + char n-grams
FS2: a?(t?)n + bigr + POS + emot. + char n-grams
Table 3: Results on the Facebook dataset for various TFIDF-weighted features, classification into 2 classes. Macro F-
measure, 95% confidence interval = ?0.01. Underlined numbers show the best results for TFIDF-weighted features.
Bold numbers denote the best overall results.
Figure 2: Performance wrt. data size. Using ShCl pre-
processing and MaxEnt classifier.
fier, all features, ShCl preprocessing). Table 4 shows
the results as confusion matrices. For each class
(p?positive, n?negative, 0?neutral) we also re-
port precision, recall, and F-measure. The row head-
ings denote gold labels, the column headings repre-
sent values assigned by the annotators or the sys-
tem.18 The annotators? results show what can be ex-
pected from a ?perfect? system that would solve the
task the way a human would.
In general, both annotators judge all three classes
with very similar F-measure. By contrast, the sys-
tem?s F-measure is very low for negative posts (0.54
vs. ? 0.75 for neutral and positive). We offer the
following explanation. First, many of the negative
posts surprisingly contain happy emoticons, which
18Even though the task has three classes, the annotators also
used ?b? for ?bipolar and ??? for ?cannot decide?.
71
Annotator 1
0 n p ? b P R Fm
0 4867 136 115 2 54 .93 .94 .93
n 199 1753 6 0 33 .93 .88 .90
p 175 6 2376 0 30 .95 .92 .93
Macro Fm: .92
Annotator 2
0 n p ? b P R Fm
0 4095 495 573 3 8 .95 .79 .86
n 105 1878 6 0 2 .79 .94 .86
p 100 12 2468 3 4 .81 .95 .88
Macro Fm: .86
Best system
0 n p P R Fm
0 4014 670 490 .74 .78 .76
n 866 1027 98 .57 .52 .54
p 563 102 1922 .77 .74 .75
Macro Fm: .69
Table 4: Confusion matrices for three-class classification.
?Best system? configuration: all features (unigram, bi-
gram, POS, emoticons, character n-grams), ShCl prepro-
cessing, and MaxEnt classifier. 95% confidence interval
= ?0.01.
could be a misleading feature for the classifier. Sec-
ond, the language of the negative posts in not as ex-
plicit as for the positive ones in many cases; the neg-
ativity is ?hidden? in irony, or in a larger context (i.e.,
?Now I?m sooo satisfied with your competitor :))?).
This remains an open issue for the future research.
5.2 Product and movie reviews
For the other two datasets, the product reviews and
movie reviews, we slightly changed the configura-
tion. First, we removed the character n-grams from
the feature sets, otherwise the feature space would
become too large for feasible computing. Second,
we abandoned SVM as it became computationally
infeasible for such a large datasets.
Table 5 (left-hand part) presents results on the
product reviews. The combination of unigrams and
bigrams works best, almost regardless of the prepro-
cessing. By contrast, POS features rapidly decrease
the performance. We suspect that POS features do
not carry any useful information in this case and by
introducing a lot of ?noise? they cause that the op-
timization function in the MaxEnt classifier fails to
find a global minimum.
In the right-hand part of Table 5 we can see the
results on the movie reviews. Again, the bigram fea-
ture performs best, paired with combination of HPS
stemmer and phonetic transcription (ShPe). Adding
POS-related features causes a large drop in perfor-
mance. We can conclude that for larger texts, the
bigram-based feature outperforms unigram features
and, in some cases, a proper preprocessing may fur-
ther significantly improve the results.
6 Conclusion
This article presented an in-depth research of super-
vised machine learning methods for sentiment ana-
lysis of Czech social media. We created a large
Facebook dataset containing 10,000 posts, accom-
panied by human annotation with substantial agree-
ment (Cohen?s ? 0.66). The dataset is freely avail-
able for non-commercial purposes.19 We thoroughly
evaluated various state-of-the-art features and clas-
sifiers as well as different language-specific prepro-
cessing techniques. We significantly outperformed
the baseline (unigram feature without preprocess-
ing) in three-class classification and achieved F-
measure 0.69 using a combination of features (un-
igrams, bigrams, POS features, emoticons, charac-
ter n-grams) and preprocessing techniques (unsu-
pervised stemming and phonetic transcription). In
addition, we reported results in two other domains
(movie and product reviews) with a significant im-
provement over the baseline.
To the best of our knowledge, this article is the
only of its kind that deals with sentiment analysis
in Czech social media in such a thorough manner.
Not only it uses a dataset that is magnitudes larger
than any from the related work, but also incorporates
state-of-the-art features and classifiers. We believe
that the outcomes of this article will not only help
to set the common ground for sentiment analysis for
the Czech language but also help to extend the re-
search outside the mainstream languages in this re-
search field.
Acknowledgement
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
19We encourage other researchers to download our dataset
for their research in the sentiment analysis field.
72
Product reviews, 3 classes Movie reviews, 3 classes
FS1 FS2 FS3 FS4 FS1 FS2 FS3 FS4
SnCk 0.70 0.74 0.52 0.49 0.76 0.77 0.71 0.61
SnCl 0.71 0.75 0.51 0.52 0.76 0.77 0.71 0.70
SlCk 0.67 0.75 0.59 0.55 0.78 0.78 0.73 0.72
SlCl 0.67 0.75 0.56 0.57 0.78 0.78 0.71 0.71
ShCk 0.67 0.75 0.57 0.57 0.78 0.78 0.74 0.72
ShCl 0.67 0.74 0.55 0.57 0.77 0.78 0.73 0.73
SnPe 0.69 0.74 0.50 0.55 0.77 0.78 0.69 0.72
SlPe 0.67 0.75 0.55 0.57 0.78 0.78 0.73 0.73
ShPe 0.68 0.74 0.56 0.59 0.78 0.79 0.74 0.73
Lp 0.66 0.75 0.56 0.57 0.77 0.77 0.68 0.70
Table 5: Results on the product and movie review datasets, classification into 3 classes. FSx denote different feature
sets. FS1 = Unigrams; FS2 = Uni + bigrams; FS3 = Uni + big + POS features; FS4 = Uni + big + POS + emot. Macro
F-measure, 95% confidence interval ?0.002 (products), ?0.003 (movies). Bold numbers denote the best results.
systems and by the European Regional Develop-
ment Fund (ERDF), project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
ter of Excellence, CZ.1.05/1.1.00/02.0090. The
access to computing and storage facilities owned
by parties and projects contributing to the Na-
tional Grid Infrastructure MetaCentrum, provided
under the programme ?Projects of Large Infrastruc-
ture for Research, Development, and Innovations?
(LM2010005) is highly acknowledged.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Julie Kane Ahkter and Steven Soria. 2010. Sentiment
analysis: Facebook status messages. Technical report,
Stanford University. Final Project CS224N.
Alexandra Balahur and Hristo Tanev. 2012. Detecting
entity-related events and sentiments from tweets us-
ing multilingual resources. In Proceedings of the 2012
Conference and Labs of the Evaluation Forum Infor-
mation Access Evaluation meets Multilinguality, Mul-
timodality, and Visual Analytics.
Ben Blamey, Tom Crick, and Giles Oatley. 2012. R U
: -) or : -( ? character- vs. word-gram feature selec-
tion for sentiment classification of OSN corpora. In
Proceedings of AI-2012, The Thirty-second SGAI In-
ternational Conference on Innovative Techniques and
Applications of Artificial Intelligence, pages 207?212.
Springer.
A. Celikyilmaz, D. Hakkani-Tu?r, and Junlan Feng. 2010.
Probabilistic model-based sentiment analysis of twit-
ter messages. In Spoken Language Technology Work-
shop (SLT), 2010 IEEE, pages 79?84. IEEE.
Ljiljana Dolamic and Jacques Savoy. 2009. Indexing and
stemming approaches for the czech language. Infor-
mation Processing and Management, 45(6):714?720,
November.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague de-
pendency treebank 2.0. Linguistic Data Consortium,
Philadelphia.
Mohammad Sadegh Hajmohammadi, Roliana Ibrahim,
and Zulaiha Ali Othman. 2012. Opinion mining and
sentiment analysis: A survey. International Journal of
Computers & Technology, 2(3).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
73
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Media,
Barcelona, Catalonia, Spain, July 17-21, 2011. The
AAAI Press.
Gustavo Laboreiro, Lu??s Sarmento, Jorge Teixeira, and
Euge?nio Oliveira. 2010. Tokenizing micro-blogging
messages using a text classification approach. In Pro-
ceedings of the fourth workshop on Analytics for noisy
unstructured text data, AND ?10, pages 81?88, New
York, NY, USA. ACM.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415?463. Springer.
Roque Lo?pez, Javier Tejada, and Mike Thelwall. 2012.
Spanish sentistrength as a tool for opinion mining pe-
ruvian facebook and twitter. In Artificial Intelligence
Driven Solutions to Business and Engineering Prob-
lems, pages 82?85. ITHEA, Sofia, Bulgaria.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis. In
Proceedings of the Third International Conference on
Weblogs and Social Media, ICWSM 2009, San Jose,
California, USA. The AAAI Press.
A. Montejo-Ra?ez, E. Mart??nez-Ca?mara, M. T. Mart??n-
Valdivia, and L. A. Uren?a Lo?pez. 2012. Random
walk weighting over sentiwordnet for sentiment po-
larity detection on twitter. In Proceedings of the 3rd
Workshop in Computational Approaches to Subjectiv-
ity and Sentiment Analysis, WASSA ?12, pages 3?10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, LREC 2010. European Lan-
guage Resources Association.
Georgios Paltoglou and Mike Thelwall. 2010. A study of
information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 1386?1395, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing - Volume 10, EMNLP ?02, pages 79?
86, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
James Pustejovsky and Amber Stubbs. 2013. Natural
Language Annotation for Machine Learning. O?Reilly
Media, Sebastopol, CA 95472.
Josef Steinberger, Polina Lenkova, Mijail Alexandrov
Kabadjov, Ralf Steinberger, and Erik Van der Goot.
2011. Multilingual entity-centered sentiment analy-
sis evaluated by parallel corpora. In Proceedings of
the 8th International Conference on Recent Advances
in Natural Language Processing, RANLP?11, pages
770?775.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Alexandrov Kabadjov, Polina
Lenkova, Ralf Steinberger, Hristo Tanev, Silvia
Va?zquez, and Vanni Zavarella. 2012. Creating senti-
ment dictionaries via triangulation. Decision Support
Systems, 53:689??694.
E.A. Stepanov and G. Riccardi. 2011. Detecting gen-
eral opinions from customer surveys. In Data Mining
Workshops (ICDMW), 2011 IEEE 11th International
Conference on, pages 115?122.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Mikalai Tsytsarau and Themis Palpanas. 2012. Survey
on mining subjective data on the web. Data Mining
and Knowledge Discovery, 24(3):478?514, May.
Kater?ina Veselovska?, Jan Hajic? Jr., and Jana S?indlerova?.
2012. Creating annotated resources for polarity classi-
fication in Czech. In Proceedings of KONVENS 2012,
pages 296?304. O?GAI, September. PATHOS 2012
workshop.
Liang-Chih Yu, Jheng-Long Wu, Pei-Chann Chang, and
Hsuan-Shou Chu. 2013. Using a contextual entropy
model to expand emotion words and their intensity
for the sentiment classification of stock market news.
Knowledge Based Syst, 41:89?97.
Kunpeng Zhang, Yu Cheng, Yusheng Xie, Daniel Honbo,
Ankit Agrawal, Diana Palsetia, Kathy Lee, Wei keng
Liao, and Alok N. Choudhary. 2011. SES: Sentiment
elicitation system for social media data. In Data Min-
ing Workshops (ICDMW), 2011 IEEE 11th Confer-
ence on, Vancouver, BC, Canada, December 11, 2011,
pages 129?136. IEEE.
Dan Zhang, Luo Si, and Vernon J. Rego. 2012. Senti-
ment detection with auxiliary data. Information Re-
trieval, 15(3-4):373?390.
74
