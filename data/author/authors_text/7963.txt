Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 77?80,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Syntactic and Lexical-Based Discourse Segmenter
Milan Tofiloski
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
mta45@sfu.ca
Julian Brooke
Department of Linguistics
Simon Fraser University
Burnaby, BC, Canada
jab18@sfu.ca
Maite Taboada
Department of Linguistics
Simon Fraser University
Burnaby, BC, Canada
mtaboada@sfu.ca
Abstract
We present a syntactic and lexically based
discourse segmenter (SLSeg) that is de-
signed to avoid the common problem of
over-segmenting text. Segmentation is the
first step in a discourse parser, a system
that constructs discourse trees from el-
ementary discourse units. We compare
SLSeg to a probabilistic segmenter, show-
ing that a conservative approach increases
precision at the expense of recall, while re-
taining a high F-score across both formal
and informal texts.
1 Introduction?
Discourse segmentation is the process of de-
composing discourse into elementary discourse
units (EDUs), which may be simple sentences or
clauses in a complex sentence, and from which
discourse trees are constructed. In this sense, we
are performing low-level discourse segmentation,
as opposed to segmenting text into chunks or top-
ics (e.g., Passonneau and Litman (1997)). Since
segmentation is the first stage of discourse parsing,
quality discourse segments are critical to build-
ing quality discourse representations (Soricut and
Marcu, 2003). Our objective is to construct a dis-
course segmenter that is robust in handling both
formal (newswire) and informal (online reviews)
texts, while minimizing the insertion of incorrect
discourse boundaries. Robustness is achieved by
constructing discourse segments in a principled
way using syntactic and lexical information.
Our approach employs a set of rules for insert-
ing segment boundaries based on the syntax of
each sentence. The segment boundaries are then
further refined by using lexical information that
?This work was supported by an NSERC Discovery Grant
(261104-2008) to Maite Taboada. We thank Angela Cooper
and Morgan Mameni for their help with the reliability study.
takes into consideration lexical cues, including
multi-word expressions. We also identify clauses
that are parsed as discourse segments, but are not
in fact independent discourse units, and join them
to the matrix clause.
Most parsers can break down a sentence into
constituent clauses, approaching the type of out-
put that we need as input to a discourse parser.
The segments produced by a parser, however, are
too fine-grained for discourse purposes, breaking
off complement and other clauses that are not in a
discourse relation to any other segment. For this
reason, we have implemented our own segmenter,
utilizing the output of a standard parser. The pur-
pose of this paper is to describe our syntactic and
lexical-based segmenter (SLSeg), demonstrate its
performance against state-of-the-art systems, and
make it available to the wider community.
2 Related Work
Soricut and Marcu (2003) construct a statistical
discourse segmenter as part of their sentence-level
discourse parser (SPADE), the only implemen-
tation available for our comparison. SPADE is
trained on the RST Discourse Treebank (Carlson
et al, 2002). The probabilities for segment bound-
ary insertion are learned using lexical and syntac-
tic features. Subba and Di Eugenio (2007) use
neural networks trained on RST-DT for discourse
segmentation. They obtain an F-score of 84.41%
(86.07% using a perfect parse), whereas SPADE
achieved 83.1% and 84.7% respectively.
Thanh et al (2004) construct a rule-based
segmenter, employing manually annotated parses
from the Penn Treebank. Our approach is con-
ceptually similar, but we are only concerned with
established discourse relations, i.e., we avoid po-
tential same-unit relations by preserving NP con-
stituency.
77
3 Principles For Discourse Segmentation
Our primary concern is to capture interesting dis-
course relations, rather than all possible relations,
i.e., capturing more specific relations such as Con-
dition, Evidence or Purpose, rather than more gen-
eral and less informative relations such as Elabo-
ration or Joint, as defined in Rhetorical Structure
Theory (Mann and Thompson, 1988). By having a
stricter definition of an elementary discourse unit
(EDU), this approach increases precision at the ex-
pense of recall.
Grammatical units that are candidates for dis-
course segments are clauses and sentences. Our
basic principles for discourse segmentation follow
the proposals in RST as to what a minimal unit
of text is. Many of our differences with Carl-
son and Marcu (2001), who defined EDUs for the
RST Discourse Treebank (Carlson et al, 2002),
are due to the fact that we adhere closer to the orig-
inal RST proposals (Mann and Thompson, 1988),
which defined as ?spans? adjunct clauses, rather
than complement (subject and object) clauses. In
particular, we propose that complements of at-
tributive and cognitive verbs (He said (that)..., I
think (that)...) are not EDUs. We preserve con-
sistency by not breaking at direct speech (?X,? he
said.). Reported and direct speech are certainly
important in discourse (Prasad et al, 2006); we do
not believe, however, that they enter discourse re-
lations of the type that RST attempts to capture.
In general, adjunct, but not complement clauses
are discourse units. We require all discourse seg-
ments to contain a verb. Whenever a discourse
boundary is inserted, the two newly created seg-
ments must each contain a verb. We segment coor-
dinated clauses (but not coordinated VPs), adjunct
clauses with either finite or non-finite verbs, and
non-restrictive relative clauses (marked by com-
mas). In all cases, the choice is motivated by
whether a discourse relation could hold between
the resulting segments.
4 Implementation
The core of the implementation involves the con-
struction of 12 syntactically-based segmentation
rules, along with a few lexical rules involving a list
of stop phrases, discourse cue phrases and word-
level parts of speech (POS) tags. First, paragraph
boundaries and sentence boundaries using NIST?s
sentence segmenter1 are inserted. Second, a sta-
tistical parser applies POS tags and the sentence?s
syntactic tree is constructed. Our syntactic rules
are executed at this stage. Finally, lexical rules,
as well as rules that consider the parts-of-speech
for individual words, are applied. Segment bound-
aries are removed from phrases with a syntactic
structure resembling independent clauses that ac-
tually are used idiomatically, such as as it stands
or if you will. A list of phrasal discourse cues
(e.g., as soon as, in order to) are used to insert
boundaries not derivable from the parser?s output
(phrases that begin with in order to... are tagged as
PP rather than SBAR). Segmentation is also per-
formed within parentheticals (marked by paren-
theses or hyphens).
5 Data and Evaluation
5.1 Data
The gold standard test set consists of 9 human-
annotated texts. The 9 documents include 3 texts
from the RST literature2, 3 online product reviews
from Epinions.com, and 3 Wall Street Journal ar-
ticles taken from the Penn Treebank. The texts av-
erage 21.2 sentences, with the longest text having
43 sentences and the shortest having 6 sentences,
for a total of 191 sentences and 340 discourse seg-
ments in the 9 gold-standard texts.
The texts were segmented by one of the au-
thors following guidelines that were established
from the project?s beginning and was used as the
gold standard. The annotator was not directly in-
volved in the coding of the segmenter. To ensure
the guidelines followed clear and sound principles,
a reliability study was performed. The guidelines
were given to two annotators, both graduate stu-
dents in Linguistics, that had no direct knowledge
of the project. They were asked to segment the 9
texts used in the evaluation.
Inter-annotator agreement across all three anno-
tators using Kappa was .85, showing a high level
of agreement. Using F-score, average agreement
of the two annotators against the gold standard was
also high at .86. The few disagreements were pri-
marily due to a lack of full understanding of the
guidelines (e.g., the guidelines specify to break ad-
junct clauses when they contain a verb, but one
of the annotators segmented prepositional phrases
1http://duc.nist.gov/duc2004/software/
duc2003.breakSent.tar.gz
2Available from the RST website http://www.sfu.ca/rst/
78
Epinions Treebank Original RST Combined Total
System P R F P R F P R F P R F
Baseline .22 .70 .33 .27 .89 .41 .26 .90 .41 .25 .80 .38
SPADE (coarse) .59 .66 .63 .63 1.0 .77 .64 .76 .69 .61 .79 .69
SPADE (original) .36 .67 .46 .37 1.0 .54 .38 .76 .50 .37 .77 .50
Sundance .54 .56 .55 .53 .67 .59 .71 .47 .57 .56 .58 .57
SLSeg (Charniak) .97 .66 .79 .89 .86 .87 .94 .76 .84 .93 .74 .83
SLSeg (Stanford) .82 .74 .77 .82 .86 .84 .88 .71 .79 .83 .77 .80
Table 1: Comparison of segmenters
that had a similar function to a full clause). With
high inter-annotator agreement (and with any dis-
agreements and errors resolved), we proceeded to
use the co-author?s segmentations as the gold stan-
dard.
5.2 Evaluation
The evaluation uses standard precision, recall and
F-score to compute correctly inserted segment
boundaries (we do not consider sentence bound-
aries since that would inflate the scores). Precision
is the number of boundaries in agreement with the
gold standard. Recall is the total number of bound-
aries correct in the system?s output divided by the
number of total boundaries in the gold standard.
We compare the output of SLSeg to SPADE.
Since SPADE is trained on RST-DT, it inserts seg-
ment boundaries that are different from what our
annotation guidelines prescribe. To provide a fair
comparison, we implement a coarse version of
SPADE where segment boundaries prescribed by
the RST-DT guidelines, but not part of our seg-
mentation guidelines, are manually removed. This
version leads to increased precision while main-
taining identical recall, thus improving F-score.
In addition to SPADE, we also used the Sun-
dance parser (Riloff and Phillips, 2004) in our
evaluation. Sundance is a shallow parser which
provides clause segmentation on top of a basic
word-tagging and phrase-chunking system. Since
Sundance clauses are also too fine-grained for our
purposes, we use a few simple rules to collapse
clauses that are unlikely to meet our definition of
EDU. The baseline segmenter in Table 1 inserts
segment boundaries before and after all instances
of S, SBAR, SQ, SINV, SBARQ from the syntac-
tic parse (text spans that represent full clauses able
to stand alone as sentential units). Finally, two
parsers are compared for their effect on segmenta-
tion quality: Charniak (Charniak, 2000) and Stan-
ford (Klein and Manning, 2003).
5.3 Qualitative Comparison
Comparing the outputs of SLSeg and SPADE on
the Epinions.com texts illustrates key differences
between the two approaches.
[Luckily we bought the extended pro-
tection plans from Lowe?s,] # [so we
are waiting] [for Whirlpool to decide]
[if they want to do the costly repair] [or
provide us with a new machine].
In this example, SLSeg inserts a single bound-
ary (#) before the word so, whereas SPADE in-
serts four boundaries (indicated by square brack-
ets). Our breaks err on the side of preserving se-
mantic coherence, e.g., the segment for Whirlpool
to decide depends crucially on the adjacent seg-
ments for its meaning. In our opinion, the rela-
tions between these segments are properly the do-
main of a semantic, but not a discourse, parser. A
clearer example that illustrates the pitfalls of fine-
grained discourse segmenting is shown in the fol-
lowing output from SPADE:
[The thing] [that caught my attention
was the fact] [that these fantasy novels
were marketed...]
Because the segments are a restrictive relative
clause and a complement clause, respectively,
SLSeg does not insert any segment boundaries.
6 Results
Results are shown in Table 1. The combined in-
formal and formal texts show SLSeg (using Char-
niak?s parser) with high precision; however, our
overall recall was lower than both SPADE and the
baseline. The performance of SLSeg on the in-
formal and formal texts is similar to our perfor-
79
mance overall: high precision, nearly identical re-
call. Our system outperforms all the other systems
in both precision and F-score, confirming our hy-
pothesis that adapting an existing system would
not provide the high-quality discourse segments
we require.
The results of using the Stanford parser as an
alternative to the Charniak parser show that the
performance of our system is parser-independent.
High F-score in the Treebank data can be at-
tributed to the parsers having been trained on Tree-
bank. Since SPADE also utilizes the Charniak
parser, the results are comparable.
Additionally, we compared SLSeg and SPADE
to the original RST segmentations of the three
RST texts taken from RST literature. Performance
was similar to that of our own annotations, with
SLSeg achieving an F-score of .79, and SPADE
attaining .38. This demonstrates that our approach
to segmentation is more consistent with the origi-
nal RST guidelines.
7 Discussion
We have shown that SLSeg, a conservative rule-
based segmenter that inserts fewer discourse
boundaries, leads to higher precision compared to
a statistical segmenter. This higher precision does
not come at the expense of a significant loss in
recall, as evidenced by a higher F-score. Unlike
statistical parsers, our system requires no training
when porting to a new domain.
All software and data are available3. The
discourse-related data includes: a list of clause-
like phrases that are in fact discourse markers
(e.g., if you will, mind you); a list of verbs used
in to-infinitival and if complement clauses that
should not be treated as separate discourse seg-
ments (e.g., decide in I decided to leave the car
at home); a list of unambiguous lexical cues for
segment boundary insertion; and a list of attribu-
tive/cognitive verbs (e.g., think, said) used to pre-
vent segmentation of floating attributive clauses.
Future work involves studying the robustness of
our discourse segments on other corpora, such as
formal texts from the medical domain and other
informal texts. Also to be investigated is a quan-
titative study of the effects of high-precision/low-
recall vs. low-precision/high-recall segmenters on
the construction of discourse trees. Besides its use
in automatic discourse parsing, the system could
3http://www.sfu.ca/?mtaboada/research/SLSeg.html
assist manual annotators by providing a set of dis-
course segments as starting point for manual an-
notation of discourse relations.
References
Lynn Carlson and Daniel Marcu. 2001. Discourse
Tagging Reference Manual. ISI Technical Report
ISI-TR-545.
Lynn Carlson, Daniel Marcu and Mary E. Okurowski.
2002. RST Discourse Treebank. Philadelphia, PA:
Linguistic Data Consortium.
Eugene Charniak. 2000. A Maximum-Entropy In-
spired Parser. Proc. of NAACL, pp. 132?139. Seat-
tle, WA.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions, and the Structure of Discourse.
Computational Linguistics, 12:175?204.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natu-
ral Language Parsing. Advances in NIPS 15 (NIPS
2002), Cambridge, MA: MIT Press, pp. 3?10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8:243?281.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, MA.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1):103?139.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi and Bonnie Webber. 2006. Attribution and its
Annotation in the Penn Discourse TreeBank. Traite-
ment Automatique des Langues, 47(2):43?63.
Ellen Riloff and William Phillips. 2004. An Introduc-
tion to the Sundance and AutoSlog Systems. Univer-
sity of Utah Technical Report #UUCS-04-015.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. Proc. of HLT-NAACL, pp. 149?156. Ed-
monton, Canada.
Rajen Subba and Barbara Di Eugenio. 2007. Auto-
matic Discourse Segmentation Using Neural Net-
works. Proc. of the 11th Workshop on the Se-
mantics and Pragmatics of Dialogue, pp. 189?190.
Rovereto, Italy.
Huong Le Thanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Automated Discourse Segmentation
by Syntactic Information and Cue Phrases. Proc. of
IASTED. Innsbruck, Austria.
80
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Prosodic Correlates of Rhetorical Relations
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh EH8 9LW
gabriel.murray@ed.ac.uk
Maite Taboada
Dept. of Linguistics
Simon Fraser University
Vancouver V5A 1S6
mtaboada@sfu.ca
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh EH8 9LW
s.renals@ed.ac.uk
Abstract
This paper investigates the usefulness of
prosodic features in classifying rhetori-
cal relations between utterances in meet-
ing recordings. Five rhetorical relations
of contrast, elaboration, summary, ques-
tion and cause are explored. Three train-
ing methods - supervised, unsupervised,
and combined - are compared, and classi-
fication is carried out using support vector
machines. The results of this pilot study
are encouraging but mixed, with pairwise
classification achieving an average of 68%
accuracy in discerning between relation
pairs using only prosodic features, but
multi-class classification performing only
slightly better than chance.
1 Introduction
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) attempts to describe a given text
in terms of its coherence, i.e. how it is that the parts
of the text are related to one another and how each
part plays a role. Two adjacent text spans will of-
ten exhibit a nucleus-satellite relationship, where the
satellite plays a role that is relative to the nucleus.
For example, one sentence might make a claim and
the following sentence give evidence for the claim,
with the second sentence being a satellite and the
evidence relation existing between the two spans.
In a text containing many sentences, these nucleus-
satellite pairs can be built up to produce a document-
wide rhetorical tree. Figure 1 gives an example of a
rhetorical tree for a three-sentence text1.
Theories such as RST have been popular for some
time as a way of describing the multi-levelled rhetor-
ical relations that exist in text, with relevant appli-
cations such as automatic summarization (Marcu,
1997) and natural language generation (Knott and
Dale, 1996). However, implementing automatic
rhetorical parsers has been a problematic area of
research. Techniques that rely heavily on explicit
signals, such as discourse markers, are of limited
use both because only a small percentage of rhetori-
cal relations are signalled explicitly and because ex-
plicit markers can be ambiguous. RST trees are bi-
nary branching trees distinguishing between nuclei
and satellites, and automatically determining nucle-
arity is also far from trivial. Furthermore, there
are some documents which are simply not amenable
to being described by a document-wide rhetorical
tree (Mann and Thompson, 1988). Finally, some-
times more than one relation can hold between two
given units (Moore and Pollack, 1992). Given the
problems of automatically parsing text for rhetori-
cal relations, it seems prohibitively difficult to at-
tempt rhetorical parsing of speech documents - data
which are marked by disfluencies, low information
density, and sometimes little cohesion. For that rea-
son, this pilot study sets out a comparatively mod-
est task: to determine whether one of five relations
holds between two adjacent dialogue acts in meet-
ing speech. All relations are of the form nucleus-
satellite, and the five relation types are contrast,
1Contrast is in fact often realized with a multi-nuclear struc-
ture
1
 	





 ffProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62?70,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Genre-Based Paragraph Classification for Sentiment Analysis 
 
 
Maite Taboada 
Department of Linguistics 
Simon Fraser University 
Burnaby, BC, Canada 
mtaboada@sfu.ca 
Julian Brooke 
Department of Computer Science 
University of Toronto 
Toronto, ON, Canada 
jbrooke@cs.toronto.edu 
Manfred Stede 
Institute of Linguistics 
University of Potsdam 
Potsdam, Germany 
stede@ling.uni-
potsdam.de 
 
  
 
 
Abstract 
We present a taxonomy and classification 
system for distinguishing between differ-
ent types of paragraphs in movie reviews: 
formal vs. functional paragraphs and, 
within the latter, between description and 
comment. The classification is used for 
sentiment extraction, achieving im-
provement over a baseline without para-
graph classification. 
1 Introduction 
Much of the recent explosion in sentiment-
related research has focused on finding low-level 
features that will help predict the polarity of a 
phrase, sentence or text. Features, widely unders-
tood, may be individual words that tend to ex-
press sentiment, or other features that indicate 
not only sentiment, but also polarity. The two 
main approaches to sentiment extraction, the se-
mantic or lexicon-based, and the machine learn-
ing or corpus-based approach, both attempt to 
identify low-level features that convey opinion. 
In the semantic approach, the features are lists of 
words and their prior polarity, (e.g., the adjective 
terrible will have a negative polarity, and maybe 
intensity, represented as -4; the noun masterpiece 
may be a 5). Our approach is lexicon-based, but 
we make use of information derived from ma-
chine learning classifiers. 
Beyond the prior polarity of a word, its local 
context obviously plays an important role in 
conveying sentiment. Polanyi and Zaenen (2006) 
use the term ?contextual valence shifters? to refer 
to expressions in the local context that may 
change a word?s polarity, such as intensifiers, 
modal verbs, connectives, and of course negation. 
Further beyond the local context, the overall 
structure and organization of the text, influenced 
by its genre, can help the reader determine how 
the evaluation is expressed, and where it lies. 
Polanyi and Zaenen (2006) also cite genre con-
straints as relevant factors in calculating senti-
ment.  
Among the many definitions of genre, we take 
the view of Systemic Functional Linguistics that 
genres are purposeful activities that develop in 
stages, or parts (Eggins and Martin, 1997), which 
can be identified by lexicogrammatical proper-
ties (Eggins and Slade, 1997). Our proposal is 
that, once we have identified different stages in a 
text, the stages can be factored in the calculation 
of sentiment, by weighing more heavily those 
that are more likely to contain evaluation, an ap-
proach also pursued in automatic summarization 
(Seki et al, 2006). 
To test this hypothesis, we created a taxonomy 
of stages specific to the genre of movie reviews, 
and annotated a set of texts. We then trained 
various classifiers to differentiate the stages. 
Having identified the stages, we lowered the 
weight of those that contained mostly description. 
Our results show that we can achieve improve-
ment over a baseline when classifying the polar-
ity of texts, even with a classifier that can stand 
to improve (at 71.1% accuracy). The best per-
formance comes from weights derived from the 
output of a linear regression classifier. 
We first describe our inventory of stages and 
the manual annotation (Section 2), and in Sec-
tion 3 turn to automatic stage classification. After 
describing our approach to sentiment classifica-
tion of texts in Section 4, we describe experi-
ments to improve its performance with the in-
formation on stages in Section 5. Section 6 dis-
62
cusses related work, and Section 7 provides con-
clusions.  
2 Stages in movie reviews 
Within the larger review genre, we focus on 
movie reviews. Movie reviews are particularly 
difficult to classify (Turney, 2002), because large 
portions of the review contain description of the 
plot, the characters, actors, director, etc., or 
background information about the film. 
Our approach is based on the work of Bieler et 
al. (2007), who identify formal and functional 
zones (stages) within German movie reviews. 
Formal zones are parts of the text that contribute 
factual information about the cast and the credits, 
and also about the review itself (author, date of 
publication and the reviewer?s rating of the mov-
ie). Functional zones contain the main gist of the 
review, and can be divided roughly into descrip-
tion and comment. Bieler et al showed that func-
tional zones could be identified using 5-gram 
SVM classifiers built from an annotated German 
corpus.  
2.1 Taxonomy 
In addition to the basic Describe/Comment dis-
tinction in Bieler et al, we use a De-
scribe+Comment label, as in our data it is often 
the case that both description and comment are 
present in the same paragraph. We decided that a 
paragraph could be labeled as De-
scribe+Comment when it contained at least a 
clause of each, and when the comment part could 
be assigned a polarity (i.e., it was not only sub-
jective, but also clearly positive or negative).  
Each of the three high-level tags has a subtag, 
a feature also present in Bieler et al?s manual 
annotation. The five subtags are: overall, plot, 
actors/characters, specific and general. ?Specific? 
refers to one particular aspect of the movie (not 
plot or characters), whereas ?general? refers to 
multiple topics in the same stage (special effects 
and cinematography at the same time). Outside 
the Comment/Describe scale, we also include 
tags such as Background (discussion of other 
movies or events outside the movie being 
reviewed), Interpretation (subjective but not 
opinionated or polar), and Quotes. Altogether, 
the annotation system includes 40 tags, with 22 
formal and 18 functional zones. Full lists of 
zone/stage labels are provided in Appendix A. 
2.2 Manual annotation 
We collected 100 texts from rottentomatoes.com, 
trying to include one positive and one negative 
review for the same movie. The reviews are part 
of the ?Top Critics? section of the site, all of 
them published in newspapers or on-line maga-
zines. We restricted the texts to ?Top Critics? 
because we wanted well-structured, polished 
texts, unlike those found in some on-line review 
sites. Future work will address those more in-
formal reviews. 
The 100 reviews contain 83,275 words and 
1,542 paragraphs. The annotation was performed 
at the paragraph level. Although stages may span 
across paragraphs, and paragraphs may contain 
more than one stage, there is a close relationship 
between paragraphs and stages. The restriction 
also resulted in a more reliable annotation, per-
formed with the PALinkA annotation tool (Ora-
san, 2003). 
The annotation was performed by one of the 
authors, and we carried out reliability tests with 
two other annotators, one another one of the au-
thors, who helped develop the taxonomy, and the 
third one a project member who read the annota-
tion guidelines1, and received a few hours? train-
ing in the labels and software. We used Fleiss? 
kappa (Fleiss, 1971), which extends easily to the 
case of multiple raters (Di Eugenio and Glass, 
2004). We all annotated four texts. The results of 
the reliability tests show a reasonable agreement 
level for the distinction between formal and 
functional zones (.84 for the 3-rater kappa). The 
lowest reliability was for the 3-way distinction in 
the functional zones (.68 for the first two raters, 
and .54 for the three raters). The full kappa val-
ues for all the distinctions are provided in Ap-
pendix B. After the reliability test, one of the 
authors performed the full annotation for all 100 
texts. Table 1 shows the breakdown of high-level 
stages for the 100 texts.  
 
Stage Count 
Describe 347 
Comment 237 
Describe+Comment 237 
Background 51 
Interpretation 22 
Quote 2 
Formal 646 
Table 1. Stages in 100 text RT corpus 
                                                 
1Available from http://www.sfu.ca/~mtaboada/nserc-
project.html 
63
3 Classifying stages 
Our first classification task aims at distinguishing 
the two main types of functional zones, Com-
ment and Describe, vs. Formal zones.  
3.1 Features 
We test two different sets of features. The first, 
following Bieler et al (2007), consists of 5-
grams (including unigrams, bigrams, 3-grams 
and 4-grams), although we note in our case that 
there was essentially no performance benefit 
beyond 3-grams. We limited the size of our fea-
ture set to n-grams that appeared at least 4 times 
in our training corpus. For the 2 class task (no 
formal zones), this resulted in 8,092 binary fea-
tures, and for the 3 and 4 class task there were 
9,357 binary n-gram features. 
The second set of features captures different 
aspects of genre and evaluation, and can in turn 
be divided into four different types, according to 
source. With two exceptions (features indicating 
whether a paragraph was the first or last para-
graph in text), the features were numerical (fre-
quency) and normalized to the length of the pa-
ragraph. 
The first group of genre features comes from 
Biber (1988), who attempted to characterize di-
mensions of genre. The features here include fre-
quency of first, second and third person pro-
nouns; demonstrative pronouns; place and time 
adverbials; intensifiers; and modals, among a 
number of others. 
The second category of genre features in-
cludes discourse markers, primarily from Knott 
(1996), that indicate contrast, comparison, causa-
tion, evidence, condition, and similar relations. 
The third type of genre features was a list of 
500 adjectives classified in terms of Appraisal 
(Martin and White, 2005) as indicating Apprec-
iation, Judgment or Affect. Appraisal categories 
have been shown to be useful in improving the 
performance of polarity classifiers (Whitelaw et 
al., 2005).  
Finally, we also include text statistics as fea-
tures, such as average length of words and sen-
tences and position of paragraphs in the text.  
3.2 Classifiers 
To classify paragraphs in the text, we use the 
WEKA suite (Witten and Frank, 2005), testing 
three popular machine learning algorithms: 
Na?ve Bayes, Support Vector Machine, and Li-
near Regression (preliminary testing with Deci-
sion Trees suggests that it is not appropriate for 
this task). Training parameters were set to default 
values. 
In order to use Linear Regression, which pro-
vides a numerical output based on feature values 
and derived feature weights, we have to conceive 
of Comment/Describe/Describe+Comment not as 
nominal (or ordinal) classes, but rather as corres-
ponding to a Comment/Describe ratio, with 
?pure? Describe at one end and ?pure? Comment 
at the other. For training, we assign a 0 value (a 
Comment ratio) to all paragraphs tagged De-
scribe and a 1 to all Comment paragraphs; for 
Describe+Comment, various options (including 
omission of this data) were tested. The time re-
quired to train a linear regression classifier on a 
large feature set proved to be prohibitive, and 
performance with smaller sets of features gener-
ally quite poor, so for the linear regression clas-
sifier we present results only for our compact set 
of genre features. 
3.3 Performance 
Table 2 shows the performance of classifi-
er/feature-set combinations for the 2-, 3-, and 4-
class tasks on the 100-text training set, with 10-
fold cross-validation, in terms of precision (P), 
recall (R) and F-measure 2 . SVM and Na?ve 
Bayes provide comparable performance, al-
though there is considerable variation, particular-
ly with respect to the feature set; the SVM is a 
significantly (p<0.05) better choice for our genre 
features 3 , while for the n-gram features the 
Bayes classification is generally preferred. The 
SVM-genre classifier significantly outperforms 
the other classifiers in the 2-class task; these ge-
nre features, however, are not as useful as 5-
grams at identifying Formal zones (the n-gram 
classifier, by contrast, can make use of words 
such as cast). In general, formal zone classifica-
tion is fairly straightforward, whereas identifica-
tion of Describe+Comment is quite difficult, and 
the SVM-genre classifier, which is more sensi-
tive to frequency bias, elects to (essentially) ig-
nore this category in order to boost overall accu-
racy.  
To evaluate a linear regression (LR) classifier, 
we calculate correlation coefficient ?, which re-
flects the goodness of fit of the line to the da-
ta. Table 3 shows values for the classifiers built 
from the corpus, with various Comment ratios
                                                 
2 For the 2- and 3-way classifiers, Describe+Comment pa-
ragraphs are treated as Comment. This balances the num-
bers of each class, ultimately improving performance. 
3 All significance tests use chi-square (?2). 
64
Classifier 
Comment Describe Formal Desc+Comm Overall 
Accuracy P R F P R F P R F P R F 
2-class-5-gram-Bayes .66 .79 .72 .70 .55 .62 - - - - - - 68.0 
2-class-5-gram-SVM .53 .63 .64 .68 .69 .69 - - - - - - 66.8 
2-class-genre-Bayes .66 .75 .70 .67 .57 .61 - - - - - - 66.2 
2-class-genre-SVM .71 .76 .74 .71 .65 .68 - - - - - - 71.1 
3-class-5-gram-Bayes .69 .49 .57 .66 .78 .71 .92 .97 .95 - - - 78.1 
3-class-5-gram-SVM .64 .63 .63 .68 .65 .65 .91 .97 .94 - - - 77.2 
3-class-genre-Bayes .68 .68 .66 .67 .46 .55 .84 .96 .90 - - - 74.0 
3-class-genre-SVM .66 .71 .68 .67 .56 .61 .90 .94 .92 - - - 76.8 
4-class-5-gram-Bayes .46 .35 .38 .69 .47 .56 .92 .97 .95 .42 .64 .51 69.0 
4-class-5-gram-SVM .43 .41 .44 .59 .62 .60 .91 .97 .94 .45 .41 .42 69.6 
4-class-genre-Bayes .38 .31 .34 .66 .30 .41 .86 .97 .90 .33 .60 .42 62.3 
4-class-genre-SVM .46 .32 .38 .53 .82 .65 .87 .94 .90 .26 .03 .06 67.4 
Table 2. Stage identification performance of various categorical classifiers 
 
(C) assigned to paragraphs with the De-
scribe+Comment tag, and with De-
scribe+Comment paragraphs removed from con-
sideration. 
 
Classifier ? 
LR, Des+Com C = 0 .37 
LR, Des+Com C = 0.25 .44 
LR, Des+Com C = 0.5 .47 
LR, Des+Com C = 0.75 .46 
LR, Des+Com C = 1 .43 
LR, No Des+Com .50 
Table 3. Correlation coefficients for LR 
classifiers 
The drop in correlation when more extreme 
values are assigned to Describe+Comment sug-
gests that Describe+Comment paragraphs do in-
deed belong in the middle of the Comment spec-
trum. Since there is a good deal of variation in 
the amount of comment across De-
scribe+Comment paragraphs, the best correlation 
comes with complete removal of these somewhat 
unreliable paragraphs. Overall, these numbers 
indicate that variations in relevant features are 
able to predict roughly 50% of the variation in 
Comment ratio, which is fairly good considering 
the small number and simplistic nature of the 
features involved. 
4 Sentiment detection: SO-CAL 
In this section, we outline our semantic orienta-
tion calculator, SO-CAL. SO-CAL extracts 
words from a text, and aggregates their semantic 
orientation value, which is in turn extracted from 
a set of dictionaries. SO-CAL uses five dictionar-
ies: four lexical dictionaries with 2,257 adjec-
tives, 1,142 nouns, 903 verbs, and 745 adverbs, 
and a fifth dictionary containing 177 intensifying 
expressions. Although the majority of the entries 
are single words, the calculator also allows for 
multiword entries written in regular expression-
like language.  
The SO-carrying words in these dictionaries 
were taken from a variety of sources, the three 
largest a corpus of 400 reviews from Epin-
ions.com, first used by Taboada and Grieve 
(2004), a 100 text subset of the 2,000 movie re-
views in the Polarity Dataset (Pang and Lee, 
2004), and words from the General Inquirer dic-
tionary (Stone, 1997). Each of the open-class 
words were given a hand-ranked SO value be-
tween 5 and -5 (neutral or zero-value words are 
not included in the dictionary) by a native Eng-
lish speaker. The numerical values were chosen 
to reflect both the prior polarity and strength of 
the word, averaged across likely interpretations. 
For example, the word phenomenal is a 5, nicely 
a 2, disgust a -3, and monstrosity a -5. The dic-
tionary was later reviewed by a committee of 
three other researchers in order to minimize the 
subjectivity of ranking SO by hand. 
Our calculator moves beyond simple averag-
ing of each word?s semantic orientation value, 
and implements and expands on the insights of 
Polanyi and Zaenen (2006) with respect to con-
textual valence shifters. We implement negation 
by shifting the SO value of a word towards the 
opposite polarity (not terrible, for instance, is 
calculated as -5+4 = -1). Intensification is mod-
eled using percentage modifiers (very engaging: 
4x125% = 5). We also ignore words appearing 
within the scope of irrealis markers such as cer-
tain verbs, modals, and punctuation, and de-
crease the weight of words which appear often in 
the text. In order to counter positive linguistic 
65
bias (Boucher and Osgood, 1969), a problem for 
lexicon-based sentiment classifiers (Kennedy and 
Inkpen, 2006), we increase the final SO of any 
negative expression appearing in the text. 
The performance of SO-CAL tends to be in 
the 76-81% range. We have tested on informal 
movie, book and product reviews and on the Po-
larity Dataset (Pang and Lee, 2004). The perfor-
mance on movie reviews tends to be on the lower 
end of the scale. Our baseline for movies, de-
scribed in Section 5, is 77.7%. We believe that 
we have reached a ceiling in terms of word- and 
phrase-level performance, and most future im-
provements need to come from discourse fea-
tures. The stage classification described in this 
paper is one of them.  
5 Results 
The final goal of a stage classifier is to use the 
information about different stages in sentiment 
classification. Our assumption is that descriptive 
paragraphs contain less evaluative content about 
the movie being reviewed, and they may include 
noise, such as evaluative words describing the 
plot or the characters. Once the paragraph clas-
sifier had assigned labels we used those labels to 
weigh paragraphs. 
5.1 Classification with manual tags 
Before moving on to automatic paragraph classi-
fication, we used the 100 annotated texts to see 
the general effect of weighting paragraphs with 
the ?perfect? human annotated tags on sentiment 
detection, in order to show the potential im-
provements that can be gained from this ap-
proach.  
Our baseline polarity detection performance 
on the 100 annotated texts is 65%, which is very 
low, even for movie reviews. We posit that for-
mal movie reviews might be particularly difficult 
because full plot descriptions are more common 
and the language used to express opinion less 
straightforward (metaphors are common). How-
ever, if we lower the weight on non-Comment 
and mixed Comment paragraphs (to 0, except for 
Describe+Comment, which is maximized by a 
0.1 weight), we are able to boost performance to 
77%, an improvement which is significant at the 
p<0.05 level. Most of the improvement (7%) is 
due to disregarding Describe paragraphs, but 2% 
comes from Describe+Comment, and 1% each 
from Background, Interpretation, and (all) For-
mal tags. There is no performance gain, however, 
from the use of aspect tags (e.g., by increasing 
the weight on Overall paragraphs), justifying our 
decision to ignore subtags for text-level polarity 
classification.  
5.2 Categorical classification 
We evaluated all the classifiers from Table 2, but 
we omit discussion of the worst performing. The 
evaluation was performed on the Polarity Dataset 
(Pang and Lee, 2004), a collection of 2,000 on-
line movie reviews, balanced for polarity. The 
SO performance for the categorical classifiers is 
given in Figure 1. When applicable, we always 
gave Formal Zones (which Table 2 indicates are 
fairly easy to identify) a weight of 0, however for 
Describe paragraphs we tested at 0.1 intervals 
between 0 and 1. Testing all possible values of 
Describe+Comment was not feasible, so we set 
the weights of those to a value halfway between 
the weight of Comment paragraphs (1) and the 
weight of the Describe paragraph. 
Most of the classifiers were able to improve 
performance beyond the 77.7% (unweighted) 
baseline. The best performing model (the 2-
class-genre-SVM) reached a polarity identifica-
tion accuracy of 79.05%, while the second best 
(the 3-class 5-gram-SVM) topped out at 78.9%. 
Many of the classifiers showed a similar pattern 
with respect to the weight on Describe, increas-
ing linearly as weight on Describe was decreased 
before hitting a maximum in the 0.4-0.1 range, 
and then dropping afterwards (often precipitous-
ly). Only the classifiers which were more con-
servative with respect to Describe, such as the 4-
class-5-gram-Bayes, avoided the drop, which can 
be attributed to low precision Describe identifi-
cation: At some point, the cost associated with 
disregarding paragraphs which have been mis-
tagged as Describe becomes greater that the ben-
efit of disregarding correctly-labeled ones. In-
deed, the best performing classifier for each class 
option is exactly the one that has the highest pre-
cision for identification of Describe, regardless 
of other factors. This suggests that improving 
precision is key, and, in lieu of that, weighting is 
a better strategy than simply removing parts of 
the text. 
In general, increasing the complexity of the 
task (increasing the number of classes) decreases 
performance. One clear problem is that the iden-
tification of Formal zones, which are much more 
common in our training corpus than our test cor-
pus, does not add important information, since 
most Formal zones have no SO valued words. 
The delineation of an independent De-
scribe+Comment class is mostly ineffective, 
66
 
Figure 1. SO Performance with various paragraph tagging classifiers, by weight on Describe 
 
probably because this class is not easily distin-
guishable from Describe and Comment (nor in 
fact should it be). 
We can further confirm that our classifier is 
properly distinguishing Describe and Comment 
by discounting Comment paragraphs rather than 
Describe paragraphs (following Pang and Lee 
2004). When Comment paragraphs tagged by the 
best performing classifier are ignored, SO-CAL?s 
accuracy drops to 56.65%, just barely above 
chance. 
5.3 Continuous classification 
Table 4 gives the results for the linear regression 
classifier, which assigns a Comment ratio to each 
paragraph used for weighting.  
 
Model Accuracy 
LR, Des+Com C = 0 78.75 
LR, Des+Com C = 0.25 79.35 
LR, Des+Com C = 0.5 79.00 
LR, Des+Com C = 0.75 78.90 
LR, Des+Com C = 1 78.95 
LR, No Des+Com 79.05 
Table 4. SO Performance with linear regression 
 
The linear regression model trained with a 
0.25 comment ratio on Describe+Comment para-
graphs provides the best performance of all clas-
sifiers we tested (an improvement of 1.65% from 
baseline). The correlation coefficients noted 
in Table 4 are reflected in these results, but the 
spike at C = 0.25 is most likely related to a gen-
eral preference for low (but non-zero) weights on 
Describe+Comment paragraphs also noted when 
weights were applied using the manual tags; 
these paragraphs are unreliable (as compared to 
pure Comment), but cannot be completely dis-
counted. There were some texts which had only 
Describe+Comment paragraphs.  
Almost a third of the tags assigned by the 2-
class genre feature classifier were different than 
the corresponding n-gram classifier, suggesting 
the two classifiers might have different strengths. 
However, initial attempts to integrate the various 
high performing classifiers?including collaps-
ing of feature sets, metaclassifiers, and double 
tagging of paragraphs?resulted in similar or 
worse performance. We have not tested all poss-
ible options (there are simply too many), but we 
think it unlikely that additional gains will be 
made with these simple, surface feature sets. Al-
though our testing with human annotated texts 
and the large performance gap between movie 
reviews and other consumer reviews both sug-
gest there is more potential for improvement, it 
will probably require more sophisticated and 
precise models. 
6 Related work 
The bulk of the work in sentiment analysis has 
focused on classification at either the sentence 
level, e.g., the subjectivity/polarity detection of 
Wiebe and Riloff (2005), or alternatively at the 
level of the entire text. With regards to the latter, 
two major approaches have emerged: the use of 
machine learning classifiers trained on n-grams 
77
78
79
80
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
SO
?C
al
cu
la
to
r?
A
cc
ur
ac
y
Weight?on?Describe?Paragraph
No?tagging?Baseline
2?Class?5?gram?SVM
2?Class?5?gram?Bayes
2?Class?genre?Bayes
2?Class?genre?SVM
3?Class?5?gram?Bayes
3?Class?5?gram?SVM
3?Class?genre?Bayes
4?Class?5?gram?Bayes
4?Class?5?gram?SVM
4?Class?genre?Bayes
67
or similar features (Pang et al, 2002), and the 
use of sentiment dictionaries (Esuli and Sebas-
tiani, 2006; Taboada et al, 2006). Support Vec-
tor Machine (SVM) classifiers have been shown 
to out-perform lexicon-based models within a 
single domain (Kennedy and Inkpen, 2006); 
however they have trouble with cross-domain 
tasks (Aue and Gamon, 2005), and some re-
searchers have argued for hybrid classifiers (An-
dreevskaia and Bergler, 2008). 
Pang and Lee (2004) attempted to improve the 
performance of an SVM classifier by identifying 
and removing objective sentences from the texts. 
Results were mixed: The improvement was mi-
nimal for the SVM classifier (though the perfor-
mance of a na?ve Bayes classifier was signifi-
cantly boosted), however testing with parts of the 
text classified as subjective showed that the elim-
inated parts were indeed irrelevant. In contrast to 
our findings, they reported a drop in performance 
when paragraphs were taken as the only possible 
boundary between subjective and objective text 
spans. 
Other research that has dealt with identifying 
more or less relevant parts of the text for the pur-
poses of sentiment analysis include Taboada and 
Grieve (2004), who improved the performance of 
a lexicon-based model by weighing words to-
wards the end of the text; Nigam and Hurst 
(2006), who detect polar expressions in topic 
sentences; and Voll and Taboada (2007), who 
used a topic classifier and discourse parser to 
eliminate potentially off-topic or less important 
sentences. 
7 Conclusions 
We have described a genre-based taxonomy for 
classifying paragraphs in movie reviews, with 
the main classification being a distinction be-
tween formal and functional stages, and, within 
those, between mainly descriptive vs. comment 
stages. The taxonomy was used to annotate 100 
movie reviews, as the basis for building classifi-
ers.  
We tested a number of different classifiers. 
Our results suggest that a simple, two-way or 
continuous classification using a small set of lin-
guistically-motivated features is the best for our 
purposes; a more complex system is feasible, but 
comes at the cost of precision, which seems to be 
the key variable in improving sentiment analysis. 
Ultimately, the goal of the classification was 
to improve the accuracy of SO-CAL, our seman-
tic orientation calculator. Using the manual an-
notations, we manage to boost performance by 
12% over the baseline. With the best automatic 
classifier, we still show consistent improvement 
over the baseline. Given the relatively low accu-
racy of the classifiers, the crucial factor involves 
using fine-grained weights on paragraphs, rather 
than simply ignoring Describe-labeled para-
graphs, as Pang and Lee (2004) did for objective 
sentences.  
An obvious expansion to this work would in-
volve a larger dataset on which to train, to im-
prove the performance of the classifier(s). We 
would also like to focus on the syntactic patterns 
and verb class properties of narration, aspects 
that are not captured with simply using words 
and POS labels. Connectives in particular are 
good indicators of the difference between narra-
tion (temporal connectives) and opinion (contras-
tive connectives). There may also be benefit to 
combining paragraph- and sentence-based ap-
proaches. Finally, we would like to identify 
common sequences of stages, such as plot and 
character descriptions appearing together, and 
before evaluation stages. This generic structure 
has been extensively studied for many genres 
(Eggins and Slade, 1997). 
Beyond sentiment extraction, our taxonomy 
and classifiers can be used for searching and in-
formation retrieval. One could, for instance, ex-
tract paragraphs that include mostly comment or 
description. Using the more fine-grained labels, 
searches for comment/description on actors, di-
rectors, or other aspects of the movie are possible. 
Acknowledgements 
This work was supported by SSHRC (410-2006-
1009) and NSERC (261104-2008) grants to 
Maite Taboada. 
References 
Andreevskaia, Alina & Sabine Bergler. 2008. When 
specialists and generalists work together: Domain 
dependence in sentiment tagging. Proceedings of 
46th Annual Meeting of the Association for Com-
putational Linguistics (pp. 290-298). Columbus, 
OH. 
Aue, Anthony & Michael Gamon. 2005. Customizing 
sentiment classifiers to new domains: A case study. 
Proceedings of the International Conference on 
Recent Advances in Natural Language Processing. 
Borovets, Bulgaria. 
Biber, Douglas. 1988. Variation across Speech and 
Writing. Cambridge: Cambridge University Press. 
68
Bieler, Heike, Stefanie Dipper & Manfred Stede. 
2007. Identifying formal and functional zones in 
film reviews. Proceedings of the 8th SIGdial 
Workshop on Discourse and Dialogue (pp. 75-78). 
Antwerp, Belgium. 
Boucher, Jerry D. & Charles E. Osgood. 1969. The 
Pollyanna hypothesis. Journal of Verbal Learning 
and Verbal Behaviour, 8: 1-8. 
Di Eugenio, Barbara & Michael Glass. 2004. The 
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101. 
Eggins, Suzanne & James R. Martin. 1997. Genres 
and registers of discourse. In Teun A. van Dijk 
(ed.), Discourse as Structure and Process. Dis-
course Studies: A Multidisciplinary Introduction 
(pp. 230-256). London: Sage. 
Eggins, Suzanne & Diana Slade. 1997. Analysing 
Casual Conversation. London: Cassell. 
Esuli, Andrea & Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for 
opinion mining. Proceedings of 5th International 
Conference on Language Resources and Evaluation 
(LREC) (pp. 417-422). Genoa, Italy. 
Fleiss, Joseph L. 1971. Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76: 378-382. 
Kennedy, Alistair & Diana Inkpen. 2006. Sentiment 
classification of movie and product reviews using 
contextual valence shifters. Computational Intelli-
gence, 22(2): 110-125. 
Knott, Alistair. 1996. A Data-Driven Methodology for 
Motivating a Set of Coherence Relations. Edin-
burgh, UK: University of EdinburghThesis Type. 
Martin, James R. & Peter White. 2005. The Language 
of Evaluation. New York: Palgrave. 
Nigam, Kamal & Matthew Hurst. 2006. Towards a 
robust metric of polarity. In Janyce Wiebe (ed.), 
Computing Attitude and Affect in Text: Theory 
and Applications (pp. 265-279). Dordrecht: Sprin-
ger. 
Orasan, Constantin. 2003. PALinkA: A highly custo-
mizable tool for discourse annotation. Proceedings 
of 4th SIGdial Workshop on Discourse and Dialog 
(pp. 39 ? 43). Sapporo, Japan. 
Pang, Bo & Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. Proceedings of 
42nd Meeting of the Association for Computation-
al Linguistics (pp. 271-278). Barcelona, Spain. 
Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
Machine Learning techniques. Proceedings of Con-
ference on Empirical Methods in NLP (pp. 79-86). 
Polanyi, Livia & Annie Zaenen. 2006. Contextual 
valence shifters. In James G. Shanahan, Yan Qu & 
Janyce Wiebe (eds.), Computing Attitude and Af-
fect in Text: Theory and Applications (pp. 1-10). 
Dordrecht: Springer. 
Seki, Yohei, Koji Eguchi & Noriko Kando. 2006. 
Multi-document viewpoint summarization focused 
on facts, opinion and knowledge. In Janyce Wiebe 
(ed.), Computing Attitude and Affect in Text: 
Theory and Applications (pp. 317-336). Dordrecht: 
Springer. 
Stone, Philip J. 1997. Thematic text analysis: New 
agendas for analyzing text content. In Carl Roberts 
(ed.), Text Analysis for the Social Sciences. Mah-
wah, NJ: Lawrence Erlbaum. 
Taboada, Maite, Caroline Anthony & Kimberly Voll. 
2006. Creating semantic orientation dictionaries. 
Proceedings of 5th International Conference on 
Language Resources and Evaluation (LREC) (pp. 
427-432). Genoa, Italy. 
Taboada, Maite & Jack Grieve. 2004. Analyzing ap-
praisal automatically. Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text (AAAI Technical Report SS-04-07) (pp. 158-
161). Stanford University, CA. 
Turney, Peter. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews. Proceedings of 40th Meeting 
of the Association for Computational Linguistics 
(pp. 417-424). 
Voll, Kimberly & Maite Taboada. 2007. Not all 
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. Pro-
ceedings of the 20th Australian Joint Conference 
on Artificial Intelligence (pp. 337-346). Gold 
Coast, Australia. 
Whitelaw, Casey, Navendu Garg & Shlomo Arga-
mon. 2005. Using Appraisal groups for sentiment 
analysis. Proceedings of ACM SIGIR Conference 
on Information and Knowledge Management 
(CIKM 2005) (pp. 625-631). Bremen, Germany. 
Wiebe, Janyce & Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unan-
notated texts. Proceedings of Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics (CICLing-2005). Mex-
ico City, Mexico. 
Witten, Ian H. & Eibe Frank. 2005. Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd edn.). San Francisco: Morgan Kaufmann. 
 
69
Appendix A: Full lists of formal and functional zones 
 
 
Figure A1. Functional zones 
 
 
Figure A2. Formal zones 
 
Describe
Comment
Plot
Character
Specific
General
Content
Plot
Actors+characters
Specific
General
Overall
Plot
Actors+characters
Specific
General
Content
Structural
elements
Information
about the
film
Tagline
Structure
Off-topic
Title, Title+year, Runtime,
Country+year, Director,
Genre, Audience-restriction,
Cast, Credits, Show-Loc+date,
Misc-Movie-Info
Source, Author, Author-Bio,
Place, Date, Legal-Notice,
Misc-Review-Info, Rating
 
 
Appendix B: Kappa values for annotation task 
 
Classes 2-rater 
kappa 
3-rater 
kappa 
Describe/Comment/Describe+Comment/Formal .82 .73 
Describe/Comment/Formal .92 .84 
Describe/Comment/Describe+Comment .68 .54 
Describe/Comment .84 .69 
Table B1. Kappa values for stage annotations 
 
 
70
Proceedings of the SIGDIAL 2013 Conference, pages 92?96,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
On the contribution of discourse structure to topic segmentation 
 
Paula C. F. Cardoso1, Maite Taboada2, Thiago A. S. Pardo1 
1N?cleo Interinstitucional de Lingu?stica Computacional (NILC) 
Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
Av. Trabalhador S?o-carlense, 400 - Centro 
Caixa Postal: 668 ? CEP: 13566-970 ? S?o Carlos/SP 
2Department of Linguistics ? Simon Fraser University  
8888 University Dr., Burnaby, B.C., V5A 1S6 - Canada 
pcardoso@icmc.usp.br, mtaboada@sfu.ca, taspardo@icmc.usp.br 
 
 
Abstract 
In this paper, we describe novel methods for 
topic segmentation based on patterns of dis-
course organization. Using a corpus of news 
texts, our results show that it is possible to use 
discourse features (based on Rhetorical Struc-
ture Theory) for topic segmentation and that 
we outperform some well-known methods. 
1 Introduction 
Topic segmentation aims at finding the bounda-
ries among topic blocks in a text (Chang and 
Lee, 2003). This task is useful for a number of 
important applications such as information re-
trieval (Prince and Labadi?, 2007), automatic 
summarization (Wan, 2008) and question-
answering systems (Oh et al, 2007). 
In this paper, following Hearst (1997), we as-
sume that a text or a set of texts develop a main 
topic, exposing several subtopics as well. We 
also assume that a topic is a particular subject 
that we write about or discuss (Hovy, 2009), and 
subtopics are represented in pieces of text that 
cover different aspects of the main topic (Hearst, 
1997; Hennig, 2009). Therefore, the task of topic 
segmentation aims at dividing a text into topical-
ly coherent segments, or subtopics. The granular-
ity of a subtopic is not defined, as a subtopic may 
contain one or more sentences or paragraphs. 
Several methods have been tested for topic 
segmentation. There are, however, no studies on 
how discourse structure directly mirrors topic 
boundaries in texts and how they may contribute 
to such task, although such possible correlation 
has been suggested (e.g., Hovy and Lin, 1998). 
In this paper, we follow this research line, 
aiming at exploring the relationship of discourse 
and subtopics. In particular, our interest is main-
ly on the potential of Rhetorical Structure Theory 
(RST) (Mann and Thompson, 1987) for this task. 
We propose and evaluate automatic topic seg-
mentation strategies based on the rhetorical 
structure of a text. We also compare our results 
to some well-known algorithms in the area, 
showing that we outperform these algorithms. 
Our experiments were performed using a corpus 
of news texts manually annotated with RST and 
subtopics. 
The remainder of this paper is organized as 
follows. Section 2 gives a brief background on 
text segmentation. Section 3 describes our auto-
matic strategies to find the subtopics. The corpus 
that we use is described in Section 4. Section 5 
presents some results and Section 6 contains the 
conclusions and future work. 
2 Related work 
Several approaches have tried to measure the 
similarity across sentences and to estimate where 
topic boundaries occur. One well-known ap-
proach, that is heavily used for topic segmenta-
tion, is TextTiling (Hearst, 1997), which is based 
on lexical cohesion. For this strategy, it is as-
sumed that a set of lexical items is used during 
the development of a subtopic in a text and, 
when that subtopic changes, a significant propor-
tion of vocabulary also changes.  
Passoneau and Litman (1997), in turn, have 
combined multiple linguistic features for topic 
segmentation of spoken text, such as pause, cue 
words, and referential noun phrases. Hovy and 
Lin (1998) have used various complementary 
92
techniques for topic segmentation, including 
those based on text structure, cue words and 
high-frequency indicative phrases for topic iden-
tification in a summarization system. Although 
the authors do not mention an evaluation of these 
features, they suggested that discourse structure 
might help topic identification. For this, they 
suggested using RST.  
RST represents relations among propositions 
in a text and discriminates nuclear and satellite 
information. In order to present the differences 
among relations, they are organized in two 
groups: subject matter and presentational rela-
tions. In the former, the text producer intends 
that the reader recognizes the relation itself and 
the information conveyed, while in the latter the 
intended effect is to increase some inclination on 
the part of the reader (Taboada and Mann, 2006). 
The relationships are traditionally structured in a 
tree-like form (where larger units ? composed of 
more than one proposition ? are also related in 
the higher levels of the tree).  
To the best of our knowledge, we have not 
found any proposal that has directly employed 
RST for topic segmentation purposes. Following 
the suggestion of the above authors, we investi-
gated how discourse structure mirrors topic shifts 
in texts. Next section describes our approach to 
the problem. 
3 Strategies for topic segmentation  
For identifying and partitioning the subtopics of 
a text, we developed four baseline algorithms 
and six other algorithms that are based on dis-
course features. 
The four baseline algorithms segment at para-
graphs, sentences, random boundaries (randomly 
selecting any number of boundaries and where 
they are in a text) or are based on word reitera-
tion. The word reiteration strategy is an adapta-
tion of TextTiling1 (Hearst, 1997) for the charac-
teristics of the corpus that we used (introduced 
latter in this paper). 
The algorithms based on discourse consider 
the discourse structure itself and the RST rela-
tions in the discourse tree. The first algorithm 
(which we refer to as Simple Cosine) is based on 
Marcu?s idea (2000) for measuring the ?good-
ness? of a discourse tree. He assumes that a dis-
course tree is ?better? if it exhibits a high-level 
structure that matches as much as possible the 
                                                 
1  We have specifically used the block comparison 
method with block size=2. 
topic boundaries of the text for which that struc-
ture was built. Marcu associates a clustering 
score to each node of a tree. For the leaves, this 
score is 0; for the internal nodes, the score is giv-
en by the lexical similarity between the immedi-
ate children. The hypothesis underlying such 
measurements is that better trees show higher 
similarity among their nodes. We have adopted 
the same idea using the cosine measure. We have 
proposed that text segments with similar vocabu-
lary are likely to be part of the same topic seg-
ment. In our case, nodes with scores below the 
average score are supposed to indicate possible 
topic boundaries. 
The second algorithm (referred to as Cosine 
Nuclei) is also a proposal by Marcu (2000). It is 
assumed that whenever a discourse relation holds 
between two textual spans, that relation also 
holds between the most salient units (nuclei) as-
sociated with those spans. We have used this 
formalization and measured the similarity be-
tween the salient units associated with two spans 
(instead of measuring among all the text spans of 
the relation, as in the previous algorithm).  
The third (Cosine Depth) and fourth (Nuclei 
Depth) algorithms are variations of Simple Co-
sine and Cosine Nuclei. For these new strategies, 
the similarity for each node is divided by the 
depth where it occurs, traversing the tree in a 
bottom-up way. These should guarantee that 
higher nodes are weaker and might better repre-
sent topic boundaries. Therefore, we have the 
assumption that topic boundaries are more likely 
to be mirrored at the higher levels of the dis-
course structure. We also have used the average 
score to find out less similar nodes. Figure 1 
shows a sample RST tree. The symbols N and S 
indicate the nucleus and satellite of each rhetori-
cal relation. For this tree, the score between 
nodes 3 and 4 is divided by 1 (since we are at the 
leaf level); the score between Elaboration and 
node 5 is divided by 2 (since we are in a higher 
level, 1 above the leaves on the left); and the 
score between Sequence and Volitional-result is 
divided by 3 (1 above the leaves on the right). 
 
 
Figure 1. Example of an RST structure 
93
The next algorithms are based on the idea that 
some relations are more likely to represent topic 
shifts. For estimating this, we have used the 
CSTNews (described in next section), which is 
manually annotated with subtopics and RST. 
In this corpus, there are 29 different types of 
RST relations that may connect textual spans. In 
an attempt to characterize topic segmentation 
based on rhetorical relations, we recorded the 
frequency of those relations in topic boundaries. 
We realized that some relations were more fre-
quent on topic boundaries, whereas others never 
occurred at the boundaries of topics. Out of the 
29 relations, 16 appeared in the reference annota-
tion. In topic boundaries, Elaboration was the 
most frequent relation (appearing in 60% of the 
boundaries), followed by List (20%) and Non-
Volitional Result (5%). Sequence and Evidence 
appeared in 2% of the topic boundaries, and 
Background, Circumstance, Comparison, Con-
cession, Contrast, Explanation, Interpretation, 
Justify, and Non-Volitional Cause in 1% of the 
boundaries. 
We used this knowledge about the relations? 
frequency and attributed a weight associated with 
the possibility that a relation indicates a bounda-
ry, in accordance with its frequency on topic 
boundaries in the reference corpus. Figure 2 
shows how the 29 relations were distributed. One 
relation is weak if it usually indicates a bounda-
ry; in this case, its weight is 0.4. One relation is 
medium because it may indicate a boundary or 
not; therefore, its weight is 0.6. On the other 
hand, a strong relation almost never indicates a 
topic boundary; therefore, its weight is 0.8. Such 
values were empirically determined. Another 
factor that may be observed is that all presenta-
tional relations are classified as strong, with the 
exception of Antithesis. This is related to the def-
inition of presentational relations, and Antithesis 
was found in the reference segmentation with a 
low frequency. 
 
Class Relations 
Weak 
(0.4) 
Elaboration, Contrast, Joint, List 
Medium 
(0.6) 
Antithesis, Comparison, Evaluation 
Means, Non-Volitional Cause, Non-
Volitional Result, Solutionhood, Voli-
tional Cause, Volitional Result, Sequence 
Strong 
(0.8) 
Background, Circumstance, Concession, 
Conclusion, Condition, Enablement, Evi-
dence, Explanation, Interpretation, Justi-
fy, Motivation, Otherwise, Purpose, Re-
statement, Summary 
Figure 2. Classification of RST relations 
From this classification we created two more 
strategies: Relation_Depth and Nu-
clei_Depth_Relation. Relation_Depth associates 
a score to the nodes by dividing the relations 
weight by the depth where it occurs, in a bottom-
up way of traversing the tree. We also have used 
the average score to find out nodes that are less 
similar. As we have observed that some im-
provement might be achieved every time nuclei 
information was used, we have tried to combine 
this configuration with the relations? weight. 
Hence, we computed the scores of the Nuclei 
Depth strategy times the proposed relations 
weight. This was the algorithm that we called 
Nuclei_Depth_Relation. Therefore, these two 
last algorithms enrich the original Cosine Depth 
and Nuclei Depth strategies with the relation 
strength information.  
The next section presents the data set we have 
used for our evaluation. 
4 Overview of the corpus 
We used the CSTNews corpus2 that is composed 
of 50 clusters of news articles written in Brazili-
an Portuguese, collected from several sections of 
mainstream news agencies: Politics, Sports, 
World, Daily News, Money, and Science. The 
corpus contains 140 texts altogether, amounting 
to 2,088 sentences and 47,240 words. On aver-
age, the corpus conveys in each cluster 2.8 texts, 
41.76 sentences and 944.8 words. All the texts in 
the corpus were manually annotated with RST 
structures and topic boundaries in a systematic 
way, with satisfactory annotation agreement val-
ues (more details may be found in Cardoso et al, 
2011; Cardoso et al, 2012). Specifically for topic 
boundaries, groups of trained annotators indicat-
ed possible boundaries and the ones indicated by 
the majority of the annotators were assumed to 
be actual boundaries. 
5 Evaluation 
This section presents comparisons of the results 
of the algorithms over the reference corpus. 
The performance of topic segmentation is usu-
ally measured using Recall (R), Precision (P), 
and F-measure (F) scores. These scores quantify 
how closely the system subtopics correspond to 
the ones produced by humans. Those measures 
compare the boundary correspondences without 
considering whether these are close to each oth-
er: if they are not the same (regardless of wheth-
                                                 
2 www2.icmc.usp.br/~taspardo/sucinto/cstnews.html 
94
er they are closer or farther from one another), 
they score zero. However, it is also important to 
know how close the identified boundaries are to 
the expected ones, since this may help to deter-
mine how serious the errors made by the algo-
rithms are. We propose a simple measure to this, 
which we call Deviation (D) from the reference 
annotations. Considering two algorithms that 
propose the same amount of boundaries for a text 
and make one single mistake each (having, there-
fore, the same P, R, and F scores), the best one 
will be the one that deviates the least from the 
reference. The best algorithm should be the one 
with the best balance among P, R, F, and D 
scores.  
The results achieved for the investigated 
methods are reported in Table 1. The first 4 rows 
show the results for the baselines. The algorithms 
based on RST are in the last 6 rows. The last row 
represents the human performance, which we 
refer by topline. It is interesting to have a topline 
because it possibly indicates the limits that au-
tomatic methods may achieve in the task. To find 
the topline, a human annotator of the corpus was 
randomly selected for each text and his annota-
tion was compared with the reference one. 
As expected, the paragraph baseline was very 
good, having the best F values of the baseline 
set. This shows that, in most of the texts, the sub-
topics are organized in paragraphs. Although the 
sentence baseline has the best R, it has the worst 
D. This is due to the fact that not every sentence 
is a subtopic, and to segment all of them be-
comes a problem when we are looking for major 
groups of subtopics. TextTiling is the algorithm 
that deviates the least from the reference seg-
mentation. This happens because it is very con-
servative and detects only a few segments, some-
times only one (the end of the text), causing it to 
have a good deviation score, but penalizing R. 
 
Algorithm R P F D 
TextTiling 0.405 0.773 0.497 0.042 
Paragraph 0.989 0.471 0.613 0.453 
Sentence 1.000 0.270 0.415 1.000 
Randomly 0.674 0.340 0.416 0.539 
Simple Cosine 0.549 0.271 0.345 0.545 
Cosine Nuclei 0.631 0.290 0.379 0.556 
Cosine Depth 0.873 0.364 0.489 0.577 
Nuclei Depth 0.899 0.370 0.495 0.586 
Relation_Depth 0.901 0.507 0.616 0.335 
Nuclei_Depth 
Relation 
0.908 0.353 0.484 0.626 
Topline 0.807 0.799 0.767 0.304 
Table 1. Evaluation of algorithms 
In the case of the algorithms based on RST, we 
may notice that they produced the best results in 
terms of R, P, and F, with acceptable D values. 
We note too that every time the salient units 
were used, R and P increase, except for Nu-
clei_Depth_Relation. Examining the measures, 
we notice that the best algorithm was Rela-
tion_Depth. Although its F is close to the one of 
the Paragraph baseline, the Relation_Depth algo-
rithm shows a much better D value. One may see 
that the traditional TextTiling was also outper-
formed by Relation_Depth.  
As expected, the Topline (the human, there-
fore) has the best F with acceptable D. Its F val-
ue is probably the best that an automatic method 
may expect to achieve. It is 25% better than our 
best method (Relation_Depth). There is, there-
fore, room for improvements, possibly using oth-
er discourse features. 
We have run t-tests for pairs of algorithms for 
which we wanted to check the statistical differ-
ence. As expected, the F difference is not signifi-
cant for Relation_Depth and the Paragraph algo-
rithms, but it was significant with 95% confi-
dence for the comparison of Relation_Depth with 
Nuclei_Depth and TextTiling (also regarding the 
F values). Finally, the difference between Rela-
tion_Depth and the Topline was also significant. 
6 Conclusions and future work 
In this paper we show that discourse structures 
mirror, in some level, the topic boundaries in the 
text. Our results demonstrate that discourse 
knowledge may significantly help to find bound-
aries in a text. In particular, the relation type and 
the level of the discourse structure in which the 
relation happens are important features. To the 
best of our knowledge, this is the first attempt to 
correlate RST structures with topic boundaries, 
which we believe is an important theoretical ad-
vance. 
At this stage, we opted for a manually anno-
tated corpus, because we believe an automatic 
RST analysis would surely decrease the corre-
spondence that was found. However, better dis-
course parsers have arisen and this may not be a 
problem anymore in the future. 
Acknowledgments 
The authors are grateful to FAPESP, CAPES, 
CNPq and Natural Sciences and Engineering Re-
search Council of Canada (Discovery Grant 
261104-2008) for supporting this work. 
 
95
References  
Paula C.F. Cardoso, Erick G. Maziero, Maria L.R. 
Castro Jorge, Eloize M.R. Seno, Ariani Di Fellipo, 
L?cia H.M. Rino, Maria G.V. Nunes, Thiago A.S. 
Pardo. 2011. CSTNews ? A discourse-annotated 
corpus for single and multidocument summariza-
tion of texts in Brazilian Portuguese. In: Proceed-
ings of the 3rd RST Brazilian Meeting, pp. 88-105. 
Paula C.F. Cardoso, Maite Taboada, Thiago A.S. Par-
do. 2013. Subtopics annotation in a corpus of news 
texts: steps towards automatic subtopic segmenta-
tion. In: Proceedings of the Brazilian Symposium 
in Information and Human Language Technology. 
T-H Chang and C-H Lee. 2003. Topic segmentation 
for short texts. In: Proceedings of the 17th Pacific 
Asia Conference Language, pp. 159-165. 
Marti Hearst. 1997. TextTiling: Segmenting Text into 
Multi-Paragraph Subtopic Passages. Computation-
al Linguistics 23(1), pp. 33-64. 
Leonhard Hennig. 2009. Topic-based multi-document 
summarization with probabilistic latent semantic 
analysis. In: Recent Advances in Natural Language 
Processing, pp. 144-149. 
Eduard Hovy and C-Y Lin. 1998. Automated Text 
Summarization and the SUMMARIST system. In: 
Proceedings of TIPSTER, pp. 197-214. 
Eduard Hovy. 2009. Text Summarization. In: Ruslan 
Mitkov. The Oxford Handbook of Computational 
Linguistics, pp. 583-598. United States: Oxford 
University. 
Anna Kazantseva and Stan Szpakowicz. 2012. Topi-
cal Segmentation: a study of human performance 
and a new measure of quality. In:  Proceedings of 
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pp. 211-220. 
Willian C. Mann and Sandra A. Thompson. 1987. 
Rhetorical Structure Theory: A Theory of Text Or-
ganization. Technical Report ISI/RS-87-190. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT 
Press. Cambridge, Massachusetts. 
Hyo-Jung Oh, Sung Hyon Myaeng and Myung-Gil 
Jang. 2007. Semantic passage on sentence topics 
for question answering. Information Sciences 
177(18), pp. 3696-3717. 
Rebecca J. Passonneau and Diane J. Litman. 1997. 
Discourse segmentation by human and automated 
means. Computational Linguistics 23(1), pp. 103-
109. 
Violaine Prince and Alexandre Labadi?. 2007. Text 
segmentation based on document understanding for 
information retrieval. In: Proceedings of the 12th 
International Conference on Applications of Natu-
ral Language to Information Systems, pp. 295-304. 
Maite Taboada and William C. Mann. 2006. Rhetori-
cal Structure Theory: Looking back and moving 
ahead. Discourse Studies 8(3), pp.423-459. 
Xiaojun Wan. 2008. An exploration of document im-
pact on graph-based multi-document summariza-
tion. In: Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pp. 
755-762. 
 
96
