Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 74?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrating High Precision Rules with Statistical Sequence Classifiers for
Accuracy and Speed
Wenhui Liao, Marc Light, and Sriharsha Veeramachaneni
Research and Development,Thomson Reuters
610 Opperman Drive, Eagan MN 55123
Abstract
Integrating rules and statistical systems is a
challenge often faced by natural language pro-
cessing system builders. A common sub-
class is integrating high precision rules with a
Markov statistical sequence classifier. In this
paper we suggest that using such rules to con-
strain the sequence classifier decoder results
in superior accuracy and efficiency. In a case
study of a named entity tagging system, we
provide evidence that this method of combina-
tion does prove efficient than other methods.
The accuracy was the same.
1 Introduction
Sequence classification lies at the core of several
natural language processing applications, such as
named entity extraction, Asian language segmen-
tation, Germanic language noun decompounding,
and event identification. Statistical models with a
Markov dependency have been successful employed
to perform these tasks, e.g., hidden Markov mod-
els (HMMs)(Rabiner, 1989) and conditional random
fields (CRFs)(Lafferty et al, 2001). These statistical
systems employ a Viterbi (Forney, 1973) decoder at
runtime to efficiently calculate the most likely la-
bel sequence based on the observed sequence and
model. Statistical machine translation systems make
use of similar decoders.
In many situations it is beneficial, and some-
times required, for these systems to respect con-
straints from high precision rules. And thus when
building working sequence labeling systems, re-
searchers/software engineers are often faced with
the task of combining these two approaches. In
this paper we argue for a particular method of com-
bining statistical models with Markov dependencies
and high precision rules. We outline a number of
ways to do this and then argue that guiding the de-
coder of the statistical system has many advantages
over other methods of combination.
But first, does the problem of combining multi-
ple approaches really happen? In our experience the
need arises in the following way: a statistical ap-
proach with a Markov component is chosen because
it has the best precision/recall characteristics and has
reasonable speed. However, a number of rules arise
for varied reasons. For example, the customer pro-
vides domain knowledge not present in the training
data or a particular output characteristic is more im-
portant that accuracy. Consider the following ficti-
tious but plausible situation: A named entity tagging
system is built using a CRF. The customer then pro-
vides a number of company names that cannot be
missed, i.e., false negatives for these companies are
catastrophic but false positives can be tolerated. In
addition, it is known that, unlike in the training data,
the runtime data will have a company name immedi-
ately before every ticker symbol. The question fac-
ing the builder of the system is how to combine the
CRF with rules based on the must-find company list
and the company-name-before-every-ticker-symbol
fact.
Similar situations arise for the other sequence tag-
ging situations mentioned above and for machine
translation. We suspect that even for non-language
applications, such as gene sequence labeling, similar
situations arise.
74
In the next section we will discuss a number of
methods for combining statistical systems and high
precision rules and argue for guiding the decoder
of the statistical model. Then in section 3, we de-
scribe an implementation of the approach and give
evidence that the speed benefits are substantial.
2 Methods for Combining a Markov
Statistical System and High Precision
Rules
One method of combination is to encode high preci-
sion rules as features and then train a new model that
includes these features. One advantage is that the
system stays a straightforward statistical system. In
addition, the rules are fully integrated into the sys-
tem allowing the statistical model weigh the rules
against other evidence. However, the model may
not give the rules high weight if training data does
not bear out their high precision or if the rule trig-
ger does not occur often enough in the training data.
Thus, despite a ?rule? feature being on, the system
may not ?follow? the rule in its result labeling. Also,
addition or modification of a rule would require a
retraining of the model for optimal accuracy. The
retraining process may be costly and/or may not be
possible in the operational environment.
Another method is to run both the statistical sys-
tem and the rules and then merge the resulting labels
giving preference to the labels resulting from the
high precision rules. The benefits are that the rules
are always followed. However, the statistical system
does not have the information needed to give an op-
timal solution based on the results of the high preci-
sion rules. In other words, the results will be incon-
sistent from the view of the statistical system; i.e., if
it had know what the rules were going to say, then it
would have calculated the remaining part of the label
sequence differently. In addition, the decoder con-
siders part of the label sequence search space that is
only going to be ruled out, pun intended, later.
Now for the preferred method: run the rules first,
then use their output to guide the decoder for the
statistical model. The benefits of this method are
that the rules are followed, the statistical system is
informed of constraints imposed by the rules and
thus the statistical system calculates optimal paths
given these constraints. In addition, the decoder
considers only those label sequences consistent with
these constraints, resulting in a smaller search space.
Thus, we would expect this method to produce both
a more accurate and a faster implementation.
Consider Figure 1 which shows a lattice that rep-
resents all the labeling sequences for the input ...
Microsoft on Monday announced a ... The possible
labels are O (out), P (person), C (company), L (lo-
cation) . Assume Microsoft is in a list of must-find
companies and that on and Monday are part of a rule
that makes them NOT names in this context. The
bold points are constraints from the high-precision
rules. In other words, only sequences that include
these bold points need to be considered.
Figure 1: Guiding decoding with high-precision rules
Figure 1 also illustrates how the constraints re-
duce the search space. Without constraints, the
search space includes 46 = 4096 sequences, while
with constraints, it includes only 43 = 64.
It should also be noted that we do not claim to
have invented the idea of constraining the decoder.
For example, in the context of active learning, where
a human corrects some of the errors made by a CRF
sequence classifier, (Culota et al, 2006) proposed a
constrained Viterbi algorithm that finds the path with
maximum probability that passes through the labels
assigned by the human. They showed that constrain-
ing the path to respect the human labeling consider-
ably improves the accuracy on the remaining tokens
in the sequence. Our contribution is noticing that
constraining the decoder is a good way to integrate
rule output.
3 A Case Study: Named Entity
Recognition
In this section, we flesh out the discussion of named
entity (NE) tagging started above. Since the entity
type of a word is determined mostly by the context
of the word, NE tagging is often posed as a sequence
75
classification problem and solved by Markov statis-
tical systems.
3.1 A Named Entity Recognition System
The system described here starts with a CRF which
was chosen because it allows for the use of numer-
ous and arbitrary features of the input sequence and
it can be efficiently trained and decoded. We used
the Mallet toolkit (McCallum, 2002) for training the
CRF but implemented our own feature extraction
and runtime system. We used standard features such
as the current word, the word to the right/left, ortho-
graphic shape of the word, membership in word sets
(e.g., common last names), features of neighboring
words, etc.
The system was designed to run on news wire text
and based on this data?s characteristics, we designed
a handful of high precision rules including:
Rule 1: if a token is in a must-tag list, this token
should be marked as Company no matter what the
context is.
Rule 2: if a capitalized word is followed by cer-
tain company suffix such as Ltd, Inc, Corp, etc., la-
bel both as Company.
Rule 3: if a token sequence is in a company list
and the length of the sequence is larger than 3, label
them as Company.
Rule 4: if a token does not include any uppercase
letters, is not pure number, and is not in an excep-
tions list, label it as not part of a name. (The ex-
ceptions list includes around 70 words that are not
capitalized but still could be an NE, such as al, at,
in, -, etc.)
Rule 5: if a token does not satisfy rule 4 but its
neighboring tokens satisfy rule 4, then if this token
is a time related word, label it as not part of a name.
(Example time tokens are January and Monday.)
The first three rules aim to find company names
and the last two to find tokens that are not part of a
name.
These rules are integrated into the system as de-
scribed in section 2: we apply the rules to the input
token sequence and then use the resulting labels, if
any, to constrain the Viterbi decoder for the CRF.
A further optimization of the system is based on
the following observation: features need not be cal-
culated for tokens that have already received labels
from the rules. (An exception to this is when fea-
tures are copied to a neighbor, e.g., the token to my
left is a number.) Thus, we do not calculate many
features of rule-labeled tokens. Note that feature ex-
traction can often be a major portion of the compu-
tational cost of sequence labeling systems (see Table
1(b))
3.2 Evidence of Computational Savings
Resulting from Our Proposed Method of
Integration
We compare the results when high-precision rules
are integrated into CRF for name entity extraction
(company, person, and location) in terms of both ac-
curacy and speed for different corpora. Three cor-
pora are used, CoNLL (CoNLL 2003 English shared
task official test set), MUC (Message Understanding
Conference), and TF (includes around 1000 news ar-
ticles from Thomson Financial).
Table 1(a) shows the results for each corpora re-
spectively. The baseline method does not use any
high-precision rules, the Post-corr uses the high-
precision rules to correct the labeling from the CRF,
and Constr-viti uses the rules to constrain the label
sequences considered by the Viterbi decoder. In gen-
eral, Constr-viti achieves slightly better precision
and recall.
(a)
(b)
Figure 2: (b) A test example : (a) without constraints; (b)
with constraints
To better understand how our strategy could im-
prove the accuracy, we did some analysis on the
76
Table 1: Experiment Results
Database Methods Precision Recall F1
CoNLL Baseline 84.38 83.02 83.69
Post-corr 85.87 84.86 85.36
Constr-viti 85.98 85.55 85.76
TF Baseline 88.39 82.42 85.30
Post-corr 87.69 88.30 87.99
Constr-viti 88.02 88.54 88.28
MUC Baseline 92.22 88.72 90.43
Post-Corr 91.28 88.87 90.06
Constr-viti 90.86 89.37 90.11
(a)Precision and Recall
Methods Rules Features Viterbi Overall
Baseline 0 0.78 0.22 1
Post-corr 0.08 0.78 0.22 1.08
Constr-vite 0.08 0.35 0.13 0.56
Baseline 0 0.85 0.15 1
Post-Corr 0.14 0.85 0.15 1.14
Constr-vite 0.14 0.38 0.1 0.62
Baseline 0 0.79 0.21 1
Post-corr 0.12 0.79 0.21 1.12
Constr-vite 0.12 0.36 0.12 0.60
(b)Time Efficiency
testing data. In one example as shown in Figure 2,
Steel works as an attorney, without high-precision
rules, Steel works is tagged as a company since it is
in our company list. Post-correction changes the la-
bel of works to O, but it is unable to fix Steel. With
our strategy, since works is pinned as O in the Vert-
ibi algorithm, Steel is tagged as Per. Thus, com-
pared to post-correction, the advantage of constrain-
ing Viterbi is that it is able to affect the whole path
where the token is, instead a token itself. However,
the improvements were not significant in our case
study. We have not done an error analysis. We can
only speculate that the high precision rules do not
have perfect precision and thus create a number of
errors that the statistical model would not have made
on its own.
We also measured how much the constrained
Viterbi method improves efficiency. We divide the
computational time to three parts: time in applying
rules, time in feature extraction, and time in Viterbi
computation. Table 1(b) lists the time efficiency. In-
stead using specific time unit (e.g. second), we use
ratio instead by assuming the overall time for the
baseline method is 1. As shown in the table, for
the three data sets, the overall time of our method
is 0.56, 0.62, and 0.60 of the time of the baseline
algorithm respectively. The post-correction method
is the most expensive one because of the extra time
spending in rules. Overall, the constrained Viterbi
method is substantially faster than the Baseline and
Post-corr methods in addition to being more accu-
rate.
4 Conclusions
The contribution of this paper is the repurposing of
the idea of constraining a decoder: we constrain the
decoder as a way to integrate high precision rules
with a statistical sequence classifier. In a case study
of named entity tagging, we show that this method
of combination does in fact increase efficiency more
than competing methods without any lose of ac-
curacy. We believe analogous situations exist for
other sequence classifying tasks such as Asian lan-
guage segmentation, Germanic language noun de-
compounding, and event identification.
References
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence Journal, 170:1101?1122.
G. D. Forney. 1973. The viterbi algorithm. Proceedings
of the IEEE, 61(3):268?278.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289.
A.K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, pages 257?286.
77
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Surrogate Learning -
From Feature Independence to Semi-Supervised Classification
Sriharsha Veeramachaneni and Ravi Kumar Kondadadi
Thomson Reuters Research and Development
Eagan, MN 55123, USA
[harsha.veeramachaneni,ravikumar.kondadadi]@thomsonreuters.com
Abstract
We consider the task of learning a classi-
fier from the feature space X to the set of
classes Y = {0, 1}, when the features can
be partitioned into class-conditionally inde-
pendent feature sets X1 and X2. We show
that the class-conditional independence can be
used to represent the original learning task
in terms of 1) learning a classifier from X2
to X1 (in the sense of estimating the prob-
ability P (x1|x2))and 2) learning the class-
conditional distribution of the feature set X1.
This fact can be exploited for semi-supervised
learning because the former task can be ac-
complished purely from unlabeled samples.
We present experimental evaluation of the idea
in two real world applications.
1 Introduction
Semi-supervised learning is said to occur when the
learner exploits (a presumably large quantity of) un-
labeled data to supplement a relatively small labeled
sample, for accurate induction. The high cost of la-
beled data and the simultaneous plenitude of unla-
beled data in many application domains, has led to
considerable interest in semi-supervised learning in
recent years (Chapelle et al, 2006).
We show a somewhat surprising consequence of
class-conditional feature independence that leads
to a principled and easily implementable semi-
supervised learning algorithm. When the feature set
can be partitioned into two class-conditionally in-
dependent sets, we show that the original learning
problem can be reformulated in terms of the problem
of learning a first predictor from one of the partitions
to the other, plus a second predictor from the latter
partition to class label. That is, the latter partition
acts as a surrogate for the class variable. Assum-
ing that the second predictor can be learned from
a relatively small labeled sample this results in an
effective semi-supervised algorithm, since the first
predictor can be learned from only unlabeled sam-
ples.
In the next section we present the simple yet in-
teresting result on which our semi-supervised learn-
ing algorithm (which we call surrogate learning) is
based. We present examples to clarify the intuition
behind the approach and present a special case of
our approach that is used in the applications section.
We then examine related ideas in previous work and
situate our algorithm among previous approaches
to semi-supervised learning. We present empirical
evaluation on two real world applications where the
required assumptions of our algorithm are satisfied.
2 Surrogate Learning
We consider the problem of learning a classifier
from the feature space X to the set of classes Y =
{0, 1}. Let the features be partitioned into X =
X1 ? X2. The random feature vector x ? X will be
represented correspondingly as x = (x1,x2). Since
we restrict our consideration to a two-class problem,
the construction of the classifier involves the esti-
mation of the probability P (y = 0|x1,x2) at every
point (x1,x2) ? X .
We make the following assumptions on the joint
probabilities of the classes and features.
10
1. P (x1,x2|y) = P (x1|y)P (x2|y) for y ?
{0, 1}. That is, the feature sets x1 and x2
are class-conditionally independent for both
classes. Note that, when X1 and X2 are one-
dimensional, this condition is identical to the
Naive Bayes assumption, although in general
our assumption is weaker.
2. P (x1|x2) 6= 0, P (x1|y) 6= 0 and P (x1|y =
0) 6= P (x1|y = 1). These assumptions are
to avoid divide-by-zero problems in the alge-
bra below. If x1 is a discrete valued random
variable and not irrelevant for the classification
task, these conditions are often satisfied.
We can now show that P (y = 0|x1,x2) can
be written as a function of P (x1|x2) and P (x1|y).
When we consider the quantity P (y,x1|x2), we
may derive the following.
P (y,x1|x2) = P (x1|y,x2)P (y|x2)
? P (y,x1|x2) = P (x1|y)P (y|x2)
(from the independence assumption)
? P (y|x1,x2)P (x1|x2) = P (x1|y)P (y|x2)
? P (y|x1,x2)P (x1|x2)P (x1|y) = P (y|x2) (1)
Since P (y = 0|x2) + P (y = 1|x2) = 1, Equa-
tion 1 implies
P (y = 0|x1,x2)P (x1|x2)
P (x1|y = 0) +
P (y = 1|x1,x2)P (x1|x2)
P (x1|y = 1) = 1
? P (y = 0|x1,x2)P (x1|x2)P (x1|y = 0) +
(1? P (y = 0|x1,x2))P (x1|x2)
P (x1|y = 1) = 1(2)
Solving Equation 2 for P (y = 0|x1,x2), we ob-
tain
P (y = 0|x1,x2) =
P (x1|y = 0)
P (x1|x2) ?
P (x1|y = 1)? P (x1|x2)
P (x1|y = 1)? P (x1|y = 0)(3)
We have succeeded in writing P (y = 0|x1,x2) as
a function of P (x1|x2) and P (x1|y). Although this
result was previously observed in a different context
by Abney in (Abney, 2002), he does not use it to
derive a semi-supervised learning algorithm. This
result can lead to a significant simplification of the
learning task when a large amount of unlabeled data
is available. The semi-supervised learning algorithm
involves the following two steps.
1. From unlabeled data learn a predictor from the
feature space X2 to the space X1 to predict
P (x1|x2). There is no restriction on the learner
that can be used as long as it outputs posterior
class probability estimates.
2. Estimate the quantity P (x1|y) from a labeled
samples. In case x1 is finite valued, this can
be done by just counting. If X1 has low car-
dinality the estimation problem requires very
few labeled samples. For example, if x1 is
binary, then estimating P (x1|y) involves esti-
mating just two Bernoulli probabilities.
Thus, we can decouple the prediction problem into
two separate tasks, one of which involves predict-
ing x1 from the remaining features. In other words,
x1 serves as a surrogate for the class label. Fur-
thermore, for the two steps above there is no neces-
sity for complete samples. The labeled examples can
have the feature x2 missing.
At test time, an input sample (x1,x2) is classified
by computing P (x1|y) and P (x1|x2) from the pre-
dictors obtained from training, and plugging these
values into Equation 3. Note that these two quanti-
ties are computed for the actual value of x1 taken by
the input sample.
The following example illustrates surrogate learn-
ing.
??????????????
Example 1
Consider the following variation on a problem
from (Duda et al, 2000) of classifying fish on a con-
veryor belt as either salmon (y = 0) or sea bass
(y = 1). The features describing the fish are x1,
a binary feature describing whether the fish is light
(x1 = 0) or dark (x1 = 1), and x2 describes the
length of the fish which is real-valued. Assume (un-
realistically) that P (x2|y), the class-conditional dis-
tribution of x2, the length for salmon is Gaussian,
11
and for the sea bass is Laplacian as shown in Fig-
ure 1.
?4 ?2 0 2 40
0.5
x2
P(x2|y=0) P(x2|y=1)
Figure 1: Class-conditional probability distributions of
the feature x2.
Because of the class-conditional feature in-
dependence assumption, the joint distribution
P (x1,x2,y) = P (x2|y)P (x1,y) can now be
completely specified by fixing the joint probabil-
ity P (x1,y). Let P (x1 = 0,y = 0) = 0.3,
P (x1 = 0,y = 1) = 0.1, P (x1 = 1,y = 0) = 0.2,
and P (x1 = 1,y = 1) = 0.4. I.e., a salmon is more
likely to be light than dark and a sea bass is more
likely to be dark than light.
The full joint distribution is depicted in Figure 2.
Also shown in Figure 2 are the conditional distribu-
tions P (x1 = 0|x2) and P (y = 0|x1,x2).
Assume that we build a predictor to decide be-
tween x1 = light and x1 = dark from the length us-
ing a data set of unlabeled fish. On a random salmon,
this predictor will most likely decide that x1 = light
(because, for a salmon, x1 = light is more likely
than x1 = dark, and similarly for a sea bass the
predictor often decides that x1 = dark. Conse-
quently the predictor provides information about the
true class label y. This can also be seen in the sim-
ilarities between the curves P (y = 0|x1,x2) to the
curve P (x1|x2) in Figure 2.
Another way to interpret the example is to note
that if a predictor for P (x1|x2) were built on only
the salmons then P (x1 = light|x2) will be a con-
stant value (0.6). Similarly the value of P (x1 =
light|x2) for sea basses will also be a constant value
(0.2). That is, the value of P (x1 = light|x2) for
a sample is a good predictor of its class. However,
?4 ?2 0 2 4
0.5
x1 = 0
x1 = 1
x2
P(x1=1,y=0,x2) P(x1=1,y=1,x2)
P(x1=0,y=1,x2)P(x1=0,y=0,x2)
P(y=0|x1=1,x2)
P(y=0|x1=0,x2)
P(x1=0|,x2)
Figure 2: The joint distributions and the posterior distri-
butions of the class y and the surrogate class x1.
surrogate learning builds the predictor P (x1|x2) on
unlabeled data from both types of fish and there-
fore additionally requires P (x1|y) to estimate the
boundary between the classes.
2.1 A Special Case
The independence assumptions made in the setting
above may seem too strong to hold in real problems,
especially because the feature sets are required to
be class-conditionally independent for both classes.
We now specialize the setting of the classification
problem to the one realized in the applications we
present later.
We still wish to learn a classifier from X = X1 ?
X2 to the set of classes Y = {0, 1}. We make the
following slightly modified assumptions.
1. x1 is a binary random variable. That is, X1 =
{0, 1}.
2. P (x1,x2|y = 0) = P (x1|y = 0)P (x2|y =
0). We require that the feature x1 be class-
conditionally independent of the remaining fea-
tures only for the class y = 0.
3. P (x1 = 0,y = 1) = 0. This assumption says
that x1 is a ?100% recall? feature for y = 11.
Assumption 3 simplifies the learning task to the
estimation of the probability P (y = 0|x1 = 1,x2)
for every point x2 ? X2. We can proceed as before
1This assumption can be seen to trivially enforce the inde-
pendence of the features for class y = 1.
12
to obtain the expression in Equation 3.
P (y = 0|x1 = 1,x2)
= P (x1 = 1|y = 0)P (x1 = 1|x2) . . .
. . . P (x1 = 1|y = 1)? P (x1 = 1|x2)P (x1 = 1|y = 1)? P (x1 = 1|y = 0)
= P (x1 = 1|y = 0)P (x1 = 1|x2) ?
1? P (x1 = 1|x2)
1? P (x1 = 1|y = 0)
= P (x1 = 1|y = 0)P (x1 = 1|x2) ?
P (x1 = 0|x2)
P (x1 = 0|y = 0)
= P (x1 = 1|y = 0)P (x1 = 0|y = 0) ?
P (x1 = 0|x2)
(1? P (x1 = 0|x2))(4)
Equation 4 shows that P (y = 0|x1 = 1,x2)
is a monotonically increasing function of P (x1 =
0|x2). This means that after we build a predictor
from X2 to X1, we only need to establish the thresh-
old on P (x1 = 0|x2) to yield the optimum classi-
fication between y = 0 and y = 1. Therefore the
learning proceeds as follows.
1. From unlabeled data learn a predictor from the
feature space X2 to the binary space X1 to pre-
dict the quantity P (x1|x2).
2. Use labeled sample to establish the thresh-
old on P (x1 = 0|x2) to achieve the desired
precision-recall trade-off for the original clas-
sification problem.
Because of our assumptions, for a sample from
class y = 0 it is impossible to predict whether
x1 = 0 or x1 = 1 better than random by looking
at the x2 feature, whereas a sample from the posi-
tive class always has x1 = 1. Therefore the samples
with x1 = 0 serve to delineate the positive exam-
ples among the samples with x1 = 1. We therefore
call the samples that have x1 = 1 as the target sam-
ples and those that have x1 = 0 as the background
samples.
3 Related Work
Although the idea of using unlabeled data to im-
prove classifier accuracy has been around for several
decades (Nagy and Shelton, 1966), semi-supervised
learning has received much attention recently due
to impressive results in some domains. The com-
pilation of chapters edited by Chappelle et al is an
excellent introduction to the various approaches to
semi-supervised learning, and the related practical
and theoretical issues (Chapelle et al, 2006).
Similar to our setup, co-training assumes that the
features can be split into two class-conditionally
independent sets or ?views? (Blum and Mitchell,
1998). Also assumed is the sufficiency of either
view for accurate classification. The co-training al-
gorithm iteratively uses the unlabeled data classified
with high confidence by the classifier on one view,
to generate labeled data for learning the classifier on
the other.
The intuition underlying co-training is that the er-
rors caused by the classifier on one view are inde-
pendent of the other view, hence can be conceived
as uniform2 noise added to the training examples
for the other view. Consequently, the number of la-
bel errors in a region in the feature space is propor-
tional to the number of samples in the region. If the
former classifier is reasonably accurate, the propor-
tionally distributed errors are ?washed out? by the
correctly labeled examples for the latter classifier.
Seeger showed that co-training can also be viewed
as an instance of the Expectation-Maximization al-
gorithm (Seeger, 2000).
The main distinction of surrogate learning from
co-training is the learning of a predictor from one
view to the other, as opposed to learning predictors
from both views to the class label. We can there-
fore eliminate the requirement that both views be
sufficiently informative for reasonably accurate pre-
diction. Furthermore, unlike co-training, surrogate
learning has no iterative component.
Ando and Zhang propose an algorithm to regu-
larize the hypothesis space by simultaneously con-
sidering multiple classification tasks on the same
feature space (Ando and Zhang, 2005). They then
use their so-called structural learning algorithm for
semi-supervised learning of one classification task,
by the artificial construction of ?related? problems
on unlabeled data. This is done by creating prob-
lems of predicting observable features of the data
and learning the structural regularization parame-
ters from these ?auxiliary? problems and unlabeled
data. More recently in (Ando and Zhang, 2007) they
2Whether or not a label is erroneous is independent of the
feature values of the latter view.
13
showed that, with conditionally independent feature
sets predicting from one set to the other allows the
construction of a feature representation that leads
to an effective semi-supervised learning algorithm.
Our approach directly operates on the original fea-
ture space and can be viewed another justification
for the algorithm in (Ando and Zhang, 2005).
Multiple Instance Learning (MIL) is a learning
setting where training data is provided as positive
and negative bags of samples (Dietterich et al,
1997). A negative bag contains only negative ex-
amples whereas a positive bag contains at least one
positive example. Surrogate learning can be viewed
as artificially constructing a MIL problem, with the
targets acting as one positive bag and the back-
grounds acting as one negative bag (Section 2.1).
The class-conditional feature independence assump-
tion for class y = 0 translates to the identical and
independent distribution of the negative samples in
both bags.
4 Two Applications
We applied the surrogate learning algorithm to the
problems of record linkage and paraphrase genera-
tion. As we shall see, the applications satisfy the
assumptions in our second (100% recall) setting.
4.1 Record Linkage/ Entity Resolution
Record linkage is the process of identification and
merging of records of the same entity in different
databases or the unification of records in a single
database, and constitutes an important component of
data management. The reader is referred to (Win-
kler, 1995) for an overview of the record linkage
problem, strategies and systems. In natural language
processing record linkage problems arise during res-
olution of entities found in natural language text to
a gazetteer.
Our problem consisted of merging each of ?
20000 physician records, which we call the update
database, to the record of the same physician in
a master database of ? 106 records. The update
database has fields that are absent in the master
database and vice versa. The fields in common in-
clude the name (first, last and middle initial), sev-
eral address fields, phone, specialty, and the year-
of-graduation. Although the last name and year-
of-graduation are consistent when present, the ad-
dress, specialty and phone fields have several incon-
sistencies owing to different ways of writing the ad-
dress, new addresses, different terms for the same
specialty, missing fields, etc. However, the name
and year alone are insufficient for disambiguation.
We had access to ? 500 manually matched update
records for training and evaluation (about 40 of these
update records were labeled as unmatchable due to
insufficient information).
The general approach to record linkage involves
two steps: 1) blocking, where a small set of can-
didate records is retrieved from the master record
database, which contains the correct match with
high probability, and 2) matching, where the fields
of the update records are compared to those of the
candidates for scoring and selecting the match. We
performed blocking by querying the master record
database with the last name from the update record.
Matching was done by scoring a feature vector of
similarities over the various fields. The feature val-
ues were either binary (verifying the equality of a
particular field in the update and a master record) or
continuous (some kind of normalized string edit dis-
tance between fields like street address, first name
etc.).
The surrogate learning solution to our matching
problem was set up as follows. We designated the
binary feature of equality of year of graduation3 as
the ?100% recall? feature x1, and the remaining fea-
tures are relegated to x2. The required conditions
for surrogate learning are satisfied because 1) in our
data it is highly unlikely for two records with differ-
ent year- of-graduation to belong to the same physi-
cian and 2) if it is known that the update record
and a master record belong to two different physi-
cians, then knowing that they have the same (or dif-
ferent) year-of-graduation provides no information
about the other features. Therefore all the feature
vectors with the binary feature indicating equality
of year-of-graduation are targets and the remaining
are backgrounds.
First, we used feature vectors obtained from the
records in all blocks from all 20000 update records
to estimate the probability P (x1|x2). We used lo-
3We believe that the equality of the middle intial would have
worked just as well for x1.
14
Table 1: Precision and Recall for record linkage.
Training Precision Recall
proportion
Surrogate 0.96 0.95
Supervised 0.5 0.96 0.94
Supervised 0.2 0.96 0.91
gistic regression for this prediction task. For learn-
ing the logistic regression parameters, we discarded
the feature vectors for which x1 was missing and
performed mean imputation for the missing values
of other features. Second, the probability P (x1 =
1|y = 0) (the probability that two different ran-
domly chosen physicians have the same year of
graduation) was estimated straightforwardly from
the counts of the different years-of-graduation in the
master record database.
These estimates were used to assign the score
P (y = 1|x1 = 1,x2) to the records in a block (cf.
Equation 4). The score of 0 is assigned to feature
vectors which have x1 = 0. The only caveat is cal-
culating the score for feature vectors that had miss-
ing x1. For such records we assign the score P (y =
1|x2) = P (y = 1|x1 = 1,x2)P (x1 = 1|x2). We
have estimates for both quantities on the right hand
side. The highest scoring record in each block was
flagged as a match if it exceeded some appropriate
threshold.
We compared the results of the surrogate learn-
ing approach to a supervised logistic regression
based matcher which used a portion of the manual
matches for training and the remaining for testing.
Table 1 shows the match precision and recall for
both the surrogate learning and the supervised ap-
proaches. For the supervised algorithm, we show the
results for the case where half the manually matched
records were used for training and half for testing,
as well as for the case where a fifth of the records of
training and the remaining four-fifths for testing. In
the latter case, every record participated in exactly
one training fold but in four test folds.
The results indicate that the surrogate learner per-
forms better matching by exploiting the unlabeled
data than the supervised learner with insufficient
training data. The results although not dramatic are
still promising, considering that the surrogate learn-
ing approach used none of the manually matched
records.
4.2 Paraphrase Generation for Event
Extraction
Sentence classification is often a preprocessing step
for event or relation extraction from text. One of the
challenges posed by sentence classification is the di-
versity in the language for expressing the same event
or relationship. We present a surrogate learning ap-
proach to generating paraphrases for expressing the
merger-acquisition (MA) event between two organi-
zations in financial news. Our goal is to find para-
phrase sentences for the MA event from an unla-
beled corpus of news articles, that might eventually
be used to train a sentence classifier that discrimi-
nates between MA and non-MA sentences.
We assume that the unlabeled sentence corpus is
time-stamped and named entity tagged with orga-
nizations. We further assume that a MA sentence
must mention at least two organizations. Our ap-
proach to generate paraphrases is the following. We
first extract all the so-called source sentences from
the corpus that match a few high-precision seed pat-
terns. An example of a seed pattern used for the
MA event is ?<ORG1> acquired<ORG2>? (where
<ORG1> and <ORG2> are place holders for
strings that have been tagged as organizations). An
example of a source sentence that matches the seed
is ?It was announced yesterday that <ORG>Google
Inc.<ORG> acquired <ORG>Youtube <ORG>?.
The purpose of the seed patterns is to produce pairs
of participant organizations in an MA event with
high precision.
We then extract every sentence in the corpus that
contains at least two organizations, such that at least
one of them matches an organization in the source
sentences, and has a time-stamp within a two month
time window of the matching source sentence. Of
this set of sentences, all that contain two or more or-
ganizations from the same source sentence are des-
ignated as target sentences, and the rest are desig-
nated as background sentences.
We speculate that since an organization is unlikely
to have a MA relationship with two different orga-
nizations in the same time period the backgrounds
are unlikely to contain MA sentences, and more-
over the language of the non-MA target sentences is
15
Table 2: Patterns used as seeds and the number of source
sentences matching each seed.
Seed pattern # of sources
1 <ORG> acquired <ORG> 57
2 <ORG> bought <ORG> 70
3 offer for <ORG> 287
4 to buy <ORG> 396
5 merger with <ORG> 294
indistinguishable from that of the background sen-
tences. To relate the approach to surrogate learning,
we note that the binary ?organization-pair equality?
feature (both organizations in the current sentence
being the same as those in a source sentence) serves
as the ?100% recall? feature x1. Word unigram, bi-
gram and trigram features were used as x2. This
setup satisfies the required conditions for surrogate
learning because 1) if a sentence is about MA, the
organization pair mentioned in it must be the same
as that in a source sentence, (i.e., if only one of the
organizations match those in a source sentence, the
sentence is unlikely to be about MA) and 2) if an un-
labeled sentence is non-MA, then knowing whether
or not it shares an organization with a source does
not provide any information about the language in
the sentence.
If the original unlabeled corpus is sufficiently
large, we expect the target set to cover most of the
paraphrases for the MA event but may contain many
non-MA sentences as well. The task of generating
paraphrases involves filtering the target sentences
that are non-MA and flagging the rest of the tar-
gets as paraphrases. This is done by constructing a
classifier between the targets and backgrounds. The
feature set used for this task was a bag of word un-
igrams, bigrams and trigrams, generated from the
sentences and selected by ranking the n-grams by
the divergence of their distributions in the targets
and backgrounds. A support vector machine (SVM)
was used to learn to classify between the targets and
backgrounds and the sentences were ranked accord-
ing to the score assigned by the SVM (which is a
proxy for P (x1 = 1|x2)). We then thresholded the
score to obtain the paraphrases.
Our approach is similar in principle to the ?Snow-
ball? system proposed in (Agichtein and Gravano,
2000) for relation extraction. Similar to us, ?Snow-
ball? looks for known participants in a relationship in
an unlabeled corpus, and uses the newly discovered
contexts to extract more participant tuples. How-
ever, unlike surrogate learning, which can use a rich
set of features for ranking the targets, ?Snowball?
scores the newly extracted contexts according to a
single feature value which is confidence measure
based only on the number of known participant tu-
ples that are found in the context.
Example 2 below lists some sentences to illustrate
the surrogate learning approach. Note that the tar-
gets may contain both MA and non-MA sentences
but the backgrounds are unlikely to be MA.
??????????????
Example 2
Seed Pattern
?offer for <ORG>?
Source Sentences
1. <ORG>US Airways<ORG> said Wednesday it will
increase its offer for <ORG>Delta<ORG>.
Target Sentences (SVM score)
1.<ORG>US Airways<ORG> were to combine with a
standalone <ORG>Delta<ORG>. (1.0008563)
2.<ORG>US Airways<ORG> argued that the nearly
$10 billion acquisition of <ORG>Delta<ORG> would
result in an efficiently run carrier that could offer low
fares to fliers. (0.99958149)
3.<ORG>US Airways<ORG> is asking
<ORG>Delta<ORG>?s official creditors commit-
tee to support postponing that hearing. (-0.99914371)
Background Sentences (SVM score)
1. The cities have made various overtures to
<ORG>US Airways<ORG>, including a promise
from <ORG>America West Airlines<ORG> and the
former <ORG>US Airways<ORG>. (0.99957752)
2. <ORG>US Airways<ORG> shares rose 8 cents
to close at $53.35 on the <ORG>New York Stock
Exchange<ORG>. (-0.99906444)
??????????????
We tested our algorithm on an unlabeled corpus of
approximately 700000 financial news articles. We
experimented with the five seed patterns shown in
Table 2. We extracted a total of 870 source sentences
from the five seeds. The number of source sentences
matching each of the seeds is also shown in Table 2.
Note that the numbers add to more than 870 because
it is possible for a source sentence to match more
than one seed.
The participants that were extracted from sources
16
Table 3: Precision/Recall of surrogate learning on the
MA paraphrase problem for various thresholds. The
baseline of using all the targets as paraphrases for MA
has a precision of 66% and a recall of 100%.
Threshold Precision Recall
0.0 0.83 0.94
-0.2 0.82 0.95
-0.8 0.79 0.99
corresponded to approximately 12000 target sen-
tences and approximately 120000 background sen-
tences. For the purpose of evaluation, 500 randomly
selected sentences from the targets were manually
checked leading to 330 being tagged as MA and the
remaining 170 as non-MA. This corresponds to a
66% precision of the targets.
We then ranked the targets according to the score
assigned by the SVM trained to classify between the
targets and backgrounds, and selected all the targets
above a threshold as paraphrases for MA. Table 3
presents the precision and recall on the 500 manu-
ally tagged sentences as the threshold varies. The
results indicate that our approach provides an effec-
tive way to rank the target sentences according to
their likelihood of being about MA.
To evaluate the capability of the method to find
paraphrases, we conducted five separate experi-
ments using each pattern in Table 2 individually as a
seed and counting the number of obtained sentences
containing each of the other patterns (using a thresh-
old of 0.0). These numbers are shown in the differ-
ent columns of Table 4. Although new patterns are
obtained, their distribution only roughly resembles
the original distribution in the corpus. We attribute
this to the correlation in the language used to de-
scribe a MA event based on its type (merger vs. ac-
quisition, hostile takeover vs. seeking a buyer, etc.).
Finally we used the paraphrases, which were
found by surrogate learning, to augment the train-
ing data for a MA sentence classifier and evaluated
its accuracy. We first built a SVM classifier only
on a portion of the labeled targets and classified the
remaining. This approach yielded an accuracy of
76% on the test set (with two-fold cross validation).
We then added all the targets scored above a thresh-
old by surrogate learning as positive examples (4000
Table 4: Number of sentences found by surrogate learn-
ing matching each of the remaining seed patterns, when
only one of the patterns was used as a seed. Each column
is for one experiment with the corresponding pattern used
as the seed. For example, when only the first pattern was
used as the seed, we obtained 18 sentences that match the
fourth pattern.
Seeds 1 2 3 4 5
1 2 2 5 1
2 5 6 7 5
3 4 6 152 103
4 18 16 93 57
5 3 9 195 57
positive sentences in all were added), and all the
backgrounds that scored below a low threshold as
negative examples (27000 sentences), to the training
data and repeated the two-fold cross validation. The
classifier learned on the augmented training data im-
proved the accuracy on the test data to 86% .
We believe that better designed features (than
word n-grams) will provide paraphrases with higher
precision and recall of the MA sentences found by
surrogate learning. To apply our approach to a new
event extraction problem, the design step also in-
volves the selection of the x1 feature such that the
targets and backgrounds satisfy our assumptions.
5 Conclusions
We presented surrogate learning ? an easily imple-
mentable semi-supervised learning algorithm that
can be applied when the features satisfy the required
independence assumptions. We presented two appli-
cations, showed how the assumptions are satisfied,
and presented empirical evidence for the efficacy of
our algorithm. We have also applied surrogate learn-
ing to problems in information retrieval and docu-
ment zoning. We expect that surrogate learning is
sufficiently general to be applied in many NLP ap-
plications, if the features are carefully designed. We
briefly note that a surrogate learning method based
on regression and requiring only mean independence
instead of full statistical independence can be de-
rived using techniques similar to those in Section 2
? this modification is closely related to the problem
and solution presented in (Quadrianto et al, 2008).
17
References
S. Abney. 2002. Bootstrapping. In In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 360?367.
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing Relations from Large Plain-Text Collections. In
Proceedings of the 5th ACM International Conference
on Digital Libraries (ACM DL), pages 85?94, June,
2-7.
R. K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and unla-
beled data. JMLR, 6:1817?1853.
R. K. Ando and T. Zhang. 2007. Two-view feature gen-
eration model for semi-supervised learning. In ICML,
pages 25?32.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT, pages 92?
100.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez.
1997. Solving the multiple instance problem with
axis-parallel rectangles. Artificial Intelligence, 89(1-
2):31?71.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern
Classification. Wiley-Interscience Publication.
G. Nagy and G. L. Shelton. 1966. Self-corrective charac-
ter recognition system. IEEE Trans. Information The-
ory, 12(2):215?222.
N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le.
2008. Estimating labels from label proportions. In
ICML ?08: Proceedings of the 25th international con-
ference on Machine learning, pages 776?783.
M. Seeger. 2000. Input-dependent regularization
of conditional density models. Technical re-
port, Institute for ANC, Edinburgh, UK. See
http://www.dai.ed.ac.uk/?seeger/papers.html.
W. E. Winkler. 1995. Matching and record linkage. In
Business Survey Methods, pages 355?384. Wiley.
18
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 58?65,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Simple Semi-supervised Algorithm For
Named Entity Recognition
Wenhui Liao and Sriharsha Veeramachaneni
Research and Develpment,Thomson Reuters
610 Opperman Drive, Eagan MN 55123
{wenhui.liao, harsha.veeramachaneni}@thomsonreuters.com
Abstract
We present a simple semi-supervised learning
algorithm for named entity recognition (NER)
using conditional random fields (CRFs). The
algorithm is based on exploiting evidence that
is independent from the features used for a
classifier, which provides high-precision la-
bels to unlabeled data. Such independent ev-
idence is used to automatically extract high-
accuracy and non-redundant data, leading to a
much improved classifier at the next iteration.
We show that our algorithm achieves an aver-
age improvement of 12 in recall and 4 in pre-
cision compared to the supervised algorithm.
We also show that our algorithm achieves high
accuracy when the training and test sets are
from different domains.
1 Introduction
Named entity recognition (NER) or tagging is the
task of finding names such as organizations, persons,
locations, etc. in text. Since whether or not a word is
a name and the entity type of a name are determined
mostly by the context of the word as well as by the
entity type of its neighbors, NER is often posed as a
sequence classification problem and solved by meth-
ods such as hidden Markov models (HMM) and con-
ditional random fields (CRF).
Automatically tagging named entities (NE) with
high precision and recall requires a large amount of
hand-annotated data, which is expensive to obtain.
This problem presents itself time and again because
tagging the same NEs in different domains usually
requires different labeled data. However, in most
domains one often has access to large amounts of
unlabeled text. This fact motivates semi-supervised
approaches for NER.
Semi-supervised learning involves the utilization
of unlabeled data to mitigate the effect of insuf-
ficient labeled data on classifier accuracy. One
variety of semi-supervised learning essentially at-
tempts to automatically generate high-quality train-
ing data from an unlabeled corpus. Algorithms such
as co-training (Blum and Mitchell, 1998)(Collins
and Singer, 1999)(Pierce and Cardie, 2001) and
the Yarowsky algorithm (Yarowsky, 1995) make as-
sumptions about the data that permit such an ap-
proach.
The main requirement for the automatically gen-
erated training data in addition to high accuracy,
is that it covers regions in the feature space with
low probability density. Furthermore, it is neces-
sary that all the classes are represented according to
their prior probabilities in every region in the fea-
ture space. One approach to achieve these goals is
to select unlabeled data that has been classified with
low confidence by the classifier trained on the orig-
inal training data, but whose labels are known with
high precision from independent evidence. Here in-
dependence means that the high-precision decision
rule that classifies these low confidence instances
uses information that is independent of the features
used by the classifier.
We propose two ways of obtaining such inde-
pendent evidence for NER. The first is based on
the fact that multiple mentions of capitalized to-
kens are likely to have the same label and occur
in independently chosen contexts. We call this the
58
multi-mention property. The second is based on the
fact that entities such as organizations, persons, etc.,
have context that is highly indicative of the class,
yet is independent of the other context (e.g. com-
pany suffixes like Inc., Co., etc., person titles like
Mr., CEO, etc.). We call such context high precision
independent context.
Let us first look at two examples.
Example 1:
1) said Harry You, CEO of HearingPoint ....
2) For this year?s second quarter, You said the
company?s ...
The classifier tags ?Harry You? as person (PER)
correctly since its context (said, CEO) makes it an
obvious name. However, in the second sentence, the
classifier fails to tag ?You? as a person since ?You?
is usually a stopword. The second sentence is ex-
actly the type of data needed in the training set.
Example 2:
(1) Medtronic Inc 4Q profits rise 10 percent...
(2) Medtronic 4Q profits rise 10 percent...
The classifier tags ?Medtronic? correctly in the
first sentence because of the company suffix ?Inc?
while it fails to tag ?Medtronic? in the second
sentence since ?4Q profits? is a new pattern and
?Medtronic? is unseen in the training data. Thus the
second sentence is what we need in the training set.
The two examples have one thing in common. In
both cases, the second sentence has a new pattern
and incorrect labels, which can be fixed by using
either multi-mention or high-precision context from
the first sentence. We actually artificially construct
the second sentence to be added to the training set in
Example 2 although only the first sentence exists in
the unlabeled corpus.
By leveraging such independent evidence, our
algorithm can automatically extract high-accuracy
and non-redundant data for training, and thus ob-
tain an improved model for NER. Specifically, our
algorithm starts with a model trained with a small
amount of gold data (manually tagged data). This
model is then used to extract high-confidence data,
which is then used to discover low-confidence data
by using other independent features. These low-
confidence data are then added to the training data
to retrain the model. The whole process repeats
until no significant improvement can be achieved.
Our experiments show that the algorithm is not only
much better than the initial model, but also better
than the supervised learning when a large amount
of gold data are available. Especially, even when
the domain from which the original training data is
sampled is different from the domain of the testing
data, our algorithm still provides significant gains in
classification accuracy.
2 Related Work
The Yarowsky algorithm (Yarowsky, 1995), orig-
inally proposed for word sense disambiguation,
makes the assumption that it is very unlikely for two
occurrences of a word in the same discourse to have
different senses. This assumption is exploited by
selecting words classified with high confidence ac-
cording to sense and adding other contexts of the
same words in the same discourse to the training
data, even if they have low confidence. This allows
the algorithm to learn new contexts for the senses
leading to higher accuracy. Our algorithm also uses
multi-mention features. However, the application
of the Yarowsky algorithm to NER involves several
domain-specific choices as will become evident be-
low.
Wong and Ng (Wong and Ng, 2007) use the same
idea of multiple mentions of a token sequence be-
ing to the same named entity for feature engineer-
ing. They use a named entity recognition model
based on the maximum entropy framework to tag a
large unlabeled corpus. Then the majority tags of
the named entities are collected in lists. The model
is then retrained by using these lists as extra fea-
tures. This method requires a sufficient amount of
manually tagged data initially to work. Their paper
shows that, if the initial model has a low F-score,
the model with the new features leads to low F-score
too. Our method works with a small amount of gold
data because, instead of constructing new features,
we use independent evidence to enrich the training
data with high-accuracy and non-redundant data.
The co-training algorithm proposed by Blum and
Mitchell (Blum and Mitchell, 1998) assumes that
the features can be split into two class-conditionally
independent sets or ?views? and that each view is
sufficient for accurate classification. The classifier
built on one of the views is used to classify a large
unlabeled corpus and the data classified with high-
59
confidence are added to the training set on which
the classifier on the other view is trained. This pro-
cess is iterated by interchanging the views. The
main reason that co-training works is that, because
of the class-conditional independence assumptions,
the high-confidence data from one view, in addition
to being highly precise, is unbiased when added to
the training set for the other view. We could not
apply co-training for semi-supervised named entity
recognition because of the difficulty of finding infor-
mative yet class-conditionally independent feature
sets.
Collins et al(Collins and Singer, 1999) proposed
two algorithms for NER by modifying Yarowsky?s
method (Yarowsky, 1995) and the framework sug-
gested by (Blum and Mitchell, 1998). However, all
their features are at the word sequence level, instead
of at the token level. At the token level, the seed
rules they proposed do not necessarily work. In ad-
dition, parsing sentences into word sequences is not
a trivial task, and also not necessary for NER, in our
opinion.
Jiao et al propose semi-supervised conditional
random fields (Jiao et al, 2006) that try to maxi-
mize the conditional log-likelihood on the training
data and simultaneously minimize the conditional
entropy of the class labels on the unlabeled data.
This approach is reminiscent of the semi-supervised
learning algorithms that try to discourage the bound-
ary from being in regions with high density of unla-
beled data. The resulting objective function is no
longer convex and may result in local optima. Our
approach in contrast avoids changing the CRF train-
ing procedure, which guarantees global maximum.
3 Named Entity Recognition
As long as independent evidence exists for one type
of NE, our method can be directly applied to classify
such NE. As an example, we demonstrate how to ap-
ply our method to classify three types of NEs: orga-
nization (ORG), person (PER), and location (LOC)
since they are the most common ones. A non-NE is
annotated as O.
3.1 Conditional Random Fields for NER
We use CRF to perform classification in our frame-
work. CRFs are undirected graphical models trained
to maximize the conditional probability of a se-
quence of labels given the corresponding input se-
quence. Let X , X = x1...xN , be an input sequence,
and Y , Y = y1....yN , be the label sequence for the
input sequence. The conditional probability of Y
given X is:
P (Y |X) = 1Z(X) exp(
N?
n=1
?
k
?kfk(yn?1, yn, X, n))
(1)
where Z(X) is a normalization term, fk is a feature
function, which often takes a binary value, and ?k
is a learned weight associated with the feature fk.
The parameters can be learned by maximizing log-
likelihood ` which is given by
` = ?
i
logP (Yi|Xi)?
?
k
?2k
2?2k
(2)
where ?2k is the smoothing (or regularization) pa-
rameter for feature fk. The penalty term, used for
regularization, basically imposes a prior distribution
on the parameters.
It has been shown that ` is convex and thus a
global optimum is guaranteed (McCallum, 2003).
Inferring label sequence for an input sequence X
involves finding the most probable label sequence,
Y ? = argmax
Y
P (Y |X), which is done by the
Viterbi algorithm (Forney, 1973).
3.2 Features for NER
One big advantage of CRF is that it can naturally
represent rich domain knowledge with features.
3.2.1 Standard Features
Part of the features we used for our CRF classifier
are common features that are widely used in NER
(McCallum and Li, 2003), as shown below.
1) Lexicon. Each token is itself a feature.
2) Orthography. Orthographic information is
used to identify whether a token is capitalized, or
an acronym, or a pure number, or a punctuation, or
has mixed letters and digits, etc.
3) Single/multiple-token list. Each list is a collec-
tion of words that have a common sematic meaning,
such as last name, first name, organization, company
suffix, city, university, etc.
60
4) Joint features. Joint features are the conjunc-
tions of individual features. For example, if a token
is in a last name list and its previous token is in a
title list, the token will have a joint feature called as
Title+Name.
5) Features of neighbors. After extracting the
above features for each token, its features are then
copied to its neighbors (The neighbors of a token in-
clude the previous two and next two tokens) with a
position id. For example, if the previous token of a
token has a feature ?Cap@0?, this token will have a
feature ?Cap@-1?.
3.2.2 Label Features
One unique and important feature used in our al-
gorithm is called Label Features. A label feature
is the output label of a token itself if it is known.
We designed some simple high-precision rules to
classify each token, which take precedence over the
CRF. Specifically, if a token does not include any
uppercase letter, is not a number, and it is not in the
nocap list (which includes the tokens that are not
capitalized but still could be part of an NE, such as
al, at, in, -, etc), the label of this token is ?O?.
Table 1: An example of extracted features
Tokens Feature
Monday W=Monday@0 O@0
vice W=vice@0 O@0
chairman W=chairman@0 title@0 O@0
Goff W=Goff@0 CAP@0 Lastname@0
W=chairman@-1 title@-1 O@-1
W=vice@-2 O@-2
W=said@1 O@1 W=it@2 O@2
said W=said@0 O@0
the W=it@0 O@0
company W=company@0 O@0
In addition, if a token is surrounded by ?O? to-
kens and is in a Stopword list, or in a Time list (a
collection of date, time related tokens), or in a no-
cap list, or a nonNE list (a collection of tokens that
are unlikely to be an NE), or a pure number, its label
is ?O? as well. For example, in the sentence ?Ford
has said there is no plan to lay off workers?, all the
tokens except ?Ford? have ?O? labels. More rules
can be designed to classify NE labels. For example,
if a token is in an unambiguousORG list, it has a
label ?ORG?.
For any token with a known label, unless it is a
neighbor of a token with its label unknown (i.e., not
pretagged with high precision), its features include
only its lexicon and its label itself. No features will
be copied from its neighbors either. Table 1 gives
an example to demonstrate the features used in our
algorithm. For the sentence ?Monday vice chairman
Goff said the company ...?, only ?Goff? includes its
own features and features copied from its neighbors,
while most of the other tokens have only two fea-
tures since they are ?O? tokens based on the high-
precision rules.
Usually, more than half the tokens will be classi-
fied as ?O?. This strategy greatly saves feature ex-
traction time, training time, and inference time, as
well as improving the accuracy of the model. Most
importantly, this strategy is necessary in the semi-
supervised learning, which will be explained in the
next section.
4 Semi-supervised Learning Algorithm
Our semi-supervised algorithm is outlined in Ta-
ble 2. We assume that we have a small amount of
labeled data L and a classifier Ck that is trained on
L. We exploit a large unlabeled corpus U from the
test domain from which we automatically and grad-
ually add new training data D to L, such that L
has two properties: 1) accurately labeled, meaning
that the labels assigned by automatic annotation of
the selected unlabeled data are correct, and 2) non-
redundant, which means that the new data is from
regions in the feature space that the original training
set does not adequately cover. Thus the classifier Ck
is expected to get better monotonically as the train-
ing data gets updated.
Table 2: The semi-supervised NER algorithm
Given:
L - a small set of labeled training data
U - unlabeled data
Loop for k iterations:
Step 1: Train a classifier Ck based on L;
Step 2: Extract new data D based on Ck;
Step 3: Add D to L;
At each iteration, the classifier trained on the pre-
vious training data (using the features introduced in
the previous section) is used to tag the unlabeled
data. In addition, for each O token and NE seg-
ment, a confidence score is computed using the con-
61
strained forward-backward algorithm (Culotta and
McCallum, 2004), which calculates the LcX , the sum
of the probabilities of all the paths passing through
the constrained segment (constrained to be the as-
signed labels).
One way to increase the size of the training data
is to add all the tokens classified with high confi-
dence to the training set. This scheme is unlikely
to improve the accuracy of the classifier at the next
iteration because the newly added data is unlikely
to include new patterns. Instead, we use the high
confidence data to tag other data by exploiting inde-
pendent features.
? Tagging ORG
If a sequence of tokens has been classified as
?ORG? with high confidence score (> T )1,
we force the labels of other occurrences of the
same sequence in the same document, to be
?ORG? and add all such duplicate sequences
classified with low confidence to the training
data for the next iteration. In addition if a high
confidence segment ends with company suf-
fix, we remove the company suffix and check
the multi-mentions of the remaining segment
also. In addition to that, we reclassify the sen-
tence after removing the company suffix and
check if the labels are still the same with high-
confidence. If not, the sequence will be added
to the training data. As shown in Example 4,
?Safeway shares ticked? is added to training
data because ?Safeway? has low confidence af-
ter removing ?Inc.?.
Example 4:
High-confidence ORG: Safeway Inc. shares
ticked up ...
Low-confidence ORG:
1) Safeway shares ticked up ...
2) Wall Street expects Safeway to post earnings
...
? Tagging PER
If a PER segment has a high confidence score
and includes at least two tokens, both this seg-
1Through the rest of the paper, a high confidence score
means the score is larger than T. In our experiments, T is set
as 0.98. A low confidence score means the score is lower than
0.8.
ment and the last token of this segment are
used to find their other mentions. Similarly,
we force their labels to be PER and add them
to the training data if their confidence score is
low. However, if these mentions are followed
by any company suffix and are not classified
as ORG, their labels, as well as the company
suffix are forced to be ORG (e.g., Jefferies &
Co.). We require the high-confidence PER seg-
ment to include at least two tokens because the
classifier may confuse single-token ORG with
PER due to their common context. For ex-
ample, ?Tesoro proposed 1.63 billion purchase
of...?, Tesoro has high-confidence based on the
model, but it represents Tesoro Corp in the doc-
ument and thus is an ORG.
In addition, the title feature can be used simi-
larly as the company suffix features. If a PER
with a title feature has a high confidence score,
but has a low score after the title feature is
removed, the PER and its neighbors will be
put into training data after removing the title-
related tokens.
Example 5:
High-confidence PER:
1)Investor AB appoints Johan Bygge as CFO...
2)He is replacing Chief CEO Avallone...
Low-confidence PER:
1) Bygge is employed at...
2) He is replacing Avallone ...
(It is obvious for a human-being that Bygge is
PER because of the existence of ?employed?.
However, when the training data doesn?t in-
clude such cases, the classifier just cannot rec-
ognize it.)
? Tagging LOC
The same approach is used for a LOC segment
with a high confidence score. We force the la-
bels of its other mentions to be LOC and add
them to the training data if their confidence
score is low. Again, if any of these mentions
follows or is followed by an ORG segment with
a high confidence score, we force the labels to
be ORG as well. This is because when a LOC
is around an ORG, the LOC is usually treated
as part of an ORG, e.g., Google China.
62
Example 6:
High-confidence LOC: The former Soviet re-
public of Azerbaijan is...
Low-confidence PER:
Azerbaijan energy reserves better than...
Change LOC to ORG: shareholders of the
Chicago Board of Trade...
? Tagging O
Since all the NE segments added to the train-
ing data have low confidence scores based on
the original model, and especially since many
of them were incorrectly classified before cor-
rection, these segments form good training data
candidates. However, all of them are positive
examples. To balance the training data, we
need negative examples as well. If a token is
classified as ?O? with high confidence score
and does not have a label feature ?O?, this to-
ken will be used as a negative example to be
added to the training data.
Since the features of each token include the fea-
tures copied from its neighbors, in addition to those
extracted from the token itself, its neighbors need to
be added to the training set alo. If the confidence of
the neighbors are low, the neighbors will be removed
from the training data after copying their features to
the token of interest. If the confidence scores of the
neighbors are high, we further extend to the neigh-
bors of the neighbors until low-confidence tokens
are reached. We remove low-confidence neighbors
in order to reduce the chances of adding training ex-
amples with false labels.
Table 3: Step 2 of the semi-supervised algorithm
Step 2: Extract new data D based on Ck
i) Classify kth portion of U and compute confidence
scores;
ii) Find high-confidence NE segments and use them
to tag other low-confidence tokens
iii) Find qualified O tokens
iv) Extract selected NE and O tokens as well as
their neighbors
v) Shuffle part of the NEs in the extracted data
vi) Add extracted data to D
Now we have both negative and positive training
examples. However, one problem with the positive
data is that the same NE may appear too many times
since the multi-mention property is used. For ex-
ample, the word ?Citigroup? may appear hundreds
of times in recent financial articles because of the
subprime crisis. To account for this bias in the data
we randomly replace these NEs. Specifically, we
replace a portion of such NEs with NEs randomly
chosen from our NE lists. The size of the portion is
decided by the ratio of the NEs that are not in our
NE list over all the NEs in the gold data.
Table 3 summarizes the key sub-steps in Step 2
of the algorithm. At each step, more non-redundant
and high-accuracy data is added into the training set
and thus improves the model gradually.
5 Experiments
The data set used in the experiments is explained
in Table 4. Although we have 1000 labeled news
documents from the Thomson Financial (TF) News
source, only 60 documents are used as the initial
training data in our algorithm. For the evaluation,
the gold data was split into training and test sets as
appropriate. The toolbox we used for CRF is Mallet
(McCallum, 2002).
Table 4: Data source. Tokens include words, punctuation
and sentence breaks.
Gold Data 1000 docs from TF news
(around 330 tokens per doc)
Unlabeled Corpus 100,000 docs from TF news
0.5 0.6 0.7 0.8 0.9
93
94
95
96
97
98
Confidence Score Threshold
Acc
ura
cy
Figure 1: Token accuracy vs confidence score.
We first investigated our assumption that a high
confidence score indicates high classification accu-
racy. Figure 1 illustrates how accuracy varies as
CRF confidence score changes when 60 documents
63
are used as training data and the remaining are used
as testing data. When the threshold is 0.98, the token
accuracy is close to 99%. We believe this accuracy is
sufficiently high to justify using the high confidence
score to extract tokens with correct labels.
Table 5: Precision and recall of the automatically ex-
tracted training data
NE Precision% Recall% F-score%
LOC 94.5 96.8 95.6
ORG 96.6 93.4 94.9
PER 95.0 89.6 92.2
We wished to study the accuracy of our training
data generation strategy from how well it does on the
gold data. We treat the remaining gold data (except
the data trained for the initial model) as if they were
unlabeled, and then applied our data extraction strat-
egy on them. Table 5 illustrates the precision and re-
call for the three types of NEs of the extracted data,
which only accounts for a small part of the gold data.
The average F-score is close to 95%. Although the
precision and recall are not perfect, we believe they
are good enough for the training purpose, consider-
ing that human tagged data is seldom more accurate.
We compared the semi-supervised algorithm with
a supervised algorithm using the same features. The
semi-supervised algorithm starts with 60 labeled
documents (around 20,000 tokens) and ends with
around 1.5 million tokens. We trained the supervised
algorithm with two data sets: using only 60 docu-
ments (around 20,000 tokens) and using 700 doc-
uments (around 220,000 tokens) respectively. The
reason for the choice of the training set size is
the fact that 20,000 tokens are a reasonably small
amount of data for human to tag, and 220,000 tokens
are the amount usually used for supervised algo-
rithms (CoNLL 2003 English NER (Sang and Meul-
der, 2003) training set has around 220,000 tokens).
Table 6 illustrates the results when 300 docu-
ments are used for testing. As shown in Table 6,
starting with only 6% of the gold data, the semi-
supervised algorithm achieves much better results
than the semi-supervised algorithm when the same
amount of gold data is used. For LOC, ORG, and
PER, the recall increases 5.5, 16.8, and 8.2 respec-
tively, and the precision increases 2.4, 1.5, and 6.8
respectively. Even compared with the model trained
with 220,000 tokens, the semi-supervised learning
algorithm is better. Especially, for PER, the pre-
cision and recall increase 2.8 and 4.6 respectively.
Figure 2 illustrates how the classifier is improved at
each iteration in the semi-supervised learning algo-
rithm.
Table 6: Classification results. P/R represents Preci-
sion/Recall. The numbers inside the parentheses are the
result differences when the model trained from 60 docs is
used as baseline.
Training Data P/R(LOC) P/R(ORG) P/R(PER)
60 docs 88.1/85.6 86.0/64.2 74.5/81.2
700 docs 91.2/88.2 90.5/76.6 78.3/84.8
(3.1/3.6) (4.5/12.4) (3.8/3.6)
semi-supervised 90.5/91.1 87.5/81.0 81.1/89.4
(60 docs) (2.4/5.5) (1.5/16.8) (6.6/8.2)
1 2 3 4 5 6 7 8 9
82
83
84
85
Iteration
Ove
rall 
F?S
core
Figure 2: Overall F-score vs iteration numbers
Table 7 compares the results when the multi-
mention property is also used in testing as a high-
precision rule. Comparing Table 7 to Table 6, we
can see that with the same training data, using multi-
mention property helps improve classification re-
sults. However, this improvement is less than that
obtained by using this property to extract training
data thus improve the model itself. (For a fair com-
parison, the model used in the semi-supervised algo-
rithm in Table 6 only uses multi-mention property to
extract data.)
Our last experiment is to test how this method can
be used when the initial gold data and the testing
data are from different domains. We use the CoNLL
2003 English NER (Sang and Meulder, 2003) train-
ing set as the initial training data, and automatically
extract training data from the TF financial news cor-
pus. The CoNLL data is a collection of news wire
64
documents from the Reuters Corpus, while TF data
includes financial-related news only. Table 8 illus-
trates the results. As shown in the table, with only
CoNLL data, although it contains around 220,000
tokens, the results are not better than the results
when only 60 TF docs (Table 6) are used for train-
ing. This indicates that data from different domains
can adversely affect NER accuracy for supervised
learning. However, the semi-supervised algorithm
achieves reasonably high accuracy. For LOC, ORG,
and PER, the recall increases 16, 20.3, and 4.7 re-
spectively, and the precision increases 4.5, 5.5, and
4.7 respectively. Therefore our semi-supervised ap-
proach is effective for situation where the test and
training data are from different sources.
Table 7: Classification results when multi-mention prop-
erty (M) is used in testing
Trainig Data P/R(LOC) P/R(ORG) P/R(PER)
60 docs +M 89.9/87.6 82.4/71.4 78.2/87.3
700 docs+M 91.2/89.1 90.2/78.3 79.4/91.1
(1.3/1.5) (7.8/6.9) (1.2/3.8)
semi-supervised 90.0/91.0 86.6/82.4 81.3/90.6
+M (60 docs) (1.1/3.4) (4.2/11.0) (3.1/3.3)
Table 8: Classification results trained on CoNLL data and
test on TF data. Training data for the semi-supervised
algorithm are automatically extracted using both multi-
mention and high-precision context from TF corpus.
Training Data P/R(LOC) P/R(ORG) P/R(PER)
CoNLL 85.6/74.7 75.2/65.9 72.4/85.2
Semi-supervised 90.1/90.7 81.7/86.2 77.1/90.5
(CoNLL) (4.5/16) (5.5/20.3) (4.7/4.7)
6 Conclusion
We presented a simple semi-supervised learning al-
gorithm for NER using conditional random fields
(CRFs). In addition we proposed using high preci-
sion label features to improve classification accuracy
as well as to reduce training and test time.
Compared to other semi-supervised learning al-
gorithm, our proposed algorithm has several advan-
tages. It is domain and data independent. Although
it requires a small amount of labeled training data,
the data is not required to be from the same domain
as the one in which are interested to tag NEs. It can
be applied to different types of NEs as long as in-
dependent evidence exists, which is usually avail-
able. It is simple and, we believe not limited by the
choice of the classifier. Although we used CRFs in
our framework, other models can be easily incorpo-
rated to our framework as long as they provide accu-
rate confidence scores. With only a small amount of
training data, our algorithm can achieve a better NE
tagging accuracy than a supervised algorithm with a
large amount of training data.
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. Proceedings of the
Workshop on Computational Learning Theory, pages
92?100.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
A. Culotta and A. McCallum. 2004. Confidence estima-
tion for information extraction. HLT-NAACL.
G. D. Forney. 1973. The viterbi algorithm. Proceedings
of the IEEE, 61(3):268?278.
Feng Jiao, Shaojun Wang, Chi H. Lee, Russell Greiner,
and Dale Schuurmans. 2006. Semi-supervised condi-
tional random fields for improved sequence segmen-
tation and labeling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics,
pages 209?216, July.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
CoNLL.
A.K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In EMNLP.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. CoNLL, pages
142?147.
Yingchuan Wong and Hwee Tou Ng. 2007. One class
per named entity: Exploiting unlabeled text for named
entity recognition. IJCAI, pages 1763?1768.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189?196.
65
