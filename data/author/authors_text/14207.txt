Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1752?1763, Dublin, Ireland, August 23-29 2014.
A Novel Distributional Approach to Multilingual Conceptual Metaphor
Recognition
Michael Mohler and Bryan Rink and David Bracewell and Marc Tomlinson
Language Computer Corp.
Richardson, Texas, USA
{michael,bryan,david,marc}@languagecomputer.com
Abstract
We present a novel approach to the problem of multilingual conceptual metaphor recognition.
Our approach extends recent work in conceptual metaphor discovery by combining a complex
methodology for facet-based concept induction with a distributional vector space model of lin-
guistic and conceptual metaphor. In the evaluation of our system in English, Spanish, Russian,
and Farsi, we experiment with several state-of-the-art vector space models and demonstrate a
clear benefit to the fine-grained concept representation that forms the basis of our methodology
for conceptual metaphor recognition.
1 Introduction
The role of metaphor in language has been defined by Lakoff et al. (1980; 1993) as a cognitive phe-
nomenon which operates at the level of mental processes, whereby one concept or domain is viewed
systematically in terms of another. For example, the phrase ?to cure poverty? is a metaphor which subtly
conveys a wide variety of information to the listener. In order to mentally process this phrase, we must
first recognize that a metaphor is being used and that ?cure? (as a medical term) is being used figura-
tively. Then, we assume some relationship between ?poverty? and ?things that can be medically cured?
which leads to the conceptual mapping ?POVERTY as DISEASE.? This conceptual mapping enables the
listener to transfer a variety of properties and associations between the two concepts, such as their as-
sociation with a feeling of helplessness, the existence of sustained efforts to end them, the potential for
them to spread, and their mutual relationship with ill-health and death. Therefore, by identifying the con-
ceptual domains associated with this linguistic metaphor, we are able to reason about the target domain
(POVERTY) using concepts and terms associated with the source domain (DISEASE).
Any natural language processing system capable of processing metaphor in text with human-level
competence must, therefore, overcome three problems in sequence:
1. the identification of metaphorical expressions (also known as linguistic metaphors (LMs))
2. the discovery of a conceptual domain mapping or conceptual metaphor (CM) which consists of
(a) the conceptual domain of the metaphor target (e.g., POVERTY); and
(b) the conceptual domain of the metaphor source (e.g., DISEASE)
3. the real-world interpretation of the metaphorical text which uses the conceptual metaphor frame-
work to transfer knowledge between the source and target domains.
While a significant amount of recent work has presented interesting and promising methodologies for
multilingual LM identification (Shutova and Sun, 2013; Wilks et al., 2013; Strzalkowski et al., 2013),
the work presented in this paper is focused on (2), the problem of multilingual CM recognition, which
will be made to serve as the foundation for a more fine-grained interpretation of metaphor.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1752
We cast the CM recognition process as a two-part methodology which (a) selects the target domain
associated with a particular LM that has been detected; and (b) determines the source domain to which
it should be mapped in order to produce a satisfactory interpretation. In this work, we assume that the
target domains are known and belong to one of the following conceptual spaces: POVERTY, WEALTH, or
TAXATION. Pragmatically speaking, research in CM recognition presupposes some methodology for LM
identification, and to this end, we have employed an existing state-of-the-art LM identification system
which has been developed to detect linguistic metaphors in four languages: English, Spanish, Russian,
and Farsi (Bracewell et al., 2014).
In order to generate a CM which can serve as the basis for an interpretation of an LM, we have
developed an approach that is based on the following hypotheses:
CONCEPTUAL HYPOTHESIS: When an LM has been identified as a pair of lexical items that
represent the source (e.g., ?cure?) and the target (e.g., ?poverty?), we can generate a conceptual
mapping by selecting the conceptual domains that are, a priori, the most likely for the source and
target lexemes.
1
DISTRIBUTIONAL HYPOTHESIS: It is possible to decide which conceptual space better repre-
sents a given lexeme by
1. expanding the lexical space with additional terms (which we call ?grammatical co-occurrents?)
that are strongly associated with the lexeme through grammatical relations such as AGENT,
PATIENT, INSTRUMENT, and ATTRIBUTE;
2. using these lexical expansions to produce distributional vectors; and
3. uncovering the selectional constraints of particular domain facets by clustering the distribu-
tional vectors within a semantic space.
DOMAIN HYPOTHESIS: The grammatical co-occurrents of the LM are themselves very likely to
belong to the same conceptual domain as the lexeme (e.g., ?cure patient?, ?cured of AIDS?, and
?doctor cured?).
MAPPING HYPOTHESIS: The semantic space representations of both the LM source and its gram-
matically associated terms can be used to produce mappings into a high dimensional space in which
source domains are known to exist.
While other computational linguistics research in metaphor has made use of the CONCEPTUAL and
DISTRIBUTIONAL hypotheses, to our knowledge the DOMAIN and MAPPING hypotheses have not
yet been explored in combination with a distributional approach.
The remainder of this work is organized as follows. In Section 2, we discuss related work in the field
of metaphor interpretation and unsupervised concept induction. In Section 3, we introduce the overall
architecture of our CM recognition system. In Section 4, we describe our method for representing lexical
items and conceptual metaphors in a distributional vector space. Then, in Section 5, we explain our
methodology for creating and ranking clusters of LM co-occurrents which are then mapped to conceptual
metaphors within our vector space. In Section 6, we describe our experimental setup and provide the
results of our experiments. Finally, in Section 7 we present our conclusions.
2 Related Work
Research in metaphor processing can broadly be divided into two categories: metaphor identification and
metaphor interpretation. Although some recent work on metaphor interpretation has skirted the issue of
conceptual metaphor entirely by casting the problem of metaphor interpretation as an instance of lexical
paraphrase (Shutova, 2010; Bollegala and Shutova, 2013) or textual entailment (Mohler et al., 2013), the
mapping and modeling of conceptual metaphors has historically served as an important foundation for
1
If the target domains are pre-selected, this hypotheses is reduced to selecting only the most likely source domain.
1753
more robust interpretation of metaphor. Indeed, a significant amount of research in metaphor interpreta-
tion has been concentrated on the development of highly-structured, manually curated representations of
both the CM source and CM target domains. Notable in this regard are the KARMA system (Feldman
and Narayanan, 2004) which was designed to simulate neurological modeling of verbs ? both abstract
and metaphorical ? and the ISOMETA system (Beust et al., 2003) which made use of differential tables
of CM domain lexical items to drive their metaphor interpretation process. The CorMet system (Ma-
son, 2004) sought to model conceptual metaphors by detecting individual source-target mappings that
provide evidence for a known CM by quantifying the overlap between clusters of terms with a strong
selectional preference to the most representative verbs within the source and target domains. After a man-
ual inspection of the source/target cluster pairs across domains, the directionality and the systematicity
of these underlying conceptual mappings were quantified in order to produce an overall confidence in
the mapping. As part of their development of the Hamburg Metaphor Database (HMD), Reining and
L?onneker-Rodman (2007) performed a a manual categorization of lexical items into conceptual source
domains with a facet-level granularity and enriched their domains using a WordNet-based lexical expan-
sion. In the same vein, Chung et al. (2005) chose to model source domains by expanding their lexical
items by exploiting the links between WordNet glosses and the SUMO ontology.
In recent years, however, research has focused on automating the modeling and classification of con-
ceptual metaphors as much as possible in order to encourage the scaling up of metaphor research in
general. Veale and Hao (2008), as part of the Talking Points system, developed what they refer to as a
Slipnet which defines linked chains of meaning that connect a source to a target through shared (or re-
lated) attributes and actions. As a step in this process, they combined WordNet relations with pragmatic
relations extracted from text and clustered nouns according to their relation (and attribute) similarity in
order to define a weak conceptual mapping within the clusters. In a similar way, Shutova et al. (2010),
beginning with a seed set of noun/verb linguistic metaphor pairs, performed spectral clustering on a large
set of nouns and verbs in order to predict metaphors which participate in the same conceptual metaphor
mapping. In particular, she modeled verbs according to their subcategorization frames parameterized by
a model of their selectional preferences, while nouns were modeled according to the verbs with which
they frequently co-occurred in a dependency relation.
More recently, Gandy et al. (2013) approached the CM discovery problem as a set covering problem.
For a given nominal target lexeme, they began by finding all facets (i.e., verbs/adjectives) that share
a positive PMI with the target. Then, they would find the set of nouns that also have a positive PMI
with those facets, compute their confidence in each association, and heuristically select pairs of concepts
(defined as rooted WordNet synset trees) which subsume a large percentage of those nouns and cover a
large portion of the overlapping facets. Similarly, Shutova and Sun (2013) detect conceptual mappings by
performing hierarchical graph factorization clustering on a graph in which the vertices are defined to be
nouns (i.e., concepts) and the edges are weighted using Jensen-Shannon Divergence. For a given input
LM source, its likely conceptual metaphors are then discovered by determining its non-literal cluster
membership. Finally, Strzalkowski et al. (2013) discovered terms (literal and metaphoric) which often
co-occur with an LM source in a corpus and clustered those terms using WordNet and corpus statistics
to form ?ProtoSources? which could be further inspected to define CM source concepts.
Two vector-based approaches to concept representation are of particular interest in understanding the
present work. In the first of these, Sch?utze (1998) described an approach to word sense identification
using second-order co-occurrence vectors which were used to cluster first-order vectors of the in-context
terms into senses.
2
Lin (1998), in developing a methodology for evaluating the quality of thesauri,
defined a word vector space that moved beyond simple co-occurrence by integrating information about
the relations between the word and its co-occurrents. In particular, a word?s vector was defined by the
number of times that word occurred within a set of (word, relation, word) tuples. Our DepVec space
represents an extension to Lin?s space insofar as we incorporate additional information about relational
(i.e., selectional) preference.
2
While context is critical in word sense disambiguation, we hasten to point out that one mark of metaphoricity is its discon-
nect from the surrounding literal context.
1754
Figure 1: The architecture of our conceptual metaphor recognition system. This system takes a linguistic
metaphor as input, induces potential concepts using vector-space clustering, and maps these clusters onto
a conceptual metaphor domain.
3 A New Methodology for Conceptual Metaphor Recognition
Figure 1 shows the overall flow of our metaphor processing architecture. We begin with a set of doc-
uments gathered from a variety of online news-wire sources. These documents are fed to our state-
of-the-art LM detection system which employs a binary logistic regression classifier using a variety of
feature modules including imageability and concreteness estimation, topicality modeling, pattern match-
ing, semantic categorization, selectional preference violation, and source/target vector space similarity.
The methodology used in this system is beyond the scope of this work, but it is described in detail by
Bracewell et al. (2014). The LMs provided by the detection system are validated by a group of native-
language experts before being sent for CM recognition system for concept-level interpretation.
Once the LMs have been collected and validated, the CM recognition system begins by extracting,
weighting, and clustering the common grammatical contexts of the LM source term. By grammatical
context, we refer to the syntactic relations (along with their arguments) which have been found to fre-
quently co-occur with the LM source term in open text. In order to model this grammatical context, we
have syntactically parsed a wide collection of documents in each of our focus languages: English, Span-
ish, Russian, and Farsi. From these parsed documents, we have extracted the most common grammatical
co-occurrents of each word in the corpus along with the relation that connects them and the number
of times they are connected by that relation. For a given word, we refer to the set of its grammatical
co-occurrents as the ?concept candidates? associated with that word, as they represent potential concepts
within the same conceptual domain as the given word (the DOMAIN HYPOTHESIS). For example,
grammatical co-occurrents of the noun ?battle? would include many WAR concepts such as ?fought?,
?died in?, ?waged?, ?naval?, and ?losing?.
Since a conceptual domain is made up of several interacting concepts, we perform a clustering over
the grammatical co-occurrents to produce groups of terms which are likely to represent individual con-
cepts within a domain. The clustering is performed within a high-dimensional, distributional vector
space which we describe in Section 4. The clusters are then merged and aligned with a set of 51 pre-
defined source concept domains (see Table 1) that have been found to occur frequently in conceptual
metaphors about POVERTY, WEALTH, or TAXATION. For each of these known conceptual domains, we
have amassed a collection of lexical items for the purpose of modeling the domains and aligning them
to our automatically discovered domains. The collection of lexical items associated with each domain
have been further partitioned into three to five facets which provide a more fine-grained representation of
the domain. For instance, the conceptual domain of ABYSS as been subdivided into facets representing
1755
Full Source Concept List
A GOD COMPETITION ENSLAVEMENT LIGHT NATURAL PHYSICAL FORCE PORTAL
A RIGHT CONFINEMENT FOOD LOW POINT OBESITY RESOURCE
ABYSS CRIME FORCEFUL EXTRACTION MACHINE PARASITE SCHISM
ACCIDENT CROP GAME MAZE PATHWAY STRUGGLE
ADDICTION DARKNESS GEOGRAPHIC FEATURE MEDICINE PHYSICAL BURDEN VERTICAL SCALE
ANIMAL DESTROYER GOAL DIRECTED MONSTER PHYSICAL HARM VISION
BLOOD SYSTEM DISEASE HIGH POINT MORAL DUTY PHYSICAL LOCATION
BODY OF WATER ENABLER HUMAN BODY MOVEMENT PHYSICAL OBJECT
BUILDING ENERGY IMPURITY MOVEMENT ON A VERTICAL SCALE PLANT
Sample Lexical Items
ANIMAL bite, bark, claw, bird, beaver MEDICINE dosage, prescription, heal
ENSLAVEMENT servant, oppression, ruler STRUGGLE enemy, fight, combat, attack
Table 1: The 51 source conceptual domains along with some sample English lexical items for a subset
of them.
DEPTH (e.g., ?deep?, ?bottomless?), ENTRANCE (e.g., ?plunged into?, ?falling into?), and EXIT (e.g.,
?climb out of?).
3.1 Motivating Example
Table 2 shows a sample of the concept candidates associated with the word ?cure? along with the relation
that connects them. Our methodology for extracting these terms is discussed in Section 5.1.
nsubj
NIH, WHO, therapist, doctor, vaccine,
prep of
cancer, AIDS, HIV, malaria, influenza,
drug, medicine, chef, butcher seizures, allergies
dobj
cancer, polio, Goji Berries, man,
prep by
bone marrow transplant, spleen cells,
genetic defects, aging, infant, woman, acupuncture, smoking, salting,
depression, meat, fish, garlic doxycycline, drying, burying, dipping
prep without
surgery, operation, suppuration, salt
prep to
?1
need, project, brine, mineral,
chemotherapy, injections coalition, run, walk, salt, nitrite
prep in
mice, children, baby, spices, salt,
prep for
grinding, smoking, voyages, lox,
monkeys, drug trial, breakthrough, transportation, preservation, jerky,
brine, smokehouse, basement, fridge sausages, bacon, sale
Table 2: Terms that are frequently a part of the grammatical context of ?cure? along with their associated
relations
It is clear from the concept candidates shown that there are at least two coarse-grained senses of
?cure? present ? corresponding to the domains of MEDICINE and FOOD. Table 3 shows a sample
result of clustering these concept candidates. These clusters are organized according to their domain
with MEDICINE-related clusters in the left grouping, FOOD-related clusters in the top-right grouping,
and clusters not strongly related to either domain in the bottom-right grouping. Each row of the table
represents a single cluster. In addition, it can be observed that these clusters correspond to particular
semantic facets of the conceptual domain. For instance, there is a cluster that defines ?procedures which
result in medical cures? (?acupuncture?, ?surgery?, ?operation?, etc.), one that defines ?individuals who
cure food products? (?chef?, ?butcher?), and one that defines ?diseases that can (potentially) be cured?
(?cancer?, ?polio?, ?AIDS?, etc.). Our methodology for automatically inducing such clusters is described
in Section 5.2.
Once the clusters have been identified, they can be used to define a mapping from the original LM
(?cure?) onto a pre-defined set of CM source domains (the MAPPING HYPOTHESIS). In particular,
individual concept candidates are mapped to CM domains by calculating the distance between the can-
didate and one or more vectors representing each domain in a high-dimensional distributional vector
space.
4 Distributional Representations
Our method for identifying conceptual metaphor domains relies on determining when multiple words
should be grouped as belonging to the same conceptual class (the DISTRIBUTIONAL HYPOTHESIS).
Previous work in semantic similarity has shown two types of approaches to work well: (a) hand-coded
knowledge such as WordNet or SUMO, and (b) distributional approaches which rely on statistics of
1756
NIH, WHO, therapist, doctor chef, butcher
vaccine, drug, medicine, doxycycline project, coalition
spleen cells, bone marrow transplant meat, fish, sausages, jerky, bacon, lox
acupuncture, surgery, operation garlic, Goji Berries
chemotherapy, injections, suppuration smoking, salting, drying, dipping
HIV, malaria, influenza burying
cancer, polio, AIDS salt, brine, spices, nitrite, mineral
genetic defects, aging, depression smokehouse, basement, fridge
seizures, allergies run, walk
drug trial, breakthrough voyages, transportation
infant, man, woman, children, baby mice, monkeys
Table 3: Terms from Table 2 grouped into conceptual clusters ? one per line. These clusters are organized
according to their domain association: MEDICINE (left), FOOD (top-right), unclear (bottom-right).
word usage in corpora. We adopt the distributional approach in order to facilitate research in languages
(such as Farsi) for which coverage of existing knowledge bases is limited. The only requirements for our
approach are a corpus with documents written in that language and a syntactic parser for the language.
We use the Malt dependency parser to obtain syntactic parses for web documents in each language.
Table 2 of Section 3.1 shows some of the words which participate regularly with the word ?cure?
in a dependency relation. These syntactic contexts of the word ?cure? form the basis for one semantic
representation we use to find other similar words, which we will call DepVec. All of the dependency
relations for a word are used to form a vector-based distributional representation for that word. This
representation projects words which are semantically similar to one another onto vectors which are near
to each other in the vector space. In the following subsection, we describe DepVec along with LSA and
word2vec which are alternative vector space models of word meaning. These vector spaces are then used
to calculate similarities between words in order to cluster them and to align them with lexicons which
model our existing conceptual spaces.
4.1 Dependency Vectors (DepVec) space
In our DepVec vector space model, each word is represented by a vector whose elements correspond
to syntactic contexts of the word. Each element of the vector for word w corresponds to the fre-
quency of a unique dependency relation (w, r, w
?
) seen in the corpus. For example, if the relation
(whale, nsubj
?1
, swim) is extracted once, then the vector for ?whale? contains a 1 in the element
for (nsubj
?1
, swim) , and the vector for ?swim? contains a 1 for the element (nsubj, whale). This
representation corresponds that proposed by Lin (1998).
However, the use of raw frequency counts in these vectors leads to a situation in which words that
are more frequent in the corpus (e.g., ?of?, ?the?, ?one?) will have higher frequencies in the vectors by
chance alone, and so a high co-occurrence count for those words is not indicative of a significant relation
to the word. We overcome this limitation by replacing the raw frequency counts in each vector with their
corresponding G-test scores. The G-test is a measure of statistical significance for proportions, similar to
the Chi-square test, which measures the degree to which a particular triple (w, r, w
?
) was found to occur
more frequently than expected given all relations (w
??
, r, w
?
). If w
?
occurs far more often with w than
it does with other words, then it will receive a high G-test score for w. In particular, the G-test score is
computed according to the following equation:
G = 2
?
i
O
i
? ln(O
i
/E
i
)
where the index i ranges over the four cells of a 2x2 contingency table, O
i
is the observed count in cell
i, and E
i
is the expected count in the same cell.
1757
Language Source # Documents Language Source # Documents
English ClueWeb 13,361,743 Spanish ClueWeb 3,682,478
Russian ruWac 1,173,590 Farsi Online news sites 835,588
Table 4: Statistics of the corpora used to construct the vector space models
4.2 Latent Semantic Analysis (LSA)
While the DepVec model provides information about the immediate contexts a word can be expected
to occur in, it does not directly capture information about the broader contexts typical of that word,
such as topical information. Latent Semantic Analysis (LSA) is a well-studied model (Landauer and
Dumais, 1997) which does capture such topical information. The LSA model utilizes a singular value
decomposition of a TF-IDF weighted matrix representation of the term-document co-occurrences. Terms
and documents are then represented in a reduced dimensionality space using only the information from
the eigenvectors with the k largest eigenvalues.
4.3 Continuous skip-gram model (W2V)
Mikolov et al. (2013) recently presented a new method for determining distributional word representa-
tions based on a shallow neural network model. The values of the latent vector for each word are trained
to optimize prediction of the words within a 10 token window. This prediction is performed using the
term?s latent vector as the input to a series of log-linear classifiers with outputs which correspond to
probability distributions over the tokens within the context window. Each position in the context window
is assigned its own classifier weights, so that the model used for making predictions about words imme-
diately following the input term is different than the model which makes predictions about the words two
tokens after the term, and so on. Because these latent vector representations are in a low dimensionality
space (300 dimensions in our case), the training process will tend to move the representations for similar
words closer together in this space in order to maximize the predictive accuracy of their contexts.
One benefit of the continuous skip-gram model is that it creates representations which capture some
local context as in the DepVec model, which is required to make predictions about the previous and next
tokens. However, it must also encode some topical knowledge in order to make accurate predictions
about the words seven tokens away. Therefore, using the latent term representations from the continuous
skip-gram model as a vector space puts it in a convenient position in between the two others we presented.
4.4 Corpus Processing
The vector models described above were developed using web-scale corpora collected from a combina-
tion of frequently used NLP corpora and web crawls on news websites. Table 4 indicates the number of
documents used for each language along with their source. These corpora were part-of-speech tagged
with in-house POS taggers for English and Spanish, TreeTagger
3
for Russian, and hunpos
4
for Farsi. The
open-source MaltParser was used to produce dependency parses for all four languages (Nivre, 2003). De-
pendency counts for all words occurring fewer than 40 times and for triples occurring fewer than three
times were discarded to minimize noise.
5 Concept Induction and CM Recognition
In Section 4, we described our DepVec representation of terms as vectors in a high-dimensional dis-
tributional space. These vector representations encode both the dominant grammatical contexts of a
term as well as the selectional preference information associated with it in the form of G-test scores. In
this section, we describe our methodology for inducing conceptual domains for a linguistic metaphor
by adapting techniques for unsupervised word-sense induction (Erk and Pad?o, 2008; Korkontzelos and
Manandhar, 2010; Hope and Keller, 2013). In particular, we induce conceptual domains in an uncon-
strained manner by extracting the grammatical co-occurrents of an LM source term (i.e., the ?concept
candidates?) and clustering them into semantically-related concept clusters. Both the clusters and our
3
http://www.cis.uni-muenchen.de/
?
schmid/tools/TreeTagger/
4
http://code.google.com/p/hunpos/
1758
given source domains are then mapped into a distributional vector space, allowing us to compute cluster-
to-domain scores. Finally, each source domain is assigned a score based on its affinity to each individual
cluster with these affinity scores weighted according to cluster quality. This results in an overall weighted
ranking of the given source conceptual domains for the linguistic metaphor.
5.1 Extracting Concept Candidates
Given a linguistic metaphor which consists of a metaphor source, s (e.g., ?cure?), and a metaphor target,
t (e.g., ?poverty?), our system extracts a set of terms (i.e., ?concept candidates?) from the typical gram-
matical contexts of s as found in the web-scale corpus described in Section 4.4. In order to extract these
candidates, we first determine the syntactic relation, r, which exists between s and t. This relation is the
key point of interaction between the domains of the source and the target for the given LM and, as such,
it provides an indication of which terms will contribute the most to our understanding of the underly-
ing conceptual mapping. In addition, we make use of a predefined set of relations that are semantically
meaningful ? specifically the subjects and objects of verbs (i.e., ?nsubj?, ?nsubjpass?, and ?dobj?),
5
at-
tributes and verbs associated with nouns (i.e., ?amod?, ?dobj
?1
?, ?nsubj
?1
?, and ?nsubjpass
?1
?), the
terms modified by adjectives or adverbs (i.e., ?advmod
?1
? and ?amod
?1
?), and prepositional relations
(e.g., ?prep by?, ?prep of?, ?prep for?). Using this set of relations, R, we extract the set of candidate
terms, X , that have been found to co-occur with the term s within some relation r
i
? R in the prepro-
cessed, web-scale corpus described in Section 4.4 such that X = {x|(s, r
i
, x)exists in the corpus}.
To improve the quality of our extracted candidates, we apply three criteria to isolate those that best
exemplify the underlying non-metaphorical senses of s. First, we anticipate that any term in X which
does not co-occur with s at least k times will not be informative,
6
and so we remove such terms from
further processing. Next, we predict that poorly imageable terms (i.e., highly abstract terms) are likely to
represent metaphorical usages of s and so are unlikely to be integral to a given literal source domain, so
these are filtered out as well.
7
Finally, to improve our ability to map these candidates into a conceptual
domain, we remove terms that are not significantly related to any of our provided source domains (i.e.,
those that are off-topic) along with terms that are strongly related to multiple source domains (i.e., those
that are ambiguous) as these provide little evidence to distinguish the most appropriate concept for the
given LM.
8
We determine the relatedness of a term to a source domain by measuring the similarity of
the term and domain vectors in our distributional space as described in Section 5.3.
5.2 Clustering Concept Candidates
Once the candidates have been extracted, they are clustered using a hierarchical agglomerative clustering
algorithm with the distance metric defined as the cosine distance between the vectors within one of our
distributional vector spaces. Each cluster is then assigned a quality score based on its size (to prefer large
clusters with a large amount of semantic evidence), average internal distance (to prefer tighter clusters),
and co-occurrence frequency with the LM source (to prefer more closely related terms). Formally, we
define the weight associated with a given cluster using the following equation:
w(C) = (1? IDIST (C)) ? (S2(C) + FREQ(C) ? (1 + S2(C)))
S2(C) =
max(SIZE(C)
2
, k)
k
where IDIST (C) represents the average vector distance between all pairs of terms in cluster C,
FREQ(C) represents the total co-occurrence frequency of the terms in C with the original LM,
5
These dependency relation types come from the MaltParser.
6
We empirically set k to 3.
7
We estimate candidate imageability by combining the scores of the candidate?s most distributionally similar words for
which an imageability score is available in the MRC psycholinguistics database (Coltheart, 1981) using the ranked weighting
methodology described in Mohler et al. (2014).
8
Note that filtering by conceptual domain relatedness is only necessary when mapping the induced concepts to a predefined
set of source concepts.
1759
SIZE(C) represents the number of unique terms in C, and k is a tuning parameter meant to favor
large clusters.
9
Singleton clusters are discarded.
5.3 Assigning Domain Scores to Concept Candidates
We propose two methods for calculating domain scores for candidates ? one which attempts to compare
candidate vectors to a source domain directly, and and another which attempts to compare them to indi-
vidual facets of the domain. These two methods rely on representing sources [CentS], or facets [CentF],
as centroids which take the average of the vectors of each the lexemes assigned to that source (or facet).
Our three vector spaces ? DepVec, W2V, and LSA ? along with our two methods for mapping terms to
domains ? CentS and CentF ? correspond to six approaches to modeling a CM domain in some vector
space.
In each case, the result for a given candidate is a distribution over all source domain scores. This
distribution is then normalized by subtracting the mean score between the candidate vector and any of
the source concepts. Formally, we define the normalized distribution for concept candidate x as:
S(x,D
y
) = (1?DIST (x,D
y
))?
?
D
k
?D
(1?DIST (x,D
k
))
|D|
where D is defined as the set of all known source domains and DIST (x, d) is the cosine distance from
x to a CM domain d in one of our vector spaces.
5.3.1 Assigning Domain Scores to Clusters
Within a given cluster (found as described in Section 5.2), the individual concept domain scores can then
be combined to produce cluster-level domain scores. For a given cluster C
x
, the score associated with a
particular source domain D
y
is defined as follows:
S(C
x
, D
y
) =
N
?
i=1
S(C
xi
, D
y
)
?
i
where N represents the number of concepts in C
x
with a positive score for the domain D
y
, C
xi
is the
i-th highest score associated with any candidate in the cluster, and ? is a tuning parameter which bounds
the growth of the cluster-level score.
10
Any cluster with a maximum domain score that does not exceed
a threshold is discarded as being weakly related to any CM source domain.
5.3.2 Assigning Domain Scores to the Linguistic Metaphor
We then sum the cluster-level source domain scores, scaling each by its associated cluster quality weight
w(c) as computed in Section 5.2. By scaling cluster domain scores in this way, we ensure that the most
pure and discriminating clusters contribute the most to the overall LM domain scores. The final result
measuring the association between the given LM and the source domain D
y
is then defined as:
S(D
y
) =
?
C
x
?C
w(C
x
) ? S(C
x
, D
y
)
Applied across all known domains, we therefore produce a ranked and scored list of CM source do-
mains (i.e., a mapping) that are associated with the given linguistic metaphor and can be used to drive
more robust interpretation of the metaphor.
6 Evaluation
We evaluate two aspects of our end-to-end CM recognition system. First, we analyze the impact of our
choice of vector space. Specifically, we compare the use of our DepVec space to link concept candidates
9
In our experiments, k is set to 5.
10
We have used a value of ? = 2 which ensures that the result remains within the bounds [0.0,1.0].
1760
with source domains against two off-the-shelf vector space models ? the continuous skip-gram model
[W2V] (Mikolov et al., 2013)
11
and latent semantic analysis [LSA] (Landauer and Dumais, 1997). Both
alternative models were trained over the same corpus as in our DepVec space using a predefined number
of dimensions (300 for W2V; 400 for LSA). Second, we have experimented with two different metrics for
calculating the distance between a vector and a source concept ? the cosine distance to the source-level
centroid (CentS) and the cosine distance to the facet-level centroid (CentF).
Our evaluation dataset consists of a held out, unseen set of documents taken from a variety of news
articles, opinion pages, and blogs on the open web. These documents consist of 3 to 5 sentences each
and cover four of our focus languages.
12
They were then annotated by two native-proficiency speakers in
the following way. For each LM, they were instructed to choose the most closely related source concept
from our list of 51 provided. Any source concepts selected by at least one annotator were considered
correct. Since our CM recognition system produces a ranked list of source concepts, we report both the
accuracy associated with our top-ranked concept and the accuracy of the system when allowed to select
two.
Cluster Linking
English Spanish Russian Farsi
Vector Space Distance Acc@1 Acc@2 Acc@1 Acc@2 Acc@1 Acc@2 Acc@1 Acc@2
DepVec CentS 28.0% 44.1% 33.3% 43.4% 24.4% 32.6% 16.5% 27.5%
CentF 25.8% 40.9% 33.3% 49.4% 25.6% 34.9% 26.4% 40.7%
LSA CentS 34.4% 45.2% 31.0% 41.4% 27.9% 41.9% 22.0% 27.5%
CentF 38.7% 54.9% 27.6% 46.0% 29.1% 47.7% 31.9% 44.0%
W2V CentS 24.7% 36.6% 42.5% 55.2% 31.4% 43.0% 25.3% 34.1%
CentF 28.0% 44.1% 46.0% 58.6% 34.9% 48.8% 35.2% 48.4%
Table 5: The accuracy of our conceptual interpretation system. We experiment with three vector spaces
(LSA, W2V, and DepVec) and two source concept centroid representations ? source-level (CentS) and
facet-level (CentF).
These results indicate that the continuous skip-gram vector space [W2V] is well suited to the task of
cluster-level concept mapping, consistently and significantly outperforming both the LSA space and the
DepVec space in every language but English. We believe that this is a result of its probabilistic represen-
tation of local context which implicitly collects many of the same relations as the DepVec model while
incorporating the advantages associated with dimensionality reduction which has not been incorporated
into our DepVec model.
13
We further observe an unmistakable dominance of the facet-level centroid
representation over the source-level representation. Based on these results, we believe that we have suc-
cessfully demonstrated the contribution of our system?s vector-space clustering component which groups
concept candidates at a facet-level granularity.
7 Conclusion
In this paper, we have presented a novel approach to the problem of multilingual conceptual metaphor
recognition which combines facet-based concept induction with a distributional vector space represen-
tation of metaphor. We have experimentally demonstrated the advantage of our fine-grained concept
induction approach within a variety of vector space models, including our novel DepVec space. Taken
together, we hypothesize that a facet-level conceptual model represented in a relational context vec-
tor space will serve as a reliable foundation enabling high-quality metaphoric interpretation in future
metaphor research. Future work includes expanding the set of concept candidates through higher-order
dependency contexts, improved clustering techniques, and evaluating the induced clusters directly.
11
We make use of the implementation included as part of the gensim python package: http://radimrehurek.com/
gensim/
12
This dataset consists of the following counts of documents: English (92), Spanish (86), Russian (85), Farsi (90).
13
During our pilot experiments, we applied singular value decomposition (SVD) to the DepVec space without any significant
improvement to system performance.
1761
Acknowledgments
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via De-
partment of Defense US Army Research Laboratory contract number W911NF-12-C-0025. The U.S.
Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily representing the official policies or endorse-
ments, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.
References
Pierre Beust, St?ephane Ferrari, Vincent Perlerin, et al. 2003. NLP model and tools for detecting and interpreting
metaphors in domain-specific corpora. In Proceedings of the Corpus Linguistics 2003 conference, volume 16,
pages 114?123. Citeseer.
Danushka Bollegala and Ekaterina Shutova. 2013. Metaphor interpretation using paraphrases extracted from the
web. PloS one, 8(9):e74304.
D. Bracewell, M. Tomlinson, M. Mohler, and B. Rink. 2014. A tiered approach to the recognition of metaphor. In
Computational Linguistics and Intelligent Text Processing.
Siaw-Fong Chung, Kathleen Ahrens, and Chu-Ren Huang. 2005. Source domains as concept domains
in metaphorical expressions. International Journal of Computational Linguistics and Chinese Language
Processing, 10(4):553?570.
Max Coltheart. 1981. The MRC psycholinguistic database. The Quarterly Journal of Experimental Psychology,
33(4):497?505.
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897?906. As-
sociation for Computational Linguistics.
J. Feldman and S. Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and language,
89(2):385?392.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder, Newton Howard, Sergey Kanareykin, Moshe Koppel, Mark
Last, Yair Neuman, and Shlomo Argamon. 2013. Automatic identification of conceptual metaphors with limited
knowledge. In Twenty-Seventh AAAI Conference on Artificial Intelligence.
David Hope and Bill Keller. 2013. MaxMax: a graph-based soft clustering algorithm applied to word sense
induction. In Computational Linguistics and Intelligent Text Processing, pages 368?381. Springer.
Ioannis Korkontzelos and Suresh Manandhar. 2010. UoY: Graphs of unambiguous vertices for word sense in-
duction and disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
G. Lakoff and M. Johnson. 1980. Metaphors we live by, volume 111. Chicago London.
G. Lakoff. 1993. The contemporary theory of metaphor. Metaphor and thought, 2:202?251.
T.K. Landauer and S.T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis theory of acqui-
sition, induction, and representation of knowledge. Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Z.J. Mason. 2004. CorMet: A computational, corpus-based conventional metaphor extraction system.
Computational Linguistics, 30(1):23?44.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781.
Michael Mohler, Marc Tomlinson, and David Bracewell. 2013. Applying textual entailment to the interpretation
of metaphor. In IEEE Seventh International Conference on Semantic Computing (ICSC), pages 118?125. IEEE.
1762
Michael Mohler, Marc Tomlinson, David Bracewell, and Bryan Rink. 2014. Semi-supervised methods for expand-
ing psycholinguistics norms by integrating distributional similarity with the structure of WordNet. Language
Resources and Evaluation Conference 2014.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th
International Workshop on Parsing Technologies (IWPT. Citeseer.
Astrid Reining and Birte L?onneker-Rodman. 2007. Corpus-driven metaphor harvesting. In Proceedings of the
Workshop on Computational Approaches to Figurative Language, pages 5?12. Association for Computational
Linguistics.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised metaphor identification using hierarchical graph factorization
clustering. In Proceedings of NAACL-HLT, pages 978?988.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor identification using verb and noun clustering. In
Proceedings of the 23rd International Conference on Computational Linguistics, pages 1002?1010. Associa-
tion for Computational Linguistics.
Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, pages 1029?1037. Association for Computational Linguistics.
Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh,
Ting Liu, Kit Cho, Umit Boz, Ignacio Cases, et al. 2013. Robust extraction of metaphors from novel data.
Meta4NLP 2013, page 67.
T. Veale and Y. Hao. 2008. A fluid knowledge representation for understanding and generating creative metaphors.
In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 945?952.
Association for Computational Linguistics.
Yorick Wilks, Lucian Galescu, James Allen, and Adam Dalton. 2013. Automatic metaphor detection using large-
scale lexical resources and conventional metaphor extraction. Meta4NLP 2013, page 36.
1763
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 519?528,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A generative model for unsupervised discovery of relations and argument
classes from clinical texts
Bryan Rink and Sanda Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX, USA
{bryan,sanda}@hlt.utdallas.edu
Abstract
This paper presents a generative model for
the automatic discovery of relations between
entities in electronic medical records. The
model discovers relation instances and their
types by determining which context tokens ex-
press the relation. Additionally, the valid se-
mantic classes for each type of relation are de-
termined. We show that the model produces
clusters of relation trigger words which bet-
ter correspond with manually annotated re-
lations than several existing clustering tech-
niques. The discovered relations reveal some
of the implicit semantic structure present in
patient records.
1 Introduction
Semantic relations in electronic medical records
(EMRs) capture important meaning about the as-
sociations between medical concepts. Knowledge
about how concepts such as medical problems, treat-
ments, and tests are related can be used to improve
medical care by speeding up the retrieval of relevant
patient information or alerting doctors to critical in-
formation that may have been overlooked. When
doctors write progress notes and discharge sum-
maries they include information about how treat-
ments (e.g., aspirin, stent) were administered for
problems (e.g. pain, lesion) along with the out-
come, such as an improvement or deterioration. Ad-
ditionally, a doctor will describe the tests (e.g., x-
ray, blood sugar level) performed on a patient and
whether the tests were conducted to investigate a
known problem or revealed a new one. These textual
descriptions written in a patient?s record encode im-
portant information about the relationships between
the problems a patients has, the treatments taken for
the problems, and the tests which reveal and investi-
gate the problems.
The ability to accurately detect semantic rela-
tions in EMRs, such as Treatment-Administered-for-
Problem, can aid in querying medical records. Af-
ter a preprocessing phase in which the relations are
detected in all records they can be indexed and re-
trieved later as needed. A doctor could search for
all the times that a certain treatment has been used
on a particular problem, or determine all the treat-
ments used for a specific problem. An additional
application is the use of the relational information
to flag situations that merit further review. If a pa-
tient?s medical record indicates a test that was found
to reveal a critical problem but no subsequent treat-
ment was performed for the problem, the patient?s
record could be flagged for review. Similarly, if
a Treatment-Worsens-Problem relation is detected
previously in a patient?s record, that information can
be brought to the attention of a doctor who advises
such a treatment in the future. By considering all
of the relations present in a corpus, better medical
ontologies could be built automatically or existing
ones can be improved by adding additional connec-
tions between concepts that have a relation in text.
Given the large size of EMR repositories, we ar-
gue that it is quite important to have the ability to
perform relation discovery between medical con-
cepts. Relations between medical concepts benefit
translational medicine whenever possible relations
are known. Uzuner et al (2011) show that super-
519
vised methods recognize such relations with high ac-
curacy. However, large sets of annotated relations
need to be provided for this purpose. To address
both the problem of discovering unknown relations
between medical concepts and the related problem
of generating examples for known relations, we have
developed an unsupervised method. This approach
has the advantages of not requiring an expensive an-
notation effort to provide training data for seman-
tic relations, which is particularly difficult for medi-
cal records, characterized by many privacy concerns.
Our analysis shows a high level of overlap between
the manually annotated relations and those that were
discovered automatically. Our experimental results
show that this approach improves upon simpler clus-
tering techniques.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the related work. Section
3 reports our novel generative model for discovering
relations in EMRs, Section 4 details the inference
and parameter estimation of our method. Section
5 details our experiments, Section 6 discusses our
findings. Section 7 summarizes the conclusions.
2 Related Work
Previous methods for unsupervised relation dis-
covery have also relied on clustering techniques.
One technique uses the context of entity arguments
to cluster, while another is to perform a post-
processing step to cluster relations found using an
existing relation extraction system. The approaches
most similar to ours have taken features from the
context of pairs of entities and used those features to
form a clustering space. In Hasegawa et al (2004),
those features are tokens found within a context win-
dow of the entity pair. Distance between entity pairs
is then computed using cosine similarity. In another
approach, Rosenfeld and Feldman (2007) use hierar-
chical agglomerative clustering along with features
based on token patterns seen in the context, again
compared by cosine similarity.
Other approaches to unsupervised relation dis-
covery have relied on a two-step process where a
number of relations are extracted, usually from a
predicate-argument structure. Then similar relations
are clustered together since synonymous predicates
should be considered the same relation (e.g. ?ac-
quire? and ?purchase?). Yates (2009) considers the
output from an open information extraction system
(Yates et al, 2007) and clusters predicates and argu-
ments using string similarity and a combination of
constraints. Syed and Viegas (2010) also perform a
clustering on the output of an existing relation ex-
traction system by considering the number of times
two relations share the same exact arguments. Sim-
ilar relations are expected to have the same pairs
of arguments (e.g. ?Ford produces cars? and ?Ford
manufactures cars?). These approaches and others
(Agichtein and Gravano, 2000; Pantel and Pennac-
chiotti, 2006) rely on an assumption that relations
are context-independent, such as when a person is
born, or the capital of a nation. Our method will
discover relations that can depend on the context as
well. For instance, ?penicillin? may be causally re-
lated to ?allergic reaction? in one patient?s medical
record but not in another. The relation between the
two entities is not globally constant and should be
considered only within the scope of one patient?s
records.
Additionally, these two-step approaches tend
to rely on predicate-argument structures such as
subject-verb-object triples to detect arbitrary rela-
tions (Syed and Viegas, 2010; Yates et al, 2007).
Such approaches can take advantage of the large
body of research that has been done on extracting
syntactic parse structure and semantic role infor-
mation from text. However, these approaches can
overlook relations in text which do not map easily
onto those structures. Unlike these approaches, our
model can detect relations that are not expressed as
a verb, such as ?[cough] + [green sputum]? to ex-
press a conjunction or ?[Cl] 119 mEq / L [High]? to
express that a test reading is indicating a problem.
The 2010 i2b2/VA Challenge (Uzuner et al,
2011) developed a set of annotations for medical
concepts and relations on medical progress notes
and discharge summaries. One task at the challenge
involved developing systems for the extraction of
eight types of relations between concepts. We use
this data set to compare our unsupervised method
with others.
The advantage of our work over existing unsu-
pervised approaches is the simultaneous clustering
of both argument words and relation trigger words.
These broad clusters handle: (i) synonyms, (ii) argu-
520
ment semantic classes, and (iii) words belonging to
the same relation.
3 A Generative Model for Discovering
Relations
3.1 Unsupervised Relation Discovery
A simple approach to discovering relations between
medical entities in clinical texts uses a clustering ap-
proach, e.g. Latent Dirichlet Allocation (LDA) (Blei
et al, 2003). We start with an assumption that rela-
tions exist between two entities, which we call argu-
ments, and may be triggered by certain words be-
tween those entities which we call trigger words.
For example, given the text ?[x-ray] revealed [lung
cancer]?, the first argument is x-ray, the second ar-
gument is lung cancer, and the trigger word is re-
vealed. We further assume that the arguments must
belong to a small set of semantic classes specific to
the relation. For instance, x-ray belongs to a class
of medical tests, whereas lung cancer belongs to a
class of medical problems. While relations may ex-
ist between distant entities in text, we focus on those
pairs of entities in text which have no other entities
between them. This increases the likelihood of a re-
lation existing between the entities and minimizes
the number of context words (words between the en-
tities) that are not relevant to the relation.
With these assumptions we build a baseline rela-
tion discovery using LDA. LDA is used as a baseline
because of its similarities with our own generative
model presented in the next section. Each consec-
utive pair of entities in text is extracted, along with
the tokens found between them. Each of the entities
in a pair is split into tokens which are taken along
with the context tokens to form a single pseudo-
document. When the LDA is processed on all such
pseudo-documents, clusters containing words which
co-occur are formed. Our assumption that relation
arguments come from a small set of semantic classes
should lead to clusters which align with relations
since the two arguments of a relation will co-occur
in the pseudo-documents. Furthermore, those argu-
ment tokens should co-occur with relation trigger
words as well.
This LDA-based approach was examined on elec-
tronic medical records from the 2010 i2b2/VA Chal-
lenge data set (Uzuner et al, 2011). The data set
Cluster 1
Words: secondary, due, likely, patient, disease,
liver, abdominal, cancer, pulmonary, respiratory,
elevated, volume, chronic, edema, related
?Correct? instances: [Metastatic colon cancer]
with [abdominal carcinomatosis]; [symptoms]
were due to [trauma]
?Incorrect? instances: [mildly improving symp-
toms] , plan will be to continue with [his cur-
rent medicines]; [prophylaxis] against [peptic ul-
cer disease]
Cluster 2:
Words: examination, no, positive, culture, exam,
blood, patient, revealed, cultures, physical, out,
urine, notable, showed, cells
?Correct? instances: [a blood culture] grew out
[Staphylococcusaureus]; [tamponade] by [exam-
ination]
?Incorrect? instances: [the intact drain] drain-
ing [bilious material]; [a Pseudomonas cellulitis]
and [subsequent sepsis]
Figure 1: Two clusters found by examining the most
likely words under two LDA topics. The instances are
pseudo-documents whose probability of being assigned
to that cluster was over 70%
contains manually annotated medical entities which
were used to form the pairs of entities needed. For
example, Figure 1 illustrates examples of two clus-
ters out of 15 discovered automatically using LDA
on the corpus. The first cluster appears to contain
words which indicate a relation whose two argu-
ments are both medical problems (e.g. ?disease?,
?cancer?, ?edema?). The trigger words seem to in-
dicate a possible causal relation (e.g., ?due?, ?re-
lated?, ?secondary?). The second cluster contains
words relevant to medical tests (e.g. ?examination?,
?culture?) and their findings (?revealed?, ?showed?,
?positive?). As illustrated in Figure 1, some of the
context words are not necessarily related to the re-
lation. The word ?patient? for instance is present
in both clusters but is not a trigger word because
it is likely to be seen in the context of any rela-
tion in medical text. The LDA-based model treats
all words equally and cannot identify which words
are likely trigger words and which ones are general
words, which merely occur frequently in the context
521
of a relation.
In addition, while the LDA approach can de-
tect argument words which co-occur with trigger
words (e.g., ?examination? and ?showed?), the clus-
ters produced with LDA do not differentiate between
contextual words and words which belong to the ar-
guments of the relation. An approach which mod-
els arguments separately from context words could
learn the semantic classes of those arguments and
thus better model relations. Considering the exam-
ples from Figure 1, a model which could cluster
?examination?, ?exam?, ?cultures?, and ?culture?
into one medical test cluster and ?disease?, ?cancer?
and ?edema? into a medical problem cluster separate
from the relation trigger words and general words
should model relations more accurately by better re-
flecting the implicit structure of the text. Because of
these limitations many relations discovered in this
way are not accurate, as can be seen in Figure 1.
3.2 Relation Discovery Model (RDM)
The limitations identified in the LDA-based ap-
proach are solved by a novel relation discovery
model (RDM) which jointly models relation argu-
ment semantic classes and considers them separately
from the context words. Relations triggered by pairs
of medical entities enable us to consider three ob-
servable features: (A1) the first argument; (A2)
the second argument; and (CW) the context words
found between A1 and A2.
For instance, in sentence S1 the arguments are
A1=?some air hunger? and A2=?his tidal volume?
while the context words are ?last?, ?night?, ?when?,
?I?, and ?dropped?.
S1: He developed [some air hunger]PROB last night
when I dropped [his tidal volume]TREAT from 450
to 350.
In the RDM, the contextual words are assumed to
come from a mixture model with 2 mixture compo-
nents: a relation trigger word (x = 0), or a general
word (x = 1), where x is a variable representing
which mixture component a word belongs to. In
sentence S1 for example, the word ?dropped? can
be seen as a trigger word for a Treatment-Causes-
Problem relation. The remaining words are not trig-
ger words and hence are seen as general words.
Under the RDM?s mixture model, the probability
of a context word is:
P (wC |tr, z) =
P (wC |tr, x = 0) ? P (x = 0|tr) +
P (wC |z, x = 1) ? P (x = 1|tr)
Where wC is a context word, the variable tr is
the relation type, and z is the general word class.
The variable x chooses whether a context word
comes from a relation-specific distribution of trig-
ger words, or from a general word class. In the
RDM, the two argument classes are modeled jointly
as P (c1, c2|tr), where c1 and c2 are two semantic
classes associated with a relation of type tr. How-
ever the assignment of classes to arguments depends
on a directionality variable d. If d = 0, then the first
argument is assigned semantic class c1 and the sec-
ond is assigned class c2. When d = 1 however, the
class assignments are swapped. This models the fact
that a relation?s arguments do not come in a fixed
order, ?[MRI] revealed [tumor]? is the same type of
relation as ?[tumor] was revealed by [x-ray]?. Fig-
ure 2 shows the graphical model for the RDM. Each
candidate relation is modeled independently, with a
total of I relation candidates. Variable w1 is a word
observed from the first argument, and w2 is a word
observed from the second argument. The model
takes parameters for the number of relations types
(R), the number of argument semantic classes (A),
and the number of general word classes (K). The
generative process for the RDM is:
1. For relation type r = 1..R:
(a) Draw a binomial distribution ?r from
Beta(?x) representing the mixture distri-
bution for relation r
(b) Draw a joint semantic class distribution
?1,2r ? RC?C from Dirichlet(?1,2).
2. Draw a categorical word distribution ?zz? from
Dirichlet(?z) for each general word class
z? = 1..K
3. Draw a categorical word distribution ?rr? from
Dirichlet(?r) for each r? = 1..R
4. for semantic class a? = 1..A:
(a) Draw categorical word distributions
?1a? and ?2a? from Dirichlet(?1) and
Dirichlet(?2) for the first and second
arguments, respectively.
522
tr
d
c1,2
w1
w2
x
z
wC
?
?
?
?
?1,2
?z
?r
?x
?d
?1,2
?z
?r
?1
?2
?z
?r
?1
?2
W1
W2
WC
I
R
R
R
K
R
A
A
Figure 2: Graphical model for the RDM. c1,2 represents the joint generation of c1 and c2
P (tr, d|tr?i,d?i, c1,2?i ,x?i,z?i,wC?i,w1?i,w2?i;?,?) ? u1 ? u2 ? u3
u1 = f(t
r)+?r
I+R?r ?
f(tr ,d)+?dd
f(tr)+?d0+?d1
? f(tr ,c1,c2)+?1,2f(tr)+C?C?1,2
u2 =
?WC
j
fi(zj)+?z
WC+K?z ?
f(tr ,xi)+?x
f(tr)+2?x ?(1x=0
f(tr ,wCj )+?r
f(tr)+W?r + 1x=1
f(zj ,wCj )+?z
f(zj)+W?z )
u3 =
?W1
j
f(a1,w1j )+?1
f(a1)+W?1 ?
?W2
j
f(a2,w2j )+?2
f(a2)+W?2
Figure 3: Gibbs sampling update equation for variables tr and d for the ith relation candidate. The variables a1 = c1
and a2 = c2 if d = 0, or a1 = c2 and a2 = c1 if d = 1. W is the size of the vocabulary. f(?) is the count of
the number of times that event occurred, excluding assignments for the relation instance being sampled. For instance,
f(tr, d) =?Ik 6=i I[trk = tri ? dk = di]
5. Draw a categorical relation type distribution ?
from Dirichlet(?r)
6. For each pair of consecutive entities in the cor-
pus, i = 1..I:
(a) Sample a relation type tr from ?
(b) Jointly sample semantic classes c1 and c2
for the first and second arguments from
?1,2tr
(c) Draw a general word class categorical dis-
tribution ? from Dirichlet(?z)
(d) For each token j = 1..W1 in the first ar-
gument: Sample a word w1j from ?1c1 if
d = 0 or ?2c2 if d = 1
(e) For each token j = 1..W2 in the second
argument: Sample a word w2j from ?2c2 if
d = 0 or ?1c1 if d = 1
(f) For each token j = 1..WC in the context
of the entities:
i. Sample a general word class z from ?
ii. Sample a mixture component x from
?tr
iii. Sample a word from ?rtr if x = 0 or
?zz if x = 1.
In the RDM, words from the arguments are in-
formed by the relation through an argument seman-
tic class which is sampled from P (c1, c2|tr) = ?1,2tr .
Furthermore, words from the context are informed
by the relation type. These dependencies enable
more coherent relation clusters to form during pa-
rameter estimation because argument classes and re-
lation trigger words are co-clustered.
We chose to model two distinct sets of entity
words (?1 and ?2) depending on whether the entity
occurred in the first argument or the second argu-
ment of the relation. The intuition for using disjoint
sets of entities is based on the observation that an
entity may be expressed differently if it comes first
or second in the text.
4 Inference and Parameter Estimation
Assignments to the hidden variables in RDM can
be made by performing collapsed Gibbs sampling
(Griffiths and Steyvers, 2004). The joint probability
of the data is:
523
P (wC,w1,w2;?,?) ?
P (?|?x)P (?|?r)P (?|?d)P (?1,2|?1,2)
?P (?z|?z)P (?r|?r)P (?1|?1)P (?2|?2)
??Ii [P (?i|?z)P (tri |?)P (di|tr, ?tr )P (c1i , c2i |tr, ?1,2)
??WC,ij P (zi,j |?i)P (xi,j |tri , ?tri )P (wCi,j |xi,j , tri , zi,j)
??W1,ij P (w1j |di, c
1,2
i , ?1)
??W2,ij P (w2j |di, c
1,2
i , ?2)]
We need to sample variables tr, d, c1,2, x, and
z. We sample tr and d jointly while each of the
other variables is sampled individually. After
integrating out the multinomial distributions, we
can sample tr and d from the equation in Figure 3
The update equations for the remaining variables
can be derived from the same equation by dropping
terms which are constant across changes in that vari-
able.
In our experiments the hyperparameters were set
to ?x = 1.0, ?z = 1.0, ?1,2 = 1.0, ?d0 = 2, ?d1 =
1, ?r = 0.01, ?z = 0.01, ?1 = 1.0, ?2 = 1.0.
Changing the hyperparameters did not significantly
affect the results.
5 Experimental Results
5.1 Experimental Setup
We evaluated the RDM using a corpus of electronic
medical records provided by the 2010 i2b2/VA
Challenge (Uzuner et al, 2011). We used the
training set, which consists of 349 medical records
from 4 hospitals, annotated with medical concepts
(specifically problems, treatments, and tests),
along with any relations present between those
concepts. We used these manually annotated
relations to evaluate how well the RDM performs
at relation discovery. The corpus is annotated
with a set of eight relations: Treatment-Addresses-
Problem, Treatment-Causes-Problem, Treatment-
Improves-Problem, Treatment-Worsens-Problem,
Treatment-Not-Administered-due-to-Problem, Test-
Reveals-Problem, Test-Conducted-for-Problem, and
Problem-Indicates-Problem. The data contains
13,460 pairs of consecutive concepts, of which
3,613 (26.8%) have a relation belonging to the list
above. We assess the model using two versions of
this data set consisting of: those pairs of consecutive
Relation 1 Relation 2 Relation 3 Relation 4
mg ( due showed
p.r.n. ) consistent no
p.o. Working not revealed
hours ICD9 likely evidence
prn Problem secondary done
q Diagnosis patient 2007
needed 30 ( performed
day cont started demonstrated
q. ): most without
4 closed s/p normal
2 SNMCT seen shows
every **ID-NUM related found
one PRN requiring showing
two mL including negative
8 ML felt well
Figure 4: Relation trigger words found by the RDM
entities which have a manually annotated relation
(DS1), and secondly, all consecutive pairs of entities
(DS2). DS1 allows us to assess the RDM?s cluster-
ing without the noise introduced from those pairs
lacking a true relation. Evaluations on DS2 will
indicate the level of degradation caused by large
numbers of entity pairs that have no true relation.
We also use a separate test set to assess how well
the model generalizes to new data. The test set
contains 477 documents comprising 9,069 manually
annotated relations.
5.2 Analysis
Figure 4 illustrates four of the fifteen trigger word
clusters (most likely words according to ?r) learned
from dataset DS1 using the best set of parameters
according to normalized mutual information (NMI)
as described in section 5.3. These parameters were:
R = 9 relations, K = 15 general word classes, and
A = 15 argument classes. Examination of the most
likely words reveals a variety of trigger words, be-
yond obvious explicit ones. Example sentences for
the relation types from Figure 4 are presented in Fig-
ure 5 and discussed below.
Relation Type 1
Instances of this discovered relation are often found
embedded in long lists of drugs prescribed to the
patient. Tokens such as ?p.o.? and ?p.r.n.?, mean-
ing respectively ?by mouth? and ?when necessary?,
are indicative of a prescription relation. The learned
relation specifically considers arguments of a drug
524
Instances of Relation Type 1
1. Haldol 0.5-1 milligrams p.o. q.6-8h. p.r.n. agitation
2. plavix every day to prevent failure of these stents
3. KBL mouthwash , 15 ccp .o. q.d. prn mouth discomfort
4. Miconazole nitrate powder tid prn for groin rash
5. AmBisome 300 mg IV q.d. for treatment of her hepatic candidiasis
Instances of Relation Type 2
1. MAGNESIUM HYDROXIDE SUSP 30 ML ) , 30 mL , Susp , By Mouth , At Bedtime , PRN , For Constipation
2. Depression , major ( ICD9 296.00 , Working , Problem ) cont NOS home meds
3. Diabetes mellitus type II ( ICD9 250.00 , Working , Problem ) cont home meds
4. ASCITES ( ICD9 789.5 , Working , Diagnosis ) on spironalactone
5. *Dilutional hyponatremia ( SNMCT **ID-NUM , Working , Diagnosis ) improved with fluid restriction
Instances of Relation Type 3
1. ESRD secondary to her DM
2. slightly lightheaded and with increased HR
3. a 40% RCA , which was hazy
4. echogenic kidneys consistent with renal parenchymal disease
5. *Librium for alcohol withdrawal
Instances of Relation Type 4
1. V-P lung scan was performed on May 24 2007 , showed low probability of PE
2. a bedside transthoracic echocardiogram done in the Cardiac Catheterization laboratory without evidence of
an effusion
3. exploration of the abdomen revealed significant nodularity of the liver
4. Echocardiogram showed moderate dilated left atrium
5. An MRI of the right leg was done which was equivocal for osteomyelitis
Figure 5: Examples for four of the discovered relations. Those marked with an asterisk have a different manually
chosen relation than the others
and a symptom treated by that drug. The closest
manually chosen relation is Treatment-Addresses-
Problem which included drugs as treatments.
Relation Type 2
Relation 2 captures a similar kind of relation to Re-
lation 1. All five examples for Relation 1 in Fig-
ure 5 came from a different set of hospitals than the
examples for Relation 2. This indicates the model
is detecting stylistic differences in addition to se-
mantic differences. This is one of shortcomings of
simple generative models. Because they cannot re-
flect the true underlying distribution of the data they
will model the observations in ways that are irrel-
evant to the task at hand. Relation 2 also contains
certain punctuation, such as parentheses which the
examples show are used to delineate a treatment
code. Instances of Relation 2 were often marked
as Treatment-Addresses-Problem relations by anno-
tators.
Relation Type 3
The third relation captures problems which are re-
lated to each other. The manual annotations contain
a very similar relation called Problem-Indicates-
Problem. This relation is also similar to Cluster 1
from Section 3.1, however under the RDM the words
are much more specific to the relation. This relation
is difficult to discover accurately because of the in-
frequent use of strong trigger words to indicate the
relation. Instead, the model must rely more on the
semantic classes of the arguments, which in this case
will both be types of medical problems.
Relation Type 4
The fourth relation is detecting instances where a
medical test has revealed some problem. This cor-
responds to the Test-Reveals-Problem relation from
the data. Many good trigger words for that relation
have high probability under Relation 4. A compar-
ison of the RDM?s Relation 4 with LDA?s cluster 2
from Figure 1 shows that many words not relevant
to the relation itself are now absent.
Argument classes
Figure 6 shows the 3 most frequent semantic classes
525
Concept 1 Concept 2 Concept 3
CT pain Percocet
scan disease Hgb
chest right Hct
x-ray left Anion
examination renal Vicodin
Chest patient RDW
EKG artery Bili
MRI - RBC
culture symptoms Ca
head mild Gap
Figure 6: Concept words found by the RDM
for the first argument of a relation (?1). Most of the
other classes were assigned rarely, accounting for
only 19% of the instances collectively. Human an-
notators of the data set chose three argument classes:
Problems, Treatments, and Tests. Concept 1 aligns
closely with a test semantic class. Concept 2 seems
to be capturing medical problems and their descrip-
tions. Finally, Concept 3 appears to be a combina-
tion of treatments (drugs) and tests. Tokens such as
?Hgb?, ?Hct?, ?Anion?, and ?RDW? occur almost
exclusively in entities marked as tests by annotators.
It is not clear why this cluster contains both types
of words, but many of the high ranking words be-
yond the top ten do correspond to treatments, such as
?Morphine?, ?Albumin?, ?Ativan?, and ?Tylenol?.
Thus the discovered argument classes show some
similarity to the ones chosen by annotators.
5.3 Evaluation
For a more objective analysis of the relations de-
tected, we evaluated the discovered relation types
by comparing them with the manually annotated
ones from the data using normalized mutual infor-
mation (NMI) (Manning et al, 2008). NMI is an
information-theoretic measure of the quality of a
clustering which indicates how much information
about the gold classes is obtained by knowing the
clustering. It is normalized to have a range from 0.0
to 1.0. It is defined as:
NMI(?;C) = I(?;C)[H(?) +H(C)]/2
where ? is the system-produced clustering, C is the
gold clustering, I is the mutual information, and H
is the entropy. The mutual information of two clus-
terings can be defined as:
I(?,C) =
?
k
?
j
|?k ? cj |
N log2
N |?k ? cj |
|?k||cj |
where N is the number of items in the clustering.
The entropy is defined as
H(?) = ?
?
k
|?k|
N log2
|?k|
N
The reference clusters consist of all relations an-
notated with the same relation type. The predicted
clusters consist of all relations which were assigned
the same relation type.
In addition to NMI, we also compute the F mea-
sure (Amigo? et al, 2009). The F measure is com-
puted as:
F =
?
i
|Li|
n maxj{F (Li, Cj)}
where
F (Li, Cj) =
2 ?Recall(Li, Cj) ? Precision(Li, Cj)
Recall(Li, Cj) + Precision(Li, Cj)
and Precision is defined as:
Precision(Ci, Lj) =
|Ci ? Lj|
|Ci|
while Recall is simply precision with the arguments
swapped:
Recall(L,C) = Precision(C,L)
Table 1 shows the NMI and F measure scores for
several baselines along with the RDM. Evaluation
was performed on both DS1 (concept pairs having
a manually annotated relation) and DS2 (all con-
secutive concept pairs). For DS2 we learned the
models using all of the data, and evaluated on those
entity pairs which had a manual relation annotated.
The LDA-based model from Section 3.1 is used as
one baseline. Two other baselines are K-means and
Complete-Link hierarchical agglomerative cluster-
ing using TF-IDF vectors of the context and argu-
ment words (similar to Hasegawa et al (2004)).
526
Method DS1 DS2
NMI F NMI F
Train set
Complete-link 4.2 37.8 N/A N/A
K-means 8.25 38.0 5.4 38.1
LDA baseline 12.8 23.0 15.6 26.2
RDM 18.2 39.1 18.1 37.4
Test set
LDA baseline 10.0 26.1 11.5 26.3
RDM 11.8 37.7 14.0 36.4
Table 1: NMI and F measure scores for the RDM and
baselines. The first two columns of numbers show the
scores when evaluation is restricted to only those pairs
of concepts which had a relation identified by annotators.
The last two columns are the NMI and F measure scores
when each method clusters all consecutive entity pairs,
but is only evaluated on those with a relation identified
by annotators.
Complete-link clustering did not finish on DS2
because of the large size of the data set. This high-
lights another advantage of the RDM. Hierarchical
agglomerative clustering is quadratic in the size of
the number of instances to be clustered, while the
RDM?s time and memory requirements both grow
linearly in the number of entity pairs. The scores
shown in Table 1 use the best parameterization of
each model as measured by NMI. For DS1 the
best LDA-based model used 15 clusters. K-means
achieved the best result with 40 clusters, while the
best Complete-Link clustering was obtained by us-
ing 40 clusters. The best RDM model used parame-
ters R = 9 relation, K = 15 general word classes,
and A = 15 argument classes. For DS2 the best
number of clusters for LDA was 10, while K-means
performed best with 58 clusters. The best RDM
model used R = 100 relations, K = 50 general
word classes, and A = 15 argument classes. The
LDA-based approach saw an improvement when us-
ing the larger data set, however the RDM still per-
formed the best.
To assess how well the RDM performs on unseen
data we also evaluated the relations extracted by the
model on the test set. Only the RDM and LDA mod-
els were evaluated as clusters produced by K-means
and hierarchical clustering are valid only for the data
used to generate the clusters. Generative models on
the other hand can provide an estimate of the proba-
bility for each relation type on unseen text. For each
model we generate 10 samples after a burn in pe-
riod of 30 iterations and form clusters by assigning
each pair of concepts to the relation assigned most
often in the samples. The results of this evaluation
are presented in Table 1. While these cluster scores
are lower than those on the data used to train the
models, they still show the RDM outperforming the
LDA baseline model.
6 Discussion
The relation and argument clusters determined by
the RDM provide a better unsupervised relation dis-
covery method than the baselines. The RDM does
this using no knowledge about syntax or semantics
outside of that used to determine concepts. The
analysis shows that words highly indicative of rela-
tions are detected and clustered automatically, with-
out the need for prior annotation of relations or even
the choice of a predetermined set of relation types.
The discovered relations can be interpreted by a hu-
man or labeled automatically using a technique such
as the one presented in Pantel and Ravichandran
(2004). The fact that the discovered relations and ar-
gument classes align well with those chosen by an-
notators on the same data justify our assumptions
about relations being present and discoverable by
the way they are expressed in text. Table 1 shows
that the model does not perform as well when many
of the pairs of entities do not have a relation, but it
still performs better than the baselines.
While the RDM relies in large part on trigger
words for making clustering decisions it is also ca-
pable of including examples which do not contain
any contextual words between the arguments. In ad-
dition to modeling trigger words, a joint distribution
on argument semantic classes is also incorporated.
This allows the model to determine a relation type
even in the absence of triggers. For example, con-
sider the entity pair ?[lung cancer] [XRT]?, where
XRT stands for external radiation therapy. By deter-
mining the semantic classes for the arguments (lung
cancer is a Problem, and XRT is a test), the set of
possible relations between the arguments can be nar-
rowed down. For instance, XRT is unlikely to be
in a causal relationship with a problem, or to make
527
a problem worse. A further aid is the fact that the
learned relationships may be specialized. For in-
stance, there may be a learned relation type such
as ?Cancer treatment addresses cancer problem?. In
this case, seeing a type of cancer (lung cancer) and a
type of cancer treatment (XRT) would be strong ev-
idence for that type of relation, even without trigger
words.
7 Conclusions
We presented a novel unsupervised approach to dis-
covering relations in the narrative of electronic med-
ical records. We developed a generative model
which can simultaneously cluster relation trigger
words as well as relation arguments. The model
makes use of only the tokens found in the con-
text of pairs of entities. Unlike many previous ap-
proaches, we assign relations to entities at the lo-
cation those entities appear in text, allowing us to
discover context-sensitive relations. The RDM out-
performs baselines built using Latent Dirichlet Allo-
cation and traditional clustering methods. The dis-
covered relations can be used for a number of ap-
plications such as detecting when certain treatments
were administered or determining if a necessary test
has been performed. Future work will include trans-
forming the RDM into a non-parametric model by
using the Chinese Restaurant Process (CRP) (Blei et
al., 2010). The CRP can be used to determine the
number of relations, argument classes, and general
word classes automatically.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM Conference on Digi-
tal libraries, pages 85?94, San Antonio, Texas, United
States. ACM.
E. Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo. 2009. A
comparison of extrinsic clustering evaluation metrics
based on formal constraints. Information Retrieval,
12(4):461?486.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
David M. Blei, Thomas L. Griffiths, and Michael I. Jor-
dan. 2010. The nested chinese restaurant process and
bayesian nonparametric inference of topic hierarchies.
J. ACM, 57(2):1?30.
T. L Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics. ACM ID: 1219008.
C. D Manning, P. Raghavan, and H. Schu?tze. 2008. In-
troduction to information retrieval, volume 1. Cam-
bridge University Press.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Annual Meeting Association for
Computational Linguistics, volume 44, page 113.
P. Pantel and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
HLT/NAACL, volume 4, page 321?328.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for unsupervised relation identification. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, page 411?418, New York, NY, USA.
ACM. ACM ID: 1321499.
Z. Syed and E. Viegas. 2010. A hybrid approach to
unsupervised relation discovery based on linguistic
analysis and semantic typing. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, page 105?113.
Ozlem Uzuner, Brett South, Shuying Shen, and Scott Du-
Vall. 2011. 2010 i2b2/VA challenge on concepts, as-
sertions, and relations in clinical text. Accepted for
publication.
A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-
head, and S. Soderland. 2007. TextRunner: open in-
formation extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, page 25?26.
Alexander Yates. 2009. Unsupervised resolution of ob-
jects and relations on the web. Journal of Artificial
Intelligence Research, 34(1).
528
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 256?259,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UTD: Classifying Semantic Relations by Combining
Lexical and Semantic Resources
Bryan Rink and Sanda Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, Texas
{bryan,sanda}@hlt.utdallas.edu
Abstract
This paper describes our system for
SemEval-2010 Task 8 on multi-way clas-
sification of semantic relations between
nominals. First, the type of semantic re-
lation is classified. Then a relation type-
specific classifier determines the relation
direction. Classification is performed us-
ing SVM classifiers and a number of fea-
tures that capture the context, semantic
role affiliation, and possible pre-existing
relations of the nominals. This approach
achieved an F1 score of 82.19% and an ac-
curacy of 77.92%.
1 Introduction
SemEval-2010 Task 8 evaluated the multi-way
classification of semantic relations between nom-
inals in a sentence (Hendrickx et al, 2010).
Given two nominals embedded in a sentence,
the task requires identifying which of the fol-
lowing nine semantic relations holds between
the nominals: Cause-Effect, Instrument-Agency,
Product-Producer, Content-Container, Entity-
Origin, Entity-Destination, Component-Whole,
Member-Collection, Message-Topic, or Other if
no other relation is appropriate. For instance, the
following sentence provides an example of the
Entity-Destination relation:
?A small [piece]
E1
of rock landed into the
[trunk]
E2
.?
The two nominals given for this sentence are
E
1
(piece) and E
2
(trunk). This is an Entity-
Destination relation because the piece of rock
originated from outside of the trunk, but ended
up there. Finally, the direction of the relation is
(E
1
,E
2
) because E
1
, the piece, is the Entity and E
2
,
the trunk, is the Destination.
Analysis of the training data revealed three ma-
jor classes of knowledge required for recognizing
semantic relations: (i) examples that require back-
ground knowledge of an existing relation between
the nominals (e.g., example 5884 below), (ii) ex-
amples using background knowledge regarding
the typical role of one of the nominals (e.g., ex-
ample 3402), and (iii) examples that require con-
textual cues to disambiguate the role between the
nominals (e.g., example 5710).
Example 5884 ?The Ca content in the [corn]
E1
[flour]
E2
has also a strong dependence on
the pericarp thickness.?
Example 3402 ?The [rootball]
E1
was in a
[crate]
E2
the size of a refrigerator, and some
of the arms were over 12 feet tall.?
Example 5710 ?The seniors poured [flour]
E1
into wax [paper]
E2
and threw the items as
projectiles on freshmen during a morning pep
rally.?
In example 5884, the background knowledge
that flour is often made or derived from corn can
directly lead to the classification of the example
as containing an Entity-Origin relation. Likewise,
knowing that crates often act as containers is a
strong reason for believing that example 3402 is
a Content-Container relation. However, in exam-
ple 5710, neither the combination of the nominals
nor their individual affiliations lead to an obvious
semantic relation. After taking the context into
account, it becomes clear that this is an Entity-
Destination relation because E
1
is going into E
2
.
2 Approach
We cast the task of determining a semantic re-
lation and its direction as a classification task.
Rather than classifying both pieces of informa-
tion (relation and direction) simultaneously, one
classifier is used to determine the relation type,
and then, for each relation type, a separate clas-
sifier determines the direction. We used a total
of 45 feature types (henceforth: features), which
256
were shared among all of the direction classi-
fiers and the one relation classifier. These fea-
ture types can be partitioned into 8 groups: lexical
features, hypernyms from WordNet
1
, dependency
parse, PropBank parse, FrameNet parse, nominal-
ization, predicates from TextRunner, and nomi-
nal similarity derived from the Google N-Gram
data set. All features were treated as FEATURE-
TYPE:VALUE pairs which were then presented to
the SVM
2
classifier as a boolean feature (0 or 1).
We further group our features into the three
classes described above: Contextual, Nominal af-
filiation, and Pre-existing relations. Table 1 illus-
trates sample feature values from example 117 of
the training set.
3 Contextual and Lexical Features
The contextual features consist of lexical features
and features based on dependency, PropBank, and
FrameNet parses. For lexical features, we extract
the words and parts of speech for E
1
and E
2
, the
words, parts of speech, and prefixes of length 5 for
tokens between the nominals, and the words be-
fore and single word after E
1
and E
2
respectively.
The words between the nominals can be strong
indicators for the type of relation. For example
the words into, produced, and caused are likely
to occur in Entity-Destination, Product-Producer,
and Cause-Effect relations, respectively. Using
the prefixes of length 5 for the words between the
nominals provides a kind of stemming (produced
? produ, caused ? cause).
Inspired by a feature from (Beamer et al, 2007),
we extract a coarse-grained part of speech se-
quence for the words between the nominals. This
is accomplished by building a string using the first
letter of each token?s Treebank POS tag. This fea-
ture is motivated by the fact that relations such as
Member-Collection usually invoke prepositional
phrases such as: of, in the, and of various. The
corresponding POS sequences we extract are: ?I?,
?I D?, and ?I J?. Finally, we also use the num-
ber of words between the nominals as a feature
because relations such as Product-Producer and
Entity-Origin often have no intervening tokens
(e.g., organ builder or Coconut oil).
Syntactic and semantic parses capture long dis-
tance relationships between phrases in a sentence.
Instead of a traditional syntactic parser, we chose
1
http://wordnet.princeton.edu/
2
We used Weka?s SMO classifier
http://www.cs.waikato.ac.nz/ml/weka/
the Stanford dependency parser
3
for the simpler
syntactic structure it produces. Our dependency
features are based on paths in the dependency tree
of length 1 and length 2. The paths encode the de-
pendencies and words those dependencies attach
to. To generalize the paths, some of the features
replace verbs in the path with their top-level Levin
class, as determined by running a word sense dis-
ambiguation system (Mihalcea and Csomai, 2005)
followed by a lookup in VerbNet
4
. One of the fea-
tures for length 2 paths generalizes further by re-
placing all words with their location relative to the
nominals, either BEFORE, BETWEEN, or AFTER.
Consider example 117 from Table 1. The length
2 dependency path (feature depPathLen2VerbNet)
neatly captures the fact that E
1
is the subject of a
verb falling into Levin class 27, and E
2
is the di-
rect object. Levin class 27 is the class of engender
verbs, such as cause, spawn, and generate. This
path is indicative of a Cause-Effect relation.
Semantic parses such as ASSERT?s PropBank
parse
5
and LTH?s FrameNet parse
6
identify predi-
cates in text and their semantic roles. These parses
go beyond the dependency parse and identify the
specific role each nominal assumes for the pred-
icates in the sentence, so the parses should be a
more reliable indicator for the relation type be-
tween nominals. We have features for the iden-
tified predicates and for the roles assigned to each
nominal. Several of the features are only triggered
if both nominals are arguments for the same pred-
icate. The values from Table 1 show that the fea-
tures correctly determined that E
1
and E
2
are gov-
erned by a verb of Levin class 27, and that the lex-
ical unit is cause.v.
4 Nominal Role Affiliation Features
Although context can be critical to identifying the
semantic relation present in some examples, in
others we must bring some background knowledge
to bear regarding the types of nominals involved.
Knowing that a writer is a person provides sup-
porting evidence for that nominal taking part in
a PRODUCER role. Additionally, writer nominal-
izes the verb write which is classified by Levin
(Levin, 1993) as an ?Image creation? or ?Creation
and Transformation? verb. This provides further
support for assigning writer to a PRODUCER role.
3
http://nlp.stanford.edu/software/lex-parser.shtml
4
http://verbs.colorado.edu/ mpalmer/projects/verbnet.html
5
http://cemantix.org/assert.html
6
http://nlp.cs.lth.se/software/semantic parsing: framenet frames/
257
Example 117: Forward [motion]
E1
of the vehicle through the air caused a [suction]
E2
on the road draft tube.
Feature Set Feature Values
Lexical e1Word=motion, e2Word=suction, e1OrE2Word={motion,suction}, wordsBetween={of, the, vehicle,
through, the, air, caused, a}, posE1=NN, posE2=NN, posE1orE2=NN posBetween=I D N I D N V D,
distance=8, wordsOutside={Forward, on}, prefix5Between={air, cause, a, of, the, vehic, throu, the}
Dependency
depPathLen1={caused?nsubj?<E1>, caused?dobj?<E2>,...}
depPathLen1VerbNet={vn:27?nsubj?<E1>, vn:27?dobj?<E2>,...}
depPathLen2VerbNet={<E1>?nsubj?vn:27?dobj?<E2>},
depPathLen2Location={<E1>?nsubj?BETWEEN?dobj?<E2>}
PropBank
pbPredStem=caus, pbVerbNet=27, pbE1CoarseRole=ARG0, pbE2CoarseRole=ARG1,
pbE1orE2CoarseRole={ARG1,ARG2}, pbNumPredToks=1,
pbE1orE2PredHyper = {cause#v#1, create#v#1}
FrameNet fnAnyLU={cause.v, vehicle.n, road.n}, fnAnyTarget={cause,vehicle,road}, fnE2LU=cause.v,
fnE1OrE2LU=cause.v
Hypernym hyperE1={gesture#n#2, communication#n#2, entity#n#1, ...}, hyperE2={suction#n#1, phe-
nomenon#n#1, entity#n#1,...}, hyperE1orE2={gesture#n#2, communication#n#2, entity#n#1, suc-
tion#n#1, phenomenon#n#1, ...}, hyperBetween={quality#n#1, cause#v#1, create#v#1, ...}
NomLex-Plus Features did not fire
NGrams knnE1={motion, amendment, action, appeal, decision}, knnE2={suction, hose, pump, vacuum, nozzle},
knnE1Role=Message, knnE2Role=Component
TextRunner trE1 E2={may result from, to contact, created, moves, applies, causes, falls below, corresponds to which},
trE2 E1={including, are moved under, will cause, according to, are effected by, repeats, can match},
trE1 E2Hyper={be#v#6, agree#v#3, cause#v#1, ensue#v#1, contact#v#1, apply#v#1, ...}
Table 1: All of the feature types and values for example 117 from the training data. Despite the errors in
disambiguation the system still correctly classifies this as Cause-Effect(E
1
,E
2
)
We capture this background knowledge by lever-
aging four sources of lexical and semantic knowl-
edge: WordNet, NomLex-Plus
7
, VerbNet, and the
Google N-Gram data
8
.
We utilize a word sense disambiguation sys-
tem (Mihalcea and Csomai, 2005) to determine the
best sense for each nominal and use all of the hy-
pernyms as a feature. Hypernyms are also deter-
mined for the words between the nominals, how-
ever only the top three levels are used as a feature.
Following (Beamer et al, 2007), we also incor-
porate a nominalization feature for each nominal
based on NomLex-Plus. Rather than use the agen-
tial information as they did, we determine the verb
being nominalized and retrieve the verb?s top-level
Levin class from VerbNet. This reduces the spar-
sity problem for nominalizations while still cap-
turing their semantics.
Our final role-affiliation features make use of
the Google N-Gram data. Using the 5-grams we
determined the top 1,000 words that occur most
often in the context of each nominal. Nominals
were then compared to each other using Jaccard
similarity of their contexts and the 4 closest neigh-
bors were retained. For each nominal, we have a
feature containing the nominal itself and its 4 near-
est neighbors from the training set. Additional fea-
tures determine the most frequent role assigned to
the neighbors. Examples of all these features can
7
http://nlp.cs.nyu.edu/meyers/NomBank.html
8
Available from LDC as LDC2006T13
be seen in Table 1 in the row for NGrams. The
neighbors for motion in the table show the diffi-
culty this feature has with ambiguity, incorrectly
picking up words similar to the sense meaning a
proposal for action.
5 Pre-existing Relation Features
For some examples the context and the individ-
ual nominal affiliations provide little help in de-
termining the semantic relation, such as example
5884 from before (i.e., corn flour). These ex-
amples require knowledge of the interaction be-
tween the nominals and we cannot rely solely
on determining the role of one nominal or the
other. We turned to TextRunner (Yates et al,
2007) as a large source of background knowl-
edge about pre-existing relations between nom-
inals. TextRunner is a queryable database of
NOUN-VERB-NOUN triples extracted from a large
corpus of webpages. For example, the phrases re-
trieved from TextRunner for ?corn flour?
include: ?is ground into?, ?to make?, ?to ob-
tain?, and ?makes?. Querying in the reverse direc-
tion, for ?flour corn? returns phrases such
as: ?contain?, ?filled with?, ?comprises?, and ?is
made from?. We use the top ten phrases for the
?<E
1
> <E
2
>? query results, and also for
the ?<E
2
> <E
1
>? results, forming two fea-
tures. In addition, we include a feature that has all
of the hypernyms for the content words in the verb
phrases from the queries for the E
1
-E
2
direction.
258
Relation P R F1
Cause-Effect 89.63 89.63 89.63
Component-Whole 74.34 81.73 77.86
Content-Container 84.62 85.94 85.27
Entity-Destination 88.22 89.73 88.96
Entity-Origin 83.87 80.62 82.21
Instrument-Agency 71.83 65.38 68.46
Member-Collection 84.30 87.55 85.89
Message-Topic 81.02 85.06 82.99
Product-Producer 82.38 74.89 78.46
Other 52.97 51.10 52.02
Overall 82.25 82.28 82.19
Table 2: Overall and individual relation scores on
the test set, along with precision and recall
6 Results
Our system achieved the best overall score as mea-
sured by macro-averaged F1 (for scoring details
see (Hendrickx et al, 2010)) among the ten teams
that participated in the semantic relation task at
SemEval-2010. The results in Table 2 show the
performance of the system on the test set for each
relation type and the overall score.
The training data consisted of 8,000 annotated
instances, including the numbered examples intro-
duced earlier, and the test set contained 2,717 ex-
amples. To assess the learning curve for this task
we trained on sets of size 1000, 2000, 4000, and
8000, obtaining test scores of 73.08, 77.02, 79.93,
and 82.19, respectively. These results indicate that
more training data does help, but going from 1,000
training instances to 8,000 only boosts the score by
about 9 points of F-measure.
Because our approach makes use of many dif-
ferent features, we ran ablation tests on the 8 sets
of features from Table 1 to determine which types
of features contributed the most to classifying se-
mantic relations. We evaluated all 256 (2
8
) combi-
nations of the feature sets on the training data us-
ing 10-fold cross validation. The results are shown
in Table 3. The last lines of Tables 2 and 3 corre-
spond to the system submitted for SemEval-2010
Task 8. The score on the training data is lower be-
cause the data includes examples from SemEval-
2007, which has more of the harder to classify
Other relations
9
.
These tests have shown that the NomLex-Plus
feature likely did not help. Further, the depen-
dency parse feature added little beyond PropBank
and FrameNet. Given the high score for the lexical
feature set we split it into smaller sets to see their
contributions in the top portion of Table 3. This
9
To confirm this we performed a 10 fold cross validation
of examples 1-7109, adding examples 7110-8000 (the 2007
data) to each training set. This resulted in an F1 of 82.18
Feature Sets F1
E
1
and E
2
only 48.7
Words between only 64.0
E
1
, E
2
, and words between 72.5
All word features (incl. before and after) 73.1
1 Lexical 73.8
2 +Hypernym 77.8
3 +FrameNet 78.9
4 +NGrams 79.7
5 -FrameNet +PropBank +TextRunner 80.5
6 +FrameNet 81.1
7 +Dependency 81.3
8 +NomLex-Plus 81.3
Table 3: Scores obtained for various sets of fea-
tures on the training set. The bottom portion of
the table shows the best combination containing 1
to 8 feature sets
reveals the best individual feature is for the words
between the two nominals.
7 Conclusion
By combining various linguistic resources we
were able to build a state of the art system for
recognizing semantic relations in text. While the
large training size available in SemEval-2010 Task
8 enables achieving high scores using only word-
based features, richer linguistic and background-
knowledge resources still provide additional aid in
identifying semantic relations.
Acknowledgments
The authors would like to thank Kirk Roberts for
providing code and insightful comments.
References
B. Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskaya,
and R. Girju. 2007. UIUC: a knowledge-rich
approach to identifying semantic relations between
nominals. In ACL SemEval07 Workshop.
I. Hendrickx, S.N. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, Upp-
sala, Sweden.
B. Levin. 1993. English verb classes and alternations:
A preliminary investigation. Chicago, Il.
R. Mihalcea and A. Csomai. 2005. SenseLearner:
word sense disambiguation for all words in unre-
stricted text. In Proceedings of the ACL 2005 on
Interactive poster and demonstration sessions. ACL.
A. Yates, M. Cafarella, M. Banko, O. Etzioni,
M. Broadhead, and S. Soderland. 2007. Text-
Runner: open information extraction on the web. In
Proceedings of HLT: NAACL: Demonstrations.
259
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 413?418,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UTD: Determining Relational Similarity Using Lexical Patterns
Bryan Rink and Sanda Harabagiu
University of Texas at Dallas
P.O. Box 830688; MS EC31
Richardson, TX, 75083-0688, USA
{bryan,sanda}@hlt.utdallas.edu
Abstract
In this paper we present our approach for
assigning degrees of relational similarity to
pairs of words in the SemEval-2012 Task 2.
To measure relational similarity we employed
lexical patterns that can match against word
pairs within a large corpus of 12 million docu-
ments. Patterns are weighted by obtaining sta-
tistically estimated lower bounds on their pre-
cision for extracting word pairs from a given
relation. Finally, word pairs are ranked based
on a model predicting the probability that they
belong to the relation of interest. This ap-
proach achieved the best results on the Se-
mEval 2012 Task 2, obtaining a Spearman cor-
relation of 0.229 and an accuracy on reproduc-
ing human answers to MaxDiff questions of
39.4%.
1 Introduction
Considerable prior research has examined and elab-
orated upon a wide variety of semantic relations
between concepts along with techniques for auto-
matically discovering pairs of concepts for which
a relation holds (Bejar et al, 1991; Stephens and
Chen, 1996; Rosario and Hearst, 2004; Khoo and
Na, 2006; Girju et al, 2009). However, most pre-
vious work has considered membership assignment
for a semantic relation as a binary property. In this
paper we discuss an approach which assigns a de-
gree of membership to a pair of concepts for a given
relation. For example, for the semantic relation
CLASS-INCLUSION (Taxonomic), the concept pairs
weapon:spear and bird:robin are stronger members
Consider the following word pairs: millionaire:money,
author:copyright, robin:nest. These X:Y pairs share a
relation ?X R Y?. Now consider the following word
pairs:
(1) teacher:students
(2) farmer:crops
(3) homeowner:door
(4) shrubs:roots
Which of the numbered word pairs is the MOST illus-
trative example of the same relation ?X R Y??
Which of the above numbered word pairs is the
LEAST illustrative example of the same relation ?X
R Y??
Figure 1: Example Phase 2 MaxDiff question for the re-
lation 2h PART-WHOLE: Creature:Possession.
of the relationship than hair:brown, because brown
may describe many things other than hair, and brown
is also used much less frequently as a noun than the
words in the first two word pairs. Task 2 of Se-
mEval 2012 (Jurgens et al, 2012) was designed to
evaluate the effectiveness of automatic approaches
for determining the similarity of a pair of concepts
to a specific semantic relation. The task focused on
79 semantic relations from Bejar et al (1991) which
broadly fall into the ten categories enumerated in Ta-
ble 1.
The data for the task was collected in two phases
using Amazon Mechanical Turk 1. During Phase
1, Turkers were asked to provide pairs of words
which fit a relation template, such as ?X pos-
sesses/owns/has Y?. Turkers provided word pairs
such as expert:experience, mall:shops, letters:words,
and doctor:degree. A total of 3,218 word pairs
1http://www.mturk.com/mturk/
413
Category Example word pairs Relations
CLASS-INCLUSION flower:tulip, weapon:knife, clothing:shirt, queen:Elizabeth 5
PART-WHOLE car:engine, fleet:ship, mile:yard, kickoff:football 10
SIMILAR car:auto, stream:river, eating:gluttony, colt:horse 8
CONTRAST alive:dead, old:young, east:west, happy:morbid 8
ATTRIBUTE beggar:poor, malleable:molded, soldier:fight, exercise:vigorous 8
NON-ATTRIBUTE sound:inaudible, exemplary:criticized, war:tranquility, dull:cunning 8
CASE RELATIONS tailor:suit, farmer:tractor, teach:student, king:crown 8
CAUSE-PURPOSE joke:laughter, fatigue:sleep, gasoline:car, assassin:death 8
SPACE-TIME bookshelf:books, coast:ocean, infancy:cradle, rivet:girder 9
REFERENCE smile:friendliness, person:portrait, recipe:cake, astronomy:stars 6
Table 1: The ten categories of semantic relations used in SemEval 2012 Task 2. Each word pair has been taken from a
different subcategory of each major category.
across 79 relations were provided by Turkers in
Phase 1. Some of these word pairs are naturally
more representative of the relationship than others.
Therefore, in the second phase, each word pair was
presented to a different set of Turkers for ranking
in the form of MaxDiff (Louviere and Woodworth,
1991) questions. Figure 1 shows an example MaxD-
iff question for the relation 2h PART-WHOLE: Crea-
ture:Possession (?X possesses/owns/has Y?). In each
MaxDiff question, Turkers were simply asked to se-
lect the word pair which was the most illustrative
of the relation and the word pair which was the
least illustrative of the relation. For the example in
Figure 1, most Turkers chose either shrubs:roots or
farmer:crops as the most illustrative of the Crea-
ture:Possession relation, and homeowner:door as
the least illustrative. When Turkers select a pair of
words they are performing a semantic inference that
we wanted to also perform in a computational man-
ner. In this paper we present a method for automat-
ically ranking word pairs according to their related-
ness to a given semantic relation.
2 Approach for Determining Relational
Similarity
In the vein of previous methods for determining re-
lational similarity (Turney, 2011; Turney, 2008a;
Turney, 2008b; Turney, 2005), we propose two ap-
proaches using patterns generated from the contexts
in which the word pairs occur. Our corpus consists
of 8.4 million documents from Gigaword (Parker
and Consortium, 2009) and over 4 million articles
from Wikipedia. For each word pair, <W1>, <W2>
provided by Turkers in Phase 1, as well as the three
relation examples, we collected all contexts which
matched the schema:
? [0 or more non-content words] <W1> [0 to 7
words] <W2> [0 or more non-content words]?
We also include those contexts where W1 and W2
are swapped. The window size of seven words was
determined based on experiments on the training set
of ten relations provided by the task organizers. For
the non-content words, we considered closed class
words such as determiners (the, who, every), prepo-
sitions (in, on, instead of), and conjunctions (and,
but). Members of these classes were collected from
their corresponding Wikipedia pages. Below we
provide a sample of the 7,022 contexts found for the
word pair love:hate:
?they <W1> to <W2> it?
?<W1> and <W2> the most . by?
?between <W1> & <W2>?
?<W1> you then i <W2> you and?
We restrict the context before and after the word pair
to non-content words in order to match longer con-
texts without introducing exponential growth in the
number of patterns and the consequential sparsity
problems. These contexts are directly used as pat-
terns. To generate additional patterns we have one
method for shortening contexts and two methods for
generating patterns from contexts.
Any contexts which contain words before <W1>
or after <W1> are used to create additional shorter
contexts by successively removing leading and trail-
ing words. For example, the context ?as much
<W1> in the <W2> as his? for the word pair
money:bank would generate the following shortened
contexts:
?much <W1> in the <W2> as his?
?<W1> in the <W2> as his?
414
?as much <W1> in the <W2>? as
?as much <W1> in the <W2>?
?much <W1> in the <W2> as?
?<W1> in the <W2> as?
?<W1> in the <W2>?
These shortened contexts are used, along with the
original context, to generate patterns.
The first pattern generation method replaces each
word between<W1> and<W2> with a wildcard ([?
]+ means one or more non-space characters). For ex-
ample:
?as much <W1> [? ]+ the <W2> as?
?as much <W1> in [? ]+ <W2> as?
The second pattern generation technique allows for
a single word to be matched in the context between
the arguments <W1> and <W2>, along with arbi-
trary matching of other tokens in the context. For
example, the context for red:stop ?the <W1> flag is
flagged to indicate a <W2>? will generate new pat-
terns such as:
?the <W1>.* flag .*<W2>?
?the <W1>.* is .*<W2>?
?the <W1>.* flagged .*<W2>?
?the <W1>.* indicate .*<W2>?
After all patterns have been generated, they are used
by our two approaches to assign relational similarity
scores to word pairs.
2.1 UTD-NB Approach
The first of our two approaches, UTD-NB, assigns
weights to patterns which are then used to assign
similarity scores to word pairs. The approach begins
by obtaining all word pairs associated with a rela-
tion. Each relation is associated with a target set (T )
of word pairs from two sources: (i) the three or four
example word pairs provided for each relation, and
(ii) the word pairs provided by Turkers in Phase 1.
We collect all of the contexts for those word pairs to
generate patterns. The UTD-NB approach assumes
that the word pairs provided by Turkers, while noisy,
can be used to characterize the relation. As an exam-
ple, consider these word pairs provided by Turkers
for the relation 8a (Cause:Effect) illness:discomfort,
fire:burns, accident:damage. A pattern which ex-
tracts these word pairs is: ?<W1> that caused [? ]+
<W2>?. This pattern is unlikely to match the con-
texts of word pairs from other relations. Therefore,
we use the statistics about how many target word
Figure 2: Probabilistic model for the word pairs extracted
by patterns, for a single relation.
pairs a pattern extracts versus how many non-target
pairs a pattern extracts to assign a weight to the pat-
tern. A pattern which matches many of the word
pairs from the target relation and few (or none) of the
word pairs from other relations is likely to be a good
indicator of that relation. For example, the pattern
P1 for the relation 8a (Cause:Effect): ?the <W1>.*
caused .*<W2> to his? matches only three word
pairs: explosion:damage, accident:damage, and in-
jury:pain, all of them belonging to the target rela-
tion. Conversely, the pattern P2: ?<W1>.* caus-
ing .*<W2> but? matches five words pairs. How-
ever, only three of them belong to the target relation:
hit:injury, explosion:damage, germs:sickness. The
remaining two: city:people, action:alarm belong to
other relations: .
We use the number of target word pairs extracted,
x, and the total number of word pairs extracted, n,
to calculate ? : the probability that a word pair ex-
tracted by the pattern will belong to the target re-
lation. The maximum likelihood estimate for ? is
x
n , however for small values of x this estimate has
a high variance and can significantly overestimate
the true value. Therefore, we used the Wilson in-
terval score for determining a lower bound on ? at
a 99.9% confidence level. This gives the pattern P1
above with x = 3 and n = 3 a lower bound on ?
of 21.7% and P2 with x = 3 and n = 5 a lower
bound on ? of 16.6%. We use this lower bound as
the pattern?s weight. These pattern weights are then
combined to score each word pair for the target rela-
tion.
We model the word pairs extracted by the patterns
as a generative process shown in Figure 2. Each pat-
tern, p, is associated with with a precision, ? , which
is the probability that a word pair extracted by that
pattern is a member of the target relation. The ob-
415
served word pairs extracted by a pattern are denoted
by w. Our model assumes that a word pair extracted
by a pattern may be drawn from one of two distinct
distributions over word pairs: a distribution for the
target relation ~t, and a background distribution over
word pairs ~b. The generation of a word pair be-
gins with a binary variable x drawn from a Bernoulli
distribution parametrized by ? (the pattern?s preci-
sion), which represents whether a word pair is gen-
erated according to a relation specific distribution, or
a background distribution. More explicitly, if x = 1,
then a word pair w is generated by the target relation
distribution ~t, and if x = 0, a word pair is generated
by the background distribution~b.
We may not yet perform any meaningful infer-
ence because no evidence has been observed to cor-
rectly infer whether the target distribution or the
background distribution generated w. Therefore we
use the pattern weights derived above (based on the
lower bounds on the pattern precisions) as that pat-
tern?s value of ? . For estimating the distributions
~t and ~b, we assume that x is 1 (w is generated by
~t) if and only if ? ? 0.1 and the word pair w be-
longs to the target set of word pairs T . This thresh-
old on ? has a filtering effect on the patterns, and
those patterns below the threshold are treated as non-
indicative of the relation. These assumptions allow
us to estimate the parameters for ~t and~b:
P (w|~t) =
{
#(w,h)
#(h) if w ? T
0 if w 6? T
(1)
P (w|~b) =
#(w,?h) + #(w, h)1w 6?T
?
u #(u,?h) + #(u, h)1u6?T
(2)
where #(w, h) is the number of times w was ex-
tracted by a high precision pattern (? ? 10%), and
#(h) is the number of word pairs extracted by a high
precision pattern.
The only remaining hidden variable in the model
is x which we can now estimate using the inferred
distributions for the other variables. We chose to use
the probability of x for a word pair w as the score
by which we rank the word pairs. Furthermore, we
use only the probability of x for the highest ranking
pattern p which extracted w:
P (x = 1|p, w) =
P (x = 1, w|p)
P (w|p)
(3)
where P (x = 1, w|p) = ?p ? ~t(w) and P (w|p) =
P (x = 1, w|p) + P (x = 0, w|p)
This method of scoring word pairs accounts for
how common a word pair is overall. For example
for the relation 4c (CONTRAST: Reverse), the word
pair white:black occurs very commonly in both high
precision patterns and low precision patterns (those
more likely associated with other relations). There-
fore even though the word pair shares its highest
ranking pattern with the pair eat:fast, white:black re-
ceives a score of 0.019 while eat:fast receives a score
of 0.216 because ~t(white : black) = 0.006 and
~b(white : black) = 0.104, while ~t(eat : fast) =
0.0016 and ~b(eat : fast) = 0.0018. However,
if a pattern with 100% precision were to extract
white:black, the pair would appropriately receive a
score of 1.0 despite being much more common in the
background distribution. This is motivated by our
assumption that such a pattern can only extract word
pairs which truly belong to the relation. Another
motivation for scoring word pairs by their highest
ranking pattern is that it does not depend on any
assumption of independence between the patterns
which extract the pairs. For example, the pattern
?<W1> , not <W2> . ? extracts largely the same
word pairs as ?<W1> [? ]+ not<W2> .? and thus its
matches should not be taken as additional evidence
about the word pairs.
2.2 UTD-SVM Approach
Our second approach uses an SVM-rank (Joachims,
2006) model to rank the word pairs. Each word pair
from a target relation is represented as a binary fea-
ture vector indicating which patterns extracted the
word pair. We train the SVM-rank classifier by as-
signing all word pairs from the target relation rank 2,
and all word pairs from other relations with rank 1.
The SVM model is then trained and used to classify
the word pairs from the target relation. Even though
the model is used to classify the same word pairs it
was trained on, it still provides higher scores to word
pairs more likely to belong to the target relation. We
directly rank the word pairs using these scores.
3 Discussion
The organizers of SemEval 2012 Task 2 viewed re-
lational similarity in two different ways. The first
416
Word pair % Most illustrative -
% Least illustrative
?freezing:warm? 56.0
?earsplitting:quiet? 36.0
?evil:angelic? 18.0
?ancient:modern? 12.0
?disastrous:peaceful? 6.0
?ecstatic:disgruntled? 2.0
?disgusting:tasty? 0.0
?beautiful:plain? -2.0
?dirty:sterile? -4.0
?wrinkled:smooth? -6.0
?sweet:sour? -20.0
?disgruntled:ecstatic? -32.0
?white:gray? -54.0
Table 2: A sample of the 41 word pairs provided by
Amazon Mechanical Turk participants for the relation 4f
(CONTRAST: Asymmetric Contrary - X and Y are at op-
posite ends of the same scale). The word pairs are ranked
by how illustrative of the relation participants found each
pair to be.
view was that of solving a MaxDiff problem, ques-
tion in which participants are shown a list of four
word pairs and asked to select the most and least
illustrative pairs. The second view of relation simi-
larity considers the task of assigning scores to a ac-
cording to their similarity to the relation of interest.
The first column of Table 2 provides an example of
word pairs that Amazon Turkers said belonged to the
4f: CONTRAST: Asymmetric Contrary relation in
Phase 1, ranked according to how well other Turk-
ers felt they represented the relation. The score in
the second column is calculated as the percentage of
how often Turkers rated a word pair as the most il-
lustrative and how often Turkers rated the word pair
as the least illustrative.
Both of our approaches for determining relation
similarity assign scores directly to the word pairs
collected in Phase 1, with the goal of ranking the
words in the same order that was induced from the
responses by Amazon Mechanical Turkers.
3.1 Evaluation Measures
SemEval-2012 Task 2 had two official evaluation
metrics. The first directly measured the accuracy
of automatically choosing the most and least illus-
trative word pairs among a set of four word pairs
taken from responses during Phase 1. The accuracy
of choosing the most illustrative word pair and the
Team-Algorithm Spearman MaxDiff
UTD-NB 0.229 39.4
UTD-SVM 0.116 34.7
Duluth-V0 0.050 32.4
Duluth-V1 0.039 31.5
Duluth-V2 0.038 31.1
BUAP 0.014 31.7
Random 0.018 31.2
Table 3: Results for all systems participating in SemEval
2012 Task 2 on relational similarity, including a random
baseline.
accuracy of choosing the least illustrative word pair
were calculated separately and averaged to produce
the MaxDiff accuracy.
The second evaluation metric measured the corre-
lation between an automatic ranking of word pairs
for a relation and a ranking induced by the Turkers?
responses to the MaxDiff questions. The word pairs
were given scores equal to the percentage of times
they were chosen by Turkers as the most illustra-
tive example for a relation minus the percentage of
times they were chosen as the least illustrative. Sys-
tems were then evaluated according to their Spear-
man rank correlation with the ranking of word pairs
induced by that score. Spearman correlations range
from -1 for a negative correlation to 1.0 for a perfect
correlation.
3.2 Results
Table 3 shows the results for the six systems which
participated in SemEval-2012 Task 2, along with the
results for a baseline which ranks each word pair
randomly. Our two approaches achieved the best re-
sults on both evaluation metrics. Our UTD-NB ap-
proach achieves much better performance than our
UTD-SVM approach, likely due to the unconven-
tional use of the SVM to classify its own training
data. That said, the results are still significantly
higher than those of other participants. This may
be attributed to our incorporation of better patterns
or our use of a large corpus. It might also be a con-
sequence of our approaches considering all of the
testing word pairs simultaneously.
Table 4 shows the results for each of the ten cat-
egories of relations. The best results are achieved
on SPACE-TIME relations, while the lowest perfor-
mance is on the NON-ATTRIBUTE relations. NON-
417
Category Rndm BUAP UTD UMD
NB V0
1 CLASS-INCLUSION 0.057 0.064 0.233 0.045
2 PART-WHOLE 0.012 0.066 0.252 -0.061
3 SIMILAR 0.026 -0.036 0.214 0.183
4 CONTRAST -0.049 0.000 0.206 0.142
5 ATTRIBUTE 0.037 -0.095 0.158 0.044
6 NON-ATTRIBUTE -0.070 0.009 0.098 0.079
7 CASE RELATIONS 0.090 -0.037 0.241 -0.011
8 CAUSE-PURPOSE -0.011 0.114 0.183 0.021
9 SPACE-TIME 0.013 0.035 0.375 0.055
10 REFERENCE 0.142 -0.001 0.346 0.028
Table 4: Spearman correlation results for the best system
from each team, across all ten categories of relations.
ATTRIBUTE relations associate objects and actions
with an atypical attribute (harmony:discordant, im-
mortal:death, recluse:socialize). Because the pairs
of words associated with these relation are not typ-
ically associated together, our approach likely per-
forms poorly on these relations because our ap-
proach is based on finding the pairs of words to-
gether in a large corpus.
An interesting consequence of the 10% precision
threshold used in the UTD-NB approach is that 24
relations had no patterns exceeding the threshold
and therefore produced zeroes as scores for all word
pairs. However, word pairs which never occurred
within seven tokens of each other in our corpus re-
ceived a negative score and were ranked lower. Such
rankings tend to produce Spearman scores around
0.0. Our lowest Spearman score was -0.068, while
other teams had low scores of -0.344 and -0.266,
both occurring on relations for which UTD-NB pro-
duced no positive word pair scores. There are two
lessons to be learned from this result: (i) the UTD-
NB approach does a good job of recognizing when
it cannot rank word pairs, and (ii) such relations are
likely difficult and worth further investigation.
4 Conclusion
We described the UTD approaches to determining
relation similarity using lexical patterns from a large
corpus. Combined with a probabilistic model for
word pair extraction by those patterns, we were able
to achieve the highest performance at the SemEval
2012 Task 2. Our results showed the approach
significantly outperformed a model which used an
SVM-rank model used to classify its own training
set. The approach also performed well across a wide
range of relation types and argument classes which
included nouns, adjectives, verbs, and adverbs. This
implies that the approaches presented in this pa-
per could be successfully applied to other domains
which involve semantic relations.
References
Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson.
1991. Cognitive and psychometric analysis of analog-
ical problem solving. Recent research in psychology.
Springer-Verlag Publishing.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2009.
Classification of semantic relations between nominals.
Language Resources and Evaluation, 43(2):105?121.
Thorsten Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD inter-
national conference KDD ?06, page 217, New York,
New York, USA, August. ACM Press.
David A. Jurgens, Saif M. Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. SemEval-2012 Task 2:
Measuring Degrees of Relational Similarity. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012).
Christopher S G Khoo and Jin-cheon Na. 2006. Seman-
tic relations in information science. Annual Review of
Information Science and Technology, 40(1):157?228.
Jordan J Louviere and G G Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Robert Parker and Linguistic Data Consortium. 2009.
English gigaword fourth edition. Linguistic Data Con-
sortium.
Barbara Rosario and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience texts. In Proceedings
of the AACL ?04, pages 430?es, July.
Larry M. Stephens and Yufeng F. Chen. 1996. Principles
for organizing semantic relations in large knowledge
bases. IEEE Transactions on Knowledge and Data
Engineering, 8(3):492?496, June.
Peter D. Turney. 2005. Measuring Semantic Similarity
by Latent Relational Analysis. In International Joint
Conference On Artificial Intelligence, volume 19.
Peter D. Turney. 2008a. A Uniform Approach to Analo-
gies, Synonyms, Antonyms, and Associations. In Pro-
ceedings of COLING ?08, August.
Peter D. Turney. 2008b. The Latent Relation Mapping
Engine: Algorithm and Experiments. Journal of Arti-
ficial Intelligence Research, 33:615?655.
Peter D. Turney. 2011. Analogy perception applied
to seven tests of word comprehension. Journal of
Experimental & Theoretical Artificial Intelligence,
23(3):343?362, July.
418
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 461?466,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UTDHLT: COPACETIC System for Choosing Plausible Alternatives
Travis Goodwin, Bryan Rink, Kirk Roberts, Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas Dallas
Richardson TX, 75080
{travis,bryan,kirk,sanda}@hlt.utdallas.edu
Abstract
The Choice of Plausible Alternatives (COPA)
task in SemEval-2012 presents a series of
forced-choice questions wherein each question
provides a premise and two viable cause or ef-
fect scenarios. The correct answer is the cause
or effect that is the most plausible. This paper
describes the COPACETIC system developed
by the University of Texas at Dallas (UTD) for
this task. We approach this task by casting it
as a classification problem and using features
derived from bigram co-occurrences, TimeML
temporal links between events, single-word po-
larities from the Harvard General Inquirer, and
causal syntactic dependency structures within
the gigaword corpus. Additionally, we show
that although each of these components im-
proves our score for this evaluation, the dif-
ference in accuracy between using all of these
features and using bigram co-occurrence infor-
mation alone is not statistically significant.
1 The Problem
?The surfer caught the wave.? This statement, al-
though almost tautological for human understanding,
requires a considerable depth of semantic reasoning.
What is a surfer? What does it mean to ?catch a
wave?? How are these concepts related? What if
we want to ascertain, given that the surfer caught the
wave, whether the most likely next event is that ?the
wave carried her to the shore? or that ?she paddled her
board into the ocean?? This type of causal and tempo-
ral reasoning requires a breadth of world-knowledge,
often called commonsense understanding.
Question 15 (Find the EFFECT)
Premise: I poured water on my sleeping friend.
Alternative 1: My friend awoke.
Alternative 2: My friend snored.
Question 379 (Find the CAUSE)
Premise: The man closed the umbrella.
Alternative 1: He got out of the car.
Alternative 2: He approached the building.
Figure 1: An example of each type of question, one target-
ing an effect, and another targeting a cause.
The seventh task of SemEval-2012 evaluates pre-
cisely this type of cogitation. COPA: Choice of Plau-
sible Alternatives presents 1,0001 sets of two-choice
questions (presented as a premise and two alterna-
tives) provided in simple English sentences. The
goal for each question is to choose the most plausible
cause or effect entailed by the premise (the dataset
provided an equal distribution of cause and effect
targetting questions). Additionally, each question is
labeled so as to describe whether the answer should
be a cause or an effect, as indicated in Figure 1.
The topics of these questions were drawn from two
sources:
1. Randomly selected accounts of personal stories
taken from a collection of Internet weblogs (Gor-
don and Swanson, 2009).
2. Randomly selected subject terms from the Li-
brary of Congress Thesaurus for Graphic Mate-
rials (of Congress. Prints et al, 1980).
Additionally, the incorrect alternatives were authored
1This data set was split into a 500 question development (or
training) set and a 500 question test set.
461
Figure 2: Architecture of the COPACETIC System
with the intent of impeding ?purely associative meth-
ods? (Roemmele et al, 2011). The task aims to
evaluate the state of commonsense causal reasoning
(Roemmele et al, 2011).
2 System Architecture
Given a question, such as Question 15 (as shown
in Figure 1), our system selects the most plausible
alternative by using the output of an SVM classifier,
trained on the 500 provided development questions
and tested on the 500 provided test questions. The
classifier operates with features describing informa-
tion extracted from the processing of the question?s
premise and alternatives. As illustrated by Figure 2,
the preprocessing involves part of speech (POS) tag-
ging, and syntactic dependency parsing provided
by the Stanford parser (Klein and Manning, 2003;
Toutanova et al, 2003), multi-word expression detec-
tion using Wikipedia, automatic TimeML annotation
using TARSQI (Verhagen et al, 2005; Pustejovsky
et al, 2003), and Brown clustering as provided in
(Turian, 2010).
The architecture of the COPACETIC system is di-
vided into offline (independent of any question) and
online (question dependent) processing. The online
aspect of our system inspects each question using
an SVM and selects the most likely alternative. Our
system?s offline functions focus on pre-processing
resources so that they may be used by components
of the online aspect of our system. In the next sec-
tion, we describe the offline processing upon which
our system is built, and in the following section, the
online manner in which we evaluate each question.
2.1 Offline Processing
Because the questions presented in this task require
a wealth of commonsense knowledge, we first ex-
tracted commonsense and temporal facts. This sub-
section describes the process of mining this informa-
tion from the fourth edition of the English Gigaword
corpus2 (Parker et al, 2009).
We collected commonsense facts by extracting
cause and effect pairs using twenty-four hand-crafted
patterns. Rather than lexical patterns, we used pat-
terns over syntactic dependency structures in order
to capture the syntactic role each word plays. Fig-
ure 3 illuminates two examples of the dependency
structures encoded by our causal patterns. Causal
Pattern 1 captures all cases of causality indicated by
the verb causes, while Causal Pattern 2 illustrates a
more sophisticated pattern, in which the phrasal verb
brought on indicates causality.
In order to extract this information, we first parsed
the syntactic dependence structure of each sentence
using the Stanford parser (Klein and Manning, 2003).
Next, we loaded each sentence?s dependence tree
2The LDC Catalog number of the English Gigaword Fourth
Edition corpus is LDC2009T13.
462
CAUSAL PATTERN 1:
"causes"
?cause
nsubj
?effect
dobj
CAUSAL PATTERN 2:
"causenb"
?cause
jdsco
"uj"
a
?effect
uco
Figure 3: The dependency structures associated with
the causal patterns: ?cause ?causes? ?effect, and
?cause ?brought on? ?effect.
into the RDF3X (Neumann and Weikum, 2008)
implementation of an RDF3 database. Then, we
represented our dependency structures using in the
SPARQL4query language and extracted cause and
effect pairs by issuing SPARQL queries against the
RDF3X database. We used SPARQL and RDF repre-
sentations because they allowed us to easily represent
and reason over graphical structures, such as those of
our dependency trees.
It has been shown that causality often manifests as
a temporal relation (Bethard, 2008; Bethard and Mar-
tin, 2008). The questions presented in this task are
no exception: many of the alternative-premise pairs
necessitate temporal understanding. For example,
consider question 63 provided in Figure 4.
Question 63 (Find the EFFECT)
Premise: The man removed his coat.
Alternative 1: He entered the house.
Alternative 2: He loosened his tie.
Figure 4: Example question 63, which illustrates the ne-
cessity for temporal reasoning.
3The Resource Description Framework (RDF) is is a spec-
ification from the W3C. Information on RDF is available at
http://www.w3.org/RDF/.
3The SPARQL Query Language is defined at http://www.
w3.org/TR/rdf-sparql-query/. An examples of the
WHERE clause for a SPARQL query associated with the brought
on pattern from Figure 3 is provided below:
{ ?a <nsubj> ?cause ;
<token> "brought" ;
<prep> ?b .
?b <token> "on" ;
<pobj> ?effect . }
In order to extract this temporal information, we
automatically annotated our corpus with TimeML
annotations using the TARSQI Toolkit (Verhagen
et al, 2005). Unfortunately, the events represented
in this corpus were too sparse to use directly. To
mitigate this sparsity, we clustered events using the
3,200 Brown clusters5 described in (Turian, 2010).
After all such offline processing has been com-
pleted, we incorporate the knowledge encoded by
this processing in the online components of our sys-
tem (online preprocessing, and feature extraction) as
described in the following section.
2.2 Online Processing
We cast the task of selecting the most plausible al-
ternative as a classification problem, using a support
vector machine (SVM) supervised classifier (using
a linear kernel). To this end, we pre-process each
question for lexical information. We extract parts
of speech (POS) and syntactic dependencies using
the Stanford CoreNLP parser (Klein and Manning,
2003; Toutanova et al, 2003). Stopwords are re-
moved using a manually curated list of one hundred
and one common stopwords; non-content words (de-
fined as words whose POS is not a noun, verb, or
adjective) are also discarded. Additionally, we ex-
tract multi-word expressions (noun collocations6 and
phrasal verbs7). Finally, in order to utilize our of-
fline TimeML annotations, we extract events using
POS. Examples of the retained content words are
underlined in Figures 5, 6, 7 and 8.
After preprocessing each question, we convert
it into two premise-alternative pairs (PREMISE-
ALTERNATIVE1, and PREMISE-ALTERNATIVE2).
For each of these pairs, we attempt to form a bridge
from the causal sentence to the effect sentence, with-
out distinction over whether the cause or effect origi-
nated from the premise or the alternative. This bridge
is provided by four measures, or features, described
in the following section.
5These clusters are available at http://metaoptimize.
com/projects/wordreprs/.
6These were detected using a list of English Wikipedia ar-
ticle titles available at http://dumps.wikimedia.org/
backup-index.html.
7Phrasal verbs were determined using a list avail-
able at http://www.learn-english-today.com/
phrasal-verbs/phrasal-verb-list.htm.
463
3 The Features of the COPACETIC
System
In determining the causal relatedness between a cause
and an effect sentence, we utilize four features. Each
feature calculates a value indicating the perceived
strength of the causal relationship between a cause
and an effect using a different measure of causality.
The four features used by our COPACETIC system
are described in the following subsections.
3.1 Bigram Relatedness
Our first feature measures the degree of relatedness
between all pairs of bigrams (at the token level) in the
cause and effect pair. We do this by calculating the
point-wise mutual Information (PMI) (Fano, 1961)
for all bigram combinations between the candidate
alternative and its premise in the English Gigaword
corpus (Parker et al, 2009) as shown in Equation 1.
PMI(x; y) ? log
p(x, y)
p(x)p(y)
(1)
Under the assumption that distance words are un-
likely to causally influence each other, we only con-
sider co-occurrences within a window of one hundred
tokens when calculating the joint probability of the
PMI. Additionally, we allow for up to two tokens
to occur within a single bigram?s occurrence (e.g.
the phrase pierced her ears would be considered a
match for the bigram pierced ears ). Although these
relaxations skew the values of our calculated PMIs
by artificially lowering the joint probability, we are
only concerned with how the values compare to each
other. Note that because we employ no smoothing,
the PMI of an unseen bigram is set to zero. The max-
imum PMI over all pairs of bigrams is retained as the
value for this feature. Figure 5 illustrates this feature
for Question 495.
3.2 Temporal Relatedness
Although most of the questions in this task focus on
causal relationships, for many questions, the nature
of this causal relationship manifests instead as a tem-
poral one (Bethard and Martin, 2008; Bethard, 2008).
We use temporal link information from TimeML
(Pustejovsky et al, 2005; Pustejovsky et al, 2003)
annotations on our corpus to determine how tempo-
rally related a given cause and effect sentence are.
Question 495 (Find the EFFECT)
Premise: The girl wanted to wear earrings.
Alternative 1: She got her ears pierced.
Alternative 2: She got a tattoo.
Alternative 1 Alternative 2
PMI(wear earrings, pierced ears) = -10.928 PMI(wear earrings, tattoo) = -12.77
PMI(wanted wear, pierced ears) = -13.284 PMI(wanted wear, tattoo) = -14.284
PMI(girl wanted, pierced ears) = -13.437 PMI(girl wanted, tattoo) = -14.762
PMI(girl, pierced ears) = -15.711 PMI(girl, tattoo) = -14.859
Maximum PMI = -10.928 Maximum PMI = -12.77
Figure 5: Example PMI values for bigrams and unigrams
(with content words underlined). Alternative 1 is correctly
chosen as it has largest maxi mum PMI.
This is accomplished by using the point-wise mutual
information (PMI) between all pairs of events from
the cause to the effect (see Equation 1). We define
the relevant probabilities as follows:
? The joint probability (P (x, y)) of a cause and
effect event is defined as the number of times
the cause event participates in a temporal link
ending with the effect event.
? The probability of a cause event (P (x)) is de-
fined as the number of times the cause event
precipitates a temporal link to any event.
? The probability of an effect event (P (y)) is de-
fined as the number of times the effect event
ends a temporal link begun by any event.
We define the PMI to be zero for any unseen pair of
events (and for any pairs involving an unseen event).
The summation of all pairs of PMIs is used as the
value of this feature. Figure 6 shows how this feature
behaves.
Question 468 (Find the CAUSE)
Premise: The dog barked.
Alternative 1: The cat lounged on the couch.
Alternative 2: A knock sounded at the door.
Alternative 1 Alternative 2
PMI(lounge, bark) = 5.60436 PMI(knock, bark) = 5.77867
PMI(sound, bark) = 5.26971
Figure 6: Example temporal PMI values (with content
words underlined). Alternative 2 is correctly chosen as it
has the highest summation.
3.3 Causal Dependency Structures
We attempted to capture the degree of direct causal re-
latedness between a cause sentence and an effect sen-
tence. To determine the strength of this relationship,
464
we considered how often phrases from the cause and
effect sentences occur within a causal dependency
structure. We detect this through the use of twenty-
four8 manually crafted causal patterns (described in
Section 2.1). The alternative that has the maximum
number of matched dependency structures with the
premise is retained as the correct choice. Figure 7
illustrates this feature.
Question 490 (Find the EFFECT)
Premise: The man won the lottery.
Alternative 1: He became rich.
Alternative 2: He owed money.
Alternative 1 Alternative 2
won? rich = 15 won? owed = 5
Figure 7: Example casual dependency matches (with con-
tent words underlined). Alternative 1 is correctly selected
because more patterns extracted ?won? causing ?rich? than
?won? causing ?owed?.
3.4 Polarity Comparison
We observed that many of the questions involve the
dilemma of determining whether a positive premise
is more related to a positive or negative alternative
(and vice-versa). This differs from sentiment analysis
in that rather than determining if a sentence expresses
a negative statement or view, we instead desire the
overall sentimental connotation of a sentence (and
thus of each word). For example, the premise from
Question 494 (Figure 8) is ?the woman became fa-
mous.? Although this sentence makes no positive or
negative claims about the woman, the word ?famous?
? when considered on its own ? implies positive con-
notations.
We capture this information using the Harvard
General Inquirer (Stone et al, 1966). Originally de-
veloped in 1966, the Harvard General Inquirer pro-
vides a mapping from English words to their polarity
(POSITIVE, or NEGATIVE). For example, it de-
notes the word ?abandon? as NEGATIVE, and the
word ?abound? as POSITIVE. We use this informa-
tion by summing the score for all words in a sen-
tence (assigning POSITIVE words a score of 1.0,
NEGATIVE words a score of -1.0, and NEUTRAL or
unseen words a score of 0.0). The difference between
8Twenty-four patterns was deemed sufficient due to time
constraints.
these scores between the cause sentence and the ef-
fect sentence is used as the value of this feature. This
feature is illustrated in Figure 8.
Question 494 (Find the CAUSE)
Premise: The woman became famous.
Alternative 1: Photographers followed her.
Alternative 2: Her family avoided her.
Premise Alternative 1 Alternative 2
famous POSITIVE 1.0 follow NEUTRAL 0.0 avoid NEGATIVE?1.0
photographer NEUTRAL 0.0 family NEUTRAL 0.0
Sum 1.0 Sum 0.0 Sum ?1.0
Figure 8: Example polarity comparison (with content
words underlined). Alternative 1 is correctly chosen as it
has the least difference from the score of the premise.
4 Results
The COPA task of SemEval-2012 provided partici-
pants with 1,000 causal questions, divided into 500
questions for development or training, and 500 ques-
tions for testing. We submitted two systems to the
COPA Evaluation for SemEval-2012, both of which
are trained on the 500 development questions. Our
first system uses only the bigram PMI feature and is
denoted as bigram pmi. Our second system uses
all four features and is denoted as svm combined.
The accuracy of our two systems on the 500 provided
test questions is provided in Table 1 (Gordon et al,
2012). On this task, accuracy is defined as the quo-
tient of dividing the number of questions for which
the correct alternative was chosen by the number of
questions. Although multiple groups registered, ours
were the only submitted results. Note that the differ-
ence in performance between our two systems is not
statistically significant (p = 0.411) (Gordon et al,
2012).
Team ID System ID Score
UTDHLT bigram pmi 0.618
UTDHLT svm combined 0.634
Table 1: Accuracy of submitted systems
The primary hindrance to our approach is in com-
bining each feature ? that is, determining the con-
fidence of each feature?s judgement. Because the
questions vary significantly in their subject matter
and the nature of the causal relationship between
given causes and effects, a single approach is unlikely
465
to satisfy all scenarios. Unfortunately, the problem
of determining which feature best applies to a give
question requires non-trivial reasoning over implicit
semantics between the premise and alternatives.
5 Conclusion
This evaluation has shown that although common-
sense causal reasoning is trivial for humans, it belies
deep semantic reasoning and necessitates a breadth of
world knowledge. Additional progress towards cap-
turing world knowledge by leveraging a large number
of cross-domain knowledge resources is necessary.
Moreover, distilling information not specific to any
domain ? that is, a means of inferring basic and fun-
damental information about the world ? is not only
necessary but paramount to the success of any fu-
ture system desiring to build chains of commonsense
or causal reasoning. At this point, we are merely
approximating such possible distillation.
6 Acknowledgements
We would like to thank the organizers of SemEval-
2012 task 7 for their work constructing the dataset
and overseeing the task.
References
[Bethard and Martin2008] S. Bethard and J.H. Martin.
2008. Learning semantic links from a corpus of parallel
temporal and causal relations. Proceedings of the 46th
Annual Meeting of the ACL-HLT.
[Bethard2008] S Bethard. 2008. Building a corpus of
temporal-causal structure. Proceedings of the Sixth
LREC.
[Fano1961] RM Fano. 1961. Transmission of Information:
A Statistical Theory of Communication.
[Gordon and Swanson2009] A. Gordon and R. Swanson.
2009. Identifying personal stories in millions of weblog
entries. In Third International Conference on Weblogs
and Social Media, Data Challenge Workshop, San Jose,
CA.
[Gordon et al2012] Andrew Gordon, Zornitsa Kozareva,
and Melissa Roemmele. 2012. (2012) SemEval-2012
Task 7: Choice of Plausible Alternatives: An Evalua-
tion of Commonsense Causal Reasoning. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), Montreal.
[Klein and Manning2003] D. Klein and C.D. Manning.
2003. Accurate unlexicalized parsing. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics-Volume 1, pages 423?430.
Association for Computational Linguistics.
[Neumann and Weikum2008] Thomas Neumann and Ger-
hard Weikum. 2008. RDF-3X: a RISC-style engine for
RDF. Proceedings of the VLDB Endowment.
[of Congress. Prints et al1980] Library
of Congress. Prints, Photographs Division, and
E.B. Parker. 1980. Subject headings used in the library
of congress prints and photographs division. Prints and
Photographs Division, Library of Congress.
[Parker et al2009] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2009. English
Gigaword Fourth Edition.
[Pustejovsky et al2003] J Pustejovsky, J Castano, and
R Ingria. 2003. TimeML: Robust specification of
event and temporal expressions in text. AAAI Spring
Symposium on New Directions in Question-Answering.
[Pustejovsky et al2005] J Pustejovsky, Bob Ingria, Roser
Sauri, Jose Castano, Jessica Littman, Rob Gaizauskas,
Andrea Setzer, G. Katz, and I. Mani. 2005. The speci-
fication language TimeML. The Language of Time: A
Reader.
[Roemmele et al2011] Melissa Roemmele, Cos-
min Adrian Bejan, and Andrew S. Gordon. 2011.
Choice of Plausible Alternatives: An Evaluation of
Commonsense Causal Reasoning. 2011 AAAI Spring
Symposium Series.
[Stone et al1966] P. J. Stone, D.C. Dunphy, and M. S.
Smith. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
[Toutanova et al2003] K. Toutanova, D. Klein, C.D. Man-
ning, and Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceed-
ings of the 2003 Conference of NAACL-HLT, pages
173?180. Association for Computational Linguistics.
[Turian2010] J Turian. 2010. Word representations: a sim-
ple and general method for semi-supervised learning.
Proceedings of the 48th Annual Meeting of the ACL,
pages 384?394.
[Verhagen et al2005] M Verhagen, I Mani, and R Sauri.
2005. Automating Temporal Annotation with TARSQI.
In Proceedings of the ACL 2005, pages 81?84.
466
