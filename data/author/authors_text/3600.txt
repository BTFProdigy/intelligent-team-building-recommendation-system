Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1?8
Manchester, August 2008
Two-Phased Event Relation Acquisition:
Coupling the Relation-Oriented and Argument-Oriented Approaches
Shuya Abe Kentaro Inui Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
{shuya-a,inui,matsu}@is.naist.jp
Abstract
Addressing the task of acquiring semantic
relations between events from a large cor-
pus, we first argue the complementarity be-
tween the pattern-based relation-oriented
approach and the anchor-based argument-
oriented approach. We then propose a two-
phased approach, which first uses lexico-
syntactic patterns to acquire predicate pairs
and then uses two types of anchors to iden-
tify shared arguments. The present results
of our empirical evaluation on a large-scale
Japanese Web corpus have shown that (a)
the anchor-based filtering extensively im-
proves the accuracy of predicate pair ac-
quisition, (b) the two types of anchors are
almost equally contributive and combining
them improves recall without losing accu-
racy, and (c) the anchor-based method also
achieves high accuracy in shared argument
identification.
1 Introduction
The growing interest in practical NLP applications
such as question answering, information extrac-
tion and multi-document summarization places in-
creasing demands on the processing of relations
between textual fragments such as entailment and
causal relations. Such applications often need to
rely on a large amount of lexical semantic knowl-
edge. For example, a causal (and entailment) rela-
tion holds between the verb phrases wash some-
thing and something is clean, which reflects the
commonsense notion that if someone has washed
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
something, this object is clean as a result of the
washing event. A crucial issue is how to ob-
tain and maintain a potentially huge collection of
such event relation instances. This paper addresses
the issue of how to automatically acquire such in-
stances of relations between events (henceforth,
event relation instances) from a large-scale text
collection.
Motivated by this issue, several research groups
have reported their experiments on automatic ac-
quisition of causal, temporal and entailment rela-
tions between event expressions (typically verbs
or verb phrases) (Lin and Pantel, 2001; Inui et
al., 2003; Chklovski and Pantel, 2005; Torisawa,
2006; Pekar, 2006; Zanzotto et al, 2006; Abe et
al., 2008, etc.). As we explain below, however,
none of these studies fully achieves the goal we
pursue in this paper.
An important aspect to consider in event relation
acquisition is that each event has arguments. For
example, the causal relation between wash some-
thing and something is clean can be represented
naturally as:
(1) wash(obj:X) ?
cause
is clean(subj:X),
where X is a logical variable denoting that the
filler of the object slot of the wash event should be
shared (i.e. identical) with the filler of the subject
slot of the is clean event.
To be more general, an instance of a given rela-
tion R can be represented as:
(2) pred
1
(arg
1
:X) ?
R
pred
2
(arg
2
:X),
where pred
i
is a natural language predicate, typ-
ically a verb or adjective, and X is a logical vari-
able denoting which argument of one predicate and
which argument of the other are shared. The goal
we pursue in this paper is therefore not only (a)
to find predicate pairs that are of a given relation
1
type, but also (b) to identify the arguments shared
between the predicates if any. We call the for-
mer subtask predicate pair acquisition and the lat-
ter shared argument identification. As we review
in the next section, however, existing state-of-the-
art methods for event relation acquisition are de-
signed to achieve only either of these two subtasks
but not both. In this paper, we propose a two-
phased method, which first uses lexico-syntactic
patterns to acquire predicate pairs for a given re-
lation type and then uses two kinds of anchors to
identify shared arguments.
2 Previous work
Existing methods for event relation acquisition can
be classified into two approaches, which we call
the pattern-based approach and anchor-based ap-
proach in this paper.
The common idea behind the pattern-based ap-
proach is to use a small number of manually se-
lected generic lexico-syntactic co-occurrence pat-
terns (LSPs or simply patterns). Perhaps the sim-
plest way of using LSPs for event relation acqui-
sition can be seen in the method Chklovski and
Pantel (2005) employ to develop their knowledge
resource called VerbOcean. Their method uses a
small number of manually selected generic LSPs
such as to ?Verb-X? and then ?Verb-Y?
1
to obtain
six types of semantic relations including strength
(e.g. taint ? poison) and happens-before (e.g.
marry ? divorce). The use of such generic patterns,
however, tends to be high recall but low precision.
Chklovski and Pantel (2005), for example, report
that their method obtains about 29,000 verb pairs
with 65.5% precision.
This low-precision problem requires an addi-
tional component for pruning extracted relations.
This issue has been addressed from a variety of
angles. For example, some devise heuristic sta-
tistical scores and report their impact on preci-
sion (Chklovski and Pantel, 2005; Torisawa, 2006;
Zanzotto et al, 2006). Another way is to incor-
porate a classifier trained with supervision. Inui
et al (2003), for example, use a Japanese generic
causal connective marker tame (because) and a
supervised classifier learner to separately obtain
four types of causal relations: cause, precondi-
tion, effect and means. More recently, Abe et
al. (2008) propose to extend Pantel and Pennac-
1
A ?? included in an LSP denotes, throughout this paper,
a variable slot to be filled with an event expression. The filler
of ?? denotes either the lexical or syntactic constraints on the
slot or an example that is to fill the slot.
chiotti (2006)?s Espresso algorithm, which induces
specific reliable LSPs in a bootstrapping man-
ner for entity-entity relation extraction, so that
the extended algorithm can apply to event rela-
tions. Their method learns a large number of rel-
atively specific patterns such as cannot ?find out
(something)? due to the lack of ?investigation? in a
boot-strapping fashion, which produces a remark-
able improvement on precision.
The anchor-based approach, on the other hand,
has emerged mainly in the context of paraphrase
and entailment acquisition. This approach uses in-
formation of argument fillers (i.e. anchors) of each
event expression as a useful clue for identifying
event relations. A popular way of using such ar-
gument information relies on the distributional hy-
pothesis (Harris, 1968) and identifies synonymous
event expressions by seeking a set of event expres-
sions whose argument fillers have a similar distri-
bution. Such algorithms as DIRT (Lin and Pantel,
2001) and TE/ASE (Szpektor et al, 2004) repre-
sent this line of research.
Another way of using argument information is
proposed by Pekar (2006), which identifies candi-
date verb pairs for the entailment relation by im-
posing criteria: (a) the two verbs must appear in
the same local discourse-related context and (b)
their arguments need to refer to the same par-
ticipant, i.e. anchor. For example, if a pair of
clauses Mary bought a house. and The house be-
longs to Mary. appear in a single local discourse-
related context, two pairs of verbs, buy(obj:X) ?
belong(subj:X) and buy(subj:X) ? belong(to:X) are
identified as candidate entailment pairs.
It is by now clear that the above two approaches,
which apparently have emerged somewhat inde-
pendently, could play a complementary role with
each other. Pattern-based methods, on the one
hand, are designed to be capable of discriminat-
ing relatively fine-grained relation types. For ex-
ample, the patterns used by Chklovski and Pan-
tel (2005) identify six relation types, while Abe
et al (2008) identify two of the four causal rela-
tion types defined by Inui et al (2003). However,
these methods are severely limited for the purpose
of shared argument identification because lexico-
syntactic patterns are not a good indication of
argument-shared structure in general. The anchor-
based approach, on the other hand, works well for
identifying shared arguments simply because it re-
lies on argument information in identifying syn-
onymous or entailment verb pairs. However, it has
no direct means to discriminate more fine-grained
2
specific relations such as causality and backward
presupposition. To sum up, the pattern-based ap-
proach tends to be rather relation-oriented while
the anchor-based approach tends to be argument-
oriented.
In spite of this complementarity, however, to our
best knowledge, the issue of how to benefit from
both approaches has never been paid enough at-
tention. An interesting exception could be found
in Torisawa (2006)?s method of combining verb
pairs extracted with a highly generic connective
pattern ?Verb-X? and ?Verb-Y? together with the
co-occurrence statistics between verbs and their ar-
guments. While the reported results for inference
rules with temporal ordering look promising, it is
not clear yet, however, whether the method ap-
plies to other types of relations because it relies
on relation-specific heuristics.
3 Two-phased event relation acquisition
3.1 The basic idea
The complementarity between the pattern-based
relation-oriented approach and the anchor-based
argument-oriented approach as discussed above
naturally leads us to consider combining them.
The method we explore in this paper is illustrated
in Figure 1. The overall process has two phrases:
predicate pair acquisition followed by shared ar-
gument identification. Given a relation type for ac-
quisition, we first acquire candidate predicate pairs
that are likely to be of the given relation exploiting
a state-of-the-art pattern-based method. We then,
in the second phase, seek anchors indicative of the
shared argument for each acquired predicate pair.
We consider two kinds of anchors: instance-based
anchors and type-based anchors. If anchors are
found, the predicate pair is verified and the asso-
ciated argument pair is identified as the shared ar-
gument; otherwise, the predicate pair is discarded.
As we demonstrate in the section for empirical
evaluation, this verification process boosts the ac-
curacy as well as identifying shared arguments.
3.2 Predicate pair acquisition
For predicate pair acquisition, we can choose
one from a range of state-of-the-art pattern-based
methods. Among others, in our experiments, we
adopted Abe et al (2008)?s method because it had
an advantage in that it was capable of learning pat-
terns as well as relation instances.
Abe et al (2008)?s method is based on Pan-
tel and Pennacchiotti (2006)?s Espresso algorithm,
which is originally designed to acquire relations
between entities. Espresso takes as input a small
number of seed instances of a given target rela-
tion and iteratively learns co-occurrence patterns
and relation instances in a bootstrapping manner.
Abe et al have made several extensions to it so
that it can be applied to event relations. Since the
details of this phase are not the focus of this paper,
we refer the reader to (Abe et al, 2008) for further
information.
3.3 Shared argument identification
For each of the predicate pairs acquired in the pre-
vious phase, in shared argument identification, we
use anchors to identify which argument is shared
between the predicate pair. To find anchors indica-
tive of shared arguments, we have so far examined
two methods. We detail each below.
3.3.1 Instance-based anchors
Inspired by Pekar (2006)?s way of using an-
chors for verb entailment acquisition, we assume
that if two related predicates have a shared argu-
ment, they must tend to appear in the same local
discourse-related context with the shared argument
filled with the same noun phrase (i.e. anchor).
As an example, let us consider discourse (2a) in
Figure 1. In this local discourse context, the noun
bread appears twice, and one bread fills the subject
slot of burn while the other fills the object slot of
bake. In such a case, we assume the two breads re-
fer to the same object, namely anchor, and the sub-
ject of burn and the object of bake are shared with
each other. We call such anchors instance-based
anchors for the sake of contrast with type-based
anchors, which we describe in 3.3.2.
We implement this assumption in the following
way. Given a pair of predicates Pred
1
and Pred
2
,
we search a corpus for tuples ?Pred
1
-Arg
1
; Pred
2
,
Arg
2
; Anc? satisfying the following conditions:
(a) Anchor word Anc is the head of a noun phrase
filling argument Arg
1
of Pred
1
appearing in a
Web page.
(b) Anc also fills argument Arg
2
of Pred
2
appear-
ing in the same Web page as above.
(c) Anc must not be any of those in the stop list.
(d) pmi(Pred
i
, Arg
i
) ? ?1.0 for i ? {1, 2}
For our experiments, we manually created the stop
list, which contained 219 words including pro-
nouns, numerals and highly generic nouns such as
3
Figure 1: Two-phased event relation acquisition
4
??? (thing)?, ??? (thing)? and ??? (time)?.
pmi(Pred
i
, Arg
i
) in condition (d) is the point-wise
mutual information between Pred
i
and Arg
i
. This
condition is imposed for pruning wrong anchors
misidentified due to parsing errors.
While Pekar carefully defines boundaries of lo-
cal discourse-related context, we simply assume
that every pair of predicates sharing an anchor in
a Web page is somewhat related ? unlike Pekar,
we do not impose such constraints as paragraph
boundaries. Nevertheless, as we show later in
the evaluation section, our assumption works pre-
cisely enough because the looseness of our dis-
course boundary constraint is compensated by the
constraints imposed by lexico-syntactic patterns.
We finally calculate an anchor set for each argu-
ment pair Pred
1
-Arg
1
and Pred
2
-Arg
2
by accumu-
lating the obtained tuples:
AnchorSet(Pred
1
-Arg
1
, Pred
2
-Arg
2
)
= {Arg|?Pred
1
-Arg
1
; Pred
2
-Arg
2
; Anc?}.
3.3.2 Type-based anchors
Let us consider sentences (3a) and (3b) in
Figure 1. These two sentences both contain pred-
icates bake and burn. In (3a), the noun bread fills
the object slot of bake, while in (3b) the same noun
bread fills the subject slot of burn. In such a case,
we assume the noun bread to be an anchor indi-
cating that the object of bake and the subject of
burn are shared with each other. We call such an-
chors type-based anchors because bread in (3a)
and bread in (3b) do not refer to the same object
but are identical just as type.
Given a pair of predicates Pred
1
and Pred
2
, we
search a corpus for sentences where Pred
1
and
Pred
2
co-occur, and calculate the frequency counts
of their argument fillers appearing in those sen-
tences:
? If argument Arg
1
of Pred
1
is filled by noun
Anc, increment the count of ?Pred
1
-Arg
1
;
Pred
2
; Anc?.
? If argument Arg
2
of Pred
2
is filled by noun
Anc, increment the count of ?Pred
1
; Pred
2
-
Arg
2
; Anc?.
We then identify the intersection between the filler
sets of Pred
1
-Arg
1
and Pred
2
-Arg
2
as the anchor
set of that argument pair. Namely,
AnchSet(Pred
1
-Arg
1
, Pred
2
-Arg
2
) = S
1
? S
2
,
where
S
1
= {Arg|?Pred
1
-Arg
1
; Pred
2
; Anc?},
S
2
= {Arg|?Pred
1
; Pred
2
-Arg
2
; Anc?}.
3.3.3 Application of anchor sets
We say an argument pair covered by anchors
only if any anchor is found for it. Analogously,
we say a predicate pair covered by anchors only if
any argument pair associated with it is covered by
anchors. In the phase of shared argument identifi-
cation, for each given predicate pair, we carry out
the following procedure:
1. Discard the predicate pair if it is not covered
by anchors.
2. Choose maximally k-most frequent argument
pairs associated with the predicate pair (k = 3
in our experiments).
3. Choose maximally l-most frequent anchors
for each chosen argument pair (l = 3).
4 Experiments
4.1 Settings
For an empirical evaluation, we used a sample
of approximately 500M sentences taken from the
Web corpus collected by Kawahara and Kuro-
hashi (2006). The sentences were dependency-
parsed with CaboCha (Kudo and Matsumoto,
2002), and co-occurrence samples of event men-
tions were extracted. Event mentions with patterns
whose frequency was less than 20 were discarded
in order to reduce computational costs.
In our experiments, we considered two of Inui et
al. (2003)?s four types of causal relations: action-
effect relations (Effect in Inui et al?s terminology)
and action-means relations (Means). An action-
effect relation holds between events x and y if and
only if non-volitional event y is likely to happen as
either a direct or indirect effect of volitional action
x. For example, the action X-ga undou-suru (X ex-
ercises) and the event X-ga ase-o-kaku (X sweats)
are considered to be in this type of relation. We
did not require the necessity for an effect. For ex-
ample, while nomu (drink) does not necessarily re-
sult in futsukayoi-ni naru (have a hangover), the
assessors judged this pair correct because one can
at least say that the latter sometimes happens as a
result of the former. An action-means relation, on
the other hand, holds between events x and y if and
only if volitional action y is likely to be done as a
part/means of volitional action x. For example, if
5
case a event-pair is X-ga hashiru (X runs) is con-
sidered as a typical action that is often done as a
part of the action X-ga undou-suru (X exercises).
For our experiments, we manually built a lex-
icon of over 12,000 verbs with volitionality la-
bels, obtaining 8,968 volitional verbs, 3,597 non-
volitional and 547 ambiguous. Volitional verbs
include taberu (eat) and kenkyu-suru (research),
while non-volitional verbs include atatamaru (get
warm), kowareru (to break-vi) and kanashimu (be
sad). Volitionality information was used as a fea-
ture of predicate slots in pattern-based predicate
pair acquisition.
4.2 Results and discussion
4.2.1 Predicate pair acquisition
We ran the extended Espresso algorithm start-
ing with 25 positive and 4 negative seed rela-
tion instances for the action-effect relation and 174
positive and 131 negative seed relations for the
action-means relation. As a result, we obtained
9,511 patterns with 22,489 relation instances for
action-effect and 14,119 co-occurrence patterns
with 13,121 relation instances for action-means
after 40 iterations of pattern and instance rank-
ing/selection. The threshold parameters for select-
ing patterns and instances were decided in a pre-
liminary trial. Some of the acquired instances are
shown in Table 1.
We next randomly sampled 100 predicate pairs
from each of four sections (1?500, 501?1500,
1501?3500 and 3500?7500) of the ranks of the ac-
quired pairs for each relation class. Two annotators
were asked to judge the correctness of each pred-
icate pair (i.e. 800 pairs in total). They judged a
predicate pair to be correct if they could produce
an appropriate relation instance from that pair by
adding some shared argument. For example, the
pair??? (hang/put/call) and???? (connect)
was judged correct because it could constitute such
a relation instance as:
(3) ??? (?:X) ?
effect
???? (?:X)
(X ? {?? })
make(obj:X) ?
effect
go-through(subj:X)
(X ? {phone-call})
Unfortunately, the two annotators did not agree
with each other very much. out of the 400 sam-
ples, they agreed only on 294 for action-effect and
297 for action-means. However, a closer look at
the results revealed that the judgements of the one
annotator were considerably but very consistently
Table 2: Accuracy and recall of relation classifica-
tion
LSPs covered by anchors
all top-N instance type combined
action-effect 400 254 175 169 254
269 185 144 143 206
(accuracy) (0.67) (0.72) (0.82) (0.84) (0.81)
(recall) (1.00) (0.68) (0.53) (0.53) (0.76)
action-means 400 254 178 176 254
280 193 143 140 200
(accuracy) (0.70) (0.75) (0.80) (0.79) (0.78)
(recall) (1.00) (0.68) (0.51) (0.50) (0.71)
more tolerant than the other. Assuming that the
judgements of the latter correct, the precision and
recall of those of the former would be 0.71 and
0.97 for action-effect, and 0.75 and 0.99 for action-
means. These figures indicate that the two annota-
tors agreed quite well with respect to the ?good-
ness? of a sample, while having different criteria
for strictness. For our evaluation, we decided to
lean to the strict side and considered a sample cor-
rect only if it was judged correct by both anno-
tators. The accuracy and recall achieved by the
pattern-based model is shown in the column ?all?
under ?LSPs? in Table 2.
We then applied the anchor-based methods de-
scribed in Section 3.3 to the above 800 sampled
predicate pairs. The results are shown in the col-
umn ?covered by anchors? of Table 2. Since the
tendency for both relation classes is more or less
the same, let us focus only on the results for action-
effect.
As shown in the column ?all? under ?LSPs? in
the table, the pattern-based method covered 269
out of the 400 predicate pairs sampled above. The
instance-based anchors (?instance?) covered 175
out of the 400 predicate pairs sampled above, and
144 of them were correct with respect to relation
type. We calculate its accuracy by dividing 144
by 175 and recall by dividing 144 by 269. These
figures indicate that the instance-based anchors
chose correct predicate pairs at a very high accu-
racy while sacrificing recall. The recall, however,
can be extensively improved without losing accu-
racy by combining the instance-based and type-
based anchors, where we considered a predicate
pair covered if it was covered by either of the
instance-based and type-based anchors. The re-
sults are shown in the column ?combined? under
?covered by anchors? in the same table. While the
type-based anchors exhibited the same tendency as
the instance-based anchors (namely, high accuracy
6
Table 1: Examples
Pred1 Arg1 Pred2 Arg2 Anc
action-effect begin(????) obj(?) finish(????) subj(?) installation(? ? ? ? ? ?),
transaction(????????)
action-effect design(??????) obj(?) be pretty(????) subj(?) logotype(??)
action-effect sleep(??) in(?) be sleep(???) in(?) bed(???), futon(??)
action-means cure(????) by(?) prescribe(????) obj(?) medicine(?)
action-means cure(????) obj(?) prescribe(????) for(?) patient(??)
action-means go home(????) by(?) drive(????) obj(?) car(?), car(???)
action-means use(????) obj(?) copy(?????) obj(?) file(????), data(???)
and low recall), their coverage reasonably differed
from each other, which contributed to the improve-
ment of recall.
To summarize so far, the pattern-based method
we adopted in the experiment generated a sub-
stantial number of predicate pairs with a accuracy
comparative to the state of the art. The accuracy
was, however, further boosted by applying both
instance-based and type-based anchors. This ef-
fect is particularly important because, to our best
knowledge, very few pattern-based relation acqui-
sition models have been reported to achieve as high
a accuracy as what we achieved. In the case of our
pattern-based model, for reference, the 254 highly
ranked pairs of the 400 samples included only 185
correct pairs, which is worse than the 206 pairs
covered by anchors for both accuracy and recall
(see the ?top-N? column under ?LSPs? in Table 2.
This difference also leads us to consider incor-
porating our anchor-based filtering into the boot-
strapping cycles of pattern-based predicate pair ac-
quisition.
4.2.2 Shared argument identification
We next investigated the accuracy of shared ar-
gument identification. For each of the aforemen-
tioned predicate pairs covered by anchors (the 254
pairs for action-effect and 254 for action-means),
we asked the same two annotators as above to
judge the correctness of the shared argument in-
formation. The results of combination are shown
in Table 3.
?arg-strict? shows the results of the strict judg-
ments where the shared argument was considered
to be correctly identified only when the most fre-
quent argument pair was judged correct, while
?arg-lenient? shows the results of the lenient judg-
ments where the shared argument was considered
to be correctly identified when either of the three
most frequent argument pairs was judged correct.
For judging the correctness of an argument pair,
we had three degrees of strictness. In the most
strict criterion (?anc-strict?), an argument pair was
judged correct only when its maximally three an-
chor words were all correct, while in ?anc-lenient?,
an argument pair was judged correct when any of
the three most frequent anchor words was correct.
In ?anc-any?, an argument pair was judged correct
as far as an annotator could think of any appropri-
ate anchor word for it. While the inter-annotator
agreement was not very high, with the kappa co-
efficient in the ?arg-strict? and ?anc-any? setting
0.47 for action-effect and 0.42 for action-effect),
one was again consistently more tolerant than the
other. For the same reason as argued in 4.2.1, we
considered an acquired relation correct only if both
annotators judged it correct.
In this experiment, predicate pairs that had been
judged wrong with respect to relation types were
all considered wrong in all the settings. The upper
bounds of accuracy, therefore, are given by those
in Table 2. For ?arg-?? with the ?combined? an-
chors, for example, the upper bound of accuracy
is 0.81. Since ?arg-lenient? with ?combined? and
?anc-lenient? achieved 0.76 accuracy, our method
turned out to be reasonably precise in identifying
argument pairs and their fillers. Paying attention
to ?arg-strict? and ?anc-strict?, on the other hand,
one can see a considerable drop from the lenient
case, which needs to be further investigated.
5 Conclusion and future work
Motivated by the complementarity between the
pattern-based relation-oriented approach and the
anchor-based argument-oriented approach to event
relation acquisition, we have explored a two-
phased approach, which first uses patterns to ac-
quire predicate pairs and then uses two types of
anchors to identify shared arguments, reporting on
the present results of our empirical evaluation. The
results have shown that (a) the anchor-based fil-
tering extensively improves the accuracy of pred-
icate pair acquisition, (b) the instance-based and
type-based anchors are almost equally contributive
and combining them improves recall without los-
7
Table 3: Accuracy of shared argument identification
action-effect action-means
anc-strict anc-lenient anc-any anc-strict anc-lenient anc-any
instance 0.64 0.71 0.71 0.61 0.66 0.66
arg-strict type 0.60 0.63 0.65 0.61 0.65 0.67
combined 0.60 0.65 0.66 0.58 0.62 0.64
instance 0.78 0.80 0.80 0.73 0.75 0.76
arg-lenient type 0.68 0.71 0.72 0.67 0.69 0.71
combined 0.74 0.76 0.77 0.71 0.73 0.74
ing accuracy, and (c) the anchor-based method also
achieves high accuracy in shared argument identi-
fication.
Our future direction will be two-fold. One is
evaluation. Clearly, more comprehensive evalu-
ation needs to be done. For example, the ac-
quired relation instances should be evaluated in
some task-oriented manner. The other intriguing
issue is how our anchor-based method for shared
argument identification can benefit from recent ad-
vances in coreference and zero-anaphora resolu-
tion (Iida et al, 2006; Komachi et al, 2007, etc.).
References
Abe, Shuya, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning
cooccurrence patterns and fertilizing cooccurrence
samples with verbal nouns. In Proceedings of the
3rd International Joint Conference on Natural Lan-
guage Processing, pages 497?504.
Chklovski, Timothy and Patrick Pantel. 2005. Global
path-based refinement of noisy graphs applied to
verb semantics. In Proceedings of Joint Conference
on Natural Language Processing.
Harris, Zelling. 1968. Mathematical structures of
language. Interscience Tracts in Pure and Applied
Mathematics.
Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2006.
Exploiting syntactic patterns as clues in zero-
anaphora resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL, pages
625?632. Association for Computational Linguis-
tics.
Inui, Takashi, Kentaro Inui, and Yuji Matsumoto. 2003.
What kinds and amounts of causal knowledge can
be acquired from text by using connective markers
as clues? In Proceedings of the 6th International
Conference on Discovery Science,, pages 180?193.
Kawahara, Daisuke and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 176?183.
Komachi, Mamoru, Ryu Iida, Kentaro Inui, and Yuji
Matsumoto. 2007. Learning based argument struc-
ture analysis of event-nouns in japanese. In Proceed-
ings of the Conference of the Pacific Association for
Computational Linguistics, pages 120?128.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning 2002 (COLING 2002 Post-Conference
Workshops), pages 63?69.
Lin, Dekang and Patrick Pantel. 2001. Dirt: discovery
of inference rules from text. In Proceedings of the
seventh ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 323?
328.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
ACL, pages 113?120.
Pekar, Viktor. 2006. Acquisition of verb entailment
from text. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 49?56.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Lin, Dekang and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
41?48, Barcelona, Spain. Association for Computa-
tional Linguistics.
Torisawa, Kentaro. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of Human Language Technology Con-
ference/North American chapter of the Association
for Computational Linguistics annual meeting (HLT-
NAACL06), pages 57?64.
Zanzotto, Fabio Massimo, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 849?856.
8
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 881?888
Manchester, August 2008
Emotion Classification Using Massive Examples Extracted from the Web
Ryoko TOKUHISA??
?Toyota Central R&D Labs., INC.
Nagakute Aichi JAPAN
tokuhisa@mosk.tytlabs.co.jp
Kentaro INUI?
?Nara Institute of Science and Technology
Ikoma Nara JAPAN
{ryoko-t,inui,matsu}@is.naist.jp
Yuji MATSUMOTO?
Abstract
In this paper, we propose a data-oriented
method for inferring the emotion of a
speaker conversing with a dialog system
from the semantic content of an utterance.
We first fully automatically obtain a huge
collection of emotion-provoking event in-
stances from the Web. With Japanese cho-
sen as a target language, about 1.3 million
emotion provoking event instances are ex-
tracted using an emotion lexicon and lexi-
cal patterns. We then decompose the emo-
tion classification task into two sub-steps:
sentiment polarity classification (coarse-
grained emotion classification), and emo-
tion classification (fine-grained emotion
classification). For each subtask, the
collection of emotion-proviking event in-
stances is used as labelled examples to
train a classifier. The results of our ex-
periments indicate that our method signif-
icantly outperforms the baseline method.
We also find that compared with the single-
step model, which applies the emotion
classifier directly to inputs, our two-step
model significantly reduces sentiment po-
larity errors, which are considered fatal er-
rors in real dialog applications.
1 Introduction
Previous research into human-computer interac-
tion has mostly focused on task-oriented dialogs,
where the goal is considered to be to achieve a
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
given task as precisely and efficiently as possi-
ble by exchanging information required for the
task through dialog (Allen et al, 1994, etc.).
More recent research (Foster, 2007; Tokuhisa and
Terashima, 2006, etc.), on the other hand, has been
providing evidence for the importance of the af-
fective or emotional aspect in a wider range of di-
alogic contexts, which has been largely neglected
in the context of task-oriented dialogs.
A dialog system may be expected to serve, for
example, as an active listening 1 partner of an el-
derly user living alone who sometimes wishes to
have a chat. In such a context, the dialog system
is expected to understand the user?s emotions and
sympathize with the user. For example, given an
utterence I traveled far to get to the shop, but it
was closed from the user, if the system could infer
the user?s emotion behind it, it would know that
it would be appropriate to say That?s too bad or
That?s really disappointing. It can be easily imag-
ined that such affective behaviors of a dialog sys-
tem would be beneficial not only for active listen-
ing but also for a wide variety of dialog purposes
including even task-oriented dialogs.
To be capable of generating sympathetic re-
sponses, a dialog system needs a computational
model that can infer the user?s emotion behind
his/her utterence. There have been a range of stud-
ies for building a model for classifying a user?s
emotions based on acoustic-prosodic features and
facial expressions (Pantic and Rothkrantz, 2004,
etc.). Such methods are, however, severely lim-
ited in that they tend to work well only when the
user expresses his/her emotions by ?exaggerated?
1Active listening is a specific communication skill, based
on the work of psychologist Carl Rogers, which involves giv-
ing free and undivided attention to the speaker (Robertson,
2005).
881
	
		
	
	


	
	



	

	

	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1065?1074, Prague, June 2007. c?2007 Association for Computational Linguistics
Extracting Aspect-Evaluation and Aspect-of Relations in Opinion Mining
Nozomi Kobayashi ? Kentaro Inui, and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{nozomi-k,inui,matsu}@is.naist.jp
Abstract
The technology of opinion extraction allows
users to retrieve and analyze people?s opin-
ions scattered over Web documents. We de-
fine an opinion unit as a quadruple consist-
ing of the opinion holder, the subject being
evaluated, the part or the attribute in which
the subject is evaluated, and the value of the
evaluation that expresses a positive or neg-
ative assessment. We use this definition as
the basis for our opinion extraction task. We
focus on two important subtasks of opinion
extraction: (a) extracting aspect-evaluation
relations, and (b) extracting aspect-of re-
lations, and we approach each task using
methods which combine contextual and sta-
tistical clues. Our experiments on Japanese
weblog posts show that the use of contex-
tual clues improve the performance for both
tasks.
1 Introduction
The explosive increase in Web communication has
attracted increasing interest in technologies for auto-
matically mining personal opinions from Web doc-
uments such as product reviews and weblogs. Such
technologies would benefit users who seek reviews
on certain consumer products of interest.
Previous approaches to the task of mining a large-
scale document collection of customer opinions (or
? Currently, NTT Cyber Space Laboratories,
1-1, Hikarinooka, Yokosuka, Kanagawa, 239-0847 Japan
reviews) can be classified into two approaches: Doc-
ument classification and information extraction. The
former is the task of classifying documents or pas-
sages according to their semantic orientation such as
positive vs. negative. This direction has been form-
ing the mainstream of research on opinion-sensitive
text processing (Pang et al, 2002; Turney, 2002,
etc.). The latter, on the other hand, focuses on the
task of extracting opinions consisting of information
about, for example, ?who feels how about which as-
pect of what product? from unstructured text data.
In this paper, we refer to this information extraction-
oriented task as opinion extraction. In contrast to
sentiment classification, opinion extraction aims at
producing richer information and requires an in-
depth analysis of opinions, which has only recently
been attempted by a growing but still relatively small
research community (Yi et al, 2003; Hu and Liu,
2004; Popescu and Etzioni, 2005, etc.).
Most previous work on customer opinion ex-
traction assumes the source of information to be
customer reviews collected from customer review
sites (Popescu and Etzioni, 2005; Hu and Liu, 2004;
Liu et al, 2005). In contrast, in this paper, we con-
sider the task of extracting customer opinions from
unstructured weblog posts. Compared with extrac-
tion from review articles, extraction from weblogs
is more challenging because weblog posts tend to
exhibit greater diversity in topics, goals, vocabu-
lary, style, etc. and are much more likely to in-
clude descriptions irrelevant to the subject in ques-
tion. In this paper, we first describe our task set-
ting of opinion extraction. We conducted a corpus
study and investigated the feasibility of the task def-
1065
inition by showing the statistics and inter-annotator
agreement of our corpus annotation. Next, we show
that the crucial body of the above opinion extrac-
tion task can be decomposed into two kinds of re-
lation extraction, i.e. aspect-evaluation relation ex-
traction and aspect-of relation extraction. For exam-
ple, the passage ?I went out for lunch at the Deli
and ordered a curry with chicken. It was pretty
good? has an aspect-evaluation relation ?curry with
chicken, was good? and an aspect-of relation ?The
Deli, curry with the chicken?. The former task can
be regarded as a special type of predicate-argument
structure analysis or semantic role labeling. The
latter, on the other hand, can be regarded as bridg-
ing reference resolution (Clark, 1977), which is the
task of identifying relations between definite noun
phrases and discourse-new entities implicitly related
to some previously mentioned entities.
Most of the previous work on customer opinion
extraction, however, does not adopt the state-of-the-
art techniques in those fields, relying only on sim-
ple proximity-based or pattern-based methods. In
this context, this paper empirically shows that incor-
porating machine learning-based techniques devised
for predicate-argument structure analysis and bridg-
ing reference resolution improve the performance
of both aspect-evaluation and aspect-of relation ex-
traction. Furthermore, we also show that combin-
ing contextual clues with a common co-occurrence
statistics-based technique for bridging reference res-
olution makes a significant improvement on aspect-
of relation extraction.
2 Opinion extraction: Task design
Our present goal is to build a computational model
to extract opinions from Web documents in such a
form as: Who feels how on which aspects of which
subjects. Given the passage presented in Figure 1,
for example, the opinion we want to extract is: ?the
writer feels that the colors of pictures taken with
Powershot (product) are beautiful.? As suggested
by this example, we consider it reasonable to start
with an assumption that most evaluative opinions
can be structured as a frame composed of the fol-
lowing constituents:
Opinion holder The person who is making an eval-
uation. An opinion holder is typically the first
  

		Exploiting Lexical Conceptual Structure
for Paraphrase Generation
Atsushi Fujita1, Kentaro Inui2, and Yuji Matsumoto2
1 Graduate School of Informatics, Kyoto University
fujita@pine.kuee.kyoto-u.ac.jp
2 Graduate School of Information Science, Nara Institute of Science and Technology
{inui, matsu}@is.naist.jp
Abstract. Lexical Conceptual Structure (LCS) represents verbs as semantic
structures with a limited number of semantic predicates. This paper attempts to
exploit how LCS can be used to explain the regularities underlying lexical and
syntactic paraphrases, such as verb alternation, compound word decomposition,
and lexical derivation. We propose a paraphrase generation model which trans-
forms LCSs of verbs, and then conduct an empirical experiment taking the para-
phrasing of Japanese light-verb constructions as an example. Experimental results
justify that syntactic and semantic properties of verbs encoded in LCS are useful
to semantically constrain the syntactic transformation in paraphrase generation.
1 Introduction
Automatic paraphrasing has recently been attracting increasing attention due to its po-
tential in a broad range of natural language processing tasks. For example, a system that
is capable of simplifying a given text, or showing the user several alternative expres-
sions conveying the same content, would be useful for assisting a reader.
There are several classes of paraphrase that exhibit a degree of regularity. For exam-
ple, paraphrasing associated with verb alternation, lexical derivation, compound word
decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such
classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in
which a compound noun is transformed into a verb phrase. Likewise, paraphrases in-
volving an LVC as in (3) and (4) (from [4]) have considerable similarities.
(1) s. His machine operation is very good.
t. He operates the machine very well.
(2) s. My son?s bat control is unskillful yet.
t. My son controls his bat poorly yet.
(3) s. Steven made an attempt to stop playing.
t. Steven attempted to stop playing.
(4) s. It had a noticeable effect on the trade.
t. It noticeably affected the trade.
1 For each example, ?s? and ?t? denote an original sentence and its paraphrase, respectively. Note
that our target language is Japanese. English examples are used for an explanatory purpose.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 908?919, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
Exploiting Lexical Conceptual Structure for Paraphrase Generation 909
However, the regularity we find in these examples is not so simple that it cannot be
captured only in syntactic terms. For example, the transformation pattern as in (1) and
(2) does not apply to another compound noun ?machine translation.? We can also find
a range of varieties in paraphrasing of LVCs as we describe in Section 3.
In spite of this complexity, the regularity each paraphrase class exhibits were ex-
plained by recent advances in lexical semantics, such as the Lexical Conceptual Struc-
ture (LCS) [8] and the Generative Lexicon [17]. According to the LCS, for instance,
a wide variety of paraphrases including word association within compounds, transitiv-
ity alternation, and lexical derivation, were explained by means of the syntactic and
semantic properties of the verb involved. The systematicity underlying such linguistic
accounts is intriguing also from the engineering viewpoint as it could enable us to take
a more theoretically motivated but still practical approach to paraphrase generation.
The issue we address in this paper is to empirically clarify (i) what types of regular-
ities underlying paraphrases can be explained by means of lexical semantics and how,
and (ii) how lexical semantics theories can be enhanced with feedback from practical
use, namely, paraphrase generation. We make an attempt to exploit the LCS among sev-
eral lexical semantics frameworks, and propose a paraphrase generation model which
utilizes LCS combining with syntactic transformation.
2 Lexical Conceptual Structure
2.1 Basic Framework
Among several frameworks of lexical semantics, we focus on the Lexical Conceptual
Structure (LCS) [8] due to the following reasons. First, several studies [9,3,19] have
shown that the theory of the LCS provides a systematic explanation of semantic de-
composition as well as syntax determines. In particular, Kageyama [9] has shown that
even a simple typology of LCS can explain a wide variety of linguistic phenomena in-
cluding word association within compounds, transitivity alternation, and lexical deriva-
tion. Second, large-scale LCS dictionaries have been developed through practical use
on machine translation and compound noun analysis [3,19]. The LCS dictionary for
English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classifica-
tion [12] with an expansion for the semantic role delivered to arguments. For Japanese,
Takeuchi et al [19] developed a 1,210-verbs LCS dictionary (with 12 LCS types) called
the T-LCS dictionary, following Kageyama?s analysis [9]. In this paper, we make use of
the current version of the T-LCS dictionary, because it provides a set of concrete rules
for LCS assignment, which ensures the reliability of the dictionary.
Examples of LCS in the T-LCS dictionary are shown in Table 1. An LCS consists
of a combination of semantic predicates (?CONTROL,? ?BE AT,? etc.) and their argu-
ment slots (x, y, and z). Each argument slot corresponds to a semantic role, such as
?Agent,? ?Theme,? and ?Goal,? depending on its surrounding semantic predicates. Let
us take ?yakusu (to translate)? as an example. The inner structure ?[y BE AT z]? de-
notes the state of affairs where z (?Goal?) indicates the state or physical location of y
(?Theme?). The predicate ?BECOME? expresses a change of y. In the case of example
phrase in Table 1, the change of the language of the book is represented. The leftmost
910 A. Fujita, K. Inui, and Y. Matsumoto
Table 1. Examples of LCS
LCS for verb (example verb)
example Japanese phrase
[y BE AT z] (ichi-suru (to locate), sonzai-suru (to exist))
gakkou-ga kawa-no chikaku-ni ichi-suru.
school-NOM river-GEN near-DAT to locate-PRES
The school (Theme) locates near the river (Goal).
[BECOME [y BE AT z]] (houwa-suru (to become saturate), bunpu-suru (to be distributed))
kono-hana-ga sekaiju-ni bunpu-suru.
this flower-NOM all over the world-DAT to distribute-PRES
This flower (Theme) is distributed all over the world (Goal).
[x CONTROL [BECOME [y BE AT z]]] (yakusu (to translate), shoukai-suru (to introduce))
kare-ga hon-o nihongo-ni yakusu.
he-NOM book-ACC Japanese-DAT to translate-PRES
He (Agent) translates the book (Theme) into Japanese (Goal).
[x ACT ON y] (unten-suru (to drive), sousa-suru (to operate))
kare-ga kikai-o sousa-suru.
he-NOM machine-ACC to operate-PRES
He (Agent) operates the machine (Theme).
[y MOVE TO z] (ido-suru (to move), sen?i-suru (to propagate))
ane-ga tonarimachi-ni ido-suru.
my sister-NOM neighboring town-DAT to move-PRES
My sister (Theme) moves to a neighboring town (Goal).
part ?[x CONTROL . . .]? denotes that the ?Agent? causes the state change. The differ-
ence between ?BECOME BE AT? and ?MOVE TO? is underlying their telicity: the for-
mer indicates telic, and thus the verb can be perfective, while the latter atelic. Likewise,
?CONTROL? implicates a state change, while ?ACT ON? merely denotes an action. The
following are examples of syntactic and semantic properties represented in LCS:
? Semantic role of argument (e.g. ?[x CONTROL . . .]? indicates x=?Agent?)
? Syntactic case particle pattern (e.g. ?[y MOVE TO z]? indicates y=NOM, z=DAT)
? Aspectual property (e.g. ?MOVE TO? is atelic (??ket-tearu (to kick-PERF)?), while
?BECOME BE AT? is telic (?oi-tearu (to place-PERF).?))
? Focus of statement
(e.g. x is focused in ?[x CONTROL . . .]?, while z in ?[z BE WITH . . .]?)
? Semantic relations in lexical derivation
? transitivity alternation (?kowasu (to break (vt))? ? ?kowareru (to break (vi))?)
? lexical active-passive alternation (?oshieru (to teach)? ? ?osowaru (to be
taught)?)
2.2 Disambiguation in LCS Analysis
In principle, a verb is associated with more than one LCS if it has multiple senses.
The mapping from syntactic case assignments to argument slots in LCS is also many-
Exploiting Lexical Conceptual Structure for Paraphrase Generation 911
to-many in general. In the case of Japanese, the case particle ?ni? tends to be highly
ambiguous as demonstrated in (5).
(5) a. shuushin-jikan-o yoru-11ji-ni henkou-shita.
bedtime-ACC 11 p.m.-DAT (complement) to change-PAST
I changed my bedtime to 11 p.m.
b. yoru-11ji-ni yuujin-ni mail-o okut-ta.
11 p.m.-DAT (adjunct) friends-DAT (complement) mail-ACC to send-PAST
I sent a mail to my friends at 11 p.m.
Resolution of these sorts of ambiguity is called semantic parsing and has been ac-
tively studied by many researchers recently [6,2] as semantically annotated corpora and
lexical resources such as the FrameNet [1] and the Proposition Bank [16] have become
available. Relying on the promising results of this trend of research, we do not address
the issue of semantic parsing in this paper to focus our attention on the generation side
of the whole problem.
3 Paraphrasing of Light-Verb Constructions
In this paper, we focus our discussion on one class of paraphrases, i.e., paraphrasing of
light-verb constructions (LVCs). Sentence (6s) shows an example of an LVC. An LVC
is a verb phrase (?kandou-o atae-ta (made an impression),? c.f., Figure 1) that consists
of a light-verb (?atae-ta (to give-PAST)?) that syntactically governs a deverbal noun
(?kandou (an impression)?). A paraphrase of (6s) is shown in sentence (6t), where the
deverbal noun functions as the main verb with its verbalized form (?kandou-s-ase-ta (to
be impressed-CAUSATIVE-PAST)?).
(6) s. eiga-ga kare-ni saikou-no kandou-o atae-ta.
film-NOM him-DAT supreme-GEN impression-ACC to give-PAST
The film made an supreme impression on him.
t. eiga-ga kare-o saikou-ni kandou-s-ase-ta.
film-NOM him-ACC supreme-DAT to be impressed-CAUSATIVE-PAST
The film supremely impressed him.
Example (6) indicates that we need an information to determine how the voice of target
sentence must be changed and how the case particles of the nominal elements must be
reassigned. These decisions depend not only on the syntactic and semantic attributes of
the light-verb, but also on those of the deverbal noun [14]. LVC paraphrasing is thus a
novel challenging material for exploiting LCS.
Figure 1 demonstrates tree representations of source and target expressions involved
in LVC paraphrasing, taking (6) as an example. To generate this type of paraphrase, we
need a computational model that is capable of the following operations:
Change of the dependence: Change the dependences of the elements (a) and (b) due
to the elimination of the original modifiee, the light-verb. This operation can be
done by just making them dependent on the resultant verb.
912 A. Fujita, K. Inui, and Y. Matsumoto
saikou-ni
i - i
kare-o
r -
eiga-ga
i -
*
kandou-o
-
kare-ni
r - i
(c) <noun>+GEN
saikou-no
i -
*
(d) <adjective> or <embedded clause>
(a) <adverb>
(b) <noun>
+ <case particle>
LVC
kandou-
s-ase-ta
-
- -t
<deverbal noun>
+ <verbal suffixes>
(a)
(b)
(d)
eiga-ga
i -
atae-ta
t -t
*
*
(c)<deverbal noun>
+ <case particle>
<light-verb>
+ <verbal suffixes>
Fig. 1. Dependency structure showing the range which the LVC paraphrasing affects. The oval
objects denote Japanese base-chunks so-called bunsetsu.
Re-conjugation: Change the conjugation form of the elements (d) and occasionally
(c), according to the syntactic category change of their modifiee: the given deverbal
noun is verbalized. This operation can be carried out independently of the LVC
paraphrasing.
Selection of the voice: Choose the voice of the target sentence among active, passive,
causative, etc. In example (6), the causative (the auxiliary verb ?ase?) is chosen.
The decision depends on the syntactic and semantic attributes of both the given
light-verb and the deverbal noun [14].
Reassignment of the cases: Assign the case particles of the elements (b) and (c), the
arguments of the main verb. In (6), the syntactic case of ?kare (him),? which was
originally assigned dative case ?ni? is changed to accusative ?o.?
Among these operations, this paper focuses on the last two, namely handling the ele-
ment (b), the sibling cases of the deverbal noun. Triangles in both trees in Figure 1 indi-
cate the range which we handle. Henceforth, elements outside of the triangles, namely,
(a), (c), and (d), are used only for explanatory purposes.
4 LCS-Based Paraphrase Generation Model
Figure 2 illustrates how our model paraphrases the LVC, taking (7) as an example.
(7) s. Ken-ga eiga-ni shigeki-o uke-ta.
Ken-NOM film-DAT inspiration-ACC to receive-PAST
Ken received an inspiration from the film.
t. Ken-ga eiga-ni shigeki-s-are-ta.
Ken-NOM film-DAT to inspire-PASSIVE-PAST
Ken was inspired by the film.
The generation process consists of the following three steps:
Step 1. Semantic analysis: The model first analyzes a given input sentence including
an LVC to obtain its LCS representation. In Figure 2, this step generates LCSV 1 by
filling arguments of LCSV 0 with nominal elements.
Exploiting Lexical Conceptual Structure for Paraphrase Generation 913
LCSdic.
i .
Step 2
LCS transformation
t  
 tr f r ti
+
Step 3
Surface generation
t  
rf  r ti
?ukeru?
(to receive)
z-NOM y-ACC x-DAT
?shigeki-suru?
(to inspire)
x?-NOM y?-ACC
Step 1
Semantic analysis
t  
ti  l i
LCS
N0
LCS
V0
LCS
V1
LCS
N1
Input sentence
Paraphrased sentence
Ken-NOM film-DAT to inspire-PASSIVE
fil t  i i I
Ken-NOM film-DAT inspiration-ACC to receive-ACT
fil i i ti t  i
[BECOME [[Ken]z BE WITH
[[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[  [[ ]  I
[[i i ti ]    [fil ]  [ ] ]]]
[[film]x? ACT ON [Ken]y?]
[[fil ]   [ ] ]
[BECOME [[Ken]z BE WITH ?] 
[  [[ ]  I  ] 
[[film]x? ACT ON [Ken]y?]
[[fil ]   [ ] ]
LCS
S
[BECOME [[Ken]z BE WITH
[[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[  [[ ]  I
[[i i ti ]    [fil ]  [ ] ]]]
Fig. 2. LCS-based paraphrase generation model
Step 2. Semantic transformation (LCS transformation): The model then transfers
the obtained semantic structure to another semantic structure so that the target struc-
ture consists of the LCS of the verbalized form of the deverbal noun. In our exam-
ple, this step generates LCSN1 together with the supplement ?[BECOME [. . .]]?. We
refer to such a supplement as LCSS .
Step 3. Surface generation: Having obtained the target LCS representation, the model
finally generates the output sentence from it. LCSS triggers another syntactic alter-
nation such as passivization and causativization.
The idea is to use the LCS representation as a semantic representation and to re-
trieve semantic constraints to relieve the syntactic underspecificity underlying the LVC
paraphrasing. Each step consists of a handful of linguistically explainable rules, and
thus is scalable when the typology and resource of LCS is given. The rest of this sec-
tion elaborates on each step, differentiating symbols to denote arguments; x, y, and z
for LCSV , and x?, y?, and z? for LCSN .
4.1 Semantic Analysis
Given an input sentence (a simple clause with an LVC), the model first looks up the
LCS template LCSV 0 for the given light-verb in the T-LCS dictionary, and then applies
the case assignment rule below to obtain its LCS representation LCSV 1:
? In the case of the LCSV 0 having argument x, fill the leftmost argument of the
LCSV 0 with the nominative case of the input, the second leftmost with the ac-
cusative, and the rest with the dative case.
? Otherwise, fill arguments y and z of the LCSV 0 with the nominative and the dative
cases, respectively.
This rule is proposed in [19] instead of semantic parsing in order to tentatively
automate LCS-based processing. In the example shown in Figure 2, LCSV 0 for the
914 A. Fujita, K. Inui, and Y. Matsumoto
[BECOME [[Ken]z BE WITH
[[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[  [[ ]  I
[[i i ti ]    [fil ]  [ ] ]]]
[[film]x? ACT ON [Ken]y?]
[[fil ]   [ ] ]
[BECOME [[Ken]z BE WITH ?] 
[  [[ ]  I  ] 
+
LCS
V1
LCS
N1
Predicate and
argument matching
Treatment of
non-transferred predicates 
LCS
S
Fig. 3. An example of LCS transformation
given light-verb ?ukeru (to receive)? has argument x, thus the nominative case, ?Ken,?
fills the leftmost argument z. Accordingly, the accusative (?shigeki (inspiration)?) and
the dative (?eiga (film)?) fill y and x, respectively.
4.2 LCS Transformation
The second step matches LCSV 1 with the another LCS for the verbalized form of the
deverbal noun LCSN0 to generate the target LCS representation LCSN1. Figure 3 shows
a more detailed view of this process for the example shown in Figure 2.
Muraki [14] described that the direction of action and the focus of statement are
important clues to determine the voice in LVC paraphrasing. We therefore incorporate
the below assumptions into matching process. The model first matches predicates in
LCSV 1 and LCSN0, assuming that the agentive argument x is relevant to the direc-
tion of action. We classify the semantic predicates into the following three groups: (i)
agentive predicates (involve argument x): ?CONTROL,? ?ACT ON,? ?ACT TO,? ?ACT,?
and ?MOVE FROM TO,? (ii) state of affair predicates (involve only argument y or z):
?MOVE TO,? ?BE AT,? and ?BE WITH,? and (iii) aspectual predicates (with no argu-
ment): ?BECOME,? and allowed any pair of predicates in the same group to match. In
our example, ?MOVE FROM TO? matches ?ACT ON? as shown in Figure 3.
Having matched the predicates, the model then fills each argument slot in LCSN0
with its corresponding argument in LCSV 1. In Figure 3, argument z is matched with
y?, and x with x?. As a result, ?Ken? and ?eiga? come to y? and x? slots, respectively.
When an argument is filled with another LCS, arguments within the inner LCS are also
taken into account. Likewise, we introduced some exceptional rules assuming that the
input sentences are periphrastic. For instance, arguments filled with the implicit filler
(e.g. ?name? for ?to sign? is usually not expressed in Japanese) and the deverbal noun,
which is already represented by LCSN0 are never matched. Argument z in LCSV 1 is
allowed to match with y? in LCSN0.
LCS representations have right-embedding structures, and inner-embedded pred-
icates denote the state of affairs. We thus prioritize the rightmost predicates in this
matching process. In other words, the proceeds from the rightmost inner predicates
to the outer ones, and the matching process is repeated until the leftmost predicate in
LCSN0 or that in LCSV 1 matched.
If LCSV 1 has any non-transferred part LCSS when the predicate and argument
matching has been completed, it represents the semantic content that is not expressed
by LCSN1 and needs to be expressed by auxiliary linguistic devices such as voice
auxiliaries. As described in Section 2.1, the leftmost part specifies the focus of state-
Exploiting Lexical Conceptual Structure for Paraphrase Generation 915
ment. The model thus attaches LCSS to LCSN0 as a supplement, and then use it to
determine auxiliaries in the next step, the surface generation. In the case of Figure 3,
?[BECOME [[Ken]z BE WITH]]? in LCSV 1 remains non-transferred and be attached.
4.3 Surface Generation
The model again applies the aforementioned case assignment rule to generate a sentence
from the resultant LCS. From the LCSN1 in Figure 2, sentence (8) is generated.
(8) eiga-ga Ken-o shigeki-shi-ta.
film-NOM Ken-ACC to inspire-PAST
The film inspired Ken.
The model then makes the final decision on the selection of the voice and the reas-
signment of the cases. As we described above, the attached structure LCSS is a clue to
determine what the focus is. We therefore use the following decision list:
1. If the leftmost argument of LCSS has the same value as the leftmost argument in
LCSN1, the viewpoints of LCSS and LCSN1 are same. Thus, the active voice is
selected and the case structure is left as is.
2. If the leftmost argument of LCSS has the same value as either z? or y? in LCSN1,
the model makes the argument a subject (nominative). That is, the passive voice is
selected and case alternation (passivization) is applied.
3. If LCSS has ?BE WITH? and its argument has the same value as x? in LCSN1, the
causative voice is selected and case alternation (causativization) is applied.
4. If LCSS has an agentive predicate, and its argument is filled with a value different
from those of the other arguments, then the causative voice is selected and case
alternation (causativization) is applied.
5. Otherwise, active voice is selected and thus no modification is applied.
The example in Figure 2 satisfies the second condition, thus the model chooses ?s-
are-ru (PASSIVE)? and passivizes the sentence (8). As a result, ?Ken? becomes to be
the nominative ?ga? as in (7t).
5 Experiment
5.1 Paraphrase Generation and Evaluation
To conduct an empirical experiment, we collected the following data sets. Note that
more than one LCS was assigned to a verb if it was polysemous.
Deverbal nouns: We regard ?sahen-nouns? and adverbial forms of verbs as deverbal
nouns. We retrieved 1,210 deverbal nouns from the T-LCS dictionary. The set con-
sists of (i) activity nouns (e.g., ?sasoi (invitation)? and ?odoroki (surprise)?), (ii) Sino-
Japanese verbal nouns (e.g., ?kandou (impression)? and ?shigeki (inspiration)?), and
(iii) English borrowings (e.g., ?drive? and ?support?).
Tuples of light-verb and case particle: A verb takes different meanings when it con-
stitutes LVCs with different case particles, and not every tuple of a light-verb v and a
916 A. Fujita, K. Inui, and Y. Matsumoto
case particle c functions as an LVC. We therefore tailored an objective collection of
tuples ?v, c? from corpus in the following manner:
Step 1. From a corpus consisting of 25 million parsed sentences of newspaper articles,
we collected 876,101 types of triplet ?v, c, n?, where v, c, and n denote a base form
of verb, a case particle, and an deverbal noun.
Step 2. For each of the 50 most frequent ?v, c? tuples, we extracted the 10 most fre-
quent triplets ?v, c, n?.
Step 3. Each ?v, c, n? was manually evaluated to determine whether it functioned as an
LVC. If any of 10 triplets functioned as an LVC, the tuple ?v, c? was merged into
the list of light-verbs, assigning an LCS according to the linguistic tests examined
in [19]. As a result, we collected 40 types of ?v, c? for light-verbs.
Paraphrase examples: A collection of paraphrase examples, pairs of an LVC and its
correct paraphrase, were constructed in the following way:
Step 1. From the 876,101 types of triplet ?v, c, n? collected above, 23,608 types of
?v, c, n? were extracted, whose components, n and ?v, c?, were in the dictionaries.
Step 2. For each of the 245 most frequent ?v, c, n?, the 3 most frequent simple clauses
including the ?v, c, n? were extracted from the same corpus.
Step 3. Two native speakers of Japanese, adults graduated from university, were em-
ployed to build a gold-standard collection. 711 out of 735 sentences were manually
paraphrased in the manner of LVC, while the remaining 24 sentences were not
because ?v, c, n? within them did not function as LVCs.
The real coverage of these 245 ?v, c, n? with regard to all LVCs among the corpus falls
in the range between the below two:
Lower bound: If every ?v, c, n? is an LVC, the coverage of the collection is estimated
at 6.47% (492,737 / 7,621,089) of tokens.
Upper bound: If the dictionaries cover all light-verbs and deverbal nouns, the collec-
tion covers 24.1% (492,737 / 2,044,387) of tokens.
In the experiment, our model generated all the possible paraphrases when a given
verb was polysemous with multiple entries in the T-LCS dictionary. As a result, the
model generated 822 paraphrases from the 735 input sentences, at least one for each
input. We then classified the resultant paraphrases as correct and incorrect by compar-
ing them with the gold-standard, where we ignored ordering of syntactic cases, and
obtained 624 correct and 198 incorrect paraphrases Recall, precision, and F-measure
(? = 0.5) were 0.878 (624 / 711), 0.759 (624 / 822), and 0.814, respectively.
As the baseline, we employed a statistical language model developed in [5]. Among
all the combinations of the voice and syntactic cases, the baseline model selects the
one that has the highest probability. Although the model is trained on a large amount
of data, the generated expression often falls out of the vocabulary. In such a case, the
probability cannot be calculated, and the model outputs nothing for the given sentence.
As a result of an application of this baseline model to the same set of input sentences,
we obtained 320 correct and 215 incorrect paraphrases (Recall: 0.450 (320 / 711), Pre-
cision: 0.598 (320 / 535), and F-measure: 0.514). The significant improvement indicates
that our lexical-semantics-based account benefited on the decisions we considered.
Exploiting Lexical Conceptual Structure for Paraphrase Generation 917
The language model can also be complementary used to our LCS-based paraphrase
generation. By filtering implausible paraphrases out, 66 incorrect and 15 correct para-
phrases were filtered, and the performance was further improved (Recall: 0.857, Preci-
sion: 0.822, and F-measure: 0.839).
5.2 Discussion
Although the performance has room for further improvement, we think the perfor-
mance is reasonably high under the current stage of the T-LCS dictionary. In other
words, the tendency of errors does not so differ from our expectation. As we expected
in Section 2.2, the ambiguity of dative case ?ni? (c.f. (5)) occupied the largest portion
of errors (78 / 198). This was because the case assignment was performed by a rule in-
stead of semantic parsing. Each rule in our model has been created relying on a set of
linguistic tests used in the theory of LCS and our linguistic intuition on handling LCS.
However, the rule set was not sufficiently sophisticated, so that led to 59 errors. Equally,
30 errors occurred due to the immature typology of the T-LCS dictionary.
We consider the improvement of the LCS typology as the primal issue, because
our transformation rules depend on it. For the moment, we have the following two
suggestions. First, more variety of semantic roles should be handled step by step. For
example, we need to handle the object of ?eikyou-suru (to affect),? which is marked by
not accusative but dative. Second, the necessity of ?Source? is inconsistent. Verbs such
as ?hairu (to enter)? do not require this argument (?BECOME BE AT?) , while some
other verbs, such as ?ukeru (to receive),? explicitly require it (?MOVE FROM TO?). The
telicity of ?MOVE FROM TO? should also be discussed. With such a feedback from
the application and an extensive investigation into lexicology, we have to enhance the
typology, and enlarge the dictionary preserving its consistency.
6 Related Work
The paraphrases associated with LVCs are not idiosyncratic to Japanese but also appear
commonly in other languages such as English, French, and Spanish [13,7,4] as shown
in (3) and (4). Our approach raises an interesting issue of whether the paraphrasing of
LVCs can be modeled in an analogous way across languages.
Iordanskaja et al [7] proposed a set of paraphrasing rules including one for LVC
paraphrasing based on the Meaning-Text Theory introduced by [13]. The model seemed
to properly handle LVC paraphrasing, because their rules were described according to
the deep semantic analysis and heavily relied on what were called lexical functions,
such as lexical derivation (e.g., S0(affect) = effect ) and light-verb generation (e.g.,
Oper1(attempt) = make). To take this approach, however, a vast amount of lexical
knowledge to form each lexical function is required, because they only virtually specify
all the choices relevant to LVC paraphrasing for every combination of deverbal noun
and light-verb individually. In contrast, our approach is to employ lexical semantics
to provide a general account of those classes of choices, and thus contributes to the
knowledge development in terms of reducing human-labor and preserving consistency.
Kaji et al [10] proposed a paraphrase generation model which utilized an monolin-
gual dictionary for human. Given an input LVC, their model paraphrases it referring to
918 A. Fujita, K. Inui, and Y. Matsumoto
the glosses of both the deverbal noun and light-verb, and a manually assigned semantic
feature of the light-verb. Their model looks robust due to the availability of resource.
However, their model fails to explain the difference between examples (7) and (9) in
the voice selection, because it selects the voice based only on the light-verb irrespec-
tive of the deverbal noun: the light-verb ?ukeru (to receive)? is always mapped to the
passive voice.
(9) s. musuko-ga kare-no hanashi-ni kandou-o uke-ta.
son-NOM his-GEN talk-DAT impression-ACC to receive-PAST
My son was given a good impression by his talk.
t. musuko-ga kare-no hanashi-ni kandou-shi-ta.
son-NOM his-GEN talk-DAT to be impressed-PAST
My son was impressed by his talk.
In their model, the target expression is restricted only to the LVC itself (c.f., Figure 1).
Hence, their model is unable to reassign the case particles as we saw in example (6).
There is another trend in the research of paraphrase generation: i.e., the automatic
paraphrase acquisition from existing lexical resources such as ordinary dictionaries,
parallel/comparable corpora, and non-parallel corpora. This type of approach may be
able to reduce the cost of resource development. However, there are drawbacks that
must be overcome before they can work practically. First, automatic methods require
large amounts of training data. The issue is how to collect enough large size of data at
low cost. Second, automatically extracted knowledge tends to be rather noisy, requiring
manual correction and maintenance. In contrast, our approach, which focuses on the
regularity underlying paraphrases, is a complementary avenue to develop and maintain
knowledge resources that cover a sufficiently wide range of paraphrases.
Previous case studies [14,18,11] have employed some syntactic properties of verbs
to constrain syntactic transformations in paraphrase generation: e.g. subject agentiv-
ity, aspectual property, passivizability, and causativizability. Several classifications of
verbs have also been proposed [12,15] based on various types of verb alternation and
syntactic case patterns. In contrast, the theory of lexical semantics integrates syntactic
and semantic properties including those above, and gives a perspective to formalize and
maintain the syntactic and semantic properties of words.
7 Conclusion
In this paper, we explored what sorts of lexical properties encoded in LCS can explain
the regularity underlying paraphrases. Based on an existing LCS dictionary, we built
an LCS-based paraphrase generation model, and conducted an empirical experiment on
paraphrasing of LVC. The experiment confirmed that the proposed model was capable
of generating paraphrases accurately in terms of selecting the voice and reassigning the
syntactic cases, and revealed potential difficulties that we have to overcome toward a
practical use of our lexical-semantics-based account. To make our model more accu-
rate, we need further discussion on (i) the enhancement of the T-LCS dictionary with
feedback from experiments, (ii) the LCS transformation algorithm, and (iii) the seman-
tic parsing. Another goal is to practically clarify what extent can be done by LCS for
other classes of paraphrase, such as those exemplified in Section 1.
Exploiting Lexical Conceptual Structure for Paraphrase Generation 919
References
1. C. F. Baker, C. J. Fillmore, and J. B. Lowe. The Berkeley FrameNet project. In Proceedings
of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th
International Conference on Computational Linguistics (COLING-ACL), pages 86?90, 1998.
2. X. Carreras and L. Ma`rques. Introduction to the CoNLL-2004 shared task: semantic role
labeling. In Proceedings of 8th Conference on Natural Language Learning (CoNLL), pages
89?97, 2004.
3. B. J. Dorr. Large-scale dictionary construction for foreign language tutoring and interlingual
machine translation. Machine Translation, 12(4):271?322, 1997.
4. M. Dras. Tree adjoining grammar and the reluctant paraphrasing of text. Ph.D. thesis,
Division of Information and Communication Science, Macquarie University, 1999.
5. A. Fujita, K. Inui, and Y. Matsumoto. Detection of incorrect case assignments in automat-
ically generated paraphrases of Japanese sentences. In Proceedings of the 1st International
Joint Conference on Natural Language Processing (IJCNLP), pages 14?21, 2004.
6. D. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288, 2002.
7. L. Iordanskaja, R. Kittredge, and A. Polgue`re. Lexical selection and paraphrase in a meaning-
text generation model. In C. L. Paris, W. R. Swartout, and W. C. Mann, editors, Natural
Language Generation in Artificial Intelligence and Computational Linguistics, pages 293?
312. Kluwer Academic Publishers, 1991.
8. R. Jackendoff. Semantic structures. The MIT Press, 1990.
9. T. Kageyama. Verb semantics. Kurosio Publishers, 1996. (in Japanese).
10. N. Kaji and S. Kurohashi. Recognition and paraphrasing of periphrastic and overlapping
verb phrases. In Proceedings of the 4th International Conference on Language Resources
and Evaluation (LREC) Workshop on Methodologies and Evaluation of Multiword Units in
Real-world Application, 2004.
11. K. Kondo, S. Sato, and M. Okumura. Paraphrasing by case alternation. IPSJ Journal,
42(3):465?477, 2001. (in Japanese).
12. B. Levin. English verb classes and alternations: a preliminary investigation. Chicago Press,
1993.
13. I. Mel?c?uk and A. Polgue`re. A formal lexicon in meaning-text theory (or how to do lexica
with words). Computational Linguistics, 13(3-4):261?275, 1987.
14. S. Muraki. Various aspects of Japanese verbs. Hitsuji Syobo, 1991. (in Japanese).
15. A. Oishi and Y. Matsumoto. Detecting the organization of semantic subclasses of Japanese
verbs. International Journal of Corpus Linguistics, 2(1):65?89, 1997.
16. M. Palmer, D. Gildea, and P. Kingsbury. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?106, 2005.
17. J. Pustejovsky. The generative lexicon. The MIT Press, 1995.
18. S. Sato. Automatic paraphrase of technical papers? titles. IPSJ Journal, 40(7):2937?2945,
1999. (in Japanese).
19. K. Takeuchi, K. Kageura, and T. Koyama. An LCS-based approach for analyzing Japanese
compound nouns with deverbal heads. In Proceedings of the 2nd International Workshop on
Computational Terminology (CompuTerm), pages 64?70, 2002.
Opinion Extraction Using a Learning-Based
Anaphora Resolution Technique
Nozomi Kobayashi Ryu Iida Kentaro Inui Yuji Matsumoto
Nara Institute of Science and Technology
Takayama, Ikoma, Nara, 630-0192, Japan
{nozomi-k,ryu-i,inui,matsu}@is.naist.jp
Abstract
This paper addresses the task of extract-
ing opinions from a given document
collection. Assuming that an opinion
can be represented as a tuple ?Subject,
Attribute, Value?, we propose a compu-
tational method to extract such tuples
from texts. In this method, the main
task is decomposed into (a) the pro-
cess of extracting Attribute-Value pairs
from a given text and (b) the process of
judging whether an extracted pair ex-
presses an opinion of the author. We
apply machine-learning techniques to
both subtasks. We also report on the
results of our experiments and discuss
future directions.
1 Introduction
The explosive spread of communication on the
Web has attracted increasing interest in technolo-
gies for automatically mining large numbers of
message boards and blog pages for opinions and
recommendations.
Previous approaches to the task of mining a
large-scale document collection for opinions can
be classified into two groups: the document clas-
sification approach and the information extrac-
tion approach. In the document classification
approach, researchers have been exploring tech-
niques for classifying documents according to se-
mantic/sentiment orientation such as positive vs.
negative (e.g. (Dave et al, 2003; Pang and Lee,
2004; Turney, 2002)). The information extraction
approach, on the other hand, focuses on the task
of extracting elements which constitute opinions
(e.g. (Kanayama and Nasukawa, 2004; Hu and
Liu, 2004; Tateishi et al, 2001)).
The aim of this paper is to extract opinions
that represent an evaluation of a products together
with the evidence. To achieve this, we consider
our task from the information extraction view-
point. We term the above task opinion extraction
in this paper.
While they can be linguistically realized in
many ways, opinions on a product are in fact often
expressed in the form of an attribute-value pair.
An attribute represents one aspect of a subject and
the value is a specific language expression that
qualifies or quantifies the aspect. Given this ob-
servation, we approach our goal by reducing the
task to a general problem of extracting four-tuples
?Product, Attribute, Value, Evaluation? from a
large-scale text collection. Technology for this
opinion extraction task would be useful for col-
lecting and summarizing latent opinions from the
Web. A straightforward application might be gen-
eration of radar charts from collected opinions as
suggested by Tateishi et al (2004).
Consider an example from the automobile do-
main, I am very satisfied with the powerful engine
(of a car). We can extract the four-tuple ?CAR, en-
gine, powerful, satisfied? from this sentence. Note
that the distinction between Value and Evaluation
is not easy. Many expressions used to express a
Value can also be used to express an Evaluation.
For this reason, we do not distinguish value and
evaluation, and therefore consider the task of ex-
tracting triplets ?Product, Attribute, Value?. An-
other problem with opinion extraction is that we
want to get only subjective opinions. Given this
setting, the opinion extraction task can be decom-
posed into two subtasks: extraction of attribute-
value pairs related to a product and determination
of its subjectivity.
As we discuss in section 3, an attribute and its
value may not appear in a fixed expression and
may be separated. In some cases, the attribute
may be missing from a sentence. In this respect,
finding the attribute of a value is similar to finding
the missing antecedent of an anaphoric expres-
sion. In this paper, we discuss the similarities
and differences between opinion extraction and
anaphora resolution. Then, we apply a machine
learning-based method used for anaphora reso-
173
lution to the opinion extraction problem and re-
port on our experiments conducted on a domain-
restricted set of Japanese texts excerpted from re-
view pages on the Web.
2 Related work
In this section, we discuss previous approaches
to the opinion extraction problem. In the pattern-
based approach (Murano and Sato, 2003; Tateishi
et al, 2001), pre-defined extraction patterns and a
list of evaluative expressions are used. These ex-
traction patterns and the list of evaluation expres-
sions need to be manually created. However, as
is the case in information extraction, manual con-
struction of rules may require considerable cost to
provide sufficient coverage and accuracy.
Hu and Liu (2004) attempt to extract the at-
tributes of target products on which customers
have expressed their opinions using association
mining, and to determine whether the opinions
are positive or negative. Their aim is quite sim-
ilar to our aim, however, our work differs from
theirs in that they do not identify the value corre-
sponding to an attribute. Their aim is to extract
the attributes and their semantic orientations.
Taking the semantic parsing-based approach,
Kanayama and Nasukawa (2004) apply the idea
of transfer-based machine translation to the ex-
traction of attribute-value pairs. They regard the
extraction task as translation from a text to a sen-
timent unit which consists of a sentiment value,
a predicate, and its arguments. Their idea is
to replace the translation patterns and bilingual
lexicons with sentiment expression patterns and
a lexicon that specifies the polarity of expres-
sions. Their method first analyzes the predicate-
argument structure of a given input sentence mak-
ing use of the sentence analysis component of an
existing machine translation engine, and then ex-
tracts a sentiment unit from it, if any, using the
transfer component.
One important problem the semantic parsing
approach encounters is that opinion expres-
sions often appear with anaphoric expressions
and ellipses, which need to be resolved to
accomplish the opinion extraction task. Our
investigation of an opinion-tagged Japanese
corpus (described below) showed that 30% of
the attribute-value pairs we found did not have a
direct syntactic dependency relation within the
sentence, mostly due to ellipsis. For example1,
?dezain-wa?a hen-daga watashi-wa ?-ga ?suki-da?v
?design?a weird I [it] ?like?v
(The design is weird, but I like it.)
This type of case accounted for 46 out of 100
pairs that did not have direct dependency rela-
tions. To analyze predicate argument structure
robustly, we have to solve this problem. In the
next section, we discuss the similarity between
the anaphora resolution task and the opinion
extraction task and propose to apply to opinion
extraction a method used for anaphora resolution.
3 Method for opinion extraction
3.1 Analogy with anaphora resolution
We consider the task of extracting opinion tu-
ples ?Product, Attribute, Value? from review sites
and message boards on the Web dedicated to pro-
viding and exchanging information about retail
goods. On these Web pages, products are often
specified clearly and so it is frequently a trivial
job to extract the information for the Product slot.
We therefore in this paper focus on the problem
of extracting ?Attribute, Value? pairs.
In the process of attribute-value pair identifi-
cation for opinion extraction, we need to deal
with the following two cases: (a) both a value
and its corresponding attribute appear in the text,
and (b) a value appears in the text while its at-
tribute is missing since it is inferable form the
value expression and the context. The upper half
of Figure 1 illustrates these two cases in the auto-
mobile domain. In (b), the writer is talking about
the ?size? of the car, but the expression ?size? is
not explicitly mentioned in the text. In addition,
(b) includes the case where the writer evaluates
the product itself. For example, ?I?m very satis-
fied with my car!?: in this case, a value expres-
sion ?satisfied? evaluates the product as a whole,
therefore a corresponding attribute does not ex-
ists.
For the case (a), we first identify a value ex-
pression (like in Figure 1) in a given text and then
look for the corresponding attribute in the text.
Since we also see the case (b), on the other hand,
we additionally need to consider the problem of
whether the corresponding attribute of the identi-
fied value expression appears in the text or not.
The structure of these problems is analogous to
that of anaphora resolution; namely, there are ex-
actly two cases in anaphora resolution that have
a clear correspondence with the above two cases
as illustrated in Figure 1: in (a) the noun phrase
(NP) is anaphoric; namely, the NP?s antecedent
appears in the text, and in (b) the noun phrase is
non-anaphoric. A non-anaphoric NP is either ex-
1
??
a
denotes the word sequence corresponding to the At-
tribute. Likewise, we also use ??
v
for the Value.
174
Taro-wa shisetsu-wo??-ga?shirabe-te
houkokusho-o sakusei-shita
(a) (b)
Dezain-wa     hen-desuga
watashi-wa ??-ga? suki-desu
?????
(?-ga) Ookii-kedo atsukai-yasui
( it )        large   but    easy to handle
(a) (b)
anaphora resolution
opinion extraction
anaphorantecedent
Attribute
Value
(The design is weird, but I like it.)
omitted Attribute
(It is large, but easy to handle)
Tar?-NOM  attendance-ACC                   noted
report-ACC             wrote
(Taro noted the attendance 
and wrote a report.)
design-NOM            weird
I-NOM          ( it )             like
Value
Onaka-ga hetta-node
kaerouto (?-ga) omou
hungry
go home       (I)
exophora
anaphor
(I think I?ll go home  because I?m hungry.)
Figure 1: Similarity between opinion extraction
and anaphora resolution
ophoric (i.e. the NP has an implicit referent) or in-
definite. While the figure shows Japanese exam-
ples, the similarity between anaphora resolution
and opinion extraction is language independent.
This analogy naturally leads us to think of apply-
ing existing techniques for anaphora resolution to
our opinion extraction task since anaphora reso-
lution has been studied for a considerably longer
period in a wider range of disciplines as we briefly
review below.
3.2 Existing techniques for anaphora
resolution
Corpus-based empirical approaches to anaphora
resolution have been reasonably successful. This
approach, as exemplified by (Soon et al, 2001;
Iida et al, 2003; Ng, 2004), is cost effective,
while achieving a better performance than the
best-performing rule-based systems for the test
sets of MUC-6 and MUC-7 2.
As suggested by Figure 1, anaphora resolution
can be decomposed into two subtasks: anaphoric-
ity determination and antecedent identification.
Anaphoricity determination is the task of judg-
ing whether a given NP is anaphoric or non-
anaphoric. Recent research advances have pro-
vided several important findings as follows:
? Learning-based methods for antecedent
identification can also benefit from the use of
linguistic clues inspired by Centering The-
ory (Grosz et al, 1995).
? One useful clue for anaphoricity determina-
tion is the availability of a plausible candi-
date for the antecedent. If an appropriate
candidate for the antecedent is found in the
preceding discourse context, the NP is likely
to be anaphoric.
For these reasons, an anaphora resolution model
performs best if it carries out the following pro-
2The 7th Message Understanding Conference (1998):
www.itl.nist.gov/iaui/894.02/related projects/muc/
??????????????interia ?????seki??
Dezain-wa   hen-desuga   watashi-wa suki-desu ?????
interior                  seat
design-NOM       weird    I-NOM           like
candidates
design like
interior like
seat like
design like
candidate attributes
real attribute
Select the best 
candidate attribute
Decide whether the 
candidate attribute 
stands for the real 
attribute or not
design like
design like
real attribute
pairedness
determination
attribute 
identification
opinionhood
determination
Judge whether the pair 
expresses an opinion or not
opinion
Attribute
dictionary
Value
dictionary
interior
seat
design
like
good
?.
target value
initialization
pair extraction
Figure 2: Process of opinion extraction
cess in the given order (Iida et al, 2005): (1)
Antecedent identification: Given an NP, iden-
tify the best candidate antecedent for it, and (2)
Anaphoricity determination: Judge whether the
candidate really stands for the true antecedent of
the NP.
3.3 An opinion extraction model inspired by
analogy with anaphora resolution
As illustrated in Figure 2, an opinion extraction
model derived from the aforementioned analogy
with anaphora resolution as follows:
1. Initialization: Identify attribute and value
candidates by dictionary lookup
2. Attribute identification: Select a value and
identify the best candidate attribute corre-
sponding to the value
3. Pairedness determination: Decide whether
the candidate attribute stands for the real at-
tribute of the value or not (i.e. the value
has no explicit corresponding attribute in the
text)
4. Opinionhood determination: Judge wheth-
er the obtained attribute-value pair3 ex-
presses an opinion or not
Here, the attribute identification and pairedness
determination processes respectively correspond
to the antecedent identification and anaphoricity
determination processes in anaphora resolution.
Note that our opinion extraction task requires
an additional subtask, opinionhood determination
? an attribute-value pair appearing in a text does
not necessarily constitute an opinion. We elabo-
rate on the notion of opinionhood in section 4.1.
From the above discussion, we can expect that
the findings for anaphora resolution mentioned in
3.2 stated above apply to opinion extraction as
well. In fact, the information about the candidate
3For simplicity, we call a value both with and without an
attribute uniformly by the term attribute-value pair unless
the distinction is important.
175
attribute is likely to be useful for pairedness deter-
mination. We therefore expect that carrying out
attribute identification before pairedness determi-
nation should outperform the counterpart model
which executes the two subtasks in the reversed
order. The same analogy also applies to opinion-
hood determination; namely, we expect that opin-
ion determination is bet performed after attribute
determination. Furthermore, our opinion extrac-
tion model also can be implemented in a totally
machine learning-based fashion.
4 Evaluation
We conducted experiments with Japanese Web
documents to empirically evaluate the perfor-
mance of our opinion extraction model, focus-
ing particularly on the validity of the analogy dis-
cussed in the previous section.
4.1 Opinionhood
In these experiments, we define an opinion as fol-
lows: An opinion is a description that expresses
the writer?s subjective evaluation of a particular
subject or a certain aspect of it.
By this definition, we exclude requests, factual
or counter-factual descriptions and hearsay evi-
dence from our target opinions. For example, The
engine is powerful is an opinion, while a counter-
factual sentence such as If only the engine were
more powerful is not regarded as opinion.
4.2 Opinion-tagged corpus
We created an opinion-tagged Japanese corpus
consisting of 288 review articles in the automo-
bile domain (4,442 sentences). While it is not
easy to judge whether an expression is a value or
an attribute, we asked the annotator to identify at-
tribute and value expressions according to their
subjective judgment.
If some attributes are in a hierarchical rela-
tion with each other, we asked the annotator to
choose the attribute lowest in the hierarchy as the
attribute of the value. For example, in a sound
system with poor sound, only sound is annotated
as the attribute of the value poor.
The corpus contains 2,191 values with an at-
tribute and 420 values without an attribute. Most
of the attributes appear in the same sentence as
their corresponding values or in the immediately
preceding sentence (99% of the total number of
pairs). Therefore, we extract attributes and their
corresponding values from the same sentence or
from the preceding sentence.
4.3 Experimental method
As preprocessing, we analyzed the opinion-
tagged corpus using the Japanese morphological
analyzer ChaSen4 and the Japanese dependency
structure analyzer CaboCha 5.
We used Support Vector Machines to train the
models for attribute identification, pairedness de-
termination and opinionhood determination. We
used the 2nd order polynomial kernel as the ker-
nel function for SVMs. Evaluation was per-
formed by 10-fold cross validation using all the
data.
4.3.1 Dictionaries
We use dictionaries for identification of at-
tribute and value candidates. We constructed a
attribute dictionary and a value dictionary from
review articles about automobiles (230,000 sen-
tences in total) using the semi-automatic method
proposed by Kobayashi et al (2004). The data
used in this process was different from the
opinion-tagged corpus. Furthermore, we added
to the dictionaries expressions which frequently
appearing in the opinion-tagged corpus. The final
size of the dictionaries was 3,777 attribute expres-
sions and 3,950 value expressions.
4.3.2 Order of model application
To examine the effects of appropriately choos-
ing the order of model application we mentioned
in the previous section, we conducted four ex-
periments using different orders (AI indicates at-
tribute identification, PD indicates pairedness de-
termination and OD indicates opinion determina-
tion):
Proc.1: OD?PD?AI, Proc.2: OD?AI?PD
Proc.3: AI?OD?PD, Proc.4: AI?PD?OD
Note that Proc.4 is our proposed ordering.
In addition to these models, we adopted a base-
line model. In this model, if the candidate value
and a candidate attribute are connected via a de-
pendency relation, the candidate value is judged
to have an attribute. When none of the candidate
attributes have a dependency relation, the candi-
date value is judged not to have an attribute.
We adopted the tournament model for attribute
identification (Iida et al, 2003). This model im-
plements a pairwise comparison (i.e. a match)
between two candidates in reference to the given
value treating it as a binary classification prob-
lem, and conducting a tournament which consists
of a series of matches, in which the one that pre-
vails through to the final round is declared the
4http://chasen.naist.jp/
5http://chasen.org/?taku/software/cabocha/
176
winner, namely, it is identified as the most likely
candidate attribute. Each of the matches is con-
ducted as a binary classification task in which one
or other of the candidate wins.
The pairedness determination task and the
opinionhood determination task are also binary
classification tasks. In Proc.1, since pair identifi-
cation is conducted before finding the best candi-
date attribute, we used Soon et al?s model (Soon
et al, 2001) for pairedness determination. This
model picks up each possible candidate attribute
for a value and determines if it is the attribute for
that value. If all the candidates are determined not
to be the attribute, the value is judged not to have
an attribute. In Proc.4, we can use the information
about whether the value has a corresponding at-
tribute or not for opinionhood determination. We
therefore create two separate models for when the
value does and does not have an attribute.
4.3.3 Features
We extracted the following two types of fea-
tures from the candidate attribute and the candi-
date value:
(a) surface spelling and part-of-speech of the
target value expression, as well as those of its
dependent phrase and those in its depended
phrase(s)
(b) relation between the target value and can-
didate attribute (distance between them, ex-
istence of dependency, existence of a co-
occurrence relation)
We extracted (b) if the model could use both the
attribute and the value information. Existence of a
co-occurrence relation is determined by reference
to a predefined co-occurrence list that contains
attribute-value pair information such as ?height
of vehicle ? low?. We created the list from the
230,000 sentences described in section 4.3.1 by
applying the attribute and value dictionary and
extracting attribute-value pairs if there is a de-
pendency relation between the attribute and the
value. The number of pairs we extracted was
about 48,000.
4.4 Results
Table 1 shows the results of opinion extraction.
We evaluated the results by recall R and preci-
sion P defined as follows (For simplicity, we sub-
stitute ?A-V? for attribute-value pair):
R =
correctly extracted A-V opinions
total number of A-V opinions
,
P =
correctly extracted A-V opinions
total number of A-V opinions found by the system
.
In order to demonstrate the effectiveness of
the information about the candidate attribute, we
evaluated the results of pair extraction and opin-
ionhood determination separately. Table 2 shows
the results. In the pair extraction, we assume that
the value is given, and evaluate how successfully
attribute-value pairs are extracted.
4.5 Discussions
As Table 1 shows, our proposed ordering is out-
performed on the recall in Proc.3, however, the
precision is higher than Proc.3 and get the best F-
measure. In what follows, we discuss the results
of pair extraction and opinionhood determination.
Pair extraction From Table 2, we can see that
carrying out attribute identification before paired-
ness determination outperforms the reverse order-
ing by 11% better precision and 3% better recall.
This result supports our expectation that knowl-
edge of attribute information assists attribute-
value pair extraction. Focusing on the rows la-
beled ?(dependency)? and ?(no dependency)? in
Table 2, while 80% of the attribute-value pairs in
a direct dependency relation are successfully ex-
tracted with high precision, the model achieves
only 51.7% recall with 61.7% precision for the
cases where an attribute and value are not in a di-
rect dependency relation.
According to our error analysis, a major source
of errors lies in the attribute identification task. In
this experiment, the precision of attribute identifi-
cation is 78%. A major reason for this problem
was that the true attributes did not exist in our
dictionary. In addition, a major cause of error in
the pair determination stage is cases where an at-
tribute appearing in the preceding sentence causes
a false decision. We need to conduct further in-
vestigations in order to resolve these problems.
Opinionhood determination Table 2 also
shows that carrying out attribute identification
followed by opinionhood determination out-
performs the reverse ordering, which supports
our expectation that knowing the attribute
information aids opinionhood determination.
While it produces better results, our proposed
method still has room for improvement in both
precision and recall. Our current error analysis
has not identified particular error patterns ? the
types of errors are very diverse. However, we
need to at least address the issue of modifying
the feature set to make the model more sensitive
to modality-oriented distinctions such as subjunc-
tive and conditional expressions.
177
Table 1: The precision and the recall for opinion extraction
procedure value with attribute value without attribute attribute-value pairs
baseline precision 60.5% (1130/1869) 10.6% (249/2340) 32.8% (1379/4209)
recall 51.6% (1130/2191) 59.3% (249/420) 52.8% (1379/2611)
F-measure 55.7 21.0 40.5
Proc.1 precision 47.3% (864/1828) 21.6% ( 86/399) 42.7% ( 950/2227)
recall 39.4% (864/2191) 20.5% ( 86/420) 36.4% ( 950/2611)
F-measure 43.0 21.0 39.3
Proc.2 precision 63.0% (1074/1706) 38.0% (198/521) 57.1% (1272/2227)
recall 49.0% (1074/2191) 47.1% (198/420) 48.7% (1272/2611)
F-measure 55.1 42.0 52.6
Proc.3 precision 74.9% (1277/1632) 29.1% (151/519) 63.8% (1373/2151)
recall 55.8% (1222/2191) 36.0% (151/420) 52.6% (1373/2611)
F-measure 64.0 32.2 57.7
Proc.4 precision 80.5% (1175/1460) 30.2% (150/497) 67.7% (1325/1957)
recall 53.6% (1175/2191) 35.7% (150/420) 50.7% (1325/2611)
F-measure 64.4 32.7 58.0
Table 2: The result of pair extraction and opinionhood determination
procedure precision recall
pair extraction
baseline (dependency) 71.1% (1385/1929) 63.2% (1385/2191)
PD?AI 65.3% (1579/2419) 72.1% (1579/2191)
AI?PD 76.6% (1645/2148) 75.1% (1645/2191)
(dependency) 87.7% (1303/1486) 79.6% (1303/1637)
(no dependency) 51.7% ( 342/ 662) 61.7% ( 342/ 554)
opinionhood determination OD 74.0% (1554/2101) 60.2% (1554/2581)AI?OD 82.2% (1709/2078) 66.2% (1709/2581)
5 Conclusion
In this paper, we have proposed a machine
learning-based method for the extraction of opin-
ions on consumer products by reducing the prob-
lem to that of extracting attribute-value pairs from
texts. We have pointed out the similarity between
the tasks of anaphora resolution and opinion ex-
traction, and have applied the machine learning-
based method designed for anaphora resolution to
opinion extraction. The experimental results re-
ported in this paper show that identifying the cor-
responding attribute for a given value expression
is effective in both pairedness determination and
opinionhood determination.
References
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: opinion extraction and semantic
classification of product reviews. In Proc. of the 12th In-
ternational World Wide Web Conference, pages 519?528.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Center-
ing: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203?226.
M. Hu and B. Liu. 2004. Mining and summarizing customer
reviews. In Proc. of the Tenth International Conference
on Knowledge Discovery and Data Mining, pages 168?
177.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proc. of the EACL Workshop on the
Computational Treatment of Anaphora, pages 23?30.
R. Iida, K. Inui, Y. Matsumoto, and S. Sekine. 2005. Noun
phrase coreference resolution in Japanese base on most
likely antecedant candidates. Journal of Information Pro-
cessing Society of Japan, 46(3). (in Japanese).
H. Kanayama and T. Nasukawa. 2004. Deeper sentiment
analysis using machine translation technology. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics, pages 494?500.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expressions
for opinion extraction. In Proc. of the 1st International
Joint Conference on Natural Language Processing, pages
584?589.
S. Murano and S. Sato. 2003. Automatic extraction of sub-
jective sentences using syntactic patterns. In Proc. of the
Ninth Annual Meeting of the Association for Natural Lan-
guage Processing, pages 67?70. (in Japanese).
V. Ng. 2004. Learning noun phrase anaphoricity to improve
coreference resolution: Issues in representation and opti-
mization. In Proc. of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 152?159.
B. Pang and L. Lee. 2004. A sentiment education: Sen-
timent analysis using subjectivity summarization based
on minimum cuts. In Proc. of the 42nd Annual Meeting
of the Association for Computational Linguistics, pages
271?278.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
K. Tateishi, Y. Ishiguro, and T. Fukushima. 2001. Opinion
information retrieval from the Internet. In IPSJ SIGNL
Note 144-11, pages 75?82. (in Japanese).
K. Tateishi, T. Fukushima, N. Kobayashi, T. Takahashi,
A. Fujita, K. Inui, and Y. Matsumoto. 2004. Web opin-
ion extraction and summarization based on viewpoints
of products. In IPSJ SIGNL Note 163, pages 1?8. (in
Japanese).
P. D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of re-
views. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 417?424.
178
A Class-oriented Approach to Building a Paraphrase Corpus
Atsushi Fujita
Graduate School of Informatics,
Kyoto University
fujita@pine.kuee.kyoto-u.ac.jp
Kentaro Inui
Graduate School of Information Science,
Nara Institute of Science and Technology
inui@is.naist.jp
Abstract
Towards deep analysis of composi-
tional classes of paraphrases, we have
examined a class-oriented framework
for collecting paraphrase examples, in
which sentential paraphrases are col-
lected for each paraphrase class sep-
arately by means of automatic can-
didate generation and manual judge-
ment. Our preliminary experiments on
building a paraphrase corpus have so
far been producing promising results,
which we have evaluated according to
cost-efficiency, exhaustiveness, and re-
liability.
1 Introduction
Paraphrases are alternative ways of conveying the
same content. The technology for paraphrase
generation and recognition has drawn the atten-
tion of an increasing number of researchers be-
cause of its potential contribution to a broad range
of natural language applications.
Paraphrases can be viewed as monolingual
translations. From this viewpoint, research on
paraphrasing has adapted techniques fostered in
the literature of machine translation (MT), such
as transformation algorithms (Lavoie et al, 2000;
Takahashi et al, 2001), corpus-based techniques
for paraphrase pattern acquisition (Barzilay and
McKeown, 2001; Shinyama and Sekine, 2003;
Quirk et al, 2004), and fluency measurements
(Lapata, 2001; Fujita et al, 2004).
One thing the paraphrasing community is still
lacking is shared collections of paraphrase ex-
amples that could be used to analyze problems
underlying the tasks and to evaluate the perfor-
mance of systems under development. To our best
knowledge, the paraphrase corpus developed by
Dolan et al (2004) is one of the very few collec-
tions available for free1. Development of para-
phrase corpora raises several issues: what sorts
of paraphrases should be collected, where para-
phrase examples can be obtained from, how the
coverage and quality of the corpus can be ensured,
how manual annotation cost can be effectively re-
duced, and how collected examples should be or-
ganized and annotated.
Obviously these issues should be discussed
with the purpose of each individual corpus taken
into account. In this paper, we address the is-
sues of building a gold-standard corpus that is to
be used to evaluate paraphrase generation models
and report on our preliminary experiences taking
Japanese as a target language. Our approach is
characterized by the following:
? We define a set of paraphrase classes based
on the syntactic features of transformation
patterns.
? We separately collect paraphrase examples
for each paraphrase class that are considered
to be linguistically explainable.
? We use a paraphrase generation system to
exhaustively collect candidate paraphrases
from a given text collection, which are then
manually labeled.
15801 sentence pairs from their comparable corpus have
been judged manually and available from
http://research.microsoft.com/research/nlp/msr paraphrase.htm
25
2 Goal
Paraphrases exhibit a wide variety of patterns
ranging from lexical paraphrases to syntactic
transformations and their combinations. Some
of them are highly inferential or idiomatic and
do not seem easy to generate only with syntactic
and semantic knowledge. Such groups of para-
phrases require us to pursue corpus-based acquisi-
tion methods such as those described in Section 3.
More importantly, however, we can also find
quite a few patterns of paraphrases that exhibit a
degree of regularity. Those groups of paraphrases
have a potential to be compositionally explained
by combining syntactic and semantic properties
of their constituent words. For instance, the fol-
lowing paraphrases2 in Japanese are considered to
be of these groups.
(1) s. eiga-ni shigeki-o uke-ta.
film-DAT inspiration-ACC to receive-PAST
I received an inspiration from the film.
t. eiga-ni shigeki-s-are-ta.
film-DAT to inspire-PASS-PAST
I was inspired by the film.
(2) s. sentakumono-ga soyokaze-ni yureru.
laundry-NOM breeze-DAT to sway-PRES
The laundry sways in the breeze.
t. soyokaze-ga sentakumono-o yurasu.
breeze-NOM laundry-ACC to sway-PRES
The breeze makes the laundry sways.
(3) s. glass-ni mizu-o mitashi-ta.
glass-DAT water-ACC to fill-PAST
I filled water into the glass.
t. glass-o mizu-de mitashi-ta.
glass-ACC water-IMP to fill-PAST
I filled the glass with water.
(4) s. kare-wa kikai-sousa-ga jouzu-da.
he-TOP machine operation-NOM be good-PRES
He is good at machine operation.
t. kare-wa kikai-o jouzu-ni sousa-suru.
he-TOP machine-ACC well-ADV to operate-PRES
He operates machines well.
(5) s. heya-wa mou atatamat-teiru.
room-TOP already to be warmed-PERF
The room has already been warmed up.
t. heya-wa mou atatakai.
room-TOP already be warm-PRES
The room is warm.
2For each example, ?s? and ?t? denote an original sen-
tence and its paraphrase, respectively.
In example (1), a verb phrase, ?shigeki-o uke-
ta (to receive an inspiration),? is paraphrased into
a verbalized form of the noun, ?shigeki-s-are-ta
(to be inspired).? We can find a number of para-
phrases that exhibit a similar pattern of syntactic
transformation in the same language and group
such paraphrases into a single class, which is
possibly labeled ?paraphrasing of light-verb con-
struction.? Likewise, paraphrases exemplified by
(2) constitute another class, so-called transitiv-
ity alternation. Example (3) is of the locative
alternation class and example (4) the compound
noun decomposition class. In example (5), a verb
?atatamaru (to be warmed)? is paraphrased into
its adjective form, ?atatakai (be warm).? Para-
phrases involving such a lexical derivation are
also in our concern.
One can learn the existence of such groups
of paraphrases and the regularity each group ex-
hibits from the linguistic literature (Mel?c?uk and
Polgue`re, 1987; Jackendoff, 1990; Kageyama,
2001). According to Jackendoff and Kageyama,
for instance, both transitivity alternation and loca-
tive alternation can be explained in terms of the
syntactic and semantic properties of the verb in-
volved, which are represented by what they call
Lexical Conceptual Structure. The systematicity
underlying such linguistic accounts is intriguing
also from the engineering point of view as it could
enable us to take a more theoretically motivated
but still practical approach to paraphrase genera-
tion.
Aiming at this goal leads us to consider build-
ing a paraphrase corpus which enables us to eval-
uate paraphrase generation systems and conduct
error analysis for each paraphrase class sepa-
rately. Our paraphrase corpus should therefore be
organized according to paraphrase classes. More
specifically, we consider a paraphrase corpus such
that:
? The corpus consists of a set of subcorpora.
? Each subcorpus is a collection of paraphrase
sentence pairs of a paraphrase class.
? Paraphrases collected in a subcorpus suffi-
ciently reflect the distribution of the occur-
rences in the real world.
Given a paraphrase class and a text collection,
the goal of building a paraphrase corpus is to col-
lect paraphrase examples belonging to the class
26
as exhaustively as possible from the text collec-
tion at a minimal human labor cost. The resultant
corpus should also be reliable.
3 Related work
Previous work on building paraphrase corpus
(collecting paraphrase examples) can be classified
into two directions: manual production of para-
phrases and automatic paraphrase acquisition.
3.1 Manual production of paraphrases
Manual production of paraphrase examples has
been carried out in MT studies.
For example, Shirai et al (2001) and
Kinjo et al (2003) use collections of Japanese-
English translation sentence pairs. Given
translation pairs, annotators are asked to produce
new translations for each side of the languages.
Sentences that have an identical translation
are collected as equivalents, i.e., paraphrases.
Shimohata (2004), on the other hand, takes a
simpler approach in which he asks annotators to
produce paraphrases of a given set of English
sentences.
Obviously, if we simply asked human annota-
tors to produce paraphrases of a given set of sen-
tences, the labor cost would be expensive while
the coverage not guaranteed. Previous work,
however, has averted their eyes from evaluating
the cost-efficiency of the method and the cover-
age of the collected paraphrases supposedly be-
cause their primary concern was to enhance MT
systems.
3.2 Automatic paraphrase acquisition
Recently, paraphrase examples have been auto-
matically collected as a source of acquiring para-
phrase knowledge, such as pairs of synonymous
phrases and syntactic transformation templates.
Some studies exploit topically related articles
derived from multiple news sources (Barzilay and
Lee, 2003; Shinyama and Sekine, 2003; Quirk et
al., 2004; Dolan et al, 2004). Sentence pairs that
are likely to be paraphrases are automatically col-
lected from the parallel or comparable corpora,
using such clues as overlaps of content words and
named entities, syntactic similarity, and reference
description, such as date of the article and posi-
tions of sentences in the articles.
Automatic acquisition from parallel or compa-
rable corpora, possibly in combination with man-
ual correction, could be more cost-efficient than
manual production. However, it would not ensure
coverage and quality, because sentence pairing al-
gorithms virtually limit the range of obtainable
paraphrases and products tend to be noisy.
Nevertheless, automatic methods are useful to
discover a variety of paraphrases that need further
exploration. We hope that our approach to corpus
construction, which we present below, will work
complementary to those directions of research.
4 Proposed method
Recall that we require a corpus that reflects the
distribution of the occurrences of potential para-
phrases of each class because we aim to use it for
linguistic analysis and quantitative evaluation of
paraphrase generation models.
Since the issues we address here are highly em-
pirical, we need to empirically examine a range
of possible methods to gain useful methodologi-
cal insights. As an initial attempt, we have so far
examined a simple method which falls in the mid-
dle of the aforementioned two approaches. The
method makes use of an existing paraphrase gen-
eration system to reduce human labor cost as well
as to ensure coverage and quality:
Step 1. For a given paraphrase class, develop a
set of morpho-syntactic paraphrasing pat-
terns and lexical resources.
Step 2. Apply the patterns to a given text collec-
tion using the paraphrasing system to gener-
ate a set of candidate paraphrases.
Step 3. Annotate each candidate paraphrase with
information of the appropriateness accord-
ing to a set of judgement criteria.
We use morpho-syntactic paraphrasing patterns
derived from paraphrase samples in an analogous
way to previous methods such as (Dras, 1999).
For instance, from example (1), we derive a para-
phrasing pattern for paraphrasing of light-verb
constructions:
(6) s. N -o(?V ) V
N -ACC V
t. V (N)
V (N)
whereN is a variable which matches with a noun,
V a verb, V (N) denotes the verbalized form of
27
(e) confirmed (revised)
paraphrase
( ) fir  (r i )
r r
(c) annotator?s judge
(correct / incorrect)
( ) t t r?  j
( rr t / i rr t)
(d) error tags
( ) rr r t
(a) source sentence
( ) r  t
(b) automatically
generated
paraphrase
( ) t ti ll
r t
r r
(c) second opinion
(correct / incorrect)
( )  i i
( rr t / i rr t)
Given
Obligatory
Obligatory
Optional
(f) free comments
(f) fr  t
Optional
Figure 1: Annotation schema.
N , and the subscripted arrow in (6s) indicates that
N -o depends on V .
To exhaustively collect paraphrase examples
from a given text collection, we should not exces-
sively constrain paraphrasing patterns. To avoid
overly generating anomalies, on the other hand,
we make use of several lexical resources. For in-
stance, pairs of a deverbal noun and its transitive
form are used to constrainN and V (N) in pattern
(6). This way, we combine syntactic transforma-
tion patterns with lexical constraints to specify a
paraphrase class. This approach is practical given
the recent advances of shallow parsers.
For the judgement on appropriateness in Step 3,
we create a set of criteria separately for each para-
phrase class. When the paraphrase class in focus
is specified, the range of potential errors in candi-
date generation tends to be predictable. We there-
fore specify judgement criteria in terms of a ty-
pology of potential errors (Fujita and Inui, 2003);
namely, we provide annotators with a set of con-
ditions for ruling out inappropriate paraphrases.
Annotators judge each candidate paraphrase
with a view of an RDB-based annotation tool
(Figure 1). Given (a) a source sentence and
(b) an automatically generated candidate para-
phrase, human annotators are asked to (c) judge
the appropriateness of it and, if it is inappropri-
ate, they are also asked to (d) classify the un-
derlying errors into a predefined taxonomy, and
make (e) appropriate revisions (if possible) and
(f) format-free comments.
5 Preliminary trials
To examine how the proposed method actually
work regarding the issues, we conducted prelim-
inary trials, taking two classes of Japanese para-
phrases: paraphrasing of light-verb constructions
and transitivity alternation. This section de-
scribes the settings for each paraphrase class.
We sampled a collection of source sentences
from one year worth of newspaper articles: Ni-
hon Keizai Shinbun3, 2000, where the average
sentence length was 25.3 words. The reason
why we selected newspaper articles as a sample
source was that most of the publicly available
shallow parsers for Japanese were trained on a
tree-bank sampled from newspaper articles, and a
newspaper corpus was available in a considerably
large scale. We used for candidate generation the
morphological analyzer ChaSen4, the dependency
structure analyzer CaboCha5, and the paraphrase
generation system KURA6.
Two native speakers of Japanese, adults grad-
uated from university, were employed as annota-
tors. The process of judging each candidate para-
phrase is illustrated in Figure 2. The first annota-
tor was asked to make judgements on each candi-
date paraphrase. The second annotator inspected
all the candidates judged correct by the first an-
3http://sub.nikkeish.co.jp/gengo/zenbun.htm
4http://chasen.naist.jp/
5http://chasen.org/?taku/software/cabocha/
6http://cl.naist.jp/kura/doc/
28
Candidate
paraphrase
i t
r r
Correct
Incorrect
1st annotator 2nd annotator
Correct
Incorrect
Correct
Incorrect
Correct
Deferred
Incorrect
Discussion
Unseen
Deferred
f
Correct
t
Incorrect
I t
Label
Figure 2: Judgement procedure.
notator. To reduce the labor cost, only a small
subset of candidates that the first annotator judged
incorrect were checked by the second annotator,
leaving the rest labeled incorrect. Once in sev-
eral days, the annotators discussed cases on which
they disagreed, and if possible revised the anno-
tation criteria. When the discussion did not reach
a consensus, the judgement was deferred.
5.1 Paraphrasing of light-verb constructions
(LVC)
An example of this class is given in (1). A light-
verb construction consists of a deverbal noun
(?shigeki (inspiration)? in example (1)) governed
by a light-verb (?ukeru (to receive)?). A para-
phrase of this class is a pair of a light-verb con-
struction and its unmarked form, which consists
of the verbalized form of the deverbal noun where
the light-verb is removed.
Let N , V be a deverbal noun and a verb, and
V (N) be the verbalized form of N . Paraphrases
of this class can be represented by the following
paraphrasing pattern:
(7) s. N -{ga, o, ni}(?V ) V
N -{NOM, ACC, DAT} V
t. V (N)
V (N)
In the experiment, we used three more patterns to
gain the coverage.
We then extracted 20,155 pairs of deverbal
noun and its verbalized form (e.g. ?shigeki (in-
spiration)? and ?shigeki-suru (to inspire)?) from
the Japanese word dictionary, IPADIC (version
2.6.3)3. This set was used as a restriction on
nouns that can match with N in a paraphrasing
pattern. On the other hand, we made no restric-
tion on V , because we had no exhaustive list
of light-verbs. The patterns were automatically
compiled into pairs of dependency trees with
uninstantiated components, and were applied to
source sentences with the paraphrase generation
system, which carried out dependency structure-
based pattern matching. 2,566 candidate para-
phrases were generated from 10,000 source sen-
tences.
In the judgement phase, the annotators were
also asked to revise erroneous candidates if pos-
sible. The following revision operations were al-
lowed for LVC:
? Change of conjugations
? Change of case markers
? Insert adverbs
? Append verbal suffixes, such as voice, as-
pect, or mood devices
When pattern (7) is applied to sentence (1s), for
instance, we need to add a voice device, ?are (pas-
sive),? to correctly produce (1t). In example (8),
on the other hand, an aspectual device, ?dasu (in-
choative),? is appended, and a case marker, ?no
(GEN),? is replaced with ?o (ACC).?
(8) s. concert-no ticket-no hanbai-o hajime-ta.
concert-GEN ticket-GEN sale-ACC to start-PAST
We started to sale tickets for concerts.
t. concert-no ticket-o hanbai-shi-dashi-ta.
concert-GEN ticket-ACC to sell-INCHOATIVE-PAST
We started selling tickets for concerts.
So far, 1,114 candidates have been judged7 with
agreements on 1,067 candidates, and 591 para-
phrase examples have been collected.
5.2 Transitivity alternation (TransAlt)
This class of paraphrases requires a collection of
pairs of intransitive and transitive verbs, such as
?yureru (to sway)? and ?yurasu (to sway)? in ex-
ample (2). Since there was no available resource
of such knowledge, we newly created a mini-
mal set of intransitive-transitive pairs that were
required to cover all the verbs appearing in the
source sentence set (25,000 sentences). We first
retrieved all the verbs from the source sentences
using a set of extraction patterns implemented in
the same manner as paraphrasing patterns. Ex-
ample (9) is one of the patterns used, where Nx
matches with a noun, and V a verb.
7983 candidates for the first 4,500 sentences were fully
judged, and 131 candidates were randomly sampled from
the remaining portion.
29
(9) s. N1-ga(?V ) N2-ni(?V ) V
N1-NOM N2-DAT V
t. no change.
We then manually examined the transitivity of
each of 800 verbs that matched with V , and col-
lected 212 pairs of intransitive verb vi and its tran-
sitive form vt. Using them as constraints, we im-
plemented eight paraphrasing patterns as in (10).
(10) s. N1-ga(?Vi) N2-ni(?Vi) Vi
N1-NOM N2-DAT Vi
t. N2-ga(?Vt(Vi)) N1-o(?Vt(Vi)) Vt(Vi)
N2-NOM N1-ACC Vt(Vi)
where Vi and Vt(Vi) are variables that match with
vi and vt, respectively. By applying the patterns
to the same set of source sentences, we obtained
985 candidate paraphrases.
We created a set of criteria for judging ap-
propriateness (an example will be given in
Section 6.4) and revision examples for the follow-
ing operations allowed for this trial:
? Change of conjugations
? Change of case markers
? Change of voices
964 candidates have gained an agreement, and
484 paraphrase examples have been collected.
6 Results and discussion
Table 1 gives some statistics of the resultant para-
phrase corpora. Figures 3 and 4 show the number
of candidate paraphrases, where the horizontal
axes denote the total working hours of two anno-
tators, and the vertical axes the number of candi-
date paraphrases. The numbers of judged, correct,
incorrect, and deferred candidates are shown.
6.1 Efficiency
2,031 candidate paraphrases have so far been
judged in total and 1,075 paraphrase examples
have been collected in 287.5 hours. The judge-
ment was performed at a constant pace: 7.1 can-
didates (3.7 examples) in one hour. It is hard to
compare these results with other work because
no previous study quantitatively evaluate the effi-
ciency in terms of manual annotation cost. How-
ever, we feel that the results have so far been sat-
isfiable.
For each candidate paraphrase judged incor-
rect, the annotators were asked to classify the un-
derlying errors into the fixed error types ((d) in
Table 1: Statistics of the resultant corpora.
Paraphrase class LVC TransAlt
# of source sentences 10,000 25,000
# of patterns 4 8
Type of lexical resources ?n, vn? ?vi, vt?
Size of lexical resource 20,155 212
# of candidates 2,566 985
# of judged candidates 1,067 964
# of incorrect candidates 520 503
# of correct candidates 547 461
# of paraphrase examples 591 484
Working hours 118 169.5
Figure 1). This error classification consumed ex-
tra time because it required linguistic expertise
which the annotators were not familiar with.
TransAlt was 1.75 times more time-consuming
than LVC because the definition of TransAlt in-
volved several delicate issues, which made the
judgement process complicated. We return to this
issue in Section 6.4.
6.2 Exhaustiveness
To estimate how exhaustively the proposed
method collected paraphrase examples, we ran-
domly sampled 750 sentences from the 4,500
sentences that were used in the trial for LVC,
and manually checked whether the LVC para-
phrasing could apply to each of them. As a re-
sult, 206 examples were obtained, 158 of which
were those already collected by the proposed
method. Thus, the estimated exhaustiveness was
77% (158 / 206). Our manual investigation into
the missed examples has revealed that 47 misses
could have been automatically generated by en-
hancing paraphrasing patterns and dictionaries,
while only one example was missed due to an er-
ror in shallow parsing. 34 cases of the 48 misses
could have been collected by adding a couple of
paraphrasing patterns. For example, pattern (11)
verbalizes a noun followed by a nominalizing suf-
fix, ?ka (-ize),? as in (12).
(11) s. N -ka-{ga, o, ni}(?V ) V
N -ize-{NOM, ACC, DAT} V
t. V (N -ka)
V (N -ize)
(12) s. kore-wa kin?yu-shijo-no kassei-ka-ni
this-TOP financial market-GEN activation-DAT
muke-ta kisei-kanwa-saku-da.
to address-PAST deregulation plan-COP
This is a deregulation plan aiming at the
activation of financial market.
30
 0
 200
 400
 600
 800
 1000
 1200
 0  20  40  60  80  100  120
# 
of
 ju
dg
ed
 ca
nd
ida
tes
working hours
Judged
Correct
Incorrect
Deferred
Figure 3: # of judged candidates (LVC).
 0
 200
 400
 600
 800
 1000
 0  20  40  60  80  100  120  140  160  180
# 
of
 ju
dg
ed
 ca
nd
ida
tes
working hours
Judged
Correct
Incorrect
Deferred
Figure 4: # of judged candidates (TransAlt).
t. kore-wa kin?yu-shijo-o
this-TOP financial market-ACC
kassei-ka-suru kisei-kanwa-saku-da.
to activate-PRES deregulation plan-COP
This is a deregulation plan which activates
financial market.
We cannot know if we have adequate para-
phrasing patterns and resources before trials.
Therefore, manual examination is necessary to re-
fine them to bridge gap between the range of para-
phrases that can be automatically generated and
those of the specific class we consider.
6.3 Reliability
Ideally, more annotators should be employed to
ensure the reliability of the products, which, how-
ever, leads to a matter of balancing the trade-off.
Instead, we specified the detailed judgement cri-
teria for each paraphrase class, and asked the an-
notators to reconsider marginal cases several days
later and to make a discussion when judgements
disagreed. The agreement ratio for correct candi-
dates between two annotators increased as they
became used to the task. In the trial for LVC,
for example, the agreement ratio for each day
changed from 74% (day 3) to 77% (day 6), 88%
(day 9), and 93% (day 11). This indicates that the
judgement criteria were effectively refined based
on the feedback from inter-annotator discussions
on marginal and disagreed cases. To evaluate the
reliability of our judgement procedure more pre-
cisely, we are planing to employ the third annota-
tor who will be asked to judge all the cases inde-
pendently of the others.
6.4 How we define paraphrase classes
One of the motivations behind our class-based ap-
proach is an expectation that specifying the target
classes of paraphrases would simplify the awk-
ward problem of defining the boundary between
paraphrases an non-paraphrases. Our trials for the
two paraphrase classes, however, have revealed
that it can still be difficult to create a clear cri-
terion for judgement even when the paraphrase
class in focus is specified.
As one of the criteria for TransAlt, we tested
the agentivity of the nominative case of intransi-
tive verbs. The test used an adverb, ?muzukara
(by itself),? and classified a candidate paraphrase
as incorrect if the adverb could be inserted im-
mediately before the intransitive verb. For ex-
ample, we considered example (13) as a correct
paraphrase of the TransAlt class whereas (14) in-
correct because the agentivity exhibited by (14s)
did not remain in (14t).
(13) s. kare-ga soup-o atatame-ta.
he-NOM soup-ACC to warm up-PAST
He warmed the soup up.
t. soup-ga atatamat-ta. (correct)
soup-NOM to be warmed up-PAST
The soup was warmed up (by somebody).
(14) s. kare-ga koori-o tokashi-ta.
he-NOM ice-ACC to melt (vt)-PAST
He melted the ice.
t. koori-ga toke-ta. (incorrect)
ice-NOM to melt (vi)-PAST
The ice melted (by itself).
However, one might regard both paraphrases
incorrect because the information given by the
nominative argument of the source sentence is
31
dropped in the target in both cases. Thus, the
problem still remains. Nevertheless, our approach
will provide us with a considerable amounts of
concrete data, which we hope will lead us to bet-
ter understanding of the issue.
7 Conclusion
Towards deep analysis of compositional classes of
paraphrases, we have examined a class-oriented
framework for collecting paraphrase examples,
in which sentential paraphrases are collected for
each paraphrase class separately by means of au-
tomatic candidate generation and manual judge-
ment. Our preliminary experiments on building
a paraphrase corpus have so far been producing
promising results, which we have evaluated ac-
cording to cost-efficiency, exhaustiveness, and re-
liability. The resultant corpus and resources will
be available for free shortly. Our next step is di-
rected to targeting a wider range of paraphrase
classes.
References
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of the 2003 Human Language Technology Confer-
ence and the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 16?23.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources.
In Proceedings of the 20th International Con-
ference on Computational Linguistics (COLING),
pages 350?356.
Mark Dras. 1999. Tree adjoining grammar and the re-
luctant paraphrasing of text. Ph.D. thesis, Division
of Information and Communication Science, Mac-
quarie University.
Atsushi Fujita and Kentaro Inui. 2003. Explor-
ing transfer errors in lexical and structural para-
phrasing. IPSJ Journal, 44(11):2826?2838. (in
Japanese).
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2004. Detection of incorrect case assignments in
automatically generated paraphrases of Japanese
sentences. In Proceedings of the 1st International
Joint Conference on Natural Language Processing
(IJCNLP), pages 14?21.
Ray Jackendoff. 1990. Semantic structures. The MIT
Press.
Taro Kageyama, editor. 2001. Semantics and syntax
of verb: comparable study between Japanese and
English. Taishukan Shoten. (in Japanese).
Yumiko Kinjo, Kunio Aono, Keishi Yasuda, Toshiyuki
Takezawa, and Genichiro Kikui. 2003. Collec-
tion of Japanese paraphrases of basic expressions
on travel conversation. In Proceedings of the 9th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 101?104. (in Japanese).
Maria Lapata. 2001. A corpus-based account of reg-
ular polysemy: the case of context-sensitive ad-
jectives. In Proceedings of the 2nd Meeting of
the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 63?70.
Benoit Lavoie, Richard Kittredge, Tanya Korelsky,
and Owen Rambow. 2000. A framework for MT
and multilingual NLG systems based on uniform
lexico-structural processing. In Proceedings of the
6th Applied Natural Language Processing Confer-
ence and the 1st Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (ANLP-NAACL), pages 60?67.
Igor Mel?c?uk and Alain Polgue`re. 1987. A formal
lexicon in meaning-text theory (or how to do lex-
ica with words). Computational Linguistics, 13(3-
4):261?275.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 142?149.
Mitsuo Shimohata. 2004. Acquiring paraphrases
from corpora and its application to machine trans-
lation. Ph.D. thesis, Graduate School of Informa-
tion Science, Nara Institute of Science and Tech-
nology.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of the 2nd International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP), pages 65?71.
Satoshi Shirai, Kazuhide Yamamoto, and Francis
Bond. 2001. Japanese-English paraphrase corpus.
In Proceedings of the 6th Natural Language Pro-
cessing Pacific Rim Symposium (NLPRS) Workshop
on Language Resources in Asia, pages 23?30.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, At-
sushi Fujita, and Kentaro Inui. 2001. KURA:
a transfer-based lexico-structural paraphrasing en-
gine. In Proceedings of the 6th Natural Language
Processing Pacific Rim Symposium (NLPRS) Work-
shop on Automatic Paraphrasing: Theories and Ap-
plications, pages 37?46.
32
Acquiring Event Relation Knowledge by Learning Cooccurrence Patterns
and Fertilizing Cooccurrence Samples with Verbal Nouns
Shuya Abe Kentaro Inui Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
{shuya-a,inui,matsu}@is.naist.jp
Abstract
Aiming at acquiring semantic relations be-
tween events from a large corpus, this paper
proposes several extensions to a state-of-the-
art method originally designed for entity re-
lation extraction, reporting on the present re-
sults of our experiments on a Japanese Web
corpus. The results show that (a) there are
indeed specific cooccurrence patterns use-
ful for event relation acquisition, (b) the
use of cooccurrence samples involving ver-
bal nouns has positive impacts on both re-
call and precision, and (c) over five thou-
sand relation instances are acquired from a
500M-sentence Web corpus with a precision
of about 66% for action-effect relations.
1 Introduction
The growing interest in practical NLP applications
such as question answering, information extraction
and multi-document summarization places increas-
ing demands on the processing of relations between
textual fragments such as entailment and causal rela-
tions. Such applications often need to rely on a large
amount of lexical semantic knowledge. For exam-
ple, a causal (and entailment) relation holds between
the verb phrases wash something and something is
clean, which reflects the commonsense notion that if
someone has washed something, this object is clean
as a result of the washing event. A crucial issue is
how to obtain and maintain a potentially huge col-
lection of such event relations instances.
Motivated by this background, several research
groups have reported their experiments on automatic
acquisition of causal, temporal and entailment re-
lations between event mentions (typically verbs or
verb phrases) (Lin and Pantel, 2001; Inui et al,
2003; Chklovski and Pantel, 2005; Torisawa, 2006;
Pekar, 2006; Zanzotto et al, 2006, etc.). The com-
mon idea behind them is to use a small number of
manually selected generic lexico-syntactic cooccur-
rence patterns (LSPs or simply patterns). to Verb-X
and then Verb-Y, for example, is used to obtain tem-
poral relations such as marry and divorce (Chklovski
and Pantel, 2005). The use of such generic patterns,
however, tends to be high recall but low precision,
which requires an additional component for pruning
extracted relations. This issue has been addressed in
basically two approaches, either by devising heuris-
tic statistical scores (Chklovski and Pantel, 2005;
Torisawa, 2006; Zanzotto et al, 2006) or training
classifiers for disambiguation with heavy supervi-
sion (Inui et al, 2003).
This paper explores a third way for enhancing
present LSP-based methods for event relation acqui-
sition. The basic idea is inspired by the following
recent findings in relation extraction (Ravichandran
and Hovy, 2002; Pantel and Pennacchiotti, 2006,
etc.), which aims at extracting semantic relations be-
tween entities (as opposed to events) from texts. (a)
The use of generic patterns tends to be high recall
but low precision, which requires an additional com-
ponent for pruning. (b) On the other hand, there are
specific patterns that are highly reliable but they are
much less frequent than generic patterns and each
makes only a small contribution to recall. (c) Com-
bining a few generic patters with a much larger col-
lection of reliable specific patterns boosts both pre-
497
cision and recall. Such specific patterns can be ac-
quired from a very large corpus with seeds.
Given these insights, an intriguing question is
whether the same story applies to event relation ac-
quisition as well or not. In this paper, we explore this
issue through the following steps. First, while previ-
ous methods use only verb-verb cooccurrences, we
use cooccurrences between verbal nouns and verbs
such as cannot ?find out (something)? due to the
lack of ?investigation? as well as verb-verb cooc-
currences. This extension dramatically enlarge the
pool of potential candidate LSPs (Section 4.1). Sec-
ond, we extend Pantel and Pennacchiotti (2006)?s
Espresso algorithm, which induces specific reliable
LSPs in a bootstrapping manner for entity relation
extraction, so that the extended algorithm can apply
to event relations (Sections 4.2 to 4.4). Third, we
report on the present results of our empirical experi-
ments, where the extended algorithm is applied to a
Japanese 500M-sentence Web corpus to acquire two
types of event relations, action-effect and action-
means relations (Section 5)
2 Related work
Perhaps a simplest way of using LSPs for event rela-
tion acquisition can be seen in the method Chklovski
and Pantel (2005) employ to develop VerbOcean.
Their method uses a small number of manually se-
lected generic LSPs such as to Verb-X and then Verb-
Y to obtain six types of semantic relations including
strength (e.g. taint ? poison) and happens-before
(e.g. marry ? divorce) and obtain about 29,000 verb
pairs with 65.5% precision.
One way for pruning extracted relations is to in-
corporate a classifier trained with supervision. Inui
et al (2003), for example, use a Japanese generic
causal connective marker tame (because) and a su-
pervised classifier learner to separately obtain four
types of causal relations: cause, precondition, effect
and means.
Torisawa (2006), on the other hand, acquires en-
tailment relations by combining the verb pairs ex-
tracted with a highly generic connective pattern
Verb-X and Verb-Y together with the cooccurrence
statistics between verbs and their arguments. While
the results Torisawa reports look promising, it is not
clear yet if the method applies to other types of rela-
tions because it relies on relation-specific heuristics.
Another direction from (Chklovski and Pantel,
2005) is in the use of LSPs involving nominalized
verbs. Zanzotto et al (2006) obtain, for example, an
entailment relation X wins ? X plays from such a
pattern as player wins. However, their way of using
nominalized verbs is highly limited compared with
our way of using verbal nouns.
3 Espresso
This section overviews Pantel and Pennacchiotti
(2006)?s Espresso algorithm. Espresso takes as input
a small number of seed instances of a given target
relation and iteratively learns cooccurrence patterns
and relation instances in a bootstrapping manner.
Ranking cooccurrence patterns For each given
relation instance {x, y}, Espresso retrieves the sen-
tences including both x and y from a corpus and
extracts from them cooccurrence samples. For ex-
ample, given an instance of the is-a relation such
as ?Italy,country?, Espresso may find cooccurrence
samples such as countries such as Italy and extract
such a pattern as Y such as X. Espresso defines the
reliability rpi(p) of pattern p as the average strength
of its association with each relation instance i in
the current instance set I , where each instance i is
weighted by its reliability r?(i):
rpi(p) = 1|I|
?
i?I
pmi(i, p)
max pmi ? r?(i) (1)
where pmi(i, p) is the pointwise mutual information
between i and p, and maxpmi is the maximum PMI
between all patterns and all instances.
Ranking relation instances Intuitively, a reliable
relation instance is one that is highly associated with
multiple reliable patterns. Hence, analogously to the
above pattern reliability measure, Espresso defines
the reliability r?(i) of instance i as:
r?(i) = 1|P |
?
p?P
pmi(i, p)
max pmi ? rpi(p) (2)
where rpi(p) is the reliability of pattern p, defined
above in (1), and maxpmi is as before. r?(i) and
rpi(p) are recursively defined, where r?(i) = 1 for
each manually supplied seed instance i1.
1For our extension, r?(i) = ?1 for each manually supplied
negative instance.
498
4 Event relation acquisition
Our primary concerns are whether there are in-
deed specific cooccurrence patterns useful for ac-
quiring event relations and whether such patterns
can be found in a bootstrapping manner analogous to
Espresso. To address these issues, we make several
extensions to Espresso, which is originally designed
for entity relations (not scoping event relations).
4.1 Cooccurences with verbal nouns
Most previous methods for event relation acquisition
rely on verb-verb cooccurrences because verbs (or
verb phrases) are the most typical device for refer-
ring to events. However, languages have another
large class of words for event reference, namely
verbal nouns or nominalized forms of verbs. In
Japanese, for example, verbal nouns such as kenkyu
(research) constitute the largest morphological cate-
gory used for event reference.
Japanese verbal nouns have dual statuses, as verbs
and nouns. When occurring with the verb suru (do-
PRES), verbal nouns function as a verb as in (1a).
On the other hand, when accompanied by case mark-
ers such as ga (NOMINATIVE) and o (ACCUSATIVE),
they function as a noun as in (1b). Finally, but even
more importantly, when accompanied by a large va-
riety of suffixes, verbal nouns constitute compound
nouns highly productively as in (1c).
(1) a. Ken-ga gengo-o kenkyu-suru
Ken-NOM language-ACC research-PRES
Ken researches on language.
b. Ken-ga gengo-no kenkyu-o yame-ta
Ken-NOM language-on research-ACC quit-PAST
Ken quitted research on language.
c. -sha (person):
e.g. kenkyu-sha (researcher)
-shitsu (place):
e.g. kenkyu-shitsu (laboratory)
-go (after):
e.g. kenkyu-go (after research)
These characteristics of verbal nouns can be made
use of to substantially increase both cooccurrence
instances and candidate cooccurrence patterns (see
Section 5.1 for statistics). For example, the verbal
noun kenkyu (research) often cooccurs with the verb
jikken (experiment) in the pattern of (2a). From
those cooccurrences, one may learn that jikken-suru
(to experiment) is an action that is often taken as a
part of kenkyu-suru (to research). In such a case, we
may consider a pattern as shown in (2b) useful for
acquiring part-of relations between actions.
(2) a. kenkyu-shitsu-de jikken-suru
research-place-in experiment-VERB
conduct experiments in the laboratory
b. (Act-X)-shitsu-de (Act-Y)-suru
(Act-X)-place-in (Act-X)-VERB
(Act-Y) is often done in doing (Act-X)
When functioning as a noun, verbal nouns are po-
tentially ambiguous between the event reading and
the entity/object reading. For example, the ver-
bal noun denwa (phone) in the context denwa-de
(phone-by) may refer to either a phone-call event
or a physical phone. While, ideally, such event-
hood ambiguities should be resolved before collect-
ing cooccurrence samples with verbal nouns, we
simply use all the occurrences of verbal nouns in
collecting cooccurrences in our experiments. It is
an interesting issue for future work whether event-
hood determination would have a strong impact on
the performance of event relation extraction.
4.2 Selection of arguments
One major step from the extraction of entity rela-
tions to the extraction of event relations is how to
address the issue of generalization. In entity rela-
tion extraction, relations are typically assumed to
hold between chunks like named entities or simply
between one-word terms, where the issue of deter-
mining the appropriate level of the generality of ex-
tracted relations has not been salient. In event rela-
tion extraction, on the other hand, this issue imme-
diately arises. For example, the cooccurrence sam-
ple in (3) suggests the action-effect relation between
niku-o yaku (grill the meat) and (niku-ni) kogeme-ga
tsuku ((the meat) gets brown)2.
(3) ( kogeme-ga tsuku ) -kurai niku-o yaku
a burn-NOM get -so that meat-ACC grill
grill the meat so that it gets brown
(grill the meat to a deep brown)
In this relation, the argument niku (meat) of the
verb yaku (grill) can be dropped and generalized
2The parenthesis in the first row of (3) indicates a subordi-
nate clause.
499
to something to grill; namely the action-effect rela-
tion still holds between X-o yaku (grill X) and X-ni
kogeme-ga tsuku (X gets brown). On the other hand,
however, the argument kogeme (a burn) of the verb
tsuku (get) cannot be dropped; otherwise, the rela-
tion would no longer hold.
One straightforward way to address this problem
is to expand each cooccurrence sample to those cor-
responding to different degrees of generalization and
feed them to the relation extraction model so that its
scoring function can select appropriate event pairs
from expanded samples. For example, cooccurrence
sample (3) is expanded to those as in (4):
(4) a. ( kogeme-ga tsuku ) -kurai niku-o yaku
a burn-NOM get -so that meat-ACC grill
b. ( tsuku ) -kurai niku-o yaku
get -so that meat-ACC grill
c. ( kogeme-ga tsuku ) -kurai yaku
a burn-NOM get -so that grill
d. ( tsuku ) -kurai yaku
get -so that grill
In practice, in our experiments (Section 5), we re-
strict the number of arguments for each event up to
one to avoid the explosion of the types of infrequent
candidate relation instances.
4.3 Volitionality of events
Inui et al (2003) discuss how causal rela-
tions between events should be typologized for
the purpose of semantic inference and classify
causal relations basically into four types ? Ef-
fect, Means, Precondition and Cause relations
? based primarily on the volitionality of in-
volved events. For example, Effect relations hold
between volitional actions and their resultative
non-volitional states/happenings/experiences, while
Cause relations hold between only non-volitional
states/happenings/experiences.
Following this typology, we are concerned with
the volitionality of each event mention. For our
experiments, we manually built a lexicon of over
12,000 verbs (including verbal nouns) with volition-
ality labels, obtaining 8,968 volitional verbs, 3,597
non-volitional and 547 ambiguous. Volitional verbs
include taberu (eat) and kenkyu-suru (research),
while non-volitional verbs include atatamaru (get
warm), kowareru (to break-vi) and kanashimu (be
sad). We discarded the ambiguous verbs in the ex-
periments.
4.4 Dependency-based cooccurrence patterns
The original Espresso encodes patterns simply as a
word sequence because entity mentions in the rela-
tions it scopes tend to cooccur locally in a single
phrase or clause. In event relation extraction, how-
ever, cooccurrence patterns of event mentions in the
relations we consider (causal relations, temporal re-
lations, etc.) can be captured better as a path on
a syntactic dependency tree because (i) such men-
tion pairs tend to cooccur in a longer dependency
path and (ii) as discussed in Section 4.2, we want
to exclude the arguments of event mentions from
cooccurrence patterns, which would be difficult with
word sequence-based representations of patterns.
A Japanese sentence can be analyzed as a se-
quence of base phrase (BP) chunks called bunsetsu
chunks, each which typically consists of one con-
tent (multi-)word followed by functional words. We
assume each sentence of our corpus is given a de-
pendency parse tree over its BP chunks. Let us call
a BP chunk containing a verb or verbal noun an
event chunk. We create a cooccurrence sample from
any pair of event chunks that cooccur if either (a)
one event chunk depends directly on the other, or
(b) one event chunk depends indirectly on the other
via one intermediate chunk. Additionally, we apply
the Japanese functional expressions dictionary (Mat-
suyoshi et al, 2006) to a cooccurrence pattern for
generalization.
In (5), for example, the two event chunks,
taishoku-go-ni (after retirement) and hajimeru (be-
gin), meet the condition (b) above and the depen-
dency path designated by bold font is identified as a
candidate cooccurrence pattern. The argument PC-o
of the verb hajimeru is excluded from the path.
(5) (taishoku-go-no tanoshimi)-ni PC-o hajimeru
retirement-after as a hobby PC-ACC begin
begin a PC as a hobby after retirement
5 Experiments
5.1 Settings
For an empirical evaluation, we used a sample
of approximately 500M sentences taken from the
500
Table 1: Examples of acuired cooccurrence patterns and relatio instances for the action-effect relation
freq cooccurrence patterns relation instances
94477 ?verb;action?temo?verb;effect?nai
(to do ?action? though ?effect? dose not happen)
sagasu::mitsukaru (search::be found),
asaru::mitsukaru (hunt::be found), purei-suru::kuria-
suru (play::finish)
6250 ?verb;action?takeredomo?verb;effect?nai
(to do ?action? though ?effect? dose not happen)
shashin-wo-toru::toreru (shot photograph::be shot),
meiru-wo-okuru::henji-ga-kaeru (send a mail::get an
answer)
1851 ?noun;action?wo-shitemo?verb;effect?nai
(to do ?action? though ?effect? dose not happen)
setsumei-suru::nattoku-suru (explain::agree), siai-
suru::katsu (play::win), siai-suru::makeru (play::lose)
1329 ?verb;action?yasukute?adjective;effect?
(to simply do ?action? and ?effect?)
utau::kimochiyoi (sing::feel good),
hashiru::kimochiyoi (run::feel good)
4429 ?noun;action?wo-kiite?verb;effect?
(to hear ?action? so that ?effect?)
setsumei-suru::nattoku-suru (explain::agree), setsumei-
suru::rikai-dekiru (explain::can understand)
Web corpus collected by Kawahara and Kuro-
hashi (2006). The sentences were dependency-
parsed with Cabocha (Kudo and Matsumoto, 2002),
and cooccurrence samples of event mentions were
extracted. Event mentions with patterns whose fre-
quency was less than 20 were discarded in order to
reduce computational costs. As a result, we obtained
34M cooccurrence tokens with 11M types. Note
that among those cooccurrence samples 15M tokens
(44%) with 4.8M types (43%) are those with ver-
bal nouns, suggesting the potential impacts of using
verbal nouns.
In our experiments, we considered two of Inui et
al. (2003)?s four types of causal relations: action-
effect relations (Effect in Inui et al?s terminology)
and action-means relations (Means). An action-
effect relation holds between events x and y if and
only if non-volitional event y is likely to happen as
either a direct or indirect effect of volitional action
x. For example, the action X-ga undou-suru (X exer-
cises) and the event X-ga ase-o kaku (X sweats) are
considered to be in this type of relation. A action-
means relation holds between events x and y if and
only if volitional action y is likely to be done as a
part/means of volitional action x. For example, if
case a event-pair is X-ga hashiru (X runs) is consid-
ered as a typical action that is often done as a part of
the action X-ga undou-suru (X exercises).
Note that in these experiments we do not differ-
entiate between relations with the same subject and
those with a different subject. However we plan to
conduct further experiments in the future that make
use of this distinction.
In addition, we have collected action-effect rela-
tion instances for a baseline measure. The baseline
consists of instances that cooccur with eleven pat-
terns that indicate action-effect relation. The dif-
ference between the extended Espresso and baseline
is caused by the low number and constant scores of
patterns.
5.2 Results
We ran the extended Espresso algorithm starting
with 971 positive and 1069 negative seed relation
instances for action-effect relation and 860 positive
and 74 negative seed relations for action-means re-
lation. As a result, we obtained 34,993 cooccurrence
patterns with 173,806 relation instances for the
action-effect relation and 23,281 coocurrence rela-
tions with 237,476 relation instances for the action-
means relation after 20 iterations of pattern rank-
ing/selection and instance ranking/selection. The
threshold parameters for selecting patterns and in-
stances were decided in a preliminary trial. Some
of the acquired patterns and instances for the action-
effect relation are shown in Table 1.
5.2.1 Precision
To estimate precision, 100 relation instances were
randomly sampled from each of four sections of the
ranks of the acquired instances for each of the two
relations (1?500, 501?1500, 1501?3500 and 3500?
7500), and the correctness of each sampled instance
was judged by two graduate students (i.e. 800 rela-
tion instances in total were judged).
Note that in these experiments we asked the asses-
sors to both (a) the degree of the likeliness that the
effect/means takes place and (b) which arguments
are shared between the two events. For example,
while nomu (drink) does not necessarily result in
501
futsukayoi-ni naru (have a hangover), the assessors
judged this pair correct because one can at least say
that the latter sometimes happens as a result of the
former. For criterion (b), as shown in Table 1, the
relation instances judged correct include both the X-
ga VP1::X-ga VP2 type (i.e. two subjects are shared)
and the X-o VP1::X-ga VP2 type (the object of the
former and the subject of the latter are shared). The
issue of how to control patterns of argument sharing
is left for future work.
The precision for the assessed samples are shown
in Figures 1 to 3. ?2 judges? means that an instance
is acceptable to both judges. ?1 judges? means that
it is an acceptable instance to at least one of the two
judges. ?strict? indicates correct instance relations
while ?lenient?3 indicates correct instance relations
? when a judge appends the right cases.
As a result of this strictness in judgement, the
inter-assessor agreement turned out to be poor. The
kappa statistics was 0.53 for the action-effect rela-
tions, 0.49 for the action-effect relations (=baseline)
and 0.55 for action-means relations.
The figures show that both types of relations were
acquired with reasonable precision not only for the
higher-ranked instances but also for lower-ranked
instances. It may seem strange that the precision
of the lower-ranked action-means instances is some-
times even better than the higher-ranked ones, which
may mean that the scoring function given in Section
3 did not work properly. While further investiga-
tion is clearly needed, it should also be noted that
higher-ranked instances tended to be more specific
than lower-ranked ones.
5.2.2 Effects of seed number
We reran the extended Espresso algorithm for the
action-effect relation, starting with 500 positive and
500 negative seed relation instances. The preci-
sion is shown in Figure 44. This precision is fairly
lower than that of action-effect relations with all
seed instances. Additionally, the number of seed in-
stances affects the precision of both higher-ranked
and lower-ranked instances. This result indicates
that while the proposed algorithm is designed to
work with a small seed set, in reality its performance
3If an instance is judged as ?strict? by one assessor and ?le-
nient? by the other, then the instance is assessed as ?lenient?.
4It was only judged by one assessor.
severely depends on the number of seeds.
5.2.3 Effects of using verbal nouns
We also examine the effect of using verbal nouns.
Of the 500 highest scored patterns for the action-
effect relation, 128 patterns include verbal noun
slots, and for action-means, 495 patterns. Hence,
the presence of verbal nouns greatly effects some
acquired instances. Additionally, to see the influ-
ence of frequency, of the 500 high frequent patterns
selected from the 2000 highest scored patterns for
action-effect relation, 177 include verbal noun slots,
and for action-means, 407 patterns. This result pro-
vides further evidence that the inclusion of verbal
nouns has a positive effect in this task.
5.2.4 Argument selection
According to our further investigation on argu-
ment selection, 49 instances (12%) of the correct
action-effect relation instances that are judged cor-
rect have a specific argument in at least one event,
and all of them would be judged incorrect (i.e. over-
generalized) if they did not have those arguments
(Recall the example of kogeme-ga tsuku (get brown)
in Section 4.2). This figure indicates that our method
for argument selection works to a reasonable degree.
However, clearly there is still much room for im-
provement. According to our investigation, up to
26% of the instances that are judged incorrect could
be saved if appropriate arguments were selected. For
example, X-ga taberu (X eats) and X-ga shinu (X
dies) would constitute an action-effect relation if the
former event took such an argument as dokukinoko-
o (toadstool-ACC). The overall precision could be
boosted if an effective method for argument selec-
tion method were devised.
6 Conclusion and future work
In this paper, we have addressed the issue of how
to learn lexico-syntactic patterns useful for acquir-
ing event relation knowledge from a large corpus,
and proposed several extensions to a state-of-the-art
method originally designed for entity relation ex-
traction, reporting on the present results of our em-
pirical evaluation. The results have shown that (a)
there are indeed specific cooccurrence patterns use-
ful for event relation acquisition, (b) the use of cooc-
currence samples involving verbal nouns has pos-
502
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000
pre
cis
ion
 [%
]
rank
strict (2 judged)
lenient (2 judged)
strict (1 judged)
lenient (1 judged)
Figure 1: action-effect
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000
pre
cis
ion
 [%
]
rank
strict (2 judged)
lenient (2 judged)
strict (1 judged)
lenient (1 judged)
Figure 2: action-means
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000
pre
cis
ion
 [%
]
rank
strict (2 judged)
lenient (2 judged)
strict (1 judged)
lenient (1 judged)
Figure 3: action-effect (baseline)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000
pre
cis
ion
 [%
]
rank
system (strict)
system (lenient)
baseline (strict)
baseline (lenient)
half (strict)
half (lenient)
Figure 4: action-effect (half seed)
503
itive impacts on both recall and precision, and (c)
over five thousand relation instances are acquired
from the 500M-sentence Web corpus with a preci-
sion of about 66% for action-effect relations.
Clearly, there is still much room for exploration
and improvement. First of all, more comprehensive
evaluations need to be done. For example, the ac-
quired relations should be evaluated in terms of re-
call and usefulness. A deep error analysis is also
needed. Second, the experiments have revealed that
one major problem to challenge is how to optimize
argument selection. We are seeking a way to incor-
porate a probabilistic model of predicate-argument
cooccurrences into the ranking function for relation
instances. Related to this issue, it is also crucial
to devise a method for controlling argument shar-
ing patterns. One possible approach is to employ
state-of-the-art techniques for coreference and zero-
anaphora resolution (Iida et al, 2006; Komachi et
al., 2007, etc.) in preprocessing cooccurrence sam-
ples.
References
Timothy Chklovski and Patrick Pantel. 2005. Global
path-based refinement of noisy graphs applied to verb
semantics. In Proceedings of Joint Conference on Nat-
ural Language Processing (IJCNLP-05), pages 792?
803.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the ACL, pages 625?632.
Takashi Inui, Kentaro Inui, and Yuji Matsumoto. 2003.
What kinds and amounts of causal knowledge can be
acquired from text by using connective markers as
clues? In Proceedings of the 6th International Con-
ference on Discovery Science, pages 180?193. An ex-
tended version: Takashi Inui, Kentaro Inui, and Yuji
Matsumoto (2005). Acquiring causal knowledge from
text using the connective marker tame. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 4(4):435?474.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 176?183.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Learning based argument structure
analysis of event-nouns in japanese. In Proceedings
of the Conference of the Pacific Association for Com-
putational Linguistics (PACLING), pages 120?128.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In CoNLL
2002: Proceedings of the 6th Conference on Natu-
ral Language Learning 2002 (COLING 2002 Post-
Conference Workshops), pages 63?69.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining 2001, pages 323?328.
Suguru Matsuyoshi, Satoshi Sato, and Takehito Utsuro.
2006. Compilation of a dictionary of japanese func-
tional expressions with hierarchical organization. In
Proceedings of the 21st International Conference on
Computer Processing of Oriental Languages, pages
395?402.
Patric Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 113?120.
Viktor Pekar. 2006. Acquisition of verb entailment from
text. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages 49?56.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 21st International Conference
on Computational Linguistics and 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 41?47.
Kentaro Torisawa. 2006. Acquiring inference rules with
temporal constraints by using japanese coordinated
sentences and noun-verb co-occurrences. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 57?64.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 849?856.
504
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 625?632,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Syntactic Patterns as Clues in Zero-Anaphora Resolution
Ryu Iida, Kentaro Inui and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{ryu-i,inui,matsu}@is.naist.jp
Abstract
We approach the zero-anaphora resolu-
tion problem by decomposing it into
intra-sentential and inter-sentential zero-
anaphora resolution. For the former prob-
lem, syntactic patterns of the appearance
of zero-pronouns and their antecedents are
useful clues. Taking Japanese as a target
language, we empirically demonstrate that
incorporating rich syntactic pattern fea-
tures in a state-of-the-art learning-based
anaphora resolution model dramatically
improves the accuracy of intra-sentential
zero-anaphora, which consequently im-
proves the overall performance of zero-
anaphora resolution.
1 Introduction
Zero-anaphora is a gap in a sentence that has an
anaphoric function similar to a pro-form (e.g. pro-
noun) and is often described as ?referring back?
to an expression that supplies the information nec-
essary for interpreting the sentence. For example,
in the sentence ?There are two roads to eternity,
a straight and narrow, and a broad and crooked,?
the gaps in ?a straight and narrow (gap)? and ?a
broad and crooked (gap)? have a zero-anaphoric
relationship to ?two roads to eternity.?
The task of identifying zero-anaphoric relations
in a given discourse, zero-anaphora resolution,
is essential in a wide range of NLP applications.
This is the case particularly in such a language as
Japanese, where even obligatory arguments of a
predicate are often omitted when they are inferable
from the context. In fact, in our Japanese newspa-
per corpus, for example, 45.5% of the nominative
arguments of verbs are omitted. Since such gaps
can not be interpreted only by shallow syntac-
tic parsing, a model specialized for zero-anaphora
resolution needs to be devised on the top of shal-
low syntactic and semantic processing.
Recent work on zero-anaphora resolution can
be located in two different research contexts. First,
zero-anaphora resolution is studied in the con-
text of anaphora resolution (AR), in which zero-
anaphora is regarded as a subclass of anaphora. In
AR, the research trend has been shifting from rule-
based approaches (Baldwin, 1995; Lappin and Le-
ass, 1994; Mitkov, 1997, etc.) to empirical, or
corpus-based, approaches (McCarthy and Lehnert,
1995; Ng and Cardie, 2002a; Soon et al, 2001;
Strube and Mu?ller, 2003; Yang et al, 2003) be-
cause the latter are shown to be a cost-efficient
solution achieving a performance that is compa-
rable to best performing rule-based systems (see
the Coreference task in MUC1 and the Entity De-
tection and Tracking task in the ACE program2).
The same trend is observed also in Japanese zero-
anaphora resolution, where the findings made in
rule-based or theory-oriented work (Kameyama,
1986; Nakaiwa and Shirai, 1996; Okumura and
Tamura, 1996, etc.) have been successfully
incorporated in machine learning-based frame-
works (Seki et al, 2002; Iida et al, 2003).
Second, the task of zero-anaphora resolution
has some overlap with Propbank3-style semantic
role labeling (SRL), which has been intensively
studied, for example, in the context of the CoNLL
SRL task4. In this task, given a sentence ?To at-
tract younger listeners, Radio Free Europe inter-
sperses the latest in Western rock groups?, an SRL
1http://www-nlpir.nist.gov/related projects/muc/
2http://projects.ldc.upenn.edu/ace/
3http://www.cis.upenn.edu/?mpalmer/project pages/ACE.htm
4http://www.lsi.upc.edu/?srlconll/
625
model is asked to identify the NP Radio Free Eu-
rope as the A0 (Agent) argument of the verb at-
tract. This can be seen as the task of finding
the zero-anaphoric relationship between a nomi-
nal gap (the A0 argument of attract) and its an-
tecedent (Radio Free Europe) under the condition
that the gap and its antecedent appear in the same
sentence.
In spite of this overlap between AR and SRL,
there are some important findings that are yet to
be exchanged between them, partly because the
two fields have been evolving somewhat indepen-
dently. The AR community has recently made two
important findings:
? A model that identifies the antecedent of an
anaphor by a series of comparisons between
candidate antecedents has a remarkable ad-
vantage over a model that estimates the ab-
solute likelihood of each candidate indepen-
dently of other candidates (Iida et al, 2003;
Yang et al, 2003).
? An AR model that carries out antecedent
identification before anaphoricity determina-
tion, the decision whether a given NP is
anaphoric or not (i.e. discourse-new), sig-
nificantly outperforms a model that executes
those subtasks in the reverse order or simulta-
neously (Poesio et al, 2004; Iida et al, 2005).
To our best knowledge, however, existing SRL
models do not exploit these advantages. In SRL,
on the other hand, it is common to use syntactic
features derived from the parse tree of a given in-
put sentence for argument identification. A typ-
ical syntactic feature is the path on a parse tree
from a target predicate to a noun phrase in ques-
tion (Gildea and Jurafsky, 2002; Carreras and Mar-
quez, 2005). However, existing AR models deal
with intra- and inter-sentential anaphoric relations
in a uniform manner; that is, they do not use as rich
syntactic features as state-of-the-art SRL models
do, even in finding intra-sentential anaphoric rela-
tions. We believe that the AR and SRL communi-
ties can learn more from each other.
Given this background, in this paper, we show
that combining the aforementioned techniques de-
rived from each research trend makes signifi-
cant impact on zero-anaphora resolution, taking
Japanese as a target language. More specifically,
we demonstrate the following:
? Incorporating rich syntactic features in a
state-of-the-art AR model dramatically im-
proves the accuracy of intra-sentential zero-
anaphora resolution, which consequently im-
proves the overall performance of zero-
anaphora resolution. This is to be considered
as a contribution to AR research.
? Analogously to inter-sentential anaphora, de-
composing the antecedent identification task
into a series of comparisons between candi-
date antecedents works remarkably well also
in intra-sentential zero-anaphora resolution.
We hope this finding to be adopted in SRL.
The rest of the paper is organized as follows.
Section 2 describes the task definition of zero-
anaphora resolution in Japanese. In Section 3,
we review previous approaches to AR. Section 4
described how the proposed model incorporates
effectively syntactic features into the machine
learning-based approach. We then report the
results of our experiments on Japanese zero-
anaphora resolution in Section 5 and conclude in
Section 6.
2 Zero-anaphora resolution
In this paper, we consider only zero-pronouns that
function as an obligatory argument of a predicate
for two reasons:
? Providing a clear definition of zero-pronouns
appearing in adjunctive argument positions
involves awkward problems, which we be-
lieve should be postponed until obligatory
zero-anaphora is well studied.
? Resolving obligatory zero-anaphora tends to
be more important than adjunctive zero-
pronouns in actual applications.
A zero-pronoun may have its antecedent in the dis-
course; in this case, we say the zero-pronoun is
anaphoric. On the other hand, a zero-pronoun
whose referent does not explicitly appear in the
discourse is called a non-anaphoric zero-pronoun.
A zero-pronoun may be non-anaphoric typically
when it refers to an extralinguistic entity (e.g. the
first or second person) or its referent is unspecified
in the context.
The following are Japanese examples. In sen-
tence (1), zero-pronoun ?i is anaphoric as its an-
tecedent, ?shusho (prime minister)?, appears in the
same sentence. In sentence (2), on the other hand,
?j is considered non-anaphoric if its referent (i.e.
the first person) does not appear in the discourse.
(1) shushoi-wa houbeisi-te ,
prime ministeri-TOP visit-U.S.-CONJ PUNC
626
ryoukoku-no gaikou-o
both countries-BETWEEN diplomacy-OBJ
(?i-ga) suishinsuru
(?i-NOM) promote-ADNOM
houshin-o akirakanisi-ta .
plan-OBJ unveil-PAST PUNC
The prime minister visited the united states
and unveiled the plan to push diplomacy
between the two countries.
(2) (?j-ga) ie-ni kaeri-tai .
(?j -NOM) home-DAT want to go back PUNC
(I) want to go home.
Given this distinction, we consider the task of
zero-anaphora resolution as the combination of
two sub-problems, antecedent identification and
anaphoricity determination, which is analogous to
NP-anaphora resolution:
For each zero-pronoun in a given dis-
course, find its antecedent if it is
anaphoric; otherwise, conclude it to be
non-anaphoric.
3 Previous work
3.1 Antecedent identification
Previous machine learning-based approaches to
antecedent identification can be classified as ei-
ther the candidate-wise classification approach or
the preference-based approach. In the former ap-
proach (Soon et al, 2001; Ng and Cardie, 2002a,
etc.), given a target anaphor, TA, the model esti-
mates the absolute likelihood of each of the candi-
date antecedents (i.e. the NPs preceding TA), and
selects the best-scored candidate. If all the can-
didates are classified negative, TA is judged non-
anaphoric.
In contrast, the preference-based ap-
proach (Yang et al, 2003; Iida et al, 2003)
decomposes the task into comparisons of the
preference between candidates and selects the
most preferred one as the antecedent. For exam-
ple, Iida et al (2003) proposes a method called
the tournament model. This model conducts a
tournament consisting of a series of matches in
which candidate antecedents compete with each
other for a given anaphor.
While the candidate-wise classification model
computes the score of each single candidate inde-
pendently of others, the tournament model learns
the relative preference between candidates, which
is empirically proved to be a significant advan-
tage over candidate-wise classification (Iida et al,
2003).
3.2 Anaphoricity determination
There are two alternative ways for anaphoric-
ity determination: the single-step model and the
two-step model. The single-step model (Soon et
al., 2001; Ng and Cardie, 2002a) determines the
anaphoricity of a given anaphor indirectly as a
by-product of the search for its antecedent. If
an appropriate candidate antecedent is found, the
anaphor is classified as anaphoric; otherwise, it is
classified as non-anaphoric. One disadvantage of
this model is that it cannot employ the preference-
based model because the preference-based model
is not capable of identifying non-anaphoric cases.
The two-step model (Ng, 2004; Poesio et al,
2004; Iida et al, 2005), on the other hand, car-
ries out anaphoricity determination in a separate
step from antecedent identification. Poesio et
al. (2004) and Iida et al (2005) claim that the lat-
ter subtask should be done before the former. For
example, given a target anaphor (TA), Iida et al?s
selection-then-classification model:
1. selects the most likely candidate antecedent
(CA) of TA using the tournament model,
2. classifies TA paired with CA as either
anaphoric or non-anaphoric using an
anaphoricity determination model. If the
CA-TA pair is classified as anaphoric, CA is
identified as the antecedent of TA; otherwise,
TA is conclude to be non-anaphoric.
The anaphoricity determination model learns the
non-anaphoric class directly from non-anaphoric
training instances whereas the single-step model
cannot not use non-anaphoric cases in training.
4 Proposal
4.1 Task decomposition
We approach the zero-anaphora resolution prob-
lem by decomposing it into two subtasks: intra-
sentential and inter-sentential zero-anaphora reso-
lution. For the former problem, syntactic patterns
in which zero-pronouns and their antecedents ap-
pear may well be useful clues, which, however,
does not apply to the latter problem. We there-
fore build a separate component for each sub-
task, adopting Iida et al (2005)?s selection-then-
classification model for each component:
1. Intra-sentential antecedent identification:
For a given zero-pronoun ZP in a given
sentence S, select the most-likely candidate
antecedent C?1 from the candidates appearing
in S by the intra-sentential tournament model
627
2. Intra-sentential anaphoricity determination:
Estimate plausibility p1 that C?1 is the true an-
tecedent, and return C?1 if p1 ? ?intra (?intra
is a preselected threshold) or go to 3 other-
wise
3. Inter-sentential antecedent identification:
Select the most-likely candidate antecedent
C?2 from the candidates appearing outside of
S by the inter-sentential tournament model.
4. Inter-sentential anaphoricity determination:
Estimate plausibility p2 that C?2 is the true
antecedent, and return C?2 if p2 ? ?inter
(?inter is a preselected threshold) or return
non-anaphoric otherwise.
4.2 Representation of syntactic patterns
In the first two of the above four steps, we use syn-
tactic pattern features. Analogously to SRL, we
extract the parse path between a zero-pronoun to
its antecedent to capture the syntactic pattern of
their occurrence. Among many alternative ways
of representing a path, in the experiments reported
in the next section, we adopted a method as we
describe below, leaving the exploration of other al-
ternatives as future work.
Given a sentence, we first use a standard depen-
dency parser to obtain the dependency parse tree,
in which words are structured according to the de-
pendency relation between them. Figure 1(a), for
example, shows the dependency tree of sentence
(1) given in Section 2. We then extract the path
between a zero-pronoun and its antecedent as in
Figure 1(b). Finally, to encode the order of sib-
lings and reduce data sparseness, we further trans-
form the extracted path as in Figure 1(c):
? A path is represented by a subtree consist-
ing of backbone nodes: ? (zero-pronoun),
Ant (antecedent), Node (the lowest common
ancestor), LeftNode (left-branch node) and
RightNode.
? Each backbone node has daughter nodes,
each corresponding to a function word asso-
ciated with it.
? Content words are deleted.
This way of encoding syntactic patterns is used
in intra-sentential anaphoricity determination. In
antecedent identification, on the other hand, the
tournament model allows us to incorporate three
paths, a path for each pair of a zero-pronoun and
left and right candidate antecedents, as shown in
  
	
 

 	
 


Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647?655,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Capturing Salience with a Trainable Cache Model for Zero-anaphora
Resolution
Ryu Iida
Department of Computer Science
Tokyo Institute of Technology
2-12-1, ?Ookayama, Meguro,
Tokyo 152-8552, Japan
ryu-i@cl.cs.titech.ac.jp
Kentaro Inui Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma
Nara 630-0192, Japan
{inui,matsu}@is.naist.jp
Abstract
This paper explores how to apply the notion
of caching introduced by Walker (1996) to
the task of zero-anaphora resolution. We
propose a machine learning-based imple-
mentation of a cache model to reduce the
computational cost of identifying an an-
tecedent. Our empirical evaluation with
Japanese newspaper articles shows that the
number of candidate antecedents for each
zero-pronoun can be dramatically reduced
while preserving the accuracy of resolving
it.
1 Introduction
There have been recently increasing concerns
with the need for anaphora resolution to make
NLP applications such as IE and MT more reli-
able. In particular, for languages such as Japanese,
anaphora resolution is crucial for resolving a
phrase in a text to its referent since phrases, es-
pecially nominative arguments of predicates, are
frequently omitted by anaphoric functions in dis-
course (Iida et al, 2007b).
Many researchers have recently explored ma-
chine learning-based methods using considerable
amounts of annotated data provided by, for exam-
ple, the Message Understanding Conference and
Automatic Context Extraction programs (Soon et
al., 2001; Ng and Cardie, 2002; Yang et al, 2008;
McCallum and Wellner, 2003, etc.). These meth-
ods reach a level comparable to or better than the
state-of-the-art rule-based systems (e.g. Baldwin
(1995)) by recasting the task of anaphora resolution
into classification or clustering problems. How-
ever, such approaches tend to disregard theoretical
findings from discourse theories, such as Center-
ing Theory (Grosz et al, 1995). Therefore, one of
the challenging issues in this area is to incorporate
such findings from linguistic theories into machine
learning-based approaches.
A typical machine learning-based approach
to zero-anaphora resolution searches for an an-
tecedent in the set of candidates appearing in all
the preceding contexts. However, computational
time makes this approach largely infeasible for
long texts. An alternative approach is to heuristi-
cally limit the search space (e.g. the system deals
with candidates only occurring in the N previous
sentences). Various research such as Yang et al
(2008) has adopted this approach, but it also leads
to problems when an antecedent is located far from
its anaphor, causing it to be excluded from target
candidate antecedents.
On the other hand, rule-based methods derived
from theoretical background such as Centering
Theory (Grosz et al, 1995) only deal with the
salient discourse entities at each point of the dis-
course status. By incrementally updating the dis-
course status, the set of candidates in question
is automatically limited. Although these meth-
ods have a theoretical advantage, they have a
serious drawback in that Centering Theory only
retains information about the previous sentence.
A few methods have attempted to overcome this
fault (Suri and McCoy, 1994; Hahn and Strube,
1997), but they are overly dependent upon the re-
strictions fundamental to the notion of centering.
We hope that by relaxing such restrictions it will
be possible for an anaphora resolution system to
achieve a good balance between accuracy and com-
putational cost.
From this background, we focus on the issue
of reducing candidate antecedents (discourse en-
tities) for a given anaphor. Inspired by Walker?s
argument (Walker, 1996), we propose a machine
learning-based caching mechanism that captures
the most salient candidates at each point of the
discourse for efficient anaphora resolution. More
specifically, we choose salient candidates for each
sentence from the set of candidates appearing in
that sentence and the candidates which are already
647
in the cache. Searching only through the set of
salient candidates, the computational cost of zero-
anaphora resolution is effectively reduced. In the
empirical evaluation, we investigate how efficiently
this caching mechanism contributes to reducing the
search space while preserving accuracy. This pa-
per focuses on Japanese though the proposed cache
mechanism may be applicable to any language.
This paper is organized as follows. First,
Section 2 presents the task of zero-anaphora res-
olution and then Section 3 gives an overview
of previous work. Next, in Section 4 we pro-
pose a machine learning-based cache model.
Section 5 presents the antecedent identification and
anaphoricity determination models used in the ex-
periments. To evaluate the model, we conduct sev-
eral empirical evaluations and report their results
in Section 6. Finally, we conclude and discuss the
future direction of this research in Section 7.
2 Zero-anaphora resolution
In this paper, we consider only zero-pronouns
that function as an obligatory argument of a predi-
cate. A zero-pronoun may or may not have its an-
tecedent in the discourse; in the case it does, we say
the zero-pronoun is anaphoric. On the other hand,
a zero-pronoun whose referent does not explicitly
appear in the discourse is called a non-anaphoric
zero-pronoun. A zero-pronoun is typically non-
anaphoric when it refers to an extralinguistic entity
(e.g. the first or second person) or its referent is
unspecified in the context.
The task of zero-anaphora resolution can be
decomposed into two subtasks: anaphoricity de-
termination and antecedent identification. In
anaphoricity determination, the model judges
whether a zero-pronoun is anaphoric (i.e. a zero-
pronoun has an antecedent in a text) or not. If a
zero-pronoun is anaphoric, the model must detect
its antecedent. For example, in example (1) the
model has to judge whether or not the zero-pronoun
in the second sentence (i.e. the nominative argu-
ment of the predicate ?to hate?) is anaphoric, and
then identify its correct antecedent as ?Mary.?
(1) Mary
i
-wa John
j
-ni (?
j
-ga) tabako-o
Mary
i
-TOP John
j
-DAT (?
j
-NOM) smoking-OBJ
yameru-youni it-ta .
quit-COMP say-PAST PUNC
Mary told John to quit smoking.
(?
i
-ga) tabako-o kirai-dakarada .
(?
i
-NOM) smoking-OBJ hate-BECAUSE PUNC
Because (she) hates people smoking.
3 Previous work
Early methods for zero-anaphora resolution were
developed with rule-based approaches in mind.
Theory-oriented rule-based methods (Kameyama,
1986; Walker et al, 1994), for example, focus
on the Centering Theory (Grosz et al, 1995) and
are designed to collect the salient candidate an-
tecedents in the forward-looking center (Cf ) list,
and then choose the most salient candidate, Cp,
as an antecedent of a zero-pronoun according to
heuristic rules (e.g. topic > subject > indirect ob-
ject > direct object > others1). Although these
methods have a theoretical advantage, they have
a serious drawback in that the original Centering
Theory is restricted to keeping information about
the previous sentence only. In order to loosen this
restriction, the Centering-based methods have been
extended for reaching an antecedent appearing fur-
ther from its anaphor. For example, Suri and Mc-
Coy (1994) proposed a method for capturing two
kinds of Cp, that correspond to the most salient
discourse entities within the local transition and
within the global focus of a text. Hahn and Strube
(1997) estimate hierarchical discourse segments of
a text by taking into account a series of Cp and then
the resolution model searches for an antecedent in
the estimated segment. Although these methods
remedy the drawback of Centering, they still overly
depend on the notion of Centering such as Cp.
On the other hand, the existing machine
learning-based methods (Aone and Bennett, 1995;
McCarthy and Lehnert, 1995; Soon et al, 2001;
Ng and Cardie, 2002; Seki et al, 2002; Isozaki
and Hirao, 2003; Iida et al, 2005; Iida et al,
2007a, etc.) have been developed with less atten-
tion given to such a problem. These methods ex-
haustively search for an antecedent within the list
of all candidate antecedents until the beginning of
the text. Otherwise, the process to search for an-
tecedents is heuristically carried out in a limited
search space (e.g. the previous N sentences of an
anaphor) (Yang et al, 2008).
4 Machine learning-based cache model
As mentioned in Section 2, the procedure for
zero-anaphora resolution can be decomposed into
two subtasks, namely anaphoricity determination
and antecedent identification. In this paper,
these two subtasks are carried out according to
the selection-then-classification model (Iida et al,
1
?A > B? means A is more salient than B.
648
2005), chosen because it it has the advantage of
using broader context information for determining
the anaphoricity of a zero-pronoun. It does this by
examining whether the context preceding the zero-
pronoun in the discourse has a plausible candidate
antecedent or not. With this model, antecedent
identification is performed first, and anaphoricity
determination second, that is, the model identifies
the most likely candidate antecedent for a given
zero-pronoun and then it judges whether or not the
zero-pronoun is anaphoric.
As discussed by Iida et al (2007a), intra-
sentential and inter-sentential zero-anaphora reso-
lution should be dealt with by taking into account
different kinds of information. Syntactic patterns
are useful clues for intra-sentential zero-anaphora
resolution, whereas rhetorical clues such as con-
nectives may be more useful for inter-sentential
cases. Therefore, the intra-sentential and inter-
sentential zero-anaphora resolution models are sep-
arately trained by exploiting different feature sets
as shown in Table 2.
In addition, as mentioned in Section 3, inter-
sentential cases have a serious problem where the
search space of zero-pronouns grows linearly with
the length of the text. In order to avoid this prob-
lem, we incorporate a caching mechanism origi-
nally addressed by Walker (1996) into the follow-
ing procedure of zero-anaphora resolution by lim-
iting the search space at step 3 and by updating the
cache at step 5.
Zero-anaphora resolution process:
1. Intra-sentential antecedent identification: For
a given zero-pronoun ZP in a given sentence S,
select the most-likely candidate antecedent A
1
from the candidates appearing in S by the intra-
sentential antecedent identification model.
2. Intra-sentential anaphoricity determination:
Estimate plausibility p
1
that A
1
is the true an-
tecedent, and return A
1
if p
1
? ?
intra
2 or go to
3 otherwise.
3. Inter-sentential antecedent identification: Se-
lect the most-likely candidate antecedent A
2
from the candidates appearing in the cache as
explained in Section 4.1 by the inter-sentential
antecedent identification model.
4. Inter-sentential anaphoricity determination:
Estimate plausibility p
2
that A
2
is the true an-
tecedent, and return A
2
if p
2
? ?
inter
3 or return
2?
intra
is a preselected threshold.
3?
inter
is a preselected threshold.
non-anaphoric otherwise.
5. After processing all zero-pronouns in S, the
cache is updated. The resolution process is con-
tinued until the end of the discourse.
4.1 Dynamic cache model
Because the original work of the cache model by
Walker (1996) is not fully specified for implemen-
tation, we specify how to retain the salient candi-
dates based on machine learning in order to capture
both local and global foci of discourse.
In Walker (1996)?s discussion of the cache
model in discourse processing, it was presumed to
operate under a limited attention constraint. Ac-
cording to this constraint, only a limited number of
candidates can be considered in processing. Ap-
plying the concept of cache to computer hardware,
the cache represents working memory and the main
memory represents long-term memory. The cache
only holds the most salient entities, while the rest
are moved to the main memory for possible later
consideration as a cache candidate. If a new can-
didate antecedent is retrieved from main memory
and inserted into the cache, or enters the cache di-
rectly during processing, other candidates in the
cache have to be displaced due to the limited ca-
pacity of the cache. Which candidate to displace is
determined by a cache replacement policy. How-
ever, the best policy for this is still unknown.
In this paper, we recast the cache replacement
policy as a ranking problem in machine learning.
More precisely, we choose the N best candidates
for each sentence from the set of candidates ap-
pearing in that sentence and the candidates that are
already in the cache. Following this cache model,
named the dynamic cache model, anaphora resolu-
tion is performed by repeating the following two
processes.
1. Cache update: cache C
i
for sentence S
i
is cre-
ated from the candidates in the previous sen-
tence S
i?1
and the ones in the previous cache
C
i?1
.
2. Inter-sentential zero-anaphora resolution:
cache C
i
is used as the search space for
inter-sentential zero-anaphora resolution in
sentence S
i
(see Step 3 of the aforementioned
zero-anaphora resolution process).
For each cache update (see Figure 1), a current
cache C
i
is created by choosing the N most salient
candidates from the M candidates in S
i?1
and the
N candidates in the previous cache C
i?1
. In order
to implement this mechanism, we train the model
649
...
1)1( ?i
c
2)1( ?i
c
Mi
c
)1( ?
...
2)1( ?i
e
Ni
e
)1( ?
1?i
S
1?i
C
i
C
cache sentence
cache update
antecedent identification
1)1( ?i
e
...
2i
e
iN
e
1i
e
ij
?
Figure 1: Anaphora resolution using the dynamic
cache model
so that it captures the salience of each candidate.
To reflect this, each training instance is labeled
as either retained or discarded. If an instance is re-
ferred to by an zero-pronoun appearing in any of
the following sentences, it is labeled as retained;
otherwise, it is labeled as discarded. Training in-
stances are created in the algorithm detailed in
Figure 2. The algorithm is designed with the fol-
lowing two points in mind.
First, the cache model must capture the salience
of each discourse entity according to the recency
of its entity at each discourse status because typi-
cally the more recently an entity appears, the more
salient it is. To reflect this, training instances
are created from candidates as they appear in the
text, and are labeled as retained from the point of
their appearance until their referring zero-pronoun
is reached, at which time they are labeled as dis-
carded if they are never referred to by any zero-
pronouns in the succeeding context.
Suppose, the situation shown in Figure 3, where
c
ij
is the j-th candidate in sentence S
i
. In this
situation, for example, candidate c
12
is labeled
as retained when creating training instances for
sentence S
1
, but labeled as discarded from S
2
onwards, because of the appearance of its zero-
pronoun. Another candidate c
13
which is never re-
ferred to in the text is labeled as discarded for all
training instances.
Second, we need to capture the ?relative?
salience of candidates appearing in the current dis-
course for each cache update, as also exploited in
the tournament-based or ranking-based approaches
to anaphora resolution (Iida et al, 2003; Yang et
al., 2003; Denis and Baldridge, 2008). To solve
it, we use a ranker trained on the instances created
as described above. In order to train the ranker,
we adopt the Ranking SVM algorithm (Joachims,
2002), which learns a weight vector to rank candi-
dates for a given partial ranking of each discourse
entity. Each training instance is created from the
set of retained candidates, R
i
, paired with the set
of discarded candidates, D
i
, in each sentence. To
Function makeTrainingInstances (T : input text)
C := NULL // set of preceding candidates
S := NULL // set of training instances
i := 1; // init
while (exists s
i
) // s
i
: i-th sentence in T
E
i
:= extractCandidates(s
i
)
R
i
:= extractRetainedInstances(E
i
, T )
D
i
:= E
i
\R
i
r
i
:= extractRetainedInstances(C, T )
R
i
:= R
i
? r
i
D
i
:= D
i
? (C\r
i
)
S := S ? {?R
i
, D
i
?}
C := updateSalienceInfo(C)
C := C ? E
i
i := i + 1
endwhile
return S
end
Function extractRetainedInstances (S, T )
R := NULL // init
while (elm ? S)
if (elm is anaphoric with a zero-pronoun located
in the following sentences of T )
R := R ? elm
endif
endwhile
return R
end
Function updateSalienceInfo (C, s
i
)
while (c ? C)
if (c is anaphoric with a zero pronoun in s
i
)
c.position := i; // update the position information
endif
endwhile
return C
end
Figure 2: Pseudo-code for creating training in-
stances
1
S
11
c
12
c
13
c
14
c
2
S
21
c
22
c
23
c
i
?
j
?
3
S
31
c
32
c
33
c
k
?
retained discarded
11
c
12
c
13
c
14
c
l
?
training instances
retained discarded
11
c
22
c
13
c
14
c
21
c
23
c
12
c
Figure 3: Creating training instnaces
define the partial ranking of candidates, we simply
rank candidates in R
i
as first place and candidates
in D
i
as second place.
4.2 Static cache model
Other research on discourse such as Grosz and
Sidner (1986) has studied global focus, which gen-
erally refers to the entity or set of entities that
are salient throughout the entire discourse. Since
global focus may not be captured by Centering-
based models, we also propose another cache
model which directly captures the global salience
of a text.
To train the model, all the candidates in a text
which have an inter-sentential anaphoric relation
with zero-pronouns are used as positive instances
and the others used as negative ones. Unlike the
650
Table 1: Feature set used in the cache models
Feature Description
POS Part-of-speech of C followed by
IPADIC4.
IN QUOTE 1 if C is located in a quoted sentence;
otherwise 0.
BEGINNING 1 if C is located in the beginnig of a text;
otherwise 0.
CASE MARKER Case marker, such as wa (TOPIC) and
ga (SUBJECT), of C.
DEP END 1 if C has a dependency relation with
the last bunsetsu unit (i.e. a basic unit
in Japanese) in a sentence ; otherwise 0.
CONN* The set of connectives intervening be-
tween C and Z. Each conjunction is en-
coded into a binary feature.
IN CACHE* 1 if C is currently stored in the cache;
otherwise 0.
SENT DIST* Distance between C and Z in terms of a
sentence.
CHAIN NUM The number of anaphoric chain, i.e. the
number of antecedents of Z in the situa-
tion that zero-pronouns in the preceding
contexts are completely resolved by the
zero-anaphora resolution model.
C is a candidate antecedent, and Z stands for a target zero-
pronoun. Features marked with an asterisk are only used in
the dynamic cache model.
dynamic cache model, this model does not update
the cache dynamically, but simply selects for each
given zero-pronoun the N most salient candidates
from the preceding sentences according to the rank
provided by the trained ranker. We call this model
the static cache model.
4.3 Features used in the cache models
The feature set used in the cache model is shown
in Table 1. The ?CASE MARKER? feature roughly
captures the salience of the local transition dealt
with in Centering Theory, and is also intended to
capture the global foci of a text coupled with the
BEGINNING feature. The CONN feature is expected
to capture the transitions of a discourse relation be-
cause each connective functions as a marker of a
discourse relation between two adjacent discourse
segments.
In addition, the recency of a candidate an-
tecedent can be even important when an entity oc-
curs as a zero-pronoun in discourse. For example,
when a discourse entity e appearing in sentence s
i
is referred to by a zero-pronoun later in sentence
s
j(i<j)
, entity e is considered salient again at the
point of s
j
. To reflect this way of updating salience,
we overwrite the information about the appearance
position of candidate e in s
j
, which is performed by
the function updateSalienceInfo in Figure 2. This
allows the cache model to handle updated salience
4http://chasen.naist.jp/stable/ipadic/
features such as CHAIN NUM in proceeding cache
updates.
5 Antecedent identification and anaphoric-
ity determination models
As an antecedent identification model, we adopt
the tournament model (Iida et al, 2003) because
in a preliminary experiment it achieved better per-
formance than other state-of-the-art ranking-based
models (Denis and Baldridge, 2008) in this task
setting. To train the tournament model, the training
instances are created by extracting an antecedent
paired with each of the other candidates for learn-
ing a preference of which candidate is more likely
to be an antecedent. At the test phase, the model
conducts a tournament consisting of a series of
matches in which candidate antecedents compete
with one another. Note that in the case of inter-
sentential zero-anaphora resolution the tournament
is arranged between candidates in the cache. For
learning the difference of two candidates in the
cache, training instances are also created by only
extracting candidates from the cache.
For anaphoricity determination, the model has to
judge whether a zero-pronoun is anaphoric or not.
To create the training instances for the binary clas-
sifier, the most likely candidate of each given zero-
pronoun is chosen by the tournament model and
then it is labeled as anaphoric (positive) if the cho-
sen candidate is indeed the antecedent of the zero-
pronoun5, or otherwise labeled as non-anaphoric
(negative).
To create models for antecedent identification
and anaphoricity determination, we use a Support
Vector Machine (Vapnik, 1998)6 with a linear ker-
nel and its default parameters. To use the feature
set shown in Table 2, morpho-syntactic analysis of
a text is performed by the Japanese morpheme ana-
lyzer Chasen and the dependency parser CaboCha.
In the tournament model, the features of two com-
peting candidates are distinguished from each other
by adding the prefix of either ?left? or ?right.?
6 Experiments
We investigate how the cache model contributes
to candidate reduction. More specifically, we ex-
5In the original selection-then-classification model (Iida et
al., 2005), positive instances are created by all the correct pairs
of a zero-pronoun and its antecedent, however in this paper we
use only antecedents selected by the tournament model as the
most likely candidates in the set of candidates because this
method leads to better performance.
6http://svmlight.joachims.org/
651
Table 2: Feature set used in zero-anaphora resolution
Feature Type Feature Description
Lexical HEAD BF Characters of right-most morpheme in NP (PRED).
PRED FUNC Characters of functional words followed by PRED.
Grammatical PRED VOICE 1 if PRED contains auxiliaries such as ?(ra)reru?; otherwise 0.
POS Part-of-speech of NP (PRED) followed by IPADIC (Asahara and Matsumoto, 2003).
PARTICLE Particle followed by NP, such as ?wa (topic)?, ?ga (subject)?, ?o (object)?.
Semantic NE Named entity of NP: PERSON, ORGANIZATION, LOCATION, ARTIFACT, DATE, TIME,
MONEY, PERCENT or N/A.
SELECT PREF The score of selectional preference, which is the mutual information estimated from a
large number of triplets ?Noun, Case, Predicate?.
Positional SENTNUM Distance between NP and PRED.
BEGINNING 1 if NP is located in the beggining of sentence; otherwise 0.
END 1 if NP is located in the end of sentence; otherwise 0.
PRED NP 1 if PRED precedes NP; otherwise 0.
NP PRED 1 if NP precedes PRED; otherwise 0.
Discourse CL RANK A rank of NP in forward looking-center list.
CL ORDER A order of NP in forward looking-center list.
CONN** The connectives intervesing between NP and PRED.
Path PATH FUNC* Characters of functional words in the shortest path in the dependency tree between
PRED and NP.
PATH POS* Part-of-speech of functional words in shortest patn in the dependency tree between
PRED and NP.
NP and PRED stand for a bunsetsu-chunk of a candidate antecedent and a bunsetsu-chunk of a predicate which has a target
zero-pronoun respectively. The features marked with an asterisk are used during intra-sentential zero-anaphora resolution. The
feature marked with two asterisks is used during inter-sentential zero-anaphora resolution.
plore the candidate reduction ratio of each cache
model as well as its coverage, i.e. how of-
ten each cache model retains correct antecedents
(Section 6.2). We also evaluate the performance
of both antecedent identification on inter-sentential
zero-anaphora resolution (Section 6.3) and the
overall zero-anaphora resolution (Section 6.4).
6.1 Data set
In this experiment, we take the ellipsis of nom-
inative arguments of predicates as target zero-
pronouns because they are most frequently omitted
in Japanese, for example, 45.5% of the nominative
arguments of predicates are omitted in the NAIST
Text Corpus (Iida et al, 2007b).
As the data set, we use part of the NAIST Text
Corpus, which is publicly available, consisting of
287 newspaper articles in Japanese. The data set
contains 1,007 intra-sentential zero-pronouns, 699
inter-sentential zero-pronouns and 593 exophoric
zero-pronouns, totalling 2299 zero-pronouns. We
conduct 5-fold cross-validation using this data set.
A development data set consists of 60 articles for
setting parameters of inter-sentential anaphoricity
determination, ?
inter
, on overall zero-anaphora res-
olution. It contains 417 intra-sentential, 298 inter-
sentential and 174 exophoric zero-pronouns.
6.2 Evaluation of the caching mechanism
In this experiment, we directly compare the pro-
posed static and dynamic cache models with the
heuristic methods presented in Section 2. Note that
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.2  0.4  0.6  0.8  1
co
ve
ra
ge
# of classification in antecedent identification process
n=5
n=10
n=15 n=20
n=all
CM
SM (s=1)
SM (s=2)
SM (s=3)
DCM (w/o ZAR)
DCM (with ZAR)
SCM
CM: centering-based cache model, SM: sentence-based cache
model, SCM: static cache model, DCM (w/o ZAR): dynamic
cache model disregarding updateSalienceInfo, DCM (with
ZAR): dynamic cache model using the information of correct
zero-anaphoric relations, n: cache size and s: # of sentences.
Figure 4: Coverage of each cache model
the salience information (i.e. the function update-
SalienceInfo) in the dynamic cache model is disre-
garded in this experiment because its performance
crucially depends on the performance of the zero-
anaphora resolution model. The performance of
the cache model is evaluated by coverage, which
is a percentage of retained antecedents when ap-
pearing zero-pronouns refer to an antecedent in a
preceding sentence, i.e. we evaluate the cases of
inter-sentential anaphora resolution.
As a baseline, we adopt the following two cache
models. One is the Centering-derived model which
only stores the preceding ?wa? (topic)-marked or
652
?ga? (subject)-marked candidate antecedents in the
cache. It is an approximation of the model pro-
posed by Nariyama (2002) for extending the lo-
cal focus transition defined by Centering Theory.
We henceforth call this model the centering-based
cache model. The other baseline model stores can-
didates appearing in the N previous sentences of a
zero-pronoun to simulate a heuristic approach used
in works like Soon et al (2001). We call this model
the sentence-based cache model. By comparing
these baselines with our cache models, we can see
whether our models contribute to more efficiently
storing salient candidates or not.
The above dynamic cache model retains the
salient candidates independently of the results of
antecedent identification conducted in the preced-
ing contexts. However, if the zero-anaphora res-
olution in the current utterance is performed cor-
rectly, it will be available for use as information
about the recency of candidates and the anaphoric
chain of each candidate. Therefore, we also in-
vestigate whether correct zero-anaphora resolution
contributes to the dynamic cache model or not.
To integrate zero-anaphora resolution information,
we create training instances of the dynamic cache
model by updating the recency using the function
?updateSalienceInfo? shown in Figure 2 and also
using an additional feature, CHAIN NUM, defined
in Table 1.
The results are shown in Figure 47. We can
see the effect of the machine learning-based cache
models in comparison to the other two heuristic
models. The results demonstrate that the former
achieves good coverage at each point compared to
the latter. In addition, the difference between the
static and dynamic cache models demonstrates that
the dynamic one is always better then the static. It
may be this way because the dynamic cache model
simultaneously retains global focus of a given text
and the locally salient entities in the current dis-
course.
By comparing the dynamic cache model using
correct zero-anaphora resolution (denoted by DCM
(with ZAR) in Figure 4) and the one without it
(DCM (w/o ZAR)), we can see that correct zero-
anaphora resolution contributes to improving the
caching for every cache size. However, in the
practical setting the current zero-anaphora resolu-
7Expressions such as verbs were rarely annotated as an-
tecedents, so these are not extracted as candidate antecedents
in our current setting. This is the reason why the coverage of
using all the candidates is less than 1.0.
tion system sometimes chooses the wrong candi-
date as an antecedent or does not choose any can-
didate due to wrong anaphoricity determination,
negatively impacting the performance of the cache
model. For this reason, in the following two exper-
iments we decided not to use zero-anaphora reso-
lution in the dynamic cache model.
6.3 Evaluation of inter-sentential zero-
anaphora resolution
We next investigate the impact of the dynamic
cache model shown in Section 4.1 on the an-
tecedent identification task of inter-sentential zero-
anaphora resolution altering the cache size from
5 to the number of all candidates. We compare
the following three cache model within the task
of inter-sentential antecedent identification: the
centering-based cache model, the sentence-based
cache model and the dynamic cache model disre-
garding updateSalienceInfo (i.e. DCM (w/o ZAR)
in Figure 4). We also investigate the computational
time of the process of inter-sentential antecedent
identification with each cache model altering its pa-
rameter 8.
The results are shown in Table 3. From these
results, we can see the antecedent identification
model using the dynamic cache model obtains al-
most the same accuracy for every cache size. It
indicates that if the model can acquire a small num-
ber of the most salient discourse entities in the cur-
rent discourse, the model achieves accuracy com-
parable to the model which searches all the pre-
ceding discourse entities, while drastically reduc-
ing the computational time.
The results also show that the current antecedent
identification model with the dynamic cache model
does not necessarily outperform the model with the
baseline cache models.
For example, the sentence-based cache model
using the preceding two sentences (SM (s=2))
achieved an accuracy comparable to the dynamic
cache model with the cache size 15 (DCM (n=15)),
both spending almost the same computational time.
This is supposed to be due to the limited accu-
racy of the current antecedent identification model.
Since the dynamic cache models provide much bet-
ter search spaces than the baseline models as shown
in Figure 4, there is presumably more room for im-
provement with the dynamic cache models. More
investigations are to be concluded in our future
8All experiments were conducted on a 2.80 GHz Intel
Xeon with 16 Gb of RAM.
653
Table 3: Results on antecedent identification
model accuracy runtime coverage
(Figure 4)
CM 0.441 (308/699) 11m03s 0.651
SM(s=1) 0.381 (266/699) 6m54s 0.524
SM(s=2) 0.448 (313/699) 13m14s 0.720
SM(s=3) 0.466 (326/699) 19m01s 0.794
DCM(n=5) 0.446 (312/699) 4m39s 0.664
DCM(n=10) 0.441 (308/699) 8m56s 0.764
DCM(n=15) 0.442 (309/699) 12m53s 0.858
DCM(n=20) 0.443 (310/699) 16m35s 0.878
DCM(n=1000) 0.452 (316/699) 53m44s 0.928
CM: centering-based cache model, SM: sentence-based cache
model, DCM: dynamic cache model, n: cache size, s: number
of the preceding sentences.
work.
6.4 Overall zero-anaphora resolution
We finally investigate the effects of introducing
the proposed model on overall zero-anaphora res-
olution including intra-sentential cases. The res-
olution is carried out according to the procedure
described in Section 4. By comparing the zero-
anaphora resolution model with different cache
sizes, we can see whether or not the model using
a small number of discourse entities in the cache
achieves performance comparable to the original
one in a practical setting.
For intra-sentential zero-anaphora resolution, we
adopt the model proposed by Iida et al (2007a),
which exploits syntactic patterns as features that
appear in the dependency path of a zero-pronoun
and its candidate antecedent. Note that for sim-
plicity we use bag-of-functional words and their
part-of-speech intervening between a zero-pronoun
and its candidate antecedent as features instead
of learning syntactic patterns with the Bact algo-
rithm (Kudo and Matsumoto, 2004).
We illustrated the recall-precision curve of each
model by altering the threshold parameter of intra-
sentential anaphoricity determination, which is
shown in Figure 5. The results show that all mod-
els achieved almost the same performance when
decreasing the cache size. It indicates that it is
enough to cache a small number of the most salient
candidates in the current zero-anaphora resolution
model, while coverage decreases when the cache
size is smaller as shown in Figure 4.
7 Conclusion
We propose a machine learning-based cache
model in order to reduce the computational cost of
zero-anaphora resolution. We recast discourse sta-
tus updates as ranking problems of discourse en-
tities by adopting the notion of caching originally
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6
pr
ec
isi
on
recall
n=5
n=10
n=15
n=20
n=1000
Figure 5: Recall-precision curves on overall ze-
ro-anaphora resolution
introduced by Walker (1996). More specifically,
we choose the N most salient candidates for each
sentence from the set of candidates appearing in
that sentence and the candidates which are already
in the cache. Using this mechanism, the compu-
tational cost of the zero-anaphora resolution pro-
cess is reduced by searching only the set of salient
candidates. Our empirical evaluation on Japanese
zero-anaphora resolution shows that our learning-
based cache model drastically reduces the search
space while preserving accuracy.
The procedure for zero-anaphora resolution
adopted in our model assumes that resolution is
carried out linearly, i.e. an antecedent is inde-
pendently selected without taking into account any
other zero-pronouns. However, trends in anaphora
resolution have shifted from such linear approaches
to more sophisticated ones which globally opti-
mize the interpretation of all the referring expres-
sions in a text. For example, Poon and Domingos
(2008) has empirically reported that such global
approaches achieve performance better than the
ones based on incrementally processing a text. Be-
cause their work basically builds on inductive logic
programing, we can naturally extend this to incor-
porate our caching mechanism into the global op-
timization by expressing cache constraints as pred-
icate logic, which is one of our next challenges in
this research area.
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proceedings of 33th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 122?129.
M. Asahara and Y. Matsumoto, 2003. IPADIC User Manual.
Nara Institute of Science and Technology, Japan.
B. Baldwin. 1995. CogNIAC: A Discourse Processing En-
gine. Ph.D. thesis, Department of Computer and Informa-
tion Sciences, University of Pennsylvania.
P. Denis and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 660?669.
654
B. J. Grosz and C. L. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Linguistics,
12:175?204.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A
framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?226.
U. Hahn and M. Strube. 1997. Centering in-the-large: com-
puting referential discourse segments. In Proceedings of
the 8th conference on European chapter of the Association
for Computational Linguistics, pages 104?111.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proceedings of the 10th EACL Work-
shop on The Computational Treatment of Anaphora, pages
23?30.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora resolu-
tion by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language In-
formation Processing (TALIP), 4(4):417?434.
R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-anaphora
resolution by learning rich syntactic pattern features. ACM
Transactions on Asian Language Information Processing
(TALIP), 6(4).
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b.
Annotating a japanese text corpus with predicate-argument
and coreference relations. In Proceeding of the ACL Work-
shop ?Linguistic Annotation Workshop?, pages 132?139.
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun res-
olution based on ranking rules and machine learning. In
Proceedings of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 184?191.
T. Joachims. 2002. Optimizing search engines using click-
through data. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD), pages
133?142.
M. Kameyama. 1986. A property-sharing constraint in cen-
tering. In Proceedings of the 24th ACL, pages 200?206.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for
classification of semi-structured text. In Proceedings of the
2004 EMNLP, pages 301?308.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In Proceedings of the IJCAI Workshop on In-
formation Integration on the Web, pages 79?84.
J. F. McCarthy and W. G. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence,
pages 1050?1055.
S. Nariyama. 2002. Grammar for ellipsis resolution in
japanese. In Proceedings of the 9th International Confer-
ence on Theoretical and Methodological Issues in Machine
Translation, pages 135?145.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104?111.
H. Poon and P. Domingos. 2008. Joint unsupervised corefer-
ence resolution with Markov Logic. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 650?659.
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing japanese anaphora integrating zero
pronoun detection and resolution. In Proceedings of the
19th COLING, pages 911?917.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
L. Z. Suri and K. F. McCoy. 1994. Raft/rapr and center-
ing: a comparison and discussion of problems related to
processing complex sentences. Computational Linguistics,
20(2):301?317.
V. N. Vapnik. 1998. Statistical Learning Theory. Adaptive
and Learning Systems for Signal Processing Communica-
tions, and control. John Wiley & Sons.
M. Walker, M. Iida, and S. Cote. 1994. Japanese discourse
and the process of centering. Computational Linguistics,
20(2):193?233.
M. A. Walker. 1996. Limited attention and discourse struc-
ture. Computational Linguistics, 22(2):255?264.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st ACL, pages 176?183.
X. Yang, J. Su, J. Lang, C. L. Tan, T. Liu, and S. Li. 2008.
An entity-mention model for coreference resolution with
inductive logic programming. In Proceedings of ACL-08:
HLT, pages 843?851.
655
Text Simplification for Reading Assistance: A Project Note
Kentaro Inui Atsushi Fujita Tetsuro Takahashi Ryu Iida
Nara Advanced Institute of Science and Technology
Takayama, Ikoma, Nara, 630-0192, Japan
finui,atsush-f,tetsu-ta,ryu-ig@is.aist-nara.ac.jp
Tomoya Iwakura
Fujitsu Laboratories Ltd.
Kamikodanaka, Nakahara, Kawasaki, Kanagawa, 211-8588, Japan
iwakura.tomoya@jp.fujitsu.com
Abstract
This paper describes our ongoing research
project on text simplification for congenitally
deaf people. Text simplification we are aiming
at is the task of offering a deaf reader a syn-
tactic and lexical paraphrase of a given text for
assisting her/him to understand what it means.
In this paper, we discuss the issues we should
address to realize text simplification and re-
port on the present results in three different
aspects of this task: readability assessment,
paraphrase representation and post-transfer er-
ror detection.
1 Introduction
This paper reports on our ongoing research into
text simplification for reading assistance. Potential
users targeted in this research are congenitally deaf
people (more specifically, students at (junior-)high
schools for the deaf), who tend to have difficulties
in reading and writing text. We are aiming at the
development of the technology of text simplification
with which a reading assistance system lexically and
structurally paraphrases a given text into a simpler
and plainer one that is thus more comprehensible.
The idea of using paraphrases for reading as-
sistance is not necessarily novel. For example,
Carroll et al (1998) and Canning and Taito (1999)
report on their project in which they address syn-
tactic transforms aiming at making newspaper text
accessible to aphasics. Following this trend of re-
search, in this project, we address four unexplored
issues as below besides the user- and task-oriented
evaluation of the overall system.
Before going to the detail, we first clarify the four
issues we have addressed in the next section. We
then reported on the present results on three of the
four, readability assessment, paraphrase representa-
tion and post-transfer error detection, in the subse-
quent sections.
2 Research issues and our approach
2.1 Readability assessment
The process of text simplification for reading as-
sistance can be decomposed into the following three
subprocesses:
a. Problem identification: identify which portions of
a given text will be difficult for a given user to
read,
b. Paraphrase generation: generate possible candi-
date paraphrases from the identified portions, and
c. Evaluation: re-assess the resultant texts to choose
the one in which the problems have been resolved.
Given this decomposition, it is clear that one of the
key issues in reading assistance is the problem of as-
sessing the readability or comprehensibility1 of text
because it is involved in subprocesses (a) and (c).
Readability assessment is doubtlessly a tough is-
sue (Williams et al, 2003). In this project, however,
we argue that, if one targets only a particular popu-
lation segment and if an adequate collection of data
is available, then corpus-based empirical approaches
may well be feasible. We have already proven that
one can collect such readability assessment data by
conducting survey questionnaires targeting teachers
at schools for the deaf.
1In this paper, we use the terms readability and comprehen-
sibility interchangeably, while strictly distinguishing them from
legibility of each fragment (typically, a sentence or paragraph)
of a given text.
2.2 Paraphrase acquisition
One of the good findings that we obtained through
the aforementioned surveys is that there are a broad
range of paraphrases that can improve the readabil-
ity of text. A reading assistance system is, therefore,
hoped to be able to generate sufficient varieties of
paraphrases of a given input. To create such a sys-
tem, one needs to feed it with a large collection of
paraphrase patterns. Very timely, the acquisition of
paraphrase patterns has been actively studied in re-
cent years:
 Manual collection of paraphrases in the context of
language generation, e.g. (Robin and McKeown,
1996),
 Derivation of paraphrases through existing lexical
resources, e.g. (Kurohashi et al, 1999),
 Corpus-based statistical methods inspired by the
work on information extraction, e.g. (Jacquemin,
1999; Lin and Pantel, 2001), and
 Alignment-based acquisition of paraphrases from
comparable corpora, e.g. (Barzilay and McKe-
own, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003).
One remaining issue is how effectively these meth-
ods contribute to the generation of paraphrases in our
application-oriented context.
2.3 Paraphrase representation
One of the findings obtained in the previous stud-
ies for paraphrase acquisition is that the automatic
acquisition of candidates of paraphrases is quite re-
alizable for various types of source data but acquired
collections tend to be rather noisy and need manual
cleaning as reported in, for example, (Lin and Pan-
tel, 2001). Given that, it turns out to be important to
devise an effective way of facilitating manual correc-
tion and a standardized scheme for representing and
storing paraphrase patterns as shared resources.
Our approach is (a) to define first a fully express-
ible formalism for representing paraphrases at the
level of tree-to-tree transformation and (b) devise an
additional layer of representation on its top that is de-
signed to facilitate handcoding transformation rules.
2.4 Post-transfer text revision
In paraphrasing, the morpho-syntactic informa-
tion of a source sentence should be accessible
throughout the transfer process since a morpho-
syntactic transformation in itself can often be a mo-
tivation or goal of paraphrasing. Therefore, such
an approach as semantic transfer, where morpho-
syntactic information is highly abstracted away as
in (Dorna et al, 1998; Richardson et al, 2001),
does not suit this task. Provided that the morpho-
syntactic stratum be an optimal level of abstraction
for representing paraphrasing/transfer patterns, one
must recall that semantic-transfer approaches such as
those cited above were motivated mainly by the need
for reducing the complexity of transfer knowledge,
which could be unmanageable in morpho-syntactic
transfer.
Our approach to this problem is to (a) leave the de-
scription of each transfer pattern underspecified and
(b) implement the knowledge about linguistic con-
straints that are independent of a particular trans-
fer pattern separately from the transfer knowledge.
There are a wide range of such transfer-independent
linguistic constraints. Constraints on morpheme
connectivity, verb conjugation, word collocation,
and tense and aspect forms in relative clauses are typ-
ical examples of such constraints.
These four issues can be considered as different
aspects of the overall question how one can make
the development and maintenance of a gigantic re-
source for paraphrasing tractable. (1) The introduc-
tion of readability assessment would free us from
cares about the purposiveness of each paraphrasing
rule in paraphrase acquisition. (2) Paraphrase ac-
quisition is obviously indispensable for scaling up
the resource. (3) A good formalism for representing
paraphrasing rules would facilitate the manual re-
finement and maintenance of them. (4) Post-transfer
error detection and revision would make the system
tolerant to flows in paraphrasing rules.
While many researchers have addressed the issue
of paraphrase acquisition reporting promising results
as cited above, the other three issues have been left
relatively unexplored in spite of their significance in
the above sense. Motivated by this context, in the
rest of this paper, we address these remaining three.
3 Readability assessment
To the best of our knowledge, there have never
been no reports on research to build a computational
model of the language proficiency of deaf people, ex-
cept for the remarkable reports by Michaud and Mc-
Coy (2001). As a subpart of their research aimed at
developing the ICICLE system (McCoy and Master-
man, 1997), a language-tutoring application for deaf
learners of written English, Michaud and McCoy de-
veloped an architecture for modeling the writing pro-
ficiency of a user called SLALOM. SLALOM is de-
signed to capture the stereotypic linear order of ac-
quisition within certain categories of morphological
and/or syntactic features of language. Unfortunately,
the modeling method used in SLALOM cannot be
directly applied to our domain for three reasons.
 Unlike writing tutoring, in reading assistance, tar-
get sentences are in principle unlimited. We
therefore need to take a wider range of morpho-
syntactic features into account.
 SLALOM is not designed to capture the difficulty
of any combination of morpho-syntactic features,
which it is essential to take into account in reading
assistance.
 Given the need to consider feature combinations,
a simple linear order model that is assumed in
SLALOM is unsuitable.
3.1 Our approach: We ask teachers
To overcome these deficiencies, we took yet an-
other approach where we designed a survey ques-
tionnaire targeting teachers at schools for the deaf,
and have been collecting readability assessment data.
In this questionnaire, we ask the teachers to compare
the readability of a given sentence with paraphrases
of it. The use of paraphrases is of critical importance
in our questionnaire since it makes manual readabil-
ity assessment significantly easier and more reliable.
3.1.1 Targets
We targeted teachers of Japanese or English liter-
acy at schools for the deaf for the following reasons.
Ideally, this sort of survey would be carried out
by targeting the population segment in question, i.e.,
deaf students in our study. In fact, pedagogists and
psycholinguists have made tremendous efforts to ex-
amine the language proficiency of deaf students by
giving them proficiency tests. Such efforts are very
important, but they have had difficulty in capturing
enough of the picture to develop a comprehensive
and implementable reading proficiency model of the
population due to the expense of extensive language
proficiency testing.
In contrast, our approach is an attempt to model
the knowledge of experts in this field (i.e., teaching
deaf students). The targeted teachers have not only
rich experiential knowledge about the language pro-
ficiency of their students but are also highly skilled in
paraphrasing to help their students? comprehension.
Since such knowledge gleaned from individual ex-
periences already has some generality, extracting it
through a survey should be less costly and thus more
comprehensive than investigation based on language
proficiency testing.
3.1.2 Questionnaire
In the questionnaire, each question consists of sev-
eral paraphrases, as shown in Figure 1 (a), where
(A) is a source sentence, and (B) and (C) are para-
phrases of (A). Each respondent was asked to as-
sess the relative readability of the paraphrases given
for each source sentence, as shown in Figure 1 (b).
The respondent judged sentence (A) to be the most
difficult and judged (B) and (C) to be comparable.
A judgment that sentence s
i
is easier than sentence
s
j
means that s
i
is judged likely to be understood
by a larger subset of students than s
j
. We asked
the respondents to annotate the paraphrases with
format-free comments, giving the reasons for their
judgments, alternative paraphrases, etc., as shown in
Figure 1 (b).
To make our questionnaire efficient for model ac-
quisition, we had to carefully control the variation in
paraphrases. To do that, we first selected around 50
morpho-syntactic features that are considered influ-
ential in sentence readability for deaf people. For
each of those features, we collected several sim-
ple example sentences from various sources (literacy
textbooks, grammar references, etc.). We then man-
ually produced several paraphrases from each of the
collected sentences so as to remove the feature that
characterized the source sentence from each para-
phrase. For example, in Figure 1, the feature char-
acterizing sentence (A) is a non-restrictive relative
clause (i.e., sentence (A) was selected as an example
of this feature). Neither (B) nor (C) has this feature.
We also controlled the lexical variety to minimize
the effect of lexical factors on readability; we also
restricted the vocabulary to a top-2000 basic word
set (NIJL, 1991).
3.1.3 Administration
We administrated a preliminary survey targeting
three teachers. Through the survey, we observed that
(a) the teachers largely agreed in their assessments of
relative readability, (b) their format-free comments
indicated that the observed differences in readabil-
ity were largely explainable in terms of the morpho-
syntactic features we had prepared, and (c) a larger-
scaled survey was needed to obtain a statistically re-
liable model. Based on these observations, we con-
ducted a more comprehensive survey, in which we
prepared 770 questions and sent questionnaires with
a random set of 240 of them to teachers of Japanese
or English literacy at 50 schools for the deaf. We
Figure 1: Sample question and response
asked them to evaluate as many as possible anony-
mously. We obtained 4080 responses in total (8.0
responses per question).
3.2 Readability ranking model
The task of ranking a set of paraphrases can be de-
composed into comparisons between two elements
combinatorially selected from the set. We consider
the problem of judging which of a given pair of para-
phrase sentences is more readable/comprehensible
for deaf students. More specifically, given para-
phrase pair (s
i
; s
j
), our problem is to classify it into
either left (s
i
is easier), right (s
j
is easier), or com-
parable (s
i
and s
j
are comparable).
Once the problem is formulated this way, we can
use various existing techniques for classifier learn-
ing. So far, we have examined a method of using the
support vector machine (SVM) classification tech-
nique.
A training/testing example is paraphrase pair
(s
i
; s
j
) coupled with its quantified class label
D(s
i
; s
j
) 2 [ 1; 1]. Each sentence s
i
is character-
ized by a binary feature vector F
s
i
, and each pair
(s
i
; s
j
) is characterized by a triple of feature vectors
hF
C
s
i
s
j
; F
L
s
i
s
j
; F
R
s
i
s
j
i, where
 F
C
s
i
s
j
= F
s
i
^ F
s
j
(features shared by s
i
and s
j
),
 F
L
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
i
),
 F
R
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
j
).
D(s
i
; s
j
) represents the difference in readability be-
tween s
i
and s
j
; it is computed in the following way.
1. Let T
s
i
s
j
be the set of respondents who assessed
(s
i
; s
j
).
2. Given the degree of readability respondent t as-
signed to s
i
(s
j
), map it to real value dor(t; s) 2
[0; 1] so that the lowest degree maps to 0 and the
highest degree maps to 1. For example, the de-
gree of readability assigned to (A) in Figure 1 (b)
maps to around 0.1, whereas that assigned to (B)
maps to around 0.9.
3. D(s
i
; s
j
) =
1
jT
s
i
s
j
j
P
t2T
s
i
s
j
dor(t; s
i
)  dor(t; s
j
):
Output score Sc
M
(s
i
; s
j
) 2 [ 1; 1] for input
(s
i
; s
j
) was given by the normalized distance be-
tween (s
i
; s
j
) and the hyperplane.
3.3 Evaluation and discussion
To evaluate the two modeling methods, we con-
ducted a ten-fold cross validation on the set of 4055
paraphrase pairs derived from the 770 questions used
in the survey. To create a feature vector space, we
used 355 morpho-syntactic features. Feature annota-
tion was done semi-automatically with the help of a
morphological analyzer and dependency parser.
The task was to classify a given paraphrase pair
into either left, right, or comparable. Model M ?s
output class for (s
i
; s
j
) was given by
Cls
M
(s
i
; s
j
) =
(
left (Sc
M
(s
i
; s
j
)   
m
)
right (Sc
M
(s
i
; s
j
)  
m
)
comparable (otherwise)
;
where 
m
2 [ 1; 1] is a variable threshold used to
balance precision with recall.
We used the 473 paraphrase pairs that satisfied the
following conditions:
 jD(s
i
; s
j
)j was not less than threshold 
a
(
a
=
0:5). The answer of (s
i
; s
j
) is given by
Cls
Ans
(s
i
; s
j
) =
n
left (D(s
i
; s
j
)   
a
)
right (D(s
i
; s
j
)  
a
) :
 (s
i
; s
j
) must have been assessed by more then one
respondent, i.e., jT
s
i
s
j
j > 1:
 Agreement ratio Agr(s
i
; s
j
) must be suffi-
ciently high, i.e., Agr(s
i
; s
j
)  0:9, where
Agr(s
i
; s
j
) = (for (s
i
; s
j
)   agst(s
i
; s
j
))=
jT
s
i
s
j
j, and for (s
i
; s
j
) and agst(s
i
; s
j
) are the
number of respondents who agreed and disagreed
with Cls
Ans
(s
i
; s
j
), respectively.
We judged output class Cls
M
(s
i
; s
j
) correct if and
only if Cls
M
(s
i
; s
j
) = Cls
Ans
(s
i
; s
j
). The overall
performance was evaluated based on recall Rc and
precision Pr:
Rc =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
Ans
(s
i
;s
j
)2fleft;rightggj
Pr =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
M
(s
i
;s
j
)2fleft;rightgj
.
The model achieved 95% precision with 89% re-
call. This result confirmed that the data we collected
through the questionnaires were reasonably noiseless
and thus generalizable. Furthermore, both models
exhibited a clear trade-off between recall and preci-
sion, indicating that their output scores can be used
as a confidence measure.
4 Paraphrase representation
We represent paraphrases as transfer patterns be-
tween dependency trees. In this section, we propose
a three-layered formalism for representing transfer
patterns.
4.1 Types of paraphrases of concern
There are various levels of paraphrases as the fol-
lowing examples demonstrate:
(1) a. She burst into tears, and he tried to comfort
her.
b. She cried, and he tried to console her.
(2) a. It was a Honda that John sold to Tom.
b. John sold a Honda to Tom.
c. Tom bought a Honda from John.
(3) a. They got married three years ago.
b. They got married in 2000.
Lexical vs. structural paraphrases Example (1)
includes paraphrases of the single word ?comfort?
and the canned phrase ?burst into tears?. The sen-
tences in (2), on the other hand, exhibit structural
and thus more general patterns of paraphrasing. Both
types of paraphrases, lexical and structural para-
phrases, are considered useful for many applications
including reading assistance and thus should be in
the scope our discussion.
Atomic vs. compositional paraphrases The pro-
cess of paraphrasing (2a) into (2c) is compositional
because it can be decomposed into two subpro-
cesses, (2a) to (2b) and (2b) to (2c). In develop-
ing a resource for paraphrasing, we have only to
cover non-compositional (i.e., atomic) paraphrases.
Compositional paraphrases can be handled if an ad-
ditional computational mechanism for combining
atomic paraphrases is devised.
Meaning-preserving vs. reference-preserving
paraphrases It is also useful to distinguish
reference-preserving paraphrases from meaning-
preserving ones. The above example in (3) is of the
reference-preserving type. This types of paraphras-
ing requires the computation of reference to objects
outside discourse and thus should be excluded from
our scope for the present purpose.
4.2 Dependency trees (MDSs)
Previous work on transfer-based machine transla-
tion (MT) suggests that the dependency-based repre-
sentation has the advantage of facilitating syntactic
transforming operations (Meyers et al, 1996; Lavoie
et al, 2000). Following this, we adopt dependency
trees as the internal representations of target texts.
We suppose that a dependency tree consists of a set
of nodes each of which corresponds to a lexeme or
compound and a set of edges each of which repre-
sents the dependency relation between its ends. We
call such a dependency tree a morpheme-based de-
pendency structure (MDS). Each node in an MDS is
supposed to be annotated with an open set of typed
features that indicate morpho-syntactic and semantic
information. We also assume a type hierarchy in de-
pendency relations that consists of an open set of de-
pendency classes including dependency, compound,
parallel, appositive and insertion.
4.3 Three-layered representation
Previous work on transfer-based MT sys-
tems (Lavoie et al, 2000; Dorna et al, 1998)
and alignment-based transfer knowledge acqui-
sition (Meyers et al, 1996; Richardson et al,
2001) have proven that transfer knowledge can be
best represented by declarative structure mapping
(transforming) rules each of which typically consists
of a pair of source and target partial structures as in
the middle of Figure 2.
Adopting such a tree-to-tree style of representa-
tion, however, one has to address the issue of the
trade-off between expressibility and comprehensi-
bility. One may want a formalism of structural
rule editing
translation
compilation
simplified MDS transfer rule
N shika V- nai  ->  V no wa N dake da.
(someone does not V to nothing but N)   (it is only to N that someone does V)
MDS transfer rule
sp_rule(108, negation, RefNode) :-
  match(RefNode, X4=[pos:postp,lex: shika]),
  depend(X3=[pos:verb], empty, X4),
  depend(X1=[pos:aux_verb,lex: nai],
         X2=[pos:aux_verb*], X3),
  depend(X4, empty, X5=[pos:noun]),
  replace(X1, X6=[pos:aux_verb,lex: da]),
  substitute(X5, X12=[pos:noun]),
  move_dtrs(X5, X12),
  substitute(X3, X10=[pos:verb]),
                            :
pos: postp
lex: shika (except)
pos: aux_verb
lex:  da (copula)
pos: postp
lex: wa (TOP)
X6
X11
X12pos: nounlex:  no (thing)
pos: postp
lex: dake (only)
pos: noun
pos: noun
aux_verb*
pos: aux_verb
lex: nai (not)
pos: verbX3
X4
X1
X5
X2 X7
X8
X10 pos: verb
X9 vws
MDS processing operators
(=X5)
(=X2)
(=X3)
Figure 2: Three-layered rule representation
transformation patterns that is powerful enough to
represent a sufficiently broad range of paraphrase
patterns. However, highly expressible formalisms
would make it difficult to create and maintain rules
manually.
To mediate this trade-off, we devised a new layer
of representation to add on the top of the layer of
tree-to-tree pattern representation as illustrated in
Figure 2. At this new layer, we use an extended natu-
ral language to specify transformation patterns. The
language is designed to facilitate the task of hand-
coding transformation rules. For example, to define
the tree-to-tree transformation pattern given in the
middle of Figure 2, a rule editor needs only to spec-
ify its simplified form:
(4) N shika V- nai ! V no ha N dake da.
(Someone does V to nothing but N ! It is only to
N that someone does V)
A rule of this form is then automatically translated
into a fully-specified tree-to-tree transformation rule.
We call a rule of the latter form an MDS rewriting
rule (SR rule), and a rule of the former form a sim-
plified SR rule (SSR rule).
The idea is that most of the specifications of an SR
rule can usually be abbreviated if a means to auto-
matically complement it is provided. We use a parser
and macros to do so; namely, the rule translator com-
plements an SSR rule by macro expansion and pars-
ing to produce the corresponding SR rule specifica-
tions. The advantages of introducing the SSR rule
layer are the following:
 The SSR rule formalism allows a rule writer to
edit rules with an ordinary text editor, which
makes the task of rule editing much more efficient
than providing her/him with a GUI-based com-
plex tool for editing SR rules directly.
 The use of the extended natural language also
has the advantage in improving the readability of
rules for rule writers, which is particularly impor-
tant in group work.
 To parse SSR rules, one can use the same parser
as that used to parse input texts. This also im-
proves the efficiency of rule development because
it significantly reduces the burden of maintaining
the consistency between the POS-tag set used for
parsing input and that used for rule specifications.
The SSR rule layer shares underlying motiva-
tions with the formalism reported by Hermjakob et
al. (2002). Our formalism is, however, considerably
extended so as to be licensed by the expressibility of
the SR rule representation and to be annotated with
various types of rule applicability conditions includ-
ing constraints on arbitrary features of nodes, struc-
tural constraints, logical specifications such as dis-
junction and negation, closures of dependency rela-
tions, optional constituents, etc.
The two layers for paraphrase representation
are fully implemented on our paraphrasing engine
KURA (Takahashi et al, 2001) coupled with another
layer for processing MDSs (the bottom layer illus-
trated in Figure 2). The whole system of KURA
and part of the transer rules implemented on it
(see Section 5 below) are available at http://cl.aist-
nara.ac.jp/lab/kura/doc/.
5 Post-transfer error detection
What kinds of transfer errors tend to occur in lex-
ical and structural paraphrasing? To find it out, we
conducted a preliminary investigation. This section
reports a summary of the results. See (Fujita and
Inui, 2002) for further details.
We implemented over 28,000 transfer rules for
Japanese paraphrases on the KURA paraphrasing en-
gine based on the rules previously reported in (Sato,
1999; Kondo et al, 1999; Kondo et al, 2001; Iida et
al., 2001) and existing lexical resources such as the-
sauri and case frame dictionaries. The implemented
rules ranged from such lexical paraphrases as those
that replace a word with its synonym to such syn-
tactic/structural paraphrases as those that remove a
cleft construction from a sentence, devide a sentence,
etc. We then fed KURA with a set of 1,220 sentences
randomly sampled from newspaper articles and ob-
tained 630 transferred output sentences.
The following are the tendencies we observed:
 The transfer errors observed in the experiment ex-
hibited a wide range of variety from morphologi-
cal errors to semantic and discourse-related ones.
 Most types of errors tended to occur regardless
of the types of transfer. This suggests that if one
creates an error detection module specialized for
a particular error type, it works across different
types of transfer.
 The most frequent error type involved inappropri-
ate conjugation forms of verbs. It is, however,
a matter of morphological generation and can be
easily resolved.
 Errors in regard to verb valency and selectional
restriction also tended to be frequent and fatal,
and thus should have preference as a research
topic.
 The next frequent error type was related to the
difference of meaning between near synonyms.
However, this type of errors could often be de-
tected by a model that could detect errors of verb
valency and selectional restriction.
Based on these observations, we concluded that
the detection of incorrect verb valences and verb-
complement cooccurrence was one of the most se-
rious problems that should have preference as a re-
search topic. We are now conducting experiments
on empirical methods for detecting this type of er-
rors (Fujita et al, 2003).
6 Conclusion
This paper reported on the present results of our
ongoing research on text simplification for reading
assistance targeting congenitally deaf people. We
raised four interrelated issues that we needed address
to realize this application and presented our previ-
ous activities focuing on three of them: readabil-
ity assessment, paraphrase representation and post-
transfer error detection.
Regarding readability assessment, we proposed a
novel approach in which we conducted questionnaire
surveys to collect readability assessment data and
took a corpus-based empirical method to obtain a
readability ranking model. The results of the sur-
veys show the potential impact of text simplification
on reading assistance. We conducted experiments on
the task of comparing the readability of a given para-
phrase pair and obtained promising results by SVM-
based classifier induction (95% precision with 89%
recall). Our approach should be equally applicable
to other population segments such as aphasic read-
ers and second-language learners. Our next steps
includes the investigation of the drawbacks of the
present bag-of-features modeling approach. We also
need to consider a method to introduce the notion
of user classes (e.g. beginner, intermediate and ad-
vanced). Textual aspects of readability will also need
to be considered, as discussed in (Inui and Nogami,
2001; Siddahrthan, 2003).
Regarding paraphrase representation, we pre-
sented our revision-based lexico-structural para-
phrasing engine. It provides a fully expressible
scheme for representating paraphrases, while pre-
serving the easiness of handcraft paraphrasing rules
by providing an extended natural language as a
means of pattern editting. We have handcrafted over
a thousand transfer rules that implement a broad
range of lexical and structural paraphrasing.
The problem of error detection is also critical.
When we find a effective solution to it, we will be
ready to integrate the technologies into an applica-
tion system of text simplification and conduct user-
and task-oriented evaluations.
Acknowledgments
The research presented in this paper was partly
funded by PREST, Japan Science and Technology
Corporation. We thank all the teachers at the schools
for the deaf who cooperated in our questionnaire sur-
vey and Toshihiro Agatsuma (Joetsu University of
Education) for his generous and valuable coopera-
tion in the survey. We also thank Yuji Matsumoto
and his colleagues (Nara Advanced Institute of Sci-
ence and Technology) for allowing us to use their
NLP tools ChaSen and CaboCha, Taku Kudo (Nara
Advanced Institute of Science and Technology) for
allowing us to use his SVM tool, and Takaki Makino
and his colleagues (Tokyo University) for allow-
ing us to use LiLFeS, with which we implemented
KURA. We also thank the anonymous reviewers for
their suggestive and encouraging comments.
References
Barzilay, R. and McKeown, K. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 50?57.
Barzilay, R. and Lee, L. 2003. Learning to paraphrases: an
unsupervised approach using multiple-sequence align-
ment. In Proc. of HLT-NAACL.
Canning, Y. and Taito, J. 1999. Syntactic simplification of
newspaper text for aphasic readers. In Proc. of the 22nd
Annual International ACM SIGIR Conference (SIGIR).
Carroll, J., Minnen, G., Canning, Y., Devlin, S. and Tait, J.
1998. Practical simplification of English newspaper
text to assist aphasic readers. In Proc. of AAAI-98
Workshop on Integrating Artificial Intelligence and As-
sistive Technology.
Dorna, M., Frank, A., Genabith, J. and Emele, M. 1998.
Syntactic and semantic transfer with F-structures. In
Proc. of COLING-ACL, pages 341?347.
Fujita, A. and Inui, K. 2002. Decomposing linguistic
knowledge for lexical paraphrasing. In Information
Processing Society of Japan SIG Technical Reports,
NL-149, pages 31?38. (in Japanese)
Fujita, A., Inui, K. and Matsumoto, Y. 2003. Automatic
detection of verb valency errors in paraphrasing. In In-
formation Processing Society of Japan SIG Technical
Reports, NL-156. (in Japanese)
Hermjakob, U., Echihabi, A. and Marcu, D. 2002. Nat-
ural language based reformulation resource and Web
exploitation for question answering. In Proc. of the
TREC-2002 Conference.
Iida, R., Tokunaga, Y., Inui, K. and Eto, J. 2001. Explo-
ration of clause-structural and function-expressional
paraphrasing using KURA. In Proc. of the 63th Annual
Meeting of Information Processing Society of Japan,
pages 5?6. (in Japanese).
Inui, K. and Nogami, M. 2001. A paraphrase-based explo-
ration of cohesiveness criteria. In Proc. of the Eighth
European Workshop on Natulan Language Generation,
pages 101?110.
Jacquemin, C. 1999. Syntagmatic and paradigmatic rep-
resentations of term variations. In Proc. of the 37th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 341?349.
Kondo, K., Sato, S. and Okumura, M. 1999. Paraphras-
ing of ?sahen-noun + suru?. Journal of Information
Processing Society of Japan, 40(11):4064?4074. (in
Japanese).
Kondo, K., Sato, S. and Okumura, M. 2001. Para-
phrasing by case alternation. Journal of Informa-
tion Processing Society of Japan, 42(3):465?477. (in
Japanese).
Kurohashi, S. and Sakai, Y. 1999. Semantic analysis of
Japanese noun phrases: a new approach to dictionary-
based understanding. In Proc. of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 481?488.
Lavoie, B. Kittredge, R. Korelsky, T. Rambow, O. 2000.
A framework for MT and multilingual NLG ystems
based on uniform lexico-structural processing. In Proc.
of ANLP-NAACL.
Lin, D. and Pantel, P. 2001. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
McCoy ,K. F. and Masterman (Michaud), L. N. 1997. A
Tutor for Teaching English as a Second Language for
Deaf Users of American Sign Language, In Proc. of
ACL/EACL ?97 Workshop on Natural Language Pro-
cessing for Communication Aids.
Meyers, A., Yangarber, R. and Grishman, R. 1996. Align-
ment of shared forests for bilingual corpora. In Proc.
of the 16th International Conference on Computational
Linguistics (COLING), pages 460?465.
Michaud, L. N. and McCoy, K. F. 2001. Error profiling:
toward a model of English acquisition for deaf learn-
ers. In Proc. of the 39th Annual Meeting and the 10th
Conference of the European Chapter of Association for
Computational Linguistics (EACL), pages 386?393.
NIJL, the National Institute for Japanese Language. 1991.
Nihongo Kyo?iku-no tame-no Kihon-Goi Cho?sa (The
basic lexicon for the education of Japanese). Shuei
Shuppan, Japan. (In Japanese)
Richardson, S., Dolan, W., Menezes, A. and Corston-
Oliver, M. 2001. Overcoming the customization bottle-
neck using example-based MT. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 9?16.
Robin, J. and McKeown, K. 1996. Empirically designing
and evaluating a new revision-based model for sum-
mary generation. Artificial Intelligence, 85(1?2):135?
179.
Sato, S. 1999. Automatic paraphrase of technical pa-
pers? titles. Journal of Information Processing Society
of Japan, 40(7):2937?2945. (in Japanese).
Shinyama, Y., Sekine, S. Kiyoshi, Sudo. and Grishman,
R. 2002. Automatic paraphrase acquisition from news
articles. In Proc. of HLT, pages 40?46.
Siddahrthan, A. 2003. Preserving discourse structure
when simplifying text. In Proc. of European Workshop
on Natural Language Generation, pages 103?110.
Takahashi, T., Iwakura, T., Iida, R., Fujita, A. and Inui, K.
2001. KURA: a transfer-based lexico-structural para-
phrasing engine. In Proc. of the 6th Natural Language
Processing Pacific Rim Symposium (NLPRS) Workshop
on Automatic Paraphrasing: Theories and Applica-
tions, pages 37?46.
Williams, S., Reiter, E. and Osman, L. 2003. Experiments
with discourse-level choices and readability. In Proc. of
European Workshop on Natural Language Generation,
pages 127?134.
Paraphrasing of Japanese Light-verb Constructions
Based on Lexical Conceptual Structure
Atsushi Fujita? Kentaro Furihata? Kentaro Inui? Yuji Matsumoto? Koichi Takeuchi?
?Graduate School of Information Science,
Nara Institute of Science and Technology
{atsush-f,kenta-f,inui,matsu}@is.naist.jp
?Department of Information Technology,
Okayama University
koichi@it.okayama-u.ac.jp
Abstract
Some particular classes of lexical paraphrases such
as verb alteration and compound noun decomposi-
tion can be handled by a handful of general rules
and lexical semantic knowledge. In this paper, we
attempt to capture the regularity underlying these
classes of paraphrases, focusing on the paraphras-
ing of Japanese light-verb constructions (LVCs).
We propose a paraphrasing model for LVCs that
is based on transforming the Lexical Conceptual
Structures (LCSs) of verbal elements. We also pro-
pose a refinement of an existing LCS dictionary. Ex-
perimental results show that our LCS-based para-
phrasing model characterizes some of the semantic
features of those verbs required for generating para-
phrases, such as the direction of an action and the
relationship between arguments and surface cases.
1 Introduction
Automatic paraphrase generation technology offers
the potential to bridge gaps between the authors and
readers of documents. For example, a system that
is capable of simplifying a given text, or showing
the user several alternative expressions conveying
the same content, would be useful for assisting a
reader (Carroll et al, 1999; Inui et al, 2003).
In Japanese, like other languages, there are sev-
eral classes of paraphrasing that exhibit a degree
of regularity that allows them to be explained by
a handful of sophisticated general rules and lexical
semantic knowledge. For example, paraphrases as-
sociated with voice alteration, verb/case alteration,
compounds, and lexical derivations all fall into such
classes. In this paper, we focus our discussion on
another useful class of paraphrases, namely, the
paraphrasing of light-verb constructions (LVCs),
and propose a computational model for generating
paraphrases of this class.
Sentence (1s) is an example of an LVC1. An LVC
is a verb phrase (?kandou-o ataeta (made an impres-
sion)? in (1s)) that consists of a light-verb (?ataeta
(give-PAST)?) that grammatically governs a nomi-
1For each example, s denotes an input and t denotes its para-
phrase.
nalized verb (?kandou (an impression)?) (also see
Figure 1 in Section 2.2). A paraphrase of (1s) is sen-
tence (1t), in which the nominalized verb functions
as the main verb with its verbal form (?kandou-s-as
e-ta (be impressed-CAU, PAST)?).
(1) s. Eiga-ga kare-ni kandou-o ataeta.
film-NOM him-DAT impression-ACC give-PAST
The film made an impression on him.
t. Eiga-ga kare-o kandou-s-ase-ta.
film-NOM him-ACC be impressed-CAUSATIVE, PAST
The film impressed him.
To generate this type of paraphrase, we need a com-
putational model that is capable of the following
two classes of choice (also see Section 2.2):
Selection of the voice: The model needs to be able
to choose the voice of the target sentence from
active, passive, causative, etc. In example (1),
the causative voice is chosen, which is indi-
cated by the auxiliary verb ?ase (causative)?.
Reassignment of the cases: The model needs to
be able to reassign a case marker to each ar-
gument of the main verb. In (1), the gram-
matical case of ?kare (him),? which was orig-
inally assigned the dative case, is changed to
accusative.
The task is not as simple as it may seem, because
both decisions depend not only on the syntactic and
semantic attributes of the light-verb, but also on
those of the nominalized verb (Muraki, 1991).
In this paper, we propose a novel lexical
semantics-based account of the LVC paraphrasing,
which uses the theory of Lexical Conceptual Struc-
ture (LCS) of Japanese verbs (Kageyama, 1996;
Takeuchi et al, 2001). The theory of LCS offers
an advantage as the basis of lexical resources for
paraphrasing, because it has been developed to ex-
plain varieties of linguistic phenomena including
lexical derivations, the construction of compounds,
and verb alteration (Levin, 1993; Dorr et al, 1995;
Kageyama, 1996; Takeuchi et al, 2001), all of
which are associated with the systematic paraphras-
ing we mentioned above.
The paraphrasing associated with LVCs is not id-
iosyncratic to Japanese but also appears commonly
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 9-16
in other languages such as English (Mel?c?uk and
Polgue`re, 1987; Iordanskaja et al, 1991; Dras,
1999, etc.), as indicated by the following examples.
(2) s. Steven made an attempt to stop playing.
t. Steven attempted to stop playing.
(3) s. It had a noticeable effect on the trade.
t. It noticeably affected the trade.
Our approach raises the interesting issue of whether
the paraphrasing of LVCs can be modeled in an
analogous way across languages.
Our aim in this paper are: (i) exploring the reg-
ularity of the LVC paraphrasing based a lexical
semantics-based account, and (ii) assessing the im-
mature Japanese semantic typology through a prac-
tical task.
The following sections describe our motiva-
tion, target, and related work on LVC paraphras-
ing (Section 2), the basics of LCS and the refine-
ments we made (Section 3), our paraphrasing model
(Section 4), and our experiments (Section 5). Fi-
nally, we conclude this paper with a brief of descrip-
tion of work to be done in the future (Section 6).
2 Motivation, target, and related work
2.1 Motivation
One of the critical issues that we face in para-
phrase generation is how to develop and maintain
knowledge resources that covers a sufficiently wide
range of paraphrasing patterns such as those in-
dicating that ?to make an attempt? can be para-
phrased into ?to attempt,? and that ?potential? can
be paraphrased into ?possibility.? Several attempts
have been made to develop such resources manually
(Sato, 1999; Dras, 1999; Inui and Nogami, 2001);
those work have, however, tended to restrict their
scope to specific classes of paraphrases, and cannot
be used to construct a sufficiently comprehensive re-
source for practical applications.
There is another trend in the research in this field,
namely, the automatic acquisition of paraphrase pat-
terns from parallel or comparable corpora (Barzilay
and McKeown, 2001; Lin and Pantel, 2001; Pang et
al., 2003; Shinyama and Sekine, 2003, etc.). This
type of approach may be able to reduce the cost
of resource development. There are problems that
must be overcome, however, before they can work
practically. First, automatically acquired patterns
tend to be complex. For example, from the para-
phrase of (4s) into (4t), we can naively obtain the
pattern: ?X is purchased by Y ? Y buys X.?
(4) s. This car was purchased by him.
t. He bought this car.
This could also, however, be regarded as a combi-
nation of a simpler pattern of lexical paraphrasing
(?purchase ? buy?) and a voice activization (?X
Adjective
Noun + Case particle"no"(GEN)
Nominalized verb + Case particle
Noun + Case Particle
Adverb
Light-verb (+suffixes)
Embedded clause
LVC
Target of this paper
(a)
(b)
(c)
(d)
(e)
Figure 1: Dependency structure showing the range
which the LVC paraphrasing affects.
be VERB-PP by Y ? Y VERB X?). If we were to
use an acquisition scheme that is not capable of de-
composing such complex paraphrases correctly, we
would have to collect a combinatorial number of
paraphrases to gain the required coverage. Second,
the results of automatic acquisition would likely in-
clude many inappropriate patterns, which would re-
quire manual correction. Manual correction, how-
ever, would be impractical if we were collecting a
combinatorial number of patterns.
Our approach to this dilemma is as follows: first,
we manually develop the resources needed to cover
those paraphrases that appear regularly, and then de-
compose and automatically refine the acquired para-
phrasing patterns using those resources. The work
reported in this paper is aimed at this resource de-
velopment.
2.2 Target structure and required operations
Figure 1 shows the range which the LVC para-
phrasing affects, where the solid boxes denote
Japanese base-chunk so-called ?bunsetsu.?2 Being
involved in the paraphrasing, the modifiers of the
LVC need the following operations:
Change of the dependence: The dependences of
the elements (a) and (b) need to be changed
because the original modifiee, the light-verb,
is eliminated by the paraphrasing.
Re-conjugation: The conjugation form of the ele-
ments (d), (e), and occasionally (c) need to be
changed according to the category change of
their modifiee, the nominalized verb.
Reassignment of the cases: As described in the
previous section, the case markers of the ele-
ments (b) and often (c) need to be reassigned.
Selection of the voice: The voice of the nominal-
ized verb needs to be chosen according to the
combination of the nominalized verb, the light-
verb, and the original voice.
The first two operations are trivial in the field of
text generation. Moreover, they can be done inde-
pendently of the LVC paraphrasing. The most deli-
cate operation is for the element (c) because it acts
either as an adverb or as a case, relying on the con-
2The modifiee of the LVC is not affected because the part-
of-speech of the light-verb and main verb are the same.
Table 1: Examples of LCS
Verb LCS for verb Verb phrase
move [y MOVE TO z] My sister (Theme) moves to a neighboring town (Goal).
transmit [x CONTROL [y MOVE TO z]] The enzyme (Agent) transmits messages (Theme) to the muscles (Goal).
locate [y BE AT z] The school (Theme) locates near the river (Goal).
maintain [x CONTROL [y BE AT z]] He (Agent) maintains a machine (Theme) in good condition (Goal).
text. In the former case, it needs the second opera-
tion. In the latter case, it needs the third operation
as well as the element (b).
In this paper, we take into account only the ele-
ment (b), namely, the sibling cases of the nominal-
ized verb.
2.3 Related work
Based on the Meaning-Text Theory (Mel?c?uk and
Polgue`re, 1987), Iordanskaja et al (1991) pro-
poses a set of paraphrasing rules including one for
LVC paraphrasing. Their rule heavily relies on what
are called lexical functions, by which they virtually
specify all the choices relevant to LVC paraphras-
ing for every combination of nominalized verb and
light-verb individually. Our approach is to employ
lexical semantics to provide a general account of
those classes of choices.
On the other hand, Kaji and Kurohashi (2004)
proposes a paraphrasing model which bases on an
ordinary dictionary. Given an input LVC, their
model paraphrases it using the gloss of both the
nominalized verb and the light-verb with the seman-
tic feature of the light-verb. Their model looks ro-
bust because of the availability of an ordinary dic-
tionary. However, their model fails to explain the
difference in the voice selection between examples
(5) and (6) since it selects the voice based only
on the light-verb ? in their approach, the light-
verb ?ukeru (to receive)? always maps to the passive
voice irrespective of the nominalized verb.
(5) s. Enkai-eno shoutai-o uketa.
party-GEN invitation-ACC receive-PAST
I received an invitation to the party.
t. Enkai-ni shoutai-s-are-ta.
party-DAT invite-PAS, PAST
I was invited to the party.
(6) s. Kare-no hanashi-ni
his-GEN talk-DAT
kandou-o uketa.
impression-ACC receive-PAST
I was given a good impression by his talk.
t. Kare-no hanashi-ni kandou-shi-ta.
his-GEN talk-DAT be impressed-ACT, PAST
I was impressed by his talk.
In (Kaji and Kurohashi, 2004), the target expres-
sion is restricted only to the LVC itself (also see
Figure 1). Hence, their model is unable to reassign
the cases as we saw in example (1).
3 Lexical Conceptual Structure
3.1 Basic framework of LCS
The theory of Lexical Conceptual Structure
(LCS) associates a verb with a semantic struc-
ture as exemplified by Table 1. An LCS consists
of semantic predicates (?CONTROL,? ?BE AT,?
etc.) and their argument slots (x, y, z). Argument
slots x, y, and z correspond to the semantic roles
?Agent,? ?Theme,? and ?Goal,? respectively. Tak-
ing the LCS of the verb ?transmit? as an example,
[y MOVE TO z] denotes the state of affairs that the
state of the ?Theme? changes to the ?Goal,? and
[x CONTROL . . .] denotes that the ?Agent? causes the
state change.
3.2 Refinements
We make use of the TLCS dictionary, a Japanese
verb LCS dictionary developed by Takeuchi et al
(2001), because it offers the following advantages:
? It is based on solid linguistic work, as in
(Kageyama, 1996).
? Its scale is considerably larger than any other
existing collections of verb LCS entries.
? It provides a set of concrete rules for LCS as-
signment, which ensures the reliability of the
dictionary.
In spite of these advantages, our preliminary ex-
amination of the dictionary revealed that further re-
finements were needed. To refine the typology of
TLCS, we collected the following sets of words:
Nominalized verbs: We regard ?sahen-nouns?4
and nominal forms of verbs as nominalized
verbs. We retrieved 1,210 nominalized verbs
from the TLCS dictionary.
Light-verbs: Since a verb takes different meanings
when it is a part of LVCs with different case
particles, we collected pairs ?c, v? of case par-
ticle c and verb v in the following way:
Step 1. We collected 876,101 types of triplets
?n, c, v? of nominalized verb n, case par-
ticle c, and base form of verb v from the
parsed5 sentences of newspaper articles6.
4A sahen-noun is a verbal noun in Japanese, which acts as
a verb in the form of ?sahen-noun + suru?.
5We used the statistical Japanese dependency parser
CaboCha (Kudo and Matsumoto, 2002) for parsing.
http://chasen.naist.jp/?taku/software/cabocha/
6Excerpts from 9 years of the Mainichi Shinbun and 10
years of the Nihon Keizai Shinbun, giving a total of 25,061,504
sentences, were used.
Table 2: Extensions of LCS
Verb Verb phrase and its LCS representation
Ext.1 hankou-suru [[Ken]y BE AGAINST [parents]z]
(resist) Ken-ga oya-ni hankou-suru.
Ken-NOM parents-DAT resist-PRES (Ken resists his parents.)
Ext.2 ukeru [BECOME [[salesclerk]z BE WITH [[complaint]y MOVE FROM [customer]x TO [salesclerk]z]]]
(receive) Ten?in-ga kyaku-kara kujo-o ukeru.
salesclerk-NOM customer-ABL complaint-ACC receive-PRES
(The salesclerk receives a complaint from a customer.)
Ext.3 motomeru [[Ken]x CONTROL [[apology]y MOVE FROM [George]z TO [FILLED]]]3
(ask) Ken-ga George-ni shazai-o motomeru.
Ken-NOM George-DAT apology-ACC ask-PRES (Ken asks George for an apology.)
Ext.4 kandou-suru [BECOME [[Ken]z BE WITH [[FILLED]y MOVE FROM [music]x TO [Ken]z]]]
(be impressed) Ken-ga ongaku-ni kandou-suru.
Ken-NOM music-DAT be impressed-PRES (Ken is impressed by the music.)
Step 2. For each of the 50 most frequent ?c, v?
tuples, we extracted the 10 most frequent
?n, c, v?.
Step 3. Each ?n, c, v? was manually evaluated
to determine whether it was an LVC. If
any of 10 triplets was determined to be
an LVC, ?c, v? was merged into the list of
light-verbs. As a result, we collected 40
types of ?c, v? for light-verbs.
Through investigating the above 1,210 nominal-
ized verbs and 40 light-verbs, we extended the ty-
pology of TLCS as shown below (also see Table 2).
Ext. 1. Treatment of ?Partner?: The dative case
of ?hankou-suru (resist)? and ?eikyo-suru (af-
fect)? does not indicate the ?Goal? of the ac-
tion but the ?Partner.?
Ext. 2. Verbs of obtaining (Levin, 1993): In con-
trast with ?ataeru (give),? the nominative case
of ?ukeru (receive)? and ?eru (acquire)? is the
?Goal? of the ?Theme,? while the ablative case
indicates ?Source.?
Ext. 3. Require verb: ?motomeru (ask)? and
?yokyu-suru (require)? denote the existence of
the external ?Agent? who controls the action of
the other ?Agent? or ?Theme.?
Ext. 4. Verbs of psychological state (Levin,
1993): ?kandou-suru (be impressed)? and ?os-
oreru (fear)? indicate the change of psycholog-
ical state of the ?Agent.? The ascriptive part of
the change has to be described.
Consequently, we defined a new LCS typology
consisting of 16 types. Note that more than one LCS
can be assigned to a verb if it has a polysemy. For
convenience, we refer to the extended dictionary as
the LCSdic7.
6The predicate ?FILLED? represents an implicit argument
of the verb and the verb assigned this LCS cannot take this
argument. Taking the LCS of the verb ?sign? as an example,
?FILLED? in [x CONTROL [BECOME [[FILLED]y BE AT
z]]] denotes the name of ?Agent.?
7The latest version of the LCSdic is available from
http://cl.it.okayama-u.ac.jp/rsc/lcs/
4 Paraphrasing model
In this section, we describe how we generate para-
phrases of LVCs. Figure 2 illustrates how our model
paraphrases the LVC of example (7).
(7) s. Ken-ga eiga-ni shigeki-o uketa.
Ken-NOM film-DAT inspiration-ACC receive-PAST
Ken received inspiration from the film.
t. Ken-ga eiga-ni shigeki-s-are-ta.
Ken-NOM film-DAT inspire-PAS, PAST
Ken was inspired by the film.
The idea is to exploit the LCS representation as a
semantic representation and to model the LVC para-
phrasing by the transformation of the LCS represen-
tation. The process consists of the following three
steps:
Step 1. Semantic analysis: The model first ana-
lyzes a given input sentence including an LVC
to obtain its semantic structure in terms of the
LCS representation. In Figure 2, this step pro-
duces LCS
V 1
.
Step 2. Semantic transformation (LCS transfor-
mation): The model then transfers the ob-
tained semantic structure to another semantic
structure so that the target structure consists of
the LCS of the nominalized verb of the input.
In our example, this step generates LCS
N1
to-
gether with the supplement [BECOME [. . .]].
Step 3. Surface generation: Having obtained the
target LCS representation, the model finally
lexicalizes it to generate the output sentence.
So, the key issue is how to control the second step,
namely, the transformation of the LCS representa-
tion.
The rest of this section elaborates on each step,
using different symbols to denote arguments; x, y,
and z for LCS
V
, and x?, y?, and z? for LCS
N
.
4.1 Semantic analysis
Given an input sentence, which we assume to be a
simple clause with an LVC, we first look up the LCS
template LCS
V 0
for the given light-verb, and then
apply the case assignment rule, below (Takeuchi et
(2) LCS transformation
(3) Surface generation
LCS dictionary
[x? ACT ON y?]
[BECOME [z BE WITH [y MOVE FROM x TO z]]]
Paraphrased sentence
Input sentence
ukeru (receive)
shigeki-suru (inspire)
Ken-ga (Ken-NOM) eiga-ni (film-DAT)
shigeki-o (inspiration-ACC) uketa (receive-PAST).
[[film]x? ACT ON [Ken]y?]
[BECOME [[Ken]z BE WITH
    [[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[BECOME [[Ken]z BE WITH]] +
LCSV0
LCSV1
LCSN0
LCSN1
Ken-ga (Ken-NOM) eiga-ni (film-DAT)
               shigeki-s-are-ta (inspire-PAS, PAST).
(1) Semantic analysis
Figure 2: The LCS-based paraphrasing model.
al., 2001), to obtain its LCS representation LCS
V 1
:
Case assignment rule:
? In the case of the LCS
V 0
having argument x,
fill the leftmost argument of the LCS
V 0
with
the nominative case of the input, the second
leftmost with the accusative, and the rest with
the dative.
? Otherwise, fill arguments y and z of the LCS
V 0
with the nominative and the dative, respec-
tively.
In the example shown in Figure 2, the nominative
?Ken? fills the leftmost argument z. Accordingly,
the accusative ?shigeki (inspiration)? and the dative
?eiga (film)? fill y and x, respectively.
(8) s. Ken-ga eiga-ni shigeki-o uketa.
Ken-NOM film-DAT inspiration-ACC receive-PAST
Ken received inspiration from the film.
LCS
V 0
[BECOME [z BE WITH [y MOVE FROM x
TO z]]]
LCS
V 1
[BECOME [[Ken]z BE WITH [[inspiration]y
MOVE FROM [film]x TO [Ken]z]]]
4.2 LCS transformation
The second step of our paraphrasing model
matches the resultant LCS representation (LCS
V 1
in Figure 2) with the LCS of the nominalized verb
(LCS
N0
) to generate the target LCS representation
(LCS
N1
). Figure 3 shows a more detailed view of
this process for the example shown in Figure 2.
4.2.1 Predicate matching
The first step is to determine the predicate in
LCS
V 1
that should be matched with the predicate
in LCS
N0
. Assuming that only the agentivity is rel-
evant to the selection of the voice in the paraphras-
ing of LVC, which is our primary concern, we clas-
sify the semantic predicates into the following two
classes:
Agentive predicates: ?CONTROL,? ?ACT ON,?
?ACT,? ?BE AGAINST,? and ?MOVE FROM
TO.?
[[film]x? ACT ON [Ken]y?][BECOME [[Ken]z BE WITH]] +
[BECOME [[Ken]z BE WITH
    [[inspiration]y MOVE FROM [film]x TO [Ken]z]]]
[x? ACT ON y?]
(b) Argument matching(a) Predicate matching
(c) Attaching the remaining structure
LCSN0
LCSN1
LCSV1
Figure 3: LCS transformation.
State of affair predicates: ?MOVE TO,? ?BE
AT,? and ?BE WITH.?
Aspectual predicates: ?BECOME.?
We also assume that any pair of predicates of
the same class is allowed to match, and that the
aspectual predicates are ignored. In our example,
?MOVE FROM TO? matches ?ACT ON,? as shown
in Figure 3.
LCS representations have right-branching (or
right-embedding) structures. Since inner-embedded
predicates denote the state of affairs, they take pri-
ority in the matching. In other words, the matching
proceeds from the rightmost inner predicates to the
outer predicates.
Having matched the predicates, we then fill each
argument slot in LCS
N0
with its corresponding ar-
gument in LCS
V 1
. In Figure 3, argument z is
matched with y?, and x with x?. As a result, ?Ken?
comes to the y? slot and ?eiga (film)? comes to the
x? slot8.
This process is repeated until the leftmost predi-
cate in LCS
N0
or that in LCS
V 1
is matched.
4.2.2 Treatment of non-transfered predicates
If LCS
V 1
has any non-transfered predicates when
the predicate matching has been completed, they
represent the semantic content that is not covered by
LCS
N1
and which needs to be lexicalized by aux-
iliary linguistic devices such as voice auxiliaries.
In the case of Figure 3, [BECOME [[Ken]z BE WITH]]
in LCS
V 1
remains non-transfered. In such a case,
we attach the non-transfered predicates to LCS
N0
,
which are then lexicalized by auxiliaries in the next
step, the surface generation.
4.3 Surface generation
We again apply the aforementioned case assignment
rule to generate a sentence from the resultant LCS
representation. In this process, the model makes the
final decisions on the selection of the voice and the
reassignment of the cases, according to the follow-
ing decision list:
8When an argument is filled with another LCS, arguments
within the inner LCS are also matched. Likewise, with regard
to an assumption that the input sentences are periphrastic, we
introduced some exceptional rules. That is, arguments filled
with the implicit filler represented by ?FILLED? or the target
nominalized verb N are never matched, and ?Goal? in LCS
V 1
can be matched to ?Theme? in LCS
N0
.
1. If the attached predicate is filled with the same
argument as the leftmost argument in LCS
N1
,
the ?active? voice is selected and the case
structure is left as is.
2. If the argument of the attached predicate has
the same value as either z? or y? in LCS
N1
,
lexicalization is performed to make the argu-
ment a subject. Therefore, the ?passive? voice
is selected and case alternation (passivization)
is applied.
3. If the attached predicate is ?BE WITH? and its
argument has the same value as x? in LCS
N1
,
the ?causative? voice is selected and case alter-
nation (causativizaton) is applied.
4. If the attached predicate is an agentive predi-
cate, and its argument is filled with a value dif-
ferent from those of the other arguments, then
the ?causative? voice is selected and case alter-
nation (causativization) is applied.
5. Otherwise, no modification is applied.
Since the example in Figure 2 satisfies the second
condition, the model chooses ?s-are-ru (passive)?
and passivizes the sentence so that ?Ken? fills the
nominative case.
(9) LCS
N1
[BECOME [[Ken]z BE WITH]]
+ [[film]x? ACT ON [Ken]y?]
t. Ken-ga eiga-ni shigeki-s-are-ta.
Ken-NOM film-DAT inspire-PAS, PAST
Ken was inspired by the film.
5 Experiment
5.1 Paraphrase generation and evaluation
To empirically evaluate our paraphrasing model and
the LCSdic, and to clarify the remaining problems,
we analyzed a set of automatically generated para-
phrase candidates. The sentences used in the exper-
iment were collected in the following way:
Step 1. From the 876,101 types of triplet ?n, c, v?
collected in Section 3.2, 23,608 types of
?n, c, v? were extracted, whose components, n
and ?c, v?, are listed in the LCSdic.
Step 2. For each of the 245 most frequent ?n, c, v?,
the 3 most frequent simple clauses includ-
ing the ?n, c, v? were extracted from the cor-
pus from which ?n, c, v? s were extracted in
Section 3.2. As a result, we collected 735 sen-
tences.
Step 3. We input these 735 sentences into our para-
phrasing model, and then automatically gener-
ated paraphrase candidates. When more than
one LCS is assigned to a verb in the LCSdic
due to its polysemy or ergative verb such as
?kaifuku-suru (recover),? our model generates
all the possible paraphrase candidates. As a re-
sult, 825 paraphrase candidates, that is, at least
one for each input, were generated.
Table 3: Error sources
Correct candidates 621 (75.8%)
Erroneous candidates 198 (24.2%)
Definition of LCS 30
LCS for light-verb 24
LCS for nominalized verb 6
Paraphrasing model 61
LCS transformation algorithm 59
Treatment of ?suru (to do)? 2
Ambiguity 107
Ambiguous thematic role of dative 78
Recognition of LVC 24
Selection of transitive/intransitive 5
We manually classified the resultant 825 para-
phrase candidates into 621 correct and 198 erro-
neous candidates. The remaining 6 candidates were
not classified. The precision of the paraphrase gen-
eration was 75.8% (621 / 819).
5.2 Error analysis
To clarify the cause of the erroneous paraphrases,
we manually classified 198 erroneous paraphrase
candidates. Table 3 lists the error sources.
5.2.1 LCS transformation algorithm
The experiment came close to confirming that the
right-first matching algorithm in our paraphrasing
model operates correctly. Unfortunately, the match-
ing rules produced some erroneous paraphrases in
LCS transformation.
Errors in predicate matching: To paraphrase
(10s) below, ?CONTROL? in LCS
V 1
must be
matched with ?CONTROL? in LCS
N0
, and x to x?.
However, our model first matched ?CONTROL? in
LCS
V 1
with ?MOVE FROM TO? in LCS
N0
. Thus,
x was incorrectly matched with z? and x? remained
empty. The desired form of LCS
N1
is shown in
(11).
(10) s. kacho-ga buka-ni
section-chief-NOM subordinate-DAT
shiji-o dasu.
order-ACC issue-PRES
The section chief issues orders to his subordinates.
(N=?order?, V =?issue?)
LCS
V 1
[[chief ]x CONTROL [BECOME [[order]y
BE AT [subordinate]z]]]
LCS
N0
[x? CONTROL [y? MOVE FROM z? TO
[FILLED]]]
LCS
N1
?[x? CONTROL [[subordinate]y? MOVE
FROM [chief ]z? TO [FILLED]]]
(11) LCS
N1
[[chief ]x? CONTROL [y? MOVE FROM
[subordinate] TO [FILLED]]]
This error was caused by the mis-matching of
?CONTROL? with ?MOVE FROM TO.? Although
we regard some predicates as being in the same
classes as those described in Section 4.2.1, these
need to be considered carefully. In particular
?MOVE FROM TO? needs further investigation be-
cause it causes many errors whenever it has the
?FILLED? argument.
Errors in argument matching: Even if all the
predicates are matched properly, there would still
be a chance of errors being caused by incorrect ar-
gument matching. With the present algorithm, z
can be matched with y? if and only if z? contains
?FILLED.? In the case of (12), however, z has to
be matched with y?, even though z? is empty. The
desired form of LCS
N1
is shown in (13).
(12) s. Jikan-ni seigen-ga aru.
time-DAT limitation-NOM exist-PRES
There is a time limitation.
(N=?limitation?, V =?exist?)
LCS
V 1
[BECOME [[limitation]y BE AT [time]z]]
LCS
N0
[x? CONTROL [BECOME [y? BE AT z?]]]
LCS
N1
?[x? CONTROL [BECOME [y? BE AT
[time]z?]]]
(13) LCS
N1
[x? CONTROL [BECOME [[timey]y? BE AT
z?]]]
5.2.2 Ambiguous thematic role of dative
In contrast to dative cases in English, in Japanese,
the dative case has ambiguity. That is, it can be a
complement to the verb or an adjunct9. However,
since LCS is not capable of determining whether the
case is a complement or an adjunct, z is occasion-
ally incorrectly filled with an adjunct. For exam-
ple, ?medo-ni? in (14s) should not fill z, because it
acts as an adverb, even though it consists of a noun,
?medo (prospect)? and a case particle for the dative.
We found that 78 erroneous candidates constitute
this most dominant type of errors.
(14) s. Kin?you-o medo-ni sagyo-o susumeru.
Friday-NOM by-DAT work-ACC carry on-PRES
I plan to finish the work by Friday.
(N=?work?, V =?carry?)
LCS
V 0
[x CONTROL [BECOME [y BE AT z]]]
LCS
V 1
?[x CONTROL [BECOME [[work]y BE AT
[by]z]]]
The ambiguity of dative cases in Japanese has
been discussed in the literature of linguistics and
some natural language processing tasks (Muraki,
1991). To date, however, a practical compli-
ment/adjunct classifier has not been established. We
plan to address this topic in our future research.
Preliminary investigation revealed that only cer-
tain groups of nouns can constitute both compli-
ments and adjuncts according to the governing verb.
Therefore, generally whether a word acts as a com-
plement is determined without combining it with the
verb.
9(Muraki, 1991) classifies dative cases into 11 thematic
roles that can be regarded as complements. In contrast, there
is no typology of dative cases that act as adjuncts.
5.2.3 Recognition of LVC
In our model, we assume that a triplet ?n, c, v? con-
sisting of a nominalized verb n and a light-verb tu-
ple ?c, v? from our vocabulary lists (see Section 3.2)
always act as an LVC. However, not only the triplet
itself but also its context sometimes affects whether
the given triplet can be paraphrased. For exam-
ple, we regard ?imi-ga aru? as an LVC, because the
nominalized verb ?imi? and the tuple ??ga?, ?aru??
appear in the vocabulary lists. However, the ?n, c, v?
in (15s) does not act as an LVC, while the same
triplet in (16s) does.
(15) s. Sanka-suru-koto-ni imi-ga aru.
to participate-DAT meaning-NOM exist-PRES
There is meaning in participating.
t.?Sanka-suru-koto-o imi-suru.
to participate-ACC mean-ACT, PRES
?It means to participate in it.
(16) s. ?kennel?-niwa inugoya-toiu
?kennel?-TOP doghouse-OF
imi-ga aru.
meaning-NOM exist-PRES
?kennel? has the meaning of doghouse.
t. ?kennel?-wa inugoya-o imi-suru.
?kennel?-TOP doghouse-ACC mean-ACT, PRES
?kennel? means doghouse.
The above difference is caused by the polysemy
of the nominalized verb ?imi? that denotes ?worth?
in the context of (15s), but ?meaning? in (16s).
Although incorporating word sense disambiguation
using contextual clues complicates our model, in
fact only a limited number of nominalized verbs are
polysemous. We therefore expect that we can list
them up and use this as a trigger for making a deci-
sion as to whether we need to take the context into
account. Namely, given a ?n, c, v?, we would be
able to classify it into (a) a main verb phrase, (b) a
delicate case in terms of the dependence of its con-
text, and (c) an LVC.
We can adopt a different approach to avoiding
incorrect paraphrase generation. As described in
Section 5.1, our model generates all the possible
paraphrase candidates when more than one LCS is
assigned to a verb. Similarly, our approach can be
extended to (i) over-generate paraphrase candidates
by considering the polysemy of not only assigned
LCS types, but also that of nominalized verbs (see
(15s) and (16s)) and whether the given ?n, c, v? is
an LVC, and (ii) revise or reject the incorrect candi-
dates by using handcrafted solid rules or statistical
language models.
6 Conclusion and future work
In this paper, we presented an LCS-based para-
phrasing model for LVCs and an extension of an ex-
isting LCS dictionary. Our model achieved an accu-
racy of 75.8% in selecting the voice and reassigning
the cases.
To make our paraphrasing model more accurate,
further analysis is needed, especially for the LCS
transformation stage described in Section 4.2. Sim-
ilarly, several levels of disambiguation should also
be solved. The Japanese LCS typology has to be
refined from the theoretical point of view. For ex-
ample, since extensions are no more than human in-
tuition, we must discuss how we can assign LCSs
for given verbs based on explicit language tests, as
described in (Takeuchi et al, 2001).
In future research, we will also extend our LCS-
based approach to other classes of paraphrases that
exhibit some regularity, such as verb alteration and
compound noun decomposition as shown in (17)
and (18), below. LCS has been discussed as a
means of explaining the difference between transi-
tive/intransitive verbs, and the construction of com-
pounds. Therefore, our next goal is to show the ap-
plicability of LCS through practical tasks, namely,
paraphrasing.
(17) s. Jishin-ga building-o kowashita.
earthquake-NOM building-DAT destroy-PAST
The earthquake destroyed the building.
t. Jishin-de building-ga kowareta.
earthquake-LOC building-NOM be destroyed-PAST
The building was destroyed in the earthquake.
(18) s. Kare-wa kikai
he-TOP machine-
sousa-ga jouzu-da.
operation-NOM good-COPULA
He is good at operating the machine.
t. Kare-wa kikai-o jouzu-ni sousa-suru.
he-TOP machine-DAT well-ADV operate-PRES
He operates machines well.
References
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of the
39th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 50?57.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for language-
impaired readers. In Proceedings of the 9th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 269?270.
B. J. Dorr, J. Garman, and A. Weinberg. 1995. From
syntactic encodings to thematic roles: building lexi-
cal entries for interlingual MT. Machine Translation,
9(3):71?100.
M. Dras. 1999. Tree adjoining grammar and the reluc-
tant paraphrasing of text. Ph.D. thesis, Department of
Computing, Macquarie University.
K. Inui and M. Nogami. 2001. A paraphrase-based
exploration of cohesiveness criteria. In Proceedings
of the 8th European Workshop on Natulal Language
Generation (EWNLG), pages 101?110.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the 2nd International
Workshop on Paraphrasing: Paraphrase Acquisition
and Applications (IWP), pages 9?16.
L. Iordanskaja, R. Kittredge, and A. Polgue`re. 1991.
Lexical selection and paraphrase in a meaning-text
generation model. In Paris et al (Eds.) Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293?312. Kluwer Aca-
demic Publishers.
T. Kageyama, editor. 1996. Verb semantics. Kuroshio
Publishers. (in Japanese).
N. Kaji and S. Kurohashi. 2004. Recognition and para-
phrasing of periphrastic and overlapping verb phrases.
In Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC) Work-
shop on Methodologies and Evaluation of Multiword
Units in Real-world Application.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of 6th Conference on Natural Language Learning
(CoNLL), pages 63?69.
B. Levin. 1993. English verb classes and alternations:
a preliminary investigation. Chicago Press.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question answering. Natural Language Engineer-
ing, 7(4):343?360.
I. Mel?c?uk and A. Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
S. Muraki. 1991. Various aspects of Japanese verbs.
Hitsuji Syobo. (in Japanese).
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: extracting para-
phrases and generating new sentences. In Proceed-
ings of the 2003 Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL),
pages 102?109.
S. Sato. 1999. Automatic paraphrase of technical pa-
pers? titles. Journal of Information Processing Society
of Japan, 40(7):2937?2945. (in Japanese).
Y. Shinyama and S. Sekine. 2003. Paraphrase acquisi-
tion for information extraction. In Proceedings of the
2nd International Workshop on Paraphrasing: Para-
phrase Acquisition and Applications (IWP), pages 65?
71.
K. Takeuchi, K. Uchiyama, S. Yoshioka, K. Kageura,
and T. Koyama. 2001. Categorising deverbal nouns
based on lexical conceptual structure for analysing
Japanese compounds. In Proceedings of IEEE Sys-
tem, Man, and Cybernetics Conference, pages 904?
909.
Proceedings of the Linguistic Annotation Workshop, pages 132?139,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating a Japanese Text Corpus with
Predicate-Argument and Coreference Relations
Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{ryu-i,mamoru-k,inui,matsu}@is.naist.jp
Abstract
In this paper, we discuss how to anno-
tate coreference and predicate-argument re-
lations in Japanese written text. There
have been research activities for building
Japanese text corpora annotated with coref-
erence and predicate-argument relations as
are done in the Kyoto Text Corpus version
4.0 (Kawahara et al, 2002) and the GDA-
Tagged Corpus (Hasida, 2005). However,
there is still much room for refining their
specifications. For this reason, we discuss
issues in annotating these two types of re-
lations, and propose a new specification for
each. In accordance with the specification,
we built a large-scaled annotated corpus, and
examined its reliability. As a result of our
current work, we have released an anno-
tated corpus named the NAIST Text Corpus1,
which is used as the evaluation data set in
the coreference and zero-anaphora resolu-
tion tasks in Iida et al (2005) and Iida et al
(2006).
1 Introduction
Coreference resolution and predicate-argument
structure analysis has recently been a growing field
of research due to the demands from NLP appli-
cation such as information extraction and machine
translation. With the research focus placed on these
tasks, the specification of annotating corpora and the
1The NAIST Text Corpus is downloadable from
http://cl.naist.jp/nldata/corpus/, and it has already been
downloaded by 102 unique users.
data sets used in supervised techniques (Soon et al,
2001; Ng and Cardie, 2002, etc.) have also grown in
sophistication.
For English, several annotation schemes have al-
ready been proposed for both coreference relation
and argument structure, and annotated corpora have
been developed accordingly (Hirschman, 1997; Poe-
sio et al, 2004; Doddington et al, 2004). For in-
stance, in the Coreference task on Message Under-
standing Conference (MUC) and the Entity Detec-
tion and Tracking (EDT) task in the Automatic Con-
tent Extraction (ACE) program, which is the suc-
cessor of MUC, the details of specification of anno-
tating coreference relation have been discussed for
several years. On the other hand, the specification
of predicate-argument structure analysis has mainly
been discussed in the context of the CoNLL shared
task2 on the basis of the PropBank (Palmer et al,
2005).
In parallel with these efforts, there have also been
research activities for building Japanese text corpora
annotated with coreference and predicate-argument
relations such as the Kyoto Text Corpus version 4.0
(Kawahara et al, 2002) and the GDA3-Tagged Cor-
pus (Hasida, 2005). However, as we discuss in this
paper, there is still much room for arguing and re-
fining the specification of such sorts of semantic an-
notation. In fact, for neither of the above two cor-
pora, the adequacy and reliability of the annotation
scheme has been deeply examined.
In this paper, we discuss how to annotate coref-
erence and predicate-argument relations in Japanese
2http://www.lsi.upc.edu/?srlconll/
3The Global Document Annotation
132
text. In Section 2 to Section 4, we examine the an-
notation issues of coreference, predicate-argument
relations, and event-nouns and their argument rela-
tions respectively, and define adequate specification
of each annotation task. Then, we report the results
of actual annotation taking the Kyoto Corpus 3.0 as a
starting point. Section 6 discusses the open issues of
each annotation task and we conclude in Section 7.
2 Annotating coreference relations
2.1 Approaches to coreference annotation
Coreference annotation in English has been evolving
mainly in the context of information extraction. For
instance, in the 6th and 7th Message Understand-
ing Conferences (MUC), coreference resolution is
treated as a subtask of information extraction4. The
annotated corpora built in the MUC contain coref-
erence relations between NPs, which are used as a
gold standard data set for machine learning-based
approaches to coreference resolution by researchers
such as Soon et al (2001) and Ng and Cardie (2002).
However, van Deemter and Kibble (1999) claim
that the specification of the MUC coreference task
guides us to annotate expressions that are not nor-
mally considered coreferential, such as appositive
relations (e.g. Julius Caesari, a well-known em-
perori, ...).
In the task of Entity Detection and Tracking
(EDT) in the Automatic Content Extraction (ACE)
program (Doddington et al, 2004), the successor
of MUC, the coreference relations are redefined in
terms of two concepts, mentions and entities, in or-
der to avoid inappropriate co-indexing. In the speci-
fication of EDT, mentions are defined as the expres-
sions appearing in the texts, and entities mean the
collective set of specific entities referred to by the
mentions in the texts. Entities are limited to named
entities such as PERSON and ORGANIZATION for
adequacy and reliability of annotation. Therefore,
the ACE data set has the drawback that not all coref-
erence relations in the text are exhaustively anno-
tated. It is insufficient to resolve only the annotated
coreference relations in order to properly analyze a
text.
4http://www-nlpir.nist.gov/related projects/muc/
proceedings/co task.html
2.2 Coreference annotated corpora of Japanese
In parallel with these efforts, Japanese corpora have
been developed that are annotated with coreference
relations, such as the Kyoto Text Corpus version
4.0 (Kawahara et al, 2002) and GDA-Tagged Cor-
pus (Hasida, 2005). Before reviewing these works,
we explain the relationship between anaphora and
coreference in Japanese, referring to the following
examples. In example (1), the pronoun sorei (it)
points back to iPodi, and these two mentions refer
to the same entity in the world and thus are consid-
ered both anaphoric and coreferential.
(1) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
kare-wa sorei-de ongaku-o ki-ita .
he-TOP iti-INS music-ACC listen to-PAST PUNC
He listened to music on it.
On the other hand, in example (2), we still see an
anaphoric relation between iPodi (iPodi) and sorej
(itj) and sorej points back to iPodi. However, these
two mentions are not coreferential since they refer
to different entities in the world.
(2) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
Mary-mo sorej-o ka-tta .
Mary-TOP onej -ACC buy-PAST PUNC
Mary also bought one.
As in the above examples, an anaphoric relation
can be either coreferential or not. The former case is
called an identity-of-reference anaphora (IRA) and
the latter an identity-of-sense anaphora (ISA) (see
Mitkov (2002)). In English the difference between
IRA and ISA is clearly expressed by the anaphoric
relations formed with ?it? and ?one? respectively.
This makes it possible to treat these classes sepa-
rately. However, in Japanese, no such clear lexical
distinction can be drawn. In both the Kyoto Cor-
pus and GDA-Tagged Corpus, there is no discussion
in regards to distinction between ISA and IRA, thus
it is unclear what types of coreference relations the
annotators annotated. To make matters worse, their
approaches do not consider whether or not a mention
refers to a specific entity like in the EDT task.
2.3 Annotating IRA relations in Japanese
As described in the previous section, conventional
specifications in Japanese are not based on a pre-
133
cise definition of coreference relations, resulting in
inappropriate annotation. On the other hand, in our
specification, we consider two or more mentions as
coreferential in case they satisfy the following two
conditions:
? The mentions refer to not a generic entity but
to a specific entity.
? The relation between the mentions is consid-
ered as an IRA relation.
3 Annotating predicate-argument relations
3.1 Labels of predicate-argument relations
One debatable issue in the annotation of predicate-
argument relations is what level of abstraction we
should label those relations at.
The GDA-Tagged Corpus, for example, adopts a
fixed set of somewhat ?traditional? semantic roles
such as Agent, Theme, and Goal that are defined
across verbs. The PropBank (Palmer et al, 2005),
on the other hand, defines a set of semantic roles (la-
beled ARG0, ARG1, and AM-ADV, etc.) for each
verb and annotates each sentence in the corpus with
those labels as in (3).
(3) [ARGM?TMP A year earlier], [ARG0 the refiner] [rel
earned] [ARG1 $66 million, or $1.19 a share].
In the FrameNet (Fillmore and Baker, 2000), a spe-
cific set of semantic roles is defined for each set of
semantically-related verbs called a FrameNet frame.
However, there is still only limited consensus on
how many kinds of semantic roles should be iden-
tified and which linguistic theory we should adopt
to define them at least for the Japanese language.
An alternative way of labeling predicate-
argument relations is to use syntactic cases as
labels. In Japanese, arguments of a verb are marked
by a postposition, which functions as a case marker.
In sentence (4), for example, the verb tabe has
two arguments, each of which is marked by a
postposition, ga or o.
(4) Tom-ga ringo-o tabe-ru
Tom-NOM apple-ACC eat-PRES
(Tom eats an apple.)
Labeling predicate-argument relations in terms of
syntactic cases has a few more advantages over se-
mantic roles as far as Japanese is concerned:
? Manual annotation of syntactic cases is likely
to be more cost-efficient than semantic roles
because they are often explicitly marked by
case markers. This fact also allows us to avoid
the difficulties in defining a label set.
? In Japanese, the mapping from syntactic cases
to semantic roles tends to be reasonably
straightforward if a semantically rich lexicon of
verbs like the VerbNet (Kipper et al, 2000) is
available.
? Furthermore, we have not yet found many NLP
applications for which the utility of seman-
tic roles is actually demonstrated. One may
think of using semantic roles in textual infer-
ence as exemplified by, for example, Tatu and
Moldovan (2006). However, similar sort of
inference may well be realized with syntactic
cases as demonstrated in the information ex-
traction and question answering literature.
Taking these respects into account, we choose to
label predicate-argument relations in terms of syn-
tactic cases, which follows the annotation scheme
adopted in the Kyoto Corpus.
3.2 Syntactic case alternation
Once the level of syntactic cases is chosen for our
annotation, another issue immediately arises, alter-
ation of syntactic cases by syntactic transformations
such as passivization and causativization. For exam-
ple, sentence (5) is an example of causativization,
where Mary causes Tom?s eating action.
(5) Mary-ga Tom-ni ringo-o tabe-saseru
Mary-NOM Tom-DAT apple-ACC eat-CAUSATIVIZED
(Mary helps Tom eat an apple.)
One way of annotating these arguments is some-
thing like (6), where the relations between the
causativized predicate tabe-saseru (to make some-
one eat) and its arguments are indicated in terms of
surface syntactic cases.
(6) [REL=tabe-saseru (eat-CAUSATIVE),
GA=Mary, NI=Tom, O=ringo (apple)]
In fact, the Kyoto Corpus adopts this way of label-
ing.
An alternative way of treating such case alterna-
tions is to identify logical (or deep) case relations,
i.e. the relations between the base form of each pred-
icate and its arguments. (7) illustrates how the ar-
guments in sentence (5) are annotated with logical
case relations: Tom is labeled as the ga-case (Nom-
inative) filler of the verb tabe (to eat) and Mary is
134
labeled as the Extra-Nominative (EX-GA) which we
newly invent to indicate the Causer of a syntactically
causativized clause.
(7) [REL=tabe-(ru) (eat), GA=Tom, O=ringo (ap-
ple), EX-GA=Mary]
In the NAIST Text Corpus, we choose to this lat-
ter way of annotation motivated by such considera-
tions as follows:
? Knowing that, for example, Tom is the filler of
the ga-case (Nominative) of the verb tabe (to
eat) in (5) is more useful than knowing that Tom
is the ni-case (Dative) of the causativized verb
tabe-saseru (to make someone eat) for such ap-
plications as information extraction.
? The mapping from syntactic cases to semantic
roles should be described in terms of logical
case relations associated with bare verbs.
3.3 Zero-anaphora
In the PropBank the search space for a given pred-
icate?s arguments is limited to the sentence that
predicate appears in, because, syntactically, English
obligatory arguments are overtly expressed except
pro-form (e.g. John hopes [PRO to leave.]).
In contrast, Japanese is characterized by extensive
use of nominal ellipses, called zero-pronouns, which
behave like pronouns in English texts. Thus, if an
argument is omitted, and an expression correspond-
ing to that argument does not appear in the same
sentence, annotators should search for its antecedent
outside of the sentence. Furthermore, if an argument
is not explicitly mentioned in the text, they need to
annotate that relation as ?exophoric.? In the second
sentence of example (8), for instance, the ga (Nomi-
native) argument of the predicate kaeru (go back) is
omitted and refers to Tom in the first sentence. The
kara (Ablative) argument of that predicate is also
omitted, however the corresponding argument does
not explicitly appear in the text. In such cases, omit-
ted arguments should be considered as ?exophoric.?
(8) Tomi-wa kyo gakko-ni it-ta .
Tomi-TOP today school-LOC go-PAST PUNC
Tom went to school today.
(?i-ga) (?exophoric-kara) kae-tte suguni
?i-NOM ?exophoric-ABL go back immediately
(?i-ga) kouen-ni dekake-ta .
?i-NOM park-LOC go out-PAST PUNC
He went to the park as soon as he came back
from school.
Table 1: Comparison of annotating predicate-
argument relations
corpus label search space
PropBank semantic role intra
GDA Corpus semantic role inter, exo
Kyoto Corpus surface case intra, inter,
(voice alternation involved) exo
NAIST Corpus logical (deep) case intra, inter,
(our corpus) (relation with bare verb) exo
intra: intra-sentential relations, inter: inter-sentential relations,
exo: exophoric relations
To the best of our knowledge, the GDA-Tagged Cor-
pus does not contain intra-sentential zero-anaphoric
relations as predicate-argument relations, so it has a
serious drawback when used as training data in ma-
chine learning approaches.
Unlike coreference between two explicit nouns
where only an IRA is possible, the relation between
a zero-pronoun and its antecedent can be either IRA
or ISA. For example, in example (8), ?i is annotated
as having an IRA relation with its antecedent Tomi.
In contrast, example (9) exhibits an ISA relation be-
tween iPodi and ?i.
(9) Tom-wa iPodi-o kaa-tta .
Tom-TOP iPodi-ACC buya-PAST PUNC
Tom bought an iPod.
Mary-mo (?i-o) kab-tta .
Mary-TOP ?i-ACC buyb-PAST PUNC
Mary also bought one.
[REL=ka-(u) (buy), GA=Mary, O=iPodi]
The above examples indicate that predicate-
argument annotation in Japanese can potentially be
annotated as either an IRA or ISA relation. Note that
in Japanese these two relations cannot be explicitly
separated by syntactic clues. Thus, in our corpus
we annotate them without explicit distinction. It is
arguable that separate treatment of IRA and ISA in
predicate-argument annotation could be preferable.
We consider this issue as a task of future work.
A comparison of the specification is summarized
in Table 1.
4 Annotating event-noun-argument
relations
Meyers et al (2004) propose to annotate seman-
tic relations between nouns referring to an event
in the context, which we call event-nouns in this
135
paper. They release the NomBank corpus, in
which PropBank-style semantic relations are anno-
tated for event-nouns. In (10), for example, the
noun ?growth? refers to an event and ?dividends?
and ?next year? are annotated as ARG1 (roughly
corresponding to the theme role) and ARGM-TMP
(temporal adjunct).
(10) 12% growth in dividends next year [REL=growth,
ARG1=in dividends, ARGM-TMP=next year]
Following the PropBank-style annotation, the Nom-
Bank also restricts the search space for the argu-
ments of a given event-noun to the sentence in which
the event-noun appears. In Japanese, on the other
hand, since predicate-argument relations are often
zero-anaphoric, this restriction should be relaxed.
4.1 Labels of event-noun-relations
Regarding the choice between semantic roles and
syntactic cases, we take the same approach as
that for predicate-argument relations, which is also
adopted in the Kyoto Corpus. For example, in (11),
akajii (deficit) is identified as the ga argument of the
event-noun eikyo (influence).
(11) kono boueki akajii-wa waga kuni-no
this trade deficit-TOP our country-OF
kyosoryokuj-ni eikyo-o oyobosu
competitiveness-DAT influence-ACC affect
[REL=eikyo (influence), GA=akajii (deficit),
O=kyosoryokuj (competitiveness)]
The trade deficit affects our competitiveness.
Note that unlike verbal predicates, event-nouns can
never be a subject of voice alternation. An event-
noun-argument relation is, therefore, necessarily an-
notated in terms of the relation between the bare
verb corresponding to the event-noun and its argu-
ment. This is another reason why we consider it
reasonable to annotate the logical case relations be-
tween bare verbs and their arguments for predicate-
argument relations.
4.2 Event-hood
Another issue to be addressed is on the determina-
tion of the ?event-hood? of noun phrases, i.e. the
task of determining whether a given noun refers to
an event or not. In Japanese, since neither singular-
plural nor definite-indefinite distinction is explic-
itly marked, event-hood determination tends to be
highly context-dependent. In sentence (12), for ex-
ample, the first occurrence of denwa (phone-call),
subscripted with i, should be interpreted as Tom?s
calling event, whereas the second occurrence of the
same noun denwa should be interpreted as a physical
telephone (cellphone).
(12) karea-karano denwai-niyoruto watashib-wa
hea-ABL phone-calli according to Ib-NOM
kare-no ie-ni denwaj-o wasure-tarasii
his-OF home-LOC phonej -ACC leave-PAST
According to his phone call, I might have left
my cell phone at his home.
To control the quality of event-hood determina-
tion, we constrain the range of potential event-nouns
from two different points of view, neither of which
is explicitly discussed in designing the specifications
of the Kyoto Corpus.
First, we impose a POS-based constraint. In our
corpus annotation, we consider only verbal nouns
(sahen-verbs; e.g. denwa (phone) ) and deverbal
nouns (the nominalized forms of verbs; e.g. furumai
(behavior)) as potential event-nouns. This means
that event-nouns that are not associated with a verb,
such as jiko (accident), are out of scope of our anno-
tation.
Second, the determination of the event-hood of
a noun tends to be obscure when the noun consti-
tutes a compound. In (13), for example, the ver-
bal noun kensetsu (construction) constituting a com-
pound douro-kensetsu (road construction) can be in-
terpreted as a constructing event. We annotate it as
an event and douro (road) as the o argument.
(13) (?-ga) douro-kensetsu-o tsuzukeru
?-NOM road construction-ACC continue
Someone continues road construction.
In (14), on the other hand, since the compound
furansu kakumei (French Revolution) is a named-
entity and is not semantically decomposable, it is
not reasonable to consider any sort of predicate-
argument-like relations between its constituents fu-
ransu (France) and kakumei (revolution).
(14) furansu-kakumei-ga okoru
French Revolution-NOM take place
The French Revolution took place.
We therefore do not consider constituents of such se-
mantically non-decomposable compounds as a tar-
get of annotation.
5 Statistics of the new corpus
Two annotators annotated predicate-argument and
coreference relations according to the specifications,
136
using all the documents in Kyoto Text Corpus ver-
sion 3.0 (containing 38,384 sentences in 2,929 texts)
as a target corpus. We have so far annotated
predicate-argument relations with only three major
cases: ga (Nominative), o (Accusative) and ni (Da-
tive). We decided not to annotate other case relations
like kara-case (Ablative) because the annotation of
those cases was considered even further unreliable at
the point where we did not have enough experiences
in this annotation task. Annotating other cases is one
of our future directions.
The numbers of the annotated predicate-argument
relations are shown in Table 2. These relations are
categorized into five cases: (a) a predicate and its
argument appear in the same phrase, (b) the argu-
ment syntactically depends on its predicate or vice
versa, (c) the predicate and its argument have an
intra-sentential zero-anaphora relation, (d) the pred-
icate and its argument have an inter-sentential zero-
anaphora relation and (e) the argument does not ex-
plicitly appear in the text (i.e. exophoric). Table 2
shows that in annotation for predicates over 80%
of both o- and ni-arguments were found in depen-
dency relations, while around 60% of ga-arguments
were in zero-anaphoric relations. In comparison, in
the case of event-nouns, o- and ni-arguments are
likely to appear in the same phrase of given event-
nouns, and about 80% of ga-arguments have zero-
anaphoric relations with event-nouns. With respect
to the corpus size, we created a large-scaled anno-
tated corpus with predicate-argument and corefer-
ence relations. The data size of our corpus along
with other corpora is shown in Table 3.
Next, to evaluate the agreement between the two
human annotators, 287 randomly selected articles
were annotated by both of them. The results are
evaluated by calculating recall and precision in
which one annotation result is regarded as correct
and the other?s as the output of system. Note that
only the predicates annotated by both annotators are
used in calculating recall and precision. For eval-
uation of coreference relations, we calculated re-
call and precision based on the MUC score (Vilain
et al, 1995). The results are shown in Table 4,
where we can see that most annotating work was
done with high quality except for the ni-argument of
event-nouns. The most common source of error was
caused by verb alternation, and we will discuss this
Table 3: Data size of each corpus
corpus size
PropBank I 7,891 sentences
NomBank 0.8 24,311 sentences
ACE (2005 English) 269 articles
GDA Corpus 2,177 articles
Kyoto Corpus 555 articles (5,127 sentences)
NAIST Corpus (ours) 2,929 articles (38,384 sentences)
Table 4: Agreement of annotating each relation
recall precision
predicate 0.947 (6512/6880) 0.941 (6512/6920)
ga (NOM) 0.861 (5638/6549) 0.856 (5638/6567)
o (ACC) 0.943 (2447/2595) 0.919 (2447/2664)
ni (DAT) 0.892 (1060/1189) 0.817 (1060/1298)
event-noun 0.905 (1281/1415) 0.810 (1281/1582)
ga (NOM) 0.798 (1038/1300) 0.804 (1038/1291)
o (ACC) 0.893 (469/525) 0.765 (469/613)
ni (DAT) 0.717 (66/92) 0.606 (66/109)
coreference 0.893 (1802/2019) 0.831 (1802/2168)
issue in detail in Section 6. Such investigation of the
reliability of annotation has not been reported for ei-
ther the Kyoto Corpus or the GDA-Tagged Corpus.
However, our results also show that each annotating
task still leaves room for improvement. We summa-
rize open issues and discuss the future directions in
the next section.
6 Discussion
6.1 Identification of predicates and
event-nouns
Identification of predicates is sometimes unreliable
due to the ambiguity between a literal usage and a
compound functional usage. For instance, the ex-
pression ?to-shi-te?, which includes the verb shi (to
do), is ambiguous: either the verb shi functions as a
content word, i.e. an event-denoting word, or it con-
stitutes a multi-word expression together with to and
te. In the latter case, it does not make sense to inter-
pret the verb shi to denote an event. However, this
judgment is highly context-dependent and we have
not been able to devise a reliable criterion for it.
Tsuchiya et al (2006) have built a functional
expression-tagged corpus for automatically classify-
ing these usages. They reported that the agreement
ratio of functional expressions is higher than ours.
We believe their findings to also become helpful in-
formation for annotating predicates in our corpus.
With regards to event-nouns, a similar problem
137
Table 2: Statistics: annotating predicate-arguments relations
ga (Nominative) o (Accusative) ni (Dative)
predicates (a) in same phrase 177 (0.002) 60 (0.001) 591 (0.027)
106,628 (b) dependency relations 44,402 (0.419) 35,882 (0.835) 18,912 (0.879)
(c) zero-anaphoric (intra-sentential) 32,270 (0.305) 5,625 (0.131) 1,417 (0.066)
(d) zero-anaphoric (inter-sentential) 13,181 (0.124) 1,307 (0.030) 542 (0.025)
(e) exophoric 15,885 (0.150) 96 (0.002) 45 (0.002)
total 105,915 (1.000) 42,970 (1.000) 21,507 (1.000)
event-nouns (a) in same phrase 2,195 (0.077) 5,574 (0.506) 846 (0.436)
28,569 (b) dependency relations 4,332 (0.152) 2,890 (0.263) 298 (0.154)
(c) zero-anaphoric (intra-sentential) 9,222 (0.324) 1,645 (0.149) 586 (0.302)
(d) zero-anaphoric (inter-sentential) 5,190 (0.183) 854 (0.078) 201 (0.104)
(e) exophoric 7,525 (0.264) 42 (0.004) 10 (0.005)
total 28,464 (1.000) 11,005 (1.000) 1,941 (1.000)
also arises. If, for example, a compound noun con-
tains a verbal noun, we have to judge whether the
verbal noun can be interpreted as an event-noun or
not. Currently, we ask annotators to check if the
meaning of a given compound noun can be compo-
sitionally decomposed into those of its constituents.
However, the judgement of compositionality tends
to be highly subjective, causing the degradation of
the agreement ratio of event-nouns as shown in
Table 4. We are planning to investigate this problem
more closely and refine the current compositionality
criterion. One option is to build lexical resources of
multi-word expressions and compounds.
6.2 Identification of arguments
As we mentioned in 3.1, we use (deep) cases instead
of semantic roles as labels of predicate-argument re-
lations. While it has several advantages as discussed
in 3.1, this choice has also a drawback that should
be removed. The problem arises from lexical verb
alternation. It can sometimes be hard for annota-
tors to determine a case frame of a given predicate
when verb alternation takes place. For example, sen-
tence (15) can be analyzed simply as in (16a). How-
ever, since the verb shibaru (bind) has also another
alternative case frame as in (16b), the labeling of the
case of the argument kisoku (rule), i.e. either GA
(NOM) or DE (INST) may be undecidable if the argu-
ment is omitted.
(15) kisoku-ga hitobito-o shibaru
rule-NOM people-ACC bind
The rule binds people.
(16) a. [REL = shibaru (bind), GA = kisoku (rule), O = hitobito
(people)]
b. [REL = shibaru (bind), GA = ? (exophoric), O = hito-
bito (people), DE (Instrumental) = kisoku (rule)]
Similar problems occur for event-nouns as well.
For example, the event-noun hassei (realization) has
both transitive and intransitive readings, which may
produce awkward ambiguities.
To avoid this problem, we have two options; one
is to predefine the preference in case frames as a
convention for annotation and the other is to deal
with such alternations based on generic resources of
lexical semantics such as Lexical Conceptual Struc-
ture (LCS) (Jackendoff, 1990). Creating a Japanese
LCS dictionary is another on-going project, so we
can collaborate with them in developing the valuable
resources.
6.3 Event-hood determination
Event-nouns of some semantic types such as keiyaku
(contract), kisei (regulation) and toushi (investment)
are interpreted as either an event or an entity result-
ing from an event depending on are context. How-
ever, it is sometimes difficult to judge whether such
an event-noun should be interpreted as an event or a
resultant entity even by considering the whole con-
text, which degrades the stability of annotation. This
phenomena is also discussed in the NomBank, and
we will share their insights and refine our annotation
manual in the next step.
6.4 Identification of coreference relation
Even though coreference relation is defined as IRA
relations, the lack of agreement on the granularity of
noun classes makes the agreement ratio worse. In
other words, it is crucial to decide how to annotate
abstract nouns in order to improve the annotation.
138
Annotators judge coreference relations as whether
or not abstract nouns refer to the same entity in the
world. However, the equivalence of the referents of
abstract nouns cannot be reconciled based on real-
world existence since by definition abstract nouns
have no physical entities in the real world.
As far as predicate-argument relation is con-
cerned, there might be a need for treating generic
entities in addition to specific entities as coreferen-
tial in some application. For example, one may want
to relate kids to children in sentence (17).
(17) We all want children to be fit and healthy.
However, the current invasion of fast food is
creating overweight and unhealthy kids.
The coreference relation between generic nouns are
missed in the current specification since we annotate
only IRA relations between specific nouns. Even
though there are various discussions in the area of
semantics, the issue of how to deal with generic
nouns as either coreferential or not in real texts is
still left open.
7 Conclusion
In this paper, we reported on the current specifica-
tion of our annotated corpus for coreference reso-
lution and predicate-argument analysis. Taking the
previous work of corpus annotation into account, we
decided to annotate predicate-argument relations by
ISA and IRA relations, and coreference relations ac-
cording to IRA relations. With the Kyoto Text Cor-
pus version 3.0 as a starting point, we built a large
annotated corpus. We also discussed the revelations
made from annotating our corpus, and discussed fu-
ture directions for refining our specifications of the
NAIST Text Corpus.
Acknowledgement
This work is partially supported by the Grant-in-Aid
for Scientific Research in Priority Areas JAPANESE
CORPUS (http://tokuteicorpus.jp).
References
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. Automatic content
extraction (ace) program - task definitions and performance
measures. In Proceedings of the 4rd International Confer-
ence on Language Resources and Evaluation (LREC-2004),
pages 837?840.
Charles J. Fillmore and Collin F. Baker. 2000. Framenet:
Frame semantics meets the corpus. In Proceedings of the
74th Annual Meeting of the Linguistic Society of America.
K. Hasida. 2005. Global document annotation (gda) annotation
manual. http://i-content.org/tagman.html.
L. Hirschman. 1997. MUC-7 coreference task definition. ver-
sion 3.0.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora reso-
lution by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4:417?434.
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting syntac-
tic patterns as clues in zero-anaphora resolution. In Proced-
dings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 625?632.
R. Jackendoff. 1990. Semantic Structures. Current Studies in
Linguistics 18. The MIT Press.
D. Kawahara, T. Kurohashi, and K. Hasida. 2002. Construc-
tion of a japanese relevance-tagged corpus (in japanese). In
Proceedings of the 8th Annual Meeting of the Association for
Natural Language Processing, pages 495?498.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-based con-
struction of a verb lexicon. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence and 12th Confer-
ence on on Innovative Applications of Artificial Intelligence,
pages 691?696.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interimreport. In Proceedings of the HLT-NAACL Work-
shop on Frontiers in Corpus Annotation.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in Lan-
guage and Linguistics. Pearson Education.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104?111.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Compu-
tational Linguistics, 31(1):71?106.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceddings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 144?151.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
M. Tatu and D. Moldovan. 2006. A logic-based semantic ap-
proach to recognizing textual entailment. In Proceddings
of the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 819?826.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2006. Development and analysis of an exam-
ple database of japanese compound functional expressions.
IPSJ Journal, 47(6):1728?1741.
K. van Deemter and R. Kibble. 1999. What is coreference, and
what should coreference annotation be? In Proceedings of
the ACL ?99 Workshop on Coreference and its applications,
pages 90?96.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the 6th Message Understanding
Conference (MUC-6), pages 45?52.
139
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 150?153,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Semantic Relations Combining Facts and Opinions
Koji Murakami? Shouko Masuda?? Suguru Matsuyoshi?
Eric Nichols? Kentaro Inui? Yuji Matsumoto?
?Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192 JAPAN
?Osaka Prefecture University
1-1, Gakuen, Naka-ku, Sakai, Osaka 599-8531 JAPAN
{kmurakami,shouko,matuyosi,eric-n,inui,matsu}@is.naist.jp
Abstract
As part of the STATEMENT MAP project,
we are constructing a Japanese corpus an-
notated with the semantic relations bridg-
ing facts and opinions that are necessary
for online information credibility evalua-
tion. In this paper, we identify the se-
mantic relations essential to this task and
discuss how to efficiently collect valid ex-
amples from Web documents by splitting
complex sentences into fundamental units
of meaning called ?statements? and an-
notating relations at the statement level.
We present a statement annotation scheme
and examine its reliability by annotating
around 1,500 pairs of statements. We are
preparing the corpus for release this win-
ter.
1 Introduction
The goal of the STATEMENT MAP project (Mu-
rakami et al, 2009) is to assist internet users with
evaluating the credibility of online information by
presenting them with a comprehensive survey of
opinions on a topic and showing how they relate
to each other. However, because real text on the
Web is often complex in nature, we target a sim-
pler and more fundamental unit of meaning which
we call the ?statement.? To summarize opinions
for the statement map users, we first convert all
sentences into statements and then, organize them
into groups of agreeing and conflicting opinions
that show the logical support for each group.
For example, a user who is concerned about po-
tential connections between vaccines and autism
would be presented with a visualization of the
opinions for and against such a connection to-
gether with the evidence supporting each view as
shown in Figure 1.
When the concerned user in our example looks
at this STATEMENT MAP, he or she will see that
some opinions support the query ?Do vaccines
cause autism?? while other opinions do not, but
it will also show what support there is for each of
these viewpoints. So, STATEMENT MAP can help
user come to an informed conclusion.
2 Semantic Relations between
Statements
2.1 Recognizing Semantic Relations
To generate STATEMENT MAPs, we need to an-
alyze a lot of online information retrieved on a
given topic, and STATEMENT MAP shows users
a summary with three major semantic relations.
AGREEMENT to group similar opinions
CONFLICT to capture differences of opinions
EVIDENCE to show support for opinions
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment (RTE).
A major task of the RTE Challenge (Dagan et al,
2005) is the identification of [ENTAILMENT] or
[CONTRADICTION] between Text (T) and Hy-
pothesis (H). For this task, several corpora have
been constructed over the past few years, and an-
notated with thousands of (T,H) pairs.
While our research objective is to recognize se-
mantic relations as well, our target domain is text
from Web documents. The definition of contradic-
tion in RTE is that T contradicts H if it is very un-
likely that both T and H can be true at the same
time. However, in real documents on the Web,
there are many examples which are partially con-
tradictory, or where one statement restricts the ap-
plicability of another like in the example below.
(1) a. Mercury-based vaccines actually cause autism in
children.
150
!Mercury-based vaccine preservatives actually have caused autism in 
children. 
!It?s biologically plausible that the MMR vaccine causes autism. 
VACCINES CAUSE AUTISM 
!There is no valid scientific evidence that vaccines  
cause autism. 
!The weight of the evidence indicates that vaccines  
are not associated with autism. 
VACCINES DON?T CAUSE AUTISM 
!My son then had the MMR, and then when he was three he was 
diagnosed with autism. 
!He then had the MMR, and then when he was three he was  
diagnosed with autism. 
MY CHILD WAS DIAGNOSED WITH AUTISM 
RIGHT AFTER THE VACCINE 
!Vaccinations are given around the same time  
children can be first diagnosed. 
!The plural of anecdote is not data.  
ANECDOTES ARE NOT EVIDENCE 
[CONFLICT]!
[FOCUS]!
[EVIDENCE]!
[EVIDENCE]!
Query : Do vaccines cause autism?!
[CONFLICT]!
Figure 1: An example STATEMENT MAP for the query ?Do vaccines cause autism??
b. Vaccines can trigger autism in a vulnerable subset of
children.
While it is difficult to assign any relation to this
pair in an RTE framework, in order to construct
statement maps we need to recognize a contradic-
tion between (1a) and (1b).
There is another task of recognizing relations
between sentences, CST (Cross-Document Struc-
ture Theory) which was developed by Radev
(2000). CST is an expanded rhetorical structure
analysis based on RST (Mann and Thompson,
1988), and attempts to describe relations between
two or more sentences from both single and mul-
tiple document sets. The CSTBank corpus (Radev
et al, 2003) was constructed to annotate cross-
document relations. CSTBank is divided into clus-
ters in which topically-related articles are gath-
ered. There are 18 kinds of relations in this corpus,
including [EQUIVALENCE], [ELABORATION],
and [REFINEMENT].
2.2 Facts and Opinions
RTE is used to recognize logical and factual re-
lations between sentences in a pair, and CST is
used for objective expressions because newspa-
per articles related to the same topic are used as
data. However, the task specifications of both RTE
and CST do not cover semantic relations between
opinions and facts as illustrated in the following
example.
(2) a. There must not be a connection between vaccines
and autism.
b. I do believe that there is a link between vaccinations
and autism.
Subjective statements, such as opinions, are re-
cently the focus of many NLP research topics,
such as review analysis, opinion extraction, opin-
ion QA, or sentiment analysis. In the corpus con-
structed by the MPQA Project (Multi-Perspective
Question Answering) (Wiebe et al, 2005), indi-
vidual expressions are marked that correspond to
explicit mentions of private states, speech events,
and expressive subjective elements.
Our goal is to annotate instances of the three
major relation classes: [AGREEMENT], [CON-
FLICT] and [EVIDENCE], between pairs of state-
ments in example texts. However, each relation
has a wide range, and it is very difficult to define
a comprehensive annotation scheme. For exam-
ple, different kinds of information can act as clues
to recognize the [AGREEMENT] relations. So,
we have prepared a wide spectrum of semantic re-
lations depending on different types of informa-
tion regarded as clues to identify a relation class,
such as [AGREEMENT] or [CONFLICT]. Table 1
shows the semantic relations needed for carry-
ing out the anotation. Although detecting [EVI-
DENCE] relations is also essential to the STATE-
MENT MAP project, we do not include them in our
current corpus construction.
3 Constructing a Japanese Corpus
3.1 Targeting Semantic Relations Between
Statements
Real data on the Web generally has complex sen-
tence structures. That makes it difficult to rec-
ognize semantic relations between full sentences.
but it is possible to annotate semantic relation be-
tween parts extracted from each sentence in many
cases. For example, the two sentences A and B
in Figure 2 cannot be annotated with any of the
semantic relations in Table 1, because each sen-
tence include different types of information. How-
ever, if two parts extracted from these sentences C
and D are compared, the parts can be identified as
[EQUIVALENCE] because they are semantically
close and each extracted part does not contain a
different type of information. So, we attempt to
break sentences from the Web down into reason-
able text segments, which we call ?statements.?
When a real sentence includes several pieces of se-
151
Table 1: Definition of semantic relations and example in the corpus
Relation Class Relation Label Example
AGREEMENT
Equivalence A: The overwhelming evidence is that vaccines are unrelated to autism.B: There is no link between the MMR vaccine and autism.
Equivalent Opinion
A: We think vaccines cause autism.
B: I am the mother of a 6 year old that regressed into autism because of his 18
month vaccinations.
Specific A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines cause autism.
CONFLICT
Contradiction A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines don?t cause autism.
Confinement A: Vaccines can trigger autism in a vulnerable subset of children.B: Mercury-based vaccine actually have caused autism in children.
Conflicting Opinion A: I don?t think vaccines cause autism.B: I believe vaccines are the cause of my son?s autism.
According to Department 
of Medicine, there is no 
link between the MMR 
vaccine and autism.!
There is no link between the 
MMR vaccine and autism.!
The weight of the 
evidence indicates that 
vaccines are not 
associated with autism.!
Vaccines are not 
associated with autism.!
(A) Real sentence (1) (B) Real sentence (2)!
(C) Statement (1)! (D) Statement (2)!(E) [EQUIVALENCE]!
Figure 2: Extracting statements from sentences
and annotating a semantic relation between them
mantic segments, more than one statement can be
extracted. So, a statement can reflect the writer?s
affirmation in the original sentence. If the ex-
tracted statements lack semantic information, such
as pronouns or other arguments, human annota-
tors manually add the missing information. Fi-
nally we label pairs of statements with either one
of the semantic relations from Table 1 or with ?NO
RELATION,? which means that two sentences (1)
are not semantically related, or (2) have a relation
other than relations defined in Table 1.
3.2 Corpus Construction Procedure
We automatically gather sentences on related top-
ics by following the procedure below:
1. Retrieve documents related to a set number of
topics using a search engine
2. Extract real sentences that include major sub-
topic words which are detected based on TF or
DF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate.
Dolan and Brockett (2005) proposed a method
to narrow the range of sentence pair candidates
and collect candidates of sentence-level para-
phrases which correspond [EQUIVALENCE] in
[AGREEMENT] class in our task. It worked well
for collecting valid sentence pairs from a large
cluster which was constituted by topic-related sen-
tences. The method also seem to work well for
[CONFLICT] relations, because lexical similar-
ity based on bag-of-words (BOW) can narrow the
range of candidates with this relation as well.
We calculate the lexical similarity between the
two sentences based on BOW. We also used hy-
ponym and synonym dictionaries (Sumida et al,
2008) and a database of relations between predi-
cate argument structures (Matsuyoshi et al, 2008)
as resources. According to our preliminary exper-
iments, unigrams of KANJI and KATAKANA ex-
pressions, single and compound nouns, verbs and
adjectives worked well as features, and we calcu-
late the similarity using cosine distance. We did
not use HIRAGANA expressions because they are
also used in function words.
4 Analyzing the Corpus
Five annotators annotated semantic relations ac-
cording to our specifications in 22 document sets
as targets. We have annotated target statement
pairs with either [AGREEMENT], [CONFLICT]
or [NO RELATION]. We provided 2,303 real
sentence pairs to human annotators, and they
identified 1,375 pairs as being invalid and 928
pairs as being valid. The number of annotated
statement pairs are 1,505 ([AGREEMENT]:862,
[CONFLICT]:126, [NO RELATION]:517).
Next, to evaluate inter annotator agreement, 207
randomly selected statement pairs were annotated
by two human annotators. The annotators agreed
in their judgment for 81.6% of the examples,
which corresponds to a kappa level of 0.49. The
annotation results are evaluated by calculating re-
call and precision in which one annotation result
is treated as a gold standard and the other?s as the
output of the system, as shown in Talbe 2.
152
Table 2: Inter-annotator agreement for 2 annota-
tors
Annotator A
AGR. CON. NONE TOTAL
AGR. 146 7 9 162
Anno- CON. 0 13 1 14
tator B NONE 17 4 10 31
TOTAL 163 24 20 207
5 Discussion
The number of sentence pairs that annotators iden-
tified as invalid examples shows that around 60%
of all pairs were invalid, showing that there is still
room to improve our method of collecting sen-
tence pairs for the annotators. Developing more
effective methods of eliminating sentences pairs
that are unlikely to contain statements with plau-
sible relations is important to improve annotator
efficiency. We reviewed 50 such invalid sentence
pairs, and the results indicate two major consider-
ations: (1) negation, or antonyms have not been re-
garded as key information, and (2) verbs in KANJI
have to be handled more carefully. The polarities
of sentences in all pairs were the same although
there are sentences which can be paired up with
opposite polarities. So, we will consider the po-
larity of words and sentences as well as similarity
when considering candidate sentence pairs.
In Japanese, the words which consist of
KATAKANA expressions are generally nouns, but
those which contain KANJI can be nouns, verbs,
or adjectives. Sharing KATAKANA words was
the most common way of increasing the simi-
larity between sentences. We need to assign a
higher weight to verbs and adjectives that contain
KANJI, to more accurately calculate the similarity
between sentences.
Another approach to reducing the search space
for statement pairs is taken by Nichols et al
(2009), who use category tags and in-article hyper-
links to organize scientific blog posts into discus-
sions on the same topic, making it easier to iden-
tify relevant statements. We are investigating the
applicability of these methods to the construction
of our Japanese corpus but suffer from the lack of
a richly-interlinked data source comparable to En-
glish scientific blogs.
6 Conclusion
In this paper, we described the ongoing construc-
tion of a Japanese corpus consisting of statement
pairs annotated with semantic relations for han-
dling web arguments. We designed an annotation
scheme complete with the necessary semantic re-
lations to support the development of statement
maps that show [AGREEMENT], [CONFLICT],
and [EVIDENCE] between statements for assist-
ing users in analyzing credibility of information
in Web. We discussed the revelations made from
annotating our corpus, and discussed future direc-
tions for refining our specifications of the corpus.
We are planning to annotate relations for more
than 6,000 sentence pairs in this summer, and the
finished corpus will consist of around 10,000 sen-
tence pairs. The first release of our annotation
specifications and the corpus will be made avail-
able on the Web1 this winter.
Acknowledgments
This work is supported by the National Institute
of Information and Communications Technology
Japan.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Proc. of the PASCAL Challenges Workshop on RTE.
Bill Dolan and Chris Brockett. 2005. Automatical ly con-
structing a corpus of sentential paraphrases. In Proc. of
the IWP 2005, pages 9?16.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: towards a functional theory of text or-
ganization. Text, 8(3):243?281.
Suguru Matsuyoshi, Koji Murakami, Yuji Matsumoto, , and
Kentaro Inui. 2008. A database of relations between
predicate argument structures for recognizing textual en-
tailment and contradiction. In Proc. of the ISUC 2008.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka
Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information
credibility analysis by visualizing arguments. In Proc. of
the WICOW 2009, pages 43?50.
Eric Nichols, Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Constructing a scientific blog corpus for
information credibility analysis. In Proc. of the Annual
Meeting of ANLP.
Dragomir Radev, Jahna Otterbacher, and Zhu Zhang.
2003. CSTBank: Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Dragomir R. Radev. 2000. Common theory of informa-
tion fusion from multiple text sources step one: Cross-
document structure. In Proc. of the 1st SIGdial workshop
on Discourse and dialogue, pages 74?83.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa.
2008. Boosting precision and recall of hyponymy rela-
tion acquisition from hierarchical layouts in wikipedia. In
Proc. of the LREC 2008.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2-
3):165?210.
1http://cl.naist.jp/stmap/corpus/ja
153
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 1?4,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
WISDOM: A Web Information Credibility Analysis System 
Susumu Akamine?  Daisuke Kawahara?  Yoshikiyo Kato? 
Tetsuji Nakagawa?  Kentaro Inui?  Sadao Kurohashi??  Yutaka Kidawara? 
?National Institute of Information and Communications Technology 
? Graduate School of Informatics, Kyoto University 
{akamine, dk, ykato, tnaka, inui, kidawara}@nict.go.jp, kuro@i.kyoto-u.ac.jp 
 
 
 
 
Abstract 
We demonstrate an information credibility 
analysis system called WISDOM. The purpose 
of WISDOM is to evaluate the credibility of in-
formation available on the Web from multiple 
viewpoints. WISDOM considers the following 
to be the source of information credibility: in-
formation contents, information senders, and 
information appearances. We aim at analyzing 
and organizing these measures on the basis of 
semantics-oriented natural language processing 
(NLP) techniques. 
1. Introduction 
As computers and computer networks become 
increasingly sophisticated, a vast amount of in-
formation and knowledge has been accumulated 
and circulated on the Web. They provide people 
with options regarding their daily lives and are 
starting to have a strong influence on govern-
mental policies and business management. How-
ever, a crucial problem is that the information 
available on the Web is not necessarily credible. 
It is actually very difficult for human beings to 
judge the credibility of the information and even 
more difficult for computers. However, comput-
ers can be used to develop a system that collects, 
organizes, and relativises information and helps 
human beings view information from several 
viewpoints and judge the credibility of the in-
formation. 
Information organization is a promising en-
deavor in the area of next-generation Web search. 
The search engine Clusty provides a search result 
clustering1, and Cuil classifies a search result on 
the basis of query-related terms2. The persuasive 
technology research project at Stanford Universi-
ty discussed how websites can be designed to 
influence people?s perceptions (B. J. Fogg, 2003). 
However, as per our knowledge, no research has 
been carried out for supporting the human judg-
ment on information credibility and information 
organization systems for this purpose. 
In order to support the judgment of informa-
tion credibility, it is necessary to extract the 
background, facts, and various opinions and their 
                                                 
1 http://clusty.com/, http://clusty.jp/  
distribution for a given topic. For this purpose, 
syntactic and discourse structures must be ana-
lyzed, their types and relations must be extracted, 
and synonymous and ambiguous expressions 
should be handled properly.  
Furthermore, it is important to determine the 
identity of the information sender and his/her 
specialty as criteria for credibility, which require 
named entity recognition and total analysis of 
documents. 
In this paper, we describe an information cre-
dibility analysis system called WISDOM, which 
automatically analyzes and organizes the above 
aspects on the basis of semantically oriented 
NLP techniques. WISDOM currently operates 
over 100 million Japanese Web pages. 
2. Overview of WISDOM 
We consider the following three criteria for the 
judgment of information credibility.  
(1) Credibility of information contents,  
(2) Credibility of the information sender, and  
(3) Credibility estimated from the document 
style and superficial characteristics. 
In order to help people judge the credibility of 
information from these viewpoints, we have been 
developing an information analysis system called 
WISDOM. Figure 1 shows the analysis result of 
WISDOM on the analysis topic ?Is bio-ethanol 
good for the environment?? Figure 2 shows the 
system architecture of WISDOM. 
Given an analysis topic (query), WISDOM 
sends the query to the search engine TSUBAKI 
(Shinzato et al, 2008), and TSUBAKI returns a 
list of the top N relevant Web pages (N is usually 
set to 1000). 
Then, those pages are automatically analyzed, 
and major and contradictory expressions and eva-
luative expressions are extracted. Furthermore, 
the information senders of the Web pages, which 
were analyzed beforehand, are collected and the 
distribution is calculated. 
The WISDOM analysis results can be viewed 
from several viewpoints by changing the tabs 
using a Web browser. The leftmost tab, ?Sum-
mary,? shows the summary of the analysis, with 
major phrases and major/contradictory state-
ments first.  
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Query: ?Is bio-ethanol good for the environment?? Summary 
Figure 1. An analysis example of the information credibility analysis system WISDOM. 
 
 
Figure 2. System architecture of WISDOM. 
 
By referring to these phrases and statements, 
a user can grasp the important issues related to 
the topic at a glance. The pie diagram indicates 
the distribution of the information sender class 
spread over 1000 pages, such as company, indus-
try group, and government. The names of the 
information senders of the class can be viewed 
by placing the cursor over a class region. The last 
bar chart shows the distribution of positive and 
negative opinions related to the topic spread over 
1000 pages, for all and for each sender class. For 
example, with regard to ?Bio-ethanol,? we can 
see that the number of positive opinions is more 
than that of negative opinions, but it is the oppo-
site in the case of some sender classes. Several 
display units in the Summary tab are cursor sen-
sitive, providing links to more detailed informa-
tion (e.g., the page list including a major state-
Sender 
Opinion 
Search Result Major/Contradictory Expressions
2
ment, the page list of a sender class, and the page 
list containing negative opinions). 
The ?Search Result? tab shows the search re-
sult by TSUBAKI, i.e., ranking the relevant pag-
es according to the TSUBAKI criteria. The ?Ma-
jor/Contradictory Expressions? tab shows the list 
of major phrases and major/contradictory state-
ments about the given topic and the list of pages 
containing the specified phrase or statement. The 
?Opinion? tab shows the analysis result of the 
evaluative expressions, classified according to 
for/against, like/dislike, merit/demerit, and others, 
and it also shows the list of pages containing the 
specified type of evaluative expressions. The 
?Sender? tab classifies the pages according to the 
class of the information sender, for example, a 
user can view the pages created only by the gov-
ernment.  
Furthermore, the superficial characteristics of 
pages called as information appearance are ana-
lyzed beforehand and can be viewed in WIS-
DOM, such as whether or not the contact address 
is shown in the page and the privacy policy is on 
the page, the volume of advertisements on the 
page, the number of images, and the number of 
in/out links. 
As shown thus far, given an analysis topic, 
WISDOM collects and organizes the relevant 
information available on the Web and provides 
users with multi-faceted views. We believe that 
such a system can considerably support the hu-
man judgment of information credibility. 
3. Data Infrastructure  
We usually utilize 100 million Japanese Web 
pages as the analysis target. The Web pages have 
been converted into the standard formatted Web 
data, an XML format. The format includes sever-
al metadata such as URLs, crawl dates, titles, and 
in/out links. A text in a page is automatically 
segmented into sentences (note that the sentence 
boundary is not clear in the original HTML file), 
and the analysis results obtained by a morpholog-
ical analyzer, parser, and synonym analyzer are 
also stored in the standard format. Furthermore, 
the site operator, the page author, and informa-
tion appearance (e.g., contact address, privacy 
policy, volume of advertisements, and images) 
are automatically analyzed and stored in the 
standard format. 
4. Extraction of Major Expressions and 
Their Contradictions 
For the organization of information contents, 
WISDOM extracts and presents the major ex-
pressions and their contradictions on a given 
analysis topic (Kawahara et al, 2008). Major 
expressions are defined as expressions occurring 
at a high frequency in the set of Web pages on 
the analysis topic. They are classified into two: 
noun phrases and predicate-argument structures 
(statements). Contradictions are the predicate-
argument structures that contradict the major ex-
pressions. For the Japanese phrase yutori kyouiku 
(cram-free education), for example, tsumekomi 
kyouiku (cramming education) and ikiru chikara 
(life skills) are extracted as the major noun 
phrases; yutori kyouiku-wo minaosu (reexamine 
cram-free education) and gakuryokuga teika-suru 
(scholastic ability deteriorates), as the major pre-
dicate-argument structures; and gakuryoku-ga 
koujousuru (scholastic ability ameliorates), as its 
contradiction. This kind of summarized informa-
tion enables a user to grasp the facts and argu-
ments on the analysis topic available on the Web. 
We use 1000 Web pages for a topic retrieved 
from the search engine TSUBAKI. Our method 
of extracting major expressions and their contra-
dictions consists of the following steps: 
1. Extracting candidates of major expressions: 
The candidates of major expressions are ex-
tracted from each Web page in the search result. 
From the relevant sentences to the analysis topic 
that consist of approximately 15 sentences se-
lected from each Web page, compound nouns, 
parenthetical expressions, and predicate-
argument structures are extracted as the candi-
dates of the major expressions. 
2. Distilling major expressions: 
Simply presenting expressions at a high fre-
quency is not always information of high quality. 
This is because scattering synonymous expres-
sions such as karikyuramu (curriculum) and 
kyouiku katei (course of study) and entailing ex-
pressions such as IWC and IWC soukai (IWC 
plenary session), all of which occur frequently, 
hamper the understanding process of users. Fur-
ther, synonymous predicate-argument structures 
such as gakuryoku-ga teika-suru (scholastic 
ability deteriorates) and gakuryoku-ga sagaru 
(scholastic ability lowers) have the same problem. 
To overcome this problem, we distill major ex-
pressions by merging spelling variations with 
morphological analysis, merging synonymous 
expressions automatically acquired from an ordi-
nary dictionary and the Web, and merging ex-
pressions that can be entailed by another expres-
sion. 
3. Extracting contradictory expressions: 
Predicate-argument structures that negate the 
predicate of major ones and that replace the pre-
dicate of major ones with its antonym are ex-
tracted as contradictions. For example, gakuryo-
ku-ga teika-shi-nai (scholastic ability does not 
deteriorate) and gakuryokuga koujou-suru (scho-
lastic ability ameliorates) are extracted as the 
contradictions to gakuryoku-ga teikasuru (scho-
lastic ability deteriorates). This process is per-
formed using an antonym lexicon, which consists 
of approximately 2000 pairs; these pairs are ex-
tracted from an ordinary dictionary. 
5. Extraction of Evaluative Information 
The extraction and classification of evaluative 
information from texts are important tasks with 
3
many applications and they have been actively 
studied recently (Pang and Lee, 2008). Most pre-
vious studies on opinion extraction or sentiment 
analysis deal with only subjective and explicit 
expressions. For example, Japanese sentences 
such as watashi-wa apple-ga sukida (I like ap-
ples) and kono seido-ni hantaida (I oppose the 
system) contain evaluative expressions that are 
directly expressed with subjective expressions. 
However, sentences such as kono shokuhin-wa 
kou-gan-kouka-ga aru (this food has an anti-
cancer effect) and kono camera-wa katte 3-ka-de 
kowareta (this camera was broken 3 days after I 
bought it) do not contain subjective expressions 
but contain negative evaluative expressions. 
From the viewpoint of information credibility, it 
appears important to deal with a wide variety of 
evaluative information including such implicit 
evaluative expressions (Nakagawa et al, 2008). 
A corpus annotated with evaluative informa-
tion was developed for evaluative information 
analysis studies. Fifty topics such as ?Bio-
ethanol? and ?Pension plan? were chosen. For 
each topic, 200 sentences containing the topic 
word were collected from the Web to construct 
the corpus totaling 10,000 sentences. For each 
sentence, annotators judged whether or not the 
sentence contained evaluative expressions. When 
evaluative expressions were identified, the evalu-
ative expressions, their holders, their sentiment 
polarities (positive or negative), and their relev-
ance to the topic were annotated. 
We developed an automatic analyzer of evalu-
ative information using the corpus. We per-
formed experiments of sentiment polarity classi-
fication using Support Vector Machines. Word 
forms, POS tags, and sentiment polarities from 
an evaluative word dictionary of all the words in 
evaluative expressions were used as features, and 
an accuracy of 83% was obtained. From the error 
analysis, we found that it was difficult to classify 
domain-specific evaluative expressions; we are 
now planning the automatic acquisition of evalu-
ative word dictionaries. 
6. Information Sender Analysis 
The source of information (or information sender) 
is one of the important elements when judging the 
credibility of information. It is rather easy for human 
beings to identify the information sender of a Web 
page. When reading a Web page, whether it is deli-
berate or not, we attribute some characteristics to the 
information sender and accordingly form our atti-
tudes toward the information. However, the state-of-
the-art search engines do not provide facilities to 
organize a vast amount of information on the basis 
of the information sender. If we can organize the 
information on a topic on the basis of who or what 
type the information sender is, it would enable the 
user to grasp an overview of the topic or to judge the 
credibility of relevant information. 
WISDOM automatically identifies the site op-
erators of Web pages and classifies them into 
predefined categories of information sender 
called information sender class. A site operator 
of a Web page is the governing body of a website 
on which the page is published. The information 
sender class categorizes the information sender 
on the basis of axes such as individuals vs. or-
ganizations and profit vs. nonprofit organizations. 
The list below shows the categories of informa-
tion sender class. 
 
 
 
1. Organization (cont?d) 
  (c) Press 
    i. Broadcasting Station 
    ii. Newspaper 
    iii. Publisher 
2. Individual 
  (a) Real Name 
  (b) Anonymous,  
Screen Name 
 
1. Organization 
  (a) Profit Organization 
    i. Company 
    ii. Industry Group 
  (b) Nonprofit Organization 
    i. Academic Society 
    ii. Government 
    iii. Political Organization 
    iv. Public Service Corp., 
         Nonprofit Organization 
    v. University 
    vi. Voluntary Association 
   vii. Education Institution
WISDOM allows the user to organize the in-
formation on the basis of the information sender 
class assigned to each Web page. Technical de-
tails of the information sender analysis employed 
in WISDOM can be found in (Kato et al, 2008). 
7. Conclusions 
This paper has described an information analy-
sis system called WISDOM. As shown in this pa-
per, WISDOM already provides a reasonably nice 
organized view for a given topic and can serve as a 
useful tool for handling informational queries and 
for supporting human judgment of information 
credibility. WISDOM is freely available at 
http://wisdom-nict.jp/.  
References 
B. J. Fogg. 2003. Persuasive Technology: Using Com-
puters to Change What We Think and Do (The Mor-
gan Kaufmann Series in Interactive Technologies). 
Morgan Kaufmann. 
K. Shinzato, T. Shibata, D. Kawahara, C. Hashimoto, 
and S. Kurohashi 2008. TSUBAKI: An open search 
engine infrastructure for developing new information 
access methodology. In Proceedings of IJCNLP2008. 
D. Kawahara, S. Kurohashi, and K. Inui 2008. Grasping 
major statements and their contradictions toward in-
formation credibility analysis of web contents. In 
Proceedings of  WI?08. 
B. Pang and L. Lee 2008. Opinion mining and senti-
ment analysis, Foundations and Trends in Informa-
tion Retrieval, Volume 2, Issue 1-2, 2008. 
T. Nakagawa, T. Kawada, K. Inui, and S. Kurohashi 
2008. Extracting subjective and objective evaluative 
expressions from the web. In Proceedings of 
ISUC2008. 
Y. Kato, D. Kawahara, K. Inui, S. Kurohashi, and T. 
Shibata 2008. Extracting the author of web pages. In 
Proceedings of WICOW2008. 
4
Committee-based Decision Making 
in Probabilistic Partial Parsing 
INU I  Takashi*  and INUI  Kentaro  *t 
? Department  of Artificial Intelligence, Kyushu Inst itute of Technology 
? PRESTO,  Japan Science and Technology Corporat ion 
{ t_ inu i ,  inu i}@plut  o. a i .  kyutech ,  ac.  jp  
Abst ract  
This paper explores two directions ibr the 
next step beyond the state of the art of statis- 
tical parsing: probabilistic partial parsing and 
committee-based decision making. Probabilis- 
tic partial parsing is a probabilistic extension of 
the existing notion of partial parsing~ which en- 
ables fine-grained arbitrary choice on the trade- 
off between accuracy and coverage. Committee- 
based decision making is to combine the out- 
puts from different systems to make a better 
decision. While varions committee-based tech- 
niques for NLP have recently been investigated, 
they would need to be fln'ther extended so as 
to be applicable to probabilistic partial pars- 
ing. Aiming at this coupling, this paper gives 
a general fl'amework to committee-based deci- 
sion making, which consists of a set of weight- 
ing flmctions and a combination function, and 
discusses how it can be coupled with probabilis- 
tic partial parsing. Our ext)eriments have so far 
been producing promising results. 
1 In t roduct ion  
There have been a number of attempts to use 
statistical techniques to improve parsing perfor- 
mance. While this goal has been achieved to a 
certain degree given the increasing availability 
of large tree banks, the remaining room tbr the 
improvement appears to be getting saturated 
as long as only statistical techniques are taken 
into account. This paper explores two directions 
tbr the next step beyond the state of the art of 
statistical parsing: probabilistic partial parsing 
and committee-based decision making. 
Probabilistic partial parsing is a probabilistic 
extension of the existing notion of partial pars- 
ing ( e.g. (Jensen et al, 1993)) where a parser 
selects as its output only a part of the parse tree 
that are probabilistically highly reliable. This 
decision-making scheme enables a fine-grained 
arbitrary choice on the trade-off between ac- 
curacy and coverage. Such trade-oil is impor- 
tant since there are various applications that re- 
quire reasonably high accuracy even sacrificing 
coverage. A typical example is the t)araI)hras- 
ing task embedded in summarization, sentence 
simplification (e.g. (Carroll et al, 1998)), etc. 
Enabling such trade-off" choice will make state- 
o f  the-art parsers of wider application. Partial 
parsing has also been proven useflll ibr boot- 
strapping leanfing. 
One may suspect hat the realization of par- 
tial parsing is a trivial matter in probabilistic 
parsing just because a probabilistic parser in- 
herently has the notion of "reliability" and thus 
has the trade-off:' between accuracy and cover- 
age. However, there has so far been surpris- 
ingly little research focusing on this matter and 
ahnost no work that evaluates statistical parsers 
according to their coverage-accuracy (or recall- 
precision) curves. Taking the significance of 
partial parsing into account, therefi)re in this 
paper, we evaluate parsing perfbrmance accord- 
ing tO coverage-accuracy cnrves. 
Committee-based decision making is to con> 
bine the outputs from several difl'erent systems 
(e.g. parsers) to make a better decision. Re- 
cently, there have been various attempts to at)- 
ply committee-based techniques to NLP tasks 
such as POS tagging (Halteren et al, 1998; 
Brill et al, 1998), parsing (Henderson and 
Brill, 1999), word sense disambiguation (Peder- 
sen, 2000), machine translation (lh'ederking and 
Nirenburg, 1994), and speech recognition (Fis- 
cus, 1997). Those works empirically demon- 
strated that combining different systems often 
achieved significant improvelnents over the pre- 
vious best system. 
In order to couple those committee-based 
348 
schemes with t)robat)ilistic t)artial parsing, how- 
ever, Olle would still need to make a fllrther ex- 
tension. Ainling at this coupling, ill this t)at)er, 
we consider a general framework of (:ommil, tee- 
based decision making that consists of ~ set 
of weighting flmctions mid a combination flmc- 
tion, and (lis('uss how that Kalnework enal)les 
the coupling with t)robal)ilistic t)artial t)m:sing. 
To denionstr~te how it works, we ret)ort the re- 
sults of our t)arsing exl)eriments on a Japanese 
tree bank. 
2 Probabilistic partial parsing 
2.1 Dependency  probab i l i ty  
In this t)at)er, we consider the task of (le(:id- 
ing the det)endency structure of a Jat);mese in- 
put sentence. Note that, while we restrict ore: 
discussion to analysis of Jat)anese senl;(;nc(;s in 
this t)~l)er, what we present l)elow should also 
t)e strnightfi?rwardly ?xt)plical)h~ to more wide- 
ranged tasks such as English det)endency anal- 
ysis just  like the t)roblem setting considered t)y 
Collins (1996). 
Givell ;m inl)ut sentence ,s as a sequence, of 
B'unset,su-t)hrases (BPs) J, lq b2 . . .  lh~, our task 
is to i(tent, i\[y their inter-BP del)endency struc- 
t , ,e  n = l,j)l,: = ',,,}, where 
(tenot;es that bi (let)on(Is on (or modities) bj. 
Let us consider a dependency p'roba, bility (I)P): 
P('r(bi, bj)l.s'), a t)rol)al)ility l;lu~t 'r(bi, b:j) hohts 
in a Given senl:ence s: Vi. E j  P(','(51, t , j ) l .4 = a. 
2.2  Es t imat ion  o f  DPs  
Some of the state-of:the-art 1)rol)at)ilis(;ic bm- 
guage inodels such as the l)ottomu t) models 
P(l~,l.,.) propos,,d by Collins (1:)96) and Fujio 
et al (1998) directly est imate DPs tbr :~ given 
int)ut , whereas other models su('h as PCFO-  
t)ased tel)down generation mod(;ls P(H,,,s) do 
not, (Charnink, 1997; Collins, 1997; Shir~fi et ~rl., 
1998). If the latter type of mod(,'ls were total ly 
exchlded fronl any committee, our commit;tee- 
based framework would not work well in I)rac- 
lice. Fortm:ately, how(:ver, even tbr such a 
model, one can still est imate l)l?s in the follow- 
ing way if the rood(;1 provides the n-best del)en- 
1A bunsctsu phrase (BP) is a chunk of words (-on- 
sist;ing of a content word (noun, verl), adjective, etc.) 
accoml)mfied by sonic flmctional word(s) (i)arti(:le, mlx- 
iliary, etc.). A .lai)anes(' sentc'nce can 1)c analyzed as a 
sequence of BPs, which constitutes an inter-BP deI)en- 
dency structure 
dency structure candidates cout)led with prot)- 
abilistic scores. 
Let Ri be the i-th best del)endency st;ruct;ure 
(i = 1 , . . . ,  'n) of ;~ given input ,s' according to a 
given model, and h;t ~H l)e a set; of H,i. Then, 
,.,u, l,e csl;ima|;ed by the following 
ai)l)l"OXilnation equation: 
./)F 
? 7~H P(',(b,z, (1) 
where P'R.u is the probal)ilit;y mass of H, E 7~Lr, 
and prn. is the probabi l i ty mass of R ~ ~H 
that suppori;s 'r(bi, bj). Tile approximation er- 
ror c is given 1)y c < l;r~--1%, where l),p,, is 1;t2(; 
- -  l~p~ ' 
prol)abilil;y mass of all the dependency struc- 
ture candidates for s (see (Peele, 1993) for the 
l?roof). This means that the al)t)roximation er- 
ror is negligil)le if P'R,, is sut\[iciently close to 
1),R, which holds for a reasonably small mlmt)er 
'n in lnOSt cases in practical statistical parsing. 
2.3 Coverage-accuracy  curves  
We then conside, r the task of selecting depen- 
dency relations whose est imated probabi l i ty is 
higher I:han a (:e|:i;ain l;hreshoht o- (0 < a < 1). 
When (r is set 1;o be higher (closer to 1.0), t;he 
accuracy is cxt)ected to become higher, while 
the coverage is ext)ecl;ed to become lowe,:, and 
vi(:e versm Here, (;over~ge C* and a,(;ctlra(;y A 
are defined as follows: 
# of the. decided relations 
C 
# of nil the re, lations in I;\]le t;est so,}i2 )/~ 
# of the COl'rectly decided relati?n~3~vJ A 
# of the decided relations 
Moving the threshohl cr from 1.0 down to- 
ward 0.0, one (:an obtain a coverage-a(:cura(:y 
(:urve (C-A curve). In 1)rol)al)ilistic t)artial pars- 
ing, we ewflunte the t)erforman('e of a model 
~mcording to its C-A curve. A few examt)les 
are shown in Figure 1, which were obtained 
in our ext)erim(mt (see Section 4). Ot)viously, 
Figure 1 shows that model A outt)erformed the 
or, her two. To summarize a C-A cIlrve, we use 
the l l -t)oint average of accuracy (l l-t)oint at:- 
curacy, hereafl;er), where the eleven points m'e 
C = 0.5, 0 .55 , . . . ,  1.0. The accuracy of total 
parsing correst)onds to the accuracy of the t)oint 
in a C-A curve where C = 1.0. We call it total 
~ccuracy to distinguish it from l\]- l)oint at:el> 
racy. Not;('. that two models with equal achieve- 
349 
! 
A 
0.95 
0.9 
0.85  
0 .8  
0 0 .2  0 .4  0 .6  0 .8  1 
coverage 
Figure 1: C-A curves 
meuts in total accuracy may be different in l l -  
point accuracy. In fact, we found such cases in 
our experiments reported below. Plotting C-A 
curves enable us to make a more fine-grained 
perfbrmance valuation of a model. 
3 Committee-based probabilis- 
tic partial parsing 
We consider a general scheme of comnfittee- 
based probabilistic partial parsing as illustrated 
in Figure 2. Here we assume that each connnit- 
tee member M~ (k = 1 , . . . ,  m) provides a DP 
matrix PM~(r(bi, bj)ls ) (bi, bj E s) tbr each in- 
put 8. Those matrices are called inlmt matrices, 
and are give:: to the committee as its input. 
A committee consists of a set of weighting 
functions and a combination flmction. The role 
assigned to weighting flmctions is to standardize 
input matrices. The weighting function associ- 
ated with model Mk transforms an input ma-  
trix given by MI~ to a weight matrix WaG- The 
majority flmction then combines all the given 
weight matrices to produce an output matrix O, 
which represents the final decision of the con> 
mittee. One can consider various options for 
both flmctions. 
3.1 Weight ing  funct ions  
We have so far considered the following three 
options. 
S imple  The simplest option is to do nothing: 
~a~ = PA~ (,.(b~, bj)l~) (4) ij 
o Mk where wij is the ( i , j )  element of I/VMk. 
Normal  A bare DP may not be a precise es- 
timation of the actual accuracy. One can see 
this by plotting probability-accuracy curves (P- 
A curves) as shown in Figure 3. Figure 3 shows 
that model A tends to overestimate DPs, while 
/ 
O}lll l l l \[ttct ~ based  dec is ion  nlakil l l~ 
i 
i " i - -  =" ,, 
models  it lpl lt  i weight matrices i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  : 
matrices 
"tV l : :  "~Vt~iglt|iJI g \]"u n ?1\]Oll 
CF: Ct3mhinat lan I :mlc l \ [ .n 
Figure 2: Committee-based probabilistic partial 
parsing 
0,9 
;,~ 
0,8 
= 
0.7 
0.6 
C 
0.5 
0,5 0.6 0.7 0.8 0.9 
dependency  probabi l i ty  
Figure 3: P-A curves 
model C tends to underestimate DPs. This 
lneans that if A and C give different answers 
with the same DP, C's answer is more likely 
to be correct. Thus, it is :sot necessarily a 
good strategy to simply use give:: bare DPs in 
weighted majority. To avoid this problem, we 
consider the tbllowing weighting flmction: 
w~J k =@lkAM~(PMk(','(bi, b:i)l.s)) (5) 
where AMk (P) is the function that returns the 
expected accuracy of Mk's vote with its depen- 
Mk dency probability p, and oz i is a normalization 
factor. Such a function can be trained by plot- 
ting a P-A curve fbr training data. Note that 
training data should be shared by all the com- 
mittee members. In practice, tbr training a P-A 
curve, some smoothing technique should be ap- 
plied to avoid overfitting. 
C lass The standardization process in the 
above option Normal  can also be seen as an 
effort for reducing the averaged cross entropy 
of the model on test, data. Since P-A curves 
tend to defi~,r not only between different mod- 
els but also between different problem classes, 
if one incorporates ome problem classification 
into (5), the averaged cross entropy is expected 
350 
to be reduced fllrther: 
'w~  =/~';'~AM.%(i)M~(r(b~,bj)l,s)) (6) 
where AMkcl, i (P) is the P-A curve of model Mk 
only tbr the problems of class Cb~ in training 
data, and flMk is a normalization factor. For i 
probleln classification, syntactie/lexieal features 
of bi may be useful. 
3.2  Combin ing  funct ions  
For combination flmctions, we have so far con- 
sidered only simple weighted voting, which av- 
erages the given weight matrices: 
1;I, 
Mk 1 v-" Mk 
? = - -  2_, ~'J'U (7) ?iJ 'm, 
h=l 
where o.i.f/~:_ is the (i, j) element of O. 
Note that the committee-based partial pars- 
ing frmnework t)resented here can be see, n as 
a generalization of the 1)reviously proposed 
voting-based techniques in the following re- 
spects: 
(a) A committee a(:(:epts probabilistically para- 
meterized votes as its intmt. 
(d) A committee ac(:el)ts multil)le voting (i.e. it; 
allow a comnfittee menfl)er to vote not only 
to the 1)est-scored calMi(late trot also to all 
other potential candidates). 
((:) A. (:ommittee 1)rovides a metals tbr standard- 
izing original votes. 
(b) A committee outl)uts a 1)rot)abilisti(" distri- 
bution representing a tinal decision, which 
constitutes a C-A curve. 
For examt)le, none of simple voting techniques 
for word class tagging t)roposed 1)y van Hal- 
teren et al (1998) does not accepts multiple 
voting. Henderson and Brill (1999) examined 
constituent voting and naive Bayes classifi(:a- 
lion for parsing, ol)taining positive results ibr 
each. Simple constituent voting, however, does 
not accept parametric votes. While Naive Bayes 
seems to partly accept l)arametric multit)le vot- 
ing, it; does not consider either sl;andardization 
or coverage/accuracy trade-off. 
4 Exper iments  
4.1  Set t ings  
We conducted eXl)erinmnts using the tbllow- 
ing tive statistical parsers: 
Table 1: The total / l l-t)oint accuracy achieved 
1)y each individual model 
total 11-point 
A 0.8974 0.9607 
B 0.8551 0.9281 
C 0.8586 0.9291 
D 0.8470 0.9266 
E 0.7885 0.8567 
? KANA (Ehara, 1998): a bottom-up model 
based oll maxinmm entropy estimation, 
Since dependency score matrices given by 
KANA have no probabilistic semantics, we 
normalized them tbr each row using a cer- 
tain function manually tuned for this parser. 
? CI\]AGAKE (Fujio et al, 1998): an exten- 
sion of the bottom-up model proposed by 
Collins (Collins, 1996). 
? Kanaymna's parser (Kanayama et al, 1999): 
a l)o|,tom-up model coupled with an HPSG. 
? Shirai's parser (Shirai et al, 1998): a top- 
down model incorporating lexical collocation 
statistics. Equation (1) was used tbr estimat- 
ing DPs. 
? Peach Pie Parser (Uchilnoto et al, 1999): 
a bottom-up model based on maximum en- 
tropy estimation. 
Note that these models were developed flfily 
independently of ea('h other, and have siglfifi- 
Calltly different (:haracters (Ii)r a comparison of 
their performance, see %tble 1). In what Jbl- 
lows, these models are referred to anonymously. 
For the source of the training/test set, we 
used the Kyoto corpus (ver.2.0) (Kurohashi et 
al., 1.997), which is a collection of Japanese 
newspaper articles mmotated in terms of word 
boundaries, POS tags, BP boundaries, and 
inter-BP dependency relations. The corpus 
originally contained 19,956 sentences. To make 
the training/test sets~ we tirst removed all the 
sentences that were rejected by any of the above 
five parsers (3,146 sentences). For the remain- 
ing 16,810 sentences, we next checked the con- 
sistency of the BP boundaries given by the 
parsers since they had slightly different crite- 
ria tbr BP segmentation fl'om each other. In 
this process, we tried to recover as many in- 
consistent boundaries as possible. For example, 
we tbund there were quite a few cases where 
a parser recoglfized a certain word sequence ms 
a single BP, whereas ome other parser recog- 
nized the same sequence as two BPs. In such 
351 
0.965 
\[ASimple DNormal  \ [ \ ]C lass  
0.96 
0.955 
0.95 
A : 0.9607 \ \ 
na ?o ~ ~0 
0.975 \[ 
e eg  
Figure 4: l l -po int  accuracy: A included 
0.96 ', \[.~Normal mClass  
0.95 
0.94 
0.93 
0.92 
9291 f 
5 g a0 
Figure 5: l 1-point accuracy: B /C  included 
a case, we regarded that  sequence as a single 
BP under a certain condition. An a result;, we 
obtained 13,990 sentences that  can be accepted 
by all the parsers with all the BP boundaries 
consistent 2 We used thin set tbr training and 
evaluation. 
For cloned tests, we used 11,192 sentences 
(66,536 BPs a) for both  training and tests. 
For open tests, we conducted five-fold cross- 
val idation on the whole sentence set. 
2In the BP concatenation process described here, 
quite a few trivial dependency relations between eigl,- 
boring BPs were removed from the test set. This made 
our test set slightly more difficult tlmn what it should 
have 1)cert. 
3This is the total nmnber of BPs excluding the right- 
most two BPs for each sentence. Since, in Jal)anese, a 
BP ahvays depends on a BP following it, the right-most 
BP of a sentence does not (lei)(tnd on any other BP, and 
the second right-most BP ahvays depends on the right- 
most BP. Therefore, they were not seen as subjects of 
evahmtion. 
0.97 
DSimple  \[21Normal mClass  
0.965 
0.97 
0.96 
0.955 
Figure 6: l l -po in t  accuracy: +KNP 
For the classification of problems, we man- 
ually established the following twelve (:lasses, 
each of which is defined in terms of a certain 
nlol:phological pat tern  of depending BPs: 
1.. nonfinal BP wit, h a case marker "'wa (topic)" 
2. nominal BP with a case marker "no (POS)" 
3. nominal BP with a case marker "ga (NOM)" 
4. nominal BP with a case marker % (ACC)" 
5. nonlinal BP with a case marker "hi (DAT)" 
6. nominal BP with a case marker "de (LOC/. . . )"  
nominal BP (residue) 
adnominal verbal BP 
verbal BP (residue) 
adverb 
adjective 
residue 
4.2 Resu l ts  and d iscuss ion 
Table 1 shown the to ta l / l l -po in t  accuracy of 
each individual model. The  performance of each 
model widely ranged from 0.96 down to 0.86 
in l l -po int  accuracy. Remember  that  A is the 
opt imal  model, and there are two second-best 
models, B and C, which are closely comparable.  
In what tbllows, we use these achievements ms 
the baseline for evaluating the error reduct ion 
achieved by organizing a committee. 
The pertbrmanee of various committees is 
shown in Figure 4 and 5. Our pr imary inter- 
est here is whether  the weighting functions pre- 
sented above effectively contr ibute to error re- 
duction. According to those two figures, al- 
though the contr ibut ion of the f lmction Nor -  
ma l  were nor very visible, the flmction C lass  
consistently improved the accuracy. These re- 
sults can be a good evidence tbr the impor tant  
role of weighting f lmctions in combining parsers. 
7. 
8. 
9. 
1(). 
11. 
12. 
352 
0.96 
0.94 
0.92 t ~ t I I ,  \[iJ 
Figure 7: Single voting vs. Multiple voting 
While we manually tmill: the 1)roblem classiti('a- 
l;ion in our ext)erimen|;, autom;~I;ic (:lassitication 
te.chniques will also 1)e obviously worth consid- 
ering. 
We l;hen e.on(tucted another exl)e, rime.nI; to ex- 
amine the, et\['e(-l;s of muli;it)le voting. One (:an 
sl;raighi;forwardly sinn|late a single-voting com- 
nlil;tee by ret)lacing wij in equal;ion (7) with w~. i 
given by: 
, { wi.i (if' j = m'g m~xk 'wit~) 
=_ 0 (o|;he.wise) (S) 
The resull;s are showll in Figure 7, which 
corot)ares l;he original multi-voting committees 
and l;he sinmlai;e(t single-voi:ing (:olmnil;l;ees. 
Clearly, in our se|;tings, multil)le voting signif- 
icanl;ly oul;pertbrmed single vol;ing 1)arti(:ul~rly 
when t;he size of a ('ommii;tee is small. 
The nexl; issues are whel;her ~ (:Omlnil;te,(', al- 
ways oul;perform its indivi(tmd memt)ers, mtd if 
not;, what should be (-onsidered in organizing a 
commii;i;ee. Figure 4 and 5 show |;hal; COllllllil;- 
tees nol; ilmlu(ling t;he ot)timal model A achieved 
extensive imt)rovemenl;s, whereas the merit of 
organizing COlmnitl;ees including A is not very 
visible. This can be t)arl, ly attrilml;ed to the 
fa.ct that the corot)el;once of the, individual mem- 
l)ers widely diversed, and A signiti(:md;ly OUtl)er- 
forms the ol:her models. 
Given l,he good error reduct;ion achieved 
by commit, tees containing comt)ar~ble meml)ers 
sueh ~s BC, BD a, nd B@I), however, it should t)e 
reasonable 1;o eXl)ect thai; a (:omlnil,l,e,e includ- 
ing A would achieve a significant imt)rovement; if 
anol;her nearly ol)t;ilnal model was also incorl)o- 
0.8 
v 
0,7 
0.fi 
0.5 0.6 |1,7 {).g 0.9 
dependency probability 
Figure 8: P-A curves: +KNP 
rated. To empirically prove this assmnpl;ion, we, 
conduct;ed anot;her experiment, where we add 
another parser KNP (Kurohashi el; al., 1 !)94:) 1;o 
each commil;|;ee that apt)ears in Figure 4. KNI? 
is much closer to lnodel A in l;ol;al accuracy 
t;han t;he other models (0.8725 in tol;al accu- 
racy). However, il; does not provide. DP rea- 
l;rices since it is designed in a rule-l)ased fash- 
ion the current; version of KNP 1)rovides only 
the t)esl;-t)referrext parse t;ree for ea(:h inl)Ul; sen- 
tence without ~my scoring annotation. We l;hus 
let KNP 1;o simply vol;e its l;ol;al aeem:aey. Tim 
results art; shown in lqgure 6. This time all l;he 
commil;tees achieved significant improvemenl;s, 
wil;h |;he m~ximum e, rror re(hu:|;ion rate up l;o 
'3~%. 
As suggested 1)y |;he. re, suits of t;his exl)erimenl; 
with KNP, our scheme Mlows a rule-based 11011- 
t)~r;m,el:ric p~rse.r t;o pb~y in a eommil;l;e.e pre- 
serving it;s ~d)ilit:y t;o oui;t)ul; t)aralnel;rie I)P ma- 
(;ri(:es. To 1)ush (;he ~u'gumen(; fl,rl;her, SUl)pose 
;~ 1)lausil)le sil;ual;ion where we have ;m Ol)l;imal 
l)ut non-1)arametrie rule-based parser and sev- 
eral suboptimal si;atistical parsers. In su('h ~ 
case, our commil;teeA)ased scheme may t)e able 
l;o organize a commi|,tee that can 1)rovide l)P 
lnatri(:es while preserving the original tol;al ac- 
curacy of the rule-b~sed parser. To set this, we 
conducted another small experiment, where, we 
combined KNP with each of C and D, 1)oth of 
whi(:h are less compe.tent than KNP. The result- 
ing (:ommil;l;ees successflflly t)rovided reasonal)le 
P-A curves as shown in Figure 8, while even 
further lint)roving the original |;ol;al at:curacy of 
KNP (0.8725 to 0.8868 tbr CF and 0.8860 for 
DF). Furthermore, t;he COmlnittees also gained 
the 11-point accuracy over C and D (0.9291 to 
353 
0.9600 tbr CF and 0.9266 to 0.9561 for DF). 
These. results suggest hat our committee-based 
scheme does work even if the most competent 
member of a committee is rule-based and thus 
non-parametric. 
5 Conclusion 
This paper presented a general committee- 
based frmnework that can be coupled with prob- 
abilistic partial parsing. In this framework, a 
committee accepts parametric multiple votes, 
and then standardizes them, and finally pro- 
vides a probabilistic distribution. We presented 
a general method for producing probabilistic 
multiple votes (i.e. DP matrices), which al- 
lows most of the existing probabilistic models 
for parsing to join a committee. Our experi- 
ments revealed that (a) if more than two compa- 
rably competent models are available, it is likely 
to be worthwhile to combine them, (b) both 
multit)le voting and vote standardization effec- 
tively work in committee-based partial parsing, 
(c) our scheme also allows a non-parametric 
rule-based parser to make a good contribution. 
While our experiments have so far been produc- 
ing promising results, there seems to be much 
room left for investigation and improvement. 
Acknowledgments 
We would like to express our special thanks to 
all the creators of the parsers used here for en- 
abling ~fll of this research by providing us their 
systems. We would also like to thank the re- 
viewers tbr their suggestive comments. 
References 
Brill, E. and J. Wu. Classifier Combination for ha- 
proved Lexical Disambiguation. In Proc. of the 
17th COLING, pp.191-195, 1998. 
Carroll, J. ,G. Minnen, Y. Cmming, S. Devlin and 
J. Tait. Practical Simplification of English News- 
paper Text to Assist Aphasic Readers. In Prvc. of 
AAAI-98 Workshop on Integrating Artificial In- 
telligence and Assistive Technology,1998. 
Charniak, E. Statistical parsing with a context- 
free grammar and word statistics. In Prvc. of the 
AAAI, pp.598 603, 1997. 
Collins, M. J. A new statistical parser based on bi- 
grmn lexical dependencies. In Proc. of the 3~th 
ACL, pp.184-191, 1996. 
Collins, M. J. Three generative, lexicalised models 
for statistical parsing. In Proc. of the 35th A CL, 
pp.16-23, 1997. 
Ehara, T. Estinlating the consistency of Japanese 
dependency relations based on the maximam en-  
trot)y modeling. Ill Proc. of the/~th Annual Meet- 
ing of The Association of Natural Language Pro- 
cessing, 1)t).382-385, 1998. (In Japanese) 
Fiscus, J. G. A post-processing system to yield re- 
duced word error rates: Recognizer output voting 
error reduction (ROVER). In EuroSpccch, 1997. 
Fk'ederking, R. and S. Nirenburg. Three heads are 
better titan one. In Proc. of the dth ANLP, 1994. 
Fujio, M. and Y. Matsmnoto. Japmmse dependency 
structure analysis based on lexicalized statistics. 
In Proc. of the 3rd EMNLP, t)I).87-96, 1998. 
Henderson, J. C. and E. Brill. Exploiting Diver- 
sity in Natural Language Processing: Combining 
Parsers. In Proc. of the 1999 Joint SIGDAT Con- 
fcrcncc on EMNLP and I/LC, pt).187--194. 
Jensen, K., G. E. Heidorn, and S. D. Richardson, 
editors, natural anguage processing: The PLNLP 
AppTvach. Kluwer Academic Publishers, 1993. 
Kanayama, H., K. Torisawa, Y. Mitsuisi, and 
J. Tsujii. Statistical Dependency Analysis with 
an HPSG-based Japanese Grainmar. In Proc. of 
the NLPRS, pp.138-143, 1999. 
Kurohashi, S. and M. Nagao. Building a Jat)anese 
parsed corpus while lint)roving tile parsing system. 
In Proc. of NLPRS, pp.151-156, 1997. 
Kurohashi, S. and M. Nagao. KN Parser : Japanese 
Dependency/Case Structure Analyzer. in Proc. of 
Th.e httcrnational Worksh.op on Sharablc Natural 
Lang'aagc Rcso'arccs, pp.48-55, 1994. 
Poole, D. Average-case analysis of a search algo- 
rithm fl)r estimating prior and 1)ostcrior probabil- 
ities in Bayesian etworks with extreme 1)rot)abil- 
ities, thc i3th LICAL pp.606 612, 1993. 
Pedersen, T. A Simple AI)l)roach to Building En- 
sembles of Naive Bayesian Classifiers for Word 
Sense Dismnbiguation In Proc. of the NAACL, 
pp.63-69, 2000. 
Shirai, K., K. hmi, T. Tokunaga and H. Tanaka 
An empirical evaluation on statistical 1)arsing 
of Japanese sentences using a lexical association 
statistics, thc 3rd EMNLP, pp.80-87, 1998. 
Uchimoto, K., S. Sekine, and H. Isahara. Japanese 
dependency structure analysis based on maxi- 
mum entopy models. In Proc. of thc 13th EACL, 
pp.196-203, 1999. 
van Halteren, H., J. Zavrel, and W. Daelemans. hn- 
t)roving data driven wordclass tagging 1)y system 
combination. In Proc. of the 17th COLING, 1998. 
354 
A Paraphrase-Based Exploration of Cohesiveness Criteria
INUI Kentaro
 
and NOGAMI Masaru
 
 
Department of Artificial Intelligence, Kyushu Institute of Technology, JAPAN

PRESTO, Japan Science and Technology Corporation, JAPAN

inui,m nogami  @pluto.ai.kyutech.ac.jp
Abstract
This paper proposes an empirical ap-
proach to the development of a com-
putational model for assessing texts ac-
cording to cohesiveness. We argue that
the NLG technologies for the gener-
ation of structural paraphrases can be
used to efficiently create what we call a
cohesion-variant parallel corpus, which
would serve as a good resource for em-
pirical acquisition of cohesiveness cri-
teria. We also present our pilot case
study, in which we took a particular
type of paraphrasing that separates a
relative clause from a sentence. We
have so far created a cohesion-variant
parallel corpus containing 499 cohesive
instances and 841 incohesive instances.
Based on this corpus, we conducted
a preliminary experiment on cohesion
evaluation, obtaining encouraging re-
sults.
1 Introduction
In NLP tasks such as translation, summarization
and text generation, where a system produces
texts as its output, the cohesiveness of output texts
is an important criterion for assessing the sys-
tem?s performance. An output text should not be
just a collection of syntactically and semantically
well-formed sentences, but is required to be well-
organized also at the discourse level. Cohesive
relations (i.e. coreference relations, rhetorical re-
lations, etc.) between entities contained in a text
should be properly realized by means of linguis-
tic cohesive devices. According to Halliday and
Hasan (1976) and Halliday (1994), there are four
types of cohesive devices in English: reference,
ellipsis, conjunction, and lexical cohesion. A sys-
tem thus needs to know how to use such cohesive
devices effectively1.
The translation and summarization research
communities are, in fact, increasingly getting
concerned with the notion of cohesion, though
only recently, as technologies for intra-sentential
processing make progress. For example, Marcu
et al (2000) proposes a computational model
for transforming rhetorical structures between
a source and target languages, aiming at the
improvement of translation quality. Mani et
al. (1999) proposes to incorporate the cohesion-
concerned revision process into summarization.
What is commonly required in such tasks is a
technology for assessing a given text according
to cohesiveness. Such a technology would en-
able us to design, for example, a translation model
which would choose the best cohesive translation
from more than one generated candidate. Al-
ternatively, it might also be feasible to design a
framework where a system would revise an initial
translation according to cohesion. In this paper,
1In this paper, we use the term cohesiveness to refer to
the degree to which cohesive devices are properly chosen to
realize cohesive relations in a text, whether it is quantified or
not. It is a matter of surface realization or sentence planning
(Wanner and Hovy, 1996; Reiter and Dale, 1999). Here we
distinguish it from the notion of coherence, which is typi-
cally used to refer to the degree to which the text contents
themselves are well-organized at the conceptual level, and
thus a matter of content selection/organization. This paper
concentrates its focus on the former stratum.
we argue that the NLG technologies for generat-
ing certain types of structural paraphrases can be
used to efficiently create training and testing data
which would serve as a good resource for empir-
ical corpus-based study on cohesiveness evalua-
tion.
To give our intuition to the reader, let us con-
sider the following example, where (1t.1) and
(1t.2) are both paraphrases of a source passage
(1s)2:
(1s) Sma?land, which is located to the south-west
of Stockholm, is called ?The Kingdom of
Glass?. The reason is that there are sixteen
glass manufacturers in this area.
(1t.1) Sma?land is located to the south-west of
Stockholm. It is called ?The Kingdom of
Glass?. The reason is that there are sixteen
glass manufacturers in this area.
(1t.2) Sma?land is called ?The Kingdom of
Glass?. It is located to the south-west of
Stockholm. The reason is that there are six-
teen glass manufacturers in this area.
One paraphrase (1t.1) is cohesive as well as the
original passage (1s). The other paraphrase (1t.2)
cannot be considered cohesive, however, since the
REASON relation between the first and third sen-
tences is interfered by the second sentence.
As suggested by this example, particular sorts
of structural paraphrasing have the effect of
changing some aspects of the textual struc-
ture of a given original text. Sentence divi-
sion/aggregation, clause order scrambling, top-
icalization, extraposition3, and voice-switching
are typical sorts of such structural paraphrasing.
These sorts of paraphrasing are likely to change
either of discourse-relation scoping, theme-rheme
chaining, coreference chaining, salience, and so
on. This change may preserve the original cohe-
siveness, but may also be likely to break it. Mak-
ing use of this nature, one can systematically and
efficiently create a large text collection containing
both cohesive and incohesive instances in paral-
lel, which would then serve as a resource for ex-
2This example is a congruent translation of a passage
originally written in Japanese.
3A typical example is the transfer from a simple-clause
sentence to a cleft sentence.
ploration of cohesiveness criteria. We call such a
text collection a cohesion-variant parallel corpus.
In this paper, we concentrate our focus only
on the cohesiveness of the linguistic realization
level rather than on the coherence of the concep-
tual level. Our present goal is, therefore, to cre-
ate a computational model that can evaluate and
compare a given set of paraphrase variants as-
sociated with the same contents according to lo-
cal cohesiveness. A pair of passages (1t.1) and
(1t.2) above is an example of such a paraphrase
variant set. The scope of the variations of lin-
guistic cohesive devices we would like to con-
sider here largely covers the matters of sentence
planning (or micro planning) (Wanner and Hovy,
1996; Reiter and Dale, 1999) including referring
expressions, discourse markers, sentence group-
ing, clause configuration, topicalization, etc.
In the following sections, we first review the lit-
erature on choice of cohesive devices in text gen-
eration in section 2. We next describe an overview
of our paraphrase-based approach to this issue in
section 3. We then present our pilot case study, in
which we took a particular type of paraphrasing
that separates a relative clause from a sentence,
reporting the results of a preliminary experiment,
in section 4 and 5.
2 Choice of cohesive devices in text
generation
One may expect that criteria or techniques for co-
hesiveness evaluation should be easily found in
the literature on text generation for a couple of
reasons:
 There have been quite a few cohesion-related
works in this field. For example, works
on discourse marker choice (Scott and de
Souza, 1990; Vander Linden, 1994; Grote
and Stede, 1998; Oates, 1999), generation
of referring expressions (Dale, 1992), and
clause aggregation based on discourse rela-
tion (Dalianis and Hovy, 1996; Show, 1998)
are all related to linguistic realization of co-
hesive relations.
 Text generation has the advantage of serving
as a good device for systematically produc-
ing diverse text variants from the same in-
put. Imagine, for example, that a NLG sys-
tem could generate passages (1s), (1t.1) and
(1t.2) in parallel from the same content spec-
ifications. If given such a collection of text
variants containing both cohesive and inco-
hesive instances in parallel, one could carry
out steady explorations of cohesiveness cri-
teria, since having negative instances besides
positive ones would significantly facilitate
generalization.
Unfortunately, however, there have so far been
very few works in this field that have shown com-
prehensive and concrete criteria of cohesiveness.
This is supposed to be partly due to the following
problems:
 In text generation, input topics tend to
strongly depend on each application system,
which makes it difficult to conduct large-
scale experiments to generate text variants
of diverse contents. This seems to have pre-
vented transfer of cohesion-related technolo-
gies in text generation to other fields such
as translation and summarization that are re-
quired to handle relatively unrestricted texts.
 Ideally, for computational purposes, co-
hesiveness criteria should be represented
declaratively as, for example, a set of declar-
ative constraints, so that they could be reem-
ployed in other NLP tasks. In most exist-
ing generation systems, however, the process
of considering cohesiveness tends to be dis-
tributed over a number of choice points in
the search space of generation (more specifi-
cally, for example, decision experts in black-
board-based a sentence planner (Wanner and
Hovy, 1996; Grote and Stede, 1998)), which
has obstructed reuse of implemented knowl-
edge.
 While NL understanding/analysis commu-
nities have recently developed empirical
corpus-based approaches gaining remark-
able success, it seems that the NLG com-
munity has not made so much effort on this
frontier. For example, it has not yet accumu-
lated significant shared text data (corpora)
that are richly annotated for the purpose of
empirical discourse analysis. Coherence-
oriented knowledge proposed so far in the
literature, e.g. the assumptions and heuris-
tics proposed by Scott and de Souza (1990),
would be more easy to refine, extend or cus-
tomize, and thus would be more reusable, if
it were available together with the text data
from which it had been extracted.
Concerning the first and third problems, one
may see a remarkable exception in Marcu?s em-
pirical approach (Marcu, 1997). Marcu first auto-
matically sampled a large collection of text frag-
ments including discourse markers in question,
then carried out manual annotation, and finally
attempted to acquire local constraints based on
statistics. Marcu?s approach is considered fairly
powerful, yet it still has a weakness: Since it
is not designed to use negative instances in the
constraint acquisition, it requires huge amount of
training data, which then requires considerable
cost for manual annotation.
3 Paraphrase-based generation of
cohesion-variant parallel corpora
Our approach can be decomposed into the follow-
ing steps:
1. Manual production of paraphrases: Given
a collection of several-clause-long passages,
we first manually create a structural para-
phrase for each passage so that it preserves
the cohesiveness of the original passage.
Each paraphrase created in this step is called
a core positive instance.
2. Manual extraction of a choice system: We
next manually analyze the differences be-
tween the core positive instances and their
original passages to extract a choice system
of paraphrasing that exhaustively covers all
the core positive instances. Here, we assume
a choice system to be a sort of a system net-
work in the systemic sense (Halliday, 1994).
3. Implementation: We implement the choice
system on a paraphrase generator.
4. Automatic generation of annotated para-
phrases: We then generate diverse para-
phrases from each of given input passages
systematically by making random choices on
the choice system. In this process, the gen-
erator simultaneously annotates each para-
phrase instance with rich tags to indicate all
the choices made to generate it as well as
various syntactic and semantic attributes of
it.
5. Manual cohesiveness evaluation: Theoret-
ically, the resultant set of paraphrases should
cover all the core positive instances. In ad-
dition, it also include a much larger num-
ber of other newly generated paraphrases.
For each instance of the latter, we manually
judge whether it is positive (i.e. cohesive)
or negative (i.e. incohesive), and annotate it
accordingly.
As a result of steps 1 through 5, we obtain an an-
notated cohesion-variant parallel corpus. It con-
sists of paraphrase groups, each of which further
consists of both cohesive and incohesive para-
phrases associated with the same clause set.
6. Modeling and testing of cohesiveness cri-
teria: Such a sufficiently large annotated
corpus being obtained, one should be able
to employ it to create and test computational
models for cohesiveness evaluation. For this
modeling step, one might also be able to ap-
ply some recently advanced machine learn-
ing techniques, since the task of cohesive-
ness evaluation we consider here can also be
simply regarded as a classification problem
where a given passage (paraphrase) is to be
classified into two classes: cohesive or inco-
hesive.
Our paraphrase-based approach has the follow-
ing advantages, while preserving the advantage of
previous verification-by-generation approaches.
 Structural paraphrasing tends to change only
a very small part of a given original sentence.
This makes it easier to implement a struc-
tural paraphrase generator that guarantees
at least the intra-clausal syntactic/semantic
well-formedness of its output, compared
with the case of generation from knowledge
base.
 It is also relatively easy to generate text
variants of sufficiently diverse contents,
since paraphrasing does not require either
application-dependent artifactual input or a
grammar and lexicon that fully cover the
texts to generate.
 Manual semantic/discourse annotation is re-
quired in principle only for source instances;
a much larger number of derivative instances
can be annotated fully automatically. This
facilitates scaling up of an instance collec-
tion.
 The second advantage potentially enables us
to make so-called selective sampling, which
has been empirically proven to effectively
accelerate learning, while reducing manual
annotation costs, in many knowledge acqui-
sition tasks, e.g. (Fujii et al, 1998).
4 A case study
We conducted a pilot case study, taking a partic-
ular type of paraphrasing which separates a rel-
ative clause from a given sentence as in exam-
ple (1) in section 1. Hereafter, for simplicity, we
use the term paraphras  e,ing  to refer to para-
phras  e,ing  of this type, as far as the present
case study is concerned. Furthermore, for con-
venience, we call the sentence originating from
a relative clause a satellite sentence (or simply a
satellite), and the sentence that consists of the re-
maining constituents of the source sentence a nu-
cleus sentence (or simply a nucleus).
The target language we have so far explored is
only Japanese. Note, however, that our method-
ology is expected to be in principle equally appli-
cable to any language. Example (2) below is an
actual example of Japanese paraphrasing:
(2s) [Sweden-no (Sweden-POS) shuto (capital-APPOS)
Stockholm-no (of Stockholm) nanse?bu-ni (to the
south-west) itisuru (to be located-ADNOM)] 
	 
Sma?land-tiho?-wa (Sma?land-TOP) betume? (another
name) ?garasu-no o?koku?-to (as ?Kingdom of Glass?)
yobareteiru (to be called).
(2t)  satellite  Sma?land-tiho?-wa (Sma?land-TOP) Sweden-
no (Sweden-POS) shuto (capital-APPOS) Stockholm-no
(of Stockholm) nanse?bu-ni (to the south-west) itisuru
(to be located).
 nucleus  kono-tiho?-wa (this region-TOP) betume? (an-
other name) ?garasu-no o?koku?-to (as ?Kingdom of
Glass?) yobareteiru (to be called).
Japanese relative clauses can be classified as ei-
ther gapping or non-gapping. While gapping rel-
ative clauses contain a unique gap for the mod-
ified head, the associated case slot of which can
be a complement or adjunct, non-gapping rela-
tive clauses do not contain any gap. The former
class of relative clauses can be further semanti-
cally classified into restrictive or non-restrictive
relative clauses. Among those three subtypes, for
our present study, we restricted the source objects
of paraphrasing to non-restrictive gapping relative
clauses, since this type of relative clauses can be
separated from the matrix clause most straightfor-
wardly.
We first collected 275 non-restrictive gapping
relative clauses from newspaper articles of di-
verse genres (1,840 sentences in total) excerpted
from the Kyoto corpus (Kurohashi and Nagao,
1997). For each of them, we manually created
a core positive instance, taking its context into
account. We next manually analyzed those in-
stances, and obtained a choice system consisting
of seven major simultaneous choice points as fol-
lows:
(c1) Tense and aspect: whether the tense of
the satellite should be of the ta (past)
form or ru (non-past) form, and whether
the aspect should be of the teiru (progres-
sive/resultative) form or ru (base) form
(c2) Case marker alteration: the case marker
no used as a nominative case marker in a rel-
ative clause should be obligatorily replaced
with the proper subjective case marker ga in
the satellite sentence
(c3) Punctuation: punctuation should be
changed accordingly
(c4) Connective: whether the rhetorical relation
between the nucleus and satellite should be
verbalized as a connective expression or not,
and which expression should be chosen if
necessary
(c5) Sentence order: whether the nucleus sen-
tence precedes or the satellite sentence pre-
cedes
(c6) Topicalization: whether the filler of the gap
of the satellite should be topicalized or not
(c7) Copulativization: whether the satellite
should be further transferred to construct a
copula or not:
(3s) [NTT-ga (NTT-NOM) 4-gatu kara (from April)
te?kyo?-suru (to provide-ADNOM)]

	 
zisedai-ko?soku-tu?sin-kaisen (new generation
telecommunication network)
(the new generation telecommunication
network, which NTT will provide from April)
(3t.1)  non-copula  NTT-ga (NTT-NOM) 4-gatu kara
(from April) zisedai-ko?soku-tu?sin-kaisen-o (new
generation telecommunication network-ACC)
te?kyo?-suru (to provide).
(NTT will provide a new generation telecommu-
nication network from April.)
(3t.2)  copula  zisedai-ko?soku-tu?sin-kaisen-wa (new
generation telecommunication network-TOP)
[NTT-ga (NTT-NOM) 4-gatu kara (from April)
te?kyo?-suru (to provide)] 
	 
 sa?bisu-da (to
be a service).
(The new generation telecommunication net-
work is a service that NTT will provide from
April.)
(c7) Anaphora/ellipsis: Each anaphoric expres-
sion and ellipsis should be reconsidered with
the options including at least the following:
? NP with a demonstrative adjective
?kono/sono (this/that)?
? bare NP without a demonstrative adjec-
tive
? head noun with a demonstrative adjec-
tive
? demonstrative pronoun ?kore/sore/
(this/that)?
? personal pronoun
? ellipsis (zero pronoun)
We then implemented the above choice system
on our paraphrasing engine FUNE (Fujita et al,
2000), and obtained 1,343 paraphrase instances
from the 195 source instances, which were those
randomly sampled from the above 275 source in-
stances. To generate these paraphrases, we made
random choices only for the choice points (c4) to
(c7), while making an optimal choice for each of
the rest, (c1) to (c3), since our preliminary inves-
tigation have proven the latter set of choice points
to be almost independent of the context.
For the input to FUNE, we provided the follow-
ing sorts of information:
 morphological and dependency structure in-
formation (given by the Kyoto corpus)
 semantic information (semiautomatically
annotated) such as the grammatical role of
the gap of a relative clause
 textual information (semiautomatically an-
notated) such as the rhetorical relations be-
tween clauses (Mann and Thompson, 1987)
and the antecedent of each anaphor/ellipsis
Here, we mean by ?semiautomatically annotate?
that the preprocessing module analyzed the input
to obtain semantic/textual information while leav-
ing uncertain parts of analysis in our hands.
Finally, we manually assessed all the para-
phrase instances. 449 instances were judged to be
acceptably cohesive (positive), 841 instances un-
acceptable (negative), and 53 instances were left
unjudged. When more than one positive instance
were derived from a single source instance, we
further ranked them.
The assessment was carried out by two of us.
Unfortunately, we were not able to estimate the
agreement rate between the two assessors, since
we had frequently discussed all the cases of which
either of us had felt unsure. The psychological
estimation of the feasibility of human judgment
in this task will be a future work.
5 A cohesiveness evaluation model
The cohesiveness criteria can be modelled as a set
of constraints and preferences. The constraints
would discriminate between positive instances
and negative instances, whereas the preferences
would rank positive instances according to flu-
ency. As mentioned before, to create such a com-
putational model, one might be able to employ
various machine learning techniques for classifi-
cation problems. We considered, however, that
for the present case study, which is still at the very
preliminary stage, it should be more important to
get a sense of the properties of the task by manual
analysis. In this section, we briefly but exhaus-
tively enumerate the hypothetical constraints and
preferences we have so far obtained by manual
analysis.
5.1 Clause ordering
We considered that one way to approach the is-
sue of clause ordering would be to start with Mi-
nami?s linguistic theory on intra-sentential hier-
archical structure (Minami, 1974). According
to Minami, a Japanese sentence has a center-
embedding hierarchical structure as illustrated in
Figure 1 (a), where the event description level
(A) is embedded in the speaker?s attitude level
(B), which is then embedded in the presentation
level (C), but not vise versa. Given this view of
sentence structure, one can predict, for example,
that a subordinate adverbial clause stating REA-
SON (level B) can be embedded in another sub-
ordinate clause stating CONCESSION (level C),
but not vice versa, as illustrated in Figure 1 (b).
This constraint has, in fact, been used by NLP re-
searchers such as Shirai et al (1995) for disam-
biguation of intra-sentential inter-clausal depen-
dency structures.
Concession
Reason Concession
Reason
(a)
(b)
Presentation (C) Attitude (B) Description (A)
...shita-ga [Concession] ...node [Reason] ...nagara [ParallelEvent]
Figure 1: Hierarchical structure of Japanese sen-
tence
In order to apply this constraint to our task of
clause ordering, we need to extend it in the fol-
lowing respects:
 Since Minami?s theory covers only intra-
sentential rhetorical structures, we need to
prove whether it holds beyond sentence
boundaries, and also whether it holds even
if the nucleus precedes the satellite (In a
Japanese sentence, a subordinate (satellite)
clause always precedes the matrix clause
(nucleus) it depends on.)
 The ELABORATION relation, which is one
of the most common rhetorical relations that
appear in inter-sentential rhetorical struc-
tures, is out of scope in Minami?s theory
since the ELABORATION relation does not
appear in adverbial inter-clausal dependen-
cies. We need to investigate where the
ELABORATION relation should be located in
Minami?s hierarchy of rhetorical relations.
Our analysis has so far supported the following
constraints and several well-known preferences,
although obviously they still need further investi-
gation and refinement.
Constraint 1.1 If three continuous discourse
segments constitute either of the rhetorical pat-
terns (A) or (B) shown in Figure 2, relation 
should be of a higher level of the rhetorical hier-
archy than relation  , where:
 ELABORATION constitutes a new class
whose level in the hierarchy is higher than
the level (B) (e.g. REASON), and lower than
the level (C) (e.g. CONCESSION), and
 the constraint holds beyond sentence bound-
aries, and is independent of the order of the
nucleus and satellite, except that pattern (A)
is not acceptable if  is ELABORATION.
R1
R2
R1
R2
(A) (B)
Figure 2: Local patterns of rhetorical dependency
structure
The Sma?land example taken in section 1 is
a good example that satisfies this constraint,
where (4t.1) has no embedding, (4t.2) is the case
where  =ELABORATION and  =REASON,
and (4t.3) is the case where  =REASON and
 =ELABORATION:
(4t.1) Sma?land-tiho?-wa (Sma?land-TOP) Sweden-no
(Sweden-POS) shuto (capital-APPOS) Stockholm-no
(of Stockholm) nan- se?bu-ni (to the south-west) itisuru
(to be located).
kono-tiho?-wa (this region-TOP) betume? (another
name) ?garasu-no o?koku?-to (as ?Kingdom of Glass?)
yobareteiru (to be called).
16-mo-no (sixteen-EMPHASIS) garasu-ko?jo?-ga (glass
manufacturers-MON) kono-tiho?-ni (in this region)
tenzai-site-iru-kara-da (to exist-REASON).
(Sma?land is located to the south-west of Stockholm,
the capital of Sweden. It is also called ?Kingdom
of Glass?. The reason is that there are sixteen glass
manufacturers in this area.)
(4t.2) Sma?land-tiho?-wa (Sma?land-TOP) betume? (another
name) ?garasu-no o?koku?-to (as ?Kingdom of Glass?)
yobareteiru (to be called).
16-mo-no (sixteen-EMPHASIS) garasu-ko?jo?-ga (glass
manufacturers-MON) kono-tiho?-ni (in this region)
tenzai-site-iru-kara-da (to exist-REASON).
Sma?land-tiho?-wa (this region-TOP) Sweden-no
(Sweden-POS) shuto (capital-APPOS) Stockholm-no
(of Stockholm) nan- se?bu-ni (to the south-west) itisuru
(to be located).
(Sma?land is called ?Kingdom of Glass?. The reason is
that there are sixteen glass manufacturers in this area.
Sma?land is located to the south-west of Stockholm,
the capital of Sweden. )
(4t.3)  Sma?land-tiho?-wa (Sma?land-TOP) betume? (another
name) ?garasu-no o?koku?-to (as ?Kingdom of Glass?)
yobareteiru (to be called).
kono-tiho?-wa (this region-TOP) Sweden-no (Sweden-
POS) shuto (capital-APPOS) Stockholm-no (of Stock-
holm) nan- se?bu-ni (to the south-west) itisuru (to be
located).
16-mo-no (sixteen-EMPHASIS) garasu-ko?jo?-ga (glass
manufacturers-MON) kono-tiho?-ni (in this region)
tenzai-site-iru-kara-da (to exist-REASON).
(  Sma?land is called ?Kingdom of Glass?. It is located
to the south-west of Stockholm, the capital of Sweden.
The reason is that there are sixteen glass manufacturers
in this area. )
Constraint 1.2 The satellite of an ELABORA-
TION relation cannot precede the nucleus, except
for the case where the satellite has no preceding
context.
Preference 1.1 If there is a coreference relation
between two segments, they are preferred to be
adjacent to each other.
Preference 1.2 If there is a temporal SUBSE-
QUENCE relation between two segments, they are
preferred to be placed in that temporal order.
Preference 1.3 If the nucleus and satellite sen-
tences are in the CONTRAST relation, the former
is preferred to precede.
5.2 Discourse markers
Concerning discourse markers, we have not
widely explored options for either of marker oc-
currence, marker placement and marker selection.
For the moment, we have implemented only the
following constraint for the experiment we will
describe in the next section.
Constraint 3.1 The rhetorical relation should
be verbalized by means of a proper connective ex-
pression in those cases including the following:
 a case where the rhetorical relation is REA-
SON, and the satellite follows the nucleus
 a case where the rhetorical relation is CON-
CESSION, and the nucleus follows the satel-
lite
5.3 Topicalization
Constraint 2.1 If the gap of the relative clause
is associated with the nominative case, the gap
filler should be topicalized in the satellite sen-
tence, except for the case where the satellite is of
the form ? Coling 2010: Poster Volume, pages 534?542,
Beijing, August 2010
Identifying Contradictory and Contrastive Relations between Statements
to Outline Web Information on a Given Topic
Daisuke Kawahara? Kentaro Inui?? Sadao Kurohashi??
?National Institute of Information and Communications Technology
?Graduate School of Information Sciences, Tohoku University
?Graduate School of Informatics, Kyoto University
dk@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
Abstract
We present a method for producing a
bird?s-eye view of statements that are ex-
pressed on Web pages on a given topic.
This method aggregates statements that
are relevant to the topic, and shows con-
tradictory and contrastive relations among
them. This view of contradictions and
contrasts helps users acquire a top-down
understanding of the topic. To realize
this, we extract such statements and re-
lations, including cross-document implicit
contrastive relations between statements,
in an unsupervised manner. Our experi-
mental results indicate the effectiveness of
our approach.
1 Introduction
The quantity of information on theWeb is increas-
ing explosively. Online information includes news
reports, arguments, opinions, and other coverage
of innumerable topics. To find useful information
from such a mass of information, people gener-
ally use conventional search engines such as Ya-
hoo! and Google. They input keywords to a search
engine as a query and obtain a list of Web pages
that are relevant to the keywords. They then use
the list to check several dozen top-ranked Web
pages one by one.
This method of information access does not
provide a bird?s-eye view of the queried topic;
therefore it can be highly time-consuming and dif-
ficult for a user to gain an overall understanding of
what is written on the topic. Also, browsing only
top-ranked Web pages may provide the user with
biased information. For example, when a user
direct contrastive statement ?A is more P than B?
contrastive keyword pair (A, B)
contradictory relation ?A is P? ? ?A is not P?
contrastive relation ?A is P? ? ?B is P (not P)?
Table 1: Overview of direct contrastive state-
ments, contrastive keyword pairs and contradic-
tory/contrastive relations. Note that ?P? is a pred-
icate.
searches for information on ?agaricus,? claimed
to be a health food, using a conventional search
engine, many commercial pages touting its health
benefits appear at the top of the ranks, while other
pages remain low-ranked. The user may miss an
existing Web page that indicates its unsubstanti-
ated health benefits, and could be unintentionally
satisfied by biased or one-sided information.
This paper proposes a method for produc-
ing a bird?s-eye view of statements that are ex-
pressed on Web pages on a given query (topic).
In particular, we focus on presenting contradic-
tory/contrastive relations and statements on the
topic. This presentation enables users to grasp
what arguing points exist and furthermore to see
contradictory/contrastive relations between them
at a glance. Presenting these relations and state-
ments is thought to facilitate users? understanding
of the topic. This is because people typically think
about contradictory and contrastive entities and is-
sues for decision-making in their daily lives.
Our system presents statements and relations
that are important and relevant to a given topic,
including the statements and relations listed in Ta-
ble 1. Direct contrastive statements compare two
entities or issues in a single sentence. The con-
trasted entities or issues are also extracted as con-
trastive keyword pairs. In addition to them, our
534
sekken-wa gosei senzai-to chigai, kankyo-ni yoi. 
!"#$%&'%("")%*"+%,-.%./0&+"/1./,%#'%2"1$#+.)%,"%'3/,-.42%).,.+(./,5!
gosei senzai-de yogore-ga ochiru (15) 
6#'-%',#&/'%6&,-%'3/,-.42%).,.+(./,!
gosei senzai-ni dokusei-ga aru (9) 
'3/,-.42%).,.+(./,%-#'%,"7&2&,3! gosei senzai-ni dokusei-ga nai (2) '3/,-.42%).,.+(./,%&'%/",%,"7&2!
sekken-de yogore-ga ochi-nai (6) 
/",%+.1"0.%',#&/%6&,-%'"#$! sekken-de yogore-ga ochiru (4) +.1"0.%',#&/'%6&,-%'"#$!
goseisenzai-de te-ga areru (7) 
13%-#/)%(.,'%+"8(-%6&,-%'3/).,!
gosei senzai-wa kaimen kasseizai-wo fukumu (5) 
'3/,-.42%).,.+(./,%2"/,#&/'%'8+*#2,#/,!
[direct contrastive statement]!
contrastive relation!
contradictory relation!
Legend:!
Figure 1: Examples of statements on ?gosei senzai? (synthetic detergent), which are represented by
rounded rectangles. Each statement is linked with the pages from which it is extracted. The number in
a parenthesis represents the number of pages.
system shows contradictory and contrastive rela-
tions between statements. Contradictory relations
are the relations between statements that are con-
tradictory about an entity or issue. Contrastive
relations are the relations between statements in
which two entities or issues are contrasted.
In particular, we have the following two novel
contributions.
? We identify contrastive relations between
statements, which consist of in-document
and cross-document implicit relations.
These relations complement direct con-
trastive statements, which are explicitly
mentioned in a single sentence.
? We precisely extract direct contrastive state-
ments and contrastive keyword pairs in an
unsupervised manner, whereas most previ-
ous studies used supervised methods (Jindal
and Liu, 2006b; Yang and Ko, 2009).
Our system focuses on the Japanese language.
For example, Figure 1 shows examples of ex-
tracted statements on the topic ?gosei senzai?
(synthetic detergent). Rounded rectangles repre-
sent statements relevant to this topic. The first
statement is a direct contrastive statement, which
refers to a contrastive keyword pair, ?gosei sen-
zai? (synthetic detergent) and ?sekken? (soap).
The pairs of statements connected with a broad
arrow have contradictory relations. The pairs of
statements connected with a thin arrow have con-
trastive relations. Users not only can see what is
written on this topic at a glance, but also can check
out the details of a statement by following its links
to the original pages.
2 Related Work
Studies have been conducted on automatic extrac-
tion of direct contrastive sentences (comparative
sentences) for English (Jindal and Liu, 2006b) and
for Korean (Yang and Ko, 2009). They prepared a
set of keywords that serve as clues to direct con-
trastive sentences and proposed supervised tech-
niques on the basis of tagged corpora. We pro-
pose an unsupervised method for extracting direct
contrastive sentences without constructing tagged
corpora.
From direct contrastive sentences, Jindal and
Liu (2006a) and Satou and Okumura (2007) pro-
posed methods for extracting quadruples of (tar-
get, basis, attribute, evaluation). Jindal and Liu
(2006a) extracted these quadruples and obtained
an F-measure of 70%-80% for the extraction of
?target? and ?basis.? Since this extraction was
535
not their main target, they did not perform er-
ror analysis on the extracted results. Satou and
Okumura (2007) extracted quadruples from blog
posts. They provided a pair of named entities
for ?target? and ?basis,? whereas we automati-
cally identify such pairs. Ganapathibhotla and Liu
(2008) proposed a method for detecting which en-
tities (?target? and ?basis?) in a direct contrastive
statement are preferred by its author.
There is also related work that focuses on non-
contrastive sentences. Ohshima et al (2006) ex-
tracted coordinated terms, which are semantically
broader than our contrastive keyword pairs, using
hit counts from a search engine. They made use
of syntactic parallelism among coordinated terms.
Their task was to input one of coordinated terms
as a query, which is different from ours. Soma-
sundaran and Wiebe (2009) presented a method
for recognizing a stance in online debates. They
formulated this task as debate-side classification
and solved it by using automatically learned prob-
abilities of polarity.
To aggregate statements and detect relations be-
tween them, one of important modules is recogni-
tion of synonymous, entailed, contradictory and
contrastive statements. Studies on rhetorical
structure theory (Mann and Thompson, 1988) and
recognizing textual entailment (RTE) deal with
these relations. In particular, evaluative work-
shops on RTE have been held and this kind of re-
search has been actively studied (Bentivogli et al,
2009). The recent workshops of this series set up
a task that recognizes contradictions. Harabagiu
et al (2006), de Marneffe et al (2008), Voorhees
(2008), and Ritter et al (2008) focused on rec-
ognizing contradictions. For example, Harabagiu
et al (2006) used negative expressions, antonyms
and contrast discourse relations to recognize con-
tradictions. These methods only detect relations
between given sentences, and do not create a
bird?s-eye view.
To create a kind of bird?s-eye view, Kawahara et
al. (2008), Statement Map (Murakami et al, 2009)
and Dispute Finder (Ennals et al, 2010) identi-
fied various relations between statements includ-
ing contradictory relations, but do not handle con-
trastive relations, which are one of the important
relations for taking a bird?s-eye view on a topic.
Lerman and McDonald (2009) proposed a method
for generating contrastive summaries about given
two entities on the basis of KL-divergence. This
study is related to ours in the aspect of extracting
implicit contrasts, but contrastive summaries are
different from contrastive relations between state-
ments in our study.
3 Our Method
We propose a method for grasping overall infor-
mation on the Web on a given query (topic). This
method extracts and presents statements that are
relevant to a given topic, including direct con-
trastive statements and contradictory/contrastive
relations between these statements.
As a unit for statements, we use a predicate-
argument structure (also known as a case structure
and logical form). A predicate-argument struc-
ture represents a ?who does what? event. Pro-
cesses such as clustering, summarization, compar-
ison with other knowledge and logical consistency
verification, which are required for this study and
further analysis, are accurately performed on the
basis of predicate-argument structures. The ex-
traction of our target relations and statements is
performed via identification and aggregation of
synonymous, contrastive, and contradictory rela-
tions between predicate-argument structures.
As stated in section 1, we extract direct con-
trastive statements, contrastive keyword pairs, rel-
evant statements, contrastive relations and contra-
dictory relations. We do this with the following
steps:
1. Extraction and aggregation of predicate-
argument structures
2. Extraction of contrastive keyword pairs and
direct contrastive statements
3. Identification of contradictory relations
4. Identification of contrastive relations
Below, we first describe our method of extract-
ing and aggregating predicate-argument struc-
tures. Then, we explain our method of extract-
ing direct contrastive statements with contrastive
keyword pairs, and identifying contradictory and
contrastive relations in detail.
536
3.1 Extraction and Aggregation of
Predicate-argument Structures
A predicate-argument structure consists of a pred-
icate and one or more arguments that have a de-
pendency relation to the predicate.
We extract predicate-argument structures from
automatic parses of Web pages on a given topic
by using the method of Kawahara et al (2008).
We apply the following procedure to Web pages
that are retrieved from the TSUBAKI (Shinzato
et al, 2008) open search engine infrastructure, by
inputting the topic as a query.
1. Extract important sentences from each Web
page. Important sentences are defined as sen-
tences neighboring the topic word(s).
2. Obtain results of morphological analysis
(JUMAN1) and dependency parsing (KNP2)
of the important sentences, and extract
predicate-argument structures from them.
3. Filter out functional and meaningless
predicate-argument structures, which are
not relevant to the topic. Pointwise mutual
information between the entire Web and the
target Web pages for a topic is used.
Note that the analyses in step 2 are performed be-
forehand and stored in an XML format (Shinzato
et al, 2008).
Acquired predicate-argument structures vary
widely in their representations of predicates and
arguments. In particular, many separate predicate-
argument structures have the same meaning due to
spelling variations, transliterations, synonymous
expressions and so forth. To cope with this prob-
lem, we apply ?keyword distillation? (Shibata
et al, 2009), which is a process of absorbing
spelling variations, synonymous expressions and
keywords with part-of relations on a set of Web
pages about a given topic. As a knowledge source
to merge these expressions, this process uses a
knowledge base that is automatically extracted
from an ordinary dictionary and the Web. For
instance, the following predicate-argument struc-
tures are judged to be synonymous3.
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
3In this paper, we use the following abbreviations:
(1) a. sekken-wo
soap-ACC
tsukau
use
b. sopu-wo
soap-ACC
tsukau
use
c. sekken-wo
soap-ACC
shiyou-suru
utilize
We call the predicate-argument structures that
are obtained as the result of the above proce-
dure statement candidates. The final output of
our system consists of direct contrastive state-
ments (with contrastive keyword pairs), top-N
statements (major statements) in order of fre-
quency of statement candidates, and statements
with contradictory/contrastive relations. Contra-
dictory/contrastive relations are identified against
major statements by searching statement candi-
dates.
Another outcome of keyword distillation is a re-
sultant set of keywords that are important and rel-
evant to the topic. We call this set of keywords
relevant keywords, which also include words or
phrases in the query. Relevant keywords are used
to extract contrastive keyword pairs.
3.2 Extraction of Contrastive Keyword Pairs
and Direct Contrastive Statements
We extract contrastive keyword pairs from con-
trastive constructs, which are manually speci-
fied as patterns of predicate-argument structures.
Statements that contain contrastive constructs are
defined as direct contrastive statements.
For example, the following sentence is a typi-
cal direct contrastive statement, which contains a
contrastive verb ?chigau? (differ).
(2) sekken-wa
soap-TOP
gosei senzai-to
synthetic detergent-ABL
chigai,
differ
? ? ?
(soap differs from synthetic detergent, ? ? ?)
From this sentence, a contrastive keyword pair,
?sekken? (soap) and ?gosei senzai? (synthetic de-
tergent), is extracted. The above sentence is ex-
tracted as a direct contrastive statement.
We preliminarily evaluated this simple pattern-
based method and found that it has the following
three problems.
NOM (nominative), ACC (accusative), DAT (dative),
ABL (ablative), CMI (comitative), GEN (genitive) and
TOP (topic marker).
537
? Keyword pairs that are mentioned in a con-
trastive construct are occasionally not rele-
vant to the given topic.
? Non-contrastive keyword pairs are erro-
neously extracted due to omissions of at-
tributes and targets of comparisons.
? Non-contrastive keyword pairs that have an
is-a relation are erroneously extracted.
To deal with the first problem, we filter out key-
word pairs that are contrastive but that are not rel-
evant to the topic. For this purpose, we apply fil-
tering by using relevant keywords, which are de-
scribed in section 3.1.
As an example of non-contrastive keyword
pairs (the second problem), from the following
sentence, a keyword pair, ?tokkyo seido? (patent
system) and ?nihon? (Japan), is incorrectly ex-
tracted by the pattern-based method.
(3) amerika-no
America-GEN
tokkyo seido-wa
patent system-TOP
nihon-to
Japan-ABL
kotonari,
different
? ? ?
(patent system of America is different from ? of
Japan ? ? ?)
In this sentence, ?nihon? (Japan) has a meaning of
?nihon-no tokkyo seido? (patent system of Japan).
That is to say, ?tokkyo seido? (patent system),
which is the attribute of comparison, is omitted.
In this study, in addition to patterns of con-
trastive constructs, we use checking and filtering
on the basis of similarity. The use of similarity
is inspired by the semantic parallelism between
contrasted keywords. As this similarity, we em-
ploy distributional similarity (Lin, 1998), which
is calculated using automatic dependency parses
of 100 million Japanese Web pages. By search-
ing similar keywords from the above sentence, we
successfully extract a contrastive keyword pair,
?amerika? (America) and ?nihon? (Japan), and
the above sentence as a direct contrastive state-
ment.
Similarly, a target of comparison can be omitted
as in the following sentence.
(4) nedan-wa
price-TOP
gosei senzai-yori
synthetic detergent-ABL
takaidesu
high
(price of ? is higher than synthetic detergent)
In this example, the similarity between ?nedan?
(price) and ?gosei senzai? (synthetic detergent) is
lower than a threshold, and this sentence and the
extracts from it are filtered out.
As for the third problem, we may extract non-
contrastive keyword pairs that have an is-a rela-
tion. From the following sentence, we incorrectly
extract a contrastive keyword pair, ?konbini? (con-
venience store) and ?7-Eleven,? which cannot be
filtered out due to its high similarity.
(5) 7-Eleven-wa
7-Eleven-TOP
hokano
other
konbini-to
convenience store-ABL
kurabete,
compare
? ? ?
(7-Eleven is ? ? ? compared to other convenience
stores)
To deal with this problem, we use a filter on the
basis of a set of words that indicate the existence
of hypernyms, such as ?hokano? (other) and ip-
panno (general). We prepare six words for this
purpose.
To sum up, we use the following procedure to
identify contrast keyword pairs.
1. Extract predicate-argument structures that do
not match the above is-a patterns and match
one of the following patterns. They are ex-
tracted from the statement candidates.
? X-wa Y-to {chigau | kotonaru | kuraberu}
(X {differ | vary | compare} from/with Y)
? X-wa Y-yori [adjective]
(X is more ? ? ? than Y)
Note that each of X and Y is a noun phrase
in the argument position.
2. Extract (x, y) that satisfies both the follow-
ing conditions as a contrastive keyword pair.
Note that (x, y) is part of a word sequence in
(X, Y), respectively.
? Both x and y are included in a set of rel-
evant keywords.
? (x, y) has the highest similarity among
any other candidates of (x, y), and this
similarity is higher than a threshold.
Note that the threshold is determined based on a
preliminary experiment using a set of synonyms
(Aizawa, 2007). We extract the sentence that con-
tains the predicate-argument structure used in step
1 as a direct contrastive statement.
538
3.3 Identification of Contradictory Relations
We identify contradictory relations between state-
ment candidates. In this paper, contradictory re-
lations are defined as the following two types
(Kawahara et al, 2008).
negation of predicate
If the predicate of a candidate statement is
negated, its contradiction has the same or synony-
mous predicate without negation. If not, its con-
tradiction has the same or synonymous predicate
with negation.
(6) a. sekken-ga
soap-NOM
kankyou-ni
environment-DAT
yoi
good
b. sekken-ga
soap-NOM
kankyou-ni
environment-DAT
yoku-nai
not good
antonym of predicate
The predicate of a contradiction is an antonym
of that of a candidate statement. To judge antony-
mous relations, we use an antonym lexicon ex-
tracted from a Japanese dictionary (Shibata et al,
2008). This lexicon consists of approximately
2,000 entries.
(7) a. gosei senzai-ga
synthetic detergent-NOM
anzen-da
safe
b. gosei senzai-ga
synthetic detergent-NOM
kiken-da
dangerous
To identify contradictory relations between
statements in practice, we search statement can-
didates that satisfy one of the above conditions
against major statements.
3.4 Identification of Contrastive Relations
We identify contrastive relations between state-
ment candidates. In this paper, we define a con-
trastive relation as being between a pair of state-
ment candidates whose arguments are contrastive
keyword pairs and whose predicates have synony-
mous or contradictory relations. Contradictory re-
lations of predicates are defined in the same way
as section 3.3.
In the following example, (a, b) and (a, c) have
a contrastive relation. Also, (b, c) has a contradic-
tory relation.
(8) a. gosei senzai-de
synthetic detergent-CMI
yogore-ga
stain-NOM
ochiru
wash
Topic: bio-ethanol
? (bio-ethanol fuel, gasoline)
(bio-ethanol car, electric car)
Topic: citizen judgment system
? (citizen judgment system, jury system)
(citizen judgment system, lay judge system)
Topic: patent system
? (patent system, utility model system)
(large enterprise, small enterprise)
Topic: Windows Vista
? (Vista, XP)
Table 2: Examples of extracted contrastive key-
word pairs (translated into English).
b. sekken-de
soap-CMI
yogore-ga
stain-NOM
ochiru
wash
c. sekken-de
soap-CMI
yogore-ga
stain-NOM
ochi-nai
not wash
The process of identifying contrastive relations
between statements is performed in the same way
as the identification of contradictory relations.
That is to say, we search statement candidates
that satisfy the definition of contrastive relations
against major statements.
4 Experiments
We conducted experiments for extracting con-
trastive keyword pairs, direct contrastive state-
ments and contradictory/contrastive relations on
50 topics, such as age of adulthood, anticancer
drug, bio-ethanol, citizen judgment system, patent
system and Windows Vista.
We retrieve at most 1,000 Web pages for a topic
from the search engine infrastructure, TSUBAKI.
As major statements, we extract 10 statement can-
didates in order of frequency.
Below, we first evaluate the extracted con-
trastive keyword pairs and direct contrastive state-
ments, and then evaluate the identified contradic-
tory and contrastive relations between statements.
4.1 Evaluation of Contrastive Keyword Pairs
and Direct Contrastive Statements
Contrastive keyword pairs and direct contrastive
statements were extracted on 30 of 50 topics. 99
direct contrastive statements and 73 unique con-
trastive keyword pairs were obtained on 30 topics.
The average number of obtained contrastive key-
word pairs for a topic was approximately 2.4. Ta-
539
Topic: ?tyosakuken hou? (copyright law)
?syouhyouken-wa tyosakuken-yori zaisantekina kachi-wo motsu.?
The trademark right has more financial value than the copyright.
?tyosakuken hou-de hogo-sareru? ? ?tyosakuken hou-de hogo-sare-nai?protected by the copyright law not protected by the copyright law
?tyosakuken-wo shingai-suru? ? ?tyosakuken-wo shingai-shi-nai?infringe the copyright not infringe the copyright
? ?syouhyouken-wo shingai-shi-nai?not infringe the trademark right
Topic: ?genshiryoku hatsuden syo? (nuclear power plant)
?genshiryoku hatsuden syo-wa karyoku hatsuden syo-to chigau.?
Nuclear power plants are different from thermoelectric power plants.
?CO2-wo hassei-shi-nai? ? ?CO2-wo hassei-suru?not emit carbon dioxide emit carbon dioxide
?genpatsu-wo tsukuru? ? ?genshiryoku hatsuden syo-wo tsukura-nai?construct a nuclear power plant not construct a nuclear power plant
? ?karyoku hatsuden syo-wo tsukuru?construct a thermoelectric power plant
Table 3: Examples of identified direct contrastive statements, contradictory relations and contrastive
relations. The sentences with two underlined parts are direct contrastive statements. The arrows ???
and ??? represent a contradictory relation and a contrastive relation, respectively.
ble 2 lists examples of obtained contrastive key-
word pairs. We successfully extracted not only
contrastive keyword pairs including topic words,
but also those without them.
Our manual evaluation of the extracted con-
trastive keyword pairs found that 89% (65/73) of
the contrastive keyword pairs are actually con-
trasted in direct contrastive statements. Correct
contrastive keyword pairs were extracted on 28 of
30 topics. We also evaluated the contrastive key-
word pairs extracted without similarity filtering.
In this case, 190 contrastive keyword pairs on 41
topics were extracted and 44% (84/190) of them
were correct. Correct contrastive keyword pairs
were extracted on 31 of 41 topics. Therefore, sim-
ilarity filtering did not largely decrease the recall,
but significantly increased the precision.
We have eight contrastive keyword pairs that
were incorrectly extracted by our proposed
method. These contrastive keyword pairs acciden-
tally have similarity that is higher than the thresh-
old. Major errors were caused by the ambiguity of
Japanese ablative keyword ?yori.?
(9) heisya-wa
our company-TOP
bitWallet sya-yori
bitWallet, Inc.-ABL
Edy gifuto-no
Edy gift-GEN
gyomu itaku-wo
entrustment-ACC
ukete-imasu
have
(Our company is entrusted with Edy gift by bitWal-
let, Inc.)
In this example, ?yori? means not the basis of
contrast but the source of action. The similar-
ity filtering usually prevents incorrect extraction
from such a non-contrastive sentence. However,
in this case, the pair of ?heisya? (our company)
and ?bitWallet sya? (bitWallet, Inc.) was not fil-
tered due to the high similarity between them. To
cope with this problem, it is necessary to use lin-
guistic knowledge such as case frames.
4.2 Evaluation of Contradictory and
Contrastive Relations
Contradictory relations were identified on 49 of
50 topics. For 49 topics, 268 contradictory re-
lations were identified. The average number of
identified contradictory relations for a topic was
5.5. Contrastive relations were identified on 18
of 30 topics, on which contrastive keyword pairs
were extracted. For the 18 topics, 60 contrastive
relations were identified. The average number of
identified contrastive relations for a topic was 3.3.
Table 3 lists examples of the identified contra-
dictory and contrastive relations as well as direct
contrastive statements. We manually evaluated
the identified contradictory relations and the con-
trastive relations that were identified for correct
contrastive keyword pairs. As a result, we con-
cluded that they completely obey our definitions.
We also classified each of the obtained contra-
dictory and contrastive relations into two classes:
?cross-document? and ?in-document.? ?Cross-
540
Topic: age of adulthood
lower the age of adulthood to 18
? lower the voting age to 18
Topic: anticancer drug
anticancer drugs have side effects
? anticancer drugs have effects
Table 4: Examples of unidentified contrastive re-
lations (translated into English).
document? means that a contradictory/contrastive
relation is obtained not from a single page but
across multiple pages. If a relation can be
obtained from both, we classified it into ?in-
document.? As a result, 67% (179/268) of contra-
dictory relations and 70% (42/60) of contrastive
relations were ?cross-document.? We can see that
many cross-document implicit relations that can-
not be retrieved from a single page were success-
fully identified.
4.3 Discussions
We successfully identified contradictory relations
on almost all the topics. However, out of 50 top-
ics, we extracted contrastive keyword pairs on 30
topics and contrastive relations on 18 topics. To
investigate the resultant contrastive relations from
the viewpoint of recall, we manually checked
whether there were unidentified contrastive rela-
tions among 100 statement candidates for each
topic. We actually checked 20 topics and found
six unidentified contrastive relations in total. Ta-
ble 4 lists examples of the unidentified contrastive
relations. Out of 20 topics, in total, 44 contrastive
relations are manually discovered on 13 topics,
but out of 13 topics, 38 contrastive relations are
identified on eight topics by our method. There-
fore, we achieved a recall of 86% (38/44) at rela-
tion level and 62% (8/13) at topic level. We can
see that our method was able to cover a relatively
wide range of contrastive relations on the topics
on which our method successfully extracted con-
trastive keyword pairs.
To detect such unidentified contrastive rela-
tions, it is necessary to robustly extract contrastive
keyword pairs. In the future, we will employ a
bootstrapping approach to identify patterns of di-
rect contrastive statements and contrastive key-
!"#$"%&'(#)*$+#'#,-#&'!
+.$&.'$!"#$"%&'(#)*$+#'#,-#&'!
!"#$"./0!
1/"($12'($"%&'(#)*$+#'#,-#&'!
1/"($12'($"./0!
3!#,%4$"%&'(#)*$+#'#,-#&'!
5678$92#1$.:$;!"#$"%&'(#)*$+#'#,-#&'<!
5678$92#1$.:$;+.$&.'$!"#$"%&'(#)*$+#'#,-#&'<!
Figure 2: A view of major, contradictory and con-
trastive statements in WISDOM.
word pairs. We will also use patterns of con-
trastive discourse structures as well as those of
predicate-argument structures.
5 Conclusion
This paper has described a method for producing a
bird?s-eye view of statements that are expressed in
Web pages on a given topic. This method aggre-
gates statements relevant to the topic and shows
the contradictory/contrastive relations and state-
ments among them.
In particular, we successfully extracted direct
contrastive statements in an unsupervised man-
ner. We specified only several words for the
extraction patterns and the filtering. Therefore,
our method for Japanese is thought to be easily
adapted to other languages. We also proposed
a novel method for identifying contrastive rela-
tions between statements, which included cross-
document implicit relations. These relations com-
plemented direct contrastive statements.
We have incorporated our proposed method
into an information analysis system, WISDOM4
(Akamine et al, 2009), which can show multi-
faceted information on a given topic. Now, this
system can show contradictory/contrastive rela-
tions and statements as well as their contexts as
a view of KWIC (keyword in context) (Figure 2).
This kind of presentation facilitates users? under-
standing of an input topic.
4http://wisdom-nict.jp/
541
References
Aizawa, Akiko. 2007. On calculating word similarity
using web as corpus. In Proceedings of IEICE Tech-
nical Report, SIG-ICS, pages 45?52 (in Japanese).
Akamine, Susumu, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-
hashi, and Yutaka Kidawara. 2009. WISDOM:
A web information credibility analysis system. In
Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 1?4.
Bentivogli, Luisa, Ido Dagan, Hoa Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In Proceedings of TAC 2009 Workshop.
de Marneffe, Marie-Catherine, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047.
Ennals, Rob, Beth Trushkowsky, and John Mark
Agosta. 2010. Highlighting disputed claims on the
web. In Proceedings of WWW 2010.
Ganapathibhotla, Murthy and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings
of COLING 2008, pages 241?248.
Harabagiu, Sanda, Andrew Hickl, and Finley Laca-
tusu. 2006. Negation, contrast and contradiction
in text processing. In Proceedings of AAAI-06.
Jindal, Nitin and Bing Liu. 2006a. Identifying com-
parative sentences in text documents. In Proceed-
ings of SIGIR 2006.
Jindal, Nitin and Bing Liu. 2006b. Mining compar-
ative sentences and relations. In Proceedings of
AAAI-06.
Kawahara, Daisuke, Sadao Kurohashi, and Kentaro
Inui. 2008. Grasping major statements and their
contradictions toward information credibility analy-
sis of web contents. In Proceedings of WI?08, short
paper, pages 393?397.
Lerman, Kevin and Ryan McDonald. 2009. Con-
trastive summarization: An experiment with con-
sumer reviews. In Proceedings of NAACL-HLT
2009, Companion Volume: Short Papers, pages
113?116.
Lin, Dekang. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774.
Mann, William and Sandra Thompson. 1988. Rhetor-
ical structure theory: toward a functional theory of
text organization. Text, 8(3):243?281.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009. Statement map: Assisting
information credibility analysis by visualizing argu-
ments. In Proceedings of WICOW 2009.
Ohshima, Hiroaki, Satoshi Oyama, and Katsumi
Tanaka. 2006. Searching coordinate terms with
their context from the web. In Proceedings of WISE
2006, pages 40?47.
Ritter, Alan, Stephen Soderland, Doug Downey, and
Oren Etzioni. 2008. It?s a contradiction ? no, it?s
not: A case study using functional relations. In Pro-
ceedings of EMNLP 2008, pages 11?20.
Satou, Toshinori and Manabu Okumura. 2007. Ex-
traction of comparative relations from Japanese we-
blog. In IPSJ SIG Technical Report 2007-NL-181,
pages 7?14 (in Japanese).
Shibata, Tomohide, Michitaka Odani, Jun Harashima,
Takashi Oonishi, and Sadao Kurohashi. 2008.
SYNGRAPH: A flexible matching method based on
synonymous expression extraction from an ordinary
dictionary and a web corpus. In Proceedings of IJC-
NLP 2008, pages 787?792.
Shibata, Tomohide, Yasuo Banba, Keiji Shinzato, and
Sadao Kurohashi. 2009. Web information organi-
zation using keyword distillation based clustering.
In Proceedings of WI?09, short paper, pages 325?
330.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. TSUBAKI: An open search engine in-
frastructure for developing new information access
methodology. In Proceedings of IJCNLP 2008,
pages 189?196.
Somasundaran, Swapna and Janyce Wiebe. 2009.
Recognizing stances in online debates. In Proceed-
ings of ACL-IJCNLP 2009, pages 226?234.
Voorhees, Ellen M. 2008. Contradictions and justi-
fications: Extensions to the textual entailment task.
In Proceedings of ACL-08: HLT, pages 63?71.
Yang, Seon and Youngjoong Ko. 2009. Extract-
ing comparative sentences from korean text docu-
ments using comparative lexical patterns and ma-
chine learning techniques. In Proceedings of ACL-
IJCNLP 2009 Conference Short Papers, pages 153?
156.
542
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 786?794,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dependency Tree-based Sentiment Classification using CRFs with Hidden
Variables
Tetsuji Nakagawa?, Kentaro Inui?? and Sadao Kurohashi??
?National Institute of Information and Communications Technology
?Tohoku University
?Kyoto University
tnaka@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we present a dependency tree-
based method for sentiment classification of
Japanese and English subjective sentences us-
ing conditional random fields with hidden
variables. Subjective sentences often con-
tain words which reverse the sentiment po-
larities of other words. Therefore, interac-
tions between words need to be considered
in sentiment classification, which is difficult
to be handled with simple bag-of-words ap-
proaches, and the syntactic dependency struc-
tures of subjective sentences are exploited in
our method. In the method, the sentiment po-
larity of each dependency subtree in a sen-
tence, which is not observable in training data,
is represented by a hidden variable. The po-
larity of the whole sentence is calculated in
consideration of interactions between the hid-
den variables. Sum-product belief propaga-
tion is used for inference. Experimental re-
sults of sentiment classification for Japanese
and English subjective sentences showed that
the method performs better than other meth-
ods based on bag-of-features.
1 Introduction
Sentiment classification is a useful technique for an-
alyzing subjective information in a large number of
texts, and many studies have been conducted (Pang
and Lee, 2008). A typical approach for sentiment
classification is to use supervised machine learning
algorithms with bag-of-words as features (Pang et
al., 2002), which is widely used in topic-based text
classification. In the approach, a subjective sen-
tence is represented as a set of words in the sen-
tence, ignoring word order and head-modifier rela-
tion between words. However, sentiment classifi-
cation is different from traditional topic-based text
classification. Topic-based text classification is gen-
erally a linearly separable problem ((Chakrabarti,
2002), p.168). For example, when a document con-
tains some domain-specific words, the document
will probably belong to the domain. However, in
sentiment classification, sentiment polarities can be
reversed. For example, let us consider the sentence
?The medicine kills cancer cells.? While the phrase
cancer cells has negative polarity, the word kills re-
verses the polarity, and the whole sentence has pos-
itive polarity. Thus, in sentiment classification, a
sentence which contains positive (or negative) polar-
ity words does not necessarily have the same polar-
ity as a whole, and we need to consider interactions
between words instead of handling words indepen-
dently.
Recently, several methods have been proposed to
cope with the problem (Zaenen, 2004; Ikeda et al,
2008). However, these methods are based on flat
bag-of-features representation, and do not consider
syntactic structures which seem essential to infer
the polarity of a whole sentence. Other methods
have been proposed which utilize composition of
sentences (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Jia et al, 2009), but these methods
use rules to handle polarity reversal, and whether po-
larity reversal occurs or not cannot be learned from
labeled data. Statistical machine learning can learn
useful information from training data and generally
robust for noisy data, and using it instead of rigid
rules seems useful. Wilson et al (2005) proposed
a method for sentiment classification which utilizes
head-modifier relation and machine learning. How-
ever, the method is based on bag-of-features and po-
larity reversal occurred by content words is not han-
dled. One issue of the approach to use sentence
composition and machine learning is that only the
whole sentence is labeled with its polarity in gen-
eral corpora for sentiment classification, and each
component of the sentence is not labeled, though
such information is necessary for supervised ma-
786
Whole Dependency Tree
Polarities of Dependency Subtrees
It cancer and heart disease.
prevents
cancer and heart disease.
prevents
cancer and heart disease.
+? ?
Figure 1: Polarities of Dependency Subtrees
chine learning to infer the sentence polarity from its
components.
In this paper, we propose a dependency tree-based
method for Japanese and English sentiment classifi-
cation using conditional random fields (CRFs) with
hidden variables. In the method, the sentiment po-
larity of each dependency subtree, which is not ob-
servable in training data, is represented by a hidden
variable. The polarity of the whole sentence is cal-
culated in consideration of interactions between the
hidden variables.
The rest of this paper is organized as follows: Sec-
tion 2 describes a dependency tree-based method
for sentiment classification using CRFs with hid-
den variables, and Section 3 shows experimental re-
sults on Japanese and English corpora. Section 4
discusses related work, and Section 5 gives conclu-
sions.
2 Dependency Tree-based Sentiment
Classification using CRFs with Hidden
Variables
In this study, we handle a task to classify the polar-
ities (positive or negative) of given subjective sen-
tences. In the rest of this section, we describe a prob-
abilistic model for sentiment classification based on
dependency trees, methods for inference and param-
eter estimation, and features we use.
2.1 A Probabilistic Model based on
Dependency Trees
Let us consider the subjective sentence ?It prevents
cancer and heart disease.? In the sentence, cancer
and heart disease have themselves negative polari-
It cancer and heart disease.prevents
s0+
<root>
s10 s2+ s3? s4?
Figure 2: Probabilistic Model based on Dependency Tree
s0 s1 s2 s3 s4
g1 g2 g3 g4
g5g6 g7 g8
Figure 3: Factor Graph
ties. However, the polarities are reversed by modi-
fying the word prevents, and the dependency subtree
?prevents cancer and heart disease? has positive po-
larity. As a result, the whole dependency tree ?It
prevents cancer and heart disease.? has positive po-
larity (Figure 1). In such a way, we can consider
the sentiment polarity for each dependency subtree
of a subjective sentence. Note that we use phrases as
a basic unit instead of words in this study, because
phrases are useful as a meaningful unit for sentiment
classification1. In this paper, a dependency subtree
means the subtree of a dependency tree whose root
node is one of the phrases in the sentence.
We use a probabilistic model as shown in Fig-
ure 2. We consider that each phrase in the subjective
sentence has a random variable (indicated by a cir-
cle in Figure 2). The random variable represents the
polarity of the dependency subtree whose root node
is the corresponding phrase. Two random variables
are dependent (indicated by an edge in Figure 2) if
their corresponding phrases have head-modifier re-
lation in the dependency tree. The node denoted as
<root> in Figure 2 indicates a virtual phrase which
represents the root node of the sentence, and we re-
gard that the random variable of the root node is the
polarity of the whole sentence. In usual annotated
corpora for sentiment classification, only each sen-
tence is labeled with its polarity, and each phrase
(dependency subtree) is not labeled, so all the ran-
dom variables except the one for the root node are
1From an empirical view, in our preliminary experiments
with the proposed method, phrase-based processing performed
better than word-based processing in accuracy and in computa-
tional efficiency.
787
hidden variables that cannot be observed in labeled
data (indicated by gray circles in Figure 2). With
such a probabilistic model, it is possible to utilize
properties such that phrases which contain positive
(or negative) words tend to have positive (negative)
polarities, and two phrases with head-modifier rela-
tion tend to have opposite polarities if the head con-
tains a word which reverses sentiment polarity.
Next, we define the probabilistic model as shown
in Figure 2 in detail. Let n denote the number of
phrases in a subjective sentence, wi the i-th phrase,
and hi the head index of the i-th phrase. Let si de-
note the random variable which represents the po-
larity of the dependency subtree whose root is the
i-th phrase (si ? {+1,?1}), and let p denote the
polarity of the whole sentence (p ? {+1,?1}). We
regard the 0-th phrase as a virtual phrase which rep-
resents the root of the sentence. w,h, s respectively
denote the sequence of wi, hi, si.
w = w1 ? ? ?wn, h = h1 ? ? ?hn, s = s0 ? ? ? sn,
p = s0.
For the example sentence in Figure 1, w1 =It,
w2 =prevents, w3 =cancer, w4 =and heart dis-
ease., h1 = 2, h2 = 0, h3 = 2, h4 = 2. We define
the joint probability distribution of the sentiment po-
larities of dependency subtrees s, given a subjective
sentence w and its dependency tree h, using log-
linear models:
P?(s|w,h)=
1
Z?(w,h)
exp
{ K
?
k=1
?kFk(w,h, s)
}
,
(1)
Z?(w,h)=
?
s
exp
{ K
?
k=1
?kFk(w,h, s)
}
, (2)
Fk(w,h, s)=
n
?
i=1
fk(i,w,h, s), (3)
where ? = {?1, ? ? ? , ?K} is the set of parameters
of the model. fk(i,w,h, s) is the feature function
of the i-th phrase, and is classified to node feature
which considers only the corresponding node, or
edge feature which considers both the correspond-
ing node and its head, as follows:
fk(i,w,h, s)=
{ fnk (wi, si) (k ? Kn),
f ek(wi, si, whi , shi) (k ? Ke),
(4)
where Kn and Ke respectively represent the sets of
indices of node features and edge features.
2.2 Classification of Sentiment Polarity
Let us consider how to infer the sentiment polarity
p ? {+1,?1}, given a subjective sentence w and
its dependency tree h. The polarity of the root node
(s0) is regarded as the polarity of the whole sentence,
and p can be calculated as follows:
p=argmax
p?
P?(p?|w,h), (5)
P?(p|w,h)=
?
s:s0=p
P?(s|w,h). (6)
That is, the polarity of the subjective sentence is ob-
tained as the marginal probability of the root node
polarity, by summing the probabilities for all the
possible configurations of hidden variables. How-
ever, enumerating all the possible configurations of
hidden variables is computationally hard, and we use
sum-product belief propagation (MacKay, 2003) for
the calculation.
Belief propagation enables us to efficiently calcu-
late marginal probabilities. In this study, the graph-
ical model to be solved has a tree structure (identi-
cal to the syntactic dependency tree) which has no
loops, and an exact solution can be obtained us-
ing belief propagation. Dependencies among ran-
dom variables in Figure 2 are represented by a factor
graph in Figure 3. The factor graph consists of vari-
able nodes si indicated by circles, and factor (fea-
ture) nodes gi indicated by squares. In the exam-
ple in Figure 3, gi(1 ? i ? 4) correspond to the
node features in Equation (4), and gi(5 ? i ? 8)
correspond to the edge features. In belief propa-
gation, marginal distribution is calculated by pass-
ing messages (beliefs) among the variables and fac-
tors connected by edges in the factor graph (Refer
to (MacKay, 2003) for detailed description of belief
propagation).
2.3 Parameter Estimation
Let us consider how to estimate model parameters?,
given L training examples D = {?wl,hl, pl?}Ll=1.
In this study, we use the maximum a posteriori es-
timation with Gaussian priors for parameter estima-
tion. We define the following objective function L?,
788
and calculate the parameters ?? which maximize the
value:
L?=
L
?
l=1
logP?(pl|wl,hl) ?
1
2?2
K
?
k=1
?2k, (7)
??=argmax
?
L?, (8)
where ? is a parameter of Gaussian priors and is set
to 1.0 in later experiments. The partial derivatives of
L? are as follows:
?L?
??k
=
L
?
l=1
[
?
s
P?(s|wl,hl, pl)Fk(wl,hl, s)
?
?
s
P?(s|wl,hl)Fk(wl,hl, s)
]
? 1
?2
?k.
(9)
The model parameters can be calculated with the
L-BFGS quasi-Newton method (Liu and Nocedal,
1989) using the objective function and its partial
derivatives. While the partial derivatives contain
summation over all the possible configurations of
hidden variables, it can be calculated efficiently us-
ing belief propagation as explained in Section 2.2.
This parameter estimation method is same to one
used for Latent-Dynamic Conditional Random Field
(Morency et al, 2007). Note that the objective func-
tion L? is not convex, and there is no guarantee for
global optimality. The estimated model parameters
depend on the initial values of the parameters, and
the setting of the initial values of model parameters
will be explained in Section 2.4.
2.4 Features
Table 1 shows the features used in this study. Fea-
tures (a)?(h) in Table 1 are used as the node fea-
tures (Equation (4)) for the i-th phrase, and fea-
tures (A)?(E) are used as the edge features for the
i-th and j-th phrases (j=hi). In Table 1, si denotes
the hidden variable which represents the polarity of
the dependency subtree whose root node is the i-
th phrase, qi denotes the prior polarity of the i-th
phrase (explained later), ri denotes the polarity re-
versal of the i-th phrase (explained later), mi de-
notes the number of words in the i-th phrase, ui,k,
bi,k, ci,k, fi,k respectively denote the surface form,
base form, coarse-grained part-of-speech (POS) tag,
Node Features
a si
b si&qi
c si&qi&ri
d si&ui,1, ? ? ? , si&ui,mi
e si&ci,1, ? ? ? , si&ci,mi
f si&fi,1, ? ? ? , si&fi,mi
g si&ui,1&ui,2, ? ? ? , si&ui,mi?1&ui,mi
h si&bi,1&bi,2, ? ? ? , si&bi,mi?1&bi,mi
Edge Features
A si&sj
B si&sj&rj
C si&sj&rj&qj
D si&sj&bi,1, ? ? ? , si&sj&bi,mi
E si&sj&bj,1, ? ? ? , si&sj&bj,mj
Table 1: Features Used in This Study
fine-grained POS tag of the k-th word in the i-th
phrase.
We used the morphological analysis system JU-
MAN and the dependency parser KNP2 for pro-
cessing Japanese data, and the POS tagger MX-
POST (Ratnaparkhi, 1996) and the dependency
parser MaltParser3 for English data. KNP outputs
phrase-based dependency trees, but MaltParser out-
puts word-based dependency trees, and we con-
verted the word-based ones to phrase-based ones us-
ing simple heuristic rules explained in Appendix A.
The prior polarity of a phrase qi ? {+1, 0,?1} is
the innate sentiment polarity of a word contained in
the phrase, which can be obtained from sentiment
polarity dictionaries. We used sentiment polarity
dictionaries made by Kobayashi et al (2007) and Hi-
gashiyama et al (2008)4 for Japanese experiments
(The resulting dictionary contains 6,974 positive ex-
pressions and 8,428 negative expressions), and a dic-
tionary made by Wilson et al (2005)5 for English
experiments (The dictionary contains 2,289 positive
expressions and 4,143 negative expressions). When
a phrase contains the words registered in the dictio-
naries, its prior polarity is set to the registered po-
larity, otherwise the prior polarity is set to 0. When
a phrase contains multiple words in the dictionaries,
the registered polarity of the last (nearest to the end
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
3http://maltparser.org/
4http://cl.naist.jp/?inui/research/EM/sentiment-lexicon.html
5http://www.cs.pitt.edu/mpqa/
789
of the sentence) word is used.
The polarity reversal of a phrase ri ? {0, 1} rep-
resents whether it reverses the polarities of other
phrases (1) of not (0). We prepared polarity revers-
ing word dictionaries, and the polarity reversal of
a phrase is set to 1 if the phrase contains a word
in the dictionaries, otherwise set to 0. We con-
structed polarity reversing word dictionaries which
contain such words as decrease and vanish that re-
verse sentiment polarity. A Japanese polarity revers-
ing word dictionary was constructed from an auto-
matically constructed corpus, and the construction
procedure is described in Appendix B (The dictio-
nary contains 219 polarity reversing words). An
English polarity reversing word dictionary was con-
structed from the General Inquirer dictionary6 in the
same way as Choi and Cardie (2008), by collecting
words which belong to either NOTLW or DECREAS
categories (The dictionary contains 121 polarity re-
versing words).
Choi and Cardie (2008) categorized polarity re-
versing words into two categories: function-word
negators such as not and content-word negators such
as eliminate. The polarity reversal of a phrase ri ex-
plained above handles only the content-word nega-
tors, and function-word negators are handled in an-
other way, since the scope of a function-word nega-
tor is generally limited to the phrase containing it in
Japanese, and the number of function-word negators
is small. The prior polarity qi and the polarity rever-
sal ri of a phrase are changed to the following q?i and
r?i, if the phrase contains a function-word negator (in
Japanese) or if the phrase is modified by a function-
word negator (in English):
q?i=?qi, (10)
r?i=1 ? ri. (11)
In this paper, unless otherwise noted, the word po-
larity reversal is used to indicate polarity reversing
caused by content-word negators, and function-word
negators are assumed to be applied to qi and ri in the
above way beforehand.
As described in Section 2.3, there is no guaran-
tee of global optimality for estimated parameters,
since the objective function is not convex. In our
6http://www.wjh.harvard.edu/ inquirer/
preliminary experiments, L-BFGS often did not con-
verge and classification accuracy was unstable when
the initial values of parameters were randomly set.
Therefore, in later experiments, we set the initial
values in the following way. For the feature (A) in
Table 1 in which si and sj are equal, we set the ini-
tial parameter ?i of the feature to a random number
in [0.9, 1.1], otherwise we set to a random number in
[?0.1, 0.1]7. By setting such initial values, the initial
model parameters have a property that two phrases
with head-modifier relation tend to have the same
polarity, which is intuitively reasonable.
3 Experiments
We conducted experiments of sentiment classifica-
tion on four Japanese corpora and four English cor-
pora.
3.1 Data
We used four corpora for experiments of Japanese
sentiment classification: the Automatically Con-
structed Polarity-tagged corpus (ACP) (Kaji and
Kitsuregawa, 2006), the Kyoto University and NTT
Blog corpus (KNB) 8, the NTCIR Japanese opinion
corpus (NTC-J) (Seki et al, 2007; Seki et al, 2008),
the 50 Topics Evaluative Information corpus (50
Topics) (Nakagawa et al, 2008). The ACP corpus
is an automatically constructed corpus from HTML
documents on the Web using lexico-syntactic pat-
terns and layout structures. The size of the corpus
is large (it consists of 650,951 instances), and we
used 1/100 of the whole corpus. The KNB corpus
consists of Japanese blogs, and is manually anno-
tated. The NTC-J corpus consists of Japanese news-
paper articles. There are two NTCIR Japanese opin-
ion corpora available, the NTCIR-6 corpus and the
NTCIR-7 corpus; and we combined the two cor-
pora. The 50 Topics corpus is collected from various
pages on the Web, and is manually annotated.
We used four corpora for experiments of English
sentiment classification: the Customer Review data
7The values of most learned parameters distributed between
-1.0 and 1.0 in our preliminary experiments. Therefore, we de-
cided to give values around the upper bound (1.0) and the mean
(0.0) to the features in order to incorporate minimal prior knowl-
edge into the model.
8http://nlp.kuee.kyoto-u.ac.jp/kuntt/
790
(CR)9, the MPQA Opinion corpus (MPQA)10, the
Movie Review Data (MR) 11, and the NTCIR En-
glish opinion corpus (NTC-E) (Seki et al, 2007;
Seki et al, 2008). The CR corpus consists of re-
view articles about products such as digital cameras
and cellular phones. There are two customer review
datasets, the 5 products dataset and the 9 products
dataset, and we combined the two datasets. In the
MPQA corpus, sentiment polarities are attached not
to sentences but expressions (sub-sentences), and we
regarded the expressions as sentences and classified
the polarities. There are two NTCIR English cor-
pora available, the NTCIR-6 corpus and the NTCIR-
7 corpus, and we combined the two corpora.
The statistical information of the corpora we used
is shown in Table 2. We randomly split each corpus
into 10 portions, and conducted 10-fold cross valida-
tion. Accuracy of sentiment classification was cal-
culated as the number of correctly predicted labels
(polarities) divided by the number of test examples.
3.2 Compared Methods
We compared our method to 6 baseline methods,
and this section describes them. In the following,
p0 ? {+1,?1} denotes the major polarity in train-
ing data, Hi denotes the set consisting of all the an-
cestor nodes of the i-th phrase in the dependency
tree, and sgn(x) is defined as below:
sgn(x)=
?
?
?
?
?
+1 (x > 0),
0 (x = 0),
?1 (x < 0).
Voting without Polarity Reversal The polarity of
a subjective sentence is decided by voting of
each phrase?s prior polarity. In the case of a
tie, the major polarity in the training data is
adopted.
p=sgn
( n
?
i=1
qi + 0.5p0
)
. (12)
Voting with Polarity Reversal Same to Voting
without Polarity Reversal, except that the po-
larities of phrases which have odd numbers of
9http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
10http://www.cs.pitt.edu/mpqa/
11http://www.cs.cornell.edu/People/pabo/movie-review-
data/
reversal phrases in their ancestors are reversed
before voting.
p=sgn
( n
?
i=1
qi
?
j?Hi
(?1)rj + 0.5p0
)
. (13)
Rule The polarity of a subjective sentence is deter-
ministically decided basing on rules, by con-
sidering the sentiment polarities of dependency
subtrees. The polarity of the dependency sub-
tree whose root is the i-th phrase is decided by
voting the prior polarity of the i-th phrase and
the polarities of the dependency subtrees whose
root nodes are the modifiers of the i-th phrase.
The polarities of the modifiers are reversed if
their head phrase has a reversal word. The de-
cision rule is applied from leaf nodes in the de-
pendency tree, and the polarity of the root node
is decided at the last.
si=sgn
(
qi +
?
j:hj=i
sj(?1)ri
)
, (14)
p=sgn(s0 + 0.5p0). (15)
Bag-of-Features with No Dictionaries The polar-
ity of a subjective sentence is classified us-
ing Support Vector Machines. Surface forms,
base forms, coarse-grained POS tags and fine-
grained POS tags of word unigrams and bi-
grams in the subjective sentence are used as
features12. The second order polynomial ker-
nel is used and the cost parameter C is set to
1.0. No prior polarity information (dictionary)
is used.
Bag-of-Features without Polarity Reversal Same
to Bag-of-Features with No Dictionaries, ex-
cept that the voting result of prior polarities
(one of positive, negative or tie) is also used
as a feature.
Bag-of-Features with Polarity Reversal Same to
Bag-of-Features without Polarity Reversal, ex-
cept that the polarities of phrases which have
12In experiments on English corpora, only the features of un-
igrams are used and those of bigrams are not used, since the
bigram features decreased accuracies in our preliminary experi-
ments as reported in previous work (Andreevskaia and Bergler,
2008).
791
Language Corpus Number of Instances (Positive / Negative)
ACP 6,510 (2,738 / 3,772)
Japanese KNB 2,288 (1,423 / 865)
NTC-J 3,485 (1,083 / 2,402)
50 Topics 5,366 (3,175 / 2,191)
CR 3,772 (2,406 / 1,366)
English MPQA 10,624 (3,316 / 7,308)
MR 10,662 (5,331 / 5,331)
NTC-E 3,812 (1,226 / 2,586)
Table 2: Statistical Information of Corpora
Method Japanese English
ACP KNB NTC-J 50 Topics CR MPQA MR NTC-E
Voting-w/o Rev. 0.686 0.764 0.665 0.727 0.714 0.804 0.629 0.730
Voting-w/ Rev. 0.732 0.792 0.714 0.765 0.742 0.817 0.631 0.740
Rule 0.734 0.792 0.742 0.764 0.743 0.818 0.629 0.750
BoF-no Dic. 0.798 0.758 0.754 0.761 0.793 0.818 0.757 0.768
BoF-w/o Rev. 0.812 0.823 0.794 0.805 0.802 0.840 0.761 0.793
BoF-w/ Rev. 0.822 0.830 0.804 0.819 0.814 0.841 0.764 0.797
Tree-CRF 0.846* 0.847* 0.826* 0.841* 0.814 0.861* 0.773* 0.804
(* indicates statistical significance at p < 0.05)
Table 3: Accuracy of Sentiment Classification
odd numbers of reversal phrases in their ances-
tors are reversed before voting.
Tree-CRF The proposed method based on depen-
dency trees using CRFs, described in Section 2.
3.3 Experimental Results
The experimental results are shown in Table 3. The
proposed method Tree-CRF obtained the best ac-
curacies for all the four Japanese corpora and the
four English corpora, and the differences against
the second best methods were statistically signifi-
cant (p < 0.05) with the paired t-test for the six
of the eight corpora. Tree-CRF performed better
for the Japanese corpora than for the English cor-
pora. For both the Voting methods and the Bag-of-
Features methods, the methods with polarity rever-
sal performed better than those without it13.
Both BoF-w/ Rev. and Tree-CRF use supervised
machine learning and the same dictionaries (the
13The Japanese polarity reversing word dictionary was con-
structed from the ACP corpus as described in Appendix B, and
it is not reasonable to compare the methods with and without
polarity reversal on the ACP corpus. However, the tendency
can be seen on the other 7 corpora.
prior polarity dictionaries and the polarity revers-
ing word dictionaries), but the latter performed bet-
ter than the former. Our error analysis showed that
BoF-w/ Rev. was not robust for erroneous words in
the prior polarity dictionaries. BoF-w/ Rev. uses the
voting result of the prior polarities as a feature, and
the feature is sensitive to the errors in the dictionary,
while Tree-CRF uses several information as well as
the prior polarities to decide the polarities of depen-
dency subtrees, and was robust to the dictionary er-
rors. We investigated the trained model parameters
of Tree-CRF, and found that the features (E) in Ta-
ble 1, in which the head and the modifier have op-
posite polarities and the head word is such as pro-
tect and withdraw, have large positive weights. Al-
though these words were not included in the polar-
ity reversing word dictionary, the property that these
words reverse polarities of other words seems to be
learned with the model.
4 Related Work
Various studies on sentiment classification have
been conducted, and there are several methods pro-
792
posed for handling reversal of polarities. In this pa-
per, our method was not directly compared with the
other methods, since it is difficult to completely im-
plement them or conduct experiments with exactly
the same settings.
Choi and Cardie (2008) proposed a method to
classify the sentiment polarity of a sentence bas-
ing on compositional semantics. In their method,
the polarity of the whole sentence is determined
from the prior polarities of the composing words by
pre-defined rules, and the method differs from ours
which uses the probabilistic model to handle interac-
tions between hidden variables. Syntactic structures
were used in the studies of Moilanen and Pulman
(2007) and, Jia et al (2009), but their methods are
based on rules and supervised learning was not used
to handle polarity reversal. As discussed in Sec-
tion 1, Wilson et al (2005) studied a bag-of-features
based statistical sentiment classification method in-
corporating head-modifier relation.
Ikeda et al (2008) proposed a machine learning
approach to handle sentiment polarity reversal. For
each word with prior polarity, whether the polarity is
reversed or not is learned with a statistical learning
algorithm using its surrounding words as features.
The method can handle only words with prior polar-
ities, and does not use syntactic dependency struc-
tures.
Conditional random fields with hidden variables
have been studied so far for other tasks. Latent-
Dynamic Conditional Random Fields (LDCRF)
(Morency et al, 2007; Sun et al, 2008) are prob-
abilistic models with hidden variables for sequen-
tial labeling, and belief propagation is used for in-
ference. Out method is similar to the models, but
there are several differences. In our method, only
one variable which represents the polarity of the
whole sentence is observable, and dependency re-
lation among random variables is not a linear chain
but a tree structure which is identical to the syntactic
dependency.
5 Conclusion
In this paper, we presented a dependency tree-based
method for sentiment classification using condi-
tional random fields with hidden variables. In this
method, the polarity of each dependency subtree
of a subjective sentence is represented by a hid-
den variable. The values of the hidden variables
are calculated in consideration of interactions be-
tween variables whose nodes have head-modifier re-
lation in the dependency tree. The value of the
hidden variable of the root node is identified with
the polarity of the whole sentence. Experimental
results showed that the proposed method performs
better for Japanese and English data than the base-
line methods which represents subjective sentences
as bag-of-features.
Appendix
A Rules for Converting Word Sequence to
Phrase Sequence
Let v1, ? ? ? , vN denote an English word sequence, yi
the part-of-speech of the i-th word, and zi the head
index of the i-th word. The word sequence was con-
verted to a phrase sequence as follows, by applying
rules which combine two adjacent words:
LT ? {?,(,-LRB-,-LSB-,-LCB-,CC}
RT ? {?,),,,--,.,:,POS,-RRB-,-RSB-,-RCB-}
PP ? {IN,RP,TO,DT,PDT,PRP,WDT,WP,WP$,WRB}
NN ? {CD,FW,NN,NNP,NNPS,NNS,SYM,JJ}
do
for i := 1 to N ? 1
if xi and xi+1 are not yet combined ?
(xi ? LT ?
xi+1 ? RT ?
((yi = yi+1 ? yi = i+ 1 ? yi+1 = i) ?
(xi ? PP ?
(xi ? NN ? xi+1 ? NN )))) then
Combine the words vi and vi+1
until No rules are applied
B Construction of Japanese Polarity
Reversing Word Dictionary
We constructed a Japanese polarity reversing word
dictionary from the Automatically Constructed
Polarity-tagged corpus (Kaji and Kitsuregawa,
2006). First, we collected sentences, each of which
contains just one phrase having prior polarity, and
the phrase modifies a phrase which modifies the root
node. Among them, we selected sentences in which
the prior polarity is not equal to the polarity of the
whole sentence. We extracted all the words in the
head phrase, and manually checked them whether
they should be put into the dictionary or not. The ra-
tionale behind the procedure is that the prior polarity
can be considered to be reversed by a certain word
in the head phrase.
793
References
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Overcom-
ing Domain Dependence in Sentiment Tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 290?298.
Soumen Chakrabarti. 2002. Mining the Web: Dis-
covering Knowledge from Hypertext Data. Morgan-
Kauffman.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Masahiko Higashiyama, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring Noun Polarity Knowledge
Using Selectional Preferences. In Proceedings of the
14th Annual Meeting of the Association for Natural
Language Processing, pages 584?587. (in Japanese).
Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov, and
Manabu Okumura. 2008. Learning to Shift the Po-
larity of Words for Sentiment Classification. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Language Processing, pages 296?303.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The Ef-
fect of Negation on Sentiment Analysis and Retrieval
Effectiveness. In Proceeding of the 18th ACM Con-
ference on Information and Knowledge Management,
pages 1827?1830.
Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Auto-
matic Construction of Polarity-Tagged Corpus from
HTML Documents. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 452?459.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Opinion Mining from Web Documents: Extrac-
tion and Structurization. Journal of the Japanese So-
ciety for Artificial Intelligence, 22(2):227?238.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
David J. C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of the Recent Advances
in Natural Language Processing International Confer-
ence, pages 378?382.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-Dynamic Discriminative Mod-
els for Continuous Gesture Recognition. In Proceed-
ings of the 2007 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1?8.
Tetsuji Nakagawa, Takuya Kawada, Kentaro Inui, and
Sadao Kurohashi. 2008. Extracting Subjective and
Objective Evaluative Expressions from the Web. In
Proceedings of the 2nd International Symposium on
Universal Communication.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
1996 Conference on Empirical Methods in Natural
Language Processing Conference, pages 133?142.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-His
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of Opinion Analysis Pilot Task at NTCIR-
6. In Proceedings of the 6th NTCIR Workshop, pages
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kando. 2008. Overview
ofMultilingual Opinion Analysis Task at NTCIR-7. In
Proceedings of the 7th NTCIR Workshop.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 841?848.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Joint Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354.
Livia Polanyi Annie Zaenen. 2004. Contextual Lexical
Valence Shifters. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
794
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?391,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Is a 204 cm Man Tall or Small ?
Acquisition of Numerical Common Sense from the Web
Katsuma Narisawa1 Yotaro Watanabe1 Junta Mizuno2
Naoaki Okazaki1,3 Kentaro Inui1
1Graduate School of Information Sciences, Tohoku University
2National Institute of Information and Communications Technology (NICT)
3Japan Science and Technology Agency (JST)
{katsuma,yotaro-w,junta-m,okazaki,inui}@ecei.tohoku.ac.jp
Abstract
This paper presents novel methods for
modeling numerical common sense: the
ability to infer whether a given number
(e.g., three billion) is large, small, or nor-
mal for a given context (e.g., number of
people facing a water shortage). We first
discuss the necessity of numerical com-
mon sense in solving textual entailment
problems. We explore two approaches for
acquiring numerical common sense. Both
approaches start with extracting numeri-
cal expressions and their context from the
Web. One approach estimates the distribu-
tion of numbers co-occurring within a con-
text and examines whether a given value is
large, small, or normal, based on the distri-
bution. Another approach utilizes textual
patterns with which speakers explicitly ex-
presses their judgment about the value of
a numerical expression. Experimental re-
sults demonstrate the effectiveness of both
approaches.
1 Introduction
Textual entailment recognition (RTE) involves a
wide range of semantic inferences to determine
whether the meaning of a hypothesis sentence (h)
can be inferred from another text (t) (Dagan et
al., 2006). Although several evaluation campaigns
(e.g., PASCAL/TAC RTE challenges) have made
significant progress, the RTE community recog-
nizes the necessity of a deeper understanding of
the core phenomena involved in textual inference.
Such recognition comes from the ideas that cru-
cial progress may derive from decomposing the
complex RTE task into basic phenomena and from
solving each basic phenomenon separately (Ben-
tivogli et al, 2010; Sammons et al, 2010; Cabrio
and Magnini, 2011; Toledo et al, 2012).
Given this background, we focus on solving one
of the basic phenomena in RTE: semantic infer-
ence related to numerical expressions. The spe-
cific problem we address is acquisition of numeri-
cal common sense. For example,
(1) t : Before long, 3b people will face a water
shortage in the world.
h : Before long, a serious water shortage
will occur in the world.
Although recognizing the entailment relation be-
tween t and h is frustratingly difficult, we assume
this inference is decomposable into three phases:
3b people face a water shortage.
? 3,000,000,000 people face a water shortage.
|= many people face a water shortage.
|= a serious water shortage.
In the first phase, it is necessary to recognize 3b
as a numerical expression and to resolve the ex-
pression 3b into the exact amount 3,000,000,000.
The second phase is much more difficult because
we need subjective but common-sense knowledge
that 3,000,000,000 people is a large number.
In this paper, we address the first and sec-
ond phases of inference as an initial step towards
semantic processing with numerical expressions.
The contributions of this paper are four-fold.
1. We examine instances in existing RTE cor-
pora, categorize them into groups in terms of
the necessary semantic inferences, and dis-
cuss the impact of this study for solving RTE
problems with numerical expressions.
2. We describe a method of normalizing numer-
ical expressions referring to the same amount
in text into a unified semantic representation.
3. We present approaches for aggregating nu-
merical common sense from examples of nu-
merical expressions and for judging whether
a given amount is large, small, or normal.
382
4. We demonstrate the effectiveness of this ap-
proach, reporting experimental results and
analyses in detail. Although it would be ideal
to evaluate the impact of this study on the
overall RTE task, we evaluate each phase sep-
arately. We do this because the existing RTE
data sets tend to exhibit very diverse linguis-
tic phenomena, and it is difficult to employ
such data for evaluating the real impact of
this study.
2 Related work
Surprisingly, NLP research has paid little atten-
tion to semantic processing of numerical expres-
sions. This is evident when we compare with tem-
poral expressions, for which corpora (e.g., ACE-
20051, TimeBank2) were developed with annota-
tion schemes (e.g., TIMEX3, TimeML4).
Several studies deal with numerical expressions
in the context of information extraction (Bakalov
et al, 2011), information retrieval (Fontoura et al,
2006; Yoshida et al, 2010), and question answer-
ing (Moriceau, 2006). Numbers such as prod-
uct prices and weights have been common targets
of information extraction. Fontoura et al (2006)
and Yoshida et al (2010) presented algorithms and
data structures that allow number-range queries
for searching documents. However, these studies
do not interpret the quantity (e.g., 3,000,000,000)
of a numerical expression (e.g., 3b people), but
rather treat numerical expressions as strings.
Banerjee et al (2009) focused on quantity con-
sensus queries, in which there is uncertainty about
the quantity (e.g., weight airbus A380 pounds).
Given a query, their approach retrieves documents
relevant to the query and identifies the quantities
of numerical expressions in the retrieved docu-
ments. They also proposed methods for enumer-
ating and ranking the candidates for the consen-
sus quantity intervals. Even though our study
shares a similar spirit (modeling of consensus for
quantities) with Banerjee et al (2009), their goal
is different: to determine ground-truth values for
queries.
In question answering, to help ?sanity check?
answers with numerical values that were
1http://www.itl.nist.gov/iad/mig/
tests/ace/ace05/
2http://www.timeml.org/site/timebank/
timebank.html
3http://timex2.mitre.org/
4http://timeml.org/site/index.html
way out of common-sense ranges, IBM?s PI-
QUANT (Prager et al, 2003; Chu-Carroll et al,
2003) used information in Cyc (Lenat, 1995).
For example, their question-answering system
rejects 200 miles as a candidate answer for the
height of Mt. Everest, since Cyc knows mountains
are between 1,000 and 30,000 ft. high. They
also consider the problem of variations in the
precision of numbers (e.g., 5 million, 5.1 million,
5,200,390) and unit conversions (e.g., square
kilometers and acres).
Some recent studies delve deeper into the se-
mantic interpretation of numerical expressions.
Aramaki et al (2007) focused on the physical size
of an entity to predict the semantic relation be-
tween entities. For example, knowing that a book
has a physical size of 20 cm ? 25 cm and that a li-
brary has a size of 10 m ? 10 m, we can estimate
that a library contains a book (content-container
relation). Their method acquires knowledge about
entity size from the Web (by issuing queries like
?book (*cm x *cm)?), and integrates the knowl-
edge as features for the classification of relations.
Davidov and Rappoport (2010) presented a
method for the extraction from the Web and ap-
proximation of numerical object attributes such as
height and weight. Given an object-attribute pair,
the study expands the object into a set of compa-
rable objects and then approximates the numerical
values even when no exact value can be found in a
text. Aramaki et al (2007) and Davidov and Rap-
poport (2010) rely on hand-crafted patterns (e.g.,
?Object is * [unit] tall?), focusing on a specific set
of numerical attributes (e.g., height, weight, size).
In contrast, this study can handle any kind of target
and situation that is quantified by numbers, e.g.,
number of people facing a water shortage.
Recently, the RTE community has started to
pay some attention to the appropriate processing
of numerical expressions. Iftene (2010) presented
an approach for matching numerical ranges ex-
pressed by a set of phrases (e.g., more than and at
least). Tsuboi et al (2011) designed hand-crafted
rules for matching intervals expressed by temporal
expressions. However, these studies do not nec-
essarily focus on semantic processing of numeri-
cal expressions; thus, these studies do not normal-
ize units of numerical expressions nor make infer-
ences with numerical common sense.
Sammons et al (2010) reported that most sys-
tems submitted to RTE-5 failed on examples
383
where numeric reasoning was necessary. They ar-
gued the importance of aligning numerical quanti-
ties and performing numerical reasoning in RTE.
LoBue and Yates (2011) identified 20 categories
of common-sense knowledge that are prevalent in
RTE. One of the categories comprises arithmetic
knowledge (including computations, comparisons,
and rounding). They concluded that many kinds
of the common-sense knowledge have received
scarce attention from researchers even though the
knowledge is essential to RTE. These studies pro-
vided a closer look at the phenomena involved in
RTE, but they did not propose a solution for han-
dling numerical expressions.
3 Investigation of textual-entailment
pairs with numerical expressions
In this section, we investigate textual entailment
(TE) pairs in existing corpora in order to study
the core phenomena that establish an entailment
relation. We used two Japanese TE corpora:
RITE (Shima et al, 2011) and Odani et al (2008).
RITE is an evaluation workshop of textual entail-
ment organized by NTCIR-9, and it targets the
English, Japanese, and Chinese languages. We
used the Japanese portions of the development
and training data. Odani et al (2008) is another
Japanese corpus that was manually created. The
total numbers of text-hypothesis (T -H) pairs are
1,880 (RITE) and 2,471 (Odani).
We manually selected sentence pairs in which
one or both of the sentences contained a numerical
expression. Here, we define the term numerical
expression as an expression containing a number
or quantity represented by a numeral and a unit.
For example, 3 kilometers is a numerical expres-
sion with the numeral 3 and the unit kilometer.
Note that intensity of 4 is not a numerical expres-
sion because intensity is not a unit.
We obtained 371 pairs from the 4,351 T -H
pairs. We determined the inferences needed to
prove ENTAILMENT or CONTRADICTION of the
hypotheses, and classified the 371 pairs into 11
categories. Note that we ignored T -H pairs in
which numerical expressions were unnecessary
to prove the entailment relation (e.g., Socrates
was sentenced to death by 500 jury members and
Socrates was sentenced to death). Out of 371
pairs, we identified 114 pairs in which numerical
expressions played a central role in the entailment
relation.
Table 1 summarizes the categories of TE phe-
nomena we found in the data set. The largest cate-
gory is numerical matching (32 pairs). We can in-
fer an entailment relation in this category by align-
ing two numerical expressions, e.g., 2.2 million
|= over 800 thousand. This is the most funda-
mental task in numerical reasoning, interpreting
the amount (number, unit, and range) in a numer-
ical expression. We address this task in Section
4.1. The second largest category requires com-
mon sense about numerical amounts. In order to
recognize textual entailment of pairs in this cat-
egory, we need common-sense knowledge about
humans? subjective judgment of numbers. We
consider this problem in Section 5.
To summarize, this study covers 37.9% of the
instances in Table 1, focusing on the first and sec-
ond categories. Due to space limitations, we omit
the explanations for the other phenomena, which
require such things as lexical knowledge, arith-
metic operations, and counting. The coverage of
this study might seem small, but it is difficult to
handle varied phenomena with a unified approach.
We believe that this study forms the basis for in-
vestigating other phenomena of numerical expres-
sions in the future.
4 Collecting numerical expressions from
the Web
In this paper, we explore two approaches to acquir-
ing numerical common sense. Both approaches
start with extracting numerical expressions and
their context from the Web. We define a context
as the verb and its arguments that appear around a
numerical expression.
For instance, the context of 3b people in the sen-
tence 3b people face a water shortage is ?face?
and ?water shortage.? In order to extract and
aggregate numerical expressions in various doc-
uments, we converted the numerical expressions
into semantic representations (to be described in
Section 4.1), and extracted their context (to be de-
scribed in Section 4.2).
The first approach for acquiring numerical com-
mon sense estimates the distribution of numbers
that co-occur within a context, and examines
whether a given value is large, small, or normal
based on that distribution (to be described in Sec-
tion 5.1). The second approach utilizes textual
patterns with which speakers explicitly expresses
their judgment about the value of a numerical ex-
384
Category Definition Example #
Numerical matching
Aligning numerical expres-
sions in T and H, considering
differences in unit, range, etc.
t: It is said that there are about 2.2 million alcoholics in the whole country.
h: It is estimated that there are over 800 thousand people who are alcoholics. 32
Numerical common sense
Inferring by interpreting the
numerical amount (large or
small).
t: In the middle of the 21st century, 7 billion people, corresponding to 70% of the
global population, will face a water shortage.
h: It is concerning that a serious water shortage will spread around the world in the
near future.
12
Lexical knowledge Inferring by using numericalaspects of word meanings.
t: Mr. and Ms. Sato celebrated their 25th wedding anniversary.
h: Mr. and Ms. Sato celebrated their silver wedding anniversary. 12
Arithmetic Arithmetic operations includ-ing addition and subtraction.
t: The number of 2,000-yen bills in circulation has increased to 450 million, in
contrast with 440 million 5,000-yen bills.
h: The number of 2,000-yen bills in circulation exceeds the number of 5,000-yen
bills by 10 million bills.
11
Numeric-range expression
of verbs
Numerical ranges expressed by
verbs (e.g., exceed).
t: It is recorded that the maximum wave height reached 13.8 meters during the Sea
of Japan Earthquake Tsunami in May 1983.
h: During the Sea of Japan Earthquake, the height of the tsunami exceeded 10meters.
9
Simple Rewrite Rule This includes various simplerules for rewriting.
t: The strength of Taro?s grip is No. 1 in his class.
h: Taro?s grip is the strongest in his class. 7
State change Expressing the change of avalue by a multiplier or ratio.
t: Consumption of pickled plums is 1.5 times the rate of 20 years ago.
h: Consumption of pickled plums has increased. 6
Ordinal numbers Inference by interpreting ordi-nal numbers.
t: Many precious lives were sacrificed in the Third World War.
h: So far, there have been at least three World Wars. 6
Temporal expression
Inference by interpreting tem-
poral expressions such as an-
niversary, age, and ordinal
numbers.
t: Mr. and Ms. Sato celebrate their 25th wedding anniversary.
h: Mr. and Ms. Sato got married 25 years ago. 3
Count Counting up the number of var-ious entities.
t: In Japan, there are the Asian Triopsidae, the American Triopsidae, and the Euro-
pean Triopsidae.
h: In Japan, there are 3 types of Triopsidae.
3
Others 15
All 116
Table 1: Frequency and simple definitions for each category of the entailment phenomena in the survey.
Numerical Semantic representation
Expression Value Unit Mod.
about seven grams 7 g about
roughly 7 kg 7000 g about
as heavy as 7 tons 7? 106 g large
as cheap as $1 1 $ small
30?40 people [30, 40] nin (people)
more than 30 cars 30 dai (cars) over
7 km per hour 7000 m/h
Table 2: Normalized representation examples
pression (to be explained in Section 5.2).
In this study, we acquired numerical common
sense from a collection of 8 billion sentences in
100 million Japanese Web pages (Shinzato et al,
2012). For this reason, we originally designed
text patterns specialized for Japanese dependency
trees. For the sake of the readers? understand-
ing, this paper uses examples with English trans-
lations for explaining language-independent con-
cepts, and both Japanese and English translations
for explaining language-dependent concepts.
4.1 Extracting and normalizing numerical
expressions
The first step for collecting numerical expres-
sions is to recognize when a numerical expression
is mentioned and then to normalize it into a seman-
tic representation. This is the most fundamental
String Operation
gram(s) set-unit: ?g?
kilogram(s) set-unit: ?g?; multiply-value: 1,000
kg set-unit: ?g?; multiply-value: 1,000
ton(s) set-unit: ?g?; multiply-value: 1,000,000
nin (people) set-unit: ?nin? (person)
about set-modifier: ?about?
as many as set-modifier: ?large?
as little as set-modifier: ?small?
Table 3: An example of unit/modifier dictionary
step in numerical reasoning and has a number of
applications. For example, this step handles cases
of numerical matching, as in Table 1.
The semantic representation of a numerical ex-
pression consists of three fields: the value or range
of the real number(s)5, the unit (a string), and the
optional modifiers. Table 2 shows some exam-
ples of numerical expressions and their semantic
representations. During normalization, we identi-
fied spelling variants (e.g., kilometer and km) and
transformed auxiliary units into their correspond-
ing canonical units (e.g., 2 tons and 2,000 kg to
2,000,000 grams). When a numerical expression
is accompanied by a modifier such as over, about,
or more than, we updated the value and modifier
fields appropriately.
5Internally, all values are represented by ranges (e.g., 75
is represented by the range [75, 75]).
385
We developed an extractor and a normalizer for
Japanese numerical expressions6. We will outline
the algorithm used in the normalizer with an exam-
ple sentence: ?Roughly three thousand kilograms
of meats have been provided every day.?
1. Find numbers in the text by using regular ex-
pressions and convert the non-Arabic num-
bers into their corresponding Arabic num-
bers. For example, we find three thousand7
and represent it as 3, 000.
2. Check whether the words that precede or fol-
low the number are units that are registered in
the dictionary. Transform any auxiliary units.
In the example, we find that kilograms8 is a
unit. We multiply the value 3, 000 by 1, 000,
and obtain the value 3, 000, 000 with the unit
g.
3. Check whether the words that precede or fol-
low the number have a modifier that is regis-
tered in the dictionary. Update the value and
modifier fields if necessary. In the example,
we find roughly and set about in the modifier
field.
We used a dictionary9 to perform procedures 2
and 3 (Table 3). If the words that precede or fol-
low an extracted number match an entry in the dic-
tionary, we change the semantic representation as
described in the operation.
The modifiers ?large? and ?small? require elab-
oration because the method in Section 5.2 relies
heavily on these modifiers. We activated the mod-
ifier ?large? when a numerical expression occurred
with the Japanese word mo, which roughly cor-
responds to as many as, as large as, or as heavy
as in English10. Similarly, we activated the modi-
fier ?small? when a numerical expression occurred
with the word shika, which roughly corresponds
to as little as, as small as, or as light as11. These
modifiers are important for this study, reflecting
the writer?s judgment about the amount.
6The software is available at http://www.cl.
ecei.tohoku.ac.jp/?katsuma/software/
normalizeNumexp/
7In Japanese 3, 000 is denoted by the Chinese symbols ?
???.
8We write kilograms as ??????? in Japanese.
9The dictionary is bundled with the tool. See Footnote 6.
10In Japanese, we can use the word mo with a numerical
expression to state that the amount is ?large? regardless of
how large it is (e.g., large, big, many, heavy).
11Similarly, we can use the word shika with any adjective.
?? ??? ??? ????? ????
He gave to a friend$300 at the bank.
Japanese:
English:
nsubj dobj prep_to
prep_at
Number: {value: 300; unit: ?$? }
Context: {verb: ?give? ; nsubj: ?he? ; 
 prep_to: ?friend? ; prep_at: ?bank? }
Figure 1: Example of context extraction
4.2 Extraction of context
The next step in acquiring numerical common
sense is to capture the context of numerical ex-
pressions. Later, we will aggregate numbers that
share the same context (see Section 5). The con-
text of a numerical expression should provide suf-
ficient information to determine what it measures.
For example, given the sentence, ?He gave $300 to
a friend at the bank,? it would be better if we could
generalize the context to someone gives money to
a friend for the numerical expression $300. How-
ever, it is a nontrivial task to design an appropriate
representation of varying contexts. For this rea-
son, we employ a simple rule to capture the con-
text of numerical expressions: we represent the
context with the verb that governs the numerical
expression and its typed arguments.
Figure 1 illustrates the procedure for extracting
the context of a numerical expression12. The com-
ponent in Section 4.1 recognizes $300 as a numer-
ical expression, then normalizes it into a semantic
representation. Because the numerical expression
is a dependent of the verb gave, we extract the verb
and its arguments (except for the numerical ex-
pression itself) as the context. After removing in-
flections and function words from the arguments,
we obtain the context representation of Figure 1.
5 Acquiring numerical common sense
In this section, we present two approaches for ac-
quiring numerical common sense from a collec-
tion of numerical expressions and their contexts.
Both approaches start with collecting the numbers
(in semantic representation) and contexts of nu-
merical expressions from a large number of sen-
tences (Shinzato et al, 2012), and storing them
12The English dependency tree might look peculiar be-
cause it is translated from the Japanese dependency tree.
386
in a database. When a context and a value are
given for a prediction (hereinafter called the query
context and query value, respectively), these ap-
proaches judge whether the query value is large,
small, or normal.
5.1 Distribution-based approach
Given a query context and query value, this
approach retrieves numbers associated with the
query context and draws a distribution of normal-
ized numbers. This approach considers the dis-
tribution estimated for the query context and de-
termines if the value is within the top 5 percent
(large), within the bottom 5 percent (small), or is
located in between these regions (normal).
The underlying assumption of this approach is
that the real distribution of a query (e.g., money
given to a friend) can be approximated by the dis-
tribution of numbers co-occurring with the context
(e.g., give and friend) on the Web. However, the
context space generated in Section 4.2 may be too
sparse to find numbers in the database, especially
when a query context is fine-grained. Therefore,
when no item is retrieved for the query context,
we employ a backoff strategy to drop some of the
uninformative elements in the query context: ele-
ments are dropped from the context based on the
type of argument, in this order: he (prep to), kara
(prep from), ha (nsubj), yori (prep from), made
(prep to), nite (prep at), de (prep at, prep by), ni
(prep at), wo (dobj), ga (nsubj), and verb.
5.2 Clue-based approach
This approach utilizes textual clues with which a
speaker explicitly expresses his or her judgment
about the amount of a numerical expression. We
utilize large and small modifiers (described in Sec-
tion 4.1), which correspond to textual clues mo
(as many as, as large as) and shika (only, as
few as), respectively, for detecting humans? judg-
ments. For example, we can guess that $300 is
large if we find an evidential sentence13, He gave
as much as $100 to a friend.
Similarly to the distribution-based approach,
this approach retrieves numbers associated with
the query context. This approach computes the
13Although the sentence states a judgment about $100, we
can infer that $300 is also large because $300 > $100.
largeness L(x) of a value x:
L(x) = pl(x)ps(x) + pl(x)
, (1)
pl(x) =
??{r|rv < x ? rm 3 large}
??
??{r|rm 3 large}
?? , (2)
ps(x) =
??{r|rv > x ? rm 3 small}
??
??{r|rm 3 small}
?? . (3)
In these equations, r denotes a retrieved item for
the query context, and rv and rm represent the nor-
malized value and modifier flags, respectively, of
the item r. The numerator of Equation 2 counts
the number of numerical expressions that support
the judgment that x is large14, and its denominator
counts the total number of numerical expressions
with large as a modifier. Therefore, pl(x) com-
putes the ratio of times there is textual evidence
that says that x is large, to the total number of
times there is evidences with large as a modifier.
In an analogous way, ps(x) is defined to be the ra-
tio for evidence that says x is small. Hence, L(x)
approaches 1 if everyone on the Web claims that
x is large, and approaches 0 if everyone claims
that x is small. This approach predicts large if
L(x) > 0.95, small if L(x) < 0.05, and normal
otherwise.
6 Experiments
6.1 Normalizing numerical expressions
We evaluated the method that we described in Sec-
tion 4.1 for extracting and normalizing numerical
expressions. In order to prepare a gold-standard
data set, we obtained 1,041 sentences by randomly
sampling about 1% of the sentences containing
numbers (Arabic digits and/or Chinese numerical
characters) in a Japanese Web corpus (100 million
pages) (Shinzato et al, 2012). For every numer-
ical expression in these sentences, we manually
determined a tuple of the normalized value, unit,
and modifier. Here, non-numerical expressions
such as temporal expressions, telephone numbers,
and postal addresses, which were very common,
were beyond the scope of the project15. We ob-
tained 329 numerical expressions from the 1,041
sentences.
We evaluated the correctness of the extraction
and normalization by measuring the precision and
14This corresponds to the events where we find an evidence
expression ?as many as rv?, where rv < x.
15If a tuple was extracted from a non-numerical expres-
sion, we regarded this as a false positive
387
recall using the gold-standard data set16. Our
method performed with a precision of 0.78 and a
recall of 0.92. Most of the false negatives were
caused by the incompleteness of the unit dictio-
nary. For example, the proposed method could not
identify 1Ghz as a numerical expression because
the unit dictionary did not register Ghz but GHz.
It is trivial to improve the recall of the method by
enriching the unit dictionary.
The major cause of false positives was the se-
mantic ambiguity of expressions. For example, the
proposed method identified Seven Hills as a nu-
merical expression although it denotes a location
name. In order to reduce false positives, it may
be necessary to utilize broader contexts when lo-
cating numerical expressions; this could be done
by using, for example, a named entity recognizer.
This is the next step to pursue in future work.
However, these errors do not have a large effect
on the estimation of the distribution of the numer-
ical values that occur with specific named entities
and idiomatic phrases. Moreover, as explained in
Section 5, we draw distributions for fine-grained
contexts of numerical expressions. For these rea-
sons, we think that the current performance is suf-
ficient for acquiring numerical common sense.
6.2 Acquisition of numerical common sense
6.2.1 Preparing an evaluation set
We built a gold-standard data set for numerical
common sense. We applied the method in Sec-
tion 4.1 to sentences sampled at random from the
Japanese Web corpus (Shinzato et al, 2012), and
we extracted 2,000 numerical expressions. We
asked three human judges to annotate every nu-
merical expression with one of six labels, small,
relatively small, normal, relatively large, large,
and unsure. The label relatively small could be
applied to a numerical expression when the judge
felt that the amount was rather small (below the
normal) but hesitated to label it small. The la-
bel relatively large was defined analogously. We
gave the following criteria for labeling an item as
unsure: when the judgment was highly dependent
on the context; when the sentence was incompre-
hensible; and when it was a non-numerical expres-
sions (false positives of the method are discussed
in Section 4.1).
Table 4 reports the inter-annotator agreement.
16All fields (value, unit, modifier) of the extracted tuple
must match the gold-standard data set.
Agreement # expressions
3 annotators 735 (36.7%)
2 annotators 963 (48.2%)
no agreement 302 (15.1%)
Total 2000 (100.0%)
Table 4: Inter-annotator agreement
0"
100"
200"
300"
400"
500"
0"
100"
200"
130" 140" 150" 160" 170" 180" 190" 200" 210"[cm]
distribu7on:based"clue:based(large)"clue:based(small)"
[#"extrac7on]"(distribu7on:based)[#"extrac7on]"(clue:based)
Figure 2: Distributions of numbers with large and
small modifiers for the context human?s height.
For the evaluation of numerical expressions in the
data set, we used those for which at least two anno-
tators assigned the same label. After removing the
unsure instances, we obtained 640 numerical ex-
pressions (20 small, 35 relatively small, 152 nor-
mal, 263 relatively large, and 170 large) as the
evaluation set.
6.2.2 Results
The proposed method extracted about 23 million
pairs of numerical expressions and their context
from the corpus (with 100 million Web pages).
About 15% of the extracted pairs were accom-
panied by either a large or small modifier. Fig-
ure 2 depicts the distributions of the context hu-
man?s height produced by the distribution-based
and clue-based approaches. These distributions
are quite reasonable as common-sense knowledge:
we can interpret that numbers under 150 cm are
perceived as small and those above 180 cm as
large.
We measured the correctness of the proposed
methods on the gold-standard data. For this
evaluation, we employed two criteria for correct-
ness: strict and lenient. With the strict crite-
rion, the method must predict a label identical to
that in the gold-standard. With the lenient crite-
rion, the method was also allowed to predict either
large/small or normal when the gold-standard la-
bel was relatively large/small.
Table 5 reports the precision (P), recall (R), F1
(F1), and accuracy (Acc) of the proposed methods.
388
No. System Gold Sentence Remark
1 small small
I think that three men can
create such a great thing in
the world.
Correct
2 normal normal I have two cats. Correct
3 large large It?s above 32 centigrade. Correct
4 large large I earned 10 million yen fromhorse racing. Correct
5 small normal There are 2 reasons. Difficulty in judging small. Since a few people say, ?There areonly 2 reasons,? our approach predicted a small label.
6 small large
Ten or more people came,
and my eight-mat room was
packed.
Difficulty in modeling the context because this sentence omits
the locational argument for the verb came. We should extract
the context as the number of people who came to my eight-mat
room instead of the number of people who came.
7 small normal
I have two friends who
have broken up with their
boyfriends recently.
Difficulty in modeling the context. We should extract context as
the number of friends who have broken up with their boyfriends
recently instead of the number of friends.
8 small large
Lack of knowledge. We extract the context as the number of
heads of a turtle, but no corresponding information was found
on the Web.
Table 6: Output example and error analysis. We present translations of the sentences, which were origi-
nally in Japanese.
Approach Label P R F1 Acc
large+ 0.892 0.498 0.695
Distribution normal+ 0.753 0.935 0.844 0.760
small+ 0.273 0.250 0.262
large 0.861 0.365 0.613
Distribution normal 0.529 0.908 0.719 0.590
small 0.222 0.100 0.161
large+ 0.923 0.778 0.851
Clue normal+ 0.814 0.765 0.790 0.770
small+ 0.228 0.700 0.464
large 0.896 0.659 0.778
Clue normal 0.593 0.586 0.590 0.620
small 0.164 0.550 0.357
Table 5: Precision (P), recall (R), F1 score (F1),
and accuracy (Acc) of the acquisition of numerical
common sense.
Labels with the suffix ?+? correspond to the lenient
criterion. The clue-based approach achieved 0.851
F1 (for large), 0.790 F1 (for normal), and 0.464
(for small) with the lenient criterion. The perfor-
mance is surprisingly good, considering the sub-
jective nature of this task.
The clue-based approach was slightly better
than the distribution-based approach. In particu-
lar, the clue-based approach is good at predicting
large and small labels, whereas the distribution-
based approach is good at predicting normal la-
bels. We found some targets for which the distri-
bution on the Web is skewed from the ?real? dis-
tribution. For example, let us consider the distri-
bution of the context ?the amount of money that a
person wins in a lottery?. We can find a number
of sentences like if you won the 10-million-dollar
lottery, .... In other words, people talk about a
large amount of money even if they did not win
any money at all. In order to remedy this problem,
we may need to enrich the context representation
by introducing, for example, the factuality of an
event.
6.2.3 Discussion
Table 6 shows some examples of predictions from
the clue-based approach. Because of space limita-
tions, we mention only the false instances of this
approach.
The clue-based approach tends to predict small
even if the gold-standard label is normal. About
half of the errors of the clue-based approach were
of this type; this is why the precision for small and
the recall for normal are low. The cause of this er-
ror is exemplified by the sentence, ?there are two
reasons.? Human judges label normal to the nu-
merical expression two reasons, but the method
predicts small. This is because a few people say
there are only two reasons, but no one says there
are as many as two reasons. In order to handle
these cases, we may need to incorporate the distri-
bution information with the clue-based approach.
We found a number of examples for which
modeling the context is difficult. Our approach
represents the context of a numerical expression
with the verb that governs the numerical expres-
sion and its typed arguments. However, this ap-
proach sometimes misses important information,
especially when an argument of the verb is omit-
ted (Example 6). The approach also suffers from
the relative clause in Example 7, which conveys an
essential context of the number. These are similar
to the scope-ambiguity problem such as encoun-
389
tered with negation and quantification; it is diffi-
cult to model the scope when a numerical expres-
sion refers to a situation.
Furthermore, we encountered some false exam-
ples even when we were able to precisely model
the context. In Example 8, the proposed method
was unable to predict the label correctly because
no corresponding information was found on the
Web. The proposed method might more easily pre-
dict a label if we could generalize the word turtle
as animal. It may be worth considering using lan-
guage resources (e.g., WordNet) to generalize the
context.
7 Conclusions
We proposed novel approaches for acquiring nu-
merical common sense from a collection of texts.
The approaches collect numerical expressions and
their contexts from the Web, and acquire numeri-
cal common sense by considering the distributions
of normalized numbers and textual clues such as
mo (as many as) and shika (only, as few as). The
experimental results showed that our approaches
can successfully judge whether a given amount
is large, small, or normal. The implementations
and data sets used in this study are available on
the Web17. We believe that acquisition of numer-
ical common sense is an important step towards a
deeper understanding of inferences with numbers.
There are three important future directions for
this research. One is to explore a more sophis-
ticated approach for precisely modeling the con-
texts of numbers. Because we confirmed in this
paper that these two approaches have different
characteristics, it would be interesting to incorpo-
rate textual clues into the distribution-based ap-
proach by using, for example, machine learning
techniques. Finally, we are planning to address the
?third phase? of the example explained in Section
1: associating many people face a water shortage
with a serious water shortage.
Acknowledgments
This research was partly supported by JST,
PRESTO. This research was partly supported by
JSPS KAKENHI Grant Numbers 23240018 and
23700159.
17http://www.cl.ecei.tohoku.ac.jp/
?katsuma/resource/numerical common sense/
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and
Kazuhiko Ohe. 2007. Uth: Svm-based semantic
relation classification using physical sizes. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 464?467.
Anton Bakalov, Ariel Fuxman, Partha Pratim Talukdar,
and Soumen Chakrabarti. 2011. SCAD: collective
discovery of attribute values. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 447?456.
Somnath Banerjee, Soumen Chakrabarti, and Ganesh
Ramakrishnan. 2009. Learning to rank for quantity
consensus queries. In Proceedings of the 32nd inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?09,
pages 243?250.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2010. Building textual entailment special-
ized data sets: a methodology for isolating linguis-
tic phenomena relevant to inference. Proceedings of
the Seventh International Conference on Language
Resources and Evaluation, pages 3542?3549.
Elena Cabrio and Bernardo Magnini. 2011. Towards
component-based textual entailment. In Proceed-
ings of the Ninth International Conference on Com-
putational Semantics, IWCS ?11, pages 320?324.
Jennifer Chu-Carroll, David A. Ferrucci, John M.
Prager, and Christopher A. Welty. 2003. Hybridiza-
tion in question answering systems. In New Direc-
tions in Question Answering?03, pages 116?121.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine Learning Challenges. Eval-
uating Predictive Uncertainty, Visual Object Classi-
fication, and Recognising Tectual Entailment, pages
177?190.
Dmitry Davidov and Ari Rappoport. 2010. Extrac-
tion and approximation of numerical attributes from
the web. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1308?1317.
Marcus Fontoura, Ronny Lempel, Runping Qi, and Ja-
son Zien. 2006. Inverted index support for numeric
search. Internet Mathematics, 3(2):153?185.
Adrian Iftene and Mihai-Alex Moruz. 2010. UAIC
participation at RTE-6. In Proceedings of the Third
Text Analysis Conference (TAC 2010) November.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Peter LoBue and Alexander. Yates. 2011. Types of
common-sense knowledge needed for recognizing
390
textual entailment. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers-Volume 2, pages 329?334.
Ve?ronique Moriceau. 2006. Generating intelligent
numerical answers in a question-answering system.
In Proceedings of the Fourth International Natural
Language Generation Conference, INLG ?06, pages
103?110.
Michitaka Odani, Tomohide Shibata, Sadao Kurohashi,
and Takayuki Nakata. 2008. Building data of
japanese text entailment and recognition of infer-
encing relation based on automatic achieved similar
expression. In Proceeding of 14th Annual Meeting
of the Association for ?atural Language Processing,
pages 1140?1143.
John M. Prager, Jennifer Chu-Carroll, Krzysztof
Czuba, Christopher A. Welty, Abraham Ittycheriah,
and Ruchi Mahindru. 2003. IBM?s PIQUANT in
TREC2003. In TREC, pages 283?292.
Mark Sammons, Vinod V.G. Vydiswaran, and Dan
Roth. 2010. Ask not what textual entailment can do
for you... In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1199?1208.
Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee,
Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of ntcir-9 rite: Recognizing inference in text. In Pro-
ceeding of NTCIR-9 Workshop Meeting, pages 291?
301.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
and Sadao Kurohashi. 2012. Tsubaki: An open
search engine infrastructure for developing informa-
tion access methodology. Journal of Information
Processing, 20(1):216?227.
Assaf Toledo, Sophia Katrenko, Stavroula Alexan-
dropoulou, Heidi Klockmann, Asher Stern, Ido Da-
gan, and Yoad Winter. 2012. Semantic annotation
for textual entailment recognition. In Proceedings of
the 11th Mexican International Conference on Arti-
ficial Intelligence, MICAI ?12.
Yuta Tsuboi, Hiroshi Kanayama, Masaki Ohno, and
Yuya Unno. 2011. Syntactic difference based ap-
proach for ntcir-9 rite task. In Proceedings of the
9th NTCIR Workshop, pages 404?411.
Minoru Yoshida, Issei Sato, Hiroshi Nakagawa, and
Akira Terada. 2010. Mining numbers in text using
suffix arrays and clustering based on dirichlet pro-
cess mixture models. Advances in Knowledge Dis-
covery and Data Mining, pages 230?237.
391
Proceedings of the ACL Student Research Workshop, pages 110?116,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Detecting Chronic Critics Based on
Sentiment Polarity and User?s Behavior in Social Media
Sho Takase? Akiko Murakami? Miki Enoki? Naoaki Okazaki? Kentaro Inui?
Tohoku University? IBM Research - Tokyo?
{takase, okazaki, inui}@ecei.tohoku.ac.jp
{akikom, enomiki}@jp.ibm.com
Abstract
There are some chronic critics who al-
ways complain about the entity in social
media. We are working to automatically
detect these chronic critics to prevent the
spread of bad rumors about the reputation
of the entity. In social media, most com-
ments are informal, and, there are sarcas-
tic and incomplete contexts. This means
that it is difficult for current NLP technol-
ogy such as opinion mining to recognize
the complaints. As an alternative approach
for social media, we can assume that users
who share the same opinions will link to
each other. Thus, we propose a method
that combines opinion mining with graph
analysis for the connections between users
to identify the chronic critics. Our ex-
perimental results show that the proposed
method outperforms analysis based only
on opinion mining techniques.
1 Introduction
On a social media website, there may be millions
of users and large numbers of comments. The
comments in social media are related to the real
world in such fields as marketing and politics. An-
alyzing comments in social media has been shown
to be effective in predicting the behaviors of stock
markets and of voters in elections (Bollen et al,
2011; Tumasjan et al, 2010; O?Connor et al,
2010). Because of their effects on the real world,
some complaints may harm the reputation of a cor-
poration or an individual and cause serious dam-
age. Consider a comment such as ?Working for
Company A is really awful? as an example. The
complaint gives viewers a negative impression of
Company A and can increase the number of people
who think the company is bad.
Some complaints are expressed by a specific
user who is always criticizing a specific target en-
tity (in this example, Company A). We call this
user a chronic critic of that entity, a person who
is deliberately trying to harm the reputation of the
entity. That is, a chronic critic is trying to run a
negative campaign against the entity. If the entity
is aware of its own chronic critics, then it is able
to take prompt action to stop the malicious com-
plaints. When the complaints are false, the entity
can use that defense. In contrast, if the chronic
critics are justified, then the entity should address
the concerns to limit the damage. Hence, to han-
dle malicious rumors, it is important to detect the
chronic critics.
However, it is generally quite difficult for a
computer to detect a chronic critic?s comments,
since especially the comments in social media are
often quite informal. In addition, there are com-
plexities such as sarcasm and incomplete contexts.
For example, if Company A has been involved in a
widely recognized fiasco, then some chronic crit-
ics might sarcastically write ?good job? or ?won-
derful? about Company A. They are using posi-
tive words, but in the context they are effectively
criticizing Company A. Some chronic critics bash
a target entity solely with sarcasm, so they dam-
age the target with positive words. It is exceed-
ingly difficult to directly detect these chronic crit-
ics based on their comments. In an example of
an incomplete context, if one author starts an ex-
change with a comment such as ?The new prod-
uct from Company A is difficult to use? and an-
other user responds with something like ?Fool?,
we cannot easily recognize the meaning of this
comment as related to ?Company A being foolish
because the product really is difficult to use? or
whether ?the user is the fool because the product
is easy for other people to use?. To find chronic
critics for a given entity, we need to identify the
actual target of the complaints. Take the comment
?Company B is much worse than Company A? for
110
example. This comment is probably complaining
about Company B but not Company A. In contrast,
most of the previous work on sentiment analysis
in social media does not consider these kinds of
problems (Barbosa and Feng, 2010; Davidov et
al., 2010; Speriosu et al, 2011).
Switching to the behavior of each user, in so-
cial media we often see that users who have sim-
ilar ideas will tend to cooperate with each other.
In fact, previous work suggests that users who
have the same opinions tend to create links to each
other (Conover et al, 2011b; Yang et al, 2012).
Because chronic critics share the purpose of at-
tacking some target?s reputation, they may also
decide to cooperate. For this reason, to detect
chronic critics, we believe that information about
the connections among users will be effective.
In this paper, we present a method that com-
bines opinion mining based on NLP and graph
analysis of the connections among users to rec-
ognize the chronic critics. In the experiments, we
demonstrate the difficulty in detecting chronic crit-
ics by analyzing only the individual comments. In
addition, we investigate the effectiveness of using
the connections between users, i.e., using the pro-
posed method. For our experiments, we used Twit-
ter, a popular social media service. In particular,
we focus on Japanese comments on Twitter.
This paper is organized as follows. Section 2
reviews related work. Section 3 presents the pro-
posed method which applies the opinion mining
and graph analysis. Section 4 demonstrates the ef-
fectiveness of the proposed method and discusses
the experimental results. Section 5 concludes this
paper.
2 Related Work
In recent years, an interest in opinion mining in
online communities has emerged (Conover et al,
2011a; O?Connor et al, 2010; Speriosu et al,
2011; Murakami and Raymond, 2010; Barbosa
and Feng, 2010; Davidov et al, 2010). O?Connor
et al (2010), Barbosa and Feng (2010), Davidov
et al (2010), and Speriosu et al (2011) proposed
methods to predict a sentiment polarity (i.e., pos-
itive or negative) of a comment in social media.
O?Connor et al (2010) studied a subjectivity lexi-
con. Barbosa and Feng (2010) and Davidov et al
(2010) used machine learning approaches. Spe-
riosu et al (2011) introduced connections between
words, emoticons, tags, n-grams, comments and
users. These studies did not identify the target of
the polarized sentiment of each comment.
Conover et al (2011a) proposed a method that
predicts the political polarity of a social media
user based on the connections between users and
tags. They demonstrated that label propagation
on the graph representing the connections between
users is effective. However, this method is not
guaranteed to obtain the optimal solution. In con-
trast, our research uses graph analysis that con-
verges on the optimal solution.
Murakami and Raymond (2010) proposed a
method that uses the connections between users
to predict each user?s opinion, i.e., support or op-
pose a topic in online debates. They analyzed
the content of the discussions to infer the connec-
tions. However, in social media, it is difficult to in-
fer connections based on content because of such
complexities as incomplete contexts. To address
these problem, we analyzed the behavior of the
users to predict the connections between users.
Our task is similar to spammer detection (Wang,
2010; Yang et al, 2012). Wang (2010) pro-
posed a method using a classifier to detect spam-
mers. They used the content in the comments
and the number of linked users as features. Yang
et al (2012) analyzed spammer communities and
demonstrated that spammers closely link to each
other in social media. They also proposed a
method that extracts spammers using the connec-
tions between users. WhileWang (2010) and Yang
et al (2012) required manually annotated data for
training or as seeds, we extract the seeds for the
graph analysis automatically through opinion min-
ing.
3 Proposed Method
Figure 1 presents an overview of the proposed
method. The proposed method has two phases,
opinion mining and graph analysis. First, we ex-
tract a few chronic critics by analyzing the opin-
ions of many users referencing the target entity.
For the opinion mining, we are initially looking
for users who strongly criticize the target entity. In
Figure 1, given Company A as a target entity, we
find users ?b? and ?e? since they said ?Working
for Company A is really awful? and ?This product
from Company A is useless?. However, we may
miss the other chronic critics since they used sar-
casm and incomplete contexts.
Next, we find the users who are linked to the
111
????
Opinion mining
Making graph Company A is very very nice.I use only products of Company A.
I like products of Company A.
The attitude of Company A is perfect.
Company A always makes shitty produces.
Working for Company A is really awful.
Comments in social media
Company B is much worse than Company A.
A product of Company A is wonderful.
Fiasco of Company A occurs almost every day.
Company A, good job!
The product from Company A is difficult to use.
This product from Company A is useless.
Company A is a bad company.
Why do people praise Company A?a
d
c
b
g
f
e
An account of a target entity x
Working for Company A is really awful.Company A always makes shitty produces.
A product of Company A is useless.A new product of Company A is difficult to use.
Chronic critics extracted by the opinion mining
b
e
The graph based on a relationship of agreements  Results of the graph analysis
Graph analysis 
Chronic critics
a
d
c
b
g
f
e
a
d
c
b
g
f
e
x x
?
Figure 1: Overview of proposed method
chronic critics that were detected through opinion
mining. We built a graph in which the users are
represented by nodes and the links between the
users are represented by edges.We recognize ad-
ditional chronic critics based on the graph anal-
ysis. In the example of Figure 1, we find more
chronic critics not recognized by the opinion min-
ing, such as ?a? and ?c?, because they are linked
to the chronic critics ?b? and ?e?. In this section,
we explain the opinion mining and graph analysis.
Since a comment in Twitter is called a tweet, we
use the term tweet below.
3.1 Opinion Mining
As defined in Section 1, we defined a user who
frequently criticizes a target entity as a chronic
critic. Therefore, we classify the tweets of each
user into critical or non-critical and label any users
who complain about the target entity many times
as chronic critics. Because we want to investi-
gate the opinions of each user in public, we an-
alyze public tweets, excluding the private conver-
sations between users. In Twitter, this means we
ignore a reply that is a response to a specific user
named username (written in the format ?@user-
name response?) and QT that is a mention in a
quoted tweet from username (written in the format
?mention RT @username: quoted tweet?).
We assume a phrase representing negative po-
larity or profanity to be critical phrases. The pro-
posed method determines whether a tweet com-
plains about the target entity by investigating a
critical phrase and the target of the phrase.
Note that a negative polarity is represented by
declinable words or substantives. We used the
sentiment analyzer created by Kanayama and Na-
sukawa (2012) to detect a phrase representing neg-
   A??????????????
Working for   Company A is    really    awful.
Figure 2: Example of critic tweet
ative polarity by using declinable words. We used
the lexicon collected by Higashiyama et al (2008)
to find negative polarity in substantives. For de-
tecting profanity, we use a profane lexicon col-
lected by Ogino et al (2012).
The sentiment analyzer can find not only senti-
ment phrases but the targets of the phrases based
on syntactic parsing and the case frames1. How-
ever, because there are many informal tweets and
because most users omit the grammatical case in
tweets, the sentiment analyzer often fails to cap-
ture any target. To address this problem, in ad-
dition to a target extracted by the sentiment ana-
lyzer, we obtain a target based on the dependency
tree. We extract nouns in parent and child phrases
within distance 2 from a critical phrase in the de-
pendency tree.
Figure 2 shows an example of a Japanese tweet
criticizing Company A and its English translation.
The Japanese tweet is split into phrase-like units
(bunsetsu). Each English phrase is linked to the
corresponding bunsetsu by a dotted line. The de-
pendency relationships among the bunsetsu are ex-
pressed by the arrows. In the tweet, the black-
edged phrase ?awful? is a critical phrase. We ex-
tract the nouns in ?Working for? and ?Company
A is? as targets of the critical phrase since these
1A case frame is a list which represents grammatical cases
of a predicate.
112
phrases are parents within distance 2 of the criti-
cal phrase. Therefore, we decide that the tweet is
criticizing Company A.
Since a chronic critic frequently complains
about the target entity, we can predict that most
of the tweets written by a chronic critic of the tar-
get entity will be critical tweets. Therefore, we
can calculate a ratio of critical tweets for all of the
tweets about the target entity. We score the user ui
with equation (1).
scorei =
ni
Ni
(1)
Ni is the number of all tweets about the target en-
tity and ni is the number of critical tweets about
the entity by that user 2. We extract the top M
users based on scorei as chronic critics.
3.2 Graph Analysis
In social media, it is often very difficult to deter-
mine whether a tweet is critical since many tweets
include sarcasm or incomplete contexts. The opin-
ion mining may miss numerous complaints with
sarcasm or incomplete contexts. To resolve this
problem, we apply user behaviors. In social me-
dia, we assume that users having the same opinion
interact with each other in order to demonstrate the
correctness of their opinion. In particular, since
the purpose of chronic critics is to spread the bad
reputation, we assume that they want to assist each
other. We supplement the opinion mining by a
graph analysis using this assumption. Thus, we
make a graph representing connections among the
users and use label propagation on the graph based
on the results of the opinion mining as the seeds.
In addition, we believe that a user will try to
spread user matching opinions. This implies that a
user who spreads the opinion of another of agrees
with the author of that opinion. In Twitter, a user
can spread an opinion as an RT, which is a repost-
ing of a tweet by a username (written in the format
?RT @username: tweet?). Conover et al (2011b)
demonstrated that they can make a graph repre-
senting the connections among users who support
each others opinions by using RTs. Hence, an RT
expresses a relationship of endorsement. We also
created a graph based on this feature.
Our graph has m users (U = {u1, ..., um}) as
nodes, where ui connects with uj via an edge that
2The formula (1) assigns a high score to a user if the user
only produces one or two tweets about the target entity and
those tweets are negative. To prevent this, we disregard the
users whose the number of tweets are fewer than 5.
has weightwij (0 ? wij ? 1) andwij corresponds
to the degree to which ui supports uj . We calcu-
late wij by using Equation (2).
wij =
1
2
(
rij
Ri
+ rjiRj
)
(2)
rij is the total RT tweets of uj by ui and Ri is the
number of RTs by ui. Therefore, the more ui and
uj RT each other, the more weight wij is close to
1. In contrast, if ui and uj rarely RT each other, the
value of wij will approach 0. In addition, this wij
definition is symmetric means (i.e., wij = wji).
We find more new chronic critics by label prop-
agation on the graph. We use the chronic critics
obtained by the opinion mining as seeds. It is as-
sumed that a user who supports the target entity is
not a chronic critic. Using this knowledge, we use
the account of the target entity as a seed.
The label propagation assigns a confidence
score c = (c1, ..., cm) to each node U =
u1, ..., um, where the score is a real number be-
tween ?1 and 1. A score close to 1 indicates
that we are very confident that the node (user) is
a chronic critic. A score close to ?1 indicates that
we are sure that the node is not a chronic critic. In
addition, the scores of seeds are fixed and cannot
be changed. The scores of chronic critics obtained
by the opinion mining are 1 and the score of the
target entity is set to ?1. To formulate the label
propagation as an optimization problem, we used
the loss function proposed by Zhu et al (2003),
because wij ? 0 for all i, j.
E(c) = 12
?
i,j
wij(ci ? cj)2 (3)
To minimize E(c), ci is close to cj when wij
is greater than 0. That is, if the users support
each other, the scores of the users are close to
each other. Thus, by minimizing E(c), we as-
sign the confidence scores considering the results
of the opinion mining and agreement relationships
among the users. We find the users that have
scores greater than the threshold.
We believe that if the distance between users on
the graph is large, then users slightly support each
other. However, we can assign a score of 1 to each
node in any subgraph that has chronic critics ex-
tracted by the opinion mining to minimize E(c)
if the subgraph does not include the account of
the target entity, no matter how far away a node
113
Table 1: Properties of the experimental datasets
Target entity Tweets Critics Kappa
Company A 35,807 112 0.81
Politician A 45,378 254 1.0
is from the seeds. To avoid this problem, Yin and
Tan (2011) introduced a neutral fact, which de-
creases each confidence score by considering the
distance from the seeds. The neutral fact has a
fixed confidence score 0 and connects with all of
the nodes except the seeds. Suppose u1 is the neu-
tral fact, Ul = {u2, ..., ul} is the set of seeds and
Ut = {ul+1, ..., um} is the set of all nodes except
seeds. To assign the weight of the edge between
u1 and other nodes considering the degrees of the
nodes, we calculate the weight by as:
w1i =
{
0 i = 1, ..., l
??j>1 |wij | i = l + 1, ...,m
(4)
where ? is a small constant. Thus, the weight is
proportional to the total weight of the edges from
each node.
4 Experiments
4.1 Experimental Setting
For our experiment, we gathered tweets by using
the Twitter search API. The twitter search API re-
turns the tweets that contain an input query. We
used the name of a target entity, words related to
the entity3, and the account name of the entity as
queries. In this research, there were two target
entities, Company A and Politician A. We found
many critical tweets about these target entities.
The entities have their own accounts in Twitter.
We collected the Japanese tweets for one month.
We want to extract the users who frequently ex-
press a public opinion related to a target entity.
For this reason, we eliminated users whose the
number of tweets except conversation (i.e., reply,
QT, RT) are fewer than 5. In addition, to elimi-
nate bots that automatically post specific tweets,
we eliminated users whose conversational tweets
were fewer than 2. We selected some of the re-
maining users for the experiment. To satisfy our
definition, a chronic critic must tweet about the
target entity many times. Therefore, we focused
3We manually prepared the words that have a correlation
with the entity. In this paper, we only used the name of the
political party of Politician A as the related word.
on the top 300 users based on the number of tweets
as our experimental users. Table 1 shows the total
numbers of tweets by the top 300 users, excluding
the account of the target entity.
We created an evaluation set by manually di-
viding the experimental users into chronic critics
and regular users. A chronic critic actively com-
plained and tried to harm the reputation of the
target entity. We also regarded a user who fre-
quently reposted a critic?s tweets and unfavorable
news about the target entity as a chronic critic. For
the experimental users tweeting aboutCompany A,
we asked two human annotators to judge whether
a user was a chronic critic based on one month of
tweets. The Cohen?s kappa value was 0.81 which
inter-annotator agreement was good. We selected
the arbitrarily annotating by one of the annotators
as our evaluation set. Table 1 expresses the num-
ber of chronic critics for each target entity in the
evaluation set. For the experimental users tweet-
ing about Politician A, we randomly extracted 50
users randomly to calculate Cohen?s kappa, which
is displayed in Table 1.
We evaluated the effects of combining the opin-
ion mining with the graph analysis. We compared
opinion mining (OM), graph analysis (GA), and
the combination of opinion mining and graph anal-
ysis (our proposed method). GA randomly se-
lected M users from experimental users as seeds
and takes the average of the results obtained by
performing label propagation three times. The
number of chronic critics extracted by the opinion
mining (i.e., the valuable M ) was set to 30. The
parameter ?, that we use to calculate the weight of
the edges connected to neutral fact, was set to 0.1.
4.2 Results
Figure 3 represents the precision and recall of each
method for each target entity. In OM, we varied
the threshold from 0 to 0.2 in increments of 0.02
and accepted a user with a score over the threshold
as a chronic critic. In GA, we varied the threshold
from 0.35 to 0.8 in increments of 0.05.
In Figure 3, the results for Company A and
Politician A are quite different, though there are
some similar characteristics. Figure 3 shows that
OM achieved high precision but it was difficult to
improve the recall. In contrast, GA easily achieved
high recall. The proposed method achieved high
precision similar to OM and high recall. In
other words, the proposed method found many
114
!"#$%!"&%
!"&$%!"$%
!"$$%!"'%
!"'$%!"(%
!"($%!")%
!")$%
!% !"*% !"+% !"#% !"&% !"$% !"'% !"(% !")% !",% *%
Prec
ision

Recall
OM GA The proposed method 
(a) Company A
!"#$
!"#%$
!"&$
!"&%$
'$
!$ !"'$ !"($ !")$ !"*$ !"%$ !"+$ !",$ !"#$ !"&$ '$
Prec
ision

Recall
OM GA The proposed method 
(b) Politician A
Figure 3: Precision and recall of each method for each target entity
Table 2: Users connected with the target entity
Target entity Users Non-critics
Company A 45 39
Politician A 74 35
chronic critics while retaining high precision of
OM. Therefore, the combination of the opinion
mining and the graph analysis improved the per-
formance of recognizing the chronic critics.
Figure 3 shows that the recall of OM was low,
which means that OM missed some of the critical
tweets. In this paper, we used domain-independent
lexicons to detect the critical phrases. Therefore,
OM failed to find domain-dependent critic phrases
such as slang words. In addition, some chronic
critics do not express criticism clearly in their own
tweets. To spread the bad reputation, they refer-
ence only a title and link to a webpage that criti-
cizes the target entity such as:
This shows the reality of Company A.
Why do you buy products from this
company? http://xxx
We believe that is often done because each tweet is
limited to 140 characters. It is difficult to classify
the tweet as a complaint based only on its content.
However, the proposed method recognized most
chronic critics that complain with these methods
based on the GA.
It cannot reasonably be assumed that a user
who supports the account of the target entity is a
chronic critic. For this reason, in the graph analy-
sis, we used the entity?s account to recognize non-
critics. We believe that using the account corrects
for mistakes in selecting the seed chronic critics.
Table 2 shows the number of users connected with
the account. Table 2 also shows the number of
non-critics among the users. As seen in Table 2,
many non-critics were connected with the account.
Especially for Politician A, most of the non-critics
in the evaluation set were connected with the ac-
count. Therefore, incorporating the account into
the graph analysis can correct for errors in the
seeding of chronic critics. However, some chronic
critics were connected with the target?s account
and reposted tweets from the account. We noticed
that they mentioned their negative opinions about
the content of such a tweet immediately after re-
posting that tweet. Hence, we need to analyze the
contexts before and after each RT.
For Politician A, Table 1 shows that most of the
users in the evaluation set criticized the politician.
We were able to find most of the chronic critics
by extracting the users linked to each other. How-
ever, for Company A, the precision of GA was low.
This means we need high accuracy in selecting the
seeds to correctly capture chronic critics. Because
we used the users extracted by the opinion mining
as the seeds, the proposed method outperformed
OM and GA.
5 Conclusion
In this paper, we proposed a method that uses not
only opinion mining but graph analysis of the con-
nections between users to detect chronic critics.
In our experiments, we found that the proposed
method outperformed each technique.
In our study, we used two entities. To im-
prove reliability, we should study more entities.
We used a relationship between users that support
each other. However, we suspect that the rela-
tionship includes adversaries. We hope to address
these topics in the future.
115
Acknowledgments
This research was partly supported by JSPS KAK-
ENHI Grant Numbers 23240018. The authors
would like to acknowledge Hiroshi Kanayama and
Shiho Ogino in IBM Research-Tokyo for provid-
ing their tools for our experiments.
References
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 36?44.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Michael D. Conover, Bruno Gonc?alves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011a. Predicting the Political Alignment
of Twitter Users. In Proceedings of the 3rd IEEE
Conference on Social Computing, pages 192?199.
Michael D. Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Alessandro Flammini, and
Filippo Menczer. 2011b. Political Polarization on
Twitter. In Proceeding of the 5th International AAAI
Conference on Weblogs and Social Media, pages
89?96.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hash-
tags and Smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 241?249.
Masahiko Higashiyama, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Learning Polarity of Nouns by Se-
lectional Preferences of Predicates (in Japanese). In
Proceedings of the 14th Annual Meeting of The As-
sociation for Natural Language Processing, pages
584?587.
Hiroshi Kanayama and Tetsuya Nasukawa. 2012. Un-
supervised Lexicon Induction for Clause-Level De-
tection of Evaluations. Natural Language Engineer-
ing, 18(1):83?107.
Akiko Murakami and Rudy Raymond. 2010. Support
or Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 869?875, Beijing,
China.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
4h International AAAI Conference on Weblogs and
Social Media, pages 122?129.
Shiho Ogino, Tetsuya Nasukawa, Hiroshi Kanayama,
and Miki Enoki. 2012. Knowledge Discovery Us-
ing Swearwords (in Japanese). In Proceedings of the
8th Annual Meeting of The Association for Natural
Language Processing, pages 58?61.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter Polarity Classification
with Label Propagation over Lexical Links and the
Follower Graph. In Proceedings of the 1st workshop
on Unsupervised Learning in NLP, pages 53?63.
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
Elections with Twitter: What 140 Characters Reveal
about Political Sentiment. In Proceedings of the 4th
International AAAI Conference on Weblogs and So-
cial Media, pages 178?185.
Alex Hai Wang. 2010. Don?t Follow Me - Spam De-
tection in Twitter. In Proceedings of the 5th Inter-
national Conference on Security and Cryptography,
pages 142?151.
Chao Yang, Robert Harkreader, Jialong Zhang, Seung-
won Shin, and Guofei Gu. 2012. Analyzing Spam-
mers? Social Networks for Fun and Profit: A Case
Study of Cyber Criminal Ecosystem on Twitter. In
Proceedings of the 21st international conference on
World Wide Web, pages 71?80.
Xiaoxin Yin and Wenzhao Tan. 2011. Semi-
Supervised Truth Discovery. In Proceedings of the
20th international conference on World Wide Web,
pages 217?226.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-Supervised Learning Using Gaussian
Fields and Harmonic Functions. In Proceedings
of the 20th International Conference on Machine
Learning, pages 912?919.
116
Proceedings of the 8th Workshop on Asian Language Resources, pages 1?8,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
A Thesaurus of Predicate-Argument Structure for Japanese Verbs
to Deal with Granularity of Verb Meanings
Koichi Takeuchi
Okayama University /
koichi@cl.cs.
okayama-u.ac.jp
Kentaro Inui
Tohoku University /
inui@ecei.
tohoku.ac.jp
Nao Takeuchi
Free Language
Analyst
Atsushi Fujita
Future University Hakodate /
fujita@fun.ac.jp
Abstract
In this paper we propose a framework
of verb semantic description in order to
organize different granularity of similar-
ity between verbs. Since verb mean-
ings highly depend on their arguments
we propose a verb thesaurus on the ba-
sis of possible shared meanings with
predicate-argument structure. Motiva-
tions of this work are to (1) construct a
practical lexicon for dealing with alter-
nations, paraphrases and entailment re-
lations between predicates, and (2) pro-
vide a basic database for statistical learn-
ing system as well as a theoretical lex-
icon study such as Generative Lexicon
and Lexical Conceptual Structure. One
of the characteristics of our description
is that we assume several granularities
of semantic classes to characterize verb
meanings. The thesaurus form allows us
to provide several granularities of shared
meanings; thus, this gives us a further re-
vision for applying more detailed analy-
ses of verb meanings.
1 Introduction
In natural language processing, to deal with
similarities/differences between verbs is essen-
tial not only for paraphrase but also textual en-
tailment and QA system which are expected
to extract more valuable facts from massively
large texts such as the Web. For example, in
the QA system, assuming that the body text
says ?He lent her a bicycle?, the answer of the
question ?He gave her a bicycle?? should be
?No?, however the answer of ?She rented the
bicycle?? should be ?Yes?. Thus construct-
ing database of verb similarities/differences en-
ables us to deal with detailed paraphrase/non-
paraphrase relations in NLP.
From the view of the current language re-
source, how the shared/different meanings of
?He lent her a bicycle? and ?He gave her a bi-
cycle? can be described? The shared mean-
ing of lend and give in the above sentences is
that they are categorized to Giving Verbs, as
in Levin?s English Verb Classes and Alterna-
tions (EVCA) (Levin, 1993), while the different
meaning will be that lend does not imply own-
ership of the theme, i.e., a bicycle. One of the
problematic issues with describing shared mean-
ing among verbs is that semantic classes such as
Giving Verbs should be dependent on the gran-
ularity of meanings we assumed. For example,
the meaning of lend and give in the above sen-
tences is not categorized into the same Frame
in FrameNet (Baker et al, 1998). The reason
for this different categorization can be consid-
ered to be that the granularity of the semantic
class of Giving Verbs is larger than that of the
Giving Frame in FrameNet1. From the view of
natural language processing, especially dealing
the with propositional meaning of verbs, all of
the above classes, i.e., the wider class of Giv-
ing Verbs containing lend and give as well as
the narrower class of Giving Frame containing
give and donate, are needed. Therefore, in this
work, in order to describe verb meanings with
several granularities of semantic classes, a the-
saurus form is adopted for our verb dictionary.
Based on the background, this paper presents
a thesaurus of predicate-argument structure for
verbs on the basis of a lexical decompo-
sitional framework such as Lexical Concep-
tual Structure (Jackendoff, 1990); thus our
1We agree with the concept of Frame and FrameEle-
ments in FrameNet but what we propose in this paper is the
necessity for granularities of Frames and FrameElements.
1
proposed thesaurus can deal with argument
structure level alternations such as causative,
transitive/intransitive, stative. Besides, tak-
ing a thesaurus form enables us to deal with
shared/differenciate meaning of verbs with con-
sistency, e.g., a verb class node of ?lend? and
?rent? can be described in the detailed layer of
the node ?give?.
We constructed this thesaurus on Japanese
verbs and the current status of the verb thesaurus
is this: we have analyzed 7,473 verb meanings
(4,425 verbs) and organized the semantic classes
in a ve-layer thesaurus with 71 semantic roles
types. Below, we describe background issues,
basic design issues, what kind of problems re-
main, limitations and perspectives of applica-
tions.
2 Existing Lexical Resources and
Drawbacks
2.1 Lexical Resources in English
From the view of previous lexical databases
In English, several well-considered lexical
databases are available, e.g., EVCA, Dorr?s
LCS (Dorr, 1997), FrameNet, WordNet (Fell-
baum, 1998), VerbNet (Kipper-Schuler, 2005)
and PropBank (Palmer et al, 2005). Be-
sides there is the research project (Pustejovsky
and Meyers, 2005) to nd general descriptional
framework of predicate argument structure by
merging several lexical databases such as Prop-
Bank, NomBank, TimeBank and PennDiscouse
Treebank.
Our approach corresponds partly to each
lexical database, (i.e., FrameNet?s Frame and
FrameElements correspond to our verb class
and semantic role labels, and the way to orga-
nize verb similarity classes with thesaurus cor-
responds with WordNet?s synset), but is not
exactly the same; namely, there is no lex-
ical database describing several granularities
of semantic classes between verbs with argu-
ments. Of course, since the above English lex-
ical databases have links with each other, it is
possible to produce a verb dictionary with sev-
eral granularities of semantic classes with argu-
ments. However, the basic categories of classify-
ing verbs would be little different due to the dif-
ferent background theory of each English lexical
database; it must be not easy to add another level
of semantic granularity with keeping consistency
for all the lexical databases; thus, thesaurus form
is needed to be a core form for describing verb
meanings2.
2.2 Lexical Resources in Japanese
In previous studies, several Japanese lexicons
were published: IPAL (IPA, 1986) focuses on
morpho-syntactic classes but IPAL is small3.
EDR (Jap, 1995) consists of a large-scale lex-
icon and corpus (See Section 3.4). EDR is
a well-considered and wide coverage dictio-
nary focusing on translation between Japanese
and English, but EDR?s semantic classes were
not designed with linguistically-motivated lex-
ical relations between verbs, e.g., alternations,
causative, transitive, and detransitive relations
between verbs. We believe these relations must
be key for dealing with paraphrase in NLP.
Recently Japanese FrameNet (Ohara et al,
2006) and Japanese WordNet (Bond et al, 2008)
are proposed. Japanese FrameNet currently
published only less than 100 verbs4. Besides
Japanese WordNet contains 87000 words and
46000 synsets, however, there are three major
difculty of dealing with paraphrase relations
between verbs: (1) there is no argument informa-
tion; (2) existing many similar synsets force us to
solve ne disambiguation between verbs when
we map a verb in a sentence to WordNet; (3) the
basic verbs of Japanese (i.e., highly ambiguous
verbs) are wrongly assigned to unrelated synsets
because they are constructed by translation from
English to Japanese.
2As Kipper (Kipper-Schuler, 2005) showed in their
examples mapping between VerbNet and WordNet verb
senses, most of the mappings are many-to-many relations;
this indicates that some two verbs grouped in a same se-
mantic type in VerbNet can be categorized into different
synsets in WordNet. Since WordNet does not have argu-
ment structure nor syntactic information, we cannot pur-
chase what is the different features for between the synsets.
3It contains 861 verbs and 136 adjectives.
4We are supplying our database to Japanese FrameNet
project.
2
3 Thesaurus of Predicate-Argument
Structure
The proposed thesaurus of predicate-argument
structure can deal with several levels of verb
classes on the basis of granularity of dened verb
meaning. In the thesaurus we incorporate LCS-
based semantic description for each verb class
that can provide several argument structure such
as construction grammar (Goldberg, 1995). This
must be high advantage to describe the different
factors from the view of not only syntactic func-
tions but also internal semantic relations. Thus
this characteristics of the proposed thesaurus can
be powerful framework for calculating similar-
ity and difference between verb senses. In the
following sections we explain the total design of
thesaurus and the details.
3.1 Design of Thesaurus
The proposed thesaurus consists of hierarchy of
verb classes we assumed. A verb class, which
is a conceptual class, has verbs with a shared
meaning. A parent verb class includes concepts
of subordinate verb class; thus a subordinate
verb class is a concretization of the parent verb
class. A verb class has a semantic description
that is a kind of semantic skeleton inspired from
lexical conceptual structure (Jackendoff, 1990;
Kageyama, 1996; Dorr, 1997). Thus a seman-
tic description in a verb class describes core se-
mantic relations between arguments and shadow
arguments of a shared meaning of the verb class.
Since verb can be polysemous, each verb sense is
designated with example sentences. Verb senses
with a shared meaning are assigned to a verb
class. Every example sentence is analyzed into
their arguments and semantic role types; and
then their arguments are linked to variables in se-
mantic description of verb class. This indicates
that one semantic description in a verb class can
provide several argument structure on the basis
of syntactic structure. This architecture is related
to construction grammar.
Here we explain this structure using verbs
such as rent, lend, give, hire, borrow, lease. We
assume that each verb sense we focus on here is
designated by example sentences, e.g., ?Mother
gives a book to her child?, ?Kazuko rents a bicy-
cle from her friend?, and ?Taro lend a car to his
friend?. As Figure 1 shows that all of the above
verb senses are involved in the verb classMoving
of One?s Possession 5. The semantic description,
which expresses core meaning of the verb class
Moving of One?s Possession is
([Agent] CAUSE) ?
BECOME [Theme] BE AT [Goal].
Where the brackets [] denote variables that can
be lled with arguments in example sentences.
Likewise parentheses () denote occasional factor.
?Agent? and ?Theme? are semantic role labels
that can be annotated to all example sentences.
Figure 1 shows that the children of the verb
class Moving of One?s Possession are the two
verb classesMoving of One?s Possession/Renting
and Moving of One?s Possession/Lending. In the
Renting class, rent, hire and borrow are there,
while in the Lending class, lend and lease exist.
Both of the semantic descriptions in the children
verb classes are more detailed ones than the par-
ent?s description.
	






	
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 21?30,
Beijing, August 2010
Automatic Classification of Semantic Relations
between Facts and Opinions
Koji Murakami? Eric Nichols? Junta Mizuno?? Yotaro Watanabe?
Hayato Goto? Megumi Ohki? Suguru Matsuyoshi? Kentaro Inui? Yuji Matsumoto?
?Nara Institute of Science and Technology
?Tohoku University
{kmurakami,matuyosi,hayato-g,megumi-o,matsu}@is.naist.jp
{eric-n,junta-m,inui}@ecei.tohoku.ac.jp
Abstract
Classifying and identifying semantic re-
lations between facts and opinions on
the Web is of utmost importance for or-
ganizing information on the Web, how-
ever, this requires consideration of a
broader set of semantic relations than are
typically handled in Recognizing Tex-
tual Entailment (RTE), Cross-document
Structure Theory (CST), and similar
tasks. In this paper, we describe the con-
struction and evaluation of a system that
identifies and classifies semantic rela-
tions in Internet data. Our system targets
a set of semantic relations that have been
inspired by CST but that have been gen-
eralized and broadened to facilitate ap-
plication to mixed fact and opinion data
from the Internet. Our system identi-
fies these semantic relations in Japanese
Web texts using a combination of lexical,
syntactic, and semantic information and
evaluate our system against gold stan-
dard data that was manually constructed
for this task. We will release all gold
standard data used in training and eval-
uation of our system this summer.
1 Introduction
The task of organizing the information on the In-
ternet to help users find facts and opinions on
their topics of interest is increasingly important
as more people turn to the Web as a source of
important information. The vast amounts of re-
search conducted in NLP on automatic summa-
rization, opinion mining, and question answer-
ing are illustrative of the great interest in mak-
ing relevant information easier to find. Provid-
ing Internet users with thorough information re-
quires recognizing semantic relations between
both facts and opinions, however the assump-
tions made by current approaches are often in-
compatible with this goal. For example, the
existing semantic relations considered in Rec-
ognizing Textual Entailment (RTE) (Dagan et
al., 2005) are often too narrow in scope to be
directly applicable to text on the Internet, and
theories like Cross-document Structure Theory
(CST) (Radev, 2000) are only applicable to facts
or second-hand reporting of opinions rather than
relations between both.
As part of the STATEMENT MAP project we
proposed the development of a system to sup-
port information credibility analysis on the Web
(Murakami et al, 2009b) by automatically sum-
marizing facts and opinions on topics of inter-
est to users and showing them the evidence and
conflicts for each viewpoint. To facilitate the de-
tection of semantic relations in Internet data, we
defined a sentence-like unit of information called
the statement that encompasses both facts and
opinions, started compiling a corpus of state-
ments annotated with semantic relations (Mu-
rakami et al, 2009a), and begin constructing a
system to automatically identify semantic rela-
tions between statements.
In this paper, we describe the construction and
evaluation of a prototype semantic relation iden-
tification system. We build on the semantic rela-
tions proposed in RTE and CST and in our pre-
vious work, refining them into a streamlined set
of semantic relations that apply across facts and
opinions, but that are simple enough to make
automatic recognition of semantic relations be-
tween statements in Internet text possible.Our
semantic relations are [AGREEMENT], [CON-
FLICT], [CONFINEMENT], and [EVIDENCE].
[AGREEMENT] and [CONFLICT] are expansions
of the [EQUIVALENCE] and [CONTRADICTION]
21
relations used in RTE. [CONFINEMENT] and
[EVIDENCE] are new relations between facts
and opinions that are essential for understanding
how statements on a topic are inter-related.
Our task differs from opinion mining and sen-
timent analysis which largely focus on identify-
ing the polarity of an opinion for defined param-
eters rather than identify how facts and opinions
relate to each other, and it differs from web doc-
ument summarization tasks which focus on ex-
tracting information fromweb page structure and
contextual information from hyperlinks rather
than analyzing the semantics of the language on
the webpage itself.
We present a system that automatically iden-
tifies semantic relations between statements in
Japanese Internet texts. Our system uses struc-
tural alignment to identify statement pairs that
are likely to be related, then classifies seman-
tic relations using a combination of lexical, syn-
tactic, and semantic information. We evaluate
cross-statement semantic relation classification
on sentence pairs that were taken from Japanese
Internet texts on several topics and manually an-
notated with a semantic relation where one is
present. In our evaluation, we look closely at the
impact that each of the resources has on semantic
relation classification quality.
The rest of this paper is organized as follows.
In Section 2, we discuss related work in summa-
rization, semantic relation classification, opinion
mining, and sentiment analysis, showing how
existing classification schemes are insufficient
for our task. In Section 3, we introduce a set of
cross-sentential semantic relations for use in the
opinion classification needed to support informa-
tion credibility analysis on the Web. In Section
4, we present our cross-sentential semantic re-
lation recognition system, and discuss the algo-
rithms and resources that are employed. In Sec-
tion 5, we evaluate our system in a semantic rela-
tion classification task. In Section 6, we discuss
our findings and conduct error analysis. Finally,
we conclude the paper in Section 7.
2 Related Work
2.1 Recognizing Textual Entailment
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment, the
task of deciding whether the meaning of one
text is entailed from another text. A major
task in the RTE Challenge (Recognizing Tex-
tual Entailment Challenge) is classifying the se-
mantic relation between a Text (T) and a Hy-
pothesis (H) into [ENTAILMENT], [CONTRA-
DICTION], or [UNKNOWN]. Over the last sev-
eral years, several corpora annotated with thou-
sands of (T,H) pairs have been constructed for
this task. In these corpora, each pair was tagged
indicating its related task (e.g. Information Ex-
traction, Question Answering, Information Re-
trieval or Summarization).
The RTE Challenge has successfully em-
ployed a variety of techniques in order to rec-
ognize instances of textual entailment, including
methods based on: measuring the degree of lex-
ical overlap between bag of words (Glickman
et al, 2005; Jijkoun and de Rijke, 2005), the
alignment of graphs created from syntactic or se-
mantic dependencies (Marsi and Krahmer, 2005;
MacCartney et al, 2006), statistical classifiers
which leverage a wide range of features (Hickl
et al, 2005), or reference rule generation (Szpek-
tor et al, 2007). These approaches have shown
great promise in RTE for entailment pairs in the
corpus, but more robust models of recognizing
logical relations are still desirable.
The definition of contradiction in RTE is that
T contradicts H if it is very unlikely that both T
and H can be true at the same time. However, in
real documents on the Web, there are many pairs
of examples which are contradictory in part, or
where one statement confines the applicability of
another, as shown in the examples in Table 1.
2.2 Cross-document Structure Theory
Cross-document Structure Theory (CST), devel-
oped by Radev (2000), is another task of rec-
ognizing semantic relations between sentences.
CST is an expanded rhetorical structure analy-
sis based on Rhetorical Structure Theory (RST:
(William and Thompson, 1988)), and attempts
to describe the semantic relations that exist
between two or more sentences from differ-
ent source documents that are related to the
same topic, as well as those that come from
a single source document. A corpus of cross-
document sentences annotated with CST rela-
tions has also been constructed (The CSTBank
Corpus: (Radev et al, 2003)). CSTBank is
organized into clusters of topically-related ar-
ticles. There are 18 kinds of semantic rela-
tions in this corpus, not limited to [EQUIVA-
LENCE] or [CONTRADICTION], but also includ-
ing [JUDGEMENT], [ELABORATION], and [RE-
22
Query Matching sentences Output
??????????????
???
?????????????????????????????????? ??
The cavity-prevention effects are greater the more Xylitol is included. [AGREEMENT].
????????????????????????????? ??
Xylitol is effective at preventing
cavities.
Xylitol shows effectiveness at maintaining good oral hygiene and preventing cavities. [AGREEMENT]
???????????????????????????????????
?????????????
??
There are many opinions about the cavity-prevention effectiveness of Xylitol, but it
is not really effective.
[CONFLICT]
?????????
???????????????????????????????? ??
Reduced water, which has weak alkaline ions, supports the health of you and your
family.
[AGREEMENT]
?????????????????????????????????? ??
Reduced water is good for the
health.
Reduced water is said to remove active oxygen from the body, making it effective at
promoting good health.
[AGREEMENT]
??????????????????????? ??
Even if oxidized water tastes good, it does not help one?s health. [CONFLICT]
??????????????
???
???????????????????????????????????
?????????
??
Isoflavone is effective at
maintaining good health.
Taking too much soy isoflavone as a supplement will have a negative effect on one?s
health
[CONFINEMENT]
Table 1: Example semantic relation classification.
FINEMENT]. Etoh et al (Etoh and Okumura,
2005) constructed a Japanese Cross-document
Relation Corpus, and they redefined 14 kinds of
semantic relations in their corpus.
CST was designed for objective expressions
because its target data is newspaper articles re-
lated to the same topic. Facts, which can be ex-
tracted from newspaper articles, have been used
in conventional NLP research, such as Informa-
tion Extraction or Factoid Question Answering.
However, there are a lot of opinions on the Web,
and it is important to survey opinions in addition
to facts to give Internet users a comprehensive
view of the discussions on topics of interest.
2.3 Cross-document Summarization Based
on CST Relations between Sentences
Zhang and Radev (2004) attempted to classify
CST relations between sentence pairs extracted
from topically related documents. However, they
used a vector space model and tried multi-class
classification. The results were not satisfactory.
This observation may indicate that the recog-
nition methods for each relation should be de-
veloped separately. Miyabe et al (2008) at-
tempted to recognize relations that were defined
in a Japanese cross-document relation corpus
(Etoh and Okumura, 2005). However, their tar-
get relations were limited to [EQUIVALENCE]
and [TRANSITION]; other relations were not tar-
geted. Recognizing [EVIDENCE] is indispens-
able for organizing information on the Internet.
We need to develop satisfactory methods of [EV-
IDENCE] recognition.
2.4 Opinion Mining and Sentiment Analysis
Subjective statements, such as opinions, have
recently been the focus of much NLP re-
search including review analysis, opinion ex-
traction, opinion question answering, and senti-
ment analysis. In the corpus constructed in the
Multi-Perspective Question Answering (MPQA)
Project (Wiebe et al, 2005), individual expres-
sions are tagged that correspond to explicit men-
tions of private states, speech event, and expres-
sive subjective elements.
The goal of opinion mining to extract expres-
sions with polarity from texts, not to recognize
semantic relations between sentences. Sentiment
analysis also focus classifying subjective expres-
sions in texts into positive/negative classes. In
comparison, although we deal with sentiment in-
formation in text, our objective is to recognize
semantic relations between sentences. If a user?s
query requires positive/negative information, we
will also need to extract sentences including sen-
timent expression like in opinion mining, how-
ever, our semantic relation, [CONFINEMENT], is
more precise because it identifies the condition
or scope of the polarity. Queries do not neces-
sarily include sentiment information; we also ac-
cept queries that are intended to be a statement
of fact. For example, for the query ?Xylitol is
effective at preventing cavities.? in Table 1, we
extract a variety of sentences from the Web and
recognize semantic relations between the query
and many kinds of sentences.
23
3 Semantic Relations between
Statements
In this section, we define the semantic relations
that we will classify in Japanese Internet texts as
well as their corresponding relations in RTE and
CST. Our goal is to define semantic relations that
are applicable over both fact and opinions, mak-
ing them more appropriate for handling Internet
texts. See Table 1 for real examples.
3.1 [AGREEMENT]
A bi-directional relation where statements have
equivalent semantic content on a shared topic.
Here we use topic in a narrow sense to mean that
the semantic contents of both statements are rel-
evant to each other.
The following is an example of [AGREE-
MENT] on the topic of bio-ethanol environmental
impact.
(1) a. Bio-ethanol is good for the environment.
b. Bio-ethanol is a high-quality fuel, and it
has the power to deal with the environ-
ment problems that we are facing.
Once relevance has been established,
[AGREEMENT] can range from strict logi-
cal entailment or identical polarity of opinions.
Here is an example of two statements that
share a broad topic, but that are not classified as
[AGREEMENT] because preventing cavities and
tooth calcification are not intuitively relevant.
(2) a. Xylitol is effective at preventing cavities.
b. Xylitol advances tooth calcification.
3.2 [CONFLICT]
A bi-directional relation where statements have
negative or contradicting semantic content on a
shared topic. This can range from strict logical
contradiction to opposite polarity of opinions.
The next pair is a [CONFLICT] example.
(3) a. Bio-ethanol is good for our earth.
b. There is a fact that bio-ethanol further the
destruction of the environment.
3.3 [EVIDENCE]
A uni-directional relation where one statement
provides justification or supporting evidence for
the other. Both statements can be either facts or
opinions. The following is a typical example:
(4) a. I believe that applying the technology of
cloning must be controlled by law.
b. There is a need to regulate cloning, be-
cause it can be open to abuse.
The statement containing the evidence con-
sists of two parts: one part has a [AGREEMENT]
or [CONFLICT] with the other statement, the
other part provides support or justification for it.
3.4 [CONFINEMENT]
A uni-directional relation where one statement
provides more specific information about the
other or quantifies the situations in which it ap-
plies. The pair below is an example, in which
one statement gives a condition under which the
other can be true.
(5) a. Steroids have side-effects.
b. There is almost no need to worry about
side-effects when steroids are used for lo-
cal treatment.
4 Recognizing Semantic Relations
In order to organize the information on the
Internet, we need to identify the [AGREE-
MENT], [CONFLICT], [CONFINEMENT], and
[EVIDENCE] semantic relations. Because iden-
tification of [AGREEMENT] and [CONFLICT] is
a problem of measuring semantic similarity be-
tween two statements, it can be cast as a sen-
tence alignment problem and solved using an
RTE framework. The two sentences do not need
to be from the same source.
However, the identification of [CONFINE-
MENT] and [EVIDENCE] relations depend on
contextual information in the sentence. For ex-
ample, conditional statements or specific dis-
course markers like ?because? act as important
cues for their identification. Thus, to identify
these two relations across documents, we must
first identify [AGREEMENT] or [CONFLICT] be-
tween sentences in different documents and then
determine if there is a [CONFINEMENT] or [EV-
IDENCE] relation in one of the documents.
Furthermore, the surrounding text often con-
tains contextual information that is important for
identifying these two relations. Proper handling
of surrounding context requires discourse analy-
sis and is an area of future work, but our basic
detection strategy is as follows:
1. Identify a [AGREEMENT] or [CONFLICT] re-
lation between the Query and Text
2. Search the Text sentence for cues that iden-
tify [CONFINEMENT] or [EVIDENCE]
24
3. Infer the applicability of the [CONFINE-
MENT] or [EVIDENCE] relations in the Text
to the Query
4.1 System Overview
We have finished constructing a prototype sys-
tem that detects semantic relation between state-
ments. It has a three-stage architecture similar to
the RTE system of MacCartney et al (2006):
1. Linguistic analysis
2. Structural alignment
3. Feature extraction for detecting [EVIDENCE]
and [CONFINEMENT]
4. Semantic relation classification
However, we differ in the following respects.
First, our relation classification is broader than
RTE?s simple distinction between [ENTAIL-
MENT], [CONTRADICTION], and [UNKNOWN];
in place of [ENTAILMENT] and [CONTRA-
DICTION, we use broader [AGREEMENT] and
[CONFLICT] relations. We also consider cover
gradations of applicability of statements with the
[CONFINEMENT] relation.
Second, we conduct structural alignment with
the goal of aligning semantic structures. We do
this by directly incorporating dependency align-
ments and predicate-argument structure informa-
tion for both the user query and the Web text
into the alignment scoring process. This allows
us to effectively capture many long-distance
alignments that cannot be represented as lexical
alignments. This contrasts with MacCartney et
al. (2006), who uses dependency structures for
the Hypothesis to reduce the lexical alignment
search space but do not produce structural align-
ments and do not use the dependencies in detect-
ing entailment.
Finally, we apply several rich semantic re-
sources in alignment and classification: extended
modality information that helps align and clas-
sify structures that are semantically similar but
divergent in tense or polarity; and lexical simi-
larity through ontologies like WordNet.
4.2 Linguistic Analysis
In order to identify semantic relations between
the user query (Q) and the sentence extracted
from Web text (T), we first conduct syntactic and
semantic linguistic analysis to provide a basis for
alignment and relation classification.
For syntactic analysis, we use the Japanese
dependency parser CaboCha (Kudo and Mat-
!"#$%&'!"#$!%&'()*'+$! ()*!,'-!.-'/'0+1!#$)23#!+,-!$4$50*$!./!%&!6! 7!!"#$%&'!"#$!%&'()*'+$!01234!'&3$'.'-'&%&!
567!*)-%'8&!
897:;!&85#!)&!9!3-$)3/$+3!
<=>?@A/!#)&!:$$+!&#';+!
()BC-! B! C!?!
)!
5!:!J!3'!#)*$!!#$)23#!)..2%5)0'+&!
C!J!
J!
B!
B!C!
Figure 1: An example of structural alignment
sumoto, 2002) and the predicate-argument struc-
ture analyzer ChaPAS (Watanabe et al, 2010).
CaboCha splits the Japanese text into phrase-like
chunks and represents syntactic dependencies
between the chunks as edges in a graph. Cha-
PAS identifies predicate-argument structures in
the dependency graph produced by CaboCha.
We also conduct extended modality analysis
using the resources provided by Matsuyoshi et
al. (2010), focusing on tense, modality, and po-
larity, because such information provides impor-
tant clues for the recognition of semantic rela-
tions between statements.
4.3 Structural Alignment
In this section, we describe our approach to
structural alignment. The structural alignment
process is shown in Figure 1. It consists of the
following two phases:
1. lexical alignment
2. structural alignment
We developed a heuristic-based algorithm to
align chunk based on lexical similarity infor-
mation. We incorporate the following informa-
tion into an alignment confidence score that has
a range of 0.0-1.0 and align chunk whose scores
cross an empirically-determined threshold.
? surface level similarity: identical content
words or cosine similarity of chunk contents
? semantic similarity of predicate-argument
structures
predicates we check for matches in predi-
cate entailment databases (Hashimoto et
al., 2009; Matsuyoshi et al, 2008) consid-
ering the default case frames reported by
ChaPAS
arguments we check for synonym or hy-
pernym matches in the Japanese WordNet
(2008) or the Japanese hypernym collec-
tion of Sumida et al (2008)
25
>?@???????????????????AB?C????DEF)!
>?'???????????????????AB?C????GHF)!I!
T :!
H :!
(field) (in)!(agricultural chemicals) (ACC)! (use)!
(field) (on)!(agricultural chemicals) (ACC)! (spray)!
Figure 2: Determining the compatibility of se-
mantic structures
We compare the predicate-argument structure
of the query to that of the text and determine
if the argument structures are compatible. This
process is illustrated in Figure 2 where the T(ext)
?Agricultural chemicals are used in the field.? is
aligned with the H(ypothesis) ?Over the field,
agricultural chemicals are sprayed.? Although
the verbs used and sprayed are not directly se-
mantically related, they are aligned because they
share the same argument structures. This lets up
align predicates for which we lack semantic re-
sources. We use the following information to de-
termine predicate-argument alignment:
? the number of aligned children
? the number of aligned case frame arguments
? the number of possible alignments in a win-
dow of n chunk
? predicates indicating existence or quantity.
E.g. many, few, to exist, etc.
? polarity of both parent and child chunks us-
ing the resources in (Higashiyama et al,
2008; Kobayashi et al, 2005)
We treat structural alignment as a machine
learning problem and train a Support Vector Ma-
chine (SVM) model to decide if lexically aligned
chunks are semantically aligned.
We train on gold-standard labeled alignment
of 370 sentence pairs. This data set is described
in more detail in Section 5.1. As features for our
SVM model, we use the following information:
? the distance in edges in the dependency graph
between parent and child for both sentences
? the distance in chunks between parent and
child in both sentences
? binary features indicating whether each
chunk is a predicate or argument according
to ChaPAS
? the parts-of-speech of first and last word in
each chunk
? when the chunk ends with a case marker, the
case of the chunk , otherwise none
? the lexical alignment score of each chunk
pair
4.4 Feature Extraction for Detecting
Evidence and Confinement
Once the structural alignment system has iden-
tified potential [AGREEMENT] or [CONFLICT]
relations, we need to extract contextual cues in
the Text as features for detecting [CONFINE-
MENT] and [EVIDENCE] relations. Conditional
statements, degree adverbs, and partial negation,
which play a role in limiting the scope or degree
of a query?s contents in the statement, can be im-
portant cues for detecting the these two semantic
relations. We currently use a set of heuristics to
extract a set of expressions to use as features for
classifying these relations using SVM models.
4.5 Relation Classification
Once the structural alignment is successfully
identified, the task of semantic relation classi-
fication is straightforward. We also solve this
problem with machine learning by training an
SVM classifier. As features, we draw on a com-
bination of lexical, syntactic, and semantic infor-
mation including the syntactic alignments from
the previous section. The feature set is:
alignments We define two binary function,
ALIGNword(qi, tm) for the lexical align-
ment and ALIGNstruct((qi, qj), (tm, tk))
for the structural alignment to be true if and
only if the node qi, qj ? Q has been seman-
tically and structurally aligned to the node
tm, tk ? T . Q and T are the (Q)uery and the
(T)ext, respectively. We also use a separate
feature for a score representing the likelihood
of the alignment.
modality We have a feature that encodes all of
the possible polarities of a predicate node
from modality analysis, which indicates the
utterance type, and can be assertive, voli-
tional, wish, imperative, permissive, or in-
terrogative. Modalities that do not repre-
sent opinions (i.e. imperative, permissive and
interrogative) often indicate [OTHER] rela-
tions.
antonym We define a binary function
ANTONYM(qi, tm) that indicates if
the pair is identified as an antonym. This
information helps identify [CONFLICT].
26
Relation Measure 3-class Cascaded 3-class ?
[AGREEMENT] precision 0.79 (128 / 162) 0.80 (126 / 157) +0.01
[AGREEMENT] recall 0.86 (128 / 149) 0.85 (126 / 149) -0.01
[AGREEMENT] f-score 0.82 0.82 -
[CONFLICT] precision 0 (0 / 5) 0.36 (5 / 14) +0.36
[CONFLICT] recall 0 (0 / 12) 0.42 (5 / 12) +0.42
[CONFLICT] f-score 0 0.38 +0.38
[CONFINEMENT] precision 0.4 (4 / 10) 0.8 (4 / 5) +0.4
[CONFINEMENT] recall 0.17 (4 / 23) 0.17 (4 / 23) -
[CONFINEMENT] f-score 0.24 0.29 +0.05
Table 2: Semantic relation classification results comparing 3-class and cascaded 3-class approaches
negation To identify negations, we primar-
ily rely on a predicate?s Actuality value,
which represents epistemic modality and
existential negation. If a predicate pair
ALIGNword(qi, tm) has mismatching actu-
ality labels, the pair is likely a [OTHER].
contextual cues This set of features is used to
mark the presence of any contextual cues
that identify of [CONFINEMENT] or [EVI-
DENCE] relations in a chunk . For example,
??? (because)? or ??? (due to)? are typ-
ical contextual cues for [EVIDENCE], and ?
?? (when)? or ???? (if)? are typical for
[CONFINEMENT].
5 Evaluation
5.1 Data Preparation
In order to evaluate our semantic relation clas-
sification system on realistic Web data, we con-
structed a corpus of sentence pairs gathered from
a vast collection of webpages (2009a). Our basic
approach is as follows:
1. Retrieve documents related to a set number
of topics using the Tsubaki1 search engine
2. Extract real sentences that include major sub-
topic words which are detected based on
TF/IDF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate
5. Annotate corresponding sentences with
[AGREEMENT], [CONFLICT], [CONFINE-
MENT], or [OTHER]
1http://tsubaki.ixnlp.nii.ac.jp/
Although our target semantic relations in-
clude [EVIDENCE], they difficult annotate con-
sistently, so we do not annotate them at this
time. Expanding our corpus and semantic re-
lation classifier to handle [EVIDENCE] remains
and area of future work.
The data that composes our corpus comes
from a diverse number of sources. A hand sur-
vey of a random sample of the types of domains
of 100 document URLs is given below. Half of
the URL domains were not readily identifiable,
but the known URL domains included govern-
mental, corporate, and personal webpages. We
believe this distribution is representative of in-
formation sources on the Internet.
type count
academic 2
blogs 23
corporate 10
governmental 4
news 5
press releases 4
q&a site 1
reference 1
other 50
We have made a partial release of our corpus
of sentence pairs manually annotated with the
correct semantic relations2. We will fully release
all the data annotated semantic relations and with
gold standard alignments at a future date.
5.2 Experiment Settings
In this section, we present results of empiri-
cal evaluation of our proposed semantic rela-
tion classification system on the dataset we con-
structed in the previous section. For this experi-
ment, we use SVMs as described in Section 4.5
2http://stmap.naist.jp/corpus/ja/
index.html (in Japanese)
27
to classify semantic relations into one of the four
classes: [AGREEMENT], [CONFLICT], [CON-
FINEMENT], or [OTHER] in the case of no re-
lation. As data we use 370 sentence pairs that
have been manually annotated both with the cor-
rect semantic relation and with gold standard
alignments. Annotations are checked by two na-
tive speakers of Japanese, and any sentence pair
where annotation agreement is not reached is
discarded. Because we have limited data that is
annotated with correct alignments and semantic
relations, we perform five-fold cross validation,
training both the structural aligner and semantic
relation classifier on 296 sentence pairs and eval-
uating on the held out 74 sentence pairs. The
figures presented in the next section are for the
combined results on all 370 sentence pairs.
5.3 Results
We compare two different approaches to classi-
fication using SVMs:
3-class semantic relations are directly classified
into one of [AGREEMENT], [CONFLICT],
and [CONFINEMENT] with all features de-
scribed in 4.5
cascaded 3-class semantic relations are first
classified into one of [AGREEMENT], [CON-
FLICT] without contextual cue features. Then
an additional judgement with all features de-
termines if [AGREEMENT] and [CONFLICT]
should be reclassified as [CONFINEMENT]
Initial results using the 3-class classifica-
tion model produced high f-scores for [AGREE-
MENT] but unfavorable results for [CONFLICT]
and [CONFINEMENT]. We significantly im-
proved classification of [CONFLICT] and [CON-
FINEMENT] by adopting the cascaded 3-class
model. We present these results in Table 2 and
successfully recognized examples in Table 1.
6 Discussion and Error Analysis
We constructed a prototype semantic relation
classification system by combining the compo-
nents described in the previous section. While
the system developed is not domain-specific and
capable of accepting queries on any topic, we
evaluate its semantic relation classification on
several queries that are representative of our
training data.
Figure 3 shows a snapshot of the semantic re-
lation classification system and the various se-
mantic relations it recognized for the query.
Baseline Structural Upper-boundAlignment
Precision 0.44 0.52 0.74(56/126) (96/186) (135/183)
Recall 0.30 0.52 0.73(56/184) (96/184) (135/184)
F1-score 0.36 0.52 0.74
Table 3: Comparison of lexical, structural, and
upper-bound alignments on semantic relation
classification
In the example (6), recognized as [CONFINE-
MENT] in Figure 3, our system correctly identi-
fied negation and analyzed the description ?Xyl-
itol alone can not completely? as playing a role
of requirement.
(6) a. ?????????????????
(Xylitol is effective at preventing cavi-
ties.)
b. ?????????????????
????
(Xylitol alone can not completely prevent
cavities.)
Our system correctly identifies [AGREE-
MENT] relations in other examples about re-
duced water from Table 1 by structurally align-
ing phrases like ?promoting good health? and
?supports the health? to ?good for the health.?
These examples show how resources like
(Matsuyoshi et al, 2010) and WordNet (Bond et
al., 2008) have contributed to the relation clas-
sification improvement of structural alignment
over them baseline in Table 3. Focusing on sim-
ilarity of syntactic and semantic structures gives
our alignment method greater flexibility.
However, there are still various examples
which the system cannot recognized correctly.
In examples on cavity prevention, the phrase
?effective at preventing cavities? could not be
aligned with ?can prevent cavities? or ?good for
cavity prevention,? nor can ?cavity prevention?
and ?cavity-causing bacteria control.?
The above examples illustrate the importance
of the role played by the alignment phase in the
whole system?s performance.
Table 3 compares the semantic relation classi-
fication performance of using lexical alignment
only (as the baseline), lexical alignment and
structural alignment, and, to calculate the maxi-
mum possible precision, classification using cor-
rect alignment data (the upper-bound). We can
28
Figure 3: Alignment and classification example for the query ?Xylitol is effective at preventing
cavities.?
see that structural alignment makes it possible to
align more words than lexical alignment alone,
leading to an improvement in semantic relation
classification. However, there is still a large gap
between the performance of structural alignment
and the maximum possible precision. Error anal-
ysis shows that a big cause of incorrect classifi-
cation is incorrect lexical alignment. Improving
lexical alignment is a serious problem that must
be addressed. This entails expanding our cur-
rent lexical resources and finding more effective
methods of apply them in alignment.
The most serious problem we currently face is
the feature engineering necessary to find the op-
timal way of applying structural alignments or
other semantic information to semantic relation
classification. We need to conduct a quantita-
tive evaluation of our current classification mod-
els and find ways to improve them.
7 Conclusion
Classifying and identifying semantic relations
between facts and opinions on the Web is of ut-
most importance to organizing information on
the Web, however, this requires consideration of
a broader set of semantic relations than are typi-
cally handled in RTE, CST, and similar tasks. In
this paper, we introduced a set of cross-sentential
semantic relations specifically designed for this
task that apply over both facts and opinions. We
presented a system that identifies these semantic
relations in Japanese Web texts using a combina-
tion of lexical, syntactic, and semantic informa-
tion and evaluated our system against data that
was manually constructed for this task. Prelimi-
nary evaluation showed that we are able to detect
[AGREEMENT] with high levels of confidence.
Our method also shows promise in [CONFLICT]
and [CONFINEMENT] detection. We also dis-
cussed some of the technical issues that need to
be solved in order to identify [CONFLICT] and
[CONFINEMENT].
Acknowledgments
This work is supported by the National Institute
of Information and Communications Technology
Japan.
References
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and
Kiyotaka Uchimoto. 2008. Boot-strapping a
wordnet using multiple existing wordnets. In Proc.
of the 6th International Language Resources and
Evaluation (LREC?08).
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proc. of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Etoh, Junji and Manabu Okumura. 2005. Cross-
document relationship between sentences corpus.
29
In Proc. of the 14th Annual Meeting of the Associa-
tion for Natural Language Processing, pages 482?
485. (in Japanese).
Glickman, Oren, Ido Dagan, and Moshe Koppel.
2005. Web based textual entailment. In Proc. of
the First PASCAL Recognizing Textual Entailment
Workshop.
Hashimoto, Chikara, Kentaro Torisawa, Kow
Kuroda, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition
from the web. In Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP2009), pages 1172?1181.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2005. Recog-
nizing textual entailment with lcc?s groundhog sys-
tem. In Proc. of the Second PASCAL Challenges
Workshop.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring noun polarity knowl-
edge using selectional preferences. In Proc. of the
14th Annual Meeting of the Association for Natu-
ral Language Processing.
Jijkoun, Valentin and Maarten de Rijke. 2005. Rec-
ognizing textual entailment using lexical similar-
ity. In Proc. of the First PASCAL Challenges Work-
shop.
Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto,
Kenji Tateishi, and Toshikazu Fukushima. 2005.
Collecting evaluative expressions for opinion ex-
traction. Journal of natural language processing,
12(3):203?222.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc of CoNLL 2002, pages 63?69.
MacCartney, Bill, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D.
Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proc. of
HLT/NAACL 2006.
Marsi, Erwin and Emiel Krahmer. 2005. Classifi-
cation of semantic relations by humans and ma-
chines. In Proc. of ACL-05 Workshop on Empiri-
cal Modeling of Semantic Equivalence and Entail-
ment, pages 1?6.
Matsuyoshi, Suguru, Koji Murakami, Yuji Mat-
sumoto, and Kentaro Inui. 2008. A database of re-
lations between predicate argument structures for
recognizing textual entailment and contradiction.
In Proc. of the Second International Symposium
on Universal Communication, pages 366?373, De-
cember.
Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao,
Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2010. Annotating event mentions in text
with modality, focus, and source information. In
Proc. of the 7th International Language Resources
and Evaluation (LREC?10), pages 1456?1463.
Miyabe, Yasunari, Hiroya Takamura, and Manabu
Okumura. 2008. Identifying cross-document re-
lations between sentences. In Proc. of the 3rd In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-08), pages 141?148.
Murakami, Koji, Shouko Masuda, Suguru Mat-
suyoshi, Eric Nichols, Kentaro Inui, and Yuji Mat-
sumoto. 2009a. Annotating semantic relations
combining facts and opinions. In Proceedings of
the Third Linguistic Annotation Workshop, pages
150?153, Suntec, Singapore, August. Association
for Computational Linguistics.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009b. Statement map: Assist-
ing information credibility analysis by visualizing
arguments. In Proc. of the 3rd ACM Workshop
on Information Credibility on the Web (WICOW
2009), pages 43?50.
Radev, Dragomir, Jahna Otterbacher,
and Zhu Zhang. 2003. CSTBank:
Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Radev, Dragomir R. 2000. Common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In Proc. of the 1st SIG-
dial workshop on Discourse and dialogue, pages
74?83.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in wikipedia. In Proc. of the 6th International
Language Resources and Evaluation (LREC?08).
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
456?463.
Watanabe, Yotaro, Masayuki Asahara, and Yuji Mat-
sumoto. 2010. A structured model for joint learn-
ing of argument roles and predicate senses. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (to appear).
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165?210.
William, Mann and Sandra Thompson. 1988.
Rhetorical structure theory: towards a functional
theory of text organization. Text, 8(3):243?281.
Zhang, Zhu and Dragomir Radev. 2004. Combin-
ing labeled and unlabeled data for learning cross-
document structural relationships. In Proc. of the
Proceedings of IJC-NLP.
30
Recognizing Confinement in Web Texts
Megumi Ohki?
Suguru Matsuyoshi?
Junta Mizuno?
Kentaro Inui?
Nara Insutitute of Science and Technology?
Eric Nichols?
Koji Murakami??
Shouko Masuda?
Yuji Matsumoto?
Tohoku University?
{megumi-o,shouko,matuyosi,matsu}@is.naist.jp
{eric,junta-m,inui}@tohoku.ac.jp
koji.murakami@mail.rakuten.co.jp
Abstract
In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three se-
mantic relations: ENTAILMENT, CONTRADICTION or UNKNOWN. While we find some sentence pairs
hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one
another depending on a specific situation. These partial contradiction sentence pairs contain useful infor-
mation for opinion mining and other such tasks, but it is difficult for Internet users to access this knowledge
because current frameworks do not differentiate between full contradictions and partial contradictions. In
this paper, under current approaches to semantic relation recognition, we define a new semantic relation
known as CONFINEMENT in order to recognize this useful information. This information is classified as
either CONTRADICTION or ENTAILMENT. We provide a series of semantic templates to recognize CON-
FINEMENT relations in Web texts, and then implement a system for recognizing CONFINEMENT between
sentence pairs. We show that our proposed system can obtains a F-score of 61% for recognizing CON-
FINEMENT in Japanese-language Web texts, and it outperforms a baseline which does not use a manually
compiled list of lexico-syntactic patterns to instantiate the semantic templates.
1 Introduction
On the Internet, there are various kinds of documents, and they often include conflicting opinions or
differing information on a single topic. Collecting and organizing this diverse information is an important
part of multi-document summarization.
When searching with a particular query on the Internet, we want information that tells us what other
people think about the query: e.g. do they believe it is true or not; what are the necessary conditions
for it to apply. For example, consider the hypothetical search results for the query given in (1). You get
opinion (2a), which supports the query, and opinion (2b) which opposes it.
(1) Xylitol is effective at preventing tooth decay.
(2) a. Xylitol can prevent tooth decay.
b. Xylitol is not effective at all at preventing tooth decay.
A major task in the Recognizing Textual Entailment (RTE) Challenge (Giampiccolo et al (2007)) is
classifying the semantic relation between a Text and a Hypothesis into ENTAILMENT, CONTRADICTION,
or UNKNOWN. Murakami et al (2009) report on the STATEMENT MAP project, the goal of which is
to help Internet users evaluate the credibility of information sources by analyzing supporting evidence
from a variety of viewpoints on their topics of interest and presenting them to users together with the
supporting evidence in a way that makes it clear how they are related. A variety of techniques have been
successfully employed in the RTE Challenge in order to recognize instances of textual entailment.
?Current afflication: Rakuten Institute of Technology
215
However, as far as we know, there have been no studies on recognizing sentences which specify condi-
tions under which a query applies, despite the fact that these relations are useful information for Internet
users. Such useful sentences are plentiful on the Web. Consider the following examples of CONTRA-
DICTION and ENTAILMENT.
(3) a. Xylitol can not prevent tooth decay if it not at least 50%.
b. The effect of Xylitol on preventing tooth decay is limited.
In example (3a), the necessary condition to prevent tooth decay by Xylitol is ?it contains at least fifty
percent Xylitol?. That condition is expressed by the phrase in bold in (3a). This sentence informs users
that if they want to prevent tooth decay, the products they use must contain a certain amount of Xylitol to
be effective. In example (3b), we obtain information on uncertainty of Xylitol?s tooth decay prevention
effectiveness from the phrase ?is limited?. It tells that Xylitol is not necessarily effective at preventing
tooth decay, and thus it is not completely in agreement with or contradiction to the original sentence (1).
It is important to recognize the semantic relation shown in (3) because it provides more specific infor-
mation about the query or specifies the conditions under which the statement holds or does not. This is
valuable information for Internet users and needs to be distinguished from fully contradicting or agreeing
opinions.
We call this semantic relation CONFINEMENT because it confines the situation under which a query
applies. In this paper, we give a language independent definition of the CONFINEMENT relation in pred-
icate logic and provide a framework for detecting the relation through a series of semantic templates that
take logical and semantic features as input. We implement a system that detects CONFINEMENT rela-
tions between sentence pairs in Japanese by instantiating the semantic templates using rules and a list of
lexico-semantic patterns. Finally, we conduct empirical evaluation of recognition of the CONFINEMENT
relation between queries and sentences in Japanese-language Web texts.
2 Related Work
In RTE research, only three types of relations are defined: ENTAILMENT, CONTRADICTION, and
UNKNOWN. RTE is an important task and has been the target of much research (Szpektor et al (2007);
Sammons et al (2009)). However, none of the previous research has introduced relations corresponding
to CONFINEMENT.
Cross-document Structure Theory (CST, Radev (2000)) is another approach to recognizing semantic
relations between sentences. CST is an extended rhetorical structure analysis based on Rhetorical Struc-
ture Theory (RST). It attempts to describe the semantic relations between two or more sentences from
different source documents that are related to the same topic. It defines 18 kinds of semantic relations
between sentences. Etoh and Okumura (2005) constructed a Japanese Cross-document Relation Corpus
and defined 14 kinds of semantic relations. It is difficult to consider CONFINEMENT relations in the
CST categorical semantic relations because it focuses on comparing sentences in terms of equivalence
and difference between sentences. At first glance, CONFINEMENT may seem to be defined in terms of
difference between sentences, but this approach does not capture the idea of restriction on a sentence?s
applicability. Thus, it is beyond the scope of CST.
In the field of linguistics, Nakagawa and Mori (1995) discussed restrictions as represented in the
four Japanese subordinate clause patterns. Abe (1996) researched the role of quantifiers in quantitative
restrictions and the role of ??? (only).? There is much other researches on expressions representing
?confinement? in a sentence in linguistics. These expressions are useful in order to recognize phrases
which contradict each other. However, as far as we know, there is no research on the relation of CON-
FINEMENT between two sentences in the linguistics literature. The absence of related research makes
defining and recognizing CONFINEMENT a very challenging task.
3 The CONFINEMENT Relation
We present the definition of the CONFINEMENT relation and describe its differences from ENTAIL-
MENT and CONTRADICTION. In essence, a pair of sentences is in the CONFINEMENT relation if either
the premise or consequent of the second sentence has a certain condition or restriction, and without such
condition or restriction the pair is equivalent to either ENTAILMENT or CONTRADICTION.
216
Consider an example of CONFINEMENT setence pair: (2a) and (3a). The statement ?it (Xylitol) is not at
least 50%? is a condition of the statement ?Xylitol can not prevent tooth decay.? It is a CONTRADICTION
if the conditional statement is satisfied. Because the truth value of the whole statement depends on
various conditions to be satisfied, it is important to properly define a framework to define them.
3.1 A Logical Definition of CONFINEMENT
We present a definition of CONFINEMENT in predicate logic. We define CONFINEMENT as a semantic
relation between two sentences, where the first sentence corresponds to RTE?s Hypothesis, or the user
Query, and the second sentence corresponds to RTE?s Text that has some semantic relation with the
Query, which we want to identify.
Here we consider sentence pairs where the Query matches the logical pattern ?x(P (x) ? C(x)),
where we call P (x) the Premise and C(x) the Consequence. There are many ways of representing
sentences as logical expressions, and we think that the logical pattern (?(P (x) ? C(x))) can cover a
variety of queries. For example, the sentence ?Xylitol is effective at preventing tooth decay.? can be
represented as ?x(isXylitol(x) ? effectiveAtPreventingToothDecay(x)). Consider the case where one
sentence contains only a Consequence. This case can be regarded as a special case of the above formula.
We write such a sentence as ?x(T ? C(x)) showing that the Premise is always True.
In this paper, we limit discussion of the CONFINEMENT relation to the Query matching to the above
logical pattern. Recognizing CONFINEMENT between the Text and the Query having more complex
semantic patterns is an area of future work. Here, we split the definition of CONFINEMENT into subtypes
according to: (i) conditions to satisfy in addition to the Premise, and (ii) limitations on the degree of the
Consequence.
Premise side Additional conditions for achieving the Consequence
Explicit constraint
Some conditional sentences use an expression correspoinding to logical ?only if,? which explicitly
means two way conditions as the following formula.
?x((P (x) ?AdditionalCondition(x) ? C(x)) (1)
?(P (x) ? ?AdditionalCondition(x) ? ?C(x)))
For example, S1 in Table 1, ?Xylitol is effective at preventing cavities only when it is 100%?,
explicitly specify that Xylitol is effective if it is 100% and is not effective if it is not 100%. So,
we assume the form of the above formula for this type of statement.
Implicit constraint
This type of sentence specifies an additional condition on the Premise and is represented by the
following formula. The Premise needs to be satisfied for the consequence to be achieved.
?x((P (x) ?AdditionalCondition(x) ? C(x)) (2)
Example S5 in Table 1 says ?Xylitol is effective at preventing tooth decay if it is 100%?, which
is assumed by Formula (2). S5 does not contain an expression such as ?only (??)?, which
explicitly specifies that C(x) does not hold when an additional condition is not satisfied. One may
understand that it implicitly means ?Xylitol is not effective at preventing tooth decay if it is not
100%,? but S5 does not structly require this.
Consequence side Constraints on the degree of achieving the Consequence
There are sentences in partial entailment or contradiction where the degree of achieving of the Con-
sequence is limited. To represent these limitations on the Consequence side, we define a CONFINE-
MENT relation where the degrees of the Consequence are limited as in Example (3b). We define the
following formula to represent these limitations on the Consequence side.
?x((P (x) ? Cr(x)) (3)
Cr(x) represents C(x) with additional restriction. For example, S3 in Table 1 says that Xylitol is
somewhat effective at preventing tooth decay, which means that there are cases in which Xylitol can
not prevent tooth decay. In the case of S3, Cr(x) is ?is a bit effective?. This type of CONFINE-
MENT provides valuable information about Xylitol?s limited ability to promote dental hygiene in S3.
217
All CONFINEMENTs on the Consequence side are of type EXPLICIT CONFINEMENT, because they
explicitly mean that a part of the Consequence is achieved but no other parts are achieved.
3.2 Semantic Templates
We propose a series of semantic templates to classify sentence pairs into one of the CONFINEMENT
relation subtypes we define. The semantic templates take a set of features as input and use their values
to categorize the sentence pair. In Section 4, we evaluate the coverage of the semantic templates by
classifying a small set of sentence pairs using manually set feature values. In Section 6, we provide
more realistic evaluation by using a proposed system to set the feature values automatically and classify
sentence pairs as ENTAILMENT / CONTRADICTION, or CONFINEMENT.
We assume that each sentence consists of a Premise and Consequence, and that each sentence pair
which has a CONFINEMENT relation contains at least one additional condition or one additional limitation
as defined in Section 3.1.
We know that there are a variety of expressions that indicate the presence of a CONFINEMENT relation.
For example, both ?Only 100% pure Xylitol is effective at preventing tooth decay.? and ?Xylitol is not
effective at preventing tooth decay unless it is 100% pure.? are CONFINEMENTs of ?Xylitol is effective
at preventing tooth decay.? Since it is impossible to handle all possible expressions that indicate CON-
FINEMENT, we focus on covering as many as possible with three features: (1) the type of constraint, (2)
the type of Premise, and (3) the type of Consequence. The features are defined in more detail below.
IF-Constraint This feature indicates the type of logical constraint in the Text sentence. Its values can
be ?IF,? ?ONLY-IF.?
Premise This feature indicates the type of Premise in the Text sentence. The value ?P+A? or ?notP+A?
means there is an Additional Condition on the Premise. The value ?P? or ?notP? means there is just
a Premise. ?not? represents the Premise have a negation.
Consequence This feature indicates the type of Consequence. Its possible values are ?C? (just a Conse-
quence), ?notC? (negated Consequence), ?Cr? or ?notCr? (certain partial Consequence).
Semantic templates consist of a tuple of four feature values and a mapping to the confinement type they
indicate. A full list of templates is given in Table 1. In the templates, a wildcard asterisk ?*? indicates
that any feature value can match in that slot of the template. The abbreviations ENT, CONT and CONF
stand for ENTAILMENT, CONFINEMENT and CONFINEMENT respectively.
Semantic templates are applied in turn from top pattern by determining the value of each feature and
looking up the corresponding relation type in Table 1. We give a classification examples below. The user
query is sentence S0. Sentences S1 are Web texts.
Query : S0. Xylitol is effective at preventing tooth decay.
Text [ONLY-IF P(x) ? AC(x) then C(x) ]: S1. Xylitol is effective at preventing tooth decay when you
take it every day without fail.
In Example, IF-Constraint is ?ONLY-IF?, Premise is ?P+A?, and the type of Consequence is ?C?.
This instance has an additional condition and the Consequence matches the Query, so it is identified as
an EXPLICIT CONFINEMENT.
4 Verifying Semantic Templates
In this section, we verify the effectiveness of semantic templates in recognizing CONFINEMENT rela-
tions by testing them on real-world data in Japanese. To directly evaluate the quality of the templates,
we construct a small data set of sentence pairs and manually annotate them with the correct values for
each of the features defined in Section 3.2.
4.1 Data
We constructed the Development set and the Open-test set of sample Japanese user queries and Inter-
net text pairs following the methodology of Murakami et al (2009). However, Murakami et al (2009)
annotated Query-Text pairs with coarse-grained AGREEMENT and CONFLICT relations that subsume the
218
Table 1: Semantic templates for recognizing CONFINEMENT
Semantic features Relation Number of Number of Example
IF-constraint Premise Consequence positive negative S0:?????????????????.
example example Xylitol is effective at preventing tooth decay.
ONLY-IF P+A * EXPLICIT 8 0 S1:??????? 100%?????????????????.
CONF Xylitol is effective at preventing tooth decay only when it is 100%.
ONLY-IF notP+A * EXPLICIT 0 0 S2:??????? 50%??????????????????????.
CONF Xylitol is effective at preventing tooth decay only when it is not under 50%.
* * Cr EXPLICIT 11 0 S3:??????????????????????.
CONF Xylitol is a bit effective at preventing tooth decay.
* * notCr EXPLICIT 12 0 S4:????????????????????????.
CONF Xylitol is not almost of effective at preventing tooth decay.
IF P+A * IMPLICT 62 0 S5:??????? 100%???????????????.
CONF Xylitol is effective at preventing tooth decay if it is 100%.
IF notP+A * IMPLICIT 1 0 S6:??????? 100%???????????????????.
CONF Xylitol is not effective at preventing tooth decay if it is not 100%
IF P C ENT 279 0 S7:???????????????????????.
Xylitol is effective at preventing tooth decay if it is eaten.
IF notP C CONT 0 0 S8:????????????????????????.
Xylitol is effective at preventing tooth decay if it is not eaten.
IF P notC CONT 13 0 S9:????????????????????????.
Xylitol is not effective at preventing tooth decay if it is eaten.
IF notP notC ENT 0 0 S10:????????????????????????.
Xylitol is not effective at preventing tooth decay if it is not eaten.
ONLY-IF P C ENT 3 0 S11:??????????????????????????.
Xylitol is effective at preventing tooth decay only when it is eaten.
ONLY-IF notP C CONT 0 0 S12:?????????????????????????????.
Xylitol is effective at preventing tooth decay only when it is not eaten.
ONLY-IF P notC CONT 0 0 S13:??????????????????????????.
Xylitol is effective at preventing tooth decay only when it is eaten.
ONLY-IF notP notC ENT 0 0 S14:?????????????????????????????.
Xylitol is not effective at preventing tooth decay only when it is not eaten.
Table 2: Data set (Counts of sentences out of parenthesis and statements in parentheses)
Entailment Contradiction Confinement All
Development 258 (282) 8 (13) 79 (94) 345 (389)
Open-test 230 170 200 600
RTE relations of ENTAILMENT and CONTRADICTION. As our task is to discriminate between CON-
FINEMENT and RTE relations, we annotate each sentence pair or each statement1 pair with one of the
following relations instead: ENTAILMENT, CONTRADICTION, or CONFINEMENT. In the case of CON-
FINEMENT, we annotate Query-Text pairs which are not full ENTAILMENT or CONTRADICTION but
these Text partially agrees and disagrees with the Query. Annotations were checked by two native speak-
ers of Japanese, and any sentence pair where annotation agreement is not reached was discarded. Table
2 shows that how many sentences or statements are in each data set. Annotated statements counts are
written in parentheses. We use the Development set for evaluation of verifying semantic templates and
develop list of lexical and syntactic patterns for semantic features extraction, and the Open-test set for
evaluation in Section 6.
4.2 Verification Result
After the data was prepared, we annotated it with the correct feature values for use with the semantic
templates. This was done by manually checking for words or phrases in the sentences that indicated one
of the features in Table 1. Once the features were set, we used them to classify each sentence pair.
We give the numbers of instances that we could confirm for each pattern in the sixth column of Table
1 and the numbers of negative instances in the seventh column, which satisfy semantic template but does
not agree Relation values in the fifth column. As a result we find that there were no statement pairs that
could not be successfully classified. We grasp CONFINEMENT relation with semantic templates for the
most part. This verification data does not cover all combinations of patterns in our semantic templates, so
we can not rule out the possibility of existence of an exception that cannot be classified by the semantic
templates. However, we find these results to be an encouraging indication of the usefulness of semantic
templates. Here are some example classifications found in the verification data.
Coordinate clauses Combining multiple of IMPLICIT CONFINEMENTs results in an EXPLICIT CON-
FINEMENT relation
(4)S0. ????????????.
Steroid has side-effects.
S1. ???????????????????????????????????????
1Murakami et al define a ?statement? as the smallest unit that can convey a complete thought or viewpoint. In practice, this
can be a sentence or something smaller such as a clause.
219
???????????????????.
Long-term use of steroid causes side-effects, but there is no need to worry about side-effects
in short-term usage.
In Example (4), S1 is an EXPLICIT CONFINEMENT for S0. This is derived from the combination of
CONFINEMENT of the two coordinate clauses of S1: the former phrase ?Long-term use of steroid causes
side-effects? of S1 is an IMPLICIT CONFINEMENT for S0 by our semantic templates and the latter phrase
is an IMPLICIT CONFINEMENT for S0.
Additional information for whole Query Combining of a CONTRADICTION and an IMPLICIT CON-
FINEMENT result in an EXPLICIT CONFINEMENT
(5)S0. ????????????????.
Xylitol is effective at preventing tooth decay.
S1. ??????????????????????,???????????????????????????????????????.
Tooth decay can not be prevented with Xylitol alone, but it can be fundamentally prevented
with an appropriate diet and by taking Xylitol after every meal.
The first clause before the comma in S1 of Example (5) corresponds to the entire sentence of S0. The
second clause after the comma helps us recognize that it is a CONFINEMENT relation. This instance
is also a combination of semantic templates, so we need to recognize negation of each statement and
adversative conjunction but we do not need to add new features to Table 1.
5 Proposed System
We propose a system which uses semantic templates for recognizing CONFINEMENT consists of six
steps: (I) linguistic analysis, (II) structural alignment, (III) Premise and Consequence identification,
(IV) semantic feature extraction, (V) adversative conjunction identification, and (VI) semantic template
application. Figure 1 shows the work flow of the system. This system takes as input corresponding to S0
and S1, and return a semantic relation.
5.1 I. Linguistic Analysis
In linguistic analysis, we conduct word segmentation, POS tagging, dependency parsing, and extended
modality analysis. This linguistic analysis acts as the basis for alignment and semantic feature extrac-
tion. For syntactic analysis, we identify words and POS tags with the Japanese morphological analyser
Mecab2, and we use the Japanese dependency parser CaboCha (Kudo and Matsumoto (2002)) to pro-
duce dependency trees. We also conduct extended modality analysis using the resources provided by
Matsuyoshi et al (2010).
5.2 II. Structural Alignment
To identify the consequence of S0 in S1, we use Structural Alignment (Mizuno et al (2010)). In Struc-
tural Alignment, dependency parent-child links are aligned across sentences using a variety of resources
to ensure semantic relatedness.
5.3 III. Premise and Consequence identification
In this step, we identify the Premise and the Consequence in S1. When a sentence pair satisfies all
items is satisfying, we can identify a focused chunk as the Consequence in S1:
1. A chunk?s modality in S0 is assertion, this chunk is the Consequence in S0
2. A chunk in S1 align with the Consequence in S0
We identify the Premise in S1 when a sentence pair satisfies first, and either second or third item of
the following conditions:
1. A case particle of chunks in S0 is either ?? (agent marker)? or ?? (topic marker)? and these chunks
are children of the Consequence in S0?s dependency tree
2. The subject in S0 aligns with the subject of S1
3. All of the dependants of the expression ??? (to, for)? have alignments in S0 dependency tree
2http://chasen.org/taku/ software/mecab/.
220
Figure 1: An overview of a proposal system to recognize CONFINEMENT
5.4 IV. Semantic Feature Extraction
We extract features for the semantic templates using a list of lexical and syntactic patterns. These
patterns were manually compiled using the development data set introduced in Section 4. Features for
the semantic templates are then automatically extracted by applying these patterns to input sentence
pairs. The following overviews our extraction approach for each feature.
5.4.1 IF-Constraint Feature Extraction
Using CaboCha, we manually constructed lists of words and their POS that are indicators of the
semantic condition under which a Premise occurs. We extract as features any words in the input sentences
that appear in the list with the corresponding POS. The ?IF? lexical type lists conjunctions that are the
results of a conditional chunk or noun phrases that indicate a case or situation. The ?ONLY-IF? lexical
type is used to represent the most constraining situations. The following is our list of expressions.
? IF: ?? (in case),?/??/? (when),?/??/?? (if),? (with)
? ONLY-IF: ??/??? (for this time), ??/??/?? (only), ??? (for the first time), ?? (to,
for)
5.4.2 Premise Feature Extraction
We treat the words or phrases which are extracted from the constraint as conditions, and need to decide
whether a given condition is the Premise or an additional condition for the Premise. The Premise is set
to ?P? when first step and either the second or third step of the following conditions are satisfied, and it
is set to ?P+A? otherwise:
1. ? The condition have children in the S1?s dependency tree or the condition?s children are not aligned
to chunks in S0
2. ? The condition?s parent in S0?s dependency tree has any chunk with a child aligned with the Conse-
quence in S0, or the condition?s parent is not aligned with chunks in S0
3. ? The condition?s parent does not have any expression with the meaning of ?use? in the S0?s depen-
dency tree
When these step are satisfied and negation exists in conditional chunks, Premise is set to ?notP+A,? if
these step are not satisfied, Premise is set to ?notP.? In the third step, we identify expressions with the
meaning of ?use? with our lexical list. For example ?? (use), ??? (eat), ?? (take) and so on. If
the condition?s parent has words in our lexical list, we identify that ?Xylitol? and ?eating Xylitol? and
?using Xylitol? are equivalent.
5.4.3 Consequence Feature Extraction
This feature is used to indicate the semantic relationship between Consequences of the sentences pair.
Sentences with Consequences that share a certain amount of similarity in polarity and syntax are judged
to have ENTAILMENT, otherwise they are in CONTRADICTION. In order to be judged as ENTAILMENT,
the following conditions must all be true:
1. The modality of the Consequences must be identical.
2. The polarity of the Consequences must be identical as indicated by the resources in (Sumida et al
(2008))
3. The Premises of both sentences must align with each other
221
4. ? The sentences must not contain expressions that limit range or degree such as ????? (almost)?
or ??? (degree)?
When all item are satisfied, the Consequence is set to ?C?, otherwise it is set to ?notC.? We identify
whether the consequence has expressions which limit the degree or not. The Consequence is set to ?Cr?
or ?notCr? when the following all conditions is satisfied:
1. Any of the children of the Consequence align with a chunk in S0?s dependency tree.
2. ? There are expressions limiting the degree of the Consequence or the siblings in S1?s dependency
tree
When this two steps are satisfied and the all four steps to judge whether sentence pairs is ENTAILMENT
or not are not satisfied, Consequence is set to ?notCr.?
5.5 V. Adversative Conjunction Identification
We manually compiled a list of target expressions including conjunctions such as ?? (but).? When a
S1 chunk containing an adversative conjunction that aligns with the Premise of S0 or the S0?s Premise
depends on S1 chunk containing an adversative conjunction, we set each feature set in a chunk before an
adversative conjunction and after an adversative conjunction to semantic templates.
5.6 VI. Semantic Template Application
We apply semantic feature extracted in Step IV to semantic templates. If S1 matches multiple semantic
templates with an adversative conjunction from Step V, we combine the semantic templates. We get a
relation for a sentence pair in this step.
5.7 Example of Semantic Features Extraction
Feature extraction is illustrated in greater detail in the examples S0 which is the query and S1 in
Table 1. First, we identify words represented IF-Constraint is ?ONLY-IF?: ?? (when)? is in S1 and the
conditional chunk has a word ??? (only).? Next, we evaluate each the type of Premise of each chunk to
determine if it is a premise or an additional condition. The subject word ?Xylitol? align between S0 and
S1, and the conditional chunk?s sibling in dependency tree of S1 is a chunk which has the subject. And
the conditional chunk have a child which is not aligned any chunk in S0, it is ?100%? (100%).? And the
conditional chunk has no negations. So, Premise is set to ?P+A.? Finally, we check if the consequences
to the conditions are aligned to the verbs and nouns indicating consequences in S0: ?prevent? and ?is
effective? are aligned, the modality and polarity of the Consequence are identical, these depended on by
the condition, and the Consequence has no expressions which limited range or degree. Consequence is
set to ?C.?We set the semantic template features and get a result which the sentences relation is EXPLICIT
CONFINEMENT. Ideally patterns for setting semantic feature for semantic templates should be learned
automatically, but this remains an area of future work. Nonetheless, our current experiment gives a good
measure of the effectiveness of semantic templates in recognizing CONFINEMENT relations.
6 Evaluation
In Section 4, we verified that the semantic templates defined in Section 3.2 can successfully classify
semantic relations as CONFINEMENT given the correct feature values. In this Section, we present the
results of an experiment in a more realistic setting by using semantic templates together with the features
automatically extracted as described with our proposed system in Section 5 to determine whether or not
a sentence pair has a CONFINEMENT relation.
6.1 Setting up Evaluation
While more research on recognizing ENTAILMENT or CONTRADICTION between sentences pairs is
necessary, it is important to recognize new relations that cannot be analysed in existing frameworks in
order to provide Internet users with the information they need. Thus, We assume that unrelated sentence
pairs will be discarded before classification, in this experiment we focus only on the recognition of
CONFINEMENT relations. So our goal in this experiment is to classify between CONFINEMENT and NOT
CONFINEMENT. We will evaluate determining whether CONFINEMENT sentence pairs are Explicit or
Implicit in future. In our experiment, we used a gold data for structural alignment to evaluate semantic
feature extraction.
222
Table 3: Results of recognizing confinement relations with our proposal system
Recall Precision F-Score
proposed system 0.65(129/200) 0.57(129/225) 0.61
baseline system 0.96(192/200) 0.34(192/562) 0.50
Table 4: Instances of incorrect classification
S0 S1
A ???????????????. ??????????????????????????????.
False A person can regain their health with isoflavon. Excess intake of isoflavon to boost its health effects is prohibited.
Negative B ?????????????????. ?????????????????????????????????????????????????????????.
Xylitol has effects on preventing tooth decay. The use of xylitol is effective at preventing tooth decay when done while eating properly and brushing one?s
teeth regularly.
C ????????????????????. ?????????????????????????????????????.
Xylitol can prevent tooth decay. It is a big mistake to think that one can prevent tooth decay if they put Xylitol in ? their mouth.
False D ??????????????. ???????????????????????????.
Positive Steroids can cure illnesses. Atrophic dermatitis will heal completely if steroid use is stopped.
E ???????????????. ?????????????????????????????????????????????.
Side effects are a worry for steroids. The amount of steroids or period of time that causes side effects differs from person to person.
6.2 Baseline System
We developed a baseline system that does not use our manually-compiled lexico-syntactic patterns
in order to act as a point of comparison for the proposed system in evaluating their contribution to
CONFINEMENT recognition.
The baseline system consists of performing all of the steps from of our proposed system that do not
rely on manually compiled lexico-syntactic patterns. Step relying on these resources are marked with a
? in Section 5 and are skipped in the baseline. Essentially, we conduct Steps I, II, and III, the parts of
Step IV that can be done without manually-compiled patterns, and, finally, Step VI.
In Step IV, we determine if there are any limitations on the Consequence in the Consequence Feature
subset, but we do not judge whether the Consequence is ENTAILMENT or CONTRADICTION in the
baseline system.
6.3 Result and Error Analysis
The results are given in Table 3. We find that our system has much higher precision than the baseline,
improving by over 20%. In our system, the list of semantic patterns is effective at recognizing CON-
FINEMENT. On the other hand recall has gone down compared to the baseline. The baseline judged that
almost sentences are CONFINEMENT, so the list of semantic patterns employed in our rule-based system
is useful at eliminating false positives. Table 4 shows some instances of incorrect classification. Each
instance is a pair (S0, S1).
Example A-S1 means ?Excess intake of isoflavon can not boost one?s health? and ?excess intake? is
an additional condition for A-S1. In this case ?excess? is a lexical specifier of the specific condition and
is indicated by the particle ???. The particle ?? (topic marker)? is not currently used as a feature in the
semantic templates since it is very noisy, so this instance can not be detected. We need to expand our
method of acquiring semantic patterns to better handle such cases.
The additional condition phrase in Example B-S1 modifies ?The use of Xylitol? instead of ?is effective
at preventing tooth decay?, preventing us from properly recognizing the limiting condition in this case.
We need to conduct deeper scopal analysis to determine when the modifier of an embedded chunk should
be considered as an additional condition.
Example C-S1 is an instance where the system fails to recognize that ?put in their mouth? is an expres-
sion meaning ?use? since our lists of lexical words for features did not have it. We should increase our
ability to recognize synonyms of ?to use? by automatically mining data for paraphrases or approaching
it as a machine learning task in order to handle examples like C-S1. On the other hands ?if steroid use
is stopped? in example D-S1 is the premise which should indicate an IF condition and Negation exists,
however we can not recognize it correctly since the phrase lacks negation. We will make a list of words
and phrases that are antonyms of ?use? in order to recognize such instances.
The condition in example E-S1 is about how side-effects appear, and not a condition for the other
sentence example E-S0. This instance requires detailed semantic analysis and cannot be solved with
alignment-based approaches. It represents a very difficult class of problems.
223
7 Conclusion
On theWeb, much of the information and opinions we encounter indicates the conditions or limitations
under which a statement is true. This information is important to Internet users who are interested in
determining the validity of a query of interest, but such information cannot be represented under the
prevalent RTE framework containing only ENTAILMENT and CONTRADICTION.
In this paper, we provided a logical definition of the CONFINEMENT relation and showed how it
could be used to represent important information that is omitted under an RTE framework. We also
proposed a set of semantic templates that use set of features extracted from sentences pairs to recognize
CONFINEMENT relations between two sentences. Preliminary investigations showed that given correct
feature input, semantic templates could effectively recognize CONFINEMENT relations.
In addition, we presented empirical evaluation of the effectiveness of semantic templates and
automatically-extracted features at recognizing CONFINEMENT between user queries and Web text pairs,
and conducted error analysis of the results. Currently, our system does not deal with unknown instances
well since it extracts features for semantic template using manually constructed lexical patterns. In fu-
ture work, we will learn features for the semantic templates directly from data to better handle unknown
instances.
Acknowledgment
This work is supported by the National Institute of Information and Communications Technology
Japan.
References
Abe, T. (1996). Restriction with? dake ?and modification with quantifier. Tsukuba Japanese Research 1, 4?20. in Japanese.
Etoh, J. and M. Okumura (2005). Cross-document relationship between sentences corpus. In Proceedings of the 14th Annual
Meeting of the Association for Natural Language Processing, pp. 482?485. (in Japanese).
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan (2007). The third pascal recognizing textual entailment challenge. In
Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE ?07, Morristown, NJ, USA, pp.
1?9. Association for Computational Linguistics.
Kudo, T. and Y. Matsumoto (2002). Japanese dependency analysis using cascaded chunking. In CoNLL 2002: In Proceedings
of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops), pp. 63?69.
Matsuyoshi, S., M. Eguchi, C. Sao, K. Murakami, K. Inui, and Y. Matsumoto (2010). Annotating event mentions in text
with modality, focus, and source information. In Proceedings of the 7th International Language Resources and Evaluation
(LREC?10).
Mizuno, J., H. Goto, Y. Watanabe, K. Murakami, K. Inui, and Y. Matsumoto (2010). Local Structural Alignment for Recogniz-
ing Semantic Relations between Sentences. In Proceedings of IPSJ-NL196. (in Japanese).
Murakami, K., S. Matsuyoshi, K. Inui, and Y. Matsumoto (2009). A corpus of statement pairs with semantic relations in
Japanese. In Proceedings of the 15th Annual Meeting of the Association for Natural Language Processing.
Murakami, K., E. Nichols, S. Matsuyoshi, A. Sumida, S. Masuda, K. Inui, and Y. Matsumoto (2009). Statement map: As-
sisting information credibility analysis by visualizing arguments. In Proceedings of the 3rd ACM Workshop on Information
Credibility on the Web (WICOW 2009), pp. 43?50.
Nakagawa, H. and T. Mori (1995). Pragmatic analysis of aspect morphemes in manual sentences in Japanese. The Association
for Natural Language Processing 2(4), 19 ? 36. in Japanese.
Radev, D. R. (2000). Common theory of information fusion from multiple text sources step one: Cross-document structure. In
Proceedings of the 1st SIGdial workshop on Discourse and dialogue, pp. 74?83.
Sammons, M., V. G. V. Vydiswaran, T. Vieira, N. Johri, M.-W. Chang, D. Goldwasser, V. Srikumar, G. Kundu, Y. Tu, K. Small,
J. Rule, Q. Do, and D. Roth (2009). Relation alignment for textual entailment recognition. In Proceedings of Recognizing
Textual Entailment 2009.
Sumida, A., N. Yoshinaga, and K. Torisawa (2008). Boosting precision and recall of hyponymy relation acquisition from
hierarchical layouts in Wikipedia. In Proceedings of the 6th International Language Resources and Evaluation (LREC?08).
Szpektor, I., E. Shnarch, and I. Dagan (2007). Instance-based evaluation of entailment rule acquisition. In Proceedings of the
45th Annual Meeting of the Association of Computational Linguistics, pp. 456?463.
224
LAW VIII - The 8th Linguistic Annotation Workshop, pages 70?74,
Dublin, Ireland, August 23-24 2014.
A Corpus Study for Identifying Evidence on Microblogs
Paul Reisert
1
Junta Mizuno
2
Miwa Kanno
1
Naoaki Okazaki
1,3
Kentaro Inui
1
1
Gradute School of Information Sciences, Tohoku University / Miyagi, Japan
2
Resilient ICT Research Center, NICT / Miyagi, Japan
3
Japan Science and Technology Agency (JST) / Tokyo, Japan
preisert@ecei.tohoku.ac.jp junta-m@nict.go.jp {meihe, okazaki, inui}@ecei.tohoku.ac.jp
Abstract
Microblogs are a popular way for users to communicate and have recently caught the attention
of researchers in the natural language processing (NLP) field. However, regardless of their rising
popularity, little attention has been given towards determining the properties of discourse rela-
tions for the rapid, large-scale microblog data. Therefore, given their importance for various NLP
tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations.
As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study
to identify such relations on Twitter, a popular microblogging service. We create annotation
guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence
relations. Finally, we report our observations, annotation difficulties, and data statistics.
1 Introduction
Microblogs have become a popular method for users to express their ideas and communicate with other
users. Twitter
1
, a popular microblogging service, has recently been the attraction of many natural lan-
guage processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender
inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid large-
scale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists
for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this
work is to determine and annotate evidence relations on microblogs.
Our primary motivation behind focusing on evidence relations includes the possibility of discovering
support for a claim which can support the debunking of false information. During the March 2011
Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain
information on current conditions, such as family member whereabouts, refuge center information, and
general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil
explosion causing toxic rain, interfered with those looking to find correct information on the status of
the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false
information is necessary in order to provide accurate information to victims and others relying on and
trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such
as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post
with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for
their claim. An example is provided in Figure 1.
We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013;
Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of
discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end
up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim
found in the parent post. However, we consider source mentions or hyperlinks, which can either stand
alone or be contained in a statement, question, or request, as a way to answer the above question.
To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In
terms of argumentation corpora, the Araucaria Argumentation Corpus
2
exists which utilizes various
argumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995). In this work, we
1
https://twitter.com
2
http://araucaria.computing.dundee.ac.uk/doku.php
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
70
# 