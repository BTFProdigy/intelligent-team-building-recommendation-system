Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142?1151,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
On the Role of Lexical Features in Sequence Labeling
Yoav Goldberg
?
and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We use the technique of SVM anchoring to
demonstrate that lexical features extracted
from a training corpus are not necessary to
obtain state of the art results on tasks such
as Named Entity Recognition and Chunk-
ing. While standard models require as
many as 100K distinct features, we derive
models with as little as 1K features that
perform as well or better on different do-
mains. These robust reduced models in-
dicate that the way rare lexical features
contribute to classification in NLP is not
fully understood. Contrastive error analy-
sis (with and without lexical features) in-
dicates that lexical features do contribute
to resolving some semantic and complex
syntactic ambiguities ? but we find this
contribution does not generalize outside
the training corpus. As a general strat-
egy, we believe lexical features should not
be directly derived from a training corpus
but instead, carefully inferred and selected
from other sources.
1 Introduction
Common NLP tasks, such as Named Entity
Recognition and Chunking, involve the identifi-
cation of spans of words belonging to the same
phrase. These tasks are traditionally reduced to
a tagging task, in which each word is to be clas-
sified as either Beginning a span, Inside a span,
or Outside of a span. The decision is based on
the word to be classified and its neighbors. Fea-
tures supporting the classification usually include
?
Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
the word forms themselves and properties derived
from the word forms, such as prefixes, suffixes,
capitalization information, and parts-of-speech.
While early approaches to the NP-chunking task
(Cardie and Pierce, 1998) relied on part-of-speech
information alone, it is widely accepted that lexi-
cal information (word forms) is crucial for build-
ing accurate systems for these tasks. Indeed,
all the better-performing systems in the CoNLL
shared tasks competitions for Chunking (Sang and
Buchholz, 2000) and Named Entity Recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) make extensive use of such
lexical information.
Is this belief justified? In this paper, we show
that the influence of lexical features on such se-
quence labeling tasks is more complex than is gen-
erally assumed. We find that exact word forms
aren?t necessary for accurate classification. This
observation is important because relying on the
exact word forms that appear in a training corpus
leads to over-fitting, as well as to larger models.
In this work, we focus on learning with Support
Vector Machines (SVMs) (Vapnik, 1995). SVM
classifiers can handle very large feature spaces,
and produce state-of-the-art results for NLP ap-
plications (see e.g. (Kudo and Matsumoto, 2000;
Nivre et al, 2006)). Alas, when trained on pruned
feature sets, in which rare lexical items are re-
moved, SVM models suffer a loss in classifica-
tion accuracy. It would seem that rare lexical
items are indeed crucial for SVM classification
performance. However, in Goldberg and Elhadad
(2007), we suggested that the SVM learner is us-
ing the rare lexical features for singling out hard
cases rather than for learning meaningful general-
izations. We provide further evidence to support
this claim in this paper.
1142
We show that by using a variant of SVM ?
Anchored SVM Learning (Goldberg and Elhadad,
2007) with a polynomial kernel, one can learn
accurate models for English NP-chunking (Mar-
cus and Ramshaw, 1995), base-phrase chunking
(CoNLL 2000), and Dutch Named Entity Recog-
nition (CoNLL 2002), on a heavily pruned feature
space. Our models make use of only a fraction
of the lexical features available in the training set
(less than 1%), and yet provide highly-competitive
accuracies.
For the Chunking and NP-Chunking tasks, the
most heavily pruned experiments, in which we
consider only features appearing at least 100 times
in the training corpus, do show a small but signif-
icant drop in accuracy on the testing corpus com-
pared to the non-pruned models exposed to all
available features in the training data. We pro-
vide detailed error analysis of a development set
in Section 6, revealing the causes for these differ-
ences. We suggest one additional binary feature
in order to account for some of the performance
gap. Moreover, we show that the differences in
accuracy vanish when the lexicalized and unlexi-
calized models are tested on text from slightly dif-
ferent sources than the training corpus (Section 7).
This goes to show that with an appropriate
learning method, orthographic and structural (in
the form of POS tag sequences) information is suf-
ficient for achieving state-of-the-art performance
on these kind of sequence labeling tasks. This
does not mean semantic information is not needed
for these tasks. It does mean that current models
capture only a tiny amount of such semantic in-
formation through rare lexical features, and in a
manner that does not generalize well.
We believe this data motivates a different strat-
egy to incorporate lexical features into classifica-
tion models: instead of collecting the raw lexical
forms appearing in a training corpus, we should at-
tempt to actively construct a feature space includ-
ing lexical features derived from external sources.
The feature representation of (Collobert and We-
ston, 2008) could be a step in that direction. We
also believe that hard cases for sequence labeling
(POS ambiguity, coordination, long syntactic con-
structs) could be directly approached with special-
ized classifiers.
1.1 Related Work
This work complements a similar line of results
from the parsing literature. While it was ini-
tially believed that lexicalization of PCFG parsers
(Collins, 1997; Charniak, 2000) is crucial for
obtaining good parsing results, Gildea (2001)
demonstrated that the lexicalized Model-1 parser
of Collins (1997) does not benefit from bilexical
information when tested on a new text domain,
and only marginally benefits from such informa-
tion when tested on the same text domain as the
training corpora. This was followed by (Bikel,
2004) who showed that bilexical-information is
used in only 1.49% of the decisions in Collins?
Model-2 parser, and that removing this informa-
tion results in ?an exceedingly small drop in per-
formance?. However, uni-lexical information was
still considered crucial. Klein and Manning (2003)
bridged the gap between lexicalized and unlexi-
calized parsing performance, providing a compet-
itive unlexicalized parsing model, relying on lex-
ical information for only a few closed-class lex-
ical items. This was recently followed by (Mat-
suzaki et al, 2005; Petrov et al, 2006) who intro-
duce state-of-the-art nearly unlexicalized PCFG
parsers.
Similarly for discriminative dependency pars-
ing, state-of-the-art parsers (McDonald, 2006;
Nivre et al, 2006) are highly lexicalized. How-
ever, the model analysis in (McDonald, 2006)
reveals that bilexical features hardly contribute
to the performance of a discriminative MST-
based dependency parser, while Kawahara and
Uchimoto (2007) demonstrate that minimally-
lexicalized shift-reduce based dependency parsers
can produce near state-of-the-art accuracy.
In this work, we address the same question of
determining the impact of lexical features on a dif-
ferent family of tasks: sequence labeling, as illus-
trated by named entity recognition and chunking.
As discussed above, all state-of-the-art published
methods rely on lexical features for such tasks
(Zhang et al, 2001; Sha and Pereira, 2003; Finkel
et al, 2005; Ratinov and Roth, 2009). Sequence
labeling includes both a structural aspect (bracket-
ing the chunks) and a tagging aspect (classifying
the chunks). While we expect the structural aspect
can benefit from techniques similar to those used
in the parsing literature, it is unclear whether the
tagging component could perform well without
detailed lexical information. We demonstrate in
this work that, indeed, lexical features are not nec-
essary to obtain competitive performance. Our ap-
proach consists in performing a detailed analysis
1143
of the role played by rare lexical features in SVM
models. We distinguish the information brought to
the model by such features from the role they play
in a specific learning method.
2 Learning with Less Features
We adopt the common feature representation in
which each data-point is represented as a sparse
D dimensional binary-valued vector f . Each of
the D possible features f
i
is an indicator func-
tion. The indicator functions look at properties of
the current or neighbouring words. An example
of such function f
i
is 1 iff the previous
word-form is DOG, 0 otherwise. The
lexical (word-form) features result in extremely
high-dimensional (yet very sparse) feature vectors
? each word-form in the vocabulary of the training
set correspond to (at-least) one indicator function.
Due to the Zipfian distribution of language data,
many of the lexical features are very rare, and ap-
pear only a couple times in the training set. Ide-
ally, we would like our classifiers to learn only
from robust features: consider only features that
appear at least k times in the training data (rare-
feature pruning). These features are more likely to
appear in unseen test data, and thus such features
can support more robust generalization.
However, we find empirically that performing
such feature pruning prior to learning SVM mod-
els hurts the performance of the learned models.
Our intuition is that this sensitivity to rare lexi-
cal features is not explained by the richness of in-
formation such rare features bring to the model.
Instead, we believe that rare lexical features help
the classifier because they make the data artifi-
cially more separable. To demonstrate this claim,
we experiment with anchored SVM, which intro-
duces artificial mechanical anchors into the model
to achieve separability, and make rare lexical fea-
tures unnecessary.
3 Learning Method
SVM are discriminative, max-margin, linear clas-
sifiers (Vapnik, 1995), which can be kernelized.
For the formulation of SVMs in the context of
NLP applications, see (Kudo and Matsumoto,
2001). SVMs with a polynomial kernel of degree
2 were shown to provide state-of-the-art perfor-
mance in many NLP application, see for example
(Kudo and Matsumoto, 2000; Nivre et al, 2006;
Isozaki and Kazawa, 2002; Goldberg et al, 2006).
SVMs cope with inseparable data by introduc-
ing a soft-margin ? allowing some of the training
instances to be classified incorrectly subject to a
penalty, controlled by a parameter C.
Anchored SVM As we show in Section 5, the
soft-margin heuristic performs sub-optimally for
NLP tasks when the data is inseparable. We use in-
stead the Anchored Learning heuristic, introduced
in (Goldberg and Elhadad, 2007). The idea behind
anchored learning is that some training instances
are inherently ambiguous. This ambiguity stems
from ambiguity in language structure, which can-
not be resolved with a given feature representa-
tion. When a data-point cannot be classified, it
might be due to missing information, which is not
available in the data representation. Instead of al-
lowing ambiguous items to be misclassified during
training, we make the training data artificially sep-
arable. This is achieved by adding a unique feature
to each training example (an anchor). These an-
chor features cause each data-point to be slightly
more similar to itself than to any other data point.
At test time, we remove anchor features.
In terms of kernel-based learning, anchored learn-
ing can be achieved by redefining the dot product
between two vectors to take into account the iden-
tity of the vectors: x
i
?
anc
x
j
= x
i
? x
j
+ ?
ij
.
The classifier learned over the anchored data
takes into account the fine interactions between
the various inseparable data points. In our ex-
periments, SVM models over anchored data have
many more support vectors than soft-margin SVM
models. However, the anchored models generalize
much better when less features are available.
Relation to L2 SVM The classic soft-margin
SVM formulation uses L1-penalty for misclassi-
fied instances. Specifically, the objective of the
learner is to minimize
1
2
||w||
2
+ C
?
i
?
i
subject
to some margin constraints, where w is a weight
vector to be learned and ?
i
is the misclassification
error for instance i. This is equivalent to maximiz-
ing the dual problem:
?
M
i=1
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
K(x
i
, x
j
)
Another variant is L2-penalty SVM (Koshiba
and Abe, 2003), in which there is a quadratic
penalty for misclassified instances.
Here, the learning objective is to minimize:
1
2
||w||
2
+
1
2
C
?
i
?
2
i
or alternatively maximize the
dual:
?
i
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
(K(x
i
, x
j
) +
?
ij
C
).
Interestingly, for the linear kernel, SVM-
1144
anchoring reduces to L2-SVM with C=1. How-
ever, for the case of non-linear kernels, anchored
and L2-SVM produce different results, as the an-
choring is applied prior to the kernel expansion.
Specifically for the case of the second-degree
polynomial kernel, L2-SVM aims to maximize:
?
i
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
((x
i
? x
j
+ 1)
2
+
?
ij
C
),
while the anchored-SVM variant would maxi-
mizes:
?
i
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
(x
i
?x
j
+?
ij
+1)
2
.
In our experiments, as discussed in Section
5.4, we find that anchored-SVM and soft-margin
SVM with tuned C value both reach good re-
sults when we reduce the amount of lexical fea-
tures. Anchored-SVM, however, does not require
fine-tuning of the error-parameter C since it in-
sures separability. As a result, we learn anchored-
SVM models quickly (few hours) as opposed to
several days per model for C-tuned soft-margin
SVM. Anchored-SVMs also provide an easy ex-
planation of the role of features in terms of sepa-
rability. Therefore, we use anchored-SVMs in our
experiments as the learning method, but we expect
that other learning methods are capable of learning
with the same reduced feature sets.
4 Experiment Setup
How important are the rare lexical features for
learning accurate NLP models? To investigate
this question, we experiment with 3 different NLP
sequence-labeling tasks. For each task, we train a
sequence of polynomial kernel (d=2) SVM classi-
fiers, using both soft-margin (C=1) and anchored
SVM. Each classifier is trained on a pruned fea-
ture set, in which only features appearing at least
k times in the training data are kept. We vary the
pruning parameter k. Pruning is performed over
all the features in the model, but lexical features
are most affected by it.
For all the models, we use the B-I-O represen-
tation, and perform multiclass classification using
pairwise-voting. For our features, we consider
properties of tokens in a 5-token window centered
around the token to be classified, as well as the
two previous classifier predictions. Results are re-
ported as F-measure over labeled identified spans.
Polynomial vs. Linear models The polynomial
kernel of degree 2 allows us to efficiently and im-
plicitly include in our models all feature pairs.
Syntactic structure information as captured by
pairs of POS-tags and Word-POS pairs is certainly
important for such syntactic tasks as Chunking
and NER, as demonstrated by the many systems
described in (Sang and Buchholz, 2000; Tjong
Kim Sang, 2002). By using the polynomial ker-
nel, we can easily make use of this information
without intensive feature-tuning for the most suc-
cessful feature pairs.
L1-SVM, L2-SVM and the choice of the C pa-
rameter Throughout our experiments, we use the
?standard? variant of SVM, L1-penalty soft mar-
gin SVM, as implemented by the TinySVM
1
soft-
ware package, with the default C value of 1. This
setting is shown to produce good results for se-
quence labeling tasks in previous work (Kudo and
Matsumoto, 2000), and is what most end-users of
SVM classifiers are likely to use. As we show
in Sect.5.4, fine-tuning the C parameter reaches
better accuracy than L1-SVM with C=1. How-
ever, as this fine-tuning is computationally expen-
sive, we first report the comparison L1-SVM/C=1
vs. anchored-SVM, which consistently reached
the best results, and was the quickest to train.
Feature Pruning vs. Feature Selection Our aim
in this set of experiments is not to find the optimal
set of lexical features, but rather to demonstrate
that most lexical items are not needed for accurate
classification in sequence labeling tasks. To this
end, we perform very crude frequency based fea-
ture pruning. We believe better motivated feature
selection technique taking into account linguistic
(e.g. prune only open-class words) or statistic in-
formation could result in slightly more accurate
models with even fewer lexical items.
5 Experiments and Results
5.1 Named Entity Recognition (NER)
We use the Dutch data set from the CoNLL 2002
shared task (Tjong Kim Sang, 2002). The aim is to
identify named entities (persons, locations, orga-
nizations and miscellaneous) in text. The task has
two stages: identification of the entities, and clas-
sification of the identified entities into their corre-
sponding types. We focus here on the identifica-
tion task.
Features: We use the following properties for
each of the relevant tokens: word-form, POS,
ORT, prefix1, prefix2, prefix3, suffix1, suffix2,
suffix3. The ORT feature can take one of the fol-
lowing values: {number, contains-digit, contains-
hyphen, capitalized, all-capitalized, URL, punctu-
ation, regular}.
1
http://chasen.org/?taku/software/TinySVM/
1145
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 186,421 90.92 90.78
100 5,804 90.73 90.75
1000 1,207 88.56 90.10
1500 821 85.92 89.29
Table 1: Named Entity Identification results (F-
score) on dev set, with various pruning thresholds.
Results are presented in Table 1. Without fea-
ture pruning, we achieve an F-score of 90.9. This
dataset proved to be quite resilient to feature prun-
ing. Pruning features appearing less than 100
times results in just a slight decrease in F-score.
Extremely aggressive pruning, keeping only fea-
tures appearing more than 1,000 or 1,500 times in
the training data, results in a big drop in F-score
for the soft-margin SVM (from about 91 to 86).
Much less so for the Anchored-SVM. Using An-
chored SVM we achieve an F-score of 90.1 after
pruning with k = 1, 000. This model has 1207 ac-
tive features, and 27 unique active lexical forms.
5.2 NP Chunking
The goal of this task (Marcus and Ramshaw, 1995)
is the identification of non-recursive NPs. We use
the data from the CoNLL 2000 shared task: NP
chunks are extracted from Sections 15-18 (train)
and 20 (test) of the Penn WSJ corpus. POS tagged
are automatically assigned by the Brill Tagger.
Features: We consider the POS and word-form of
each token.
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 92,805 94.12 94.08
1 46,527 93.78 94.09
2 32,583 93.58 94.00
5 18,092 93.42 94.01
10 10,812 93.00 93.98
20 5,952 92.48 93.92
50 2,436 92.33 93.96
100 1,168 91.94 93.83
Table 2: NP-Chunking results (F-score), with var-
ious pruning thresholds.
Results are presented in Table 2. Without fea-
ture pruning (k = 0), the soft-margin SVM per-
forms slightly better than the Anchored-SVM. Ei-
ther of the results are state-of-the-art for this task.
However, even modest pruning (k = 2) hurts
the soft-margin model significantly. Not so for
the anchored-SVM. Even with relatively aggres-
sive pruning (k = 100), the anchored model still
achieves an impressive F-score of 93.83. Remark-
ably, in that last model, there are only 1,168 active
features, and only 209 unique active lexical forms.
5.3 Chunking
The goal of the Chunking task (Sang and Buch-
holz, 2000) is the identification of an assortment
of linguistic base-phrases. We use the data from
the CoNLL 2000 shared task.
Features: We perform two experiments. In the
first experiment, we consider the POS and word-
form of each token. In this setting, feature pruning
resulted in a bigger loss in performance than in
the two previous tasks. Preliminary error analysis
revealed that many errors are due to tagger errors,
especially of the present participle forms. This led
us to the second experiment, in which we added as
features the 2- and 3- letter suffixes for the word to
be classified (but not for the surrounding words).
Results are presented in Tables 3 and 4. In the
first experiment (POS + Word), the non-pruned
soft-margin model is the same system as the top-
performing system in the original shared task,
and yields state-of-the-art results. Unlike the NP-
chunking case, here feature pruning has a rela-
tively large impact on the results even for the an-
chored models. However, the anchored models
are still far more robust than the soft-margin ones.
With k = 100 pruning, the soft-margin model suf-
fers a drop of 2.5 F points, while the anchored
model suffers a drop of only 0.84 F points. Even
after this drop, the anchored k = 100 model still
performs above the top-third system in the CoNLL
2000 shared task. This anchored k = 100 model
has 1,180 active features, and only 209 unique ac-
tive lexical features.
The second experiment (POS + word-form +
suffixes for main word) adds crude morphological
information to the learner, helping it to avoid com-
mon tagger mistakes. This additional information
is helpful: pruning with k = 100 leads to an ac-
curate anchored model (93.12 F) with only 209
unique lexical items. Note that with the addition
of the suffix features, the pruned model k = 20
beats the purely lexical model (no suffix features)
with no pruning (93.51 vs. 93.44) with 10 times
less features. When we combine suffixes and all
lexical forms, we still see a slight advantage to
the lexical model (93.73 vs. 93.12 with pruning
at k = 100).
Even less lexicalization How robust are the suf-
fixes? We performed a third experiment, in which
1146
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 92,837 93.44 93.40
1 46,557 93.20 93.32
2 32,614 93.10 93.31
5 18,126 92.89 93.29
10 10,834 92.73 93.23
20 5,975 92.18 93.16
50 2,463 91.80 92.89
100 1,180 90.94 92.56
Table 3: Chunking results (F), with various prun-
ing thresholds. Experiment 1. Features: POS,
Word.
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 104,304 93.73 93.69
1 72,228 93.56 93.68
2 57,578 93.50 93.64
5 37,210 93.35 93.62
10 23,968 93.26 93.56
20 14,060 92.84 93.51
50 6,326 92.28 93.37
100 3,340 91.83 93.12
Table 4: Chunking results (F), with various prun-
ing thresholds. Experiment 2. Features: POS,
Word, {Suff2, Suff3} of main Word.
we replaced any explicit word-forms by 2- and 3-
letter suffixes. This gives us the complete word
form of many function words, and a reasonable
amount of morphological marking. Results are
presented in Table 5. Surprisingly, this infor-
mation proves to be quite robust. Without fea-
ture pruning, both the anchored and soft-margin
model achieve near state-of-the-art performance
of 93.25F. Pruning with k = 100 hurts the re-
sult of the soft-margin model, but the anchored
model remains robust with an F-score of 93.18.
This last model has 2,563 active features. With
further pruning (k = 250), the result of the an-
chored model drops to 92.87F (still 3rd place in
the CoNLL shared task), with only 1,508 active
features in the model.
5.4 Fine-tuned soft-margin SVMs
For the sake of completeness, and to serve as a bet-
ter comparison to the soft-margin SVM, we report
results of some experiments with both L1 and L2
SVMs, with tuned C values. NP-chunking perfor-
mance with tuned C values and various pruning
thresholds is presented in Table 6.
For these results, the C parameter was tuned
on a development set using Brent?s 1-dimension
minimization method (Brent, 1973). While tak-
ing about 40 hours of computation to fit, the fi-
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 19,910 93.25 93.23
100 2,563 92.87 93.18
250 1,508 92.40 92.87
Table 5: Chunking results (F), with various prun-
ing thresholds. Experiment 3. Features: POS ,
Suff2, Suff3 .
K L1 (C) L2 (C) ANCHORED
0 94.12 (1.0001) 94.09 (2.6128) 94.08
50 93.79 (0.0524) 93.71 (0.0082) 93.96
100 93.72 (0.0567) 93.59 (0.0072) 93.83
Table 6: NP-Chunking results (F), with various
pruning thresholds K, for L1 and L2 SVMs with
tuned C values
nal results catch up with those of the anchored-
SVM but still remain slightly lower. This further
highlights our main point: accurate models can
be achieved also with mostly unlexicalized mod-
els, and the lexical features do not contribute sub-
stantial semantic information, but rather affect the
separability of the data. This is nicely demon-
strated by SVM-anchoring, in which lexical infor-
mation is practically replaced by artificial seman-
tically void indexes, but similar performance can
also be achieved by fine-tuning other learning pa-
rameters.
6 Error Analysis
Our experiments so far indicate that very aggres-
sive feature pruning hurts performance slightly (by
about 0.5F point). The feature-pruned models are
still accurate, indicating that lexical features con-
tribute little to the classification accuracy. We now
investigate the differences between the lexicalized
and pruned models, in order to characterize the
kind of information that is available to the lexi-
calized models but missing from the pruned ones.
In the next section, we also verify that pruned-
models are more stable than the fully lexicalized
ones when tested over different text genres and do-
mains.
We focus our analysis on the chunking task, which
is a superset of the NP-chunking task. We compare
the fully lexicalized soft-margin SVM model with
the POS+suffix2+suffix3 anchored-SVM model
with k = 100 pruning. We analyze the mod-
els? respective performance on section 05 of the
WSJ corpus. This dataset is different than the of-
ficial test set. It is, however, part of the same an-
notated corpus as both the training and test sets.
On this dataset, the fully lexicalized SVM model
1147
achieves an F-score of 93.24, vs. 92.59 for the
suffix-based pruned anchored-SVM model. (The
pruned anchored-SVM model (k = 100) from ex-
periment 2, achieve a slightly higher F-score of
92.84)
We investigate only those chunks which are
identified correctly by one model but not by the
other. Overall, there are 440 chunks (363 unique)
which are identified correctly only by the lexical-
ized model, and 258 chunks (232 unique) only by
the pruned model.
Where the pruned model is always wrong
Some errors are unique to the pruned model.
Over 45 of the cases that are identified correctly
only in the lexicalized model (more than 10%) are
due to the words ?including? (18 cases) and ?If?
(9 cases), as well as other -ing forms such as ?fol-
lowing?, ?according?, ?rising? and ?suspecting.?
The word ?including? appears 80 times in the
training data, always tagged as VBG and func-
tioning as a PP chunk, which is an odd chunk
for VBGs. The lexicalized model easily picked
up on this behaviour, while the pruned model
couldn?t. Similarly, the word ?following/VBG?
appears 32 times, 20 of which as PP, and the
word ?according/VBG? 53 times, all of them as
PP. The pruned model could not distinguish those
from the rest of the VBGs and tagged them as
VPs. What seems to happen in these cases, is
that certain verbal forms participate in idiomatic
constructions and behave syntactically as preposi-
tions. The POS tagger does not pick this ambigu-
ity in function and contributes only the most likely
tag for the words (VBG). Lexical models learn that
certain VBGs are ?becoming? prepositions in the
observed dataset. These words do not appear as
specific features in the pruned models, and hence
these usage shifts are often misclassified. Interest-
ingly, the pruned model did learn that verbal forms
can sometimes be PPs: it made use of that infor-
mation by mis-identifying 11 verbal VBGs and 6
verbal VBNs as PPs.
The word ?If/IN?, unlike most prepositions, it
always starts an SBAR rather than a PP chunk in
the corpus. The pruned model learned this be-
haviour correctly for the lower-cased ?if/IN?, but
missed the upper-cased version appearing in 79
sentence initial locations in the corpus.
These cases are caused by a mismatch between
the POS tag and the syntactic function observed in
the chunked dataset.
Additional cases include the adverbs (Already,
Nearby, Soon, Maybe, Perhaps, once, Then): they
are sometimes not chunked as ADVP but are left
outside of any chunk. Some one-word ADJP
chunks being chunked as NPs (short, general, sure,
worse, . . . ) (6 cases) and some are chunked as
ADVPs (hard, British-born, . . . ) (4 cases).
There are 10 cases where the pruned model
splits an NP into ADVP and NP, such as:
[later] [this week], [roughly][18 more U.S. stores]. In
addition, the pruned model failed to learn the con-
struction ?typical of?, resulting in 2 NP chunks
such as: [The more intuitive approach typical].
Some mistakes of the pruned model seem
like mistakes/pecularities of the annotated corpus,
which the lexicalized model found a way to work
around. Consider the following gold-standard
cases from the annotated corpus:
- [ VP seems ] [ ADVP rarely ] [ VP to cut ]
- [ ADVP just ] [ PP after ]
- [ VP is ] [ NP anything ] [ O but ] [ VP fixing ]
- [ ADJP as high ] [ PP as ] [ NP 8.3 % ]
- [ ADJP less ] [ PP than ] [ ADJP rosy ]
- [ NP 40 % ] [ PP to ] [ NP 45 % ]
Which were each identified as a single chunk by
the pruned model. It can be argued these are mis-
takes in the tagged dataset.
Where the lexical model is sometimes better
Both models fail on conjunctions, but the lexical-
ized model do slightly better. Conjunction error
types come in two main varieties, either chunking
[x][and][y] instead of [x and y] (pruned: 21 cases,
lex: 14 cases) or chunking [x and y] instead of
[x][and][y] (pruned: 26 cases, lex: 24 cases).
Joining VP and NP into an NP, due
to a verb/adj ambiguity. For exam-
ple chunking [NP fired six executives] in-
stead of [VP fired] [NP six executives],
or [NP keeping viewers] instead of
[VP keeping] [NP viewers]. 12 such cases are
resolved correctly only by the lexicalized model,
and 5 only by the pruned one.
SBAR/PP confusion for words such as:
?as?,?after?,?with?,?since? (both ways). 13 cases
for the pruned model, 6 for lexicalized one.
Where both model are similar
Merging back-to-back NPs: Both models tend
to erroneously join back-to-back NPs to a sin-
gle NP, e.g. : [NP Westinghouse this year], or
[NP themselves fashion enterprises]. No model is bet-
ter than the other on these cases, each model failed
1148
on 16 cases the other model succeeded on.
Joining NP and VP into an NP due to
Verb/Noun ambiguity and tagger mistakes:
- [NP the weekend] [VP making] ? [NP the weekend making]
- [NP the competition] [VP can] ? [NP the competition can]
(lexicalized: 6 errors, pruned: 8 errors)
And splitting some NPs to VP+NP due to the
same reasons:
- [VP operating] [NP profit]
- [VP improved] [NP average yield]
(lexicalized: 5 errors, pruned: 7 errors)
The word ?that? is confused between SBAR and
NP (5 mistakes for each model)
Erroneously splitting range NPs, e.g. :
- [about $115][to][$125] (2 cases for each model).
Where the pruned model is better
There are some cases where the pruned models is
doing better than the lexicalized one:
VP wrongly split into VP and ADJP:
- [remains] [banned]
4 mistakes for lexicalized, 1 for pruned
VP wrongly split into VP and VP:
- [were scheduled] [to meet]
- [used] [to complain]
3 mistakes for lexicalized, 1 for pruned
VP wrongly split into ADVP and VP:
- [largly][reflecting]
- [selectively][leaking]
6 mistakes for lexicalized, 1 for pruned
PP and SBAR confusion:
- of, with, As, after
9 mistakes for lex, 5 for pruned
VP chunked as NP due to tagger mistake:
- [NP ruling], [NP drives], [NP cuts]
6 mistakes for lex, 2 for pruned
?that? tagged as NP instead of SBAR:
2 mistakes for lex, 0 for pruned
To conclude
Both the pruned and the fully lexicalized models
have problems dealing with non-local phenomena
such as coordination and relative clauses, as well
as verb/adjective ambiguities and VBG/Noun am-
biguities. They also perform poorly on embeded
syntactic constructions (such as an NP containing
an ADJP), and on identification of back-to-back
NPs, which often requires semantic knowledge.
Both models suffer from tagging mistakes of the
underlying tagger and systematic ambiguity be-
tween the morphological tag assigned by the tag-
ger and the syntactic tag in which the word oper-
ates (e.g., ?including? used as a preposition).
The main advantage of the fully lexcialized
model is in dealing with:
? Some coordinated constructions.
? Some cases of verb/adjective ambiguities.
? Specific function words not seen much in
training.
? Idiomatic usages of some VBG/VBN forms
functioning as prepositions.
The first two items are semantic in nature, and hint
that lexical features do capture some semantic in-
formation. While this might be true on the spe-
cific corpus, we believe that such corpus-derived
semantic knowledge is very restricted, is not gen-
eralizable, and will not transfer well to other cor-
pora, even on the same genre. We provide evi-
dence for this claim in Section 7.
The last two items are syntactic. We address
them by introducing a slightly modified feature
model.
6.1 Another chunking Experiment
Based on the observations from the error analy-
sis, we performed another pruned-chunking exper-
iment, with the following features:
? Word and POS for a -2,+2 window around
the current token, and 2-and-3-letter suffixes
of the token to be classified (same as Experi-
ment 2 in Section 5.2 above).
? Features of words appearing as a preposi-
tion (IN) anywhere in the training set are
not pruned (this result in a model with 310
unique lexical items after k = 100 pruning).
? An additional binary feature indicating for
each token whether it can function as a PP.
The list of possible-PP forms is generated by
considering all tokens seen inside a PP in the
training corpus. It can be easily extended if
additional lexicographic resources are avail-
able, without retraining the model.
This last proposed feature incorporates important
lexical knowledge without relying on features for
specific lexical forms, and is more generalizable.
The accuracy of this new model on the develop-
ment and test set with various pruning thresholds
is presented in Table 7.
The addition of the CanBePrep feature im-
proves the fully-lexicalized model accuracy on the
development set (93.24 to 93.68), and does not af-
fect fully lexicalized result on the test set (93.71
1149
CORPUS SOURCE CONTENT #TOKENS
WSJ 4 articles from wsj.com business Magazine, business 2,671
Jaguar Wikipedia page on Jaguar Well edited text, animals 5,396
FreeWill Wikipedia page on Free Will Well edited text, philosophy 9,428
LJ-Life 4 LiveJournal posts Noisy teenage writing, life 870
Table 8: Corpus Variation Text Sources
PRUNING #FEATURES SOFT-MARGIN ANCHORED
Dev Set
0 92,989 93.71 ?
100 4,066 ? 93.22
Test Set
0 92,989 93.68 ?
100 4,066 ? 93.26
Table 7: Chunking results (F), with various prun-
ing thresholds. Experiment 4. Features: POS,
Word , Suff2, Suff3 for main word, CanBePrep .
vs. 93.73). The pruned model performance im-
proves in both cases, more so on the development
set (93.12 to 93.22 on the test set, 92.84 to 93.26
on the development set). The new model helps
bridging the gap between the fully lexicalized and
the pruned model, yet we still observe a lead of
0.4F for the fully lexicalized model. We now turn
to explore how meaningful this difference is in
real-world situation in which one does not operate
on the Penn-WSJ corpus.
7 Corpus Variation and Model
Performance
When tested on the exact same resource as the
models are trained on, the fully lexicalized model
still has a slight edge over the pruned ones. How
well does this lexical knowledge transfer to dif-
ferent text genres? We compare the models? per-
formance on text from various genres, ranging
from very similar to the training material (re-
cent articles from the WSJ Business section) to a
well-edited but different domain text (?Featured-
content? wikipedia pages) to a non-edited noisy
text (live-journal blog posts from the ?life? cate-
gory). As we do not have gold-annotated data for
these text genres, we analyze the few differences
between the models, manually inspecting the in-
stances on which the models disagree.
Table 8 describes our test corpora for this ex-
periment. We applied the fully-lexicalized and
the pruned (k = 100) anchored models described
in Section 6.1 to these texts, and compared the
chunking results. The results are presented in Ta-
ble 9.
When moving outside of the canonic training
TEXT #DIFF PRUNED LEX BOTH
CORRECT CORRECT WRONG
WSJ 13 9 4 0
Jaguar 45 20 20 7
FreeWill 118 51 38 29
LJ-Life 15 8 6 1
Table 9: Comparison of Models? performance on
different text genres
corpus, the fully lexicalized model have no advan-
tage over the heavily pruned one. On the contrary,
the pruned models seem to have a small advantage
in most cases (though it is hard to tell if the differ-
ences are significant). This is true even for texts
in the very same domain, genre and editing guide-
lines as the training corpus was derived from.
8 Discussion
For all the sequence labeling tasks we analyzed,
the anchored-SVM proved to be robust to feature
pruning. The experiments support the claim that
rare lexical features do not provide substantial in-
formation to the model, but instead play a role in
maintaining separability. When this role is taken
over by anchoring, we can obtain the same level
of performance with very few robust lexical fea-
tures. Yet, we cannot conclude that lexical infor-
mation is not needed. There is a significant differ-
ence between the pruned and non-pruned models
for the chunking task. We showed that this dif-
ference can be bridged to some extent by a binary
feature relating to idiomatic word usage, and that
the difference vanishes when testing outside of the
annotated corpus. The high classification accura-
cies achieved with the heavily pruned anchored-
SVM models sheds new light on the actual role
of lexical features, and indicating that there is still
a lot to be learned regarding the effective incor-
poration of lexical and semantic information into
our models. It is our view that semantic knowl-
edge should not be expected to be learned by in-
spection of raw lexical counts from an annotated
text corpus, but instead collected from sources ex-
ternal to the annotated corpora ? either based on
a very large unannotated corpora, or on manually
constructed lexical resources.
1150
References
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4).
Richard P. Brent, 1973. Algorithms for Minimization
without Derivatives, chapter 4. Prentice-Hall.
Claire Cardie and David Pierce. 1998. Error-driven
pruning of treebank grammars for base noun phrase
identification. In ACL-1998.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc of NAACL.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proc of EACL.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proc. of
ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc of ACL.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc of EMNLP.
Yoav Goldberg and Michael Elhadad. 2007. SVM
Model Tampering and Anchored Learning: A Case
Study in Hebrew. NP Chunking. In ACL2007.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006. Noun Phrase Chunking in Hebrew: Influence
of Lexical and Morphological Features. In COL-
ING/ACL2006.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
Support Vector Classifiers For Named Entity Recog-
nition. In COLING2002.
Daisuke Kawahara and Kiyotaka Uchimoto. 2007.
Miniamlly lexicalized dependency parsing. In Proc
of ACL (Short papers).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL.
Yoshiaki Koshiba and Shigeo Abe. 2003. Comparison
of L1 and L2 support vector machines. In Proc. of
the International Joint Conference on Neural Net-
works, volume 3.
Taku Kudo and Yuji Matsumoto. 2000. Use of Sup-
port Vector Learning for Chunk Identification. In
CoNLL-2000.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL ?01.
Mitch P. Marcus and Lance A. Ramshaw. 1995.
Text Chunking Using Transformation-Based Learn-
ing. In 3rd ACL Workshop on Very Large Corpora.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proc of ACL.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A Data-Driven Parser-Generator for
Dependency Parsing. In LREC2006.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc of ACL.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc of CONLL.
Erik F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: chunking.
In CoNLL-2000.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc of NAACL.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In CoNLL-2002.
Vladimir Vapnik. 1995. The nature of statistical learn-
ing theory. Springer-Verlag New York, Inc.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Proc of
ACL.
1151
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327?335,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Enhancing Unlexicalized Parsing Performance
using a Wide Coverage Lexicon, Fuzzy Tag-set Mapping,
and EM-HMM-based Lexical Probabilities
Yoav Goldberg1? Reut Tsarfaty2? Meni Adler1? Michael Elhadad1
1Department of Computer Science, Ben Gurion University of the Negev
{yoavg|adlerm|elhadad}@cs.bgu.ac.il
2Institute for Logic, Language and Computation, University of Amsterdam
R.Tsarfaty@uva.nl
Abstract
We present a framework for interfacing
a PCFG parser with lexical information
from an external resource following a dif-
ferent tagging scheme than the treebank.
This is achieved by defining a stochas-
tic mapping layer between the two re-
sources. Lexical probabilities for rare
events are estimated in a semi-supervised
manner from a lexicon and large unanno-
tated corpora. We show that this solu-
tion greatly enhances the performance of
an unlexicalized Hebrew PCFG parser, re-
sulting in state-of-the-art Hebrew parsing
results both when a segmentation oracle is
assumed, and in a real-word parsing sce-
nario of parsing unsegmented tokens.
1 Introduction
The intuition behind unlexicalized parsers is that
the lexicon is mostly separated from the syntax:
specific lexical items are mostly irrelevant for ac-
curate parsing, and can be mediated through the
use of POS tags and morphological hints. This
same intuition also resonates in highly lexicalized
formalism such as CCG: while the lexicon cate-
gories are very fine grained and syntactic in na-
ture, once the lexical category for a lexical item is
determined, the specific lexical form is not taken
into any further consideration.
Despite this apparent separation between the
lexical and the syntactic levels, both are usually es-
timated solely from a single treebank. Thus, while
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
?Funded by the Dutch Science Foundation (NWO), grant
number 017.001.271.
?Post-doctoral fellow, Deutsche Telekom labs at Ben Gu-
rion University
PCFGs can be accurate, they suffer from vocabu-
lary coverage problems: treebanks are small and
lexicons induced from them are limited.
The reason for this treebank-centric view in
PCFG learning is 3-fold: the English treebank is
fairly large and English morphology is fairly sim-
ple, so that in English, the treebank does provide
mostly adequate lexical coverage1; Lexicons enu-
merate analyses, but don?t provide probabilities
for them; and, most importantly, the treebank and
the external lexicon are likely to follow different
annotation schemas, reflecting different linguistic
perspectives.
On a different vein of research, current POS tag-
ging technology deals with much larger quantities
of training data than treebanks can provide, and
lexicon-based unsupervised approaches to POS
tagging are practically unlimited in the amount
of training data they can use. POS taggers rely
on richer knowledge than lexical estimates de-
rived from the treebank, have evolved sophisti-
cated strategies to handle OOV and can provide
distributions p(t|w, context) instead of ?best tag?
only.
Can these two worlds be combined? We pro-
pose that parsing performance can be greatly im-
proved by using a wide coverage lexicon to sug-
gest analyses for unknown tokens, and estimating
the respective lexical probabilities using a semi-
supervised technique, based on the training pro-
cedure of a lexicon-based HMM POS tagger. For
many resources, this approach can be taken only
on the proviso that the annotation schemes of the
two resources can be aligned.
We take Modern Hebrew parsing as our case
study. Hebrew is a Semitic language with rich
1This is not the case with other languages, and also not
true for English when adaptation scenarios are considered.
327
morphological structure. This rich structure yields
a large number of distinct word forms, resulting in
a high OOV rate (Adler et al, 2008a). This poses
a serious problem for estimating lexical probabili-
ties from small annotated corpora, such as the He-
brew treebank (Sima?an et al, 2001).
Hebrew has a wide coverage lexicon /
morphological-analyzer (henceforth, KC Ana-
lyzer) available2, but its tagset is different than the
one used by the Hebrew Treebank. These are not
mere technical differences, but derive from dif-
ferent perspectives on the data. The Hebrew TB
tagset is syntactic in nature, while the KC tagset
is lexicographic. This difference in perspective
yields different performance for parsers induced
from tagged data, and a simple mapping between
the two schemes is impossible to define (Sec. 2).
A naive approach for combining the use of the
two resources would be to manually re-tag the
Treebank with the KC tagset, but we show this ap-
proach harms our parser?s performance. Instead,
we propose a novel, layered approach (Sec. 2.1),
in which syntactic (TB) tags are viewed as contex-
tual refinements of the lexicon (KC) tags, and con-
versely, KC tags are viewed as lexical clustering
of the syntactic ones. This layered representation
allows us to easily integrate the syntactic and the
lexicon-based tagsets, without explicitly requiring
the Treebank to be re-tagged.
Hebrew parsing is further complicated by the
fact that common prepositions, conjunctions and
articles are prefixed to the following word and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes can be am-
biguous and must be determined in a specific con-
text only. Thus, the leaves of the syntactic parse
trees do not correspond to space-delimited tokens,
and the yield of the tree is not known in advance.
We show that enhancing the parser with external
lexical information is greatly beneficial, both in an
artificial scenario where the token segmentation is
assumed to be known (Sec. 4), and in a more re-
alistic one in which parsing and segmentation are
handled jointly by the parser (Goldberg and Tsar-
faty, 2008) (Sec. 5). External lexical informa-
tion enhances unlexicalized parsing performance
by as much as 6.67 F-points, an error reduction
of 20% over a Treebank-only parser. Our results
are not only the best published results for pars-
ing Hebrew, but also on par with state-of-the-art
2http://mila.cs.technion.ac.il/hebrew/resources/lexicons/
lexicalized Arabic parsing results assuming gold-
standard fine-grained Part-of-Speech (Maamouri
et al, 2008).3
2 A Tale of Two Resources
Modern Hebrew has 2 major linguistic resources:
the Hebrew Treebank (TB), and a wide coverage
Lexicon-based morphological analyzer developed
and maintained by the Knowledge Center for Pro-
cessing Hebrew (KC Analyzer).
The Hebrew Treebank consists of sentences
manually annotated with constituent-based syn-
tactic information. The most recent version (V2)
(Guthmann et al, 2009) has 6,219 sentences, and
covers 28,349 unique tokens and 17,731 unique
segments4.
The KC Analyzer assigns morphological analy-
ses (prefixes, suffixes, POS, gender, person, etc.)
to Hebrew tokens. It is based on a lexicon of
roughly 25,000 word lemmas and their inflection
patterns. From these, 562,439 unique word forms
are derived. These are then prefixed (subject to
constraints) by 73 prepositional prefixes.
It is interesting to note that even with these
numbers, the Lexicon?s coverage is far from com-
plete. Roughly 1,500 unique tokens from the He-
brew Treebank cannot be assigned any analysis
by the KC Lexicon, and Adler et al(2008a) report
that roughly 4.5% of the tokens in a 42M tokens
corpus of news text are unknown to the Lexicon.
For roughly 400 unique cases in the Treebank, the
Lexicon provides some analyses, but not a correct
one. This goes to emphasize the productive nature
of Hebrew morphology, and stress that robust lex-
ical probability estimates cannot be derived from
an annotated resource as small as the Treebank.
Lexical vs. Syntactic POS Tags The analyses
produced by the KC Analyzer are not compatible
with the Hebrew TB.
The KC tagset (Adler et al, 2008b; Netzer et
al., 2007; Adler, 2007) takes a lexical approach to
POS tagging (?a word can assume only POS tags
that would be assigned to it in a dictionary?), while
the TB takes a syntactic one (?if the word in this
particular positions functions as an Adverb, tag it
as an Adverb, even though it is listed in the dictio-
nary only as a Noun?). We present 2 cases that em-
phasize the difference: Adjectives: the Treebank
3Our method is orthogonal to lexicalization and can be
used in addition to it if one so wishes.
4In these counts, all numbers are conflated to one canoni-
cal form
328
treats any word in an adjectivial position as an Ad-
jective. This includes also demonstrative pronouns
?? ??? (this boy). However, from the KC point of
view, the fact that a pronoun can be used to modify
a noun does not mean it should appear in a dictio-
nary as an adjective. The MOD tag: similarly,
the TB has a special POS-tag for words that per-
form syntactic modification. These are mostly ad-
verbs, but almost any Adjective can, in some cir-
cumstances, belong to that class as well. This cat-
egory is highly syntactic, and does not conform to
the lexicon based approach.
In addition, many adverbs and prepositions in
Hebrew are lexicalized instances of a preposition
followed by a noun (e.g., ?????, ?in+softness?,
softly). These can admit both the lexical-
ized and the compositional analyses. Indeed,
many words admit the lexicalized analyses in
one of the resource but not in the other (e.g.,
????? ?for+benefit? is Prep in the TB but only
Prep+Noun in the KC, while for ??? ?from+side?
it is the other way around).
2.1 A Unified Resource
While the syntactic POS tags annotation of the TB
is very useful for assigning the correct tree struc-
ture when the correct POS tag is known, there are
clear benefits to an annotation scheme that can be
easily backed by a dictionary.
We created a unified resource, in which every
word occurrence in the Hebrew treebank is as-
signed a KC-based analysis. This was done in a
semi-automatic manner ? for most cases the map-
ping could be defined deterministically. The rest
(less than a thousand instances) were manually as-
signed. Some Treebank tokens had no analyses
in the KC lexicon, and some others did not have
a correct analysis. These were marked as ?UN-
KNOWN? and ?MISSING? respectively.5
The result is a Treebank which is morpho-
logically annotated according to two different
schemas. On average, each of the 257 TB tags
is mapped to 2.46 of the 273 KC tags.6 While this
resource can serve as a basis for many linguisti-
cally motivated inquiries, the rest of this paper is
5Another solution would be to add these missing cases to
the KC Lexicon. In our view this act is harmful: we don?t
want our Lexicon to artificially overfit our annotated corpora.
6A ?tag? in this context means the complete morphologi-
cal information available for a morpheme in the Treebank: its
part of speech, inflectional features and possessive suffixes,
but not prefixes or nominative and accusative suffixes, which
are taken to be separate morphemes.
devoted to using it for constructing a better parser.
Tagsets Comparison In (Adler et al, 2008b),
we hypothesized that due to its syntax-based na-
ture, the Treebank morphological tagset is more
suitable than the KC one for syntax related tasks.
Is this really the case? To verify it, we simulate a
scenario in which the complete gold morpholog-
ical information is available. We train 2 PCFG
grammars, one on each tagged version of the Tree-
bank, and test them on the subset of the develop-
ment set in which every token is completely cov-
ered by the KC Analyzer (351 sentences).7 The
input to the parser is the yields and disambiguated
pre-terminals of the trees to be parsed. The parsing
results are presented in Table 1. Note that this sce-
nario does not reflect actual parsing performance,
as the gold information is never available in prac-
tice, and surface forms are highly ambiguous.
Tagging Scheme Precision Recall
TB / syntactic 82.94 83.59
KC / dictionary 81.39 81.20
Table 1: evalb results for parsing with Oracle
morphological information, for the two tagsets
With gold morphological information, the TB
tagging scheme is more informative for the parser.
The syntax-oriented annotation scheme of the
TB is more informative for parsing than the lexi-
cographic KC scheme. Hence, we would like our
parser to use this TB tagset whenever possible, and
the KC tagset only for rare or unseen words.
A Layered Representation It seems that learn-
ing a treebank PCFG assuming such a different
tagset would require a treebank tagged with the
alternative annotation scheme. Rather than assum-
ing the existence of such an alternative resource,
we present here a novel approach in which we
view the different tagsets as corresponding to dif-
ferent aspects of the morphosyntactic representa-
tion of pre-terminals in the parse trees. Each of
these layers captures subtleties and regularities in
the data, none of which we would want to (and
sometimes, cannot) reduce to the other. We, there-
fore, propose to retain both tagsets and learn a
fuzzy mapping between them.
In practice, we propose an integrated represen-
tation of the tree in which the bottommost layer
represents the yield of the tree, the surface forms
7For details of the train/dev splits as well as the grammar,
see Section 4.2.
329
are tagged with dictionary-based KC POS tags,
and syntactic TB POS tags are in turn mapped onto
the KC ones (see Figure 1).
TB: KC: Layered:
...
JJ-ZYTB
??
...
PRP-M-S-3-DEMKC
??
...
JJ-ZYTB
PRP-M-S-3-DEMKC
??
...
INTB
??????
...
INKC
?
...
NN-F-SKC
?????
...
INTB
INKC
?
NN-F-SKC
?????
Figure 1: Syntactic (TB), Lexical (KC) and
Layered representations
This representation helps to retain the informa-
tion both for the syntactic and the morphologi-
cal POS tagsets, and can be seen as capturing the
interaction between the morphological and syn-
tactic aspects, allowing for a seamless integra-
tion of the two levels of representation. We re-
fer to this intermediate layer of representation as
a morphosyntactic-transfer layer and we formally
depict it as p(tKC |tTB).
This layered representation naturally gives rise
to a generative model in which a phrase level con-
stituent first generates a syntactic POS tag (tTB),
and this in turn generates the lexical POS tag(s)
(tKC). The KC tag then ultimately generates the
terminal symbols (w). We assume that a morpho-
logical analyzer assigns all possible analyses to a
given terminal symbol. Our terminal symbols are,
therefore, pairs: ?w, t?, and our lexical rules are of
the form t? ?w, t?. This gives rise to the follow-
ing equivalence:
p(?w, tKC?|tTB) = p(tKC |tTB)p(?w, tKC?|tKC)
In Sections (4, 5) we use this layered gener-
ative process to enable a smooth integration of
a PCFG treebank-learned grammar, an external
wide-coverage lexicon, and lexical probabilities
learned in a semi-supervised manner.
3 Semi-supervised Lexical Probability
Estimations
A PCFG parser requires lexical probabilities
of the form p(w|t) (Charniak et al, 1996).
Such information is not readily available in
the lexicon. However, it can be estimated
from the lexicon and large unannotated cor-
pora, by using the well-known Baum-Welch
(EM) algorithm to learn a trigram HMM tagging
model of the form p(t1, . . . , tn, w1, . . . , wn) =
argmax
?
p(ti|ti?1, ti?2)p(wi|ti), and taking
the emission probabilities p(w|t) of that model.
In Hebrew, things are more complicated, as
each emission w is not a space delimited token, but
rather a smaller unit (a morphological segment,
henceforth a segment). Adler and Elhadad (2006)
present a lattice-based modification of the Baum-
Welch algorithm to handle this segmentation am-
biguity.
Traditionally, such unsupervised EM-trained
HMM taggers are thought to be inaccurate, but
(Goldberg et al, 2008) showed that by feeding the
EM process with sufficiently good initial proba-
bilities, accurate taggers (> 91% accuracy) can be
learned for both English and Hebrew, based on a
(possibly incomplete) lexicon and large amount of
raw text. They also present a method for automat-
ically obtaining these initial probabilities.
As stated in Section 2, the KC Analyzer (He-
brew Lexicon) coverage is incomplete. Adler
et al(2008a) use the lexicon to learn a Maximum
Entropy model for predicting possible analyses for
unknown tokens based on their orthography, thus
extending the lexicon to cover (even if noisily) any
unknown token. In what follows, we use KC Ana-
lyzer to refer to this extended version.
Finally, these 3 works are combined to create
a state-of-the-art POS-tagger and morphological
disambiguator for Hebrew (Adler, 2007): initial
lexical probabilities are computed based on the
MaxEnt-extended KC Lexicon, and are then fed
to the modified Baum-Welch algorithm, which is
used to fit a morpheme-based tagging model over
a very large corpora. Note that the emission prob-
abilities P (W |T ) of that model cover all the mor-
phemes seen in the unannotated training corpus,
even those not covered by the KC Analyzer.8
We hypothesize that such emission probabili-
ties are good estimators for the morpheme-based
P (T ? W ) lexical probabilities needed by a
PCFG parser. To test this hypothesis, we use it
to estimate p(tKC ? w) in some of our models.
4 Parsing with a Segmentation Oracle
We now turn to describing our first set of exper-
iments, in which we assume the correct segmen-
8P (W |T ) is defined also for words not seen during train-
ing, based on the initial probabilities calculation procedure.
For details, see (Adler, 2007).
330
tation for each input sentence is known. This is
a strong assumption, as the segmentation stage
is ambiguous, and segmentation information pro-
vides very useful morphological hints that greatly
constrain the search space of the parser. However,
the setting is simpler to understand than the one
in which the parser performs both segmentation
and POS tagging, and the results show some in-
teresting trends. Moreover, some recent studies on
parsing Hebrew, as well as all studies on parsing
Arabic, make this oracle assumption. As such, the
results serve as an interesting comparison. Note
that in real-world parsing situations, the parser is
faced with a stream of ambiguous unsegmented to-
kens, making results in this setting not indicative
of real-world parsing performance.
4.1 The Models
The main question we address is the incorporation
of an external lexical resource into the parsing pro-
cess. This is challenging as different resources fol-
low different tagging schemes. One way around
it is re-tagging the treebank according to the new
tagging scheme. This will serve as a baseline
in our experiment. The alternative method uses
the Layered Representation described above (Sec.
2.1). We compare the performance of the two ap-
proaches, and also compare them against the per-
formance of the original treebank without external
information.
We follow the intuition that external lexical re-
sources are needed only when the information
contained in the treebank is too sparse. There-
fore, we use treebank-derived estimates for reli-
able events, and resort to the external resources
only in the cases of rare or OOV words, for which
the treebank distribution is not reliable.
Grammar and Notation For all our experi-
ments, we use the same grammar, and change
only the way lexical probabilities are imple-
mented. The grammar is an unlexicalized
treebank-estimated PCFG with linguistically mo-
tivated state-splits.9
In what follows, a lexical event is a word seg-
ment which is assigned a single POS thereby func-
tioning as a leaf in a syntactic parse tree. A rare
9Details of the grammar: all functional information is re-
moved from the non-terminals, finite and non-finite verbs, as
well as possessive and other PPs are distinguished, definite-
ness structure of constituents is marked, and parent annota-
tion is employed. It is the same grammar as described in
(Goldberg and Tsarfaty, 2008).
(lexical) event is an event occurring less than K
times in the training data, and a reliable (lexical)
event is one occurring at least K times in the train-
ing data. We use OOV to denote lexical events ap-
pearing 0 times in the training data. count(?) is
a counting function over the training data, rare
stands for any rare event, and wrare is a specific
rare event. KCA(?) is the KC Analyzer function,
mapping a lexical event to a set of possible tags
(analyses) according to the lexicon.
Lexical Models
All our models use relative frequency estimated
probabilities for reliable lexical events: p(t ?
w|t) = count(w,t)count(t) . They differ only in their treat-
ment of rare (including OOV) events.
In our Baseline, no external resource is used.
We smooth for rare and OOV events using a per-
tag probability distribution over rare segments,
which we estimate using relative frequency over
rare segments in the training data: p(wrare|t) =
count(rare,t)
count(t) . This is the way lexical probabilities
in treebank grammars are usually estimated.
We experiment with two flavours of lexical
models. In the first, LexFilter, the KC Analyzer is
consulted for rare events. We estimate rare events
using the same per-tag distribution as in the base-
line, but use the KC Analyzer to filter out any in-
compatible cases, that is, we force to 0 the proba-
bility of any analysis not supported by the lexicon:
p(wrare|t) =
{
count(rare,t)
count(t) t ? KCA(wrare)
0 t /? KCA(wrare)
Our second flavour of lexical models, Lex-
Probs, the KC Analyzer is consulted to propose
analyses for rare events, and the probability of an
analysis is estimated via the HMM emission func-
tion described in Section 3, which we denote B:
p(wrare|t) = B(wrare, t)
In both LexFilter and LexProbs, we resort to
the relative frequency estimation in case the event
is not covered in the KC Analyzer.
Tagset Representations
In this work, we are comparing 3 different rep-
resentations: TB, which is the original Treebank,
KC which is the Treebank converted to use the KC
Analyzer tagset, and Layered, which is the layered
representation described above.
The details of the lexical models vary according
to the representation we choose to work with.
For the TB setting, our lexical rules are of the form
331
ttb ? w. Only the Baseline models are relevant
here, as the tagset is not compatible with that of
the external lexicon.
For the KC setting, our lexical rules are of the form
tkc ? w, and their probabilities are estimated as
described above. Note that this setting requires our
trees to be tagged with the new (KC) tagset, and
parsed sentences are also tagged with this tagset.
For the Layered setting, we use lexical rules of
the form ttb ? w. Reliable events are esti-
mated as usual, via relative frequency over the
original treebank. For rare events, we estimate
p(ttb ? w|ttb) = p(ttb ? tkc|ttb)p(tkc ? w|tkc),
where the transfer probabilities p(ttb ? tkc) are
estimated via relative frequencies over the layered
trees, and the emission probabilities are estimated
either based on other rare events (LexFilter) or
based on the semi-supervised method described in
Section 3 (LexProbs).
The layered setting has several advantages:
First, the resulting trees are all tagged with the
original TB tagset. Second, the training proce-
dure does not require a treebank tagged with the
KC tagset: Instead of learning the transfer layer
from the treebank we could alternatively base our
counts on a different parallel resource, estimate it
from unannotated data using EM, define it heuris-
tically, or use any other estimation procedure.
4.2 Experiments
We perform all our experiments on Version 2 of
the Hebrew Treebank, and follow the train/test/dev
split introduced in (Tsarfaty and Sima?an, 2007):
section 1 is used for development, sections 2-12
for training, and section 13 is the test set, which
we do not use in this work. All the reported re-
sults are on the development set.10 After removal
of empty sentences, we have 5241 sentences for
training, and 483 for testing. Due to some changes
in the Treebank11, our results are not directly com-
parable to earlier works. However, our baseline
models are very similar to the models presented
in, e.g. (Goldberg and Tsarfaty, 2008).
In order to compare the performance of the
model on the various tagset representations (TB
tags, KC tags, Layered), we remove from the test
set 51 sentences in which at least one token is
marked as not having any correct segmentation in
the KC Analyzer. This introduces a slight bias in
10This work is part of an ongoing work on a parser, and the
test set is reserved for final evaluation of the entire system.
11Normalization of numbers and percents, correcting of
some incorrect trees, etc.
favor of the KC-tags setting, and makes the test
somewhat easier for all the models. However, it
allows for a relatively fair comparison between the
various models.12
Results and Discussion
Results are presented in Table 2.13
Baseline
rare: < 2 rare: < 10
Prec Rec Prec Rec
TB 72.80 71.70 67.66 64.92
KC 72.23 70.30 67.22 64.31
LexFilter
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.18 76.31 77.34 76.20
Layered 76.69 76.40 76.66 75.74
LexProbs
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.29 76.65 77.22 76.36
Layered 76.81 76.49 76.85 76.08
Table 2: evalb results for parsing with a
segmentation Oracle.
As expected, all the results are much lower than
those with gold fine-grained POS (Table 1).
When not using any external knowledge (Base-
line), the TB tagset performs slightly better than
the converted treebank (KC). Note, however, that
the difference is less pronounced than in the gold
morphology case. When varying the rare words
threshold from 2 to 10, performance drops consid-
erably. Without external knowledge, the parser is
facing difficulties coping with unseen events.
The incorporation of an external lexical knowl-
edge in the form of pruning illegal tag assignments
for unseen words based on the KC lexicon (Lex-
Filter) substantially improves the results (? 72 to
? 77). The additional lexical knowledge clearly
improves the parser. Moreover, varying the rare
words threshold in this setting hardly affects the
parser performance: the external lexicon suffices
to guide the parser in the right direction. Keep-
ing the rare words threshold high is desirable, as it
reduces overfitting to the treebank vocabulary.
We expected the addition of the semi-
supervised p(t ? w) distribution (LexProbs) to
improve the parser, but found it to have an in-
significant effect. The correct segmentation seems
12We are forced to remove these sentences because of the
artificial setting in which the correct segmentation is given. In
the no-oracle setting (Sec. 5), we do include these sentences.
13The layered trees have an extra layer of bracketing
(tTB ? tKC ). We remove this layer prior to evaluation.
332
to remove enough ambiguity as to let the parser
base its decisions on the generic tag distribution
for rare events.
In all the settings with a Segmentation Oracle,
there is no significant difference between the KC
and the Layered representation. We prefer the lay-
ered representation as it provides more flexibility,
does not require trees tagged with the KC tagset,
and produces parse trees with the original TB POS
tags at the leaves.
5 Parsing without a Segmentation Oracle
When parsing real world data, correct token seg-
mentation is not known in advance. For method-
ological reasons, this issue has either been set-
aside (Tsarfaty and Sima?an, 2007), or dealt with
in a pipeline model in which a morphological dis-
ambiguator is run prior to parsing to determine the
correct segmentation. However, Tsarfaty (2006)
argues that there is a strong interaction between
syntax and morphological segmentation, and that
the two tasks should be modeled jointly, and not
in a pipeline model. Several studies followed this
line, (Cohen and Smith, 2007) the most recent of
which is Goldberg and Tsarfaty (2008), who pre-
sented a model based on unweighted lattice pars-
ing for performing the joint task.
This model uses a morphological analyzer to
construct a lattice over all possible morphologi-
cal analyses of an input sentence. The arcs of
the lattice are ?w, t? pairs, and a lattice parser
is used to build a parse over the lattice. The
Viterbi parse over the lattice chooses a lattice path,
which induces a segmentation over the input sen-
tence. Thus, parsing and segmentation are per-
formed jointly.
Lexical rules in the model are defined over the
lattice arcs (t? ?w, t?|t), and smoothed probabil-
ities for them are estimated from the treebank via
relative frequency over terminal/preterminal pairs.
The lattice paths themselves are unweighted, re-
flecting the intuition that all morphological anal-
yses are a-priori equally likely, and that their per-
spective strengths should come from the segments
they contain and their interaction with the syntax.
Goldberg and Tsarfaty (2008) use a data-driven
morphological analyzer derived from the treebank.
Their better models incorporated some external
lexical knowledge by use of an Hebrew spell
checker to prune some illegal segmentations.
In what follows, we use the layered represen-
tation to adapt this joint model to use as its mor-
phological analyzer the wide coverage KC Ana-
lyzer in enhancement of a data-driven one. Then,
we further enhance the model with the semi-
supervised lexical probabilities described in Sec 3.
5.1 Model
The model of Goldberg and Tsarfaty (2008) uses a
morphological analyzer to constructs a lattice for
each input token. Then, the sentence lattice is built
by concatenating the individual token lattices. The
morphological analyzer used in that work is data
driven based on treebank observations, and em-
ploys some well crafted heuristics for OOV tokens
(for details, see the original paper). Here, we use
instead a morphological analyzer which uses the
KC Lexicon for rare and OOV tokens.
We begin by adapting the rare vs. reliable events
distinction from Section 4 to cover unsegmented
tokens. We define a reliable token to be a token
from the training corpus, which each of its possi-
ble segments according to the training corpus was
seen in the training corpus at least K times.14 All
other tokens are considered to be rare.
Our morphological analyzer works as follows:
For reliable tokens, it returns the set of analyses
seen for this token in the treebank (each analysis
is a sequence of pairs of the form ?w, tTB?).
For rare tokens, it returns the set of analyses re-
turned by the KC analyzer (here, analyses are se-
quences of pairs of the form ?w, tKC?).
The lattice arcs, then, can take two possible
forms, either ?w, tTB? or ?w, tKC?.
Lexical rules of the form tTB ? ?w, tTB? are reli-
able, and their probabilities estimated via relative
frequency over events seen in training.
Lexical rules of the form tTB ? ?w, tKC?
are estimated in accordance with the transfer
layer introduced above: p(tTB ? ?w, tKC?) =
p(tKC |tTB)p(?w, tKC?|tKC).
The remaining question is how to estimate
p(?w, tKC?|tKC). Here, we use either the LexFil-
ter (estimated over all rare events) or LexProbs
(estimated via the semisupervised emission prob-
abilities)models, as defined in Section 4.1 above.
5.2 Experiments
As our Baseline, we take the best model of (Gold-
berg and Tsarfaty, 2008), run against the current
14Note that this is more inclusive than requiring that the
token itself is seen in the training corpus at least K times, as
some segments may be shared by several tokens.
333
version of the Treebank.15 This model uses the
same grammar as described in Section 4.1 above,
and use some external information in the form of a
spell-checker wordlist. We compare this Baseline
with the LexFilter and LexProbs models over the
Layered representation.
We use the same test/train splits as described in
Section 4. Contrary to the Oracle segmentation
setting, here we evaluate against all sentences, in-
cluding those containing tokens for which the KC
Analyzer does not contain any correct analyses.
Due to token segmentation ambiguity, the re-
sulting parse yields may be different than the gold
ones, and evalb can not be used. Instead, we use
the evaluation measure of (Tsarfaty, 2006), also
used in (Goldberg and Tsarfaty, 2008), which is
an adaptation of parseval to use characters instead
of space-delimited tokens as its basic units.
Results and Discussion
Results are presented in Table 3.
rare: < 2 rare: < 10
Prec Rec Prec Rec
Baseline 67.71 66.35 ? ?
LexFilter 68.25 69.45 57.72 59.17
LexProbs 73.40 73.99 70.09 73.01
Table 3: Parsing results for the joint parsing+seg
task, with varying external knowledge
The results are expectedly lower than with the
segmentation Oracle, as the joint task is much
harder, but the external lexical information greatly
benefits the parser also in the joint setting. While
significant, the improvement from the Baseline to
LexFilter is quite small, which is due to the Base-
line?s own rather strong illegal analyses filtering
heuristic. However, unlike the oracle segmenta-
tion case, here the semisupervised lexical prob-
abilities (LexProbs) have a major effect on the
parser performance (? 69 to ? 73.5 F-score), an
overall improvement of ? 6.6 F-points over the
Baseline, which is the previous state-of-the art for
this joint task. This supports our intuition that rare
lexical events are better estimated using a large
unannotated corpus, and not using a generic tree-
bank distribution, or sparse treebank based counts,
and that lexical probabilities have a crucial role in
resolving segmentation ambiguities.
15While we use the same software as (Goldberg and Tsar-
faty, 2008), the results reported here are significantly lower.
This is due to differences in annotation scheme between V1
and V2 of the Hebrew TB
The parsers with the extended lexicon were un-
able to assign a parse to about 10 of the 483 test
sentences. We count them as having 0-Fscore
in the table results.16 The Baseline parser could
not assign a parse to more than twice that many
sentences, suggesting its lexical pruning heuris-
tic is quite harsh. In fact, the unparsed sen-
tences amount to most of the difference between
the Baseline and LexFilter parsers.
Here, changing the rare tokens threshold has
a significant effect on parsing accuracy, which
suggests that the segmentation for rare tokens is
highly consistent within the corpus. When an un-
known token is encountered, a clear bias should
be taken toward segmentations that were previ-
ously seen in the same corpus. Given that that ef-
fect is remedied to some extent by introducing the
semi-supervised lexical probabilities, we believe
that segmentation accuracy for unseen tokens can
be further improved, perhaps using resources such
as (Gabay et al, 2008), and techniques for incor-
porating some document, as opposed to sentence
level information, into the parsing process.
6 Conclusions
We present a framework for interfacing a parser
with an external lexicon following a differ-
ent annotation scheme. Unlike other studies
(Yang Huang et al, 2005; Szolovits, 2003) in
which such interfacing is achieved by a restricted
heuristic mapping, we propose a novel, stochastic
approach, based on a layered representation. We
show that using an external lexicon for dealing
with rare lexical events greatly benefits a PCFG
parser for Hebrew, and that results can be further
improved by the incorporation of lexical probabil-
ities estimated in a semi-supervised manner using
a wide-coverage lexicon and a large unannotated
corpus. In the future, we plan to integrate this
framework with a parsing model that is specifi-
cally crafted to cope with morphologically rich,
free-word order languages, as proposed in (Tsar-
faty and Sima?an, 2008).
Apart from Hebrew, our method is applicable
in any setting in which there exist a small tree-
bank and a wide-coverage lexical resource. For
example parsing Arabic using the Arabic Tree-
bank and the Buckwalter analyzer, or parsing En-
glish biomedical text using a biomedical treebank
and the UMLS Specialist Lexicon.
16When discarding these sentences from the test set, result
on the better LexProbs model leap to 74.95P/75.56R.
334
References
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological
disambiguation. In Proc. of COLING/ACL2006.
Meni Adler, Yoav Goldberg, David Gabay, and
Michael Elhadad. 2008a. Unsupervised lexicon-
based resolution of unknown words for full morpho-
logical analysis. In Proc. of ACL 2008.
Meni Adler, Yael Netzer, David Gabay, Yoav Goldberg,
and Michael Elhadad. 2008b. Tagging a hebrew
corpus: The case of participles. In Proc. of LREC
2008.
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael Littman, and John McCann. 1996. Taggers
for parsers. Artif. Intell., 85(1-2):45?57.
Shay B. Cohen and Noah A. Smith. 2007. Joint mor-
phological and syntactic disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, pages 208?217.
David Gabay, Ziv Ben Eliahu, and Michael Elhadad.
2008. Using wikipedia links to construct word seg-
mentation corpora. In Proc. of the WIKIAI-08 Work-
shop, AAAI-2008 Conference.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proc. of ACL 2008.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start). In Proc. of ACL 2008.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc. of TLT.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008. Enhanced annotation and parsing of the ara-
bic treebank. In INFOS 2008, Cairo, Egypt, March
27-29, 2008.
Yael Netzer, Meni Adler, David Gabay, and Michael
Elhadad. 2007. Can you tag the modal? you should!
In ACL07 Workshop on Computational Approaches
to Semitic Languages, Prague, Czech.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
P. Szolovits. 2003. Adding a medical lexicon to an
english parser. In Proc. AMIA 2003 Annual Sympo-
sium.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morpholog-
ically rich languages. In Proc. of IWPT 2007.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proc. of CoLING, pages
889?896, Manchester, UK, August. Coling 2008.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. In
Proceedings of ACL-SRW-06.
MS Yang Huang, MD Henry J. Lowe, PhD Dan Klein,
and MS Russell J. Cucina, MD. 2005. Improved
identification of noun phrases in clinical radiology
reports using a high-performance statistical natural
language parser augmented with the umls specialist
lexicon. J Am Med Inform Assoc, 12(3), May.
335
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 105?108,
New York, June 2006. c?2006 Association for Computational Linguistics
Using Semantic Authoring for Blissymbols Communication Boards
Yael Netzer
Dept. of Computer Science
Ben Gurion University
Beer Sheva, Israel
yaeln@cs.bgu.ac.il
Michael Elhadad
Dept. of Computer Science
Ben Gurion University
Beer Sheva, Israel
elhadad@cs.bgu.ac.il
Abstract
Natural language generation (NLG) refers
to the process of producing text in a spo-
ken language, starting from an internal
knowledge representation structure. Aug-
mentative and Alternative Communica-
tion (AAC) deals with the development
of devices and tools to enable basic con-
versation for language-impaired people.
We present an applied prototype of an
AAC-NLG system generating written out-
put in English and Hebrew from a se-
quence of Bliss symbols. The system does
not ?translate? the symbols sequence, but
instead, it dynamically changes the com-
munication board as the choice of sym-
bols proceeds according to the syntactic
and semantic content of selected symbols,
generating utterances in natural language
through a process of semantic authoring.
1 Introduction
People who suffer from severe language impair-
ments lack the ability to express themselves through
natural usage of language and cannot achieve var-
ious forms of communication. The field of Aug-
mentative and Alternative Communication (AAC) is
concerned with methods that can be added to the
natural communication. In the most common form,
iconic symbols are presented on a display (or a com-
munication board). Communication is conducted by
the sequential selection of symbols on the display
(with vocal output when available), which are then
interpreted by the partner in the interaction.
AAC devices are characterized by three aspects:
(i) Selection method i.e., the physical choice of sym-
bols on the communication board; (ii) input lan-
guage and (iii) output medium. In a computerized
system, as (McCoy and Hershberger, 1999) mention,
a processing method aspect is added to this list. This
method refers to the process which creates the out-
put once symbols are inserted.
We specifically study the set of symbols (as an in-
put language) called Blissymbolics (Bliss in short).
Bliss is a graphic meaning-referenced language, cre-
ated by Charles Bliss to be used as a written univer-
sal language (Bliss, 1965); since 1971, Blissymbols
are used for communication with severely language-
impaired children. Bliss is designed to be a written-
only language, with non-arbitrary symbols. Sym-
bols are constructed from a composition of atomic
icons. Because words are structured from seman-
tic components, the graphic representation by itself
provides information on words? connectivity 1.
In the last decade, several systems that integrate
NLG techniques for AAC systems have been devel-
oped ((McCoy, 1997), (Vaillant, 1997) for example).
These systems share a common architecture: a tele-
graphic input sequence (words or symbols) is first
parsed, and then a grammatical sentence that repre-
sents the message is generated.
This paper presents an NLG-AAC system that
generates messages through a controlled process of
authoring, where each step in the selection of sym-
bols is controlled by the input specification defined
1See http://www.bci.org for reference on the language
105
for the linguistic realizer.
2 Generating Messages via Translation
A major difficulty when parsing a telegraphic se-
quence of words or symbols, is that many of the
hints that are used to capture the structure of the
text and, accordingly, the meaning of the utterance,
are missing. Moreover, as an AAC device is usu-
ally used for real-time conversation, the interpreta-
tion of utterances relies heavily on pragmatics ? time
of mentioned events, reference to the immediate en-
vironment.
Previous works dealing with translating tele-
graphic text, such as (Grishman and Sterling, 1989),
(Lee et al, 1997) requires to identify dependency
relations among the tokens of the telegraphic input.
Rich lexical knowledge is needed to identify possi-
ble dependencies in a given utterance, i.e., to find
the predicate and to apply constraints, such as selec-
tional restrictions to recognize its arguments.
Similar methods were used for AAC applica-
tions, COMPANSION (McCoy, 1997) for example
? where the telegraphic text is expanded to full sen-
tences, using a word order parser, and a semantic
parser to build the case frame structure of the verb
in the utterance, filling the slots with the rest of the
content words given. The system uses the semantic
representation to re-generate fluent text, relying on
lexical resources and NLG techniques.
The main questions at stake in this approach are
how good can a semantic parser be, in order to re-
construct the full structure of the sentence from tele-
graphic input and are pragmatic gaps in the given
telegraphic utterances recoverable in general.
3 Generating Messages via Semantic
Authoring
Our approach differs from previous NLG-AAC sys-
tems in that, with the model of semantic authoring
(Biller et al, 2005), we intervene during the process
of composing the input sequence, and thus can pro-
vide early feedback (in the form of display composi-
tion and partial text feedback), while preventing the
need for parsing a telegraphic sequence.
Semantic parsing is avoided by constructing a se-
mantic structure explicitly while the user inputs the
sequence incrementally. It combines three aspects
into an integrated approach for the design of an AAC
system:
? Semantic authoring drives a natural language
realization system and provides rich semantic
input.
? A display is updated on the fly as the authoring
system requires the user to select options.
? Ready-made inputs, corresponding to prede-
fined pragmatic contexts are made available to
the user as semantic templates.
In this method, each step of input insertion is con-
trolled by a set of constraints and rules, which are
drawn from an ontology. The system offers, at each
step, only possible complements to a small set of
concepts. For example, if the previous symbol de-
notes a verb which requires an instrumental theme,
only symbols that can function as instruments are
presented on the current display. Other symbols are
accessible through navigation operations, which are
interpreted in the context of the current partial se-
mantic specification. The general context of each
utterance or conversation can be determined by the
user, therefore narrowing the number of symbols
displayed in the board.
The underlying process of message generation is
based on layered lexical knowledge bases (LKB)
and an ontology. The ontology serves as a basis
for the semantic authoring process; it includes a hi-
erarchy of concepts and relations, and the informa-
tion it encodes interacts with the conceptual graphs
processing performed as part of content determina-
tion and lexical choice. The ontology was acquired
with a semi-automatic tool, which relies on WordNet
(Miller, 1995) and VerbNet (Kipper et al, 2000).
We designed and implemented the Bliss lexicon
for both Hebrew and English. The lexicon can be
used either as a stand-alone lexicon or as part of an
application through an API. The design of the lexi-
con takes advantage of the unique properties of the
language. The Bliss lexicon provides the list of sym-
bols accessible to the user, along with their graphic
representation, semantic information, and the map-
ping of symbols to English and Hebrew words. The
lexicon can be searched by keyword (learn), or by
semantic/graphic component: searching all words in
the lexicon that contain both food and meat returns
the symbols hamburger, hot-dog, meatball etc. (see
106
Fig. 1). The lexicon currently includes 2,200 en-
tries.
Figure 1: A snapshot of the Bliss Lexicon Web Ap-
plication
The core of the processing machinery of the
AAC message generation system is based on SAUT
(Biller et al, 2005) ? an authoring system for logical
forms encoded as conceptual graphs (CG). The sys-
tem belongs to the family of WYSIWYM (What You
See Is What You Mean) (Power and Scott, 1998) text
generation systems: logical forms are entered inter-
actively and the corresponding linguistic realization
of the expressions is generated in several languages.
The system maintains a model of the discourse con-
text corresponding to the authored documents to en-
able reference planning in the generation process.
Generating language from pictorial inputs, and
specifically from Bliss symbols using semantic au-
thoring in the WYSIWYM approach is not only a
pictorial application of the textual version, but it also
addresses specific needs of augmentative communi-
cation.
As was mentioned above, generating text from a
telegraphic message for AAC usage must take the
context of the conversation into account. We address
this problem in two manners: (1) adding pre-defined
inputs into the system (yet alowing accurate text
generation that considers syntactic variations), and
(2) enabling the assignment of default values to each
conversation (such as participants, tense, mood). We
also take advantage of the unique properties of the
Bliss symbols; the set of symbols that are offered
in each display can be filtered using their seman-
tic/graphical connectivity; the reduction of the num-
ber of possible choices that are to be made by the
user in each step of the message generation affects
the cognitive load and can affect the rate of commu-
nication.
4 Evaluation
We evaluate our system as an AAC application for
message generation from communication boards.
From an NLG evaluation perspective, this corre-
sponds to an intrinsic evaluation, i.e. judging quality
criteria of the generated text and its adequacy rela-
tive to the input (Bangalore et al, 1998). Since the
prototype of our system is not yet adjusted to inter-
act with alternative pointing devices, we could not
test it on actual Bliss users, and could not perform a
full extrinsic (task-based) evaluation.
However, as argued in (Higginbotham, 1995),
evaluations of AAC systems with nondisabled sub-
jects, when appropriately used, is easier to per-
form, and in some cases provide superior results.
Higginbotham?s claims rely on the observation that
the methods of message production are not unique
to AAC users and analogous communication situa-
tions exist both for disabled and nondisabled users.
Nondisabled subjects can contribute to the under-
standing of the cognitive processes underlying the
acquisition of symbol and device performance com-
petencies. We believe that the evaluation of effi-
ciency for non-AAC users should be served as base-
line.
The approach we offer for message generation re-
quires users to plan their sentences abstractly. (Mc-
Coy and Hershberger, 1999) points that novel sys-
tems may be found to slow communication but to in-
crease literacy skills. We therefore tested both speed
of message generation and semantic coverage (the
capability to generate a given message correctly).
The usage of semantic authoring was evaluated on
nondisabled subjects through a user study of 10 sub-
jects. This provides a reliable approximation of the
learning curve and usability of the system in general
(Biller et al, 2005).
In order to evaluate the keystroke savings of the
system we have collected a set of 19 sentences writ-
ten in Bliss and their full English correspondents.
We compared the number of the words in the Eng-
lish sentences with the number of choices needed
to generate the sentence with our system. The total
number of choice steps is 133, while the total num-
107
ber of words in the sentences is 122. This simple ra-
tio shows no improvement of keystrokes saving us-
ing our system. Savings, therefore, must be calcu-
lated in terms of narrowing the choice possibilities
in each step of the process.
However, counting the number of words does not
include morphology which in Bliss symbols requires
additional choices. We have counted the words
in the sentences considering morphology markers
of inflections as additional words, all summing to
138, as was suggested in (McCoy and Hershberger,
1999).
Assuming a display with 50 symbols (and addi-
tional keys for functions) ? a vocabulary of requires
50 different screens. Assuming symbols are orga-
nized by frequencies (first screens present the most
frequently used words) or by semantic domain.
The overall number of selections is reduced using
our communication board since the selectional re-
strictions narrow the number of possible choices that
can be made at each step. The extent to which selec-
tion time can be reduced at each step depends on the
application domain and the ontology structure. We
cannot evaluate it in general, but expect that a well-
structured ontology could support efficient selection
mechanisms, by grouping semantically related sym-
bols in dedicated displays.
In addition, the semantic authoring approach can
generate fluent output in other languages (English
and Hebrew, beyond the Bliss sequence ? without re-
quiring noisy translation). We also hypothesize that
ontologically motivated grouping of symbols could
speed up each selection step ? but this claim must be
assessed empirically in a task-based extrinsic evalu-
ation, which remains to be done in the future.
We are now building the environment for AAC
users with cooperation with ISAAC-ISRAEL 2, in
order to make the system fully accessible and to be
tested by AAC-users. However, this work is still in
progress. Once this will be achieved, full evaluation
of the system will be plausible.
5 Conclusions and Future Work
This work offers a new approach for message gen-
eration in the context of AAC displays using seman-
2Israeli chapter of the International Society for Augmenta-
tive and Alternative Communication
tic authoring and preventing the need to parse and
re-generate. We have designed and implemented a
Bliss lexicon for both Hebrew and English, which
can either be used a stand-alone lexicon for refer-
ence usage or as a part of an application.
Future work includes an implementation of a sys-
tem with full access for alternative devices, expan-
sion of the underlying lexicon for Hebrew genera-
tion, and adding voice output.
References
Srinivas Bangalore, Anoop Sarkar, Christy Doran, and Beth-
Ann Hockey. 1998. Grammar and parser evaluation in the
XTAG project. In Proc. of Workshop on Evaluation of Pars-
ing Systems, Granada, Spain, May.
Ofer Biller, Michael Elhadad, and Yael Netzer. 2005. Interac-
tive authoring of logical forms for multilingual generation.
In Proc. of the 10th workshop of ENLG, Aberdeen, Scotland.
Charles K. Bliss. 1965. Semantography (Blissymbolics). Se-
mantography Press, Sidney.
Ralph Grishman and John Sterling. 1989. Analyzing tele-
graphic messages. In Proc. of DARPA Speech and Natural
Language Workshop, pages 204?208, Philadelphia, Febru-
ary.
D. Jeffery Higginbotham. 1995. Use of nondisabled subjects
in AAC research: Confessions of a research infidel. AAC
Augmentative and Alternative Communication, 11, March.
AAC Research forum.
K. Kipper, H. Trang Dang, and M. Palmer. 2000. Class-based
construction of a verb lexicon. In Proceeding of AAAI-2000.
Young-Suk Lee, Clifford Weinstein, Stephanie Seneff, and Di-
nesh Tummala. 1997. Ambiguity resolution for machine
translation of telegraphic messages. In Proc. of the 8th con-
ference on EACL, pages 120?127.
Kathleen F. McCoy and Dave Hershberger. 1999. The role
of evaluation in bringing NLP to AAC: A case to consider.
In Filip T. Loncke, John Clibbens, Helen H. Arvidson, and
Lyle L. Lloyd, editors, AAC: New Directions in Research and
Practice, pages 105?122. Whurr Publishers, London.
Kathleen F. McCoy. 1997. Simple NLP techiques for expand-
ing telegraphic sentences. In Proc. of workshop on NLP for
Communication Aids, Madrid, July. ACL/EACL.
George A. Miller. 1995. WORDNET: a lexical database for
English. Commun. ACM, 38(11):39?41.
Roger Power and Donia Scott. 1998. Multilingual authoring
using feedback texts. In Proc. of COLING-ACL 98, Mon-
treal, Canada.
Pascal Vaillant. 1997. A semantic-based communication sys-
tem for dysphasic subjects. In Proc. of the 6th conference
on AI in Medicine Europe (AIME?97), Grenoble, France,
March.
108
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 665?672,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Unsupervised Morpheme-Based HMM for Hebrew
Morphological Disambiguation
Meni Adler
Department of Computer Science
Ben Gurion University of the Negev
84105 Beer Sheva, Israel
adlerm@cs.bgu.ac.il
Michael Elhadad
Department of Computer Science
Ben Gurion University of the Negev
84105 Beer Sheva, Israel
elhadad@cs.bgu.ac.il
Abstract
Morphological disambiguation is the pro-
cess of assigning one set of morphologi-
cal features to each individual word in a
text. When the word is ambiguous (there
are several possible analyses for the word),
a disambiguation procedure based on the
word context must be applied. This paper
deals with morphological disambiguation
of the Hebrew language, which combines
morphemes into a word in both agglutina-
tive and fusional ways. We present an un-
supervised stochastic model ? the only re-
source we use is a morphological analyzer ?
which deals with the data sparseness prob-
lem caused by the affixational morphology
of the Hebrew language.
We present a text encoding method for
languages with affixational morphology in
which the knowledge of word formation
rules (which are quite restricted in He-
brew) helps in the disambiguation. We
adapt HMM algorithms for learning and
searching this text representation, in such
a way that segmentation and tagging can
be learned in parallel in one step. Results
on a large scale evaluation indicate that
this learning improves disambiguation for
complex tag sets. Our method is applicable
to other languages with affix morphology.
1 Introduction
Morphological disambiguation is the process of as-
signing one set of morphological features to each
individual word in a text, according to the word
context.
In this work, we investigate morphological dis-
ambiguation in Modern Hebrew. We explore unsu-
pervised learning method, which is more challeng-
ing than the supervised case. The main motivation
for this approach is that despite the development
?This work is supported by the Lynn and William
Frankel Center for Computer Sciences, and by the
Knowledge Center for Hebrew Processing, Israel Sci-
ence Ministry.
of annotated corpora in Hebrew1, there is still not
enough data available for supervised training. The
other reason, is that unsupervised methods can
handle the dynamic nature of Modern Hebrew, as
it evolves over time.
In the case of English, because morphology is
simpler, morphological disambiguation is generally
covered under the task of part-of-speech tagging.
The main morphological variations are embedded
in the tag name (for example, Ns and Np for
noun singular or plural). The tagging accuracy
of supervised stochastic taggers is around 96%-
97% (Manning and Schutze, 1999, 10.6.1). Meri-
aldo (1994) reports an accuracy of 86.6% for an un-
supervised word-based HMM, trained on a corpus
of 42,186 sentences (about 1M words), over a tag
set of 159 different tags. Elworthy (1994), in con-
trast, reports an accuracy of 75.49%, 80.87% and
79.12% for unsupervised word-based HMM trained
on parts of the LOB corpora, with a tagset of
134 tags. With good initial conditions, such as
good approximation of the tag distribution for each
word, Elworthy reports an improvement to 94.6%,
92.27% and 94.51% on the same data sets. Meri-
aldo, on the other hand, reports an improvement
to 92.6% and 94.4% for the case where 100 and
2000 sentences of the training corpus are manually
tagged.
Modern Hebrew is characterized by rich mor-
phology, with a high level of ambiguity. On aver-
age, in our corpus, the number of possible analyses
per word reached 2.4 (in contrast to 1.4 for En-
glish). In Hebrew, several morphemes combine into
a single word in both agglutinative and fusional
ways. This results in a potentially high number of
tags for each word.
In contrast to English tag sets whose sizes range
from 48 to 195, the number of tags for Hebrew,
based on all combinations of the morphological
attributes (part-of-speech, gender, number, per-
son, tense, status, and the affixes? properties2),
1The Knowledge Center for Hebrew processing is
developing such corpora: http://mila.cs.technion.ac.il/
2The list of morphological attributes is described in
(Yona and Wintner, 2005). An in-depth discussion of
the Hebrew word form is provided in (Allon, 1995, pp.
665
can grow theoretically to about 300,000 tags. In
practice, we found only 1,934 tags in a corpus of
news stories we gathered, which contains about 6M
words.
The large size of such a tag set (about 10 times
larger than the most comprehensive English tag
set) is problematic in term of data sparseness.
Each morphological combination appears rarely,
and more samples are required in order to learn
the probabilistic model.
In this paper, we hypothesize that the large set
of morphological features of Hebrew words, should
be modeled by a compact morpheme model, based
on the segmented words (into prefix, baseform, and
suffix). Our main result is that best performance
is obtained when learning segmentation and mor-
pheme tagging in one step, which is made possible
by an appropriate text representation.
2 Hebrew and Arabic Tagging -
Previous Work
Several works have dealt with Hebrew tagging in
the past decade. In Hebrew, morphological anal-
ysis requires complex processing according to the
rules of Hebrew word formation. The task of a
morphological analyzer is to produce all possible
analyses for a given word. Recent analyzers pro-
vide good performance and documentation of this
process (Yona and Wintner, 2005; Segal, 2000).
Morphological analyzers rely on a dictionary, and
their performance is, therefore, impacted by the oc-
currence of unknown words. The task of a morpho-
logical disambiguation system is to pick the most
likely analysis produced by an analyzer in the con-
text of a full sentence.
Levinger et al (1995) developed a context-free
method in order to acquire the morpho-lexical
probabilities, from an untagged corpus. Their
method handles the data sparseness problem by
using a set of similar words for each word, built
according to a set of rules. The rules produce vari-
ations of the morphological properties of the word
analyses. Their tests indicate an accuracy of about
88% for context-free analysis selection based on the
approximated analysis distribution. In tests we re-
produced on a larger data set (30K tagged words),
the accuracy is only 78.2%. In order to improve
the results, the authors recommend merging their
method together with other morphological disam-
biguation methods ? which is the approach we pur-
sue in this work.
Levinger?s morphological disambiguation sys-
tem (Levinger, 1992) combines the above approx-
imated probabilities with an expert system, based
on a manual set of 16 syntactic constraints . In
the first phase, the expert system is applied, dis-
24?86).
ambiguating 35% of the ambiguous words with an
accuracy of 99.6%. In order to increase the applica-
bility of the disambiguation, approximated proba-
bilities are used for words that were not disam-
biguated in the first stage. Finally, the expert sys-
tem is used again over the new probabilities that
were set in the previous stage. Levinger reports
an accuracy of about 94% for disambiguation of
85% of the words in the text (overall 80% disam-
biguation). The system was also applied to prune
out the least likely analyses in a corpus but with-
out, necessarily, selecting a single analysis for each
word. For this task, an accuracy of 94% was re-
ported while reducing 92% of the ambiguous anal-
yses.
Carmel and Maarek (1999) use the fact that
on average 45% of the Hebrew words are unam-
biguous, to rank analyses, based on the number
of disambiguated occurrences in the text, normal-
ized by the total number of occurrences for each
word. Their application ? indexing for an informa-
tion retrieval system ? does not require all of the
morphological attributes but only the lemma and
the PoS of each word. As a result, for this case,
75% of the words remain with one analysis with
95% accuracy, 20% with two analyses and 5% with
three analyses.
Segal (2000) built a transformation-based tag-
ger in the spirit of Brill (1995). In the first phase,
the analyses of each word are ranked according to
the frequencies of the possible lemmas and tags in
a training corpus of about 5,000 words. Selection
of the highest ranked analysis for each word gives
an accuracy of 83% of the test text ? which con-
sists of about 1,000 words. In the second stage,
a transformation learning algorithm is applied (in
contrast to Brill, the observed transformations are
not applied, but used for re-estimation of the word
couples probabilities). After this stage, the accu-
racy is about 93%. The last stage uses a bottom-
up parser over a hand-crafted grammar with 150
rules, in order to select the analysis which causes
the parsing to be more accurate. Segal reports an
accuracy of 95%. Testing his system over a larger
test corpus, gives poorer results: Lembersky (2001)
reports an accuracy of about 85%.
Bar-Haim et al (2005) developed a word seg-
menter and PoS tagger for Hebrew. In their archi-
tecture, words are first segmented into morphemes,
and then, as a second stage, these morphemes are
tagged with PoS. The method proceeds in two
sequential steps: segmentation into morphemes,
then tagging over morphemes. The segmentation
is based on an HMM and trained over a set of 30K
annotated words. The segmentation step reaches
an accuracy of 96.74%. PoS tagging, based on un-
supervised estimation which combines a small an-
notated corpus with an untagged corpus of 340K
666
Word Segmentation Tag Translation
bclm bclm PNN name of a human rights association (Betselem)
bclm bclm VB while taking a picture
bclm bcl-m cons-NNM-suf their onion
bclm b-cl-m P1-NNM-suf under their shadow
bclm b-clm P1-NNM in a photographer
bclm b-clm P1-cons-NNM in a photographer
bclm b-clm P1-h-NNM in the photographer
hn?im h-n?im P1-VBR that are moving
hn?im hn?im P1-h-JJM the lovely
hn?im hn?im VBP made pleasant
Table 1: Possible analyses for the words bclm hn?im
words by using smoothing technique, gives an ac-
curacy of 90.51%.
As noted earlier, there is as yet no large scale
Hebrew annotated corpus. We are in the process
of developing such a corpus, and we have devel-
oped tagging guidelines (Elhadad et al, 2005) to
define a comprehensive tag set, and assist human
taggers achieve high agreement. The results dis-
cussed above should be taken as rough approxima-
tions of the real performance of the systems, until
they can be re-evaluated on such a large scale cor-
pus with a standard tag set.
Arabic is a language with morphology quite sim-
ilar to Hebrew. Theoretically, there might be
330,000 possible morphological tags, but in prac-
tice, Habash and Rambow (2005) extracted 2,200
different tags from their corpus, with an average
number of 2 possible tags per word. As reported
by Habash and Rambow, the first work on Arabic
tagging which used a corpus for training and eval-
uation was the work of Diab et al (2004). Habash
and Rambow were the first to use a morphological
analyzer as part of their tagger. They developed a
supervised morphological disambiguator, based on
training corpora of two sets of 120K words, which
combines several classifiers of individual morpho-
logical features. The accuracy of their analyzer
is 94.8% ? 96.2% (depending on the test corpus).
An unsupervised HMM model for dialectal Ara-
bic (which is harder to be tagged than written
Arabic), with accurracy of 69.83%, was presented
by Duh and Kirchhoff (2005). Their supervised
model, trained on a manually annotated corpus,
reached an accuracy of 92.53%.
Arabic morphology seems to be similar to He-
brew morphology, in term of complexity and data
sparseness, but comparison of the performances
of the baseline tagger used by Habash and Ram-
bow ? which selects the most frequent tag for a
given word in the training corpus ? for Hebrew and
Arabic, shows some intriguing differences: 92.53%
for Arabic and 71.85% for Hebrew. Furthermore,
as mentioned above, even the use of a sophisti-
cated context-free tagger, based on (Levinger et
al., 1995), gives low accuracy of 78.2%. This might
imply that, despite the similarities, morphological
disambiguation in Hebrew might be harder than in
Arabic. It could also mean that the tag set used
for the Arabic corpora has not been adapted to the
specific nature of Arabic morphology (a comment
also made in (Habash and Rambow, 2005)).
We propose an unsupervised morpheme-based
HMM to address the data sparseness problem. In
contrast to Bar-Haim et al, our model combines
segmentation and morphological disambiguation,
in parallel. The only resource we use in this work is
a morphological analyzer. The analyzer itself can
be generated from a word list and a morphologi-
cal generation module, such as the HSpell wordlist
(Har?el and Kenigsberg, 2004).
3 Morpheme-Based Model for
Hebrew
3.1 Morpheme-Based HMM
The lexical items of word-based models are the
words of the language. The implication of this
decision is that both lexical and syntagmatic re-
lations of the model, are based on a word-oriented
tagset. With such a tagset, it must be possible to
tag any word of the language with at least one tag.
Let us consider, for instance, the Hebrew phrase
bclm hn?im3, which contains two words. The word
bclm has several possible morpheme segmentations
and analyses4 as described in Table 1. In word-
based HMM, we consider such a phrase to be gen-
erated by a Markov process, based on the word-
oriented tagset of N = 1934 tags/states and about
M = 175K word types. Line W of Table 2 de-
scribes the size of a first-order word-based HMM,
built over our corpus. In this model, we found 834
entries for the ? vector (which models the distri-
bution of tags in first position in sentences) out of
possibly N = 1934, about 250K entries for the A
matrix (which models the transition probabilities
from tag to tag) out of possibly N 2 ? 3.7M , and
about 300K entries for the B matrix (which models
3Transcription according to Ornan (2002).
4The tagset we use for the annotation follows the
guidelines we have developed (Elhadad et al, 2005).
667
States PI A A2 B B2
W 1934 834 250K 7M 300K 5M
M 202 145 20K 700K 130K 1.7M
Table 2: Model Sizes
the emission probabilities from tag to word) out of
possibly M ?N ? 350M . For the case of a second-
order HMM, the size of the A2 matrix (which mod-
els the transition probabilities from two tags to the
third one), grows to about 7M entries, where the
size of the B2 matrix (which models the emission
probabilities from two tags to a word) is about 5M.
Despite the sparseness of these matrices, the num-
ber of their entries is still high, since we model the
whole set of features of the complex word forms.
Let us assume, that the right segmentation for
the sentence is provided to us ? for example: b
clm hn?im ? as is the case for English text. In
such a way, the observation is composed of mor-
phemes, generated by a Markov process, based
on a morpheme-based tagset. The size of such a
tagset for Hebrew is about 200, where the size of
the ?,A,B,A2 and B2 matrices is reduced to 145,
16K, 140K, 700K, and 1.7M correspondingly, as
described in line M of Table 2 ? a reduction of
90% when compared with the size of a word-based
model.
The problem in this approach, is that ?someone?
along the way, agglutinates the morphemes of each
word leaving the observed morphemes uncertain.
For example, the word bclm can be segmented in
four different ways in Table 1, as indicated by the
placement of the ?-? in the Segmentation column,
while the word hn?im can be segmented in two dif-
ferent ways. In the next section, we adapt the pa-
rameter estimation and the searching algorithms
for such uncertain output observation.
3.2 Learning and Searching Algorithms
for Uncertain Output Observation
In contrast to standard HMM, the output observa-
tions of the above morpheme-based HMM are am-
biguous. We adapted Baum-Welch (Baum, 1972)
and Viterbi (Manning and Schutze, 1999, 9.3.2) al-
gorithms for such uncertain observation. We first
formalize the output representation and then de-
scribe the algorithms.
Output Representation The learning and
searching algorithms of HMM are based on the
output sequence of the underlying Markov pro-
cess. For the case of a morpheme-based model,
the output sequence is uncertain ? we don?t see the
emitted morphemes but the words they form. If,
for instance, the Markov process emitted the mor-
phemes b clm h n?im, we would see two words (bclm
hn?im) instead. In order to handle the output am-
biguity, we use static knowledge of how morphemes
are combined into a word, such as the four known
combinations of the word bclm, the two possible
combinations of the word hn?im, and their possi-
ble tags within the original words. Based on this
information, we encode the sentence into a struc-
ture that represents all the possible ?readings? of
the sentence, according to the possible morpheme
combinations of the words, and their possible tags.
The representation consists of a set of vectors,
each vector containing the possible morphemes and
their tags for each specific ?time? (sequential posi-
tion within the morpheme expansion of the words
of the sentence). A morpheme is represented by
a tuple (symbol, state, prev, next), where symbol
denotes a morpheme, state is one possible tag for
this morpheme, prev and next are sets of indexes,
denoting the indexes of the morphemes (of the pre-
vious and the next vectors) that precede and follow
the current morpheme in the overall lattice, repre-
senting the sentence. Fig. 2 describes the repre-
sentation of the sentence bclm hn?im. An emission
is denoted in this figure by its symbol, its state
index, directed edges from its previous emissions,
and directed edges to its next emissions.
In order to meet the condition of Baum-Eagon
inequality (Baum, 1972) that the polynomial
P (O|?) ? which represents the probability of an
observed sequence O given a model ? ? be homo-
geneous, we must add a sequence of special EOS
(end of sentence) symbols at the end of each path
up to the last vector, so that all the paths reach
the same length.
The above text representation can be used to
model multi-word expressions (MWEs). Consider
the Hebrew sentence: hw? ?wrk dyn gdwl, which can
be interpreted as composed of 3 units (he lawyer
great / he is a great lawyer) or as 4 units (he edits
law big / he is editing an important legal deci-
sion). In order to select the correct interpretation,
we must determine whether ?wrk dyn is an MWE.
This is another case of uncertain output observa-
tion, which can be represented by our text encod-
ing, as done in Fig. 1.
?wrk dyn 6 gdwl 19 EOS 17 EOS 17
dyn 6 gdwl 19?wrk 18
hw? 20
Figure 1: The sentence hw? ?wrk dyn gdwl
This representation seems to be expensive in
term of the number of emissions per sentence.
However, we observe in our data that most of the
words have only one or two possible segmentations,
and most of the segmentations consist of at most
one affix. In practice, we found the average number
of emissions per sentence in our corpus (where each
symbol is counted as the number of its predecessor
emissions) to be 455, where the average number
of words per sentence is about 18. That is, the
668
cost of operating over an ambiguous sentence rep-
resentation increases the size of the sentence (from
18 to 455), but on the other hand, it reduces the
probabilistic model by a factor of 10 (as discussed
above).
Morphological disambiguation over such a se-
quence of vectors of uncertain morphemes is similar
to words extraction in automatic speech recogni-
tion (ASR)(Jurafsky and Martin, 2000, chp. 5,7).
The states of the ASR model are phones, where
each observation is a vector of spectral features.
Given a sequence of observations for a sentence,
the encoding ? based on the lattice formed by the
phones distribution of the observations, and the
language model ? searches for the set of words,
made of phones, which maximizes the acoustic like-
lihood and the language model probabilities. In a
similar manner, the supervised training of a speech
recognizer combines a training corpus of speech
wave files, together with word-transcription, and
language model probabilities, in order to learn the
phones model.
There are two main differences between the typi-
cal ASR model and ours: (1) an ASR decoder deals
with one aspect - segmentation of the observations
into a set of words, where this segmentation can
be modeled at several levels: subphones, phones
and words. These levels can be trained individ-
ually (such as training a language model from a
written corpus, and training the phones model for
each word type, given transcripted wave file), and
then combined together (in a hierarchical model).
Morphological disambiguation over uncertain mor-
phemes, on the other hand, deals with both mor-
pheme segmentation and the tagging of each mor-
pheme with its morphological features. Model-
ing morpheme segmentation, within a given word,
without its morphology features would be insuf-
ficient. (2) The supervised resources of ASR are
not available for morphological disambiguation: we
don?t have a model of morphological features se-
quences (equivalent to the language model of ASR)
nor a tagged corpus (equivalent to the transcripted
wave files of ASR).
These two differences require a design which
combines the two dimensions of the problem, in or-
der to support unsupervised learning (and search-
ing) of morpheme sequences and their morpholog-
ical features, simultaneously.
Parameter Estimation We present a variation
of the Baum-Welch algorithm (Baum, 1972) which
operates over the lattice representation we have de-
fined above. The algorithm starts with a proba-
bilistic model ? (which can be chosen randomly
or obtained from good initial conditions), and at
each iteration, a new model ?? is derived in order to
better explain the given output observations. For a
given sentence, we define T as the number of words
in the sentence, and T? as the number of vectors of
the output representation O = {ot}, 1 ? t ? T? ,
where each item in the output is denoted by olt =
(sym, state, prev, next), 1 ? t ? T? , 1 ? l ? |ot|.
We define ?(t, l) as the probability to reach olt at
time t, and ?(t, l) as the probability to end the se-
quence from olt. Fig. 3 describes the expectation
and the maximization steps of the learning algo-
rithm for a first-order HMM. The algorithm works
in O(T? ) time complexity, where T? is the total num-
ber of symbols in the output sequence encoding,
where each symbol is counted as the size of its prev
set.
Searching for best state sequence The
searching algorithm gets an observation sequence
O and a probabilistic model ?, and looks for the
best state sequence that generates the observation.
We define ?(t, l) as the probability of the best state
sequence that leads to emission olt, and ?(t, l) as
the index of the emission at time t?1 that precedes
olt in the best state sequence that leads to it. Fig. 4
describes the adaptation of the Viterbi (Manning
and Schutze, 1999, 9.3.2) algorithm to our text rep-
resentation for first-order HMM, which works in
O(T? ) time.
4 Experimental Results
We ran a series of experiments on a Hebrew corpus
to compare various approaches to the full morpho-
logical disambiguation and PoS tagging tasks. The
training corpus is obtained from various newspa-
per sources and is characterized by the following
statistics: 6M word occurrences, 178,580 distinct
words, 64,541 distinct lemmas. Overall, the ambi-
guity level is 2.4 (average number of analyses per
word).
We tested the results on a test corpus, manually
annotated by 2 taggers according to the guidelines
we published and checked for agreement. The test
corpus contains about 30K words. We compared
two unsupervised models over this data set: Word
model [W], and Morpheme model [M]. We also
tested two different sets of initial conditions. Uni-
form distribution [Uniform]: For each word, each
analysis provided by the analyzer is estimated with
an equal likelihood. Context Free approximation
[CF]: We applied the CF algorithm of Levinger et
al.(1995) to estimate the likelihood of each analy-
sis.
Table 3 reports the results of full morphologi-
cal disambiguation. For each morpheme and word
models, three types of models were tested: [1]
First-order HMM, [2-] Partial second-order HMM -
only state transitions were modeled (excluding B2
matrix), [2] Second-order HMM (including the B2
matrix).
Analysis If we consider the tagger which selects
the most probable morphological analysis for each
669
clm 7
m 3
n?im 16
clm 10
cl 9
hn?im 14
hn?im 15
h 2
n?im 16
h 2
EOS 17
clm 8
hn?im 11
hn?im 12
m 4
hn?im 14
hn?im 15
h 2
hn?im 11
hn?im 12
EOS 17
hn?im 14
hn?im 15
hn?im 11
hn?im 12
EOS 17
n?im 16
EOS 17
bcl 6
b 1
bclm 5
b 0
Figure 2: Representation of the sentence bclm hn?im
Expectation
?(1, l) = piol1.statebol1.state,ol1.sym (1)
?(t, l) = bolt.state,olt.sym
?
l??olt.prev
?(t? 1, l?)aol?t?1.state,olt.state
?(T? , l) = 1 (2)
?(t, l) =
?
l??olt.next
aolt.state,ol?t+1.statebol?t+1.state,ol?t+1.sym?(t+ 1, l
?)
Maximization
?i =
?
l:ol1.state=i ?(1, l)?(1, l)
?
l ?(1, l)?(1, l)
(3)
a?i,j =
?T?
t=2
?
l:olt.state=j
?
l??olt.prev:ol
?
t?1.state=i
?(t? 1, l?)ai,jbj,olt.sym?(t, l)
?T??1
t=1
?
l:olt.state=i ?(t, l)?(t, l)
(4)
b?i,k =
?T?
t=1
?
l:olt.sym=k,olt.state=i ?(t, l)?(t, l)
?T?
t=1
?
l:olt.state=i ?(t, l)?(t, l)
(5)
Figure 3: The learning algorithm for first-order model
Initialization
?(1, l) = piol1.statebol1.state,ol1.sym (6)
Induction
?(t, l) = max
l??olt.prev
?(t? 1, l?)aol?t?1.state,olt.statebolt.state,olt.sym (7)
?(t, l) = argmax
l??olt.prev
?(t? 1, l?)aol?t?1.state,olt.statebolt.state,olt.sym (8)
Termination and path readout
X?T? = argmax1?l?|T? | ?(T? , l) (9)
X?t = ?(t+ 1, X?t+1)
P (X?) = max
1?l?|OT? |
?(T? , l) (10)
Figure 4: The searching algorithm for first-order model
670
Order Uniform CF
W 1 82.01 84.08
W 2- 80.44 85.75
W 2 79.88 85.78
M 1 81.08 84.54
M 2- 81.53 88.5
M 2 83.39 85.83
Table 3: Morphological Disambiguation
word in the text, according to Levinger et al (1995)
approximations, with accuracy of 78.2%, as the
baseline tagger, four steps of error reduction can
be identified. (1) Contextual information: The
simplest first-order word-based HMM with uniform
initial conditions, achieves error reduction of 17.5%
(78.2 ? 82.01). (2) Initial conditions: Error reduc-
tions in the range: 11.5% ? 37.8% (82.01 ? 84.08
for word model 1, and 81.53 ? 88.5 for morhpeme
model 2-) were achieved by initializing the various
models with context-free approximations. While
this observation confirms Elworthy (1994), the im-
pact of error reduction is much less than reported
there for English - about 70% (79 ? 94). The key
difference (beside the unclear characteristic of El-
worthy initial condition - since he made use of an
annotated corpus) is the much higher quality of the
uniform distribution for Hebrew. (3) Model order:
The partial second-order HMM [2-] produced the
best results for both word (85.75%) and morpheme
(88.5%) models over the initial condition. The full
second-order HMM [2] didn?t upgrade the accu-
racy of the partial second-order, but achieved the
best results for the uniform distribution morpheme
model. This is because the context-free approxima-
tion does not take into account the tag of the previ-
ous word, which is part of model 2. We believe that
initializing the morpheme model over a small set of
annotated corpus will set much stronger initial con-
dition for this model. (4) Model type: The main
result of this paper is the error reduction of the
morpheme model with respect to the word model:
about 19.3% (85.75 ? 88.5).
In addition, we apply the above models for the
simpler task of segmentation and PoS tagging, as
reported in Table 4. The task requires picking the
correct morphemes of each word with their correct
PoS (excluding all other morphological features).
The best result for this task is obtained with the
morpheme model 2: 92.32%. For this simpler task,
the improvement brought by the morpheme model
over the word model is less significant, but still
consists of a 5% error reduction.
Unknown words account for a significant
chunk of the errors. Table 5 shows the distribution
of errors contributed by unknown words (words
that cannot be analyzed by the morphological an-
alyzer). 7.5% of the words in the test corpus are
unknown: 4% are not recognized at all by the mor-
phological analyzer (marked as [None] in the ta-
Order Uniform CF
W 1 91.07 91.47
W 2- 90.45 91.93
W 2 90.21 91.84
M 1 89.23 91.42
M 2- 89.77 91.76
M 2 91.42 92.32
Table 4: Segmentation and PoS Tagging
ble), and for 3.5%, the set of analyses proposed by
the analyzer does not contain the correct analy-
sis [Missing]. We extended the lexicon to include
missing and none lexemes of the closed sets. In
addition, we modified the analyzer to extract all
possible segmentations of unknown words, with all
the possible tags for the segmented affixes, where
the remaining unknown baseforms are tagged as
UK. The model was trained over this set. In the
next phase, the corpus was automatically tagged,
according to the trained model, in order to form a
tag distribution for each unknown word, according
to its context and its form. Finally, the tag for
each unknown word were selected according to its
tag distribution. This strategy accounts for about
half of the 7.5% unknown words.
None Missing %
Proper name 26 36 62
Closed Set 8 5.6 13.6
Other 16.5 5.4 21.9
Junk 2.5 0 2.5
53 47 100
Table 5: Unknown Word Distribution
Table 6 shows the confusion matrix for known
words (5% and up). The key confusions can be at-
tributed to linguistic properties of Modern Hebrew:
most Hebrew proper names are also nouns (and
they are not marked by capitalization) ? which ex-
plains the PN/N confusion. The verb/noun and
verb/adjective confusions are explained by the na-
ture of the participle form in Hebrew (beinoni) ?
participles behave syntactically almost in an iden-
tical manner as nouns.
Correct Error %
proper name noun 17.9
noun verb 15.3
noun proper name 6.6
verb noun 6.3
adjective noun 5.4
adjective verb 5.0
Table 6: Confusion Matrix for Known Words
5 Conclusions and Future Work
In this work, we have introduced a new text encod-
ing method that captures rules of word formation
in a language with affixational morphology such as
Hebrew. This text encoding method allows us to
671
learn in parallel segmentation and tagging rules in
an unsupervised manner, despite the high ambigu-
ity level of the morphological data (average num-
ber of 2.4 analyses per word). Reported results on
a large scale corpus (6M words) with fully unsu-
pervised learning are 92.32% for PoS tagging and
88.5% for full morphological disambiguation.
In this work, we used the backoff smoothing
method, suggested by Thede and Harper (1999),
with an extension of additive smoothing (Chen,
1996, 2.2.1) for the lexical probabilities (B and B2
matrices). To complete this study, we are currently
investigating several smoothing techniques (Chen,
1996), in order to check whether the morpheme
model is critical for the data sparseness problem,
or whether it can be handled with smoothing over
a word model.
We are currently investigating two major meth-
ods to improve our results: first, we have started
gathering a larger corpus of manually tagged text
and plan to perform semi-supervised learning on
a corpus of 100K manually tagged words. Second,
we plan to improve the unknown word model, such
as integrating it with named entity recognition sys-
tem (Ben-Mordechai, 2005).
References
Emmanuel Allon. 1995. Unvocalized Hebrew Writ-
ing. Ben Gurion University Press. (in Hebrew).
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.
2005. Choosing an optimal architecture for seg-
mentation and pos-tagging of modern Hebrew.
In Proceedings of ACL-05 Workshop on Compu-
tational Approaches to Semitic Languages.
Leonard E. Baum. 1972. An inequality and asso-
ciated maximization technique in statistical es-
timation for probabilistic functions of a Markov
process. Inequalities, 3:1?8.
Na?ama Ben-Mordechai. 2005. Named entities
recognition in Hebrew. Master?s thesis, Ben Gu-
rion University of the Negev, Beer Sheva, Israel.
(in Hebrew).
Eric Brill. 1995. Transformation-based error-
driven learning and natural languge processing:
A case study in part-of-speech tagging. Compu-
tational Linguistics, 21:543?565.
David Carmel and Yoelle S. Maarek. 1999. Mor-
phological disambiguation for Hebrew search
systems. In Proceeding of NGITS-99.
Stanley F. Chen. 1996. Building Probabilistic
Models for Natural Language. Ph.D. thesis, Har-
vard University, Cambridge, MA.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic tagging of Arabic text: From
raw text to base phrase chunks. In Proceeding
of HLT-NAACL-04.
Kevin Duh and Katrin Kirchhoff. 2005. Pos tag-
ging of dialectal Arabic: A minimally supervised
approach. In Proceedings of ACL-05 Workshop
on Computational Approaches to Semitic Lan-
guages.
Michael Elhadad, Yael Netzer, David Gabay, and
Meni Adler. 2005. Hebrew morphological tag-
ging guidelines. Technical report, Ben Gurion
University, Dept. of Computer Science.
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceeding of
ANLP-94.
Nizar Habash and Owen Rambow. 2005. Arabic
tokenization, part-of-speech tagging and mor-
phological disambiguation in one fell swoop. In
Proceeding of ACL-05.
Nadav Har?el and Dan Kenigsberg. 2004. HSpell
- the free Hebrew spell checker and morphologi-
cal analyzer. Israeli Seminar on Computational
Linguistics, December 2004.
Daniel Jurafsky and James H. Martin. 2000.
Speech and language processing. Prentice-Hall.
Gennady Lembersky. 2001. Named entities recog-
nition; compounds: approaches and recognitions
methods. Master?s thesis, Ben Gurion Univer-
sity of the Negev, Beer Sheva, Israel. (in He-
brew).
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995.
Learning morpholexical probabilities from an
untagged corpus with an application to Hebrew.
Computational Linguistics, 21:383?404.
Moshe Levinger. 1992. Morhphological disam-
biguation in hebrew. Master?s thesis, Technion,
Haifa, Israel. (in Hebrew).
Christopher D. Manning and Hinrich Schutze.
1999. Foundation of Statistical Language Pro-
cessing. MIT Press.
Bernard Merialdo. 1994. Tagging English text
with probabilistic model. Computatinal Linguis-
tics, 20:155?171.
Uzi Ornan. 2002. Hebrew in latin script.
Le?s?one?nu, LXIV:137?151. (in Hebrew).
Erel Segal. 2000. Hebrew morphological ana-
lyzer for Hebrew undotted texts. Master?s the-
sis, Technion, Haifa, Israel. (in Hebrew).
Scott M. Thede and Mary P. Harper. 1999. A
second-order hidden Markov model for part-of-
speech tagging. In Proceeding of ACL-99.
Shlomo Yona and Shuly Wintner. 2005. A finite-
state morphological grammar of Hebrew. In
Proceedings of ACL-05 Workshop on Computa-
tional Approaches to Semitic Languages.
672
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 689?696,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Noun Phrase Chunking in Hebrew 
Influence of Lexical and Morphological Features 
 
 
Yoav Goldberg  and  Meni Adler  and  Michael Elhadad 
Computer Science Department 
Ben Gurion University of the Negev 
P.O.B 653 Be'er Sheva 84105, Israel  
{yoavg,adlerm,elhadad}@cs.bgu.ac.il 
 
  
 
Abstract 
We present a method for Noun Phrase 
chunking in Hebrew. We show that the 
traditional definition of base-NPs as non-
recursive noun phrases does not apply in 
Hebrew, and propose an alternative defi-
nition of Simple NPs.  We review syntac-
tic properties of Hebrew related to noun 
phrases, which indicate that the task of 
Hebrew SimpleNP chunking is harder 
than base-NP chunking in English. As a 
confirmation, we apply methods known 
to work well for English to Hebrew data. 
These methods give low results (F from 
76 to 86) in Hebrew. We then discuss our 
method, which applies SVM induction 
over lexical and morphological features. 
Morphological features improve the av-
erage precision by ~0.5%, recall by ~1%, 
and F-measure by ~0.75, resulting in a 
system with average performance of 93% 
precision, 93.4% recall and 93.2 F-
measure.* 
1 Introduction 
Modern Hebrew is an agglutinative Semitic lan-
guage, with rich morphology.  Like most other 
non-European languages, it lacks NLP resources 
and tools, and specifically there are currently no 
available syntactic parsers for Hebrew.  We ad-
dress the task of NP chunking in Hebrew as a 
                                                 
*
 This work was funded by the Israel Ministry of Sci-
ence and Technology under the auspices of the 
Knowledge Center for Processing Hebrew.  Addi-
tional funding was provided by the Lynn and William 
Frankel Center for Computer Sciences.  
first step to fulfill the need for such tools.  We 
also illustrate how this task can successfully be 
approached with little resource requirements, and 
indicate how the method is applicable to other 
resource-scarce languages. 
NP chunking is the task of labelling noun 
phrases in natural language text. The input to this 
task is free text with part-of-speech tags.  The 
output is the same text with brackets around base 
noun phrases.  A base noun phrase is an NP 
which does not contain another NP (it is not re-
cursive).  NP chunking is the basis for many 
other NLP tasks such as shallow parsing, argu-
ment structure identification, and information 
extraction 
We first realize that the definition of base-NPs 
must be adapted to the case of Hebrew (and 
probably other Semitic languages as well) to cor-
rectly handle its syntactic nature.  We propose 
such a definition, which we call simple NPs and 
assess the difficulty of chunking such NPs by 
applying methods that perform well in English to 
Hebrew data.  While the syntactic problem in 
Hebrew is indeed more difficult than in English, 
morphological clues do provide additional hints, 
which we exploit using an SVM learning 
method.  The resulting method reaches perform-
ance in Hebrew comparable to the best results 
published in English. 
2 Previous Work 
Text chunking (and NP chunking in particular), 
first proposed by Abney (1991), is a well studied 
problem for English. The CoNLL2000 shared 
task (Tjong Kim Sang et al, 2000) was general 
chunking. The best result achieved for the shared 
task data was by Zhang et al(2002), who 
achieved NP chunking results of 94.39% preci-
sion, 94.37% recall and 94.38 F-measure using a 
689
generalized Winnow algorithm, and enhancing 
the feature set with the output of a dependency 
parser. Kudo and Matsumoto (2000) used an 
SVM based algorithm, and achieved NP chunk-
ing results of 93.72% precision, 94.02% recall 
and 93.87 F-measure for the same shared task 
data, using only the words and their PoS tags. 
Similar results were obtained using Conditional 
Random Fields on similar features (Sha and 
Pereira, 2003). 
The NP chunks in the shared task data are 
base-NP chunks ? which are non-recursive NPs, 
a definition first proposed by Ramshaw and 
Marcus (1995). This definition yields good NP 
chunks for English, but results in very short and 
uninformative chunks for Hebrew (and probably 
other Semitic languages). 
Recently, Diab et al(2004) used SVM based 
approach for Arabic text chunking.  Their chunks 
data was derived from the LDC Arabic TreeBank 
using the same program that extracted the chunks 
for the shared task.  They used the same features 
as Kudo and Matsumoto (2000), and achieved 
over-all chunking performance of 92.06% preci-
sion, 92.09% recall and 92.08 F-measure (The 
results for NP chunks alone were not reported).  
Since Arabic syntax is quite similar to Hebrew, 
we expect that the issues reported below apply to 
Arabic results as well. 
3 Hebrew Simple NP Chunks 
The standard definition of English base-NPs is 
any noun phrase that does not contain another 
noun phrase, with possessives treated as a special 
case, viewing the possessive marker as the first 
word of a new base-NP (Ramshaw and Marcus, 
1995).  To evaluate the applicability of this defi-
nition to Hebrew, we tested this definition on the 
Hebrew TreeBank (Sima?an et al 2001) pub-
lished by the Hebrew Knowledge Center. We 
extracted all base-NPs from this TreeBank, 
which is similar in genre and contents to the 
English one.  This results in extremely simple 
chunks.  
 
English 
BaseNPs 
Hebrew 
BaseNPs 
Hebrew 
SimpleNPs 
Avg # of words 2.17 1.39 2.49 
% length 1 30.95 63.32 32.83 
% length 2 39.35 35.48 32.12 
% length 3 18.68 0.83 14.78 
% length 4 6.65 0.16 9.47 
% length 5 2.70 0.16 4.56 
% length > 5 1.67 0.05 6.22 
Table 1.  Size of Hebrew and English NPs 
Table 1 shows the average number of words in a 
base-NP for English and Hebrew.  The Hebrew 
chunks are basically one-word groups around 
Nouns, which is not useful for any practical pur-
pose, and so we propose a new definition for He-
brew NP chunks, which allows for some nested-
ness. We call our chunks Simple NP chunks.  
3.1 Syntax of NPs in Hebrew 
One of the reasons the traditional base-NP defi-
nition fails for the Hebrew TreeBank is related to 
syntactic features of Hebrew ? specifically, 
smixut (construct state ? used to express noun 
compounds), definite marker and the expression 
of possessives. These differences are reflected to 
some extent by the tagging guidelines used to 
annotate the Hebrew Treebank and they result in 
trees which are in general less flat than the Penn 
TreeBank ones.  
Consider the example base noun phrase [The 
homeless people]. The Hebrew equivalent is 
(1)  	
  
 which by the non-recursive NP definition will be 
bracketed as: 
   	
  , or, loosely translating 
back to English: [the home]less [people].  
In this case, the fact that the bound-morpheme 
less appears as a separate construct state word 
with its own definite marker (ha-) in Hebrew 
would lead the chunker to create two separate 
NPs for a simple expression.  We present below 
syntactic properties of Hebrew which are rele-
vant to NP chunking. We then present our defini-
tion of Simple NP Chunks.  
 
Construct State: The Hebrew genitive case is 
achieved by placing two nouns next to each other. 
This is called ?noun construct?, or smixut. The 
semantic interpretation of this construct is varied 
(Netzer and Elhadad, 1998), but it specifically 
covers possession. The second noun can be 
treated as an adjective modifying the next noun. 
The first noun is morphologically marked in a 
form known as the construct form (denoted by 
const). The definite article marker is placed on 
the second word of the construction: 
(2)  
 beit sefer / house-[const] book 
 School 
(3)  
 beit ha-sefer / house-[const] the-book 
 The school 
 
The construct form can also be embedded: 
(4) 	


 
690
misrad ro$ ha-mem$ala  
Office-[const poss] head-[const] the-government 
The prime-minister?s office 
 
Possessive: the smixut form can be used to indi-
cate possession. Other ways to express posses-
sion include the possessive marker  - ?$el? / 
?of? - (5), or adding a possessive suffix on the 
noun (6). The various forms can be mixed to-
gether, as in (7): 
(5) 	
 
ha-bait $el-i / the-house of-[poss 1st person] 
My house 
(6)  
beit-i / house-[poss 1st person] 
My house 
(7) 	

	

  
misrad-o $el ro$ ha-mem$ala 
Office-[poss 3rd] of head-[const] the-government 
The prime minister office 
 
Adjective: Hebrew adjectives come after the 
noun, and agree with it in number, gender and 
definite marker: 
(8)  
ha-tapu?ah ha-yarok / the-Apple the-green 
The green apple 
 
Some aspects of the predicate structure in He-
brew directly affect the task of NP chunking, as 
they make the decision to ?split? NPs more or 
less difficult than in English. 
 
Word order and the preposition 'et': Hebrew 
sentences can be either in SVO or VSO form. In 
order to keep the object separate from the sub-
ject, definite direct objects are marked with the 
special preposition 'et', which has no analog in 
English.  
 
Possible null equative: The equative form in 
Hebrew can be null. Sentence (9) is a non-null 
equative, (10) a null equative, while (11) and 
(12) are predicative NPs, which look very similar 
to the null-equative form: 
 
(9) 	 
ha-bait hu gadol 
The-house is big 
The house is big 
 
(10) 	 
ha-bait gadol 
The-house big 
The house is big 
 
(11) 	 
bait gadol 
House big 
A big house 
(12) 	 
ha-bait ha-gadol 
The-house the-big 
The big house 
 
Morphological Issues: In Hebrew morphology, 
several lexical units can be concatenated into a 
single textual unit.  Most prepositions, the defi-
nite article marker and some conjunctions are 
concatenated as prefixes, and possessive pro-
nouns and some adverbs are concatenated as suf-
fixes.  The Hebrew Treebank is annotated over a 
segmented version of the text, in which prefixes 
and suffixes appear as separate lexical units.  On 
the other hand, many bound morphemes in Eng-
lish appear as separate lexical units in Hebrew.  
For example, the English morphemes re-, ex-, 
un-, -less, -like, -able, appear in Hebrew as sepa-
rate lexical units ? , 	, 

 , , , 
, . 
  
In our experiment, we use as input to the 
chunker the text after it has been morphologi-
cally disambiguated and segmented. Our 
analyzer provides segmentation and PoS tags 
with 92.5% accuracy and full morphology with 
88.5% accuracy (Adler and Elhadad, 2006). 
3.2 Defining Simple NPs 
Our definition of Simple NPs is pragmatic. We 
want to tag phrases that are complete in their 
syntactic structure, avoid the requirement of tag-
ging recursive structures that include full clauses 
(relative clauses for example) and in general, tag 
phrases that have a simple denotation. To estab-
lish our definition, we start with the most com-
plex NPs, and break them into smaller parts by 
stating what should not appear inside a Simple 
NP. This can be summarized by the following 
table: 
 
Outside SimpleNP Exceptions 
Prepositional Phrases 
Relative Clauses 
Verb Phrases 
Apposition1 
Some conjunctions 
(Conjunctions are 
marked according to the 
TreeBank guidelines)2. 
% related PPs are 
allowed:  

5% of the sales 
 
Possessive  - '$el' / 
'of' - is not consid-
ered a PP 
Table 2.   Definition of Simple NP chunks 
Examples for some Simple NP chunks resulting 
from that definition: 
 
                                                 
1
 Apposition structure is not annotated in the TreeBank. As 
a heuristic, we consider every comma inside a non conjunct-
ive NP which is not followed by an adjective or an adjective 
phrase to be marking the beginning of an apposition. 
2
 As a special case, Adjectival Phrases and possessive con-
junctions are considered to be inside the Simple NP.  
691
   	   
	

Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 224?231,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
SVM Model Tampering and Anchored Learning: A Case Study in Hebrew
NP Chunking
Yoav Goldberg and Michael Elhadad
Computer Science Department
Ben Gurion University of the Negev
P.O.B 653 Be?er Sheva 84105, Israel
yoavg,elhadad@cs.bgu.ac.il
Abstract
We study the issue of porting a known NLP
method to a language with little existing NLP
resources, specifically Hebrew SVM-based
chunking. We introduce two SVM-based
methods ? Model Tampering and Anchored
Learning. These allow fine grained analysis
of the learned SVM models, which provides
guidance to identify errors in the training cor-
pus, distinguish the role and interaction of
lexical features and eventually construct a
model with ?10% error reduction. The re-
sulting chunker is shown to be robust in the
presence of noise in the training corpus, relies
on less lexical features than was previously
understood and achieves an F-measure perfor-
mance of 92.2 on automatically PoS-tagged
text. The SVM analysis methods also provide
general insight on SVM-based chunking.
1 Introduction
While high-quality NLP corpora and tools are avail-
able in English, such resources are difficult to obtain
in most other languages. Three challenges must be
met when adapting results established in English to
another language: (1) acquiring high quality anno-
tated data; (2) adapting the English task definition
to the nature of a different language, and (3) adapt-
ing the algorithm to the new language. This paper
presents a case study in the adaptation of a well
known task to a language with few NLP resources
available. Specifically, we deal with SVM based He-
brew NP chunking. In (Goldberg et al, 2006), we
established that the task is not trivially transferable
to Hebrew, but reported that SVM based chunking
(Kudo and Matsumoto, 2000) performs well. We
extend that work and study the problem from 3 an-
gles: (1) how to deal with a corpus that is smaller
and with a higher level of noise than is available in
English; we propose techniques that help identify
?suspicious? data points in the corpus, and identify
how robust the model is in the presence of noise;
(2) we compare the task definition in English and in
Hebrew through quantitative evaluation of the differ-
ences between the two languages by analyzing the
relative importance of features in the learned SVM
models; and (3) we analyze the structure of learned
SVM models to better understand the characteristics
of the chunking problem in Hebrew.
While most work on chunking with machine
learning techniques tend to treat the classification
engine as a black-box, we try to investigate the re-
sulting classification model in order to understand
its inner working, strengths and weaknesses. We in-
troduce two SVM-based methods ? Model Tamper-
ing and Anchored Learning ? and demonstrate how
a fine-grained analysis of SVM models provides in-
sights on all three accounts. The understanding of
the relative contribution of each feature in the model
helps us construct a better model, which achieves
?10% error reduction in Hebrew chunking, as well
as identify corpus errors. The methods also provide
general insight on SVM-based chunking.
2 Previous Work
NP chunking is the task of marking the bound-
aries of simple noun-phrases in text. It is a well
studied problem in English, and was the focus of
CoNLL2000?s Shared Task (Sang and Buchholz,
224
2000). Early attempts at NP Chunking were rule
learning systems, such as the Error Driven Prun-
ing method of Pierce and Cardie (1998). Follow-
ing Ramshaw and Marcus (1995), the current dom-
inant approach is formulating chunking as a clas-
sification task, in which each word is classified as
the (B)eginning, (I)nside or (O)outside of a chunk.
Features for this classification usually involve local
context features. Kudo and Matsumoto (2000) used
SVM as a classification engine and achieved an F-
Score of 93.79 on the shared task NPs. Since SVM
is a binary classifier, to use it for the 3-class classi-
fication of the chunking task, 3 different classifiers
{B/I, B/O, I/O} were trained and their majority vote
was taken.
NP chunks in the shared task data are BaseNPs,
which are non-recursive NPs, a definition first pro-
posed by Ramshaw and Marcus (1995). This defini-
tion yields good NP chunks for English. In (Gold-
berg et al, 2006) we argued that it is not applica-
ble to Hebrew, mainly because of the prevalence
of the Hebrew?s construct state (smixut). Smixut
is similar to a noun-compound construct, but one
that can join a noun (with a special morphologi-
cal marking) with a full NP. It appears in about
40% of Hebrew NPs. We proposed an alterna-
tive definition (termed SimpleNP) for Hebrew NP
chunks. A SimpleNP cannot contain embedded rel-
atives, prepositions, VPs and NP-conjunctions (ex-
cept when they are licensed by smixut). It can
contain smixut, possessives (even when they are
attached by the ???/of? preposition) and partitives
(and, therefore, allows for a limited amount of re-
cursion). We applied this definition to the Hebrew
Tree Bank (Sima?an et al, 2001), and constructed
a moderate size corpus (about 5,000 sentences) for
Hebrew SimpleNP chunking. SimpleNPs are differ-
ent than English BaseNPs, and indeed some meth-
ods that work well for English performed poorly
on Hebrew data. However, we found that chunk-
ing with SVM provides good result for Hebrew Sim-
pleNPs. We analyzed that this success comes from
SVM?s ability to use lexical features, as well as two
Hebrew morphological features, namely ?number?
and ?construct-state?.
One of the main issues when dealing with Hebrew
chunking is that the available tree bank is rather
small, and since it is quite new, and has not been
used intensively, it contains a certain amount of in-
consistencies and tagging errors. In addition, the
identification of SimpleNPs from the tree bank also
introduces some errors. Finally, we want to investi-
gate chunking in a scenario where PoS tags are as-
signed automatically and chunks are then computed.
The Hebrew PoS tagger we use introduces about 8%
errors (compared with about 4% in English). We
are, therefore, interested in identifying errors in the
chunking corpus, and investigating how the chunker
operates in the presence of noise in the PoS tag se-
quence.
3 Model Tampering
3.1 Notation and Technical Review
This section presents notation as well as a technical
review of SVM chunking details relevant to the cur-
rent study. Further details can be found in Kudo and
Matsumoto (2000; 2003).
SVM (Vapnik, 1995) is a supervised binary clas-
sifier. The input to the learner is a set of l train-
ing samples (x1, y1), . . . , (xl, yl), x ? Rn, y ?
{+1,?1}. xi is an n dimensional feature vec-
tor representing the ith sample, and yi is the la-
bel for that sample. The result of the learning pro-
cess is the set SV of Support Vectors, the asso-
ciated weights ?i, and a constant b. The Support
Vectors are a subset of the training vectors, and to-
gether with the weights and b they define a hyper-
plane that optimally separates the training samples.
The basic SVM formulation is of a linear classifier,
but by introducing a kernel function K that non-
linearly transforms the data from Rn into a higher
dimensional space, SVM can be used to perform
non-linear classification. SVM?s decision function
is: y(x) = sgn
(
?
j?SV yj?jK(xj , x) + b
)
where
x is an n dimensional feature vector to be classi-
fied. In the linear case, K is a dot product oper-
ation and the sum w = ? yj?jxj is an n dimen-
sional weight vector assigning weight for each of
the n features. The other kernel function we con-
sider in this paper is a polynomial kernel of degree
2: K(xi, xj) = (xi ? xj + 1)2. When using binary
valued features, this kernel function essentially im-
plies that the classifier considers not only the explic-
itly specified features, but also all available pairs of
features. In order to cope with inseparable data, the
learning process of SVM allows for some misclas-
sification, the amount of which is determined by a
225
parameter C, which can be thought of as a penalty
for each misclassified training sample.
In SVM based chunking, each word and its con-
text is considered a learning sample. We refer to
the word being classified as w0, and to its part-of-
speech (PoS) tag, morphology, and B/I/O tag as p0,
m0 and t0 respectively. The information consid-
ered for classification is w?cw . . . wcw, p?cp . . . pcp,
m?cm . . .mcm and t?ct . . . t?1. The feature vector
F is an indexed list of all the features present in
the corpus. A feature fi of the form w+1 = dog
means that the word following the one being clas-
sified is ?dog?. Every learning sample is repre-
sented by an n = |F | dimensional binary vector x.
xi = 1 iff the feature fi is active in the given sample,
and 0 otherwise. This encoding leads to extremely
high dimensional vectors, due to the lexical features
w?cw . . . wcw.
3.2 Introducing Model Tampering
An important observation about SVM classifiers is
that features which are not active in any of the Sup-
port Vectors have no effect on the classifier deci-
sion. We introduce Model Tampering, a procedure
in which we change the Support Vectors in a model
by forcing some values in the vectors to 0.
The result of this procedure is a new Model in
which the deleted features never take part in the clas-
sification.
Model tampering is different than feature selec-
tion: on the one hand, it is a method that helps us
identify irrelevant features in a model after training;
on the other hand, and this is the key insight, re-
moving features after training is not the same as re-
moving them before training. The presence of the
low-relevance features during training has an impact
on the generalization performed by the learner as
shown below.
3.3 The Role of Lexical Features
In Goldberg et al (2006), we have established that
using lexical features increases the chunking F-
measure from 78 to over 92 on the Hebrew Tree-
bank. We refine this observation by using Model
Tampering, in order to assess the importance of lex-
ical features in NP Chunking. We are interested in
identifying which specific lexical items and contexts
impact the chunking decision, and quantifying their
effect. Our method is to train a chunking model
on a given training corpus, tamper with the result-
ing model in various ways and measure the perfor-
mance1 of the tampered models on a test corpus.
3.4 Experimental Setting
We conducted experiments both for English and He-
brew chunking. For the Hebrew experiments, we use
the corpora of (Goldberg et al, 2006). The first one
is derived from the original Treebank by projecting
the full syntactic tree, constructed manually, onto a
set of NP chunks according to the SimpleNP rules.
We refer to the resulting corpus as HEBGold since
PoS tags are fully reliable. The HEBErr version
of the corpus is obtained by projecting the chunk
boundaries on the sequence of PoS and morphology
tags obtained by the automatic PoS tagger of Adler
& Elhadad (2006). This corpus includes an error
rate of about 8% on PoS tags. The first 500 sen-
tences are used for testing, and the rest for training.
The corpus contains 27K NP chunks. For the En-
glish experiments, we use the now-standard training
and test sets that were introduced in (Marcus and
Ramshaw, 1995)2. Training was done using Kudo?s
YAMCHA toolkit3. Both Hebrew and English mod-
els were trained using a polynomial kernel of de-
gree 2, with C = 1. For English, the features used
were: w?2 . . . w2, p?2 . . . p2, t?2 . . . t?1. The same
features were used for Hebrew, with the addition of
m?2 . . .m2. These are the same settings as in (Kudo
and Matsumoto, 2000; Goldberg et al, 2006).
3.5 Tamperings
We experimented with the following tamperings:
TopN ? We define model feature count to be the
number of Support Vectors in which a feature is ac-
tive in a given classifier. This tampering leaves in the
model only the top N lexical features in each classi-
fier, according to their count.
NoPOS ? all the lexical features corresponding to
a given part-of-speech are removed from the model.
For example, in a NoJJ tampering, all the features of
the form wi = X are removed from all the support
vectors in which pi = JJ is active.
Loc6=i ? all the lexical features with index i are
removed from the model e.g., in a Loc6=+2 tamper-
1The performance metric we use is the standard Preci-
sion/Recall/F measures, as computed by the conlleval program:
http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
2ftp://ftp.cis.upenn.edu/pub/chunker
3http://chasen.org/?taku/software/yamcha/
226
ing, features of the form w+2 = X are removed).
Loc=i ? all the lexical features with an index other
than i are removed from the model.
3.6 Results and Discussion
Highlights of the results are presented in Tables (1-
3). The numbers reported are F measures.
TopN HEBGold HEBErr ENG
ALL 93.58 92.48 93.79
N=0 78.32 76.27 90.10
N=10 90.21 88.68 90.24
N=50 91.78 90.85 91.22
N=100 92.25 91.62 91.72
N=500 93.60 92.23 93.12
N=1000 93.56 92.41 93.30
Table 1: Results of TopN Tampering.
The results of the TopN tamperings show that for
both languages, most of the lexical features are irrel-
evant for the classification ? the numbers achieved
by using all the lexical features (about 30,000 in He-
brew and 75,000 in English) are very close to those
obtained using only a few lexical features. This
finding is very encouraging, and suggests that SVM
based chunking is robust to corpus variations.
Another conclusion is that lexical features help
balance the fact that PoS tags can be noisy: we
know both HEBErr and ENG include PoS tag-
ging errors (about 8% in Hebrew and 4% in En-
glish). While in the case of ?perfect? PoS tagging
(HEBGold), a very small amount of lexical features
is sufficient to reach the best F-result (500 out of
30,264), in the presence of PoS errors, more than
the top 1000 lexical features are needed to reach the
result obtained with all lexical features.
More striking is the fact that in Hebrew, the
top 10 lexical features are responsible for an im-
provement of 12.4 in F-score. The words cov-
ered by these 10 features are the following: Start
of Sentence marker and comma, quote,
?of/???, ?and/??, ?the/?? and ?in/??.
This finding suggests that the Hebrew PoS tagset
might not be informative enough for the chunking
task, especially where punctuation 4 and preposi-
tions are concerned. The results in Table 2 give fur-
ther support for this claim.
4Unlike the WSJ PoS tagset in which most punctuations get
unique tags, our tagset treat punctuation marks as one group.
NoPOS HEBG HEBE NoPOS HEBG HEBE
Prep 85.25 84.40 Pronoun 92.97 92.14
Punct 88.90 87.66 Conjunction 92.31 91.67
Adverb 92.02 90.72 Determiner 92.55 91.39
Table 2: Results of Hebrew NoPOS Tampering.
Other scores are ? 93.3(HEBG), ? 92.2(HEBE).
When removing lexical features of a specific
PoS, the most dramatic loss of F-score is reached
for Prepositions and Punctuation marks, followed
by Adverbs, and Conjunctions. Strikingly, lexi-
cal information for most open-class PoS (including
Proper Names and Nouns) has very little impact on
Hebrew chunking performance.
From this observation, one could conclude that
enriching a model based only on PoS with lexical
features for only a few closed-class PoS (prepo-
sitions and punctuation) could provide appropri-
ate results even with a simpler learning method,
one that cannot deal with a large number of fea-
tures. We tested this hypothesis by training the
Error-Driven Pruning (EDP) method of (Cardie and
Pierce, 1998) with an extended set of features. EDP
with PoS features only produced an F-result of 76.3
on HEBGold. By adding lexical features only for
prepositions {? ? ? ? ??}, one conjunction {?} and
punctuation, the F-score on HEBGold indeed jumps
to 85.4. However, when applied on HEBErr, EDP
falls down again to 59.4. This striking disparity, by
comparison, lets us appreciate the resilience of the
SVM model to PoS tagging errors, and its gener-
alization capability even with a reduced number of
lexical features.
Another implication of this data is that commas
and quotation marks play a major role in deter-
mining NP boundaries in Hebrew. In Goldberg
et al (2006), we noted the Hebrew Treebank is not
consistent in its treatment of punctuation, and thus
we evaluated the chunker only after performing nor-
malization of chunk boundaries for punctuations.
We now hypothesize that, since commas and quo-
tation marks play such an important role in the clas-
sification, performing such normalization before the
training stage might be beneficial. Indeed results on
the normalized corpus show improvement of about
1.0 in F score on both HEBErr and HEBGold. A
10-fold cross validation experiment on punctuation
normalized HEBErr resulted in an F-Score of 92.2,
improving the results reported by (Goldberg et al,
227
2006) on the same setting (91.4).
Loc=I HEBE ENG Loc6=I HEBE ENG
-2 78.26 89.79 -2 91.62 93.87
-1 76.96 90.90 -1 91.86 93.03
0 90.33 92.37 0 79.44 91.16
1 76.90 90.47 1 92.33 93.30
2 76.55 90.06 2 92.18 93.65
Table 3: Results of Loc Tamperings.
We now turn to analyzing the importance of con-
text positions (Table 3). For both languages, the
most important lexical feature (by far) is at position
0, that is, the word currently being classified. For
English, it is followed by positions 1 and -1, and
then positions 2 and -2. For Hebrew, back context
seems to have more effect than front context. In
Hebrew, all the positions positively contribute to the
decision, while in English removing w2/?2 slightly
improves the results (note also that including only
feature w2/?2 performs worse than with no lexical
information in English).
3.7 The Real Role of Lexical Features
Model tampering (i.e., removing features after the
learning stage) is not the same as learning without
these features. This claim is verified empirically:
training on the English corpus without the lexical
features at position ?2 yields worse results than with
them (93.73 vs. 93.79) ? while removing the w?2
features via tampering on a model trained with w?2
yields better results (93.87). Similarly, for all cor-
pora, training using only the top 1,000 features (as
defined in the Top1000 tampering) results in loss of
about 2 in F-Score (ENG 92.02, HEBErr 90.30,
HEBGold 91.67), while tampering Top1000 yields
a result very close to the best obtained (93.56, 92.41
or 93.3F).
This observation leads us to an interesting conclu-
sion about the real role of lexical features in SVM
based chunking: rare events (features) are used to
memorize hard examples. Intuitively, by giving a
heavy weight to rare events, the classifier learns spe-
cific rules such as ?if the word at position -2 is X and
the PoS at position 2 is Y, then the current word is
Inside a noun-phrase?. Most of these rules are acci-
dental ? there is no real relation between the partic-
ular word-pos combination and the class of the cur-
rent word, it just happens to be this way in the train-
ing samples. Marking the rare occurrences helps the
learner achieve better generalization on the other,
more common cases, which are similar to the outlier
on most features, except the ?irrelevant ones?. As
the events are rare, such rules usually have no effect
on chunking accuracy: they simply never occur in
the test data. This observation refines the common
conception that SVM chunking does not suffer from
irrelevant features: in chunking, SVM indeed gener-
alizes well for the common cases but also over-fits
the model on outliers.
Model tampering helps us design a model in two
ways: (1) it is a way to ?open the black box? ob-
tained when training an SVM and to analyze the re-
spective importance of features. In our case, this
analysis allowed us to identify the importance of
punctuation and prepositions and improve the model
by defining more focused features (improving over-
all result by ?1.0 F-point). (2) The analysis also led
us to the conclusion that ?feature selection? is com-
plex in the case of SVM ? irrelevant features help
prevent over-generalization by forcing over-fitting
on outliers.
We have also confirmed that the model learned re-
mains robust in the presence of noise in the PoS tags
and relies on only few lexical features. This veri-
fication is critical in the context of languages with
few computational resources, as we expect the size
of corpora and the quality of taggers to keep lagging
behind that achieved in English.
4 Anchored Learning
We pursue the observation of how SVM deals
with outliers by developing the Anchored Learning
method. The idea behind Anchored Learning is to
add a unique feature ai (an anchor) to each training
sample (we add as many new features to the model
as there are training samples). These new features
make our data linearly separable. The SVM learner
can then use these anchors (which will never occur
on the test data) to memorize the hard cases, de-
creasing this burden from ?real? features.
We present two uses for Anchored Learning. The
first is the identification of hard cases and corpus er-
rors, and the second is a preliminary feature selec-
tion approach for SVM to improve chunking accu-
racy.
4.1 Mining for Errors and Hard Cases
Following the intuition that SVM gives more weight
to anchor features of hard-to-classify cases, we can
228
actively look for such cases by training an SVM
chunker on anchored data (as the anchored data is
guaranteed to be linearly separable, we can set a very
high value to the C parameter, preventing any mis-
classification), and then investigating either the an-
chors whose weights5 are above some threshold t or
the top N heaviest anchors, and their corresponding
corpus locations. These locations are those that
the learner considers hard to classify. They can
be either corpus errors, or genuinely hard cases.
This method is similar to the corpus error detec-
tion method presented by Nakagawa and Matsumoto
(2002). They constructed an SVM model for PoS
tagging, and considered Support Vectors with high
? values to be indicative of suspicious corpus loca-
tions. These locations can be either outliers, or cor-
rectly labeled locations similar to an outlier. They
then looked for similar corpus locations with a dif-
ferent label, to point out right-wrong pairs with high
precision.
Using anchors improves their method in three as-
pects: (1) without anchors, similar examples are of-
ten indistinguishable to the SVM learner, and in case
they have conflicting labels both examples will be
given high weights. That is, both the regular case
and the hard case will be considered as hard exam-
ples. Moreover, similar corpus errors might result
in only one support vector that cover all the group of
similar errors. Anchors mitigate these effects, result-
ing in better precision and recall. (2) The more er-
rors there are in the corpus, the less linearly separa-
ble it is. Un-anchored learning on erroneous corpus
can take unreasonable amount of time. (3) Anchors
allow learning while removing some of the impor-
tant features but still allow the process to converge
in reasonable time. This lets us analyze which cases
become hard to learn if we don?t use certain features,
or in other words: what problematic cases are solved
by specific features.
The hard cases analysis achieved by anchored
learning is different from the usual error analysis
carried out on observed classification errors. The
traditional methods give us intuitions about where
the classifier fails to generalize, while the method
we present here gives us intuition about what the
classifier considers hard to learn, based on the
training examples alone.
5As each anchor appear in only one support vector, we can
treat the vector?s ? value as the anchor weight
The intuition that ?hard to learn? examples are
suspect corpus errors is not new, and appears also
in Abney et al (1999) , who consider the ?heaviest?
samples in the final distribution of the AdaBoost al-
gorithm to be the hardest to classify and thus likely
corpus errors. While AdaBoost models are easy to
interpret, this is not the case with SVM. Anchored
learning allows us to extract the hard to learn cases
from an SVM model. Interestingly, while both Ad-
aBoost and SVM are ?large margin? based classi-
fiers, there is less than 50% overlap in the hard cases
for the two methods (in terms of mistakes on the test
data, there were 234 mistakes shared by AdaBoost
and SVM, 69 errors unique to SVM and 126 errors
unique to AdaBoost)6. Analyzing the difference in
what the two classifiers consider hard is interesting,
and we will address it in future work. In the current
work, we note that for finding corpus errors the two
methods are complementary.
Experiment 1 ? Locating Hard Cases
A linear SVM model (Mfull) was trained on
the training subset of the anchored, punctuation-
normalized, HEBGold corpus, with the same fea-
tures as in the previous experiments, and a C value
of 9,999. Corpus locations corresponding to anchors
with weights >1 were inspected. There were about
120 such locations out of 4,500 sentences used in the
training set. Decreasing the threshold t would result
in more cases. We analyzed these locations into 3
categories: corpus errors, cases that challenge the
SimpleNP definition, and cases where the chunking
decision is genuinely difficult to make in the absence
of global syntactic context or world knowledge.
Corpus Errors: The analysis revealed the fol-
lowing corpus errors: we identified 29 hard cases
related to conjunction and apposition (is the comma,
colon or slash inside an NP or separating two distinct
NPs). 14 of these hard cases were indeed mistakes
in the corpus. This was anticipated, as we distin-
guished appositions and conjunctive commas using
heuristics, since the Treebank marking of conjunc-
tions is somewhat inconsistent.
In order to build the Chunk NP corpus, the syn-
tactic trees of the Treebank were processed to derive
chunks according to the SimpleNP definition. The
hard cases analysis identified 18 instances where this
6These numbers are for pairwise Linear SVM and AdaBoost
classifiers trained on the same features.
229
transformation results in erroneous chunks. For ex-
ample, null elements result in improper chunks, such
as chunks containing only adverbs or only adjec-
tives.
We also found 3 invalid sentences, 6 inconsisten-
cies in the tagging of interrogatives with respect to
chunk boundaries, as well as 34 other specific mis-
takes. Overall, more than half of the locations iden-
tified by the anchors were corpus errors. Looking for
cases similar to the errors identified by anchors, we
found 99 more locations, 77 of which were errors.
Refining the SimpleNP Definition: The hard
cases analysis identified examples that challenge
the SimpleNP definition proposed in Goldberg
et al (2006). The most notable cases are:
The ?et? marker : ?et? is a syntactic marker of defi-
nite direct objects in Hebrew. It was regarded as a
part of SimpleNPs in their definition. In some cases,
this forces the resulting SimpleNP to be too inclu-
sive:
[???????? ????? ??? ????? ,?????? ??]
[?et? (the government, the parliament and the media)]
Because in the Treebank the conjunction depends on
?et? as a single constituent, it is fully embedded in
the chunk. Such a conjunction should not be consid-
ered simple.
The ?? preposition (?of?) marks generalized posses-
sion and was considered unambiguous and included
in SimpleNPs. We found cases where ???? causes
PP attachment ambiguity:
[??????] ?? [?????] ? [???? ??? ????]
[president-cons house-cons the-law] for [discipline] of [the
police] / The Police Disciplinary Court President
Because 2 prepositions are involved in this NP, ????
(of) and ??? (for), the ???? part cannot be attached
unambiguously to its head (?court?). It is unclear
whether the ??? preposition should be given special
treatment to allow it to enter simple NPs in certain
contexts, or whether the inconsistent handling of
the ???? that results from the ??? inter-position is
preferable.
Complex determiners and quantifiers: In many
cases, complex determiners in Hebrew are multi-
word expressions that include nouns. The inclusion
of such determiners inside the SimpleNPs is not
consistent.
Genuinely hard cases were also identified.
These include prepositions, conjunctions and multi-
word idioms (most of them are adjectives and prepo-
sitions which are made up of nouns and determin-
ers, e.g., as the word unanimously is expressed in
Hebrew as the multi-word expression ?one mouth?).
Also, some adverbials and adjectives are impossible
to distinguish using only local context.
The anchors analysis helped us improve the
chunking method on two accounts: (1) it identified
corpus errors with high precision; (2) it made us fo-
cus on hard cases that challenge the linguistic defi-
nition of chunks we have adopted. Following these
findings, we intend to refine the Hebrew SimpleNP
definition, and create a new version of the Hebrew
chunking corpus.
Experiment 2 ? determining the role of
contextual lexical features
The intent of this experiment is to understand the
role of the contextual lexical features (wi, i 6= 0).
This is done by training 2 additional anchored lin-
ear SVM models, Mno?cont and Mnear. These are
the same as Mfull except for the lexical features
used during training. Mno?cont uses only w0, while
Mnear uses w0,w?1,w+1.
Anchors are again used to locate the hard exam-
ples for each classifier, and the differences are ex-
amined. The examples that are hard for Mnear but
not for Mfull are those solved by w?2,w+2. Sim-
ilarly, the examples that are hard for Mno?cont but
not for Mnear are those solved by w?1,w+1. Table 4
indicates the number of hard cases identified by the
anchor method for each model. One way to inter-
pret these figures, is that the introduction of features
w?1,+1 solves 5 times more hard cases than w?2,+2.
Model Number of hard
cases (t = 1)
Hard cases for
classifier B-I
Mfull 120 2
Mnear 320 (+ 200) 12
Mno?cont 1360 (+ 1040) 164
Table 4: Number of hard cases per model type.
Qualitative analysis of the hard cases solved by
the contextual lexical features shows that they con-
tribute mostly to the identification of chunk bound-
aries in cases of conjunction, apposition, attachment
of adverbs and adjectives, and some multi-word ex-
pressions.
The number of hard cases specific to the B-I clas-
sifier indicates how the features contribute to the de-
cision of splitting or continuing back-to-back NPs.
Back-to-back NPs amount to 6% of the NPs in
HEBGold and 8% of the NPs in ENG. However,
230
while in English most of these cases are easily re-
solved, Hebrew phenomena such as null-equatives
and free word order make them harder. To quantify
the difference: 79% of the first words of the second
NP in English belong to one of the closed classes
POS, DT, WDT, PRP, WP ? categories which mostly
cannot appear in the middle of base NPs. In con-
trast, in Hebrew, 59% are Nouns, Numbers or Proper
Names. Moreover, in English the ratio of unique first
words to number of adjacent NPs is 0.068, while in
Hebrew it is 0.47. That is, in Hebrew, almost every
second such NP starts with a different word.
These figures explain why surrounding lexical in-
formation is needed by the learner in order to clas-
sify such cases. They also suggest that this learning
is mostly superficial, that is, the learner just mem-
orizes some examples, but these will not generalize
well on test data. Indeed, the most common class of
errors reported in Goldberg et al , 2006 are of the
split/merge type. These are followed by conjunction
related errors, which suffer from the same problem.
Morphological features of smixut and agreement can
help to some extent, but this is still a limited solu-
tion. It seems that deciding the [NP][NP] case is
beyond the capabilities of chunking with local con-
text features alone, and more global features should
be sought.
4.2 Facilitating Better Learning
This section presents preliminary results using An-
chored Learning for better NP chunking. We present
a setting (English Base NP chunking) in which
selected features coupled together with anchored
learning show an improvement over previous results.
Section 3.6 hinted that SVM based chunking
might be hurt by using too many lexical features.
Specifically, the features w?2,w+2 were shown to
cause the chunker to overfit in English chunking.
Learning without these features, however, yields
lower results. This can be overcome by introduc-
ing anchors as a substitute. Anchors play the same
role as rare features when learning, while lowering
the chance of misleading the classifier on test data.
The results of the experiment using 5-fold cross
validation on ENG indicate that the F-score im-
proves on average from 93.95 to 94.10 when using
anchors instead of w?2 (+0.15), while just ignoring
the w?2 features drops the F-score by 0.10. The im-
provement is minor but consistent. Its implication
is that anchors can substitute for ?irrelevant? lexical
features for better learning results. In future work,
we will experiment with better informed sets of lex-
ical features mixed with anchors.
5 Conclusion
We have introduced two novel methods to under-
stand the inner structure of SVM-learned models.
We have applied these techniques to Hebrew NP
chunking, and demonstrated that the learned model
is robust in the presence of noise in the PoS tags, and
relies on only a few lexical features. We have iden-
tified corpus errors, better understood the nature of
the task in Hebrew ? and compared it quantitatively
to the task in English.
The methods provide general insight in the way
SVM classification works for chunking.
References
S. Abney, R. Schapire, and Y. Singer. 1999. Boosting
applied to tagging and PP attachment. EMNLP-1999.
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological dis-
ambiguation. In COLING/ACL2006.
C. Cardie and D. Pierce. 1998. Error-driven pruning of
treebank grammars for base noun phrase identification.
In ACL-1998.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
phrase chunking in hebrew: Influence of lexical and
morphological features. In COLING/ACL2006.
T. Kudo and Y. Matsumoto. 2000. Use of support vector
learning for chunk identification. In CoNLL-2000.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
M. Marcus and L. Ramshaw. 1995. Text Chunking Us-
ing Transformation-Based Learning. In Proc. of the
3rd ACL Workshop on Very Large Corpora.
T. Nakagawa and Y. Matsumoto. 2002. Detecting er-
rors in corpora using support vector machines. In
COLING-2002.
Erik F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the conll-2000 shared task: chunking. In
CoNLL-2000.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
V. Vapnik. 1995. The nature of statistical learning the-
ory. Springer-Verlag New York, Inc.
231
Proceedings of ACL-08: HLT, pages 728?736,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Lexicon-Based Resolution of Unknown Words for Full
Morphological Analysis
Meni Adler and Yoav Goldberg and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science?
POB 653 Be?er Sheva, 84105, Israel
{adlerm,goldberg,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Morphological disambiguation proceeds in 2
stages: (1) an analyzer provides all possible
analyses for a given token and (2) a stochastic
disambiguation module picks the most likely
analysis in context. When the analyzer does
not recognize a given token, we hit the prob-
lem of unknowns. In large scale corpora, un-
knowns appear at a rate of 5 to 10% (depend-
ing on the genre and the maturity of the lexi-
con).
We address the task of computing the distribu-
tion p(t|w) for unknown words for full mor-
phological disambiguation in Hebrew. We in-
troduce a novel algorithm that is language in-
dependent: it exploits a maximum entropy let-
ters model trained over the known words ob-
served in the corpus and the distribution of
the unknown words in known tag contexts,
through iterative approximation. The algo-
rithm achieves 30% error reduction on dis-
ambiguation of unknown words over a com-
petitive baseline (to a level of 70% accurate
full disambiguation of unknown words). We
have also verified that taking advantage of a
strong language-specific model of morpholog-
ical patterns provides the same level of disam-
biguation. The algorithm we have developed
exploits distributional information latent in a
wide-coverage lexicon and large quantities of
unlabeled data.
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
1 Introduction
The term unknowns denotes tokens in a text that can-
not be resolved in a given lexicon. For the task of
full morphological analysis, the lexicon must pro-
vide all possible morphological analyses for any
given token. In this case, unknown tokens can be
categorized into two classes of missing informa-
tion: unknown tokens are not recognized at all by
the lexicon, and unknown analyses, where the set
of analyses for a lexeme does not contain the cor-
rect analysis for a given token. Despite efforts on
improving the underlying lexicon, unknowns typi-
cally represent 5% to 10% of the number of tokens
in large-scale corpora. The alternative to continu-
ously investing manual effort in improving the lex-
icon is to design methods to learn possible analy-
ses for unknowns from observable features: their
letter structure and their context. In this paper, we
investigate the characteristics of Hebrew unknowns
for full morphological analysis, and propose a new
method for handling such unavoidable lack of in-
formation. Our method generates a distribution of
possible analyses for unknowns. In our evaluation,
these learned distributions include the correct anal-
ysis for unknown words in 85% of the cases, con-
tributing an error reduction of over 30% over a com-
petitive baseline for the overall task of full morpho-
logical analysis in Hebrew.
The task of a morphological analyzer is to pro-
duce all possible analyses for a given token. In
Hebrew, the analysis for each token is of the form
lexeme-and-features1: lemma, affixes, lexical cate-
1In contrast to the prefix-stem-suffix analysis format of
728
gory (POS), and a set of inflection properties (ac-
cording to the POS) ? gender, number, person, sta-
tus and tense. In this work, we refer to the mor-
phological analyzer of MILA ? the Knowledge Cen-
ter for Processing Hebrew2 (hereafter KC analyzer).
It is a synthetic analyzer, composed of two data re-
sources ? a lexicon of about 2,400 lexemes, and a
set of generation rules (see (Adler, 2007, Section
4.2)). In addition, we use an unlabeled text cor-
pus, composed of stories taken from three Hebrew
daily news papers (Aruts 7, Haaretz, The Marker),
of 42M tokens. We observed 3,561 different com-
posite tags (e.g., noun-sing-fem-prepPrefix:be) over
this corpus. These 3,561 tags form the large tagset
over which we train our learner. On the one hand,
this tagset is much larger than the largest tagset used
in English (from 17 tags in most unsupervised POS
tagging experiments, to the 46 tags of the WSJ cor-
pus and the about 150 tags of the LOB corpus). On
the other hand, our tagset is intrinsically factored as
a set of dependent sub-features, which we explicitly
represent.
The task we address in this paper is morphologi-
cal disambiguation: given a sentence, obtain the list
of all possible analyses for each word from the an-
alyzer, and disambiguate each word in context. On
average, each token in the 42M corpus is given 2.7
possible analyses by the analyzer (much higher than
the average 1.41 POS tag ambiguity reported in En-
glish (Dermatas and Kokkinakis, 1995)). In previ-
ous work, we report disambiguation rates of 89%
for full morphological disambiguation (using an un-
supervised EM-HMM model) and 92.5% for part of
speech and segmentation (without assigning all the
inflectional features of the words).
In order to estimate the importance of unknowns
in Hebrew, we analyze tokens in several aspects: (1)
the number of unknown tokens, as observed on the
corpus of 42M tokens; (2) a manual classification
of a sample of 10K unknown token types out of the
200K unknown types identified in the corpus; (3) the
number of unknown analyses, based on an annotated
corpus of 200K tokens, and their classification.
About 4.5% of the 42M token instances in the
Buckwalter?s Arabic analyzer (2004), which looks for any le-
gal combination of prefix-stem-suffix, but does not provide full
morphological features such as gender, number, case etc.
2http://mila.cs.technion.ac.il.html
training corpus were unknown tokens (45% of the
450K token types). For less edited text, such as ran-
dom text sampled from the Web, the percentage is
much higher ? about 7.5%. In order to classify these
unknown tokens, we sampled 10K unknown token
types and examined them manually. The classifica-
tion of these tokens with their distribution is shown
in Table 13. As can be seen, there are two main
classes of unknown token types: Neologisms (32%)
and Proper nouns (48%), which cover about 80%
of the unknown token instances. The POS distribu-
tion of the unknown tokens of our annotated corpus
is shown in Table 2. As expected, most unknowns
are open class words: proper names, nouns or adjec-
tives.
Regarding unknown analyses, in our annotated
corpus, we found 3% of the 100K token instances
were missing the correct analysis in the lexicon
(3.65% of the token types). The POS distribution of
the unknown analyses is listed in Table 2. The high
rate of unknown analyses for prepositions at about
3% is a specific phenomenon in Hebrew, where
prepositions are often prefixes agglutinated to the
first word of the noun phrase they head. We observe
the very low rate of unknown verbs (2%) ? which are
well marked morphologically in Hebrew, and where
the rate of neologism introduction seems quite low.
This evidence illustrates the need for resolution
of unknowns: The naive policy of selecting ?proper
name? for all unknowns will cover only half of the
errors caused by unknown tokens, i.e., 30% of the
whole unknown tokens and analyses. The other 70%
of the unknowns ( 5.3% of the words in the text in
our experiments) will be assigned a wrong tag.
As a result of this observation, our strategy is to
focus on full morphological analysis for unknown
tokens and apply a proper name classifier for un-
known analyses and unknown tokens. In this paper,
we investigate various methods for achieving full
morphological analysis distribution for unknown to-
kens. The methods are not based on an annotated
corpus, nor on hand-crafted rules, but instead ex-
ploit the distribution of words in an available lexicon
and the letter similarity of the unknown words with
known words.
3Transcription according to Ornan (2002)
729
Category Examples DistributionTypes Instances
Proper names ?asulin (family name) oileq`
?a?udi (Audi) ice`` 40% 48%
Neologisms ?agabi (incidental) iab`
tizmur (orchestration) xenfz 30% 32%
Abbreviation mz?p (DIFS) t"fnkb?t (security officer) h"aw 2.4% 7.8%
Foreign
presentacyah (presentation) divhpfxt
?a?ut (out) he``
right
3.8% 5.8%
Wrong spelling
?abibba?ah
.
ronah (springatlast) dpexg`aaia`
?idiqacyot (idication) zeivwici`
ryus?alaim (Rejusalem) milyeix
1.2% 4%
Alternative spelling ?opyynim (typical) mipiite`priwwilegyah (privilege ) diblieeixt 3.5% 3%
Tokenization ha?sap (the?threshold) sq"d
?al/17 (on/17) 71/lr 8% 2%
Table 1: Unknown Hebrew token categories and distribution.
Part of Speech Unknown Tokens Unknown Analyses Total
Proper name 31.8% 24.4% 56.2%
Noun 12.6% 1.6% 14.2%
Adjective 7.1% 1.7% 8.8%
Junk 3.0% 1.3% 4.3%
Numeral 1.1% 2.3% 3.4%
Preposition 0.3% 2.8% 3.1%
Verb 1.8% 0.4% 2.2%
Adverb 0.9% 0.9% 1.8%
Participle 0.4% 0.8% 1.2%
Copula / 0.8% 0.8%
Quantifier 0.3% 0.4% 0.7%
Modal 0.3% 0.4% 0.7%
Conjunction 0.1% 0.5% 0.6%
Negation / 0.6% 0.6%
Foreign 0.2% 0.4% 0.6%
Interrogative 0.1% 0.4% 0.5%
Prefix 0.3% 0.2% 0.5%
Pronoun / 0.5% 0.5%
Total 60% 40% 100%
Table 2: Unknowns Hebrew POS Distribution.
730
2 Previous Work
Most of the work that dealt with unknowns in the last
decade focused on unknown tokens (OOV). A naive
approach would assign all possible analyses for each
unknown token with uniform distribution, and con-
tinue disambiguation on the basis of a learned model
with this initial distribution. The performance of a
tagger with such a policy is actually poor: there are
dozens of tags in the tagset (3,561 in the case of He-
brew full morphological disambiguation) and only
a few of them may match a given token. Several
heuristics were developed to reduce the possibility
space and to assign a distribution for the remaining
analyses.
Weischedel et al (1993) combine several heuris-
tics in order to estimate the token generation prob-
ability according to various types of information ?
such as the characteristics of particular tags with
respect to unknown tokens (basically the distribu-
tion shown in Table 2), and simple spelling fea-
tures: capitalization, presence of hyphens and spe-
cific suffixes. An accuracy of 85% in resolving un-
known tokens was reported. Dermatas and Kokki-
nakis (1995) suggested a method for guessing un-
known tokens based on the distribution of the ha-
pax legomenon, and reported an accuracy of 66% for
English. Mikheev (1997) suggested a guessing-rule
technique, based on prefix morphological rules, suf-
fix morphological rules, and ending-guessing rules.
These rules are learned automatically from raw text.
They reported a tagging accuracy of about 88%.
Thede and Harper (1999) extended a second-order
HMM model with a C = ck,i matrix, in order to en-
code the probability of a token with a suffix sk to
be generated by a tag ti. An accuracy of about 85%
was reported.
Nakagawa (2004) combine word-level and
character-level information for Chinese and
Japanese word segmentation. At the word level, a
segmented word is attached to a POS, where the
character model is based on the observed characters
and their classification: Begin of word, In the
middle of a word, End of word, the character is a
word itself S. They apply Baum-Welch training over
a segmented corpus, where the segmentation of each
word and its character classification is observed, and
the POS tagging is ambiguous. The segmentation
(of all words in a given sentence) and the POS
tagging (of the known words) is based on a Viterbi
search over a lattice composed of all possible word
segmentations and the possible classifications of
all observed characters. Their experimental results
show that the method achieves high accuracy over
state-of-the-art methods for Chinese and Japanese
word segmentation. Hebrew also suffers from
ambiguous segmentation of agglutinated tokens into
significant words, but word formation rules seem to
be quite different from Chinese and Japanese. We
also could not rely on the existence of an annotated
corpus of segmented word forms.
Habash and Rambow (2006) used the
root+pattern+features representation of Arabic
tokens for morphological analysis and generation
of Arabic dialects, which have no lexicon. They
report high recall (95%?98%) but low precision
(37%?63%) for token types and token instances,
against gold-standard morphological analysis. We
also exploit the morphological patterns characteris-
tic of semitic morphology, but extend the guessing
of morphological features by using contextual
features. We also propose a method that relies
exclusively on learned character-level features and
contextual features, and eventually reaches the same
performance as the patterns-based approach.
Mansour et al (2007) combine a lexicon-based
tagger (such as MorphTagger (Bar-Haim et al,
2005)), and a character-based tagger (such as the
data-driven ArabicSVM (Diab et al, 2004)), which
includes character features as part of its classifica-
tion model, in order to extend the set of analyses
suggested by the analyzer. For a given sentence, the
lexicon-based tagger is applied, selecting one tag for
a token. In case the ranking of the tagged sentence is
lower than a threshold, the character-based tagger is
applied, in order to produce new possible analyses.
They report a very slight improvement on Hebrew
and Arabic supervised POS taggers.
Resolution of Hebrew unknown tokens, over a
large number of tags in the tagset (3,561) requires
a much richer model than the the heuristics used
for English (for example, the capitalization feature
which is dominant in English does not exist in He-
brew). Unlike Nakagawa, our model does not use
any segmented text, and, on the other hand, it aims
to select full morphological analysis for each token,
731
including unknowns.
3 Method
Our objective is: given an unknown word, provide
a distribution of possible tags that can serve as the
analysis of the unknown word. This unknown anal-
ysis step is performed at training and testing time.
We do not attempt to disambiguate the word ? but
only to provide a distribution of tags that will be dis-
ambiguated by the regular EM-HMM mechanism.
We examined three models to construct the distri-
bution of tags for unknown words, that is, whenever
the KC analyzer does not return any candidate anal-
ysis, we apply these models to produce possible tags
for the token p(t|w):
Letters A maximum entropy model is built for
all unknown tokens in order to estimate their tag
distribution. The model is trained on the known
tokens that appear in the corpus. For each anal-
ysis of a known token, the following features are
extracted: (1) unigram, bigram, and trigram letters
of the base-word (for each analysis, the base-word
is the token without prefixes), together with their
index relative to the start and end of the word. For
example, the n-gram features extracted for the word
abc are { a:1 b:2 c:3 a:-3 b:-2 c:-1
ab:1 bc:2 ab:-2 bc:-1 abc:1 abc:-1
} ; (2) the prefixes of the base-word (as a single
feature); (3) the length of the base-word. The class
assigned to this set of features, is the analysis of the
base-word. The model is trained on all the known
tokens of the corpus, each token is observed with its
possible POS-tags once for each of its occurrences.
When an unknown token is found, the model
is applied as follows: all the possible linguistic
prefixes are extracted from the token (one of the 76
prefix sequences that can occur in Hebrew); if more
than one such prefix is found, the token is analyzed
for each possible prefix. For each possible such
segmentation, the full feature vector is constructed,
and submitted to the Maximum Entropy model.
We hypothesize a uniform distribution among the
possible segmentations and aggregate a distribution
of possible tags for the analysis. If the proposed
tag of the base-word is never found in the corpus
preceded by the identified prefix, we remove this
possible analysis. The eventual outcome of the
model application is a set of possible full morpho-
logical analyses for the token ? in exactly the same
format as the morphological analyzer provides.
Patterns Word formation in Hebrew is based on
root+pattern and affixation. Patterns can be used to
identify the lexical category of unknowns, as well
as other inflectional properties. Nir (1993) investi-
gated word-formation in Modern Hebrew with a spe-
cial focus on neologisms; the most common word-
formation patterns he identified are summarized in
Table 3. A naive approach for unknown resolution
would add all analyses that fit any of these patterns,
for any given unknown token. As recently shown by
Habash and Rambow (2006), the precision of such
a strategy can be pretty low. To address this lack of
precision, we learn a maximum entropy model on
the basis of the following binary features: one fea-
ture for each pattern listed in column Formation of
Table 3 (40 distinct patterns) and one feature for ?no
pattern?.
Pattern-Letters This maximum entropy model is
learned by combining the features of the letters
model and the patterns model.
Linear-Context-based p(t|c) approximation
The three models above are context free. The
linear-context model exploits information about the
lexical context of the unknown words: to estimate
the probability for a tag t given a context c ? p(t|c)
? based on all the words in which a context occurs,
the algorithm works on the known words in the
corpus, by starting with an initial tag-word estimate
p(t|w) (such as the morpho-lexical approximation,
suggested by Levinger et al (1995)), and iteratively
re-estimating:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?C p(t|c)p(c|w)allow(t, w)
Z
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of contexts.
allow(t, w) is a binary function indicating whether t
is a valid tag for w. p(c|w) and p(w|c) are estimated
via raw corpus counts.
Loosely speaking, the probability of a tag given a
context is the average probability of a tag given any
732
Category Formation Example
Verb Template
?iCCeC ?ibh
.
en (diagnosed) oga`
miCCeC mih
.
zer (recycled) xfgn
CiCCen timren (manipulated) oxnz
CiCCet tiknet (programmed) zpkz
tiCCeC ti?arek (dated) jx`z
Participle Template
meCuCaca ms?wh
.
zar (reconstructed) xfgeyn
muCCaC muqlat
.
(recorded) hlwen
maCCiC malbin (whitening) oialn
Noun
Suffixation
ut h
.
aluciyut (pioneership) zeivelg
ay yomanay (duty officer) i`pnei
an ?egropan (boxer) otexb`
on pah
.
on (shack) oegt
iya marakiyah (soup tureen) diiwxn
it t
.
iyulit (open touring vehicle) zileih
a lomdah (courseware) dcnel
Template
maCCeC mas?neq (choke) wpyn
maCCeCa madgera (incubator) dxbcn
miCCaC mis?ap (branching) srqn
miCCaCa mignana (defensive fighting) dppbn
CeCeCa pelet
.
(output) hlt
tiCCoCet tiproset (distribution) zqextz
taCCiC tah
.
rit
.
(engraving) hixgz
taCCuCa tabru?ah (sanitation) d`exaz
miCCeCet micrepet (leotard) ztxvn
CCiC crir (dissonance) xixv
CaCCan bals?an (linguist) oyla
CaCeCet s?ah
.
emet (cirrhosis) zngy
CiCul t
.
ibu? (ringing) reaih
haCCaCa hanpas?a (animation) dytpd
heCCeC het?em (agreement) m`zd
Adjective
Suffixationb
i nora?i (awful) i`xep
ani yeh
.
idani (individual) ipcigi
oni t
.
elewizyonic (televisional) ipeifieelh
a?i yed
.
ida?i (unique) i`cigi
ali st
.
udentiali (student) il`ihpcehq
Template C1C2aC3C2aC3
d metaqtaq (sweetish) wzwzn
CaCuC rapus (flaccid ) qetx
Adverb Suffixation
ot qcarot (briefly) zexvw
it miyadit (immediately) zicin
Prefixation b bekeip (with fun) sika
aCoCeC variation: wzer ?wyeq (a copy).
bThe feminine form is made by the t and iya suffixes: ipcigi yeh
.
idanit (individual), dixvep nwcriya (Christian).
cIn the feminine form, the last h of the original noun is omitted.
dC1C2aC3C2oC3 variation: oehphw qt.ant.wn (tiny).
Table 3: Common Hebrew Neologism Formations.
733
Model Analysis Set MorphologicalDisambiguationCoverage Ambiguity Probability
Baseline 50.8% 1.5 0.48 57.3%
Pattern 82.8% 20.4 0.10 66.8%
Letter 76.7% 5.9 0.32 69.1%
Pattern-Letter 84.1% 10.4 0.25 69.8%
WordContext-Pattern 84.4% 21.7 0.12 66.5%
TagContext-Pattern 85.3% 23.5 0.19 64.9%
WordContext-Letter 80.7% 7.94 0.30 69.7%
TagContext-Letter 83.1% 7.8 0.22 66.9%
WordContext-Pattern-Letter 85.2% 12.0 0.24 68.8%
TagContext-Pattern-Letter 86.1% 14.3 0.18 62.1%
Table 4: Evaluation of unknown token full morphological analysis.
of the words appearing in that context, and similarly
the probability of a tag given a word is the averaged
probability of that tag in all the (reliable) contexts
in which the word appears. We use the function
allow(t, w) to control the tags (ambiguity class) al-
lowed for each word, as given by the lexicon.
For a given word wi in a sentence, we examine
two types of contexts: word context wi?1, wi+1,
and tag context ti?1, ti+1. For the case of word con-
text, the estimation of p(w|c) and p(c|w) is simply
the relative frequency over all the events w1, w2, w3
occurring at least 10 times in the corpus. Since the
corpus is not tagged, the relative frequency of the
tag contexts is not observed, instead, we use the
context-free approximation of each word-tag, in or-
der to determine the frequency weight of each tag
context event. For example, given the sequence
icnl ziznerl daebz tgubah l?umatit lmadai (a quite
oppositional response), and the analyses set pro-
duced by the context-free approximation: tgubah
[NN 1.0] l?umatit [] lmadai [RB 0.8, P1-NN 0.2].
The frequency weight of the context {NN RB} is
1 ? 0.8 = 0.8 and the frequency weight of the con-
text {NN P1-NN} is 1 ? 0.2 = 0.2.
4 Evaluation
For testing, we manually tagged the text which is
used in the Hebrew Treebank (consisting of about
90K tokens), according to our tagging guideline (?).
We measured the effectiveness of the three mod-
els with respect to the tags that were assigned to the
unknown tokens in our test corpus (the ?correct tag?),
according to three parameters: (1) The coverage of
the model, i.e., we count cases where p(t|w) con-
tains the correct tag with a probability larger than
0.01; (2) the ambiguity level of the model, i.e., the
average number of analyses suggested for each to-
ken; (3) the average probability of the ?correct tag?,
according to the predicted p(t|w). In addition, for
each experiment, we run the full morphology dis-
ambiguation system where unknowns are analyzed
according by the model.
Our baseline proposes the most frequent tag
(proper name) for all possible segmentations of the
token, in a uniform distribution. We compare the
following models: the 3 context free models (pat-
terns, letters and the combined patterns and letters)
and the same models combined with the word and
tag context models. Note that the context models
have low coverage (about 40% for the word context
and 80% for the tag context models), and therefore,
the context models cannot be used on their own. The
highest coverage is obtained for the combined model
(tag context, pattern, letter) at 86.1%.
We first show the results for full morphological
disambiguation, over 3,561 distinct tags in Table 4.
The highest coverage is obtained for the model com-
bining the tag context, patterns and letters models.
The tag context model is more effective because
it covers 80% of the unknown words, whereas the
word context model only covers 40%. As expected,
our simple baseline has the highest precision, since
the most frequent proper name tag covers over 50%
of the unknown words. The eventual effectiveness of
734
Model Analysis Set POS TaggingCoverage Ambiguity Probability
Baseline 52.9% 1.5 0.52 60.6%
Pattern 87.4% 8.7 0.19 76.0%
Letter 80% 4.0 0.39 77.6%
Pattern-Letter 86.7% 6.2 0.32 78.5%
WordContext-Pattern 88.7% 8.8 0.21 75.8%
TagContext-Pattern 89.5% 9.1 0.14 73.8%
WordContext-Letter 83.8% 4.5 0.37 78.2%
TagContext-Letter 87.1% 5.7 0.28 75.2%
WordContext-Pattern-Letter 87.8 6.5 0.32 77.5%
TagContext-Pattern-Letter 89.0% 7.2 0.25 74%
Table 5: Evaluation of unknown token POS tagging.
the method is measured by its impact on the eventual
disambiguation of the unknown words. For full mor-
phological disambiguation, our method achieves an
error reduction of 30% (57% to 70%). Overall, with
the level of 4.5% of unknown words observed in our
corpus, the algorithm we have developed contributes
to an error reduction of 5.5% for full morphological
disambiguation.
The best result is obtained for the model com-
bining pattern and letter features. However, the
model combining the word context and letter fea-
tures achieves almost identical results. This is an
interesting result, as the pattern features encapsulate
significant linguistic knowledge, which apparently
can be approximated by a purely distributional ap-
proximation.
While the disambiguation level of 70% is lower
than the rate of 85% achieved in English, it must
be noted that the task of full morphological disam-
biguation in Hebrew is much harder ? we manage
to select one tag out of 3,561 for unknown words as
opposed to one out of 46 in English. Table 5 shows
the result of the disambiguation when we only take
into account the POS tag of the unknown tokens.
The same models reach the best results in this case
as well (Pattern+Letters and WordContext+Letters).
The best disambiguation result is 78.5% ? still much
lower than the 85% achieved in English. The main
reason for this lower level is that the task in He-
brew includes segmentation of prefixes and suffixes
in addition to POS classification. We are currently
investigating models that will take into account the
specific nature of prefixes in Hebrew (which encode
conjunctions, definite articles and prepositions) to
better predict the segmentation of unknown words.
5 Conclusion
We have addressed the task of computing the distri-
bution p(t|w) for unknown words for full morpho-
logical disambiguation in Hebrew. The algorithm
we have proposed is language independent: it ex-
ploits a maximum entropy letters model trained over
the known words observed in the corpus and the dis-
tribution of the unknown words in known tag con-
texts, through iterative approximation. The algo-
rithm achieves 30% error reduction on disambigua-
tion of unknown words over a competitive baseline
(to a level of 70% accurate full disambiguation of
unknown words). We have also verified that tak-
ing advantage of a strong language-specific model
of morphological patterns provides the same level
of disambiguation. The algorithm we have devel-
oped exploits distributional information latent in a
wide-coverage lexicon and large quantities of unla-
beled data.
We observe that the task of analyzing unknown to-
kens for POS in Hebrew remains challenging when
compared with English (78% vs. 85%). We hy-
pothesize this is due to the highly ambiguous pattern
of prefixation that occurs widely in Hebrew and are
currently investigating syntagmatic models that ex-
ploit the specific nature of agglutinated prefixes in
Hebrew.
735
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos-tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer, version 2.0.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proceeding of HLT-NAACL-
04.
Michael Elhadad, Yael Netzer, David Gabay, and Meni
Adler. 2005. Hebrew morphological tagging guide-
lines. Technical report, Ben-Gurion University, Dept.
of Computer Science.
Nizar Habash and Owen Rambow. 2006. Magead: A
morphological analyzer and generator for the arabic
dialects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 681?688, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In ACL07 Workshop on Computational Ap-
proaches to Semitic Languages, Prague, Czech Repub-
lic.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405?423.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th international
conference on Computational Linguistics, Geneva.
Raphael Nir. 1993. Word-Formation in Modern Hebrew.
The Open University of Israel, Tel-Aviv, Israel.
Uzi Ornan. 2002. Hebrew in Latin script. Le?s?one?nu,
LXIV:137?151. (in Hebrew).
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
736
Proceedings of ACL-08: HLT, pages 746?754,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
EM Can Find Pretty Good HMM POS-Taggers
(When Given a Good Start)?
Yoav Goldberg and Meni Adler and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,adlerm,elhadad}@cs.bgu.ac.il
Abstract
We address the task of unsupervised POS tag-
ging. We demonstrate that good results can be
obtained using the robust EM-HMM learner
when provided with good initial conditions,
even with incomplete dictionaries. We present
a family of algorithms to compute effective
initial estimations p(t|w). We test the method
on the task of full morphological disambigua-
tion in Hebrew achieving an error reduction of
25% over a strong uniform distribution base-
line. We also test the same method on the stan-
dard WSJ unsupervised POS tagging task and
obtain results competitive with recent state-of-
the-art methods, while using simple and effi-
cient learning methods.
1 Introduction
The task of unsupervised (or semi-supervised) part-
of-speech (POS) tagging is the following: given a
dictionary mapping words in a language to their pos-
sible POS, and large quantities of unlabeled text
data, learn to predict the correct part of speech for
a given word in context. The only supervision given
to the learning process is the dictionary, which in
a realistic scenario, contains only part of the word
types observed in the corpus to be tagged.
Unsupervised POS tagging has been traditionally
approached with relative success (Merialdo, 1994;
Kupiec, 1992) by HMM-based generative mod-
els, employing EM parameters estimation using the
Baum-Welch algorithm. However, as recently noted
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
by Banko and Moore (2004), these works made use
of filtered dictionaries: dictionaries in which only
relatively probable analyses of a given word are pre-
served. This kind of filtering requires serious su-
pervision: in theory, an expert is needed to go over
the dictionary elements and filter out unlikely anal-
yses. In practice, counts from an annotated corpus
have been traditionally used to perform the filtering.
Furthermore, these methods require rather compre-
hensive dictionaries in order to perform well.
In recent work, researchers try to address these
deficiencies by using dictionaries with unfiltered
POS-tags, and testing the methods on ?diluted dic-
tionaries? ? in which many of the lexical entries are
missing (Smith and Eisner, 2005) (SE), (Goldwater
and Griffiths, 2007) (GG), (Toutanova and Johnson,
2008) (TJ).
All the work mentioned above focuses on unsu-
pervised English POS tagging. The dictionaries are
all derived from tagged English corpora (all recent
work uses the WSJ corpus). As such, the setting of
the research is artificial: there is no reason to per-
form unsupervised learning when an annotated cor-
pus is available. The problem is rather approached
as a workbench for exploring new learning methods.
The result is a series of creative algorithms, that have
steadily improved results on the same dataset: unsu-
pervised CRF training using contrastive estimation
(SE), a fully-bayesian HMM model that jointly per-
forms clustering and sequence learning (GG), and
a Bayesian LDA-based model using only observed
context features to predict tag words (TJ). These so-
phisticated learning algorithms all outperform the
traditional baseline of EM-HMM based methods,
746
while relying on similar knowledge: the lexical con-
text of the words to be tagged and their letter struc-
ture (e.g., presence of suffixes, capitalization and
hyphenation).1
Our motivation for tackling unsupervised POS
tagging is different: we are interested in develop-
ing a Hebrew POS tagger. We have access to a good
Hebrew lexicon (and a morphological analyzer), and
a fair amount of unlabeled training data, but hardly
any annotated corpora. We actually report results
on full morphological disambiguation for Hebrew, a
task similar but more challenging than POS tagging:
we deal with a tagset much larger than English (over
3,561 distinct tags) and an ambiguity level of about
2.7 per token as opposed to 1.4 for English. Instead
of inventing a new learning framework, we go back
to the traditional EM trained HMMs. We argue that
the key challenge to learning an effective model is
to define good enough initial conditions. Given suf-
ficiently good initial conditions, EM trained models
can yield highly competitive results. Such models
have other benefits as well: they are simple, robust,
and computationally more attractive.
In this paper, we concentrate on methods for de-
riving sufficiently good initial conditions for EM-
HMM learning. Our method for learning initial con-
ditions for the p(t|w) distributions relies on a mix-
ture of language specific models: a paradigmatic
model of similar words (where similar words are
words with similar inflection patterns), simple syn-
tagmatic constraints (e.g., the sequence V-V is ex-
tremely rare in English). These are complemented
by a linear lexical context model. Such models are
simple to build and test.
We present results for unsupervised PoS tagging
of Hebrew text and for the common WSJ English
test sets. We show that our method achieves state-of-
the-art results for the English setting, even with a rel-
atively small dictionary. Furthermore, while recent
work report results on a reduced English tagset of
17 PoS tags, we also present results for the complete
45 tags tagset of the WSJ corpus. This considerably
raises the bar of the EM-HMM baseline. We also
report state-of-the-art results for Hebrew full mor-
1Another notable work, though within a slightly differ-
ent framework, is the prototype-driven method proposed by
(Haghighi and Klein, 2006), in which the dictionary is replaced
with a very small seed of prototypical examples.
phological disambiguation.
Our primary conclusion is that the problem of
learning effective stochastic classifiers remains pri-
marily a search task. Initial conditions play a domi-
nant role in solving this task and can rely on linguis-
tically motivated approximations. A robust learn-
ing method (EM-HMM) combined with good initial
conditions based on a robust feature set can go a
long way (as opposed to a more complex learning
method). It seems that computing initial conditions
is also the right place to capture complex linguistic
intuition without fear that over-generalization could
lead a learner to diverge.
2 Previous Work
The tagging accuracy of supervised stochastic tag-
gers is around 96%?97% (Manning and Schutze,
1999). Merialdo (1994) reports an accuracy
of 86.6% for an unsupervised token-based EM-
estimated HMM, trained on a corpus of about 1M
words, over a tagset of 159 tags. Elworthy (1994), in
contrast, reports accuracy of 75.49%, 80.87%, and
79.12% for unsupervised word-based HMM trained
on parts of the LOB corpora, with a tagset of 134
tags. With (artificially created) good initial condi-
tions, such as a good approximation of the tag distri-
bution for each word, Elworthy reports an improve-
ment to 94.6%, 92.27%, and 94.51% on the same
data sets. Merialdo, on the other hand, reports an im-
provement to 92.6% and 94.4% for the case where
100 and 2,000 sentences of the training corpus are
manually tagged. Later, Banko and Moore (2004)
observed that earlier unsupervised HMM-EM re-
sults were artificially high due to use of Optimized
Lexicons, in which only frequent-enough analyses
of each word were kept. Brill (1995b) proposed
an unsupervised tagger based on transformation-
based learning (Brill, 1995a), achieving accuracies
of above 95%. This unsupervised tagger relied on
an initial step in which the most probable tag for
each word is chosen. Optimized lexicons and Brill?s
most-probable-tag Oracle are not available in realis-
tic unsupervised settings, yet, they show that good
initial conditions greatly facilitate learning.
Recent work on unsupervised POS tagging for
English has significantly improved the results on this
task: GG, SE and most recently TJ report the best re-
747
sults so far on the task of unsupervised POS tagging
of the WSJ with diluted dictionaries. With dictionar-
ies as small as 1249 lexical entries the LDA-based
method with a strong ambiguity-class model reaches
POS accuracy as high as 89.7% on a reduced tagset
of 17 tags.
While these 3 methods rely on the same feature
set (lexical context, spelling features) for the learn-
ing stage, the LDA approach bases its predictions
entirely on observable features, and excludes the tra-
ditional hidden states sequence.
In Hebrew, Levinger et al (1995) introduced the
similar-words algorithm for estimating p(t|w) from
unlabeled data, which we describe below. Our
method uses this algorithm as a first step, and refines
the approximation by introducing additional linguis-
tic constraints and an iterative refinement step.
3 Initial Conditions For EM-HMM
The most common model for unsupervised learning
of stochastic processes is Hidden Markov Models
(HMM). For the case of tagging, the states corre-
spond to the tags ti, and words wi are emitted each
time a state is visited. The parameters of the model
can be estimated by applying the Baum-Welch EM
algorithm (Baum, 1972), on a large-scale corpus of
unlabeled text. The estimated parameters are then
used in conjunction with Viterbi search, to find the
most probable sequence of tags for a given sentence.
In this work, we follow Adler (2007) and use a vari-
ation of second-order HMM in which the probability
of a tag is conditioned by the tag that precedes it and
by the one that follows it, and the probability of an
emitted word is conditioned by its tag and the tag
that follows it2. In all experiments, we use the back-
off smoothing method of (Thede and Harper, 1999),
with additive smoothing (Chen, 1996) for the lexical
probabilities.
We investigate methods to approximate the initial
parameters of the p(t|w) distribution, from which
we obtain p(w|t) by marginalization and Bayesian
inversion. We also experiment with constraining the
p(t|t?1, t+1) distribution.
2Technically this is not Markov Model but a Dependency
Net. However, bidirectional conditioning seem more suitable
for language tasks, and in practice the learning and inference
methods are mostly unaffected. See (Toutanova et al, 2003).
General syntagmatic constraints We set linguis-
tically motivated constraints on the p(t|t?1, t+1)
distribution. In our setting, these are used to force
the probability of some events to 0 (e.g., ?Hebrew
verbs can not be followed by the of preposition?).
Morphology-based p(t|w) approximation
Levinger et al (1995) developed a context-free
method for acquiring morpho-lexical probabilities
(p(t|w)) from an untagged corpus. The method is
based on language-specific rules for constructing a
similar words (SW) set for each analysis of a word.
This set is composed of morphological variations
of the word under the given analysis. For example,
the Hebrew token ??? can be analyzed as either a
noun (boy) or a verb (gave birth). The noun SW set
for this token is composed of the definiteness and
number inflections ????,?????,?????? (the boy, boys,
the boys), while the verb SW set is composed
of gender and tense inflections ????,???? (she/they
gave birth). The approximated probability of each
analysis is based on the corpus frequency of its SW
set. For the complete details, refer to the original
paper. Cucerzan and Yarowsky (2000) proposed
a similar method for the unsupervised estimation
of p(t|w) in English, relying on simple spelling
features to characterize similar word classes.
Linear-Context-based p(t|w) approximation
The method of Levinger et al makes use of Hebrew
inflection patterns in order to estimate context free
approximation of p(t|w) by relating a word to its
different inflections. However, the context in which
a word occurs can also be very informative with
respect to its POS-analysis (Schu?tze, 1995). We
propose a novel algorithm for estimating p(t|w)
based on the contexts in which a word occurs.3
The algorithm starts with an initial p(t|w) esti-
mate, and iteratively re-estimates:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?RELC p(t|c)p(c|w)allow(t, w)
Z
3While we rely on the same intuition, our use of context
differs from earlier works on distributional POS-tagging like
(Schu?tze, 1995), in which the purpose is to directly assign the
possible POS for an unknown word. In contrast, our algorithm
aims to improve the estimate for the whole distribution p(t|w),
to be further disambiguated by the EM-HMM learner.
748
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of all contexts,
andRELC ? C is a set of reliable contexts, defined
below. allow(t, w) is a binary function indicating
whether t is a valid tag for w. p(c|w) and p(w|c) are
estimated via raw corpus counts.
Intuitively, we estimate the probability of a tag
given a context as the average probability of a tag
given any of the words appearing in that context, and
similarly the probability of a tag given a word is the
averaged probability of that tag in all the (reliable)
contexts in which the word appears. At each round,
we define RELC , the set of reliable contexts, to be
the set of all contexts in which p(t|c) > 0 for at most
X different ts.
The method is general, and can be applied to dif-
ferent languages. The parameters to specify for each
language are: the initial estimation p(t|w), the esti-
mation of the allow relation for known and OOV
words, and the types of contexts to consider.
4 Application to Hebrew
In Hebrew, several words combine into a single to-
ken in both agglutinative and fusional ways. This
results in a potentially high number of tags for each
token. On average, in our corpus, the number of pos-
sible analyses per known word reached 2.7, with the
ambiguity level of the extended POS tagset in cor-
pus for English (1.41) (Dermatas and Kokkinakis,
1995).
In this work, we use the morphological analyzer
of MILA ? Knowledge Center for Processing He-
brew (KC analyzer). In contrast to English tagsets,
the number of tags for Hebrew, based on all com-
binations of the morphological attributes, can grow
theoretically to about 300,000 tags. In practice, we
found ?only? about 3,560 tags in a corpus of 40M
tokens training corpus taken from Hebrew news ma-
terial and Knesset transcripts. For testing, we man-
ually tagged the text which is used in the Hebrew
Treebank (Sima?an et al, 2001) (about 90K tokens),
according to our tagging guidelines.
4.1 Initial Conditions
General syntagmatic constraints We define 4
syntagmatic constraints over p(t|t?1, t+1): (1) a
construct state form cannot be followed by a verb,
preposition, punctuation, existential, modal, or cop-
ula; (2) a verb cannot be followed by the preposition
?? s?el (of), (3) copula and existential cannot be fol-
lowed by a verb, and (4) a verb cannot be followed
by another verb, unless one of them has a prefix, or
the second verb is an infinitive, or the first verb is
imperative and the second verb is in future tense.4
Morphology-Based p(t|w) approximation We
extended the set of rules used in Levinger et al , in
order to support the wider tagset used by the KC an-
alyzer: (1) The SW set for adjectives, copulas, exis-
tentials, personal pronouns, verbs and participles, is
composed of all gender-number inflections; (2) The
SW set for common nouns is composed of all num-
ber inflections, with definite article variation for ab-
solute noun; (3) Prefix variations for proper nouns;
(4) Gender variation for numerals; and (5) Gender-
number variation for all suffixes (possessive, nomi-
native and accusative).
Linear-Context-based p(t|w) approximation
For the initial p(t|w) we use either a uniform distri-
bution based on the tags allowed in the dictionary,
or the estimate obtained by using the modified
Levinger et al algorithm. We use contexts of the
form LR=w?1, w+1 (the neighbouring words). We
estimate p(w|c) and p(c|w) via relative frequency
over all the events w1, w2, w3 occurring at least
10 times in the corpus. allow(t, w) follows the
dictionary. Because of the wide coverage of the
Hebrew lexicon, we take RELC to be C (all
available contexts).
4.2 Evaluation
We run a series of experiments with 8 distinct ini-
tial conditions, as shown in Table 1: our baseline
(Uniform) is the uniform distribution over all tags
provided by the KC analyzer for each word. The
Syntagmatic initial conditions add the p(t|t?1, t+1)
constraints described above to the uniform base-
line. The Morphology-Based and Linear-Context
initial conditions are computed as described above,
while the Morph+Linear is the result of applying
the linear-context algorithm over initial values com-
puted by the Morphology-based method. We repeat
4This rule was taken from Shacham and Wintner(2007).
749
Initial Condition Dist Context-Free EM-HMMFull Seg+Pos Full Seg+Pos
Uniform 60 63.8 71.9 85.5 89.8
Syntagmatic Pair Constraints 60 / / 85.8 89.8Init-Trans 60 / / 87.9 91
Morpho-Lexical
Morph-Based 76.8 76.4 83.1 87.7 91.6
Linear-Context 70.1 75.4 82.6 85.3 89.6
Morph+Linear 79.8 79.0 85.5 88 92
PairConst+Morph
Morph-Based / / / 87.6 91.4
Linear-Context / / / 84.5 89.0
Morph+Linear / / / 87.1 91.5
InitTrans+Morph
Morph-Based / / / 89.2 92.3
Linear-Context / / / 87.7 90.9
Morph+Linear / / / 89.4 92.4
Table 1: Accuracy (%) of Hebrew Morphological
Disambiguation and POS Tagging over various initial
conditions
these last 3 models with the addition of the syntag-
matic constraints (Synt+Morph).
For each of these, we first compare the computed
p(t|w) against a gold standard distribution, taken
from the test corpus (90K tokens), according to the
measure used by (Levinger et al, 1995) (Dist). On
this measure, we confirm that our improved morpho-
lexical approximation improves the results reported
by Levinger et al from 74% to about 80% on a
richer tagset, and on a much larger test set (90K vs.
3,400 tokens).
We then report on the effectiveness of p(t|w) as
a context-free tagger that assigns to each word the
most likely tag, both for full morphological analy-
sis (3,561 tags) (Full) and for the simpler task of
token segmentation and POS tag selection (36 tags)
(Seg+Pos). The best results on this task are 80.8%
and 87.5% resp. achieved on the Morph+Linear ini-
tial conditions.
Finally, we test effectiveness of the initial con-
ditions with EM-HMM learning. We reach 88%
accuracy on full morphological and 92% accuracy
for POS tagging and word segmentation, for the
Morph+Linear initial conditions.
As expected, EM-HMM improves results (from
80% to 88%). Strikingly, EM-HMM improves the
uniform initial conditions from 64% to above 85%.
However, better initial conditions bring us much
over this particular local maximum ? with an error
reduction of 20%. In all cases, the main improve-
ment over the uniform baseline is brought by the
morphology-based initial conditions. When applied
on its own, the linear context brings modest im-
provement. But the combination of the paradigmatic
morphology-based method with the linear context
improves all measures.
A most interesting observation is the detrimental
contribution of the syntagmatic constraints we in-
troduced. We found that 113,453 sentences of the
corpus (about 5%) contradict these basic and ap-
parently simple constraints. As an alternative to
these common-sense constraints, we tried to use a
small seed of randomly selected sentences (10K an-
notated tokens) in order to skew the initial uniform
distribution of the state transitions. We initialize the
p(t|t?1, t+1) distribution with smoothed ML esti-
mates based on tag trigram and bigram counts (ig-
noring the tag-word annotations). This small seed
initialization (InitTrans) has a great impact on ac-
curacy. Overall, we reach 89.4% accuracy on full
morphological and 92.4% accuracy for POS tagging
and word segmentation, for the Morph+Linear con-
ditions ? an error reduction of more than 25% from
the uniform distribution baseline.
5 Application to English
We now apply the same technique to English semi-
supervised POS tagging. Recent investigations of
this task use dictionaries derived from the Penn WSJ
corpus, with a reduced tag set of 17 tags5 instead of
the original 45-tags tagset. They experiment with
full dictionaries (containing complete POS informa-
tion for all the words in the text) as well as ?diluted?
dictionaries, from which large portions of the vo-
cabulary are missing. These settings are very dif-
ferent from those used for Hebrew: the tagset is
much smaller (17 vs. ?3,560) and the dictionaries
are either complete or extremely crippled. However,
for the sake of comparison, we have reproduced the
same experimental settings.
We derive dictionaries from the complete WSJ
corpus6, and the exact same diluted dictionaries used
in SE, TJ and GG.
5ADJ ADV CONJ DET ENDPUNC INPUNC LPUNC
RPUNC N POS PRT PREP PRT TO V VBG VBN WH
6The dictionary derived from the WSJ data is very noisy:
many of the stop words get wrong analyses stemming from tag-
ging mistakes (for instance, the word the has 6 possible analyses
in the data-derived dictionary, which we checked manually and
found all but DT erroneous). Such noise is not expected in a real
world dictionary, and our algorithm is not designed to accomo-
date it. We corrected the entries for the 20 most frequent words
in the corpus. This step could probably be done automatically,
but we consider it to be a non-issue in any realistic setting.
750
Syntagmatic Constraints We indirectly incor-
porated syntagmatic constraints through a small
change to the tagset. The 17-tags English tagset
allows for V-V transitions. Such a construction is
generally unlikely in English. By separating modals
from the rest of the verbs, and creating an addi-
tional class for the 5 be verbs (am,is,are,was,were),
we made such transition much less probable. The
new 19-tags tagset reflects the ?verb can not follow
a verb? constraint.
Morphology-Based p(t|w) approximation En-
glish morphology is much simpler compared to that
of Hebrew, making direct use of the Levinger con-
text free approximation impossible. However, some
morphological cues exist in English as well, in par-
ticular common suffixation patterns. We imple-
mented our morphology-based context-free p(t|w)
approximation for English as a special case of the
linear context-based algorithm described in Sect.3.
Instead of generating contexts based on neighboring
words, we generate them using the following 5 mor-
phological templates:
suff=S The word has suffix S (suff=ing).
L+suff=W,S The word appears just after word W ,
with suffix S (L+suff=have,ed).
R+suff=S,W The word appears just before wordW ,
with suffix S (R+suff=ing,to)
wsuf=S1,S2 The word suffix is S1, the same stem is
seen with suffix S2 (wsuf=,s).
suffs=SG The word stem appears with the SG group
of suffixes (suffs=ed,ing,s).
We consider a word to have a suffix only if the
word stem appears with a different suffix somewhere
in the text. We implemented a primitive stemmer
for extracting the suffixes while preserving a us-
able stem by taking care of few English orthogra-
phy rules (handling, e.g., , bigger ? big er, nicer
? nice er, happily ? happy ly, picnicking ? pic-
nic ing). For the immediate context W in the tem-
plates L+suff,R+suff, we consider only the 20 most
frequent tokens in the corpus.
Linear-Context-based p(t|w) approximation
We expect the context based approximation to be
particularly useful in English. We use the following
3 context templates: LL=w?2,w?1, LR=w?1,w+1
and RR=w+1,w+2. We estimate p(w|c) and p(c|w)
by relative frequency over word triplets occurring at
least twice in the unannotated training corpus.
Combined p(t|w) approximation This approx-
imation combines the morphological and linear
context approximations by using all the above-
mentioned context templates together in the iterative
process.
For all three p(t|w) approximations, we take
RELC to be contexts containing at most 4 tags.
allow(t, w) follows the dictionary for known words,
and is the set of all open-class POS for unknown
words. We take the initial p(t|w) for each w to be
uniform over all the dictionary specified tags for w.
Accordingly, the initial p(t|w) = 0 for w not in the
dictionary. We run the process for 8 iterations.7
Diluted Dictionaries and Unknown Words
Some of the missing dictionary elements are as-
signed a set of possible POS-tags and corresponding
probabilities in the p(t|w) estimation process. Other
unknown tokens remain with no analysis at the
end of the initial process computation. For these
missing elements, we assign an ambiguity class by
a simple ambiguity-class guesser, and set p(t|w)
to be uniform over all the tags in the ambiguity
class. Our ambiguity-class guesser assigns for each
word the set of all open-class tags that appeared
with the word suffix in the dictionary. The word
suffix is the longest (up to 3 characters) suffix of the
word that also appears in the top-100 suffixes in the
dictionary.
Taggers We test the resulting p(t|w) approxima-
tion by training 2 taggers: CF-Tag, a context-free
tagger assigning for each word its most probable
POS according to p(t|w), with a fallback to the most
probable tag in case the word does not appear in
the dictionary or if ?t, p(t|w) = 0. EM-HMM,
a second-order EM-HMM initialized with the esti-
mated p(t|w).
Baselines As baseline, we use two EM-trained
HMM taggers, initialized with a uniform p(t|w) for
every word, based on the allowed tags in the dic-
tionary. For words not in the dictionary, we take
the allowed tags to be either all the open-class POS
7This is the first value we tried, and it seems to work fine.
We haven?t experimented with other values. The same applies
for the choice of 4 as the RELC threshold.
751
(uniform(oc)) or the allowed tags according to our
simple ambiguity-class guesser (uniform(suf)).
All the p(t|w) estimates and HMM models are
trained on the entire WSJ corpus. We use the same
24K word test-set as used in SE, TJ and GG, as well
as the same diluted dictionaries. We report the re-
sults on the same reduced tagsets for comparison,
but also include the results on the full 46 tags tagset.
5.1 Results
Table 2 summarizes the results of our experiments.
Uniform initialization based on the simple suffix-
based ambiguity class guesser yields big improve-
ments over the uniform all-open-class initialization.
However, our refined initial conditions always im-
prove the results (by as much as 40% error re-
duction). As expected, the linear context is much
more effective than the morphological one, espe-
cially with richer dictionaries. This seem to indi-
cate that in English the linear context is better at re-
fining the estimations when the ambiguity classes
are known, while the morphological context is in
charge of adding possible tags when the ambigu-
ity classes are not known. Furthermore, the bene-
fit of the morphology-context is bigger for the com-
plete tagset setting, indicating that, while the coarse-
grained POS-tags are indicated by word distribu-
tion, the finer distinctions are indicated by inflec-
tions and orthography. The combination of linear
and morphology contexts is always beneficial. Syn-
tagmatic constraints (e.g., separating be verbs and
modals from the rest of the verbs) constantly im-
prove results by about 1%. Note that the context-free
tagger based on our p(t|w) estimates is quite accu-
rate. As with the EM trained models, combining lin-
ear and morphological contexts is always beneficial.
To put these numbers in context, Table 3 lists
current state-of-the art results for the same task.
CE+spl is the Contrastive-Estimation CRF method
of SE. BHMM is the completely Bayesian-HMM
of GG. PLSA+AC, LDA, LDA+AC are the mod-
els presented in TJ, LDA+AC is a Bayesian model
with a strong ambiguity class (AC) component, and
is the current state-of-the-art of this task. The other
models are variations excluding the Bayesian com-
ponents (PLSA+AC) or the ambiguity class.
While our models are trained on the unannotated
text of the entire WSJ Treebank, CE and BHMM use
much less training data (only the 24k words of the
test-set). However, as noted by TJ, there is no reason
one should limit the amount of unlabeled data used,
and in addition other results reported in GG,SE show
that accuracy does not seem to improve as more un-
labeled data are used with the models. We also re-
port results for training our EM-HMM tagger on the
smaller dataset (the p(t|w) estimation is still based
on the entire unlabeled WSJ).
All the abovementioned models follow the as-
sumption that all 17 tags are valid for the unknown
words. In contrast, we restrict the set of allowed
tags for an unknown word to open-class tags. Closed
class words are expected to be included in a dictio-
nary, even a small one. The practice of allowing only
open-class tags for unknown words goes back a long
way (Weischedel et al, 1993), and proved highly
beneficial also in our case.
Notice that even our simplest models, in which
the initial p(t|w) distribution for each w is uniform,
already outperform most of the other models, and,
in the case of the diluted dictionaries, by a wide
margin. Similarly, given the p(t|w) estimate, EM-
HMM training on the smaller dataset (24k) is still
very competitive (yet results improve with more un-
labeled data). When we use our refined p(t|w) dis-
tribution as the basis of EM-HMM training, we get
the best results for the complete dictionary case.
With the diluted dictionaries, we are outperformed
only by LDA+AC. As we outperform this model in
the complete dictionary case, it seems that the ad-
vantage of this model is due to its much stronger
ambiguity class model, and not its Bayesian com-
ponents. Also note that while we outperform this
model when using the 19-tags tagset, it is slightly
better in the original 17-tags setting. It could be that
the reliance of the LDA models on observed surface
features instead of hidden state features is beneficial
avoiding the misleading V-V transitions.
We also list the performance of our best mod-
els with a slightly more realistic dictionary setting:
we take our dictionary to include information for all
words occurring in section 0-18 of the WSJ corpus
(43208 words). We then train on the entire unanno-
tated corpus, and test on sections 22-24 ? the stan-
dard train/test split for supervised English POS tag-
ging. We achieve accuracy of 92.85% for the 19-
tags set, and 91.3% for the complete 46-tags tagset.
752
Initial Conditions Full dict ? 2 dict ? 3 dict
(49206 words) (2141 words) (1249 words)
CF-Tag EM-HMM CF-Tag EM-HMM CF-Tag EM-HMM
Uniform(oc) 81.7 88.7 68.4 81.9 62.5 79.6
Uniform(suf) NA NA 76.8 83.4 76.9 81.6
17tags Morph-Cont 82.2 88.6 73.3 83.9 69.1 81.7
Linear-Cont 90.1 92.9 81.1 87.8 78.3 85.8
Combined-Cont 89.9 93.3 83.1 88.5 81.1 86.4
Uniform(oc) 79.9 91.0 66.6 83.4 60.7 84.7
Uniform(suf) NA NA 75.1 86.5 73.1 86.7
19tags Morph-Cont 80.5 89.2 71.5 86.5 67.5 87.1
Linear-Cont 88.4 93.7 78.9 89.0 76.3 86.9
Combined-Cont 88.0 93.8 81.1 89.4 79.2 87.4
Uniform(oc) 76.7 88.3 61.2 * 55.7 *
Uniform(suf) NA NA 64.2 81.9 60.3 79.8
46tags Morph-Cont 74.8 88.8 65.6 83.0 61.9 80.3
Linear-Cont 85.5 91.2 74.5 84.0 70.1 82.2
Combined-Cont 85.9 91.4 75.4 85.5 72.4 83.3
Table 2: Accuracy (%) of English POS Tagging over various initial conditions
Dict InitEM-HMM (24k) LDA LDA+AC PLSA+AC CE+spl BHMM
Full 93.8 (91.1) 93.4 93.4 89.7 88.7 87.3
? 2 89.4 (87.9) 87.4 91.2 87.8 79.5 79.6
? 3 87.4 (85.9) 85 89.7 85.9 78.4 71
Table 3: Comparison of English Unsupervised POS Tagging Methods
6 Conclusion
We have demonstrated that unsupervised POS tag-
ging can reach good results using the robust EM-
HMM learner when provided with good initial con-
ditions, even with incomplete dictionaries. We pre-
sented a general family of algorithms to compute ef-
fective initial conditions: estimation of p(t|w) rely-
ing on an iterative process shifting probabilities be-
tween words and their contexts. The parameters of
this process (definition of the contexts and initial es-
timations of p(t|w) can safely encapsulate rich lin-
guistic intuitions.
While recent work, such as GG, aim to use the
Bayesian framework and incorporate ?linguistically
motivated priors?, in practice such priors currently
only account for the fact that language related dis-
tributions are sparse - a very general kind of knowl-
edge. In contrast, our method allow the incorpora-
tion of much more fine-grained intuitions.
We tested the method on the challenging task
of full morphological disambiguation in Hebrew
(which was our original motivation) and on the stan-
dard WSJ unsupervised POS tagging task.
In Hebrew, our model includes an improved ver-
sion of the similar words algorithm of (Levinger et
al., 1995), a model of lexical context, and a small
set of tag ngrams. The combination of these knowl-
edge sources in the initial conditions brings an error
reduction of more than 25% over a strong uniform
distribution baseline. In English, our model is com-
petitive with recent state-of-the-art results, while us-
ing simple and efficient learning methods.
The comparison with other algorithms indicates
directions of potential improvement: (1) our initial-
conditions method might benefit the other, more so-
phisticated learning algorithms as well. (2) Our
models were designed under the assumption of a
relatively complete dictionary. As such, they are
not very good at assigning ambiguity-classes to
OOV tokens when starting with a very small dic-
tionary. While we demonstrate competitive results
using a simple suffix-based ambiguity-class guesser
which ignores capitalization and hyphenation infor-
mation, we believe there is much room for improve-
ment in this respect. In particular, (Haghighi and
Klein, 2006) presents very strong results using a
distributional-similarity module and achieve impres-
sive tagging accuracy while starting with a mere
116 prototypical words. Experimenting with com-
bining similar models (as well as TJ?s ambiguity
class model) with our p(t|w) distribution estimation
method is an interesting research direction.
753
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of Coling
2004, pages 556?561, Geneva, Switzerland, Aug 23?
Aug 27. COLING.
Leonard E. Baum. 1972. An inequality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of a Markov process. In-
equalities, 3:1?8.
Eric Brill. 1995a. Transformation-based error-driven
learning and natural languge processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543?565.
Eric Brill. 1995b. Unsupervised learning of disam-
biguation rules for part of speech tagging. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
1?13, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Stanley F. Chen. 1996. Building Probabilistic Models for
Natural Language. Ph.D. thesis, Harvard University,
Cambridge, MA.
Silviu Cucerzan and David Yarowsky. 2000. Language
independent, minimally supervised induction of lex-
ical probabilities. In ACL ?00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 270?277, Morristown, NJ,
USA. Association for Computational Linguistics.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
David Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceeding of ANLP-94.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In Proceeding of ACL 2007, Prague,
Czech Republic.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320?
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
J. Kupiec. 1992. Robust part-of-speech tagging using
hidden Markov model. Computer Speech and Lan-
guage, 6:225?242.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundation of Statistical Language Processing. MIT
Press.
Bernard Merialdo. 1994. Tagging English text
with probabilistic model. Computational Linguistics,
20:155?171.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
on European chapter of the Association for Computa-
tional Linguistics, pages 141?148, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical disambiguation of hebrew: A case study in
classifier combination. In Proceeding of EMNLP-07,
Prague, Czech.
Khalil Sima?an, Alon Itai, Alon Altman Yoad Winter,
and Noa Nativ. 2001. Building a tree-bank of mod-
ern Hebrew text. Journal Traitement Automatique des
Langues (t.a.l.). Special Issue on NLP and Corpus
Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354?362, Ann Arbor, Michigan, June.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20. MIT Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
754
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 237?240,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
splitSVM: Fast, Space-Efficient, non-Heuristic, Polynomial Kernel
Computation for NLP Applications
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,elhadad}@cs.bgu.ac.il
Abstract
We present a fast, space efficient and non-
heuristic method for calculating the decision
function of polynomial kernel classifiers for
NLP applications. We apply the method to
the MaltParser system, resulting in a Java
parser that parses over 50 sentences per sec-
ond on modest hardware without loss of accu-
racy (a 30 time speedup over existing meth-
ods). The method implementation is available
as the open-source splitSVM Java library.
1 Introduction
Over the last decade, many natural language pro-
cessing tasks are being cast as classification prob-
lems. These are then solved by of-the-shelf
machine-learning algorithms, resulting in state-of-
the-art results. Support Vector Machines (SVMs)
have gained popularity as they constantly outper-
form other learning algorithms for many NLP tasks.
Unfortunately, once a model is trained, the de-
cision function for kernel-based classifiers such as
SVM is expensive to compute, and can grow lin-
early with the size of the training data. In contrast,
the computational complexity for the decisions func-
tions of most non-kernel based classifiers does not
depend on the size of the training data, making them
orders of magnitude faster to compute. For this rea-
son, research effort was directed at speeding up the
classification process of polynomial-kernel SVMs
(Isozaki and Kazawa, 2002; Kudo and Matsumoto,
2003; Wu et al, 2007). Existing accelerated SVM
solutions, however, either require large amounts of
memory, or resort to heuristics ? computing only an
approximation to the real decision function.
This work aims at speeding up the decision func-
tion computation for low-degree polynomial ker-
nel classifiers while using only a modest amount of
memory and still computing the exact function. This
is achieved by taking into account the Zipfian nature
of natural language data, and structuring the compu-
tation accordingly. On a sample application (replac-
ing the libsvm classifier used by MaltParser (Nivre
et al, 2006) with our own), we observe a speedup
factor of 30 in parsing time.
2 Background and Previous Work
In classification based NLP algorithms, a word and
its context is considered a learning sample, and en-
coded as Feature Vectors. Usually, context data in-
cludes the word being classified (w0), its part-of-
speech (PoS) tag (p0), word forms and PoS tags of
neighbouring words (w?2, . . . , w+2, p?2, . . . , p+2,
etc.). Computed features such as the length of a
word or its suffix may also be added. A feature vec-
tor (F ) is encoded as an indexed list of all the fea-
tures present in the training corpus. A feature fi of
the form w+1 = dog means that the word follow-
ing the one being classified is ?dog?. Every learning
sample is represented by an n = |F | dimensional
binary vector x. xi = 1 iff the feature fi is active
in the given sample, 0 otherwise. n is the number
of different features being considered. This encod-
ing leads to vectors with extremely high dimensions,
mainly because of lexical features wi.
SVM is a supervised binary classifier. The re-
sult of the learning process is the set SV of Sup-
237
port Vectors, associated weights ?i, and a constant
b. The Support Vectors are a subset of the training
feature vectors, and together with the weights and b
they define a hyperplane that optimally separates the
training samples. The basic SVM formulation is of a
linear classifier, but by introducing a kernel function
K that non-linearly transforms the data fromRn into
a space of higher dimension, SVM can be used to
perform non-linear classification. SVM?s decision
function is:
y(x) = sgn
(?
j?SV yj?jK(xj , x) + b
)
where x is an n dimensional feature vector to
be classified. The kernel function we consider
in this paper is a polynomial kernel of degree d:
K(xi, xj) = (?xi ? xj + c)d. When using binary
valued features (with ? = 1 and c = 1), this kernel
function essentially implies that the classifier con-
siders not only the explicitly specified features, but
also all available sets of size d of features. For
d = 2, this means considering all feature pairs,
while for d = 3 all feature triplets. In practice, a
polynomial kernel with d = 2 usually yields the
best results in NLP tasks, while higher degree ker-
nels tend to overfit the data.
2.1 Decision Function Computation
Note that the decision function involves a summa-
tion over all support vectors xj in SV . In natu-
ral language applications, the size |SV | tends to be
very large (Isozaki and Kazawa, 2002), often above
10,000. In particular, the size of the support vectors
set can grow linearly with the number of training ex-
amples, of which there are usually at least tens of
thousands. As a consequence, the computation of
the decision function is computationally expensive.
Several approaches have been designed to speed up
the decision function computation.
Classifier Splitting is a common, application
specific heuristic, which is used to speed up the
training as well as the testing stages (Nivre et al,
2006). The training data is split into several datasets
according to an application specific heuristic. A sep-
arate classifier is then trained for each dataset. For
example, it might be known in advance that nouns
usually behave differently than verbs. In such a
case, one can train one classifier on noun instances,
and a different classifier on verb instances. When
testing, only one of the classifiers will be applied,
depending on the PoS of the word. This technique
reduces the number of support vectors in each clas-
sifier (because each classifier was trained on only a
portion of the data). However, it relies on human in-
tuition on the way the data should be split, and usu-
ally results in a degradation in performance relative
to a single classifier trained on all the data points.
PKI ? Inverted Indexing (Kudo and Matsumoto,
2003), stores for each feature the support vectors in
which it appears. When classifying a new sample,
only the set of vectors relevant to features actually
appearing in the sample are considered. This ap-
proach is non-heuristic and intuitively appealing, but
in practice brings only modest improvements.
Kernel Expansion (Isozaki and Kazawa, 2002)
is used to transform the d-degree polynomial kernel
based classifier into a linear one, with a modified
decision function y(x) = sgn(w ? xd + b). w is a
very high dimensional weight vector, which is cal-
culated beforehand from the set of support vectors
and their corresponding ?i values. (the calculation
details appear in (Isozaki and Kazawa, 2002; Kudo
and Matsumoto, 2003)). This speeds up the decision
computation time considerably, as only |x|d weights
need to be considered, |x| being the number of ac-
tive features in the sample to be classified, which
is usually a very small number. However, even the
sparse-representation version of w tends to be very
large: (Isozaki and Kazawa, 2002) report that some
of their second degree expanded NER models were
more than 80 times slower to load than the original
models (and 224 times faster to classify).1 This ap-
proach obviously does not scale well, both to tasks
with more features and to larger degree kernels.
PKE ? Heuristic Kernel Expansion, was intro-
duced by (Kudo and Matsumoto, 2003). This heuris-
tic method addresses the deficiency of the Kernel
Expansion method by using a basket-mining algo-
rithm in order to greatly reduce the number of non-
zero elements in the calculated w. A parameter is
used to control the number of non-zero elements in
w. The smaller the number, the smaller the memory
requirement, but setting this number too low hurts
classification performance, as only an approxima-
1Using a combination of 33 classifiers, the overall loading
time is about 31 times slower, and classification time is about
21 times faster, than the non-expanded classifiers.
238
tion of the real decision function is calculated.
?Semi Polynomial Kernel? was introduced by
(Wu et al, 2007). The intuition behind this opti-
mization is to ?extend the linear kernel SVM toward
polynomial?. It does not train a polynomial kernel
classifier, but a regular linear SVM. A basket-mining
based feature selection algorithm is used to select
?useful? pairs and triplets of features prior to the
training stage, and a linear classifier is then trained
using these features. Training (and testing) are faster
then in the polynomial kernel case, but the result suf-
fer quite a big loss in accuracy as well.2.
3 Fast, Non-Heuristic Computation
We now turn to present our fast, space efficient and
non-heuristic approach for computing the Polyno-
mial Kernel decision function.3 Our approach is a
combination of the PKI and the Kernel Expansion
methods. While previous works considered kernels
of the form K(x, y) = (x ? y + 1)d, we consider
the more general form of the polynomial kernel:
K(x, y) = (?x ? y + c)d.
Our key observation is that in NLP classifica-
tion tasks, few of the features (e.g., PoS is X,
or prev word is the) are very frequent, while
most others are extremely rare (e.g., next word
is polynomial). The common features are ac-
tive in many of the support-vectors, while the rare
features are active only in few support vectors. This
is true for most language related tasks: the Zipfian
nature of language phenomena is reflected in the dis-
tribution of features in the support vectors.
It is because of common features that the PKI re-
verse indexing method does not yield great improve-
ments: if at least one of the features of the current
instance is active in a support vector, this vector is
taken into account in the sum calculation, and the
common features are active in many support vectors.
On the other hand, the long tail of rare features
is the reason the Kernel Expansion methods requires
2This loss of accuracy in comparison to the PKE approach
is to be expected, as (Goldberg and Elhadad, 2007) showed that
the effect of removing features prior to the learning stage is
much more severe than removing them after the learning stage.
3Our presentation is for the case where d = 2, as this is by
far the most useful kernel. However, the method can be easily
adapted to higher degree kernels as well. For completeness, our
toolkit provides code for d = 3 as well as 2.
so much space: every rare feature adds many possi-
ble feature pairs.
We propose a combined method. We first split
common from rare features. We then use Kernel
Expansion on the few common features, and PKI
for the remaining rare features. This ensures small
memory footprint for the expanded kernel vector,
while at the same time keeping a low number of vec-
tors from the reverse index.
3.1 Formal Details
The polynomial kernel of degree 2 is: K(x, y) =
(?x ? y + c)2, where x and y are binary feature vec-
tors. x ?y is the dot product between the vectors, and
in the case of binary feature vectors it corresponds
to the count of shared features among the vectors. F
is the set of all possible features.
We define FR and FC to be the sets of rare and
common features. FR?FC = ?, FR?FC = F . The
mapping function ?R(x) zeros out all the elements
of x not belonging to FR, while ?C(x) zeroes out
all the elements of x not in FC . Thus, for every x:
?R(x)+?C(x) = x, ?R(x)??C(x) = 0. For brevity,
denote ?C(x) = xC , ?R(x) = xR.
We now rewrite the kernel function:
K(x, y) = K(xR + xC , yR + yC) =
= (?(xR + xC) ? (yR + yC) + c)
2
= (?xR ? yR + ?xC ? yC + c)
2
= (?xR ? yR)
2
+ 2?2(xR ? yR)(xC ? yC)
+ 2c?(xR ? yR)
+ (?(xC ? yC) + c)
2
The first 3 terms are non-zero only when at
least one rare feature exists. We denote their sum
KR(x, y). The last term involves only common fea-
tures. We denote it KC(x, y). Note that KC(x, y) is
the polynomial kernel of degree 2 over feature vec-
tors of only common features.
We can now write the SVM decision function as:
?
j?SV
yj?jKR(xj , xR) +
?
j?SV
yj?jKC(xj , xC) + b
We calculate the first sum via PKI, taking into ac-
count only support-vectors which share at least one
feature with xR. The second sum is calculated via
kernel expansion while taking into account only the
239
common features. Thus, only pairs of common fea-
tures appear in the resulting weight vector using the
same expansion as in (Kudo and Matsumoto, 2003;
Isozaki and Kazawa, 2002). In our case, however,
the expansion is memory efficient, because we con-
sider only features in FC , which is small.
Our approach is similar to the PKE approach
(Kudo and Matsumoto, 2003), which used a basket
mining approach to prune many features from the
expansion. In contrast, we use a simpler approach to
choose which features to include in the expansion,
and we also compensate for the feature we did not
include by the PKI method. Thus, our method gen-
erates smaller expansions while computing the exact
decision function and not an approximation of it.
We take every feature occurring in less than s sup-
port vectors to be rare, and the other features to be
common. By changing s we get a trade-of between
space and time complexity: smaller s indicate more
common features (bigger memory requirement) but
also less rare features (less support vectors to in-
clude in the summation), and vice-versa. In con-
trast to other methods, changing s is guaranteed not
to change the classification accuracy, as it does not
change the computed decision function.
4 Toolkit and Evaluation
Using this method, one can accelerate SVM-based
NLP application by just changing the classification
function, keeping the rest of the logic intact. We
implemented an open-source software toolkit, freely
available at http://www.cs.bgu.ac.il/?nlpproj/. Our
toolkit reads models created by popular SVM pack-
ages (libsvm, SVMLight, TinySVM and Yamcha)
and transforms them into our format. The trans-
formed models can then be used by our efficient Java
implementation of the method described in this pa-
per. We supply wrappers for the interfaces of lib-
svm and the Java bindings of SVMLight. Changing
existing Java code to accommodate our fast SVM
classifier is done by loading a different model, and
changing a single function call.
4.1 Evaluation: Speeding up MaltParser
We evaluate our method by using it as the classi-
fication engine for the Java version of MaltParser,
an SVM-based state of the art dependency parser
(Nivre et al, 2006). MaltParser uses the libsvm
classification engine. We used the pre-trained En-
glish models (based on sections 0-22 of the Penn
WSJ) supplied with MaltParser. MaltParser already
uses an effective Classifiers Splitting heuristic when
training these models, setting a high baseline for our
method. The pre-trained parser consists of hundreds
of different classifiers, some very small. We report
here on actual memory requirement and parsing time
for sections 23-24, considering the classifier combi-
nation. We took rare features to be those appear-
ing in less than 0.5% of the support vectors, which
leaves us with less than 300 common features in
each of the ?big? classifiers. The results are summa-
rized in Table 1. As can be seen, our method parses
Method Mem. Parsing Time Sents/Sec
Libsvm 240MB 2166 (sec) 1.73
ThisPaper 750MB 70 (sec) 53
Table 1: Parsing Time for WSJ Sections 23-24 (3762
sentences), on Pentium M, 1.73GHz
about 30 times faster, while using only 3 times as
much memory. MaltParser coupled with our fast
classifier parses above 3200 sentences per minute.
5 Conclusions
We presented a method for fast, accurate and mem-
ory efficient calculation for polynomial kernels de-
cisions functions in NLP application. While the
method is applied to SVMs, it generalizes to other
polynomial kernel based classifiers. We demon-
strated the method on the MaltParser dependency
parser with a 30-time speedup factor on overall pars-
ing time, with low memory overhead.
References
Y. Goldberg and M. Elhadad. 2007. SVM model tamper-
ing and anchored learning: A case study in hebrew. np
chunking. In Proc. of ACL2007.
H. Isozaki and H. Kazawa. 2002. Efficient support vector
classifiers for named entity recognition. In Proc. of
COLING2002.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
J. Nivre, J. Hall, and J. Nillson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC2006.
Y. Wu, J. Yang, and Y. Lee. 2007. An approximate ap-
proach for training polynomial kernel svms in linear
time. In Proc. of ACL2007 (short-paper).
240
Integrating a Large-scale, Reusable Lexicon with a Natural 
Language Generator 
Hongyan 3 ing  
Department of Computer  Science 
Columbia University 
New York, NY 10027, USA 
hjing@cs.columbia.edu 
Yael Dahan Netzer 
Department of Computer  Science 
Ben-Gurion University 
Be'er-Sheva, 84105, Israel 
yaeln@cs.bgu.ac.il 
Michael Elhadad 
Department  of Computer  Science 
Ben-Gurion University 
Be'er-Sheva, 84105, Israel 
elhadad@cs.bgu.ac.i l  
Kathleen R. McKeown 
Department of Computer  Science 
Columbia University 
New York, NY 10027, USA 
kathy@cs.columbia.edu 
Abst rac t  
This paper presents the integration of a large- 
scale, reusable lexicon for generation with the 
FUF/SURGE unification-based syntactic realizer. 
The lexicon was combined from multiple xisting re- 
sources in a semi-automatic process. The integra- 
tion is a multi-step unification process. This inte- 
gration allows the reuse of lexical, syntactic, and 
semantic knowledge ncoded in the lexicon in the 
development of lexical chooser module in a genera- 
tion system. The lexicon also brings other benefits 
to a generation system: for example, the ability to 
generate many lexical and syntactic paraphrases and 
the ability to avoid non-grammatical output. 
1 In t roduct ion  
Natural language generation requires lexical, syn- 
tactic, and semantic knowledge in order to produce 
meaningful and fluent output. Such knowledge is 
often hand-coded anew when a different application 
is developed. We present in this paper the integra- 
tion of a large-scale, reusable lexicon with a natural 
language generator, FUF/SURGE (Elhadad, 1992; 
Robin, 1994); we show that by integrating the lexi- 
con with FUF/SURGE as a tactical component, we 
can reuse the knowledge ncoded in the lexicon and 
automate to some extent he development of the lex- 
ical realization component in a generation applica- 
tion. 
The integration of the lexicon with FUF/SURGE 
also brings other benefits to generation, including 
the possibility to accept a semantic input at the 
level of WordNet synsets, the production of lexical 
and syntactic paraphrases, the prevention of non- 
grammatical output, reuse across applications, and 
wide coverage. 
We present he process of integrating the lexicon 
with FUF/SUR(;E. including how to represenl the 
lexicon in FUF format, how to unify input with the 
lexicon incrementally to generate more sophisticated 
and informative representations, and how to design 
an appropriate semantic input format so that the 
integration of the lexicon and FUF/SURGE can be 
done easily. 
This paper is organized as follows. In Section 2, 
we explain why a reusable lexical chooser for gen- 
eration needs to be developed. In Section 3, we 
present he large-scale, reusable lexicon which we 
combined from multiple resources, and illustrate its 
benefits to generation by examples. In Section 4, we 
describe the process of integrating the lexicon with 
FUF/SURGE, which includes four unification steps, 
with each step adding additional lexical or syntac- 
tic information. Other applications and comparison 
with related work are presented inSection 5. Finally, 
we conclude by discussing future work. 
2 Bu i ld ing  a reusab le  lex ica l  chooser  
for generat ion  
While reusable components have been widely used in 
generation applications, the concept of a "reusable 
lexical chooser" for generation remains novel. 
There are two main reasons why such a lexical 
chooser has not been developed in the past: 
1. In the overall architecture of a generator, the 
lexical chooser is an internal component that 
depends on the semantic representation a d for- 
.:malism and onthe syntactic realizer used by the 
application. 
2. The lexical chooser links conceptual e ements to 
lexical items. Conceptual elements are by defi- 
nition domain and application dependent ( hey 
are the primitive concepts used in an applica- 
tion knowledge base). These primitives are not 
easily ported from application to application. 
209 
The emergence of standard architectures for gen- 
erators (RAGS, (Reiter, 1994))and the possibility 
to use a standard syntactic realizer answer the first 
issue. 
To address the second issue, one must realize that 
if the whole lexical chooser can not be made domain- 
independent, major parts can be made reusable. 
The main argument is that lexical knowledge is mod- 
ular. Therefore, while choice of words is constrained 
by domain-specific conceptual knowledge (what in- 
formation the sentences are to represent) on the one 
hand, it is also affected by several other dimensions: 
* inter-lexical constraints: collocations among 
words 
o pragmatic onstraints: connotations of words 
o stylistic constraints: familiarity of words 
* syntactic constraints: government patterns of 
words, e.g., thematic structure of verbs. 
We show in this paper how the separation of the 
syntactic and conceptual interfaces of lexical item 
definitions allows us to reuse a large amount of lex- 
ical knowledge across appli.cations. 
3 The  lex icon  and  i t s  benef i t s  to  
generat ion  
3.1  A large-scale,  reusab le  lexicon for 
generat ion  
Natural Language generation starts from semantic 
concepts and then finds words to realize such seman- 
tic concepts. Most existing lexical resources, how- 
ever, are indexed by words rather than by semantic 
concepts. Such resources, therefore, can not be used 
for generation directly. Moreover, generation eeds 
different ypes of knowledge, which typically are en- 
coded in different resources. However, the different 
representation formats used by these resources make 
it impossible to use them simultaneously in a single 
system. 
To overcome these limitations, we built a large- 
scale, reusable lexicon for generation by combining 
multiple existing resources. The resources that are 
combined include: 
o Tile WordNet Lexical Database (Miller et al, 
1990). WordNet is the largest lexical database 
to date, consisting of over 120,000 unique words 
(version 1.6). It also encodes many types of 
lexical relations between words, including syn- 
onytny, antonymy, and many more. 
o English Verb Classes and Alternations 
(EVCA) (Levin, 1993). It categorized 3.104 
verbs into classes based on their syntactic 
properties and studied verb alternations. An 
alternation is a variation in the realization of 
verb arguments. For example, the alternation 
"there-insertion" transforms A ship appeared 
~-on..the horizon_to There,appeared a ship..o~....the 
horizon. A total of 80 alternations for 3,104 
verbs were studied. 
The COMLEX syntax dictionary (Grishman et 
al., 1994). COMLEX contains syntactic infor- 
mation for over 38,000 English words. 
The Brown Corpus tagged with WordNet senses 
(Miller et al, 1993). We use this corpus for 
frequency measurement. . 
In combining these resources, we focused on verbs, 
since they play a more important role in deciding 
sentence structures. The combined lexicon includes 
rich lexical and syntactic knowledge for 5,676 verbs. 
It is indexed by WordNet synsets(which are at the 
semantic oncept level) as required by the generation 
task. The knowledge in the lexicon includes: 
Q A complete list of subcategorizations for each 
sense of a verb. 
o A large variety of alternations for each sense of 
a verb. 
o Frequency of lexical items and verb subcatego- 
rizations in the tagged Brown corpus 
Rich lexicat relations between words 
The sample entry for the verb "appear" is shown 
in Figure 1. It shows that the verb appear has eight 
senses (the sense distinctions come from WordNet). 
For each sense, the lexicon lists all the applicable 
subcategorization for that particular sense of the 
verb. The subcategorizations are represented using 
the same format as in COMLEX. For each sense, 
the lexicon also lists applicable alternations, which 
we encoded based on the information in EVCA. In 
addition, for each subcategorization a d alternation, 
the lexicon lists the semantic ategory constraints on 
verb arguments. In the figure, we omitted the fre- 
quency information derived from Brown Corpus and 
lexical relations (the lexical relations are encoded in 
WordNet). 
The construction of the lexicon is semi-automatic. 
First, COMLEX and EVCA were merged, produc- 
ing a list of syntactic subcategorizations and alter- 
nations for each verb. Distinctions in these syntac- 
tic restrictions according to each sense of a verb 
are achieved in the second stage, where WordNet 
is merged with the result of the first step. Finally, 
the corpus information is added, complementing the 
static resources with actual usage counts for each 
syntactic pattern. For a detailed description of the 
combination process, refer to (Jing and Mchieown, 
1998). 
210 
appear: 
sense  1 give an impress ion  
((PP-TO-INF-gS :PVAL ("to") :SO ((sb,  - ) ) )  
(TO-INF-RS :S0 ((sb, --))) 
(NP-PRED-RS :S0 ((sb,  --))) 
(ADJP-PRED-RS :SO ((sb,  - )  (sth, - - ) ) ) ) )  
sense 2 become v is ib le  
((PP-T0-INF-KS :PVAL ("to") 
:S0 ((sb, -) (sth, -))) 
(INTRANS TIIERE-V-SUB J 
. . . . . . . . .  . . _  
: ALT there-insertion 
:S0 ((sb, --) (sth, --)))) 
sense 8 have an outward express ion 
((NP-PRED-RS :SO ((sth, --))) 
(ADJP-PRED-RS :S0 ((sb, --) (sth, --)))) 
Figure I: Lexicon entry for the verb appear 
3.2 The  benefits of  the  lex icon  
There are a number of benefits that this combined 
lexicon can bring to language generation. 
First, the use of synsets as semantic tags can 
help map an application conceptual model to lexi- 
cal items. Whenever application concepts are repre- 
sented at the abstraction level of a WordNet synset, 
they can be directly accepted as input to the lexi- 
con. By this way, the lexicon can actually lead to 
the generation of many lexical paraphrases. For ex- 
ample, (look, seem, appear} is a WordNet synset; it 
includes a list of words that can convey the seman- 
tic concept ' 'g ive  an impression o f '  '. We can 
use synsets to find words that can lexicalize the se- 
mantic concepts in the semantic input. By choosing 
different words in a synset, we can therefore gen- 
erate lexical paraphrases. For instance, using the 
above synset, the system can generate the following 
paraphrases: 
"He seems happy. "
"He looks happy. "
"He appears happy.'" 
Secondly, the subcategorization information i  the 
lexicon prevents generating a non-grammatical out- 
put. As shown in Figure 1, the lexicon lists appli- 
cable subcategorizations for each sense of a verb. It 
will not allow the generation of sentences like 
"*He convinced me in his innocence" 
(wrong preposition) 
"*He convinced to go to the party" 
(missing object) 
"*Th.e bread cuts" 
(missing adverb (e.g., "'easily" )) 
"*The book consists three parts" 
( m issing t)reposit.ion) 
In addition, alternation information can help gen- 
erate .syntactic paraphrases. For instance, using 
the "simple reciprocal intransitive" alternation, the 
system can generate the following syntactic para- 
phrases: ? , 
"Brenda agreed with Molly." 
"Brenda and Molly agreed?" 
"Brenda and Molly agreed with each other." 
Finally, the corpus frequency information can help 
............... _the.lexicat.. -~ice.proeesa~.,When:multiple .words can 
be used to realize a semantic oncept, the system 
can use corpus frequency information in addition 
to other constraints to choose the most appropriate 
word. 
The knowledge ncoded in the lexicon is general, 
thus it can be used in different applications. The 
lexicon has wide coverage: the final lexicon consists 
of 5,676 verbs in total, over 14,100 senses (on average 
2.5 senses/verb), and over 11,000 semantic oncepts 
(synsets). It uses 147 patterns to represent the sub- 
categorizations and includes 80 alternations. 
To exploit the lexicon's many benefits, its format 
must be made compatible with the architecture of a 
generator. We have integrated the lexicon with the 
FUF/SURGE syntactic realizer to form a combined 
lexico-grammar. 
4 Integration Process 
In this section, we first explain how lexical choosers 
are interfaced with FUF/SURGE. We then describe 
step by step how the lexicon is integrated with 
FUF/SURGE and show that this integration pro- 
cess helps to automate the development of a lexical 
realization component. 
4.1 FUF /SURGE and the lexical chooser 
FUF (Elhadad, 1992) uses a functional unification 
formalism for generation. It unifies the input that a 
user provides with a grammar to generate sentences. 
SURGE (Elhadad and Robin, 1996) is a comprehen- 
sive English Grammar written in FUF. Tile role of 
a lexical realization component is to map a semantic 
representation drawn from the application domain 
to an input format acceptable by SURGE, adding 
necessary lexical and syntactic information during 
this process. 
Figure 2 shows a sample semantic input (a), the 
lexicalization module that is used to map this se- 
mantic input to SURGE input (b), and 'thefinal 
SURGE input (c) - -  taken from a real application 
system(Passoneau et al, 1996). The functions of the 
lexicalization module include selecting words that 
can be used to realize the semalltic oncepts in the 
input, adding syntactic features, and mapping tile 
arguments in tile semantic input to the thematic 
roles in SURGE. 
211 
Sentence :  / t  has 24 activities, including 20 tasks and four decisions. 
concept 
args 
total-node-count 
theme concept 
ref 
concept 
rheme args 
pronounPr?cess-fl?wgraph \] 
elaboration 
concept 
theme args 
expansion concept 
args 
cardinality \] 
\[ theme \[1\] \] / 
t value \[21 l -I. s.ubset-node-countJ 
concept flownode \] 
\[1\] = ref full 
concept 
proc 
partic 
cat  
proc 
partic 
\[2\] = 
concept cardinal \] 
cardinal 24 
ref full 
(a) The semantic input (i.e., input of lexicalization module) 
#(under  total-node-count) 
type possessive \] 
possessor cat pronoun / 
i 
cat common 
cardinal \[ value 
definite no 
head 
possessed 
qualifier 
\[,l\] 
lex "activity" \] 
cat clause 
mood present-participle 
type locative 
proc lex "include" 
partic location \[ cat 
k 
(b) Tile lexicalization module 
\] 
clause 
type possessive \] 
possessor cat pronoun / 
I 
cat  COn l l l l on  
cardinal \[ value 24 \] 
definite no 
head lex "activhy" \] 
possessed cat clause 
mood present-participle 
type locative \] 
qualifier proc lex "include" 
(c) Tile SURGE input (ie., output of lexicalization module) 
1 
I 
I 
I 
I 
I 
Figure 2: A samph~ lexicalization component 
212 
The development of the lexicalizer component was 
done by hand in the past. Furthermore, for. each 
new application, a new lexicatizer component had 
to be written despite the fact that some lexical and 
syntactic information is repeatedly used in different 
applications. The integration process we describe, 
however, partially automates this process. 
4.2 The  in tegrat ion  s teps  
The integration of the lexicon with FUF/SURGE 
is done through incremental unification, using four 
unification steps as shown in Figure 3. Each  step 
adds information to the semantic input, and at the 
end of the four unification steps, the semantic input 
has been mapped to the SURGE input format. 
(1) The semantic input 
Different generation systems usually use different 
representation formats for semantic input. Some 
systems use case roles ; some systems use flat 
attribute-value r presentation (Kukich et al, 1994). 
For the integrated lexicon and FUF/SURGE pack- 
age to be easily pluggable in applications, we need to 
define a standard semantic input format. It should 
be designed in such a way that applications can eas- 
ily adapt their particular semantic inputs to this 
standard format. It should also be easily mapped 
to the SURGE input format. 
In this paper, we only consider the issue of seman- 
tic input format for the expression of the predicate- 
argument relation. Two questions need to be an- 
swered in the design of the standard semantic input 
format: one, how to represent semantic oncepts; 
and two, how to represent he predicate-argument 
relation. 
We use WordNet synsets to represent semantic 
concepts. The input can refer to synsets in several 
ways: either using a globally unique synset num- 
ber I or by specifying a word and its sense number 
in WordNet. 
The representation of verb arguments is a more 
complicated issue. Case roles are frequently used in 
generation systems to represent verb arguments in 
semantic inputs. For example, (Dorr et al, 1998) 
used 20 case roles in their lexical conceptual struc- 
ture corresponding to underlying positions in a com- 
positional lexical structure. (Langkilde and Knight. 
1998) use a list of case roles in their interlingua rep- 
resentations. 
We decided to use numbered arguments (similar to 
the DSyntR in MTT (Mel'cuk and Perstov, 1987)) 
instead of case roles. The difference between the two 
1Since there are a huge number of synsets in WordNet, we 
will provide a searchable database of synsets o that users can 
look up a synset and its index number easily. For a part icular 
appl ication, users can adapt  the synsets to their specific do- 
main, such as removing non-relevant synsets, merging synsets. 
and relabel ing the synsets for convenience, as discussed in 
(,ling, 1998). 
is not critical but the numbered argument approach 
? avoids the need? to commit: the: lexicon to a specific 
ontology and seems to be easier to learn 2. 
Figure 4 shows a sample semantic input. For easy 
understanding, we refer to  the semantic concepts 
using their definitions rather than numerical index 
numbers. There are two arguments in the input. 
The intended output sentence for this semantic in- 
put is "A boat appeared on the horizon" or its para- 
phrases. 
(2) Lexical unification 
In this step, we map the semantic oncepts in the " 
semantic input to concrete words. To do this, we use 
the synsets in WordNet. All the words in the same 
synset can be used to convey the same semantic on- 
cept. For the above example, the semantic oncepts 
"become visible" and "a small vessel for travel on 
water" can be realized by the the verb appear and 
the noun boat respectively. This is the step that can 
produce lexical paraphrases. Note that when the 
system chooses a word, it also determines the par- 
ticular sense number of the word, since a word as 
it belongs to a synset has a unique sense number in 
WordNet. 
We represented all the synsets in Wordnet in FUF 
format. Each synset includes its numerical index 
number and the list of word senses included in the 
synsets. This lexical unification, works for both 
nouns and verbs. 
(3) Structural unification 
After the system has chosen a verb (actually a 
particular sense of a verb), it uses that information 
as an index to unify with the subcategorization a d 
alternations the particular verb sense has. This step 
adds additional syntactic information to the origi- 
nal input and has the capacity to produce syntactic 
paraphrases using alternation information. 
(4) Constraints on the number of arguments 
Next, we use the constraints that a subcategoriza- 
tion has on the number of arguments it requires to 
restrict unification with subcategorization patterns. 
\~k~ use 147 possible patterns. For example, the in- 
put in Figure 4 has two arguments. Although IN- 
TRANS (meaning intransitive) is listed as a possi- 
ble subcategorization pattern for "appear" (see sense 
2 in Figure 1), the input will fail to unify with it 
since INTRANS requires a single argument only. 
This prevents the generation of non-grammatic'A 
sentences. This step adds a feature which specifies 
the transitivity of the verb to FUF/SURGE input, 
selecting one from the lexicon when there is more 
than one possibility for the given verb. 
2The difference between numbered arguments and labeled 
roles is s imi lar  to that between amed semantic primit ives and 
synsets in \.VordNet. Verb classes share the same definition 
of which argument is denoted by l, 2 etc. if they share some 
syntact ic properties as far as argument aking properties are  
concerned. 
213 
Semantic input Synsets verbs lexicon si~ucts Input for SURGE 
Figure 3: The integration process 
\[rel-- i--ept --evisible J 1\] 
1 \[ concept  "a  smal l  vesse l  fo r  t rave l  on  water ' '  \] 
args 2 \[ concept ' ' the  l i ne  at  which the sky and Earth appear to  meet' '  \] 
Figure 4: The semantic input using numbered arguments 
(5) Mapping structures to SURGE input 
In the last step, the subcategorization a d alter- 
nations are mapped to SURGE input format. The 
mapping from subcategorizations to SURGE input 
was manually encoded in the lexicon for each one 
of the 147 patterns. This mapping information can 
be reused for all applications, which is more effi- 
cient than composing SURGE input in the lexical- 
ization component of each different application. Fig- 
ure 5 shows how the subcategorization NP-WITH- 
NP (e.g., The clown amused the children with his 
antics) is mapped to the SURGE input format. This 
mapping mainly involves matching the numbered ar- 
guments in the semantic input to appropriate l xical 
roles and syntactic ategories so that FIJF/SURGE 
can generate them in the correct order. 
The final SURGE input for the sentence ",4 boat 
appeared on the horizon" is shown in Figure 6. Us- 
ing the "THERE-INSERTION" alternation that the 
verb "appear" (sense 2) authorizes, the system can 
also generate the syntactic paraphrase "There ap- 
peared a boat on the hor izon".  The SURGE input 
the system generates for "There appeared a boat on 
the horizon" is very different .from that for "A boat 
appeared on the horizon".  
It is possible that for a given application some 
generated paraphrases are not appropriate. In this 
case, users can edit the synsets and the alternations 
to filter out tile paraphrases tile) do not want. 
Tile four unification steps are completely auto- 
matic. Tile system can send feedback upon failure 
struct 
relation 
args 
proc 
lex-roles 
np-with-np 
1 \[21<...> 
2 \[al<...> 
3 \[41<...> 
type lexical 
lex Ill 
t 
1 
2 
subcat 2 
3 
\ [1  \[all 2 \[3\] 
3 \[41 
cat np \] 
121 
\[rat .p \] 
\[al 
cat ip  
prep lex 
np \[41 
"with" \] 1 
Figure 5: Mapping subcategorization "NP-\VITH- 
NP" to SURGE input 
of unification. 
5 Re la ted  Work  
The lexicon, after it is integrated with 
FUF/SURGE, can also be used for other tasks in 
language generation. For example, revision (Robin, 
1994) is a technique for building semantic inputs 
incrementally. The revision process decides whether 
it is appropriate to attach a new constituent to the 
current semantic input, for example, by adding an 
214 
relation 
args 
struct 
argl 
cat 
lexical-roles 
concept 
word 
1 concept 
word 
concept 
2 word 
ppb 
2 ~ given 
c lause c 
d 
c 'become ~is ib le '  ' \] 
\] "appear"a 
'a small  vessel  for travel on water'' \] 
J "boa~"a 
'Cthe l ine  at  which the sky and Earth appear to meet \] 
"hor,izon ''a \] 
"Enriched in first step 
bEnriched in second step 
CEnriched in third step 
dEnriched in fourth step 
Figure 6: SURGE input for "A boat appeared on the horizon" 
object or an adverb. Such decisions are constrained 
by syntactic properties of verbs. The integrated 
lexicon is useful to verify these properties. 
Nitrogen (Langkilde and Knight, 1998), a natural 
language generation system developed at ISI, also 
includes a large-scale l xicon to support the genera- 
tion process. Given that Nitrogen and FUF/SURGE 
use very different methods for generation, the way 
that we integrate the lexicon with the generation sys- 
tem is also very different. Nitrogen combines ym- 
bolic rules with statistics learned from text corpora, 
while FUF/SURGE is based on Functional Unifica- 
tion Grammar. Other related work includes (Stede, 
1998), which suggests a lexicon structure for multi- 
lingual generation in a knowledge-based generation 
system. The main idea is to handle multilingual gen- 
eration in the same way as paraphrasing of the same 
language. Stede's work concerns mostly the lexical 
semantics of the transitivity alternations. 
6 Conc lus ion  
We have presented in this paper the integration of 
a large-scale, reusable lexicon for generation with 
FUF/SURGE, a unification-based natural language 
generator. This integration makes it possible to 
reuse major parts of a lexical chooser, which is tile 
component in a generation system that is responsi- 
ble for mapping semantic inputs to surface genera- 
tor inputs. We show that although the whole lexical " 
chooser can not be made domain-independent, it is 
possible to reuse a large amount of lexical, syntactic, 
and semantic knowledge across applications. 
In addition, tile lexicon other benefits to a genera- 
tion system, inchiding the abilities to generate nlany 
lexical paraphrases automatically, generate syntac -  
tic paraphrases, av(fid n(m-grammatical output, and 
choose the most frequently used word when there is 
more than one candidate words. Since the lexical, 
syntactic, and semantic knowledge ncoded in the  
lexicon is general and the lexicon has a wide cover- 
age, it can be reused for different applications. 
In the future, we plan to validate the paraphrases 
the lexicon can generate by asking human subjects to 
read the generated paraphrases and judge whether 
they are acceptable. We would like to investigate 
ways that can systematically filter out paraphrases 
that are considered unacceptable. We are also inter- 
ested in exploring the usage of this system in multi- 
lingual generation. 
Re ferences  
B. J. Doff, N. Habash, 
A thematic hierarchy 
from lexical-conceptual. 
and D. Traum. 1998. 
for efficient generation 
Technical Report CS- 
TR-3934, Institute for Advanced Computer Stud- 
ies, Department of Computer Science, University 
of Maryland, October. 
M. Elhadad and J. Robin. 1996. An overview of 
SURGE: a re-usable comprehensive syntactic re- 
alization component. In INLG'96, Brighton, UK. 
(demonstration session). 
M. Elhadad. 1992. Using Argumentation to Control 
Lezical Choice: A Functional Unification-Based 
Approach. Ph.D. thesis, Department of Computer 
Science, Columbia University. 
R. Grishman, C. Macleod, and A. Meyers. 1994. 
COMLEX syntax: Building a computational 
lexicon. In Proceedings of COLING'94, Kyoto, 
,Japan. 
H.. l ing and K. McKeown. 1998. Combining mul- 
tiple, large-scale resources in a reusable lexicon 
for natural language generation. In Proceedings 
215 
of the 36th Annual Meeting of the Association for 
Computational Linguistics and the .17th Interna- 
tional Conference on Computational Linguistics, 
volume 1, pages 607-613, Universit(~ de MontrEal, 
Quebec, Canada, August. 
H. Jing. 1998. Applying wordnet o natural an- 
guage generation. In Proceedings of COLING- 
ACL'98 workshop on the Usage of WordNet in 
Natural Language Processing Systems, University 
of Montreal, Montreal, Canada, August. 
K. Kukich, K. McKeown, J. Shaw, J. Robin, N. Mor- 
gan, and J. Phillips. "1994. User-needs analysis 
and design methodology for an automated oc- 
ument generator. In A. Zampolli, N. Calzolari, 
and M. Palmer, editors, Current Issues in Com- 
putational Linguistics: In Honour of Don Walker. 
Kluwer Academic Press, Boston. 
I. Langkilde and K. Knight. 1998. The practical 
value of n-grams in generation. In INLG'98, pages 
248-255, Niagara-on-the-Lake, Canada, August. 
B. Levin. 1993. English Verb Classes and Alterna- 
tions: A Preliminary Investigation. University of 
Chicago Press, Chicago, Illinois. 
I.A. Mel'cuk and N.V. Perstov. 1987. Surface- 
syntax of English, a formal model in the 
Meaning Text Theory. Benjamins, Amster- 
dam/Philadelphia. 
G. Miller, R. Beckwith C. Fellbaum, and D. Gross K. 
Miller. 1990. Introduction to WordNet: An on- 
line lexical database. International Journal of 
Lexicography (special issue), 3 (4) :235-312. 
G.A. Miller, C. Leacock, R. Tengi, and R.T. Bunker. 
1993. A semantic oncordance. Cognitive Science 
Laboratory, Princeton University. 
R. Passoneau, K. Kukich, J. Robin, V. Hatzivas- 
siloglou, L. Lefkowitz, and H. Jing. 1996. Gen- 
erating summaries of workflow diagrams. In Pro- 
ceedings of the International Conference on Nat- 
ural Language Processing and Industrial Appli- 
cations (NLP-IA'96), Moncton, New Brunswick, 
Canada. 
E. Reiter. 1994. Has a consensus nl generation ar- 
chitecture appeared, and is it psyeholinguistically 
plausible? In Proceedings of the Seventh Interna- 
tional Workshop on Natural Language Generation 
(INLGW-1994), pages 163-170, Kennebunkport, 
Maine, USA. available from the cmp-lg archive as 
paper cmp-lg/9411032. 
J. Robin. 1994. Revision-Based Generation of Nat- 
.ural Language Summaries Providing Historical 
Background: Corpus-Based Analysis, Design, Im- 
plementation, and Evaluation. Ph.D. thesis, De- 
partment of Computer Science, Cohnnbia Univer- 
sity. Also Technical Report CU-CS-034-94. 
M. Stede. 1998. A generative l)ersl}ective on vert} al- 
ternations. Computational Lin.quistics. 24(3):4{}1- 
_430-,September" 
216 
Interactive Authoring of Logical Forms for Multilingual Generation?
Ofer Biller, Michael Elhadad, Yael Netzer
Department of Computer Science
Ben Gurion University
Be?er-Sheva, 84105, Israel
{billero, elhadad, yaeln}@cs.bgu.ac.il
Abstract
We present an authoring system for logical forms
encoded as conceptual graphs (CG). The system
belongs to the family of WYSIWYM (What You
See Is What You Mean) text generation systems:
logical forms are entered interactively and the cor-
responding linguistic realization of the expressions
is generated in several languages. The system
maintains a model of the discourse context corre-
sponding to the authored documents.
The system helps users author documents formu-
lated in the CG format. In a first stage, a domain-
specific ontology is acquired by learning from ex-
ample texts in the domain. The ontology acquisi-
tion module builds a typed hierarchy of concepts
and relations derived from the WordNet and Verb-
net.
The user can then edit a specific document, by en-
tering utterances in sequence, and maintaining a
representation of the context. While the user en-
ters data, the system performs the standard steps
of text generation on the basis of the authored log-
ical forms: reference planning, aggregation, lexi-
cal choice and syntactic realization ? in several lan-
guages (we have implemented English and Hebrew
- and are exploring an implementation using the
Bliss graphical language). The feedback in natural
language is produced in real-time for every single
modification performed by the author.
We perform a cost-benefit analysis of the applica-
tion of NLG techniques in the context of authoring
cooking recipes in English and Hebrew. By com-
bining existing large-scale knowledge resources
(WordNet, Verbnet, the SURGE and HUGG real-
ization grammars) and techniques from modern in-
tegrated software development environment (such
as the Eclipse IDE), we obtain an efficient tool for
the generation of logical forms, in domains where
content is not available in the form of databases.
?Research supported by the Israel Ministry of Science - Knowl-
edge Center for Hebrew Computational Linguistics and by the
Frankel Fund
1 Introduction
Natural language generation techniques can be applied to
practical systems when the ?input? data to be rendered in text
can be obtained in a cost-effective manner, and when the ?out-
put? requires such variability (multiple styles or languages,
or customization to specific users or classes) that producing
documents manually becomes prohibitively expensive.
The input data can be either derived from an existing appli-
cation database or it can be authored specifically to produce
documents. Applications where the data is available in a data-
base include report generators (e.g., ANA [Kukich, 1983],
PlanDoc [Shaw et al, 1994], Multimeteo [Coch, 1998], FOG
[Goldberg et al, 1994]). In other cases, researchers identi-
fied application domains where some of the data is available,
but not in sufficient detail to produce full documents. The
?WYSIWYM? approach was proposed ([Power and Scott,
1998], [Paris and Vander Linden, 1996]) as a system design
methodology where users author and manipulate an underly-
ing logical form through a user interface that provides feed-
back in natural language text.
The effort invested in authoring logical forms ? either from
scratch or from a partial application ontology ? is justified
when the logical form can be reused. This is the case when
documents must be generated in several languages. The field
of multilingual generation (MLG) has addressed this need
([Bateman, 1997], [Stede, 1996]). When documents must be
produced in several versions, adapted to various contexts or
users, the flexibility resulting from generation from logical
forms is also valuable. Another motivation for authoring logi-
cal forms (as opposed to textual documents) is that the logical
form can be used for other applicative requirements: search,
summarization of multiple documents, inference. This con-
cern underlies the research programme of the Semantic Web,
which promotes the encoding in standardized forms of on-
tological knowledge such as KIF [Berners-Lee et al, 2001],
[Genesereth and Fikes, 1992].
In this paper, we analyze an application of the WYSIWYM
method to author logical forms encoded in Sowa?s Concep-
tual Graphs (CG) format [Sowa, 1987]. In a first stage, users
submit sample texts in a domain to the system. The system
learns from the samples a hierarchy of concepts and relations.
Given this ontology, the author then enters expressions using
a simple variant of the CG Interchange Format (CGIF) which
we have designed to speed editing operations. The system
provides realtime feedback to the author in English and He-
brew.
We evaluate the specific features of such a system which
make it cost-effective as a tool to author logical forms. We
select the CG formalism as one of the representatives of
the family of knowledge encoding formalisms, which bene-
fits from well-established inference and quantification mech-
anisms and standard syntax encodings in graphical and linear
formats.
The editing system we developed can be seen as CG ed-
itor motivated and expanded by natural language generation
(NLG) techniques. The mixing of a practical ontology edit-
ing perspective with NLG techniques yielded the following
benefits:
? Generation tasks such as aggregation and reference plan-
ning are easily expressed as operations upon CGs.
? The construction and maintenance of context according
to models of text planning [Reiter and Dale, 1992], allow
the author to break a complex CG into a manageable
collection of small utterances. Each utterance links to a
global context in a natural manner.
? We designed a compact form to edit a textual encoding
of CGs taking into account defaults, knowledge of types
of concepts, sets and individual instances and context.
This format syntactically looks like a simple object-
oriented programming language with objects, methods
and attributes. We use an editing environment similar to
a modern programming language development environ-
ment ? with a browser of types and instances, intelligent
typing completion based on type analysis, and context-
specific tooltip assistance.
? The simultaneous generation of text in two languages
(Hebrew and English) is important to distinguish be-
tween un-analyzed terms in the ontology and their lin-
guistic counterpart.
We evaluate the overall effectiveness of the authoring en-
vironment in the specific domain of cooking recipes (inspired
by [Dale, 1990]). We perform various usability studies to
evaluate the overall cost of authoring cooking recipes as log-
ical forms and evaluate the relative contribution of each com-
ponent of the system: ontology, natural language feedback,
user interface. We conclude that the combination of these
three factors results in an effective environment for authoring
logical forms.
In the paper, we first review the starting points upon which
this study builds in generation and knowledge editing. We
then present the tool we have implemented ? its architecture,
the knowledge acquisition module and the editor, we finally
present the evaluation experiments and their results, and con-
clude with their analysis.
2 Related Work
Our work starts from several related research traditions: mul-
tilingual generation systems; WYSIWYM systems; knowl-
edge and ontology editors. We review these in this section in
turn.
2.1 Multilingual Generation
Multilingual texts generation (MLG) is a well motivated
method for the automatic production of technical documents
in multiple languages. The benefits of MLG over translation
from single language source were documented in the past and
include the high cost of human translation and the inaccu-
racy of automatic machine translation [Stede, 1996], [Coch,
1998], [Bateman, 1997]. In an MLG system, users enter data
in an interlingua, from which the target languages are gener-
ated.
MLG Systems aim to be as domain independent as pos-
sible (since development is expensive) but usually refer to a
narrow domain, since the design of the interlingua refers to
domain information. MLG systems share a common archi-
tecture consisting of the following modules:
? A language-independent underlying knowledge repre-
sentation: knowledge represented as AI plans [Ro?sner
and Stede, 1994] [Delin et al, 1994], [Paris and Van-
der Linden, 1996], knowledge bases (or ontologies)
such as LOOM, the Penman Upper-model and other
(domain-specific) concepts and instances [Ro?sner and
Stede, 1994].
? Micro-structure planning (rhetorical structure) - lan-
guage independent - this is usually done by the human
writers using the MLG application GUI.
? Sentence planning - different languages can express the
same content in various rhetorical structures, and plan-
ning must take it into consideration: either by avoiding
the tailoring of structure to a specific language [Ro?sner
and Stede, 1994] or by taking advantage of knowledge
on different realizations of rhetorical structures in differ-
ent languages at the underlying representation [Delin et
al., 1994].
? Lexical and syntactic realization resources (e.g., Eng-
lish PENMAN/German NIGEL in [Ro?sner and Stede,
1994])
As an MLG system, our system also includes similar mod-
ules. We have chosen to use Conceptual Graphs as an inter-
lingua for encoding document data [Sowa, 1987]. We use ex-
isting generation resources for English ? SURGE [Elhadad,
1992] for syntactic realization and the lexical chooser de-
scribed in [Jing et al, 2000] and the HUGG grammar for
syntactic realization in Hebrew [Netzer, 1997]. For micro-
planning, we have implemented the algorithm for reference
planning described in [Reiter and Dale, 1992] and the ag-
gregation algorithm described in [Shaw, 1995]. The NLG
components rely on the C-FUF implementation of the FUF
language [Kharitonov, 1999] [Elhadad, 1991] ? which is fast
enough to be used interactively in realtime for every single
editing modification of the semantic input.
2.2 WYSIWYM
In an influential series of papers [Power and Scott, 1998],
WYSIWYM (What You See Is What You Mean) was pro-
posed as a method for the authoring of semantic information
through direct manipulation of structures rendered in natural
language text. A WYSIWYM editor enables the user to edit
information at the semantic level. The semantic level is a di-
rect controlled feature, and all lower levels which are derived
from it, are considered as presentational features. While edit-
ing content, the user gets a feedback text and a graphical rep-
resentation of the semantic network. These representations
can be interactively edited, as the visible data is linked back
to the underlying knowledge representation.
Using this method, a domain expert produces data by edit-
ing the data itself in a formal way, using a tool that requires
only knowledge of the writer?s natural language. Knowledge
editing requires less training, and the natural language feed-
back strengthens the confidence of users in the validity of the
documents they prepare.
The system we have developed belongs to the WYSIWYM
family. The key aspects of the WYSIWYM method we in-
vestigate are the editing of the semantic information. Text
is generated as a feedback for every single editing operation.
Specifically, we evaluate how ontological information helps
speed up semantic data editing.
2.3 Controlled Languages
A way to ensure that natural language text is unambiguous
and ?easy to process? is to constrain its linguistic form. Re-
searchers have designed ?controlled languages? to ensure that
words in a limited vocabulary and simple syntactic struc-
tures are used (see for example [Pulman, 1996]). This notion
is related to that of sublanguage [Kittredge and Lehrberger,
1982], which has been used to analyze and generate text in
specific domains such as weather reports.
With advances in robust methods for text analysis, it is be-
coming possible to parse text with high accuracy and recover
partial semantic information. For example, the DIRT system
[Lin and Pantel, 2001] recovers thematic structures from free
text in specific domains. Combined with lexical resources
(WordNet [Miller, 1995] and Verbnet [Kipper et al, 2000]),
it is now possible to confirm the thesis that controlled lan-
guages are easy to process automatically.
Complete semantic interpretation of text remains however
too difficult for current systems. In our system, we rely on
automatic interpretation of text samples in a specific sublan-
guage to assist in the acquisition of a domain-specific ontol-
ogy, as described below.
2.4 Graphical Editors for Logical Forms
Since many semantic encodings are described as graphs,
knowledge editing tools have traditionally been proposed as
graphical editors ? where concepts are represented as nodes
and relations as edges. Such a ?generic graphical editor? is
presented for example in [Paley et al, 1997].
Conceptual graphs have also been traditionally represented
graphically, and there is a standard graphical encoding for
CGs. Graphical editors for CGs are available (e.g., [Delu-
gach, 2001]).
While graphical editors are attractive, they suffer from
known problems of visual languages: they do not scale well
(large networks are particularly difficult to edit and under-
stand). Editing graphical representations is often slower than
editing textual representations. Finally, graphical representa-
tions convey too much information, as non-meaningful data
may be inferred from graphical features such as layout of
font, which is not constrained by the underlying visual lan-
guage.
2.5 Generation from CG
CGs have been used as an input to text generation in a variety
of systems in the past [Cote and Moulin, 1990], [Bontcheva,
1995] and others.
In our work, we do not view the CG level as a direct in-
put to a generation system. Instead, we view the CG level
as an ontological representation, lacking communicative in-
tention levels, and not linked directly to linguistic considera-
tions. The CG level is justified by its inferencing and query
retrieval capabilities, while taking into account sets, quantifi-
cation and nested contexts.
Processing is required to link the CG representation level
(see Fig. 1) to linguistically motivated rhetorical structures,
sentence planning and lexical choice. In our work, CGs are
formally converted to an input to a generation system by a
text planner and a lexical chooser, as described below. Ex-
isting generation components for lexical choice and syntac-
tic realization based on functional unification are used on the
output of the text planner.
Figure 1: Conceptual Graph in a linear representation.
3 Method and Architecture
We now present the system we have implemented, which we
have called SAUT (Semantic AUthoring Tool). Our objective
is to perform usability studies to evaluate:
? How ontological knowledge in the form of concept and
relation hierarchies is useful for semantic authoring;
? How natural language feedback improves the authoring
? and how feedback in two languages modifies the au-
thoring process;
? How user interface functionality improves the speed and
accuracy of the authoring.
The architecture of the system is depicted in Fig. 2.
The two key components of the system are the knowledge
acquisition system and the editing component. The knowl-
edge acquisition system is used to derive an ontology from
sample texts in a specific domain. In the editing component,
users enter logical expressions on the basis of the ontology.
3.1 Knowledge Acquisition
For the acquisition of the concepts/relations database, we use
two main sources: Verbnet [Kipper et al, 2000] and WordNet
[Miller, 1995].
We use the information for bootstrapping concept and rela-
tion hierarchies. Given sample texts in the target domain, we
Figure 2: Architecture of the SAUT system
perform shallow syntactic analysis and extract nouns, verbs
and adjectives from the text. Dependency structures for verbs
and nouns are also extracted. We currently perform manually
anaphora resolution and word sense disambiguation, since
automatic methods do not produce accurate enough results.
Given the set of nouns and adjectives, we induce the hyper-
nym hierarchy from WordNet, resulting in a tree of concepts
? one for each synset appearing in the list of words in the
sample texts.1
In addition to the concept hierarchy, we derive relations
among the concepts and predicates by using the Verbnet lexi-
cal database [Kipper et al, 2000]. Verbnet supplies informa-
tion on the conceptual level, in the form of selectional restric-
tions for the thematic roles.
These relations allow us to connect the concepts and rela-
tions in the derived ontology to nouns, verbs and adjectives.
The selectional restrictions in Verbnet refer to the WordNet
conceptual hierarchy. In Verbnet, verbs are classified fol-
lowing Levin?s classes [Levin, 1993] and thus its represen-
tation is easily adjustable with our verb lexicon [Jing et al,
2000], which combined information on argument structure of
verbs from Levin, Comlex [Macleod and Grishman, 1995]
and WordNet. The rich information on argument structure
and selectional restrictions can be automatically adopted to
the domain concepts database. Thus, by connecting a con-
cept to a verb, given all the concepts that stand in relation to
it in a specific CG (the verb?s arguments and circumstantials)
? our lexical chooser finds the suitable structure (alternation)
to map the CG to a syntactic structure.
The outcome of this process is useful in the lexical and syn-
tactic module of the system due to the flexibility it offers to
the lexical chooser (a general word can be used instead of a
1Although hypernym relations in WordNet define a forest of
trees, we connect all trees with a general node.
specific word i.e. vehicles instead of cars, and for the gener-
ality of selectional restrictions on verb/adjective arguments.
Since there are no Hebrew parallels to WordNet/verbnet,
we use a ?naive? scheme of translating the English LC to He-
brew, with manual corrections of specific structures when er-
rors are found.
Once the knowledge is acquired, we automatically updated
a lexical chooser adopted to the domain. The lexical chooser
maps the ontological concepts and relations to nouns, verbs
and adjectives in the domain.
3.2 The SAUT Editor
To describe the SAUT editor, we detail the process of author-
ing a document using the tool. When the authoring tool is
initiated, the next windows are presented (see Fig. 3):
? Input window
? Global context viewer
? Local context viewer
? CG feedback viewer
? Feedback text viewer
? Generated document viewer.
The user operates in the input window. This window in-
cludes three panels:
? Defaults: rules that are enforced by default on the rest of
the document. The defaults can be changed while edit-
ing. Defaults specify attribute values which are auto-
matically copied to the authored CGs according to their
type.
? Participants: a list of objects to which the document
refers. Each participant is described by an instance (or a
generic) CG, and is given an alias. The system provides
Figure 3: Snapshot of editing state in the SAUT system
an automatic identifier for participants, but these can be
changed by the user to a meaningful identifier.
? Utterances: editing information proposition by proposi-
tion.
The system provides suggestions to complete expressions
according to the context in the form of popup windows. In
these suggestion windows, the user can either scroll or choose
with the mouse or by entering the first letters of the desired
word, when the right word is marked by the system, the user
can continue, and the word will be automatically completed
by the system. For example, when creating a new participant,
the editor presents a selection window with all concepts in
the ontology that can be instantiated. If the user chooses the
concept type ?Dog? the system creates a new object of type
dog, with the given identifier. The user can further enrich this
object with different properties. This is performed using the
?.? notation to modify a concept with an attribute. While the
user enters the instance specification and its initial properties,
a feedback text and a conceptual graph in linear form are gen-
erated simultaneously. When the user moves to the next line,
the new object is updated on the global context view. Each
object is placed in a folder corresponding to its concept type,
and will include its instance name and its description in CG
linear form.
In the Utterances panel, the author enters propositions in-
volving the objects he declared in the participants section. To
create an utterance, the user first specifies the object which is
the topic of the utterance. The user can choose one of the par-
ticipants declared earlier from an identifiers list, or by choos-
ing a concept type from a list. Choosing a concept type will
result in creating a new instance of this concept type. Every
instance created in the system will be viewed in the context
viewer. After choosing an initial object, the user can add ex-
pressions in order to add information concerning this object.
After entering the initial object in an utterance, the user can
press the dot key which indicates that he wants to enrich this
object with information. The system will show the user list
of expressions that can add information on this object. In CG
terms, the system will fill the list with items which fall in one
of the following three categories:
? Relations that can be created by the system and their se-
lectional restrictions are such that they allow the modi-
fied object as a source for the relation.
? Properties that can be added to the concept object such
as name and quantity.
? Concept types that expect relations, the first of whom
can connect to the new concept. For example the con-
cept type ?Eat? expects a relation ?Agent? and a relation
?Patient.? The selectional restriction on the destination
of ?Agent? will be for example ?Animate?. Therefore
the concept ?Eat? will appear on the list of an object of
type ?Dog?.
The author can modify and add information to the active
object by pressing the dot key. An object which itself mod-
ifies an object previously entered, can be modified with new
relations, properties and concepts in the same manner. The
global context is updated whenever a new instance is created
in the utterances. When the author has finished composing
the utterance, the system will update the local context and
will add this information to the generated natural language
document.
The comma operator (?,?) is used to define sets in exten-
sion. For example, in Fig.3, the set ?salt and pepper? is
created by entering the expression #sa,#pe. The set itself be-
comes an object in the context and is assigned its own identi-
fier.
The dot notation combined with named variables allows
for easy and intuitive editing of the CG data. In addition,
the organization of the document as defaults, participants and
context (local and global) ? provides an intuitive manner to
organize documents.
Propositions, after they are entered as utterances, can also
be named, and therefore can become arguments for further
propositions. This provides a natural way to cluster large con-
ceptual graphs into smaller chunks.
The text generation component proceeds from this infor-
mation, according to the following steps:
? Pronouns are generated when possible using the local
and global context information.
? Referring expression are planned using the competing
expressions from the context information, excluding and
including information and features of the object in the
generated text, so the object identity can be resolved by
the reader, but without adding unnecessary information.
? Aggregation of utterances which share certain features
using the aggregation algorithm described in [Shaw,
1995].
Consider the example cooking recipe in Fig.3. The author
uses the participants section in order to introduce the ingre-
dients needed for this recipe. One of the ingredients is ?six
large eggs?. The author first chooses an identifier name for
the eggs, for example ?eg?. From the initial list of concepts
types proposed by the system, we choose the concept type
?egg?. Pressing the dot key will indicate we want to pro-
vide the system with further information about the newly cre-
ated object. We choose ?quantity? from a given list by typ-
ing ?qu?. seeing that the word ?quantity? was automatically
marked in the list. Pressing the space key will automatically
open brackets, which indicates we have to provide the system
with an argument. A tool tip text will pop to explain the user
what is the function of the required argument. After entering
number, we will hit the space bar to indicate we have no more
information to supply about the ?quantity?; the brackets will
be automatically closed. After the system has been told no
more modification will be made on the quantity, the ?egg?
object is back to be the active one. The system marks the ac-
tive object in any given time by underline the related word in
the input text.
Pressing the dot will pop the list box with the possible mod-
ifications for the object. We will now choose ?attribute?.
Again the system will open brackets, and a list of possible
concepts will appear. The current active node in the graph is
?attribute?. Among the possible concepts we will choose the
?big? concept, and continue by clicking the enter key (the
lexical chooser will map the ?big? concept to the collocation
?large? appropriate for ?eggs?). A new folder in the global
context view will be added with the title of ?egg? and will
contain the new instance with its identifier and description as
a CG in linear form.
Each time a dot or an identifier is entered, the system con-
verts the current expression to a CG, maps the CG to a FUF
Functional Description which serves as input to the lexical
chooser; lexical choice and syntactic realization is performed,
and feedback is provided in both English and Hebrew.
The same generated sentence is shown without context (in
the left part of the screen), and in context (after reference
planning and aggregation).
When generating utterances, the author can refer to an ob-
ject from the context by clicking on the context view. This
enters the corresponding identifier in the utterance graph.
4 Evaluation
The objectives of the SAUT authoring system are to pro-
vide the user with a fast, intuitive and accurate way to com-
pose semantic structures that represent meaning s/he wants to
convey, then presenting the meaning in various natural lan-
guages. Therefore, an evaluation of these aspects (speed, in-
tuitiveness, accuracy and coverage) is required, and we have
conducted an experiment with human subjects to measure
them. The experiment measures a snapshot of these parame-
ters at a given state of the implementation. In the error analy-
sis we have isolated parameters which depend on specifics
of the implementation and those which require essential revi-
sions to the approach followed by SAUT.
4.1 User Experiment
We have conducted a user experiment, in which ten subjects
were given three to four recipes in English (all taken from
the Internet) from a total pool of ten. The subjects had to
compose semantic documents for these recipes using SAUT
2
. The ontology and lexicon for the specific domain of cook-
ing recipes were prepared in advance, and we have tested the
tool by composing these recipes with the system. The docu-
ments the authors prepared are later used as a ?gold standard?
(we refer to them as ?reference documents?). The experi-
ment was managed as follows: first, a short presentation of
the tool (20 minutes) was given. Then, each subject recieved
a written interactive tutorial which took approximately half
an hour to process. Finally, each subject composed a set of 3
to 4 documents. The overall time taken for each subject was
2.5 hours.
4.2 Evaluation
We have measured the following aspects of the system during
the experiment.
Coverage - answers the questions ?can I say everything I
mean? and ?how much of the possible meanings that can be
expressed in natural language can be expressed using the in-
put language?. In order to check the coverage of the tool,
we examined the reference documents. We compared the
text generated from the reference documents with the orig-
inal recipes and checked which parts of the information were
2All subjects were computer science students.
included, excluded or expressed in a partial way with respect
to the original. We counted each of these in number of words
in the original text, and expressed these 3 counts as a per-
centage of the words in the original recipe. We summed up
the result as a coverage index which combined the 3 counts
(correct, missing, partial) with a factor of 70% for the partial
count.
The results were checked by two authors independently
and we report here the average of these two verifications. On
a total of 10 recipes, containing 1024 words overall, the cov-
erage of the system is 91%. Coverage was uniform across
recipes and judges. We performed error analysis for the re-
maining 9% of the un-covered material below.
Intuitiveness - to assess the ease of use of the tool,
we measured the ?learning curve? for users first using the
system, and measuring the time it takes to author a recipe for
each successive document (1st, 2nd, 3rd, 4th). For 10 users
first facing the tool, the time it took to author the documents
is as follows:
Document # Average Time to author
1st 36 mn
2nd 28 mn
3rd 22 mn
4th 14 mn
The time distribution among 10 users was extremely uni-
form. We did not find variation in the quality of the authored
documents across users and across number of document.
The tool is mastered quickly, by users with no prior train-
ing in knowledge representation or natural language process-
ing. Composing the reference documents (approximately
100-words recipes) by the authors took an average of 12
minutes.
Speed - we measured the time required to compose a docu-
ment as a semantic representation, and compare it to the time
taken to translate the same document in a different language.
We compare the average time for trained users to author a
recipe (14 minutes) with that taken by 2 trained translators to
translate 4 recipes (from English to Hebrew).
Semantic Authoring Time Translation Time
14 (minutes) 6 (minutes)
The comparison is encouraging - it indicates that a tool for
semantic authoring could become cost-effective if it is used
to generate in 2 or 3 languages.
Accuracy - We analyzed the errors in the documents pre-
pared by the 10 users according to the following breakup:
? Words in the source document not present in the seman-
tic form
? Words in the source document presented inaccurately in
the semantic form
? Users? errors in semantic form that are not included in
the former two parameters.
We calculated the accuracy for each document produced
by the subjects during the experiment. Then we compared
each document with the corresponding reference document
(used here as a gold standard). Relative accuracy of this
form estimates a form of confidence ? ?how sure can the
user be that s/he wrote what s/he meant?? This measurement
depends on the preliminary assumption that for a given
recipe, any two readers (in the experiment environment ?
including the authors), will extract similar information. This
assumption is warranted for cooking recipes. This measure
takes into account the limitations of the tool and reflects the
success of users to express all that the tool can express:
Document # Accuracy
1st 93%
2nd 92%
3rd 95%
4th 90%
Accuracy is quite consistent during the experiment ses-
sions, i.e., it does not change as practice increases. The aver-
age 92.5% accuracy is quite high.
We have categorized the errors found in subjects? docu-
ments in the following manner:
? Content can be accurately expressed with SAUT (user
error)
? Content will be accurately expressed with changes in the
SAUT?s lexicon and ontology (ontology deficit)
? Content cannot be expressed in the current implemen-
tation, and requires further investigation of the concept
(implementation and conceptual limitations)
Document # Accuracy
User error 44%
Ontology deficit 23%
Tool limitations 33%
This breakdown indicates that the tool can be improved by
investing more time in the GUI and feedback quality and by
extending the ontology. The difficult conceptual issues (those
which will require major design modifications, or put in ques-
tion our choice of formalism for knowledge encoding) repre-
sent 33% of the errors ? overall accounting for 2.5% of the
words in the word count of the generated text.
5 Analysis
The current prototype of SAUT proves the feasibility of se-
mantic authoring combined with natural language generation.
The system includes a lexical chooser of several hundred
verbs and nouns derived from WordNet in a specific domain.
The system is easy to use and requires training of less than
one hour. User interface features make it very fast to enter
CGs of the type required for a recipe. If the documents are
generated in more than 2 languages, the tool can even become
cost effective at its current level of ergonomy.
The current prototype indicates that combining techniques
from NLG with User Interfaces techniques from program-
ming languages editors results in an efficient knowledge edi-
tor. In future work, we intend to evaluate how to use semantic
forms for summarization and inferencing. We also will evalu-
ate how rhetorical information can be managed in the system,
by applying the tool to different domains.
References
[Bateman, 1997] John Bateman. Enabling technology for
multilingual natural language generation: the KPML de-
velopment. Natural Language Engineering, 1(1):1 ? 42,
1997.
[Berners-Lee et al, 2001] Tim Berners-Lee, James Hendler,
and Ora Lassila. Semantic web. Scientific American, 2001.
[Bontcheva, 1995] Kalina Bontcheva. Generation of multi-
lingual eplanations from conceptual graphs. In Proc. of
RANLP?97, Batak, Bulgaria, 1995.
[Coch, 1998] J. Coch. Interactive generation and knowledge
administration in multimeteo. In Proc. of the 9th Workshop
INLG, pages 300?303, Canada, 1998.
[Cote and Moulin, 1990] D. Cote and B. Moulin. Refin-
ing sowa?s con-ceptual graph theory for text genera-
tion. In Proc. of IEA/AIE90, volume 1, pages 528?537,
Charleston, SC, 1990.
[Dale, 1990] Robert Dale. Generating recipes: An overview
of epicure. In Michael Zock Robert Dale, Chris Mellish,
editor, Current Research in Natural Language Generation,
pages 229?255. Academic Press, New York, 1990.
[Delin et al, 1994] Judy Delin, Anthony Hartley, Ce?cile L.
Paris, Donia Scott, and Keith Vander Linden. Expressing
Procedural Relationships in Multilingual Instructions. In
Proc. of the 7th. Int. Workshop on NLG, pages 61 ? 70,
1994.
[Delugach, 2001] Harry Delugach. Charger: A graphical
conceptual graph editor. In Proc. of ICCS 2001 CGTools
Workshop, 2001.
[Elhadad, 1991] Michael Elhadad. FUF user manual - ver-
sion 5.0. Technical Report CUCS-038-91, University of
Columbia, 1991.
[Elhadad, 1992] Michael Elhadad. Using Argumentation to
Control Lexical Choice: A Functional Unification Imple-
mentation. PhD thesis, Columbia University, 1992.
[Genesereth and Fikes, 1992] M.R. Genesereth and R.E.
Fikes. Knowledge interchange format, version 3.0 ref-
erence manual. Technical Report Logic-92-1, Computer
Science Department, Stanford University, 1992.
[Goldberg et al, 1994] E. Goldberg, N. Driedger, and
R. Kittredge. Using natural-language processing to pro-
duce weather forecasts. IEEE Expert, 9(2):45?53, 1994.
[Jing et al, 2000] Hongyan Jing, Yael Dahan Netzer,
Michael Elhadad, and Kathleen McKeown. Integrating
a large-scale, reusable lexicon with a natural language
generator. In Proceedings of the 1st INLG, pages 209?216,
Mitzpe Ramon, Israel, 2000.
[Kharitonov, 1999] Mark Kharitonov. Cfuf: A fast inter-
preter for the functional unification formalism. Master?s
thesis, BGU, Israel, 1999.
[Kipper et al, 2000] K. Kipper, H. Trang Dang, and
M. Palmer. Class-based construction of a verb lexicon.
In Proceeding of AAAI-2000, 2000.
[Kittredge and Lehrberger, 1982] R. Kittredge and
J. Lehrberger. Sublanguage: Studies of Language in
Restricted Semantic Domains. De Gruyter, Berlin, 1982.
[Kukich, 1983] Karen Kukich. Knowledge-based report gen-
eration: A technique for automatically generating natural
language reports from databases. In Proc. of the 6th Inter-
national ACM SIGIR Conference, 1983.
[Levin, 1993] Beth Levin. English Verb Classes and Verb
Alternations: A Preliminary Investigation. University of
Chicago Press, 1993.
[Lin and Pantel, 2001] Dekang Lin and Patrick Pantel. DIRT
@SBT@discovery of inference rules from text. In Knowl-
edge Discovery and Data Mining, pages 323?328, 2001.
[Macleod and Grishman, 1995] C. Macleod and R. Grish-
man. COMLEX Syntax Reference Manual. Proteus
Project, NYU, 1995.
[Miller, 1995] George A. Miller. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41, 1995.
[Netzer, 1997] Yael Netzer. Design and evaluation of a func-
tional input specification language for the generation of
bilingual nominal expressions (hebrew/english). Master?s
thesis, BGU, Israel, 1997.
[Paley et al, 1997] S.M. Paley, Lowrance, J.D., and P.D.
Karp. A generic knowledge-base browser and editor. In
Proc. of the 1997 National Conference on AI, 1997.
[Paris and Vander Linden, 1996] Ce?cile Paris and Keith Van-
der Linden. DRAFTER: An interactive support tool
for writing multilingual instructions. IEEE Computer,
29(7):49?56, 1996.
[Power and Scott, 1998] Roger Power and Donia Scott. Mul-
tilingual authoring using feedback texts. In Proc. of
COLING-ACL 98, Montreal, Canada, 1998.
[Pulman, 1996] Stephen Pulman. Controlled language for
knowledge representation. In Proc. of the 1st Int. Work-
shop on Controlled Language Applications, pages 233 ?
242, 1996.
[Reiter and Dale, 1992] Ehud Reiter and Robert Dale. A
fast algorithm for the generation of referring expressions.
In Proc. of the 14th COLING, pages 232?238, Nantes,
France, 1992.
[Ro?sner and Stede, 1994] D. Ro?sner and M. Stede. Generat-
ing multilingual documents from a knowledge base: The
techdoc project. In Proc. of COLING?94, pages 339?346,
Kyoto, 1994.
[Shaw et al, 1994] J. Shaw, K. Kukich, and K. Mckeown.
Practical issues in automatic documentation generation. In
Proceeding of the 4th ANLP, pages 7?14, 1994.
[Shaw, 1995] James Shaw. Conciseness through aggregation
in text generation. In Proc. of the 33rd conference on ACL,
pages 329 ? 331, Morristown, NJ, USA, 1995.
[Sowa, 1987] J. F. Sowa. Semantic networks. In S. C.
Shapiro, editor, Encyclopedia of Artificial Intelligence 2.
John Wiley & Sons, New York, 1987.
[Stede, 1996] Manfred Stede. Lexical semantics and knowl-
edge representation in multilingual sentence generation.
PhD thesis, University of Toronto, 1996.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 57?64,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Can You Tag the Modal? You Should.
Yael Netzer and Meni Adler and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yaeln,adlerm,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Computational linguistics methods are typ-
ically first developed and tested in English.
When applied to other languages, assump-
tions from English data are often applied
to the target language. One of the most
common such assumptions is that a ?stan-
dard? part-of-speech (POS) tagset can be
used across languages with only slight vari-
ations. We discuss in this paper a specific is-
sue related to the definition of a POS tagset
for Modern Hebrew, as an example to clar-
ify the method through which such varia-
tions can be defined. It is widely assumed
that Hebrew has no syntactic category of
modals. There is, however, an identified
class of words which are modal-like in their
semantics, and can be characterized through
distinct syntactic and morphologic criteria.
We have found wide disagreement among
traditional dictionaries on the POS tag at-
tributed to such words. We describe three
main approaches when deciding how to tag
such words in Hebrew. We illustrate the im-
pact of selecting each of these approaches
on agreement among human taggers, and on
the accuracy of automatic POS taggers in-
duced for each method. We finally recom-
mend the use of a ?modal? tag in Hebrew
and provide detailed guidelines for this tag.
Our overall conclusion is that tagset defini-
tion is a complex task which deserves appro-
priate methodology.
1 Introduction
In this paper we address one linguistic issue that was
raised while tagging a Hebrew corpus for part of
speech (POS) and morphological information. Our
corpus is comprised of short news stories. It in-
cludes roughly 1,000,000 tokens, in articles of typ-
ical length between 200 to 1000 tokens. The arti-
cles are written in a relatively simple style, with a
high token/word ratio. Of the full corpus, a sam-
ple of articles comprising altogether 100,000 tokens
was assembled at random and manually tagged for
part of speech. We employed four students as tag-
gers. An initial set of guidelines was first composed,
relying on the categories found in several dictionar-
ies and on the Penn treebank POS guidelines (San-
torini, 1995). Tagging was done using an automatic
tool1. We relied on existing computational lexicons
(Segal, 2000; Yona, 2004) to generate candidate tags
for each word. As many words from the corpus were
either missing or tagged in a non uniform manner in
the lexicons, we recommended looking up missing
words in traditional dictionaries. Disagreement was
also found among copyrighted dictionaries, both for
open and closed set categories. Given the lack of
a reliable lexicon, the taggers were not given a list
of options to choose from, but were free to tag with
whatever tag they found suitable. The process, al-
though slower and bound to produce unintentional
mistakes, was used for building a lexicon, and to
refine the guidelines and on occasion modify the
POS tagset. When constructing and then amending
the guidelines we sought the best trade-off between
1http://wordfreak.sourceforge.net
57
accuracy and meaningfulness of the categorization,
and simplicity of the guidelines, which is important
for consistent tagging.
Initially, each text was tagged by four different
people, and the guidelines were revised according
to questions or disagreements that were raised. As
the guidelines became more stable, the disagreement
rate decreased, each text was tagged by three peo-
ple only and eventually two taggers and a referee
that reviewed disagreements between the two. The
disagreement rate between any two taggers was ini-
tially as high as 20%, and dropped to 3% after a few
rounds of tagging and revising the guidelines.
Major sources of disagreements that were identi-
fied, include:
Prepositional phrases vs. prepositions In Hebrew,
formative letters ?      b,c,l,m2 ? can be attached
to a noun to create a short prepositional phrase. In
some cases, such phrases function as a preposition
and the original meaning of the noun is not clearly
felt. Some taggers would tag the word as a prepo-
sitional prefix + noun, while others tagged it as a
preposition, e.g., 
	 
 b?iqbot (following), that
can be tagged as 	 
 b-iqbot (in the footsteps
of).
Adverbial phrases vs. Adverbs the problem is simi-
lar to the one above, e.g.,  	  bdiyuq (exactly), can
be tagged as b-diyuq (with accuracy).
Participles vs. Adjectives as both categories can
modify nouns, it is hard to distinguish between
them, e.g,   Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 32?39,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Gaiku : Generating Haiku with Word Associations Norms
Yael Netzer? and David Gabay and Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yaeln,gabayd,yoavg,elhadad}@cs.bgu.ac.il
Abstract
creativeness / a pleasing field / of bloom
Word associations are an important element
of linguistic creativity. Traditional lexical
knowledge bases such as WordNet formalize
a limited set of systematic relations among
words, such as synonymy, polysemy and hy-
pernymy. Such relations maintain their sys-
tematicity when composed into lexical chains.
We claim that such relations cannot explain
the type of lexical associations common in
poetic text. We explore in this paper the
usage of Word Association Norms (WANs)
as an alternative lexical knowledge source
to analyze linguistic computational creativity.
We specifically investigate the Haiku poetic
genre, which is characterized by heavy re-
liance on lexical associations. We first com-
pare the density of WAN-based word asso-
ciations in a corpus of English Haiku po-
ems to that of WordNet-based associations as
well as in other non-poetic genres. These
experiments confirm our hypothesis that the
non-systematic lexical associations captured
in WANs play an important role in poetic text.
We then present Gaiku, a system to automat-
ically generate Haikus from a seed word and
using WAN-associations. Human evaluation
indicate that generated Haikus are of lesser
quality than human Haikus, but a high propor-
tion of generated Haikus can confuse human
readers, and a few of them trigger intriguing
reactions.
? Supported by Deutsche Telekom Laboratories at Ben-
Gurion University of the Negev.
? Supported by the Lynn and William Frankel Center for
Computer Sciences.
1 Introduction
Traditional lexical knowledge bases such as Word-
Net formalize a limited set of systematic relations
that exist between words, such as synonymy, pol-
ysemy, hypernymy. When such relations are com-
posed, they maintain their systematicity, and do not
create surprising, unexpected word associations.
The human mind is not limited to such system-
atic relations, and people tend to associate words to
each other with a rich set of relations, such as non
systematic paradigmatic (doctor-nurse) and syntag-
matic relations (mash-potato) as identified by Saus-
sure (1949). Such associations rely on cultural
(mash-television), emotional (math - yuck) and per-
sonal experience (autumn - Canada).
In linguistic creativity, such as prose or poetry
writing, word associations play an important role
and the ability to connect words into new, unex-
pected relations is one of the key mechanisms that
triggers the reader involvement.
We explore in this paper the usage of Word As-
sociation Norms (WANs) as an alternative lexical
knowledge source to analyze linguistic computa-
tional creativity. WANs have been developed in psy-
chological research in the past 40 years. They record
typical word associations evoked by people when
they are submitted a trigger word. Such associations
(e.g., table to chair or cloth) are non-systematic, yet
highly stable across people, time (over a period of 30
years) and languages. WANs have been compiled in
various languages, and provide an interesting source
to analyze word associations in creative writing.
We specifically investigate the Haiku poetic
32
genre, which is characterized by heavy reliance on
lexical associations. The hypothesis we investigate
is that WANs play a role in computational creativ-
ity, and better explain the type of word associations
observed in creative writing than the systematic re-
lations found in thesauri such as WordNet.
In the rest of the paper, we refine our hypothe-
sis and present observations on a dataset of English
Haikus we collected. We find that the density of
WAN-based word associations in Haikus is much
higher than in other genres, and also much higher
than the density of WordNet-based associations. We
then present Gaiku, a system we developed to auto-
matically generate Haikus from a seed word using
word association norms. Evaluation we performed
with a group of 60 human readers indicates that the
generated Haikus exhibit interesting creative charac-
teristics and sometimes receive intriguing acclaim.
2 Background and Previous Work
2.1 Computational Creativity
Computational creativity in general and linguistic in
particular, is a fascinating task. On the one hand, lin-
guistic creativity goes beyond the general NLP tasks
and requires understanding and modelling knowl-
edge which, almost by definition, cannot be formal-
ized (i.e., terms like beautiful, touching, funny or in-
triguing). On the other hand, this vagueness itself
may enable a less restrictive formalization and allow
a variety of quality judgments. Such vague formal-
izations are naturally more useful when a computa-
tional creativity system does not attempt to model
the creativity process itself, but instead focuses on
?creative products? such as poetry (see Section 2.3),
prose and narrative (Montfort, 2006), cryptic cross-
word clues (Hardcastle, 2007) and many others.
Some research focus on the creative process itself
(see (Ritchie, 2006) for a comprehensive review of
the field). We discuss in this paper what Boden
(1998) calls P-Creativity (Psychological Creativity)
which is defined relative to the initial state of knowl-
edge, and H-Creativity (Historical Creativity) which
is relative to a specific reference culture. Boden
claims that, while hard to reproduce, exploratory
creativity is most successful in computer models of
creativity. This is because the other kinds of creativ-
ity are even more elusive due to the difficulty of ap-
proaching the richness of human associative mem-
ory, and the difficulty of identifying our values and
of expressing them in computational form.
We investigate in our work one way of addressing
this difficulty: we propose to use associative data as
a knowledge source as a first approximation of hu-
man associative capabilities. While we do not ex-
plain such associations, we attempt to use them in
a constructive manner as part of a simple combina-
tional model of creativity in poetry.
2.2 Word Associations and Creativity
Associations and creativity are long known to be
strongly connected. Mendick (Mendick, 1969) de-
fines creative thinking as ?the forming of associative
elements into new combinations which either meet
specified requirements or are in some way useful.?
The usefulness criterion distinguishes original think-
ing from creative thinking. A creative solution is
reached through three main paths: serendipity (ran-
dom stimuli evoke associative elements), similar-
ity (stimuli and solution are found similar through
an association) and mediation (both ?problem? and
?solution? can be associated to similar elements).
In our work, we hypothesize that interesting Haiku
poems exhibit creative word associations. We rely
on this hypothesis to first generate candidate word
associations starting from a seed word and follow-
ing random walks through WANs, but also to rank
candidate Haiku poems by measuring the density of
WAN-based associations they exhibit.
2.3 Poetry Generation
Although several automatic and semi-automatic po-
etry generation systems were developed over the
years, most of them did not rise above the level of
?party tricks? (Manurung et al, 2000). In his the-
sis, (Manurung, 2003), defined a poem to be a text
that meets three properties: meaningfulness, gram-
maticality and poeticness. Two of the few systems
that attempt to explicitly represent all three prop-
erties are reported in (Gervas, 2001) and (D??az-
Agudo et al, 2002). Both systems take as input a
prose message provided by the user, and translate it
into formal Spanish poetry. The system proposed
in (Manurung et al, 2000) is similar in that it fo-
cuses on the syntactic and phonetic patterns of the
poem, putting less stress on the semantics. The sys-
33
tem starts with a simple seed and gradually devel-
ops a poem, by making small syntactic and semantic
changes at every step.
Specifically in the subfield of Haiku generation,
the Haiku generator presented in (Wong and Chun,
2008) produces candidate poems by combining lines
taken from blogs. The system then ranks the can-
didates according to semantic similarity, which is
computed using the results returned by a search en-
gine when querying for words in each line. Hitch-
Haiku (Tosa et al, 2008), another Haiku generation
system, starts from two seed words given by the user.
It retrieves two phrases containing these words from
a corpus, and then adds a third phrase that connects
both input words, using lexical resources.
In our work, we induce a statistical language
model of the structure of Haikus from an analysis
of a corpus of English Haikus, and explore ways to
combine chains of lexical associations into the ex-
pected Haiku syntactic structure. The key issues we
investigate are the importance of WAN-based asso-
ciations in the Haiku generation process, and how a
chain of words, linked through WAN-based associa-
tions, can be composed into a Haiku-like structure.
2.4 Haiku
Haiku is a form of poetry originated in Japan in
the sixteenth century. The genre was adopted in
Western languages in the 20th Century. The origi-
nal form of a poem is of three lines of five, seven
and five syllables (although this constraint is loos-
ened in non-Japanese versions of Haiku (Gilbert and
Yoneoka, 2000)). Haiku, by its nature, aims to re-
flect or evoke emotion using an extremely economi-
cal linguistic form; most Haiku use present tense and
use no judgmental words; in addition, functional or
syntactic words may be dropped. Traditional Haiku
involve reference to nature and seasons, but modern
and western Haiku are not restricted to this theme1.
We adopt the less ?constraining? definition of the
author Jack Kerouac (2004) for a Haiku ?I propose
that the ?Western Haiku? simply say a lot in three
short lines in any Western language. Above all, a
Haiku must be very simple and free of all poetic
1Senryu poetry, similar in form to Haiku, is the Japanese
genre of poems that relate to human and relationships, and may
be humorous. Hereafter, we use Haiku for both the original
definition and the Senryu as well.
trickery and make a little picture and yet be as airy
and graceful as a Vivaldi Pastorella.? (pp. x-xi). In
addition, we are guided by the saying ? The best
haiku should leave the reader wondering ? (Quoted
in (Blasko and Merski, 1998))
2.5 Word Association Norms
The interest in word associations is common to
many fields. Idiosyncrasy of associations was used
as a diagnostic tool at the beginning of the 20th cen-
tury, but nowadays the majority of approaches deal
less with particular associations and more with gen-
eral patterns in order to study the structure of the
mental lexicon and of semantic memory (Rubinsten
et al, 2005).
Word Association Norms (WAN) are a collection
of cue words and the set of free associations that
were given as responses to the cue, accompanied
with quantitative and statistical measures. Subjects
are given a word and asked to respond immediately
with the first word that comes to their mind. The
largest WAN we know for English is the University
of South Florida Free Association Norms (Nelson et
al., 1998).
Word Association Norms and Thesauri in NLP
Sinopalnikova and Smrz (2004) have shown that
when building and extending semantic networks,
WANs have advantages over corpus-based meth-
ods. They found that WANs cover semantic rela-
tions that are difficult to acquire from a corpus: 42%
of the non-idiosyncratic cue-target pairs in an En-
glish WAN never co-appeared in a 10 words win-
dow in a large balanced text corpus. From the point
of view of computational creativity, this is encourag-
ing, since it suggests that association-based content
generation can lead to texts that are both sensible
and novel. (Duch and Pilichowski, 2007)?s work,
from a neuro-cognitive perspective, generates neol-
ogisms based, among other data, on word associa-
tion. (Duch and Pilichowski, 2007) sums ?creativity
requires prior knowledge, imagination and filtering
of the results.?
3 WordNet vs. Associations
Word association norms add an insight on language
that is not found in WordNet or are hard to acquire
from corpora, and therefore can be used as an ad-
ditional tool in NLP applications and computational
34
creativity.
We choose the Haiku generation task using word
associations, since this genre of poetry encapsulates
meaning in a special way. Haiku tend to use words
which are connected through associative or phono-
logical connections (very often ambiguous).
We hypothesize that word-associations are good
catalyzers for creativity, and use them as a building
block in the creative process of Haiku generation.
We first test this hypothesis by analyzing a corpus of
existing Haiku poems.
3.1 Analyzing existing text
Can the creativity of text as reflected in word as-
sociations be quantified? Are Haiku poems indeed
more associative than newswire text or prose? If
this is the case, we expect Haiku to have more asso-
ciative relations, which cannot be easily recovered
by WordNet than other type of text. We view the
WAN as an undirected graph in which the nodes
are stemmed words, and two nodes are connected
iff one of them is a cue for the other. We take the
associative distance between two words to be the
number of edges in the shortest path between the
words in the associations-graph. Interestingly, al-
most any word pair in the association graph is con-
nected with a path of at most 3 edges. Thus, we
take two words to be associatively related if their
associative distance is 1 or 2. Similarly, we define
the WordNet distance between two stemmed words
to be the number of edges in the shortest path be-
tween any synset of one word to any synset of the
other word2. Two words are WordNet-related if their
WordNet distance is less than 4 (this is consistent
with works on lexical-cohesion, (Morris and Hirst,
1991)).
We take the associativity of a piece of text to be
the number of associated word pairs in the text, nor-
malized by the number of word pairs in the text of
which both words are in the WAN.3 We take the
WordNet-relations level of a piece of text to be the
number of WordNet-related word pairs in the text.
2This is the inverse of the path-similarity measure of (Ped-
ersen et al, 2004).
3This normalization is performed to account for the limited
lexical coverage of the WAN. We don?t want words that appear
in a text, but are not covered by the WAN, to affect the associa-
tivity level of the text.
SOURCE AVG. ASSOC AVG. WORDNETRELATIONS (<3) RELATIONS (<4)
News 0.26 2.02
Prose 0.22 1.4
Haiku 0.32 1.38
Table 1: Associative and WordNet relations in various
text genres
We measure the average associativity and Word-
Net levels of 200 of the Haiku in our Haiku Cor-
pus (Section 4.1), as well as of random 12-word
sequences from Project Gutenberg and from the
NANC newswire corpus.
The results are presented in Table 1.
Perhaps surprisingly, the numbers for the Guten-
berg texts are lower on all measures. This is at-
tributed to the fact that Gutenberg texts have many
more pronouns and non-content words than the
Haiku and newswire text. Haiku text appears to
be more associative than newswire text. Moreover,
newswire documents have many more WordNet-
relations than the Haiku poems ? whenever words
are related in Haiku, this relatedness tends to be cap-
tured via the association network rather than via the
WordNet relations. The same trend is apparent also
when considering the Gutenberg numbers: they have
about 15% less associations than newswire text, but
about 30% less WordNet-relations. This supports
the claim that associative information which is not
readily available in WordNet is a good indicator of
creative content.
3.2 Generating creative content
We now investigate how word-associations can help
in the process of generating Haikus. We define
a 5 stage generative process: theme selection in
which the general theme of the Haiku is decided,
syntactic planning, which sets the Haiku form and
syntactic constraints, content selection / semantic
planning which combines syntactic and aesthetic
constraints with the theme selected in the previous
stages to form good building blocks, filtered over-
generation of many Haiku based on these selected
building blocks, and finally re-ranking of the gen-
erated Haiku based on external criteria.
The details of the generation algorithm are pre-
sented in Section 4.2. Here we focus on the creative
aspect of this process ? theme selection. Our main
claim is that WANs are a good source for interest-
35
ing themes. Specifically, interesting themes can be
obtained by performing a short random walk on the
association graph induced by the WAN network.
Table 2 presents the results of several random
walks of 3 steps starting from the seed words ?Dog?,
?Winter?, ?Nature? and ?Obsession?. For compar-
ison, we also present the results of random walks
over WordNet glosses for the same seeds.
We observe that the association network is bet-
ter for our needs than WordNet. Random walks in
WordNet are more likely to stay too close to the seed
word, limiting the poetic options, or to get too far
and produce almost random connections.
4 Algorithm for generating Haiku
4.1 Dataset
We used the Word Association Norms (WAN) of the
University of South Florida 4 (Nelson et al, 1998)
for discovering associations of words. The dataset
(Appendix A, there) includes 5,019 cue words and
10,469 additional target that were collected with
more than 6,000 participants since 1973.
We have compiled a Haiku Corpus, which in-
cludes approximately 3,577 Haiku in English of var-
ious sources (amateurish sites, children?s writings,
translations of classic Japanese Haiku of Bashu and
others, and ?official? sites of Haiku Associations
(e.g., Haiku Path - Haiku Society of America).
For the content selection part of the algorithms,
we experimented with two data sources: a corpus of
1TB web-based N-grams supplied by Google, and
the complete text of Project Gutenberg. The Guten-
berg data has the advantage of being easier to POS-
tag and contains less restricted-content, while the
Google Web data is somewhat more diverse.
4.2 Algorithm Details
Our Haiku generation algorithm includes 5 stages:
theme selection, syntactic planning, content selec-
tion, filtered over generation, and ranking.
The Theme Selection stage is in charge of dictat-
ing the overall theme of our Haiku. We start with
a user-supplied seed word (e.g. WINTER). We then
consult the Association database in order to enrich
the seed word with various associations. Ideally, we
would like these associations to be close enough to
4http://w3.usf.edu/FreeAssociation/
the seed word to be understandable, yet far enough
away from it as to be interesting. After some ex-
perimenting, we came up with the following heuris-
tic, which we found to provide adequate results. We
start with the seed word, and conduct a short random
walk on the associations graph. Each random step
is comprised of choosing a random direction (either
?Cue? or ?Target?) using a uniform distribution, and
then a random neighbor according to its relative fre-
quency. We conduct several (8) such walks, each
with 3 steps, and keep all the resulting words. This
gives us mostly close, probable associations, as well
as some less probable, further away from the seed.
The syntactic planning stage determines the
form of the generated Haiku, setting syntactic and
aesthetic constraints for the generative process. This
is done in a data-driven way by considering common
line patterns from our Haiku corpus. In a training
stage, we POS-tagged each of the Haiku, and then
extracted a pattern from each of the Haiku lines. A
line-pattern is a sequence of POS-tags, in which the
most common words are lexicalized to include the
word-form in addition to the POS-tag. An example
for such a line pattern might be DT the JJ NN.
We kept the top-40 frequent patterns for each of the
Haiku lines, overall 120 patterns. When generating a
new Haiku, we choose a random pattern for the first
line, then choose the second line pattern conditioned
on the first, and the third line pattern conditioned
on the second. The line patterns are chosen with a
probability proportional to their relative frequencies
in the training corpus. For the second and third lines
we use the conditional probabilities of a pattern ap-
pearing after the previous line pattern. The result
of this stage is a 3-line Haiku skeleton, dictating the
number of words on each line, their POS-tags, and
the placement of specific function words.
In the Content Selection stage, we look for pos-
sible Haiku lines, based on our selected theme and
syntactic structure. We go over our candidate lines5,
and extract lines which match the syntactic patterns
and contain a stemmed appearance of one of the
stemmed theme words. In our current implemen-
tation, we require the first line to contain the seed
word, and the second and third line to contain any of
5These are POS-tagged n-grams extracted from a large text
corpora: the Google T1 dataset or Project Gutenberg
36
SEED WAN WORDNET
Dog puppy adorable cute heel villain villainess
Dog cat curious george hound scoundrel villainess
Winter summer heat microwave wintertime solstice equinox
Winter chill cold alergy midwinter wintertime season
Nature animals instinct animals world body crotch
Nature natural environment surrounding complexion archaism octoroon
Obsession cologne perfume smell fixation preoccupation thought
Obsession compulsion feeling symptom compulsion onomatomania compulsion
Table 2: Some random walks on the WordNet and WAN induced graphs
the theme words. Other variations, such as choos-
ing a different word set for each line, are of course
possible.
The over generation stage involves creating
many possible Haiku candidates by randomly
matching lines collected in the content selection
stage. We filter away Haiku candidates which have
an undesired properties, such as repeating the same
content-word in two different lines.
All of the generated Haiku obey the syntactic and
semantic constraints, but not all of them are interest-
ing. Thus, we rank the Haiku in order to weed out
the better ones. The top-ranking Haiku is the output
of our system. Our current heuristic prefers highly
associative Haikus. This is done by counting the
number of 1st and 2nd degree associations in each
Haiku, while giving more weight to 2nd degree as-
sociations in order to encourage ?surprises?. While
all the candidate Haiku were generated based on a
common theme of intended associative connections,
the content selection and adherence to syntactic con-
straints introduce additional content words and with
them some new, unintended associative connections.
Our re-ranking approach tries to maximize the num-
ber of such connections.6
5 Evaluation
The ultimate goal of a poetry generation system is to
produce poems that will be considered good if writ-
ten by a human poet. It is difficult to evaluate to what
extent a poetry generation system can meet this goal
(Ritchie, 2001; Manurung et al, 2000). Difficulties
arise from two major sources: first, since a creative
6While this heuristic works well, it leaves a lot to be desired.
It considers only the quantity of the associations, and not their
quality. Indeed, when looking at the Haiku candidates produced
in the generation stage, one can find many interesting pieces,
where some of the lower ranking ones are far better than the top
ranking.
work should be novel, it cannot be directly evaluated
by comparison to some gold standard. Second, it is
hard for people to objectively evaluate the quality of
poetry. Even determining whether a text is a poem
or not is not an easy task, as readers expect poetry
to require creative reading, and tolerate, to some ex-
tent, ungrammatical structures or cryptic meaning.
5.1 ?Turing Test? Experiment
To evaluate the quality of Gaiku, we asked a group
of volunteers to read a set of Haiku, indicate how
much they liked each one (on a scale of 1-5), and
classify each Haiku as written by a human or by a
computer.
We compiled two sets of Haiku. The first set
(AUTO) contained 25 Haiku. 10 Haiku chosen at
random from our Haiku corpus, and 15 computer
generated ones. The computer generated Haiku
were created by identifying the main word in the first
line of each human-written Haiku, and passing it as
a seed word to the Haiku generation algorithm (in
case a first line in human-written Haiku contained
two main words, two Haiku were generated). We in-
cluded the top-ranking Haiku returning from a single
run of the system for each seed word. The only hu-
man judgement in compiling this set was in the iden-
tification of the main words of the human Haiku.
The second set (SEL) was compiled of 9 haiku po-
ems that won awards7, and 17 computer Haiku that
were selected by us, after several runs of the auto-
matic process. (Again, each poem in the automatic
poems set shared at least one word with some poem
in the human Haiku set).
The subjects were not given any information
about the number of computer-generated poems in
the sets.
7Gerald Brady Memorial Award Collection http://www.hsa-
haiku.org/bradyawards/brady.htm 2006-2007
37
The AUTO questionnaire was answered by 40
subjects and the SEL one by 22. (Altogether, 52 dif-
ferent people took part in the experiment, as some
subjects answered both versions). The subjects were
all adults (age 18 to 74), some were native English
speakers and others were fully fluent in English. Ex-
cept a few, they did not have academic background
in literature.
5.2 Results and Discussion
Results are presented in Table 3 and Figure 1.
Overall, subjects were correct in 66.7% of their
judgements in AUTO and 61.4% in SEL. The aver-
age grade that a poem - human or machine-made -
received correlates with the percentage of subjects
who classified it as human. The average grade and
rate of acceptance as written by human were signifi-
cantly higher for the Haiku written by people. How-
ever, some computer Haiku rivaled the average hu-
man poem in both measures. This is true even for
AUTO, in which both the generation and the selec-
tion processes were completely automatic. The best
computer Haiku of SEL scored better than most hu-
man Haiku in both measures.
The best computer poem in SEL was:
early dew / the water contains / teaspoons of honey
which got an average grade of 3.09 and was classi-
fied as human by 77.2% of the subjects.
At the other extreme, the computer poem (SEL):
space journey / musical instruments mythology /
of similar drugs
was classified as human by only 9% of the subjects,
and got an average grade of 2.04.
The best Haiku in the AUTO set was:
cherry tree / poisonous flowers lie / blooming
which was classified as human by 72.2% of the sub-
jects and got an average grade of 2.75.
The second human-like computer generated
Haiku in each set were:
spring bloom / showing / the sun?s pyre
(AUTO, 63.8% human) and:
blind snakes / on the wet grass / tombstoned terror
(SEL, 77.2% human).
There were, expectedly, lots of disagreements.
Poetry reading and evaluation is subjective and by
Human Poems Gaiku
AUTO avg. % classified as Human 72.5% 37.2%
avg. grade 2.86 2.11
SEL avg. % classified as Human 71.7% 44.1%
avg. grade 2.84 2.32
Table 3: Turing-test experiment results
itself (in particular for Haiku) a creative task. In ad-
dition, people have very different ideas in mind as to
a computer?s ability to do things. (One subject said,
for example, that the computer generated
holy cow / a carton of milk / seeking a church
is too stupid to be written by a computer; how-
ever, content is very strongly connected and does
not seem random). On the other end, subjects often
remarked that some of the human-authored Haiku
contained metaphors which were too obvious to be
written by a human.
Every subject was wrong at least 3 times (at least
once in every direction); every poem was wrongly-
classified at least once. Some really bad auto-poems
got a good grade here and there, while even the most
popular human poems got a low grade sometimes.
6 Discussion and Future Work
Word association norms were shown to be a useful
tool for a computational creativity task, aiding in the
creation of an automatic Haiku-generation software,
which is able to produce ?human-like? Haiku. How-
ever, associations can be used for many other tasks.
In the last decade, lexical chains are often used in
various NLP tasks such as text summarization or text
categorization; WordNet is the main resource for
detecting the cohesive relationships between words
and their relevance to a given chain (Morris and
Hirst, 1991). We believe that using word association
norms can enrich the information found in WordNet
and enable the detection of more relevant words.
Another possible application is for assisting
word-finding problem of children with specific lan-
guage impairments (SLI). A useful tactic practiced
as an assistance to retrieve a forgotten word is by
saying all words that come to mind. The NLP task,
therefore, is for a set of a given associations, recon-
struct the targeted word.
38
0 20 40 60 80 100
1.5
2
2.5
3
3.5
4
% of subjects who classified the poem as written by a human
Av
ar
eg
e 
gr
ad
e
 
 
Gaiku poems
Human poems
0 20 40 60 80 100
1.5
2
2.5
3
3.5
4
% of subjects who classified the poem as written by a human
Av
ar
eg
e 
gr
ad
e
 
 
Gaiku poems
Human poems
Figure 1: Average grades and percentages of subjects who classified poems as written by humans, for AUTO (left)
and SEL. Circles represent Haiku written by people, and stars represent machine-made Haiku
References
D.G. Blasko and D.W. Merski. 1998. Haiku poetry
and metaphorical thought: An invention to interdisci-
plinary study. Creativity Research Journal, 11.
M.A. Boden. 1998. Creativity and artificial intelligence.
Artificial Intelligence, 103(1?2).
F. de Saussure, C. Bally, A. Riedlinger, and
A. Sechehaye. 1949. Cours de linguistique gen-
erale. Payot, Paris.
B. D??az-Agudo, P. Gerva?s, and P. A. Gonza?lez-Calero.
2002. Poetry generation in COLIBRI. In Proc. of EC-
CBR.
W. Duch and M. Pilichowski. 2007. Experiments with
computational creativity. Neural Information Process-
ing, Letters and Reviews, 11(3).
P. Gervas. 2001. An expert system for the composition of
formal Spanish poetry. Journal of Knowledge-Based
Systems, 14.
R. Gilbert and J. Yoneoka. 2000. From 5-7-5 to 8-8-8:
An investigation of Japanese Haiku metrics and impli-
cations for English Haiku. Language Issues: Journal
of the Foreign Language Education Center.
D. Hardcastle. 2007. Cryptic crossword clues: Generat-
ing text with a hidden meaning BBKCS-07-04. Tech-
nical report, Birkbeck College, London.
J. Kerouac. 2004. Book of Haikus. Enitharmon Press.
H.M. Manurung, G. Ritchie, and H. Thompson. 2000.
Towards a computational model of poetry generation.
In Proc. of the AISB?00.
H.M. Manurung. 2003. An evolutionary algorithm ap-
proach to poetry generation. Ph.D. thesis, University
of Edinburgh.
S.A. Mendick. 1969. The associative basis of the cre-
ative process. Psychological Review.
N. Montfort. 2006. Natural language generation and nar-
rative variation in interactive fiction. In Proc. of Com-
putational Aesthetics Workshop at AAAI 2006, Boston.
J. Morris and G. Hirst. 1991. Lexical cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17.
D.L. Nelson, C.L. Mcevoy, and T.A. Schreiber.
1998. The University of South Florida Word
Association, Rhyme, and Word Fragment Norms.
http://www.usf.edu/FreeAssociation/.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In HLT-NAACL 2004: Demonstrations.
G. Ritchie. 2001. Assessing creativity. In Proc. of
AISB?01 Symposium.
G. Ritchie. 2006. The transformational creativity hy-
pothesis. New Generation Computing, 24.
O. Rubinsten, D. Anaki, A. Henik, S. Drori, and Y. Faran.
2005. Free association norms in the Hebrew language.
Word Norms in Hebrew. (In Hebrew).
A. Sinopalnikova and P. Smrz. 2004. Word association
thesaurus as a resource for extending semantic net-
works. In Communications in Computing.
N. Tosa, H. Obara, and M. Minoh. 2008. Hitch haiku:
An interactive supporting system for composing haiku
poem. In Proc. of the 7th International Conference on
Entertainment Computing.
M. Tsan Wong and A. Hon Wai Chun. 2008. Automatic
Haiku generation using vsm. In Proc. of ACACOS?08,
April.
39
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 129?133,
Paris, October 2009. c?2009 Association for Computational Linguistics
Hebrew Dependency Parsing: Initial Results
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,elhadad}@cs.bgu.ac.il
Abstract
We describe a newly available Hebrew
Dependency Treebank, which is extracted
from the Hebrew (constituency) Tree-
bank. We establish some baseline un-
labeled dependency parsing performance
on Hebrew, based on two state-of-the-art
parsers, MST-parser and MaltParser. The
evaluation is performed both in an artifi-
cial setting, in which the data is assumed
to be properly morphologically segmented
and POS-tagged, and in a real-world set-
ting, in which the parsing is performed on
automatically segmented and POS-tagged
text. We present an evaluation measure
that takes into account the possibility of
incompatible token segmentation between
the gold standard and the parsed data.
Results indicate that (a) MST-parser per-
forms better on Hebrew data than Malt-
Parser, and (b) both parsers do not make
good use of morphological information
when parsing Hebrew.
1 Introduction
Hebrew is a Semitic language with rich morpho-
logical structure and free constituent order.
Previous computational work addressed unsu-
pervised Hebrew POS tagging and unknown word
resolution (Adler, 2007), Hebrew NP-chunking
(Goldberg et al, 2006), and Hebrew constituency
parsing (Tsarfaty, 2006; Golderg et al, 2009).
Here, we focus on Hebrew dependency parsing.
Dependency-parsing got a lot of research at-
tention lately, in part due to two CoNLL shared
tasks focusing on multilingual dependency parsing
(Buchholz and Erwin, 2006; Nivre et al, 2007).
These tasks include relatively many parsing re-
sults for Arabic, a Semitic language similar to He-
brew. However, parsing accuracies for Arabic usu-
ally lag behind non-semitic languages. Moreover,
while there are many published results, we could
not find any error analysis or even discussion of
the results of Arabic dependency parsing models,
or the specific properties of Arabic making it easy
or hard to parse in comparison to other languages.
Our aim is to evaluate current state-of-the-art
dependency parsers and approaches on Hebrew
dependency parsing, to understand some of the
difficulties in parsing a Semitic language, and to
establish a strong baseline for future work.
We present the first published results on Depen-
dency Parsing of Hebrew.
Some aspects that make Hebrew challenging
from a parsing perspective are:
Affixation Common prepositions, conjunctions
and articles are prefixed to the following word,
and pronominal elements often appear as suffixes.
The segmentation of prefixes and suffixes is of-
ten ambiguous and must be determined in a spe-
cific context only. In term of dependency pars-
ing, this means that the dependency relations oc-
cur not between space-delimited tokens, but in-
stead between sub-token elements which we?ll re-
fer to as segments. Furthermore, any mistakes in
the underlying token segmentations are sure to be
reflected in the parsing accuracy.
Relatively free constituent order The ordering
of constituents inside a phrase is relatively free.
This is most notably apparent in the verbal phrases
and sentential levels. In particular, while most sen-
tences follow an SVO order, OVS and VSO con-
figurations are also possible. Verbal arguments
can appear before or after the verb, and in many
ordering. For example, the message ?went from
Israel to Thailand? can be expressed as ?went to
Thailand from Israel?, ?to Thailand went from Is-
rael?, ?from Israel went to Thailand?, ?from Israel
to Thailand went? and ?to Thailand from Israel
went?. This results in long and flat VP and S struc-
tures and a fair amount of sparsity, which suggests
129
that a dependency representations might be more
suitable to Hebrew than a constituency one.
Rich templatic morphology Hebrew has a
very productive morphological structure, which
is based on a root+template system. The pro-
ductive morphology results in many distinct word
forms and a high out-of-vocabulary rate, which
makes it hard to reliably estimate lexical param-
eters from annotated corpora. The root+template
system (combined with the unvocalized writing
system) makes it hard to guess the morphological
analyses of an unknown word based on its prefix
and suffix, as usually done in other languages.
Unvocalized writing system Most vowels are
not marked in everyday Hebrew text, which re-
sults in a very high level of lexical and morpho-
logical ambiguity. Some tokens can admit as many
as 15 distinct readings, and the average number of
possible morphological analyses per token in He-
brew text is 2.7, compared to 1.4 in English (Adler,
2007). This means that on average, every token is
ambiguous with respect to its POS and morpho-
logical features.
Agreement Hebrew grammar forces morpho-
logical agreement between Adjectives and Nouns
(which should agree in Gender and Number and
definiteness), and between Subjects and Verbs
(which should agree in Gender and Number).
2 Hebrew Dependency Treebank
Our experiments are based on the Hebrew De-
pendency Treebank (henceforth DepTB), which
we derived from Version 2 of the Hebrew
Constituency Treebank (Guthmann et al, 2009)
(henceforth TBv2). We briefly discuss the conver-
sion process and the resulting Treebank:
Parent-child dependencies TBv2 marks sev-
eral kinds of dependencies, indicating the mother-
daughter percolation of features such as number,
gender, definiteness and accusativity. See (Guth-
mann et al, 2009) for the details. We follow
TBv2?s HEAD, MAJOR and MULTIPLE depen-
dency marking in our-head finding rules. When
these markings are not available we use head find-
ing rules in the spirit of Collins. The head-finding
rules were developed by Reut Tsarfaty and used
in (Tsarfaty and Sima?an, 2008). We slightly ex-
tended them to handle previously unhandled cases.
Some conventions in TBv2 annotations resulted in
bad dependency structures. We identified these
constructions and transformed the tree structure,
Figure 1: Coordinated Verbs
Figure 2: Coordinated Sentence
either manually or automatically, prior to the de-
pendency extraction process.
The conversion process revealed some errors
and inconsistencies in TBv2, which we fixed.
We take relativizers as the head S and SBAR,
and prepositions as the heads of PPs. In the case
the parent of a word X is an empty element, we
take the parent of the empty element as the par-
ent of X instead. While this may result in non-
projective structures, in practice all but 34 of the
resulting trees are projective.
We take conjunctions to be the head of a coordi-
nated structure, resulting in dependency structures
such as the one in Figures 1 and 2. Notice how
in Figure 1 the parent of the subject ????/He? is
the coordinator ??/and?, and not one of the verbs.
While this makes things harder for the parser, we
find this representation to be much cleaner and
more expressive than the usual approach in which
the first coordinated element is taken as the head
of the coordinated structure.1
Dependency labels TBv2 marks 3 kinds of
functional relations: Subject, Object and Comple-
mentizer. We use these in our conversion pro-
cess, and label dependencies as being SBJ, OBJ
or CMP, as indicated in TBv2. We also trivially
mark the ROOT dependency, and introduce the re-
lations INF PREP, AT INF POS INF RB INF be-
tween a base word and its suffix for the cases of
suffix-inflected prepositions, accusative suffixes,
possessive suffixes and inflected-adverbs, respec-
tively. Still, most dependency relations remain un-
labeled. We are currently seeking a method of re-
liably labeling the remaining edges with a rich set
1A possible alternative would be to allow multiple par-
ents, as done in (de Marneffe et al, 2006), but current parsing
algorithms require the output to be tree structured.
130
of relations. However, in the current work we fo-
cus on the unlabeled dependency structure.
POS tags The Hebrew Treebank follows a syn-
tactic tagging scheme, while other Hebrew re-
sources prefer a more morphological/dictionary-
based scheme. For a discussion of these two tag-
ging schemes in the context of parsing, see (Gold-
erg et al, 2009). In DepTB, we kept the two
tagsets, and each token has two POS tags asso-
ciated with it. However, as current dependency
parsers rely on an external POS tagger, we per-
formed all of our experiments only with the mor-
phological tagset, which is what our tagger pro-
duces.
3 The Parsing Models
To establish some baseline results for Hebrew de-
pendency parsing, we experiment with two pars-
ing models, the graph-based MST-parser (Mc-
Donald, 2006) and the transition-based MaltParser
(Nivre et al, 2006). These two parsers repre-
sent the current mainstream approaches for de-
pendency parsing, and each was shown to pro-
vide state-of-the-art results on many languages
(CoNLL Shared Task 2006, 2007).
Briefly, a graph-based parsing model works by
assigning a score to every possible attachment be-
tween a pair (or a triple, for a second-order model)
of words, and then inferring a global tree struc-
ture that maximizes the sum of these local scores.
Transition-based models work by building the de-
pendency graph in a sequence of steps, where each
step is dependent on the next input word(s), the
previous decisions, and the current state of the
parser. For more details about these parsing mod-
els as well as a discussion on the relative benefits
of each model, see (McDonald and Nivre, 2007).
Contrary to constituency-based parsers, depen-
dency parsing models expect a morphologically
segmented and POS tagged text as input.
4 Experiments
Data We follow the train-test-dev split estab-
lished in (Tsarfaty and Sima?an, 2008). Specifi-
cally, we use Sections 2-12 (sentences 484-5724)
of the Hebrew Dependency Treebank as our train-
ing set, and report results on parsing the develop-
ment set, Section 1 (sentences 0-483). We do not
evaluate on the test set in this work.
The data in the Treebank is segmented and
POS-tagged. All of the models were trained on the
gold-standard segmented and tagged data. When
evaluating the parsing models, we perform two
sets of evaluations. The first one is an oracle ex-
periment, assuming gold segmentation and tag-
ging is available. The second one is a real-world
experiment, in which we segment and POS-tag the
test-set sentences using the morphological disam-
biguator described in (Adler, 2007; Goldberg et
al., 2008) prior to parsing.
Parsers and parsing models We use the freely
available implementation of MaltParser2 and
MSTParser3, with default settings for each of the
parsers.
For MaltParser, we experiment both with the de-
fault feature representation (MALT) and the fea-
ture representation used for parsing Arabic in
CoNLL 2006 and 2007 multilingual dependency
parsing shared tasks (MALT-ARA).
For MST parser, we experimented with first-
order (MST1) and second-order (MST2) models.
We varied the amount of lexical information
available to the parser. Each of the parsers was
trained on 3 datasets: LEXFULL, in which all the
lexical items are available, LEX20, in which lexi-
cal items appearing less than 20 times in the train-
ing data were replaced by an OOV token, and
LEX100 in which we kept only lexical items ap-
pearing more than 100 times in training.
We also wanted to control the effect of the rich
morphological information available in Hebrew
(gender and number marking, person, and so on).
To this end, we trained and tested each model ei-
ther with all the available morphological informa-
tion (+MORPH) or without any morphological in-
formation (-MORPH).
Evaluation Measure We evaluate the resulting
parses in terms of unlabeled accuracy ? the percent
of correctly identified (child,parent) pairs4. To be
precise, we calculate:
number of correctly identified pairs
number of pairs in gold parse
For the oracle case in which the gold-standard
token segmentation is available for the parser, this
is the same as the traditional unlabeled-accuracy
evaluation metric. However, in the real-word set-
ting in which the token segmentation is done auto-
matically, the yields of the gold-standard and the
2http://w3.msi.vxu.se/?jha/maltparser/
3http://sourceforge.net/projects/mstparser/
4All the results are macro averaged. The micro-averaged
numbers are about 2 percents higher for all cases.
131
Features MST1 MST2 MALT MALT-ARA
-M
OR
PH Full Lex 83.60 84.31 80.77 80.32Lex 20 82.99 84.52 79.69 79.40
Lex 100 82.56 83.12 78.66 78.56
+M
OR
PH Full Lex 83.60 84.39 80.77 80.73Lex 20 83.60 84.77 79.69 79.84
Lex 100 83.23 83.80 78.66 78.56
Table 1: Unlabeled dependency accuracy with
oracle token segmentation and POS-tagging.
Features MST1 MST2 MALT MALT-ARA
-M
OR
PH Full Lex 75.64 76.38 73.03 72.94Lex 20 75.48 76.41 72.04 71.88
Lex 100 74.97 75.49 70.93 70.73
+M
OR
PH Full Lex 73.90 74.62 73.03 73.43Lex 20 73.56 74.41 72.04 72.30
Lex 100 72.90 73.78 70.93 70.97
Table 2: Unlabeled dependency accuracy with
automatic token segmentation and POS-tagging.
automatic parse may differ, and one needs to de-
cide how to handle the cases in which one or more
elements in the identified (child,parent) pair are
not present in the gold-standard parse. Our evalua-
tion metric penalizes these cases by regarding any
such case as a mistake.
5 Results and Analysis
Results are presented in Tables 1 and 2.
It seems that the graph-based parsers perform
better than the transitions-based ones. We at-
tribute this to 2 factors: first, our representa-
tion of coordinated structure is hard to capture
with a greedy local search as performed by a
transition-based parser, because we need to de-
fer many attachment decisions until the final co-
ordinator is revealed. The global inference of the
graph-based parser is much more robust to these
kinds of structure. Indeed, when evaluating the
gold-morphology, fully-lexicalized models on a
subset of the test-set (314 sentences) which does
not have coordinated structures, the accuracy of
MALT improves in 3.98% absolute (from 80.77 to
84.75), while MST improves only in 2.66% abso-
lute (from 83.60 to 86.26). Coordination is hard
for both parsing models, but more so to the transi-
tion based MALT.
Second, it might be hard for a transition-based
parser to handle the free constituent order of He-
brew, as it has no means of generalizing from the
training set to various possible constituent order-
ing. The graph-based parser?s features and infer-
ence method do not take constituent order into ac-
count, making it more suitable for free constituent
order language.
As expected, the Second-order graph based
models perform better than the first-order ones.
Surprisingly, the Arabic-optimized feature-set do
not perform better than the English one for the
transition-based parsers. Overall, morphological
information seems to contribute very little (if at
all) to any of the parsers in the gold-morphology
(oracle) setting. MALTARA gets some benefit
from the morphological information in the fully-
lexicalized case, while the MST variants benefit
from morphology in the lexically-pruned models.
Overall, full lexicalization is not needed. In-
deed, less lexicalized LEX20 2nd-order graph-
based models perform better than the fully lexi-
calized ones. This strengthens our intuition that
robust lexical statistics are hard to acquire from
small annotated corpora, even more so for a lan-
guage with productive morphology such as He-
brew.
Moving from the oracle morphological disam-
biguation to an automatic one greatly hurts the per-
formance of all the models. This is in line with re-
sults for Hebrew constituency parsing, where go-
ing from gold segmentation to a parser derived one
caused a similar drop in accuracy (Golderg et al,
2009). This suggests that we should either strive
to improve the tagging accuracy, or perform joint
inference for parsing and morphological disam-
biguation. We believe the later would be a better
way to go, but it is currently unsupported in state-
of-the-art dependency parsing algorithms.
Interestingly, in the automatic morphological
disambiguation setting MALTARA benefits a little
from the addition of morpological features, while
the MST models perform better without these fea-
tures.
6 Conclusions
We presented the first results for unlabeled de-
pendency parsing of Hebrew, with two state-of-
the-art dependency parsing models of different
families. We experimented both with gold mor-
phological information, and with an automatically
derived one. It seems that graph-based models
have a slight edge in parsing Hebrew over current
transition-based ones. Both model families are not
currently making good use of morphological infor-
mation.
132
References
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Sabine Buchholz and Marsi Erwin. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006. Noun phrase chunking in hebrew: Influence
of lexical and morphological features. In Proc. of
COLING/ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-Taggers
(when given a good start). In Proc. of ACL.
Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing
performance using a wide coverage lexicon, fuzzy
tag-set mapping, and EM-HMM-based lexical prob-
abilities. In Proc of EACL.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc of TLT.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proc. of EMNLP.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Kubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of the EMNLP-CoNLL.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proc. of CoLING, August.
Reut Tsarfaty. 2006. Integrated morphological and
syntactic disambiguation for modern hebrew. In
Proceedings of ACL-SRW.
133
Book Review
Natural Language Processing with Python
Steven Bird, Ewan Klein, and Edward Loper
(University of Melbourne, University of Edinburgh, and BBN Technologies)
Sebastopol, CA: O?Reilly Media, 2009, xx+482 pp; paperbound,
ISBN 978-0-596-51649-9, $44.99; on-line free of charge at nltk.org/book
Reviewed by
Michael Elhadad
Ben-Gurion University
This book comes with ?batteries included? (a reference to the phrase often used
to explain the popularity of the Python programming language). It is the compan-
ion book to an impressive open-source software library called the Natural Language
Toolkit (NLTK), written in Python. NLTK combines language processing tools (token-
izers, stemmers, taggers, syntactic parsers, semantic analyzers) and standard data sets
(corpora and tools to access the corpora in an efficient and uniform manner). Al-
though the book builds on the NLTK library, it covers only a relatively small part
of what can be done with it. The combination of the book with NLTK, a growing
system of carefully designed, maintained, and documented code libraries, is an extra-
ordinary resource that will dramatically influence the way computational linguistics
is taught.
The book attempts to cater to a large audience: It is a textbook on computational lin-
guistics for science and engineering students; it also serves as practical documentation
for the NLTK library, and it finally attempts to provide an introduction to programming
and algorithm design for humanities students. I have used the book and its earlier
on-line versions to teach advanced undergraduate and graduate students in computer
science in the past eight years.
The book adopts the following approach:
 It is first a practical approach to computational linguistics. It provides
readers with practical skills to solve concrete tasks related to language.
 It is a hands-on programming text: The ultimate goal of the book is to
empower students to write programs that manipulate textual data and
perform empirical experiments on large corpora. Importantly, NLTK
includes a large set of corpora?this is one of the most useful and
game-changing contributions of the toolkit.
 It is principled: It exposes the theoretical underpinnings?both
computational and linguistic?of the algorithms and techniques that
are introduced.
 It attempts to strike a pragmatic balance between theory and applications.
The goal is to introduce ?just enough theory? to fit in a single semester
course for advanced undergraduates, while still leaving room for practical
programming and experimentation.
 It aims to make working with language pleasurable.
Computational Linguistics Volume 36, Number 4
The book is not a reference to computational linguistics and it does not provide a
comprehensive survey of the theory underlying computational linguistics. The niche
for such a comprehensive review textbook in the field remains filled by Jurasky and
Martin?s Speech and Language Processing (2008). What the book does achieve very well is
to bring the ?fun? in building software tools to perform practical tasks and in exploring
large textual corpora.
As a programming book describing practical state-of-the-art techniques, it belongs
to the glorious family of Charniak et al?s Artificial Intelligence Programming (1987),
Pereira and Shieber?s Prolog and Natural Language Analysis (1987), and Norvig?s mind-
expanding Paradigms of Artificial Programming (1992). It differs from these books in its
scope (CL vs. AI) and the programming language used (Python vs. Lisp or Prolog).
Another key difference is in its organization: Whereas the classical books have a strict
distinction between chapters covering programming techniques and chapters introduc-
ing core algorithms or linguistic concepts, the authors here attempt to systematically
blend, in each section, practical programming topics with linguistic and algorithmic
topics. This mixed approach works well for me.
As the dates of these older classics indicate (they were published 20 to 25 years
ago), this book is important in closing a gap. The transition of the field from a symbolic
approach to data-driven/statistical methods in the mid 1990s has transformed what
counts as basic education in computational linguistics. Correspondingly, textbooks ex-
panded and introduced new material on probability, information theory, and machine
learning. The trend started with Allen?s (1995) textbook, which introduced a single
chapter on statistical methods. Charniak (1993) and Manning and Schu?tze (1999) fo-
cused uniquely on statistical methods and provided thorough theoretical material?but
there was no corresponding focus on programming techniques. Another impediment to
teaching was the lack of easy access to large data sets (corpora and lexical resources).
This made teaching statistical methods with hands-on exercises challenging. Combining
statistical methods for low-level tasks with higher levels (semantic analysis, discourse
analysis, pragmatics) within a one-semester course became an acrobatic exercise.
Although deciding on the proper proportion among mathematical foundations,
linguistic concepts, low-level programming techniques, advanced algorithmic methods,
and methodological principles remains challenging, this book definitely makes the life
of computational linguistics students and teachers more comfortable. It is split into five
sections: Chapters 1 to 4 are a hand-holding introduction to the scope of ?language
technologies? and Python programming. Chapters 5 to 7 cover low-level tasks (tagging,
sequence labeling, information extraction) and introduce machine learning tools and
methods (supervised learning, classifiers, evaluation metrics, error analysis). Chapters
8 and 9 cover parsing. Chapter 10 introduces Montague-like semantic analysis. Chap-
ter 11 describes how to create and manage corpora?a nice addition that feels a bit out
of place in the structure of the book. Each chapter ends with a list of 20 to 50 exercises?
ranging from clarification questions to mini-programming projects.
The chapters all include a mix of code and concepts. Chapter 1 sets the tone. In
a few pages, the reader is led into an interactive session in Python, exploring textual
corpora, computing insightful statistics about various data sets, extracting collocations,
computing a bigram model, and using it to generate random text. The presentation is
fun, exciting, and immediately piques the interest of the reader.
Chapter 2 covers one of the most critical contributions of the book. It presents
commonly used corpora packaged together with NLTK and Python code to read them.
The corpora include the Gutenberg collection, the Brown corpus, a sample of the Penn
Treebank, CoNLL shared task collections, SemCor, and lexical resources (WordNet and
768
Book Review
Verbnet). The important factor is that these resources are made thoroughly accessi-
ble, easily downloaded, and easily queried and explored using an excellent Python
programming interface. The NLTK Corpus Reader architecture is a brilliant piece of
software that is well exploited in the rest of the book.
Chapter 3 introduces programming techniques to deal with text, Unicode, down-
loading documents from various sources (URLs, RSS feeds) and excellent practical cov-
erage of regular expressions. It is typical of the book?s approach that regular expressions
are taught by example and through useful applications, and not through an introduction
to automata theory. The chapter ends with an excellent introduction to more advanced
topics in sentence and word segmentation, with examples from Chinese. Overall, this
chapter is technical but extremely useful as a practical basis.
I find Chapter 4 problematic. It is a chapter fully focused on programming, which
introduces some key techniques in Python (generators, higher-order functions) together
with basic material (what a function is, parameter passing). In my experience teaching
humanities students, the material is not sufficient for non-programmers to become suf-
ficiently proficient and not focused enough to be useful for experienced programmers.
Chapters 5 to 7 introduce the data-driven methodology that has dominated the
field in the past 15 years. Chapter 5 covers the task of part-of-speech tagging. The
linguistic concepts are clearly explained, the importance of the annotation schema is
well illustrated through examples (using a simplified 15-tag tagset and a complex
one with 50 or more tags). The chapter incrementally introduces taggers using dic-
tionaries, morphological cues, and contextual information. Students quickly grasp the
data-driven methodology: training and testing data, baseline, backoff, cross-validation,
error analysis, confusion matrix, precision, recall, evaluation metrics, perplexity. The
concepts are introduced through concrete examples and help the student construct and
improve a practical tool. Chapter 6 goes deeper into machine learning, with supervised
classifiers. The Python code that accompanies this chapter (the classifier interface and
feature extractors) is wonderful. The chapter covers a wide range of tasks where the
classification method brings excellent results (it reviews POS tagging, document clas-
sification, sequence labeling using BIO-tags, and more). The theory behind classifiers
is introduced lightly. I was impressed by the clarity of the explanations of the first
mathematical concepts that appear in the book?the presentation of the concept of
entropy, naive Bayes, and maximum entropy classifiers builds strong intuition about
the methods. (Although the book does not cover them, NLTK includes excellent code
for working with support vector machines and hidden Markov models.) Chapter 7
builds on the tools of the previous two chapters and develops competent chunkers and
named-entity recognizers. For a graduate course, the theoretical foundations would be
too superficial?and one would want to complement these chapters with theoretical
foundations on information theory and statistics. (I find that a few chapters from All of
Statistics [Wasserman 2010] and from Probabilistic Graphical Models [Koller and Friedman
2009] together with Chapter 6 of Foundations of Statistical NLP [Manning and Schu?tze
1999] on estimation methods are useful at this stage to consolidate the mathematical un-
derstanding.) Readers come out of this part of the book with an operational understand-
ing of supervised statistical methods, and with a feeling of empowerment: They have
built robust software tools, run them on the same data sets big kids use, and measured
their accuracy.
The next two chapters (8 and 9) cover syntax and parsing. They start with CFGs and
simple parsing algorithms (recursive descent and shift-reduce). CKY-type algorithms
are also covered. A short section on dependency parsing appears (Section 8.5), but
I found it too short to be useful. A very brief section is devoted to weighted CFGs.
769
Computational Linguistics Volume 36, Number 4
Chapter 9 expands CFGs into feature structures and unification grammars. The au-
thors take this opportunity to tackle more advanced syntax: inversion, unbounded
dependency.
The material on parsing is good, but too short. In contrast to the section on tag-
ging and chunking, the book does not conclude with a robust working parser. On
the conceptual side, I would have liked to see a more in-depth chapter on syntax?a
chapter similar in depth to Chapter 21 of Paradigms of AI Programming (Norvig 1992)
or the legendary Appendix B of Language as a Cognitive Process (Winograd 1983). In my
experience, students benefit from a description of clausal arguments, relative clauses,
and complex nominal constructs before they can properly gauge the complexity of
syntax. On the algorithmic side, there is no coverage of probabilistic CFGs. The material
on PCFGs is mature enough, and there is even excellent code in NLTK to perform tree
binarization (Chomsky normal form) and node annotation, which makes it possible to
build a competent PCFG constituent-based parser. The connection between probabilistic
independence and context-freeness is a wonderful story that is missed in the book.
Finally, I believe more could have been done with dependency parsing: transition-
based parsing with perceptron learning a` la MaltParser (Nivre et al 2007) is also mature
enough to be taught and reconstructed in didactic code in an effective manner.
Chapter 10 is an introduction to computational semantics. It adopts the didactic
approach of Blackburn and Bos (2005) and covers first-order logic, lambda calculus,
Montague-like compositional analysis, and model-based inferencing. The chapter ex-
tends up to Discourse Representation Theory (DRT). As usual, the presentation is
backed up by impressively readable code and concrete examples. This is a very dense
chapter?with adequate theoretical material. It could have been connected to the ma-
terial on parsing, by combining a robust parser with the semantic analysis machinery.
This would have had the benefit of creating more cohesion and illustrating the benefits
of syntactic analysis for higher-level tasks.
Chapter 11 is an interesting addition on managing and constructing corpora. The
skills required for collecting and annotating textual material are complex, and the
chapter is a unique and welcome extension to the traditional scope of CL textbooks.
Overall this book is an excellent practical introduction to modern computational lin-
guistics. As a textbook for graduate courses, it should be complemented by theoretical
material from other sources, but the introduction the authors give is never too simplistic.
The authors provide remarkably clear explanations on complex topics, together with
concrete applications.
The book builds on high-quality code and makes significant corpora accessible.
Although I still use Lisp in class to present algorithms in the most concise manner,
I am happy to see how effective Python turns out to be as the main tool to convey
practical CL in an exciting, interactive, modern manner. Python is a good choice for this
book: It is easy to learn, open-source, portable across platforms, interactive (the authors
do a brilliant job of exploiting the exploratory style that only interpreters can provide
in interspersing the book with short code snippets to make complex topics alive),
and it supports Unicode, libraries for graph drawing and layout, and graphical user
interfaces. This allows the authors to develop interactive visualization tools that vividly
demonstrate the workings of complex algorithms. The authors exploit everything this
software development platform has to deliver in an extremely convincing manner.
The decision of which material to include in the book is in general well founded. The
authors manage to cover a range of issues from word segmentation, tagging, chunking,
parsing, to semantic analysis, and even briefly reach the world of discourse. I look
forward to an expanded edition of the book that would cover probabilistic parsing, text
770
Book Review
generation, summarization, and lexical semantics. I would also have liked to see some
coverage of unsupervised and semi-supervised learning methods.
For instructors, students, and researchers, this book, together with the excellent
NLTK library, is an important milestone. No one should learn computational linguistics
without it.
References
Allen, James, 1995. Natural Language
Understanding. Benjamin/Cummings,
Menlo Park, CA, 2nd edition edition.
Blackburn, Patrick, and Johan Bos. 2005.
Representation and Inference for Natural
Language: A First Course in Computational
Semantics. CSLI Publications, Stanford, CA.
Charniak, Eugene. 1993. Statistical Language
Learning. The MIT Press, Cambridge, MA.
Charniak, Eugene, Christopher K. Riesbeck,
Drew V. McDermott, and James R. Meehan.
1987. Artificial Intelligence Programming.
Lawrence Erlbaum Associates, Hillsdale,
NJ, 2nd edition edition.
Jurafsky, Daniel and James H. Martin.
2008. Speech and Language Processing: An
Introduction to Natural Language Processing,
Computational Linguistics, and Speech
Recognition. Prentice Hall, Upper Saddle
River, NJ, 2nd edition edition.
Koller, Daphne and Nir Friedman. 2009.
Probabilistic Graphical Models: Principles and
Techniques. The MIT Press, Cambridge, MA.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it, Sandra
Ku?bler, Svetoslav Marinov, and Erwin
Marsi. 2007. MaltParser: A language-
independent system for data-driven
dependency parsing. Natural Language
Engineering, 13(2):95?135.
Norvig, Peter. 1992. Paradigms of Artificial
Intelligence Programming: Case Studies
in Common Lisp. Morgan Kaufmann,
San Francisco, CA.
Pereira, Fernando C. and Stuart M. Shieber.
1987. Prolog and Natural-Language Analysis.
CSLI Publications, Stanford, CA.
Wasserman, Larry. 2010. All of Statistics:
A Concise Course in Statistical Inference.
Springer, New York, NY.
Winograd, Terry. 1983. Language as a
Cognitive Process. Addison-Wesley,
Reading, MA.
Michael Elhadad is an associate professor at Ben-Gurion University, Israel. He has been teach-
ing computational linguistics for 15 years. His research focuses on computational models of
Modern Hebrew, text summarization, and text generation. His address is Department of Com-
puter Science, Ben-Gurion University, Beer Sheva, 84105, Israel; e-mail: elhadad@cs.bgu.ac.il.
771

Word Segmentation, Unknown-word
Resolution, and Morphological Agreement
in a Hebrew Parsing System
Yoav Goldberg?
Ben Gurion University of the Negev
Michael Elhadad??
Ben Gurion University of the Negev
We present a constituency parsing system for Modern Hebrew. The system is based on the
PCFG-LA parsing method of Petrov et al (2006), which is extended in various ways in order
to accommodate the specificities of Hebrew as a morphologically rich language with a small
treebank. We show that parsing performance can be enhanced by utilizing a language resource
external to the treebank, specifically, a lexicon-based morphological analyzer. We present a
computational model of interfacing the external lexicon and a treebank-based parser, also in the
common case where the lexicon and the treebank follow different annotation schemes. We show
that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY
lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipeline-
based model. We suggest modeling grammatical agreement in a constituency-based parser as a
filter mechanism that is orthogonal to the grammar, and present a concrete implementation of
the method. Although the constituency parser does not make many agreement mistakes to begin
with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.
These contributions extend outside of the scope of Hebrew processing, and are of general
applicability to the NLP community. Hebrew is a specific case of a morphologically rich language,
and ideas presented in this work are useful also for processing other languages, including
English. The lattice-based parsing methodology is useful in any case where the input is uncertain.
Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant
for any language with a small treebank.
1. Introduction
Different languages have different syntactic properties. In English, word order is rela-
tively fixed, whereas in other languages word order is much more flexible (in Hebrew,
the subject may appear either before or after a verb). In languages with a flexible word
order, the meaning of the sentence is realized using other structural elements, like word
? Computer Science Department, Ben Gurion University of the Negev, Israel.
E-mail: yoav.goldberg@gmail.com.
?? Computer Science Department, Ben Gurion University of the Negev, Israel.
E-mail: elhadad@cs.bgu.ac.il.
Submission received: 30 September 2011; revised submission received: 19 May 2012; accepted for publication:
3 August 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
inflections or markers, which are referred to as morphology (in Hebrew, the marker !??
is used to mark definite objects, distinguishing them from subjects in the same position.
In addition, verbs and nouns are marked for gender and number, and subject and verb
must share the same gender and number). A limited form of morphology also exists in
English: the -s and -ed suffixes are examples of Englishmorphological markings. In other
languages, morphological processes may be much more involved. The lexical units
(words) in English are always separated by white space. In Chinese, such separation
is not available. In Hebrew (and Arabic), most words are separated by white space,
but many of the function words (determiners like the, conjunctions such as and, and
prepositions like in or of ) do not stand on their own but are instead attached to the
following words.
A large part of the parsing literature is devoted to automatic parsing of English, a
language with a relatively simple morphology, relatively fixed word order, and a large
treebank. Data-driven English parsing is now at the state where naturally occurring text
in the news domain can be automatically parsed with accuracies of around 90% (accord-
ing to standard parsing evaluation measures). When moving from English to languages
with richer morphologies and less-rigid word orders, however, the parsing algorithms
developed for English exhibit a large drop in accuracy. In addition, whereas English has
a large treebank, containing over one million annotated words, many other languages
have much smaller treebanks, which also contribute to the drop in the accuracies of
the data-driven parsers. A similar drop in parsing accuracy is also exhibited in English
when moving from the news domain, on which parsers have traditionally been trained,
to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, which
use different vocabularies and, to some extent, different syntactic rules.
This work focuses on constituency parsing of Modern Hebrew, a Semitic language
with a rich and productive morphology, relatively free word order,1 and a small tree-
bank. Several natural questions arise: Can the small size of the treebank be compensated
for using other available resources or sources of information? How should the word
segmentation issue (that function words do not appear in isolation but attach to the next
word, forming ambiguous letter patterns) be handled? Can morphological information
be used effectively in order to improve parsing accuracy?
We present a system which is based on a state-of-the-art model for constituency
parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations
(PCFG-LA) model of Petrov et al (2006), as implemented in the BerkeleyParser. After
evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew tree-
bank, we discuss some of its limitations and then go on to extend the PCFG-LA parsing
model in several directions, making it more suitable for parsing Hebrew and related
languages. Our extensions are based on the following themes.
Separation of lexical and syntactic knowledge. There are two kinds of knowledge inherent
in a parsing system. One of them is syntactic knowledge governing the way in which
words can be combined to form structures, which, in turn, can be combined to form
ever larger structures. The other is lexical knowledge about the identities of individual
words, the word classes they belong to, and the kinds of syntactic structures they can
participate in. We argue that the amount of syntactic knowledge needed for a parsing
system is relatively limited, and that sufficiently large parts of it can be captured also
1 To be more precise, in Hebrew the order of constituents is relatively free, whereas the order of the words
within certain constituents is relatively fixed.
122
Goldberg and Elhadad Parsing System for Hebrew
based on a relatively small treebank. Lexical knowledge, on the other hand, is much
more vast, and we should not rely on a treebank (small or large) to provide adequate
lexical coverage. Instead, we should aim to find ways of integrating lexical knowledge,
which is external to the treebank, into the parsing process.
We extend the lexical coverage of a treebank-based parser using a dictionary-based
morphological analyzer. We present a way of integrating the two resources also for the
common case where their annotations schemes diverge. This method is very effective in
improving parsing accuracy.
Encoding input uncertainty using a lattice-based representation. Sometimes, the language
signal (the input to the parser) may be uncertain. This happens in Hebrew when a
space-delimited token such as !???? can represent either a single word (?[an] onion?) or a
sequence of two words or three words (?in shadow? and ?in the shadow,? respectively).
When computationally feasible, it is best to let the uncertainty be resolved by the parser
rather than in a separate preprocessing step.
We propose encoding the input-uncertainty in a word lattice, and use lattice parsing
(Chappelier et al 1999; Hall 2005) to perform joint word segmentation and syntactic
disambiguation (Cohen and Smith 2007; Goldberg and Tsarfaty 2008). Performing the
tasks jointly is effective, and substantially outperforms a pipeline-based model.
Using morphological information to improve parsing accuracy. Morphology provides useful
hints for resolving syntactic ambiguity, and the parsing model should have a way of
utilizing these hints. There is a range of morphological hints than can be utilized: from
functional marking elements (such as the !?? marker indicating a definite direct object);
to elements marking syntactic properties such as definiteness (such as the Hebrew !?
marker); to agreement patterns requiring a compatibility in properties such as gender,
number, and person between syntactic constituents (such as a verb and its subject or
an adjective and the noun it modifies).
We suggest modeling agreement as a filtering process that is orthogonal to the
grammar. Although the constituency parser does not make many agreement mistakes
to begin with, the filter mechanism is effective in fixing the agreement mistakes that the
parser does make, without introducing new mistakes.
Aspects of the work presented in this article are discussed in earlier publica-
tions. Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg
et al (2009) discuss ways of interfacing a treebank-derived PCFG-parser with an exter-
nal lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA
BerkeleyParser. Here we provide a cohesive presentation of the entire system, as well as
a more detailed description and an expanded evaluation. We also extend the previous
work in several dimensions: We introduce a new method of interfacing the parser and
the external lexicon, which contributes to an improved parsing accuracy, and suggest
incorporating agreement information as a filter.
The methodologies we suggest extend outside the scope of Hebrew processing,
and are of general applicability to the NLP community. Hebrew is a specific case of
a morphologically rich language, and ideas presented in this work are useful also for
processing other languages, including English. The lattice-based parsing methodology
is useful in any case where the input is uncertain. Indeed, we have used it to solve
the problem of parsing while recovering null elements in both English and Chinese
(Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation
and parsing of Arabic (Green and Manning 2010). Extending the lexical coverage of
a treebank-derived parser using an external lexicon is relevant for any language with
123
Computational Linguistics Volume 39, Number 1
a small treebank, and also for domain adaptation scenarios for English. Finally, the
agreement-as-filter methodology is applicable to any morphologically rich language,
and although its contribution to the parsing task may be limited, it is of wide applica-
bility to syntactic generation tasks, such as target-side-syntax machine translation in a
morphologically rich language.
2. Modern Hebrew
2.1 Lexical and Syntactic Properties
Some relevant lexical and syntactic properties of Modern Hebrew are highlighted in this
section.
2.1.1 Unvocalized Orthography. Most vowels are not marked in everyday Hebrew
text, which results in a very high level of lexical and morphological ambiguity. Some
tokens can admit as many as 15 distinct readings, and the average number of pos-
sible morphological analyses per token in Hebrew text is 2.7, compared with 1.4 in
English (Adler 2007). The word !??????? can be read in at least eight different ways
(?spoons,? ?square cotton headkerchiefs,? ?coercions,? ?as mouths,? ?as spouts,? ?as fairies,?
?ungratefulness,? ?fun/adjectivefeminine,plural?), the word !???? in at least six ways (?a
journalist,? ?writing,? ?script,? ?wrote,? ?added someone as a recipient,? ?was added as
a recipient?) and the word !?? can be read as a very common case-marker (appearing
before definite direct objects), a very common pronoun (?you/feminine?), and a noun
(?shovel?).
2.1.2 Affixation. Eight common prepositions, conjunctions, and articles may never
appear in isolation and must always be attached as prefixes to the following word.2
These include the function words !?? (?from?), !? (?which?/?who?/?that?), !??? (?when?),
!? (?the?), !? (?and?), !?? (?like?), !? (?to?), and !? (?in?),. Several such elements may attach
together, producing forms such as !????????? ( !?-?-??-?-???? ?and-that-from-the-sun?). Notice
that when it appears by itself, the last part of the token, the noun !???? (?sun?), can also
be interpreted as the sequence !?-??? (?who moved?). The linear order of such elements
within a token is fixed (disallowing the reading !?-?-??-?-?-??? in the previous example).
The syntactic relations of these elements with respect to the rest of the sentence
are rather free, however. The relativizer !? (?that?), for example, may attach to an
arbitrarily long relative clause that goes beyond token boundaries. The attachment in
such cases encompasses a long-distance dependency that cannot be captured by local-
context (or Markovian) sequential processes that are typically used for morphological
disambiguation. The same argument holds for resolving PP attachment of a prefixed
preposition or marking conjunction of elements of any kind.
To further complicate matters, the definite article !? (?the?) is not realized in writing
when following the particles !? (?in?), !?? (?like?), and !? (?to?). Thus, the form !???? can be
interpreted as either !?-??? (?in house?) or !?-?-??? (?in the house?).3
2 In what follows, we indicate the correct segmentations of the different forms. Naturally occurring
Hebrew text does not have such indications.
3 This overt element is in fact indicated by vocalization, but is not realized in standard written text.
124
Goldberg and Elhadad Parsing System for Hebrew
In addition, pronominal elements (clitics) may attach to nouns, verbs, adverbs,
prepositions, and others as suffixes (e.g., !????? [ !????-??, ?brought-them?], !????? [ !-?????,?on
them?]).
These affixations result in highly ambiguous token segmentations: !??????? (?[they]
assigned numbers?) vs. !??????-? (?his number? or ?the one who cuts his hair?) vs. !-???-????
(?from his book? or ?from his barber?), !?????? (?putting together?) vs. !?-????? (?the train?),
and !???? (?an onion?) vs. !?-??? (?in the shadow?) are only a few examples of ambiguities
that may arise. Quantitatively, 99,896 out of 567,483 forms (17%) in a wide-coverage
lexicon of Hebrew can admit both segmented and unsegmented analyses.
In many cases the correct segmentation cannot be determined from local context
alone, but can be disambiguated by more global syntactic constraints (in ????? ?????
!???????, the middle token is ambiguous between !????? [?sky?] and !?-???? [?that/rel water?],
and the sequence can be interpreted as either ?I saw blue skies? or ?I saw that blue water.?
On the other hand, !???? ??? ?????? ??????? ????? ????? is unambiguous because the past
verb !?????? requires the relativizer !?, allowing only the segmented !?-???? reading ?I saw
that blue water broke from the well?. In the other direction, ??????? ??????? ????? ?????
!????? is also unambiguous, allowing only the unsegmented reading ?I saw blue skies
and went to sleep?.)
2.1.3 Rich Templatic Morphology. Hebrew words follow a complex morphological struc-
ture, which is based on a root + template system, with both derivational and inflectional
elements. Word forms can encode gender, number, person, and tense, and in addition
noun-compounding is also morphologically marked (see Section 2.1.7). Although the
exact details of the system are irrelevant (but see Adler [2007] and Glinert [1989] for a
good overview), we note that this word formation mechanism results in a very high
number of possible word forms, and that it is hard to guess the part-of-speech of words
based on prefixes and suffixes alone, a method frequently used in other languages.
2.1.4 The Participle Form. The Hebrew participle form ( !????????, literally the ?middle form?
of verbs) is a form that shares morphological and syntactic properties of nouns, verbs,
and adjectives. This form causes many disagreements between human annotators, and
large disagreement is found also between major Hebrew dictionaries regarding many
word forms (see Adler et al [2008b] for a discussion from tag set design and annotation
guidelines, including many syntactic, semantic, and lexical considerations). For the
purpose of this work, this form is of interest as it highlights the inherent ambiguity
between adjectival, nominal, and verbal readings of many words, which are hard to
disambiguate even in context.
2.1.5 Relatively Free Constituent Order. The ordering of constituents inside a phrase is
relatively free. This is most notably apparent in verbal phrases and sentential levels. In
particular, whereas most sentences follow a subject-verb-object order (SVO), OVS and
VSO configurations are also possible (counting in the Hebrew Treebank reveals 5,720
SV cases and 2,613 VS cases, compared with 81,135 SV and 3,018 VS constructions in
the English WSJ Treebank). In addition, verbal arguments can appear before or after
the verb, and in many orders. Such variations in constituent order are easy to capture
using ?flat? S structures putting the verbs and all of its arguments on the same clausal
level, and this is the annotation approach adopted by the Hebrew Treebank (as well
as by treebanks of other languages, such as French [Abeille?, Cle?ment, and Toussenel
2003]). These flat structures result in the grammar having more and longer rules and the
treebank having fewer instances of each rule type, however, causing a data sparseness
125
Computational Linguistics Volume 39, Number 1
problem for statistical estimation methods based on treebank counts, and making it
more difficult to reliably estimate the grammar parameters.
2.1.6 Verbless Constructions. Several constructions in which the verb is not realized are
common in Hebrew. These include the possessive constructions such as ????????? ?????
!???? (?to-Ido toys many? meaning ?ido has many toys?), which also feature a flexible
constituent order !????? ???? ????????? (?toys many to-Ido?, ?ido has many toys?), and
copular constructions such as !????? ???? (?the-boy cute? ?the boy is cute?) and !?????? ????
(?the-boy crazy? ?the boy is crazy?).
2.1.7 NP Structure and Construct-State. Although constituent order may vary, NP
internal structure is rigid. A special morphological marker (construct state, or !????????)
is used to mark noun-compounds as well as similar phenomena (this is similar to the
idafa construction in Arabic).4 Noun compounding in Modern Hebrew is productive
and very frequent?about a quarter of the noun tokens in the Hebrew Treebank are in
the construct state. Construct-state nouns can be highly ambiguous with non-construct-
state nouns. Some forms are morphologically marked but the marking is not present in
unvocalized text ( !?????/banot vs. !?????/bnot), and some forms are not marked at all ( !????).
The construct-state marker, although ambiguous, is essential for analyzing NP internal
structure. Where regular nouns are marked for definiteness using the definite marker
!?, construct-nouns acquire the definite status of the noun-phrase they compound to.
Construct constructions may be nested, as in !???????? ?????? ?????? ???? ???? (?shadeconst
colorconst lidconst boxconst the apples,? meaning ?the shade of the color of the lid of the box
of the apples?).
2.1.8 Definiteness. Definiteness is spread across many elements in the NP. All elements
in a definite NP, except for construct-nouns and proper-names, are explicitly marked
using the functional element !? that is prefixed to the token. Proper-names are inher-
ently definite and cannot take the definite marker, and construct-nouns acquire their
definiteness status from the NP they dominate (definiteness is not explicitly marked on
construct-nouns).
2.1.9 Case Marking. Definite direct objects are marked. The case marker in this case is
the function word !?? appearing before the direct object. Subjects, indirect objects, and
non-definite direct objects are not marked.
2.1.10 Agreement. Hebrew grammar forces morphological agreement between adjec-
tives and nominals (adjectives appear after the noun, and agree in gender, number, and
definiteness), and between subjects and verbs (including the verbless copular construc-
tions), which agree in gender, number, and person. Agreement in the predicative case
is a bit complex: When the verb is overt and the predicative-complement is a noun,
as in !????? ??? ??????? (?the-tripfem isfem an-excusemasc?), gender and number agreement
are required between the subject and the verb (but not the predicative-complement),
but in the verbless case, the subject and the predicate-complement noun must agree
( !????? ???????* ?the-tripfem an-excusemasc?). When the predicate-complement is an adjec-
tive, gender and number agreement between the subject and the predicate-complement
4 The construct state is not restricted to nouns, and can also appear on numbers (e.g., !????? !?????/?tens-of
kids?) and adjectives ( !???????? !????/?biggest-of authors?).
126
Goldberg and Elhadad Parsing System for Hebrew
is required regardless of the realization of the verb/copular element: !???? ????, ?????
!????*, !???? ??? ????, !???? ??? ?????* (?the-boy tallmasc?, ?*the-boy tallfem?, ?the-boy ismasc
tallmasc?, ?*the-girl isfem tallmasc?).
2.2 Implications for Parsing
After surveying some lexical and syntactic properties of Modern Hebrew, we turn
to highlight some aspects in which Modern Hebrew differs from English from the
perspective of parsing system design.
2.2.1 Small Amount of Annotated Data. Whereas the English Treebank is relatively large
(49,208 sentences, or 1,173,766 words), the Hebrew Treebank (Guthmann et al 2009) is
much smaller, containing only 6,220 sentences, or 115,661 tokens (156,316 words5).
The small size of the Hebrew Treebank implies a smaller training set for learning-
algorithms used to construct the parser.
2.2.2 Ambiguous Word Segmentation. Syntactic parsing systems treat the input sentence
as observed data?the leaves (in constituency parsing) of the tree are known in advance,
and the parser is expected to build a parse tree around them. This is not the case in
Hebrew, where many function words are not separated by white space but instead are
prefixed to the next word and appear within the same token. This makes the word
sequence unobserved to the parser, which has to infer both the syntactic-structure and
the token segmentation.6
One possible solution to the unobserved word-sequence problem is a pipeline
system in which an initial model is in charge of token-segmentation, and the output of
the initial model is fed as the input to a second stage parser. This is a popular approach
in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and
Manning 2010). As discussed in Section 2.1.2 (as well as in Tsarfaty [2006a], Goldberg
and Tsarfaty [2008], and Cohen and Smith [2007]), however, the token-segmentation and
syntactic-parsing tasks are closely intertwined and are better performed jointly instead
of in a pipeline fashion, which is the approach we explore in this work.
2.2.3 Morphological Variation and High Out-of-Vocabulary Rate. The intrinsic deficiency
caused by the small amount of training data is made even more severe due to Hebrew?s
rich morphological inflection patterns. The high amount of morphological variation
means that many word forms will not be observed in the training data, making it harder
to reliably estimate lexical probabilities based on the annotated resources alone.
Unlike English, where parts-of-speech for words are relatively easy to guess based
on simple orthographic features (words starting with capital letters are proper nouns,
words ending in -ed are usually verbs, etc.), this is not the case for Hebrew. Among
the 773 words appearing in English test data but not in the training data, 269 start
with a capital letter, 58 end with -ed, and 49 end with -ing. Together, these three simple
heuristics cover almost half of the unobserved tokens. Such heuristics are not available
for Hebrew in the common case of unvocalized text: Proper names are not marked
5 Because of agglutination, a Hebrew token may consist of several words, for example the token !????
comprises the two words !?(?in?) and !???(?house?).
6 Token segmentation is sometimes (erroneously) referred to as morphological segmentation.
127
Computational Linguistics Volume 39, Number 1
in writing, and word prefixes and suffixes are not indicative of the part-of-speech
tags.7 Thus, the out-of-vocabulary (OOV) problem is much harder in Hebrew than in
English and other European languages: On the one hand many words are unobserved
in training, and on the other, it is more difficult to guess the analysis of such unknown
words.
A system for handling automatic processing of Hebrew text cannot rely solely on
manually annotated corpora, as such corpora cannot provide adequate lexical coverage.
Systems that attempt to perform disambiguation on the lexical level (such as sequence-
based morphological disambiguators, or syntactic parsers that perform morphological
disambiguation as part of the parsing process) should be designed to incorporate lexical
knowledge from sources external to the annotated corpora. We discuss methods of en-
hancing the system?s performance based on a resource that is external to the treebank: A
lexicon-based broad-coverage morphological analyzer enhanced with semi-supervised
probability estimates based on expectation maximization (EM) training of a hidden
Markov model (HMM) tagger on a large amount of unannotated text.
2.2.4 Morphological Agreement. The rich morphological system also means that words
carry large amounts of extra information: definiteness, gender, number, tense, and
person. Some of this information interacts with syntax through agreement constraints.
Specifically, nouns and adjectives should agree in gender and number, and subjects
and verbs should agree in gender, number, and person. Agreement constraints can
provide useful hints for disambiguating the syntactic structure. Consider for example
the sentence !?????? ?? ?????? ???? ??? (?wife of the man who ate the apple?). The
English sentence is ambiguous with respect to the entity who ate the apple, but the
Hebrew version is not?the verb !????? (?ate?) is in feminine form, indicating that it was
the wife who did the eating. Can a parsing system make use of such information? This
issue is investigated further in Section 8.2.
2.3 Existing Resources for Hebrew Text Processing
Several linguistic resources are available for Hebrew, and are used as building blocks
for the parsing systems described in this work.
2.3.1 The Hebrew Constituency-Treebank. A constituency treebank of Modern Hebrew,
incrementally developed at the Technion over the course of more than eight years
(Sima?an et al 2001; Guthmann et al 2009), is maintained by MILA, the knowledge
center for processing Hebrew.8 The current version of the treebank (Version 2) contains
6,220 sentences taken from the Israeli daily newspaper !???? (Ha?aretz). The sentences
are manually annotated on both the lexical and the syntactic levels. Each token9 is
segmented into words, and eachword is assigned a part of speech tag that also captures,
7 Although the suffixes are good indicators of gender and number ( !?? is usually plural masculine, !? is
usually singular feminine), they are not good at indicating the core part-of-speech ( !? is a suffix can
appear in adjectives !????, verbs !?????, nouns !?????, and similarly for !?? ( !?????, !?????????, !?????????). Furthermore,
due to the root+template system, in most cases the first and last letters of the word are part of the root
and not of the pattern !??????!,?????, making the suffixes even less indicative.
8 http://www.mila.cs.technion.ac.il/mila/eng/index.html.
9 As discussed in Section 2.1.2, Hebrew tokens (entities separated by white space and punctuation symbols)
do not necessarily correspond to Hebrew words. A single token may contain several words.
128
Goldberg and Elhadad Parsing System for Hebrew
where applicable, the morphological properties of the word such as number, gender,
and person. Then a constituency tree is built on top of the segmented words. The
annotation of NPs is relatively nested, and the sentence level structures are relatively
flat (the verb and all of its arguments reside on one level under S). The treebank has
115,661 tokens and 156,764 words.
The POS tagging scheme in the treebank is highly syntactic in nature: A part-of-
speech is chosen to reflect the syntactic function of the given word in context. For exam-
ple, demonstrative pronouns are tagged in the treebank as adjectives when appearing
in an adjectival position ( !?? !???, ?this/JJ child/NN?), and a special MOD tag is used to
mark non-adverbial clausal level modification (that is, modifications that can be treated
as adverbial, but that are used to modify something other than a verb). For a more
detailed description of the Constituency Treebank see Sima?an et al (2001), Guthmann
et al (2009), and Tsarfaty (2010, pages 199?216), as well as the annotation guidelines.10
2.3.2 Train/dev/test Splits. Throughout the article, we follow the established train/
dev/test split for the treebank, namely, sentences 1?483 are used for development,
sentences 484?5,740 are used for training the parser, and sentences 5,741 to 6,220 are
used as the final test set.
2.3.3 The MILA Broad-Coverage Lexicon. Aside from the Constituency Treebank, Hebrew
has a wide-coverage, lexicon-based morphological analyzer which can assign morpho-
logical analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew
tokens. The lexicon (henceforth the KC Analyzer) is developed and maintained by
the Knowledge Center for Processing Hebrew (Itai and Wintner 2008). It is based on a
lexicon of roughly 25,000word lemmas and their inflection patterns. From these, 562,439
unique word forms are derived. These are then prefixed (subject to constraints) by 73
prepositional prefixes. Even with this seemingly large vocabulary, the KC Analyzer?s
coverage is not perfect. In Adler et al (2008a), we present a machine-learning method
that is trained on the basis of the analyzer and that can guess possible analyses for
words unknown to the analyzer with reasonable accuracies. Using this extension, the
analyzer has perfect coverage (even though the quality is obviously better for words
that are present in the analyzer?s database).
The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed
in depth in BGU Computational Linguistics Group (2008).
Creating a resource such as the morphological analyzer for a morphologically rich
language is a worthwhile and cost-effective effort: After establishing the tag set, it is
relatively straightforward to add lemmas to the lexicon, and the automatic inflection
process guarantees good coverage of all the possible inflections. This is much more
efficient than annotating enough text to obtain a similar coverage.
2.3.4 Hebrew Morphological Disambiguator. The morphological analyzer provides the
possible set of analyses for each token, but does not disambiguate the correct analy-
sis in context. A morphological disambiguator (henceforth ?the Hebrew tagger? or
?tagger?) was developed by Meni Adler at Ben-Gurion University of the Negev
(Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and Elhadad 2008). After the
(extended) morphological analyzer assigns the possible analyses for each token in an
10 http://www.mila.cs.technion.ac.il/mila/files/treebank/Decisions-Corpus1-5001.v1.pdf.
129
Computational Linguistics Volume 39, Number 1
input sentence, the tagger takes the output of the analyzer as input and chooses the sin-
gle best analysis for the entire sentence (performing both token segmentation of words
and part-of-speech assignment for each word). The tagger is an HMM-based sequential
model that is trained in a semi-supervised fashion using EM based on the output of the
morphological analyzer on a large amount (about 70M words) of unannotated Hebrew
text. The tagger is described in Adler and Elhadad (2006) and Adler (2007).
The tagger is relatively accurate: It achieves 93% accuracy in predicting segmen-
tation and tagging when measured on the POS accuracy, and 90% accuracy when
measured on the complete tag set, which includes the complete set of morphological
features. Because the tagger is not trained on a particular annotated training set but
instead on a very large corpus of text spanning multiple genres, its performance is
robust across domains.
The tagger?s success is due in part to a smart initialization procedure to the EM
training process. This initialization procedure takes the output of the analyzer and
assigns a conditional probability distribution P(tag|word) for each word. In other words,
it assigns an a priori, context-free likelihood for each analysis of a word (although the
word broke can be either a verb in the past tense or an adjective, it is more likely to be the
former; such preferences can be modeled as probability distributions, and the initializa-
tion procedure attempts to learn the values of these distributions automatically from
raw data). This initialization procedure is described in Goldberg, Adler, and Elhadad
(2008).
A side effect of the EM?HMM training of the tagger is pseudo-counts for ?word, tag?
events, which are based on patterns observed in the unannotated training data. We use
these counts in order to improve the lexical-disambiguation capacity of the parser.
2.3.5 A Resource Incompatibility Issue. Unfortunately, the KC Analyzer adopted a dif-
ferent tag set than the one used in the treebank, and analyses produced by the KC
Analyzer (and hence by the morphological disambiguator) are incompatible with the
Hebrew Treebank. These are not mere technical differences, but derive from different
perspectives on the data. The Hebrew Treebank (TB) tag set is syntactic in nature (?if
the word in this particular position functions as an adverb, tag it as an adverb, even
though it is listed in the dictionary only as a noun?), whereas the KC tag set (Adler
2007; Netzer et al 2007; Adler et al 2008b) takes a lexical approach to POS tagging
(?a word can assume only POS tags that would be assigned to it in a dictionary?). The
lexical approach does not accommodate generic modification POS tags such as MOD,
nor does it allow listing of demonstrative pronouns as adjectives.
These divergent perspectives are reflected in different guidelines to human taggers,
different principles underlying tag definitions, and different verification procedures.
This difference in perspective yields different performances for parsers induced from
tagged data, and a simple mapping between the two schemes is impossible to define.
Some Hebrew forms, particularly the present participle and modal forms, are in-
herently hard to define, and the wide disagreement about their status is reflected in
practically all Hebrew dictionaries. This kind of disagreement naturally appears also
between the KC and TB. See Adler et al (2008b) and Netzer et al (2007) for further
discussion on these two interesting cases.
Bridging the discrepancy between the two resources is an important aspect in the
creation of a successful parsing system. On the one hand the syntactic annotations in the
treebank are needed in order to train the parser, and on the other hand the information
provided by the morphological analyzer is needed in order to provide a good lexical
coverage. We discuss an approach to bridging this discrepancy in Section 6.
130
Goldberg and Elhadad Parsing System for Hebrew
2.4 Section Summary
To summarize, the Hebrew language and its analysis poses several challenges to parser
design: The amount of annotated material is relatively small, precluding the possibility
of learning robust lexical parameters from the annotated corpora. The productive
nature of the morphology results in many word forms, adding another obstacle to
estimating lexical parameters from annotated data. The nature of the word-formation
mechanism in Hebrew makes it hard to guess the morphological analysis of a word
based on its prefix and suffix alone as is done in other languages, requiring the use of a
more complex system for handling unknown words. Many function words in Hebrew
are not separated by white space but are instead attached to the next token, making
the observed word sequence ambiguous. Word segmentation needs to be performed
in addition to syntactic disambiguation. Successful word segmentation may rely on
syntactic disambiguation, suggesting that it is better to perform the segmentation
and syntactic-disambiguation tasks jointly. Finally, Hebrew grammar requires various
forms of morphological agreement, a fact which hopefully can help disambiguate
otherwise ambiguous syntactic structures. The syntactic parser should be able to make
use of agreement information.
In terms of existing resources, Hebrew has a small treebank annotated with con-
stituency structure and a broad-coverage, manually constructed, lexicon-based mor-
phological analyzer. The morphological analyzer is capable of providing the possible
morphological analyses for many lexical forms, and it is extended using a machine-
learning technique to also provide possible analyses for word-forms not covered by
the lexicon. The extended lexicon provides a good lexical coverage of Hebrew. Also
available is a morphological disambiguator that is capable of associating probabilities to
the possible analyses of the lexical forms in the lexicon, and disambiguating the analyses
of a sequence of lexical items in context based on a sequential model. The constituency
treebank can be used to learn the parameters of a syntactic-model of Hebrew, and
the morphological analyzer can be used to provide broad-coverage lexical knowledge.
Unfortunately, the treebank and the lexicon/disambiguator follow different annotation
schemes, and are therefore incompatible with each other. The annotation gap between
the two resources must be bridged before they can be used together.
We now turn to survey the components of our Hebrew parsing system.
3. Latent-Annotation State-Split Grammars (PCFG-LA)
Klein and Manning (2003) demonstrated that linguistically informed splitting of non-
terminal symbols in treebank-derived grammars can result in accurate grammars. Their
work triggered investigations in automatic grammar refinement and state-splitting
(Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which was then perfected in work
by Petrov and colleagues (Petrov et al 2006; Petrov and Klein 2007; Petrov 2009).
State-split models assume that each non-terminal label has a latent annotation that
should be recovered. Instead of a single NP symbol, these models hypothesize that there
are many different NP symbols, NP1, . . . ,NPk, and each is used in a different context.
The labels are hidden, however, and we can only observe the core category label (NP).
The job of the training process is to come up with the hidden set of label assignments
to non-terminals, such that the resulting grammar assigns a high probability to the
observed treebank data. Such models are called PCFG with latent annotations (PCFG-
LA) and are shown empirically to produce very accurate parsing results.
131
Computational Linguistics Volume 39, Number 1
The model of Petrov et al (2006) and its publicly available implementation, the
BerkeleyParser,11 learns the latent annotations by starting with a bare-bones treebank-
derived grammar and automatically refining it in split-merge-smooth cycles, setting the
parameters using EM. We provide a brief description of the model and learning process
(refer to Petrov et al 2006; Petrov and Klein 2007; Petrov 2009 for the full details).
The learning works by following an iterative split-merge-smooth cycle, in which
the following steps are performed repetitively:
Splitting each non-terminal category in two All of the grammar symbols are split. In
the first round, NP is split into NP1 and NP2. In the second round these are
split into NP11, NP12, NP21, NP22, and so forth. Each splitting round results in
new grammar in which a rule of the form A ? BC is replaced by eight rules, the
result of splitting each A, B, and C in two. An EM procedure is then used to set
the probabilities of each of the split rules. The EM training is constrained by the
grammar on the one hand and by the annotated tree structures on the other.
Merging back non-effective splits Not all of the splits are useful. For example, the
punctuation POS tag will always result in punctuation, and there is no reason
to split it into two punctuation POS tags. Having a grammar with too many states
is difficult to manage in terms of memory, storage, and parsing time, and is also
prone to overfitting the data. Thus, the model aims to undo splits if they are not
useful. The splits are evaluated based on an information gain criteria, and splits
that are not useful are merged back into their parent symbol, resulting in a smaller
grammar (if the symbols B1 and B2 are merged back into B, the rules A ? B1 C
and A ? B2 C are merged into A ? B C). The merging step is also followed by an
EM procedure for setting the rule probabilities for the resulting grammar.
Smoothing the split non-terminals toward their shared ancestor Finally, split sym-
bols may still share some information (although an NP in subject position and
an NP in object position behave differently, they also retain some common prop-
erties). The smoothing procedure joggles the probability mass of the grammar
and moves some probability from the split symbol to its parent. This step is also
followed by parameter re-estimation using EM.
Performing five or six such split-merge-smooth cycles results in accurate grammars,
with annotations that capture many latent syntactic interactions. Six cycles mean that
symbols can have as many as 64 different substates.
At inference time, the latent annotations are (approximately) marginalized out,
resulting in the (approximate) most probable unannotated tree according to the refined
grammar (the score of the unsplit rule A ? B C is taken to be
?
x
?
y
?
zAx ? By Cz).
The grammar learning process is applied to binarized parse trees, with first-order
vertical and zeroth-order horizontal markovization (Klein and Manning 2003). This
means that in the initial grammar, each of the non-terminal symbols is effectively
conditioned on its parent alone, and is independent of its sisters. For example, the rule
S ? NP VP NP PP is binarized as:
S ? NP @S
@S ? VP @S
@S ? NP @S
@S ? PP
11 http://code.google.com/p/berkeleyparser/.
132
Goldberg and Elhadad Parsing System for Hebrew
indicating that S rules start with an NP, can be followed by a sequence of zero or
more NPs and VPs, and end with a PP. Such an extreme markovization suggests a
very strong independence assumption, and is too permissive on its own. It allows the
resulting refined grammar to encode its own set of dependencies between a node and
its sisters, however, as well as ordering preferences in long, flat rules. For example,
the binarized grammar allows the production S ? NP NP PP, which may be incorrect.
However, by annotating the symbols as follows:
S ? NP @S1
@S1 ? VP @S2
@S2 ? NP @S2
@S2 ? PP
the grammar now forces the VP to be produced before the NP, but still allows the NP to
be dropped. Similarly, by annotating the symbols as:
S ? NP @S1
@S1 ? VP @S2
@S2 ? NP @S3
@S3 ? PP
the grammar effectively allows only the original rule to be produced.
Initial experiments on Hebrew confirm that moving to higher order horizontal
markovization (encoding more context in the initial binarized rules) degrades parsing
performance, while producing much larger grammars.
The PCFG-LA parsing methodology is very robust, producing state-of-the-art accu-
racies for English, as well as many other languages including German (Petrov and Klein
2008), French (Candito, Crabbe?, and Seddah 2009), and Chinese (Huang and Harper
2009).
4. Baseline Experiments
The baseline system is an ?out-of-the-box? PCFG-LA parser, as described in Petrov
et al (2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12
The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact
experimental settings) after stripping all the functional and morphological information
from the non-terminals.
We evaluate the resulting models on the development set, and consider three
settings:
Seg+POS Oracle: The parser has access to the gold segmentation and POS tags.
Seg Oracle: The parser has access to the gold segmentation, but not the POS tags.
Pipeline: A POS-tagger is used to perform word segmentation, which is then used as
parser input.
A better tag set. Glossing over the parses revealed that the parser failed to learn
the distinction between finite and non-finite verbs. The importance of this linguistic
12 http://code.google.com/p/berkeleyparser/.
133
Computational Linguistics Volume 39, Number 1
Table 1
Baseline: Out-of-the-box BerkeleyParser performance on the dev-set.
Setting Tag set F1 (4 cycles) F1 (5 cycles)
Seg+POS Oracle Core 89.7 89.5
Seg Oracle Core 82.6 83.6
Pipeline Core 76.3 77.2
Seg+POS Oracle Core+Verbs 89.9 90.9
Seg Oracle Core+Verbs 83.3 83.6
Pipeline Core+Verbs 77.1 77.3
distinction for parsing is obvious, and was also noted in Klein and Manning (2003) for
English and in our previous work on parsing Hebrew (Goldberg and Tsarfaty 2008).
Finite and non-finite verbs are easily distinguishable from each other based on surface
form alone. Although finiteness is clearly annotated in the treebank, it is not on the
?core? part of the POS tags and was removed prior to training the parser. In a second
set of experiments the core tag set of the parser was modified to distinguish finite verbs,
infinitives, and modals.13 The original core?tag set aleady includes some important
distinctions, such as construct from non-construct nouns.
Results and discussion. Table 1 presents the parsing results on the development set. With
gold POS tags and segmentation, the results are very high. Accuracy drops considerably
when the parser is not given access to the gold tags (from about 90 to less than 84 F1),
indicating that the POS tags are both informative and ambiguous. Results drop even
further (from 84 to 77) in the pipeline case where the gold segmentation is not available,
indicating that correct segmentation also provides valuable information to the parser
and that segmentation mistakes are costly.
Enriching the tag set to distinguish modals and finite and infinite verbs proved
useful, with an increase of about 1 F1 points (absolute) after four split-merge-smooth
cycles, and a smaller increase after five cycles. This stresses the importance of the core
representation: The automatic learning procedure goes a long way, but it can be aided
by linguistically motivated manual interventions in some cases.
4.1 Analyzing the Learned PCFG-LA Grammar
4.1.1 Terminal-Level (Lexical) Splits. We begin by inspecting the splits at the part-of-
speech level. Table 2 displays the number of splits learned for each of the parts-of-speech
symbols. Prepositions are the most heavily split, followed closely by the somewhat-
generic MOD tag and the nouns.
Nouns and adjectives. The noun and adjective splits are somewhat hard to decipher.
Some of the groups are obvious (things appearing after numbers, last names, parts-of-dates,
time related, places, etc.). Others are are much harder to interpret.
13 Unlike previous work, the distinction is retained only at the POS tag level and not propagated to the
phrase level. The tag-level information is sufficient for the parser to learn the phrase-level distinctions on
its own. Similar observations regarding the usefulness and sufficiency of linguistically motivated manual
state-splitting of preterminals (as opposed to tree-internal nodes) prior to training a latent-variable
grammar were also made by Crabbe? and Candito (2008).
134
Goldberg and Elhadad Parsing System for Hebrew
Table 2
Number of learned splits per POS category after five split-merge cycles.
Tag # Splits Tag # Splits
H 1 CDT 6
HAM 1 CC 7
POS 1 DT 7
REL 1 JJ 7
VB 1 VB-INF 7
AT 2 PRP 8
COM 2 CD 10
JJT 2 RB 13
QW 2 NN 16
RBR 2 NNP 17
VBMD 2 NNT 22
WDT 2 MOD 24
AGR 4 IN 26
AUX 6
MOD. For the general-modification POS tags, most categories clearly single out one
or two words with very specific usage patterns, such as !?? (?no?), !?? (?also?), !?? (?only?),
!?????? (?even?), !????? (?former?), and so forth. The other categories are harder to interpret.
Verbs. Finite-verbs are not split at all, even though they form an open-class category.
Modal verbs are split into two groups: One of them is dominated by nine modals
( !?????, !??, !?????, !???, !???, !?????, !?????, !????, !??????, roughly corresponding to the English could,
should, seem/appear, hard, shouldn?t, possible, appear/seem, important, fitting/required); and
the second contains all the others. This is an interesting distinction, as the nine singled-
out modals never take a subject, whereas the modals in the other group do.14 Infinitive
verbs are split into seven categories, six of which are dominated by one or two words
each, and the last is a catch-all category.
Coordination and question-words. Coordination words are heavily split, each of the
categories dominated by one or two words, indicating different usage patterns. The
question words !??? (?what?) and !??? (?who?) are singled out from the rest.
Gender/number agreement. The verbs are not split at all, indicating that the learned
grammar cannot model subject?verb agreement. Pronouns are split by type (personals,
demonstrative, and subtypes of demonstratives), but not by gender and number. Noun
and adjective splits are sometimes hard to decipher, but they do not exhibit any group-
ing based on gender or number properties, indicating that the grammar cannot model
adjective?noun agreement. Category splits for the AGR tag do show a clear division
that follows gender and number, but it is unclear what is captured by this division as
the information cannot interact with nouns, adjectives, verbs, or pronouns.
14 In fact, the nine modals are very similar in characterization to the words identified in Netzer et al (2007)
as modals, whereas many of the modals in the other group are not necessarily considered as modal
outside of the treebank guidelines.
135
Computational Linguistics Volume 39, Number 1
Table 3
Number of learned splits per NT-category after five split-merge cycles.
Tag # Splits Tag # Splits
FRAGQ 1 ADVP 16
INTJ 6 S 16
FRAG 7 PP 22
SQ 7 VP 22
PRN 8 PREDP 25
ADJP 14 NP 32
SBAR 14
4.1.2 Grammar-Level Splits. Table 3 shows the number of splits learned for each gram-
mar non-terminal. The NP category is the most heavily split, followed by predicative
phrases, verb phrases, and PPs. With the exception of the FRAGQ category, all symbols
are split into at least six substates. What information is encapsulated in the state splits?
As noted by Petrov et al (2006), the latent state-splits learned for the grammar symbols
are harder to analyze.
One way of shedding some light on the meanings of the split-states is by using the
grammar in generationmode and by samplingword sequences from each of the states.15
By looking at the resulting strings, one can sometimes infer the kind of information
encoded in the grammar.
NP. The split-NPs encode phrase length (some splits result in very long NPs, some
in very short, some in very specific one- or two-word patterns). They also encode the
definiteness rules (either an NP is definite or not), the interaction between definiteness
and the AT marker, and a limited interaction between definiteness and construct nouns.
Other NP splits are dedicated to pronouns or to question words, or encode proper
names, monetary units, and numbers.
SBAR. The split-SBARs are split according to the word introducing the SBAR. In
addition, some split-SBARs encode quoted and parenthetical items.
S. The split-Ss differ by length. In addition, some S splits seem to be modeling verb-less
sentences, variations in word order, and sentence-level coordination.
4.2 Limitation of PCFG-LA Parsing of Modern Hebrew
The PCFG-LA baseline is a strong one, and is substantially higher than all previous
reported results for Hebrew parsing in each of the setups (Seg+POS oracle, Seg Oracle,
and no Oracle). We also identify some of its limitations, namely:
Missed splits. The learning procedure is not perfect, and fails to capture some linguis-
tically meaningful state-splits. When such splits are manually supplied (i.e., the trivial
split of verbal types) accuracy improves.
15 Sampling a word sequence is performed by starting at a given state (a split grammar symbol), randomly
choosing a right-hand-side based on the PCFG-induced distribution, expanding the state into the chosen
right-hand side, and continuing recursively until we are left with only strings.
136
Goldberg and Elhadad Parsing System for Hebrew
Sensitivity to non-gold POS. The substantial drop in accuracy when the POS tags are
unobserved and need to be predicted is staggering, which suggests that it is difficult
for the parser to assign part-of-speech tags. Of the 698 part-of-speech errors, 314 are on
words not seen in training.
Sensitivity to non-gold segmentation. The accuracy drops even further when the parser
is presented with predicted segmentation. Segmentation errors are detrimental to the
parser.
Not encoding grammatical agreement. Finally, the learned grammar does not encode
grammatical agreement. Whereas the majority of the parser mistakes are due to the
flexible constituent order or ?standard? ambiguities such as coordination and PP
attachment, a handful of them could be resolved using agreement information.
In what follows, we address these four limitations, and substantially increase the
parser accuracy for the realistic case where gold segmentation and POS tags are not
available.
5. Manual State-Splits
We experimented with several linguistically motivated state-splits which were added as
tree-annotations prior to running the parser. Most of them did not help on their own and
slightly degraded parser performance when combined with other splits. These include
splits which were proven useful in previous work, such as marking of definite NPs, and
distinguishing possessive from other PPs. We also experimented with splits based on
morphological agreement features, which are discussed in Section 8.1.
Overall, the learning procedure is capable of producing good splits on its own. We
did, however, manage to improve upon it with the following annotation (the annota-
tions were removed prior to evaluation).
Subject NPs. Hebrew phrase order is rather flexible, and the subject can appear before
or after the verb. Identifying the subject can thus help in grounding the overall structure
of the sentence. The subject is also dependent on agreement constraints with the verb.
Following Johnson (1998), Klein and Manning (2003) implicitly annotate subject-NPs
in English using parent annotation (distinguishing NPs under S from other NPs), with
good results. When applied to English, the PCFG-LA also learns to model subject NPs
well. Hebrew?s non-configurationality, however, put both Subjects and Objects directly
under S, making it much harder to learn the distinction automatically.
Explicit marking of subject NPs contributes slightly to the accuracy of the parser.
Perhapsmore important than the small increase in accuracy is the fact that the parser can
identify subjects relatively well. In contrast, marking of object NPs did not help by itself
and slightly degraded the parsing accuracy when combined with other annotations.
Note, however, that Hebrew definite objects are already clearly marked using the !??
marker, making them an easy target for the parser.
6. Better Lexical Coverage with an External Lexicon
The drop in parsing accuracy when gold core POS tags are not available and need to be
inferred by the parser is huge (from above 90 to less than 84 F1).
137
Computational Linguistics Volume 39, Number 1
The large number of possible word forms make it very difficult for manually annotated
corpora to provide adequate lexical coverage. The problem is even more severe with
the case of the Hebrew Treebank, which is especially small. Although it is big enough to
learn meaningful syntactic generalizations (as demonstrated by the high performance
of the baseline system) it is far too small to learn a good lexical model (as evidenced by
the drop in accuracy when gold tags are not available).
We suggest increasing the lexical coverage of the parser using an external resource,
namely, a lexicon-based morphological analyzer. We further extend the utility of the
analyzer with lexical tagging probabilities learned from an unannotated corpus.
6.1 A Unified Lexical Probability Model
We would like to use the KC Analyzer (Section 2.3.3) to increase the lexical coverage
of the treebank-trained parser. That is, we would like to improve the lexical model
P(T ? W) of the generative parser. As discussed in Section 2.3.5, however, the tag sets
used by the two resources differ. How can this difference be reconciled?
One possibility is to re-tag the treebank with the KC tag set and then train on this
unified resource. In Goldberg et al (2009), we show that this procedure degrades parser
performance. Instead, Goldberg et al suggest a layered generative approach that retains
the benefits of the treebank tagging for frequent words and resorts to the KC tag set only
for rare and unseen words. Under this approach, frequent words are generated from
treebank POS tags as usual, but rare words follow a generative process in which first
the treebank tag generates a KC tag, and then the KC-tag generates the word. A sample
derivation using this layered representation is presented in Figure 1.
The Treebank-to-KC tag generation probabilities represent a fuzzy, probabilistic
mapping between the two resources. In Goldberg et al (2009), the estimation of these
probabilities was done based on a re-tagging of the treebank to use the KC tag set.
The re-tagging process was far from trivial, and many tagging cases required extensive
debates between human annotators.
Here, we present a new procedure which does not require the treebank to be re-
tagged with a new tag set. It still uses the layered representation, but instead of forcing
one unique KC analysis for each location, it embraces the uncertainty and allows all of
them. This is done by treating the KC-tag assignments as hidden variables, learning
the TB-KC mapping probabilities as part of the grammar training EM process, and
marginalizing the KC tags out for the final tree. The procedure is based on the following
assumptions:
r We have access to trees in which the POS tags ttb are taken from a given tag
set TTB.
...
JJTB
PRP-M-S-3-DEMExt
!??
Figure 1
A layered POS tag representation.
138
Goldberg and Elhadad Parsing System for Hebrew
.
.
.
NNTB
NN-M-SExt NNT-M-SExt VB-M-S-3-PastExt VB-M-S-3-ImpExt
!???
Figure 2
A latent layered POS tag representation.
r We have additional access to an external resource (lexicon) mapping
words to tags text from a different tag set TExt.
r Probabilities involving words which are frequent in the treebank can and
should be based on treebank counts.
r Probabilities involving less frequent words should be smoothed in with
information from the external lexicon.
r Smoothing should have a greater effect on less-frequent words.
r Probabilities for unseen words should be based solely on the external
lexicon.
Figure 2 illustrates the representation used for words which are rare or unseen in the
treebank training data. The treebank tag NNTB (upper level) generates the word-form
!??? (lower level) by considering all the possible KC POS tags allowed for the word in
the morphological analyzer (the middle level). The probabilities related to generating
the KC POS tags are summed, and all the other probabilities are multiplied. The exact
equations are detailed in the following.
Although the needed quantity is the emission probability P(TTB ? W) = P(W|TTB),
it is more convenient (for a reason which will be discussed later) to work with the
tagging probability P(TTB|W). Once the tagging probabilites P(TTB|W) are available,
they can easily be converted to emission probabilities using Bayesian inversion, based
on the relative-frequency estimates of P(W) and P(TTB) which are calculated from the
treebank:16
P(ttb|w)P(w)
P(ttb)
= P(w|ttb) = P(ttb ? w) (1)
16 Our notation uses capital letters to denote random variables, and lower-case letters to denote specific
events. Thus, P(T|W) refers the distributions in which a tag ? T is condition on a word ? W, P(T|w) refers
to the conditional distribution of tags t ? T given a specific word w, and P(t|w) refers to the probability
mass of the specific tag t given word w.
139
Computational Linguistics Volume 39, Number 1
Let us now focus on estimating the tagging probabilities P(TTB|W) for the cases of
frequent, rare, and OOV words.
For frequent words that are seen more than K times in the treebank, we simply use
treebank-based relative-frequency estimates:17
Ptb(ttb|w) =
c(w, ttb)
c(w) (2)
where c(?) is a counting function.
For OOV words that are not seen in the treebank, the tagging probability is estimated
using:
Poov(ttb|w) =
?
text?TExt
P(text|w)P(ttb|text) (3)
where P(TExt|W) is a tagging probability using the external tag set, and P(TTB|TExt) is
a transfer probability relating the tags from the two tag sets (the estimation of these
two probabilities is discussed subsequently). What this does is assume a process in
which the word is tagged by first choosing a tag according to the external lexicon, and
then choosing a tag from the TB tag set based on the external one. The external tag
assignments are then treated as latent variables, and are marginalized out.
Finally, for rare words that are seen only a few times in the treebank, we interpolate the
two quantities, weighted by the word?s frequency in the treebank:
Prare(ttb|w) =
c(w)Ptb(ttb|w)+ Poov(ttb|w)
1+ c(w) (4)
We now turn to describing the estimation of the external tagging probability P(TExt|W)
and the tag transfer probability P(TTB|TExt).
Estimating P(TExt|W). The tagging probability follows the morphological analyzer.
The analyzer provides the possible analyses, but does not provide probabilities for
them. One simple option would be to assign each possible analysis (tag) a uniform
probability, and assign 0 probability for tags not allowed by the lexicon for the given
word. This method is referred to as Punif(TExt|W). We know that not all the possible
analyses for a given word are equally likely, however, and in practice, the actual
tagging distribution is usually biased toward one or two of the tags. These tagging
preferences can be learned in an unsupervised manner given the lexicon and a large
corpus of unannotated text, using EM training of an HMM tagging model. Adler and
Elhadad (2006) suggest such a model for accurate tagging of Hebrew, and Adler (2007)
and Goldberg, Adler, and Elhadad (2008) extend it to provide state-of-the-art tagging
accuracies for Hebrew using a smart initialization. Here, we use the pseudo-counts from
17 In practice, a small amount of smoothing is added to allow tagging a word with open-class tags if it
wasn?t seen within the treebank: Ptb(ttb|w) = (c(w, ttb )+ 0.0001 ? P(ttb ))/(c(w)+ 0.0001).
140
Goldberg and Elhadad Parsing System for Hebrew
the final round of EM training in this tagging model in order to compute Pem(TExt|W).
We show in Section 9 that this unsupervised lexical probabilities estimation does
indeed provide better parsing results.
Estimating P(TTB|TExt). The tagset-transfer probabilities capture the patterns of transfer
between the syntactic tagging scheme of the treebank and the other tagging scheme
of the external resource. They are estimated using treebank counts and the tagging
distribution P(TExt|W):
P(ttb|text) =
c(ttb, text)
c(text)
=
?
w c(ttb,w)P(text|w)
?
w c(w)P(text|w)
(5)
Integration into the PCFG-LA model. The estimation procedure is incorporated into the
training process of the PCFG-LA model. Note that in the PCFG-LA model the treebank
tag set TTB is gradually split, and each tag takes the form ?tag, substate?, where substate
is a latent variable indicating a specific split of the given tag. This means that the
treebank tagging probability and the tag set?transfer probabilities are also defined over
these split tags. Whereas the external tagging probabilities P(TExt|W) are fixed prior to
PCFG-LA training, the other distributions (P(TTBsubstate |W) and P(TTBsubstate |TExt)) are re-
estimated in the EM process following each of the split, merge, and smooth stages. This
is done by replacing the corpus counts c(?) in Equations (2) and (5) with pseudo-counts
(expectations, marginal scores) of the same events in the E step of the EM procedure.
The main reason for using the Bayesian inversion (Equation (1)) instead of working
with the emission probability P(W|T) directly is that the emission probability is
highly dependent on the vocabulary size. The treebank estimates are based on a small
vocabulary, the external lexicon estimates are based on a very large vocabulary, and
a proper combination of the two emission probabilities is not trivial. In contrast, the
tagging probabilities do not depend on the vocabulary size, allowing a very simple
combination. We can then base the counts for the emission probability on the treebank
vocabulary alone, and estimate P(W) for words unseen in training as if they were seen
once.
7. Joint Segmentation and Parsing
When applied to real text (for which the gold word-segmentation is not available), the
baseline PCFG-LA parser is supplied with word segmentation produced by a separate
tagging process.18 This seriously degrades parsing performance. A major reason for the
performance drop is that the word-segmentation task and the syntactic-disambiguation
task are highly related. Segmentation mistakes drive the parser toward wrong syntactic
structures, and many segmentation decisions require long-distance information that is
not available to a sequential process (Tsarfaty 2006a). For these reasons, we claim that
parsing and segmentation should be performed jointly.
18 Although the tagger also produces POS tag assignments, we ignore them and use only the word
segmentation. This is done for two reasons: first, the tag set of the tagger is the one used by the
morphological analyzer, and is not compatible with the treebank. Second, we believe it is better for
the parser to produce its own tag assignments.
141
Computational Linguistics Volume 39, Number 1
Figure 3
The lattice for the Hebrew sequence !?????? ????? (see footnote 19).
Joint segmentation and parsing can be achieved using lattice parsing. Instead of
parsing over a fixed input string, the parser operates on a lattice?a structure encoding
all the possible segmentations.
7.1 Lattice Representation
Formally, a lattice is a directed acyclic graph in which all paths lead from the initial state
to the end state.
For the Hebrew segmentation task, all word segmentations of a given sentence are
represented using a lattice structure. Each lattice arc corresponds to a word and its
corresponding POS tag, and a path through the lattice corresponds to a specific word-
segmentation and POS tagging of the sentence. This is by now a fairly standard repre-
sentation for multiple morphological segmentations of Hebrew utterances (Adler 2001;
Bar-Haim, Sima?an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg,
Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It
is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith,
and Tromble 2005).
Figure 3 depicts the lattice for the two-words sentence !?????? 19.????? Double-circles
indicate the space-delimited token boundaries. Note that in this construction arcs can
never cross token boundaries. Every token is independent of the others, and the sen-
tence lattice is in fact a concatenation of smaller lattices, one for each token. Further-
more, some of the arcs represent lexemes not present in the input tokens (e.g., !?/DT,
!??/POS), although these are parts of valid analyses of the token. Segments with the same
surface form but different POS tags are treated as different lexemes, and are represented
as separate arcs (e.g., the two arcs labeled !????? from node 6 to 7).
A similar structure is used in speech recognition. There, a lattice is used to represent
the possible sentences resulting from an interpretation of an acoustic model. In speech
recognition the arcs of the lattice are typically weighted in order to indicate the probabil-
ity of specific transitions. Given that weights on all outgoing arcs sum up to one, weights
induce a probability distribution on the lattice paths. In sequential tagging models such
as Smith, Smith, and Tromble (2005), Adler and Elhadad (2006), and Bar-Haim, Sima?an,
and Winter (2008) weights are assigned according to a tagging model based on linear
context. For the case of parsing, context-free weighting of lattice arcs is used: each arc
19 Whereas Hebrew is written right-to-left, the lattice is to be read left-to-right. The words on each arc
follow the Hebrew writing directions, and are written right-to-left.
142
Goldberg and Elhadad Parsing System for Hebrew
corresponds to a ?tag,word? pair, and is weighted according to the emission distribution
P(tag ? word).20
7.2 Lattice Parsing
The CKY parsing algorithm can be extended to accept a lattice, instead of a predefined
list of tokens, as its input (Chappelier et al 1999). The CKY search then finds a tree
spanning from the start-state to the end-state of the lattice, where the leaves of the tree
are lattice arcs. The lattice extension of the CKY algorithm is performed by indexing
lexical items according to their start- and end-states in the lattice instead of by their
sentence position, and changing the initialization procedure of CKY to allow terminal
and preterminal symbols of spans of sizes > 1. It is then relatively straightforward to
modify the parsing mechanism to support this change: not giving special treatments
for spans of size 1, and distinguishing lexical items from non-terminals by a specified
marking instead of by their position in the chart.
Figure 4 shows the CKY chart for the lattice in Figure 3, together with an (incorrect)
parse over the lattice. The chart is initialized with parts of speech corresponding to
the lattice arcs. Phrase-structures are then built on top of the POS tags (in blue). The
proposed structure must span the entire chart, and correspond to a path through the
lattice from the initial state (0) to the last one (7).
At training time the correct segmentation is fully observed, and the generative
parser is trained as usual over the treebank. At inference (test) time, the correct seg-
mentation is unknown, and the decoding is applied to the segmentation lattice. The best
derivation returned by the parser forces a specific segmentation. The returned parse tree
is the most probable ?segmentation, tree? pair according to the grammar.21 We modified
the PCFG-LA BerkeleyParser to accept lattice input at inference time.
Lattice parsing allows us to preserve the segmentation ambiguity and present it
to the parser, instead of committing to a specific segmentation prior to parsing. This
way segmentation decisions are performed in the parser as part of the global search
for the most probable structure, and can be affected by global syntactic considera-
tions. We show in Section 9 that this methodology is indeed superior to the pipeline
approach.
Early descriptions of algorithms for parsing over word lattices can be found in
Lang (1974, 1988) and Billott and Lang (1989). Lattice parsing was explored in the
context of parsing of speech signals by Chappelier et al (1999), Sima?an (1999), and
Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation
in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning
(2010).
20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007). There, lattice arc weights
are assigned based on aggregate quantities (forward-backward tagging marginals) derived from a
discriminative CRF tagging model. This approach is not ideal from a modeling perspective, as it makes
each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one.
In this work, a sequential tagging model is not used at all. If the use of a sequential model is desired,
an alternative method for integrating a sequence model and a syntactic model is making the models
?negotiate? an agreed upon structure that maximizes the score under both models, using optimization
techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into
natural language processing (Rush et al 2010).
21 Note that finding the most probable segmentation requires summing over all the trees resulting in each
segmentation?a much harder task, proven to be NP-complete in Sima?an (1996).
143
Computational Linguistics Volume 39, Number 1
Figure 4
Lattice initialization of the CKY chart.
8. Incorporating Morphological Agreement
Inspecting the learned grammars reveal that they do not encode any knowledge of
morphological agreement: The split categories for nouns, verbs, and adjectives do not
group words according to any relevant morphological property such as gender or
number, making it impossible for the grammar to model agreement patterns. At the
same time, inspecting some of the bad parses reveals several clear cases of agreement
mistakes. Can morphological agreement be incorporated in the parsing model?
8.1 Forcing Morphologically Motivated Splits
Our initial attempts focused on making the PCFG-LA learning procedure pick up on
agreement-relevant state-splits. When neither the core tag set nor the non-terminals
encode gender and number information, it is very hard for the parser to pick up on
agreement patterns.22
We attempted to train the parser on trees which mark the agreement features (either
the gender, the number, or both) either on the POS tags, the relevant constituents, or
22 In the external lexicon case, the external lexicon tags do encode the morphological features, making
it possible in principle for the parser to learn to map certain substates to certain agreement features.
This did not happen in practice, arguably because other structural factors were more powerful than
the agreement ones.
144
Goldberg and Elhadad Parsing System for Hebrew
both. Annotating agreement features on the POS tag?level made the parsing much
slower, but did make the parser assign certain split categories to certain gender?number
combinations, and sampling utterances from the learned grammar did indicate a notion
of grammatical agreement. This did not improve parsing accuracy, however?and even
slightly degraded it.
When propagating the agreement features and annotating them on the constituent
level, parsing accuracy dropped considerably. When inspecting the learned grammar
we observe that most of the agreement-annotated constituents (e.g., NPMasc,Plural) were
still fully split, indicating that the parser picked on patterns which were orthogonal to
the agreement mechanism. The pre-splitting according to agreement-features properties
caused data sparseness, aided over-fitting, and hurt parsing performance: The smooth-
ing procedure of the BerkeleyParser shares some probability-mass between various
splits of the same symbol, but was not applied in our case (no information flowed
between, for example, NPMasc,Plural and NPMasc,Singular). We attempted to counter this
effect by changing the smoothingmechanism of the BerkeleyParser to share information
also between the manually split symbols. This brought parsing accuracy back to the
initial level, but also caused the parser to, again, not model agreement very well. The
reason for this is clear in hindsight: Morphological agreement is an absolute concept,
not a fuzzy one (things can either agree or not). Smoothing the probabilities between
the different morphology-based split-licensed grammar rules that allow morphological
disagreement, and made the grammar lose its discrimination power. This was then
reinforced by the training process, which picked on other syntactic factors instead, and
further phased out the agreement knowledge.
A note on product-grammars. In recent work, Petrov (2010) showed that a committee of
latent-variable grammars encoding different grammatical preferences can be combined
into a product-grammar that is better than the individual ensemble members. Petrov
created the ensemble by training several PCFG-LA parsers on the same data, but using
different random seeds when initializing the EM starting point. We attempted to cre-
ate a similar ensemble by providing the learning process with different linguistically
motivated tree annotations (with and without encoding agreement features, with and
without encoding definiteness, etc.). The combined parser did increase the performance
level over that of the individual parsers, but an ensemble with the same number of
components that was produced using the random-seeds approach produced far su-
perior results. This reinforces the findings of Petrov (2010) who also reports that the
ensemble creation using random initialization is exceptionally strong and outperforms
other methods of ensemble creation.23
8.2 Agreement as Filter
We now turn to suggest an approach to modeling agreement, which rests on the follow-
ing principles:
r Agreement can be modeled as a set of hard (not probabilistic) constraints.
r Agreement is completely orthogonal to the other aspects of the grammar.
23 The product grammar approach with random seeds works well and is effective for improving the
accuracy of Hebrew parsing. As it is completely orthogonal to the approaches presented in this article,
however, we chose not to discuss it further other than commenting on its applicability.
145
Computational Linguistics Volume 39, Number 1
Based on these principles, we suggest treating agreement as a filter, a device that can
rule out illegal parses. Under the agreement-as-filter framework, we want the parser to
produce the most probable parse according to its grammar and subject to hard agreement
constraints. This approach completely decouples the grammar from the agreement ver-
ification mechanism. The agreement information is not modeled in the grammar and
is not used to guide the search for the best parse. Instead, it is a separate process that
imposes hard constraints on the search space and rules out parts of it completely. That
is, agreement is a part of the parser and not of the grammar. This is similar in spirit to
ideas from constraint-based grammars such as LFG (Falk 2001) and HPSG (Pollard and
Sag 1994), which also model aspects of the syntax as Boolean constraints.
Grammatical agreement is a relation between constituents. The relevant morpho-
logical features are propagated from one of the leaves up to the constituent level.
When constituents are combined to form a larger constituent, their morphological
features are assigned to the newly created constituent according to language-specific
rules (it is possible that different morphological features will be assigned by different
constituents). An agreement violation occurs when two or more constituents assign
conflicting features to their parent.
Implementation. In the implementation, an agreement-verification mechanism is man-
ually constructed (not learned) based on a set of simple, language-dependent rules.
First, we provide a set of rules to propagate the morphological agreement features from
the leaves to the constituents. Then, we specify an additional set of rules to inspect
local tree configuration and identify agreement violations (the Hebrew set of rules is
described later, along with a concrete example). The feature-propagation mechanism
works bottom?up and the agreement verification rules are very local, making it possible
to integrate the filtering mechanism into a bottom?up CKY parsing algorithm (refusing
to complete a constituent if it violates an agreement constraint). We did not pursue this
route for the experiments in this work, however. Instead, we opted for an approximation
in which we take the 100-best trees for each sentence, and choose the first tree that
does not have an agreement violation (this is an approximation because the 100-best
trees may not contain a valid tree, in which case we accept the agreement violation and
choose the first-best tree). The specific details of the Hebrew agreement filter are given
in the appendix.
Verifying the hard-constraint property. We verified that the hard constraint assumption
works and that the agreement verification mechanism is valid by applying the proce-
dure to the gold-standard trees in the training-set and checking that (1) the propagated
features agree with the manually marked ones, and (2) none of the training-set trees
were filtered due to agreement violation. We did find a few cases in which the prop-
agated features disagreed with the manually marked ones, and a few gold-standard
trees that the mechanism marked as containing an agreement violation. All of these
cases were due to mistakes in the manual annotation.
Connections to parse-reranking. Our implementation is similar to parse-reranking
(Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model
agreement as soft constraints, we could have incorporated this information as features
in a reranking model. The filter approach differs in that it poses hard constraints and
not soft ones, pruning away parts of the search space entirely. Thus, the use of k-best
list is merely a technical detail in our implementation?the agreement information is
146
Goldberg and Elhadad Parsing System for Hebrew
easily decomposable and the hard constraints can be efficiently incorporated into the
CKY search procedure.
9. Evaluation and Results
Data set. For all the experiments we use Version 2 of the Hebrew Treebank (Guthmann
et al 2009), with the established test-train-dev splits: Sentences 484?5,740 are used for
training, sentences 1?483 are the development set, and sentences 5,741?6,220 are used
for the final test set.
Evaluation Measure. In the cases where the gold segmentation is given, we use the well-
known evalb F1 score. Namely, each tree is treated as a set of labeled constituents.24
Each constituent is represented as a 3-tuple ?i, j,L?, in which i and j are the indices of
the first and the last words in the constituent, respectively, and L is the constituency
label. For example, (2, 4,NP) indicates an NP spanning from word 2 to word 4. The
performance of a parser is evaluated based on the amount of constituents it recovered
correctly. Let G denote the set of constituents in a gold-standard constituency tree, and
P denote the set of constituents in a predicted tree. Precision (P), recall (R), and F1 are
defined as:
precision = |G ? P||P| recall =
|G ? P|
|G|
F1 = 21
precision +
1
recall
F1 ranges from 0 to 1, and it is 1 iff both precision and recall are 1, indicating the trees
are identical. We report numbers in precentages rather than fractions.
When measuring the performance of models in which the token-segmentation is
predicted and can contradict the gold-standard, a generalization of these measures is
used. Instead of representing a constituent by a triplet ?i, j,L?, each constituent is repre-
sented by a pair containing the concatenation of the words at its yield, and its label L.
This measure was suggested by Tsarfaty (2006a) and used in subsequent work (Tsarfaty
2006b; Goldberg and Tsarfaty 2008; Goldberg et al 2009; Goldberg and Elhadad 2011).
This is equivalent to reassigning the i and j indices to represent character positions
instead of word numbers. When the yields of the gold standard and the predicted trees
are the same, this is equivalent to the standard evaluation measure using the ?i, j,L?
triplets of word indices and a label, and it will produce the same precision, recall, and
F1 as above.
Effect of external lexicon. We start by evaluating the effect of extending the parser?s lexical
model with an external lexicon, as described in Section 6.1. The rare-word threshold
is set to 100. We use the morphological analyzer described in Section 2.3.3. We test
two conditions: UNIFORM, in which the P(Text|w) distribution is uniform over all the
24 This assumes unary-chains do not contain cycles.
147
Computational Linguistics Volume 39, Number 1
Table 4
Dev-set results when incorporating an external lexicon.
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
Seg Oracle NONE 83.13 83.39
Pipeline NONE 75.98 76.65
Seg Oracle UNIFORM 84.92 84.56
Pipeline UNIFORM 77.53 77.35
Seg Oracle HMM-BASED 86.17 85.79
Pipeline HMM-BASED 78.75 78.78
analyses suggested by the morphological analyzer for the word, and HMM-BASED in
which the P(Text|w) distribution is based on pseudo-counts from the final round of EM?
HMM training of the semi-supervised POS tagger described in Section 2.3.4. Results are
presented in Table 4.
Incorporating the external lexicon helps both in the case where the correct segmen-
tation is assumed to be known, as well as in the pipeline case where the segmentation is
automatically induced by a sequential tagger. Incorporating the semi-supervised lexical
probabilities learned over large unannotated corpora (HMM-BASED) further improves
the results, up to 86.1 F1 for the gold-segmentation case and 78.7 F1 for the pipeline
case. The pipeline model still lags behind the gold-segmentation case, indicating that
the correct segmentation is very informative for the parser.
Joint segmentation and parsing. Having established that the external lexicon can be effec-
tively incorporated into the parser, we turn to evaluate the method for joint segmenta-
tion and parsing. We follow the same conditions as before (UNIFORM and HMM-BASED
lexical probabilities), but in this set of experiments the parser is allowed to choose its
preferred segmentation using the lattice-parsing methodology presented in Section 7.2.
The lattice is constructed according to the analyses licensed by the morphological
analyzer. Table 5 lists the results. Lattice parsing is effective, leading to an improvement
of about 2?3 F1 points over the pipeline model.
Agreement filter. We now turn to add the agreement filtering on top of the lexicon-
enhanced models. In this setting, the model outputs its 100-best trees for each sentence,
agreement features are propagated, and agreement violations are checked as described
Table 5
Dev-set results when using lattice parsing on top of an external lexicon/analyzer.
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
Pipeline UNIFORM 77.53 77.35
Lattice (Joint) UNIFORM 80.35 80.31
Pipeline HMM-BASED 78.75 78.78
Lattice (Joint) HMM-BASED 80.91 80.46
148
Goldberg and Elhadad Parsing System for Hebrew
Table 6
Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting from
gold segmentation).
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
No Agreement UNIFORM 84.92 84.56
Agreement as Filter UNIFORM 85.30 84.52
No Agreement HMM-BASED 86.17 85.79
Agreement as Filter HMM-BASED 86.55 86.25
in Section 12, and the first tree that does not contain any agreement violation is returned
as the final parse for the sentence (or the first-best tree in case that all of the output
trees contain an agreement violation). Table 6 lists the results when agreement filtering
is performed on top of parses based on gold segmentation, and Table 7 lists the results
when agreement filtering is performed on top of a lattice-based parsing model that does
not assume gold segmentation is available.
Discussion of agreement filter results. Although the agreement filter does not hurt
the parser performance, the benefits from it are very small. To understand why that
is the case, we analyzed the 1-best parses produced by the 5-cycles-trained grammar on
the gold-segmented development set (these conditions corresponds to the last column
of the third row in Table 6). The analysis revealed the following reasons for the low
impact of the agreement filter: (1) The grammar is strong enough to produce fairly
accurate structures, which have very few agreement mistakes to begin with, and (2)
fixing an agreement mistake does not necessarily mean fixing the entire parse?in some
cases it is very easy for the parser to fix the agreement mistake and still produce an
incorrect parse for other parts of the structure.
The 1-best trees of the 480 sentences of the development set contain 22,500 parse-
tree nodes. Of these 22,500 nodes, 2,368 nodes triggered a gender-agreement check:
about 10% of the parsing decisions could benefit from gender agreement. Of the 2,368
relevant nodes, however, 130 nodes involved conjunctions or possessives, and were
outside of the scope of our agreement verification rules. Of the remaining 2,238 parse-
tree nodes, 2,204 passed the agreement check, and only 34 nodes (1.5% of the relevant
nodes, and 0.15% of the total number of nodes) were flagged as gender-agreement
violations. Similarly for number agreement, 2,244 nodes triggered an agreement check,
of which 2,131 nodes could be handled by our system. Of these relevant nodes, 2,109
nodes passed the gender-agreement check, and only 23 nodes (1.07% of relevant nodes,
Table 7
Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser
does both segmentation and parsing).
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
No Agreement UNIFORM 80.35 80.31
Agreement as Filter UNIFORM 80.55 80.74
No Agreement HMM-BASED 80.91 80.46
Agreement as Filter HMM-BASED 81.04 80.72
149
Computational Linguistics Volume 39, Number 1
Table 8
Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or
number agreement checks, and the results of these checks.
Gender Agreement Number Agreement
Triggered agreement check 2,368 2,244
Could be handled by the system 2,238 2,131
No agreement violation 2,204 2,109
Agreement violation 34 23
and 0.1% of the total nodes) were flagged as agreement violations. The numbers are
summarized in Table 8. It is clear that the vast majority of the parser decisions are
compatible with the agreement constraints.
Turning to inspect the cases in which the agreement filter caught an agreement
violation, we note that the agreement filter marked 51 of the 480 development sentences
as having an agreement violation in the 1-best parse?about 10% of the sentences could
potentially benefit from the agreement filter. For 38 of the 51 agreement violations, the
agreement violation was fixed in the tree suggested in the 100-best list. We manually
inspected these 51 parse trees, and highlight some the trends we observed. In the
13 cases in which the 100-best list did not contain a fix to the agreement violation,
the cause was usually that the 1-best parse had many mistakes that were not related
to the agreement violation, and diversity in the 100-best list reflected fixes to these
mistakes without affecting the agreement violation. Another cause of error was an erro-
neous agreement mistake due to an omission in the lexicon. Of the 38 fixable agreement
violations, 25 were local to a noun-phrase, 10 were cases of subject?verb agreement,
and the remaining three were either corner-cases or harder to categorize. The subject?
verb agreement violations were handled almost exclusively by keeping the structure
mostly intact and changing the NPSUBJ label to some other closely related label that does
not require verb agreement, usually NP. This is a good strategy for fixing subject-less
sentences (about half of the cases), but it is only a partial fix in case the subject should
be assigned to a different NP (which does not happen in practice) or in case a more
drastic structural change to the parse-structure is needed. In one of the 10 cases, the
subject?verb agreement mistake indeed resulted in a structural change that improved
the overall parse quality. The NP internal agreement violations include many cases of
noun-compound attachments, and some cases involving coordination. The corrections
to the agreement violation were mostly local, and usually resulted in correct structure,
but sometimes introduced new errors. Figure 5 presents some examples of the different
cases. Our overall impression is that for NP internal mistakes the agreement-filtering
method was mostly doing the right thing.
To conclude, the agreement filter is useful in overcoming some errors and providing
better parses, especially with respect to noun-compound construct-state constructions.
Due to the limited number of parsing mistakes involving agreement violations, how-
ever, and because of the local nature of the agreement-violation mistakes, the total effect
of the agreement filter on the final parsing score is small.
10. The Final Model
Finally, we evaluate the best performing model on the test set. Table 9 presents the
results of parsing the test set while incorporating the external lexicon and using the
150
Goldberg and Elhadad Parsing System for Hebrew
Figure 5
NP agreement violations that were caught by the agreement filter system. (a) Noun-compound
case that was correctly handled. (b) Case involving conjunction that was correctly handled.
(c) A case where fixing the agreement violation introduces a PP-attachment mistake.
Table 9
Test-set results of the best-performing models.
Setting Model F1 (4 cycles)
Gold Segmentation HMM-Based External Lexicon 85.67
+ Agreement 85.70
Lattice-parsing HMM-Based External Lexicon 76.87
+ Agreement 76.95
151
Computational Linguistics Volume 39, Number 1
HMM-based probabilities, for a grammar trained for four split-merge iterations. This
grammar is applied both to the gold-segmentation case and to the realistic case where
segmentation and parsing are performed jointly using lattice-parsing. We also test the
effectiveness of the agreement-filter in both situations.
Agreement information does not hurt performance, but contributes very little to the
final accuracy?additionally on the test sentences, the parser makes very few agreement
mistakes to begin with.
Consistent with previous reports (Tsarfaty 2010), the test set is somewhat harder
than the development set. With gold-segmentation, the models achieve accuracies of
85.70% F1. In the realistic scenario in which the segmentation is induced by the parser,
the accuracies are around 76.9% F1. We verified that the HMM-based lexical probabili-
ties also outperform the Uniform probabilities on the test set (the F1 scores when using
uniform lexical probabilities are 84.06 and 76.30 for the gold and induced segmenta-
tions, respectively). These are the best reported results for parsing the test-set of the
Hebrew Treebank.
11. Related Work in Parsing of Morphologically Rich Languages
Coping with unknown words. Several papers show that the handling of unknown words
is a major component to be considered when adapting a parser to a new language.
For example, the work in Attia et al (2010) uses language-specific unknown-word
signatures for several languages based on various indicative prefixes and suffixes, and
Huang and Harper (2009) suggest a Chinese-specific model based on the geometric
average of the emission probabilities of the individual characters in the rare or unknown
word. Another method of coping with lexical sparsity is word clustering. In Candito
and Crabbe? (2009), the authors demonstrate that replacing words by a combination of
a morphological signature and a word-cluster (based on the linear context of a word in
a large unannotated corpus) improves parsing performance for French. The technique
provides more reliable estimates for in-vocabulary words (a given cluster appears more
frequently than the actual word form), and it also increases the known vocabulary:
Unknown words may share a cluster with known words.
Arabic. Arabic is similar to Hebrew in the challenges it presents for automatic pars-
ing. Most early work on constituency parsing of Arabic focused on straightforward
adaptations of Bikel?s parser to Arabic, with little empirical success. Attia et al (2010)
show that parsing accuracies of around 81% F1 can be achieved for Arabic (assuming
gold word segmentation) by using a PCFG-LA parser with Arabic-specific unknown-
word signatures. Recently, Green and Manning (2010) report on an extensive set of
experiments with several kinds of tree annotations and refinements, and report pars-
ing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA
BerkeleyParser, both when assuming gold word segmentation. The work of Green and
Manning also explored the use of lattice-parsing as suggested in Section 7 of this article,
as well as earlier in Goldberg and Tsarfaty (2008) and Cohen and Smith (2007), and
report promising results for joint segmentation and parsing of Arabic (an F1 score of
76% for sentences of up to 70 words). The best reported results for parsing Arabic
when the gold word segmentation is not known, however, are obtained using a pipeline
model in which a tagger and word-segmenter is applied prior to a manually state-split
constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words)
(Green and Manning 2010).
152
Goldberg and Elhadad Parsing System for Hebrew
Hebrew and relational-realizational parsing. Some related work deals directly with con-
stituency parsing of Modern Hebrew. The work of Tsarfaty and Sima?an (2007) experi-
ments with grammar refinement for Hebrew, and shows that annotating definiteness
and accusativity of constituents, together with parent annotation, improves parsing
accuracy when gold word segmentation is available.
The Relational Realizational (RR) line of work presented in Tsarfaty et al (Tsarfaty
and Sima?an 2008; Tsarfaty, Sima?an, and Scha 2009; Tsarfaty and Sima?an 2010; Tsarfaty
2010) handles the constituent-order variation in Hebrew by presenting a separation
between the form and function aspects of the grammar. Briefly, whereas plain treebank-
derived grammars have rules such as S ? NP VP PP NP PP that are applied in a
single step, the RR approach suggests a generative model in which the generation of
flat clausal structures is decomposed into three distinct steps. First, in the projection step,
a non-terminal generates the kinds of its children without specifying their form or the
order between them, using rules of the form S ? {OBJ,SBJ,PRED,COM,Adjunct}@S.
Second, in the configuration step, an order is chosen based on a separate ordering
distribution, using rules of the form
{OBJ,SBJ,PRED,COM,Adjunct}@S ? SBJ@S PRED@S Adj@S OBJ@S COM@S.
Third, in the realization step, each functional element receives a specific form, using rules
of the form SBJ@S ? NP or Adj@S ? PP. The realization rules can encode syntactic
properties that are required by the grammar for the given function?for example, a
rule such as OBJ@S ? NPdef,acc captures the requirement that definite objects in Hebrew
must be marked for accusativity using the !?? marker, and the rest if the generative
process will generate the object NP according to this specified constraint. This kind of
linguistically motivated separation of form and function is shown to produce models
with fewer parameters and result in better parsing accuracies than plain (or head-
driven) PCFGs derived from the same trees.
The relational-realizational model can accommodate agreement information. It is
shown in Tsarfaty and Sima?an (2010) that, given gold-standard POS tags that include
the gender and number information for individual words, RR models enriched with
gender and number agreement information can provide Modern Hebrew parsing ac-
curacies of 84% F1 for sentences of up to 40 words, the highest reported number for
Modern Hebrew parsing based on gold POS tags and word-segmentation by the time
of its publication.
Although the RR framework is well motivated linguistically and appealing aesthet-
ically, in the current work we chose to rely on the extreme markovization employed by
the PCFG-LA BerkeleyParser in order to cope with the constituent order variation, and
tomodel agreement as an external filter that is orthogonal to the grammar. The approach
taken in this article provides state-of-the-art results for Hebrew constituency parsing.
We leave the question of integrating the RR approach with the approach presented here
to future work.
12. Conclusions
We presented experiments on Hebrew Constituency Parsing based on the PCFG-LA
methodology of Petrov et al (2006). The PCFG-LA model performs well out-of-the-box,
especially when the gold POS tags are available to the parser. It is possible to improve
the learned grammar, however, by specifying some manual state-splits, specifically
153
Computational Linguistics Volume 39, Number 1
distinguishing between modal, finite, and infinitive verbs, and explicit marking of
subject-NPs.
Parsing accuracies drop considerably when the gold POS tags are not available, and
drop even further when using non-gold segmentation. A large part of the drop when
the gold POS tags are not available is due to the large percentage of lexical events that
are unseen or seen only a few times in the training set. This drop can be mitigated
by extending the lexical coverage of the parser using an external lexical resource such
as a wide-coverage morphological analyzer for mapping lexical items to their possible
POS tags. The POS-tagging schemes assumed by the treebank and the morphological
analyzer need not be compatible with each other: We present a method for bridging
the POS tags differences between the two resources. The morphological analyzer does
not provide lexical probabilities. Parsing accuracies can be further improved by using
lexical probabilities which are derived in a semi-supervised fashion based on the mor-
phological analyzer and a large corpus of unannotated text.
The correct token-segmentation is very important for achieving high-quality parses,
and when the gold segmentation is not available, parsing results drop considerably.
It is better to let the parser induce its preferred segmentation in interaction with the
parsing process rather than to use a segmentation based on an external sequence model
in a pipeline fashion. The joint induction of both the syntactic structure and the token-
segmentation can be performed by representing the possible segmentations in lattice
structure, and using lattice parsing. Joint parsing and segmentation is shown to outper-
form the pipeline approach. The parsing accuracies with non-gold segmentation are still
far below the accuracies when the gold-segmentation is assumed to be known, however,
and accurate parsing with non-gold segmentation remains a challenging open research
problem.
The learned PCFG-LA grammar is not capable of modeling agreement information.
We considered methods of using morphological agreement information to improve
parsing accuracy. We propose modeling agreement information as a filtering process
that is orthogonal to the grammar used for parsing. The approach works in the sense
that, in contrast to other methods of using agreement information, it does not degrade
parsing accuracy and even improves it slightly. The benefit from the agreement filtering
is small, however: With the strong grammar induced by the PCFG-LA training pro-
cedure, the parser makes very few agreement mistakes to begin with. Modeling mor-
phological agreement is probably more useful in syntactic generation than in syntactic
parsing. We expect the filtering approach we propose to be proven useful for tasks
involving syntactic generation, such as target-side-syntax machine translation into a
morphologically rich language.
Overall, we presented four enhancements to the PCFG-LA mechanism in order
to adapt it to parsing Hebrew: the introduction of manual, linguistically motivated
state-splits; extending the lexical coverage of the parser using an external morpho-
logical analyzer; performing segmentation and parsing jointly using a lattice parser;
and incorporating agreement information in a filtering framework. Together, these
enhancements result in the best published results for Hebrew Constituency Parsing to
date.
Appendix A: The Hebrew Agreement Filter
Hebrew syntax requires agreement in gender, number, and person. The implementation
considers only the gender and number features, which are the most common. Each of
154
Goldberg and Elhadad Parsing System for Hebrew
the features can take one of five values Masculine, Feminine, Both, Unknown, and NA for
Gender, and Singular, Plural, Both, Unknown and NA for Number. Masculine, Feminine,
Singular, and Plural are self-explanatory, and are assigned when the feature value is
obvious. NA means that the feature is irrelevant for the given constituent (adverbs
and PPs do not carry gender or number features). Both and Unknown are assigned
when we are uncertain about the corresponding feature value. Both and Unknown are
identical in the sense that they leave the feature value unspecified, and have the same
effect on the filtering process. From a practical perspective they could be collapsed
into the same category. We chose to maintain the distinction between the two cases
because they have slightly different semantics. Both indicates that both options are
possible (for example, the form !????? is ambiguous between the plural girls and the
singular childhood, and the titular !??, Dr. can refer both to males and females), whereas
Unknown means that the feature value could not be computed due to a limitation
of the model (for example, there is no clear rule as to the gender of a conjunction
which coordinates masculine and feminine NPs, and we are currently unable to accu-
rately infer the gender and number associated with certain complex quantifiers such
as !??? (most). Compare: !????? ?????? ???, !??????? ????? ???, !????? ??? ?????? (?most of
the classfem stayedmasc, most of the cakefem was eatenfem, most of the cakefem was
eatenmasc?).
Feature values are said to agree if they are compatible with each other. Feminine is
compatible with NA, Both, and Unknown but not with Masculine. Similarly, Singular is
compatible with NA, Both, and Unknown, but not with Plural.
Agreement cases. The system is designed to handle the following cases of morphological
agreement:
NP level agreement between nouns and adjectives. !?????? ?????? ??????? ???? (?box-ofSg
applesPl greenPl bigPl?) , !???? ?????? ??????? ???? (?box-ofSg applesPl greenPl bigSg?)
S level agreement between subject and verbs. !??? ?????? ??? (?[one-of the-kids]Sg
walkedSg?)
Predicative agreement between the subject, ADJP, and copular element. !???? ??? (?he
[is] smartmasc?), !???????? ???? ??? (?she was amazing/fem?), but not with nouns ????
!???? ??? (?she was a-symbolmasc?).
Agreement between the Verb in a relativized SBAR and the realization of the Null-
subject in the external NP.
!?????? ???? ? ?????? (?the-committeefem which [*] discussedfem the-matter?)
Morphological feature propagation. The first step of determining agreement is propagating
the relevant features from the leaves up to the constituent level.
The procedure begins by assigning each leaf gender and number features. These
are assigned based either on the TB tag assigned for the word if training on gold
POS tags, or on the morphological analyzer entries for the given word (in most cases
the number and gender features are easy to predict, even in cases where the core
POS is not clear. In the relatively rare cases where the analyzer contains both a fem-
inine and masculine (alt. singular and plural) analyses, feature value is marked as
Both).
155
Computational Linguistics Volume 39, Number 1
Table A.1
Gender and number percolation rules. FC = first child with non-NA gender/number. Rules for
each constituent type are applied in order, until a condition holds. Rules for gender and number
are applied independently of each other.
Constituent Condition Feature Values
SBAR has REL and S children S.features
SBAR otherwise NA
PREDP has ADJP child ADJP.features
PREDP has AGR child and no NP child AGR.features
PREDP otherwise NA
S has VP child and no NP-Subj child VP.features
S has VB child and no NP-Subj child VB.features
S otherwise NA
NNPG always U
NP has NNT child NNT.features
NP has CDT and NP children CDT.number NP.gender
NP is a conjunction gender=U number=Plural
NP has a ? child U
NP first child is NP, second is POS NP.features
NP has IN child FC.gender number=U
NP has child with non-NA gen/num FC.gender FC.number
NP otherwise NA
ADJP has JJT child JJT.features
ADJP has child with non-NA gen/num FC.gender FC.number
ADJP otherwise NA
VP has VB child VB.features
VP has VB-Modal child VB-Modal.features
VP has VP child VP.features
VP otherwise NA
other always NA
After each leaf is assigned feature values, the features are propagated up the tree
according to a set of rules such as the following (the complete set of rules is given in
Table A.1):
r If the constituent is an NP and has a Construct-noun child, it is assigned
the gender of the Construct-noun.
r If the constituent is a coordinated NP (has a CC child), set its number
feature to plural.
r If the constituent is an S and it has VP child but no NP-Subject child, take
the gender from the VP.
Agreement rules. Once the features are propagated from the leaves to a constituent,
agreement is verified at the constituent level according to the following rules:
NP agreement rules:
r Agreement for coordinated NPs and Possessive NPs is not checked.
156
Goldberg and Elhadad Parsing System for Hebrew
r If NP has an SBAR child, all the children up to the SBAR whose type is
nominal or adjectival must agree in gender and number.
r If NP has an ADJP child, all the children up to the ADJP whose type is
nominal or adjectival must agree in gender and number.
(a)
NP
NP
NNTFem,Sg
!??????
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(b)
NP
NP
NNTFem,Sg
!??????
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(c)
NPFem,Sg
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(d)
NPFem,Sg
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
Figure A.1
Agreement annotation and validation example: correct tree. The sentence words translate to
box-of apples green big, literally, a big box of green apples.
(a)
NP
NNTFem,Sg
!??????
NP
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(b)
NP
NNTFem,Sg
!??????
NP
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(c)
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(d)
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
Figure A.2
Agreement annotation and validation example: incorrect tree, agreement violation. box-of apples
green big, literally, a big box of green apples, though the parse tree suggests the interpretation a box
of big green apples.
157
Computational Linguistics Volume 39, Number 1
S agreement rule:
r All children of S with type in {NP-Subject, VP, VB, AUX, PREDP} must
agree in their gender and number features.
ADJP agreement rule:
r All children of ADJP with type in {NP, NP-Subject, NN, JJ, ADJP} must
agree in their gender and number features.
An example. Consider the tree in Figure A.1a. In the first stage (Figure A.1b), agreement
features are propagated according to the rules in Table A.1, resulting in the annotated
tree in Figure A.1c. Agreement is then validated in Figure A.1d (nodes in which an
agreement rule applied and passed are marked in green). In contrast, the tree in Fig-
ure A.2a has an agreement mistake. As before, the agreement features are propagated
according to the rules (Figure A.2b) resulting in Figure A.2c. Agreement validation
fails at Figure A.2d (the node in which agreement validation was applied and failed
is marked in red).
References
Abeille?, Anne, Lionel Cle?ment, and Franc?ois
Toussenel. 2003. Building a treebank for
French. In A. Abeille?, editor. Treebanks:
Building and Using Parsed Corpora.
Springer, Berlin, pages 165?188.
Adler, Meni. 2001. Hidden Markov model for
Hebrew part-of-speech tagging. Master?s
thesis, Ben-Gurion University of the
Negev.
Adler, Meni. 2007. Hebrew Morphological
Disambiguation: An Unsupervised Stochastic
Word-based Approach. Ph.D. thesis,
Ben-Gurion University of the Negev.
Adler, Meni and Michael Elhadad. 2006.
An unsupervised morpheme-based
HMM for Hebrew morphological
disambiguation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 665?672, Sydney.
Adler, Meni, Yoav Goldberg, David
Gabay, and Michael Elhadad. 2008a.
Unsupervised lexicon-based resolution of
unknown words for full morphological
analysis. In Proceedings of ACL-08: HLT,
pages 728?736, Columbus, OH.
Adler, Meni, Yael Netzer, David Gabay,
Yoav Goldberg, and Michael Elhadad.
2008b. Tagging a Hebrew corpus: The case
of participles. In Proceedings of LREC 2008,
pages 3167?3174, Marrakech.
Attia, Mohammed, Jennifer Foster, Deirdre
Hogan, Joseph Le Roux, Lamia Tounsi,
and Josef van Genabith. 2010. Handling
unknown words in statistical
latent-variable parsing models for Arabic,
English and French. In Proceedings of the
NAACL HLT 2010 First Workshop on
Statistical Parsing of Morphologically-Rich
Languages, pages 67?75, Los Angeles, CA.
Bar-Haim, Roy, Khalil Sima?an, and Yoad
Winter. 2005. Choosing an optimal
architecture for segmentation and
POS-tagging of Modern Hebrew. In
Proceedings of the ACL Workshop on
Computational Approaches to Semitic
Languages, pages 39?46, Ann Arbor, MI.
Bar-Haim, Roy, Khalil Sima?an, and Yoad
Winter. 2008. Part-of-speech tagging of
Modern Hebrew text. Natural Language
Engineering, 14(2):223?251.
Billott, Sylvie and Bernard Lang. 1989. The
structure of shared forests in ambiguous
parsing. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics, pages 143?151, Vancouver.
BGU Computational Linguistics Group.
2008. Hebrew morphological tagging
guidelines. Technical report, Ben Gurion
University of the Negev.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing with
empty elements. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 212?216, Portland, OR.
Candito, Marie and Beno??t Crabbe?. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the 11th International
158
Goldberg and Elhadad Parsing System for Hebrew
Conference on Parsing Technologies
(IWPT?09), pages 138?141, Paris.
Candito, Marie, Beno??t Crabbe?, and
Djame? Seddah. 2009. On statistical
parsing of French with supervised
and semi-supervised strategies.
In EACL 2009 Workshop Grammatical
Inference for Computational Linguistics,
pages 49?57, Athens.
Chappelier, J., M. Rajman, R. Aragues, and
A. Rozenknop. 1999. Lattice parsing for
speech recognition. In Sixth Conference
sur le Traitement Automatique du Langage
Naturel (TANL?99), pages 95?104, Carge?se.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing and
maxent discriminative reranking.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 173?180,
Ann Arbor, MI.
Cohen, Shay B. and Noah A. Smith. 2007.
Joint morphological and syntactic
disambiguation. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 208?217, Prague.
Collins, Michael and Terry Koo. 2005.
Discriminative reranking for natural
language parsing. Computational
Linguistics, 31(1):25?69.
Crabbe?, Beno??t and Marie Candito. 2008.
Expe?riences d?analyses syntaxique
statistique du franc?ais. In Proceedings
of TALN, pages 45?54, Avignon.
Dantzig, G. B. and P. Wolfe. 1960.
Decomposition principle for linear
programs. Operations Research, 8:101?111.
Falk, Yehuda N. 2001. Lexical-Functional
Grammar: An Introduction to Parallel
Constraint-Based Syntax. CSLI Publications,
Stanford, CA.
Glinert, Lewis. 1989. The Grammar of Modern
Hebrew. Cambridge University Press.
Goldberg, Yoav, Meni Adler, and Michael
Elhadad. 2008. EM can find pretty good
HMM POS-taggers (when given a good
start). In Proceedings of ACL-08: HLT,
pages 746?754, Columbus, OH.
Goldberg, Yoav and Michael Elhadad.
2011. Joint Hebrew segmentation and
parsing using a PCFGLA lattice parser. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies,
pages 704?709, Portland, OR.
Goldberg, Yoav and Reut Tsarfaty. 2008.
A single generative model for joint
morphological segmentation and syntactic
parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, OH.
Goldberg, Yoav, Reut Tsarfaty, Meni Adler,
and Michael Elhadad. 2009. Enhancing
unlexicalized parsing performance using
a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical
probabilities. In Proceedings of the 12th
Conference of the European Chapter of the
ACL (EACL 2009), pages 327?335,
Athens.
Green, Spence and Christopher D. Manning.
2010. Better Arabic parsing: Baselines,
evaluations, and analysis. In Proceedings
of the 23rd International Conference on
Computational Linguistics (Coling 2010),
pages 394?402, Beijing.
Guthmann, Noemie, Yuval Krymolowski,
Adi Milea, and Yoad Winter. 2009.
Automatic annotation of morpho-syntactic
dependencies in a Modern Hebrew
Treebank. In Proceedings of the 1st Workshop
on Treebanks and Linguistic Theories (TLT),
pages 1?12, Groningen.
Hall, Keith. 2005. Best-first Word-lattice
Parsing: Techniques for Integrated Syntactic
Language Modeling. Ph.D. thesis, Brown
University.
Huang, Zhongqiang and Mary Harper.
2009. Self-training PCFG grammars with
latent annotations across languages.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 832?841, Singapore.
Itai, Alon and Shuly Wintner. 2008. Language
resources for Hebrew. Language Resources
and Evaluation, 42(1):75?98.
Jiang, Wenbin, Liang Huang, and Qun Liu.
2009. Automatic adaptation of annotation
standards: Chinese word segmentation
and POS tagging?a case study. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 522?530, Suntec.
Johnson, Mark. 1998. PCFG models of
linguistic tree representations.
Computational Linguistics, 24:613?632.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo.
Lang, Bernard. 1974. Deterministic
techniques for efficient non-deterministic
parsers. In J. Loeckx, editor, Automata,
Languages and Programming, volume 14 of
159
Computational Linguistics Volume 39, Number 1
Lecture Notes in Computer Science. Springer,
Berlin Heidelberg, pages 255?269.
Lang, Bernard. 1988. Parsing incomplete
sentences. In Proceedings of COLING,
pages 365?371, Budapest.
Matsuzaki, Takuya, Yusuke Miyao, and
Jun?ichi Tsujii. 2005. Probabilistic CFG
with latent annotations. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05),
pages 75?82, Ann Arbor, MI.
Netzer, Yael, Meni Adler, David Gabay, and
Michael Elhadad. 2007. Can you tag the
modal? You should! In Proceedings of the
2007 Workshop on Computational Approaches
to Semitic Languages: Common Issues and
Resources, pages 57?64, Prague.
Petrov, Slav. 2009. Coarse-to-Fine Natural
Language Processing. Ph.D. thesis,
University of California at Berkeley.
Petrov, Slav. 2010. Products of random latent
variable grammars. In Proceedings of
NAACL, pages 19?27, Los Angeles, CA.
Petrov, Slav, Leon Barrett, Romain Thibaux,
and Dan Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 433?440, Sydney.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 404?411, Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German, pages 33?39, Columbus, OH.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven phrase structure grammar.
University of Chicago Press.
Prescher, Detlef. 2005. Inducing head-driven
PCFGs with latent heads: Refining a tree-
bank grammar for parsing. In Proceedings
of the European Conference on Machine
Learning (ECML), pages 292?304, Porto.
Rush, Alexander M, David Sontag, Michael
Collins, and Tommi Jaakkola. 2010.
On dual decomposition and linear
programming relaxations for natural
language processing. In Proceedings of
EMNLP, pages 1?11, Cambridge, MA.
Sima?an, Khalil. 1996. Computational
complexity of probabilistic disambiguation
by means of tree grammars. In Proceedings
of COLING, pages 1175?1180, Copenhagen.
Sima?an, Khalil. 1999. Learning Efficient
Disambiguation. Ph.D. thesis, ILLC
Dissertation Series, University of
Amsterdam.
Sima?an, Khalil, Alon Itai, Yoad Winter,
Alon Altman, and Noa Nativ. 2001.
Building a tree-bank of Modern Hebrew
text. Traitement Automatique des Langues,
42(2):1?32.
Smith, Noah A., David A. Smith, and
Roy W. Tromble. 2005. Context-based
morphological disambiguation with
random fields. In Proceedings of EMNLP,
pages 475?482, Vancouver.
Tsarfaty, Reut. 2006a. Integrated
morphological and syntactic
disambiguation for Modern Hebrew.
In Proceedings of the COLING/ACL 2006
Student Research Workshop, pages 49?54,
Sydney.
Tsarfaty, Reut. 2006b. The Interplay of Syntax
and Morphology in Building Parsing
Models for Modern Hebrew. In Proceedings
of ESSLI Student Session, pages 263?274,
Malaga.
Tsarfaty, Reut. 2010. Relational-Realizational
Parsing. Ph.D. thesis, ILLC Dissertation
Series, University of Amsterdam.
Tsarfaty, Reut and Khalil Sima?an. 2007.
Three-dimensional parametrization for
parsing morphologically rich languages.
In Proceedings of the Tenth International
Conference on Parsing Technologies,
pages 156?167, Prague.
Tsarfaty, Reut and Khalil Sima?an. 2008.
Relational-realizational parsing. In
Proceedings of CoLING, pages 889?896,
Manchester.
Tsarfaty, Reut and Khalil Sima?an. 2010.
Modeling morphosyntactic agreement
in constituency-based parsing of
Modern Hebrew. In Proceedings of the
NAACL HLT 2010 First Workshop on
Statistical Parsing of Morphologically-Rich
Languages, pages 40?48, Los Angeles, CA.
Tsarfaty, Reut, Khalil Sima?an, and Remko
Scha. 2009. An alternative to head-driven
approaches for parsing a (relatively) free
word-order language. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 842?851, Singapore.
160
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742?750,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We present a novel deterministic dependency pars-
ing algorithm that attempts to create the easiest arcs
in the dependency structure first in a non-directional
manner. Traditional deterministic parsing algorithms
are based on a shift-reduce framework: they traverse
the sentence from left-to-right and, at each step, per-
form one of a possible set of actions, until a complete
tree is built. A drawback of this approach is that
it is extremely local: while decisions can be based
on complex structures on the left, they can look only
at a few words to the right. In contrast, our algo-
rithm builds a dependency tree by iteratively select-
ing the best pair of neighbours to connect at each
parsing step. This allows incorporation of features
from already built structures both to the left and to the
right of the attachment point. The parser learns both
the attachment preferences and the order in which
they should be performed. The result is a determin-
istic, best-first, O(nlogn) parser, which is signifi-
cantly more accurate than best-first transition based
parsers, and nears the performance of globally opti-
mized parsing models.
1 Introduction
Dependency parsing has been a topic of active re-
search in natural language processing in the last sev-
eral years. An important part of this research effort
are the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al, 2007), which al-
lowed for a comparison of many algorithms and ap-
proaches for this task on many languages.
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
Current dependency parsers can be categorized
into three families: local-and-greedy transition-
based parsers (e.g., MALTPARSER (Nivre et al,
2006)), globally optimized graph-based parsers
(e.g., MSTPARSER (McDonald et al, 2005)), and
hybrid systems (e.g., (Sagae and Lavie, 2006b;
Nivre and McDonald, 2008)), which combine the
output of various parsers into a new and improved
parse, and which are orthogonal to our approach.
Transition-based parsers scan the input from left
to right, are fast (O(n)), and can make use of rich
feature sets, which are based on all the previously
derived structures. However, all of their decisions
are very local, and the strict left-to-right order im-
plies that, while the feature set can use rich struc-
tural information from the left of the current attach-
ment point, it is also very restricted in information
to the right of the attachment point: traditionally,
only the next two or three input tokens are avail-
able to the parser. This limited look-ahead window
leads to error propagation and worse performance on
root and long distant dependencies relative to graph-
based parsers (McDonald and Nivre, 2007).
Graph-based parsers, on the other hand, are glob-
ally optimized. They perform an exhaustive search
over all possible parse trees for a sentence, and find
the highest scoring tree. In order to make the search
tractable, the feature set needs to be restricted to fea-
tures over single edges (first-order models) or edges
pairs (higher-order models, e.g. (McDonald and
Pereira, 2006; Carreras, 2007)). There are several
attempts at incorporating arbitrary tree-based fea-
tures but these involve either solving an ILP prob-
lem (Riedel and Clarke, 2006) or using computa-
742
(1) ATTACHRIGHT(2)
a brown fox jumped with joy
-157
-27
-68
403
-197
-47
-152
-243
231
3
(2) ATTACHRIGHT(1)
a fox
brown
jumped with joy
-52
314
-159
0
-176
-146
246
12
(3) ATTACHRIGHT(1)
fox
a brown
jumped with joy
-133
270
-149
-154
246
10
(4) ATTACHLEFT(2)
jumped
fox
a brown
with joy
-161
-435
186
-2
(5) ATTACHLEFT(1)
jumped
fox
a brown
with
joy
430
-232
(6)
jumped
fox
a brown
with
joy
Figure 1: Parsing the sentence ?a brown fox jumped with joy?. Rounded arcs represent possible actions.
tionally intensive sampling-based methods (Naka-
gawa, 2007). As a result, these models, while accu-
rate, are slow (O(n3) for projective, first-order mod-
els, higher polynomials for higher-order models, and
worse for richer tree-feature models).
We propose a new category of dependency pars-
ing algorithms, inspired by (Shen et al, 2007): non-
directional easy-first parsing. This is a greedy, de-
terministic parsing approach, which relaxes the left-
to-right processing order of transition-based pars-
ing algorithms. By doing so, we allow the ex-
plicit incorporation of rich structural features de-
rived from both sides of the attachment point, and
implicitly take into account the entire previously de-
rived structure of the whole sentence. This exten-
sion allows the incorporation of much richer features
than those available to transition- and especially to
graph-based parsers, and greatly reduces the local-
ity of transition-based algorithm decisions. On the
other hand, it is still a greedy, best-first algorithm
leading to an efficient implementation.
We present a concrete O(nlogn) parsing algo-
rithm, which significantly outperforms state-of-the-
art transition-based parsers, while closing the gap to
graph-based parsers.
2 Easy-first parsing
When humans comprehend a natural language sen-
tence, they arguably do it in an incremental, left-to-
right manner. However, when humans consciously
annotate a sentence with syntactic structure, they
hardly ever work in fixed left-to-right order. Rather,
they start by building several isolated constituents
by making easy and local attachment decisions and
only then combine these constituents into bigger
constituents, jumping back-and-forth over the sen-
tence and proceeding from easy to harder phenom-
ena to analyze. When getting to the harder decisions
a lot of structure is already in place, and this struc-
ture can be used in deciding a correct attachment.
Our parser follows a similar kind of annotation
process: starting from easy attachment decisions,
and proceeding to harder and harder ones. When
making later decisions, the parser has access to the
entire structure built in earlier stages. During the
training process, the parser learns its own notion of
easy and hard, and learns to defer specific kinds of
decisions until more structure is available.
3 Parsing algorithm
Our (projective) parsing algorithm builds the parse
tree bottom up, using two kinds of actions: AT-
TACHLEFT(i) and ATTACHRIGHT(i) . These
actions are applied to a list of partial structures
p1, . . . , pk, called pending, which is initialized with
the n words of the sentence w1, . . . , wn. Each ac-
743
tion connects the heads of two neighbouring struc-
tures, making one of them the parent of the other,
and removing the daughter from the list of partial
structures. ATTACHLEFT(i) adds a dependency
edge (pi, pi+1) and removes pi+1 from the list. AT-
TACHRIGHT(i) adds a dependency edge (pi+1, pi)
and removes pi from the list. Each action shortens
the list of partial structures by 1, and after n?1 such
actions, the list contains the root of a connected pro-
jective tree over the sentence.
Figure 1 shows an example of parsing the sen-
tence ?a brown fox jumped with joy?. The pseu-
docode of the algorithm is given in Algorithm 1.
Algorithm 1: Non-directional Parsing
Input: a sentence= w1 . . . wn
Output: a set of dependency arcs over the
sentence (Arcs)
Acts = {ATTACHLEFT, ATTACHRIGHT}1
Arcs? {}2
pending = p1 . . . pn ? w1 . . . wn3
while length(pending) > 1 do4
best? arg max
act?Acts
1?i?len(pending)
score(act(i))
5
(parent, child)? edgeFor(best)6
Arcs.add( (parent, child) )7
pending.remove(child)8
end9
return Arcs10
edgeFor(act(i)) =
{
(pi, pi+1) ATTACHLEFT(i)
(pi+1, pi) ATTACHRIGHT(i)
At each step the algorithm chooses a spe-
cific action/location pair using a function
score(ACTION(i)), which assign scores to ac-
tion/location pairs based on the partially built
structures headed by pi and pi+1, as well as neigh-
bouring structures. The score() function is learned
from data. This scoring function reflects not only
the correctness of an attachment, but also the order
in which attachments should be made. For example,
consider the attachments (brown,fox) and (joy,with)
in Figure (1.1). While both are correct, the scoring
function prefers the (adjective,noun) attachment
over the (prep,noun) attachment. Moreover, the
attachment (jumped,with), while correct, receives
a negative score for the bare preposition ?with?
(Fig. (1.1) - (1.4) ), and a high score once the verb
has its subject and the PP ?with joy? is built (Fig.
(1.5) ). Ideally, we would like to score easy and
reliable attachments higher than harder less likely
attachments, thus performing attachments in order
of confidence. This strategy allows us both to limit
the extent of error propagation, and to make use of
richer contextual information in the later, harder
attachments. Unfortunately, this kind of ordering
information is not directly encoded in the data. We
must, therefore, learn how to order the decisions.
We first describe the learning algorithm (Section
4) and a feature representation (Section 5) which en-
ables us to learn an effective scoring function.
4 Learning Algorithm
We use a linear model score(x) = ~w ? ?(x), where
?(x) is a feature representation and ~w is a weight
vector. We write ?act(i) to denote the feature repre-
sentation extracted for action act at location i. The
model is trained using a variant of the structured per-
ceptron (Collins, 2002), similar to the algorithm of
(Shen et al, 2007; Shen and Joshi, 2008). As usual,
we use parameter averaging to prevent the percep-
tron from overfitting.
The training algorithm is initialized with a zero
parameter vector ~w. The algorithm makes several
passes over the data. At each pass, we apply the
training procedure given in Algorithm 2 to every
sentence in the training set.
At training time, each sentence is parsed using the
parsing algorithm and the current ~w. Whenever an
invalid action is chosen by the parsing algorithm, it
is not performed (line 6). Instead, we update the pa-
rameter vector ~w by decreasing the weights of the
features associated with the invalid action, and in-
creasing the weights for the currently highest scor-
ing valid action.1 We then proceed to parse the sen-
tence with the updated values. The process repeats
until a valid action is chosen.
Note that each single update does not guarantee
that the next chosen action is valid, or even different
than the previously selected action. Yet, this is still
an aggressive update procedure: we do not leave a
sentence until our parameters vector parses it cor-
1We considered 3 variants of this scheme: (1) using the high-
est scoring valid action, (2) using the leftmost valid action, and
(3) using a random valid action. The 3 variants achieved nearly
identical accuracy, while (1) converged somewhat faster than
the other two.
744
rectly, and we do not proceed from one partial parse
to the next until ~w predicts a correct location/action
pair. However, as the best ordering, and hence the
best attachment point is not known to us, we do not
perform a single aggressive update step. Instead, our
aggressive update is performed incrementally in a
series of smaller steps, each pushing ~w away from
invalid attachments and toward valid ones. This way
we integrate the search of confident attachments into
the learning process.
Algorithm 2: Structured perceptron training
for direction-less parser, over one sentence.
Input: sentence,gold arcs,current ~w,feature
representation ?
Output: weight vector ~w
Arcs? {}1
pending ? sent2
while length(pending) > 1 do3
allowed? {act(i)|isV alid(act(i), Gold,Arcs)}4
choice? arg max
act?Acts
1?i?len(pending)
~w ? ?act(i)
5
if choice ? allowed then6
(parent, child)? edgeFor(choice)7
Arcs.add( (parent, child) )8
pending.remove(child)9
else10
good? arg max
act(j)?allowed
~w ? ?act(j)
11
~w ? ~w + ?good ? ?choice12
end13
return ~w14
Function isValid(action,Gold,Arcs)
(p, c)? edgeFor(action)1
if (?c? : (c, c?) ? Gold ? (c, c?) 6? Arcs)2
? (p, c) 6? Gold then
return false3
return true4
The function isV alid(act(i), gold, arcs) (line 4)
is used to decide if the chosen action/location pair
is valid. It returns True if two conditions apply: (a)
(pi, pj) is present in gold, (b) all edges (2, pj) in
gold are also in arcs. In words, the function verifies
that the proposed edge is indeed present in the gold
parse and that the suggested daughter already found
all its own daughters.2
2This is in line with the Arc-Standard parsing strategy of
shift-reduce dependency parsers (Nivre, 2004). We are cur-
rently experimenting also with an Arc-Eager variant of the non-
5 Feature Representation
The feature representation for an action can take
into account the original sentence, as well as
the entire parse history: ?act(i) above is actually
?(act(i), sentence,Arcs, pending).
We use binary valued features, and each feature is
conjoined with the type of action.
When designing the feature representation, we
keep in mind that our features should not only di-
rect the parser toward desired actions and away from
undesired actions, but also provide the parser with
means of choosing between several desired actions.
We want the parser to be able to defer some desired
actions until more structure is available and a more
informed prediction can be made. This desire is re-
flected in our choice of features: some of our fea-
tures are designed to signal to the parser the pres-
ence of possibly ?incomplete? structures, such as an
incomplete phrase, a coordinator without conjuncts,
and so on.
When considering an action ACTION(i), we limit
ourselves to features of partial structures around the
attachment point: pi?2, pi?1, pi, pi+1, pi+2, pi+3,
that is the two structures which are to be attached by
the action (pi and pi+1), and the two neighbouring
structures on each side3.
While these features encode local context, it is lo-
cal in terms of syntactic structure, and not purely in
terms of sentence surface form. This let us capture
some, though not all, long-distance relations.
For a partial structure p, we use wp to refer to
the head word form, tp to the head word POS tag,
and lcp and rcp to the POS tags of the left-most and
right-most child of p respectively.
All our prepositions (IN) and coordinators (CC)
are lexicalized: for them, tp is in fact wptp.
We define structural, unigram, bigram and pp-
attachment features.
The structural features are: the length of the
structures (lenp), whether the structure is a word
(contains no children: ncp), and the surface distance
between structure heads (?pipj ). The unigram and
bigram features are adapted from the feature set for
left-to-right Arc-Standard dependency parsing de-
directional algorithm.
3Our sentences are padded from each side with sentence de-
limiter tokens.
745
Structural
for p in pi?2, pi?1, pi, pi+1, pi+2, pi+3 lenp , ncp
for p,q in (pi?2, pi?1),(pi?1, pi),(pi, pi+1),(pi+1, pi+ 2),(pi+2, pi+3) ?qp , ?qptptq
Unigram
for p in pi?2, pi?1, pi, pi+1, pi+2, pi+3 tp , wp , tplcp , tprcp , tprcplcp
Bigram
for p,q in (pi, pi+1),(pi, pi+2),(pi?1, pi),(pi?1, pi+2),(pi+1, pi+2) tptq , wpwq , tpwq , wptq
tptqlcplcq , tptqrcplcq
tptqlcprcq , tptqrcprcq
PP-Attachment
if pi is a preposition wpi?1wpircpi , tpi?1wpircwpi
if pi+1 is a preposition wpi?1wpi+1rcpi+1 , tpi?1wpi+1rcwpi+1
wpiwpi+1rcpi+1 , tpiwpi+1rcwpi+1
if pi+2 is a preposition wpi+1wpi+2rcpi+2 , tpi+1wpi+2rcwpi+2
wpiwpi+2rcpi+2 , tpiwpi+2rcwpi+2
Figure 2: Feature Templates
scribed in (Huang et al, 2009). We extended that
feature set to include the structure on both sides of
the proposed attachment point.
In the case of unigram features, we added features
that specify the POS of a word and its left-most and
right-most children. These features provide the non-
directional model with means to prefer some attach-
ment points over others based on the types of struc-
tures already built. In English, the left- and right-
most POS-tags are good indicators of constituency.
The pp-attachment features are similar to the bi-
gram features, but fire only when one of the struc-
tures is headed by a preposition (IN). These features
are more lexicalized than the regular bigram fea-
tures, and include also the word-form of the right-
most child of the PP (rcwp). This should help the
model learn lexicalized attachment preferences such
as (hit, with-bat).
Figure 2 enumerate the feature templates we use.
6 Computational Complexity and Efficient
Implementation
The parsing algorithm (Algorithm 1) begins with
n+1 disjoint structures (the words of the sentence +
ROOT symbol), and terminates with one connected
structure. Each iteration of the main loop connects
two structures and removes one of them, and so the
loop repeats for exactly n times.
The argmax in line 5 selects the maximal scoring
action/location pair. At iteration i, there are n ? i
locations to choose from, and a naive computation of
the argmax isO(n), resulting in anO(n2) algorithm.
Each performed action changes the partial struc-
tures and with it the extracted features and the com-
puted scores. However, these changes are limited
to a fixed local context around the attachment point
of the action. Thus, we observe that the feature ex-
traction and score calculation can be performed once
for each action/location pair in a given sentence, and
reused throughout all the iterations. After each iter-
ation we need to update the extracted features and
calculated scores for only k locations, where k is a
fixed number depending on the window size used in
the feature extraction, and usually k  n.
Using this technique, we perform only (k + 1)n
feature extractions and score calculations for each
sentence, that is O(n) feature-extraction operations
per sentence.
Given the scores for each location, the argmax can
then be computed in O(logn) time using a heap,
resulting in an O(nlogn) algorithm: n iterations,
where the first iteration involves n feature extrac-
tion operations and n heap insertions, and each sub-
sequent iteration involves k feature extractions and
heap updates.
We note that the dominating factor in polynomial-
time discriminative parsers, is by far the feature-
extraction and score calculation. It makes sense to
compare parser complexity in terms of these opera-
tions only.4 Table 1 compares the complexity of our
4Indeed, in our implementation we do not use a heap, and
opt instead to find the argmax using a simple O(n) max oper-
ation. This O(n2) algorithm is faster in practice than the heap
based one, as both are dominated by the O(n) feature extrac-
tion, while the cost of the O(n) max calculationis negligible
compared to the constants involved in heap maintenance.
746
parser to other dependency parsing frameworks.
Parser Runtime Features / Scoring
MALT O(n) O(n)
MST O(n3) O(n2)
MST2 O(n3) O(n3)
BEAM O(n ? beam) O(n ? beam)
NONDIR (This Work) O(nlogn) O(n)
Table 1: Complexity of different parsing frameworks.
MST: first order MST parser, MST2: second order MST
parser, MALT: shift-reduce left-to-right parsing. BEAM:
beam search parser, as in (Zhang and Clark, 2008)
In terms of feature extraction and score calcula-
tion operations, our algorithm has the same cost as
traditional shift-reduce (MALT) parsers, and is an
order of magnitude more efficient than graph-based
(MST) parsers. Beam-search decoding for left-to-
right parsers (Zhang and Clark, 2008) is also linear,
but has an additional linear dependence on the beam-
size. The reported results in (Zhang and Clark,
2008) use a beam size of 64, compared to our con-
stant of k = 6.
Our Python-based implementation5 (the percep-
tron is implemented in a C extension module) parses
about 40 tagged sentences per second on an Intel
based MacBook laptop.
7 Experiments and Results
We evaluate the parser using the WSJ Treebank. The
trees were converted to dependency structures with
the Penn2Malt conversion program,6 using the head-
finding rules from (Yamada and Matsumoto, 2003).7
We use Sections 2-21 for training, Section 22 for
development, and Section 23 as the final test set.
The text is automatically POS tagged using a trigram
HMM based POS tagger prior to training and pars-
ing. Each section is tagged after training the tagger
on all other sections. The tagging accuracy of the
tagger is 96.5 for the training set and 96.8 for the
test set. While better taggers exist, we believe that
the simpler HMM tagger overfits less, and is more
5http://www.cs.bgu.ac.il/?yoavg/software/
6http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
7While other and better conversions exist (see, e.g., (Johans-
son and Nugues, 2007; Sangati and Mazza, 2009)), this con-
version heuristic is still the most widely used. Using the same
conversion facilitates comparison with previous works.
representative of the tagging performance on non-
WSJ corpus texts.
Parsers We evaluate our parser against the
transition-based MALT parser and the graph-based
MST parser. We use version 1.2 of MALT parser8,
with the settings used for parsing English in the
CoNLL 2007 shared task. For the MST parser9,
we use the default first-order, projective parser set-
tings, which provide state-of-the-art results for En-
glish. All parsers are trained and tested on the same
data. Our parser is trained for 20 iterations.
Evaluation Measures We evaluate the parsers using
three common measures:
(unlabeled) Accuracy: percentage of tokens which
got assigned their correct parent.
Root: The percentage of sentences in which the
ROOT attachment is correct.
Complete: the percentage of sentences in which all
tokens were assigned their correct parent.
Unlike most previous work on English dependency
parsing, we do not exclude punctuation marks from
the evaluation.
Results are presented in Table 2. Our non-
directional easy-first parser significantly outper-
forms the left-to-right greedy MALT parser in terms
of accuracy and root prediction, and significantly
outperforms both parsers in terms of exact match.
The globally optimized MST parser is better in root-
prediction, and slightly better in terms of accuracy.
We evaluated the parsers also on the English
dataset from the CoNLL 2007 shared task. While
this dataset is also derived from the WSJ Treebank, it
differs from the previous dataset in two important as-
pects: it is much smaller in size, and it is created us-
ing a different conversion procedure, which is more
linguistically adequate. For these experiments, we
use the dataset POS tags, and the same parameters as
in the previous set of experiments: we train the non-
directional parser for 20 iterations, with the same
feature set. The CoNLL dataset contains some non-
projective constructions. MALT and MST deal with
non-projectivity. For the non-directional parser, we
projectivize the training set prior to training using
the procedure described in (Carreras, 2007).
Results are presented in Table 3.
8http://maltparser.org/dist/1.2/malt-1.2.tar.gz
9http://sourceforge.net/projects/mstparser/
747
Parser Accuracy Root Complete
MALT 88.36 87.04 34.14
MST 90.05 93.95 34.64
NONDIR (this work) 89.70 91.50 37.50
Table 2: Unlabeled dependency accuracy on PTB Section
23, automatic POS-tags, including punctuation.
Parser Accuracy Root Complete
MALT 85.82 87.85 24.76
MST 89.08 93.45 24.76
NONDIR (this work) 88.34 91.12 29.43
Table 3: Unlabeled dependency accuracy on CoNLL
2007 English test set, including punctuation.
While all models suffer from the move to the
smaller dataset and the more challenging annotation
scheme, the overall story remains the same: the non-
directional parser is better than MALT but not as
good as MST in terms of parent-accuracy and root
prediction, and is better than both MALT and MST
in terms of producing complete correct parses.
That the non-directional parser has lower accu-
racy but more exact matches than the MST parser
can be explained by it being a deterministic parser,
and hence still vulnerable to error propagation: once
it erred once, it is likely to do so again, result-
ing in low accuracies for some sentences. How-
ever, due to the easy-first policy, it manages to parse
many sentences without a single error, which lead
to higher exact-match scores. The non-directional
parser avoids error propagation by not making the
initial error. On average, the non-directional parser
manages to assign correct heads to over 60% of the
tokens before making its first error.
The MST parser would have ranked 5th in the
shared task, and NONDIR would have ranked 7th.
The better ranking systems in the shared task
are either higher-order global models, beam-search
based systems, or ensemble-based systems, all of
which are more complex and less efficient than the
NONDIR parser.
Parse Diversity The parses produced by the non-
directional parser are different than the parses pro-
duced by the graph-based and left-to-right parsers.
To demonstrate this difference, we performed an Or-
acle experiment, in which we combine the output of
several parsers by choosing, for each sentence, the
parse with the highest score. Results are presented
Combination Accuracy Complete
Penn2Malt, Train 2-21, Test 23
MALT+MST 92.29 44.03
NONDIR+MALT 92.19 45.48
NONDIR+MST 92.53 44.41
NONDIR+MST+MALT 93.54 49.79
CoNLL 2007
MALT+MST 91.50 33.64
NONDIR+MALT 91.02 34.11
NONDIR+MST 91.90 34.11
NONDIR+MST+MALT 92.70 38.31
Table 4: Parser combination with Oracle, choosing the
highest scoring parse for each sentence of the test-set.
in Table 4.
A non-oracle blending of MALT+MST+NONDIR
using Sagae and Lavie?s (2006) simplest combina-
tion method assigning each component the same
weight, yield an accuracy of 90.8 on the CoNLL
2007 English dataset, making it the highest scoring
system among the participants.
7.1 Error Analysis / Limitations
When we investigate the POS category of mistaken
instances, we see that for all parsers, nodes with
structures of depth 2 and more which are assigned
an incorrect head are predominantly PPs (headed
by ?IN?), followed by NPs (headed by ?NN?). All
parsers have a hard time dealing with PP attachment,
but MST parser is better at it than NONDIR, and both
are better than MALT.
Looking further at the mistaken instances, we no-
tice a tendency of the PP mistakes of the NONDIR
parser to involve, before the PP, an NP embedded
in a relative clause. This reveals a limitation of our
parser: recall that for an edge to be built, the child
must first acquire all its own children. This means
that in case of relative clauses such as ?I saw the
boy [who ate the pizza] with my eyes?, the parser
must decide if the PP ?with my eyes? should be at-
tached to ?the pizza? or not before it is allowed to
build parts of the outer NP (?the boy who. . . ?). In
this case, the verb ?saw? and the noun ?boy? are
both outside of the sight of the parser when decid-
ing on the PP attachment, and it is forced to make a
decision in ignorance, which, in many cases, leads
to mistakes. The globally optimized MST does not
suffer as much from such cases. We plan to address
this deficiency in future work.
748
8 Related Work
Deterministic shift-reduce parsers are restricted by a
strict left-to-right processing order. Such parsers can
rely on rich syntactic information on the left, but not
on the right, of the decision point. They are forced
to commit early, and suffer from error propagation.
Our non-directional parser addresses these deficien-
cies by discarding the strict left-to-right processing
order, and attempting to make easier decisions be-
fore harder ones. Other methods of dealing with
these deficiencies were proposed over the years:
Several Passes Yamada and Matsumoto?s (2003)
pioneering work introduces a shift-reduce parser
which makes several left-to-right passes over a sen-
tence. Each pass adds structure, which can then be
used in subsequent passes. Sagae and Lavie (2006b)
extend this model to alternate between left-to-right
and right-to-left passes. This model is similar to
ours, in that it attempts to defer harder decisions to
later passes over the sentence, and allows late deci-
sions to make use of rich syntactic information (built
in earlier passes) on both sides of the decision point.
However, the model is not explicitly trained to op-
timize attachment ordering, has an O(n2) runtime
complexity, and produces results which are inferior
to current single-pass shift-reduce parsers.
Beam Search Several researchers dealt with the
early-commitment and error propagation of deter-
ministic parsers by extending the greedy decisions
with various flavors of beam-search (Sagae and
Lavie, 2006a; Zhang and Clark, 2008; Titov and
Henderson, 2007). This approach works well and
produces highly competitive results. Beam search
can be incorporated into our parser as well. We leave
this investigation to future work.
Strict left-to-right ordering is also prevalent in se-
quence tagging. Indeed, one major influence on
our work is Shen et.al.?s bi-directional POS-tagging
algorithm (Shen et al, 2007), which combines a
perceptron learning procedure similar to our own
with beam search to produce a state-of-the-art POS-
tagger, which does not rely on left-to-right process-
ing. Shen and Joshi (2008) extends the bidirectional
tagging algorithm to LTAG parsing, with good re-
sults. We build on top of that work and present a
concrete and efficient greedy non-directional depen-
dency parsing algorithm.
Structure Restrictions Eisner and Smith (2005)
propose to improve the efficiency of a globally op-
timized parser by posing hard constraints on the
lengths of arcs it can produce. Such constraints
pose an explicit upper bound on parser accuracy.10
Our parsing model does not pose such restrictions.
Shorter edges are arguably easier to predict, and our
parses builds them early in time. However, it is
also capable of producing long dependencies at later
stages in the parsing process. Indeed, the distribu-
tion of arc lengths produced by our parser is similar
to those produced by the MALT and MST parsers.
9 Discussion
We presented a non-directional deterministic depen-
dency parsing algorithm, which is not restricted by
the left-to-right parsing order of other deterministic
parsers. Instead, it works in an easy-first order. This
strategy allows using more context at each decision.
The parser learns both what and when to connect.
We show that this parsing algorithm significantly
outperforms a left-to-right deterministic algorithm.
While it still lags behind globally optimized pars-
ing algorithms in terms of accuracy and root pre-
diction, it is much better in terms of exact match,
and much faster. As our parsing framework can eas-
ily and efficiently utilize more structural information
than globally optimized parsers, we believe that with
some enhancements and better features, it can out-
perform globally optimized algorithms, especially
when more structural information is needed, such as
for morphologically rich languages.
Moreover, we show that our parser produces
different structures than those produced by both
left-to-right and globally optimized parsers, mak-
ing it a good candidate for inclusion in an ensem-
ble system. Indeed, a simple combination scheme
of graph-based, left-to-right and non-directional
parsers yields state-of-the-art results on English de-
pendency parsing on the CoNLL 2007 dataset.
We hope that further work on this non-directional
parsing framework will pave the way to better under-
standing of an interesting cognitive question: which
kinds of parsing decisions are hard to make, and
which linguistic constructs are hard to analyze?
10In (Dreyer et al, 2006), constraints are chosen ?to be the
minimum value that will allow recovery of 90% of the left
(right) dependencies in the training corpus?.
749
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL
Shared Task, EMNLP-CoNLL.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc of EMNLP.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In Proc. of CoNLL, pages 201?
205, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Jason Eisner and Noah A. Smith. 2005. arsing with soft
and hard constraints on dependency length. In Proc.
of IWPT.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc of EMNLP.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc of NODALIDA.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proc. of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc of ACL.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of EMNLP-CoNLL.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL, pages 950?958, Columbus, Ohio,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mcdon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of EMNLP-CoNLL.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Incremental Parsing: Bringing
Engineering and Cognition Together, ACL-Workshop.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proc. of EMNLP 2006, July.
Kenji Sagae and Alon Lavie. 2006a. A best-first proba-
bilistic shift-reduce parser. In Proc of ACL.
Kenji Sagae and Alon Lavie. 2006b. Parser combination
by reparsing. In Proc of NAACL.
Federico Sangati and Chiara Mazza. 2009. An english
dependency treebank a` la tesnie`re. In Proc of TLT8.
Libin Shen and Aravind K. Joshi. 2008. Ltag depen-
dency parsing with bidirectional incremental construc-
tion. In Proc of EMNLP.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc of ACL.
Ivan Titov and James Henderson. 2007. Fast and robust
multilingual dependency parsing with a generative la-
tent variable model. In Proc. of EMNLP-CoNLL.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of
IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proc of EMNLP.
750
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704?709,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Hebrew Segmentation and Parsing
using a PCFG-LA Lattice Parser
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We experiment with extending a lattice pars-
ing methodology for parsing Hebrew (Gold-
berg and Tsarfaty, 2008; Golderg et al, 2009)
to make use of a stronger syntactic model: the
PCFG-LA Berkeley Parser. We show that the
methodology is very effective: using a small
training set of about 5500 trees, we construct
a parser which parses and segments unseg-
mented Hebrew text with an F-score of almost
80%, an error reduction of over 20% over the
best previous result for this task. This result
indicates that lattice parsing with the Berkeley
parser is an effective methodology for parsing
over uncertain inputs.
1 Introduction
Most work on parsing assumes that the lexical items
in the yield of a parse tree are fully observed, and
correspond to space delimited tokens, perhaps af-
ter a deterministic preprocessing step of tokeniza-
tion. While this is mostly the case for English, the
situation is different in languages such as Chinese,
in which word boundaries are not marked, and the
Semitic languages of Hebrew and Arabic, in which
various particles corresponding to function words
are agglutinated as affixes to content bearing words,
sharing the same space-delimited token. For exam-
ple, the Hebrew token bcl1 can be interpreted as
the single noun meaning ?onion?, or as a sequence
of a preposition and a noun b-cl meaning ?in (the)
shadow?. In such languages, the sequence of lexical
1We adopt here the transliteration scheme of (Sima?an et al,
2001)
items corresponding to an input string is ambiguous,
and cannot be determined using a deterministic pro-
cedure. In this work, we focus on constituency pars-
ing of Modern Hebrew (henceforth Hebrew) from
raw unsegmented text.
A common method of approaching the discrep-
ancy between input strings and space delimited to-
kens is using a pipeline process, in which the in-
put string is pre-segmented prior to handing it to a
parser. The shortcoming of this method, as noted
by (Tsarfaty, 2006), is that many segmentation de-
cisions cannot be resolved based on local context
alone. Rather, they may depend on long distance re-
lations and interact closely with the syntactic struc-
ture of the sentence. Thus, segmentation deci-
sions should be integrated into the parsing process
and not performed as an independent preprocess-
ing step. Goldberg and Tsarfaty (2008) demon-
strated the effectiveness of lattice parsing for jointly
performing segmentation and parsing of Hebrew
text. They experimented with various manual re-
finements of unlexicalized, treebank-derived gram-
mars, and showed that better grammars contribute
to better segmentation accuracies. Goldberg et al
(2009) showed that segmentation and parsing ac-
curacies can be further improved by extending the
lexical coverage of a lattice-parser using an exter-
nal resource. Recently, Green and Manning (2010)
demonstrated the effectiveness of lattice-parsing for
parsing Arabic.
Here, we report the results of experiments cou-
pling lattice parsing together with the currently best
grammar learning method: the Berkeley PCFG-LA
parser (Petrov et al, 2006).
704
2 Aspects of Modern Hebrew
Some aspects that make Hebrew challenging from a
language-processing perspective are:
Affixation Common function words are prefixed
to the following word. These include: m(?from?)
f (?who?/?that?) h(?the?) w(?and?) k(?like?) l(?to?)
and b(?in?). Several such elements may attach to-
gether, producing forms such as wfmhfmf (w-f-m-h-
fmf ?and-that-from-the-sun?). Notice that the last
part of the token, the noun fmf (?sun?), when ap-
pearing in isolation, can be also interpreted as the
sequence f-mf (?who moved?). The linear order
of such segmental elements within a token is fixed
(disallowing the reading w-f-m-h-f-mf in the previ-
ous example). However, the syntactic relations of
these elements with respect to the rest of the sen-
tence is rather free. The relativizer f (?that?) for
example may attach to an arbitrarily long relative
clause that goes beyond token boundaries. To fur-
ther complicate matters, the definite article h(?the?)
is not realized in writing when following the par-
ticles b(?in?),k(?like?) and l(?to?). Thus, the form
bbit can be interpreted as either b-bit (?in house?) or
b-h-bit (?in the house?). In addition, pronominal el-
ements may attach to nouns, verbs, adverbs, preposi-
tions and others as suffixes (e.g. lqxn(lqx-hn, ?took-
them?), elihm(eli-hm,?on them?)). These affixations
result in highly ambiguous token segmentations.
Relatively free constituent order The ordering of
constituents inside a phrase is relatively free. This
is most notably apparent in the verbal phrases and
sentential levels. In particular, while most sentences
follow an SVO order, OVS and VSO configurations
are also possible. Verbal arguments can appear be-
fore or after the verb, and in many ordering. This
results in long and flat VP and S structures and a fair
amount of sparsity.
Rich templatic morphology Hebrew has a very
productive morphological structure, which is based
on a root+template system. The productive mor-
phology results in many distinct word forms and a
high out-of-vocabulary rate which makes it hard to
reliably estimate lexical parameters from annotated
corpora. The root+template system (combined with
the unvocalized writing system and rich affixation)
makes it hard to guess the morphological analyses
of an unknown word based on its prefix and suffix,
as usually done in other languages.
Unvocalized writing system Most vowels are not
marked in everyday Hebrew text, which results in a
very high level of lexical and morphological ambi-
guity. Some tokens can admit as many as 15 distinct
readings.
Agreement Hebrew grammar forces morpholog-
ical agreement between Adjectives and Nouns
(which should agree on Gender and Number and
definiteness), and between Subjects and Verbs
(which should agree on Gender and Number).
3 PCFG-LA Grammar Estimation
Klein and Manning (2003) demonstrated that lin-
guistically informed splitting of non-terminal sym-
bols in treebank-derived grammars can result in ac-
curate grammars. Their work triggered investiga-
tions in automatic grammar refinement and state-
splitting (Matsuzaki et al, 2005; Prescher, 2005),
which was then perfected by (Petrov et al, 2006;
Petrov, 2009). The model of (Petrov et al, 2006) and
its publicly available implementation, the Berke-
ley parser2, works by starting with a bare-bones
treebank derived grammar and automatically refin-
ing it in split-merge-smooth cycles. The learning
works by iteratively (1) splitting each non-terminal
category in two, (2) merging back non-effective
splits and (3) smoothing the split non-terminals to-
ward their shared ancestor. Each of the steps is
followed by an EM-based parameter re-estimation.
This process allows learning tree annotations which
capture many latent syntactic interactions. At in-
ference time, the latent annotations are (approxi-
mately) marginalized out, resulting in the (approx-
imate) most probable unannotated tree according to
the refined grammar. This parsing methodology is
very robust, producing state of the art accuracies for
English, as well as many other languages including
German (Petrov and Klein, 2008), French (Candito
et al, 2009) and Chinese (Huang and Harper, 2009)
among others.
The grammar learning process is applied to bi-
narized parse trees, with 1st-order vertical and 0th-
order horizontal markovization. This means that in
2http://code.google.com/p/berkeleyparser/
705
Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond
to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have
analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the
pronominal suffix which is expanded to the sequence fl hm (?of them?, 2-4, 4-5).
the initial grammar, each of the non-terminal sym-
bols is effectively conditioned on its parent alone,
and is independent of its sisters. This is a very
strong independence assumption. However, it al-
lows the resulting refined grammar to encode its own
set of dependencies between a node and its sisters, as
well as ordering preferences in long, flat rules. Our
initial experiments on Hebrew confirm that moving
to higher order horizontal markovization degrades
parsing performance, while producing much larger
grammars.
4 Lattice Representation and Parsing
Following (Goldberg and Tsarfaty, 2008) we deal
with the ambiguous affixation patterns in Hebrew by
encoding the input sentence as a segmentation lat-
tice. Each token is encoded as a lattice representing
its possible analyses, and the token-lattices are then
concatenated to form the sentence-lattice. Figure 1
presents the lattice for the two token sentence ?bclm
hneim?. Each lattice arc correspond to a lexical item.
Lattice Parsing The CKY parsing algorithm can
be extended to accept a lattice as its input (Chap-
pelier et al, 1999). This works by indexing lexi-
cal items by their start and end states in the lattice
instead of by their sentence position, and changing
the initialization procedure of CKY to allow termi-
nal and preterminal sybols of spans of sizes > 1. It is
then relatively straightforward to modify the parsing
mechanism to support this change: not giving spe-
cial treatments for spans of size 1, and distinguish-
ing lexical items from non-terminals by a specified
marking instead of by their position in the chart. We
modified the PCFG-LA Berkeley parser to accept
lattice input at inference time (training is performed
as usual on fully observed treebank trees).
Lattice Construction We construct the token lat-
tices using MILA, a lexicon-based morphological
analyzer which provides a set of possible analyses
for each token (Itai and Wintner, 2008). While being
a high-coverage lexicon, its coverage is not perfect.
For the future, we consider using unknown handling
techniques such as those proposed in (Adler et al,
2008). Still, the use of the lexicon for lattice con-
struction rather than relying on forms seen in the
treebank is essential to achieve parsing accuracy.
Lexical Probabilities Estimation Lexical p(t ?
w) probabilities are defined over individual seg-
ments rather than for complete tokens. It is the role
of the syntactic model to assign probabilities to con-
texts which are larger than a single segment. We
use the default lexical probability estimation of the
Berkeley parser.3
Goldberg et al (2009) suggest to estimate lexi-
cal probabilities for rare and unseen segments using
emission probabilities of an HMM tagger trained us-
ing EM on large corpora. Our preliminary exper-
iments with this method with the Berkeley parser
3Probabilities for robust segments (lexical items observed
100 times or more in training) are based on the MLE estimates
resulting from the EM procedure. Other segments are assigned
smoothed probabilities which combine the p(w|t) MLE esti-
mate with unigram tag probabilities. Segments which were not
seen in training are assigned a probability based on a single
distribution of tags for rare words. Crucially, we restrict each
segment to appear only with tags which are licensed by a mor-
phological analyzer, as encoded in the lattice.
706
showed mixed results. Parsing performance on the
test set dropped slightly.When analyzing the parsing
results on out-of-treebank text, we observed cases
where this estimation method indeed fixed mistakes,
and others where it hurt. We are still uncertain if the
slight drop in performance over the test set is due to
overfitting of the treebank vocabulary, or the inade-
quacy of the method in general.
5 Experiments and Results
Data In all the experiments we use Ver.2 of the
Hebrew treebank (Guthmann et al, 2009), which
was converted to use the tagset of the MILA mor-
phological analyzer (Golderg et al, 2009). We use
the same splits as in previous work, with a train-
ing set of 5240 sentences (484-5724) and a test set
of 483 sentences (1-483). During development, we
evaluated on a random subset of 100 sentences from
the training set. Unless otherwise noted, we used the
basic non-terminal categories, without any extended
information available in them.
Gold Segmentation and Tagging To assess the
adequacy of the Berkeley parser for Hebrew, we per-
formed baseline experiments in which either gold
segmentation and tagging or just gold segmenta-
tion were available to the parser. The numbers are
very high: an F-measure of about 88.8% for the
gold segmentation and tagging, and about 82.8% for
gold segmentation only. This shows the adequacy
of the PCFG-LA methodology for parsing the He-
brew treebank, but also goes to show the highly am-
biguous nature of the tagging. Our baseline lattice
parsing experiment (without the lexicon) results in
an F-score of around 76%.4
Segmentation ? Parsing pipeline As another
baseline, we experimented with a pipeline system
in which the input text is automatically segmented
and tagged using a state-of-the-art HMM pos-tagger
(Goldberg et al, 2008). We then ignore the pro-
duced tagging, and pass the resulting segmented text
as input to the PCFG-LA parsing model as a deter-
ministic input (here the lattice representation is used
while tagging, but the parser sees a deterministic,
4For all the joint segmentation and parsing experiments, we
use a generalization of parseval that takes segmentation into ac-
count. See (Tsarfaty, 2006) for the exact details.
segmented input).5 In the pipeline setting, we either
allow the parser to assign all possible POS-tags, or
restrict it to POS-tags licensed by the lexicon.
Lattice Parsing Experiments Our initial lattice
parsing experiments with the Berkeley parser were
disappointing. The lattice seemed too permissive,
allowing the parser to chose weird analyses. Error
analysis suggested the parser failed to distinguish
among the various kinds of VPs: finite, non-finite
and modals. Once we annotate the treebank verbs
into finite, non-finite and modals6, results improve
a lot. Further improvement was gained by specifi-
cally marking the subject-NPs.7 The parser was not
able to correctly learn these splits on its own, but
once they were manually provided it did a very good
job utilizing this information.8 Marking object NPs
did not help on their own, and slightly degraded the
performance when both subjects and objects were
marked. It appears that the learning procedure man-
aged to learn the structure of objects without our
help. In all the experiments, the use of the morpho-
logical analyzer in producing the lattice was crucial
for parsing accuracy.
Results Our final configuration (marking verbal
forms and subject-NPs, using the analyzer to con-
struct the lattice and training the parser for 5 itera-
tions) produces remarkable parsing accuracy when
parsing from unsegmented text: an F-score of
79.9% (prec: 82.3 rec: 77.6) and seg+tagging F of
93.8%. The pipeline systems with the same gram-
mar achieve substantially lower F-scores of 75.2%
(without the lexicon) and 77.3 (with the lexicon).
For comparison, the previous best results for pars-
ing Hebrew are 84.1%F assuming gold segmenta-
tion and tagging (Tsarfaty and Sima?an, 2010)9, and
73.7%F starting from unsegmented text (Golderg et
5The segmentation+tagging accuracy of the HMM tagger on
the Treebank data is 91.3%F.
6This information is available in both the treebank and the
morphological analyzer, but we removed it at first. Note that the
verb-type distinction is specified only on the pre-terminal level,
and not on the phrase-level.
7Such markings were removed prior to evaluation.
8Candito et al (2009) also report improvements in accu-
racy when providing the PCFG-LA parser with few manually-
devised linguistically-motivated state-splits.
9The 84.1 figure is for sentences of length ? 40, and thus
not strictly comparable with all the other numbers in this paper,
which are based on the entire test-set.
707
System Oracle OOV Handling Prec Rec F1
Tsarfaty and Sima?an 2010 Gold Seg+Tag ? - - 84.1
Goldberg et al 2009 None Lexicon 73.4 74.0 73.8
Seg ? PCFG-LA Pipeline None Treebank 75.6 74.8 75.2
Seg ? PCFG-LA Pipeline None Lexicon 79.5 75.2 77.3
PCFG-LA + Lattice (Joint) None Lexicon 82.3 77.6 79.9
Table 1: Parsing scores of the various systems
al., 2009). The numbers are summarized in Table 1.
While the pipeline system already improves over the
previous best results, the lattice-based joint-model
improves results even further. Overall, the PCFG-
LA+Lattice parser improve results by 6 F-points ab-
solute, an error reduction of about 20%. Tagging
accuracies are also remarkable, and constitute state-
of-the-art tagging for Hebrew.
The strengths of the system can be attributed to
three factors: (1) performing segmentation, tagging
and parsing jointly using lattice parsing, (2) relying
on an external resource (lexicon / morphological an-
alyzer) instead of on the Treebank to provide lexical
coverage and (3) using a strong syntactic model.
Running time The lattice representation effec-
tively results in longer inputs to the parser. It is
informative to quantify the effect of the lattice rep-
resentation on the parsing time, which is cubic in
sentence length. The pipeline parser parsed the
483 pre-segmented input sentences in 151 seconds
(3.2 sentences/second) not including segmentation
time, while the lattice parser took 175 seconds (2.7
sents/second) including lattice construction. Parsing
with the lattice representation is slower than in the
pipeline setup, but not prohibitively so.
Analysis and Limitations When analyzing the
learned grammar, we see that it learned to distin-
guish short from long constituents, models conjunc-
tion parallelism fairly well, and picked up a lot
of information regarding the structure of quantities,
dates, named and other kinds of NPs. It also learned
to reasonably model definiteness, and that S ele-
ments have at most one Subject. However, the state-
split model exhibits no notion of syntactic agree-
ment on gender and number. This is troubling, as
we encountered a fair amount of parsing mistakes
which would have been solved if the parser were to
use agreement information.
6 Conclusions and Future Work
We demonstrated that the combination of lattice
parsing with the PCFG-LA Berkeley parser is highly
effective. Lattice parsing allows much needed flexi-
bility in providing input to a parser when the yield of
the tree is not known in advance, and the grammar
refinement and estimation techniques of the Berke-
ley parser provide a strong disambiguation compo-
nent. In this work, we applied the Berkeley+Lattice
parser to the challenging task of joint segmentation
and parsing of Hebrew text. The result is the first
constituency parser which can parse naturally occur-
ring unsegmented Hebrew text with an acceptable
accuracy (an F1 score of 80%).
Many other uses of lattice parsing are possible.
These include joint segmentation and parsing of
Chinese, empty element prediction (see (Cai et al,
2011) for a successful application), and a princi-
pled handling of multiword-expressions, idioms and
named-entities. The code of the lattice extension to
the Berkeley parser is publicly available.10
Despite its strong performance, we observed that
the Berkeley parser did not learn morphological
agreement patterns. Agreement information could
be very useful for disambiguating various construc-
tions in Hebrew and other morphologically rich lan-
guages. We plan to address this point in future work.
Acknowledgments
We thank Slav Petrov for making available and an-
swering questions about the code of his parser, Fed-
erico Sangati for pointing out some important details
regarding the evaluation, and the three anonymous
reviewers for their helpful comments. The work is
supported by the Lynn and William Frankel Center
for Computer Sciences, Ben-Gurion University.
10http://www.cs.bgu.ac.il/?yoavg/software/blatt/
708
References
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolu-
tion of unknown words for full morphological analy-
sis. In Proc. of ACL.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements.
In Proc. of ACL (short-paper).
Marie Candito, Benoit Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice Parsing for Speech Recognition.
In In Sixth Conference sur le Traitement Automatique
du Langage Naturel (TANL99), pages 95?104.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can find pretty good HMM POS-Taggers (when
given a good start). In Proc. of ACL.
Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proc. of EACL.
Spence Green and Christopher Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proc. of COLING.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proc. of TLT.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proc. of the EMNLP, pages 832?
841. Association for Computational Linguistics.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc of ACL.
Slav Petrov and Dan Klein. 2008. Parsing German with
latent variable grammars. In Proceedings of the ACL
Workshop on Parsing German.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL, Sydney,
Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA, USA.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. In Proc. of ECML.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank of
Modern Hebrew text. Traitement Automatique des
Langues, 42(2).
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Proc. of
ACL-SRW.
709
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 913?922,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Query-Chain Focused Summarization 
 
Tal Baumel 
Dept. of Computer Science  
Ben-Gurion University 
Beer-Sheva, Israel 
talbau@cs.bgu.ac.il 
Raphael Cohen 
Dept. of Computer Science  
Ben-Gurion University 
Beer-Sheva, Israel 
cohenrap@cs.bgu.ac.il 
Michael Elhadad 
Dept. of Computer Science  
Ben-Gurion University 
Beer-Sheva, Israel 
elhadad@cs.bgu.ac.il 
 
 
Abstract 
Update summarization is a form of multi-
document summarization where a document 
set must be summarized in the context of other 
documents assumed to be known. Efficient 
update summarization must focus on identify-
ing new information and avoiding repetition of 
known information. In Query-focused summa-
rization, the task is to produce a summary as 
an answer to a given query.  We introduce a 
new task, Query-Chain Summarization, which 
combines aspects of the two previous tasks: 
starting from a given document set, increas-
ingly specific queries are considered, and a 
new summary is produced at each step. This 
process models exploratory search: a user ex-
plores a new topic by submitting a sequence of 
queries, inspecting a summary of the result set 
and phrasing a new query at each step. We 
present a novel dataset comprising 22 query-
chains sessions of length up to 3 with 3 match-
ing human summaries each in the consumer-
health domain. Our analysis demonstrates that 
summaries produced in the context of such 
exploratory process are different from in-
formative summaries. We present an algorithm 
for Query-Chain Summarization based on a 
new LDA topic model variant.  Evaluation in-
dicates the algorithm improves on strong base-
lines. 
1 Introduction 
In the past 10 years, the general objective of 
text summarization has been refined into more 
specific tasks. Such summarization tasks include: 
(i) Generic Multi Document Summarization: 
aims at summarizing a cluster of topically related 
documents, such as the top results of a search 
engine query; (ii) in Update Summarization, a set 
of documents is summarized while assuming the 
user has already read a summary of earlier doc-
uments on the same topic; (iii) in Query-Focused 
Summarization, the summary of a documents set 
is produced to convey an informative answer in 
the context of a specific query. The importance 
of these specialized tasks is that they help us dis-
tinguish criteria that lead to the selection of con-
tent in a summary: centrality, novelty, relevance, 
and techniques to avoid redundancy. 
We present in this paper a variant summariza-
tion task which combines the two aspects of up-
date and query-focused summarization.  The task 
is related to exploratory search (Marchionini, 
2006). In contrast to classical information seek-
ing, in exploratory search, the user is uncertain 
about the information available, and aims at 
learning and understanding a new topic (White 
and Roth, 2009).  In typical exploratory search 
behavior, a user posts a series of queries, and 
based on information gathered at each step, de-
cides how to further explore a set of documents. 
The metaphor of berrypicking introduced in 
(Bates, 1989) captures this interactive process. 
At each step, the user may zoom in to a more 
specific information need, zoom out to a more 
general query, or pan sideways, in order to inves-
tigate a new aspect of the topic.  
We define Query-Chain Focused Summariza-
tion as follows: for each query in an exploratory 
search session, we aim to extract a summary that 
answers the information need of the user, in a 
manner similar to Query-Focused Summariza-
tion, while not repeating information already 
provided in previous steps, in a manner similar to 
Update Summarization. In contrast to query-
focused summarization, the context of a sum-
913
mary is not a single query, but the set of queries 
that led to the current step, their result sets and 
the corresponding summaries. 
We have constructed a novel dataset of Query-
Sets with matching manual summarizations in 
the consumer health domain (Cline and Haynes, 
2001). Queries are extracted from PubMed 
search logs (Dogan et al, 2009). We have ana-
lyzed this manual dataset and confirm that sum-
maries written in the context of berry-picking are 
markedly different from those written for similar 
queries on the same document set, but without 
the query-chain context. 
We have adapted well-known multi-document 
algorithms to the task, and present baseline algo-
rithms based on LexRank (Erkan and Radev, 
2004), KLSum and TopicSum (Haghighi and 
Vanderwende, 2009). We introduce a new algo-
rithm to address the task of Query-Chain Fo-
cused Summarization, based on a new LDA topic 
model variant, and present an evaluation which 
demonstrates it improves on these baselines. 
The paper is structured as follows. Section 2 
formulates the task of Query-Chain Focused 
Summarization. Section 3 reviews related work. 
In Section 4, we describe the data collection pro-
cess and the resulting dataset. We then present 
our algorithm, as well as the baseline algorithms 
used for evaluation. We conclude with evalua-
tion and discussion. 
2 Query- Chain Summarization 
In this work, we focus on the zoom in aspect 
of the exploratory search process described 
above. We formulate the Query-Chain Focused 
Summarization (QCFS) task as follows: 
Given an ordered chain of queries Q and a set 
of documents D , for each query Qqi?  a sum-
mary Si is generated from D answering 
iq  under 
the assumption that the user has already read the 
summaries Si-1 for queries
10... ?iqq . 
A typical example of query chain in the con-
sumer health domain we investigate includes the 
following 3 successive queries: (Causes of asth-
ma, Asthma and Allergy, Asthma and Mold Al-
lergy).   We consider a single set of documents 
relevant to the domain of Asthma as the refer-
ence set D.  The QCFS task consists of generat-
ing one summary of D as an answer to each que-
ry, so that the successive answers do not repeat 
information already provided in a previous an-
swer. 
3 Previous Work 
We first review the closely related tasks of 
Update Summarization and Query-Focused 
Summarization. We also review key summariza-
tion algorithms that we have selected as baseline 
and adapted to the QCFS task. 
Update Summarization focuses on identifying 
new information relative to a previous body of 
information, modeled as a set of documents. It 
has been introduced in shared tasks in DUC 2007 
and TAC 2008.  This task consists of producing a 
multi-document summary for a document set on 
a specific topic, and then a multi-document 
summary for a different set of articles on the 
same topic published at later dates. This task 
helps us understand how update summaries iden-
tified and focused on new information while re-
ducing redundancy compared to the original 
summaries.  
The TAC 2008 dataset includes 48 sets of 20 
documents, each cluster split in two subsets of 10 
documents (called A and B). Subset B docu-
ments were more recent. Original summaries 
were generated for the A subsets and update 
summaries were then produced for the B subsets. 
Human summaries and candidate systems are 
evaluated using the Pyramid method (Nenkova 
and Passonneau, 2004). For automatic evaluation, 
ROUGE (Lin, 2004) variants have been pro-
posed (Conroy et al, 2011).  In contrast to this 
setup, QCFS distinguishes the subsets of docu-
ments considered at each step of the process by 
facets of the underlying topic, and not by chro-
nology. In addition, the document subsets are not 
identified as part of the task in QCFS (as op-
posed to the explicit split in A and B subsets in 
Update Summarization). 
Most systems working on Update Summariza-
tion have focused on removing redundancy. Du-
alSum (Delort and Alfonseca, 2012) is notable in 
attempting to directly model novelty using a spe-
cialized topic-model to distinguish words ex-
pressing background information and those in-
troducing new information in each document. 
In Query-Focused Summarization (QFS), the 
task consists of identifying information in a doc-
ument set that is most relevant to a given query.  
914
This differs from generic summarization, where 
one attempts to identify central information.  
QFS helps us distinguish models of relevance 
and centrality.  Unfortunately, detailed analysis 
of the datasets produced for QFS indicates that 
these two notions are not strongly distinguished 
in practice: (Gupta et al, 2007) observed that in 
QFS datasets, up to 57% of the words in the doc-
ument sets were closely related to the query 
(through simple query expansion).  They note 
that as a consequence, a generic summarizer 
forms a strong baseline for such biased QFS 
tasks. 
We address this limitation of existing QFS da-
tasets in our definition of QCFS: we identify a 
chain of at least 3 related queries which focus on 
different facets of the same central topic and re-
quire the generation of distinct summaries for 
each query, with little repetition across the steps. 
A specific evaluation aspect of QFS measures 
responsiveness (how well the summary answers 
the specific query).  QFS must rely on Infor-
mation Retrieval techniques to overcome the 
scarceness of the query to establish relevance.  
As evidenced since (Daume and Marcu, 2006), 
Bayesian techniques have proven effective at this 
task: we construct a latent topic model on the 
basis of the document set and the query. This 
topic model effectively serves as a query expan-
sion mechanism, which helps assess the rele-
vance of individual sentences to the original que-
ry. 
In recent years, three major techniques have 
emerged to perform multi-document summariza-
tion: graph-based methods such as LexRank (Er-
kan and Radev, 2004) for multi document sum-
marization and Biased-LexRank (Otterbacher et 
al., 2008) for query focused summarization, lan-
guage model methods such as KLSum (Haghighi 
and Vanderwende, 2009) and variants of KLSum 
based on topic models such as BayesSum (Dau-
me and Marcu, 2006) and TopicSum (Haghighi 
and Vanderwende, 2009).   
LexRank is a stochastic graph-based method 
for computing the relative importance of textual 
units in a natural text. The LexRank algorithm 
builds a weighted graph ? = (?, ?) where each 
vertex in ? is a linguistic unit (in our case sen-
tences) and each weighted edge in ? is a measure 
of similarity between the nodes. In our imple-
mentation, we model similarity by computing the 
cosine distance between the ?? ? ???  vectors 
representing each node. After the graph is gener-
ated, the PageRank algorithm (Page et al, 1999) 
is used to determine the most central linguistic 
units in the graph. To generate a summary we 
use the ?  most central lexical units, until the 
length of the target summary is reached. This 
method has no explicit control to avoid redun-
dancy among the selected sentences, and the 
original algorithm does not address update or 
query-focused variants. Biased-LexRank (Otter-
bacher et al, 2008) makes LexRank sensitive to 
the query by introducing a prior belief about the 
ranking of the nodes in the graph, which reflects 
the similarity of sentences to the query. Pag-
eRank spreads the query similarity of a vertex to 
its close neighbors, so that we rank higher sen-
tences that are similar to other sentences which 
are similar to the query. As a result, Biased-
LexRank overcomes the lexical sparseness of the 
query and obtained state of the art results on the 
DUC 2005 dataset. 
KLSum adopts a language model approach to 
compute relevance: the documents in the input 
set are modeled as a distribution over words (the 
original algorithm uses a unigram distribution 
over the bag of words in documents D). KLSum 
is a sentence extraction algorithm: it searches for 
a subset of the sentences in D with a unigram 
distribution as similar as possible to that of the 
overall collection D, but with a limited length. 
The algorithm uses Kullback-Lieber (KL) diver-
gence ??(?||?) = ? log? (
?(?)
?(?)
)?(?)  to com-
pute the similarity of the distributions. It searches 
for ?? = argmin|?|<???(??||??). This search is 
performed in a greedy manner, adding sentences 
one by one to S until the length L is reached, and 
choosing the best sentence as measured by KL-
divergence at each step. The original method has 
no update or query focusing capability, but as a 
general modeling framework it is easy to adapt to 
a wide range of specific tasks. 
TopicSum uses an LDA-like topic model (Blei 
et al 2003) to classify words from a number of 
document sets (each set discussing a different 
topic) as either general non-content words, topic 
specific words and document specific word (this 
category refers to words that are specific to the 
writer and not shared across the document set). 
After the words are classified, the algorithm uses 
a KLSum variant to find the summary that best 
matches the unigram distribution of topic specif-
ic words. This method improves the results of 
915
KLSum but it also has no update summary or 
query answering capabilities.  
4 Dataset Collection 
We now describe how we have constructed a 
dataset to evaluate QCFS algorithms, which we 
are publishing freely. We selected to build our 
dataset in the Consumer Health domain, a popu-
lar domain in the web (Cline and Haynes 2001) 
providing medical information at various levels 
of complexity, ranging from layman and up to 
expert information, because consumer health il-
lustrates the need for exploratory search.   
The PubMed repository, while primarily serving 
the academic community, is also used by laymen 
to ask health related questions. The PubMed que-
ry logs (Dogan et al, 2009) provide user queries 
with timestamps and anonymized user identifica-
tion. They are publically available and include 
over 600K queries per day. In this dataset, Dogan 
and Murray found that query reformulation (typ-
ical of exploratory search) is quite frequent: "In 
our dataset, 47% of all queries are followed by a 
new subsequent query. These users did not select 
any abstract or full text views from the result set. 
We make an operational assumption that these 
users? intent was to modify their search by re-
formulating their query." We used these logs to 
extract laymen queries relating to four topics: 
Asthma, Lung Cancer 2EHVLW\ DQG $O]KHLPHU?V 
disease. We extracted a single day query log. 
From these, we extracted sessions which con-
WDLQHG WKH WHUPV ?Asthma? ?Lung Cancer ,? 
?Obesity? RU ?Alzheimer .? Sessions containing 
VHDUFK WDJV VXFK DV ?>$XWKRU@? ZHUH removed 
to reduce the number of academic searches. The 
sessions were then manually examined and used 
to create zoom-in query chains of length 3 at 
most. The queries appear below: 
Asthma: 
Asthma causes? asthma allergy? asthma mold allergy; 
Asthma treatment?asthma medication?corticosteroids; 
Exercise induced asthma? exercise for asthmatic; 
Atopic dermatitis? atopic dermatitis medications? atopic 
dermatitis side effects; 
Atopic dermatitis? atopic dermatitis children? atopic der-
matitis treatment; 
Atopic dermatitis? atopic dermatitis exercise activity?
 atopic dermatitis treatment; 
Cancer: 
Lung cancer? lung cancer causes? lung cancer symptoms; 
Lung cancer diagnosis? lung cancer treatment?lung cancer 
treatment side effects; 
Stage of lung cancer? lung cancer staging tests? lung can-
cer TNM staging system; 
Types of lung cancer?non-small cell lung cancer treat-
ment?non-small cell lung cancer surgery; 
Lung cancer in women? risk factors for lung cancer in 
women? treatment of lung cancer in women; 
Lung cancer chemotherapy? goals of lung cancer chemo-
therapy? palliative care for lung cancer; 
Obesity: 
Salt obesity?retaining fluid; 
Obesity screening?body mass index?BMI Validity; 
Childhood obesity?childhood obesity low income?chil-
dren diet and exercise; 
Causes of childhood obesity?obesity and nutrition?school 
lunch; 
Obesity and lifestyle change?obesity metabolism?super-
foods antioxidant; 
Obesity and diabetes?emergence of type 2 diabetes?type 2 
diabetes and obesity in children; 
Alzheimer?s disease: 
Alzheimer memory?helping retrieve memory alzheimer 
?alzheimer memory impairment nursing; 
Cognitive impairment?Vascular Dementia?Vascular De-
mentia difference alzheimer; 
$O]KHLPHU?V symptoms?alzheimer diagnosis?alzheimer 
medications; 
Semantic dementia?first symptoms dementia?first symp-
toms alzheimer; 
Figure 1: Queries Used to Construct Dataset 
We asked medical experts to construct four 
document collections from well-known and reli-
able consumer health websites relating to the 
four subjects (Wikipedia, WebMD, and the 
NHS), so that they would provide general infor-
mation relevant to the queries. 
We then asked medical students to manually 
produce summaries of these four document col-
lections for each query-chain. The medical stu-
dents were instructed construct a text of up to 
250 words that provides a good answer to each 
query in the chain. For each query in a chain the 
summarizers should assume that the person read-
ing the summaries is familiar with the previous 
916
summaries in the chain so they should avoid re-
dundancy. 
Three distinct human summaries were pro-
duced for each chain.  For each chain, one sum-
mary was produced for each of the three queries, 
where the person producing the summary was 
not shown the next steps in the chain when an-
swering the first query. 
To simulate the exploratory search of the user 
we provided the annotators with a Solr1  query 
interface for each document collection. The in-
terface allowed querying the document set, read-
ing the documents and choosing sentences which 
answer the query. After choosing the sentences, 
annotators can copy and edit the resulting sum-
mary in order to create an answer of up to 250 
words. After processing the first two query chain 
summaries, the annotators held a post-hoc dis-
cussion about the different summaries in order to 
adjust their conception of the task. 
The statistics on the collected dataset appear in 
the Tables below: 
Document sets # Docs # Sentences #Tokens / 
Unique 
Asthma  125 1,924 19,662 / 2,284 
Lung-Cancer 135 1,450 17,842 / 2,228 
Obesity 289 1,615 21,561 / 2,907 
$O]KHLPHU?V 'LVHDVH 191 1,163 14,813 / 2,508 
 
Queries # Sessions # Sentences #Tokens / 
Unique 
Asthma  5 15 36 / 14 
Lung-Cancer 6 18 71 / 25 
Obesity 6 17 45 / 29 
$O]KHLPHU?V 'LVHDVH 4 12 33 / 16 
 
Manual Summaries # Docs # Sentences #Tokens / 
Unique 
Asthma  45 543 6,349  / 1,011 
Lung-Cancer 54 669 8,287  / 1,130 
Obesity 51 538 7,079  / 1,270 
$O]KHLPHU?V 'LVHDVH 36 385 5,031  /    966  
Table 1: Collected Dataset Size Statistics 
A key aspect of the dataset is that the same 
documents are summarized for each step of the 
chains, and we expect the summaries for each 
step to be different (that is, each answer is indeed 
responsive to the specific query it addresses). In 
addition, each answer is produced in the context 
of the previous steps, and only provides updated 
                                                 
1 http://lucene.apache.org/solr/ 
information with respect to previous answers. To 
ensure that the dataset indeed reflects these two 
aspects (responsiveness and freshness), we em-
pirically verified that summaries created for ad-
vanced queries are different from the summaries 
created for the same queries by summarizers who 
did not see the previous summaries in the chain. 
We asked from additional annotators to create 
manual summaries of advanced queries from the 
query chain without ever seeing the queries from 
the beginning of the chain. For example, given 
the chain (asthma causes? asthma allergy?
 asthma mold allergy), we asked summarizers to 
produce an answer for the second query (asthma 
allergy) without seeing the first step, on the same 
input documents. 
We used ROUGE to perform this validation: 
ROUGE compares a summary with a set of ref-
erence summaries and source documents. We 
first computed the mean ROUGE score of the 
second query summaries. The mean ROUGE 
score is the mean score of each manual summary 
vs. all other summaries about the same query. 
We got (?1 = 0.52, ?2 = 0.22, ??4 = 0.13). The 
mean ROUGE scores of the same second query 
summaries by people who did not see the previ-
ous query were markedly lower: ( ?1 =
0.40, ?2 = 0.22, ??4 = 0.01).  We only verified 
the asthma dataset in this manner. The results, 
except for the R2 test, had statistically significant 
difference with 95%  confidence interval. 
All the data, code and an annotated example can 
be found in our site2. 
We show below slightly shortened manual 
summaries created in answer to the following 
query chain: "obesity screening -> body mass 
index -> BMI Validity". We manually annotated 
the summaries to highlight how the exploration 
process develops and indicate some of the infor-
mation retrieval challenges faced when address-
ing the task: underlined terms are directly rele-
vant to the current query; bold italic terms have 
likely triggered the curiosity of the reader and led 
to asking the next query.  In this example, a user 
asks about obesity screening, and reformulates 
his query using the technical term of BMI, which 
is prevalent in the produced answer. 
 
Q1:  obesity screening 
Body mass index (BMI) is a measure used to determine 
                                                 
2 http://www.cs.bgu.ac.il/~nlpproj/QCFS/dataset.html 
917
childhood overweight and obesity. A child's weight status 
is determined using an age- and sex-specific percentile 
for BMI rather than the BMI categories used for adults 
because children's body composition varies as they age 
and varies between boys and girls. For children and ado-
lescents (aged 2-19 years): Overweight is defined as 
a BMI at or above the 85th percentile and lower than the 
95th percentile for children of the same age and 
sex.  Obesity is defined as a BMI at or above the 95th 
percentile for children of the same age and sex. BMI is not 
a perfect measure, but rather a helpful indicator. It doesn't 
consider things like being muscular or having a larger than 
average body frame and because growth patterns vary 
greatly among children.  As a result, doctors also factor a 
child's growth and development into consideration 
when screening for overweight and obesity. Obesity 
screening of adults using BMI may be less accurate for 
adults, due to the coarseness of the measurement: Heavily 
muscled athletes may have a high BMI, as many women 
who enter menopause, who tend to gain weight due to 
increased water retention. 
Q2: body mass index 
The body mass index (BMI), or Quetelet index, is a meas-
ure for human body shape based on an individu-
al's mass and height. Devised in the mid-1800s by Adolphe 
Quetelet during the course of developing "social physics", 
it is defined as an individual's body mass divided by the 
square of their height, the units being kg/m^2. BMI was 
explicitly cited as being appropriate for population studies, 
and inappropriate for individual diagnosis. BMI provides a 
simple measure of a person's thickness, allowing health 
professionals to discuss over-weight and underweight  
problems more objectively with their patients. Howev-
er, BMI has become controversial because many people, 
including physicians, have come to rely on its appar-
ent authority for medical diagnosis. However, it was origi-
nally meant to be used as a simple means of classifying 
sedentary individuals, or rather, populations, with an aver-
age body composition. For these individuals, the current 
value settings are as follows: (...). Nick Korevaar (a mathe-
matics lecturer from the University of Utah) suggests that 
instead of squaring the body height or cubing 
the body height, it would be more appropriate to use an 
exponent of between 2.3 and 2.7 (as originally noted by 
Quetelet). 
Q3: BMI Validity 
BMI has become controversial because many people, in-
cluding physicians, have come to rely on its apparent nu-
merical authority for medical diagnosis, but that was never 
the BMI's purpose; it is meant to be used as a simple 
means of classifying sedentary populations with an average 
body composition. In an article published in the July edi-
tion of 1972 of the Journal of Chronic Diseases, Ancel Keys 
explicitly cited BMI as being appropriate for population 
studies, but inappropriate for individual diagnosis. These 
ranges of BMI values are valid only as statistical categories 
While BMI is a simple, inexpensive method of screening for 
weight categories, it is not a good diagnostic tool: It does 
not take into account age, gender, or muscle mass. (...). 
Figure 2: Query Chain Summary Annotated Example 
5 Algorithms  
In this section, we first explain how we 
adapted the previously mentioned methods to the 
QCFS task, thus producing 3 strong baselines. 
We then describe our new algorithm for QCFS. 
5.1 Focused KLSum 
We adapted KLSum to QCFS by introducing 
a simple document selection step in the algo-
rithm.  The method is: given a query step ?, we 
first select a focused subset of documents from 
?,?(?).  We then apply the usual KLSum algo-
rithm over ?(?). This approach does not make 
any effort to reduce redundancy from step to step 
in the query chain.  In our implementation, we 
compute ?(?) by selecting the top-10 documents 
in ? ranked by ?? ? ??? scores to the query, as 
implemented in SolR. 
5.2 KL-Chain-Update 
KL-Chain-Update is a slightly more sophisti-
cated variation of KLSum that answers a query 
chain (instead a single query). When construct-
ing a summary, we update the unigram distribu-
tion of the constructed summary so that it in-
cludes a smoothed distribution of the previous 
summaries in order to eliminate redundancy be-
tween the successive steps in the chain. For ex-
ample, when we summarize the documents that 
were retrieved as a result to the first query, we 
calculate the unigram distribution in the same 
manner as we did in Focused KLSum; but for the 
second query, we calculate the unigram distribu-
tion as if all the sentences we selected for the 
previous summary were selected for the current 
query too, with a damping factor. In this variant, 
the Unigram Distribution estimate of word X is 
computed as: 
918
(Count(?,??????????) +
Count(?, ???????????)
??????????????? )
Length(??????????) +
Length(PreviousSum ? ??????????)
???????????????
 
5.3 ChainSum 
ChainSum is our adaptation of TopicSum to 
the QCFS task. We developed a novel Topic 
Model to identify words that are associated to the 
current query and not shared with the previous 
queries. We achieved this with the following 
model. For each query in a chain, we consider 
the documents ??which are "good answers" to 
the query; and ?? which are the documents used 
to answer the previous steps of the chain.  We 
assume in this model that these document subsets 
are observable (in our implementation, we select 
these subsets by ranking the documents for the 
query based on TFxIDF similarity). 
1. ? is the general words topic, it is intended 
to capture stop words and non-topic spe-
cific vocabulary. Its distribution ??  is 
drawn for all the documents from 
?????????(?, ??). 
2. ?? is the document specific topic; it repre-
sents words which are local for a specific 
document.  ???  is drawn for each docu-
ment from ?????????(?, ???). 
3. ? is the new content topic, which should 
capture words that are characteristic for 
??. ?? is drawn for all the documents in 
?? from ?????????(?, ??). 
4. ?  captures old content from ?? , ??  is 
drawn for all the documents in ??  from 
?????????(?, ??). 
5. ? captures redundant information between 
??  and ??, ??  is drawn for all the docu-
ments in ?? ? ?? from ?????????(?, ??). 
6. For documents from ?? we draw from the 
distribution ??1  over topics (?, ?, ?, ??) 
from a Dirichlet prior with pseudo-
counts (10.0,15.0,15.0,1.0)3 . For each 
word in the document, we draw a topic ? 
from ??, and a word ? from the topic in-
dicated by ?. 
                                                 
3 All pseudo-counts were selected empirically  
7. For documents from ??, we draw from the 
distribution ??2  over topics (?, ?, ?, ??) 
from a Dirichlet prior with pseudo-
counts  (10.0,15.0,15.0,1.0) . The words 
are drawn in the same manner as in ?1. 
8. For documents in ? ? (?? ? ??) we draw 
from the distribution ??3  over topics 
(?, ??) from a Dirichlet prior with pseudo-
counts (10.0,1.0) . The words are also 
drawn in the same manner as in ?1. 
The plate diagram of this generative model is 
shown in Fig.3. 
 
Figure 3 Plate Model for Our Topic Model 
We implemented inference over this topic 
model using Gibbs Sampling (we distribute the 
code of the sampler together with our dataset).  
After the topic model is applied to the current 
query, we apply KLSum only on words that are 
assigned to the new content topic. Fig.4 summa-
rizes the algorithm data flow. 
When running this topic model on our dataset, 
we observe: ??  mean size was 978 words and 
375 unique words. ??   mean size was 1374 
words and 436 unique words. ??  and ??  mean 
on average 159 words. These figures show there 
is high lexical overlap between the summaries 
answering query qi and qi+1 and highlight the 
need to distinguish new and previously exposed 
content. 
In the ChainSum model, the topic R aims at 
modeling redundant information between the 
previous summaries and the new summary.  We 
intend in the future to exploit this information to 
construct a contrastive model of content selec-
tion.  In the current version, R does not play an 
active role in content selection.  We, therefore, 
tested a variant of ChainSum that did not in-
clude ??  and obtained results extremely similar 
to the full model, which we report below. 
919
 Figure 4 ChainSum Architecture 
5.4 Adapted LexRank 
In LexRank, the algorithm creates a graph 
where nodes represent the sentences from the 
text and weighted edges represent the cosine-
distance of each sentence's TFxIDF vec-
tors. After creating the graph, PageRank is run to 
rank sentences. We adapted LexRank to QCFS in 
two main ways: we extend the sentence represen-
tation scheme to capture semantic information 
and refine the model of sentences similarity so 
that it captures query answering instead of cen-
trality. We tagged each sentence with Wikipedia 
terms using the Illinois Wikifier (Ratinov et al, 
2011) and with UMLS (Bodenreider, 2004) 
terms using HealthTermFinder (Lipsky-Gorman 
and Elhadad, 2011). UMLS is a rich medical on-
tology, which is appropriate to the consumer 
health domain. 
We changed the edges scoring formula to use 
the sum of Lexical Semantic Similarity (LSS) 
functions (Li et al, 2007) on lexical terms, Wik-
ipedia terms and UMLS terms: 
?????(?, ?) = ??????????(?, ?) + ?
? ???????(?, ?) + ?
? ???????(?, ?) 
Where: 
???(?1, ?2) =
? (????(
???(??
1,??
2)
???(??
1,??
1)
)???(??
1))?
? ???(??
1)?
 
Instead of using the cosine distance, in order to 
incorporate advanced word/term similarity func-
tions. For lexical terms, we used the identity 
function, for Wikipedia term we used Wikiminer 
(Milne, 2007), and for UMLS we used Ted 
Pedersen UMLS similarity function (McInnes et 
al., 2009).  Finally, instead of PageRank, we 
used SimRank (Haveliwala, 2002) to identify the 
nodes most similar to the query node and not 
only the central sentences in the graph.  
6 Evaluation 
6.1 Evaluation Dataset 
We worked on the dataset we created for 
QCFS and added semantic tags: 10% of the to-
kens had Wikipedia annotations and 33% had a 
UMLS annotation. 
6.2 Results 
 
Figure 5: ROUGE Recall Scores (with stemming and 
stop-words) 
For Focused KLSum we received ROUGE 
scores of (r1 = 0.281, r2 = 0.061, su4 = 0.100), 
KL-Chain-Update (r1 = 0.424, r2 = 0.149, su4 = 
0.193), ChainSum (r1 = 0.44988, r2 = 0.1587, 
su4 = 0.20594), ChainSum with t Simplified 
Topic model (r1 = 0.44992, r2 = 0.15814, su4 = 
0.20507) and for Modified-LexRank (r1 = 0.444, 
r2 = 0.151, su4 = 0.201). All of the modified ver-
sions of our algorithm performed better than Fo-
cused KLSum with more than 95% confidence.  
7 Conclusions 
We presented a new summarization task tai-
lored for the needs of exploratory search system. 
This task combines elements of question answer-
ing by sentence extraction with those of update 
summarization. 
The main contribution of this paper is the def-
inition of a new summarization task that corre-
sponds to exploratory search behavior and the 
contribution of a novel dataset containing human 
summaries. This dataset is annotated with Wik-
ipedia and UMLS terms for over 30% of the to-
kens. We controlled that the summaries cover 
only part of the input document sets (and are, 
therefore, properly focused) and sensitive to the 
position of the queries in the chain. 
Four methods were evaluated for the task. The 
baseline methods based on KL-Sum show a sig-
0
0.5
R1 R2 R3 R4 SU4
Focused-KLSum KLSum-Update LexRank-U
QC-LDA QC-simplified
920
nificant improvement when penalizing redun-
dancy with the previous summarization. 
7KLV SDSHU FRQFHQWUDWHG RQ ?]RRP LQ? TXHU\ 
FKDLQV RWKHU XVHU DFWLRQV VXFK DV ?]RRP RXW? RU 
?VZLWFK WRSLF? ZHUH OHIW WR IXWXUH ZRUN This pa-
SHU FRQFHQWUDWHG RQ ?]RRP LQ? TXHU\ FKDLQV RWK
HU XVHU DFWLRQV VXFK DV ?]RRP RXW? RU ?VZLWFK 
WRSLF? ZHUH OHIW WR IXWXUH ZRUN  The task remains 
extremely challenging, and we hope the dataset 
availability will allow further research to refine 
our understanding of topic-sensitive summariza-
tion and redundancy control. 
In future work, we will attempt to derive a 
task-specific evaluation metric that exploits the 
structure of the chains to better assess relevance, 
redundancy and contrast. 
Acknowledgments 
This work was supported by the Israeli Minis-
ter of Science (Grant #3-8705) and by the Lynn 
and William Frankel Center for Computer Sci-
ences, Ben-Gurion University.  We thank the 
reviewers for extremely helpful advice. 
References  
Marcia J. Bates. 1989. The design of browsing and 
berrypicking techniques for the online search 
interface, Online Information Review, 13(5), 407-
424.  
 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent dirichlet alocation, the Journal of 
machine Learning research, 3, 993-1022. 
 
Olivier Bodenreider. 2004. The unified medical 
language system (UMLS): integrating biomedical 
terminology, Nucleic acids research, 32(suppl 1), 
D267-D270.  
 
John M. Conroy, Judith D. Schlesinger, and Dianne P. 
O'Leary. 2011. Nouveau-rouge: A novelty metric 
for update summarization, Computational 
Linguistics, 37(1), 1-8. 
 
Rebecca JW Cline, and Katie M. Haynes. 2001. 
Consumer health information seeking on the 
Internet: the state of the art, Health education 
research, 16(6), 671-692.  
 
Daume Hal and Daniel Marcu. 2006. Bayesian query-
focused summarization, In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics (pp. 
305-312). Association for Computational 
Linguistics. 
 
Jean-Yves Delort, and Enrique Alfonseca. 2012. 
DualSum: a Topic-Model based approach for 
update summarization, In Proceedings of the 13th 
Conference of the European Chapter of the 
Association for Computational Linguistics (pp. 
214-223). Association for Computational 
Linguistics.  
 
Rezarta Islamaj Dogan, G. Craig Murray, Aur?lie 
N?v?ol, and Zhiyong Lu. 2009. Understanding 
PubMed? user search behavior through log 
analysis, Database: The Journal of Biological 
Databases & Curation, 2009. 
 
G?nes Erkan, and Dragomir R. Radev. 2004. 
LexRank: Graph-based lexical centrality as 
salience in text summarization, J. Artif. Intell. 
Res.(JAIR), 22(1), 457-479.  
 
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 
2007. Measuring importance and query relevance 
in topic-focused multi-document summarization, In 
Proceedings of the 45th Annual Meeting of the 
ACL on Interactive Poster and Demonstration 
Sessions (pp. 193-196). Association for 
Computational Linguistics. 
 
Aria Haghighi, and Lucy Vanderwende. 2009. 
Exploring content models for multi-document 
summarization, In Proceedings of Human 
Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics (pp. 
362-370). Association for Computational 
Linguistics.  
 
Glen Jeh, and Jennifer Widom. 2002. SimRank: a 
measure of structural-context similarity, In 
Proceedings of the eighth ACM SIGKDD 
international conference on Knowledge discovery 
and data mining (pp. 538-543). ACM.  
 
Baoli Li, Joseph Irwin, Ernest V. Garcia, and Ashwin 
Ram. 2007. Machine learning based semantic 
inference: Experiments and Observations at RTE-
3, In Proceedings of the ACL-PASCAL Workshop 
on Textual Entailment and Paraphrasing (pp. 159-
164). Association for Computational Linguistics. 
 
Chin-Yew Lin. 2004. Rouge: A package for automatic 
evaluation of summaries, In Text Summarization 
Branches Out: Proceedings of the ACL-04 
Workshop (pp. 74-81). 
 
Sharon Lipsky-Gorman, and No?mie Elhadad 2011. 
ClinNote and HealthTermFinder: a pipeline for 
921
processing clinical notes, Columbia University 
Technical Report, Columbia University. 
 
Gary Marchionini. 2006. Exploratory search: from 
finding to understanding, Communications of the 
ACM, 49(4), 41-46.  
 
Bridget T. McInnes, Ted Pedersen, and Serguei VS 
Pakhomov. (2009). UMLS-Interface and UMLS-
Similarity: open source software for measuring 
paths and semantic similarity, AMIA Annual 
Symposium Proceedings, American Medical 
Informatics Association. 
 
David Milne. 2007. Computing semantic relatedness 
using wikipedia link structure, In Proceedings of 
the new zealand computer science research student 
conference. 
 
Ani Nenkova, and Rebecca J. Passonneau. 2004. 
Evaluating Content Selection in Summarization: 
The Pyramid Method, In HLT-NAACL (pp. 145-
152). 
 
Jahna Otterbacher, Gunes Erkan, and Dragomir R. 
Radev. 2009. Biased LexRank: Passage retrieval 
using random walks with question-based priors, 
Information Processing & Management, 45(1), 42-
54. 
 
Lawrence Page, Sergey Brin, Rajeev Motwani, and 
Terry Winograd. 1999. The PageRank citation 
ranking: bringing order to the web, 
Lev Ratinov, Dan Roth, Doug Downey, and Mike 
Anderson. 2011. Local and Global Algorithms for 
Disambiguation to Wikipedia, In ACL (Vol. 11, 
pp. 1375-1384). 
 
Ryen W. White, and Resa A. Roth. 2009. Exploratory 
search: Beyond the query-response paradigm. 
Synthesis Lectures on Information Concepts, 
Retrieval, and Services, 1(1), 1-98.  
 
 
 
922
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103?107,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Easy First Dependency Parsing of Modern Hebrew
Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We investigate the performance of an easy-
first, non-directional dependency parser on the
Hebrew Dependency treebank. We show that
with a basic feature set the greedy parser?s ac-
curacy is on a par with that of a first-order
globally optimized MST parser. The addition
of morphological-agreement feature improves
the parsing accuracy, making it on-par with a
second-order globally optimized MST parser.
The improvement due to the morphological
agreement information is persistent both when
gold-standard and automatically-induced mor-
phological information is used.
1 Introduction
Data-driven Dependency Parsing algorithms are
broadly categorized into two approaches (Ku?bler et
al., 2009). Transition based parsers traverse the
sentence from left to right1 using greedy, local in-
ference. Graph based parsers use global inference
and seek a tree structure maximizing some scoring
function defined over trees. This scoring function
is usually decomposed over tree edges, or pairs of
such edges. In recent work (Goldberg and Elhadad,
2010), we proposed another dependency parsing ap-
proach: Easy First, Non-Directional dependency
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
1Strictly speaking, the traversal order is from start to end.
This distinction is important when discussing Hebrew parsing,
as the Hebrew language is written from right-to-left. We keep
the left-to-right terminology throughout this paper, as this is the
common terminology. However, ?left? and ?right? should be
interpreted as ?start? and ?end? respectively. Similarly, ?a token
to the left? should be interpreted as ?the previous token?.
parsing. Like transition based methods, the easy-
first method adopts a local, greedy policy. How-
ever, it abandons the strict left-to-right processing
order, replacing it with an alternative order, which
attempts to make easier attachments decisions prior
to harder ones. The model was applied to English
dependency parsing. It was shown to be more accu-
rate than MALTPARSER, a state-of-the-art transition
based parser (Nivre et al, 2006), and near the perfor-
mance of the first-order MSTPARSER, a graph based
parser which decomposes its score over tree edges
(McDonald et al, 2005), while being more efficient.
The easy-first parser works by making easier de-
cisions before harder ones. Each decision can be
conditioned by structures created by previous deci-
sions, allowing harder decisions to be based on rel-
atively rich syntactic structure. This is in contrast to
the globally optimized parsers, which cannot utilize
such rich syntactic structures. It was hypothesized
in (Goldberg and Elhadad, 2010) that this rich con-
ditioning can be especially beneficial in situations
where informative structural information is avail-
able, such as in morphologically rich languages.
In this paper, we investigate the non-directional
easy-first parser performance on Modern Hebrew, a
semitic language with rich morphology, relatively
free constituent order, and a small treebank com-
pared to English. We are interested in two main
questions: (a) how well does the non-directional
parser perform on Hebrew data? and (b) can the
parser make effective use of morphological features,
such as agreement?
In (Goldberg and Elhadad, 2009), we describe
a newly created Hebrew dependency treebank, and
103
report results on parsing this corpus with both
MALTPARSER and first- and second- order vari-
ants of MSTPARSER. We find that the second-
order MSTPARSER outperforms the first order MST-
PARSER, which in turn outperforms the transition
based MALTPARSER. In addition, adding mor-
phological information to the default configurations
of these parsers does not improve parsing accu-
racy. Interestingly, when using automatically in-
duced (rather than gold-standard) morphological in-
formation, the transition based MALTPARSER?s ac-
curacy improves with the addition of the morpho-
logical information, while the scores of both glob-
ally optimized parsers drop with the addition of the
morphological information.
Our experiments in this paper show that the ac-
curacy of the non-directional parser on the same
dataset outperforms the first-order MSTPARSER.
With the addition of morphological agreement fea-
tures, the parser accuracy improves even further, and
is on-par with the performance of the second-order
MSTPARSER. The improvement due to the morpho-
logical information persists also when automatically
induced morphological information is used.
2 Modern Hebrew
Some aspects that make Hebrew challenging from a
language-processing perspective are:
Affixation Common prepositions, conjunctions
and articles are prefixed to the following word, and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes is often am-
biguous and must be determined in a specific context
only. In term of dependency parsing, this means that
the dependency relations occur not between space-
delimited tokens, but instead between sub-token el-
ements which we?ll refer to as segments. Further-
more, mistakes in the underlying token segmenta-
tions are sure to be reflected in the parsing accuracy.
Relatively free constituent order The ordering of
constituents inside a phrase is relatively free. This
is most notably apparent in the verbal phrases and
sentential levels. In particular, while most sentences
follow an SVO order, OVS and VSO configurations
are also possible. Verbal arguments can appear be-
fore or after the verb, and in many ordering. For
example, the message ?went from Israel to Thai-
land? can be expressed as ?went to Thailand from
Israel?, ?to Thailand went from Israel?, ?from Israel
went to Thailand?, ?from Israel to Thailand went?
and ?to Thailand from Israel went?. This results in
long and flat VP and S structures and a fair amount
of sparsity, which suggests that a dependency repre-
sentations might be more suitable to Hebrew than a
constituency one.
NP Structure and Construct State While con-
stituents order may vary, NP internal structure is
rigid. A special morphological marker (Construct
State) is used to mark noun compounds as well as
similar phenomena. This marker, while ambiguous,
is essential for analyzing NP internal structure.
Case Marking definite direct objects are marked.
The case marker in this case is the function word z`
appearing before the direct object.2
Rich templatic morphology Hebrew has a very
productive morphological structure, which is based
on a root+template system. The productive mor-
phology results in many distinct word forms and
a high out-of-vocabulary rate which makes it hard
to reliably estimate lexical parameters from anno-
tated corpora. The root+template system (combined
with the unvocalized writing system) makes it hard
to guess the morphological analyses of an unknown
word based on its prefix and suffix, as usually done
in other languages.
Unvocalized writing system Most vowels are not
marked in everyday Hebrew text, which results in a
very high level of lexical and morphological ambi-
guity. Some tokens can admit as many as 15 distinct
readings, and the average number of possible mor-
phological analyses per token in Hebrew text is 2.7,
compared to 1.4 in English (Adler, 2007).
Agreement Hebrew grammar forces morpholog-
ical agreement between Adjectives and Nouns
(which should agree in Gender and Number and def-
initeness), and between Subjects and Verbs (which
should agree in Gender and Number).
2The orthographic form z` is ambiguous. It can also stand
for the noun ?shovel? and the pronoun ?you?(2nd,fem,sing).
104
3 Easy First Non Directional Parsing
Easy-First Non Directional parsing is a greedy
search procedure. It works with a list of partial
structures, pi, . . . , pk, which is initialized with the
n words of the sentence. Each structure is a head
token which is not yet assigned a parent, but may
have dependants attached to it. At each stage of the
parsing algorithm, two neighbouring partial struc-
tures, (pi, pi+1) are chosen, and one of them be-
comes the parent of the other. The new dependant is
then removed from the list of partial structures. Pars-
ing proceeds until only one partial structure, corre-
sponding to the root of the sentence, is left. The
choice of which neighbouring structures to attach is
based on a scoring function. This scoring function
is learned from data, and attempts to attach more
confident attachments before less confident ones.
The scoring function makes use of features. These
features can be extracted from any pre-built struc-
tures. In practice, the features are defined on pre-
built structures which are around the proposed at-
tachment point. For complete details about training,
features and implementation, refer to (Goldberg and
Elhadad, 2010).
4 Experiments
We follow the setup of (Goldberg and Elhadad,
2009).
Data We use the Hebrew dependency treebank de-
scribed in (Goldberg and Elhadad, 2009). We use
Sections 2-12 (sentences 484-5724) as our training
set, and report results on parsing the development
set, Section 1 (sentences 0-483). As in (Goldberg
and Elhadad, 2009), we do not evaluate on the test
set in this work.
The data in the treebank is segmented and POS-
tagged. Both the parsing models were trained on
the gold-standard segmented and tagged data. When
evaluating the parsing models, we perform two sets
of evaluations. The first one is an oracle experi-
ment, assuming gold segmentation and tagging is
available. The second one is a real-world experi-
ment, in which we segment and POS-tag the test-
set sentences using the morphological disambigua-
tor described in (Adler, 2007; Goldberg et al, 2008)
prior to parsing.
Parsers and parsing models We use our freely
available implementation3 of the non-directional
parser.
Evaluation Measure We evaluate the resulting
parses in terms of unlabeled accuracy ? the percent
of correctly identified (child,parent) pairs4. To be
precise, we calculate:
number of correctly identified pairs
number of pairs in gold parse
For the oracle case in which the gold-standard to-
ken segmentation is available for the parser, this is
the same as the traditional unlabeled-accuracy eval-
uation metric. However, in the real-word setting in
which the token segmentation is done automatically,
the yields of the gold-standard and the automatic
parse may differ, and one needs to decide how to
handle the cases in which one or more elements in
the identified (child,parent) pair are not present in
the gold-standard parse. Our evaluation metric pe-
nalizes these cases by regarding them as mistakes.
5 Results
Base Feature Set On the first set of experiments,
we used the English feature set which was used in
(Goldberg and Elhadad, 2010). Our only modifica-
tion to the feature set for Hebrew was not to lex-
icalize prepositions (we found it to work somewhat
better due to the smaller treebank size, and Hebrew?s
rather productive preposition system).
Results of parsing the development set are sum-
marized in Table 1. For comparison, we list the per-
formance of the MALT and MST parsers on the same
data, as reported in (Goldberg and Elhadad, 2009).
The case marker z`, as well as the morpholog-
ically marked construct nouns, are covered by all
feature models. z` is a distinct lexical element in a
predictable position, and all four parsers utilize such
function word information. Construct nouns are dif-
ferentiated from non-construct nouns already at the
POS tagset level.
All models suffer from the absence of gold
POS/morphological information. The easy-first
non-directional parser with the basic feature set
3http://www.cs.bgu.ac.il/?yoavg/software/nondirparser/
4All the results are macro averaged.
105
(NONDIR) outperforms the transition based MALT-
PARSER in all cases. It also outperforms the first or-
der MST1 model when gold POS/morphology infor-
mation is available, and has nearly identical perfor-
mance to MST1 when automatically induced POS/-
morphology information is used.
Additional Morphology Features Error inspec-
tion reveals that most errors are semantic in nature,
and involve coordination, PP-attachment or main-
verb hierarchy. However, some small class of er-
rors reflected morphological disagreement between
nouns and adjectives. These errors were either in-
side a simple NP, or, in some cases, could affect rel-
ative clause attachments. We were thus motivated to
add specific features encoding morphological agree-
ment to try and avoid this class of errors.
Our features are targeted specifically at capturing
noun-adjective morphological agreement.5 When
attempting to score the attachment of two neigh-
bouring structures in the list, pi and pi+1, we in-
spect the pairs (pi, pi+1), (pi, pi+2), (pi?1, pi+1),
(pi?2, pi), (pi+1, pi+2). For each such pair, in case
it is made of a noun and an adjective, we add two
features: a binary feature indicating presence or ab-
sence of gender agreement, and another binary fea-
ture for number agreement.
The last row in Table 1 (NONDIR+MORPH)
presents the parser accuracy with the addition of
these agreement features. Agreement contributes
to the accuracy of the parser, making it as accu-
rate as the second-order MST2. Interestingly, the
non-directional model benefits from the agreement
features also when automatically induced POS/mor-
phology information is used (going from 75.5% to
76.2%). This is in contrast to the MST parsers,
where the morphological features hurt the parser
when non-gold morphology is used (75.6 to 73.9
for MST1 and 76.4 to 74.6 for MST2). This can
be attributed to either the agreement specific na-
ture of the morphological features added to the non-
directional parser, or to the easy-first order of the
non-directional parser, and to the fact the morpho-
logical features are defined only over structurally
close heads at each stage of the parsing process.
5This is in contrast to the morphological features used in
out-of-the-box MST and MALT parsers, which are much more
general.
Gold Morph/POS Auto Morph/POS
MALT 80.3 72.9
MALT+MORPH 80.7 73.4
MST1 83.6 75.6
MST1+MORPH 83.6 73.9
MST2 84.3 76.4
MST2+MORPH 84.4 74.6
NONDIR 83.8 75.5
NONDIR+MORPH 84.2 76.2
Table 1: Unlabeled dependency accuracy of the various
parsing models.
6 Discussion
We have verified that easy-first, non-directional de-
pendency parsing methodology of (Goldberg and El-
hadad, 2010) is successful for parsing Hebrew, a
semitic language with rich morphology and a small
treebank. We further verified that the model can
make effective use of morphological agreement fea-
tures, both when gold-standard and automatically in-
duced morphological information is provided. With
the addition of the morphological agreement fea-
tures, the non-directional model is as effective as a
second-order globally optimized MST model, while
being much more efficient, and easier to extend with
additional structural features.
While we get adequate parsing results for Hebrew
when gold-standard POS/morphology/segmentation
information is used, the parsing performance in
the realistic setting, in which gold POS/morpholo-
gy/segmentation information is not available, is still
low. We strongly believe that parsing and morpho-
logical disambiguation should be done jointly, or
at least interact with each other. This is the main
future direction for dependency parsing of Modern
Hebrew.
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Yoav Goldberg and Michael Elhadad. 2009. Hebrew De-
pendency Parsing: Initial Results. In Proc. of IWPT.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proc. of NAACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
106
EM Can find pretty good HMM POS-Taggers (when
given a good start). In Proc. of ACL.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc of ACL.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proc. of LREC.
107
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 234?242,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Inspecting the Structural Biases of Dependency Parsing Algorithms ?
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg|elhadad@cs.bgu.ac.il
Abstract
We propose the notion of a structural bias
inherent in a parsing system with respect
to the language it is aiming to parse. This
structural bias characterizes the behaviour
of a parsing system in terms of structures
it tends to under- and over- produce. We
propose a Boosting-based method for un-
covering some of the structural bias inher-
ent in parsing systems. We then apply
our method to four English dependency
parsers (an Arc-Eager and Arc-Standard
transition-based parsers, and first- and
second-order graph-based parsers). We
show that all four parsers are biased with
respect to the kind of annotation they are
trained to parse. We present a detailed
analysis of the biases that highlights spe-
cific differences and commonalities be-
tween the parsing systems, and improves
our understanding of their strengths and
weaknesses.
1 Introduction
Dependency Parsing, the task of inferring a depen-
dency structure over an input sentence, has gained
a lot of research attention in the last couple of
years, due in part to to the two CoNLL shared
tasks (Nivre et al, 2007; Buchholz and Marsi,
2006) in which various dependency parsing algo-
rithms were compared on various data sets. As a
result of this research effort, we have a choice of
several robust, efficient and accurate parsing algo-
rithms.
?We would like to thank Reut Tsarfaty for comments and
discussions that helped us improve this paper. This work is
supported in part by the Lynn and William Frankel Center for
Computer Science.
These different parsing systems achieve com-
parable scores, yet produce qualitatively different
parses. Sagae and Lavie (2006) demonstrated that
a simple combination scheme of the outputs of dif-
ferent parsers can obtain substantially improved
accuracies. Nivre and McDonald (2008) explore
a parser stacking approach in which the output of
one parser is fed as an input to a different kind of
parser. The stacking approach also produces more
accurate parses.
However, while we know how to produce accu-
rate parsers and how to blend and stack their out-
puts, little effort was directed toward understand-
ing the behavior of different parsing systems in
terms of structures they produce and errors they
make. Question such as which linguistic phenom-
ena are hard for parser Y? and what kinds of er-
rors are common for parser Z?, as well as the more
ambitious which parsing approach is most suitable
to parse language X?, remain largely unanswered.
The current work aims to fill this gap by propos-
ing a methodology to identify systematic biases in
various parsing models and proposing and initial
analysis of such biases.
McDonald and Nivre (2007) analyze the dif-
ference between graph-based and transition-based
parsers (specifically the MALT and MST parsers)
by comparing the different kinds of errors made by
both parsers. They focus on single edge errors, and
learn that MST is better for longer dependency
arcs while MALT is better on short dependency
arcs, that MALT is better than MST in predict-
ing edges further from the root and vice-versa, that
MALT has a slight advantage when predicting the
parents of nouns and pronouns, and that MST is
better at all other word categories. They also con-
clude that the greedy MALT Parser suffer from er-
ror propagation more than the globally optimized
234
MST Parser.
In what follows, we complement their work by
suggesting a different methodology of analysis of
parsers behaviour. Our methodology is based on
the notion of structural bias of parsers, further ex-
plained in Section 2. Instead of comparing two
parsing systems in terms of the errors they pro-
duce, our analysis compares the output of a pars-
ing system with a collection of gold-parsed trees,
and searches for common structures which are pre-
dicted by the parser more often than they appear in
the gold-trees or vice-versa. These kinds of struc-
tures represent the bias of the parsing systems, and
by analyzing them we can gain important insights
into the strengths, weaknesses and inner working
of the parser.
In Section 2.2 we propose a Boosting-based
algorithm for uncovering these structural biases.
Then, in Section 3 we go on to apply our analysis
methodology to four parsing systems for English:
two transition-based systems and two graph-based
systems (Sections 4 and 5). The analysis shows
that the different parsing systems indeed possess
different biases. Furthermore, the analysis high-
lights the differences and commonalities among
the different parsers, and sheds some more light
on the specific behaviours of each system.
Recent work by Dickinson (2010), published
concurrently with this one, aims to identify depen-
dency errors in automatically parsed corpora by
inspecting grammatical rules which appear in the
automatically parsed corpora and do not fit well
with the grammar learned from a manually anno-
tated treebank. While Dickinson?s main concern is
with automatic identification of errors rather than
characterizing parsers behaviour, we feel that his
work shares many intuitions with this one: auto-
matic parsers fail in predictable ways, those ways
can be analyzed, and this analysis should be car-
ried out on structures which are larger than single
edges, and by inspecting trends rather than indi-
vidual decisions.
2 Structural Bias
Language is a highly structured phenomena, and
sentences exhibit structure on many levels. For
example, in English sentences adjectives appear
before nouns, subjects tend to appear before their
verb, and syntactic trees show a tendency toward
right-branching structures.1
1As noted by (Owen Rambow, 2010), there is little sense
in talking about the structure of a language without referring
Different combinations of languages and anno-
tation strategies exhibit different structural prefer-
ences: under a specific combination of language
and annotation strategy some structures are more
frequent than others, some structures are illegal
and some are very rare.
We argue that parsers also exhibit such struc-
tural preferences in the parses they produce. These
preferences stem from various parser design deci-
sions. Some of the preferences, such as projectiv-
ity, are due to explicit design decisions and lie at
the core of some parsing algorithms. Other pref-
erences are more implicit, and are due to specific
interactions between the parsing mechanism, the
feature function, the statistical mechanism and the
training data.
Ideally, we would like the structural preferences
of a parser trained on a given sample to reflect the
general preferences of the language. However, as
we demonstrate in Section 3, that it is usually not
the case.
We propose the notion of structural bias for
quantifying the differences in structural prefer-
ences between a parsing system and the language
it is aiming to parse. The structural bias of a
parser with respect to a language is composed of
the structures that tend to occur more often in the
parser?s output than in the language, and vice-
versa.
Structural biases are related to but different than
common errors. Parser X makes many PP at-
tachment errors is a claim about a common error.
Parser X tends to produce low attachment for PPs
while the language tends to have high attachment
is a claim about structural bias, which is related to
parser errors. Parser X can never produce struc-
ture Y is a claim about a structural preference of
a parser, which may or may not be related to its
error patterns.
Structural bias is a vast and vague concept. In
order to give a more concrete definition, we pose
the following question:
Assuming we are given two parses of the same
sentence. Can we tell, by looking at the parses and
without knowing the correct parse, which parser
produced which parse?
Any predictor which can help in answering this
question is an indicator of a structural bias.
to a specific annotation scheme. In what follow, we assume a
fixed annotation strategy is chosen.
235
Definition: structural bias between sets of trees
Given two sets of parse trees, A and B, over the
same sentences, a structural bias between these
sets is the collection of all predictors which can
help us decide, for a tree t, whether it belongs to
A or to B.
The structural bias between a parsing system
and an annotated corpus is then the structural bias
between the corpus and the output of the parser
on the sentences in the corpus. Note that this
definition adheres to the error vs. bias distinction
given above.
Under this task-based definition, uncovering
structural biases between two sets of trees amounts
to finding good predictors for discriminating be-
tween parses coming from these two sets of trees.
In what follows, we present a rich class of struc-
tural predictors, and an algorithm for efficiently
searching this predictor class for good predictors.
2.1 Representing Structure
A dependency representation of sentences in-
cludes words and dependency relations between
them (one word is the ROOT of the sentence, and
each other word has a single word as its parent).
Whenever possible, we would like to equate words
with their part-of-speech tags, to facilitate gener-
alization. However, in some cases the exact iden-
tity of the word may be of interest. When ana-
lyzing a language with a relatively fixed word or-
der, such as English, we are also interested in the
linear order between words. This includes the di-
rection between a parent and its dependent (does
the parent appear before or after the dependent in
the sentence?), as well as the order among several
dependents of the same parent. The length of a de-
pendency relation (distance in words between the
parent and dependent) may also be structurally in-
teresting.2
In order to capture this kind of information, we
take a structural element of a dependency tree to
be any connected subtree, coupled with informa-
tion about the incoming edge to the root of the
subtree. Examples of such structural elements are
given in Figure 1. This class of predictors is not
complete ? it does not directly encode, for in-
stance, information about the number of siblings
2Relations can also be labeled, and labeling fit naturally
in our representation. However, we find the commonly used
set of edge labels for English to be lacking, and didn?t include
edge labels in the current analysis.
(a) JJ
3
(b) NN VB IN/with
2
Figure 1: Structural Elements Examples. (a) is an adjective
with a parent 3 words to its right. (b) is a verb whose parent
is on the left, it has a noun dependent on its left, and a prepo-
sition dependent 2 words to its right. The lexical item of the
preposition is with. The lexical items and distance to parent
are optional, while all other information is required. There
is also no information about other dependents a given word
may have.
a node has or the location of the structure relative
to the root of the tree. However, we feel it does
capture a good deal of linguistic phenomena, and
provide a fine balance between expressiveness and
tractability.
The class of predictors we consider is the set of
all structural elements. We seek to find structural
elements which appear in many trees of set A but
in few trees of set B, or vice versa.
2.2 Boosting Algorithm with Subtree
Features
The number of possible predictors is exponential
in the size of each tree, and an exhaustive search is
impractical. Instead, we solve the search problem
using a Boosting algorithm for tree classification
using subtree features. The details of the algo-
rithm and its efficient implementation are given in
(Kudo and Matsumoto, 2004). We briefly describe
the main idea behind the algorithm.
The Boosting algorithm with subtree features
gets as input two parse sets with labeled, ordered
trees. The output of the algorithm is a set of sub-
trees ti and their weights wi. These weighted sub-
trees define a linear classifier over trees f(T ) =
?
ti?T
wi, where f(T ) > 0 for trees in set A and
f(T ) < 0 for trees in set B.
The algorithm works in rounds. Initially, all
input trees are given a uniform weight. At each
round, the algorithm seeks a subtree t with a max-
imum gain, that is the subtree that classifies cor-
rectly the subset of trees with the highest cumu-
lative weight. Then, it re-weights the input trees,
so that misclassified trees get higher weights. It
continues to repeatedly seek maximum gain sub-
trees, taking into account the tree weights in the
gain calculation, and re-weighting the trees after
each iteration. The same subtree can be selected
in different iterations.
Kudo and Matsumoto (2004) present an effec-
236
(a) JJ?
d:3
(b) VB?
NN? IN?
w:with d:2
Figure 2: Encoding Structural Elements as Ordered Trees.
These are the tree encodings of the structural elements in Fig-
ure 1. Direction to parent is encoded in the node name, while
the optional lexical item and distance to parent are encoded
as daughters.
tive branch-and-bound technique for efficiently
searching for the maximum gain tree at each
round. The reader is referred to their paper for the
details.
Structural elements as subtrees The boosting
algorithm works on labeled, ordered trees. Such
trees are different than dependency trees in that
they contain information about nodes, but not
about edges. We use a simple transformation to
encode dependency trees and structural elements
as labeled, ordered trees. The transformation
works by concatenating the edge-to-parent infor-
mation to the node?s label for mandatory informa-
tion, and adding edge-to-parent information as a
special child node for optional information. Figure
2 presents the tree-encoded versions of the struc-
tural elements in Figure 1. We treat the direction-
to-parent and POS tag as required information,
while the distance to parent and lexical item are
optional.
2.3 Structural Bias Predictors
The output of the boosting algorithm is a set of
weighted subtrees. These subtrees are good can-
didates for structural bias predictors. However,
some of the subtrees may be a result of over-fitting
the training data, while the weights are tuned to
be used as part of a linear classifier. In our ap-
plication, we disregard the boosting weights, and
instead rank the predictors based on their number
of occurrences in a validation set. We seek predic-
tors which appear many times in one tree-set but
few times in the other tree-set on both the train-
ing and the validation sets. Manual inspection of
these predictors highlights the structural bias be-
tween the two sets. We demonstrate such an anal-
ysis for several English dependency parsers below.
In addition, the precision of the learned Boost-
ing classifier on the validation set can serve as a
metric for measuring the amount of structural bias
between two sets of parses. A high classification
accuracy means more structural bias between the
two sets, while an accuracy of 50% or lower means
that, at least under our class of predictors, the sets
are structurally indistinguishable.
3 Biases in Dependency Parsers
3.1 Experimental Setup
In what follows, we analyze and compare the
structural biases of 4 parsers, with respect to a de-
pendency representation of English.
Syntactic representation The dependency tree-
bank we use is a conversion of the English WSJ
treebank (Marcus et al, 1993) to dependency
structure using the procedure described in (Jo-
hansson and Nugues, 2007). We use the Mel?c?uk
encoding of coordination structure, in which the
first conjunct is the head of the coordination struc-
ture, the coordinating conjunction depends on the
head, and the second conjunct depend on the coor-
dinating conjunction (Johansson, 2008).
Data Sections 15-18 were used for training the
parsers3. The first 4,000 sentences from sections
10-11 were used to train the Boosting algorithm
and find structural predictors candidates. Sec-
tions 4-7 were used as a validation set for ranking
the structural predictors. In all experiments, we
used the gold-standard POS tags. We binned the
distance-to-parent values to 1,2,3,4-5,6-8 and 9+.
Parsers For graph-based parsers, we used
the projective first-order (MST1) and second-
order (MST2) variants of the freely available
MST parser4 (McDonald et al, 2005; McDon-
ald and Pereira, 2006). For the transition-based
parsers, we used the arc-eager (ARCE) variant of
the freely available MALT parser5 (Nivre et al,
2006), and our own implementation of an arc-
standard parser (ARCS) as described in (Huang et
al., 2009). The unlabeled attachment accuracies of
the four parsers are presented in Table 1.
Procedure For each parser, we train a boosting
classifier to distinguish between the gold-standard
trees and the parses produced for them by the
3Most work on parsing English uses a much larger train-
ing set. We chose to use a smaller set for convenience. Train-
ing the parsers is much faster, and we can get ample test data
without resorting to jackknifing techniques. As can be seen
in Table 1, the resulting parsers are still accurate.
4http://sourceforge.net/projects/mstparser/
5http://maltparser.org/
237
MST1 MST2 ARCE ARCS
88.8 89.8 87.6 87.4
Table 1: Unlabeled accuracies of the analyzed parsers
Parser Train Accuracy Val Accuracy
MST1 65.4 57.8
MST2 62.8 56.6
ARCE 69.2 65.3
ARCS 65.1 60.1
Table 2: Distinguishing parser output from gold-trees based
on structural information
parser. We remove from the training and valida-
tion sets all the sentences which the parser got
100% correct. We then apply the models to the
validation set. We rank the learned predictors
based on their appearances in gold- and parser-
produced trees in the train and validation sets, and
inspect the highest ranking predictors.
Training the boosting algorithm was done us-
ing the bact6 toolkit. We ran 400 iterations of
boosting, resulting in between 100 and 250 dis-
tinct subtrees in each model. Of these, the top 40
to 60 ranked subtrees in each model were good in-
dicators of structural bias. Our wrapping code is
available online7 in order to ease the application
of the method to other parsers and languages.
3.2 Quantitative Analysis
We begin by comparing the accuracies of the
boosting models trained to distinguish the pars-
ing results of the various parsers from the English
treebank. Table 2 lists the accuracies on both the
training and validation sets.
The boosting method is effective in finding
structural predictors. All parsers output is dis-
tinguishable from English trees based on struc-
tural information alone. The ArcEager variant of
MALT is the most biased with respect to English.
The transition-based parsers are more structurally
biased than the graph-based ones.
We now turn to analyze the specific structural
biases of the parsing systems. For each system
we present some prominent structures which are
under-produced by the system (these structures
appear in the language more often than they are
produce by the parser) and some structures which
are over-produced by the system (these structures
6http://chasen.org/?taku/software/bact/
7http://www.cs.bgu.ac.il/?yoavg/software/
are produced by the parser more often than they
appear in the language).8 Specifically, we manu-
ally inspected the predictors where the ratio be-
tween language and parser was high, ranked by
absolute number of occurrences.
4 Transition-based Parsers
We analyze two transition-based parsers (Nivre,
2008). The parsers differ in the transition sys-
tems they adopt. The ARCE system makes
use of a transition system with four transitions:
LEFT,RIGHT,SHIFT,REDUCE. The semantics of
this transition system is described in (Nivre,
2004). The ARCS system adopts an alterna-
tive transition system, with three transitions: AT-
TACHL,ATTACHR,SHIFT. The semantics of the
system is described in (Huang et al, 2009). The
main difference between the systems is that the
ARCE system makes attachments as early as pos-
sible, while the ARCS system should not attach a
parent to its dependent until the dependent has ac-
quired all its own dependents.
4.1 Biases of the Arc-Eager System
Over-produced structures The over-produced
structures of ARCE with respect to English are
overwhelmingly dominated by spurious ROOT at-
tachments.
The structures ROOT?? , ROOT?DT,
ROOT?WP are produced almost 300 times by
the parser, yet never appear in the language. The
structures ROOT?? , ROOT?WRB , ROOT?JJ
appear 14 times in the language and are produced
hundreds of time by the parser. Another interest-
ing case is ROOT ??9+ NN , produced 180 times by
the parser and appearing 7 times in the language.
As indicated by the distance marking (9+), nouns
are allowed to be heads of sentences, but then they
usually appear close to the beginning, a fact which
is not captured by the parsing system. Other, less
clear-cut cases, are ROOT as the parent of IN,
NN, NNS or NNP. Such structures do appear in
the language, but are 2-5 times more common in
the parser.
A different ROOT attachment bias is captured
by
ROOT VBZ VBD and ROOT VBD VBD ,
8One can think of over- and under- produced structures
in terms of the precision and recall metrics: over-produced
structures have low precision, while under-produced struc-
tures have low recall.
238
appearing 3 times in the language and produced
over a 100 times by the parser.
It is well known that the ROOT attachment ac-
curacies of transition-based systems is lower than
that of graph-based system. Now we can refine
this observation: the ARCE parsing system fails
to capture the fact that some categories are more
likely to be attached to ROOT than others. It also
fails to capture the constraint that sentences usu-
ally have only one main verb.
Another related class of biases are captured by
the structures?VBD ??9+ VBD,?VBD ??5?7 VBD
and ROOT?VBZ?VBZ which are produced by
the parser twice as many times as they appear
in the language. When confronted with embed-
ded sentences, the parser has a strong tendency of
marking the first verb as the head of the second
one.
The pattern ??+9 IN suggests that the parser
prefers high attachment for PPs. The pattern
DT?NN
9+
captures the bias of the parser
toward associating NPs with the preceding verb
rather than the next one, even if this preceding verb
is far away.
Under-produced structures We now turn to
ARCE?s under-produced structures. These include
the structures IN/that? , MD? , VBD? (each 4
times more frequent in the language than in the
parser) and VBP? (twice more frequent in the
language). MD and that usually have their par-
ents to the left. However, in some constructions
this is not the case, and the parser has a hard time
learning these constructions.
The structure ?$?RB appearing 20 times in
the language and 4 times in the parser, reflects a
very specific construction (?$ 1.5 up from $ 1.2?).
These constructions pop up as under-produced by
all the parsers we analyze.
The structures ??1 RB?IN and ?RB?JJ ap-
pear twice as often in the language. These
stem from constructions such as ?not/RB unex-
pected/JJ?, ?backed away/RB from/IN?, ?pushed
back/RB in/IN?, and are hard for the parser.
Lastly, the structure JJ?NN?NNS?, deviates
from the the ?standard? NP construction, and is
somewhat hard for the parser (39 times parser, 67
in language). However, we will see below that this
same construction is even harder for other parsers.
4.2 Biases of the Arc-Standard System
Over-produced structures The over-produced
structures of ARCS do not show the spurious
ROOT attachment ambiguity of ARCE. They do
include ROOT?IN, appearing twice as often in
the parser output than in the language.
The patterns ROOT?VBZ??9+, , ?VBP??9+,
, ?VBD??9+VBD and ?VB?VBD all reflect
the parser?s tendency for right-branching struc-
ture, and its inability to capture the verb-hierarchy
in the sentence correctly, with a clear preference
for earlier verbs as parents of later verbs.
Similarly, ??9+NNP and ??9+NNS indicate a ten-
dency to attach NPs to a parent on their left (as an
object) rather than to their right (as a subject) even
when the left candidate-parent is far away.
Finally, WRB MD VB , produced
48 times by the parser and twice by the language,
is the projective parser?s way of annotating the
correct non-projective structure in which the wh-
adverb is dependent on the verb.
Under-produced structures of ARCS in-
clude two structures WRB VBN and
WRB VB , which are usually part of
non-projective structures, and are thus almost
never produced by the projective parser.
Other under-produced structures include appos-
itive NPs:
? IN NN ? ?
(e.g., ?by Merill , the nation?s largest firm , ?), and
the structure NN DT NN , which can
stand for apposition (?a journalist, the first jour-
nalist to . . . ?) or phrases such as ?30 %/NN a
month?.
TO usually has its parent on its left. When this
is not the case (when it is a part of a quantifier,
such as ?x to y %?, or due to fronting: ?Due to
X, we did Y?), the parser is having a hard time to
adapt and is under-producing this structure.
Similar to the other parsers, ARCS also under-
produces NPs with the structure JJ?NN??1 NN,
and the structure?$?RB.
Finally, the parser under-produces the con-
junctive structures ?NN?CC?NN?IN and
?IN?CC?IN.
239
5 Graph-based Parsers
We analyze the behaviour of two graph-based
parsers (McDonald, 2006). Both parsers perform
exhaustive search over all projective parse trees,
using a dynamic programming algorithm. They
differ in the factorizations they employ to make
the search tractable. The first-order model em-
ploys a single-edge factorization, in which each
edge is scored independently of all other edges.
The second-order model employs a two-edge fac-
torization, in which scores are assigned to pairs
of adjacent edges rather than to a single edge at a
time.
5.1 Biases of First-order MST Parser
Over-produced structures of MST1 include:
? IN NN NN ? IN NNP NN
? IN NNP NNS ? IN NN VBZ/D
where the parsers fails to capture the fact
that prepositions only have one dependent.
Similarly, in the pattern: ?CC NN NNS
the parser fails to capture that only one phrase
should attach to the coordinator, and the patterns
NN NN VBZ NNS NNS VBP
highlight the parser?s failing to capture that
verbs have only one object.
In the structure ROOT WRB VBD , pro-
duced by the parser 15 times more than it appears
in the language, the parser fails to capture the fact
that verbs modified by wh-adverbs are not likely
to head a sentence.
All of these over-produced structures are fine
examples of cases where MST1 fails due to its
edge-factorization assumption.
We now turn to analyzing the structures under-
produced by MST1.
Under-produced structures The non-
projective structures
WRB VBN
1
and WRB VB
1
clearly cannot be produced by the projective
parser, yet they appear over 100 times in the
language.
The structure WRB?VBD?VBD which is
represented in the language five times more than
in the parser, complements the over-produced case
in which a verb modified by a wh-adverb heads the
sentence.
IN/that?, which was under-produced by
ARCE is under-produced here also, but less so
than in ARCE. ?$?RB is also under-produced
by the parser.
The structure CC??1 , usually due to conjunc-
tions such as either, nor, but is produced 29 times
by the parser and appear 54 times in the language.
An interesting under-produced structure is
?NN IN CC NN . This structure reflects
the fact that the parser is having a hard time coor-
dinating ?heavy? NPs, where the head nouns are
modified by PPs. This bias is probably a result
of the ?in-between pos-tag? feature, which lists
all the pos-tags between the head and dependent.
This feature was shown to be important to the
parser?s overall performance, but probably fails it
in this case.
The construction ??6?8JJ, where the adjective
functions as an adverb (e.g., ?he survived X
unscathed? or ?to impose Y corporate-wise?)
is also under-produced by the parser, as well
as IN NN in which the preposition
functions as a determiner/quantifier (?at least?,
?between?, ?more than?).
Finally, MST1 is under-producing NPs with
somewhat ?irregular? structures: JJ?NN?NNS
or JJ?NN?NNS (?common stock purchase war-
rants?, ?cardiac bypass patients?), or JJ?JJ? (?a
good many short-sellers?, ?West German insur-
ance giant?)
5.2 Biases of Second-order MST Parser
Over-produced structures by MST2 are differ-
ent than those of MST1. The less-extreme edge
factorization of the second-order parser success-
fully prevents the structures where a verb has two
objects or a preposition has two dependents.
One over-produced structure,
NNS JJ NNP ? ?
, produced
10 times by the parser and never in the language,
is due to one very specific construction, ?bonds
due Nov 30 , 1992 ,? where the second comma
should attach higher up the tree.
240
Another over-produced structure involves
the internal structure of proper names:
NNP NNP NNP NNP (the ?correct? analysis
more often makes the last NNP head of all the
others).
More interesting are: ??1 CC?VBD and
??1 CC?NN?IN . These capture the parser?s in-
ability to capture the symmetry of coordinating
conjunctions.
Under-produced structures of MST2 are over-
all very similar to the under-produced structures of
MST1.
The structure CC??1 which is under-produced by
MST1 is no longer under-produced by MST2. All
the other under-produced structures of MST1 reap-
pear here as well.
In addition, MST2 under-produces the struc-
tures ROOT?NNP?. (it tends not to trust NNPs
as the head of sentences) and??6?8TO??1 V B (where
the parser is having trouble attaching TO correctly
to its parent when they are separated by a lot of
sentential material).
6 Discussion
We showed that each of the four parsing systems
is structurally biased with respect to the English
training corpus in a noticeable way: we were able
to learn a classifier that can tell, based on structural
evidence, if a parse came from a parsing system
or from the training corpus, with various success
rates. More importantly, the classifier?s models are
interpretable. By analyzing the predictors induced
by the classifier for each parsing system, we un-
covered some of the biases of these systems.
Some of these biases (e.g., that transition-based
system have lower ROOT-attachment accuracies)
were already known. Yet, our analysis refines this
knowledge and demonstrates that in the Arc-Eager
system a large part of this inaccuracy is not due
to finding the incorrect root among valid ambigu-
ous candidates, but rather due to many illegal root
attachments, or due to illegal structures where a
sentence is analyzed to have two main verbs. In
contrast, the Arc-Standard system does not share
this spurious root attachment behaviour, and its
low root accuracies are due to incorrectly choos-
ing among the valid candidates. A related bias of
the Arc-Standard system is its tendency to choose
earlier appearing verbs as parents of later occur-
ring verbs.
Some constructions were hard for all the parsing
models. For example, While not discussed in the
analysis above, all parsers had biased structures
containing discourse level punctuation elements
(some commas, quotes and dashes) ? we strongly
believe parsing systems could benefit from special
treatment of such markers.
The NP construction (JJ?NN?NNS?) ap-
peared in the analyses of all the parsers, yet were
easier for the transition-based parsers than for the
graph-based ones. Other NP constructions (dis-
cussed above) were hard only for the graph-based
parsers.
One specific construction involving the dollar
sign and an adverb appeared in all the parsers,
and may deserve a special treatment. Simi-
larly, different parsers have different ?soft spots?
(e.g., ?backed away from?, ?not unexpected? for
ARCE, ?at least? for MST1, TO? for ARCS, etc.)
which may also benefit from special treatments.
It is well known that the first-order edge-
factorization of the MST1 parser is too strong.
Our analysis reveals some specific cases where
this assumptions indeed breaks down. These
cases do not appear in the second-order factoriza-
tion. Yet we show that the second-order model
under-produces the same structures as the first-
order model, and that both models have specific
problems in dealing with coordination structures,
specifically coordination of NPs containing PPs.
We hypothesize that this bias is due to the ?pos-in-
between? features used in the MST Parser.
Regarding coordination, the analysis reveals
that different parsers show different biases with re-
spect to coordination structures.
7 Conclusions and Future Work
We presented the notion of structural bias ? spe-
cific structures that are systematically over- or
under- represented in one set of parse trees relative
to another set of parse trees ? and argue that differ-
ent parsing systems exhibit different structural bi-
ases in the parses they produced due to various ex-
plicit and implicit decisions in parser design. We
presented a method for uncovering some of this
structural bias, and effectively used it to demon-
strate that parsers are indeed biased with respect
to the corpus they are trained on, and that differ-
ent parsers show different biases. We then ana-
lyzed the biases of four dependency parsing sys-
tems with respect to an English treebank. We ar-
241
gue that by studying the structural biases of pars-
ing systems we can gain a better understanding on
where dependency parsers fail, and how they dif-
fer from each other. This understanding can in turn
lead us toward designing better parsing systems.
We feel that the current study is just the tip of
the iceberg with respect to the analysis of struc-
tural bias. Any parsing system for any language
and annotation scheme can benefit from such anal-
ysis.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In Proc.
of ACL.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc of EMNLP.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proc of NODALIDA.
Richard Johansson. 2008. Dependency-based Seman-
tic Analysis of Natural-language Text. Ph.D. thesis,
Lund University.
Taku Kudo and Yuji Matsumoto. 2004. A Boost-
ing Algorithm for Classification of Semi-Structured
Text. In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marchinkiewicz. 1993. Building a large annotated
corpus of English: The penn treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proc. of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc of EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc of ACL.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL, pages 950?958.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of EMNLP-CoNLL.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Incremental Parsing:
Bringing Engineering and Cognition Together, ACL-
Workshop.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4), December.
Owen Rambow. 2010. The Simple Truth about De-
pendency and Phrase Structure Representations: An
Opinion Piece. In Proceedings of NAACL.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT-NAACL,
pages 129?133.
242
Proceedings of the 2012 Student Research Workshop, pages 43?48,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Domain Adaptation of a Dependency Parser with a Class-Class Selectional Preference Model  
 Abstract When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain.  To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast non-directional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline. 1 Introduction Dependency parsing captures a useful representation of syntactic structure for information extraction. For example, the Stanford Dependency representation has been used extensively in domain-specific relation extraction tasks such as BioNLP09 (Kim, Ohta et al 2009) and BioNLP11 (Pyysalo, Ohta et al 2011). One obstacle to widespread adoption of such syntactic representations is that parsers are generally trained on a specific domain (typically WSJ news data) and it has often been observed that the accuracy of dependency parsers drops significantly when used in a domain other than the training domain.  
Domain adaptation for dependency parsing has been explored extensively in the CoNLL 2007 Shared Task (Nivre, Hall et al 2007). The objective in this task is to adapt an existing parser from a source domain in order to achieve high parsing accuracy on a target domain in which no annotated data is available. Common approaches include self-training (McClosky, Charniak et al 2006), using word distribution features (Koo, Carreras et al 2008) and co-training (Sagae and Tsujii 2007) . Dredze et al (Dredze, Blitzer et al 2007) explored a variety of methods for domain adaptation, which consistently showed little improvement and concluded that domain adaptation for dependency parsing is indeed a hard task. Typically, parsing accuracy drops from 90+% in-domain to 80-84% in the target domain. When porting parsers to the target domain, many of the errors are related to wrong attachment of out-of-vocabulary words, i.e., words which were not observed when training on the source domain. Since there is not sufficient annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes.  Selectional preferences (SP) describe the relative affinity of arguments and head of a syntactic relation. For example, in the sentence: ?D3 activates receptors in blood cells from patients?, the preposition ?from? may be attached to either ?cells? or ?receptors?. However, the head word ?cells? has greater affinity to ?patients? than the candidate ?receptors? would have towards "patients". Note that this preference is highly context-specific. Several methods for learning SP (not in the context of domain adaptation) have been proposed. Commonly, these methods rely on learning semantic classes for arguments and learning the preference of a predicate to a semantic class. These semantic classes may be derived from manual knowledge bases such as WordNet or FrameNet, or semantic classes learned from large corpora. Recently, Ritter et al (2010) and 
Raphael Cohen* Yoav Goldberg** Michael Elhadad Ben Gurion University of the Negev Department of Computer Science POB 653 Be?er Sheva, 84105, Israel {cohenrap,yoavg,elhadad}@cs.bgu.ac.il 
?Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University **Current affiliation: Google Inc. 
43
S?aghdha (2010) both present induction methods of SP of verb-arguments using LDA (Blei, Ng et al 2003). Hartung and Frank (2011) extended the LDA-based approach to learning preference for adjective-noun phrases.  In this work, we tackle the task of domain adaptation by developing a domain-specific SP model. Our initial observation is that parsers fail on the target domain when trying to attach domain-specific words not seen during training. We observe as many as 15% of the words are unknown when applying a WSJ-trained parser on Genia and PennBioIE data, compared to only 2.5% in-domain. Parsers trained on the source domain cannot learn attachment preferences for such words. Our motivation is, therefore, to attempt to learn attachment preferences for domain specific words using un-annotated data. Specifically, we focus on acquiring a domain-specific SP model.  Our approach consists of using the low-accuracy source-domain parser on large quantities of in-domain sentences. We extract from the resulting parse trees a collection of syntactically related pairs of words. We then train an LDA model over these pairs of words and derive a domain-specific model of lexical affinities between pairs of words.  We finally re-train a parser model to exploit this domain-specific data.  To this end, we use the approach of co-training, which consists of identifying reliable parse trees in the target domain in an unsupervised manner using an ensemble of two distinct parsers, and extending the annotated training set with these reliable parse trees. Co-training alone significantly reduces the proportion of unknown words in the re-trained parser ? in the extended co-training dataset, we observe that the unknown words rate drops from 15% to 4.5%. Data sparseness, however, remains an issue: 1/3 of the domain-specific words added to the model by co-training appear only once in the extended training set, and we observe that many of the attachment errors are concentrated in a few syntactic configurations (e.g., head(V or N)-prep-pobj, N-N or head(N)-Adj).  We extend co-training by introducing our SP model, which is class-based and specific to these difficult syntactic configurations. Our method reduces error in the Genia Treebank (Tateisi, Yakushiji et al 2005) by 3.5% over co-training. Introducing additional distributional lexical features (Brown clusters learned in-domain), further reduces error to a total 4.5% reduction. Overall, our parser achieves an accuracy of 83.6% UAS on the Genia domain without annotated data in this domain. 
2 Our Approach To understand the difficulty of domain adaptation, we applied our parser trained on the WSJ news domain to the Genia and measured observed errors.  Most of the errors were found in a small set of syntactic configurations: verb-prep-noun, noun-adjective, noun-noun (together these relations make up 32 % of the errors).  For example: in ?nuclear factor-kappa-B DNA-binding activity? the parser chooses ?factor-kappa-B? as the head of ?nuclear? instead of ?activity?. We observe that these errors involve domain-specific vocabulary, and are difficult to disambiguate for non-expert humans as well. Accordingly, we try to acquire a domain-specific model of word-pairs affinities.  Our parsing model (EasyFirst) allows us to use such bi-lexical features in an efficient manner.  Because of data sparseness, however, we aim to acquire class-based features, and decide to model these lexical preferences using the LDA approach. Our method proceeds in two stages: 1. Learn selectional preferences from an automatically parsed corpus using LDA on selected syntactic configurations 2. Integrate the preferences into the parsing model as new features using co-training. 2.1 Learning Selectional Preferences Following (Ritter, Mausam et al 2010) and (S?aghdha 2010), we model lexical affinity between words in specific syntactic configurations using LDA.  Traditionally, LDA learns a set of "topics" from observed documents, based on observed word co-occurrences. In our case, we form artificial documents, which we call syntactic contexts, by collecting head-daughter pairs from parse trees. A syntactic context is constructed for each head word, which contains the related words to which it was found attached.  In the collection process, we identify two syntactic configurations that yield high error rates: head-prep-noun and noun-adj. We collect two types of syntactic contexts: the preposition contexts contain the set of nouns related to the head through any preposition and the adjective contexts contain the set of adjectives directly related to the head noun. We then learn an LDA model on each of these contexts collections.  We use Mallet (McCallum 2002) to learn topic models with hyper-parameter optimization(Wallach, Mimno et al 2009). The optimal number of topics is selected empirically based on model fit to held-out data. 
44
The resulting topics represent latent semantic classes of the daughter words. We define a measure of shared affinity between a head word h and a candidate daughter word d (in a given configuration) s: ??????? ?, ? =  ? ?(?|?) ? ?(?|?) ? ??? ? ???  where P(c|h) is the predicted probability of topic c given the syntactic context associated to head word h. That is, when we apply the LDA model on the syntactic context of h, we assign topics to each of the associated daughter words and count their proportion. Note that this affinity measure may predict a non-zero affinity to a pair (h, d) even though this word pair has never been observed. The result is a class-class SP model with reduced dimensionality compared to word-word models based for example on PMI.  Table 1 lists examples of learned topics. Note that these topics are high-quality semantic clusters that reflect domain semantics, with marked differences between the news and bio-medical domains. 2.2  Co-training to exploit domain features At this stage, we have acquired a domain-specific model of word affinity that exploits semantic classes and depends on specific syntactic configurations (head-prep-obj and noun-adj).  We now attempt to exploit this model to adapt our source parser to the target domain.  To this end, we want to re-train the parser using new features based on the SP model in addition to the original features.  We use the framework of co-training to achieve this goal (Sagae 
and Tsujii 2007): we use two different parsers: Easy-First (Goldberg and Elhadad 2010) and MALT (Nivre, Hall et al 2006) trained on the same WSJ source domain. We apply these two parsers on a large set of target-domain sentences. We select those sentences where the 2 parsers agree (produce identical trees) and add them to the original source-domain training set. We thus obtain an extended training set with many in-domain samples. We can now re-train the parser using the new SP features. 2.3 SP as features for the Easy First parser We use the deterministic non-directional Easy-First parser for re-training. This parser incrementally adds edges between words starting with the easier decisions before continuing to difficult ones. Simple structures are first created and their information is available when deciding how to connect complex ones. Easy-First operates in ?(?????) time compared to ?(??) of graph-based parsers such as MST (McDonald, Pereira et al 2005). As a baseline we use the features provided in the Easy-First distribution. We extend these features with pair-wise affinity measures based on our SP model. The affinity measure ranges from 0 to 1. We bin this measure into (low, medium, high, very-high) binary features. When attaching a preposition to its parent, we add one more feature: the affinity of the head candidate with the preposition's daughter (the pobj).  In addition to these pair-wise features, we also 
Source Relation Type Semantic Class Arguments Predicates BLLIP Arg?? Prep ?? Predicate Show Business   actors clips soundtrack genre taping characters roles immortalized starred costumes premise screening featured performances poster trumpeted star retrospective clip script  
film show movie films movies shows television series stage theater program production version music hollywood broadway 
BLLIP Arg?? Prep ?? Predicate Sports quarterbacks starters pitcher pitchers quarterback coaching receiver linebackers cornerback outfielder baseman fullback  team game league teams games time field players years baseball year rules nfl seasons level player leagues nba club history school state BLLIP Arg?? Prep ?? Predicate Work Position jockeying groom groomed relegate relieved unwinding jockeyed selected selecting appointing disqualify named  job post position draft positions candidate team one jobs which role posts successor Genia Arg?? Prep ?? Predicate Cell-cycle process stages stage process steps committed block regulator acquire switch points needed directs determinant il-21 proceeds arrest regulators relate d3  
differentiation development activation  maturation cycle hematopoiesis infection commitment lymphopoiesis stage lineage selection erythropoiesis cascade Genia Arg?? Prep ?? Predicate Cells and growing conditions supernatants co-culture co-cultured replication medium surface chemotaxis supernatant beta migration cocultured cultures hyporesponsiveness  
cell monocyte lymphocyte pbmc macrophage line blood neutrophil cd dc leukocyte t eosinophil fibroblast platelet keratinocyte Genia Adjective?? Noun Protein activity and regulation factor-induced tnfalpha-induced agonist-induced thrombin-induced il-2-induced factor-alpha-induced il-1beta-induced cd40-induced rankl-induced augmented il-4-induced  
expression activation production phosphorylation response proliferation activity binding secretion apoptosis differentiation translocation release signaling adhesion synthesis generation Table 1 High affinity classes in the Class-Class Selectional Preferences model extracted with LDA. Classes 1-5 are from preposition head/object pairs (e.g ?groomed for position? fits the third topic) and class 6 are adjective modifier pairs. Classes 1-3 are from Bllip (un-annotated WSJ corpus) (Charniak, Blaheta et al 2000) while classes 4-6 are from a corpus composed of Medline abstracts from the Genia (see section 5.1). Class 4 contains arguments and predicates concerning cell-cycle process. In class 5 arguments are cell growing conditions and predicates are types of cells. 
45
introduce features that correspond to the latent topic class of the words according to each of the 2 acquired LDA models (this introduces one binary feature for each topic).  These latent semantic class features are similar in nature to distributional lexical features as used in (Koo, Carreras et al 2008). The EasyFirst parser combines partial trees bottom-up. When deciding whether to attach the partial tree "from patients" to either "cells" or "receptors", we compute the affinities of "cells/patients" and "receptors/patients". Our model produces features indicating medium affinity for ?receptors from patients? and a high affinity for cells from patients?. 3 Experiments and Evaluation 3.1 Genia Treebank The Genia Treebank (Tateisi, Yakushiji et al 2005) contains 18K sentences from the biomedical domain, transformed into dependency trees 1  using (De Marneffe, MacCartney et al 2006) 2 . The corpus contains 2.3K sentences longer than 40 tokens that were excluded from the evaluation. The treebank was divided into test and development sets of equal size.  We created an un-annotated corpus of 200K sentences by querying Medline with the same query terms used to create Genia. We used the Genia POS Tagger on this dataset (Tsuruoka, Tateishi et al 2005). The corpus was parsed with Easy-First and MALT (arc-eager, polynomial) to create co-training data, yielding 21K sentences with 100% agreement. The parsed corpus of 200K sentences was used to produce selectional preference models for adjective-nouns, with 200 topics, and for head-prep-object with 300 topics. We used word lemmas for each pair when preparing syntactic contexts for LDA training (see Table 2).  Relation #  Pairs # Daughter # Heads preposition 360,041 1,727 2,391 adjective 384,347 1,570 2,003  Table 2. Statistics for the training data of the SP model. 3.2 Coverage Many of the features learned in training a parser are lexicalized; this is an important factor in the drop in accuracy when parsing in a new domain.  To understand the nature of the contribution of the features learned by our SP model, we calculated the coverage of the features acquired in two unsupervised methods: Brown clustering and our SP classes. We                                                       1 We use the PTB version of Genia created by Illes Solt. 2 We convert using the Stanford Parser bundle. 
count the number of tokens in the Treebank which gain a feature at training time (we ignore punctuation, coordination and preposition tokens). Our SP model covers 53% of the tokens in the test set. Brown clusters calculated with the implementation of Liang (2005) achieve coverage of 73%.  Brown clusters features are also class-based distributional features based on n-gram language models, but do not take into account syntactic configurations. 3.3 Adaptation Evaluation We use a number of baselines for the adaptation task. Three parsers were evaluated on the target domain: Easy-First, MST second order and MALT arc-eager with a polynomial kernel. We report UAS scores of trees of length < 40 without punctuation. The first baseline setting for each parser is the model trained on WSJ sections 2-21.  The second baseline we report is co-training using WSJ 2-21 combined with the 21K full agreement parse trees extracted from Medline, but without new features. Parser Training Data Features UAS (Exact Match)  MST WSJ 2-21  79.6 (10)  MALT WSJ 2-21  81.1 (16.6)  Easy-First WSJ 2-21  80.5 (12.3)  MST Co-Training  81.3 (14.1)  MALT Co-Training  82.1 (16.5)  Easy-First Co-Training  82.8 (16.2)  Easy-First Co-Training +Brown Clusters 83.1 (17) +0.3 Easy-First Co-Training +SP-Lexicalized 83.0 (16.9) +0.2 Easy-First Co-Training +SP-Lexicalized +SP-Classes 83.4 (16.6) +0.6 Easy-First Co-Training +SP-Lexicalized +SP-Classes +Brown Clusters 83.6 (17.2) +0.8 Easy-First GeniaTB Dev  89.8 (28.6)  Table 3. Accuracy for different parser settings on Genia test set.  The best performing adapted model trains with co-training data and combines SP and Brown clusters as features.  In Table 3, we see that the combined SP-Features improved the co-training baseline by 0.6%, a significant error reduction of 3.5% (p-value < 0.01).  We list improvement when introducing only pair-wise SP features, and when adding SP-based semantic classes. The effect is also additive with the Brown clusters features, producing an improvement of 0.8% when combined (error reduction of 4.5%). To evaluate the model adapted for Genia on the general biomedical domain, we used the PennBioIE Treebank . This dataset contains 6K sentences from different biomedical domains. We compared 3 models (see Table 4):  1. Easy-First, MALT and MST trained on WSJ. 2. Easy-First with co-training on Genia. 
46
3. Easy-First with co-training on Genia with Selectional Preference features. Domain adaptation to Genia carried over to the closely related PennBioIE dataset, demonstrating the generalization capability of the method. Parser Training Data Features UAS  MALT WSJ 2-21  78.8  MST WSJ 2-21  81.4  Easy-First WSJ 2-21  79.8  Easy-First Co-Training  81.9  Easy-First Co-Training +SP-Lexicalized +SP-Classes +Brown Clusters 82.2 +0.3 Table 4. Accuracy of parsers on PennBioIE Treebank.  3.4 Error Analysis We compare the parser using the SP pair-wise features for preposition attachment to the co-trained baseline on Genia. The overall accuracy of the parser is improved by 0.2%. However, the two models agree only on 90% of the edges, indicating the new SP features play a very active role when parsing. For ?E3330 inhibited this induced promoter activity in a dose-dependent manner?, the co-trained parser chose ?activity? as the head of ?in? instead of ?inhibited?. The affinity feature in our model for (?inhibited?, ?manner?) shows affinity of high (40-60%) compared to low (5-20%) for the wrong pair ("activity", "manner"). The same change occurs for ?LysoPC attenuates activation during inflammation and athero-sclerosis?, where the improved model prefers the pair (?attenuates?, ?inflammation?) to the pair (?activation?, ?inflammation?) which was chosen by the co-trained model. The modest overall improvement is due to errors introduced by the new model. In ?Tissue obtained from ectopic pregnancies may identify the mechanism of trophoblast invasion in ectopic pregnancies?, the correct governor of ?in? is ?invasion?. However, the SP model ranks the affinity of (?invasion?, ?pregnancies?) lower than that of (?mechanism?, ?pregnancies?). Most of the improvement of the full SP model (+0.6%) comes from an improvement in the N-N relation from 83% to 84.9% (11% error reduction), this improvement is due to semantic classes features learned on the relations of noun-adjective and head-prep-pobj. 3.5 Effect on NER Since most of the improvement comes from the N-N relation, we expect improvement for downstream applications such as Named Entity Recognition, a basic task frequently used in the biomedical domain. 
We use the portion of the Genia Treebank covered by the Genia NER corpus (Kim, Ohta et al 2004). We expect the inner tokens of a named entity to be connected by relation of N-N or N-Adj. We evaluate the accuracy of these two relations for NE tokens. The Easy-First with co-training baseline produces accuracy of 82.9% on this specific set of relations, improved by the SP model to 84.4%, a reduction in error of 8.7%. 4 Related Work 4.1 Learning of Selectional Preference Preference of predicate-argument pairs has been studied in depth with a number of approaches. Resnik (1993) suggested a class-based model for preference of predicates combining WordNet classes with mutual information techniques for associating an argument with a predicate class from WordNet.  Another approach models words in a corpus as context vectors (Erk and Pado 2008; Turney and Pantel 2010) for discovering predicate or argument classes using large corpora or the Web. Recently, semantic classes were successfully induced using LDA topic modeling. These methods have shown success in modeling verb argument relationship to a single predicate (Ritter, Mausam et al 2010) or a predicate pair (S?aghdha 2010), as well as for adjective-noun preference (Hartung and Frank 2011).  4.2 Learning SP for improving dependency parsing  The argument-predicate choice learned in SP is directly related to the decision of creating an edge between them in a parse tree. Van Noord (2007) modeled verb-noun preferences using pointwise mutual information (PMI) using an automatically parsed corpus in Dutch. Association scores of pairs were added as features improving the accuracy significantly from 87.4% to 87.9%.  Nakov and Hearst (Nakov and Hearst 2005) focused on resolving PP attachments and coordination. They used co-occurrence counts from web queries in order to estimate selectional restrictions. Zhou et al (2011) used N-gram counts from Google search and Google V1 to deduce word-word attachment preferences. They used these counts in a pair-wise mutual information (PMI) scheme as features for improving parsing in the News domain (WSJ) and adaptation for biomedical domain. Their evaluation showed improvement of 1% on WSJ 
47
section 23 over the vanilla MST parser and a significant increase in the domain adaptation problem.  4.3 Domain adaptation of dependency parsing Domain adaptation for dependency parsing has been studied mostly in regard to the CoNLL 2007 shared task (Nivre, Hall et al 2007). Both of the leading methods included learning from a parser ensemble. Attardi et al?s (2007) used a weak parser in order to identify common parsing errors and overcome those in the training of a stronger parser. Sagae and Tsujii (2007) used two different parsers to parse un-annotated in-domain data and used the trees where the two parsers agreed to augment the training corpus.  Dredze et al (2007) approached the ?closed? problem, i.e., without using additional un-annotated data. They used the PennBioIE Treebank and applied a number of adaptation techniques: (1) features concerning NPs such as chunking information and frequency; (2) word distribution features; (3) features encoding information from diverse parsers; (4) target focused learning ? giving greater weight in training to sentences which are more likely in a target domain language model.  These methods have not improved accuracy over the baseline of the MST parser (McDonald, Pereira et al 2005) trained on WSJ.  5 Conclusion Learning class-class selectional preferences from a large in-domain corpus assists dependency parsing significantly. We have suggested a method for learning selectional preference classes for a specific domain using an existing parser and a standard implementation of LDA topic modeling. The SP model can be used for estimating the affinity between a pair of tokens or simply as a feature of semantic class association. This approach is faster when querying the model for the affinity of a pair of words than a PMI model suggested by Zhou et al(2011). While covering fewer tokens in the target test set than Brown clusters, the method achieved a higher improvement of parsing performance. Furthermore, some of the improvement was additive and reduced UAS error by 4.5% compared to a strong co-training baseline. 6 References  Attardi, G., F. Dell?Orletta, et al (2007). Multilingual dependency parsing and domain adaptation using DeSR. ACL. Blei, D. M., A. Y. Ng, et al (2003). "Latent dirichlet alocation." JMLR 3: 993-1022. Charniak, E., D. Blaheta, et al (2000). "Bllip 1987-89 wsj corpus release 1." LDC. 
De Marneffe, M. C., B. MacCartney, et al (2006). Generating typed dependency parses from phrase structure parses. LREC. Dredze, M., J. Blitzer, et al (2007). Frustratingly hard domain adaptation for dependency parsing. CoNLL 2007. Erk, K. and S. Pado (2008). A structured vector space model for word meaning in context. EMNLP 2008: 897-906. Goldberg, Y. and M. Elhadad (2010). An efficient algorithm for easy-first non-directional dependency parsing. NAACL 2010: 742-750. Hartung, M. and A. Frank (2011). Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases. ACL. Kim, J.-D., T. Ohta, et al (2009). Overview of BioNLP'09 shared task on event extraction. Current Trends in Biomedical NLP, ACL: 1-9. Kim, J.-D., T. Ohta, et al (2004). Introduction to the bio-entity recognition task at JNLPBA. Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Geneva, Switzerland, Association for Computational Linguistics: 70-75. Koo, T., X. Carreras, et al (2008). Simple semi-supervised dependency parsing. ACL 2008: 595-603. Liang, P. (2005). Semi-supervised learning for natural language, Massachusetts Institute of Technology. McCallum, A. K. (2002). "Mallet: A machine learning for language toolkit." McClosky, D., E. Charniak, et al (2006). Effective self-training for parsing, ACL. McDonald, R., F. Pereira, et al (2005). Non-projective dependency parsing using spanning tree algorithms. EMNLP: 523-530. Nakov, P. and M. Hearst (2005). Using the web as an implicit training set: application to structural ambiguity resolution. EMNLP, Association for Computational Linguistics: 835-842. Nivre, J., J. Hall, et al (2007). The CoNLL 2007 Shared Task on Dependency Parsing, CoNLL 2007. s. 915-932. Nivre, J., J. Hall, et al (2006). Maltparser: A data-driven parser-generator for dependency parsing. Noord, G. v. (2007). Using self-trained bilexical preferences to improve disambiguation accuracy. 10th International Conference on Parsing Technologies, ACL: 1-10. Pyysalo, S., T. Ohta, et al (2011). "Overview of the Entity Relations (REL) supporting task of BioNLP 2011." ACL HLT 2011 1(480): 83. Ritter, A., Mausam, et al (2010). A latent dirichlet alocation method for selectional preferences. ACL 2010: 424-434. Sagae, K. and J.-i. Tsujii (2007). Dependency parsing and domain adaptation with LR models and parser ensembles. EMNLP-CoNLL 2007: 1044-1050. S?aghdha, D. (2010). Latent variable models of selectional preference. ACL 2010: 435-444. Tateisi, Y., A. Yakushiji, et al (2005). Syntax Annotation for the GENIA corpus. ACL. Tsuruoka, Y., Y. Tateishi, et al (2005). "Developing a robust part-of-speech tagger for biomedical text." AII: 382-392. Turney, P. D. and P. Pantel (2010). "From frequency to meaning: Vector space models of semantics." JAIR 37(1): 141-188. Wallach, H., D. Mimno, et al (2009). "Rethinking LDA: Why priors matter." NIPS 22: 1973?1981. Zhou, G., J. Zhao, et al (2011). Exploiting web-derived selectional preference to improve statistical dependency parsing. ACL. 
48
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 116?119,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Effect of Out Of Vocabulary terms on inferring eligibility criteria for a 
retrospective study in Hebrew EHR 
 
 
Raphael Cohen* 
Computer Science Dept. 
Ben-Gurion University in the Negev 
cohenrap@bgu.ac.il 
Michael Elhadad 
Computer Science Dept. 
Ben-Gurion University in the Negev 
elhadad@cs.bgu.ac.il 
 
  
 
1 Background 
The Electronic Health Record (EHR) contains 
information useful for clinical, epidemiological 
and genetic studies. This information of patient 
symptoms, history, medication and treatment is 
not completely captured in the structured part of 
the EHR but is often found in the form of free-
text narrative. 
A major obstacle for clinical studies is finding 
patients that fit the eligibility criteria of the 
study. Using EHR in order to automatically iden-
tify relevant cohorts can help speed up both clin-
ical trials and retrospective studies (Restificar, 
Korkontzelos et al 2013).  
While the clinical criteria for inclusion and 
exclusion from the study are explicitly stated in 
most studies, automating the process using the 
EHR database of the hospital is often impossible 
as the structured part of the database (age, gen-
der, ICD9/10 medical codes, etc.?) rarely covers 
all of the criteria. 
Many resources such as UMLS (Bodenreider 
2004), cTakes (Savova, Masanz et al 2010), 
MetaMap (Aronson and Lang 2010) and recently 
richly annotated corpora and treebanks (Albright, 
Lanfranchi et al 2013) are available for pro-
cessing and representing medical texts in Eng-
lish. Resource poor languages, however, suffer 
from lack in NLP tools and medical resources. 
Dictionaries exhaustively mapping medical terms 
to the UMLS medical meta-thesaurus are only 
available in a limited number of languages be-
sides English. NLP annotation tools, when they 
exist for resource poor languages, suffer from 
heavy loss of accuracy when used outside the 
domain on which they were trained, as is well 
documented for English (Tsuruoka, Tateishi et 
al. 2005; Tateisi, Tsuruoka et al 2006). 
In this work we focus on the problem of clas-
sifying patient eligibility for inclusion in retro-
spective study of the epidemiology of epilepsy in 
Southern Israel. Israel has a centralized structure 
of medical services which include advanced 
EHR systems. However, the free text sections of 
these EHR are written in Hebrew, a resource 
poor language in both NLP tools and hand-
crafted medical vocabularies. 
Epilepsy is a common chronic neurologic dis-
order characterized by seizures. These seizures 
are transient signs and/or symptoms of abnormal, 
excessive, or hyper synchronous neuronal activi-
ty in the brain. Epilepsy is one of the most com-
mon of the serious neurological disorders (Hirtz, 
Thurman et al 2007).  
2 Corpus 
We collected a corpus of patient notes from 
the Pediatric Epilepsy Unit, an outpatient clinic 
for neurology problems, not limited to epilepsy, 
in Soroka Hospital. This clinic is the only availa-
ble pediatric neurology clinic in southern Israel 
and at the time of the study was staffed by a sin-
gle expert serving approximately 225,000 chil-
dren. The clinical corpus spans 894 visits to the 
Children Epilepsy Unit which occurred in 2009 
by 516 unique patients. The corpus contains 
226K tokens / 12K unique tokens. 
?Supported by the Lynn and William Frankel Center for 
Computer Sciences, Ben Gurion University 
116
The patients were marked by the attending 
physician as positive or negative for epilepsy. In 
the study year, 2009, 208 patients were marked 
as positive examples and 292 as negative. The 
inclusion criteria were defined as history of more 
than one convulsive episode excluding febrile 
seizures. In practice, the decision for inclusion 
was more complex as some types of febrile sei-
zure syndromes are considered a type of epilepsy 
while some patients with convulsion were ex-
cluded from the study for various reasons. 
3 Method 
We developed a system to classify EHR notes in 
Hebrew into ?epilepsy? / ?non-epilepsy? classes, 
so that they can later be reviewed by a physician 
as eligible candidates into a cohort. The system 
analyzes the Hebrew text into relevant tokens by 
applying morphological analysis and word seg-
mentations, Hebrew words are then semi-
automatically aligned to the UMLS vocabulary. 
The most important tagged Hebrew words are 
then used as features fed to a statistical document 
classification system.  We evaluate the perfor-
mance of the system on our corpus, and measure 
the impact of Hebrew text analysis in improving 
the performance for patient classification. 
4 Out-Of-Vocabulary Terms 
The complex rules of Hebrew word formation 
make word segmentation the first challenge of 
any NLP pipeline in Hebrew. Agglutination of 
function words leads to high ambiguity in He-
brew (Adler and Elhadad 2006). To perform 
word segmentation, Adler and Elhadad (Adler 
and Elhadad 2006) combine segmentation and 
morpheme tagging using an HMM model over a 
lattice of possible segmentations. This learning 
method uses a lexicon to find all possible seg-
mentations for all tokens and chooses the most 
likely one according to POS sequences. Un-
known words, a class to which most borrowed 
medical terms belong, are segmented in all pos-
sible ways (there are over 150 possible prefixes 
and suffixes in Hebrew) and the most likely form 
is chosen using the context within the same sen-
tence. Beyond word segmentation, the rich mor-
phological nature of Hebrew makes POS tagging 
more complex with 2.4 possible tags per token 
on average, compared to 1.4 for English. 
Out of 12K token types in the corpus 3.9K 
(30%) were not found in the lexicon used by the 
Morphological Disambiguator compared to only 
7.5% in the Newswire domain. A sample of 2K 
unknown token was manually annotated as: 
transliteration, misspelling and Hebrew words 
missing in the lexicon. Transliterated terms made 
up most of the unknown tokens (71.5%) while 
the rest were misspelled words (16%) and words 
missing from the lexicon (13.5%). 
Error analysis of the Morphological Disam-
biguator in the medical domain corpora shows 
that in the medical domain, Adler et als un-
known model still performs well: 80% of the 
unknown tokens were still analyzed correctly. 
However, 88.5% of the segmentation errors were 
found in unknown tokens. Moreover, the translit-
erated words are mostly medical terms important 
for understanding the text. 
5 Acquiring a Transliterations Lexicon 
As transliterations account for a substantial 
amount of the errors and are usually medical 
terms, therefore of interest, we aim to automati-
cally create a dictionary mapping transliterations 
in our target corpus to a terminology or vocabu-
lary in the source language. In our case, the 
source language is medical English which is a 
mix of English and medical terms from Latin as 
represented by the UMLS vocabulary. 
The dictionary construction algorithm is based 
on two methods: noisy transliteration of the med-
ical English terms from the UMLS to Hebrew 
forms (producing all the forms an English terms 
may be written in Hebrew, see (Kirschenbaum 
and Wintner 2009)) and matching the generated 
Figure 1 ? Decision Tree for inclusion/exclusion. Sodium Valproate (dplpt) is a key term which is 
often segmented incorrectly. 
117
transliterations to the unknown Hebrew forms 
found in our target corpus. After creating a list of 
candidate pairs (Hebrew form found in the cor-
pus and transliterated UMLS concept), we filter 
the results to create an accurate dictionary using 
various heuristic measures.  
The produced lexicon contained 2,507 trans-
literated lemmas with precision of 75%. The ac-
quired lexicon reduced segmentation errors by 
50%. 
6 Experiments 
6.1 Experimental Settings 
An SVM classifier was trained using the 200 
most common nouns as features. The noun lem-
mas were extracted with the morphological dis-
ambiguator in two settings: na?ve setting using 
the newswire lexicon and an adapted setting us-
ing the acquired lexicon.  
We divided the corpus into training and testing 
sets of equal size, we report on the average re-
sults or 10 different divisions of the data. 
6.2 Results 
The classifier using the baseline lexicon achieved 
an average F-Score of 83.6%. With the extended 
in-domain transliterations lexicon the classifier 
achieves F-Score of 87%, an error reduction of 
20%. 
We repeated the experiment with decision 
trees for visualization for error analysis. With 
decision trees we see an improvement from 
76.8% to 82.6% F-score. In Figure 1, we see in 
the resulting decision tree the most commonly 
prescribed medication for epilepsy patients, So-
dium Valproate ?depalept? (???????). This word 
appears in three forms: ?depalept?, ?b+deplapet? 
and ?h+depalept?. The acquired lexicon allows 
better segmentation of this word thus removing 
noise for documents containing the agglutinated 
forms. 
7 Conclusions 
We presented the task of classifying patients? 
Hebrew free text EHR for inclusion/exclusion 
from a prospective study. Transliterated tokens 
are an important feature in medical texts. In lan-
guages with compound tokens this is likely to 
lead to segmentation errors. 
Using a lexicon adapted for the domain im-
pacts the number of segmentation errors, this 
error reduction translates into further improve-
ments when using these data for down the line 
applications such as classification. 
Creating domain adaptation methods for re-
source-poor languages can positively impact the 
use of clinical records in these languages. 
 
 
Acknowledgments 
 
 
Adler, M. and M. Elhadad (2006). An 
unsupervised morpheme-based hmm for 
hebrew morphological disambiguation. 
Proceedings of the 21st International 
Conference on Computational Linguistics 
and the 44th annual meeting of the 
Association for Computational 
Linguistics, Association for 
Computational Linguistics. 
Albright, D., A. Lanfranchi, et al (2013). 
"Towards comprehensive syntactic and 
semantic annotations of the clinical 
narrative." Journal of the American 
Medical Informatics Association. 
Aronson, A. R. and F. M. Lang (2010). "An 
overview of MetaMap: historical 
perspective and recent advances." Journal 
of the American Medical Informatics 
Association 17(3): 229-236. 
Bodenreider, O. (2004). "The unified medical 
language system (UMLS): integrating 
biomedical terminology." Nucleic Acids 
Research 32(Database Issue): D267. 
Hirtz, D., D. Thurman, et al (2007). "How 
FRPPRQ DUH WKH ?FRPPRQ? QHXURORJLF
disorders?" Neurology 68(5): 326-337. 
Kirschenbaum, A. and S. Wintner (2009). Lightly 
supervised transliteration for machine 
translation. Proceedings of the 12th 
Conference of the European Chapter of 
the Association for Computational 
Linguistics, Association for 
Computational Linguistics. 
Restificar, A., I. Korkontzelos, et al (2013). "A 
method for discovering and inferring 
appropriate eligibility criteria in clinical 
trial protocols without labeled data." 
BMC Medical Informatics and Decision 
Making 13(Suppl 1): S6. 
Savova, G. K., J. J. Masanz, et al (2010). "Mayo 
clinical Text Analysis and Knowledge 
Extraction System (cTAKES): 
architecture, component evaluation and 
applications." Journal of the American 
Medical Informatics Association 17(5): 
507-513. 
Tateisi, Y., Y. Tsuruoka, et al (2006). Subdomain 
adaptation of a POS tagger with a small 
corpus. Proceedings of the Workshop on 
118
Linking Natural Language Processing and 
Biology: Towards Deeper Biological 
Literature Analysis, Association for 
Computational Linguistics. 
Tsuruoka, Y., Y. Tateishi, et al (2005). 
"Developing a robust part-of-speech 
tagger for biomedical text." Advances in 
informatics: 382-392. 
 
 
119
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 13?19,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization corpus preparation, Part 2:
Czech, Hebrew and Spanish
Michael Elhadad
Ben-Gurion Univ.
in the Negev, Israel
elhadad@cs.bgu.ac.il
Sabino Miranda-Jim?nez
Instituto Polit?cnico
Nacional, Mexico
sabino_m@hotmail.com
Josef Steinberger
Univ. of
West Bohemia,
Czech Republic
jstein@kiv.zcu.cz
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
This document overviews the strategy, ef-
fort and aftermath of the MultiLing 2013
multilingual summarization data collec-
tion. We describe how the Data Contrib-
utors of MultiLing collected and gener-
ated a multilingual multi-document sum-
marization corpus on 10 different lan-
guages: Arabic, Chinese, Czech, English,
French, Greek, Hebrew, Hindi, Romanian
and Spanish. We discuss the rationale be-
hind the main decisions of the collection,
the methodology used to generate the mul-
tilingual corpus, as well as challenges and
problems faced per language. This paper
overviews the work on Czech, Hebrew and
Spanish languages.
1 Introduction
In this document we present the language-
specific problems and challenges faced by Con-
tributors during the corpus creation process. To
facilitate the reader we repeat some information
found in the first part of the overview (Li et al,
2013): the MultiLing tasks and the main steps of
the corpus creation process.
2 The MultiLing tasks
There are two main tasks (and a single-
document multilingual summarization pilot de-
scribed in a separate paper) in MultiLing 2013:
Summarization Task This MultiLing task aims
to evaluate the application of (partially or
fully) language-independent summarization
algorithms on a variety of languages. Each
system participating in the task was called
to provide summaries for a range of differ-
ent languages, based on corresponding cor-
pora. In the MultiLing Pilot of 2011 the lan-
guages used were 7, while this year systems
were called to summarize texts in 10 differ-
ent languages: Arabic, Chinese, Czech, En-
glish, French, Greek, Hebrew, Hindi, Roma-
nian, Spanish. Participating systems were re-
quired to apply their methods to a minimum
of two languages.
The task was aiming at the real problem of
summarizing news topics, parts of which may
be described or may happen in different mo-
ments in time. We consider, similarly to Mul-
tiLing 2011(Giannakopoulos et al, 2011) that
news topics can be seen as event sequences:
Definition 1 An event sequence is a set of
atomic (self-sufficient) event descriptions, se-
quenced in time, that share main actors, lo-
cation of occurence or some other important
factor. Event sequences may refer to topics
such as a natural disaster, a crime investiga-
tion, a set of negotiations focused on a single
political issue, a sports event.
The summarization task requires to generate
a single, fluent, representative summary from
a set of documents describing an event se-
quence. The language of the document set
will be within the given range of 10 languages
and all documents in a set share the same lan-
guage. The output summary should be of the
same language as its source documents. The
output summary should be between 240 and
250 words.
Evaluation Task This task aims to examine how
well automated systems can evaluate sum-
maries from different languages. This task
takes as input the summaries generated from
automatic systems and humans in the Sum-
marization Task. The output should be a grad-
ing of the summaries. Ideally, we would want
the automatic evaluation to maximally corre-
late to human judgement.
13
The first task was aiming at the real problem of
summarizing news topics, parts of which may be
described or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second task, summarization evaluation has
come to be a prominent research problem, based on
the difficulty of the summary evaluation process.
While commonly used methods build upon a few
human summaries to be able to judge automatic
summaries (e.g., (Lin, 2004; Hovy et al, 2005)),
there also exist works on fully automatic evalua-
tion of summaries, without human?model? sum-
maries (Louis and Nenkova, 2012; Saggion et al,
2010). The Text Analysis Conference has a sepa-
rate track, named AESOP (Dang and Owczarzak,
2009) aiming to test and evaluate different auto-
matic evaluation methods of summarization sys-
tems.
Given the tasks, a corpus needed to be gener-
ated, that would be able to:
? provide input texts in different languages to
summarization systems.
? provide model summaries in different lan-
guages as gold standard summaries, to also
allow for automatic evaluation using model-
dependent methods.
? provide human grades to automatic and hu-
man summaries in different languages, to
support the testing of summary evaluation
systems.
In the following section we show how these re-
quirements were met in MultiLing 2013.
3 Corpus collection and generation
The overall process of creating the corpus of
MultiLing 2013 was, similarly to MultiLing 2011,
based on a community effort. The main processes
consisting the generation of the corpus are as fol-
lows:
? Selection of a source corpus in a single lan-
guage.
? Translation of the source corpus to different
languages.
? Human summarization of corpus topics per
language.
? Evaluation of human summaries, as well as of
submitted system runs.
4 Language specific notes
In the following paragraphs we provide
language-specific overviews related to the corpus
contribution effort. The aim of these overviews is
to provide a reusable pool of knowledge for future
similar efforts.
In this document we elaborate on Czech, He-
brew, and Spanish languages. A second document
(Elhadad et al, 2013) elaborates on the rest of the
languages.
4.1 Czech language
The first part of the Czech subcorpus (10 top-
ics) was created for the multilingual pilot task at
TAC 2011. Five new topics were added for Mul-
tiling 2013. In total, 14 annotators participated in
the Czech corpus creation.
The most time consuming part of the annota-
tion work was the translation of the articles. The
annotators were not professional translators and
many topics required domain knowledge for cor-
rect translation. To be able to translate a per-
son name, the translator needs to know its correct
spelling in Czech, which is usually different from
English. The gender also plays an important role
in the translation, because a suffix ?ov?? must be
added to female surnames.
Translation of organisation names or person?s
functions within an organisation needs some do-
main knowledge as well. Complicated morphol-
ogy and word order in Czech (more free but some-
times very different fromEnglish) makes the trans-
lation even more difficult.
For the creation of model summaries the anno-
tator needed to analyse the topic well in order to
decide what is important and what is redundant.
Sometimes, it was very difficult, mainly in the
case of topics which covered a long period (even
5 years) and which contained articles sharing very
little information.
The main question of the evaluation part was
how to evaluate a summary which contains a read-
able, continuous text ? mainly the case of the
14
Group SysID Avg Perf
a B 4.75
a A 4.63
ab C 4.61
b D 4.21
b E 4.10
Table 1: Czech: Tukey?s HSD test groups for hu-
man summarizers
baseline system with ID6) ? however not impor-
tant information from the article cluster point of
view.
An overview of the Overall Responsiveness and
the corresponding average grades of the human
summarizers can be seen in Table 1. We note
that on average the human summaries are consid-
ered excellent (graded above 4 out of 5), but that
there exist statistically significant differences be-
tween summarizers, essentially forming two dis-
tinct groups.
4.2 Hebrew language
This section describes the process of preparing
the dataset for MultiLing 2013 in Hebrew: transla-
tion of source texts from English, and the summa-
rization for the translated texts, by the Ben Gurion
University Natural Language Processing team.
4.2.1 Translation Process
Four people participated in the translation and
the summarization of the dataset of the 50 news
articles: three graduate students, one a native En-
glish speaker with fluent Hebrew and the other two
with Hebrew as a mother tongue and very good
English skills. The process was supervised by a
professional translator with a doctoral degree with
experience in translation and scientific editing.
The average times to read an article was 2.5min-
utes (std. dev 1.2min), the average translation time
was 30 minutes (std. dev 15min), and the average
proofing time was 18.5min (std. dev 10.5min).
4.2.2 Translation Methodology
We tested two translation methodologies by dif-
ferent translators. In some of the cases, translation
was aided with Google Translate1, while in other
cases, translation was performed from scratch.
In the cases where texts were first translated
using Google Translate, the translator reviewed
1See http://translate.google.com/.
the text and edited changes according to her judg-
ment. Relying on the time that was reported for the
proofreading of each translation, we could tell that
texts that were translated using this method, re-
quired longer periods of proofreading (and some-
times more time was required to proofread than to
translate). This is most likely because once the au-
tomatic translation was available, the human trans-
lator was biased by the automatic outcome, re-
maining anchored? to the given text with reduced
criticism and creativity.
Translating the text manually, aided with online
or offline dictionaries, Wikipedia and news site on
the subject that was translated, showed better qual-
ity as analysis of time shows, where the ratio be-
tween the time needed to proofread was less than
half.
In addition, we found, that inmost cases the time
that the translation took for the first texts of a given
subject (for each article cluster), tends to be signif-
icantly longer than the subsequent articles in the
same cluster. This reflects the ?learning phase? ex-
perienced by the translators who approached each
cluster, getting to know the vocabulary of each
subject.
4.2.3 Topic Clusters
The text collection includes five clusters of ten
articles each. Some of the topics were very famil-
iar to the Hebrew-speaking readers, and some sub-
jects were less familiar or relevant. The Iranian
Nuclear issue is very common in the local news
and terminology is well known. Moreover, it was
possible to track the articles from the news as they
were published in Hebrew news websites at that
time; this was important for the usage of actual
and correct news-wise terminology. The hardest
batch to translate was on the Paralympics champi-
onship, which had no publicity in Hebrew, and the
terminology of winter sports is culturally foreign
to native Hebrew speakers.
4.2.4 Special Issues in Hebrew
A couple of issues have surfaced during the
translation and should be noted. Many words in
Hebrew have a foreign transliterated usage and an
original Hebrew word as well. For instance, the
Latin word Atomic is very common in Hebrew
and, therefore, it will be equally acceptable to use
it in the Hebrew form, ????? / ?atomi?but also
the Hebrew word ?????? (?gar? ini? / nuclear).
Traditional HebrewNews Agencies have for many
15
Summarizer Reading time Summarization
A 43 min 49 min
B 22 min 84 min
C 35 min 62 min
Table 2: Summarization process times (averaged)
years adopted an editorial line which strongly en-
courages using original Hebrew words whenever
possible. In recent years, however, this approach
is relaxed, and both registers are equally accepted.
We have tried to use a ?common notion? in all texts
using the way terms are written inWikipedia as the
voice of majority. In most cases, this meant using
many transliterations.
Another issue in Hebrew concerns the orthog-
raphy variations of plene vs. deficient spelling.
Since Hebrew can be written with or without vo-
calization, words may be written with variations.
For instance, the vocalized version of the word
?air? is ?????? (?avir? ) while the non-vocalized
version is ????? (?avvir?). The rules of spelling
related to these variations are complicated and are
not common knowledge. Even educated people
write words with high variability, and in many
cases, usage is skewed by the rules embedded in
the Microsoft Word editor. We did not make any
specific effort to enforce standard spelling in the
dataset.
4.2.5 Summarization Process
Each cluster of articles was summarized by three
persons, and each summary was proof-read by the
other summarizers. Most of the summarizers read
the texts before summarization, while translating
or proofreading them, and, therefore, the time that
was required to read all texts was reduced.
The time spent reading and summarizing was
extremely different for each of the three summa-
rizers, reflecting widely different summarization
strategies, as indicated in the Table 2 (average
times over the 5 new clusters of MultiLing 2013):
The trend indicates that investing more time up
front reading the clusters pays off later in summa-
rization time.
The instructions did not explicitly recommend
abstractive vs. extractive summarization. Two
summarizers applied abstractive methods, one
tended to use mostly extractive (C). The extractive
method did not take markedly less time than the
abstractive one. In the evaluation, the extractive
Group SysID Avg Perf
a A 4.80
ab B 4.40
b C 4.13
Table 3: Hebrew: Tukey?s HSD test groups for hu-
man summarizers
summary was found markedly less fluent.
As the best technique to summarize efficiently,
all summarizers found that ordering the texts by
date of publication was the best way to conduct the
summaries in the most fluent manner.
However, it was not completely a linear process,
since it was often found that general information,
which should be located at the beginning of the
summary as background information, appeared in
a later text. In such cases, summarizers changed
their usual strategy and consciously moved infor-
mation from a later text to the beginning of the
summary. This was felt as a distinct deviation ?
as the dominant strategy was to keep track of the
story told across the chronology of the cluster, and
to only add new and important information to the
summary that was collected so far.
The most difficult subject to summarize was
the set on Paralympic winter sports championship
which was a collection of anecdotal descriptions
which were not necessarily a developing or a se-
quential story and had no natural coherence as a
cluster.
4.2.6 Human evaluation
The results of human evaluation over the human
summarizers are provided in Table 3. It is inter-
esting to note that even between humans there ex-
ist two groups with statistically significant differ-
ences in their grades. On the other hand, the hu-
man grades are high enough to show high quality
summaries (over 4 on a 5 point scale).
4.3 Spanish language
Thirty undergraduate students, from National
Institute Polytechnic and Autonomous University
of the State of Mexico, were involved in creating
of Spanish corpus for MultiLing 2013.
The Spanish corpus built upon the Text Analy-
sis Conference (TAC) MultiLing Corpus of 2011.
The source documents were news fromWikiNews
website, in English language. The source corpus
for translating consisted of 15 topics and 10 docu-
ments per topic. In the following paragraphs, we
16
show the measured times for each stage and prob-
lems that people had to face during the generation
of corpus that includes translation of documents,
multi-document summarization, and evaluation of
human (manual) summaries.
At the translation step, people had to translate
sentence by sentence or paraphrase a sentence up
to completing the whole document. When a docu-
ment was translated, it was sent to another person
to verify the quality of the translated document.
The effort was measured by three different time
measurements: reading time, translation time, and
verification time.
The reading average at document level was 7.6
minutes (with a standard deviation of 3.4 minutes),
the average translation of each document was 19.2
minutes (with a standard deviation of 7.8 min-
utes), and the average verification was 14.9 min-
utes (with a standard deviation of 7.7 minutes).
The translation stage took 104.5 man-hours.
At summarization step, people had to read the
whole set of translated documents (topic) and cre-
ate a summary per each set of documents. The
length of a summary is between 240 and 250
words. Three summaries were created for each
topic. Also, reading time of the topic and time of
writing the summary were measured.
The average reading of a set of documents was
31.6 minutes (with a standard deviation of 10.2
minutes), and the average time to generate a sum-
mary was 27.7 minutes (with a standard deviation
of 6.5 minutes). This stage took 44.5 man-hours.
At evaluation step, people had to read the whole
set of translated documents and assess its corre-
sponding summary. The summary quality was
evaluated. Three evaluations were done for each
summary. The human judges assessed the overall
responsiveness of the summary based on covering
all important aspects of the document set, fluent
and readable language. The human summary qual-
ity average was 3.8 (on a scale 1 to 5) (with a stan-
dard deviation of 0.81). The results are detailed in
Table 4. It is interesting to note that all humans
have no statistically significant differences in their
grades. On the other hand, the human grades are
not excellent on average (i.e. exceeding 4 out of 5)
which shows that the evaluators considered human
summaries non-optimal.
Group SysID Avg Perf
a C 3.867
a B 3.778
a A 3.667
Table 4: Spanish: Tukey?s HSD test groups for hu-
man summarizers
4.3.1 Problems during Generation of Spanish
Corpus
During the translation step, translators had to
face problems related to proper names, acronyms,
abbreviations, and specific themes. For instance,
the proper name?United States?can be depicted
with different Spanish words such as ?EE. UU.?
2,?Estados Unidos?, and?EUA??all of them
are valid words. Even though translators know
all the correct translations, they decided to use the
frequent terms in a context of news (the first two
terms are frequently used).
In relation to acronyms, well-known acronyms
were translated into equivalent well-known (or fre-
quent) Spanish translations such as UN (United
Nations) became into ONU (Organizaci?n de las
Naciones Unidas), or they were kept in the source
language, because they are frequently used in
Spanish, for example, UNICEF, BBC, AP (the
news agency, Associated Press), etc.
On the contrary, for not well-known acronyms
of agencies, monitoring centers, etc., translators
looked for the common translation of the proper
name on Spanish news websites in order to cre-
ate the acronym based on the name. Other trans-
lators chose to translate the proper name, but they
kept the acronym from the source document beside
the translated name. In cases where acronyms ap-
peared alone, they kept the acronym from source
language. It is a serious problem because a set of
translated documents has a mix of acronyms.
Abbreviations were mainly faced with ranks
such as lieutenant (Lt.), Colonel (Col.), etc. Trans-
lators used an equivalent rank in Spanish. For in-
stance, lieutenant (Lt.) is translated into?teniente
(Tte.)?; however, translators preferred to use the
complete word rather than the abbreviation.
In case of specific topics, translators used Span-
ish websites related to the topic in order to know
the particular vocabulary and to decide what (tech-
2The double E and double U indicate that the letter rep-
resents a plural: e.g. EE. may stand for Asuntos Exteriores
(Foreign Affairs).
17
nical) words should be translated and how they
should be expressed.
As regards at text summarization step, sum-
marizers dealt with how to organize the sum-
mary because there were ten documents per topic,
and all documents involved dates. Two strategies
were employed to solve the problem: generating
the summary according to representative dates, or
starting the summary based on a particular date.
In the first case, summarizers took the chain
of events and wrote the summary considering the
dates of events. They gathered important events
and put together under one date, typically, the lat-
est date according to a part of the chain of events.
They grouped all events in several dates; thus, the
summary is a sequence of dates that gather events.
However, the dates are chosen arbitrary according
to the summarizers.
In the second case, summarizers started the sum-
mary based on a specific date, and continued writ-
ing the sequence of important events. The se-
quence of events represents the temporality start-
ing from a specific point of time (usually, the
first date in the set of documents). Finally, in
most cases, evaluators think that human sum-
maries meet the requirements of covering all im-
portant aspects of the document set, fluent and
readable language.
5 Conclusions and lessons learnt
The findings from the languages presented in
this paper appear to second the claims found in the
rest of the languages (Li et al, 2013):
? Translation is a non-trivial process, often re-
quiring expert know-how to be performed.
? The distribution of time in summarization can
significantly vary among human summariz-
ers: it essentially sketches different strate-
gies of summarization. It would be interest-
ing to follow different strategies and record
their effectiveness in the multilingual setting,
similarly to previous works on human-style
summarization (Endres-Niggemeyer, 2000;
Endres-Niggemeyer and Wansorra, 2004).
Our find may be related to the (implied) ef-
fort of taking notes while reading, which can
be a difficult cognitive process (Piolat et al,
2005).
? The time aspect is important when generat-
ing a summary. The exact use of time (a sim-
ple timeline? a grouping of events based on
time?) is apparently arbitrary.
We remind the reader that extended technical re-
ports recapitulating discussions and findings from
the MultiLingWorkshop will be available after the
workshop at the MultiLing Community website3,
as an addenum to the proceedings.
What can definitely be derived from all the ef-
fort and discussion related to the gathering of sum-
marization corpora is that it is a research challenge
in itself. If the future we plan to broaden the scope
of the MultiLing effort, integrating all the findings
in tools that will support the whole process and al-
low quantifying the apparent problems in the dif-
ferent stages of corpus creation. We have also been
considering to generate comparable corpora (e.g.,
see (Saggion and Szasz, 2012)) for future Multi-
Ling efforts. We examine this course of action
to avoid the significant overhead by the transla-
tion process required for parallel corpus genera-
tion. We should note here that so far we have been
using parallel corpora to:
? allow for secondary studies, related to the
human summarization effort in different lan-
guages. Having a parallel corpus is such cases
can prove critical, in that it provides a com-
mon working base.
? be able to study topic-related or domain-
related summarization difficulty across lan-
guages.
? highlight language-specific problems (such
as ambiguity in word meaning, named entity
representation across languages).
? fixes the setting in which methods can show
their cross-language applicability. Exam-
ining significantly varying results in differ-
ent languages over a parallel corpus offers
some background on how to improve exist-
ing methods and may highlight the need for
language-specific resources.
On the other hand, the significant organizational
and implementaion effort required for the transla-
tion may turn the balance towards comparable cor-
pora for future MultiLing endeavours.
3See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
18
Acknowledgments
MultiLing is a community effort and this com-
munity is what keeps it alive and interesting. We
would like to thank contributors for their organi-
zational effort, which made MultiLing possible in
so many languages and all volunteers, helpers and
researchers that helped realize individual steps of
the process. A more detailed reference of the con-
tributor teams can be found in the Appendix.
The MultiLing 2013 organization has been par-
tially supported by the NOMAD FP7 EU Project
(cf. http://www.nomad-project.eu).
References
[Dang and Owczarzak2009] Hoa Trang Dang and
K. Owczarzak. 2009. Overview of the tac 2009
summarization track, Nov.
[Elhadad et al2013] Michael Elhadad, Sabino
Miranda-Jim?nez, Josef Steinberger, and George
Giannakopoulos. 2013. Multi-document multi-
lingual summarization corpus preparation, part 2:
Czech, hebrew and spanish. In MultiLing 2013
Workshop in ACL 2013, Sofia, Bulgaria, August.
[Endres-Niggemeyer and Wansorra2004] Brigitte
Endres-Niggemeyer and Elisabeth Wansorra. 2004.
Making cognitive summarization agents work in
a real-world domain. In Proceedings of NLUCS
Workshop, pages 86?96. Citeseer.
[Endres-Niggemeyer2000] Brigitte Endres-
Niggemeyer. 2000. Human-style WWW sum-
marization. Technical report.
[Giannakopoulos et al2011] G. Giannakopoulos,
M. El-Haj, B. Favre, M. Litvak, J. Steinberger,
and V. Varma. 2011. TAC 2011 MultiLing pilot
overview. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Hovy et al2005] E. Hovy, C. Y. Lin, L. Zhou, and
J. Fukumoto. 2005. Basic elements.
[Li et al2013] Lei Li, Corina Forascu, Mahmoud El-
Haj, and George Giannakopoulos. 2013. Multi-
document multilingual summarization corpus prepa-
ration, part 1: Arabic, english, greek, chinese, ro-
manian. In MultiLing 2013 Workshop in ACL 2013,
Sofia, Bulgaria, August.
[Lin2004] C. Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and Ani
Nenkova. 2012. Automatically assessing ma-
chine summary content without a gold standard.
Computational Linguistics, 39(2):267?300, Aug.
[Piolat et al2005] Annie Piolat, Thierry Olive, and
Ronald T Kellogg. 2005. Cognitive effort dur-
ing note taking. Applied Cognitive Psychology,
19(3):291?312.
[Saggion and Szasz2012] Horacio Saggion and Sandra
Szasz. 2012. The concisus corpus of event sum-
maries. In LREC, pages 2031?2037.
[Saggion et al2010] H. Saggion, J. M. Torres-Moreno,
I. Cunha, and E. SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, page 1059?
1067.
Appendix: Contributor teams
Czech language team
Team members Brychc?n Tom??, Campr Michal,
Fiala Dalibor, Habernal Ivan, Habernalov?
Anna, Je?ek Karel, Konkol Michal, Konop?k
Miloslav, Kr?m?? Lubom?r, Nejezchlebov?
Pavla, Pelechov? Blanka, Pt??ek Tom??,
Steinberger Josef, Z?ma Martin.
Team affiliation University of West Bohemia,
Czech Republic
Contact e-mail jstein@kiv.zcu.cz
Hebrew language team
Team members Tal Baumel, Raphael Cohen,
Michael Elhadad, Sagit Fried, Avi Hayoun,
Yael Netzer
Team affiliation Computer Science Dept. Ben-
Gurion University in the Negev, Israel
Contact e-mail elhadad@cs.bgu.ac.il
Spanish language team
Team members Sabino Miranda-Jim?nez, Grig-
ori Sidorov, Alexander Gelbukh (Natural
Language and Text Processing Laboratory,
Center for Computing Research, National In-
stitute Polytechnic, Mexico City, Mexico)
Obdulia Pichardo-Lagunas (Interdisciplinary
Professional Unit on Engineering and Ad-
vanced Technologies (UPIITA), National In-
stitute Polytechnic, Mexico City, Mexico)
Contact e-mail sabino_m@hotmail.com
19
