Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 12?19,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
What pushes their buttons? Predicting comment polarity from the content
of political blog posts
Ramnath Balasubramanyan
Language Technologies Institute
Carnegie Mellon University
rbalasub@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Doug Pierce and David P. Redlawsk
Political Science Department
Rutgers University
drpierce@eden.rutgers.edu, redlawsk@rutgers.edu
Abstract
Political blogs as a form of social media al-
low for an uniquely interactive form of politi-
cal discourse. This is especially evident in fo-
cused blogs with a strong ideological identity.
We investigate techniques to identify topics
within the context of the community, which
when discussed in a blog post evoke a dis-
cernible positive or negative collective opin-
ion from readers who respond to posts in com-
ments. This is done by using computational
methods to assign sentiment polarity to blog
comments and learning community specific
models that summarize issues tackled by blogs
and predict the polarity based on the topics
discussed in a blog post.
1 Introduction
Recent work in political psychology has made it
clear that political decision-making is strongly influ-
enced by emotion. For instance, (Lodge and Taber,
2000) propose a theory of ?motivated reasoning?, in
which political information is processed in a way
that is determined, in part, by a quickly-computed
emotional react to that information. Strong exper-
imental evidence for motivated reasoning (some-
times called ?hot cognition?) exists (Huang and
Price, 2001); (Redlawsk, 2002); (Redlawsk, 2006);
(Isbell et al, 2006). However, despite some recent
proposals (Kim et al, 2008) it is unclear how to
computationally model a person?s emotional reac-
tion to news, and how to collect the data necessary
to fit such a model. One problem is that emotional
reactions are different for different people - a fact ex-
ploited in the use of political ?code words? intended
to invoke a reaction in only a particular subset of the
electorate (a technique sometimes called ?dog whis-
tle politics?).
In this paper, we evaluate the use of machine
learning methods to predict how members of a spe-
cific political community will emotionally reaction
to different types of news. More specifically, we use
a dataset of widely read (?A-list?) political blogs,
and attempt to predict the aggregate sentiment in the
comment section of blogs, as a function of the tex-
tual content of the blog posting. In this paper, we
consider only predicting polarity (positive and neg-
ative feeling). In contrast to work done traditionally
in sentiment analysis which focuses on determining
the sentiment expressed in text, in this work, we fo-
cus on the task of predicting the sentiment that a
block of text will evoke in readers, expressed in the
comment section, as a response to the blog post.
This task is related to, but distinct from, several
other studies that have been made using comments
and discussions in political communities, or analy-
sis of sentiment in comments - (Yano et al, 2009),
(O?Connor et al, 2010), (Tumasjan et al, 2010).
Below we discuss the methods used to address the
various parts of this task. First, we evaluate two
methods to automatically determine the comment
polarity: SentiWordNet (Baccianella and Sebastiani,
2010) a general purpose resource that assigns sen-
timent scores to entries in WordNet, and an auto-
12
mated corpus-specific technique based on pointwise
mutual information. The quality of the polarity as-
sessments by these techniques are made by compar-
ing them to hand annotated assessments on a small
number of blog posts. Second, we consider two
methods for predicting comment polarity from post
content: support vector machine classification, and
sLDA, a topic-modeling-based approach. Finally,
we demonstrate that emotional reactions are indeed
community-specific, compare the accuracy of this
approach to the more traditional approach of pre-
dicting sentiment of a text from the text itself, and
present our conclusions.
2 Data
In this study, we use a collection of blog posts from
five blogs: Carpetbagger(CB)1, Daily Kos(DK)2,
Matthew Yglesias(MY)3, Red State(RS)4, and Right
Wing News(RWN)5, that focus on American politics
made available by (Yano et al, 2009). The posts
were collected during November 2007 to October
2008, which preceded the US presidential elections
held in November 2008. The blogs included in the
dataset vary in political idealogy with blogs like
Daily Kos that are Democrat-leaning and blogs like
Red State tending to be much more conservative.
Since we are interested in studying the responses
to blog posts, the corpus only contains posts where
there have been at least one comment in the six days
after the post was published. It is important to note
that only the text in the blog posts and comments are
used in this study. All non-textual information like
pictures, hyperlinks, videos etc. are discarded. In
terms of text processing, for each blog, a vocabulary
is created consisting of all terms that occur at least
5 times in the blog. Stopwords are eliminated us-
ing a standard stopword list. Each blog post is then
represented as a bag of words from the post. Table
2 shows statistics of the datasets. Each dataset is
studied separately for the most part in the rest of the
paper.
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com/
3http://yglesias.thinkprogress.org/
4http://www.redstate.com/
5http://rightwingnews.com/
3 Labelling comments with sentiment
polarity
The first step in understanding the nature of posts
that evoke emotional responses is to get a measure of
the polarity in the sentiment expressed in the com-
ments section of a blog post. The measure indicates
the ability of the issues in the blog post and its treat-
ment, to evoke strong emotions in readers.
3.1 SentiWordNet
In the first stage of the study, we use SentiWord-
Net (Baccianella and Sebastiani, 2010) which as-
sociates a large number of words in WordNet with
a positive, negative and objective score (summing
up to 1). Firstly, all the comments for a blog post
in the comment section are aggregated and for the
words in the comments that are found in SentiWord-
Net, the net positive and negative scores are com-
puted. Since SentiWordNet entries are associated
with word senses and because we don?t perform
word sense disambiguation, the SentiWordNet po-
larity of the most dominant word sense is used for
words in the comment section. The sentiment in the
comment section is deemed to be positive if the net
positive score exceeds the negative score and nega-
tive otherwise. Therefore, each blog post is now as-
sociated with a binary response variable indicating
the polarity of the sentiment expressed in the com-
ments.
3.2 Using pointwise mutual information
A second technique to determine the sentiment po-
larity of comments uses the principle of pointwise
mutual information (PMI)(Turney, 2002). We first
construct a seed list of positive and negative words
by choosing the 100 topmost positive and negative
words from SentiWordNet and manually eliminat-
ing words from this list that don?t pertain to senti-
ment in our context. (Appendix A has the list of
seed words used.) This seed list is used to construct
a larger set of positive and negative words by com-
puting the PMI of the words in the seed lists with
every other word in the vocabulary. It?s important
to note that this list is constructed for the specific
corpus that we work with. Because every blog is
processed separately, we construct a different senti-
ment word list for each blog based on the statistics
13
Blog Pol align-
ment
#posts Vocabulary size Avg
#words
per post
Avg #com-
ments per
post
Avg
#words per
comment
section
Carpetbagger
(CB)
liberal 1201 4998 170 31 1306
Daily Kos (DK) liberal 2597 6400 103 198 3883
Matthew Ygle-
sias (MY)
liberal 1813 4010 69 35 1420
Red State (RS) conservative 2357 8029 158 28 806
Right Wing Na-
tion (RWN)
conservative 1184 6205 185 33 1015
Table 1: Dataset statistics
of word occurences. Words in the vocabulary are
ranked by the difference in the average of the PMI
with positive and negative seed words. The top 1000
words in the resultant sorted list are treated as pos-
itive words and the bottom 1000 words as negative
words. The comment section of every post is tagged
with a positive or negative polarity as in the previous
section by computing the total positive and negative
word counts.
Using the same seed word list, the procedure is
performed separately for each blog resulting in sen-
timent polarity lists that are particular to the com-
munity and idealogy associated with each blog. It
should be noted that while this method provides bet-
ter estimates of comment sentiment polarity (as seen
in Section 4), it involves more manual work in con-
structing a seed set than the SentiWordNet method
which does not require any manual effort.
3.3 Human labels
As a third method that is accurate but expensive, we
manually labeled comments from approximately 30
blog posts from each blog, with a positive or neg-
ative label. The guideline in labeling was to deter-
mine if the sentiment in the comment section was
positive or negative to the subject of the post. The
chief intention of this exercise is to determine the
quality of the polarity assessments of the SentiWord-
Net and PMI methods. While it is possible to di-
rectly use the assessments and train a classifier, the
performance of the classifier will be limited by the
very small number of training examples (30 instead
of thousands of examples). The accuracy of the two
Blog SentiWordNet accuracy PMI accuracy
CB 0.56 0.78
DK 0.54 0.72
MY 0.61 0.83
RS 0.54 0.74
RWN 0.64 0.84
Table 2: Measuring accuracy of automatic comment po-
larity detection
automatic methods to determine comment polarity
is shown in Table 2
The better accuracy of the PMI method can be ex-
plained by the fact that SentiWordNet is a general
purpose list that is not customized for the domain
which tends to make it noisy for text in the politi-
cal domain. The PMI technique corresponds more
closely with the human labels but it requires a little
human effort in building the initial seed list of posi-
tive and negative words.
4 Predicting sentiment from blog content
We now address the problem of using machine
learning techniques to predict the polarity of the
comments based on the blog post contents.
4.1 SVM
Firstly, we use support vector machines (SVM) to
perform classification. We frame the classification
task as follows: The input features to the classifier
are the words in the blog post i.e each blog post is
treated as a bag of words and the output variable is
the binary comment polarity computed in the previ-
14
SentiWordNet PMI
Blog SVM sLDA SVM sLDA
cb 0.56 0.58 0.79 0.79
dk 0.61 0.64 0.75 0.77
my 0.67 0.59 0.87 0.87
rs 0.53 0.55 0.74 0.76
rwn 0.57 0.59 0.90 0.90
Table 3: Accuracy: Using blog posts to predict comment
sentiment polarity
ous section. For our experiments, we used the SVM-
Light package 6 with a simple linear kernel and eval-
uated the classifier using 10 fold cross validation.
Table 3 shows the accuracy of the classifier for the
different blogs and polarity measuring schemes. The
errors in classification can be attributed in part to
the inherent difficulty of the task due to the noise of
the polarity labeling schemes and in part due to the
difficulty in obtaining a signal to predict comment
polarity from the body of the post.
4.2 Supervised LDA
Next, we use Supervised LDA (sLDA) (Blei and
McAuliffe, 2008) to do the classification. sLDA is
a model that is an extension of Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) that models each
document as having an output variable in addition to
the document contents. The output variable in the
classification case is modeled as an output of a lo-
gistic regression model that uses the posterior topic
distribution of the LDA model as features. In this
task, the output variable is +1 or -1 depending on
the polarity of the comment section. In the experi-
ments with sLDA, we set the number of topics as 15
after experimenting with a range of topics and use
10-fold cross validation. The number of topics is set
lower than it usually is with topic modeling, due to
the relatively short length and small number of doc-
uments.
The advantage of sLDA in this task is that we in-
duce topics from the bodies of the blog posts that
serve to characterize the different issues that each
blog addresses. In addition, the logistic regres-
sion parameters indicate how each topic influences
the output variable. Table 4 shows the top 1 or 2
6http://svmlight.joachims.org/
topics with the highest negative and positive logis-
tic regression coefficients for each blog. Inspect-
ing the top words of the topics confirms our no-
tions of the kinds of issues that appeal to the read-
ers of each of the blogs. For instance, in the top-
ics induced from Daily Kos, a very liberal leaning
blog, we see that the most negative topic (i.e. the
topic that contributes the most to potential nega-
tive comments) talks about the Bush adminstration
and Vice President Cheney, which was and remains
quite unpopular with people from the left. The other
negative topic concerns the war in Iraq which was
also very unpopular within people whose beliefs are
liberal-leaning. The most positive topic seemingly
focuses on campaign funding. Our conjecture for
the high comment polarity is the great success in the
then Democratic candidate Obama?s fund raising at-
tempts during the presidential campaign. In the sec-
ond blog, Right Wing News, which is a conservative
blog, we see a different picture. The most negative
topic deals with Islam and Muslim people which are
issues that have tended to evoke negative reactions
from certain sections of people with conservative
political beliefs. Global warming also evoked nega-
tive comments which is consistent with the conser-
vative viewpoint that there isn?t evidence to suggest
that greenhouse gases cause global warming. The
most positive topic seems to be about anti-abortion
issues which is an issue that frequently pops up in
conservative political discourse. Topics from the
other blogs also seem to be in line with the standard
positions taken by liberal and conservatives on lead-
ing issues in US politics like taxation, immigration,
public health and the presidential campaign which
was in full flow at the time the data was collected.
Table 3 shows the accuracy of sLDA in predict-
ing the comment polarity based on the blog posts.
It can be seen from the table that sLDA performs
marginally better than SVM when trained on blog
posts, even though documents are now represented
in the lower dimensional topic space in contrast to
the high dimensional word space that was used with
SVM. sLDA provides the additional advantage of
providing an overall summary of the corpus via the
topic tables it induces.
15
Blog Topic words Topic co-efficient
CB
* bush president news administration house white officials report fox government
office military department public cheney john journal week pentagon national
-0.79
* huckabee giuliani romney mccain republican presidential religious campaign gop
john party candidate mitt rudy mike conservative thompson support paul candidates
0.48
DK
* bush administration congress law government court house intelligence white ex-
ecutive committee time cheney federal course national act president congressional
information
-1.54
* iraq war bush troops news military american president iraqi starts maine cheers
days jeers mccain moreville rightnow day americans people
-0.60
* money health campaign foster energy district million people nrcc dccc care elec-
tion time bill change funds don global federal economy
0.62
MY
* iraq war american military iraqi government people troops bush security united
forces world country surge presence political force maliki afghanistan
-0.50
* people care health don public immigration college political education school is-
sue insurance social system policy real lot isn actually sense
1.05
RS
* economy market people financial economic markets money world rate rates fed-
eral mortgage government credit prices price term inflation reserve oil
-0.30
* tax government taxes money economic care people spending million jobs ameri-
can energy health increase pay economy private free federal business
0.61
RWN
* people muslim world country war american law muslims time police america
rights free peace death city islamic government freedom united
-0.68
* democrats warming global vote election obama energy democratic change votes
climate people john gore political gas don voters party bill
-0.39
* people life women woman time own little love person children world live read
believe god isn school feel mean
0.47
Table 4: Topics from sLDA and weights
SentiWordNet PMI
Blog SVM sLDA SVM sLDA
cb 0.66 0.56 0.79 0.79
dk 0.72 0.59 0.74 0.73
my 0.64 0.61 0.87 0.89
rs 0.65 0.57 0.75 0.80
rwn 0.65 0.60 0.90 0.90
Table 5: Accuracy: Using comments to predict comment
sentiment polarity
4.3 Using comments to predict comment
polarity
In the previous experiments we were using the bod-
ies of the blog posts to predict comment polarity.
There are multiple factors which make this a diffi-
cult task. One major factor is the difficulty of learn-
ing potentially noisy labels using automatic meth-
ods. More interestingly, we operate under the hy-
pothesis that there is signal about comment polarity
in the bodies of the blog posts. To test this hypoth-
esis, we train classifiers on the comment sections
themselves to predict comment polarity. This serves
to eliminate the effect of our hypothesis and focus
on the inherent difficulty in learning the noisy la-
bels. Table 5 shows the results of these experiments.
We see that once again, sLDA results are compara-
ble to the accuracies reported by SVM and that PMI
labels are less noisier than the labels obtained using
16
Evaluating Trained on DK Trained on RWN
DK 0.75/0.77 0.61/0.62
RWN 0.74/0.71 0.90/0.90
Table 6: Cross blog results: Accuracy using SVM/sLDA
SentiWordNet. More importantly, we note that the
accuracy in predicting the comment polarity while
higher than the accuracy in predicting the polarity
from blog posts, is not significantly higher which
strongly suggests that blog posts have quite a bit of
information regarding comment polarity.
4.4 Cross blog experiments
The effect of the nature of the blog on the classifier is
examined by training models on the blog posts from
a conservative blog (RWN) using PMI-determined
polarities as targets and by testing the model by run-
ning liberal blog data (from DK) through it. Simi-
larly, we test RWN blog entries by training it on a
classifier trained on DK posts. The results of the ex-
periments are in Table 6. For easy reference, the
table also includes the accuracies when blogs are
trained using posts from the same blog (obtained
from Table 3). We see that the accuracy in predict-
ing polarity degrades when blog posts are tested on
a classifier trained on posts from a blog of opposite
political affiliation. These results indicate that emo-
tion is tied to the blog and community that one is
involved in.
4.5 Conclusion
We addressed the task of predicting the emotional
response that is induced in political discourses. To
this end, we tackled the tasks of determining the sen-
timent polarity of comments in blogs and the task of
predicting the polarity based on the content of the
blog post. Our approach also characterized the is-
sues talked about in specific blog communities. Our
experiments show that the community specific PMI
method provides a more accurate picture of the sen-
timent in comments than the generic SentiWordNet
technique. We also see that the context of the com-
munity is key as seen in the poor performance of
models trained on blogs from one end of the politi-
cal spectrum in predicting the polarity of responses
to blog posts in communities on the other end of the
spectrum.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
David Blei and Jon McAuliffe, 2008. Supervised Topic
Models, pages 121?128. MIT Press, Cambridge, MA.
D. M Blei, A. Y Ng, and M. I Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
Li-Ning Huang and Vincent Price. 2001. Motivations,
goals, information search, and memory about political
candidates. Political Psychology, 22(4):pp. 665?692.
Linda M. Isbell, Victor C. Ottati, and Kathleen C. Bruns.
2006. Affect and politics: Effects on judgment, pro-
cessing, and information seeking. In David Redlawsk,
editor, Feeling Politics: Emotion in Political Infor-
mation Processing. Palgrave Macmillan, New York,
USA.
Sung-youn Kim, Charles S. Taber, and Milton Lodge.
2008. A Computational Model of the Citizen as Mo-
tivated Reasoner: Modeling the Dynamics of the 2000
Presidential Election. SSRN eLibrary.
Milton Lodge and Charles Taber, 2000. Three Steps
toward a Theory of Motivated Political Reasoning.
Cambridge University Press.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
David P. Redlawsk. 2002. Hot cognition or cool con-
sideration? testing the effects of motivated reasoning
on political decision making. The Journal of Politics,
64(04):1021?1044.
David Redlawsk. 2006. Motivated reasoning, affect, and
the role of memory in voter decision-making. In David
Redlawsk, editor, Feeling Politics: Emotion in Politi-
cal Information Processing. Palgrave Macmillan, New
York, USA.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elec-
tions with twitter: What 140 characters reveal about
political sentiment. In William W. Cohen and Samuel
Gosling, editors, ICWSM. The AAAI Press.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?02, pages 417?424, Stroudsburg, PA, USA.
Association for Computational Linguistics.
17
Tae. Yano, William. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of the North American Associ-
ation for Computational Linguistics Human Language
Technologies Conference.
Appendix A
18
Positive wonderfulness, admirableness, admirability, wonderful, admirable, top-flight, splendid, first-class, fantabu-
lous, excellent, good, balmy, mild, ennoble, dignified, amuse, agree, do good, benefit, vest, prefer, placate,
pacify, mollify, lenify, gentle, conciliate, assuage, appease, filigree, dazzle, admiringly, character, preem-
inence, note, eminence, distinction, radiance, amiability, bonheur, worship, adoration, divination, music,
euphony, judiciousness, essentialness, essentiality, gain, crispness, urbanity, courtesy, decency, modesty,
dedication, integrity, honourableness, honorableness, honor, goodness, good, morality, urbanity, tasteful-
ness, elegance, elegance, healthfulness, nutritiveness, nutritiousness, wholesomeness, fineness, choiceness,
loveliness, fairness, comeliness, beauteousness, picturesqueness, bluffness, good nature, character, props,
joke, jocularity, jest, worthy, salubrious, healthy, virtuous, esthetic, artistic, aesthetic, spiffing, superlative,
sterling, greatest, superb, brilliant, boss, banner, olympian, majestic, straightarrow, wide-eyed, round-eyed,
dewy-eyed, childlike, righteous, answerable, nice, decent, diffident, respected, reputable, self-respecting,
self-respectful, dignified, constructive, sweet, fabulous, fab, charming, admirable, idyllic, idealized, ide-
alised, ennobling, dignifying, nice, incumbent, clean, lucky, intellectual, formidable, awing, awful, awe-
some, awe-inspiring, amazing, important, joking, jocular, jocose, jesting, amicable, kind, genial, therapeu-
tic, sanative, remedial, healing, curative, gracious, gainly, goody-goody, good, superb, solid, good, inspired,
elysian, divine, worthy, quaint, discerning, golden, fortunate, blest, blessed, courteous, thorough, exhaus-
tive, better, benign, pretty, piquant, engaging, attractive, well, veracious, right, grace, goodwill, belong,
accommodate, serve, merit, deserve, shine, radiate, glow, beam, disillusion, disenchant, proclaim, laud,
glorify, extol, exalt, cheer, consider, purify, enervate, recuperate, amusingly, dearly, dear, affectionately,
thoroughly, soundly, well, simply, time, posterboard, fettle, mildness, clemency, successfulness, prosper-
ity, wellbeing, well-being, upbeat, wholeness, haleness, purity, pureness, innocence, antithesis, serendipity,
superordinate, superior, possible, pleaser, idolizer, idoliser, amoralist
Negative tawdry, shoddy, cheapjack, scrimy, unsound, unfit, bad, sorry, sad, pitiful, lamentable, distressing, de-
plorable, abject, unfortunate, inauspicious, humbug, trouble, inconvenience, disoblige, bother, smell, stink,
reek, twinge, sting, prick, burn, sting, burn, bite, desensitize, desensitise, resent, begrudge, pity, compassion-
ate, abreact, agonize, agonise, muddy, settle, moan, groan, impugn, repudiate, deny, reject, disapprove, snub,
repel, rebuff, sting, stick, disapprove, refute, rebut, controvert, foul, curdle, smite, afflict, ease, comfort, ail,
inflame, woefully, sadly, lamentably, deplorably, hard, unluckily, unfortunately, regrettably, alas, worst,
throe, woe, suffering, inconvenience, incommodiousness, solacement, solace, dyspnoea, dyspnea, throe,
shrew, ruffian, rowdy, roughneck, hooligan, bully, plonk, sullenness, moroseness, glumness, moodiness,
malignity, malevolence, guilt, sorrow, ruefulness, rue, regret, dolour, dolor, dolefulness, gloating, gloat,
weakness, self-torture, self-torment, suffering, hurt, distress, torment, curse, straits, pass, head, excoriation,
canard, scurrility, billingsgate, scribble, scrawl, scratch, prejudice, preconception, bias, pill, onus, load, in-
cumbrance, encumbrance, burden, poignancy, pathos, penalty, badness, bad, fault, demerit, hardness, moldi-
ness, harshness, cruelty, cruelness, spitefulness, spite, nastiness, cattiness, bitchiness, malice, malevolency,
malevolence, heinousness, barbarousness, barbarity, atrocity, atrociousness, illegitimacy, unnaturalness, dis-
agreeableness, incongruousness, incongruity, ruggedness, hardness, unneighborliness, unfriendliness, dis-
agreeableness, sadness, lugubriousness, gloominess, shlock, schlock, dreck, mongrel, bastard, shenanigan,
roguishness, roguery, rascality, mischievousness, mischief-making, mischief, deviltry, devilry, devilment,
shitwork, overexertion, overacting, hamming, shlep, schlep, worst, upset, scrofulous, sick, ill, sheltered,
occult, trashy, rubbishy, undivided, worried, upset, disturbed, distressed, disquieted, troubled, unmanage-
able, uncontrollable, mussy, messy, unsympathetic, invalidating, disconfirming, wretched, woeful, miser-
able, execrable, deplorable, bush-league, bush, tinny, sleazy, punk, crummy, chintzy, cheesy, cheap, bum,
inferior, indifferent, lowly, humble, insufficient, deficient, insubordinate, cross-grained, contrarious, spas-
tic, spasmodic, convulsive, unaccepted, unacceptable, nonstandard, unsound, asocial, antisocial, feigned,
broken-down, vicious, reprehensible, deplorable, criminal, condemnable, notorious, infamous, ill-famed,
untreated, modified, limited, unmixed, unmingled, sheer, plain, cretinous, negative, imponderable, vexing,
maddening, infuriating, exasperating, ungrateful, sore, painful, afflictive, harsh, unpeaceable, unforbearing,
unpainted, underivative, scurrilous, opprobrious, abusive, verminous, outrageous, horrific, horrid, hideous,
creepy, pestilent, pernicious, deadly, baneful, paranormal, grotty, nasty, awful, transcendental, preternatural,
otherworldly, nonnatural, simulated, imitation, faux, false, fake, substitute, ersatz, strong, smart, wicked,
terrible, severe, unpitying, ruthless, remorseless, pitiless, unlikeable, unlikable, unmourned, unlamented,
rough, harsh, woeful, woebegone, lugubrious, heartsick, heartbroken, brokenhearted, bitter
Table 7: Seed words used in the PMI technique
19
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 155?162,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Evaluating Joint Modeling of Yeast Biology Literature and Protein-Protein
Interaction Networks
Ramnath Balasubramanyan and Kathryn Rivard and William W. Cohen
School of Computer Science
Carnegie Mellon University
rbalasub,krivard,wcohen@cs.cmu.edu
Jelena Jakovljevic and John Woolford
Deparment of Biological Sciences
Carnegie Mellon University
jelena,jw17@andrew.cmu.edu
Abstract
Block-LDA is a topic modeling approach to
perform data fusion between entity-annotated
text documents and graphs with entity-entity
links. We evaluate Block-LDA in the yeast bi-
ology domain by jointly modeling PubMed R?
articles and yeast protein-protein interaction
networks. The topic coherence of the emer-
gent topics and the ability of the model to re-
trieve relevant scientific articles and proteins
related to the topic are compared to that of a
text-only approach that does not make use of
the protein-protein interaction matrix. Eval-
uation of the results by biologists show that
the joint modeling results in better topic co-
herence and improves retrieval performance in
the task of identifying top related papers and
proteins.
1 Introduction
The prodigious rate at which scientific literature
is produced makes it virtually impossible for re-
searchers to manually read every article to identify
interesting and relevant papers. It is therefore crit-
ical to have automatic methods to analyze the liter-
ature to identify topical structure in it. The latent
structure that is identified can be used for different
applications such as enabling browsing, retrieval of
papers related to a particular sub-topic etc. Such ap-
plications assist in common scenarios such as help-
ing a researcher identify a set of articles to read (per-
haps a set of well-regarded surveys) to familiarize
herself with a new sub-field; helping a researcher to
stay abreast with the latest advances in his field by
identifying relevant articles etc.
In this paper, we focus on the task of organiz-
ing a large collection of literature about yeast biol-
ogy to enable topic oriented browsing and retrieval
from the literature. The analysis is performed using
topic modeling(Blei et al, 2003) which has, in the
last decade, emerged as a versatile tool to uncover
latent structure in document corpora by identifying
broad topics that are discussed in it. This approach
complements traditional information retrieval tasks
where the objective is to fulfill very specific infor-
mation needs.
In addition to literature, there often exist other
sources of domain information related to it. In the
case of yeast biology, an example of such a resource
is a database of known protein-protein interactions
(PPI) which have been identified using wetlab exper-
iments. We perform data fusion by combining text
information from articles and the database of yeast
protein-protein interactions, by using a latent vari-
able model ? Block-LDA (Balasubramanyan and
Cohen, 2011) that jointly models the literature and
PPI networks.
We evaluate the ability of the topic models to re-
turn meaningful topics by inspecting the top papers
and proteins that pertain to them. We compare the
performance of the joint model i.e. Block-LDA with
a model that only considers the text corpora by ask-
ing a yeast biologist to evaluate the coherence of
topics and the relevance of the retrieved articles and
proteins. This evaluation serves to test the utility of
Block-LDA on a real task as opposed to an internal
evaluation (such as by using perplexity metrics for
example). Our evaluaton shows that the joint model
outperforms the text-only approach both in topic co-
155
herence and in top paper and protein retrieval as
measured by precision@10 values.
The rest of the paper is organized as follows. Sec-
tion 2 describes the topic modeling approach used
in the paper. Section 3 describes the datasets used
followed by Section 4 which details the setup of the
experiments. The results of the evaluation are pre-
sented in Section 5 which is followed by the conclu-
sion.
2 Block-LDA
The Block-LDA model (plate diagram in Figure 1)
enables sharing of information between the compo-
nent on the left that models links between pairs of
entities represented as edges in a graph with latent
block structure, and the component on the right that
models text documents, through shared latent topics.
More specifically, the distribution over the entities of
the type that are linked is shared between the block
model and the text model.
The component on the right, which is an extension
of the LDA models documents as sets of ?bags of en-
tities?, each bag corresponding to a particular type
of entity. Every entity type has a topic wise multi-
nomial distribution over the set of entities that can
occur as an instance of the entity type. This model
is termed as Link-LDA(Nallapati et al, 2008) in the
literature.
The component on the left in the figure is a gen-
erative model for graphs representing entity-entity
links with an underlying block structure, derived
from the sparse block model introduced by Parkki-
nen et al (2009). Linked entities are generated from
topic specific entity distributions conditioned on the
topic pairs sampled for the edges. Topic pairs for
edges (links) are drawn from a multinomial defined
over the Cartesian product of the topic set with it-
self. Vertices in the graph representing entities there-
fore have mixed memberships in topics. In con-
trast to Mixed-membership Stochastic Blockmodel
(MMSB) introduced by Airoldi et al (2008), only
observed links are sampled, making this model suit-
able for sparse graphs.
LetK be the number of latent topics (clusters) we
wish to recover. Assuming documents consist of T
different types of entities (i.e. each document con-
tains T bags of entities), and that links in the graph
are between entities of type tl, the generative process
is as follows.
1. Generate topics: For each type t ? 1, . . . , T , and
topic z ? 1, . . . ,K, sample ?t,z ? Dirichlet(?), the
topic specific entity distribution.
2. Generate documents. For every document d ?
{1 . . . D}:
? Sample ?d ? Dirichlet(?D) where ?d is the
topic mixing distribution for the document.
? For each type t and its associated set of entity
mentions et,i, i ? {1, ? ? ? , Nd,t}:
? Sample a topic zt,i ?Multinomial(?d)
? Sample an entity et,i ?
Multinomial(?t,zt,i)
3. Generate the link matrix of entities of type tl:
? Sample piL ? Dirichlet(?L) where piL de-
scribes a distribution over the Cartesian prod-
uct of the set of topics with itself, for links in
the dataset.
? For every link ei1 ? ei2, i ? {1 ? ? ?NL}:
? Sample a topic pair ?zi1, zi2? ?
Multinomial(piL)
? Sample ei1 ?Multinomial(?tl,zi1)
? Sample ei2 ?Multinomial(?tl,zi2)
Note that unlike the MMSB model, this model
generates only realized links between entities.
Given the hyperparameters ?D, ?L and ?, the
joint distribution over the documents, links, their
topic distributions and topic assignments is given by
p(piL,?,?, z, e, ?z1, z2?, ?e1, e2?|?D, ?L, ?) ?
(1)
K?
z=1
T?
t=1
Dir(?t,z|?t)?
D?
d=1
Dir(?d|?D)
T?
t=1
Nd,t?
i=1
?
z(d)t,i
d ?
et,i
t,z(d)t,i
?
Dir(piL|?L)
NL?
i=1
pi?zi1,zi2?L ?
ei1
tl,z1
?ei2tl,z2
156
...
?d
?L
?D
pi L
N L
?
Dim: K x K
Dim: K
z i 1 z i 2
e i 2e i 1
Links
Docs
?t,z
T
K
D
z 1,i
e 1,i
z T ,i
e T ,i
Nd,TNd, 1
?L - Dirichlet prior for the topic pair distribution for links
?D - Dirichlet prior for document specific topic distributions
? - Dirichlet prior for topic multinomials
piL - multinomial distribution over topic pairs for links
?d - multinomial distribution over topics for document d
?t,z - multinomial over entities of type t for topic z
zt,i - topic chosen for the i-th entity of type t in a document
et,i - the i-th entity of type t occurring in a document
zi1 and zi2 - topics chosen for the two nodes participating in the i-th link
ei1 and ei2 - the two nodes participating in the i-th link
Figure 1: Block-LDA
A commonly required operation when using mod-
els like Block-LDA is to perform inference on the
model to query the topic distributions and the topic
assignments of documents and links. Due to the
intractability of exact inference in the Block-LDA
model, a collapsed Gibbs sampler is used to perform
approximate inference. It samples a latent topic for
an entity mention of type t in the text corpus con-
ditioned on the assignments to all other entity men-
tions using the following expression (after collaps-
ing ?D):
p(zt,i = z|et,i, z?i, e?i, ?D, ?) (2)
? (n?idz + ?D)
n?iztet,i + ?
?
e? n
?i
zte?
+ |Et|?
Similarly, we sample a topic pair for every link con-
ditional on topic pair assignments to all other links
after collapsing piL using the expression:
p(zi = ?z1, z2?|?ei1, ei2?, z?i, ?e1, e2??i, ?L, ?)(3)
?
(
nL?i?z1,z2? + ?L
)
?
(
n?iz1tlei1+?
)(
n?iz2tlei2+?
)
(?
e n
?i
z1tle
+|Etl |?
)(?
e n
?i
z2tle
+|Etl |?
)
Et refers to the set of all entities of type t. The n?s
are counts of observations in the training set.
? nzte - the number of times an entity e of type t
is observed under topic z
? nzd - the number of entities (of any type) with
topic z in document d
? nL?z1,z2? - count of links assigned to topic pair
?z1, z2?
The topic multinomial parameters and the topic
distributions of links and documents are easily re-
covered using their MAP estimates after inference
157
using the counts of observations.
?(e)t,z =
nzte + ?
?
e? nzte? + |Et|?
, (4)
?(z)d =
ndz + ?D
?
z? ndz? +K?D
and (5)
pi?z1,z2?L =
n?z1,z2? + ?L?
z?1,z
?
2
n?z?1,z?2? +K
2?L
(6)
A de-noised form of the entity-entity link matrix
can also be recovered from the estimated parame-
ters of the model. Let B be a matrix of dimensions
K ? |Etl | where row k = ?tl,k, k ? {1, ? ? ? ,K}.
Let Z be a matrix of dimensions K ?K s.t Zp,q =
?NL
i=1 I(zi1 = p, zi2 = q). The de-noised matrix M
of the strength of association between the entities in
Etl is given by M = B
TZB.
In the context of this paper, de-noising the
protein-protein interaction networks studied is an
important application. The joint model permits in-
formation from the large text corpus of yeast publi-
cations to be used to de-noise the PPI network and
to identify potential interactions that are missing in
the observed network. While this task is important
and interesting, it is outside the scope of this paper
and is a direction for future work.
3 Data
We use a collection of publications about yeast bi-
ology that is derived from the repository of sci-
entific publications at PubMed R?. PubMed R? is a
free, open-access on-line archive of over 18 mil-
lion biological abstracts and bibliographies, includ-
ing citation lists, for papers published since 1948.
The subset we work with consists of approximately
40,000 publications about the yeast organism that
have been curated in the Saccharomyces Genome
Database (SGD) (Dwight et al, 2004) with anno-
tations of proteins that are discussed in the publi-
cation. We further restrict the dataset to only those
documents that are annotated with at least one pro-
tein from the protein-protein interactions databases
described below. This results in a protein annotated
document collection of 15,776 publications. The
publications in this set were written by a total of
47,215 authors. We tokenize the titles and abstracts
based on white space, lowercase all tokens and elim-
inate stopwords. Low frequency (< 5 occurrences)
terms are also eliminated. The vocabulary that is ob-
tained consists of 45,648 words.
The Munich Institute for Protein Sequencing
(MIPS) database (Mewes et al, 2004) includes a
hand-crafted collection of protein interactions cover-
ing 8000 protein complex associations in yeast. We
use a subset of this collection containing 844 pro-
teins, for which all interactions were hand-curated.
Finally, we use another dataset of protein-protein
interactions in yeast that were observed as a result of
wetlab experiments by collaborators of the authors
of the paper. This dataset consists of 635 interac-
tions that deal primarily with ribosomal proteins and
assembly factors in yeast.
4 Setup
We conduct three different evaluations of the emer-
gent topics. Firstly, we obtain topics from only
the text corpus using a model that comprises of the
right half of Figure 1 which is equivalent to using
the Link-LDA model. For the second evaluation,
we use the Block-LDA model that is trained on the
text corpus and the MIPS protein-protein interac-
tion database. Finally, for the third evaluation, we
replace the MIPS database with the interaction ob-
tained from the wetlab experiments. In all the cases,
we set K, the number of topics to be 15. In each
variant, we represent documents as 3 sets of entities
i.e. the words in the abstracts of the article, the set
of proteins associated with the article as indicated in
the SGD database and finally the authors who wrote
the article. Each topic therefore consists of 3 differ-
ent multinomial distributions over the sets of the 3
kinds of entities described.
Topics that emerge from the different variants can
possibly be assigned different indices even when
they discuss the same semantic concept. To com-
pare topics across variants, we need a method to
determine which topic indices from the different
variants correspond to the same semantic concept.
To obtain the mapping between topics from each
variant, we utilize the Hungarian algorithm (Kuhn,
1955) to solve the assignment problem where the
cost of aligning topics together is determined using
the Jensen-Shannon divergence measure.
Once the topics are obtained, we firstly obtain the
proteins associated with the topic by retrieving the
158
Figure 2: Screenshot of the Article Relevance Annotation Tool
Variant Num. Coherent Topics
Only Text 12 / 15
Text + MIPS 13 / 15
Text + Wetlab 15 / 15
Table 1: Topic Coherence Evaluation
top proteins from the multinomial distribution cor-
responding to proteins. Then, the top articles cor-
responding to each topic is obtained using a ranked
list of documents with the highest mass of their topic
proportion distributions (?) residing in the topic be-
ing considered.
4.1 Manual Evaluation
To evaluate the topics, a yeast biologist who is an
expert in the field was asked to mark each topic with
a binary flag indicating if the top words of the dis-
tribution represented a coherent sub-topic in yeast
biology. This process was repeated for the 3 differ-
ent variants of the model. The variant used to obtain
results is concealed from the evaluator to remove the
possibility of bias. In the next step of the evaluation,
the top articles and proteins assigned to each topic
were presented in a ranked list and a similar judge-
ment was requested to indicate if the article/protein
was relevant to the topic in question. Similar to
the topic coherence judgements, the process was re-
peated for each variant of the model. Screenshots
of the tool used for obtaining the judgments can be
seen in Figure 2. It should be noted that since the
nature of the topics in the literature considered was
highly technical and specialized, it was impractical
to get judgements from multiple annotators.
159
Topic
Pr
ec
isi
on
 @
 10
0.2
0.4
0.6
0.8
1.0 l l l l
l
l l l l l l l
Variant
l With MIPS interactions
Only Text
With Wetlab interactions
(a) Article Retrieval
Topic
Pr
ec
isi
on
 @
 10
0.2
0.4
0.6
0.8
1.0 l
l
l
l l l l l l
Variant
l With MIPS interactions
Only Text
With Wetlab interactions
(b) Protein Retrieval
Figure 3: Retrieval Performance Evaluation (Horizontal lines indicate mean across all topics)
To evaluate the retrieval of the top articles and
proteins, we measure the quality of the results by
computing its precision@10 score.
5 Results
First we evaluate the coherence of the topics ob-
tained from the 3 variants described above. Table
1 shows that out of the 15 topics that were obtained,
12 topics were deemed coherent from the text-only
model and 13 and 15 topics were deemed coherent
from the Block-LDA models using the MIPS and
wetlab PPI datasets respectively.
Next, we study the precision@10 values for each
topic and variant for the article retrieval and protein
retrieval tasks, which is shown in Figure 3. The plots
also show horizontal lines representing the mean of
the precision@10 across all topics. It can be seen
from the plots that for both the article and protein
retrieval tasks, the joint models work better than the
text-only model on average. For the article retrieval
task, the model trained with the text + MIPS resulted
in the higher mean precision@10 whereas for the
protein retrieval task, the text + Wetlab PPI dataset
returned a higher mean precision@10 value. For
both the protein retrieval and paper retrieval tasks,
the improvements shown by the joint models using
either of the PPI datasets over the text-only model
(i.e. the Link LDA model) were statistically sig-
nificant at the 0.05 level using the paired Wilcoxon
sign test. The difference in performance between the
160
Topic: Protein Structure & Interactions
Top articles using Publications Only Top articles using Block-LDA with Wetlab PPI
* X-ray fiber diffraction of amyloid fibrils. * X-ray fiber diffraction of amyloid fibrils.
* Molecular surface area and hydrophobic effect. * Scalar couplings across hydrogen bonds.
* Counterdiffusion methods for macromolecular
crystallization.
* Dipolar couplings in macromolecular structure
determination.
* Navigating the ClpB channel to solution. * Structure of alpha-keratin.
* Two Rippled-Sheet Configurations of Polypep-
tide Chains, and a Note about the Pleated Sheets.
* Stable configurations of polypeptide chains.
* Molecular chaperones. Unfolding protein fold-
ing.
* The glucamylase and debrancher of S. diastati-
cus.
* The molten globule state as a clue for under-
standing the folding and cooperativity of globular-
protein structure.
* A study of 150 cases of pneumonia.
* Unfolding and hydrogen exchange of proteins:
the three-dimensional ising lattice as a model.
* Glycobiology.
* Packing of alpha-helices: geometrical con-
straints and contact areas.
* The conformation of thermolysin.
Topic: DNA Repair
Top articles using Publications Only Top articles using Block-LDA with Wetlab PPI
* Passing the baton in base excision repair. * Telomeres and telomerase.
* The bypass of DNA lesions by DNA and RNA
polymerases.
* Enzymatic photoreactivation: overview.
* The glucamylase and debrancher of S. diastati-
cus.
* High-efficiency transformation of plasmid DNA
into yeast.
* DNA replication fidelity. * The effect of ultraviolet light on recombination
in yeast.
* Base excision repair. * T-loops and the origin of telomeres.
* Nucleotide excision repair. * Directed mutation: between unicorns and goats.
* The replication of DNA in Escherichia Coli. * Functions of DNA polymerases.
* DNA topoisomerases: why so many? * Immortal strands? Give me a break.
Table 2: Sample of Improvements in Article Retrieval
two joint models that used the two different PPI net-
works were however insignificant which indicates
that there is no observable advantage in using one
PPI dataset over the other in conjunction with the
text corpus.
Table 2 shows examples of poor results of article
retrieval obtained using the publications-only model
and the improved set of results obtained using the
joint model.
5.1 Topics
Table 3 shows 3 sample topics that were retrieved
from each variant described earlier. The table shows
the top words and proteins associated with the top-
ics. The topic label on the left column was assigned
manually during the evaluation by the expert anno-
tator.
Conclusion
We evaluated topics obtained from the joint mod-
eling of yeast biology literature and protein-protein
interactions in yeast and compared them to top-
ics that were obtained from using only the litera-
ture. The topics were evaluated for coherence and
by measuring the mean precision@10 score of the
top articles and proteins that were retrieved for each
topic. Evaluation by a domain expert showed that
161
Topic Top Words & Proteins
Protein Structure & Inter-
actions
Words: protein structure binding residues domain structural beta complex
atp proteins alpha interactions folding structures form terminal peptide helix
model interaction bound domains molecular changes conformational
(Publications Only) Proteins: CYC1 SSA1 HSP82 SUP35 HSP104 HSC82 SSA2 YDJ1 URE2
KAR2 SSB1 SSA4 GCN4 SSA3 SSB2 PGK1 PDI1 SSC1 HSP60 STI1
SIS1 RNQ1 SEC61 SSE1 CCP1
DNA Repair Words:dna recombination repair replication strand single double cells mu-
tations stranded induced base uv mutants mutation homologous virus telom-
ere human type yeast activity telomerase mutant dna polymerase
(Using MIPS PPI) Proteins: RAD52 RAD51 RAD50 MRE11 RAD1 RAD54 SGS1 MSH2
RAD6 YKU70 REV3 POL30 RAD3 XRS2 RAD18 RAD2 POL3 RAD27
YKU80 RAD9 RFA1 TLC1 TEL1 EST2 HO
Vesicular Transport Words:membrane protein transport proteins atp golgi er atpase membranes
plasma membrane vesicles cells endoplasmic reticulum complex fusion
ca2 dependent translocation vacuolar intracellular yeast lipid channel hsp90
vesicle
(Using Wetlab PPI) Proteins: SSA1 HSP82 KAR2 PMA1 HSC82 SEC18 SSA2 YDJ1 SEC61
PEP4 HSP104 SEC23 VAM3 IRE1 SEC4 SSA4 SEC1 PMR1 PEP12
VMA3 VPH1 SSB1 VMA1 SAR1 HAC1
Table 3: Sample Topics
the joint modeling produced more coherent topics
and showed better precision@10 scores in the article
and protein retrieval tasks indicating that the model
enabled information sharing between the literature
and the PPI networks.
References
Edoardo M. Airoldi, David Blei, Stephen E. Fienberg,
and Eric P. Xing. 2008. Mixed membership stochastic
blockmodels. Journal of Machine Learning Research,
9:1981?2014, September.
Ramnath Balasubramanyan and William W. Cohen.
2011. Block-LDA: Jointly modeling entity-annotated
text and entity-entity links. In SDM, pages 450?461.
SIAM / Omnipress.
David. M Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Selina S. Dwight, Rama Balakrishnan, Karen R.
Christie, Maria C. Costanzo, Kara Dolinski, Sta-
cia R. Engel, Becket Feierbach, Dianna G. Fisk,
Jodi Hirschman, Eurie L. Hong, Laurie Issel-Tarver,
Robert S. Nash, Anand Sethuraman, Barry Starr,
Chandra L. Theesfeld, Rey Andrada, Gail Binkley,
Qing Dong, Christopher Lane, Mark Schroeder, Shuai
Weng, David Botstein, and Michael Cherry J. 2004.
Saccharomyces genome database: Underlying prin-
ciples and organisation. Briefings in bioinformatics,
5(1):9.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(1-2):83?97.
Hans-Werner Mewes, C. Amid, Roland Arnold, Dmitrij
Frishman, Ulrich Gldener, Gertrud Mannhaupt, Martin
Mnsterktter, Philipp Pagel, Normann Strack, Volker
Stmpflen, Jens Warfsmann, and Andreas Ruepp. 2004.
MIPS: Analysis and annotation of proteins from whole
genomes. Nucleic Acids Res, 32:41?44.
Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and
William W. Cohen. 2008. Joint latent topic models
for text and citations. In Proceeding of the 14th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 542?550, Las Vegas,
Nevada, USA. ACM.
Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, and
Samuel Kaski. 2009. A block model suitable for
sparse graphs. In Proceedings of the 7th International
Workshop on Mining and Learning with Graphs (MLG
2009), Leuven. Poster.
162
