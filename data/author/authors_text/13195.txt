Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 49?56, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Semantic Similarity for Detecting Recognition Errors in Automatic 
Speech Transcripts 
 
 
Diana Inkpen Alain D?silets 
School of Information Technology and Engineering Institute for Information Technology 
University of Ottawa National Research Council of Canada 
Ottawa, ON, K1N 6H5, Canada Ottawa, ON, K1AOR6, Canada 
diana@site.uottawa.ca alain.desilets@nrc-cnrc.gc.ca 
 
  
Abstract 
Browsing through large volumes of spoken 
audio is known to be a challenging task for 
end users. One way to alleviate this prob-
lem is to allow users to gist a spoken audio 
document by glancing over a transcript 
generated through Automatic Speech Rec-
ognition. Unfortunately, such transcripts 
typically contain many recognition errors 
which are highly distracting and make gist-
ing more difficult. In this paper we present 
an approach that detects recognition errors 
by identifying words which are semantic 
outliers with respect to other words in the 
transcript. We describe several variants of 
this approach. We investigate a wide range 
of evaluation measures and we show that 
we can significantly reduce the number of 
errors in content words, with the trade-off 
of losing some good content words.  
1 Introduction 
Spoken audio documents are becoming more and 
more common place due to the rising popularity of 
technologies such as: video and audio conferenc-
ing, video web-casting and digital cameras for the 
consumer market. Unfortunately, speech docu-
ments are inherently hard to browse because of 
their transient nature.  For example, imagine trying 
to locate the audio segment in the recording of a 
60-minute meeting, where John talked about pro-
ject X. Typically, this would require fast forward-
ing through the audio by some amount, then 
listening and trying to remember if the current seg-
ment was spoken before or after the desired seg-
ment, then fast-forwarding or backtracking by a 
small amount, and so on.  
One way to make audio browsing of audio docu-
ments more efficient is to allow the user to navi-
gate through a textual transcript that is cross-
referenced with corresponding time points into the 
original audio (Nakatani et al 1998; Hirschberg et 
al. 1999). Such transcripts can easily be produced 
with Automatic Speech Recognition (ASR) sys-
tems today. Unfortunately, such transcripts typi-
cally contain recognition errors that make them 
hard to browse and understand. Although Word 
Error Rates (WER) of the order of 20% can be 
achieved for broadcast quality audio, the WER for 
more common situations (ex: less-than-broadcast 
quality recordings of meetings) is typically in the 
order of 50% or more.  
The work we present in this paper aims at auto-
matically identifying recognition errors and remov-
ing them from the transcript, in order to make 
gisting and browsing of the corresponding audio 
more efficient. For example, consider the follow-
ing portion of a transcript that was produced with 
the Dragon NaturallySpeaking speech recognition 
system from the audio of a meeting: 
?Weenie to decide quickly whether local for large 
expensive plasma screen aura for a bunch of 
smaller and cheaper ones and Holland together? 
Now consider the following filtered transcript 
where recognition errors were automatically blot-
ted out using our proposed algorithm:  
? ... to decide quickly whether ... large expensive 
plasma screen ... for a bunch of smaller and 
cheaper ones and ... together? 
We believe that transcripts like this second one 
may be more efficient for gisting and browsing the 
49
content of the original audio whose correct tran-
script is: 
?We need to decide quickly whether we will go for 
a large expensive plasma screen or for a bunch of 
smaller and cheaper ones and tile them together.? 
Our approach to filtering recognition errors is to 
identify semantic outliers. By this, we mean 
words that do not cohere well semantically with 
other words in the transcript. More often than not, 
such outliers turn out to be mistranscribed words. 
We present several variants of an algorithm for 
identifying semantic outliers, and evaluate them in 
terms of how well they are able to filter out recog-
nition errors. 
2 Related Work 
Hirschberg et al (1999), and Nakatani et al (1998) 
proposed the idea of using automatic transcripts for 
gisting and navigating audio documents. Text-
based summarization techniques on automatic 
speech transcription have also been used. For ex-
ample, the method of D?silets et al (2001) was 
found to produce accurate keyphrases for transcrip-
tions with Word Error Rates (WER) in the order of  
25%, but performance was less than ideal for tran-
scripts with WER in the order of 60%. With such 
transcripts, a large proportion of the extracted key-
phrases included serious transcription errors. Ink-
pen and D?silets (2004) presented an experiment 
that filters out errors in keywords extracted from 
speech, by identifying the keywords that are not 
semantically close to the rest of the keywords.  
Semantic similarity measures were used for 
many tasks. Two examples are: real-word error 
correction (Budanitsky and Hirst, 2000) and an-
swering synonym questions (Turney, 2001), 
(Jarmasz and Szpakowicz, 2003).  
There is a lot of research on confidence meas-
ures for identifying errors in speech recognition 
output. Most papers on this topic use information 
that is internal to the ASR system, generated by the 
decoder during the recognition process. Examples 
are likelihood ratios derived by a Viterbi decoder 
(Gillick et al, 1997), measures of competing 
words at a word boundary (Cox and Rose, 1996), 
word score densities in N-best lists, and various 
acoustic and phonetic features. Machine learning 
techniques were used to identify the best combina-
tions of features for classification (Chase, 1997) 
(Schaaf and Kemp, 1997) (Ma et al, 2001) 
(Skantze and Edlund, 2004) (Zhou and Meng, 
2004) (Zhou et al, 2005). Some of these methods 
achieve good performance, although they use dif-
ferent test sets and report different evaluation 
measures from the set we enumerate in Section 6.  
In our work, we use information that is external 
to the ASR system, because new knowledge seems 
likely to help in the detection of semantic outliers.  
In this respect, the work of Cox and Dasmahapatra 
(2000) is closest to ours. They compared the accu-
racy of a measure based on Latent Semantic 
Analysis (LSA) (Landauer and Dumais, 1997) to 
an ASR-based confidence measure, and found that 
the ASR-based measure (using N-best lists) outper-
formed the LSA approach. While the N-best lists 
approach was better at the high-Recall end of the 
spectrum, the LSA was better at the high-Precision 
end. They also showed that a hybrid combination 
of the two approaches worked best. Our work is 
similar to the LSA-based part of Cox and Dasma-
hapatra, except that we use Point-wise Mutual 
Information (PMI) instead of LSA. Because PMI 
scales up to very large corpora, it has been shown 
to work better than LSA for assessing the semantic 
similarity of words (Turney, 2001). Another dis-
tinguishing feature is that Cox and Dasmahapatra 
only looked at transcripts with moderate WER, 
whereas we additionally evaluate the technique for 
the purpose of doing error filtering on transcripts 
with high WER, which are more typical of non-
broadcast conversational audio.   
3 The Data 
We evaluated our algorithms on a randomly se-
lected subset of 100 stories from the TDT2 English 
Audio corpus. We conducted experiments with two 
types of automatically-generated speech tran-
scripts. The first ones were generated by the 
NIST/BBN time-adaptive speech recognizer and 
have a moderate WER (27.6%), which is represen-
tative of what can be obtained with a speaker-
independent ASR system tuned for the Broadcast 
News domain. In the rest of this paper, we refer to 
these moderate accuracy transcripts as the BBN 
dataset. The second set of transcripts was obtained 
using the Dragon NaturallySpeaking speaker-
dependent recognizer. Their WER (62.3%) was 
much higher because the voice model was not 
trained for speaker-independent broadcast quality 
audio. These transcripts approximate the type of 
50
high WER seen in more casual less-than-broadcast 
quality audio. We refer to these transcripts as the 
Dragon dataset. 
4 The method 
Our algorithm tries to detect recognition errors by 
identifying and filtering semantic outliers in the 
transcripts. In other words, it declares as recogni-
tion errors all the words with low semantic similar-
ity to other words in the transcript. The algorithm 
focuses on content words, i.e., words that do not 
appear in a list of 779 stopwords (including closed-
class words, such as prepositions, articles, etc.). 
The reason to ignore stopwords is that they tend to 
co-occur with most words, and are therefore se-
mantically coherent with most words. The basic 
algorithm for determining if a word w is a recogni-
tion error is as follows.  
 
1. Compute the neighborhood N(w) of w as the 
set of content words that occur before and after w 
in a context window (including w itself).  
 
2. Compute pair-wise semantic similarity scores 
S(wi, wj) between all pairs of words wi ? wj (in-
cluding w) in the neighborhood N(w), using a se-
mantic similarity measure. Scale up those S(wi, wj) 
by a constant so that they are all non-negative, and 
the smallest one is 0. 
 
3. For each wi in the neighborhood N(w) (includ-
ing w), compute its semantic coherence SC(wi). 
by ?aggregating? the pair-wise semantic similari-
ties S(wi, wj) of wi with all its neighbors (wi ? wj) 
into a single number. 
 
4. Let SCavg be the average of SC(wi) over all wi in 
the neighborhood N(w). 
 
5. Label w as a recognition error if SC(w) < 
K?SCavg, where K is a parameter that allows us to 
control the amount of error filtering (K% of the 
average semantic coherence score). Low values of 
K mean little error filtering and high values of K 
mean a lot of error filtering.  
 
We tested a number of variants of Steps 1-3. For 
Step 1, we experimented with two ways of com-
puting the neighborhood N(w). The first approach 
was to set N(w) to be all the words in the transcript 
(the All variant). The second neighborhood ap-
proach was to set N(w) to be the set of 10 content 
words before and after w in the transcript (the 
Window variant).  
For Step 2 we experimented with two different 
measures for evaluating the pair-wise semantic 
similarities S(wi, wj). The first measure used a 
hand-crafted dictionary (the Roget variant) 
whereas the second one used a statistical measure 
based on a large corpus (the PMI variant).  
For Step 3 we experimented with different 
schemes for ?aggregating? the pair-wise semantic 
similarities S(wi, wj) into a single semantic coher-
ence number SC(wi) for a given word wi. The first 
aggregation scheme was simply to average the 
SC(wi) values (the AVG variant). Note that with 
this scheme, we filter words that do not cohere 
well with all the words in the neighborhood N(w). 
This might be too aggressive in the case of the All 
variant, especially for longer or multi-topic audio 
documents. Therefore, we investigated other ag-
gregation schemes that only required words to co-
here well with a subset of the words in N(w). The 
second aggregation scheme was to set SC(wi) to 
the value of the most similar neighbor in N(w) (the 
MAX variant). The third aggregation scheme was 
to set SC(wi) to the average of the 3 most similar 
neighbors in N(w) (the 3MAX variant).  
Thus, there are altogether 2x2x3 = 12 possible 
configurations of the algorithm. In the rest of this 
paper, we will refer to specific configurations us-
ing the following naming scheme: Step1Variant-
Step2Variant-Step3Variant. For example, All-
PMI-AVG means the configuration that uses the 
All variant of Step 1, the PMI variant of Step 2, 
and the AVG variant of step 3. 
It is worth noting that all configurations of this 
algorithm are computationally intensive, mainly 
because of Step 2. However, since our aim is to 
provide transcripts for browsing audio recordings, 
we do not have to correct errors in real time.  
5 Choosing a semantic similarity measure 
Semantic similarity refers to the degree with which 
two words (two concepts) are related. For example, 
most human judges would agree that paper and 
pencil are more closely related than car and 
toothbrush. We use the term semantic similarity in 
this paper in a more general sense of semantic re-
latedness (two concepts can be related by their 
context of use without necessarily being similar).  
51
There are three types of semantic similarity 
measures: dictionary-based (lexical taxonomy 
structure), corpus-based, and hybrid. Most of the 
dictionary-based measures use path length in 
WordNet ? for example (Leacock and Chodorow, 
1998), (Hirst and St-Onge, 1998).  The corpus-
based measures use some form of vector similarity. 
The cosine measure uses frequency counts in its 
vectors and cosine to compute similarity; the sim-
pler methods use binary vectors and compute coef-
ficients such as: Matching, Dice, Jaccard, and 
Overlap. Examples of hybrid measures, based on 
WordNet and small corpora, are: Resnik (1995), 
Jiang and Conrath (1997), Lin (1998). All diction-
ary-based measures have the disadvantage of lim-
ited coverage: they cannot deal with many proper 
names and new words that are not in the diction-
ary. For WordNet-based approaches, there is the 
additional issue that they tend to work well only 
for nouns because the noun hierarchy in WordNet 
is the most developed. Also, most of the WordNet-
based measures do not work for words with differ-
ent part-of-speech, with small exceptions such as 
the extended Lesk measure (Banerjee and Peder-
sen, 2003).  
We did a pre-screening of the various semantic 
similarity measures in order to choose the one 
measure of each type (dictionary-based and cor-
pus-based) that seemed most promising for our 
task of detecting semantic outliers in automatic 
speech transcripts. The dictionary-based ap-
proaches that we evaluated were: the WordNet-
based measure by Leacock and Chodorow (1987), 
and one other dictionary-based measure that uses 
the Roget thesaurus. The Roget measure (Jarmasz 
and Szpakowicz, 2003) has the advantage that it 
works across part-of-speech. The corpus-based 
measures we evaluated were: (a) the cosine meas-
ure based on word co-occurrence vectors (Lesk, 
1969), (b) a new method that computes the Pearson 
correlation coefficient of the co-occurrence vectors 
instead of the cosine, and (c) a measure based on 
point-wise mutual information. We computed the 
first two measures on the 100-million-words Brit-
ish National Corpus (BNC)1, and the third one on a 
much larger-corpus of Web data (one terabyte) 
accessed through the Waterloo Multitext system 
(Clarke and Terra, 2003). The reason for using 
corpora of different sizes is that PMI is the only 
                                                          
1 http://www.natcorp.ox.ac.uk/index.html 
one of the three corpus-based approaches that 
scales up to a terabyte corpus. 
We describe here in detail the PMI corpus-based 
measure, because it is the most important for this 
paper. The semantic similarity score between two 
words w1 and w2 is defined as the probability of 
seeing the two words together divided by the prob-
ability of each word separately: PMI(w1,w2) = log 
[P(w1,w2) / (P(w1)?P(w2))] =  log [C(w1,w2)?N / 
(C(w1)?C(w2))], where C(w1,w2), C(w1), C(w2) are 
frequency counts, and N is the total number of 
words in the corpus. Such counts can easily and 
efficiently be retrieved for a terabyte corpus using 
the Waterloo Multitext system. 
In order to assess how well the semantic similar-
ity measures correlate with human perception, we 
use the set of 30 word pairs of Miller and Charles 
(1991), and the 65 pairs of Rubenstein and Goode-
nough (1965). Both used humans to judge the simi-
larity. The Miller and Charles pairs were a subset 
of the Rubenstein and Goodenough pairs. Note that 
both of those sets were limited to nouns that ap-
peared in the Roget thesaurus, and they are there-
fore favorably biased towards dictionary-based 
approaches. Table 1 shows the correlation of 5 
similarity measures for the Rubenstein and Goode-
nough (R&G) and Miller and Charles (M&C) data-
set. Note that although there are many WordNet-
based semantic similarity measures, we only show 
correlations for Leacock and Chodorow (L&C) 
because it was previously shown to be better corre-
lated (Jarmasz and Szpakowicz, 2003). We do not 
show figures for hybrid measures either because 
the same study showed L&C to be better. 
 
Table 1: Correlation between human assigned and various 
machine assigned semantic similarity scores. 
 Dictionary-based Corpus-based 
 L&C Roget Cos. Corr. PMI 
M&C 0.821 0.878 0.406 0.438 0.759 
R&G 0.852 0.818 0.472 0.517 0.746 
 
We see that the WordNet-based L&C measure 
based (Leacock and Chodorow, 1998 and the Ro-
get measure (Jarmasz and Szpakowicz, 2003) both 
achieve high correlations but the two vector cor-
pus-based measures (Cosine and Pearson Correla-
tion) achieve much lower correlation. The only 
corpus-based measure that does well is PMI, 
probably because of the much larger corpus.  
52
We decided to experiment with two of the meas-
ures (one corpus-based and one thesaurus based) 
for computing the semantic similarity of word 
pairs in Step 2 of the algorithm described in Sec-
tion 3. The two measures are: PMI computed on 
the Waterloo terabyte corpus and the Roget-based 
measure. These two seem the most promising 
given the nature of our task and the correlation fig-
ures reported above. 
6 Evaluation Measures 
We use several evaluation measures to determine 
how well our algorithm works for identifying se-
mantic outliers. As summarized in Table 2, the task 
of detecting recognition errors can be viewed as a 
classification task. For each word, the algorithm 
must predict whether or not that word was tran-
scribed correctly.  
 
Table 2: Recognition error detection can be seen as a classifi-
cation task. 
 
 
 Correctly 
transcribed 
(actual) 
NOT Correctly 
transcribed 
(actual) 
Correctly 
 transcribed 
 (predicted) 
True Positive 
(TP) 
False Positive 
(FP) 
NOT Correctly 
transcribed 
 (predicted) 
False Negative 
(FN) 
True Negative 
(TN) 
 
Note that we decide if a word is actually cor-
rectly transcribed or not by using the alignment of 
an automatic transcript with the manual transcript. 
A standard evaluation tool (sclite2) computes WER 
by counting the number of substitutions, deletions, 
and insertions needed to align a reference tran-
script with a hypothesis file. It also marks the 
words that are correct in automatic transcript (the 
hypothesis file). The rest of the words are the ac-
tual recognition errors (the insertions or substitu-
tions). The deletions ? words that are absent from 
the automatic transcript ? cannot be tagged by the 
confidence measure. 
We define the following performance measures 
in order to evaluate the improvement of the filtered 
transcripts compared to the initial transcripts:  
 
1. Word error rate in the initial transcript and in 
the filtered transcript. These measures can be com-
puted with and without stopwords (for which our 
                                                          
2 http://www.nist.gov/speech/tools/ 
algorithm does not apply). Note that WER without 
stopwords could be slightly lower than traditional 
WER mostly because content words tend to be rec-
ognized more accurately than stopwords (D?silets 
et al 2001). When filtering out semantic outliers, 
there will be gaps in the filtered transcript, there-
fore the general WER might not improve because 
it penalizes heavily the deletions.  
 
2. Content word error rate (cWER). This is the 
error rate in an automatic transcript (initial or fil-
tered) from the point of view of the confidence 
measure, for the content words only. It penalizes 
the words in the automatic transcripts that should 
not be there, but not any missing words (no dele-
tions are penalized). In the case of a transcript fil-
tered by our algorithm, it excludes not only the 
stopwords, but also the filtered words. We com-
puted cWER with sclite without penalizing for the 
gaps created by the filtered words.  
 
3. The percentage of lost good content words 
(%Lost). This is the percentage of correctly rec-
ognized content words which are lost in the proc-
ess of filtering out recognition errors, defined as:  
%Lost = 100 * FN / (TP + FN). We could also 
compute the percent of discarded words, without 
regard if they should have been filtered out or not. 
D = (TN + FN) / (TP + FP + TN + FN). 
 
4. Precision (P), Recall (R) and F-measure. Pre-
cision is the proportion of truly correct words con-
tained in the list of content words which the 
algorithm labeled as correct. Recall is the propor-
tion of truly correct content words that the algo-
rithm was able to retain. F-measure is the 
geometric mean of P and R and expresses a trade-
off between those two measures.  P = TP / (TP + 
FP); R = TP / (TP + FN); F = 2PR / (P+R). 
7 Results 
We ran various configurations of the algorithm 
described in Section 4 on the 100 story sample 
from the TDT2 corpus. This section discusses the 
results of those experiments. We studied the Preci-
sion-Recall (P-R) curves for various configurations 
of our algorithm over the 100 stories, for the two 
types of transcripts: the BBN and Dragon datasets. 
Figures 1 and 2 show an example for each dataset. 
Each point on a P-R curve shows the Precision and 
Recall for one value of K in {0, 20, 40, 60, 80, 
53
100, 120, 140, 160, 180, 200}. Points on the left 
correspond to aggressive filtering (high values of 
K), whereas points on the right correspond to leni-
ent filtering (low values of K).  
First, we looked at the relative merits of the two 
semantic similarity measures (PMI and Roget) for 
Step 2. Figures 1 and 2 plot the P-R curves for the 
All-PMI-AVG and All-Roget-AVG configurations. 
The graphs clearly indicate that PMI performs bet-
ter, especially for the high WER Dragon dataset. 
So PMI was used in the rest of the experiments.  
Next, we looked at the variants for setting up the 
neighborhood N(w) in Step 1 (All vs. Window). 
The three P-R curves for All-PMI-X and Window-
PMI-X for all aggregation approaches X in {AVG, 
MAX, 3MAX} are not shown here because they 
were similar to the P-PMI curves from Figures 1 
and 2, for the BBN dataset and for the Dragon 
dataset, respectively. The Window variant was 
marginally better for X=MAX on both datasets, as 
well as for X=3MAX on the BBN dataset. In all 
other cases, the Window and All variants per-
formed approximately the same.  
Next, we looked at the different schemes for ag-
gregating the pair-wise similarity scores in Step 3 
(AVG, MAX, 3MAX). By plotting the P-R curves 
for All-PMI-AVG, All-PMI-MAX, and All-PMI-
3MAX for both datasets we obtained again curves 
similar to the P-PMI curves from Figures 1 and 2. 
It seemed that AVG performs slightly better for 
high Recall, the difference being more marked 
when there is no windowing or when we are work-
ing on the Dragon dataset. The 3MAX and MAX 
variants seemed to be slightly better at high Preci-
sion with acceptable Recall values, with 3MAX 
being always equal or very slightly better than 
MAX. In an audio gisting and browsing context 
Precision is more important than Recall, therefore 
we can choose 3MAX. 
Having established Window-PMI-3MAX as one 
of the better configurations, we now look more 
closely at its performance.  
Figures 3 and 4 show how the content word er-
ror rate (cWER), the percentage of lost good words 
(%Lost), and the F-measure vary as we apply more 
and more aggressive error filtering (by increasing 
K) to both datasets. We see that our semantic out-
lier filtering approach is able to significantly re-
duce the number of transcription errors, while 
losing some correct words. For example, with the  
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Recall
P
re
ci
si
on
P-PMI
P-Roget
 
Fig 1: P-R curves of PMI vs. Roget (with All and AVG) on 
the BBN dataset. Each P-R point corresponds to a different 
value of the threshold K (high Recall for low values of K, high 
Precision for high values of K). 
 
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Recall
P
re
ci
si
on
P-PMI
P-Roget
 
Fig 2: P-R curves of PMI vs. Roget (with All and AVG) on 
the Dragon dataset 
 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
K (threshold)
cW
E
R
 / 
%
 lo
st
 / 
F
 
cWER-BBN
%Lost-BBN
F-measure
 
Fig.3. Content Words Error Rate (cWER), %Lost good key-
words (%Lost) and F-measure as a function of the filtering 
level K for the Window-PMI-3MAXconfiguration on the BBN 
dataset. 
 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
K (threshold)
cW
E
R
 / 
%
 lo
st
 / 
F
cWER-Dragon
%Lost-Dragon
F-measure
 
Fig.4. Content Words Error Rate (cWER), %Lost good key-
words (%Lost) and F-measure as a function of the filtering 
level K for the Window-PMI-3MAX configuration on the 
Dragon dataset. 
54
 
moderately accurate BBN dataset, we can reduce 
cWER by 50%, while losing 45% of the good con-
tent words (K=100).  For the low accuracy Dragon 
dataset, we can reduce cWER by 50%, while los-
ing 50% of the good content words (K=120). We 
can choose lower thresholds, for smaller reduction 
in cWER but smaller percent of lost good content 
words. Even small reductions in cWER are impor-
tant, especially for less-than-broadcast conditions 
where WER is initially very high.  
In general, we were not able to show an im-
provement in WER computed in a standard way 
(item 1 in Section 6), because of the high penalty 
due to deletions for both filtered semantic outliers 
and lost good content words. The percent of lost 
good words is admittedly too high, but this seems 
to be the case for speech error confidence measures 
(which do not remove the words tagged as incor-
rect). Also, for the purpose of audio browsing and 
gisting, we believe that fewer errors even with loss 
of content are preferable for intelligibility.  
Comparing our results to those reported by Cox 
and Dasmahapatra (2000) our PMI-based measure 
seems to performs better than their LSA-based 
measure, judging by the shape of the Precision-
Recall curves. (For example, at Precision=90%, 
they obtained Recall=12%, whereas we obtain 
20%. At Precision=80%, they obtain Recall=50%, 
whereas we get Recall=100%.) Note however that 
their results and ours are not completely compara-
ble since the experiments used different audio cor-
pora (WSJCAM0 vs. TDT2), but those two 
corpora seem to exhibit similar initial WERs (the 
WER appears to be around 30% for WSJCAM0; 
the WER is 27.6% for our BBN dataset). Also, it is 
worth noting the LSA measure was computed 
based on a corpus that was very similar to the au-
dio corpus used to evaluate the performance of the 
measure (both were Wall Street Journal corpora). 
If one was to evaluate this measure on audio from 
a completely different domain (ex: news in the sci-
entific or technical domain), one would expect the 
performance to drop significantly. In contrast, our 
PMI measure was computed based on a general 
sample of the World Wide Web, which was not 
tailored to the audio corpus used to evaluate its 
performance. Therefore, our numbers are probably 
more representative of what would be experienced 
with audio corpora outside of the Wall Street Jour-
nal domain.  
8 Conclusion and Future Work 
We presented a basic method for filtering recogni-
tion errors of content words from automatic speech 
transcripts, by identifying semantic outliers. We 
described and evaluated several variants of the ba-
sic algorithm.  
In future work, we plan to run our experiments 
on other datasets when they become available to 
us. In particular, we want to experiment with 
multi-topic audio documents where we expect 
more marked advantages for windowing and alter-
native aggregation schemes like MAX and 3MAX. 
We plan to explore ways to scale up other corpus-
based semantic similarity measures to large tera-
byte corpora. We plan to explore more approaches 
to detecting semantic outliers, for example cluster-
ing or lexical chains (Hirst and St-Onge, 1997).  
The most promising direction is to combine our 
method with confidence measures that use internal 
information from the ASR system (although the 
internal information is hard to obtain when using 
an ASR as a black box, and it could be recognizer-
specific). A combination is likely to improve the 
performance, with the PMI-based measure contrib-
uting at the high-Precision end and the internal 
ASR measure contributing to the high-Recall end 
of the spectrum. To increase Recall we can also 
identify named entities and not filter them out. 
Some named entities could have high semantic 
similarity with the text if they are frequently men-
tioned in the same contexts in the Web corpus, but 
some names could be common to many contexts. 
Another future direction will be to actually cor-
rect the errors instead of just filtering them out. For 
example, we might look at the top N speech recog-
nizer hypotheses (for a fairly large N like 1000) 
and choose the one that maximizes semantic cohe-
sion. A final direction for research is to conduct 
experiments with human subjects, to evaluate the 
degree to which filtered transcripts are better than 
unfiltered ones for tasks like browsing, gisting and 
searching audio clips. 
Acknowledgments 
We thank the following people: Peter Turney and his col-
leagues for useful feedback; Gerald Penn for feedback on 
earlier versions of this paper; Egidio Terra and Charlie Clarke 
for giving us permission to use the Multitext System, the NRC 
copy; Mario Jarmasz and Stan Szpakowicz for sharing their 
code for the Roget similarity measure; Aminul Islam for the 
55
correlation figures and the correlative measure. Our research is 
supported by the Natural Sciences and Engineering Research 
Council of Canada, University of Ottawa, IBM Toronto Cen-
tre for Advanced Studies, and the National Research Council.  
References 
Alexander Budanitsky and Graeme Hirst. 2001. Semantic 
distance in WordNet: An experimental, application-
oriented evaluation of five measures. Workshop on Word-
Net and Other Lexical Resources, NAACL 2001, Pitts-
burgh, PA, USA, 29-34. 
Satanjeev Banerjee, and Ted Pedersen. 2003. Gloss overlaps 
as a measure of semantic relatedness. In Proceedings of the 
Eighteenth International Joint Conference on Artificial In-
telligence (IJCAI?03), Acapulco, Mexico. 
Charlie Clarke and Egidio Terra. 2003. Passage retrieval vs. 
document retrieval for factoid question answering. ACM 
SIGIR?03, 327-328. 
Stephen Cox and Srinandan Dasmahapatra. 2000. A Semanti-
cally-Based Confidence Measure for Speech Recognition, 
Int. Conf. on Spoken Language Processing, Beijing, China, 
vol. 4, 206-209. 
Stephen Cox and R.C. Rose. 1996. Confidence Measures for 
the SWITCHBOARD Database. IEEE Conf. on Acoustics, 
Speech, and Signal Processing, 511-515. 
Lin Chase. 1997. Word and Acoustic Confidence Annotation 
for Large Vocabulary Speech Recognition, Proceedings of 
Eurospeech'97, Rhodes, Greece, 815-818. 
Alain D?silets, Berry de Brujin, and Joel Martin. 2001. Ex-
tracting keyphrases from spoken audio documents. 
SIGIR?01 Workshop on Information Retrieval Techniques 
for Speech Applications, 36-50. 
Diana Inkpen and Alain D?silets. 2004. Extracting semanti-
cally-coherent keyphrases from speech. Canadian Acous-
tics, 32(3):130-131. 
L.Gillick, Y.Ito, and J.Young. 1997. A Probabilistic Approach 
to Confidence Estimation and Evaluation. IEEE Conf. on 
Acoustics, Speech, and Signal Processing, 266-277. 
Julia Hirschberg, Steve Whittaker, Donald Hindle, Fernando 
Pereira, Amit Singhal. 1999. Finding information in audio: 
a new paradigm for audio browsing and retrieval. Proceed-
ings of the ESCA ETRW Workshop, 26-33. 
Graeme Hirst and David St-Onge. 1998. Lexical chains as 
representations of context for the detection and correction 
of malapropisms. In: C. Fellbaum (editor), WordNet: An 
electronic lexical database and some of its applications, 
The MIT Press, Cambridge, MA, 305-332. 
Mario Jarmasz and Stan Szpakowicz. 2003. Roget's thesaurus 
and semantic similarity, Proceedings of the International 
Conference RANLP-2003 (Recent Advances in Natural 
Language Processing), Borovets, Bulgaria, 212-219. 
Jay J. Jiang and David W. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. In Pro-
ceedings of the International Conference on Research in 
Computational Linguistics (ROCLING X), Taiwan. 
Thomas Landauer and Susan Dumais. 1997. A solution to 
Plato?s problem: representation of knowledge.  Psychologi-
cal Review 104: 211-240. 
Claudia Leacock and Martin Chodorow. 1998. Combining 
local context and WordNet similarity for word sense identi-
fication. In C. Felbaum (editor), WordNet: An Electronic 
Lexical Database, MIT Press, Cambridge, MA, 264-283. 
M.E. Lesk. 1969. Word-word associations in document re-
trieval systems. American Documentation 20(1): 27-38.  
Dekang Lin. 1998. An information-theoretic definition of 
similarity. In Proceedings of the 15th International Confer-
ence of Machine Learning. 
Changxue Ma, Mark A. Randolph, and Joe Drish. 2001. A 
support vector machines-based rejection technique for 
speech recognition. Proceedings of ICASSP'01, Salt Lake 
City, USA, vol. 1, 381-384.  
Lidia Mangu and M. Padmanabhan. 2001. Error corrective 
mechanisms for speech recognition. Proceedings of 
ICASSP'01, Salt Lake City, USA, vol. 1, 29-32.  
George A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity, Language and Cognitive Proc-
esses, 6(1):1-28. 
Christine Nakatani, Steve Whittaker, Julia Hirshberg. 1998. 
Now you hear it, now you don?t: Empirical Studies of Au-
dio Browsing Behavior. Proceedings of the Fifth Interna-
tional Conference on Spoken Language Processing, 
(SLP?98), Sydney, Australia. 
Philip Resnik. 1995. Using information content to evaluate 
semantic similarity. In Proceedings of the 14th Joint Inter-
national Conference of Artificial Intelligence, Montreal, 
Canada, 448-453. 
Herbert Rubenstein and John B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of ACM, 
8(10): 627-633.  
Thomas Schaaf and Thomas Kemp. 1997. Confidence meas-
ures for spontaneous speech recognition, in Proceedings of 
ICASSP?97, Munich, Germany, vol. II, 875-878. 
Gabriel Skantze and J. Edlund. 2004. Error detection on word 
level. In Proceedings of Robust 2004, Norwich. 
Peter D. Turney. 2001. Mining the Web for synonyms: PMI-
IR versus LSA on TOEFL, Proceedings of the Twelfth 
European Conference on Machine Learning (ECML-2001), 
Freiburg, Germany, 491-502.  
Lina Zhou, Jinjuan Feng, Andrew Sears, Yongmei Shi. 2005. 
Applying the Na?ve Bayes Classifier to Assist Users in De-
tecting Speech Recognition Errors. Procs. of the 38th An-
nual Hawaii International Conference on System Sciences). 
Z.Y. Zhou and Helen M. Meng, 2004. A Two-Level Schema 
for Detecting Recognition Errors, Proceedings of the 8th 
International Conference on Spoken Language Processing 
(ICSLP), Korea. 
56
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 64?72,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using First and Second Language Models to Correct Preposition Errors 
in Second Language Authoring
Matthieu Hermet Alain D?silets
School of Information Technology and 
Engineering 
Institute for Information Technology
University of Ottawa National Research Council of Canada
800, King Edward, Ottawa, 
Canada
Bldg M-50, Montreal Road, Ottawa, K1A 0R6, 
Canada
mhermet@site.uottawa.ca alain.desilets@nrc-cnrc.gc.ca
Abstract
In this paper, we investigate a novel approach 
to correcting grammatical  and lexical  errors 
in texts written by second language authors. 
Contrary to previous approaches which tend 
to use unilingual models of the user's second 
language (L2), this new approach uses a sim-
ple  roundtrip  Machine  Translation  method 
which  leverages  information about  both the 
author?s first (L1) and second languages. We 
compare  the  repair  rate  of  this  roundtrip 
translation approach to that of an existing ap-
proach based on a unilingual L2 model with 
shallow  syntactic  pruning,  on  a  series  of 
preposition choice errors. We find no statisti-
cally significant  difference between the two 
approaches,  but find that a hybrid combina-
tion of both does perform significantly better 
than either one in isolation. Finally, we illus-
trate how the translation approach has the po-
tential  of  repairing  very  complex  errors 
which would be hard to treat without leverag-
ing knowledge of the author's L1.
1 Introduction
In this paper, we investigate a novel approach to 
correcting grammatical  and lexical errors in texts 
written  by  second  language  learners  or  authors. 
Contrary to previous approaches which tend to use 
unilingual  models  of  the  user's  second  language 
(L2), this new approach uses a translation model 
based on both the user's first (L1) and second lan-
guages. It has the advantage of being able to model 
linguistic  interference  phenomena,  that  is,  errors 
which are produced through literal translation from 
the author's first language. Although we apply this 
method in the context of French-as-a-Second-Lan-
guage, its principles are largely independent of lan-
guage, and could also be extended to other classes 
of errors. Note that this is preliminary work which, 
in a first step, focuses on error correction, and ig-
nores for now the preliminary step of error detec-
tion which is left for future research.
This work is of interest to applications in Comput-
er-Assisted-Language-Learning (CALL) and Intel-
ligent Tutoring Systems (ITS), where tutoring ma-
terial  often  consists  of  drills  such  as  fill-in-the-
blanks or multiple-choice-questions. These require 
very little  use  of  a learner's  language production 
capacities, and in order to support richer free-text 
assessment capabilities, ITS systems thus need to 
use  error  detection  and  correction  functionalities 
(Heift and Schulze, 2007).
Editing Aids (EA) are tools which assist a user in 
producing written compositions. They typically use 
rules  for  grammar  checking  as  well  as  lexical 
heuristics to suggest stylistic tips, synonyms or fal-
lacious collocations. Advanced examples  of  such 
64
tools include Antidote1 for French and StyleWriter2 
for English. Text Editors like MS Word and Word 
Perfect  also include grammar  checkers,  but  their 
style checking capabilities tend to be limited. All 
these tools can provide useful assistance to editing 
style,  but  they  were  not  designed  to  assist  with 
many errors found typically in highly non-idiomat-
ic sentences produced by L2 authors.
 
Recent work in the field of error correction, espe-
cially as applied to English in the context of En-
glish as  a  Second Language (ESL),  show an in-
creasing  use  of  corpora  and  language  models. 
These have the advantage of offering a model  of 
correctness based on common usage, independent-
ly of any meta-information on correctness. Corpus-
based approaches  are  also able  to  correct  higher 
level lexical-syntactic errors, such as the choice of 
preposition which is  often semantically governed 
by other parts of the sentence.
The reminder of this paper is organized as follows. 
In  section  2,  we  give  a  detailed  account  of  the 
problem of  preposition  errors  in  a  Second  Lan-
guage Learning (SLL) context. Related work is re-
viewed in section 3 and the algorithmic framework 
is  presented  in  section  4.  An  evaluation  is  dis-
cussed in section 5, and conclusions and directions 
for future research are presented in section 6.
2 The Preposition Problem
Prepositions constitute 14% of all tokens produced 
in most languages (Fort & Guillaume 2007). They 
are  reported as  yielding  among  the  highest  error 
class rates across various languages (Izumi, 2004, 
for Japanese, Granger et al, 2001, for French). In 
their analysis of a small corpus of advanced-inter-
mediate French as a Second Language (FSL) learn-
ers,  Hermet  et  al.  (2008)  found  that  preposition 
choice accounted for 17.2 % of all errors. Preposi-
tions can be seen as a special class of cognates, in 
the sense that the same L1 preposition used in dif-
ferent L1 sentences, could translate to several dif-
ferent L2 prepositions.
  
Automatic error detection/correction methods often 
process prepositions and determiners in the same 
way because they both fall in the class of function-
1 www.druide.com
2 www.stylewriter-usa.com
words. However, one can make the argument that 
preposition errors  deserve a  different  and deeper 
kind of treatment, because they tend to be more se-
mantically motivated (event hough some preposi-
tions governed by verbs draw a purely functional 
relation).  In contrast, determiners are not semanti-
cally motivated  and only vary on the  register  of 
quantity (or genre in some languages). 
For example, there are 37 determiners in French, 
most of which can be used interchangeably without 
significantly affecting the syntax of a sentence, and 
often,  not  even  its  meaning  ("I'll  have  one 
coffee"/"I'll  have  a  coffee"/"I'll  have  some 
coffee"/"I'll have my coffee"/"I'll have coffee" are 
all rather alike). Comparatively, there are 85 sim-
ple prepositions and 222 compounds ones and they 
cannot  be  used  interchangeably  without  signifi-
cantly modifying the sense of an utterance, except 
for cases of synonymy.
In this paper, we focus our attention on preposition 
correction only, as it seems to be a more complex 
problem than determiners.  While in principle the 
methods  described  here  could  handle  determiner 
errors, we feel that our framework, which involves 
parsing in combination with a very large language 
model and Machine Translation, constitutes heav-
ier  machinery than  is  warranted  for  that  simpler 
problem.
There are two major causes of preposition errors in 
a SLL context. The first kind is caused by lexical 
confusion  within  the  second language  itself.  For 
example, a L2 author writing in English may erro-
neously use a location preposition like "at" where 
another location preposition like  "in" would have 
been more appropriate. The second kind involves 
linguistic interference between prepositions in L1 
and prepositions in L2 (Granger et al, 2001). For 
example, a Second Language Learner who wants 
to render the following two English sentences in 
French "I go to Montreal" and "I go to Argentina",  
might use the same French preposition "?" for "to", 
when in fact, French usage dictates that you write 
"? Montr?al?,  and  "en Argentine?. Note  that  the 
situation varies greatly from language to language. 
The same two English sentences rendered in Italian 
and German would in fact employ a same preposi-
tion,  whereas  in  Spanish,  different  prepositions 
would also be required as in French.
65
Studies have found that the majority of errors made 
by L2 authors (especially intermediate to advanced 
ones)  are  caused  by  such  linguistic  interference 
(Wang and Garigliano, 1992, Cowan, 1983, p 109). 
Note that this kind of linguistic interference can of-
ten lead to much more severe and hard to repair er-
rors, as illustrated by the following example, taken 
from an actual SLL corpus. Say a native English 
author wants to render "Police arrived at the scene  
of the crime" into French (her L2). Because she is 
not fluent in French, she translates the last part of 
the sentence to "?  la sc?ne de la crime". This liter-
al translation turns out to be highly unidiomatic in 
French, and should instead be written as  "sur les 
lieux du crime" (which in English, would translate 
literally to "on the location of the crime").
One  might  suspect  that  preposition  errors  of  the 
first  type  would be solvable  using unilingual  L2 
language models,  but  that  the second type  might 
benefit from a language model which also takes L1 
into account. This is the main question investigated 
in this paper. 
3 Related Work
Historically, grammatical error correction has been 
done  through  parsing-based  techniques  such  as 
syntactic  constraint-relaxation  (L'haire  &  Vande-
venter-Feltin,  2003),  or  mal-rules  modeling 
(Schneider and McCoy, 1998). But generating the 
rule-bases needed by these types of approaches in-
volves a lot of manual work, and may still in the 
end be too imprecise to convey information on the 
nature and solution of an error. Recently, more ef-
fort has been put in methods that rely on automati-
cally built language models. Typically, this kind of 
work will focus either on a restricted class of errors 
or on specific domains. Seneff and Lee (2006) pro-
pose  a  two-phased  generation-based  framework 
where  a n-gram model  re-ranked by a stochastic 
context-free-grammar model is used to correct sen-
tence-level errors in the language domain of flight 
reservation.  Brockett  et  al.  (2006)  used a Brown 
noise channel translation model to record patterns 
of  determiner  error  correction  on  a  small  set  of 
mass-nouns,  and  reducing  the  error  spectrum  in 
both class and semantic domain, but adding detec-
tion  capabilities.  Note  that  although  they  use  a 
translation model, it processes only text that is in 
one  language.  More  specifically,  the  system 
learned to "translate" from poorly written English 
into correctly written English.
Chodorow et al (2007) employed a maximum en-
tropy  model  to  estimate  the  probability  of  34 
prepositions  based  on  25  local  context  features 
ranging from words  to  NP/VP chunks.  They use 
lemmatization  as  a  means  of  generalization  and 
trained  their  model  over  7  million  prepositional 
contexts,  achieving results  of  84% precision and 
19% recall in preposition error detection in the best 
of the system's configurations. Gamon et al (2008) 
worked on a  similar  approach using only tagged 
trigram left and right contexts: a model of preposi-
tions uses serves to identify preposition errors and 
the Web provides examples of correct form. They 
evaluate their framework on the task of preposition 
identification and report results ranging from 74 to 
45% precision on a set of 13 prepositions. 
Yi et al (2008) use the Web as corpus and send 
segments of sentences of varying length as bag-of-
constituents  queries  to  retrieve  occurrence  con-
texts. The number of the queried segments is a PoS 
condition of "check-points" sensitive to typical er-
rors  made  by L2 authors.  The contexts  retrieved 
are in turn analyzed for correspondence with the 
original input. The detection and correction meth-
ods differ according to the class of the error. Deter-
miner errors call for distinct detection and correc-
tion  procedures  while  collocation  errors  use  the 
same procedure for both. Determiner errors are dis-
covered by thresholds ratios on search hits statis-
tics,  taking  into  account  probable  ambiguities, 
since multiple forms of determiners can be valid in 
a  single  context.  Collocation  errors  on  the  other 
hand, are assessed only by a threshold on absolute 
counts, that is, a form different from the input au-
tomatically signals an error and provides its correc-
tion.  This  suggests  that  detection  and  correction 
procedures coincide when the error ceases to bear 
on a function word.
Similarly, Hermet et al (2008) use a Web as cor-
pus  based  approach  to  address  the  correction  of 
preposition  errors  in  a  French-as-a-Second-Lan-
guage  (FSL)  context.  Candidate  prepositions  are 
substituted for erroneous ones following a taxono-
my of semantic classes, which produces a set of al-
66
ternate sentences for each error. The main interest 
of their study is the use of a syntax-based sentence 
generalization method to maximize the likelihood 
that  at  least  one  of  the  alternatives  will  have  at 
least one hits on the Web. They achieve accuracy 
of 69% in error repair  (no error detection),  on a 
small set of clauses written by FSL Learners. 
Very little work has been done to actually exploit 
knowledge of a L2 author's first language, in cor-
recting  errors.  Several  authors  (Wang  and 
Garigliano,  1992,   Anderson,  1995,  La  Torre, 
1999, Somers, 2001) have suggested that students 
may learn by analyzing erroneous sentences pro-
duced by a MT system, and reflecting on the prob-
able cause of errors, especially in terms of interfer-
ence between the  two languages.  In  this  context 
however, the MT system is used only to generate 
exercises,  as opposed to helping the student  find 
and correct errors in texts that he produces. 
Although it is not based on an MT model, Wang 
and Garigliano propose an algorithm which uses a 
hand-crafted,  domain-specific,  mixed  L1  and  L2 
grammar, in order to identify L1 interference errors 
in L2 sentences. L2 sentences are parsed with this 
mixed  grammar,  giving priority to  L2 rules,  and 
only employing L1 rules as a last resort. Parts of 
the sentence which required the user of  L1 rules 
are  labeled  as  errors  caused  by  L1  interference. 
The paper does not present an actual evaluation of 
the algorithm. 
Finally, a patent by Dymetman and Isabelle (2005) 
describes  several  ways  in  which  MT technology 
could  be  used  to  correct  L2  errors,  but  to  our 
knowledge,  none of  them has  been implemented 
and evaluated yet.
4 Algorithmic Framework
As discussed in section 2, L2 authoring errors can 
be caused by confusions within the L2 itself, or by 
linguistic interference between L1 and L2. In order 
to account for this duality, we investigate the use 
of two correction strategies, one which is based on 
unilingual models of L2, and one which is based 
on translation models between L1 and L2. 
The first approach, called the  Unilingual strategy, 
is illustrated by the example in Figure 1. It  uses a 
web search engine (Yahoo) as a simple, unilingual 
language  model,  where  the  probability  of  a  L2 
phrase is estimated simply by counting its number 
of occurrences in Web pages of that language. A 
severe limitation of this kind of model is that it can 
only estimate the probability of phrases that appear 
at least once on the Web. In contrast, an N-gram 
model (for example) is able to estimate the proba-
bility of phrases that it has never seen in the train-
ing corpus.  In  order  to  deal  with this  limitation, 
syntactic pruning is therefore applied to the phrase 
before it is sent to the search engine, in order to 
eliminate parts which are not core to the context of 
use of  the  preposition,   thus  increasing the odds 
that the pruned sentence will have at least one oc-
currence on the Web.
This pruning and generalization is done by carry-
ing  out  syntactic  analysis  with  the  Xerox  Incre-
mental Parser for the syntactic analysis (Ref XIP). 
XIP is an error robust, symbolic, dependency pars-
er, which outputs syntactic information at the con-
stituency and dependency levels. Its ability to pro-
duce syntactic analyses in the presence of errors is 
Input Sentence
Il y a une grande fen?tre qui permet au soleil <?> 
entrer
     (there is a large window which lets the sun come in)
Syntactic Pruning and Lemmatization
permettre <?> entrer
     (let come in)
Generation of alternate prepositions
semantically related:  dans, en, chez, sur, sous,  
au, dans, apr?s, avant, en, vers
     most common: de, avec, par, pour
Query and sort alternative phrases
   permettre d'entrer: 119 000 hits
   permettre avant entrer: 12 hits
   permettre ? entrer: 4 hits
   permettre en entrer: 2 hits
               ...
? preposition <d'> is returned as correction
Figure 1.  Typical  processing carried out by the  Unilingual 
approach. 
67
particularly  interesting  in  the  context  of  second 
language authoring where the sentences produced 
by the authors can be quite far from grammatical 
correctness. The input sentence is fed to the parser 
as two segments split at error point (in this case, at 
the location of the erroneous preposition). This en-
sures that the parses are correct and not affected at 
dependency  level  by  the  presence  of  error.  The 
syntactic analyses are needed to perform syntactic 
pruning, which is a crucial step in our framework, 
following  Hermet  et.  al  (2008).  Pruning  is  per-
formed by way of chunking heuristics, which are 
controlled  by  grammatical  features,  provided  by 
XIP's  morphological  analysis  (PoS  tagger).  The 
heuristics  are  designed  to  suppress  syntactically 
extraneous  material  in  the  sentence,  such  as  ad-
verbs, some adjectives and some NPs. Adverbs are 
removed in all cases, while adjectives are only re-
moved when they are not in a position to govern a 
Prepositional  Phrase.  NPs are suppressed in con-
trolled cases, based on the verb sub-categorization 
frame, when a PP can be attached directly to the 
preceding verb. In case of ambiguity in the attach-
ment  of the PP,  two versions of the pruned sen-
tence can be produced reflecting two different PP 
attachments.  Lemmatization  of  verbs  is  also car-
ried out in the pruning step.
After pruning, the right and left sides of the sen-
tences are re-assembled with alternate prepositions. 
The replacement  of  prepositions  is  controlled by 
way of semantics. Since prepositions are richer in 
sense than strict function words, they can therefore 
be categorized according to semantics. Saint-Dizier 
(2007)  proposes  such  a  taxonomy,  and  in  our 
framework,  prepositions  have  been  grouped in  7 
non-exclusive categories. Table 1 provides details 
of  this  categorization.  The  input  preposition  is 
mapped  to  all  the  sets  it  belongs  to,  and  corre-
sponding alternates are retrieved as correction can-
didates.  The  6  most  frequent  French  preposition 
are also added automatically to the candidates list. 
The resulting sentences are then sent to the Yahoo 
Search Engine and hits are counted. The number of 
hits returned by each of the queries is used as deci-
sion criteria, and the preposition contained in the 
query with the most hits is selected as the correc-
tion candidate.
While  the above  Unilingual strategy might  work 
for simple cases of L1 interference, one would not 
expect it to work as well  in more complex cases 
where both the preposition and its governing parts 
have been translated too literally. For example, in 
the case of the example from section 2, while the 
Unilingual strategy might be able to effect correc-
tion  ?sur la sc?ne du crime? which is marginally 
better than the original ?? la sc?ne du crime? (12K 
hits versus 1K), it cannot address the root of the 
problem,  that  is,  the  unidiomatic  expression 
?sc?ne  du  crime? which  should  instead  be  ren-
dered as ?lieux du crime? (38K hits). In this partic-
ular case, it is not really an issue because it so hap-
pens that ?sur? is the correct preposition to use for 
both  ?lieux du crime? and  ?sc?ne du crime?, but 
in our experience, that is not always the case. Note 
also  that  the  Unilingual approach  can  only  deal 
with preposition errors (although it would be easy 
enough  to  extend  it  to  other  kinds  of  function 
words),  and  cannot  deal  with  more  semantically 
deep L1 interference.
To address these issues,  we experimented with a 
second  strategy  which  we  will  refer  to  as  the 
Roundtrip  Machine  Translation approach  (or 
Roundtrip MT for short). Note that our approach is 
different from that of Brockett et al (2006), as we 
do  make  use  of  a  truly  multi-lingual  translation 
model.  In  contrast,  Brockett?s  translation  model 
was trained on texts that were written in the same 
language, with the sources being ill-written text in 
the  same  language as  the  properly-formed  target 
texts.  One drawback of our approach however is 
Category Prepositions
Localization in front, behind, after, before, above,  
in, at, on, below, above...
Temporal at, in, after, before, for, during,  
since...
Cause for, because of
Goal for, at
Manner in, by, with, according to...
Material in, of
Possession/Rela-
tion
to, at, with respect to...
Most common to, at, on, with, by, for
Table 1. Categories of prepositions ? the list is given in En-
glish, and  non exhaustive for space reasons.
68
that it may require different translation models for 
speakers with different first languages.
There  are  many  ways  in  which  error-correction 
could be carried out using MT techniques. Several 
of these have been described in a patent by Dymet-
man  and Isabelle  (2005),  but  to  our  knowledge, 
none of them have yet been implemented and eval-
uated. In this paper, we use the simplest possible 
implementation of this concept, namely,  we carry 
out a single round-trip translation. Given a poten-
tially erroneous L2 sentence written by a second 
language author, we translate it to the author's L1 
language, and then back to L2. Even with this sim-
ple approach, we often find that errors which were 
present in the original L2 sentence have been re-
paired  in  the  roundtrip  version.  This  may sound 
surprising,  since  one  would  expect  the  roundtrip 
sentence to be worse than the original, on account 
of the "Chinese Whisper" effect. Our current theo-
ry for why this is not the case in practice goes as 
follows. In the course of translating the original L2 
sentence to L1, when the MT system encounters a 
part  that  is  ill-formed,  it  will  tend  to  use  single 
word entries from its phrase table, because longer 
phrases will not have been represented in the well-
formed L2 training data. In other words, the system 
tends to generate a word for word translation of ill-
formed parts,  which mirrors exactly what L2 au-
thors do when they write poorly formed L2 sen-
tences  by  translating  too  literally  from their  L1 
thought. As a result, the L1 sentence produced by 
the MT system is often well formed for that lan-
guage. Subsequently, when the MT system tries to 
translate that well-formed L1 sentence back to L2, 
it  is  therefore able to use longer entries from its 
phrase table, and hence produce a better L2 trans-
lation of that part than what the author originally 
produced.
We use Google Translate as a translation engine 
for matter of simplicity. A drawback of using such 
an online service is that it  is essentially a closed 
box, and we therefore have little control over the 
translation process,  and no access  to  lower  level 
data generated by the system in the course of trans-
lation (e.g. phrase alignments between source and 
target sentences). In particular, this means that we 
can only generate one alternative L2 sentence, and 
have no way of assessing which parts of this single 
alternative have a high probability of being better 
than their  corresponding parts  in  the  original  L2 
sentence written by the author. In other words, we 
have no way of telling which changes are likely to 
be false positives, and which changes are likely to 
be true positives. This is the main reason why we 
focus only on error repair in this preliminary work.
 
The  roundtrip  sentences  generated  with  Google 
Translate often differ significantly from the origi-
nal L2 sentence, and in more ways than just the er-
roneous preposition used by the author. For exam-
ple, the (pruned) clause "avoir du succ?s en le re-
crutement" ("to be successful in recruiting") might 
come back as as "r?ussir ? recruter" ("to succeed 
in recruiting"). Here, the translation is acceptable, 
but the preposition used by the MT system is not 
appropriate for use in the original sentence as writ-
ten  by  the  L2  author.   Conversely,  a  roundtrip 
translation can be ill-formed, yet use a preposition 
which would be correct in the original L2 sentence. 
For example, "regarder ? des films" ("look at some 
movies") might come back as "inspecter des films" 
("inspect some films"). Here, the original meaning 
is somewhat lost, but the system correctly suggest-
ed that there should be no preposition before ?des 
films?. 
Hence,  in  the  context  of  the  Roundtrip  MT ap-
proach, we need two ways of measuring appropri-
ateness  of  the  suggested  corrections  for  given 
clauses.  The  first  approach,  which  we  call  the 
Clause criteria, looks at whether or not the whole 
clause  has  been  restored  to  a  correct  idiomatic 
form (including correct use of preposition) which 
also preserves the meaning intended by the author 
of the original sentence. Hence, according to this 
approach, an MT alternative may be deemed cor-
rect, even if it chooses a preposition which would 
have been incorrect if substituted in the original L2 
sentence as is.  In the second approach, called the 
Prep criteria, we only look at whether the preposi-
tion used by the MT system in the roundtrip trans-
lation, corresponds to the correct preposition to be 
used in the original L2 clause. Hence, with this ap-
proach, an MT alternative may be deemed correct, 
even if the preposition chosen by the MT system is 
actually inappropriate in the context of the generat-
ed  roundtrip  translation,  or,  even  worse,  if  the 
roundtrip modified the clause to a point where it 
actually means something different than what the 
author actually intended.
69
Of course, in the case of the Prep evaluation crite-
ria, having the MT system return a sentence which 
employs the proper preposition to use in the con-
text of the original L2 sentence is not the end of 
the  process.  In  an  error  correction  context,  one 
must also isolate the correct preposition and insert 
it in the appropriate place in the original L2 sen-
tence. This part of the processing chain is not cur-
rently implemented, but would be easy to do, if we 
used an MT system that provided us with the align-
ment information between the source sentence and 
the target sentence generated. The accuracy figures 
which  we  present  in  this  paper  assume  that  this 
mapping has been implemented and that this par-
ticular part of the process can be done with 100% 
accuracy  (a  claim  which,  while  plausible,  still 
needs to be demonstrated in future work).
We also investigate a third strategy called Hybrid, 
which uses the Roundtrip MT approach as a back-
up for cases where the Unilingual approach is un-
able  to  distinguish  between  different  choices  of 
preposition.  The  latter  typically  occurs  when the 
system is not able to sufficiently prune and gener-
alize the phrase, resulting in a situation where all 
pruned variants yield zero hits on the Web, no mat-
ter what preposition is used. One could of course 
also use the  Unilingual approach as a backup for 
the  Roundtrip  MT approach,  but  this  would  be 
harder to implement since the MT system always 
returns  an  answer,  and  our  use  of  the  online 
Google Translate system precludes any attempt to 
estimate the confidence level of that answer.
In conclusion to this section, we use three preposi-
tion  correction  strategies:  Unilingual,  Roundtrip 
MT and  Hybrid, and in the case of the  Roundtrip 
MT approach,  appropriateness  of  the  corrections 
can  be  evaluated  using  two  criteria:  Prep and 
Clause.
5 Evaluation and Results
5.1 Corpus and Evaluation Metric
For  evaluation,  we  extracted  clauses  containing 
preposition  errors  from  a  small  corpus  of  texts 
written by advanced-intermediate French as a Sec-
ond Language (FSL) student in the course of one 
semester.  The  corpus  contained  about  50,  000 
words  and  133  unique  preposition  errors.  While 
relatively  small,  we  believe  this  set  to  be  suffi-
ciently rich to test the approach. Most clauses also 
presented  other  errors,  including  orthographic, 
tense,  agreement,  morphologic  and  auxiliary  er-
rors, of which only the last two affect parsing. The 
clauses were fed as is to the correction algorithms, 
without first fixing the other types of errors. But to 
our surprise, XIP's robust parsing has proven resis-
tant in that it produced enough information to en-
able correct pruning based on chunking informa-
tion, and we report no pruning errors. Chodorow et 
al. (2008) stress the importance of agreement be-
tween  annotators  when  retrieving  or  correcting 
preposition errors. In our case, our policy has been 
to only retain errors reported by both authors of 
this paper, and correction of these errors has raised 
little matter of dispute.
We evaluated the various algorithms in terms of re-
pair rate, that is, the percentage of times that the al-
gorithm proposed an appropriate fix (the absence 
of a suggestion was taken to be an inappropriate 
fix). These figures are reported in Table 2.
5.2 Discussion
ANOVA of the data summarized in Table 2 reveals 
a statistically significant (p < 0.001) effect of the 
algorithm on repair rate. Although  Roundtrip MT 
performed slightly worse than  Unilingual  (66.4% 
versus 68.7%), this difference was not found to be 
statistically  significant.  On  one  hand,  we  found 
that  round-trip  translation  sometimes  result  in 
spectacular restorations of long and clumsy phrases 
caused by complex linguistic interference. Howev-
er, too often the Chinese whispers effect destroyed 
the sense of the original phrase, resulting in inap-
propriate suggestions. This is evidenced by the fact 
that repair rate of the Roundtrip MT approach was 
significantly  lower  (p  <  0.001)  when  using  the 
Algorithm Repair rate (%)
Unilingual 68.7
Roundtrip MT (Clause) 44.8
Roundtrip MT (Prep) 66.4
Hybrid (Prep) 82.1
Table 2. Results for 3 algorithms on 133 sentences. 
70
Clause criteria (44.8%) than when using the  Prep 
criteria (66.4%). It seems that, in the case of prepo-
sition correction,  roundtrip MT is best  used as a 
way to to generate an L2 alternative from which to 
mine  the  correct  preposition.  Indeed,  flawed  as 
they are,  these  distorted  roundtrip  segments  cor-
rected prepositions  errors  in  66.4% of  the  cases. 
However, for a full picture, the approach should be 
tried on more data, and on other classes of errors. 
Particularly,  we  currently  lack  sufficient  data  to 
test the hypothesis that the approach could address 
the correction of more complex literal translations 
by SL Learners.
In the Unilingual approach, the Yahoo Web search 
engine proved to be an insufficient language model 
for   31 cases  out  of  133,  meaning that  even the 
pruned and generalized phrases  got  zero hits,  no 
matter  what  alternative  preposition  was  used.  In 
those cases,  the  Hybrid approach would then at-
tempt  correction  using  MT  Roundtrip approach. 
This turned out to work quite well, since it resulted 
in an overall accuracy of 82.1%. ANOVA on the 
data for  Hybrid and the two pure approaches re-
veals a significant effect (p < 0.001) of the algo-
rithm factor. Individual t-tests between the Hybrid 
approach and each of the two pure approaches also 
reveal  statistically  significant  differences  (p  < 
0.001). The improvements provided by the hybrid 
approach are fairly substantial, and represent rela-
tive gains of 19.5% over the pure  Unilingual ap-
proach, and 23.6% over the pure Roundtrip MT ap-
proach. The  success  of  this  combined  approach 
might  be attributable to the fact that the two ap-
proaches  follow  different  paradigms.  Roundtrip 
MT uses a model  of controlled incorrectness (er-
rors of anglicism) and  Unilingual a model of cor-
rectness (occurrences of correct forms). In this re-
spect,  the  relatively  low  agreement  between  the 
two approaches (65.4%) is not surprising.
6 Conclusion and Future Work
In this paper,  we have demonstrated for the first 
time that a bilingual Machine Translation approach 
can be used to good effect to correct errors in texts 
written by Second Language Authors or Learners. 
In the case of preposition error correction we found 
that,  while  the MT approach on its  own did not 
perform significantly better  than a unilingual  ap-
proach,  a  hybrid  combination  of  both  performed 
much  better  than  the  unilingual  approach  alone. 
More work needs to be carried out in order to fully 
evaluate the potential of the MT approach. In par-
ticular, we plan to experiment with this kind of ap-
proach to deal with more complex cases of L1 in-
terference  which  result  in  severely  damaged  L2 
sentences.
In this paper, we compared the bilingual MT ap-
proach to a unilingual baseline which used a rela-
tively simple  Web as  a corpus algorithm,  whose 
accuracy is comparable to that reported in the liter-
ature for a similar preposition correction algorithm 
(Yi et al 2008). Notwithstanding the fact that such 
simple  Web  as  a  corpus  approaches  have  often 
been shown to be competitive with (if not better 
than)  more  complex  algorithms  which  cannot 
leverage the full extent of the web (Halevy et al, 
2009), it would be interesting to compare the bilin-
gual MT approach to more sophisticated unilingual 
algorithms  for  preposition  correction,  many  of 
which are referenced in section 3.
Error detection is another area for future research. 
In this paper, we limited ourselves to error correc-
tion, since it could be solved through a very simple 
round-trip translation, without requiring a detailed 
control of the MT system, or access to lower level 
information generated by the system in the course 
of translation (for example, intermediate hypothe-
ses  with  probabilities  and  alignment  information 
between source and target sentences). In contrast, 
we  believe  that  error  detection  with  an  MT ap-
proach will require this kind of finer control and 
access to the guts of the MT system. We plan to in-
vestigate  this  using  the  PORTAGE  MT  system 
(Ueffing et al, 2007). Essentially, we plan to use 
the  MT  system's  internal  information  to  assign 
confidence  scores  to  various  segments  of  the 
roundtrip translation, and label them as corrections 
if this confidence is above a certain threshold. In 
doing this, we will be following in the footsteps of 
Yi et al (2008) who use the same algorithm for er-
ror detection and error correction. The process of 
detecting  an  error  is  simply  one  of  determining 
whether the system's topmost alternative is differ-
ent  from what  appeared in  the original  sentence, 
and whether the system's confidence in that alter-
native is sufficiently high to take the risk of pre-
senting it to the user as a suggested correction. 
71
Acknowledgments
The authors are indebted to the following people 
(all from NRC) for helpful advice on how to best 
exploit MT for second language correction: Pierre 
Isabelle, George Foster and Eric Joanis.
References 
Anderson, D. D. 1995. Machine Translation As a Tool  
in Second Language Learning. CALICO Journal, v13 
n1 p68-97.
Brockett C., Dolan W. B., and Gamon M.. 2006.  Cor-
recting ESL errors using phrasal SMT techniques. In 
Proc. 21st International Conf. On Computational Lin-
guistics and the 44th annual meeting of the ACL, p. 
249?256, Sydney, Australia.
Chodorow  M.,  Tetreault  J.  R.  and  Han  N.-R..  2007. 
Detection of Grammatical Errors Involving Preposi-
tions. In Proc. ACL-SIGSEM Workshop on Preposi-
tions. Prague, Czech Republic.
Cowan, J. R. 1983. Towards a Psychological Theory of  
Interference in Second Language Learning.  In  Sec-
ond Language Learning: Contrastive Analysis, Error 
Analysis, and Related Aspects, edited by B. W. Robi-
nett, J. Schachter, pp 109-119, The Univ. of Michi-
gan Press.
Dymetman M., Isabelle, P. 2007. Second language writ-
ing advisor. US Patent #20070033002 , Feb 8, 2007. 
Fort  K.,  Guillaume B. 2007.  PrepLex: un lexique des  
pr?positions  du  fran?ais  pour  l'analyse  syntaxique. 
TALN 2007, Toulouse, June 5-8.
Gamon  M.,  Gao  J.  F.,  Brockett  C.,  Klementiev  A., 
Dolan W. B., and Vanderwende L. 2008. Using con-
textual speller techniques and language modeling for  
ESL  error  correction.  In  Proceedings  of  IJCNLP 
2008, Hyderabad, India, January.
Granger  S.,  Vandeventer  A.  &  Hamel  M.  J.  2001. 
Analyse de corpus d'apprenants  pour l'ELAO bas? 
sur le TAL. TAL 42(2), 609-621.
Halevy,  A., Norvig,  P., Pereira,  F. 2009.  "The Unrea-
sonable  Effectiveness  of  Data.",  IEEE  Intelligent 
Systems, March/April 2009, pp 8-12.
Heift, T. & Schulze, M. 2007.  Errors and Intelligence  
in  Computer-Assisted  Language  Learning:  Parsers  
and Pedagogues. Routledge.
Hermet, M., D?silets, A., Szpakowicz, S. 2008.  Using 
the Web as a Linguistic Resource to Automatically  
Correct  Lexico-Syntactic  Errors.  In  Proceedings  of 
the LREC'08. Marrakech, Morroco.
Izumi,  E.,  K.  Uchimoto,  and  H.  Isahara.  2004.  The 
overview of the sst speech corpus of Japanese learn-
er English and evaluation through the experiment on 
automatic detection of learners? errors. In LREC.
La Torre, M. D. 1999. A web-based resource to imprive 
translation skills. ReCALL, Vol 11, No3, pp. 41-49.
Lee J. and Seneff S. 2006. Automatic grammar correc-
tion  for  second-language  learners.  In  Interspeech. 
ICSLP. p. 1978-1981. Pittsburgh.
L'haire S. & Vandeventer Faltin A. 2003. Error diagno-
sis in the FreeText project. CALICO 20(3), 481-495, 
special Issue Error Analysis and Error Correction in 
Computer-Assisted  Language  Learning,  T.  Heift  & 
M. Schulze (eds.).
Schneider, D. and McCoy, K. F. 1998. Recognizing syn-
tactic errors in the writing of second language learn-
ers. In Proceedings of COLING/ACL 98.
Saint-Dizier, P. 2007. Regroupement des Pr?positions 
par sens. Undated Report. IRIT. Toulouse. 
http://www.irit.fr/recherches/ILPL/Site-
Equipe/publi_fichier/prepLCS.doc
Somers, Harold. 2001. Three Perspectives on MT in the 
Classroom, MT SUMMIT VIII Workshop on 
Teaching Machine Translation, Santiago de 
Compostela, pages 25-29.
Tetreault  J.  and  Chodorow  M.  2008.  The  Ups  and 
Downs  of  Preposition  Error  Detection. COLING, 
Manchester.
Ueffing,  N.,  Simard,  M.,  Larkin,  S.,  Johnson,  J.  H. 
(2007),  NRC's  PORTAGE system for  WMT 2007, 
ACL-2007 Workshop on SMT, Prague,  Czech Re-
public 2007.
Wang, Y. and Garigliano, R. 1992.  An Intelligent Lan-
guage Tutoring System for Handling Errors caused  
by Transfer. In Proceedings of ITS-92, pp. 395-404.
Yi X., Gao J. F., Dolan W. B., 2008. A Web-based En-
glish Proofing System for English as a Second Lan-
guage Users. In Proceedings of IJCNLP 2008, Hy-
derabad, India, January.
72
