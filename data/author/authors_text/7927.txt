Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 858?865,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Vectors and Two Kinds of Similarity
Akira Utsumi and Daisuke Suzuki
Department of Systems Engineering
The University of Electro-Communications
1-5-1 Chofugaoka, Chofushi, Tokyo 182-8585, Japan
utsumi@se.uec.ac.jp, dajie@utm.se.uec.ac.jp
Abstract
This paper examines what kind of similar-
ity between words can be represented by
what kind of word vectors in the vector
space model. Through two experiments,
three methods for constructing word vec-
tors, i.e., LSA-based, cooccurrence-based
and dictionary-based methods, were com-
pared in terms of the ability to represent
two kinds of similarity, i.e., taxonomic
similarity and associative similarity. The
result of the comparison was that the
dictionary-based word vectors better re-
flect taxonomic similarity, while the LSA-
based and the cooccurrence-based word
vectors better reflect associative similarity.
1 Introduction
Recently, geometric models have been used to rep-
resent words and their meanings, and proven to
be highly useful both for many NLP applications
associated with semantic processing (Widdows,
2004) and for human modeling in cognitive sci-
ence (Ga?rdenfors, 2000; Landauer and Dumais,
1997). There are also good reasons for studying
geometric models in the field of computational lin-
guistics. First, geometric models are cost-effective
in that it takes much less time and less effort to
construct large-scale geometric representation of
word meanings than it would take to construct dic-
tionaries or thesauri. Second, they can represent
the implicit knowledge of word meanings that dic-
tionaries and thesauri cannot do. Finally, geomet-
ric representation is easy to revise and extend.
A vector space model is the most commonly
used geometric model for the meanings of words.
The basic idea of a vector space model is that
words are represented by high-dimensional vec-
tors, i.e., word vectors, and the degree of seman-
tic similarity between any two words can be easily
computed as a cosine of the angle formed by their
vectors.
A number of methods have been proposed for
constructing word vectors. Latent semantic anal-
ysis (LSA) is the most well-known method that
uses the frequency of words in a fraction of doc-
uments to assess the coordinates of word vectors
and singular value decomposition (SVD) to reduce
the dimension. LSA was originally put forward as
a document indexing technique for automatic in-
formation retrieval (Deerwester et al, 1990), but
several studies (Landauer and Dumais, 1997) have
shown that LSA successfully mimics many hu-
man behaviors associated with semantic process-
ing. Other methods use a variety of other informa-
tion: cooccurrence of two words (Burgess, 1998;
Schu?tze, 1998), occurrence of a word in the sense
definitions of a dictionary (Kasahara et al, 1997;
Niwa and Nitta, 1994) or word association norms
(Steyvers et al, 2004).
However, despite the fact that there are differ-
ent kinds of similarity between words, or differ-
ent relations underlying word similarity such as a
synonymous relation and an associative relation,
no studies have ever examined the relationship be-
tween methods for constructing word vectors and
the type of similarity involved in word vectors in
a systematic way. Some studies on word vec-
tors have compared the performance among dif-
ferent methods on some specific tasks such as se-
mantic disambiguation (Niwa and Nitta, 1994) and
cued/free recall (Steyvers et al, 2004), but it is not
at all clear whether there are essential differences
in the quality of similarity among word vectors
constructed by different methods, and if so, what
kind of similarity is involved in what kind of word
vectors. Even in the field of cognitive psychol-
ogy, although geometric models of similarity such
as multidimensional scaling have long been stud-
ied and debated (Nosofsky, 1992), the possibility
that different methods for word vectors may cap-
858
ture different kinds of word similarity has never
been addressed.
This study, therefore, aims to examine the re-
lationship between the methods for constructing
word vectors and the type of similarity in a sys-
tematic way. Especially this study addresses three
methods, LSA-based, cooccurrence-based, and
dictionary-based methods, and two kinds of sim-
ilarity, taxonomic similarity and associative sim-
ilarity. Word vectors constructed by these meth-
ods are compared in the performance of two tasks,
i.e., multiple-choice synonym test and word asso-
ciation, which measure the degree to which they
reflect these two kinds of similarity.
2 Two Kinds of Similarity
In this study, we divide word similarity into two
categories: taxonomic similarity and associative
similarity. Taxonomic similarity, or categorical
similarity, is a kind of semantic similarity between
words in the same level of categories or clusters of
the thesaurus, in particular synonyms, antonyms,
and other coordinates. Associative similarity, on
the other hand, is a similarity between words that
are associated with each other by virtue of seman-
tic relations other than taxonomic one such as a
collocational relation and a proximity relation. For
example, the word writer and the word author are
taxonomically similar because they are synonyms,
while the word writer and the word book are as-
sociatively similar because they are associated by
virtue of an agent-subject relation.
This dichotomy of similarity is practically im-
portant. Some tasks such as automatic thesaurus
updating and paraphrasing need assessing taxo-
nomic similarity, while some other tasks such as
affective Web search and semantic disambiguation
require assessing associative similarity rather than
taxonomic similarity. This dichotomy is also psy-
chologically motivated. Many empirical studies
on word searches and speech disorders have re-
vealed that words in the mind (i.e., mental lex-
icon) are organized by these two kinds of simi-
larity (Aitchison, 2003). This dichotomy is also
essential to some cognitive processes. For ex-
ample, metaphors are perceived as being more
apt when their constituent words are associatively
more similar but categorically dissimilar (Utsumi
et al, 1998). These psychological findings suggest
that people distinguish between these two kinds of
similarity in certain cognitive processes.
3 Constructing Word Vectors
3.1 Overview
In this study, word vectors (or word spaces) are
constructed in the following way. First, all con-
tent words ti in a corpus are represented as m-
dimensional feature vectors wi.
wi = (wi1, wi2, ? ? ? , wim) (1)
Each element wij is determined by statistical anal-
ysis of the corpus, whose methods will be de-
scribed in Section 3.3. A matrix M is then con-
structed using n feature vectors as rows.
M =
( w1
.
.
.
wn
)
(2)
Finally, the dimension of row vectors wi is re-
duced from m to k by means of a SVD tech-
nique. As a result, any words are represented as
k-dimensional vectors.
3.2 Corpus
In this study, we employ three kinds of Japanese
corpora: newspaper articles, novels and a dictio-
nary. As a newspaper corpus, we use 4 months?
worth of Mainichi newspaper articles published
in 1999. They consist of 500,182 sentences in
251,287 paragraphs, and words vectors are con-
structed for 53,512 words that occur three times
or more in these articles. Concerning a corpus of
novels, we use a collection of 100 Japanese nov-
els ?Shincho Bunko No 100 Satsu? consisting of
475,782 sentences and 230,392 paragraphs. Word
vectors are constructed for 46,666 words that oc-
cur at least three times. As a Japanese dictionary,
we use ?Super Nihongo Daijiten? published by
Gakken, from which 89,007 words are extracted
for word vectors.
3.3 Methods for Computing Vector Elements
LSA-based method (LSA)
In the LSA-based method, a vector element wij
is assessed as a tf-idf score of a word ti in a piece
sj of document.
wij = tfij ?
(
log mdfi
+ 1
)
(3)
In this formula, tfij denotes the number of times
the word ti occurs in a piece of text sj , and dfi
denotes the number of pieces in which the word
ti occurs. As a unit of text piece sj , we consider
859
a sentence and a paragraph. Hence, for example,
when a sentence is used as a unit, the dimension of
feature vectors wi is equal to the number of sen-
tences in a corpus. We also use two corpora, i.e.,
newspapers and novels, and thus we obtain four
different word spaces by the LSA-based method.
Cooccurrence-based method (COO)
In the cooccurrence-based method, a vector ele-
ment wij is assessed as the number of times words
ti and tj occur in the same piece of text, and thus
M is an n ? n symmetric matrix. As in the case
of the LSA-based method, we use two units of text
piece (i.e., a sentence or a paragraph) and two cor-
pora (i.e., newspapers or novels), thus resulting in
four different word spaces.
Note that this method is similar to Schu?tze?s
(1998) method for constructing a semantic space
in that both are based on the word cooccurrence,
not on the word frequency. However they are dif-
ferent in that Schu?tze?s method uses the cooccur-
rence with frequent content words chosen as in-
dicators of primitive meanings. Burgess?s (1998)
?Hyperspace Analogue to Language (HAL)? is
also based on the word cooccurrence but does not
use any technique of dimensionality reduction.
Dictionary-based method (DIC)
In the dictionary-based method, a vector ele-
ment wij is assessed by the following formula:
wij =
?
?fij + ?
?
?
k
fikfkj + ?fji
?
?? log ndfj
(4)
where fij denotes the number of times the word
tj occurs in the sense definitions of the word ti,
and dfj denotes the number of words whose sense
definitions contain the word tj . The second term
in parentheses in Equation (4) means the square
root of the number of times the word tj occurs in
a collection of sense definitions for any words that
are included in the sense definitions of the word ti,
while the third term means the number of times ti
occurs in the sense definitions of tj . The param-
eters ? and ? are positive real constants express-
ing the weights for these information. (Following
Kasahara et al (1997), these parameters are set to
0.2 in this paper.)
Equation (4) was originally put forward by
Kasahara et al (1997), but our dictionary-based
method differs from their method in terms of how
the dimensions are reduced. Their method groups
together the dimensions for words in the same cat-
egory of a thesaurus, but our method uses SVD as
we will described next.
3.4 Reducing Dimensions
Using a SVD technique, a matrix M is factorized
as the product of three matrices U?V T , where
the diagonal matrix ? consists of r singular val-
ues that are arranged in nonincreasing order such
that r is the rank of M . When we use a k ? k ma-
trix ?k consisting of the largest k singular values,
the matrix M is approximated by Uk?kV Tk , where
the i-th row of Uk corresponds to a k-dimensional
?reduced word vector? for the word ti.
4 Experiment 1: Synonym Judgment
4.1 Method
In order to compare different word vectors in
terms of the ability to judge taxonomic similar-
ity between words, we conducted a synonym judg-
ment experiment using a standard multiple-choice
synonym test. Each item of a synonym test con-
sisted of a stem word and five alternative words
from which the test-taker was asked to choose one
with the most similar meaning to the stem word.
In the experiment, we used 32 items from the
synonym portions of Synthetic Personality In-
ventory (SPI) test, which has been widely used
for employment selection in Japanese companies.
These items were selected so that all the vector
spaces could contain the stem word and at least
four of the five alternative words. For comparison
purpose, we also used 38 antonym test items cho-
sen from the same SPI test. Furthermore, in order
to obtain a more reliable, unbiased result, we auto-
matically constructed 200 test items in such a way
that we chose the stem word randomly, one correct
alternative word randomly from words in the same
deepest category of a Japanese thesaurus as the
stem word, and other four alternatives from words
in other categories. As a Japanese thesaurus, we
used ?Goi-Taikei? (Ikehara et al, 1999).
In the computer simulation, the computer?s
choices were determined by computing cosine
similarity between the stem word and each of the
five alternative words using the vector spaces and
choosing the word with the highest similarity.
4.2 Results and Discussion
For each of the nine vector spaces, the synonym
judgment simulation described above was con-
860
0 100 200 300 400 500 600 700 800 900 1000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
 
 
  
 
 
 
 
 
 
 
 
 
 
   
 
 
 
+
+
++++
+
+++
++
++++++
+
+
 


 


 




   


 LSA
+ + COO
 
DIC
Number of Dimensions
Co
rre
ct
R
at
e
(a) SPI test items
0 100 200 300 400 500 600 700 800 900 1000
0.2
0.3
0.4
0.5
0.6
0.7
	
	
	
	
	
	
	
	
	
		
	
	
	
	
	
	
	
	
	
++
+++++++
+++++++++++























 















Number of Dimensions
Co
rre
ct
R
at
e
(b) Computer-generated test items
Figure 1: Correct rates of synonym tests
ducted and the percentage of correct choices was
calculated. This process was repeated using 20
numbers of dimensions, i.e., every 50 dimensions
between 50 and 1000.
Figure 1 shows the percentage of correct
choices for the three methods of matrix construc-
tion. Concerning the LSA-based method (denoted
by LSA) and the cooccurrence-based method (de-
noted by COO), Figure 1 plots the correct rates for
the word vectors derived from the paragraphs of
the newspaper corpus. (Such combination of cor-
pus and text unit was optimal among all combi-
nations, which will be justified later in this sec-
tion.) The most important result shown in Figure 1
is that, regardless of the number of dimensions, the
dictionary-based word vectors outperformed the
other kinds of vectors on both SPI and computer-
generated test items. This result thus suggests
that the dictionary-based vector space reflects tax-
onomic similarity between words better than the
LSA-based and the correlation-based spaces.
Another interesting finding is that there was no
clear peak in the graphs of Figure 1. For SPI test
items, correct rates of the three methods increased
linearly as the number of dimensions increased,
r = .86 for the LSA-based method, r = .72 for
the correlation-based method and r = .93 for the
dictionary-based method (all ps < .0001), while
correct rates for computer-generated test items
0 100 200 300 400 500 600 700 800 900 1000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

















 


 


 




   















 




















 LSA (synonym)
  LSA (antonym)
  DIC (synonym)
 DIC (antonym)
Number of Dimensions
Co
rre
ct
R
at
e
Figure 2: Synonym versus antonym judgment
were steady. Our finding of the absence of any
obvious optimal dimensions is in a sharp contrast
to Landauer and Dumais?s (1997) finding that the
LSA word vectors with 300 dimensions achieved
the maximum performance of 53% correct rate
in a similar multiple-choice synonym test. Note
that their maximum performance was a little bet-
ter than that of our LSA vectors, but still worse
than that of our dictionary-based vectors.
Figure 2 shows the performance of the LSA-
based and the dictionary-based methods in anto-
nym judgment, together with the result of syn-
onym judgment. (Since the performance of the
cooccurrence-based method did not differ from
that of the LSA-based method, the correct rates
of the cooccurrence-based method are not plotted
in this figure.) The dictionary-based method also
outperformed the LSA-based method in antonym
judgment but their difference was much smaller
than that of synonym judgment; at 200 or lower
dimensions LSA-based method was better than
the dictionary-based method. Interestingly, the
dictionary-based word vectors yielded better per-
formance in synonym judgment than in antonym
judgment, while the LSA-based vectors showed
better performance in antonym judgment. These
contrasting results may be attributed to the differ-
ence of corpus characteristics. Dictionary?s defi-
nitions for antonymous words are likely to involve
different words so that the differences between
their meanings can be made clear. On the other
hand, in newspaper articles (or literary texts), con-
text words with which antonymous words occur
are likely to overlap because their meanings are
about the same domain.
Finally, we show the results of comparison
among four combinations of corpora and text units
for the LSA-based and the cooccurrence-based
861
Table 1: Comparison of mean correct rate among
the combinations of two corpora and two text units
Newspaper Novel
Method Para Sent Para Sent
SPI test
LSA 0.383 0.366 0.238 0.369
COO 0.413 0.369 0.255 0.280
Computer-generated test
LSA 0.410 0.377 0.346 0.379
COO 0.375 0.363 0.311 0.310
Note. Para = Paragraph; Sent = Sentence.
methods. Table 1 lists mean correct rates of SPI
test and computer-generated test averaged over all
the numbers of dimensions. Regardless of con-
struction methods and test items, the word vectors
constructed using newspaper paragraphs achieved
the best performance, which are denoted by bold-
faces. Concerning an effect of corpus difference,
the newspaper corpus was superior to the literary
corpus. The difference of text units did not have a
clear influence on the performance of word spaces.
5 Experiment 2: Word Association
5.1 Method
In order to compare the ability of the word
spaces to judge associative similarity, we con-
ducted a word association experiment using a
Japanese word association norm ?Renso Kijun-
hyo? (Umemoto, 1969). This free-association
database was developed based on the responses of
1,000 students to 210 stimulus words. For exam-
ple, when given the word writer as a stimulus, stu-
dents listed the words shown in Table 2. (Table 2
also shows the original words in Japanese.)
For the simulation experiment, we selected 176
stimulus words that all the three corpora con-
tained. These stimuli had 27 associate words on
average. We then removed any associate words
that were synonymous with the stimulus word
(e.g., author in Table 2), since the purpose of this
experiment was to examine the ability to assess
associative similarity between words. Whether or
not each associate is synonymous with the stimu-
lus was determined according to whether they be-
long to the same deepest category of a Japanese
thesaurus ?Goi-Taikei? (Ikehara et al, 1999).
In the computer simulation, cosine similarity
Table 2: Associates for the stimulus word writer
Stimulus: writer
 
Associates:
novel pen literary work painter
	 
 
 
book author best-seller money
  