Towards a Hybrid Model for Chinese Word Segmentation 
Xiaofei Lu 
Department of Linguistics 
The Ohio State University 
Columbus, OH 43210, USA 
xflu@ling.osu.edu 
Abstract 
This paper describes a hybrid Chinese 
word segmenter that is being developed 
as part of a larger Chinese unknown 
word resolution system. The segmenter 
consists of two components: a tagging 
component that uses the transforma-
tion-based learning algorithm to tag 
each character with its position in a 
word, and a merging component that 
transforms a tagged character sequence 
into a word-segmented sentence. In ad-
dition to the position-of-character tags 
assigned to the characters, the merging 
component makes use of a number of 
heuristics to handle non-Chinese char-
acters, numeric type compounds, and 
long words. The segmenter achieved a 
92.8% F-score and a 72.8% recall for 
OOV words in the closed track of the 
Peking University Corpus in the Sec-
ond International Chinese Word Seg-
mentation Bakeoff. 
1 Introduction 
This paper describes a hybrid Chinese word 
segmenter that participated in the closed track of 
the Peking University Corpus in the Second In-
ternational Chinese Word Segmentation Bake-
off. This segmenter is still in its early stage of 
development and is being developed as part of a 
larger Chinese unknown word resolution system 
that performs the identification, part of speech 
guessing, and sense guessing of Chinese un-
known words (Lu, 2005).  
  The segmenter consists of two major compo-
nents. First, a tagging component tags each indi-
vidual character in a sentence with a position-of-
character (POC) tag that indicates the position of 
the character in a word. This could be one of the 
following four possibilities, i.e., the character is 
either a monosyllabic word or is in a word-
initial, middle, or final position. This component 
is based on the transformation-based learning 
(TBL) algorithm (Brill, 1995), where a simple 
first-order HMM tagger (Charniak et al, 1993) 
is used to produce an initial tagging of a charac-
ter sequence. Second, a merging component 
transforms the output of the tagging component, 
i.e., a POC-tagged character sequence, into a 
word-segmented sentence. Whereas this process 
relies largely on the POC tags assigned to the 
individual characters, it also takes advantage of 
a number of heuristics generalized from the 
training data to handle non-Chinese characters, 
numeric type compounds, and long words. 
The approach adopted here is reminiscent of 
the line of research that employs the idea of 
character-based tagging for Chinese word seg-
mentation and/or unknown word identification 
(Goh et al, 2003; Xue, 2003; Zhang et al, 
2002). The notion of character-based tagging 
allows us to model the tendency for individual 
characters to combine with other characters to 
form words in different contexts. This property 
gives the model a good potential for improving 
the performance of Chinese unknown word 
identification, a major concern of the Chinese 
unknown word resolution system that the seg-
menter is a part of.  
The rest of the paper is organized as follows. 
Section two describes the system architecture. 
Section three reports the results of the system in 
the bakeoff. Section four concludes the paper.  
189
2 System Description 
The overall architecture of the segmenter is de-
scribed in Figure 1. An input sentence is first 
segmented into a character sequence, with a 
space inserted after each character. The seg-
mented character sequence is then processed by 
the tagging component, where it is initially 
tagged by an HMM tagger, and then by a TBL 
tagger. Finally, the tagged character sequence is 
transformed into a word-segmented sentence by 
the merging component.  
  
Figure 1: System Architecture. 
2.1 The Tagging Component 
The tagset used by the tagging component con-
sists of the following four tags: L, M, R, and W, 
each of which indicates that the character is in a 
word-initial, word-middle, or word-final posi-
tion or is a monosyllabic word respectively. The 
transformation-based error-driven learning algo-
rithm is adopted as the backbone of the tagging 
component over other promising machine learn-
ing algorithms because, as Brill (1995) argued, it 
captures linguistic knowledge in a more explicit 
and direct fashion without compromising per-
formance. This algorithm requires a gold stan-
dard, some initial tagging of the training corpus, 
and a set of rule templates. It then learns a set of 
rules that are ranked in terms of the number of 
tagging error reductions they can achieve.    
A number of different initial tagging schemes 
can be used, e.g., tagging each character as a 
monosyllabic word or with its most probable 
POC tag. We used a simple first-order HMM 
tagger to produce an initial tagging. Specifically,
we calculate 
)|w)p(t|tp(t iii
n
i
i...tt n 1
1
1maxarg ?
=
?            (1) 
where ti denotes the ith tag in the tag sequence 
and wi denotes the ith character in the character 
sequence. The transition probabilities and lexi-
cal probabilities are estimated from the training 
data. The lexical probability for an unknown 
character, i.e., a character that is not found in the 
training data, is by default uniformly distributed 
among the four POC tags defined in the tagset. 
The Viterbi algorithm (Rabiner, 1989) is used to 
tag new texts.   
The transformation-based tagger was imple-
mented using fnTBL (Florian and Ngai, 2001). 
The rule templates used are the same as the con-
textual rule templates Brill (1995) defined for 
the POS tagging task. These templates basically 
transform the current tag into some other tag 
based on the current character/tag and the char-
acter/tag one to three positions before/after the 
current character. An example rule template is 
given below: 
(1) Change tag a to tag b if the preceding char-
acter is tagged z.   
The training process is iterative. At each itera-
tion, the algorithm picks the instantiation of a 
rule template that achieves the greatest number 
of tagging error reductions. This rule is applied 
to the text, and the learning process repeats until
no more rules reduce errors beyond a pre-
defined threshold. The learned rules can then be 
applied to new texts that are tagged by the initial
HMM tagger.  
2.2 The Merging Component 
The merging component transforms a POC-
tagged character sequence into a word-
segmented sentence. In general, the characters in 
a sequence are concatenated, and a space is in-
serted after each character tagged R (word-final 
position) or W (monosyllabic word).  
Unsegmented 
Chinese sentence 
Segmented  
character sequence 
Initial POC-tagged 
character sequence 
Final POC-tagged  
character sequence 
Word-segmented  
sentence 
Character 
segmenter 
HMM
POC tagger 
TBL
POC tagger 
Merging 
component 
190
  In addition, two sets of heuristics are used in 
this process. One set (H1) is used to handle non-
Chinese characters and numeric type compounds, 
e.g., numbers, time expressions, etc. A few pat-
terns of non-Chinese characters and numeric 
type compounds are generalized from the train-
ing data. If the merging algorithm detects such a 
pattern in the character sequence, it groups the 
characters that are part of the pattern accord-
ingly.  
The second set of heuristics (H2) is used to 
handle words that three or more characters long. 
Our hypothesis is that long words tend to have 
less fluidity than shorter words and their behav-
ior is more predictable (Lu, 2005). We extracted 
a wordlist from the training data. Based on our 
hypothesis, if the merging algorithm detects that 
a group of characters form a long word found in 
the wordlist, it groups these characters into one 
word.  
3 Results 
The segmenter was evaluated on the closed track 
of the Peking University Corpus in the bakeoff. 
In the development stage, we partitioned the 
official training data into two portions: the train-
ing set consists of 90% of the data, and the de-
velopment set consists of the other 10%. The 
POC tagging accuracy on the development set is 
summarized in Table 1. The results indicate that 
the TBL tagger significantly improves the initial 
tagging produced by the HMM tagger.  
 Accuracy 
HMM tagger 0.814 
TBL tagger 0.936 
Table 1: Tagging Results on the Development Set. 
  The performance of the merging algorithm on 
the development set is summarized in Table 2. 
To understand whether and how much the heu-
ristics contribute to improving segmentation, we 
evaluated four versions of the merging algo-
rithm. The set of heuristics used to handle non-
Chinese characters and numeric type compounds 
did not seem to improve segmentation results on 
the development set, suggesting that these char-
acters are handled well by the tagging compo-
nent. However, the second set of heuristics 
improved segmentation accuracy significantly. 
This seems to confirm our hypothesis that longer 
words tend to behave more stably.   
Resources used R P F 
POC Tags only 0.928 0.926 0.927 
+ H1 0.929 0.925 0.927 
+ H2 0.938 0.959 0.948 
+ H1 & H2 0.940 0.960 0.950 
Table 2: Segmentation Results on the Development 
Set. H1 stands for the set of heuristics used to handle 
non-Chinese characters and numeric type com-
pounds. H2 stands for the set of heuristics used to
handle long words.   
Corpus R P F ROOV RIV
PKU 0.922 0.934 0.928 0.728 0.934 
Table 3: Official Results in the Closed-Track of the 
Peking University Corpus. 
The official results of the segmenter in the 
closed-track of the Peking University Corpus are 
summarized in Table 3. It is somewhat unex-
pected that the results on the official test data 
dropped over 2% compared with the results ob-
tained on the development set. Compared with 
the other systems, the segmenter performed rela-
tively well on OOV words.  
Our preliminary error analysis indicates that 
this discrepancy in performance is partially at-
tributable to two kinds of inconsistencies be-
tween the training and test datasets. One is that 
there are many ASCII numbers in the test set, 
but none in the training set. These numbers be-
came unknown characters to the tagger and af-
fected tagging accuracy. It is possible that this 
inconsistency affected our system more than 
other systems. Second, there are also a number 
of segmentation inconsistencies between the 
training and test sets, but these should have af-
fected all systems more or less equally. The er-
ror analysis also indicates that the current 
segmenter performed poorly on transliterations 
of foreign names.  
4 Conclusions 
We described a hybrid Chinese word segmenter 
that combines the transformation-based learning 
algorithm for character-based tagging and lin-
guistic heuristics for transforming tagged char-
acter sequences into word-segmented sentences. 
191
As the segmenter is in its first stage of develop-
ment and is far from mature, the bakeoff pro-
vided an especially valuable opportunity for 
evaluating its performance. The results suggest 
that:  
1. Despite the lack of a separate mecha-
nism for unknown word recognition, the 
segmenter performed relatively well on 
OOV words. This confirms our hy-
pothesis that character-based tagging 
has a good potential for improving Chi-
nese unknown word identification. 
2. Using linguistic heuristics at the merg-
ing stage can help improve segmenta-
tion results.  
3. There is much room for improvement 
for both the tagging algorithm and the 
merging algorithm. This is being under-
taken.   
References 
Eric Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing: A case study in part-of-speech tagging. 
Computational Linguistics, 21(4):543-565. 
Eugene Charniak, Curtis Hendrickson, Neil Ja-
cobson, and Mike Perkowitz. 1993. Equations 
for part-of-speech tagging. In Proceedings of 
AAAI-1993, pp. 784-789.  
Chooi Ling Goh, Masayuki Asahara, and Yuji 
Matsumoto. 2003. Chinese unknown word 
identification using character-based tagging 
and chunking. In Proceedings of ACL-2003 
Interactive Posters and Demonstrations, pp. 
197-200. 
Xiaofei Lu. 2005. Hybrid methods for POS 
guessing of Chinese unknown words. In Pro-
ceedings of ACL-2005 Student Research 
Workshop, pp. 1-6.  
Grace Ngai and Radu Florian. 2001. Transfor-
mation-based learning in the fast lane. In Pro-
ceedings of NAACL-2001, pp. 40-47.  
Lawrence R. Rabiner. 1989. A tutorial of hidden 
Markov models and selected applications in 
speech recognition. In Proceedings of IEEE-
1989, pp. 257-286.  
Nianwen Xue. 2003. Chinese word segmenta-
tion as character tagging. International Jour-
nal of Computational Linguistics and Chinese 
Language Processing, 8(1):29-48.  
Kevin Zhang, Qin Liu, Hao Zhang, and Xue-Qi 
Cheng. 2002. Automatic recognition of Chi-
nese unknown words based on roles tagging. 
In Proceedings of the 1st SIGHAN Workshop 
on Chinese Language Processing, pp. 71-78. 
192
Proceedings of the ACL Student Research Workshop, pages 1?6,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Hybrid Methods for POS Guessing of Chinese Unknown Words
Xiaofei Lu
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
xflu@ling.osu.edu
Abstract
This paper describes a hybrid model that
combines a rule-based model with two
statistical models for the task of POS
guessing of Chinese unknown words. The
rule-based model is sensitive to the type,
length, and internal structure of unknown
words, and the two statistical models uti-
lize contextual information and the like-
lihood for a character to appear in a par-
ticular position of words of a particular
length and POS category. By combining
models that use different sources of infor-
mation, the hybrid model achieves a pre-
cision of 89%, a significant improvement
over the best result reported in previous
studies, which was 69%.
1 Introduction
Unknown words constitute a major source of diffi-
culty for Chinese part-of-speech (POS) tagging, yet
relatively little work has been done on POS guess-
ing of Chinese unknown words. The few existing
studies all attempted to develop a unified statistical
model to compute the probability of a word hav-
ing a particular POS category for all Chinese un-
known words (Chen et al, 1997; Wu and Jiang,
2000; Goh, 2003). This approach tends to miss
one or more pieces of information contributed by
the type, length, internal structure, or context of in-
dividual unknown words, and fails to combine the
strengths of different models. The rule-based ap-
proach was rejected with the claim that rules are
bound to overgenerate (Wu and Jiang, 2000).
In this paper, we present a hybrid model that com-
bines the strengths of a rule-based model with those
of two statistical models for this task. The three
models make use of different sources of information.
The rule-based model is sensitive to the type, length,
and internal structure of unknown words, with over-
generation controlled by additional constraints. The
two statistical models make use of contextual infor-
mation and the likelihood for a character to appear in
a particular position of words of a particular length
and POS category respectively. The hybrid model
achieves a precision of 89%, a significant improve-
ment over the best result reported in previous stud-
ies, which was 69%.
2 Chinese Unknown Words
The definition of what constitutes a word is prob-
lematic for Chinese, as Chinese does not have word
delimiters and the boundary between compounds
and phrases or collocations is fuzzy. Consequently,
different NLP tasks adopt different segmentation
schemes (Sproat, 2002). With respect to any Chi-
nese corpus or NLP system, therefore, unknown
words can be defined as character strings that are
not in the lexicon but should be identified as seg-
mentation units based on the segmentation scheme.
Chen and Bai (1998) categorized Chinese unknown
words into the following five types: 1) acronyms,
i.e., shortened forms of long names, e.g., be?i-da` for
be?ij??ng-da`xue? ?Beijing University?; 2) proper names,
including person, place, and organization names,
e.g., Ma?o-Ze?do?ng; 3) derived words, which are cre-
ated through affixation, e.g., xia`nda`i-hua` ?modern-
ize?; 4) compounds, which are created through com-
pounding, e.g., zh??-la?ohu? ?paper tiger?; and 5) nu-
1
meric type compounds, including numbers, dates,
time, etc., e.g., lia?ng-dia?n ?two o?clock?. Other
types of unknown words exist, such as loan words
and reduplicated words. A monosyllabic or disyl-
labic Chinese word can reduplicate in various pat-
terns, e.g., zo?u-zo?u ?take a walk? and pia`o-pia`o-
lia`ng-lia`ng ?very pretty? are formed by reduplicating
zo?u ?walk? and pia`o-lia`ng ?pretty? respectively.
The identification of acronyms, proper names,
and numeric type compounds is a separate task that
has received substantial attention. Once a charac-
ter string is identified as one of these, its POS cate-
gory also becomes known. We will therefore focus
on reduplicated and derived words and compounds
only. We will consider unknown words of the cat-
egories of noun, verb, and adjective, as most un-
known words fall under these categories (Chen and
Bai, 1998). Finally, monosyllabic words will not be
considered as they are well covered by the lexicon.
3 Previous Approaches
Previous studies all attempted to develop a uni-
fied statistical model for this task. Chen et al
(1997) examined all unknown nouns1, verbs, and
adjectives and reported a 69.13% precision using
Dice metrics to measure the affix-category associa-
tion strength and an affix-dependent entropy weight-
ing scheme for determining the weightings be-
tween prefix-category and suffix-category associa-
tions. This approach is blind to the type, length, and
context of unknown words. Wu and Jiang (2000)
calculated P(Cat,Pos,Len) for each character, where
Cat is the POS of a word containing the character,
Pos is the position of the character in that word, and
Len is the length of that word. They then calcu-
lated the POS probabilities for each unknown word
as the joint probabilities of the P(Cat,Pos,Len) of
its component characters. This approach was ap-
plied to unknown nouns, verbs, and adjectives that
are two to four characters long2. They did not re-
port results on unknown word tagging, but reported
that the new word identification and tagging mecha-
nism increased parser coverage. We will show that
this approach suffers reduced recall for multisyllabic
1Including proper names and time nouns, which we ex-
cluded for the reason discussed in section 2.
2Excluding derived words and proper names.
words if the training corpus is small. Goh (2003) re-
ported a precision of 59.58% on all unknown words
using Support Vector Machines.
Several reasons were suggested for rejecting the
rule-based approach. First, Chen et al (1997)
claimed that it does not work because the syntac-
tic and semantic information for each character or
morpheme is unavailable. This claim does not fully
hold, as the POS information about the component
words or morphemes of many unknown words is
available in the training lexicon. Second, Wu and
Jiang (2000) argued that assigning POS to Chinese
unknown words on the basis of the internal struc-
ture of those words will ?result in massive over-
generation? (p. 48). We will show that overgener-
ation can be controlled by additional constraints.
4 Proposed Approach
We propose a hybrid model that combines the
strengths of different models to arrive at better re-
sults for this task. The models we will consider are
a rule-based model, the trigram model, and the sta-
tistical model developed by Wu and Jiang (2000).
Combination of the three models will be based on
the evaluation of their individual performances on
the training data.
4.1 The Rule-Based Model
The motivations for developing a set of rules for this
task are twofold. First, the rule-based approach was
dismissed without testing in previous studies. How-
ever, hybrid models that combine rule-based and sta-
tistical models outperform purely statistical models
in many NLP tasks. Second, the rule-based model
can incorporate information about the length, type,
and internal structure of unknown words at the same
time.
Rule development involves knowledge of Chi-
nese morphology and generalizations of the train-
ing data. Disyllabic words are harder to general-
ize than longer words, probably because their mono-
syllabic component morphemes are more fluid than
the longer component morphemes of longer words.
It is interesting to see if reduction in the degree of
fluidity of its components makes a word more pre-
dictable. We therefore develop a separate set of
rules for words that are two, three, four, and five
2
Chars T1 T2 T3 T4 Total
2 1 2 1 2 6
3 2 6 2 5 15
4 2 2 0 8 12
5+ 0 1 0 1 2
Total 5 11 3 16 35
Table 1: Rule distribution
or more characters long. The rules developed fall
into the following four types: 1) reduplication rules
(T1), which tag reduplicated unknown words based
on knowledge about the reduplication process; 2)
derivation rules (T2), which tag derived unknown
words based on knowledge about the affixation pro-
cess; 3) compounding rules (T3), which tag un-
known compounds based on the POS information
of their component words; and 4) rules based on
generalizations about the training data (T4). Rules
may come with additional constraints to avoid over-
generation. The number of rules in each set is listed
in Table 1. The complete set of rules are developed
over a period of two weeks.
As will be shown below, the order in which the
rules in each set are applied is crucial for dealing
with ambiguous cases. To illustrate how rules work,
we discuss the complete set of rules for disyllabic
words here3. These are given in Figure 1, where
A and B refer to the component morpheme of an
unknown AB. As rules for disyllabic words tend to
overgenerate and as we prefer precision over recall
for the rule-based model, most rules in this set are
accompanied with additional constraints.
In the first reduplication rule, the order of the
three cases is crucial in that if A can be both a verb
and a noun, AA is almost always a verb. The sec-
ond rule tags a disyllabic unknown word formed by
attaching the diminutive suffix er to a monosyllabic
root as a noun. This may appear a hasty general-
ization, but examination of the data shows that er
rarely attaches to monosyllabic verbs except for the
few well-known cases. In the third rule, a catego-
rizing suffix is one that attaches to other words to
form a noun that refers to a category of people or
objects, e.g., jia? ?-ist?. The constraint ?A is not a
verb morpheme? excludes cases where B is polyse-
mous and does not function as a categorizing suffix
3Multisyllabic words can have various internal structures,
e.g., a disyllabic noun can have a N-N, Adj-N, or V-N structure.
if A equals B
if A is a verb morpheme, AB is a verb
else if A is a noun morpheme, AB is a noun
else if A is an adjective morpheme, AB is a stative
adjective/adverb
else if B equals er, AB is a noun
else if B is a categorizing suffix AND A is not a verb
morpheme, AB is a noun
else if A and B are both noun morphemes but not verb
morphemes, AB is a noun
else if A occurs verb-initially only AND B is not a noun
morpheme AND B does not occur noun-finally only,
AB is a verb
else if B occurs noun-finally only AND A is not a verb
morpheme AND A does not occur verb-initially only,
AB is a noun
Figure 1: Rules for disyllabic words
but a noun morpheme. Thus, this rule tags be`ng-ye`
?water-pump industry? as a noun, but not l??-ye` leave-
job ?resign?. The fourth rule tags words such as sha?-
xia?ng ?sand-box? as nouns, but the constraints pre-
vent verbs such as so?ng-ko`u ?loosen-button? from
being tagged as nouns. So?ng can be both a noun
and a verb, but it is used as a verb in this word.
The last two rules make use of two lists of char-
acters extracted from the list of disyllabic words in
the training data, i.e., those that have only appeared
in the verb-initial and noun-final positions respec-
tively. This is done because in Chinese, disyllabic
compound verbs tend to be head-initial, whereas di-
syllabic compound nouns tend to be head-final. The
fifth rule tags words such as d??ng-ya?o ?sting-bite? as
verbs, and the additional constraints prevent nouns
such as fu?-xia`ng ?lying-elephant? from being tagged
as verbs. The last rule tags words such as xue?-
be`i ?snow-quilt? as nouns, but not zha?i-sha?o pick-tip
?pick the tips?.
One derivation rule for trisyllabic words has a spe-
cial status. Following the tagging guidelines of our
training corpus, it tags a word ABC as verb/deverbal
noun (v/vn) if C is the suffix hua` ?-ize?. Disambigua-
tion is left to the statistical models.
4.2 The Trigram Model
The trigram model is used because it captures the in-
formation about the POS context of unknown words
and returns a tag for each unknown word. We as-
sume that the unknown POS depends on the previ-
ous two POS tags, and calculate the trigram proba-
bility P (t3|t1, t2), where t3 stands for the unknown
3
POS, and t1 and t2 stand for the two previous POS
tags. The POS tags for known words are taken from
the tagged training corpus. Following Brants (2000),
we first calculate the maximum likelihood probabil-
ities P? for unigrams, bigrams, and trigrams as in
(1-3). To handle the sparse-data problem, we use
the smoothing paradigm that Brants reported as de-
livering the best result for the TnT tagger, i.e., the
context-independent variant of linear interpolation
of unigrams, bigrams, and trigrams. A trigram prob-
ability is then calculated as in (4).
P? (t3) = f(t3)/N (1)
P? (t3|t2) = f(t2, t3)/f(t2) (2)
P? (t3|t1, t2) = f(t1, t2, t3)/f(t1, t2) (3)
P (t3|t1, t2) = ?1P? (t3) + ?2P? (t3|t2) + ?3P? (t3|t1, t2) (4)
As in Brants (2000), ?1 + ?2 + ?3 = 1, and the
values of ?1, ?2, and ?3 are estimated by deleted
interpolation, following Brants? algorithm for calcu-
lating the weights for context-independent linear in-
terpolation when the n-gram frequencies are known.
4.3 Wu and Jiang?s (2000) Statistical Model
There are several reasons for integrating another sta-
tistical model in the model. The rule-based model is
expected to yield high precision, as over-generation
is minimized, but it is bound to suffer low recall for
disyllabic words. The trigram model covers all un-
known words, but its precision needs to be boosted.
Wu and Jiang?s (2000) model provides a good com-
plement for the two, because it achieves a higher
recall than the rule-based model and a higher pre-
cision than the trigram model for disyllabic words.
As our training corpus is relatively small, this model
will suffer a low recall for longer words, but those
are handled effectively by the rule-based model. In
principle, other statistical models can also be used,
but Wu and Jiang?s model appears more appealing
because of its relative simplicity and higher or com-
parable precision. It is used to handle disyllabic and
trisyllabic unknown words only, as recall drops sig-
nificantly for longer words.
4.4 Combining Models
To determine the best way to combine the three
models, their individual performances are evaluated
for each unknown word
if the trigram model returns one single guess, take it
else if the rule-based model returns a non-v/vn tag, take it
else if the rule-based model returns a v/vn tag
if W&J?s model returns a list of guesses
eliminate non-v/vn tags on that list and return the
rest of it
else eliminate non-v/vn tags on the list returned by the
trigram model and return the rest of it
else if W&J?s model returns a list of guesses, take it
else return the list of guesses returned by the trigram
model
Figure 2: Algorithm for combining models
in the training data first to identify their strengths.
Based on that evaluation, we come up with the al-
gorithm in Figure 2. For each unknown word, if the
trigram model returns exactly one POS tag, that tag
is prioritized, because in the training data, such tags
turn out to be always correct. Otherwise, the guess
returned by the rule-based model is prioritized, fol-
lowed by Wu and Jiang?s model. If neither of them
returns a guess, the guess returned by the trigram
model is accepted. This order of priority is based on
the precision of the individual models in the train-
ing data. If the rule-based model returns the ?v/vn?
guess, we first check which of the two tags ranks
higher in the list of guesses returned by Wu and
Jiang?s model. If that list is empty, we then check
which of them ranks higher in the list of guesses re-
turned by the trigram model.
5 Results
5.1 Experiment Setup
The different models are trained and tested on a por-
tion of the Contemporary Chinese Corpus of Peking
University (Yu et al, 2002), which is segmented and
POS tagged. This corpus uses a tagset consisting of
40 tags. We consider unknown words that are 1) two
or more characters long, 2) formed through redupli-
cation, derivation, or compounding, and 3) in one
of the eight categories listed in Table 2. The corpus
consists of all the news articles from People?s Daily
in January, 1998. It has a total of 1,121,016 tokens,
including 947,959 word tokens and 173,057 punc-
tuation marks. 90% of the data are used for train-
ing, and the other 10% are reserved for testing. We
downloaded a reference lexicon4 containing 119,791
4From http://www.mandarintools.com/segmenter.html.
4
entries. A word is considered unknown if it is in the
wordlist extracted from the training or test data but
is not in the reference lexicon. Given this defini-
tion, we first train and evaluate the individual mod-
els on the training data and then evaluate the final
combined model on the test data. The distribution
of unknown words is summarized in Table 3.
Tag Description
a Adjective
ad Deadjectval adverb
an Deadjectival noun
n Noun
v Verb
vn Deverbal noun
vd Deverbal adjective
z Stative adjective and adverb
Table 2: Categories of considered unknown words
Chars Training Data Test Data
Types Tokens Types Tokens
2 2611 4789 387 464
3 3818 7378 520 764
4 490 1229 74 125
5+ 188 698 20 56
Total 7107 14094 1001 1509
Table 3: Unknown word distribution in the data
5.2 Results for the Individual Models
The results for the rule-based model are listed in Ta-
ble 4. Recall (R) is defined as the number of cor-
rectly tagged unknown words divided by the total
number of unknown words. Precision (P) is defined
as the number of correctly tagged unknown words
divided by the number of tagged unknown words.
The small number of words tagged ?v/vn? are ex-
cluded in the count of tagged unknown words for
calculating precision, as this tag is not a final guess
but is returned to reduce the search space for the
statistical models. F-measure (F) is computed as
2 ? RP/(R + P ). The rule-based model achieves
very high precision, but recall for disyllabic words
is low.
The results for the trigram model are listed in Ta-
ble 5. Candidates are restricted to the eight POS cat-
egories listed in Table 2 for this model. Precision for
the best guess in both datasets is about 62%.
The results for Wu and Jiang?s model are listed in
Table 6. Recall for disyllabic words is much higher
than that of the rule-based model. Precision for di-
syllabic words reaches mid 70%, higher than that of
the trigram model. Precision for trisyllabic words is
very high, but recall is low.
Chars Data R P F
2 Training 24.05 96.94 38.54
Test 27.66 96.89 43.03
3 Training 93.50 99.83 96.56
Test 93.72 99.86 96.69
4 Training 98.70 99.02 98.86
Test 99.20 99.20 99.20
5+ Training 99.86 100 99.93
Test 100 100 100
Total Training 70.60 99.40 82.56
Test 69.72 99.34 81.94
Table 4: Results for the rule-based model
Guesses 1-Best 2-Best 3-Best
Training 62.01 93.63 96.21
Test 62.96 92.64 94.30
Table 5: Results for the trigram model
Chars Data R P F
2 Training 65.19 75.57 67.00
Test 63.82 77.92 70.17
3 Training 59.50 98.41 74.16
Test 55.63 99.07 71.25
Table 6: Results for Wu and Jiang?s (2000) model
5.3 Results for the Combined Model
To evaluate the combined model, we first define the
upper bound of the precision for the model as the
number of unknown words tagged correctly by at
least one of the three models divided by the total
number of unknown words. The upper bound is
91.10% for the training data and 91.39% for the test
data. Table 7 reports the results for the combined
model. The overall precision of the model reaches
89.32% in the training data and 89.00% in the test
data, close to the upper bounds.
6 Discussion and Conclusion
The results indicate that the three models have dif-
ferent strengths and weaknesses. Using rules that do
not overgenerate and that are sensitive to the type,
length, and internal structure of unknown words,
5
Chars Training Test
2 73.27 74.47
3 97.15 97.25
4 98.78 99.20
5+ 100 100
Total 89.32 89.00
Table 7: Results for the combined model
the rule-based model achieves high precision for all
words and high recall for longer words, but recall for
disyllabic words is low. The trigram model makes
use of the contextual information of unknown words
and solves the recall problem, but its precision is rel-
atively low. Wu and Jiang?s (2000) model comple-
ments the other two, as it achieves a higher recall
than the rule-based model and a higher precision
than the trigram model for disyllabic words. The
combined model outperforms each individual model
by effectively combining their strengths.
The results challenge the reasons given in previ-
ous studies for rejecting the rule-based model. Over-
generation is a problem only if one attempts to write
rules to cover the complete set of unknown words. It
can be controlled if one prefers precision over recall.
To this end, the internal structure of the unknown
words provides very useful information. Results
for the rule-based model also suggest that as un-
known words become longer and the fluidity of their
component words/morphemes reduces, they become
more predictable and generalizable by rules.
The results achieved in this study prove a signif-
icant improvement over those reported in previous
studies. To our knowledge, the best result on this
task was reported by Chen et al (1997), which was
69.13%. However, they considered fourteen POS
categories, whereas we examined only eight. This
difference is brought about by the different tagsets
used in the different corpora and the decision to in-
clude or exclude proper names and numeric type
compounds. To make the results more compara-
ble, we replicated their model, and the results we
found were consistent with what they reported, i.e.,
69.12% for our training data and 68.79% for our test
data, as opposed to our 89.32% and 89% respec-
tively.
Several avenues can be taken for future research.
First, it will be useful to identify a statistical model
that achieves higher precision for disyllabic words,
as this seems to be the bottleneck. It will also be rel-
evant to apply advanced statistical models that can
incorporate various useful information to this task,
e.g., the maximum entropy model (Ratnaparkhi,
1996). Second, for better evaluation, it would be
helpful to use a larger corpus and evaluate the in-
dividual models on a held-out dataset, to compare
our model with other models on more compara-
ble datasets, and to test the model on other logo-
graphic languages. Third, some grammatical con-
straints may be used for the detection and correction
of tagging errors in a post-processing step. Finally,
as part of a bigger project on Chinese unknown word
resolution, we would like to see how well the general
methodology used and the specifics acquired in this
task can benefit the identification and sense-tagging
of unknown words.
References
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, pages 224?231.
Keh-Jiann Chen and Ming-Hong Bai. 1998. Unknown
word detection for Chinese by a corpus-based learning
method. International Journal of Computational Lin-
guistics and Chinese Language Processing, 3(1):27?
44.
Chao-Jan Chen, Ming-Hong Bai, and Keh-Jiann Chen.
1997. Category guessing for Chinese unknown words.
In Proceedings of NLPRS, pages 35?40.
Chooi-Ling Goh. 2003. Chinese unknown word identifi-
cation by combining statistical models. Master?s the-
sis, Nara Institute of Science and Technology, Japan.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of EMNLP, pages
133?142.
Richard Sproat. 2002. Corpus-based methods in Chinese
morphology. Tutorial at the 19th COLING.
Andy Wu and Zixin Jiang. 2000. Statistically-enhanced
new word identification in a rule-based Chinese sys-
tem. In Proceedings of the 2nd Chinese Language
Processing Workshop, pages 46?51.
Shiwen Yu, Huiming Duan, Xuefeng Zhu, and Bing Sun.
2002. The basic processing of Contemporary Chinese
Corpus at Peking University. Technical report, Insti-
tute of Computational Linguistics, Peking University,
Beijing, China.
6
Proceedings of NAACL HLT 2007, pages 188?195,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Hybrid Models for Semantic Classification of  
Chinese Unknown Words 
 
Xiaofei Lu 
Department of Linguistics and Applied Language Studies 
Pennsylvania State University 
University Park, PA 16802, USA 
xxl13@psu.edu 
 
 
 
Abstract 
This paper addresses the problem of clas-
sifying Chinese unknown words into 
fine-grained semantic categories defined 
in a Chinese thesaurus. We describe 
three novel knowledge-based models that 
capture the relationship between the se-
mantic categories of an unknown word 
and those of its component characters in 
three different ways. We then combine 
two of the knowledge-based models with 
a corpus-based model which classifies 
unknown words using contextual infor-
mation. Experiments show that the 
knowledge-based models outperform 
previous methods on the same task, but 
the use of contextual information does 
not further improve performance.  
1 Introduction 
Research on semantic annotation has focused 
primarily on word sense disambiguation (WSD), 
i.e., the task of determining the appropriate sense 
for each instance of a polysemous word out of a 
set of senses defined for the word in some lexi-
con. Much less work has been done on semantic 
classification of unknown words, i.e., words that 
are not listed in the lexicon. However, real texts 
typically contain a large number of unknown 
words. Successful classification of unknown 
words is not only useful for lexical acquisition, 
but also necessary for natural language process-
ing (NLP) tasks that require semantic annotation.  
    This paper addresses the problem of classify-
ing Chinese unknown words into fine-grained 
semantic categories defined in a Chinese thesau-
rus, Cilin (Mei et al, 1984). This thesaurus clas-
sifies over 70,000 words into 12 major catego-
ries, including human (A), concrete object (B), 
time and space (C), abstract object (D), attributes 
(E), actions (F), mental activities (G), activities 
(H), physical states (I), relations (J), auxiliaries 
(K), and honorifics (L). The 12 major categories 
are further divided into 94 medium categories, 
which in turn are subdivided into 1428 small 
categories. Each small category contains syno-
nyms that are close in meaning. For example, 
under the major category D, the medium cate-
gory Dm groups all words that refer to institu-
tions, and the small category Dm05 groups all 
words that refer to educational institutions, e.g., 
?? xu?xi?o ?school?. Unknown word classifi-
cation involves a much larger search space than 
WSD. In classifying words into small categories 
in Cilin, the search space for a polysemous 
known word consists of all the categories the 
word belongs to, but that for an unknown word 
consists of all the 1428 small categories.  
Research on WSD has concentrated on using 
contextual information, which may be limited 
for infrequent unknown words. On the other 
hand, Chinese characters carry semantic infor-
mation that is useful for predicting the semantic 
properties of the words containing them. We pre-
sent three novel knowledge-based models that 
capture the relationship between the semantic 
categories of an unknown word and those of its 
component characters in three different ways, 
and combine two of them with a corpus-based 
model that uses contextual information to clas-
sify unknown words. Experiments show that the 
combined knowledge-based model achieves an 
accuracy of 61.6% for classifying unknown 
words into small categories in Cilin, but the use 
of contextual information does not further im-
prove performance.   
The rest of the paper is organized as follows. 
Section 2 details the three novel knowledge-
based models proposed for this task. Section 3 
describes a corpus-based model. Section 4 re-
ports the experiment results of the proposed 
188
models. Section 5 compares these results with 
previous results. Section 6 concludes the paper 
and points to avenues for further research.  
2 Knowledge-based Models 
This section describes three novel knowledge-
based models for semantic classification of Chi-
nese unknown words, including an overlapping-
character model, a character-category association 
model, and a rule-based model. These models 
model the relationship between the semantic 
category of an unknown word and those of its 
component characters in three different ways. 
2.1 The Baseline Model 
The baseline model predicts the category of an 
unknown word by counting the number of over-
lapping characters between the unknown word 
and the member words in each category. As 
words in the same category are similar in mean-
ing and the meaning of a Chinese word is gener-
ally the composition of the meanings of its char-
acters, it is common for words in the same cate-
gory to share one or more character. This model 
tests the hypothesis that speakers draw upon the 
repertoire of characters that relate to a concept 
when creating new words to realize it.  
For each semantic category in Cilin, the set of 
unique characters in its member words are ex-
tracted, and the number of times each character 
occurs in word-initial, word-middle, and word-
final positions is recorded. With this informa-
tion, we develop two variants of the baseline 
model, which differ from each other in terms of 
whether it takes into consideration the positions 
in which the characters occur in words.   
In variant 1, the score of a category is the sum 
of the number of occurrences of each character 
of the target word in the category, as in (1), 
where tj denotes a category, w denotes the target 
word, ci denotes the ith character in w, n is the 
length of w, and f(ci) is the frequency of ci in tj.   
(1) ?
=
= n
i
ij cfwtScore
1
)(),(  
In variant 2, the score of a category is the sum 
of the number of occurrences of each character 
of the unknown word in the category in its corre-
sponding position, as in (2), where pi denotes the 
position of ci in w, which could be word-initial, 
word-middle, or word-final, and f(ci,pi) denotes 
the frequency of ci in position pi in tj.   
(2) =),( wtScore j ?
=
n
i
ii pcf
1
),(  
In each variant, the category with the maxi-
mum score for a target word is proposed as the 
category of the word.   
2.2 Character-Category Associations 
The relationship between the semantic category 
of an unknown word and those of its component 
characters can also be captured in a more sophis-
ticated way using information-theoretical models. 
We use two statistical measures, mutual infor-
mation and ?2, to compute character-category 
associations and word-category associations. 
Chen (2004) used the ?2 measure to compute 
character-character and word-word associations, 
but not word-category associations. We use 
word-category associations to directly predict 
the semantic categories of unknown words.  
The mutual information and ?2 measures are 
calculated as in (3) and (4), where Asso(c,tj) de-
notes the association between a character c and a 
semantic category tj, and P(X) and f(X) denote 
the probability and frequency of X respectively. 
(3)  
)()(
),(
log),(
j
j
jMI tPcP
tcP
tcAsso =   
(4) 
),(max
),(
),(2
kk
j
j tc
tc
tcAsso ?
?
? =  
(5) 
)()(
)],([
),(
2
j
j
j tfcf
tcf
tc +=?  
    Once the character-category associations are 
calculated, the association between a word w and 
a category tj, Asso(w,tj), can be calculated as the 
sum of the weighted associations between each 
of the word?s characters and the category, as in 
(6), where ci denotes the ith character of w, |w| 
denotes the length of w, and ?i denotes the weight 
of Asso(ci,tj). The ??s add up to 1. The weights 
are determined empirically based on the posi-
tions of the characters in the word.   
(6) =),( jtwAsso ?
=
||
1
),(
w
i
jii tcAsso?  
As in variant 2 of the baseline model, the 
character-category association model can also be 
made sensitive to the positions in which the 
characters occur in the words. To this end, we 
first need to compute the position-sensitive asso-
ciations between a category and a character in 
the word-initial, word-middle, and word-final 
positions separately. The position-sensitive asso-
ciation between an unknown word and a cate-
gory can then be computed as the sum of the 
weighted position-sensitive associations between 
each of its characters and the category.  
189
Once the word-category associations are com-
puted, we can propose the highest ranked cate-
gory or a ranked list of categories for each un-
known word.  
2.3 A Rule-Based Model 
The third knowledge-based model uses linguistic 
rules to classify unknown words based on the 
syntactic and semantic categories of their com-
ponent characters. Rule-based models have not 
been used for this task before. However, there 
are some regularities in the relationship between 
the semantic categories of unknown words and 
those of their component characters that can be 
captured in a more direct and effective way by 
linguistic rules than by statistical models. 
A separate set of rules are developed for 
words of different lengths. Rules are initially 
developed based on knowledge about Chinese 
word formation, and are then refined by examin-
ing the development data. In general, the com-
plete rule set takes a few hours to develop.   
 The rule in (7) is developed for bisyllabic un-
known words. This rule proposes the common 
category of a bisyllabic word?s two characters as 
its category. It is especially useful for words 
with a parallel structure, i.e., words whose two 
characters have the same meaning and syntactic 
category, e.g., ?? t?nt? ?collapse?, where ? 
t?n and ? t? both mean ?collapse? and share the 
category Id05. The thresholds for fA and fB are 
determined empirically and are both set to 1 if 
AB is a noun and to 0 and 3 respectively other-
wise.  
 
(7) For a bisyllabic word AB, if A and B share a cate-
gory c1, let fA and fB denote the number of times 
A and B occur in word-initial and word-final po-
sitions in c respectively. If fA and fB both surpass 
the predetermined thresholds, propose c for AB. 
 
A number of rules are developed for trisyl-
labic words. While most rules in the model are 
general, the first rule in this set is rather specific, 
as it handles words with three specific prefixes, 
? d? ?big?, ? xi?o ?little?, and ? l?o ?old?, 
which usually do not change the category of the 
root word. The other four rules again utilize the 
categories of the unknown word?s component 
characters. The rules in (8b) and (8c) are similar 
to the rule in (7). The ones in (8d) and (8e) 
search for neighbor words with a similar struc-
ture as the target word. Eligible neighbors have a 
                                                 
1 A and B may each belong to more than one category.  
common morpheme with the target word in the 
same position and a second morpheme that 
shares a category with the second morpheme of 
the target word. For example, an eligible 
neighbor for ??? tu?xi?o-sh?ng ?sales-man? 
is ??? xi?osh?u-sh?ng ?distribut-or?. These 
two words share the morpheme ? sh?ng ?busi-
nessman? in the word-final position, and the 
morphemes ?? tu?xi?o ?to market? and ?? 
xi?osh?u ?distribute? share the category He03. 
The rule in (8d) therefore applies in this case. 
 
(8) For a trisyllabic word ABC: 
a. If A equals ? d? ?big?, ? xi?o ?little?, or ? 
l?o ?old?, propose the category of AB for 
ABC if C is the diminutive suffix ? er or the 
category of BC for ABC otherwise. 
b. If A and BC share a category c, propose c for 
ABC. 
c. If AB and C share a category c, propose c for 
ABC. 
d. If there is a word XYC such that XY and AB 
share a category, propose the category of 
XYC for ABC. 
e. If there is a word XBC such that X and A 
share a category, propose the category of 
XBC for ABC. 
 
The rules for four-character words are given 
in (9). Like the rules in (8d) and (8e), these rules 
also search for neighbors of the target word.   
 
(9) For a four-character word ABCD: 
a. If there is a word XYZD/YZD such that 
XYZ/YZ and ABC share a category, propose 
the category of XYZ/YZ for ABCD. 
b. If there is a word ABCX such that X and D 
share a category, propose the category of 
ABCX for ABCD. 
c. If there is a word XYCD such that XY and AB 
share a category, propose the category of 
XYCD for ABCD. 
d. If there is a word XBCD/BCD, propose the 
category of XBCD/BCD for ABCD. 
3 A Corpus-Based Model 
The knowledge-based models described above 
classify unknown words using information about 
the syntactic and semantic categories of their 
component characters. Another useful source of 
information is the context in which unknown 
words occur. While contextual information is the 
primary source of information used in WSD re-
search and has been used for acquiring semantic 
lexicons and classifying unknown words in other 
languages (e.g., Roark and Charniak 1998; Ci-
190
aramita 2003; Curran 2005), it has been used in 
only one previous study on semantic classifica-
tion of Chinese unknown words (Chen and Lin, 
2000). Part of the goal of this study is to investi-
gate whether and how these two different 
sources of information can be combined to im-
prove performance on semantic classification of 
Chinese unknown words.  
To this end, we first use the knowledge-based 
models to propose a list of five candidate catego-
ries for the target word, then extract a general-
ized context for each category in Cilin from a 
corpus, and finally compute the similarity be-
tween the context of the target word and the gen-
eralized context of each of its candidate catego-
ries. Comparing the context of the target word 
with generalized contexts of categories instead 
of contexts of individual words alleviates the 
data-sparseness problem, as infrequent words 
have limited contextual information. Limiting 
the search space for each target word to the top 
five candidate categories reduces the computa-
tional cost that comes with the full search space.  
3.1 Context Extraction and Representation 
A generalized context for each semantic cate-
gory is built from the contexts of its member 
words. This is done based on the assumption that 
as the words in the same category have the same 
or similar meaning, they tend to occur in similar 
contexts. In terms of context extraction and rep-
resentation, we need to consider four factors. 
 
Member Words The issue here is whether to 
include the contexts of polysemous member 
words in building the generalized context of a 
category. Including these contexts without dis-
crimination introduces noise. To measure the 
effect of such noise, we build two versions of 
generalized context for each category, one using 
contexts of unambiguous member words only, 
and the other using contexts of all member 
words.  
 
Context Words There are two issues in select-
ing words for context representation. First, 
words that contribute little information to the 
discrimination of meaning of other words, in-
cluding conjunctions, numerals, auxiliaries, and 
non-Chinese sequences, are excluded. Second, to 
model the effect of frequency on the context 
words? contribution to meaning discrimination, 
we use two sets of context words: one consists of 
the 1000 most frequent words in the corpus; the 
other consists of all words in the corpus.  
Window Size For WSD, both topical context 
and microcontext have been used (Ide and 
V?ronis 1998). Topical context includes substan-
tive words that co-occur with the target word 
within a larger window, whereas microcontext 
includes words in a small window around the 
target word. We experiment with topical context 
and microcontext with window sizes of 100 and 
6 respectively (i.e., 50 and 3 words to the left 
and right of the target word respectively).  
 
Context Representation We represent the con-
text of a category as a vector <w1, w2, ..., wn>, 
where n is the total number of context words, 
and wi is the weight of the ith context word. To 
arrive at this representation, we first record the 
number of times each context word occurs 
within a specified window of each member word 
of a category in the corpus as a vector <f1, f2, ..., 
fn>, where fi is the number of times the ith con-
text word co-occurs with a member word of the 
category. We then compute the weight of a con-
text word w in context c, W(w, c), using mutual 
information and t-test, which were reported by 
Weeds and Weir (2005) to perform the best on a 
pseudo-disambiguation task. These weight func-
tions are computed as in (10) and (11), where N 
denotes the size of the corpus.  
(10) 
)()(
),(
log),(
cPwP
cwP
cwWPMI =  
(11) 
NcwP
cPwPcwP
cwWt
),(
)()(),(
),(
?=  
3.2 Contextual Similarity Measurement 
We compute the similarity between the context 
vectors of the unknown word and its candidate 
categories using cosine. The cosine of two n-
dimensional vectors x
r
and y
r
, cos( x
r
, y
r
), is com-
puted as in (12), where xi and yi denote the 
weight of the ith context word in x
r
and y
r
. 
(12) 
??
?
==
==
n
1i
2n
1i
2
n
1i)y,xcos(
ii
ii
yx
yxrr
 
 
4 Results 
4.1 Experiment Setup 
The models are developed and tested using the 
Contemporary Chinese Corpus from Peking 
University (Yu et al 2002) and the extended 
Cilin released by the Information Retrieval Lab 
at Harbin Institute of Technology. The corpus 
191
contains all the articles published in January, 
1999 in People?s Daily, a major newspaper in 
China. It contains over 1.12 million tokens and is 
word-segmented and POS-tagged. Table 1 sum-
marizes the distribution of words in Cilin. Of the 
76,029 words in Cilin, 35,151 are found in the 
Contemporary Chinese Corpus.  
 
Length Unambiguous Polysemous Total 
1 2,674 2,068 4,742 
2 39,057 5,403 44,460 
3 15,112 752 15,864 
4 9,397 942 10,338 
?5 590 34 624 
Total 66,830 9,199 76,029 
Table 1: Word distribution in the extended Cilin 
 
We classify words into the third-level catego-
ries in the extended Cilin, which are equivalent 
to the small categories in the original Cilin. The 
development and test sets consist of 3,000 words 
each, which are randomly selected from the sub-
set of words in Cilin that are two to four charac-
ters long, that have occurred in the Contempo-
rary Chinese Corpus, and that are tagged as 
nouns, verbs, or adjectives in the corpus. The 
words in the development and test sets are also 
controlled for frequency, with 1/3 of them occur-
ring 1-3 times, 3-6 times, and 7 or more times in 
the corpus respectively.  
As Chen (2004) noted, excluding all the 
words in the development and test data in the 
testing stage worsens the data-sparseness prob-
lem for knowledge-based models, as some cate-
gories have few member words, and some char-
acters appear in few words in some categories. 
To alleviate this problem, the remove-one 
method is used for testing the knowledge-based 
models. In other words, the models are re-trained 
for each test word using information about all 
the words in Cilin except the test word. The cor-
pus-based model is trained once using the train-
ing data only, as the data-sparseness problem is 
alleviated by using generalized contexts of cate-
gories. Finally, if a word is polysemous, it is 
considered to have been correctly classified if 
the proposed category is one of its categories. 
4.2 Results of the Baseline Model 
Tables 2 and 3 summarize the results of the 
baseline model in terms of the accuracy of its 
best guess and best five guesses respectively.  
The columns labeled ?Non-filtered? report re-
sults where all categories are considered for each 
unknown word, and the ones labeled ?POS-
filtered? report results where only the categories 
that agree with the POS category of the unknown 
word are considered. In the latter case, if the tar-
get word is a noun, only the small categories un-
der major categories A-D are considered; other-
wise, only those under major categories E-L are 
considered. The results show that using POS in-
formation about the unknown word to filter cate-
gories improves performance. Variant 2 per-
forms better when only the best guess is consid-
ered, indicating that it is useful to model the ef-
fect of position on a character?s contribution to 
word meaning in this case. However, it is not 
helpful to be sensitive to character position when 
the best five guesses are considered.  
 
Non-filtered POS-filtered Model 
variant Dev Test Dev Test 
1 0.391 0.398 0.450 0.464 
2 0.471 0.469 0.514 0.517 
Table 2: Results of the baseline model: best guess  
 
Non-filtered POS-filtered Model 
variant Dev Test Dev Test 
1 0.757 0.760 0.813 0.817 
2 0.764 0.762 0.809 0.805 
Table 3: Results of the baseline model: best 5 guesses  
4.3 Results of the Character-Category As-
sociation Model 
In this model, only categories that agree with the 
POS category of the unknown word and that 
share at least one character with the unknown 
word are considered. These filtering steps sig-
nificantly reduce the search space for this model.  
We discussed three parameters of the model in 
Section 2.2, including the statistical measure, the 
sensitivity to character position in computing 
character-category associations, and the weights 
of the associations between categories and char-
acters in different positions. In addition, the 
computation of the character-category associa-
tions can be sensitive or insensitive to the POS 
categories of the words containing the characters. 
In the POS-sensitive way, associations are com-
puted among nouns (words in categories A-D) 
and non-nouns (words in categories E-L) sepa-
rately, whereas in the POS-insensitive way, they 
are computed using all the words.  
Tables 4 and 5 summarize the results of the 
character-category association model in terms of 
the accuracy of its best guess and best five 
guesses respectively. In all cases, the weights 
assigned to word-initial, word-middle, and word-
final characters are 0.49, 0, and 0.51 respectively. 
192
In terms of the best guess, the model achieves 
a best accuracy of 58.2%, a 6.5% improvement 
over the baseline result. The results show that ?2 
consistently performs better than mutual infor-
mation, and computing position-sensitive char-
acter-category associations consistently im-
proves performance. However, computing POS-
sensitive associations gives mixed results. 
In terms of the best five guesses, the model 
achieves a best accuracy of 83.8% on the test 
data, a 2.1% improvement over the best baseline 
result. Using ?2 again achieves better results. 
However, in this case, the best results are 
achieved when the character-category associa-
tions are insensitive to both character position 
and the POS categories of words. 
 
Sensitivity Development Test 
POS Position MI ?2 MI ?2 
Yes Yes 0.482 0.586 0.507 0.582 
Yes No 0.440 0.578 0.458 0.573 
No Yes 0.487 0.565 0.511 0.567 
No No 0.457 0.555 0.459 0.559 
Table 4: Results of the character-category association 
model: best guess 
 
Sensitivity Development Test 
POS Position MI ?2 MI ?2 
Yes Yes 0.735 0.805 0.720 0.810 
Yes No 0.743 0.828 0.754 0.821 
No Yes 0.702 0.813 0.718 0.812 
No No 0.735 0.830 0.746 0.838 
Table 5: Results of the character-category association 
model: best 5 guesses 
 
Development Test Word 
Len R P F R P F 
2 0.159 0.796 0.265 0.158 0.772 0.262 
3 0.368 0.838 0.511 0.351 0.830 0.493 
4 0.582 0.852 0.692 0.540 0.900 0.675 
All 0.218 0.816 0.344 0.216 0.803 0.340 
Table 6: Results of the rule-based model: best guess 
4.4 Results of the Rule-Based Model 
Table 6 summarizes the results of the rule-
based model in terms of recall, precision and F-
score. The model returns multiple categories for 
some words, and it is considered to have cor-
rectly classified a word only when it returns a 
single, correct category for the word. Precision 
of the model is computed over all the cases 
where the model returns a single guess, and re-
call is computed over all cases. The model 
achieves an overall precision of 80.3% on the 
test data, much higher than the accuracy of the 
other two knowledge-based models. However, 
recall of the model is only 21.6%. The compara-
ble results on the development and test sets indi-
cate that the encoded rules are general. The 
model generally performs better on longer words 
than on shorter words.  
4.5 Combining the Character-Category 
Association and Rule-Based Models 
Given that the rule-based model achieves a 
higher precision but a lower recall than the char-
acter-category association model, the two mod-
els can be combined to improve the overall per-
formance. In general, if the rule-based model 
returns one or more categories, these categories 
are first ranked among themselves by their asso-
ciations with the unknown word. They are then 
followed by the other categories returned by the 
character-category association model. Tables 7 
and 8 summarize the results of combining the 
two models.  
 
Sensitivity Development Test 
POS Position MI ?2 MI ?2 
Yes Yes 0.561 0.623 0.572 0.616 
Yes No 0.536 0.622 0.542 0.615 
No Yes 0.562 0.610 0.575 0.608 
No No 0.530 0.601 0.532 0.606 
Table 7: Results of combining the character-category 
association and rule-based models: best guess 
 
Sensitivity Development Test 
POS Position MI ?2 MI ?2 
Yes Yes 0.834 0.846 0.845 0.843 
Yes No 0.791 0.860 0.801 0.851 
No Yes 0.760 0.848 0.742 0.845 
No No 0.773 0.859 0.782 0.856 
Table 8: Results of combining the character-category 
association and rule-based models: best 5 guesses 
 
In terms of the best guess, the combined 
model achieves an accuracy of 61.6%, a 3.4% 
improvement over the best result of the charac-
ter-category association model alone. This is 
achieved using ?2 with POS-sensitive and posi-
tion-sensitive computation of character-category 
associations. In terms of the best five guesses, 
the model achieves an accuracy of 85.6%, a 
1.8% improvement over the best result of the 
character-category association model alone. 
To facilitate comparison with previous studies, 
the results of the combined model in terms of its 
best guess in classifying unknown words into 
major and medium categories are summarized in 
Table 9. As ?2 consistently outperforms mutual 
information, results are reported for ?2 only. 
With POS-sensitive and position-sensitive com-
193
putation of character-category associations, the 
combined model achieves an accuracy of 83.0% 
and 69.9% for classifying unknown words into 
major and medium categories respectively.  
 
Sensitivity Development Test 
POS Position Major Med Major Med 
Yes Yes 0.840 0.705 0.830 0.699 
Yes No 0.831 0.698 0.828 0.698 
No Yes 0.832 0.692 0.825 0.692 
No No 0.821 0.687 0.821 0.689 
Table 9: Results of the combined model for classify-
ing unknown words into major and medium catego-
ries: best guess 
4.6 Results of the Corpus-Based Model 
The corpus-based model re-ranks the five high-
est ranked categories proposed by the combined 
knowledge-based model. Table 10 enumerates 
the parameters of the model and lists the labels 
used to denote the various settings in Table 11.   
 
Parameter Label Setting Label 
Member 
words 
MW All members words 
Unambiguous members 
all 
un 
Context 
words 
CW All words 
1000 most frequent 
all 
1000 
Window 
size 
WS 100 
6 
100 
6 
Weight 
function 
WF Mutual information 
t-test 
mi 
t 
Table 10: Parameter settings of the corpus-based 
model  
 
Table 11 summarizes the results of 16 runs of 
the model with different parameter settings. The 
best accuracy on the test data is 37.1%, achieved 
in run 5 with the following parameter settings: 
using unambiguous member words for building 
contexts of categories, using all words in the 
corpus for context representation, using a win-
dow size of 100, and using mutual information 
as the weight function. As the combined knowl-
edge-based model gives an accuracy of 85.6% 
for its best five guesses, the expected accuracy 
of a naive model that randomly picks a candidate 
for each word as its best guess is 17.1%. Com-
pared with this baseline, the corpus-based model 
achieves a 13.0% improvement, but it performs 
much worse than the knowledge-based models. 
Table 12 summarizes the accuracy of the top 
three runs of the model on words with different 
frequency in the corpus. Each of the three groups 
consists of 1,000 words that have occurred 1-2, 
3-6, and 7 or more times in the corpus respec-
tively. The model consistently performs better 
on words with higher frequency, suggesting that 
it may benefit from a larger corpus. 
 
Parameter Setting Accuracy Run 
ID MW CW WS WF Dev Test 
1 un 1000 100 mi 0.326 0.303 
2 un 1000 100 t 0.317 0.288 
3 un 1000 6 mi 0.304 0.301 
4 un 1000 6 t 0.299 0.301 
5 un all 100 mi 0.359 0.371 
6 un all 100 t 0.292 0.296 
7 un all 6 mi 0.370 0.365 
8 un all 6 t 0.322 0.297 
9 all 1000 100 mi 0.302 0.294 
10 all 1000 100 t 0.314 0.304 
11 all 1000 6 mi 0.313 0.314 
12 all 1000 6 t 0.308 0.308 
13 all all 100 mi 0.336 0.333 
14 all all 100 t 0.287 0.300 
15 all all 6 mi 0.356 0.356 
16 all all 6 t 0.308 0.308 
Table 11: Results of the corpus-based model 
 
Development Test Run 
ID 1-2 3-6 ?7 1-2 3-6 ?7 
5 0.331 0.360 0.385 0.323 0.389 0.402 
7 0.323 0.363 0.423 0.335 0.357 0.402 
15 0.328 0.346 0.395 0.334 0.355 0.379 
Table 12: Results of the corpus-based model on 
words with different frequency 
5 Related Work 
The few previous studies on semantic classifica-
tion of Chinese unknown word have primarily 
adopted knowledge-based models. Chen (2004) 
proposed a model that retrieves the word with 
the greatest association with the target word. 
This model is computationally more expensive 
than our character-category association model, 
as it entails computing associations between 
every character-category, category-character, 
character-character, and word-word pair. He re-
ported an accuracy of 61.6% on bisyllabic V-V 
compounds. However, he included all the test 
words in training the model. If we also include 
the test words in computing character-category 
associations, the computationally cheaper model 
achieves an overall accuracy of 75.6%, with an 
accuracy of 75.1% on verbs.  
Chen and Chen (2000) adopted similar exem-
plar-based models. Chen and Chen used a mor-
phological analyzer to identify the head of the 
target word and the semantic categories of its 
modifier. They then retrieved examples with the 
same head as the target word. Finally, they com-
puted the similarity between two words as the 
194
similarity between their modifiers, using the 
concept of information load (IC) of the least 
common ancestor (LCA) of the modifiers? se-
mantic categories. They reported an accuracy of 
81% for classifying 200 unknown nouns. Given 
the small test set of their study, it is hard to di-
rectly compare their results with ours.  
Tseng used a morphological analyzer in the 
same way, but she also derived the morpho-
syntactic relationship between the morphemes. 
She retrieved examples that share a morpheme 
with the target word in the same position and 
filtered those with a different morpho-syntactic 
relationship. Finally, she computed the similarity 
between two words as the similarity between 
their non-shared morphemes, using a similar 
concept of IC of the LCA of two categories. She 
classified unknown words into the 12 major 
categories only, and reported accuracies 65.8% 
on adjectives, 71.4% on nouns, and 52.8% on 
verbs. These results are not as good as the 83.0% 
overall accuracy our combined knowledge-based 
model achieved for classifying unknown words 
into major categories.  
Chen and Lin (2000) is the only study that 
used contextual information for the same task. 
To generate candidate categories for a word, 
they looked up its translations in a Chinese-
English dictionary and the synsets of the transla-
tions in WordNet, and mapped the synsets to the 
categories in Cilin. They used a corpus-based 
model similar to ours to rank the candidates. 
They reported an accuracy of 34.4%, which is 
close to the 37.1% accuracy of our corpus-based 
model, but lower than the 61.6% accuracy of our 
combined knowledge-based model. In addition, 
they could only classify the unknown words 
listed in the Chinese-English dictionary. 
6 Conclusions 
We presented three knowledge-based models 
and a corpus-based model for classifying Chi-
nese unknown words into fine-grained categories 
in the Chinese thesaurus Cilin, a task important 
for lexical acquisition and NLP applications that 
require semantic annotation. The knowledge-
based models use information about the catego-
ries of the unknown words? component charac-
ters, while the corpus-based model uses contex-
tual information. By combining the character-
category association and rule-based models, we 
achieved an accuracy of 61.6%. The corpus-
based model did not improve performance. 
Several avenues can be taken for further re-
search. First, additional resources, such as bilin-
gual dictionaries, morphological analyzers, par-
allel corpora, and larger corpora with richer lin-
guistic annotation may prove useful for improv-
ing both the knowledge-based and corpus-based 
models. Second, we only explored one way to 
combine the knowledge-based and corpus-based 
models. Future work may explore alternative 
ways to combine these models to make better 
use of contextual information.  
References 
C.-J. Chen. 2004. Character-sense association and 
compounding template similarity: Automatic se-
mantic classification of Chinese compounds. In 
Proceedings of the 3rd SIGHAN Workshop on Chi-
nese Language Processing, pages 33?40.  
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in WordNet. In Proceed-
ings of EMNLP-2003, pages 594-602.  
K.-J. Chen and C.-J. Chen. 2000. Automatic semantic 
classification for Chinese unknown compound 
nouns. In Proceedings of COLING-2000, pages 
173-179. 
H.-H. Chen and C.-C. Lin. 2000. Sense-tagging Chi-
nese corpus. In Proceedings of the 2nd Chinese 
Language Processing Workshop, pages 7-14. 
J. Curran. 2005. Supersense tagging of unknown 
nouns using semantic similarity. In Proceedings of 
ACL-2006, pages 26-33.  
N. Ide and J. V?ronis. 1998. Introduction on the spe-
cial issue on word sense disambiguation: The state 
of the art. Computational Linguistics 24(1):2?40. 
J. Mei, Y. Zhu, Y. Gao, and H. Yin. (eds.) 1984. 
Tongyici Cilin [A Thesaurus of Chinese Words]. 
Commercial Press, Hong Kong. 
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic 
lexicon construction. In Proceedings of COL-
ING/ACL-1998, pages 1110-1116.  
H. Tseng. 2003. Semantic classification of Chinese 
unknown words. In Proceedings of ACL-2003 Stu-
dent Research Workshop, pages 72-79.  
J. Weeds and D. Weir. 2005. Co-occurrence retrieval: 
A flexible framework for lexical distributional 
similarity. Computational Linguistics 31(4):439?
475. 
S. Yu, H. Duan, X. Zhu, and B. Sun. 2002. The basic 
processing of Contemporary Chinese Corpus at 
Peking University. Journal of Chinese Information 
Processing 16(5):49?64. 
195
