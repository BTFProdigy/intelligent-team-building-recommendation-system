Some Studies on Chinese Domain Knowledge Dictionary and Its
Application to Text Classification 
Jingbo Zhu 
 Natural Language Processing Lab 
Institute of Computer Software & Theory 
Northeastern University, Shenyang 
zhujingbo@mail.neu.edu.cn
Wenliang Chen 
Natural Language Processing Lab 
Institute of Computer Software & Theory 
Northeastern University, Shenyang 
Chenwl@mail.neu.edu.cn
Abstract
In this paper, we study some issues on 
Chinese domain knowledge dictionary 
and its application to text classification 
task. First a domain knowledge hierar-
chy description framework and our 
Chinese domain knowledge dictionary 
named NEUKD are introduced. Second, 
to alleviate the cost of construction of 
domain knowledge dictionary by hand, 
we use a boostrapping-based algorithm 
to learn new domain associated terms 
from a large amount of unlabeled data. 
Third, we propose two models (BOTW 
and BOF) which use domain knowl-
edge as textual features for text catego-
rization. But due to limitation of size of 
domain knowledge dictionary, we fur-
ther study machine learning technique 
to solve the problem, and propose a 
BOL model which could be considered 
as the extended version of BOF model. 
Na?ve Bayes classifier based on BOW 
model is used as baseline system in the 
comparison experiments. Experimental 
results show that domain knowledge is 
very useful for text categorization, and 
BOL model performs better than other 
three models, including BOW, BOTW 
and BOF models. 
1 Introduction 
It is natural for people to know the topic of the 
document when they see some specific words in 
the document. For example, when we read a 
news, if title of the news includes a word ???
(Yao Ming)?, as we know, ???(Yao Ming)? is 
a famous China basketball athlete in US NBA 
game, so we could recognize the topic of the 
document is about ??? ,?? (Basketball,
Sports)? with our domain knowledge. In this 
paper, we call the specific word ??? (Yao 
Ming)? as a Domain Associated Term (DAT). A 
DAT is a word or a phrase (compound words) 
that enable humans to recognize intuitively a 
topic of text with their domain knowledge. In 
fact, domain knowledge is a kind of common-
sense knowledge. We think that domain knowl-
edge is very useful for text understanding tasks, 
such as text classification, document summariza-
tion, and information retrieval. 
In previous literatures, some researchers 
used knowledge bases for text understanding 
tasks(Scott et al, 1998), such as WordNet for 
English and HowNet for Chinese. We know that 
WordNet and HowNet are lexical and semantic 
knowledge resources. Other researchers tried to 
use commonsense knowledge such as field-
associated terms for text understanding tasks(M. 
Fuketa et al, 2000, Sangkon Lee and Masami 
Shishibori, 2002). But the problem of limitation 
of size of such knowledge base is still a key bot-
tleneck for using domain knowledge dictionary 
for text understanding tasks, and how to solve it 
is an ongoing research focus. 
In the following content, we try to give an-
swers to four questions: 1)What is our Chinese 
domain knowledge dictionary NEUKD? 2)How 
to learn DATs from a large amount of unlabelled 
data? 3)How to use the Chinese domain knowl-
edge dictionary NEUKD for text classification? 
4)Due to the problem of limitation of size of 
domain knowledge dictionary, how to solve the 
110
problem and improve performance of text classi-
fication using domain knowledge dictionary?
2 Domain Knowledge Dictionary 
We first introduce briefly domain knowledge 
hierarchy description framework (DKF) which 
includes three levels: Domain Level (DL), Do-
main Feature Level (DFL) and Domain Associ-
ated Term Level (DATL). The DL is the top 
level which defines many domains, such as ??
?(Sports)?, ???(Military Affairs)?. The DFL 
is the second level which defines many domain 
features. A domain defined in the DL has a lot of 
domain features defined in the DFL. For 
example, domain ???(Military Affairs)? has 
many domain features, such as ??? (Army 
Feature)?, ???(Weapon Feature)? and ???
(War Feature)?. The DATL is the third level 
which defines many domain associated terms. 
Many domain associated terms could indicate a 
same domain feature defined in the DFL. For 
example, some domain associated terms, such as 
some domain associated terms, such as ????
?(Mid-East War)?, ??????(Iraq War)? 
and ??????(Afghanistan War)?, indicate 
domain feature ???(War)?. 
Since 1996 we employed a semi-automatic 
machine learning technique to acquire domain 
knowledge from a large amount of labeled and 
unlabeled corpus, and built a general-purpose 
domain knowledge dictionary named NEUKD 
according to the domain knowledge hierarchy 
description framework(Zhu Jingbo et al, 2002). 
Items defined in the NEUKD include domain 
associated term, domain feature and domain. 
Currently 40 domains, 982 domain features and 
more than 610,000 domain associated terms are 
defined in the NEUKD. Some instances of 
NEUKD are given in Table 1. Because the size 
of NEUKD is limited, so in following content 
we will study machine learning techniques to 
solve the problem of using NEUKD for text 
classification task. 
Domain Associated Terms Domain Features Domain 
??
(Yao Ming) 
??, ???
(Basketball, Athlete)  
??
(Sports ) 
????
(The Sanxia project) 
????
(Irrigation Project) 
??
(Irrigation Works) 
??
(Match Season) 
??
(Match)
??
(Sports ) 
????
(Arsenal Team) 
??
(Football)
??
(Sports)
??????
(Industrial and commercial bank of China) 
??
(Bank)
??
(Finance)
Table 1. Some instances defined in the NEUKD 
3 Bootstrapping-based DAT Learning 
Algorithm
To extend domain knowledge dictionary, in this 
paper, we will use a feature learning algorithm 
based on bootstrapping (FLB)(Zhu Jingbo et al, 
2004) to learning new DATs. In the FLB learn-
ing procedure, some seed words are given in 
advance. In fact, seed words are some important 
DATs. For example, ten seed words of domain 
???(finance)? are??(stock), ??(finance), 
??(loan), ??(stock), ??(finance and eco-
nomics), ??(bank), ??(tax), ??(foreign
exchange), ?? (investment) and ?? (stock
market).
The FLB learning procedure is described as 
follows:
z Initialization: Use a small number of seed 
words initialize DAT set 
z Iterate Bootstrapping:
? Candidate DAT Learner: Learn some 
new DATs as candidate DATs from 
unlabeled data. 
? Evaluation: Score all candidate DATs, 
and select top-n best DATs as new 
seed words, and add them into DAT set. 
In the beginning of algorithm, all words ex-
cept stopwords in the unlabeled corpus could be 
111
considered as candidate DATs. In fact, we can 
regard bootstrapping as iterative clustering. In 
the evaluation step of FLB algorithm, RlogF 
metric method(Ellen Riloff, Rosie Jones, 1999) 
is used as evaluation function which assigns a 
score to a word(candidate DAT). The score of a 
word is computed as: 
iii RXwFLogwM u ),()( 2            (1) 
Where F(wi,X) is the frequency of co-occurrence 
of word wi and X (set of seed words) in the same 
sentence, F(wi) is the frequency of wi in the cor-
pus, and Ri=F(wi,X)/F(wi). The RlogF metric 
tries to strike a balance between reliability and 
frequency: R is high when the word is highly 
correlated with set of seed words, and F is high 
when the word and X highly co-occur in the 
same sentence.
    In the experiments, we use the corpus from 
1996-1998 People?s Daily as unlabeled data 
which has about 50 million words. For domain 
??? (finance)?, we select ten seed words 
shown in above example, the bootstrapping-
based DAT learning algorithm obtains 65% pre-
cision performance within top-1000 new learned 
DATs according to human judgment. 
4 Domain Knowledge based Text Clas-
sification
In this paper, na?ve Bayes(NB) model 
(McCallum and K.Nigam, 1998) is used to build 
text classifier. We want to study how to use our 
Chinese domain knowledge dictionary NEUKD 
to improve text categorization. 
4.1 BOW Model
The most commonly used document representa-
tion is the so called vector space model(G.Salton 
and M.J.McGill, 1983). In the vector space 
model, documents are represented by vectors of 
terms (textual features, e.g. words, phases, etc.). 
Conventional bag-of-words model (BOW) uses 
common words as textual features. In the com-
parison experiments, we use the BOW model as 
baseline NB system.  
4.2 BOTW Model
As above mentioned, more than 610000 domain 
associated terms (DATs) are defined in the 
NEUKD, such as ???(Yao Ming) ?, ????
?(The Sanxia project)?, and ???????
(Industrial and commercial bank of China)? 
shown in table 1. We use domain associated 
terms and common words as textual features, 
called BOTW models (short for bag-of-terms 
and words model). For example, in the previous 
examples, the DAT ?????(The Sanxia pro-
ject, Sanxia is a LOCATION name of China)? 
can be used as a textual feature in BOTW model. 
But in BOW model(baseline system) we con-
sider two common words ???(The Sanxia)? 
and ???(project)? as two different textual fea-
tures.
4.3 BOF Model
Similar to BOTW model, we use domain fea-
tures as textual features in the NB classifier, 
called BOF model (short for bag-of-features 
model). In BOF model, we first transform all 
DATs into domain features according to defini-
tions in the NEUKD, and group DATs with 
same domain features as a cluster, called Topic 
Cluster. For Examples, Topic Cluster ???
(sports)? includes some DATs, such as ???
(match season)?, ?????(Arsenal)?, ????
(Olympic Games)?, ???? (Table Tennis)?, 
???(Yao Ming)?. In BOF model, we use topic 
clusters as textual features for text categorization. 
Also the classification computation procedure of 
BOF model is same as of BOW model.  
4.4 BOL Model 
To solve the problem of the limitation of 
NEUKD, in this paper, we propose a machine 
learning technique to improve BOF model. The 
basic ideas are that we wish to learn new DATs 
from pre-classified documents, and group them 
into the predefined topic clusters which are 
formed and used as textual features in BOF 
model discussed in section 4.3. Then these new 
topic clusters could be used as textual features 
for text categorization. We call the new model as 
BOL model(short for bag-of-learned features 
model) which could be consider as an extended 
version of BOF model.  
First we group all DATs originally defined in 
NEUKD into a lot of topic clusters as described 
in BOF model, which are used as seeds in fol-
lowing learning procedure. Then we group other 
words (not be defined in NEUKD) into these 
topic clusters. The Learning algorithm is de-
scribed as following: 
- Preprocessing: Text segmentation, extract-
ing candidate words, and sort the candidate 
words by CHI method. As above mentioned, 
all candidate words except stopwords which 
112
are not defined in NEUKD will be grouped 
into topic clusters in this process. 
- Initialization: These words, which are de-
fined in NEUKD, are first added to corre-
sponding topic clusters according to their 
associated domain features, respectively. 
- Iteration: Loop until all candidate words 
have been put into topic clusters: 
? Measure similarity of a candidate word 
and each topic cluster, respectively. 
? Put the candidate word into the most 
similar topic cluster(Note that a word 
can only be grouped into one cluster).
The important issue of above procedures is 
how to measure the similarity between a word 
and a topic cluster. Chen Wenliang et. al.(2004)
proposed a measure for word clustering algo-
rithm used in text classification. So in this paper, 
we use Chen?s measure to measure the similarity 
between a word and a topic cluster in above 
learning algorithm. The similarity of a word wt
and a topic cluster fj is defined as 
| |
1
( )( ( , ) ( , ))
( ) ( )
( )
( ) | |
t t t j j t j
t j
t L
i
i
S w w w f f w f
N w N f
w
N f W
O [ [
O
 
 ?  ?
 
?
   (2) 
Where 
( )( , )
( ) ( )
( ( | ) || ( | ))
( )
( , )
( ) ( )
( ( | ) || ( | ))
t
t t j
t j
t t j
j
j t j
t j
j t j
P ww w f
P w P f
D P C w P C w f
P f
f w f
P w P f
D P C f P C w f
[
[
?  
u ?
?  
u ?
Where we define the distribution P(C|wt) as the 
random variable over classes C, and its distribu-
tion given a particular word wt. N(fi) denote the 
number of words in the topic cluster fi, W is the 
list of candidate words. 
    To describe how to estimate distribution 
P(C|f) , we first assume that in the beginning of 
learning procedure, only a word w1 is included 
in topic cluster f1, we could say that P(C|f1) = 
P(C|w1). When a new word w2 is added into 
topic cluster f1, we could get a new topic cluster 
f2. How to estimate the new distribution P(C|f2)
is key step, where f2=w2?f1. We could use the 
following formula (3) to estimate distribution 
P(C|f2) =P(C|w2?f1). Similarly, we could know 
if the new word wn is added into topic cluster fn-1
to form a new topic cluster fn, we also could es-
timate P(C|fn)=P(C|wn?fn-1) following this way, 
and so on. 
2 2 1
2
2
2 1
1
1
2 1
( | ) ( | )
( ) ( | )
( ) ( )
( ) ( | )
( ) ( )
P C f P C w f
P w P C w
P w P f
P f P C f
P w P f
 ?
 
 
                (3) 
We turn back the question about how to meas-
ure the difference between two probability 
distributions. Kullback-Leibler divergence is 
used to do this. The KL divergence between two 
class distributions induced by wt and ws is writ-
ten as 
| |
1
( ( | ) || ( | ))
( | )
( | ) log( )
( | )
t s
C
j t
j t
j j s
D P C w P C w
P c w
P c w
P c w 
 
?              (4) 
In preprocessing step, the CHI statistic meas-
ures the lack of independence of feature t and 
category c.  
D)B)(CD)(AC)(B(A
BC)-N(ADc)2(t,
2
 F
Where t refers to a feature and c refers to a cate-
gory, A is the number of times t and c co-occur, 
B is the number of times t occurs without c, C is 
the number of times c occurs without t, D is the 
number of times neither c nor t co-occur, and N 
is the total number of documents.
5 Experimental Results 
In this paper, we use na?ve Bayes for classifying 
documents. Here we only describe multinomial 
na?ve Bayes briefly since full details have been 
presented in the paper(McCallum and K.Nigam, 
1998). The basic idea in na?ve Bayes approaches 
is to use the joint probabilities of words and 
categories to estimate the probabilities of catego-
ries when a document is given. Given a docu-
ment d for classification, we calculate the 
probabilities of each category c as follows: 
( | )| |
1
( ) ( | )( | ) ( )
( | )( ) ( | )!
iN t dT
i
i i
P c P d cP c d P d
P t cP c N t d 
 
v ?
Where N(ti|d) is the frequency of word ti in 
document d, T is the vocabulary and |T| is the 
113
size of T, ti is the ith word in the vocabulary, and 
P(ti|c) thus represents the probability that a ran-
domly drawn word from a randomly drawn 
document in category c will be the word ti.
In the experiments, we use NEU_TC data 
set(Chen Wenliang et. al. 2004) to evaluate the 
performance of baseline NB classifier and our 
classifiers. The NEU_TC data set contains Chi-
nese web pages collected from web sites. The 
pages are divided into 37 classes according to 
?Chinese Library Categorization?(CLCEB, 
1999). It consists of 14,459 documents. We do 
not use tag information of pages. We use the 
toolkit CipSegSDK(Yao Tianshun et. al. 2002) 
for word segmentation. We removed all words 
that had less than two occurrences. The resulting 
vocabulary has about 60000 words.
In the experiments, we use 5-fold cross vali-
dation where we randomly and uniformly split 
each class into 5 folds and we take four folds for 
training and one fold for testing. In the cross-
validated experiments we report on the average 
performance. For evaluating the effectiveness of 
category assignments by classifiers to docu-
ments, we use the conventional recall, precision 
and F1 measures. Recall is defined to be the ra-
tio of correct assignments by the system divided 
by the total number of correct assignments. Pre-
cision is the ratio of correct assignments by the 
system divided by the total number of the sys-
tem?s assignments. The F1 measure combines 
recall (r) and precision (p) with an equal weight 
in the following form: 
pr
rpprF  
2),(1
In fact, these scores can be computed for the 
binary decisions on each individual category 
first and then be averaged over categories. The 
way is called macro-averaging method. For 
evaluating performance average across class, we 
use the former way called micro averaging 
method in this paper which balances recall and 
precision in a way that gives them equal weight. 
The micro-averaged F1 measure has been widely 
used in cross-method comparisons.  
To evaluate the performance of these four 
models based on NB classifier, we construct four 
systems in the experiments, including BOW, 
BOTW, BOF and BOL classifier. CHI measure 
is used to feature selection in all text classifiers.
Figure 1. Experimental results of BOW, BOTW, BOF, BOL classifiers 
In figure 1, we could find that BOTW classi-
fier always performs better than BOW classifier 
when the number of features is larger than about 
500. From comparative experimental results of 
BOTW and BOW classifiers, we think that do-
main associated items are a richer and more pre-
cise representation of meaning than common 
words. Because the total number of domain fea-
tures in NEUKD is only 982, in figure 1 we find 
the maximum number of features (domain fea-
114
tures) for BOF and BOL classifier is less than 
1000. When the number of features is between 
200 and 1000, BOF classifier performs better 
than BOW and BOTW classifiers. It is also ob-
vious that BOL classifier always performs better 
than other three classifiers when the number of 
features is less than 1000. As above mentioned, 
in BOL model, we use a machine learning tech-
nique to solve the problem of limitation of size 
of NEUKD, and group rest 65.01% words into 
predefined topic clusters as textual features in 
BOL model. So the classifier based on BOL 
model can yield better performance than BOF 
model. 
6 Conclusions and Future Work 
In this paper, we first introduce our Chinese do-
main knowledge dictionary NEUKD. To allevi-
ate the cost of construction of domain 
knowledge dictionary by hand, we propose a 
boostrapping-based algorithm to learn new do-
main associated terms from a large amount of 
unlabeled data. This paper studies how to im-
prove text categorization by using domain 
knowledge dictionary. To do it, we propose two 
models using domain knowledge as textual fea-
tures. The first one is BOTW model which uses 
domain associated terms and common words as 
textual features. The other one is BOF model 
which uses domain features as textual features. 
But due to limitation of size of domain knowl-
edge dictionary, many useful words are lost in 
the training procedure. We study and use a ma-
chine learning technique to solve the problem to 
improve knowledge-based text categorization, 
and propose a BOL model which could be con-
sidered as the extension version of BOF model. 
Comparison experimental results of those four 
models (BOW, BOTW, BOF and BOL) show 
that domain knowledge is very useful for im-
proving text categorization. In fact, a lot of 
knowledge-based NLP application systems have 
to face the problem of limitation of size of 
knowledge bases. Like our work discussed in 
this paper, we think that using machine learning 
techniques is a good way to solve such problem. 
In the future work, we will study how to apply 
the domain knowledge to improve other text un-
derstanding tasks, such as information retrieval, 
information extraction, topic detection and track-
ing (TDT).
Acknowledgements 
This research was supported in part by the Na-
tional Natural Science Foundation of China & 
Microsoft Research Asia (No. 60203019), the 
Key Project of Chinese Ministry of Education 
(No. 104065), and the National Natural Science 
Foundation of China (No. 60473140). 
References
Chen Wenliang, Chang Xingzhi, Wang Huizhen, Zhu 
Jingbo, and Yao Tianshun. 2004. Automatic 
Word Clustering for Text Categorization Using 
Global Information. First Asia Information Re-
trieval Symposium (AIRS 2004), LNCS, Beijing, 
pp.1-6
CLCEB. 1999. China Library Categorization Edito-
rial Board. China Library Categorization (The 4th 
ed.) (In Chinese), Beijing, Beijing Library Press.
Ellen Riloff, Rosie Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level 
Bootstrapping, Proceedings of the Sixteenth Na-
tional Conference on Artificial Intelligence.
G.Salton and M.J.McGill, 1983. An introduction to 
modern information retrieval, McGraw-Hill.
McCallum and K.Nigam. 1998. A Comparison of 
Event Models for na?ve Bayes Text Classification, 
In AAAI-98 Workshop on Learning for Text Cate-
gorization.
M. Fuketa, S.Lee, T.Tsuji, M.Okada and J. Aoe. 2000. 
A document classification method by using field 
associated words. International Journal of Infor-
mation Sciences. 126(1-4), p57-70 
Sangkon Lee, Masami Shishibori. 2002. Passage seg-
mentation based on topic matter, International 
journal of computer processing of oriental lan-
guages, 15(3), p305-339. 
Scott, Sam?Stan Matwin. 1998. Text classification 
using WordNet hypernyms. Proceedings of the 
COLING/ACL Workshop on Usage of WordNet in 
Natural Language Processing Systems, Montreal. 
Yao Tianshun, Zhu Jingbo, Zhang li, and Yang Ying, 
2002. Natural Language Processing- research on 
making computers understand human languages, 
Tsinghua University Press, (In Chinese). 
Zhu Jingbo and Yao Tianshun. 2002. FIFA-based 
Text Classification, Journal of Chinese Informa-
tion Processing, V16, No3.(In Chinese) 
Zhu Jingbo, Chen Wenliang, and Yao Tianshun. 2004. 
Using Seed Words to Learn to Categorize Chinese 
Text. Advances in Natural Language Processing: 
4th International Conference (EsTAL 2004),
pp.464-473 
115
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 113?120
Manchester, August 2008
Learning Reliable Information for Dependency Parsing Adaptation
Wenliang Chen
?
, Youzheng Wu
?
, Hitoshi Isahara
?
?
Language Infrastructure Group
?
Spoken Language Communication Group, ATR
?
Machine Translation Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, youzheng.wu, isahara}@nict.go.jp
Abstract
In this paper, we focus on the adaptation
problem that has a large labeled data in the
source domain and a large but unlabeled
data in the target domain. Our aim is to
learn reliable information from unlabeled
target domain data for dependency pars-
ing adaptation. Current state-of-the-art sta-
tistical parsers perform much better for
shorter dependencies than for longer ones.
Thus we propose an adaptation approach
by learning reliable information on shorter
dependencies in an unlabeled target data
to help parse longer distance words. The
unlabeled data is parsed by a dependency
parser trained on labeled source domain
data. The experimental results indicate
that our proposed approach outperforms
the baseline system, and is better than cur-
rent state-of-the-art adaptation techniques.
1 Introduction
Dependency parsing aims to build the dependency
relations between words in a sentence. There
are many supervised learning methods for training
high-performance dependency parsers(Nivre et al,
2007), if given sufficient labeled data. However,
the performance of parsers declines when we are in
the situation that a parser is trained in one ?source?
domain but is to parse the sentences in a second
?target? domain. There are two tasks(Daum?e III,
2007) for the domain adaptation problem. The
first one is that we have a large labeled data in the
source domain and a small labeled data in target
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
domain. The second is similar, but instead of hav-
ing a small labeled target data, we have a large but
unlabeled target data. In this paper, we focus on
the latter one.
Current statistical dependency parsers perform
worse while the distance of two words is becoming
longer for domain adaptation. An important char-
acteristic of parsing adaptation is that the parsers
perform much better for shorter dependencies than
for longer ones (the score at length l is much higher
than the scores at length> l ).
In this paper, we propose an approach by using
the information on shorter dependencies in auto-
parsed target data to help parse longer distance
words for adapting a parser. Compared with the
adaptation methods of Sagae and Tsujii (2007) and
Reichart and Rappoport (2007), our approach uses
the information on word pairs in auto-parsed data
instead of using the whole sentences as newly la-
beled data for training new parsers. It is difficult
to detect reliable parsed sentences, but we can find
relative reliable parsed word pairs according to de-
pendency length. The experimental results show
that our approach significantly outperforms base-
line system and current state of the art techniques.
2 Motivation and prior work
In dependency parsing, we assign head-dependent
relations between words in a sentence. A simple
example is shown in Figure 1, where the arc be-
tween a and hat indicates that hat is the head of a.
Current statistical dependency parsers perform
better if the dependency lengthes are shorter (Mc-
Donald and Nivre, 2007). Here the length of the
dependency from word w
i
to word w
j
is simply
equal to |i ? j|. Figure 2 shows the results (F
1
113
The  boy  saw    a       red       hat    .
Figure 1: An example for dependency relations.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14  16  18  20
F1
Dependency Length
sameDomain
diffDomain
Figure 2: The scores relative to dependency length.
?SameDomain? refers to training and testing in the
same domain, and ?diffDomain? refers to training
and testing in two domains (domain adaptation).
score)
1
on our testing data, provided by a deter-
ministic parser, which is trained on labeled source
data. Comparing two curves at the figure, we find
that the scores of diffDomain decreases muchmore
sharply than the scores of sameDomain, when de-
pendency length increases. The score decreases
from about 92% at length 1 to 50% at 7. When
lengthes are larger than 7, the scores are below
50%. We also find that the score at length l is much
higher (around 10%) than the score at length l + 1
from length 1 to 7. There is only one exception that
the score at length 4 is a little less than the score at
length 5. But this does not change so much and the
scores at length 4 and 5 are much higher than the
one at length 6.
Two words (word w
i
and word w
j
) having a
dependency relation in one sentence can be adja-
cent words (word distance = 1), neighboring words
(word distance = 2), or the words with distance >
2 in other sentences. Here the distance of word
pair (word w
i
and word w
j
) is equal to |i ? j|. For
example, ?a? and ?hat? has dependency relation in
the sentence at Figure 1. They can also be adjacent
words in the sentence ?The boy saw a hat.? and
the words with distance = 3 in ?I see a red beauti-
ful hat.?. This makes it possible for the word pairs
with different distances to share the information.
According to the above observations, we present
1
F
1
= 2 ? precision ? recall/(precision + recall) where
precision is the percentage of predicted arcs of length d that
are correct and recall is the percentage of gold standard arcs
of length d that are correctly predicted.
an idea that the information on shorter depen-
dencies in auto-parsed target data is reliable for
parsing the words with longer distance for do-
main adaptation. Here, ?shorter? is not exactly
short. That is to say, the information on depen-
dency length l in auto-parsed data can be used to
help parse the words whose distances are longer
than l when testing, where l can be any number.
We do not use the dependencies whose lengthes
are too long because the accuracies of long depen-
dencies are very low.
In the following content, we demonstrate our
idea with an example. The example shows how to
use the information on length 1 to help parse two
words whose distance is longer than 1. Similarly,
the information on length l can also be used to help
parse the words whose distance is longer than l.
Figure 2 shows that the dependency parser per-
forms best at tagging the relations between adja-
cent words. Thus, we expect that dependencies of
adjacent words in auto-parsed target data can pro-
vide useful information for parsing words whose
distances are longer than 1. We suppose that our
task is Chinese dependency parsing adaptation.
Here, we have two words ???JJ(large-scale)?
and ???NN(exhibition)?. Figure 3 shows
the examples in which word distances of these
two words are different. For the sentences in
the bottom part, there is a ambiguity of ?JJ
+ NN1 + NN2? at ?? ?JJ(large-scale)/?
?NN(art)/??NN(exhibition)?, ???JJ(large-
scale)/? ?NN(culture)/? ?NN(art)/?
?NN(exhibition)? and ???JJ(large-scale)/?
?NR(China)/? ?NN(culture)/? ?NN(art)/?
?NN(exhibition)?. Both NN1 and NN2 could be
the head of JJ. In the examples in the upper part,
???JJ(large-scale)? and ???NN(exhibition)?
are adjacent words, for which current parsers
can work well. We use a parser to parse the
sentences in the upper part. ???(exhibition)? is
assigned as the head of ???(large-scale)?. Then
we expect the information from the upper part
can help parse the sentences in the bottom part.
Now, we consider what a learning model could
do to assign the appropriate relation between ??
?(large-scale)? and ???(exhibition)? in the
bottom part. We provide additional information
that ???(exhibition)? is the possible head of ??
?(large-scale)? in the auto-parsed data (the upper
part). In this way, the learning model may use this
information to make correct decision.
114
A1)?	
///?
A2)?///Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1129?1133,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Two-stage Parser for Multilingual Dependency Parsing
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
We present a two-stage multilingual de-
pendency parsing system submitted to the
Multilingual Track of CoNLL-2007. The
parser first identifies dependencies using a
deterministic parsing method and then labels
those dependencies as a sequence labeling
problem. We describe the features used in
each stage. For four languages with differ-
ent values of ROOT, we design some spe-
cial features for the ROOT labeler. Then we
present evaluation results and error analyses
focusing on Chinese.
1 Introduction
The CoNLL-2007 shared tasks include two tracks:
the Multilingual Track and Domain Adaptation
Track(Nivre et al, 2007). We took part the Multi-
lingual Track of all ten languages provided by the
CoNLL-2007 shared task organizers(Hajic? et al,
2004; Aduriz et al, 2003; Mart?? et al, 2007; Chen
et al, 2003; Bo?hmova? et al, 2003; Marcus et al,
1993; Johansson and Nugues, 2007; Prokopidis et
al., 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003) .
In this paper, we describe a two-stage parsing
system consisting of an unlabeled parser and a se-
quence labeler, which was submitted to the Multi-
lingual Track. At the first stage, we use the pars-
ing model proposed by (Nivre, 2003) to assign the
arcs between the words. Then we obtain a depen-
dency parsing tree based on the arcs. At the sec-
ond stage, we use a SVM-based approach(Kudo and
Matsumoto, 2001) to tag the dependency label for
each arc. The labeling is treated as a sequence la-
beling problem. We design some special features
for tagging the labels of ROOT for Arabic, Basque,
Czech, and Greek, which have different labels for
ROOT. The experimental results show that our ap-
proach can provide higher scores than average.
2 Two-Stage Parsing
2.1 The Unlabeled Parser
The unlabeled parser predicts unlabeled directed de-
pendencies. This parser is primarily based on the
parsing models described by (Nivre, 2003). The al-
gorithm makes a dependency parsing tree in one left-
to-right pass over the input, and uses a stack to store
the processed tokens. The behaviors of the parser
are defined by four elementary actions (where TOP
is the token on top of the stack and NEXT is the next
token in the original input string):
? Left-Arc(LA): Add an arc from NEXT to TOP;
pop the stack.
? Right-Arc(RA): Add an arc from TOP to
NEXT; push NEXT onto the stack.
? Reduce(RE): Pop the stack.
? Shift(SH): Push NEXT onto the stack.
Although (Nivre et al, 2006) used the pseudo-
projective approach to process non-projective de-
pendencies, here we only derive projective depen-
dency tree. We use MaltParser(Nivre et al, 2006)
1129
V0.41 to implement the unlabeled parser, and use
the SVM model as the classifier. More specifically,
the MaltParser use LIBSVM(Chang and Lin, 2001)
with a quadratic kernel and the built-in one-versus-
all strategy for multi-class classification.
2.1.1 Features for Parsing
The MaltParser is a history-based parsing model,
which relies on features of the derivation history
to predict the next parser action. We represent the
features extracted from the fields of the data repre-
sentation, including FORM, LEMMA, CPOSTAG,
POSTAG, and FEATS. We use the features for all
languages that are listed as follows:
? The FORM features: the FORM of TOP and
NEXT, the FORM of the token immediately
before NEXT in original input string, and the
FORM of the head of TOP.
? The LEMMA features: the LEMMA of TOP
and NEXT, the LEMMA of the token immedi-
ately before NEXT in original input string, and
the LEMMA of the head of TOP.
? The CPOS features: the CPOSTAG of TOP and
NEXT, and the CPOSTAG of next left token of
the head of TOP.
? The POS features: the POSTAG of TOP and
NEXT, the POSTAG of next three tokens af-
ter NEXT, the POSTAG of the token immedi-
ately before NEXT in original input string, the
POSTAG of the token immediately below TOP,
and the POSTAG of the token immediately af-
ter rightmost dependent of TOP.
? The FEATS features: the FEATS of TOP and
NEXT.
But note that the fields LEMMA and FEATS are not
available for all languages.
2.2 The Sequence Labeler
2.2.1 The Sequence Problem
We denote by x = x
1
, ..., xn a sentence with n
words and by y a corresponding dependency tree. A
dependency tree is represented from ROOT to leaves
1The tool is available at
http://w3.msi.vxu.se/?nivre/research/MaltParser.html
with a set of ordered pairs (i, j) ? y in which xj is a
dependent and xi is the head. We have produced the
dependency tree y at the first stage. In this stage, we
assign a label l
(i,j) to each pair.
As described in (McDonald et al, 2006), we treat
the labeling of dependencies as a sequence labeling
problem. Suppose that we consider a head xi with
dependents xj1, ..., xjM . We then consider the la-
bels of (i, j1), ..., (i, jM) as a sequence. We use the
model to find the solution:
lmax = arg max
l
s(l, i, y, x) (1)
And we consider a first-order Markov chain of la-
bels.
We used the package YamCha (V0.33)2 to imple-
ment the SVM model for labeling. YamCha is a
powerful tool for sequence labeling(Kudo and Mat-
sumoto, 2001).
2.2.2 Features for Labeling
After the first stage, we know the unlabeled de-
pendency parsing tree for the input sentence. This
information forms the basis for part of the features
of the second stage. For the sequence labeler, we
define the individual features, the pair features, the
verb features, the neighbor features, and the position
features. All the features are listed as follows:
? The individual features: the FORM, the
LEMMA, the CPOSTAG, the POSTAG, and
the FEATS of the parent and child node.
? The pair features: the direction of depen-
dency, the combination of lemmata of the
parent and child node, the combination of
parent?s LEMMA and child?s CPOSTAG, the
combination of parent?s CPOSTAG and child?s
LEMMA, and the combination of FEATS of
parent and child.
? The verb features: whether the parent or child
is the first or last verb in the sentence.
? The neighbor features: the combination of
CPOSTAG and LEMMA of the left and right
neighbors of the parent and child, number of
children, CPOSTAG sequence of children.
2YamCha is available at
http://chasen.org/?taku/software/yamcha/
1130
? The position features: whether the child is the
first or last word in the sentence and whether
the child is the first word of left or right of par-
ent.
2.2.3 Features for the Root Labeler
Because there are four languages have different
labels for root, we define the features for the root
labeler. The features are listed as follows:
? The individual features: the FORM, the
LEMMA, the CPOSTAG, the POSTAG, and
the FEATS of the parent and child node.
? The verb features: whether the child is the first
or last verb in the sentence.
? The neighbor features: the combination of
CPOSTAG and LEMMA of the left and right
neighbors of the parent and child, number of
children, CPOSTAG sequence of children.
? The position features: whether the child is the
first or last word in the sentence and whether
the child is the first word of left or right of par-
ent.
3 Evaluation Results
We evaluated our system in the Multilingual Track
for all languages. For the unlabeled parser, we chose
the parameters for the MaltParser based on perfor-
mance from a held-out section of the training data.
We also chose the parameters for Yamcha based on
performance from training data.
Our official results are shown at Table 1. Perfor-
mance is measured by labeled accuracy and unla-
beled accuracy. These results showed that our two-
stage system can achieve good performance. For all
languages, our system provided better results than
average performance of all the systems(Nivre et al,
2007). Compared with top 3 scores, our system
provided slightly worse performance. The reasons
may be that we just used projective parsing algo-
rithms while all languages except Chinese have non-
projective structure. Another reason was that we did
not tune good parameters for the system due to lack
of time.
Data Set LA UA
Arabic 74.65 83.49
Basque 72.39 78.63
Catalan 86.66 90.87
Chinese 81.24 85.91
Czech 73.69 80.14
English 83.81 84.91
Greek 74.42 81.16
Hungarian 75.34 79.25
Italian 82.04 85.91
Turkish 76.31 81.92
average 78.06 83.22
Table 1: The results of proposed approach. LA-
BELED ATTACHMENT SCORE(LA) and UNLA-
BELED ATTACHMENT SCORE(UA)
4 General Error Analysis
4.1 Chinese
For Chinese, the system achieved 81.24% on labeled
accuracy and 85.91% on unlabeled accuracy. We
also ran the MaltParser to provide the labels. Be-
sides the same features, we added the DEPREL fea-
tures: the dependency type of TOP, the dependency
type of the token leftmost of TOP, the dependency
type of the token rightmost of TOP, and the de-
pendency type of the token leftmost of NEXT. The
labeled accuracy of MaltParser was 80.84%, 0.4%
lower than our system.
Some conjunctions, prepositions, and DE3 at-
tached to their head words with much lower ac-
curacy: 74% for DE, 76% for conjunctions, and
71% for prepositions. In the test data, these words
formed 19.7%. For Chinese parsing, coordination
and preposition phrase attachment were hard prob-
lems. (Chen et al, 2006) defined the special features
for coordinations for chunking. In the future, we
plan to define some special features for these words.
Now we focused words where most of the errors
occur as Table 2 shows. For ??/DE?, there was
32.4% error rate of 383 occurrences. And most of
them were assigned incorrect labels between ?prop-
erty? and ?predication?: 45 times for ?property? in-
stead of ?predication? and 20 times for ?predica-
tion? instead of ?property?. For examples, ??/DE?
3including ??/?/?/??.
1131
num any head dep both
?/ DE 383 124 35 116 27
a/ C 117 38 36 37 35
?/ P 67 20 6 19 5
??/ N 31 10 8 4 2
?/ V 72 8 8 8 8
Table 2: The words where most of errors occur in
Chinese data.
in ???/?/??/??(popular TV channel)? was
to be tagged as ?property? instead of ?predication?,
while ??/DE? in ????/?/??(volunteer of
museum)? was to be tagged as ?predication? instead
of ?property?. It was very hard to tell the labels be-
tween the words around ???. Humans can make
the distinction between property and predication for
???, because we have background knowledge of
the words. So if we can incorporate the additional
knowledge for the system, the system may assign
the correct label.
For ?a/C?, it was hard to assign the head, 36
wrong head of all 38 errors. It often appeared at
coordination expressions. For example, the head
of ?a? at ??/?/?/?/a/?/?/?/??/(Besides
extreme cool and too amazing)? was ????, and
the head of ?a? at ????/??/?/??/a/?/?
?/?/??(Give the visitors solid and methodical
knowledge)? was ????.
5 Conclusion
In this paper, we presented our two-stage depen-
dency parsing system submitted to the Multilingual
Track of CoNLL-2007 shared task. We used Nivre?s
method to produce the dependency arcs and the se-
quence labeler to produce the dependency labels.
The experimental results showed that our system can
provide good performance for all languages.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www. csie. ntu. edu. tw/cjlin/libsvm, 80:604?
611.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006.
An empirical study of chinese chunking. In COL-
ING/ACL 2006(Poster Sessions), Sydney, Australia,
July.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216?220, New
York City, June. Association for Computational Lin-
guistics.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
1132
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Inter-
national Workshop on Parsing Technologies (IWPT),
pages 149?160.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
1133
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30?39,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Semantic Dependency Parsing of NomBank and PropBank:
An Efficient Integrated Approach via a Large-scale Feature Selection
?
Hai Zhao(??)?, Wenliang Chen(???)?, Chunyu Kit?(???)
?
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong, China
?
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
We present an integrated dependency-
based semantic role labeling system for
English from both NomBank and Prop-
Bank. By introducing assistant argument
labels and considering much more fea-
ture templates, two optimal feature tem-
plate sets are obtained through an effec-
tive feature selection procedure and help
construct a high performance single SRL
system. From the evaluations on the date
set of CoNLL-2008 shared task, the per-
formance of our system is quite close to
the state of the art. As to our knowl-
edge, this is the first integrated SRL sys-
tem that achieves a competitive perfor-
mance against previous pipeline systems.
1 Introduction
We investigate the possibility to construct an effec-
tive integrated system for dependency-based se-
mantic role labeling (SRL) task. This means in
this work that a single system handles all these
sub-tasks, predicate identification/disambiguation
and argument identification/classification, regard-
less of whether the predicate is verbal or nominal.
Traditionally, a SRL task, either dependency
or constituent based, is implemented as two sub-
tasks, namely, argument identification and clas-
sification. If the predicate is unknown, then a
predicate identification or disambiguation subtask
should be additionally considered. A pipeline
framework is usually adopted to handle all these
sub-tasks. The reason to divide the whole task
?
This study is partially supported by CERG grant
9040861 (CityU 1318/03H), CityU Strategic Research Grant
7002037.
into multiple stages is two-fold, one is each sub-
task asks for its favorable features, the other is
at the consideration of computational efficiency.
Generally speaking, a joint system is slower than
a pipeline system in training. (Xue and Palmer,
2004) fount out that different features suited for
different sub-tasks of SRL, i.e. argument identifi-
cation and classification. The results from CoNLL
shared tasks in 2005 and 2008 (Carreras and Mar-
quez, 2005; Koomen et al, 2005; Surdeanu et al,
2008; Johansson and Nugues, 2008), further show
that SRL pipeline may be one of the standard to
achieve a state-of-the-art performance in practice.
In the recent years, most works on SRL, includ-
ing two CoNLL shared task in 2004 and 2005,
focus on verbal predicates with the availability
of PropBank (Palmer et al, 2005). As a com-
plement to PropBank, NomBank (Meyers et al,
2004) annotates nominal predicates and their cor-
responding semantic roles using similar semantic
framework as PropBank. Though SRL for nomi-
nal predicates offers more challenge, it draws rel-
atively little attention (Jiang and Ng, 2006).
(Pustejovsky et al, 2005) discussed the issue of
merging various treebanks, including PropBank,
NomBank, and others. The idea of merging these
two different treebanks was implemented in the
CoNLL-2008 shared task (Surdeanu et al, 2008).
However, few empirical studies support the ne-
cessity of an integrated learning strategy from
NomBank and PropBank. Though aiming at Chi-
nese SRL, (Xue, 2006) reported that their exper-
iments show that simply adding the verb data to
the training set of NomBank and extracting the
same features from the verb and noun instances
will hurt the overall performance. From the re-
sults of CoNLL-2008 shared task, the top system
by (Johansson and Nugues, 2008) also used two
30
different subsystems to handle verbal and nominal
predicates, respectively.
Despite all the above facts, an integrated SRL
system still holds some sort of merits, being eas-
ier to implement, a single-stage feature selection
benefiting the whole system, an all-in-one model
outputting all required semantic role information
and so on.
The shared tasks at the CoNLL 2008 and 2009
are devoted to the joint learning of syntactic and
semantic dependencies, which show that SRL can
be well performed using only dependency syn-
tax input. Using data and evaluation settings
of the CoNLL-2008 shared task, this work will
only focus on semantic dependency parsing and
compares the best-performing SRL system in the
CoNLL-2009 shared Task (Zhao et al, 2009b)
with those in the CoNLL-2008 shared task (Sur-
deanu et al, 2008; Haji?c et al, 2009)
1
.
Aiming at main drawbacks of an integrated ap-
proach, two key techniques will be applied. 1)
Assistant argument labels are introduced for the
further improvement of argument pruning. This
helps the development of a fast and lightweight
SRL system. 2) Using a greedy feature selec-
tion algorithm, a large-scale feature engineering is
performed on a much larger feature template set
than that in previous work. This helps us find fea-
tures that may be of benefit to all SRL sub-tasks as
long as possible. As two optimal feature template
sets have been proven available, for the first time
we report that an integrated SRL system may pro-
vide a result close to the state-of-the-art achieved
by those SRL pipelines or individual systems for
some specific predicates.
2 Adaptive Argument Pruning
A word-pair classification is used to formulate se-
mantic dependency parsing as in (Zhao and Kit,
2008). As for predicate identification or disam-
biguation, the first word is set as a virtual root
(which is virtually set before the beginning of the
sentence.) and the second as a predicate candi-
date. As for argument identification/classification,
the first word in a word pair is specified as a predi-
1
CoNLL-2008 is an English-only task, while CoNLL-
2009 is a multilingual one. Though the English corpus in
CoNLL-2009 is almost identical to the corpus in the CoNLL-
2008 shared task evaluation, the latter holds more sophisti-
cated input structure as in (Surdeanu et al, 2008). The most
difference for these two tasks is that the identification of se-
mantic predicates is required in the task of CoNLL-2008 but
not in CoNLL-2009.
cate candidate and the second as an argument can-
didate. In either of case, the first word is called a
semantic head, and noted as p in our feature rep-
resentation, the second is called a semantic depen-
dent and noted as a.
Word pairs are collected for the classifier in
such order. The first word of the pair is set to the
virtual root at first, the second word is then spec-
ified as a predicate candidate. According to the
result that the predicate candidate is classified or
proven to be non-predicate, 1) the second word is
reset to next predicate candidate if the answer is
non-predicate, otherwise, 2) the first word of the
pair is reset to the predicate that is just determined,
and the second is set to every argument candidates
one by one. The classifier will scan the input sen-
tence from left to right to check if each word is a
true predicate.
Without any constraint, all word pairs in an in-
put sequence must be considered by the classifier,
leading to poor computational efficiency and un-
necessary performance loss. Thus, the training
sample for SRL task needs to be pruned properly.
We use a simple strategy to prune predicate can-
didates, namely, only verbs and nouns are chosen
in this case.
There are two paths to collect argument candi-
dates over the sequence. One is based on an input
syntactic dependency tree, the other is based on
a linear path of the sentence. As for the former
(hereafter it is referred to synPth), we continue to
use a dependency version of the pruning algorithm
of (Xue and Palmer, 2004). The pruning algorithm
is readdressed as the following.
Initialization: Set the given predicate as the
current node;
(1) The current node and all of its syntactic
children are selected as argument candidates
(children are traversed from left to right.).
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
Note that this pruning algorithm is slightly dif-
ferent from that of (Xue and Palmer, 2004), the
predicate itself is also included in the argument
candidate list as the nominal predicate sometimes
takes itself as its argument.
The above pruning algorithm has been shown
effective. However, it is still inefficient for a SRL
31
system that needs to tackle argument identifica-
tion/classification in a single stage. Assuming that
arguments trend to surround their predicate, an as-
sistant argument label ? NoMoreArgument? is in-
troduced for further pruning. If an argument can-
didate in the above algorithm is assigned to such
a label, then the pruning algorithm will end im-
mediately. In training, this assistant label means
no more samples will be generated for the current
predicate, while in test, the decoder will not search
arguments any more. It will be seen that this adap-
tive technique more effectively prunes argument
candidates without missing more true arguments.
Along the linear path (hereafter referred to
linPth), the classifier will search all words before
and after the predicate. Similar to the pruning
algorithm for synPth, we also introduce two as-
sistant argument labels ? noLeft? and ? noRight?
to adaptively prune words too far away from the
predicate.
To show how assistant argument labels actually
work, we give an example for linP th. Suppose an
input sequence with argument labels for a predi-
cate is
a b c d e f g h .
A1 A0
Note that c and g are two boundary words as no
more arguments appear before or after them. After
two assistant argument labels are added, it will be
a b c d e f g h .
noLeft A1 A0 noRight
Training samples will generated from c to g ac-
cording to the above sequence.
We use a Maximum Entropy classifier with a
tunable Gaussian prior as usual. Our implemen-
tation of the model adopts L-BFGS algorithm for
parameter optimization.
3 Feature Templates
3.1 Elements for Feature Generation
Motivated by previous works, we carefully con-
sider those factors from a wide range of features
that can help semantic role labeling for both predi-
cate disambiguation, argument?s identification and
classification as the predicate is either verbal or
nominal. These works include (Gildea and Juraf-
sky, 2002; Carreras and Marquez, 2005; Koomen
et al, 2005; Marquez et al, 2005; Dang and
Palmer, 2005; Pradhan et al, 2005; Toutanova et
al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007;
Surdeanu et al, 2007; Johansson and Nugues,
2008; Che et al, 2008). Most feature templates
that we will adopt for this work will come from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This type of elements include
word form (form and its split form, spForm)
2
,
lemma (lemma,spLemma), and part-of-speech tag
(pos, spPos), syntactic dependency label (dprel),
and semantic dependency label (semdprel)
3
.
Syntactic Connection. This includes syn-
tactic head (h), left(right) farthest(nearest) child
(lm, ln, rm, and rn), and high(low) support
verb or noun. We explain the last item, sup-
port verb(noun). From a given word to the
syntactic root along the syntactic tree, the first
verb/noun/preposition that is met is called as its
low support verb/noun/preposition, and the near-
est one to the root is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006)
4
, we here extend
it to nouns and prepositions. In addition, we intro-
duce a slightly modified syntactic head, pphead,
it returns the left most sibling of a given word if
the word is headed by a preposition, otherwise it
returns the original head.
Path. There are two basic types of path between
the predicate and the argument candidates. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For the latter, we further divide it into four
sub-types with respect to the syntactic root, dp-
Path is the full path in the syntactic tree. Leading
two paths to the root from the predicate and the
argument, respectively, the common part of these
two paths will be dpPathShare. Assume that dp-
PathShare starts from a node r
?
, then dpPathPred
is from the predicate to r
?
, and dpPathArgu is from
the argument to r
?
.
Family. Two types of children sets for the pred-
icate or argument candidate are considered, the
2
In CoNLL-2008, Treebank tokens are split at the position
that a hyphen (-) or a forward slash (/) occurs. This leads to
two types of feature columns, non-split and split.
3
Lemma and pos for either training or test are from auto-
matically pre-analyzed columns in the input files.
4
Note that the meaning of support verb is slightly different
between (Toutanova et al, 2005) and (Xue, 2006; Jiang and
Ng, 2006)
32
first includes all syntactic children (children), the
second also includes all but excludes the left most
and the right most children (noFarChildren).
Concatenation of Elements. For all collected
elements according to linePath, children and so
on, we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
We address some other elements that are not in-
cluded by the above description as the following.
dpTreeRelation. It returns the relationship of a
and p in the input syntactic tree. The possible val-
ues for this feature include parent, sibling
etc.
isCurPred. It judges if a given word is the cur-
rent predicate. If the word is the predicate, then it
returns the predicate itself, otherwise it returns a
default value.
existCross. It judges if a forthcoming depen-
dency relation that is between a given word pair
may cause any cross with all existing dependency
relations.
distance. It counts the number of words along a
given path, either dpPath or linePath.
existSemdprel. It checks if the given argument
label for other predicates has been assigned to a
given word.
voice. This feature returns Active or Passive for
verbs, and a default value for nouns.
baseline. Two types of semantic role baseline
outputs are used for features from (Carreras and
Marquez, 2005)
5
. baseline Ax tags the head of
the first NP before the predicate as A0 and the
head of the first NP after the predicate as A1.
baseline Mod tags the dependant of the predicate
as AM-MOD as it is a modal verb.
We show some feature template examples de-
rived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
p
?1
.pos+p.pos pos of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas
5
These baseline rules were developed by Erik Tjong Kim
Sang, from the University of Antwerp, Belgium.
along the syntactic tree path from the argument
to the predicate, then removed all duplicated
ones and sort the rest, finally concatenate all as a
feature string.
a:p.highSupportNoun|linePath.dprel.seq Col-
lect all dependant labels along with the line path
from the argument to the high support noun of the
predicate, then concatenate all as a feature string.
3.2 Feature Template Selection
Based on the above mentioned elements, 781 fea-
ture templates (hereafter the set of these templates
is referred to FT )
6
are initially considered. Fea-
ture templates in this initial set are constructed in
a generalized way. For example, if we find that
a feature template a.lm.lemma was once used in
some existing work, then such three templates,
a.rm.lemma, a.rn.lemma, a.ln.lemma will be also
added into the set.
As an optimal feature template subset cannot be
expected to be extracted from so large a set by
hand, a greedy feature selection similar to that in
(Jiang and Ng, 2006; Ding and Chang, 2008) is ap-
plied. The detailed algorithm is described in Algo-
rithm 1. Assuming that the number of feature tem-
plates in a given set is n, the algorithm of (Ding
and Chang, 2008) requires O(n
2
) times of train-
ing/test routines, it cannot handle a set that con-
sists of hundreds of templates. As the time com-
plexity of Algorithm 1 is only O(n), it permits a
large scale feature selection accomplished by pay-
ing a reasonable time cost. Though the time com-
plexity of the algorithm given by (Jiang and Ng,
2006) is also linear, it should assume all feature
templates in the initial selected set ?good? enough
and handles other feature template candidates in a
strict incremental way. However, these two con-
straints are not easily satisfied in our case, while
Algorithm 1 may release these two constraints.
Choosing the first 1/10 templates in FT as
the initial selected set S, the feature selection is
performed for two argument candidate traverse
schemes, synPth and linP th, respectively. 4686
machine learning routines run for the former,
while 6248 routines for the latter. Two feature
template sets, FT
syn
and FT
lin
, are obtained at
last. These two sets are given in Table 1-3. We see
that two sets share 30 identical feature templates
as in Table 1. FT
syn
holds 51 different templates
6
This set with detailed explanation will be available at our
website.
33
p.lm.dprel
p.rm.dprel
p.spForm
p
?1
.spLemma
p.spLemma
p
?1
.spLemma+p.spLemma
p.spLemma + p
1
.spLemma
p.spLemma + p.h.spForm
p.spLemma + p.currentSense
p.lemma
p.lemma + p
1
.lemma
p
?1
.pos+p.pos
a.isCurPred.lemma
a
?2
.isCurPred.lemma + a
?1
.isCurPred.lemma
a.isCurPred.spLemma
a
?1
.isCurPred.spLemma + a.isCurPred.spLemma
a.isCurPred.spLemma + a
1
.isCurPred.spLemma
a.children.dprel.bag
a
?1
.spLemma + a.spLemma
a
?1
.spLemma + a.dprel
a
?1
.spLemma + a.dprel + a.h.spLemma
a.lm
?1
.spLemma
a.rm
?1
.dprel + a.spPos
a
?1
.lemma + a.dprel + a.h.lemma
a.lemma + p.lemma
a.pos + p.pos
a.spLemma + p.spLemma
a:p|dpPath.dprel
a:p|dpPathArgu.dprel
a:p|dpPathPred.spPos
Table 1: Feature templates for both synPth and
linP th
as in Table 2 and FT
lin
holds 57 different tem-
plates as in Table 3. In these tables, the subscripts -
2(or -1) and 1(or 2) stand for the previous and next
words, respectively. For example, a.lm
?1
.lemma
returns the lemma of the previous word of the ar-
gument?s left most child.
4 Decoding
After the predicate sense is disambiguated, an op-
timal argument structure for each predicate is de-
termined by the following maximal probability.
S
p
= argmax
?
i
P (a
i
|a
i?1
, a
i?2
, ...), (1)
where S
p
is the argument structure, P (a
i
|a
i?1
...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. A beam
search algorithm is used to find the optimal argu-
ment structure.
5 Evaluation Results
Our evaluation is performed on the standard
training/development/test corpus of CoNLL-2008
shared task. The data is derived by merging a de-
pendency version of the Penn Treebank with Prop-
Bank and NomBank. More details on the data are
Algorithm 1 Greedy Feature Selection
Input:
The set of all feature templates: FT
The set of selected feature templates: S
0
Output:
The set of selected feature templates: S
Procedure:
Let the counter i = 1
Let S
i
= S
0
and C = FT ? S
i
while do
Train a model with features according to S
i
,
test on development set and the result is p
i
.
Let C
r
= null.
for each feature template f
j
in set S
i
do
Let S
?
= S
i
? f
j
.
Train a model with features according to
S
?
, test on development set and the result
is p
?
.
if p
?
> p
i
then
C
r
= C
r
+ f
j
.
end if
end for
C = C + C
r
S
i
= S
i
? C
r
Let S
?
i
= S
i
Train a model with features according to S
?
i
,
test on development set and the result is q
i
.
Let C
r
= null
for each feature template f
j
in set C do
Let C
?
= S
?
i
+ f
j
.
Train a model with features according to
C
?
, test on development set and the result
is p
?
.
if p
?
> q
i
then
C
r
= C
r
+ f
j
.
end if
end for
C = C ? C
r
S
?
i
= S
?
i
+ C
r
if S
i
= S
i?1
(No feature templates are added
or removed) or, neither p
i
nor q
i
is larger than
p
i?1
and q
i?1
then
Output S = argmax
p
i
,q
i
{S
i
, S
?
i
} and the
algorithm ends.
else
Let i = i+ 1, S
i
=S
i?1
and C = FT ? S
i
end if
end while
34
p?1
.lemma + p.lemma
p
?2
.pos
p.pos
p
?2
.spForm + p
?1
.spForm
p
1
.spForm
p.spForm + p.children.dprel.noDup
p.lm.spPos
p.spForm + p.lm.spPos
+ p.noFarChildren.spPos.bag + p.rm.spPos
p.dprel
p.children.dprel.bag
p.children.pos.seq
p.dprel = OBJ ?
a
a.dprel
a
?1
.lemma + a
1
.lemma
a
1
.lemma
a
?1
.pos
a
1
.spPos
a.h.lemma
a.h.spLemma
a.pphead.lemma
a.pphead.spLemma
a.lm.dprel + a.spPos
a.rm
?1
.pos
a.spLemma + a.h.spPos
a.existSemdprel A1
a.dprel = OBJ ?
a.form + a.children.pos.seq
a.children.adv.bag
b
a:p|linePath.distance
a:p|dpPath.distance
a:p|existCross
a:p|dpPath.dprel.bag
a:p|dpPathPred.dprel.bag
a:p|dpPath.spForm.seq
a:p|dpPathArgu.spForm.seq
a:p|dpPathPred.spForm.bag
a:p|dpPath.spLemma.seq
a:p|dpPathArgu.spLemma.seq
a:p|dpPathArgu.spLemma.bag
a:p|dpPathPred.spLemma.bag
a:p|dpPath.spPos.bag
a:p|dpPathPred.spPos.bag
(a:p|dpPath.dprel.seq) + p.spPos
(a:p|dpTreeRelation) + a.spPos
(a:p|dpTreeRelation) + p.spPos
(a.highSupportVerb:p|dpTreeRelation) + a.spPos
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
a:p|linePath.spForm.bag
a:p|linePath.spLemma.bag
a:p|linePath.spLemma.seq
a
This feature checks if the dependant type is OBJ.
b
adv means all adverbs.
Table 2: Feature templates only for synPth
p.currentSense + a.spLemma
p.currentSense + a.spPos
p.voice + (a:p|direction)
p.rm.dprel
p.children.dprel.noDup
p.rm.form
p.lowSupportNoun.spForm
p.lowSupportProp:p|dpTreeRelation
p
?2
.form + p
?1
.form
p.voice
p.form + p.children.dprel.noDup
p.pos + p.dprel
p.spForm + p.children.dprel.bag
a.voice + (a:p|direction)
a
?1
.isCurPred.lemma
a
1
.isCurPred.lemma
a
?1
.isCurPred.lemma + a.isCurPred.lemma
a.isCurPred.lemma + a
1
.isCurPred.lemma
a
1
.isCurPred.spLemma
a
?2
.isCurPred.spLemma + a
?1
.isCurPred.spLemma
a.baseline Ax + a.voice + (a:p|direction)
a.baseline Mod
a.h.children.dprel.bag
a.lm.dprel + a.dprel
a.lm.dprel + a.pos
a.lm
?1
.lemma
a.lm.lemma
a.lm
1
.lemma
a.lm.pos + a.pos
a.lm.spForm
a.lm
?1
.spPos
a.lm.spPos
a.ln.dprel + a.pos
a.noFarChildren.spPos.bag + a.rm.spPos
a.children.spPos.seq + p.children.spPos.seq
a.rm.dprel + a.pos
a.rm
?1
.spPos
a.rm.spPos
a.rm
1
.spPos
a.rn.dprel + a.spPos
a.form
a.form + a
1
.form
a.form + a.pos
a
?1
.lemma
a
?1
.lemma + a.lemma
a
?2
.pos
a.spForm + a
1
.spForm
a.spForm + a.spPos
a.spLemma + a
1
.spLemma
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
a.spLemma + a.h.spForm
a.spLemma + a.pphead.spForm
a.existSemdprel A2
a:p|dpPathArgu.pos.seq
a:p|dpPathPred.dprel.seq
a:p|dpTreeRelation
Table 3: Feature templates only for linPth
35
in (Surdeanu et al, 2008). Note that CoNLL-2008
shared task is essentially a joint learning task for
both syntactic and semantic dependencies, how-
ever, we will focus on semantic part of this task.
The main semantic measure that we adopt is se-
mantic labeled F
1
score (Sem-F
1
). In addition, the
macro labeled F
1
scores (Macro-F
1
), which was
used for the ranking of the participating systems of
CoNLL-2008, the ratio between labeled F
1
score
for semantic dependencies and the LAS for syn-
tactic dependencies (Sem-F
1
/LAS), are also given
for reference.
5.1 Syntactic Dependency Parsers
We consider three types of syntactic information
to feed the SRL task. One is gold-standard syn-
tactic input, and other two are based on automati-
cally parsing results of two parsers, the state-of-
the-art syntactic parser described in (Johansson
and Nugues, 2008)
7
(it is referred to Johansson)
and an integrated parser described as the follow-
ing (referred to MST
ME
).
The parser is basically based on the MSTParser
8
using all the features presented by (McDonald et
al., 2006) with projective parsing. Moreover, we
exploit three types of additional features to im-
prove the parser. 1) Chen et al (2008) used fea-
tures derived from short dependency pairs based
on large-scale auto-parsed data to enhance depen-
dency parsing. Here, the same features are used,
though all dependency pairs rather than short de-
pendency pairs are extracted along with the de-
pendency direction from training data rather than
auto-parsed data. 2) Koo et al (2008) presented
new features based on word clusters obtained from
large-scale unlabeled data and achieved large im-
provement for English and Czech. Here, the same
features are also used as word clusters are gen-
erated only from the training data. 3) Nivre and
McDonald (2008) presented an integrating method
to provide additional information for graph-based
and transition-based parsers. Here, we represent
features based on dependency relations predicted
by transition-based parsers for the MSTParer. For
the sake of efficiency, we use a fast transition-
7
It is a 2-order maximum spanning tree parser with
pseudo-projective techniques. A syntactic-semantic rerank-
ing was performed to output the final results according to (Jo-
hansson and Nugues, 2008). However, only 1-best outputs of
the parser before reranking are used for our evaluation. Note
that the reranking may slightly improve the syntactic perfor-
mance according to (Johansson and Nugues, 2008).
8
It?s freely available at http://mstparser.sourceforge.net.
Parser Path Adaptive Pruning Coverage
/wo /w Rate
Gold synPth 2.13M 1.05M 98.4%
(49.30%)
linP th 5.29M 1.57M 100.0%
(29.68%)
Johansson synPth 2.15M 1.06M 95.4%
(49.30%)
linP th 5.28M 1.57M 100.0%
(29.73%)
MST
ME
synPth 2.15M 1.06M 95.0%
(49.30%)
linP th 5.29M 1.57M 100.0%
(29.68%)
Table 4: The number of training samples on argu-
ment candidates
synPth+FT
syn
linPth+FT
lin
Syn-Parser LAS Sem Sem-F
1
Sem Sem-F
1
F
1
/LAS F
1
/LAS
MST
ME
88.39 80.53 91.10 79.83 90.31
Johansson 89.28 80.94 90.66 79.84 89.43
Gold 100.00 84.57 84.57 83.34 83.34
Table 5: Semantic Labeled F
1
based parser based on maximum entropy as in
Zhao and Kit (2008). We still use the similar fea-
ture notations of that work.
5.2 The Results
At first, we report the effectiveness of the proposed
adaptive argument pruning. The numbers of argu-
ment candidates are in Table 4. The statistics is
conducted on three different syntactic inputs. The
coverage rate in the table means the ratio of how
many true arguments are covered by the selected
pruning scheme. Note that the adaptive pruning
of argument candidates using assistant labels does
not change this rate. This ratio only depends on
which path, either synPth or linP th, is chosen,
and how good the syntactic input is (if synPth
is the case). From the results, we see that more
than a half of argument candidates can be effec-
tively pruned for synPth and even 2/3 for linP th.
As mentioned by (Pradhan et al, 2004), argument
identification plays a bottleneck role in improving
the performance of a SRL system. The effective-
ness of the proposed additional pruning techniques
may be seen as a significant improvement over the
original algorithm of (Xue and Palmer, 2004). The
results also indicate that such an assumption holds
that arguments trend to close with their predicate,
at either type of distance, syntactic or linear.
Based on different syntactic inputs, we obtain
different results on semantic dependency parsing
36
as shown in Table 5. These results on differ-
ent syntactic inputs also give us a chance to ob-
serve how semantic performance varies according
to syntactic performance. The fact from the re-
sults is that the ratio Sem-F
1
/LAS becomes rela-
tively smaller as the syntactic input becomes bet-
ter. Though not so surprised, the results do show
that the argument traverse scheme synPth always
outperforms the other linP th. The result of this
comparison partially shows that an integrated se-
mantic role labeler is sensitive to the order of how
argument candidates are traversed to some extent.
The performance given by synPth is com-
pared to some other systems that participated in
the CoNLL-2008 shared task. They were cho-
sen among the 20 participating systems either be-
cause they held better results (the first four partic-
ipants) or because they used some joint learning
techniques (Henderson et al, 2008). The results of
(Titov et al, 2009) that use the similar joint learn-
ing technique as (Henderson et al, 2008) are also
included
9
. Results of these evaluations on the test
set are in Table 6. Top three systems of CoNLL-
2008, (Johansson and Nugues, 2008; Ciaramita et
al., 2008; Che et al, 2008), used SRL pipelines.
In this work, we partially use the similar
techniques (synPth) for our participation in the
shared tasks of CoNLL-2008 and 2009 (Zhao and
Kit, 2008; Zhao et al, 2009b; Zhao et al, 2009a).
Here we report that all SRL sub-tasks are tackled
in one integrated model, while the predicate dis-
ambiguation sub-task was performed individually
in both of our previous systems. Therefore, this is
our first attempt at a full integrated SRL system.
(Titov et al, 2009) reported the best result by
using joint learning technique up to now. The
comparison indicates that our integrated system
outputs a result quite close to the state-of-the-art
by the pipeline system of (Johansson and Nugues,
2008) as the same syntactic structure input is
adopted. It is worth noting that our system actu-
ally competes with two independent sub-systems
of (Johansson and Nugues, 2008), one for verbal
predicates, the other for nominal predicates. In ad-
dition, the results of our system is obtained with-
out using additional joint learning technique like
syntactic-semantic reranking. It indicates that our
system is expected to obtain some further perfor-
mance improvement by using such techniques.
9
In addition, the work of (Henderson et al, 2008) and
(Titov et al, 2009) jointly considered syntactic and semantic
dependencies, that is significantly different from the others.
6 Conclusion
We have described a dependency-based semantic
role labeling system for English from NomBank
and PropBank. From the evaluations, the result of
our system is quite close to the state of the art. As
to our knowledge, it is the first integrated SRL sys-
tem that achieves such a competitive performance
against previous pipeline systems.
According to the path that the word-pair classi-
fier traverses argument candidates, two integration
schemes are presented. Argument candidate prun-
ing and feature selection are performed on them,
respectively. These two schemes are more than
providing a trivial comparison. As assistant la-
beled are introduced to help further argument can-
didate pruning, and this techniques work well for
both schemes, it support the assumption that argu-
ments trend to surround their predicate. The pro-
posed feature selection procedure also work for
both schemes and output quite different two fea-
ture template sets, and either of the sets helps the
system obtain a competitive performance, this fact
suggests that the feature selection procedure is ro-
bust and effective, too.
Either of the presented integrated systems can
provide a competitive performance. This conclu-
sion about basic learning scheme for SRL is some
different from previous literatures. However, ac-
cording to our results, there does exist a ?harmony?
feature template set that is helpful to both predi-
cate and argument identification/classification, or
SRL for both verbal and nominal predicates. We
attribute this different conclusion to two main fac-
tors, 1) much more feature templates (for example,
ten times more than those used by Xue et al) than
previous that are considered for a successful fea-
ture engineering, 2) a maximum entropy classifier
makes it possible to accept so many various fea-
tures in one model. Note that maximum entropy is
not so sensitive to those (partially) overlapped fea-
tures, while SVM and other margin-based learners
are not so.
Acknowledgements
Our thanks give to Dr. Richard Johansson, who
kindly provided the syntactic output for his partic-
ipation in the CoNLL-2008 shared task.
37
Systems
a
LAS Sem-F
1
Macro Sem-F
1
pred-F
1
b
argu-F
1
c
Verb-F
1
d
Nomi-F
1
e
F
1
/LAS
Johansson:2008*
f
89.32 81.65 85.49 91.41 87.22 79.04 84.78 77.12
Ours:Johansson 89.28 80.94 85.12 90.66 86.57 78.30 83.66 76.93
Ours:MST
ME
88.39 80.53 84.93 91.10 86.80 77.60 82.77 77.23
Johansson:2008 89.32 80.37 84.86 89.98 85.40 78.02 84.45 74.32
Ciaramita:2008* 87.37 78.00 82.69 89.28 83.46 75.35 80.93 73.80
Che:2008 86.75 78.52 82.66 90.51 85.31 75.27 80.46 75.18
Zhao:2008* 87.68 76.75 82.24 87.53 78.52 75.93 78.81 73.59
Ciaramita:2008 86.60 77.50 82.06 89.49 83.46 74.56 80.15 73.17
Titov:2009 87.50 76.10 81.80 86.97 ? ? ? ?
Zhao:2008 86.66 76.16 81.44 87.88 78.26 75.18 77.67 73.28
Henderson:2008* 87.64 73.09 80.48 83.40 81.42 69.10 75.84 68.90
Henderson:2008 86.91 70.97 79.11 81.66 79.60 66.83 73.80 66.26
Ours:Gold 100.0 84.57 92.20 84.57 87.67 83.15 88.71 78.39
a
Ranking according to Sem-F
1
b
Labeled F
1
for predicate identification and classification
c
Labeled F
1
for argument identification and classification
d
Labeled F
1
for verbal predicates
e
Labeled F
1
for nominal predicates
f
* means post-evaluation results, which are available at the official website of CoNLL-2008 shared task,
http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.
Table 6: Comparison of the best existing systems
References
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, pages 152?
164, Ann Arbor, Michigan, USA.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A
cascaded syntactic and semantic dependency pars-
ing system. In Proceedings of CoNLL-2008, pages
238?242, Manchester, England, August.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell?Orletta, and Mihai Surdeanu. 2008. Desrl: A
linear-time semantic role labeling system. In Pro-
ceedings of CoNLL-2008, pages 258?262, Manch-
ester, England, August.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL-2005, pages 42?49, Ann Arbor,
USA.
Weiwei Ding and Baobao Chang. 2008. Improving
chinese semantic role classification with hierarchi-
cal feature selection strategy. In Proceedings of
EMNLP-2008, pages 324?323, Honolulu, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, pages 1?
18, Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008, pages
178?182, Manchester, England, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138?145, Sydney, Australia.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with propbank and nombank. In Proceedings of
CoNLL-2008, page 183?187, Manchester, UK.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In Proceedings
of CoNLL-2005, pages 181?184, Ann Arbor, Michi-
gan, USA.
Chang Liu and Hwee Tou Ng. 2007. Learning pre-
dictive structures for semantic role labeling of nom-
bank. In Proceedings of ACL-2007, pages 208?215,
Prague, Czech.
38
Lluis Marquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A robust combination strat-
egy for semantic role labeling. In Proceedings
of HLT/EMNLP-2005, page 644?651, Vancouver,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, New York City, June.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24?31, Boston, Massachusetts, USA, May 6.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of HLT/NAACL-2004, pages 233?240,
Boston, Massachusetts, USA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role labeling using different syntactic views.
In Proceedings of ACL-2005, pages 581?588, Ann
Arbor, USA.
James Pustejovsky, Adam Meyers, Martha Palmer, and
Massimo Poesio. 2005. Merging propbank, nom-
bank, timebank, penn discourse treebank and coref-
erence. In Proceedings of the Workshop on Frontiers
in Corpus Annotations II: Pie in the Sky, pages 5?12,
Ann Arbor, USA.
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29:105?151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL-2008, pages 159?177, Manchester, UK.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisation
for synchronous parsing of semantic and syntactic
dependencies. In IJCAI-2009, Pasadena, California,
USA.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP-2004, pages 88?94, Barcelona, Spain,
July 25-26.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in chinese. In Proceedings of
NAACL-2006, pages 431?438, New York City, USA,
June.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203?207, Manchester, UK.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multilin-
gual dependency learning: Exploiting rich features
for tagging syntactic and semantic dependencies. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning (CoNLL-2009),
June 4-5, pages 61?66, Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009b. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
pages 55?60, Boulder, Colorado, USA.
39
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570?579,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Improving Dependency Parsing with Subtrees from Auto-Parsed Data
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, kazama, uchimoto, torisawa}@nict.go.jp
Abstract
This paper presents a simple and effective
approach to improve dependency parsing
by using subtrees from auto-parsed data.
First, we use a baseline parser to parse
large-scale unannotated data. Then we ex-
tract subtrees from dependency parse trees
in the auto-parsed data. Finally, we con-
struct new subtree-based features for pars-
ing algorithms. To demonstrate the ef-
fectiveness of our proposed approach, we
present the experimental results on the En-
glish Penn Treebank and the Chinese Penn
Treebank. These results show that our ap-
proach significantly outperforms baseline
systems. And, it achieves the best accu-
racy for the Chinese data and an accuracy
which is competitive with the best known
systems for the English data.
1 Introduction
Dependency parsing, which attempts to build de-
pendency links between words in a sentence, has
experienced a surge of interest in recent times,
owing to its usefulness in such applications as
machine translation (Nakazawa et al, 2006) and
question answering (Cui et al, 2005). To ob-
tain dependency parsers with high accuracy, super-
vised techniques require a large amount of hand-
annotated data. While hand-annotated data are
very expensive, large-scale unannotated data can
be obtained easily. Therefore, the use of large-
scale unannotated data in training is an attractive
idea to improve dependency parsing performance.
In this paper, we present an approach that ex-
tracts subtrees from dependency trees in auto-
parsed data to improve dependency parsing. The
auto-parsed data are generated from large-scale
unannotated data by using a baseline parser. Then,
from dependency trees in the data, we extract dif-
ferent types of subtrees. Finally, we represent
subtree-based features on training data to train de-
pendency parsers.
The use of auto-parsed data is not new. How-
ever, unlike most of the previous studies (Sagae
and Tsujii, 2007; Steedman et al, 2003) that im-
proved the performance by using entire trees from
auto-parsed data, we exploit partial information
(i.e., subtrees) in auto-parsed data. In their ap-
proaches, they used entire auto-parsed trees as
newly labeled data to train the parsing models,
while we use subtree-based features and employ
the original gold-standard data to train the mod-
els. The use of subtrees instead of complete trees
can be justified by the fact that the accuracy of par-
tial dependencies is much higher than that of en-
tire dependency trees. Previous studies (McDon-
ald and Pereira, 2006; Yamada and Matsumoto,
2003; Zhang and Clark, 2008) show that the accu-
racies of complete trees are about 40% for English
and about 35% for Chinese, while the accuracies
of relations between two words are much higher:
about 90% for English and about 85% for Chinese.
From these observations, we may conjecture that
it is possible to conduct a more effective selection
by using subtrees as the unit of information.
The use of word pairs in auto-parsed data was
tried in van Noord (2007) and Chen et al (2008).
However, the information on word pairs is limited.
To provide richer information, we consider more
words besides word pairs. Specifically, we use
subtrees containing two or three words extracted
from dependency trees in the auto-parsed data. To
demonstrate the effectiveness of our proposed ap-
proach, we present experimental results on En-
570
glish and Chinese data. We show that this sim-
ple approach greatly improves the accuracy and
that the use of richer structures (i.e, word triples)
indeed gives additional improvement. We also
demonstrate that our approach and other improve-
ment techniques (Koo et al, 2008; Nivre and Mc-
Donald, 2008) are complementary and that we can
achieve very high accuracies when we combine
our method with other improvement techniques.
Specifically, we achieve the best accuracy for the
Chinese data.
The rest of this paper is as follows: Section 2
introduces the background of dependency parsing.
Section 3 proposes an approach for extracting sub-
trees and represents the subtree-based features for
dependency parsers. Section 4 explains the ex-
perimental results and Section 5 discusses related
work. Finally, in section 6 we draw conclusions.
2 Dependency parsing
Dependency parsing assigns head-dependent rela-
tions between the words in a sentence. A sim-
ple example is shown in Figure 1, where an arc
between two words indicates a dependency rela-
tion between them. For example, the arc between
?ate? and ?fish? indicates that ?ate? is the head of
?fish? and ?fish? is the dependent. The arc be-
tween ?ROOT? and ?ate? indicates that ?ate? is the
ROOT of the sentence.
ROOT    I    ate    the    fish    with    a    fork    .
Figure 1: Example for dependency structure
2.1 Parsing approach
For dependency parsing, there ar two main
types of parsing models (Nivre and McDonald,
2008): graph-based model and transition-based
model, which achieved state-of-the-art accuracy
for a wide range of languages as shown in recent
CoNLL shared tasks (Buchholz et al, 2006; Nivre
et al, 2007). Our subtree-based features can be
applied in both of the two parsing models.
In this paper, as the base parsing system, we
employ the graph-based MST parsing model pro-
posed by McDonald et al (2005) and McDonald
and Pereira (2006), which uses the idea of Max-
imum Spanning Trees of a graph and large mar-
gin structured learning algorithms. The details
of parsing model were presented in McDonald et
al. (2005) and McDonald and Pereira (2006).
2.2 Baseline Parser
In the MST parsing model, there are two well-used
modes: the first-order and the second-order. The
first-order model uses first-order features that are
defined over single graph edges and the second-
order model adds second-order features that are
defined on adjacent edges.
For the parsing of unannotated data, we use the
first-order MST parsing model, because we need
to parse a large number of sentences and the parser
must be fast. We call this parser the Baseline
Parser.
3 Our approach
In this section, we describe our approach of ex-
tracting subtrees from unannotated data. First,
we preprocess unannotated data using the Baseline
Parser and obtain auto-parsed data. Subsequently,
we extract the subtrees from dependency trees in
the auto-parsed data. Finally, we generate subtree-
based features for the parsing models.
3.1 Subtrees extraction
To ease explanation, we transform the dependency
structure into a more tree-like structure as shown
in Figure 2, the sentence is the same as the one in
Figure 1.
ate
I                             fish      with                    .
the                                       fork
ROOT
a
I       ate      the      fish      with      a      fork .
Figure 2: Example for dependency structure in
tree-format
Our task is to extract subtrees from dependency
trees. If a subtree contains two nodes, we call it a
bigram-subtree. If a subtree contains three nodes,
we call it a trigram-subtree.
3.2 List of subtrees
We extract subtrees from dependency trees and
store them in list L
st
. First, we extract bigram-
subtrees that contain two words. If two words have
571
a dependency relation in a tree, we add these two
words as a subtree into list L
st
. Similarly, we can
extract trigram-subtrees. Note that the dependency
direction and the order of the words in the original
sentence are important in the extraction. To enable
this, the subtrees are encoded in the string format
that is expressed as st = w : wid : hid(?w :
wid : hid)+
1
, where w refers to a word in the
subtree, wid refers to the ID (starting from 1) of
a word in the subtree (words are ordered accord-
ing to the positions of the original sentence)
2
, and
hid refers to an ID of the head of the word (hid=0
means that this word is the root of a subtree). For
example, ?ate? and ?fish? have a right dependency
arc in the sentence shown in Figure 2. So the
subtree is encoded as ?ate:1:0-fish:2:1?. Figure 3
shows all the subtrees extracted from the sentence
in Figure 2, where the subtrees in (a) are bigram-
subtrees and the ones in (b) are trigram-subtrees.
ateI I:1:1-ate:2:0atefish ate:1:0-fish:2:1
atefish  with ate:1:0-fish:2:1-with:3:1
atewith ate:1:0-with:2:1
ate
.
ate:1:0-.:2:1
fishthe the:1:1-fish:2:0
with fork with:1:0-fork:2:1forka a:1:1-fork:2:0
ate
with   . ate:1:0-with:2:1-.:3:1(b)
(a)
Figure 3: Examples of subtrees
Note that we only used the trigram-subtrees
containing a head, its dependent d1, and d1?s
leftmost right sibling
3
. We could not consider
the case where two children are on different
sides
4
of the head (for instance, ?I? and ?fish?
for ?ate? in Figure 2). We also do not use the
child-parent-grandparent type (grandparent-type
in short) trigram-subtrees. These are due to the
limitations of the parsing algorithm of (McDonald
and Pereira, 2006), which does not allow the fea-
tures defined on those types of trigram-subtrees.
We extract the subtrees from the auto-parsed
data, then merge the same subtrees into one en-
try, and count their frequency. We eliminate all
subtrees that occur only once in the data.
1
+ refers to matching the preceding element one or more
times and is the same as a regular expression in Perl.
2
So, wid is in fact redundant but we include it for ease of
understanding.
3
Note that the order of the siblings is based on the order
of the words in the original sentence.
4
Here, ?side? means the position of a word relative to the
head in the original sentence.
3.3 Subtree-based features
We represent new features based on the extracted
subtrees and call them subtree-based features. The
features based on bigram-subtrees correspond to
the first-order features in the MST parsing model
and those based on trigram-subtrees features cor-
respond to the second-order features.
We first group the extracted subtrees into dif-
ferent sets based on their frequencies. After ex-
periments with many different threshold settings
on development data sets, we chose the follow-
ing way. We group the subtrees into three sets
corresponding to three levels of frequency: ?high-
frequency (HF)?, ?middle-frequency (MF)?, and
?low-frequency (LF)?. HF, MF, and LF are used
as set IDs for the three sets. The following are the
settings: if a subtree is one of the TOP-10% most
frequent subtrees, it is in set HF; else if a subtree is
one of the TOP-20% subtrees, it is in set MF; else
it is in set LF. Note that we compute these levels
within a set of subtrees with the same number of
nodes. We store the set ID for every subtree in
L
st
. For example, if subtree ?ate:1:0-with:2:1? is
among the TOP-10%, its set ID is HF.
3.3.1 First-order subtree-based features
The first-order features are based on bigram-
subtrees that are related to word pairs. We gener-
ate new features for a head h and a dependent d in
the parsing process. Figure 4-(a)
5
shows the words
and their surrounding words, where h
?1
refers to
the word to the left of the head in the sentence,
h
+1
refers to the word to the right of the head, d
?1
refers to the word to the left of the dependent, and
d
+1
refers to the word to the right of the depen-
dent. Temporary bigram-subtrees are formed by
word pairs that are linked by dashed-lines in the
figure. Then we retrieve these subtrees in L
st
to
get their set IDs (if a subtree is not included in
L
st
, its set ID is ZERO. That is, we have four sets:
HF, MF, LF, and ZERO.).
Then we generate first-order subtree-based fea-
tures, consisting of indicator functions for set IDs
of the retrieved bigram-subtrees. When generating
subtree-based features, each dashed line in Figure
4-(a) triggers a different feature.
To demonstrate how to generate first-order
subtree-based features, we use an example that is
as follows. Suppose that we are going to parse the
sentence ?He ate the cake with a fork.? as shown
5
Please note that d could be before h.
572
? h
-1 h      h+1 ? d-1     d      d+1  ?
(a)
(b)
? h      ? d1 ? d2 ?
Figure 4: Word pairs and triple for feature repre-
sentation
in Figure 5, where h is ?ate? and d is ?with?.
We can generate the features for the pairs linked
by dashed-lines, such as h ? d, h ? d
+1
and so
on. Then we have the temporary bigram-subtrees
?ate:1:0-with:2:1? for h ? d and ?ate:1:0-a:2:1?
for h ? d
+1
, and so on. If we can find subtree
?ate:1:0-with:2:1? for h ? d from L
st
with set ID
HF, we generate the feature ?H-D:HF?, and if we
find subtree ?ate:1:0-a:2:1? for h?d
+1
with set ID
ZERO, we generate the feature ?H-D+1:ZERO?.
The other three features are also generated simi-
larly.
He    ate    the    cake    with    a    fork    .
h
-1 h       h+1 d-1 d      d+1
Figure 5: First-order subtree-based features
3.3.2 Second-order subtree-based features
The second-order features are based on trigram-
subtrees that are related to triples of words. We
generate features for a triple of a head h, its de-
pendent d1, and d1?s right-leftmost sibling d2.
The triple is shown in Figure 4-(b). A temporary
trigram-subtree is formed by the word forms of h,
d1, and d2. Then we retrieve the subtree in L
st
to
get its set ID. In addition, we consider the triples
of ?h-NULL?
6
, d1, and d2, which means that we
only check the words of sibling nodes without
checking the head word.
Then, we generate second-order subtree-based
features, consisting of indicator functions for set
IDs of the retrieved trigram-subtrees.
6
h-NULL is a dummy token
We also generate combined features involving
the set IDs and part-of-speech tags of heads, and
the set IDs and word forms of heads. Specifically,
for any feature related to word form, we remove
this feature if the word is not one of the Top-N
most frequent words in the training data. We used
N=1000 for the experiments in this paper. This
method can reduce the size of the feature sets.
In this paper, we only used bigram-subtrees and
the limited form of trigram-subtrees, though in
theory we can use k-gram-subtrees, which are lim-
ited in the same way as our trigram subtrees, in
(k-1)th-order MST parsing models mentioned in
McDonald and Pereira (2006) or use grandparent-
type trigram-subtrees in parsing models of Car-
reras (2007). Although the higher-order MST
parsing models will be slow with exact inference,
requiring O(n
k
) time (McDonald and Pereira,
2006), it might be possible to use higher-order k-
gram subtrees with approximated parsing model
in the future. Of course, our method can also be
easily extended to the labeled dependency case.
4 Experiments
In order to evaluate the effectiveness of the
subtree-based features, we conducted experiments
on English data and Chinese Data.
For English, we used the Penn Treebank (Mar-
cus et al, 1993) in our experiments and the tool
?Penn2Malt?
7
to convert the data into dependency
structures using a standard set of head rules (Ya-
mada and Matsumoto, 2003). To match previ-
ous work (McDonald et al, 2005; McDonald and
Pereira, 2006; Koo et al, 2008), we split the data
into a training set (sections 2-21), a development
set (Section 22), and a test set (section 23). Fol-
lowing the work of Koo et al (2008), we used
the MXPOST (Ratnaparkhi, 1996) tagger trained
on training data to provide part-of-speech tags for
the development and the test set, and we used 10-
way jackknifing to generate tags for the training
set. For the unannotated data, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ text.
8
We used the MX-
POST tagger trained on training data to assign
part-of-speech tags and used the Basic Parser to
process the sentences of the BLLIP corpus.
For Chinese, we used the Chinese Treebank
7
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
8
We ensured that the text used for extracting subtrees did
not include the sentences of the Penn Treebank.
573
(CTB) version 4.0
9
in the experiments. We also
used the ?Penn2Malt? tool to convert the data and
created a data split: files 1-270 and files 400-931
for training, files 271-300 for testing, and files
301-325 for development. We used gold standard
segmentation and part-of-speech tags in the CTB.
The data partition and part-of-speech settings were
chosen to match previous work (Chen et al, 2008;
Yu et al, 2008). For the unannotated data, we
used the PFR corpus
10
, which has approximately
15 million words whose segmentation and POS
tags are given. We used its original segmentation
though there are differences in segmentation pol-
icy between CTB and this corpus. As for POS
tags, we discarded the original POS tags and as-
signed CTB style POS tags using a TNT-based
tagger (Brants, 2000) trained on the training data.
We used the Basic Parser to process all the sen-
tences of the PFR corpus.
We measured the parser quality by the unla-
beled attachment score (UAS), i.e., the percentage
of tokens (excluding all punctuation tokens) with
the correct HEAD. And we also evaluated on com-
plete dependency analysis.
4.1 Experimental Results
In our experiments, we used MSTParser, a
freely available implementation
11
of the first- and
second-order MST parsing models. For baseline
systems, we used the first- and second-order basic
features, which were the same as the features used
by McDonald and Pereira (2006), and we used
the default settings of MSTParser throughout the
paper: iters=10; training-k=1; decode-type=proj.
We implemented our systems based on the MST-
Parser by incorporating the subtree-based features.
4.1.1 Main results of English data
English
UAS Complete
Ord1 90.95 37.45
Ord1s 91.76(+0.81) 40.68
Ord2 91.71 42.88
Ord2s 92.51(+0.80) 46.19
Ord2b 92.28(+0.57) 45.44
Ord2t 92.06(+0.35) 42.96
Table 1: Dependency parsing results for English
9
http://www.cis.upenn.edu/?chinese/.
10
http://www.icl.pku.edu.
11
http://mstparser.sourceforge.net
The results are shown in Table 1, where
Ord1/Ord2 refers to a first-/second-order
MSTParser with basic features, Ord1s/Ord2s
refers to a first-/second-order MSTParser with
basic+subtree-based features, and the improve-
ments by the subtree-based features over the basic
features are shown in parentheses. Note that
we use both the bigram- and trigram- subtrees
in Ord2s. The parsers using the subtree-based
features consistently outperformed those using
the basic features. For the first-order parser,
we found that there is an absolute improvement
of 0.81 points (UAS) by adding subtree-based
features. For the second-order parser, we got an
absolute improvement of 0.8 points (UAS) by
including subtree-based features. The improve-
ments of parsing with subtree-based features were
significant in McNemar?s Test (p < 10
?6
).
We also checked the sole effect of bigram- and
trigram-subtrees. The results are also shown in
Table 1, where Ord2b/Ord2t refers to a second-
order MSTParser with bigram-/trigram-subtrees
only. The results showed that trigram-subtrees can
provide further improvement, although the effect
of the bigram-subtrees seemed larger.
4.1.2 Comparative results of English data
Table 2 shows the performance of the systems
that were compared, where Y&M2003 refers to
the parser of Yamada and Matsumoto (2003),
CO2006 refers to the parser of Corston-Oliver et
al. (2006), Hall2006 refers to the parser of Hall
et al (2006), Wang2007 refers to the parser of
Wang et al (2007), Z&C 2008 refers to the combi-
nation graph-based and transition-based system of
Zhang and Clark (2008), KOO08-dep1c/KOO08-
dep2c refers to a graph-based system with first-
/second-order cluster-based features by Koo et al
(2008), and Carreras2008 refers to the paper of
Carreras et al (2008). The results showed that
Ord2s performed better than the first five systems.
The second-order system of Koo et al (2008) per-
formed better than our systems. The reason may
be that the MSTParser only uses sibling interac-
tions for second-order, while Koo et al (2008)
uses both sibling and grandparent interactions, and
uses cluster-based features. Carreras et al (2008)
reported a very high accuracy using information of
constituent structure of the TAG grammar formal-
ism. In our systems, we did not use such knowl-
edge.
Our subtree-based features could be combined
574
with the techniques presented in other work,
such as the cluster-based features in Koo et al
(2008), the integrating methods of Zhang and
Clark (2008), and Nivre and McDonald (2008),
and the parsing methods of Carreras et al (2008).
English
UAS Complete
Y&M2003 90.3 38.4
CO2006 90.8 37.6
Hall2006 89.4 36.4
Wang2007 89.2 34.4
Z&C2008 92.1 45.4
KOO08-dep1c 92.23 ?
KOO08-dep2c 93.16 ?
Carreras2008 93.5 ?
Ord1 90.95 37.45
Ord1s 91.76 40.68
Ord1c 91.88 40.71
Ord1i 91.68 41.43
Ord1sc 92.20 42.98
Ord1sci 92.60 44.28
Ord2 91.71 42.88
Ord2s 92.51 46.19
Ord2c 92.40 44.08
Ord2i 92.12 44.37
Ord2sc 92.70 46.56
Ord2sci 93.16 47.15
Table 2: Dependency parsing results for English,
for our parsers and previous work
To demonstrate that our approach and other
work are complementary, we thus implemented
a system using all the techniques we had at hand
that used subtree- and cluster-based features
and applied the integrating method of Nivre and
McDonald (2008). We used the word clustering
tool
12
, which was used by Koo et al (2008), to
produce word clusters on the BLLIP corpus. The
cluster-based features were the same as the fea-
tures used by Koo et al (2008). For the integrating
method, we used the transition MaxEnt-based
parser of Zhao and Kit (2008) because it was
faster than the MaltParser. The results are shown
in the bottom part of Table 2, where Ord1c/Ord2c
refers to a first-/second-order MSTParser with
cluster-based features, Ord1i/Ordli refers to a first-
/second-order MSTParser with integrating-based
features, Ord1sc/Ord2sc refers to a first-/second-
order MSTParser with subtree-based+cluster-
based features, and Ord1sci/Ord2sci refers to
a first-/second-order MSTParser with subtree-
based+cluster-based+integrating-based features.
Ord1c/Ord2c was worse than KOO08-dep1c/-
dep2c, but Ord1sci outperformed KOO08-dep1c
12
http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
and Ord2sci performed similarly to KOO08-dep2c
by using all of the techniques we had. These
results indicated that subtree-based features can
provide different information and work well with
other techniques.
4.1.3 Main results of Chinese data
The results are shown in Table 3 where abbrevia-
tions are the same as in Table 1. As in the English
experiments, parsers with the subtree-based fea-
tures outperformed parsers with the basic features,
and second-order parsers outperformed first-order
parsers. For the first-order parser, the subtree-
based features provided 1.3 absolute points im-
provement. For the second-order parser, the
subtree-based features achieved an absolute im-
provement of 1.25 points. The improvements of
parsing with subtree-based features were signifi-
cant in McNemar?s Test (p < 10
?5
).
Chinese
UAS Complete
Ord1 86.38 40.80
Ord1s 87.68(+1.30) 42.24
Ord2 88.18 47.12
Ord2s 89.43(+1.25) 47.53
Ord2b 89.16(+0.98) 47.12
Ord2t 88.55(+0.37) 47.12
Table 3: Dependency parsing results for Chinese.
4.1.4 Comparative results of Chinese data
Table 4 shows the comparative results, where
Wang2007 refers to the parser of Wang et
al. (2007), Chen2008 refers to the parser of Chen
et al (2008), and Yu2008 refers to the parser of
Yu et al (2008) that is the best reported results
for this data set. And ?all words? refers to all the
sentences in test set and ?? 40 words?
13
refers to
the sentences with the length up to 40. The table
shows that our parsers outperformed previous sys-
tems.
We also implemented integrating systems for
Chinese data as well. When we applied the
cluster-based features, the performance dropped a
little. The reason may be that we are using gold-
POS tags for Chinese data
14
. Thus we did not
13
Wang et al (2007) and Chen et al (2008) reported the
scores on these sentences.
14
We tried to use the cluster-based features for Chinese
with the same setting of POS tags as English data, then the
cluster-based features did provide improvement.
575
use cluster-based features for the integrating sys-
tems. The results are shown in Table 4, where
Ord1si/Ord2si refers to the first-order/second-
order system with subtree-based+intergrating-
based features. We found that the integrating sys-
tems provided better results. Overall, we have
achieved a high accuracy, which is the best known
result for this dataset.
Zhang and Clark (2008) and Duan et al (2007)
reported results on a different data split of Penn
Chinese Treebank. We also ran our systems
(Ord2s) on their data and provided UAS 86.70
(for non-root words)/77.39 (for root words), better
than their results: 86.21/76.26 in Zhang and Clark
(2008) and 84.36/73.70 in Duan et al (2007).
Chinese
all words ? 40 words
UAS Complete UAS Complete
Wang2007 ? ? 86.6 28.4
Chen2008 86.52 ? 88.4 ?
Yu2008 87.26 ? ? ?
Ord1s 87.68 42.24 91.11 54.40
Ord1si 88.24 43.96 91.32 55.93
Ord2s 89.43 47.53 91.67 59.77
Ord2si 89.91 48.56 92.34 62.83
Table 4: Dependency parsing results for Chinese,
for our parsers and for previous work
4.1.5 Effect of different sizes of unannotated
data
Here, we consider the improvement relative to the
sizes of the unannotated data. Figure 6 shows the
results of first-order parsers with different num-
bers of words in the unannotated data. Please note
that the size of full English unannotated data is
43M and the size of full Chinese unannotated data
is 15M. From the figure, we found that the parser
obtained more benefits as we added more unanno-
tated data.
 86
 87
 88
 89
 90
 91
 92
4332168420
U
A
S
Size of unannotated data(M)
EnglishChinese
Figure 6: Results with different sizes of large-
scale unannotated data.
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  1  2  3  4  5  6
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of unknown words
BetterNoChangeWorse
Figure 7: Improvement relative to unknown words
for English
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1  2  3  4  5  6
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of unknown words
BetterNoChangeWorse
Figure 8: Improvement relative to unknown words
for Chinese
4.2 Additional Analysis
In this section, we investigated the results on
sentence level from different views. For Fig-
ures 7-12, we classified each sentence into one of
three classes: ?Better? for those where the pro-
posed parsers provided better results relative to
the parsers with basic features, ?Worse? for those
where the proposed parsers provided worse results
relative to the basic parsers, and ?NoChange? for
those where the accuracies remained the same.
4.2.1 Unknown words
Here, we consider the unknown word
15
problem,
which is an important issue for parsing. We cal-
culated the number of unknown words in one sen-
tence, and listed the changes of the sentences with
unknown words. Here, we compared the Ord1
system and the Ord1s system.
Figures 7 and 8 show the results, where the x
axis refers to the number of unknown words in one
sentence and the y axis shows the percentages of
the three classes. For example, for the sentences
having three unknown words in the Chinese data,
31.58% improved, 23.68% worsened, and 44.74%
were unchanged. We did not show the results of
15
An unknown word is a word that is not included in the
training data.
576
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
43210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of CCs
BetterNoChangeWorse
Figure 9: Improvement relative to number of
conjunctions for English
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
3210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of CCs
BetterNoChangeWorse
Figure 10: Improvement relative to number of
conjunctions for Chinese
the sentences with more than six unknown words
because their numbers were very small. The Bet-
ter and Worse curves showed that our approach al-
ways provided better results. The results indicated
that the improvements apparently became larger
when the sentences had more unknown words for
the Chinese data. And for the English data, the
graph also showed the similar trend, although the
improvements for the sentences have three and
four unknown words were slightly less than the
others.
4.2.2 Coordinating conjunctions
We analyzed our new parsers? behavior for coordi-
nating conjunction structures, which is a very dif-
ficult problem for parsing (Kawahara and Kuro-
hashi, 2008). Here, we compared the Ord2 system
with the Ord2s system.
Figures 9 and 10 show how the subtree-based
features affect accuracy as a function of the num-
ber of conjunctions, where the x axis refers to the
number of conjunctions in one sentence and the
y axis shows the percentages of the three classes.
The figures indicated that the subtree-based fea-
tures improved the coordinating conjunction prob-
lem. In the trigram-subtree list, many subtrees
are related to coordinating conjunctions, such as
?utilities:1:3 and:2:3 businesses:3:0? and ?pull:1:0
and:2:1 protect:3:1?. These subtrees can provide
additional information for parsing models.
4.2.3 PP attachment
We analyzed our new parsers? behavior for
preposition-phrase attachment, which is also a dif-
ficult task for parsing (Ratnaparkhi et al, 1994).
We compared the Ord2 system with the Ord2s sys-
tem. Figures 11 and 12 show how the subtree-
based features affect accuracy as a function of the
number of prepositions, where the x axis refers to
the number of prepositions in one sentence and the
y axis shows the percentages of the three classes.
The figures indicated that the subtree-based fea-
tures improved preposition-phrase attachment.
5 Related work
Our approach is to incorporate unannotated data
into parsing models for dependency parsing. Sev-
eral previous studies relevant to our approach have
been conducted.
Chen et al (2008) previously proposed an ap-
proach that used the information on short de-
pendency relations for Chinese dependency pars-
ing. They only used the word pairs within two
word distances for a transition-based parsing al-
gorithm. The approach in this paper differs in
that we use richer information on trigram-subtrees
besides bigram-subtrees that contain word pairs.
And our work is focused on graph-based parsing
models as opposed to transition-based models. Yu
et al (2008) constructed case structures from auto-
parsed data and utilized them in parsing. Com-
pared with their method, our method is much sim-
pler but has great effects.
Koo et al (2008) used the Brown algorithm to
produce word clusters on large-scale unannotated
data and represented new features based on the
clusters for parsing models. The cluster-based fea-
tures provided very impressive results. In addition,
they used the parsing model by Carreras (2007)
that applied second-order features on both sibling
and grandparent interactions. Note that our ap-
proach and their approach are complementary in
that we can use both subtree- and cluster-based
features for parsing models. The experimental re-
sults showed that we achieved better accuracy for
first-order models when we used both of these two
types of features.
Sagae and Tsujii (2007) presented an co-
training approach for dependency parsing adap-
577
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  1  2  3  4  5  6  7
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of prepositions
BetterNoChangeWorse
Figure 11: Improvement relative to number of
prepositions for English
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
3210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of prepositions
BetterNoChangeWorse
Figure 12: Improvement relative to number of
prepositions for Chinese
tation. They used two parsers to parse the sen-
tences in unannotated data and selected only iden-
tical results produced by the two parsers. Then,
they retrained a parser on newly parsed sentences
and the original labeled data. Our approach repre-
sents subtree-based features on the original gold-
standard data to retrain parsers. McClosky et
al. (2006) presented a self-training approach for
phrase structure parsing and the approach was
shown to be effective in practice. However,
their approach depends on a high-quality reranker,
while we simply augment the features of an ex-
isting parser. Moreover, we could use the output
of our systems for co-training/self-training tech-
niques.
6 Conclusions
We present a simple and effective approach to
improve dependency parsing using subtrees from
auto-parsed data. In our method, first we use a
baseline parser to parse large-scale unannotated
data, and then we extract subtrees from depen-
dency parsing trees in the auto-parsed data. Fi-
nally, we construct new subtree-based features for
parsing models. The results show that our ap-
proach significantly outperforms baseline systems.
We also show that our approach and other tech-
niques are complementary, and then achieve the
best reported accuracy for the Chinese data and an
accuracy that is competitive with the best known
systems for the English data.
References
T. Brants. 2000. TnT?a statistical part-of-speech tag-
ger. Proceedings of ANLP, pages 224?231.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. Proceedings of CoNLL-X.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL 2008, pages 9?16, Manchester, Eng-
land, August. Coling 2008 Organizing Committee.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
E. Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and
M. Johnson. 2000. BLLIP 1987-89 WSJ Corpus
Release 1, LDC2000T43. Linguistic Data Consor-
tium.
WL. Chen, D. Kawahara, K. Uchimoto, YJ. Zhang, and
H. Isahara. 2008. Dependency parsing with short
dependency relations in unlabeled data. In Proceed-
ings of IJCNLP 2008.
S. Corston-Oliver, A. Aue, Kevin. Duh, and Eric Ring-
ger. 2006. Multilingual dependency parsing using
bayes point machines. In HLT-NAACL2006.
H. Cui, RX. Sun, KY. Li, MY. Kan, and TS. Chua.
2005. Question answering passage retrieval us-
ing dependency relations. In Proceedings of SIGIR
2005, pages 400?407, New York, NY, USA. ACM.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, War-
saw, Poland.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative classifiers for deterministic depen-
dency parsing. In In Proceedings of CoLING-ACL.
D. Kawahara and S. Kurohashi. 2008. Coordination
disambiguation without any similarities. In Pro-
ceedings of Coling 2008, pages 425?432, Manch-
ester, UK, August.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
ticss, 19(2):313?330.
578
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL 2005.
T. Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper nlp. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase at-
tachment. In Proceedings of HLT, pages 250?255.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser en-
sembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050.
M. Steedman, M. Osborne, A. Sarkar, S. Clark,
R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and
J. Crim. 2003. Bootstrapping statistical parsers
from small datasets. In Proceedings of EACL 2003,
pages 331?338.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of IWPT-07, June.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of IJCAI2007.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automat-
ically constructed case structures. In Proceedings
of Coling 2008, pages 1049?1056, Manchester, UK,
August.
Y. Zhang and S. Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562?571, Hon-
olulu, Hawaii, October.
H. Zhao and CY. Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceedings of CoNLL
2008, pages 203?207, Manchester, England, Au-
gust.
579
Dependency Parsing with Short Dependency Relations in Unlabeled Data
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, dk, uchimoto, yujie, isahara}@nict.go.jp
Abstract
This paper presents an effective dependency
parsing approach of incorporating short de-
pendency information from unlabeled data.
The unlabeled data is automatically parsed
by a deterministic dependency parser, which
can provide relatively high performance for
short dependencies between words. We then
train another parser which uses the informa-
tion on short dependency relations extracted
from the output of the first parser. Our pro-
posed approach achieves an unlabeled at-
tachment score of 86.52, an absolute 1.24%
improvement over the baseline system on
the data set of Chinese Treebank.
1 Introduction
In dependency parsing, we attempt to build the
dependency links between words from a sen-
tence. Given sufficient labeled data, there are sev-
eral supervised learning methods for training high-
performance dependency parsers(Nivre et al, 2007).
However, current statistical dependency parsers pro-
vide worse results if the dependency length be-
comes longer (McDonald and Nivre, 2007). Here
the length of a dependency from word w
i
and word
w
j
is simply equal to |i ? j|. Figure 1 shows the
F
1
score1 provided by a deterministic parser rela-
tive to dependency length on our testing data. From
1precision represents the percentage of predicted arcs of
length d that are correct and recall measures the percentage of
gold standard arcs of length d that are correctly predicted.
F
1
= 2? precision? recall/(precision + recall)
the figure, we find that F
1
score decreases when de-
pendency length increases as (McDonald and Nivre,
2007) found. We also notice that the parser pro-
vides good results for short dependencies (94.57%
for dependency length = 1 and 89.40% for depen-
dency length = 2). In this paper, short dependency
refers to the dependencies whose length is 1 or 2.
 30
 40
 50
 60
 70
 80
 90
 100
 0  5  10  15  20  25  30
F1
Dependency Length
baseline
Figure 1: F-score relative to dependency length
Labeled data is expensive, while unlabeled data
can be obtained easily. In this paper, we present an
approach of incorporating unlabeled data for depen-
dency parsing. First, all the sentences in unlabeled
data are parsed by a dependency parser, which can
provide state-of-the-art performance. We then ex-
tract information on short dependency relations from
the parsed data, because the performance for short
dependencies is relatively higher than others. Fi-
nally, we train another parser by using the informa-
tion as features.
The proposed method can be regarded as a semi-
supervised learning method. Currently, most semi-
88
supervised methods seem to do well with artificially
restricted labeled data, but they are unable to outper-
form the best supervised baseline when more labeled
data is added. In our experiments, we show that our
approach significantly outperforms a state-of-the-art
parser, which is trained on full labeled data.
2 Motivation and previous work
The goal in dependency parsing is to tag dependency
links that show the head-modifier relations between
words. A simple example is in Figure 2, where the
link between a and bird denotes that a is the depen-
dent of the head bird.
I    see    a    beautiful    bird    .
Figure 2: Example dependency graph.
We define that word distance of word w
i
and word
w
j
is equal to |i ? j|. Usually, the two words in a
head-dependent relation in one sentence can be adja-
cent words (word distance = 1) or neighboring words
(word distance = 2) in other sentences. For exam-
ple, ?a? and ?bird? has head-dependent relation in
the sentence at Figure 2. They can also be adjacent
words in the sentence ?I see a bird.?.
Suppose that our task is Chinese dependency
parsing. Here, the string ????JJ(Specialist-
level)/? ?NN(working)/? ?NN(discussion)?
should be tagged as the solution (a) in Figure
3. However, our current parser may choose the
solution (b) in Figure 3 without any additional
information. The point is how to assign the head for
????(Specialist-level)?. Is it ???(working)?
or ???(discussion)??
  
  
  
(b)
(a)
Figure 3: Two solutions for ????(Specialist-
level)/??(working)/??(discussion)?
As Figure 1 suggests, the current dependency
parser is good at tagging the relation between ad-
jacent words. Thus, we expect that dependencies
of adjacent words can provide useful information
for parsing words, whose word distances are longer.
When we search the string ????(Specialist-
level)/??(discussion)? at google.com, many rele-
vant documents can be retrieved. If we have a good
parser, we may assign the relations between the two
words in the retrieved documents as Figure 4 shows.
We can find that ???(discussion)? is the head of
????(Specialist-level)? in many cases.
1)?525	26
///,//?
2)?Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97?104,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Empirical Study of Chinese Chunking
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
In this paper, we describe an empirical
study of Chinese chunking on a corpus,
which is extracted from UPENN Chinese
Treebank-4 (CTB4). First, we compare
the performance of the state-of-the-art ma-
chine learning models. Then we propose
two approaches in order to improve the
performance of Chinese chunking. 1) We
propose an approach to resolve the spe-
cial problems of Chinese chunking. This
approach extends the chunk tags for ev-
ery problem by a tag-extension function.
2) We propose two novel voting meth-
ods based on the characteristics of chunk-
ing task. Compared with traditional vot-
ing methods, the proposed voting methods
consider long distance information. The
experimental results show that the SVMs
model outperforms the other models and
that our proposed approaches can improve
performance significantly.
1 Introduction
Chunking identifies the non-recursive cores of
various types of phrases in text, possibly as a
precursor to full parsing or information extrac-
tion. Steven P. Abney was the first person
to introduce chunks for parsing(Abney, 1991).
Ramshaw and Marcus(Ramshaw and Marcus,
1995) first represented base noun phrase recog-
nition as a machine learning problem. In 2000,
CoNLL-2000 introduced a shared task to tag
many kinds of phrases besides noun phrases in
English(Sang and Buchholz, 2000). Addition-
ally, many machine learning approaches, such as
Support Vector Machines (SVMs)(Vapnik, 1995),
Conditional Random Fields (CRFs)(Lafferty et
al., 2001), Memory-based Learning (MBL)(Park
and Zhang, 2003), Transformation-based Learn-
ing (TBL)(Brill, 1995), and Hidden Markov Mod-
els (HMMs)(Zhou et al, 2000), have been applied
to text chunking(Sang and Buchholz, 2000; Ham-
merton et al, 2002).
Chinese chunking is a difficult task, and much
work has been done on this topic(Li et al, 2003a;
Tan et al, 2005; Wu et al, 2005; Zhao et al,
2000). However, there are many different Chinese
chunk definitions, which are derived from differ-
ent data sets(Li et al, 2004; Zhang and Zhou,
2002). Therefore, comparing the performance of
previous studies in Chinese chunking is very dif-
ficult. Furthermore, compared with the other lan-
guages, there are some special problems for Chi-
nese chunking(Li et al, 2004).
In this paper, we extracted the chunking corpus
from UPENN Chinese Treebank-4(CTB4). We
presented an empirical study of Chinese chunk-
ing on this corpus. First, we made an evaluation
on the corpus to clarify the performance of state-
of-the-art models in Chinese chunking. Then we
proposed two approaches in order to improve the
performance of Chinese chunking. 1) We pro-
posed an approach to resolve the special prob-
lems of Chinese chunking. This approach ex-
tended the chunk tags for every problem by a tag-
extension function. 2) We proposed two novel vot-
ing methods based on the characteristics of chunk-
ing task. Compared with traditional voting meth-
ods, the proposed voting methods considered long
distance information. The experimental results
showed the proposed approaches can improve the
performance of Chinese chunking significantly.
The rest of this paper is as follows: Section 2
describes the definitions of Chinese chunks. Sec-
97
tion 3 simply introduces the models and features
for Chinese chunking. Section 4 proposes a tag-
extension method. Section 5 proposes two new
voting approaches. Section 6 explains the exper-
imental results. Finally, in section 7 we draw the
conclusions.
2 Definitions of Chinese Chunks
We defined the Chinese chunks based on the CTB4
dataset1. Many researchers have extracted the
chunks from different versions of CTB(Tan et al,
2005; Li et al, 2003b). However, these studies did
not provide sufficient detail. We developed a tool2
to extract the corpus from CTB4 by modifying the
tool Chunklink3.
2.1 Chunk Types
Here we define 12 types of chunks4: ADJP, ADVP,
CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP,
VP(Xue et al, 2000). Table 1 provides definitions
of these chunks.
Type Definition
ADJP Adjective Phrase
ADVP Adverbial Phrase
CLP Classifier Phrase
DNP DEG Phrase
DP Determiner Phrase
DVP DEV phrase
LCP Localizer Phrase
LST List Marker
NP Noun Phrase
PP Prepositional Phrase
QP Quantifier Phrase
VP Verb Phrase
Table 1: Definition of Chunks
2.2 Data Representation
To represent the chunks clearly, we represent the
data with an IOB-based model as the CoNLL00
shared task did, in which every word is to be
tagged with a chunk type label extended with I
(inside a chunk), O (outside a chunk), and B (in-
side a chunk, but also the first word of the chunk).
1More detailed information at
http://www.cis.upenn.edu/ chinese/.
2Tool is available at
http://www.nlplab.cn/chenwl/tools/chunklinkctb.txt.
3Tool is available at http://ilk.uvt.nl/software.html#chunklink.
4There are 15 types in the Upenn Chinese TreeBank. The
other chunk types are FRAG, PRN, and UCP.
Each chunk type could be extended with I or B
tags. For instance, NP could be represented as
two types of tags, B-NP or I-NP. Therefore, we
have 25 types of chunk tags based on the IOB-
based model. Every word in a sentence will be
tagged with one of these chunk tags. For in-
stance, the sentence (word segmented and Part-of-
Speech tagged) ??-NR(He) /??-VV(reached)
/??-NR(Beijing) /??-NN(airport) /?/? will
be tagged as follows:
Example 1:
S1: [NP?][VP??][NP??/??][O?]
S2: ?B-NP /??B-VP /??B-NP /??I-NP /?O /
Here S1 denotes that the sentence is tagged with
chunk types, and S2 denotes that the sentence is
tagged with chunk tags based on the IOB-based
model.
With data representation, the problem of Chi-
nese chunking can be regarded as a sequence tag-
ging task. That is to say, given a sequence of
tokens (words pairing with Part-of-Speech tags),
x = x1, x2, ..., xn, we need to generate a sequence
of chunk tags, y = y1, y2, ..., yn.
2.3 Data Set
CTB4 dataset consists of 838 files. In the ex-
periments, we used the first 728 files (FID from
chtb 001.fid to chtb 899.fid) as training data, and
the other 110 files (FID from chtb 900.fid to
chtb 1078.fid) as testing data. In the following
sections, we use the CTB4 Corpus to refer to the
extracted data set. Table 2 lists details on the
CTB4 Corpus data used in this study.
Training Test
Num of Files 728 110
Num of Sentences 9,878 5,290
Num of Words 238,906 165,862
Num of Phrases 141,426 101,449
Table 2: Information of the CTB4 Corpus
3 Chinese Chunking
3.1 Models for Chinese Chunking
In this paper, we applied four models, includ-
ing SVMs, CRFs, TBL, and MBL, which have
achieved good performance in other languages.
We only describe these models briefly since full
details are presented elsewhere(Kudo and Mat-
sumoto, 2001; Sha and Pereira, 2003; Ramshaw
and Marcus, 1995; Sang, 2002).
98
3.1.1 SVMs
Support Vector Machines (SVMs) is a pow-
erful supervised learning paradigm based on the
Structured Risk Minimization principle from com-
putational learning theory(Vapnik, 1995). Kudo
and Matsumoto(Kudo and Matsumoto, 2000) ap-
plied SVMs to English chunking and achieved
the best performance in the CoNLL00 shared
task(Sang and Buchholz, 2000). They created 231
SVMs classifiers to predict the unique pairs of
chunk tags.The final decision was given by their
weighted voting. Then the label sequence was
chosen using a dynamic programming algorithm.
Tan et al (Tan et al, 2004) applied SVMs to
Chinese chunking. They used sigmoid functions
to extract probabilities from SVMs outputs as the
post-processing of classification. In this paper, we
used Yamcha (V0.33)5 in our experiments.
3.1.2 CRFs
Conditional Random Fields is a powerful se-
quence labeling model(Lafferty et al, 2001) that
combine the advantages of both the generative
model and the classification model. Sha and
Pereira(Sha and Pereira, 2003) showed that state-
of-the-art results can be achieved using CRFs in
English chunking. CRFs allow us to utilize a large
number of observation features as well as differ-
ent state sequence based features and other fea-
tures we want to add. Tan et al (Tan et al, 2005)
applied CRFs to Chinese chunking and their ex-
perimental results showed that the CRFs approach
provided better performance than HMM. In this
paper, we used MALLET (V0.3.2)6(McCallum,
2002) to implement the CRF model.
3.1.3 TBL
Transformation based learning(TBL), first in-
troduced by Eric Brill(Brill, 1995), is mainly
based on the idea of successively transforming the
data in order to correct the error. The transforma-
tion rules obtained are usually few , yet power-
ful. TBL was applied to Chinese chunking by Li
et al(Li et al, 2004) and TBL provided good per-
formance on their corpus. In this paper, we used
fnTBL (V1.0)7 to implement the TBL model.
5Yamcha is available at
http://chasen.org/ taku/software/yamcha/
6MALLET is available at
http://mallet.cs.umass.edu/index.php/Main Page
7fnTBL is available at
http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html
3.1.4 MBL
Memory-based Learning (also called instance
based learning) is a non-parametric inductive
learning paradigm that stores training instances in
a memory structure on which predictions of new
instances are based(Walter et al, 1999). The simi-
larity between the new instance X and example Y
in memory is computed using a distance metric.
Tjong Kim Sang(Sang, 2002) applied memory-
based learning(MBL) to English chunking. MBL
performs well for a variety of shallow parsing
tasks, often yielding good results. In this paper,
we used TiMBL8(Daelemans et al, 2004) to im-
plement the MBL model.
3.2 Features
The observations are based on features that are
able to represent the difference between the two
events. We utilize both lexical and Part-Of-
Speech(POS) information as the features.
We use the lexical and POS information within
a fixed window. We also consider different combi-
nations of them. The features are listed as follows:
? WORD: uni-gram and bi-grams of words in
an n window.
? POS: uni-gram and bi-grams of POS in an n
window.
? WORD+POS: Both the features of WORD
and POS.
where n is a predefined number to denote window
size.
For instance, the WORD features at the 3rd
position (??-NR) in Example 1 (set n as 2):
?? L2 ?? L1 ?? 0 ?? R1 ? R2?(uni-
gram) and ?? ?? LB1?? ?? B0?? ?
? RB1 ?? ? RB2?(bi-gram). Thus features
of WORD have 9 items(5 from uni-gram and
4 from bi-grams). In the similar way, fea-
tures of POS also have 9 items and features of
WORD+POS have 18 items(9+9).
4 Tag-Extension
In Chinese chunking, there are some difficult prob-
lems, which are related to Special Terms, Noun-
Noun Compounds, Named Entities Tagging and
Coordination. In this section, we propose an ap-
proach to resolve these problems by extending the
chunk tags.
8TiMBL is available at http://ilk.uvt.nl/timbl/
99
In the current data representation, the chunk
tags are too generic to construct accurate models.
Therefore, we define a tag-extension function fs
in order to extend the chunk tags as follows:
Te = fs(T,Q) = T ?Q (1)
where, T denotes the original tag set, Q denotes
the problem set, and Te denotes the extended tag
set. For instance, we have an q problem(q ? Q).
Then we extend the chunk tags with q. For NP
Recognition, we have two new tags: B-NP-q and
I-NP-q. Here we name this approach as Tag-
Extension.
In the following three cases study, we demon-
strate that how to use Tag-Extension to resolve the
difficult problems in NP Recognition.
1) Special Terms: this kind of noun phrases
is special terms such as ??/ ??(Life)/ ?
?(Forbidden Zone)/ ?/?, which are bracketed
with the punctuation ??, ?, ?, ?, ?, ??.
They are divided into two types: chunks with these
punctuation and chunks without these punctua-
tion. For instance, ??/ ??/ ??/ ?/? is an
NP chunk (?B-NP/ ??I-NP/ ??I-NP/ ?I-
NP/) while ??/??(forever)/ ??(full-blown)/
?(DE)/???(Chinese Redbud)/?/? is tagged
as (?O/ ??O /??O/ ?O/ ???B-NP/
?O/). We extend the tags with SPE for Special
Terms: B-NP-SPE and I-NP-SPE.
2) Coordination: These problems are related
to the conjunctions ??(and), ?(and), ?(or),
?(and)?. They can be divided into two types:
chunks with conjunctions and chunks without
conjunctions. For instance, ???(HongKong)/
?(and)/??(Macau)/? is an NP chunk (??B-
NP/ ?I-NP/ ??I-NP/), while in ???(least)/
??(salary)/ ?(and)/ ???(living mainte-
nance)/? it is difficult to tell whether ???? is a
shared modifier or not, even for people. We extend
the tags with COO for Coordination: B-NP-COO
and I-NP-COO.
3) Named Entities Tagging: Named Enti-
ties(NE)(Sang and Meulder, 2003) are not dis-
tinguished in CTB4, and they are all tagged as
?NR?. However, they play different roles in
chunks, especial in noun phrases. For instance,
???-NR(Macau)/ ??-NN(Airport)? and ??
?-NR(Hong Kong)/??-NN(Airport)? vs ???
?-NR(Deng Xiaoping)/ ??-NN(Mr.)? and ??
??-NR(Song Weiping) ??-NN(President)?.
Here ???? and ???? are LOCATION, while
????? and ????? are PERSON. To investi-
gate the effect of Named Entities, we use a LOCA-
TION dictionary, which is generated from the PFR
corpus9 of ICL, Peking University, to tag location
words in the CTB4 Corpus. Then we extend the
tags with LOC for this problem: B-NP-LOC and
I-NP-LOC.
From the above cases study, we know the steps
of Tag-Extension. Firstly, identifying a special
problem of chunking. Secondly, extending the
chunk tags via Equation (1). Finally, replacing the
tags of related tokens with new chunk tags. After
Tag-Extension, we use new added chunk tags to
describe some special problems.
5 Voting Methods
Kudo and Matsumoto(Kudo and Matsumoto,
2001) reported that they achieved higher accuracy
by applying voting of systems that were trained
using different data representations. Tjong Kim
Sang et al(Sang and Buchholz, 2000) reported
similar results by combining different systems.
In order to provide better results, we also ap-
ply the voting of basic systems, including SVMs,
CRFs, MBL and TBL. Depending on the charac-
teristics in the chunking task, we propose two new
voting methods. In these two voting methods, we
consider long distance information.
In the weighted voting method, we can assign
different weights to the results of the individ-
ual system(van Halteren et al, 1998). However,
it requires a larger amount of computational ca-
pacity as the training data is divided and is re-
peatedly used to obtain the voting weights. In
this paper, we give the same weight to all ba-
sic systems in our voting methods. Suppose, we
have K basic systems, the input sentence is x =
x1, x2, ..., xn, and the results of K basic systems
are tj = t1j , t2j , ..., tnj , 1 ? j ? K. Then our
goal is to gain a new result y = y1, y2, ..., yn by
voting.
5.1 Basic Voting
This is traditional voting method, which is the
same as Uniform Weight in (Kudo and Mat-
sumoto, 2001). Here we name it as Basic Voting.
For each position, we have K candidates from K
basic systems. After voting, we choose the candi-
date with the most votes as the final result for each
position.
9More information at http://www.icl.pku.edu
100
5.2 Sent-based Voting
In this paper, we treat chunking as a sequence la-
beling task. Here we apply this idea in computing
the votes of one sentence instead of one word. We
name it as Sent-based Voting. For one sentence,
we have K candidates, which are the tagged se-
quences produced by K basic systems. First, we
vote on each position, as done in Basic Voting.
Then we compute the votes of every candidate by
accumulating the votes of each position. Finally,
we choose the candidate with the most votes as
the final result for the sentence. That is to say, we
make a decision based on the votes of the whole
sentence instead of each position.
5.3 Phrase-based Voting
In chunking, one phrase includes one or more
words, and the word tags in one phrase depend on
each other. Therefore, we propose a novel vot-
ing method based on phrases, and we compute the
votes of one phrase instead of one word or one sen-
tence. Here we name it as Phrase-based Voting.
There are two steps in the Phrase-based Voting
procedure. First, we segment one sentence into
pieces. Then we calculate the votes of the pieces.
Table 3 is the algorithm of Phrase-based Voting,
where F (tij , tik) is a binary function:
F (tij , tik) =
{
1 : tij = tik
0 : tij 6= tik (2)
In the segmenting step, we seek the ?O? or ?B-
XP? (XP can be replaced by any type of phrase)
tags, in the results of basic systems. Then we get a
new piece if all K results have the ?O? or ?B-XP?
tags at the same position.
In the voting step, the goal is to choose a result
for each piece. For each piece, we have K candi-
dates. First, we vote on each position within the
piece, as done in Basic Voting. Then we accumu-
late the votes of each position for every candidate.
Finally, we pick the one, which has the most votes,
as the final result for the piece.
The difference in these three voting methods is
that we make the decisions in different ranges: Ba-
sic Voting is at one word; Phrase-based Voting is
in one piece; and Sent-based Voting is in one sen-
tence.
6 Experiments
In this section, we investigated the performance of
Chinese chunking on the CTB4 Corpus.
Input:
Sequence: x = x1, ..., xn;
K results: tj = t1j , ..., tnj , 1 ? j ? K.
Output:
Voted results: y = y1, y2, ..., yn
Segmenting: Segment the sentence into pieces.
Pieces[]=null; begin = 1
For each i in (2, n){
For each j in (1,K)
if(tij is not ?O? and ?B-XP?) break;
if(j > K){
add new piece: p = xbegin, ..., xi?1 into Pieces;
begin = i; }}
Voting: Choose the result with the most votes for each
piece: p = xbegin, ..., xend.
Votes[K] = 0;
For each k in (1,K)
V otes[k] =
?
begin?i?end,1?j?K
F (tij , tik) (3)
kmax = argmax1?k?K(V otes[k]);
Choose tbegin,kmax , ..., tend,kmax as the result for
piece p.
Table 3: Algorithm of Phrase-based Voting
6.1 Experimental Setting
To investigate the chunker sensitivity to the size
of the training set, we generated different sizes of
training sets, including 1%, 2%, 5%, 10%, 20%,
50%, and 100% of the total training data.
In our experiments, we used all the default pa-
rameter settings of the packages. Our SVMs and
CRFs chunkers have a first-order Markov depen-
dency between chunk tags.
We evaluated the results as CONLL2000 share-
task did. The performance of the algorithm was
measured with two scores: precision P and recall
R. Precision measures how many chunks found by
the algorithm are correct and the recall rate con-
tains the percentage of chunks defined in the cor-
pus that were found by the chunking program. The
two rates can be combined in one measure:
F1 = 2? P ?RR+ P (4)
In this paper, we report the results with F1 score.
6.2 Experimental Results
6.2.1 POS vs. WORD+POS
In this experiment, we compared the perfor-
mance of different feature representations, in-
101
 70
 75
 80
 85
 90
 95
 0.01  0.02  0.05  0.1  0.2  0.5  1
F
1
Size of Training data
SVM_WPSVM_PCRF_WPCRF_P
Figure 1: Results of different features
cluding POS and WORD+ POS(See section 3.2),
and set the window size as 2. We also inves-
tigated the effects of different sizes of training
data. The SVMs and CRFs approaches were used
in the experiments because they provided good
performance in chunking(Kudo and Matsumoto,
2001)(Sha and Pereira, 2003).
Figure 1 shows the experimental results, where
xtics denotes the size of the training data, ?WP?
refers to WORD+POS, ?P? refers to POS. We can
see from the figure that WORD+POS yielded bet-
ter performance than POS in the most cases. How-
ever, when the size of training data was small,
the performance was similar. With WORD+POS,
SVMs provided higher accuracy than CRFs in
all training sizes. However, with POS, CRFs
yielded better performance than SVMs in large
scale training sizes. Furthermore, we found SVMs
with WORD+POS provided 4.07% higher accu-
racy than with POS, while CRFs provided 2.73%
higher accuracy.
6.2.2 Comparison of Models
In this experiment, we compared the perfor-
mance of the models, including SVMs, CRFs,
MBL, and TBL, in Chinese chunking. In the ex-
periments, we used the feature WORD+POS and
set the window size as 2 for the first two mod-
els. For MBL, WORD features were within a one-
window size, and POS features were within a two-
window size. We used the original data for TBL
without any reformatting.
Table 4 shows the comparative results of the
models. We found that the SVMs approach was
superior to the other ones. It yielded results that
were 0.72%, 1.51%, and 3.58% higher accuracy
than respective CRFs, TBL, and MBL approaches.
SVMs CRFs TBL MBL
ADJP 84.45 84.55 85.95 80.48
ADVP 83.12 82.74 81.98 77.95
CLP 5.26 0.00 0.00 3.70
DNP 99.65 99.64 99.65 99.61
DP 99.70 99.40 99.70 99.46
DVP 96.77 92.89 99.61 99.41
LCP 99.85 99.85 99.74 99.82
LST 68.75 68.25 56.72 64.75
NP 90.54 89.79 89.82 87.90
PP 99.67 99.66 99.67 99.59
QP 96.73 96.53 96.60 96.40
VP 89.74 88.50 85.75 82.51
+ 91.46 90.74 89.95 87.88
Table 4: Comparative Results of Models
Method Precision Recall F1
CRFs 91.47 90.01 90.74
SVMs 92.03 90.91 91.46
V1 91.97 90.66 91.31
V2 92.32 90.93 91.62
V3 92.40 90.97 91.68
Table 5: Voting Results
Giving more details for each category, the SVMs
approach provided the best results in ten cate-
gories, the CRFs in one category, and the TBL in
five categories.
6.2.3 Comparison of Voting Methods
In this section, we compared the performance of
the voting methods of four basic systems, which
were used in Section 6.2.2. Table 5 shows the
results of the voting systems, where V1 refers
to Basic Voting, V2 refers to Sent-based Voting,
and V3 refers to Phrase-based Voting. We found
that Basic Voting provided slightly worse results
than SVMs. However, by applying the Sent-
based Voting method, we achieved higher accu-
racy than any single system. Furthermore, we
were able to achieve more higher accuracy by ap-
plying Phrase-based Voting. Phrase-based Voting
provided 0.22% and 0.94% higher accuracy than
respective SVMs, CRFs approaches, the best two
single systems.
The results suggested that the Phrase-based Vot-
ing method is quite suitable for chunking task. The
Phrase-based Voting method considers one chunk
as a voting unit instead of one word or one sen-
tence.
102
SVMs CRFs TBL MBL V3
NPR 90.62 89.72 89.89 87.77 90.92
COO 90.61 89.78 90.05 87.80 91.03
SPE 90.65 90.14 90.31 87.77 91.00
LOC 90.53 89.83 89.69 87.78 90.86
NPR* - - - - 91.13
Table 6: Results of Tag-Extension in NP Recogni-
tion
6.2.4 Tag-Extension
NP is the most important phrase in Chinese
chunking and about 47% phrases in the CTB4 Cor-
pus are NPs. In this experiment, we presented the
results of Tag-Extension in NP Recognition.
Table 6 shows the experimental results of Tag-
Extension, where ?NPR? refers to chunking with-
out any extension, ?SPE? refers to chunking
with Special Terms Tag-Extension, ?COO? refers
to chunking with Coordination Tag-Extension,
?LOC? refers to chunking with LOCATION Tag-
Extension, ?NPR*? refers to voting of eight sys-
tems(four of SPE and four of COO), and ?V3?
refers to Phrase-based Voting method.
For NP Recognition, SVMs also yielded the
best results. But it was surprised that TBL pro-
vided 0.17% higher accuracy than CRFs. By ap-
plying Phrase-based Voting, we achieved better re-
sults, 0.30% higher accuracy than SVMs.
From the table, we can see that the Tag-
Extension approach can provide better results. In
COO, TBL got the most improvement with 0.16%.
And in SPE, TBL and CRFs got the same improve-
ment with 0.42%. We also found that Phrase-
based Voting can improve the performance signif-
icantly. NPR* provided 0.51% higher than SVMs,
the best single system.
For LOC, the voting method helped to improve
the performance, provided at least 0.33% higher
accuracy than any single system. But we also
found that CRFs and MBL provided better results
while SVMs and TBL yielded worse results. The
reason was that our NE tagging method was very
simple. We believe NE tagging can be effective
in Chinese chunking, if we use a highly accurate
Named Entity Recognition system.
7 Conclusions
In this paper, we conducted an empirical study of
Chinese chunking. We compared the performance
of four models, SVMs, CRFs, MBL, and TBL.
We also investigated the effects of using different
sizes of training data. In order to provide higher
accuracy, we proposed two new voting methods
according to the characteristics of the chunking
task. We proposed the Tag-Extension approach to
resolve the special problems of Chinese chunking
by extending the chunk tags.
The experimental results showed that the SVMs
model was superior to the other three models.
We also found that part-of-speech tags played an
important role in Chinese chunking because the
gap of the performance between WORD+POS and
POS was very small.
We found that the proposed voting approaches
can provide higher accuracy than any single sys-
tem can. In particular, the Phrase-based Voting ap-
proach is more suitable for chunking task than the
other two voting approaches. Our experimental
results also indicated that the Tag-Extension ap-
proach can improve the performance significantly.
References
Steven P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257?278. Kluwer,
Dordrecht.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2004. Timbl: Tilburg
memory-based learner v5.1.
James Hammerton, Miles Osborne, Susan Armstrong,
and Walter Daelemans. 2002. Introduction to spe-
cial issue on machine learning approaches to shallow
parsing. JMLR, 2(3):551?558.
Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In In
Proceedings of CoNLL-2000 and LLL-2000, pages
142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
103
Heng Li, Jonathan J. Webster, Chunyu Kit, and Tian-
shun Yao. 2003a. Transductive hmm based chi-
nese text chunking. In Proceedings of IEEE NLP-
KE2003, pages 257?262, Beijing, China.
Sujian Li, Qun Liu, and Zhifeng Yang. 2003b. Chunk-
ing parsing with maximum entropy principle (in chi-
nese). Chinese Journal of Computers, 26(12):1722?
1727.
Hongqiao Li, Changning Huang, Jianfeng Gao, and Xi-
aozhong Fan. 2004. Chinese chunking with another
type of spec. In The Third SIGHAN Workshop on
Chinese Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Seong-Bae Park and Byoung-Tak Zhang. 2003.
Text chunking by combining hand-crafted rules and
memory-based learning. In ACL, pages 497?504.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
David Yarovsky and Kenneth Church, editors, Pro-
ceedings of the Third Workshop on Very Large Cor-
pora, pages 82?94, Somerset, New Jersey. Associa-
tion for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL2000,
pages 127?132, Lisbin, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. JMLR, 2(3):559?594.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2004. Chinese chunk identification using svms
plus sigmoid. In IJCNLP, pages 527?536.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2005. Applying conditional random fields
to chinese shallow parsing. In Proceedings of
CICLing-2005, pages 167?176, Mexico City, Mex-
ico. Springer.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 1998. Improving data driven wordclass tag-
ging by system combination. In COLING-ACL,
pages 491?497.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
Daelemans Walter, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing.
Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu,
Tzong-Han Tsai, and Wen-Lian Hsu. 2005. Ap-
plying maximum entropy to robust chinese shallow
parsing. In Proceedings of ROCLING2005.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony
Kroch. 2000. The bracketing guidelines for the
penn chinese treebank. Technical report, University
of Pennsylvania.
Yuqi Zhang and Qiang Zhou. 2002. Chinese base-
phrases chunking. In Proceedings of The First
SIGHAN Workshop on Chinese Language Process-
ing.
Tiejun Zhao, Muyun Yang, Fang Liu, Jianmin Yao, and
Hao Yu. 2000. Statistics based hybrid approach to
chinese base phrase identification. In Proceedings
of Second Chinese Language Processing Workshop.
GuoDong Zhou, Jian Su, and TongGuan Tey. 2000.
Hybrid text chunking. In Claire Cardie, Walter
Daelemans, Claire Ne?dellec, and Erik Tjong Kim
Sang, editors, Proceedings of the CoNLL00, Lis-
bon, 2000, pages 163?165. Association for Compu-
tational Linguistics, Somerset, New Jersey.
104
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 118?121,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with Conditional Random Fields
Wenliang Chen and Yujie Zhang and Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
We present a Chinese Named Entity
Recognition (NER) system submitted to
the close track of Sighan Bakeoff2006.
We define some additional features via do-
ing statistics in training corpus. Our sys-
tem incorporates basic features and addi-
tional features based on Conditional Ran-
dom Fields (CRFs). In order to correct in-
consistently results, we perform the post-
processing procedure according to n-best
results given by the CRFs model. Our fi-
nal system achieved a F-score of 85.14 at
MSRA, 89.03 at CityU, and 76.27 at LDC.
1 Introduction
Named Entity Recognition task in the 2006 Sighan
Bakeoff includes three corpora: Microsoft Re-
search (MSRA), City University of Hong Kong
(CityU), and Linguistic Data Consortium (LDC).
There are four types of Named Entities in the cor-
pora: Person Name, Organization Name, Location
Name, and Geopolitical Entity (only included in
LDC corpus).
We attend the close track of all three cor-
pora. In the close track, we can not use any
external resources. Thus except basic features,
we define some additional features by applying
statistics in training corpus to replace external re-
sources. Firstly, we perform word segmentation
using a simple left-to-right maximum matching al-
gorithm, in which we use a word dictionary gen-
erated by doing n-gram statistics. Then we de-
fine the features based on word boundaries. Sec-
ondly, we generate several lists according to the
relative position to Named Entity (NE). We de-
fine another type of features based on these lists.
Using these features, we build a Conditional Ran-
dom Fields(CRFs)-based Named Entity Recogni-
tion (NER) System. We use the system to generate
n-best results for every sentence, and then perform
a post-processing.
2 Conditional Random Fields
2.1 The model
Conditional Random Fields(CRFs), a statistical
sequence modeling framework, was first intro-
duced by Lafferty et alLafferty et al, 2001).
The model has been used for chunking(Sha and
Pereira, 2003). We only describe the model
briefly since full details are presented in the pa-
per(Lafferty et al, 2001).
In this paper, we regard Chinese NER as a se-
quence labeling problem. For our sequence label-
ing problem, we create a linear-chain CRFs based
on an undirected graph G = (V,E), where V is
the set of random variables Y = {Yi|1 ? i ? n},
for each of n tokens in an input sentence and
E = {(Yi?1, Yi)|1 ? i ? n} is the set of n ? 1
edges forming a linear chain. For each sentence x,
we define two non-negative factors:
exp(?Kk=1 ?kfk(yi?1, yi, x)) for each edge
exp(?K?k=1 ?
?
kf
?
k(yi, x)) for each node
where fk is a binary feature function, and K and
K ? are the number of features defined for edges
and nodes respectively. Following Lafferty et
al(Lafferty et al, 2001), the conditional probabil-
ity of a sequence of tags y given a sequence of
tokens x is:
P (y|x) = 1Z(x)exp(
?
i,k
?kfk(yi?1, yi, x) +
?
i,k
??kf
?
k(yi, x))
(1)
where Z(x) is the normalization constant. Given
the training data D, a set of sentences (characters
118
Tag Meaning
0 (zero) Not part of a named entity
PER A person name
ORG An organization name
LOC A location name
GPE A geopolitical entity
Table 1: Named Entities in the Data
with their corresponding tags), the parameters of
the model are trained to maximize the conditional
log-likelihood. When testing, given a sentence x
in the test data, the tagging sequence y is given by
Argmaxy?P (y?|x).
CRFs allow us to utilize a large number of ob-
servation features as well as different state se-
quence based features and other features we want
to add.
2.2 CRFs for Chinese NER
Our CRFs-based system has a first-order Markov
dependency between NER tags.
In our experiments, we do not use feature selec-
tion and all features are used in training and test-
ing. We use the following feature functions:
f(yi?1, yi, x, i) = p(x, i)q(yi?1, yi) (2)
where p(x, i) is a predicate on the input sequence
x and current position i and q(yi?1, yi) is a predi-
cate on pairs of labels. For instance, p(x, i) might
be ?the char at position i is?(and)?.
In our system, we used CRF++ (V0.42)1 to im-
plement the CRFs model.
3 Chinese Named Entity Recognition
The training data format is similar to that of the
CoNLL NER task 2002, adapted for Chinese. The
data is presented in two-column format, where the
first column consists of the character and the sec-
ond is a tag.
Table 1 shows the types of Named Entities in the
data. Every character is to be tagged with a NE
type label extended with B (Beginning character
of a NE) and I (Non-beginning character of a NE),
or 0 (Not part of a NE).
To obtain a good-quality estimation of the con-
ditional probability of the event tag, the observa-
tions should be based on features that represent the
difference of the two events. In our system, we de-
fine three types of features for the CRFs model.
1CRF++ is available at
http://chasen.org/ taku/software/CRF++/
3.1 Basic Features
The basic features of our system list as follows:
? . Cn(n = ?2,?1, 0, 1, 2)
? . CnCn+1(n = ?1, 0)
Where C refers to a Chinese character while C0
denotes the current character and Cn(C?n) de-
notes the character n positions to the right (left)
of the current character.
For example, given a character sequence ???
????, when considering the character C0 de-
notes ???, C?1 denotes ???, C?1C0 denotes ??
??, and so on.
3.2 Word Boundary Features
The sentences in training data are based on char-
acters. However, there are many features related to
the words. For instance, the word ???? can be a
important feature for Person Name. We perform
word segmentation using the left-to-right maxi-
mum matching algorithm, in which we use a word
dictionary generated by doing n-gram statistics in
training corpus. Then we use the word boundary
tags as the features for the model.
Firstly, we construct a word dictionary by ex-
tracting N-grams from training corpus as follows:
1. Extract arbitrary N-grams (2 ? n ? 10,
Frequency ? 10 ) from training corpus. We
get a list W1.
2. Use a tool to perform statistical substring
reduction in W1[ described in (Lv et al,
2004)]2. We get a list W2.
3. Construct a character list (CH)3, in which the
characters are top 20 frequency in training
corpus.
4. Remove the strings from W2, which contain
the characters in the list CH. We get final N-
grams list W3.
Secondly, we use W3 as a dictionary for left-
to-right maximum matching word segmentation.
We assign word boundary tags to sentences. Each
character can be assigned one of 4 possible bound-
ary tags: ?B? for a character that begins a word
and is followed by another character, ?M? for a
2Tools are available at
http://homepages.inf.ed.ac.uk/s0450736/Software
3To collect some characters such as punctuation, ???,
??? and so on.
119
character that occurs in the middle of a word, ?E?
for a character that ends a word, and ?S? for a char-
acter that occurs as a single-character word.
The word boundary features of our system list
as follows:
? . WTn(n = ?1, 0, 1)
Where WT refers to the word boundary tag while
WT0 denotes the tag of current character and
WTn(WT?n) denotes the tag n positions to the
right (left) of the current character.
3.3 Char Features
If we can use external resources, we often use the
lists of surname, suffix of named entity and prefix
of named entity for Chinese NER. In our system,
we generate these lists automatically from training
corpus by the procedure as follows:
? PSur: uni-gram characters, first characters of Person
Name. (surname)
? PC: uni-gram characters in Person Name.
? PPre: bi-gram characters before Person Name. (prefix
of Person Name)
? PSuf: bi-gram characters after Person Name. (suffix of
Person Name)
? LC: uni-gram characters in Location Name or Geopo-
litical entity.
? LSuf: uni-gram characters, the last characters of Loca-
tion Name or Geopolitical Entity. (suffix of Location
Name or Geopolitical Entity)
? OC: uni-gram characters in Organization Name.
? OSuf: uni-gram characters, the last characters of Orga-
nization Name. (suffix of Organization Name)
? OBSuf: bi-gram characters, the last two characters of
Organization Name. (suffix of Organization Name)
We remove the items in uni-gram lists if their fre-
quencies are less than 5 and in bi-gram lists if
their frequencies are less than 2. Based on these
lists, we assign the tags to every character. For in-
stance, if a character is included in PSur list, then
we assign a tag ?PSur 1?, otherwise assign a tag
?PSur 0?. Then we define the char features as fol-
lows:
? . PSur0PC0;
? . PSurnPCnPSurn+1PCn+1(n = ?1, 0);
? . PPre0;
? . PSuf0;
? . LC0OC0;
S is the list of sentences, S = {s1, s2, ..., sn}.
T is m-best results of S, T = {t1, t2, ..., tn}, which ti
is a set of m-best results of si.
pij is the score of tij , that is the jth result in ti.
Collect NE list:
Loop i in [1, n]
if(pi0 ? 0.5){
Exacting all NEs from ti0 to add into NEList.}
Replacing:
Loop i in [1, n]
if(pi0 ? 0.5){
FinalResult(si) = ti0.}
else{
TmpResult = ti0.
Loop j in [m, 1]
if(the NEs in tij is included in NEList){
Replace the matching string in TmpResult with new
NE tags.}
FinalResult(si) = TmpResult.
}
Table 2: The algorithm of Post-processing
? . LCnOCnLCn+1OCn+1(n = ?1, 0);
? . LSuf0OSuf0;
? . LSufnOSufnLSufn+1OSufn+1(n = ?1, 0);
4 Post-Processing
There are inconsistently results, which are tagged
by the CRFs model. Thus we perform a post-
processing step to correct these errors.
The post-processing tries to assign the correct
tags according to n-best results for every sentence.
Our system outputs top 20 labeled sequences for
each sentence with the confident scores. The post-
processing algorithm is shown at Table 2. Firstly,
we collect NE list from high confident results.
Secondly, we re-assign the tags for low confident
results using the NE list.
5 Evaluation Results
5.1 Results on Sighan bakeoff 2006
We evaluated our system in the close track, on
all three corpora, namely Microsoft Research
(MSRA), City University of Hong Kong (CityU),
and Linguistic Data Consortium (LDC). Our offi-
cial Bakeoff results are shown at Table 3, where
the columns P, R, and FB1 show precision, recall
and F measure(? = 1). We used all three types of
features in our final system.
In order to evaluate the contribution of fea-
tures, we conducted the experiments of each type
of features using the test sets with gold-standard
dataset. Table 4 shows the experimental results,
120
MSRA P R FB1
LOC 92.81 88.53 90.62
ORG 81.93 81.07 81.50
PER 85.41 74.15 79.38
Overall 88.14 82.34 85.14
CityU P R FB1
LOC 92.21 92.00 92.11
ORG 87.83 74.23 80.46
PER 92.77 89.05 90.87
Overall 91.43 86.76 89.03
LDC P R FB1
GPE 83.78 80.36 82.04
LOC 51.11 21.70 30.46
ORG 71.79 60.82 65.85
PER 82.40 75.58 78.84
Overall 80.26 72.65 76.27
Table 3: Our official Bakeoff results
MSRA CityU LDC
F1 84.73 88.26 76.18
+F2 88.67 76.30
+F3 88.74
Post 85.23 89.03 76.66
Table 4: Results of different combinations
where F1 refers to use basic features, F2 refers to
use the word boundary features, F3 refers to use
the char features, and Post refers to perform the
post-processing.
The results indicated that word boundary fea-
tures helped on LDC and CityU, char features only
helped on CityU and the post-processing always
helped to improve the performance.
6 Conclusion
This paper presented our Named Entity Recogni-
tion system for the close track of Bakeoff2006.
Our approach was based on Conditional Random
Fields model. Except basic features, we defined
the additional features by doing statistics in train-
ing corpus. In addition, we performed a post-
processing according to n-best results generated
by the CRFs model. The evaluation results showed
that our system achieved state-of-the-art perfor-
mance on all three corpora in the close track.
References
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
Xueqiang Lv, Le Zhang, and Junfeng Hu. 2004. Statis-
tical substring reduction in linear time. In Proceed-
ings of IJCNLP-04, HaiNan island, P.R.China.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
121
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 55?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
A Huge Feature Engineering Method to Semantic Dependency Parsing ?
Hai Zhao(??)??, Wenliang Chen(???)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual semantic dependency parsing (SR-
Lonly) for our participation in the shared task
of CoNLL-2009. We illustrate that semantic
dependency parsing can be transformed into
a word-pair classification problem and im-
plemented as a single-stage machine learning
system. For each input corpus, a large scale
feature engineering is conducted to select the
best fit feature template set incorporated with a
proper argument pruning strategy. The system
achieved the top average score in the closed
challenge: 80.47% semantic labeled F1 for the
average score.
1 Introduction
The syntactic and semantic dependency parsing in
multiple languages introduced by the shared task
of CoNLL-2009 is an extension of the CoNLL-
2008 shared task (Hajic? et al, 2009). Seven lan-
guages, English plus Catalan, Chinese, Czech, Ger-
man, Japanese and Spanish, are involved (Taule? et
al., 2008; Palmer and Xue, 2009; Hajic? et al, 2006;
Surdeanu et al, 2008; Burchardt et al, 2006; Kawa-
hara et al, 2002). This paper presents our research
for participation in the semantic-only (SRLonly)
challenge of the CoNLL-2009 shared task, with a
?This study is partially supported by CERG grant 9040861
(CityU 1318/03H), CityU Strategic Research Grant 7002037,
Projects 60673041 and 60873041 under the National Natural
Science Foundation of China and Project 2006AA01Z147 un-
der the ?863? National High-Tech Research and Development
of China.
highlight on our strategy to select features from a
large candidate set for maximum entropy learning.
2 System Survey
We opt for the maximum entropy model with Gaus-
sian prior as our learning model for all classification
subtasks in the shared task. Our implementation of
the model adopts L-BFGS algorithm for parameter
optimization as usual. No additional feature selec-
tion techniques are applied.
Our system is basically improved from its early
version for CoNLL-2008 (Zhao and Kit, 2008). By
introducing a virtual root for every predicates, The
job to determine both argument labels and predicate
senses is formulated as a word-pair classification
task in four languages, namely, Catalan, Spanish,
Czech and Japanese. In other three languages, Chi-
nese, English and German, a predicate sense clas-
sifier is individually trained before argument label
classification. Note that traditionally (or you may
say that most semantic parsing systems did so) ar-
gument identification and classification are handled
in a two-stage pipeline, while ours always tackles
them in one step, in addition, predicate sense classi-
fication are also included in this unique learning/test
step for four of all languages.
3 Pruning Argument Candidates
We keep using a word-pair classification procedure
to formulate semantic dependency parsing. Specif-
ically, we specify the first word in a word pair as a
predicate candidate (i.e., a semantic head, and noted
as p in our feature representation) and the next as an
argument candidate (i.e., a semantic dependent, and
55
noted as a). We do not differentiate between verbal
and non-verbal predicates and our system handles
them in the exactly same way.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly. As predicates overtly
known in the share task, we only consider how to
effectively prune argument candidates.
We adopt five types of argument pruning strate-
gies for seven languages. All of them assume that a
syntactic dependency parsing tree is available.
As for Chinese and English, we continue to use
a dependency version of the pruning algorithm of
(Xue and Palmer, 2004) as described in (Zhao and
Kit, 2008). The pruning algorithm is readdressed as
the following.
Initialization: Set the given predicate candidate
as the current node;
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head and
repeat step (1) until the root is reached.
Note that the given predicate candidate itself is
excluded from the argument candidate list for Chi-
nese, that is slightly different from English.
The above pruning algorithm has been shown ef-
fective. However, it is still inefficient for a single-
stage argument identification/classification classifi-
cation task. Thus we introduce an assistant argument
label ? NoMoreArgument? to alleviate this difficulty.
If an argument candidate in the above algorithm is
labeled as such a label, then the pruning algorithm
will end immediately. In training, this assistant label
means no more samples will be generated for the
current predicate, while in test, the decoder will not
search more argument candidates any more. This
adaptive technique more effectively prunes the ar-
gument candidates. In fact, our experiments show
1/3 training memory and time may be saved from it.
As for Catalan and Spanish, only syntactic chil-
dren of the predicate are considered as the argument
candidates.
As for Czech, only syntactic children, grandchil-
dren, great-grandchildren, parent and siblings of the
predicate are taken as the argument candidates.
As for German, only syntactic children, grand-
children, parent, siblings, siblings of parent and sib-
lings of grandparent of the predicate are taken as the
argument candidates.
The case is somewhat sophisticated for Japanese.
As we cannot identify a group of simple predicate-
argument relations from the syntactic tree. Thus
we consider top frequent 28 syntactic relations be-
tween the predicate and the argument. The parser
will search all words before and after the predicate,
and only those words that hold one of the 28 syn-
tactic relations to the predicate are considered as
the argument candidate. Similar to the pruning al-
gorithm for Chinese/English/German, we also in-
troduce two assistant labels ? leftNoMoreArgument?
and ? rightNoMoreArgument? to adaptively prune
words too far away from the predicate.
4 Feature Templates
As we don?t think that we can benefit from know-
ing seven languages, an automatic feature template
selection is conducted for each language.
About 1000 feature templates (hereafter this tem-
plate set is referred to FT ) are initially considered.
These feature templates are from various combina-
tions or integrations of the following basic elements.
Word Property. This type of elements include
word form, lemma, part-of-speech tag (PoS), FEAT
(additional morphological features), syntactic de-
pendency label (dprel), semantic dependency label
(semdprel) and characters (char) in the word form
(only suitable for Chinese and Japanese)1.
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm, and rn), and high(low) support verb or noun.
We explain the last item, support verb(noun). From
the predicate or the argument to the syntactic root
along the syntactic tree, the first verb(noun) that is
met is called as the low support verb(noun), and the
nearest one to the root is called as the high support
verb(noun).
Semantic Connection. This includes semantic
1All lemmas, PoS, and FEAT for either training or test are
from automatically pre-analyzed columns of every input files.
56
FEATn 1 2 3 4 5 6 7 8 9 10 11
Catalan/Spanish postype gen num person mood tense punct
Czech SubPOS Gen Num Cas Neg Gra Voi Var Sem Per Ten
Table 1: Notations of FEATs
head (semhead), left(right) farthest(nearest) seman-
tic child (semlm, semln, semrm, semrn). We say
a predicate is its argument?s semantic head, and the
latter is the former?s child. Features related to this
type may track the current semantic parsing status.
Path. There are two basic types of path between
the predicate and the argument candidates. One is
the linear path (linePath) in the sequence, the other
is the path in the syntactic parsing tree (dpPath). For
the latter, we further divide it into four sub-types
by considering the syntactic root, dpPath is the full
path in the syntactic tree. Leading two paths to the
root from the predicate and the argument, respec-
tively, the common part of these two paths will be
dpPathShare. Assume that dpPathShare starts from
a node r?, then dpPathPred is from the predicate to
r?, and dpPathArgu is from the argument to r?.
Family. Two types of children sets for the predi-
cate or argument candidate are considered, the first
includes all syntactic children (children), the second
also includes all but excludes the left most and the
right most children (noFarChildren).
Concatenation of Elements. For all collected el-
ements according to linePath, children and so on, we
use three strategies to concatenate all those strings
to produce the feature value. The first is seq, which
concatenates all collected strings without doing any-
thing. The second is bag, which removes all dupli-
cated strings and sort the rest. The third is noDup,
which removes all duplicated neighbored strings.
In the following, we show some feature template
examples derived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
a.pos+p.pos The concatenation of PoS of the ar-
gument and the predicate candidates.
p?1.pos+p.pos PoS of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas along
the syntactic tree path from the argument to the pred-
icate, then removed all duplicated ones and sort the
rest, finally concatenate all as a feature string.
a:p.highSupportNoun|linePath.dprel.seq Collect
all dependant labels along the line path from the ar-
gument to the high support noun of the predicate,
then concatenate all as a feature string.
(a:p|dpPath.dprel.seq)+p.FEAT1 Collect all de-
pendant labels along the line path from the argument
to the predicate and concatenate them plus the first
FEAT of the predicate.
An important feature for the task is dpTreeRela-
tion, which returns the relationship of a and p in a
syntactic parse tree and cannot be derived from com-
bining the above basic elements. The possible values
for this feature include parent, sibling etc.
5 Automatically Discovered Feature
Template Sets
For each language, starting from a basic feature tem-
plate set (a small subset of FT ) according to our
previous result in English dependency parsing, each
feature template outside the basic set is added and
each feature template inside the basic set is removed
one by one to check the effectiveness of each fea-
ture template following the performance change in
the development set. This procedure will be contin-
uously repeated until no feature template is added or
removed or the performance is not improved.
There are some obvious heuristic rules that help
us avoid trivial feature template checking, for ex-
ample, FEAT features are only suitable for Cata-
lan, Czech and Spanish. Though FEAT features are
also available for Japanese, we don?t adopt them for
this language due to the hight training cost. To sim-
plify feature representation, we use FEAT1, FEAT2,
and so on to represent different FEAT for every lan-
guages. A lookup list can be found in Table 1. Ac-
cording to the list, FEAT4 represents person for
Catalan or Spanish, but Cas for Czech.
As we don?t manually interfere the selection pro-
cedure for feature templates, ten quite different fea-
57
Ca Ch Cz En Gr Jp Sp
Ca 53
Ch 5 75
Cz 11 10 76
En 11 11 12 73
Gr 7 7 7 14 45
Jp 6 22 13 15 10 96
Sp 22 9 18 15 9 12 66
Table 2: Feature template set: argument classifier
Ch En Gr
Ch 46
En 5 9
Gr 17 2 40
Table 3: Feature template set: sense classifier
ture template sets are obtained at last. Statistical in-
formation of seven sets for argument classifiers is in
Table 2, and those for sense classifiers are in Table 3.
Numbers in the diagonals of these two tables mean
the numbers of feature templates, and others mean
how many feature templates are identical for every
language pairs. The most matched feature template
sets are for Catalan/Spanish and Chinese/Japanese.
As for the former, it is not so surprised because these
two corpora are from the same provider.
Besides the above statistics, these seven feature
template sets actually share little in common. For
example, the intersection set from six languages, as
Chinese is excluded, only includes one feature tem-
plate, p.lemma (the lemma of the predicate candi-
date). If all seven sets are involved, then such an in-
tersection set will be empty. Does this mean human
languages share little in semantic representation? :)
It is unlikely to completely demonstrate full fea-
ture template sets for all languages in this short re-
port, we thus only demonstrate two sets, one for En-
glish sense classification in Table 4 and the other for
Catalan argument classification in Table 52.
6 Word Sense Determination
The shared task of CoNLL-2009 still asks for the
predicate sense. In our work for CoNLL-2008 (Zhao
and Kit, 2008), this was done by searching for a right
2Full feature lists and their explanation for all languages will
be available at the website, http://bcmi.sjtu.edu.cn/?zhaohai.
p.lm.pos
p.rm.pos
p.lemma
p.lemma + p.lemma1
p.lemma + p.children.dprel.noDup
p.lemma + p.currentSense
p.form
p.form?1 + p.form
p.form + p.form1
Table 4: Feature set for English sense classification
example in the given dictionary. Unfortunately, we
late found this caused a poor performance in sense
determination. This time, an individual classifier is
used to determine the sense for Chinese, English or
German, and this is done by the argument classifier
by introducing a virtual root for every predicates for
the rest four languages3. Features used for sense
determination are also selected following the same
procedure in Section 5. The difference is only pred-
icate related features are used for selection.
7 Decoding
The decoding for four languages, Catalan, Czech,
Japanese and Spanish is trivial, each word pairs will
be checked one by one. The first word of the pair
is the virtual root or the predicate, the second is the
predicate or every argument candidates. Argument
candidates are checked in the order of different syn-
tactic relations to their predicate, which are enumer-
ated by the pruning algorithms in Section 3, or from
left to right for the same syntactic relation. After
the sense of the predicate is determined, the label of
each argument candidate will be directly classified,
or, it is proved non-argument.
As for the rest languages, Chinese, English or
German, after the sense classifier outputs its result,
an optimal argument structure for each predicate is
determined by the following maximal probability.
Sp = argmax
?
i
P (ai|ai?1, ai?2, ...), (1)
where Sp is the argument structure, P (ai|ai?1...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. Note that
3For Japanese, no senses for predicates are defined. Thus it
is actually a trivial classification task in this case.
58
p.currentSense + p.lemma
p.currentSense + p.pos
p.currentSense + a.pos
p?1.FEAT1
p.FEAT2
p1.FEAT3
p.semrm.semdprel
p.lm.dprel
p.form + p.children.dprel.bag
p.lemman (n = ?1, 0)
p.lemma + p.lemma1
p.pos?1 + p.pos
p.pos1
p.pos + p.children.dprel.bag
a.FEAT1 + a.FEAT3 + a.FEAT4
+ a.FEAT5 + a.FEAT6
a?1.FEAT2 + a.FEAT2
a.FEAT3 + a1.FEAT3
a.FEAT3 + a.h.FEAT3
a.children.FEAT1.noDup
a.children.FEAT3.bag
a.h.lemma
a.lm.dprel + a.form
a.lm.form
a.lm?1.lemma
a.lmn.pos (n=0,1)
a.noFarChildren.pos.bag + a.rm.form
a.pphead.lemma
a.rm.dprel + a.form
a.rm?1.form
a.rm.lemma
a.rn.dprel + a.form
a.lowSupportVerb.lemma
a?1.form
a.form + a1.form
a.form + a.children.pos
a.lemma + a.h.form
a.lemma + a.pphead.form
a1.lemma
a1.pos + a.pos.seq
a.pos + a.children.dprel.bag
a.lemma + p.lemma
(a:p|dpPath.dprel) + p.FEAT1
a:p|linePath.distance
a:p|linePath.FEAT1.bag
a:p|linePath.form.seq
a:p|linePath.lemma.seq
a:p|linePath.dprel.seq
a:p|dpPath.lemma.seq
a:p|dpPath.lemma.bag
a:p|dpPathArgu.lemma.seq
a:p|dpPathArgu.lemma.bag
Table 5: Feature set for Catalan argument classification
P (ai|ai?1, ...) in equation (1) may be simplified as
P (ai) if the input feature template set does not con-
cerned with the previous argument label output. A
beam search algorithm is used to find the parsing de-
cision sequence.
8 Evaluation Results
Our evaluation is carried out on two computational
servers, (1) LEGA, a 64-bit ubuntu Linux installed
server with double dual-core AMD Opteron proces-
sors of 2.8GHz and 24GB memory. This server was
also used for our previous participation in CoNLL-
2008 shared task. (2) MEGA, a 64-bit ubuntu Linux
installed server with six quad-core Intel Xeon pro-
cessors of 2.33GHz and 128GB memory.
Altogether nearly 60,000 machine learning rou-
tines were run to select the best fit feature template
sets for all seven languages within two months. Both
LEGA and MEGA were used for this task. How-
ever, training and test for the final submission of
Chinese, Czech and English run in MEGA, and the
rest in LEGA. As we used multiple thread training
and multiple routines run at the same time, the exact
time cost for either training or test is hard to esti-
mate. Here we just report the actual time and mem-
ory cost in Table 7 for reference.
The official evaluation results of our system are in
Table 6. Numbers in bold in the table stand for the
best performances for the specific languages. The
results in development sets are also given. The first
row of the table reports the results using golden in-
put features.
Two facts as the following suggest that our system
does output robust and stable results. The first is that
two results for development and test sets in the same
language are quite close. The second is about out-of-
domain (OOD) task. Though for each OOD task, we
just used the same model trained from the respective
language and did nothing to strengthen it, this does
not hinder our system to obtain top results in Czech
and English OOD tasks.
In addition, the feature template sets from auto-
matical selection procedure in this task were used
for the joint task of this shared task, and also output
top results according to the average score of seman-
tic labeled F1 (Zhao et al, 2009).
59
average Catalan Chinese Czech English German Japanese Spanish
Development with Gold 81.24 81.52 78.32 86.96 84.19 77.75 78.67 81.32
Development 80.46 80.66 77.90 85.35 84.01 76.55 78.41 80.39
Test (official scores) 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Out-of-domain 74.34 85.44 73.31 64.26
Table 6: Semantic labeled F1
Catalan Chinese Czech English German Japanese Spanish
Sense Training memory (MB) 418.0 136.0 63.0
Training time (Min.) 11.0 2.5 1.7
Test time (Min.) 0.7 0.2 0.03
Argument Training memory (GB) 0.4 3.7 3.2 3.8 0.2 1.4 0.4
Training time (Hours) 3.0 13.8 24.9 12.4 0.2 6.1 4.4
Test time (Min.) 3.0 144.0 27.1 88.0 1.0 4.2 7.0
Table 7: Computational cost
9 Conclusion
As presented in the above sections, we have tackled
semantic parsing for the CoNLL-2009 shared task
as a word-pair classification problem. Incorporated
with a proper argument candidate pruning strategy
and a large scale feature engineering for each lan-
guage, our system produced top results.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2004), pages 88?94, Barcelona, Spain,
July 25-26.
Hai Zhao and Chunyu Kit. 2008. Parsing syntac-
tic and semantic dependencies with two single-stage
maximum entropy models. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 203?207, Manchester, UK, August 16-
17.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multilin-
gual dependency learning: Exploiting rich features for
tagging syntactic and semantic dependencies. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
60
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61?66,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies
Hai Zhao(??)?, Wenliang Chen(???)?,
Jun?ichi Kazama?, Kiyotaka Uchimoto?, and Kentaro Torisawa?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual syntactic and semantic dependency
parsing for our participation in the joint task
of CoNLL-2009 shared tasks. Our system
uses rich features and incorporates various in-
tegration technologies. The system is evalu-
ated on in-domain and out-of-domain evalu-
ation data of closed challenge of joint task.
For in-domain evaluation, our system ranks
the second for the average macro labeled F1 of
all seven languages, 82.52% (only about 0.1%
worse than the best system), and the first for
English with macro labeled F1 87.69%. And
for out-of-domain evaluation, our system also
achieves the second for average score of all
three languages.
1 Introduction
This paper describes the system of National In-
stitute of Information and Communications Tech-
nology (NICT) and City University of Hong Kong
(CityU) for the joint learning task of CoNLL-2009
shared task (Hajic? et al, 2009)1. The system is ba-
sically a pipeline of syntactic parser and semantic
parser. We use a syntactic parser that uses very rich
features and integrates graph- and transition-based
methods. As for the semantic parser, a group of well
selected feature templates are used with n-best syn-
tactic features.
1Our thanks give to the following corpus providers, (Taule?
et al, 2008; Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006) and (Kawahara et al, 2002).
The rest of the paper is organized as follows. The
next section presents the technical details of our syn-
tactic dependency parsing. Section 3 describes the
details of the semantic dependency parsing. Section
4 shows the evaluation results. Section 5 looks into a
few issues concerning our forthcoming work for this
shared task, and Section 6 concludes the paper.
2 Syntactic Dependency Parsing
Basically, we build our syntactic dependency parsers
based on the MSTParser, a freely available imple-
mentation2, whose details are presented in the paper
of McDonald and Pereira (2006). Moreover, we ex-
ploit rich features for the parsers. We represent fea-
tures by following the work of Chen et al (2008) and
Koo et al (2008) and use features based on depen-
dency relations predicted by transition-based parsers
(Nivre and McDonald, 2008). Chen et al (2008) and
Koo et al (2008) proposed the methods to obtain
new features from large-scale unlabeled data. In our
system, we perform their methods on training data
because the closed challenge does not allow to use
unlabeled data. In this paper, we call these new ad-
ditional features rich features.
2.1 Basic Features
Firstly, we use all the features presented by McDon-
ald et al (2006), if they are available in data. Then
we add new features for the languages having FEAT
information (Hajic? et al, 2009). FEAT is a set of
morphological-features, e.g. more detailed part of
speech, number, gender, etc. We try to align differ-
ent types of morphological-features. For example,
2http://mstparser.sourceforge.net
61
we can obtain a sequence of gender tags of all words
from a head h to its dependent d. Then we represent
the features based on the obtained sequences.
Based on the results of development data, we per-
form non-projective parsing for Czech and German
and perform projective parsing for Catalan, Chinese,
English, Japanese, and Spanish.
2.2 Features Based on Dependency Pairs
I    see    a    beautiful    bird    .
Figure 1: Example dependency graph.
Chen et al (2008) presented a method of extract-
ing short dependency pairs from large-scale auto-
parsed data. Here, we extract all dependency pairs
rather than short dependency pairs from training
data because we believe that training data are reli-
able. In a parsed sentence, if two words have de-
pendency relation, we add this word pair into a list
named L and count its frequency. We consider the
direction. For example, in figure 1, a and bird have
dependency relation in the sentence ?I see a beauti-
ful bird.?. Then we add word pair ?a-bird:HEAD?3
into list L and accumulate its frequency.
We remove the pairs which occur only once in
training data. According to frequency, we then
group word pairs into different buckets, with bucket
LOW for frequencies 2-7, bucket MID for frequen-
cies 8-14, and bucket HIGH for frequencies 15+.
We set these threshold values by following the set-
ting of Chen et al (2008). For example, the fre-
quency of pair ?a-bird:HEAD? is 5. Then it is
grouped into bucket ?LOW?. We also add a vir-
tual bucket ?ZERO? to represent the pairs that are
not included in the list. So we have four buckets.
?ZERO?, ?LOW?, ?MID?, and ?HIGH? are used as
bucket IDs.
Based on the buckets, we represent new features
for a head h and its dependent d. We check word
pairs surrounding h and d. Table 1 shows the word
pairs, where h-word refers to the head word, d-word
refers to the dependent word, h-word-1 refers to
3HEAD means that bird is the head of the pair.
the word to the left of the head in the sentence, h-
word+1 refers to the word to the right of the head,
d-word-1 refers to the word to the left of the depen-
dent, and d-word+1 refers the word to the right of
the dependent. Then we obtain the bucket IDs of
these word pairs from L.
We generate new features consisting of indicator
functions for bucket IDs of word pairs. We call these
features word-pair-based features. We also generate
combined features involving bucket IDs and part-of-
speech tags of heads.
h-word, d-word
h-word-1, d-word
h-word+1, d-word
h-word, d-word-1
h-word, d-word+1
Table 1: Word pairs for feature representation
2.3 Features Based on Word Clusters
Koo et al (2008) presented new features based on
word clusters obtained from large-scale unlabeled
data and achieved large improvement for English
and Czech. Here, word clusters are generated only
from the training data for all the languages. We per-
form word clustering by using the clustering tool4,
which also was used by Koo et al (2008). The
cluster-based features are the same as the ones used
by Koo et al (2008).
2.4 Features Based on Predicted Relations
Nivre and McDonald (2008) presented an integrat-
ing method to provide additional information for
graph-based and transition-based parsers. Here, we
represent features based on dependency relations
predicted by transition-based parsers for graph-
based parser. Based on the results on development
data, we choose the MaltParser for Catalan, Czech,
German, and Spanish, and choose another MaxEnt-
based parser for Chinese, English, and Japanese.
2.4.1 A Transition-based Parser: MaltParser
For Catalan, Czech, German, and Spanish, we
use the MaltParser, a freely available implementa-
4http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
62
tion5, whose details are presented in the paper of
Nivre (2003). More information about the parser can
be available in the paper (Nivre, 2003).
Due to computational cost, we do not select new
feature templates for the MaltParser. Following the
features settings of Hall et al (2007), we use their
Czech feature file and Catalan feature file. To sim-
ply, we apply Czech feature file for German too, and
apply Catalan feature file for Spanish.
2.4.2 Another Transition-based Parser:
MaxEnt-based Parser
In three highly projective language, Chinese,
English and Japanese, we use the maximum en-
tropy syntactic dependency parser as in Zhao and
Kit (2008). We still use the similar feature notations
of that work. We use the same greedy feature selec-
tion of Zhao et al (2009) to determine an optimal
feature template set for each language. Full feature
sets for the three languages can be found at website,
http://bcmi.sjtu.edu.cn/?zhaohai.
2.4.3 Feature Representation
For training data, we use 2-way jackknifing to
generate predicted dependency parsing trees by two
transition-based parsers. Following the features of
Nivre and McDonald (2008), we define features for
a head h and its dependent d with label l as shown in
table 2, where GTran refers to dependency parsing
trees generated by the MaltParser or MaxEnt-base
Parser and ? refers to any label. All features are
conjoined with the part-of-speech tags of the words
involved in the dependency.
Is (h, d, ?) in GTran?
Is (h, d, l) in GTran?
Is (h, d, ?) not in GTran?
Is (h, d, l) not in GTran?
Table 2: Features set based on predicted labels
3 n-best Syntactic Features for Semantic
Dependency Parsing
Due to the limited computational resource that we
have, we used the the similar learning framework as
our participant in semantic-only task (Zhao et al,
5http://w3.msi.vxu.se/?nivre/research/MaltParser.html
Normal n-best Matched
Ca 53 54 50
Ch 75 65 55
En 73 70 63
Table 3: Feature template sets:n-best vs. non-n-best
2009). Namely, three languages, a single maximum
entropy model is used for all identification and clas-
sification tasks of predicate senses or argument la-
bels in four languages, Catalan, Czech, Japanese, or
Spanish. For the rest three languages, an individual
sense classifier still using maximum entropy is ad-
ditionally used to output the predicate sense previ-
ously. More details about argument candidate prun-
ing strategies and feature template set selection are
described in Zhao et al (2009).
The same feature template sets as the semantic-
only task are used for three languages, Czech, Ger-
man and Japanese. For the rest four languages, we
further use n-best syntactic features to strengthen
semantic dependency parsing upon those automati-
cally discovered feature template sets. However, we
cannot obtain an obvious performance improvement
in Spanish by using n-best syntactic features. There-
fore, only Catalan, Chinese and English semantic
parsing adopted these types of features at last.
Our work about n-best syntactic features still
starts from the feature template set that is originally
selected for the semantic-only task. The original fea-
ture template set is hereafter referred to ?the normal?
or ?non-n-best?. In practice, only 2nd-best syntactic
outputs are actually adopted by our system for the
joint task.
To generate helpful feature templates from the
2nd-best syntactic tree, we simply let al feature tem-
plates in the normal feature set that are based on
the 1st-best syntactic tree now turn to the 2nd-best
one. Using the same notations for feature template
representation as in Zhao et al (2009), we take an
example to show how the original n-best features
are produced. Assuming a.children.dprel.bag is
one of syntactic feature templates in the normal
set, this feature means that all syntactic children of
the argument candidate (a) are chosen, and their
dependant labels are collected, the duplicated la-
bels are removed and then sorted, finally all these
strings are concatenated as a feature. The cor-
63
Language Features
Catalan p:2.lm.dprel
a.lemma + a:2.h.form
a.lemma + a:2.pphead.form
(a:2:p:2|dpPath.dprel.seq) + p.FEAT1
Chinese a:2.h.pos
a:2.children.pos.seq + p:2.children.pos.seq
a:2:p:2|dpPath.dprel.bag
a:2:p:2|dpPathPred.form.seq
a:2:p:2|dpPath.pos.bag
(a:2:p:2|dpTreeRelation) + p.pos
(a:2:p:2|dpPath.dprel.seq) + a.pos
English a:2:p:2|dpPathPred.lemma.bag
a:2:p:2|dpPathPred.pos.bag
a:2:p:2|dpTreeRelation
a:2:p:2|dpPath.dprel.seq
a:2:p:2|dpPathPred.dprel.seq
a.lemma + a:2.dprel + a:2.h.lemma
(a:2:p:2|dpTreeRelation) + p.pos
Table 4: Features for n-best syntactic tree
responding 2nd-best syntactic feature will be a :
2.children.dprel.bag. As all operations to gener-
ate the feature for a.children.dprel.bag is within
the 1st-best syntactic tree, while those for a :
2.children.dprel.bag is within the 2nd-best one. As
all these 2nd-best syntactic features are generated,
we use the same greedy feature selection procedure
as in Zhao et al (2009) to determine the best fit fea-
ture template set according to the evaluation results
in the development set.
For Catalan, Chinese and English, three opti-
mal n-best feature sets are obtained, respectively.
Though dozens of n-best features are initially gen-
erated for selection, only few of them survive af-
ter the greedy selection. A feature number statis-
tics is in Table 3, and those additionally selected
n-best features for three languages are in Table
4. Full feature lists and their explanation for
all languages will be available at the website,
http://bcmi.sjtu.edu.cn/?zhaohai.
4 Evaluation Results
Two tracks (closed and open challenges) are pro-
vided for joint task of CoNLL2009 shared task.
We participated in the closed challenge and evalu-
ated our system on the in-domain and out-of-domain
evaluation data.
avg. Cz En Gr
Syntactic (LAS) 77.96 75.58 82.38 75.93
Semantic (Labeled F1) 75.01 82.66 74.58 67.78
Joint (Macro F1) 76.51 79.12 78.51 71.89
Table 6: The official results of our submission for out-of-
domain task(%)
Test Dev
Basic ALL Basic ALL
Catalan 82.91 85.88 83.15 85.98
Chinese 74.28 75.67 73.36 75.64
Czech 77.21 79.70 77.91 80.22
English 88.63 89.19 86.35 87.40
German 84.61 86.24 83.99 85.44
Japanese 92.31 92.32 92.01 92.85
Spanish 83.59 86.29 83.73 86.22
Average 83.32 85.04 82.92 84.82
(+1.72) (+1.90)
Table 7: The effect of rich features for syntactic depen-
dency parsing
4.1 Official Results
The official results for the joint task are in Table 5,
and the out-of-domain task in Table 6, where num-
bers in bold stand for the best performances for the
specific language. For out-of-domain (OOD) eval-
uation, we did not perform any domain adaptation.
For both in-domain and out-of-domain evaluation,
our system achieved the second best performance
for the average Macro F1 scores of all the languages.
And our system provided the first best performance
for the average Semantic Labeled F1 score and the
forth for the average Labeled Syntactic Accuracy
score for in-domain evaluation.
4.2 Further results
At first, we check the effect of rich features for syn-
tactic dependency parsing. Table 7 shows the com-
parative results of basic features and all features on
test and development data, where ?Basic? refers to
the system with basic features and ?ALL? refers to
the system with basic features plus rich features. We
found that the additional features provided improve-
ment of 1.72% for test data and 1.90% for develop-
ment data.
Then we investigate the effect of different train-
ing data size for semantic parsing. The learning
64
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 85.04 85.88 75.67 79.70 89.19 86.24 92.32 86.29
Semantic (Labeled F1) 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
Joint (Macro F1) 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
Table 5: The official results of our joint submission (%)
Data Czech Chinese English
normal n-best normal n-best
25% 80.71 75.12 75.24 82.02 82.06
50% 81.52 76.50 76.59 83.52 83.42
75% 81.90 76.92 77.01 84.21 84.30
100% 82.24 77.35 77.34 84.73 84.80
Table 8: The performance in development set (semantic
labeled F1) vs. training corpus size
curves are drawn for Czech, Chinese and English.
We use 25%, 50% and 75% training corpus, respec-
tively. The results in development sets are given in
Table 8. Note that in this table the differences be-
tween normal and n-best feature template sets are
also given for Chinese and English. The results
in the table show that n-best features help improve
Chinese semantic parsing as the training corpus is
smaller, while it works for English as the training
corpus is larger.
5 Discussion
This work shows our further endeavor in syntactic
and semantic dependency parsing, based on our pre-
vious work (Chen et al, 2008; Zhao and Kit, 2008).
Chen et al (Chen et al, 2008) and Koo et al (Koo
et al, 2008) used large-scale unlabeled data to im-
prove syntactic dependency parsing performance.
Here, we just performed their method on training
data. From the results, we found that the new fea-
tures provided better performance. In future work,
we can try these methods on large-scale unlabeled
data for other languages besides Chinese and En-
glish.
In Zhao and Kit (2008), we addressed that seman-
tic parsing should benefit from cross-validated train-
ing corpus and n-best syntactic output. These two
issues have been implemented during this shared
task. Though existing work show that re-ranking for
semantic-only or syntactic-semantic joint tasks may
bring higher performance, the limited computational
resources does not permit us to do this for multiple
languages.
To analyze the advantage and the weakness of our
system, the ranks for every languages of our sys-
tem?s outputs are given in Table 9, and the perfor-
mance differences between our system and the best
one in Table 106. The comparisons in these two ta-
bles indicate that our system is slightly weaker in the
syntactic parsing part, this may be due to the reason
that syntactic parsing in our system does not ben-
efit from semantic parsing as the other joint learn-
ing systems. However, considering that the seman-
tic parsing in our system simply follows the output
of the syntactic parsing and the semantic part of our
system still ranks the first for the average score, the
semantic part of our system does output robust and
stable results. It is worth noting that semantic la-
beled F1 in Czech given by our system is 4.47%
worse than the best one. This forby gap in this lan-
guage further indicates the advantage of our system
in the other six languages and some latent bugs or
learning framework misuse in Czech semantic pars-
ing.
6 Conclusion
We describe the system that uses rich features and
incorporates integrating technology for joint learn-
ing task of syntactic and semantic dependency pars-
ing in multiple languages. The evaluation results
show that our system is good at both syntactic and
semantic parsing, which suggests that a feature-
oriented method is effective in multiple language
processing.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
6The difference for Chinese in the latter table is actually
computed between ours and the second best system.
65
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 4 4 4 4 2 3 3 4
Semantic (Labeled F1) 1 1 3 4 1 2 2 1
Joint (Macro F1) 2 1 3 4 1 3 2 1
Table 9: Our system?s rank within the joint task according to three main measures
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 0.73 1.98 0.84 0.68 0.69 1.24 0.25 1.35
Semantic (Labeled F1) - - 0.38 4.47 - 2.42 0.09 -
Joint (Macro F1) 0.12 - 0.15 2.40 - 1.22 0.37 -
Table 10: The performance differences between our system and the best one within the joint task according to three
main measures
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC-2006,
Genoa, Italy.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP-2008, Hyderabad, In-
dia, January 8-10.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL-
2009, Boulder, Colorado, USA.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech, June.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC-2002, pages 2008?
2013, Las Palmas, Canary Islands.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, USA, June.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X, New York City, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France, April 23-25.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the CoNLL-
2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage maxi-
mum entropy models. In Proceedings of CoNLL-2008,
pages 203?207, Manchester, UK, August 16-17.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of CoNLL-2009, Boul-
der, Colorado, USA.
66
Coling 2010: Poster Volume, pages 126?134,
Beijing, August 2010
Improving Graph-based Dependency Parsing with Decision History
Wenliang Chen?, Jun?ichi Kazama?, Yoshimasa Tsuruoka?? and Kentaro Torisawa?
?Language Infrastructure Group, MASTAR Project, NICT
{chenwl, kazama, torisawa}@nict.go.jp
?School of Information Science, JAIST
tsuruoka@jaist.ac.jp
Abstract
This paper proposes an approach to im-
prove graph-based dependency parsing by
using decision history. We introduce a
mechanism that considers short dependen-
cies computed in the earlier stages of pars-
ing to improve the accuracy of long de-
pendencies in the later stages. This re-
lies on the fact that short dependencies are
generally more accurate than long depen-
dencies in graph-based models and may
be used as features to help parse long de-
pendencies. The mechanism can easily
be implemented by modifying a graph-
based parsing model and introducing a set
of new features. The experimental results
show that our system achieves state-of-
the-art accuracy on the standard PTB test
set for English and the standard Penn Chi-
nese Treebank (CTB) test set for Chinese.
1 Introduction
Dependency parsing is an approach to syntactic
analysis inspired by dependency grammar. In re-
cent years, interest in this approach has surged due
to its usefulness in such applications as machine
translation (Nakazawa et al, 2006), information
extraction (Culotta and Sorensen, 2004).
Graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007) have achieved
state-of-the-art accuracy for a wide range of lan-
guages as shown in recent CoNLL shared tasks
(Buchholz et al, 2006; Nivre et al, 2007). How-
ever, to make parsing tractable, these models are
forced to restrict features over a very limited his-
tory of parsing decisions (McDonald and Pereira,
2006; McDonald and Nivre, 2007). Previous
work showed that rich features over a wide range
of decision history can lead to significant im-
provements in accuracy for transition-based mod-
els (Yamada and Matsumoto, 2003a; Nivre et al,
2004).
In this paper, we propose an approach to im-
prove graph-based dependency parsing by using
decision history. Here, we make an assumption:
the dependency relations between words with a
short distance are more reliable than ones between
words with a long distance. This is supported by
the fact that the accuracy of short dependencies
is in general greater than that of long dependen-
cies as reported in McDonald and Nivre (2007)
for graph-based models. Our idea is to use deci-
sion history, which is made in previous scans in a
bottom-up procedure, to help parse other words in
later scans. In the bottom-up procedure, short de-
pendencies are parsed earlier than long dependen-
cies. Thus, we introduce a mechanism in which
we treat short dependencies built earlier as deci-
sion history to help parse long dependencies in
later stages. It can easily be implemented by mod-
ifying a graph-based parsing model and designing
a set of features for the decision history.
To demonstrate the effectiveness of the pro-
posed approach, we present experimental results
on English and Chinese data. The results indi-
cate that the approach greatly improves the accu-
racy and that richer history-based features indeed
make large contributions. The experimental re-
sults show that our system achieves state-of-the-
art accuracy on the data.
2 Motivation
In this section, we present an example to show
the idea of using decision history in a dependency
parsing procedure.
Suppose we have two sentences in Chinese, as
shown in Figures 1 and 2, where the correct de-
pendencies are represented by the directed links.
For example, in Figure 1 the directed link from
126
w3:?(bought) to w5:?(books) mean that w3 is
the head and w5 is the dependent. In Chinese,
the relationship between clauses is often not made
explicit and two clauses may simply be put to-
gether with only a comma (Li and Thompson,
1997). This makes it hard to parse Chinese sen-
tences with several clauses.
ROOT
?? ? ? ? ? ??? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3         w4      w5      w6     w7        w8    w9     w10      w11      w12(Last year I bought some books and this year he also bought some books.)
Figure 1: Example A
ROOT
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8        w9       w10        w11(Last year I bought some books and this year too)      
Figure 2: Example B
If we employ a graph-based parsing model,
such as the model of (McDonald and Pereira,
2006; Carreras, 2007), it is difficult to assign the
relations between w3 and w10 in Example A and
between w3 and w9 in Example B. For simplicity,
we use wAi to refer to wi of Example A and wBi to
refer to wi of Example B in what follows.
The key point is whether the second clauses are
independent in the sentences. The two sentences
are similar except that the second clause of Exam-
ple A is an independent clause but that of Exam-
ple B is not. wA10 is the root of the second clause
of Example A with subject wA8 , while wB9 is the
root of the second clause of Example B, but the
clause does not have a subject. These mean that
the correct decisions are to assign wA10 as the head
of wA3 and wB3 as the head of wB9 , as shown by the
dash-dot-lines in Figures 1 and 2.
However, the model can use very limited infor-
mation. Figures 3-(a) and 4-(a) show the right
dependency relation cases and Figures 3-(b) and
4-(b) show the left direction cases. For the right
direction case of Example A, the model has the
information about wA3 ?s rightmost child wA5 and
wA10?s leftmost child wA6 inside wA3 and wA10, but it
does not have information about the other children
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12(a)
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books)
(b)w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12
Figure 3: Example A: two directions
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (also) (bought) (NULL) (books) w1         w2   w3         w4    w5      w6  w7        w8     w9         w10        w11      (a)
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) ( ) (this year) (also) (bought) (NULL) (books)
(b)
,w1         w2   w3         w4    w5      w6  w7        w8     w9         w10        w11      
Figure 4: Example B: two directions
(such as wA8 ) of wA3 and wA10, which may be useful
for judging the relation between wA3 and wA10. The
parsing model can not find the difference between
the syntactic structures of two sentences for pairs
(wA3 ,wA10) and (wB3 ,wB9 ). If we can provide the in-
formation about the other children of wA3 and wA10
to the model, it becomes easier to find the correct
direction between wA3 and wA10.
Next, we show how to use decision history to
help parse wA3 and wA10 of Example A.
In a bottom up procedure, the relations between
the words inside [wA3 , wA10] are built as follows
before the decision for wA3 and wA10. In the first
round, we build relations for neighboring words
(word distance1=1), such as the relations between
wA3 and wA4 and between wA4 and wA5 . In the sec-
ond round, we build relations for words of dis-
tance 2, and then for longer distance words until
all the possible relations between the inside words
are built. Figure 5 shows all the possible relations
inside [wA3 , wA10] that we can build. To simplify,
we use undirected links to refer to both directions
1Word distance between wi and wj is |j ? i|.
127
of dependency relations between words in the fig-
ure.
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books)   (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3         w4 w5        w6  w7        w8   w9     w10     w11      w12
Figure 5: Example A: first step
Then given those inside relations, we choose
the inside structure with the highest score for each
direction of the dependency relation between wA3
and wA10. Figure 6 shows the chosen structures.
Note that the chosen structures for two directions
could either be identical or different. In Figure
6-(a) and -(b), they are different.
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12(a)       
(b)
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12
Figure 6: Example A: second step
Finally, we use the chosen structures as deci-
sion history to help parse wA3 and wA10. For ex-
ample, the fact that wA8 is a dependent of wA10 is
a clue that suggests that the second clause may be
independent. This results in wA10 being the head of
wA3 .
This simple example shows how to use the de-
cision history to help parse the long distance de-
pendencies.
3 Background: graph-based parsing
models
Before we describe our method, we briefly intro-
duce the graph-based parsing models. We denote
input sentence w by w = (w0, w1, ..., wn), where
w0 = ROOT is an artificial root token inserted at
the beginning of the sentence and does not depend
on any other token in w and wi refers to a word.
We employ the second-order projective graph-
based parsing model of Carreras (2007), which is
an extension of the projective parsing algorithm of
Eisner (1996).
The parsing algorithms used in Carreras (2007)
independently find the left and right dependents of
a word and then combine them later in a bottom-
up style based on Eisner (1996). A subtree that
spans the words in [s, t] (and roots at s or t) is
represented by chart item [s, t, right/left, C/I],
where right (left) indicates that the root of the sub-
tree is s (t) and C means that the item is complete
while I means that the item is incomplete (Mc-
Donald, 2006). Here, complete item in the right
(left) direction means that the words other than s
(t) cannot have dependents outside [s, t] and in-
complete item in the right (left) direction, on the
other hand, means that t (s) may have dependents
outside [s, t]. In addition, t (s) is the direct depen-
dent of s (t) in the incomplete item with the right
(left) direction.
Larger chart items are created from pairs of
smaller chart items by the bottom-up procedure.
Figure 7 illustrates the cubic parsing actions of the
Eisner?s parsing algorithm (Eisner, 1996) in the
right direction, where s, r, and t refer to the start
and end indices of the chart items. In Figure 7-
(a), all the items on the left side are complete and
represented by triangles, where the triangle of [s,
r] is complete item [s, r,?, C] and the triangle of
[r + 1, t] is complete item [r + 1, t,?, C]. Then
the algorithm creates incomplete item [s, t,?, I]
(trapezoid on the right side of Figure 7-(a)) by
combining the chart items on the left side. This
action builds the dependency from s to t. In Fig-
ure 7-(b), the item of [s, r] is incomplete and
the item of [r, t] is complete. Then the algo-
rithm creates complete item [s, t,?, C]. For the
left direction case, the actions are similar. Note
that only the actions of creating the incomplete
chart items build new dependency relations be-
tween words, while the ones of creating the com-
plete items merge the existing structures without
building new relations.
Once the parser has considered the dependency
relations between words of distance 1, it goes on
128
to dependency relations between words of dis-
tance 2, and so on by the parsing actions. For
words of distance 2 and greater, it considers ev-
ery possible partition of the structures into two
parts and chooses the one with the highest score
for each direction. The score is the sum of the fea-
ture weights of the chart items. The features are
designed over edges of dependency trees and the
weights are given by model parameters (McDon-
ald and Pereira, 2006; Carreras, 2007). We store
the obtained chart items in a table. The chart item
includes the information on the optimal splitting
point of itself. Thus, by looking up the table, we
can obtain the best tree structure (with the highest
score) of any chart item.
s         r     r+1    t            s                   t
(a)
s         r     r t               s                 t
(b)
Figure 7: Cubic parsing actions of Eisner (1996)
4 Parsing with decision history
As mentioned above, the actions for creating
the incomplete items build the relations between
words. In this study, we only consider using his-
tory information when creating incomplete items.
4.1 Decision history
Suppose we are going to compute the scores of
the relations between ws and wt. There are two
possible directions for them.
By using the bottom-up style algorithm, the
scores of the structures between words with dis-
tance < |s?t| are computed in previous scans and
the structures are stored in the table. We divide
the decision history into two types: history-inside
and history-outside. The history-inside type is the
decision history made inside [s,t] and the history-
outside type is the history made outside [s,t].
4.1.1 History-inside
We obtain the structure with the highest score
for each direction of the dependency between ws
and wt. Figure 8-(b) shows the best solution (with
the highest score) of the left direction, where the
structure is split into two parts, [s, r1,?, C] and
[r1 + 1, t,?, C]. Figure 8-(c) shows the best so-
lution of the right case, where the structure is split
into two parts, [s, r2,?, C] and [r2 + 1, t,?, C].
s          r1 r1+1               t
ws ?                 wt (b)(a)
s r r +1 t2 2(c)
Figure 8: History-inside
By looking up the table, we have a subtree that
roots at ws on the right side of ws and a subtree
that roots at wt on the left side of wt. We use these
structures as the information on history-inside.
4.1.2 History-outside
For history-outside, we try to obtain the sub-
tree that roots at ws on the left side of ws and
the one that roots at wt on the right side of wt.
However, compared to history-inside, obtaining
history-outside is more complicated because we
do not know the boundaries and the proper struc-
tures of the subtrees. Here, we use an simple
heuristic method to find a subtree whose root is
at ws on the left side of ws and one whose root is
at wt on the right side of wt.
We introduce two assumptions: 1) The struc-
ture within a sub-sentence 2 is more reliable than
the one that goes across from sub-sentences. 2)
More context (more words) can result in a better
solution for determining subtree structures.
2To simplify, we split one sentence into sub-sentences
with punctuation marks.
129
Algorithm 1 Searching for history-outside
boundaries
1: Input: w, s, t
2: for k = s? 1 to 1 do
3: if(isPunct(wk)) break;
4: if(s? k >= t? s? 1) break
5: end for
6: bs = k
7: for k = t + 1 to |w| do
8: if(isPunct(wk)) break;
9: if(k ? t >= t? s? 1) break
10: end for
11: bt = k
12: Output: bs, bt
Under these two assumptions, Algorithm 1
shows the procedure for searching for history-
outside boundaries, where bs is the boundary for
for the descendants on the left side of ws , bt
is the boundary for searching the descendants on
the right side of wt, and isPunct is the function
that checks if the word is a punctuation mark. bs
should be in the same sub-sentence with s and
|s? bs| should be less than |t? s|. bt should be in
the same sub-sentence with t and |bt ? t| should
be less than |t? s|.
Next we try to find the subtree structures. First,
we collect the part-of-speech (POS) tags of the
heads of all the POS tags in training data and
remove the tags that occur fewer than 10 times.
Then, we determine the directions of the relations
by looking up the collected list. For bs and s, we
check if the POS tag of ws could be the head tag
of the POS tag of wbs by looking up the list. If
so, the direction d is ?. Otherwise, we check if
the POS tag of wbs could be the head tag of the
POS tag of ws. If so, d is ?, else d is ?. Fi-
nally, we obtain the subtree of ws from chart item
[bs, s, d, I]. Similarly, we obtain the subtree of wt.
Figure 9 shows the history-outside information for
ws and wt, where the relation between wbs and ws
and the relation between wbt and wt will be de-
termined by the above method. We have subtree
[rs, s, left, C] that roots at ws on the left side of
ws and subtree [t, rt, right, C] that roots at wt on
the right side of wt in Figure 9-(b) and (c).
4.2 Parsing algorithm
Then, we explain how to use these decision his-
tory in the parsing algorithm. We use Lst to rep-
bs rs s        t         rt bt(b)ws ?                 wt(a)
b r s t r b(c)s s t t
Figure 9: History-outside
resent the scores of basic features for the left di-
rection and Rst for the right case. Then we design
history-based features (described in Section 4.3)
based on the history-inside and history-outside in-
formation, as mentioned above. Finally, we up-
date the scores with the ones of the history-based
features by the following equations:
L+st = Lst + Ldfst (1)
R+st = Rst + Rdfst (2)
where L+st and R+st refer to the updated scores, Ldfst
and Rdfst refer to the scores of the history-based
features.
Algorithm 2 Parsing algorithm
1: Initialization: V [s, s, dir, I/C] = 0.0 ?s, dir
2: for k = 1 to n do
3: for s = 0 to n? k do
4: t = s + k
5: % Create incomplete items
6: Lst=V [s, t,?, I]= maxs?r<tV I(r);
7: Rst=V [s, t,?, I]= maxs?r<tV I(r);
8: Calculate Ldfst and Rdfst ;9: % Update the scores of incomplete chart items
10: V [s, t,?, I]=L+st=Lst + Ldfst
11: V [s, t,?, I]=R+st=Rst + Rdfst12: % Create complete items
13: V [s, t,?, C]= maxs?r<tV C(r);
14: V [s, t,?, C]= maxs<r?tV C(r);
15: end for
16: end for
Algorithm 2 is the parsing algorithm with
the history-based features, where V [s, t, dir, I/C]
refers to the score of chart item [s, t, dir, I/C],
V I(r) is a function to search for the optimal
sibling and grandchild nodes for the incomplete
items (line 6 and 7) (Carreras, 2007) given the
130
splitting point r and return the score of the struc-
ture, and V C(r) is a function to search for the op-
timal grandchild node for the complete items (line
13 and 14). Compared with the parsing algorithms
of Carreras (2007), Algorithm 2 uses history in-
formation by adding line 8, 10, and 11.
In Algorithm 2, it first creates chart items with
distance 1, then goes on to chart items with dis-
tance 2, and so on. In each round, it searches for
the structures with the highest scores for incom-
plete items shown at line 6 and 7 of Algorithm 2.
Then we update the scores with the history-based
features by Equation 1 and Equation 2 at line 10
and 11 of Algorithm 2. However, note that we can
not guarantee to find the candidate with the high-
est score with Algorithm 2 because new features
violate the assumptions of dynamic programming.
4.3 History-based features
In this section, we design features that capture the
history information in the recorded decisions.
For a dependency between two words, say s and
t, there are four subtrees that root at s or t. We de-
sign the features by combining s, twith each child
of s and t in the subtrees. The feature templates
are shown as follows: (In the following, c means
one of the children of s and t, and the nodes in the
templates are expanded to their lexical form and
POS tags to obtain actual features.):
C+Dir this feature template is a 2-tuple con-
sisting of (1) a c node and (2) the direction of the
dependency.
C+Dir+S/C+Dir+T this feature template is a 3-
tuple consisting of (1) a c node, (2) the direction
of the dependency, and (3) a s or t node.
C+Dir+S+T this feature template is a 4-tuple
consisting of (1) a c node, (2) the direction of the
dependency, (3) a s node, and (4) a t node.
s     csi r1 r1+1 cti tr2 cso cto r3
Figure 10: Structure of decision history
We use SHI to represent the subtree of s in
the history-inside, THI to represent the one of t
in the history-inside, SHO to represent the one
of s in the history-outside, and THO to represent
the one of t in the history-outside. Based on the
subtree types, the features are divided into four
sets: FSHI , FTHI , FSHO, and FTHO refer to the
features related to the children that are in subtrees
SHI , THI , SHO, and THO respectively.
Figure 10 shows the structure of decision his-
tory of a left dependency (between s and t) re-
lation. For the right case, the structure is simi-
lar. In the figure, SHI is chart item [s, r1,?, C],
THI is chart item [r1 + 1, t,?, C], SHO is
chart item [r2, s,?, C], and THO is chart item
[t, r3,?, C]. We use csi, cti, cso, and cto to repre-
sent a child of s/t in subtrees SHI , THI , SHO,
and THO respectively. The lexical form features
of FSHI and FSHO are listed as examples in Table
1, where ?L? refers to the left direction. We can
also expand the nodes in the templates to the POS
tags. Compared with the algorithm of Carreras
(2007) that only considers the furthest children of
s and t, Algorithm 2 considers all the children.
Table 1: Lexical form features of FSHI and FSHO
template FSHI FSHO
C+DIR word-csi+L word-cso+L
C+DIR+S word-csi+L+word-s word-cso+L+word-s
C+DIR+T word-csi+L+word-t word-cso+L+word-t
C+DIR word-csi+L word-cso+L
+S+T +word-s+word-t +word-s+word-t
4.4 Policy of using history
In practice, we define several policies to use the
history information for different word pairs as fol-
lows:
? All: Use the history-based features for all the
word pairs without any restriction.
? Sub-sentences: use the history-based fea-
tures only for the relation of two words from
sub-sentences. Here, we use punctuation
marks to split sentences into sub-sentences.
? Distance: use the history-based features for
the relation of two words within a predefined
distance. We set the thresholds to 3, 5, and
10.
131
5 Experimental results
In order to evaluate the effectiveness of the
history-based features, we conducted experiments
on Chinese and English data.
For English, we used the Penn Treebank (Mar-
cus et al, 1993) in our experiments and the tool
?Penn2Malt?3 to convert the data into dependency
structures using a standard set of head rules (Ya-
mada and Matsumoto, 2003a). To match previous
work (McDonald and Pereira, 2006; Koo et al,
2008), we split the data into a training set (sec-
tions 2-21), a development set (Section 22), and a
test set (section 23). Following the work of Koo
et al (2008), we used the MXPOST (Ratnaparkhi,
1996) tagger trained on training data to provide
part-of-speech tags for the development and the
test set, and we used 10-way jackknifing to gener-
ate tags for the training set.
For Chinese, we used the Chinese Treebank
(CTB) version 4.04 in the experiments. We also
used the ?Penn2Malt? tool to convert the data and
created a data split: files 1-270 and files 400-931
for training, files 271-300 for testing, and files
301-325 for development. We used gold stan-
dard segmentation and part-of-speech tags in the
CTB. The data partition and part-of-speech set-
tings were chosen to match previous work (Chen
et al, 2008; Yu et al, 2008).
We measured the parser quality by the unla-
beled attachment score (UAS), i.e., the percentage
of tokens with the correct HEAD 5. And we also
evaluated on complete dependency analysis.
In our experiments, we implemented our sys-
tems on the MSTParser6 and extended with
the parent-child-grandchild structures (McDonald
and Pereira, 2006; Carreras, 2007). For the base-
line systems, we used the first- and second-order
(parent-sibling) features that were used in Mc-
Donald and Pereira (2006) and other second-order
features (parent-child-grandchild) that were used
in Carreras (2007). In the following sections, we
call the second-order baseline systems Baseline
3http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
4http://www.cis.upenn.edu/?chinese/.
5As in previous work, English evaluation ignores any to-
ken whose gold-standard POS tag is one of {?? `` : , .} and
Chinese evaluation ignores any token whose tag is ?PU?.
6http://mstparser.sourceforge.net
and our new systems OURS.
5.1 Results with different feature settings
In this section, we test our systems with different
settings on the development data.
Table 2: Results with different policies
Chinese English
Baseline 89.04 92.43
D1 88.73 92.27
D3 88.90 92.36
D5 89.10 92.59
D10 89.32 92.57
Dsub 89.57 92.63
Table 2 shows the parsing results when we used
different policies defined in Section 4.4 with all
the types of features, where Dsub refers to apply-
ing the policy: sub-sentence, D1 refers to apply-
ing the policy: all, and D3|5|10 refers to applying
the policy: distance with the predefined distance
3, 5, or 10. The results indicated that the accu-
racies of our systems decreased if we used the
history information for short distance words. The
system with Dsub performed the best.
Table 3: Results with different types of Features
Chinese English
Baseline 89.04 92.43
+FSHI 89.14 92.53
+FTHI 89.33 92.35
+FSHO 89.25 92.47
+FTHO 88.99 92.54
Then we investigated the effect of different
types of the history-based features. Table 3 shows
the results with policy Dsub. From the table, we
found that FTHI provided the largest improve-
ment for Chinese and FTHO performed the best
for English.
In what follows, we used Dsub as the policy for
all the languages, the features FSHI + FTHI +
FSHO for Chinese, and the features FSHI +
FSHO + FTHO for English.
5.2 Main results
The main results are shown in the upper parts of
Tables 4 and 5, where the improvements by OURS
over the Baselines are shown in parentheses. The
results show that OURS provided better perfor-
mance over the Baselines by 1.02 points for Chi-
132
Table 4: Results for Chinese
UAS Complete
Baseline 88.41 48.85
OURS 89.43(+1.02) 50.86
OURS+STACK 89.53 49.42
Zhao2009 87.0 ?
Yu2008 87.26 ?
STACK 88.95 49.42
Chen2009 89.91 48.56
nese and 0.29 points for English. The improve-
ments of (OURS) were significant in McNemar?s
Test with p < 10?4 for Chinese and p < 10?3 for
English.
5.3 Comparative results
Table 4 shows the comparative results for Chinese,
where Zhao2009 refers to the result of (Zhao et
al., 2009), Yu2008 refers to the result of Yu et
al. (2008), Chen2009 refers to the result of Chen
et al (2009) that is the best reported result on
this data, and STACK refers to our implementa-
tion of the combination parser of Nivre and Mc-
Donald (2008) using our baseline system and the
MALTParser7. The results indicated that OURS
performed better than Zhao2009, Yu2008, and
STACK, but worse than Chen2009 that used large-
scale unlabeled data (Chen et al, 2009). We also
implemented the combination system of OURS
and the MALTParser, referred as OURS+STACK
in Table 4. The new system achieved further im-
provement. In future work, we can combine our
approach with the parser of Chen et al (2009).
Table 5 shows the comparative results for En-
glish, where Y&M2003 refers to the parser of Ya-
mada and Matsumoto (2003b), CO2006 refers to
the parser of Corston-Oliver et al (2006), Z&C
2008 refers to the combination system of Zhang
and Clark (2008), STACK refers to our implemen-
tation of the combination parser of Nivre and Mc-
Donald (2008), KOO2008 refers to the parser of
Koo et al (2008), Chen2009 refers to the parser
of Chen et al (2009), and Suzuki2009 refers to
the parser of Suzuki et al (2009) that is the best
reported result for this data. The results shows
that OURS outperformed the first two systems that
were based on single models. Z&C 2008 and
STACK were the combination systems of graph-
7http://www.maltparser.org/
Table 5: Results for English
UAS Complete
Baseline 91.92 44.28
OURS 92.21 (+0.29) 45.24
Y&M2003 90.3 38.4
CO2006 90.8 37.6
Z&C2008 92.1 45.4
STACK 92.53 47.06
KOO2008 93.16 ?
Chen2009 93.16 47.15
Suzuki2009 93.79 ?
based and transition-based models. OURS per-
formed better than Z&C 2008, but worse than
STACK. The last three systems that used large-
scale unlabeled data performed better than OURS.
6 Related work
There are several studies that tried to overcome
the limited feature scope of graph-based depen-
dency parsing models .
Nakagawa (2007) proposed a method to deal
with the intractable inference problem in a graph-
based model by introducing the Gibbs sampling
algorithm. Compared with their approach, our ap-
proach is much simpler yet effective. Hall (2007)
used a re-ranking scheme to provide global fea-
tures while we simply augment the features of an
existing parser.
Nivre and McDonald (2008) and Zhang and
Clark (2008) proposed stacking methods to com-
bine graph-based parsers with transition-based
parsers. One parser uses dependency predictions
made by another parser. Our results show that our
approach can be used in the stacking frameworks
to achieve higher accuracy.
7 Conclusions
This paper proposes an approach for improving
graph-based dependency parsing by using the de-
cision history. For the graph-based model, we
design a set of features over short dependen-
cies computed in the earlier stages to improve
the accuracy of long dependencies in the later
stages. The results demonstrate that our proposed
approach outperforms baseline systems by 1.02
points for Chinese and 0.29 points for English.
133
References
Buchholz, S., E. Marsi, A. Dubey, and Y. Kry-
molowski. 2006. CoNLL-X shared task on
multilingual dependency parsing. Proceedings of
CoNLL-X.
Carreras, X. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
Chen, WL., D. Kawahara, K. Uchimoto, YJ. Zhang,
and H. Isahara. 2008. Dependency parsing with
short dependency relations in unlabeled data. In
Proceedings of IJCNLP 2008.
Chen, WL., J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP
2009, pages 570?579, Singapore, August.
Corston-Oliver, S., A. Aue, Kevin. Duh, and Eric Ring-
ger. 2006. Multilingual dependency parsing using
bayes point machines. In HLT-NAACL2006.
Culotta, A. and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of
ACL 2004, pages 423?429.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
COLING 1996, pages 340?345.
Hall, Keith. 2007. K-best spanning tree parsing. In
Proc. of ACL 2007, pages 392?399, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Koo, T., X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
Li, Charles N. and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Marcus, M., B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
ticss, 19(2):313?330.
McDonald, R. and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models.
In Proceedings of EMNLP-CoNLL, pages 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
McDonald, Ryan. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nakagawa, Tetsuji. 2007. Multilingual dependency
parsing using global features. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 952?956.
Nakazawa, T., K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper NLP. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
Nivre, J. and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-
based dependency parsing. In Proc. of CoNLL
2004, pages 49?56.
Nivre, J., J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proc. of EMNLP 2009, pages
551?560, Singapore, August. Association for Com-
putational Linguistics.
Yamada, H. and Y. Matsumoto. 2003a. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Yamada, H. and Y. Matsumoto. 2003b. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Yu, K., D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automati-
cally constructed case structures. In Proceedings of
Coling 2008, pages 1049?1056, Manchester, UK,
August.
Zhang, Y. and S. Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562?571, Hon-
olulu, Hawaii, October.
Zhao, Hai, Yan Song, Chunyu Kit, and Guodong
Zhou. 2009. Cross language dependency parsing
using a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
134
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 783?793, Dublin, Ireland, August 23-29 2014.
Soft Cross-lingual Syntax Projection for Dependency Parsing
Zhenghua Li , Min Zhang?, Wenliang Chen
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University
{zhli13,minzhang,wlchen}@suda.edu.cn
Abstract
This paper proposes a simple yet effective framework of soft cross-lingual syntax projection to
transfer syntactic structures from source language to target language using monolingual treebanks
and large-scale bilingual parallel text. Here, soft means that we only project reliable dependencies
to compose high-quality target structures. The projected instances are then used as additional
training data to improve the performance of supervised parsers. The major issues for this
idea are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic
syntactic non-isomorphism between languages; 3) incomplete parse trees after projection. To
handle the first two issues, we propose to use a probabilistic dependency parser trained on the
target-language treebank, and prune out unlikely projected dependencies that have low marginal
probabilities. To make use of the incomplete projected syntactic structures, we adopt a new
learning technique based on ambiguous labelings. For a word that has no head words after
projection, we enrich the projected structure with all other words as its candidate heads as long
as the newly-added dependency does not cross any projected dependencies. In this way, the
syntactic structure of a sentence becomes a parse forest (ambiguous labels) instead of a single
parse tree. During training, the objective is to maximize the mixed likelihood of manually labeled
instances and projected instances with ambiguous labelings. Experimental results on benchmark
data show that our method significantly outperforms a strong baseline supervised parser and
previous syntax projection methods.
1 Introduction
During the past decade, supervised dependency parsing has made great progress. However, due to
the limitation of scale and genre coverage of labeled data, it is very difficult to further improve the
performance of supervised parsers. On the other hand, it is very time-consuming and labor-intensive to
manually construct treebanks. Therefore, lots of recent work has been devoted to get help from bilingual
constraints. The motivation behind are two-fold. First, a difficult syntactic ambiguity in one language
may be very easy to resolve in another language. Second, a more accurate parser on one language may
help an inferior parser on another language, where the performance difference may be due to the intrinsic
complexity of languages or the scale of accessible labeled resources.
Following the above research line, much effort has been done recently to explore bilingual constraints
for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing
of bitext, which can make use of structural correspondence features in both languages. Their method
needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et
al. (2009) compose useful parsing features based on word reordering information in source-language
sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language
sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with
target-language gold-standard dependency trees. All above work shows significant performance gain
?Correspondence author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
783
over monolingual counterparts. However, one potential disadvantage is that bilingual treebanks and
bitext with one-side annotation are difficult to obtain. Therefore, They usually conduct experiments on
treebanks with a few thousand sentences. To break this constraint, Chen et al. (2011) extend their work
in Chen et al. (2010a) and translate text of monolingual treebanks to obtain bilingual treebanks with a
statistical machine translation system.
This paper explores another line of research and aims to boost the state-of-the-art parsing accuracy
via syntax projection. Syntax projection typically works as follows. First, we train a parser on source-
language treebank, called a source parser. Then, we use the source parser to produce automatic syntactic
structures on the source side of bitext. Next, with the help of automatic word alignments, we project the
source-side syntactic structures into the target side. Finally, the target-side structures are used as gold-
standard to train new parsing models of target language. Previous work on syntax projection mostly
focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al.,
2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose
quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds
of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to
explore projected structures to further improve the performance of statistical parsers trained on full-scale
monolingual treebanks (see Section 4.4 for performance comparison).
The major issues for syntax projection are 1) errors from the source-language parser and unsupervised
word aligner; 2) intrinsic syntactic non-isomorphism between languages; 3) incomplete parse trees after
projection. Hwa et al. (2005) propose a simple projection algorithm based on the direct correspondence
assumption (DCA). They apply post-editing to the projected structures with a set of hand-crafted heuristic
rules, in order to handle some typical cross-lingual syntactic divergences. Similarly, Ganchev et al.
(2009) manually design several language-specific constrains during projection, and use projected partial
structures as soft supervision during training based on posterior regularization (Ganchev et al., 2010).
To make use of projected instances with incomplete trees, Spreyer and Kuhn (2009) propose a heuristic
method to adapt training procedures of dependency parsing. Instead of directly using incomplete trees
to train dependency parsers, Jiang et al. (2010) train a local dependency/non-dependency classifier on
projected syntactic structures, and use outputs of the classifier as auxiliary features to help supervised
parsers. One potential common drawback of above work is the lack of a systematic way to handle
projection errors and incomplete trees.
Different from previous work, this paper proposes a simple yet effective framework of soft syntax
projection for dependency parsing, and provides a more elegant and systematic way to handle the
above issues. First, we propose to use a probabilistic parser trained on target-language treebank, and
prune unlikely projected dependencies which have very low marginal probabilities. Second, we adopt
a new learning technique based on ambiguous labelings to make use of projected incomplete trees
for training. For a word that has no head words after projection, we enrich the projected structure
by adding all possible words as its heads as long as the newly-added dependency does not cross any
projected dependencies. In this way, the syntactic structure of a sentence becomes a parse forest
(ambiguous labelings) instead of a single parse tree. During training, the objective is to maximize
the mixed likelihood of manually labeled instances and projected instances with ambiguous labelings.
Experimental results on benchmark data show that our method significantly outperforms a strong baseline
supervised parser and previous syntactic projection methods.
2 Syntax Projection
Given an input sentence x = w
0
w
1
...w
n
, a dependency tree is d = {(h,m) : 0 ? h ? n, 0 < m ? n},
where (h,m) indicates a directed arc from the head word w
h
to the modifier w
m
, and w
0
is an artificial
node linking to the root of the sentence.
Syntax projection aims to project the dependency tree ds of a source-language sentence xs into the
dependency structure of its target-language translation x via word alignments a, where a word alignment
a
i
= z means the target-side word w
i
is aligned into the source-side word ws
z
, as depicted in Figure
1(a) and Figure 1(b). For simplicity, we avoid one-to-many alignments by keeping the one with highest
784
w0
things
1
I
2
did
3
w
0

1
Z
2
?
3
?
4
?
5
(a) Source tree and word alignments
w
0

1
Z
2
?
3
?
4
?
5
(b) Projected incomplete tree
w
0

1
Z
2
?
3
?
4
?
5
(c) Forest (ambiguous labelings)
Figure 1: Illustration of syntax projection from English to Chinese with a sentence fragment. The two
Chinese auxiliary words, ??
3
? (past tense marker) and ??
4
? (relative clause marker), are not aligned to
any English words.
marginal probability when the target word is aligned to multiple source words. We first introduce a
simple syntax projection approach based on DCA (Hwa et al., 2005), and then propose two extensions
to handle parsing and aligning errors and cross-lingual syntactic divergences.
Projection with DCA. If two target words w
i
and w
j
are aligned to two different source words ws
a
i
and
w
s
a
j
, and the two words compose a dependency in the source tree (a
i
, a
j
) ? d
s
, then add a dependency
(i, j) into the projected syntactic structure. For example, as shown in Figure 1(a), the two Chinese
words ?Z
2
? and ??
5
? are aligned to the two English words ?did
3
? and ?things
1
?, and the dependency
?things
1
ydid
3
? is included in the source tree. Therefore, we project the dependency into the target side
and add a dependency ?Z
2
x?
5
? into the projected structure, as shown in Figure 1(b). An obvious
drawback of DCA is that it may produce many wrong dependencies due to the errors in the automatic
source-language parse trees and word alignments. Even with manual parse trees and word alignments,
syntactic divergences between languages can also lead to projection errors.
Pruned with target-side marginals. To overcome the weakness of DCA, we propose to use target-
side marginal probabilities to constrain the projection process and prune obviously bad projections. We
train a probabilistic parser on an existing target-side treebank. For each projected dependency, we
compute its marginal probability with the target parser, and prune it off the projected structure if the
probability is below a pruning threshold ?
p
. Our study shows that dependencies with very low marginal
probabilities are mostly wrong (Figure 2).
Supplemented with target-side marginals. To further improve the quality of projected structures, we
add dependencies with high marginal probabilities according to the target parser. Specifically, if a target
word w
j
obtain a head word w
i
after projection, and if another word w
k
has higher marginal probability
than a supplement threshold ?
s
to be the head word of w
j
, then we also add the dependency (k, j) into
the projected structure. In other words, we allow one word to have multiple heads so that the projected
structure can cover more correct dependencies.
From incomplete tree to forest. Some words in the target sentence may not obtain any head words
after projection due to incomplete word alignments or the pruning process, which leads to incomplete
parse trees after projection. Also, some words may have multiple head words resulting from the
supplement process. To handle these issues, we first convert the projected structures into parse forests,
and then propose a generalized training technique based on ambiguous labelings to make use of the
projected instances. Specifically, if a word does not have head words after projection, we simply
add into the projected structure all possible words as its candidate heads as long as the newly-added
dependency does not cross any projected dependencies, as illustrated in Figure 1(c). We introduce three
new dependencies to compose candidate heads for the unattached word ??
3
?. Note that it is illegal to
add the dependency ?
1
y?
3
? since it would cross the projected dependency ?Z
2
x?
5
?.
785
3 Dependency Parsing with Ambiguous Labelings
In parsing community, two mainstream methods tackle the dependency parsing problem from different
perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view
the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005;
McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods
try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and
Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011).
3.1 Graph-based Dependency Parser (GParser)
We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic
parser, which is required to compute the marginal probabilities of dependencies and likelihood of both
manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors
the score of a dependency tree into scores of small subtrees p.
Score(x,d;w) = w ? f(x,d) =
?
p?d
Score(x,p;w) (1)
We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1
which defines the score of a dependency tree as:
Score(x,d;w) =
?
{(h,m)}?d
w
dep
? f
dep
(x, h,m) +
?
{(h,s),(h,m)}?d
w
sib
? f
sib
(x, h, s,m) (2)
where f
dep
(x, h,m) and f
sib
(x, h, s,m) are feature vectors corresponding to two kinds of subtree;
w
dep/sib
are the feature weight vectors; the dot product gives the scores contributed by the corresponding
subtrees. We adopt the state-of-the-art syntactic features proposed in Bohnet (2010).
3.2 Probabilistic CRF-based GParser
Previous work on dependency parsing mostly adopts linear models and online perceptron training, which
lack probabilistic explanations of dependency trees and likelihood of the training data. Instead, we build
a log-linear CRF-based probabilistic dependency parser, which defines the probability of a dependency
tree as:
p(d|x;w) =
exp{Score(x,d;w)}
Z(x;w)
; Z(x;w) =
?
d
?
?Y(x)
exp{Score(x,d
?
;w)} (3)
where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x.
3.3 Likelihood and Gradient of Training Data with Ambiguous Labelings
Traditional CRF models assume one gold-standard label for each training instance, which means each
sentence is labeled with a single parse tree in the case of parsing. To make use of projected instances
with ambiguous labelings, we propose to use a generalized training framework which allows a sentence
to have multiple parse trees (forest) as its gold-standard reference (Ta?ckstro?m et al., 2013). The goal
of the training procedure is to maximize the likelihood of the training data, and the model is updated to
improve the probabilities of parse forests, instead of single parse trees. In other words, the model has
the flexibility to distribute the probability mass among the parse trees inside the forest, as long as the
probability of the forest improves. In this generalized framework, a traditional instance labeled with a
single parse tree can be regarded as a special case that the forest contains only one parse tree.
The probability of a sentence x with ambiguous labelings F is defined as the sum of probabilities of
all parse tree d contained in the forest F :
p(F|x;w) =
?
d?F
p(d|x;w) (4)
1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve a little bit higher accuracy, but suffer from
higher time cost of O(n4) and system complexity. Our method is applicable to the third-order model.
786
Train Dev Test
PTB 39,832 1,346 2416
CTB5 16,091 803 1,910
CTB5X 18,104 352 348
Bitext 0.9M ? ?
Table 1: Data sets (in sentence number).
Suppose the training data set is D = {(x
i
,F
i
)}
N
i=1
. Then the log likelihood of D is:
L(D;w) =
N
?
i=1
log p(F
i
|x
i
;w) (5)
Then we can derive the partial derivative of the log likelihood with respect to w:
?L(D;w)
?w
=
N
?
i=1
(
?
d?F
i
p?(d|x
i
,F
i
;w)f(x
i
,d) ?
?
d?Y(x
i
)
p(d|x
i
;w)f(x
i
,d)
)
(6)
where p?(d|x
i
,F
i
;w) is the probability of d under the space constrained by the parse forest F
i
:
p?(d|x
i
,F
i
;w) =
exp{Score(x
i
,d;w)}
Z(x
i
,F
i
;w)
; Z(x
i
,F
i
;w) =
?
d?F
i
exp{Score(x
i
,d;w)} (7)
The first term in Eq. (6) is the model expectations in the search space constrained by F
i
, and the second
term is the model expectations in the complete search space Y(x
i
). Since Y(x
i
) contains exponentially
many legal dependency trees, direct calculation of the second term is prohibitive. Instead, we can use the
classic Inside-Outside algorithm to efficiently compute the second term within O(n3) time complexity,
where n is the length of the input sentence. Similarly, the first term can be solved by running the Inside-
Outside algorithm in the constrained search space F
i
.
3.4 Stochastic Gradient Descent (SGD) Training
With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature
weights w for our CRF-based baseline and bitext-enhanced parsers. We follow the implementation
in CRFsuite.2 At each step, the algorithm approximates a gradient with a small subset of training
examples, and then updates the feature weights. Finkel et al. (2008) show that SGD achieves optimal
test performance with far fewer iterations than other optimization routines such as L-BFGS. Moreover,
it is very convenient to parallel SGD since computation among examples in the same batch is mutually
independent.
Once the feature weights w are learnt, we can parse the test data and try to find the optimal parse tree
with the Viterbi decoding algorithm in O(n3) parsing time (Eisner, 2000; McDonald and Pereira, 2006).
d
?
= arg max
d?Y(x)
p(d|x;w) (8)
4 Experiments and Analysis
To verify the effectiveness of our proposed method, we carry out experiments on English-to-Chinese
syntax projection, and aim to enhance our baseline Chinese parser with additional training instances
projected from automatic English parse trees on bitext. For monolingual treebanks, we use Penn
English Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the standard
practice to split the data into training (sec 02-21), development (sec 22), and test (sec 23). For CTB5, we
adopt the data split of (Duan et al., 2007). We convert the original bracketed structures into dependency
2http://www.chokkan.org/software/crfsuite/
787
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
UA
S 
& 
Pe
rc
en
t (%
)
Marginal Probability Interval
Percent
UAS
Figure 2: Distribution (Percent) and accuracy (UAS) of dependencies under different marginal
probability interval for Chinese baseline parser on CTB5 development set. For example, 0.8 at x-axis
means the interval [0.8, 0.9).
structures using Penn2Malt with its default head-finding rules. We build a CRF-based bigram part-
of-speech (POS) tagger with the features described in (Li et al., 2012b), and produce POS tags for
all train/development/test datasets and bitext (10-way jackknifing for training datasets). The tagging
accuracy on test sets is 97.3% on English and 94.0% on Chinese.
To compare with the recent work on syntax projection of Jiang et al. (2010) who use a smaller test
dataset, we follow their data split of CTB5 and use gold-standard POS tags during training and test. We
refer to this setting as CTB5X.
For bitext, we collect a parallel corpus from FBIS news (LDC03E14, 0.25M sentence pairs), United
Nations (LDC04E12, 0.62M), IWSLT2008 (0.04M), and PKU-863 (0.2M). After corpus cleaning, we
obtain a large-scale bilingual parallel corpus containing 0.9M sentence pairs. We run the unsupervised
BerkeleyAligner3 (Liang et al., 2006) for 4 iterations to obtain word alignments. Besides hard
alignments, we also make use of posterior probabilities to simplify one-to-many alignments to one-to-one
as discussed in Section 2. Table 1 shows the data statistics.
For training both the baseline and bitext-enhanced parsers, we set the batch size to 100 and run SGD
until a maximum iteration number of 50 is met or the change on likelihood of training data becomes too
small. Since the number of projected sentences is much more than that of manually labeled instances
(0.9M vs. 16K), it is likely that the projected data may overwhelm manually labeled data during training.
Therefore, we adopt a simple corpus-weighting strategy. Before each iteration, we randomly sample 50K
projected sentences and 15K manually labeled sentences from all training data, and run SGD to train
feature weights using the sampled data. To speed up training, we adopt multi-thread implementation of
gradient computations in the same batch. It takes about 1 day to train our bitext-enhanced parser for one
iteration using a single CPU core, while using 24 CPU cores only needs about 2 hours.
We measure parsing performance using unlabeled attachment score (UAS, percent of words with
correct heads), excluding punctuation marks. For significance test, we adopt Dan Bikel?s randomized
parsing evaluation comparator (Noreen, 1989).4
4.1 Analysis on Marginal Probabilities
In order to gain insights for parameter settings of syntax projection, we analyse the distribution and
accuracy of dependencies under different marginal probability interval. We train the baseline Chinese
parser on CTB5 train set, and use the parser to produce the marginal probabilities of all dependencies
for sentences in CTB5 development set. We discard all dependencies that have a marginal probability
less than 0.0001 for better illustration. Figure 2 shows the results, where we can see that UAS is roughly
proportional to marginal probabilities. In other word, dependencies with higher marginal probabilities
are more accurate. For example, dependencies with probabilities under interval [0.8, 0.9) has a 80%
chance to be correct. From another aspect, we can see that 50% of dependencies fall in probability
3http://code.google.com/p/berkeleyaligner/
4http://www.cis.upenn.edu/
?
dbikel/software.html
788
 77
 78
 79
 80
 81
 82
 83
 84
 1  6  11  16  21  26  31  36  41  46
UA
S 
(%
)
Iteration Number
Supervised
DCA (0.0 1.0)
DCA Pruned (0.1 1.0)
DCA Pruned (0.5 1.0)
(a) Parameters for DCA and DCA Pruned
 77
 78
 79
 80
 81
 82
 83
 84
 1  6  11  16  21  26  31  36  41  46
UA
S 
(%
)
Iteration Number
Supervised
(0.1 0.5)
(0.1 0.6)
(0.1 0.8)
(b) Parameters for DCA Pruned & Supplemented
Figure 3: Performance with different parameter settings of (?
p
?
s
) on CTB5 development set.
interval [0, 0.1), and such dependencies have very low accuracy (4%). These observations are helpful for
our parameter selection and methodology study during syntax projection.
4.2 Results of Syntax Projection on Development Dataset
We apply the syntax projection methods described in Section 2 to the bilingual text, and use the projected
sentences with ambiguous labelings as additional training instances to train new Chinese parsers based on
the framework described in Section 3. Figure 3 shows the UAS curves on development set with different
parameters settings. The pruning threshold ?
p
(see Section 2) balances the quality and coverage of
projection. Larger ?
p
leads to more accurate but fewer projections. The supplement threshold ?
s
(see
Section 2) balances the size and oracle score of the projected forest. Smaller ?
s
can increase the oracle
score of the forest by adding more dependencies with lower marginal probabilities, but takes the risk of
making the resulted forest too ambiguous and weak to properly supervise the model during training. 5
The DCA method corresponds to the results with ?
p
= 0.0 and ?
s
= 1.0. We can see that DCA
largely decreases UAS compared with the baseline CRF-based parser. The reason is that although DCA
projects many source-language dependencies to the target side (44% of target-language words obtain
head words), it also introduces a lot of noise during projection.
DCA pruned with target-side marginals corresponds to the results with ?
p
> 0.0 and ?
s
= 1.0.
Pruning with target-side marginals can clearly improve the projection quality by pruning out bad
projections. When ?
p
= 0.1, 31% of target-language words obtain head words, and the model
outperforms the baseline parser by 0.6% at peak UAS. When ?
p
= 0.5, the projection ratio decreases to
26% and the improvement is 0.3%. Based on the results, we choose ?
p
= 0.1 in later experiments.
Figure 3(b) presents the results of DCA pruned & supplemented with different ?
s
. The supplement
process adds a small amount of dependencies of high probabilities into the projected forest and therefore
increases the oracle score, which provides the model with flexibility to distribute the probability mass to
more preferable parse trees. We can see that although the peak UAS does not increase much, the training
curve is more smooth and stable than that without supplement. Based on the results, we choose ?
s
= 0.6
in later experiments.
4.3 Final Results and Comparisons on Test Dataset
Table 2 presents the final results on CTB5 test set. For each parser, we choose the parameters
corresponding to the iteration number with highest UAS on development set. To further verify the
usefulness of syntax projection, we also conduct experiments with self-training, which is known as a
typical semi-supervised method. For the standard self-training, we use Chinese-side bitext with self-
predicted parse trees produced by the baseline parser as additional training instances, which turns out
to be hurtful to parsing performance. This is consistent with earlier results (Spreyer and Kuhn, 2009).
5Please note when ?
p
+?
s
>= 1, ?
s
becomes useless. The reason is that if the probability of a projected dependency (i, j)
is larger ?
p
, then no other word beside w
i
can have a probability larger than ?
s
of being the head word of w
j
.
789
UAS
Baseline Supervised Parser 81.04
Standard Self-training 80.51 (-0.53)
Self-training with Ambiguous Labelings 81.09 (+0.05)
DCA 78.70 (-2.34)
DCA Pruned 81.46 (+0.42 ?)
DCA Pruned & Supplemented 81.71 (+0.67 ?)
Table 2: UAS on CTB5 test set. ? indicate statistical significance at confidence level of p < 0.01.
Supervised Bitext-enhanced
Jiang et al. (2010) 87.15 87.65 (+0.50)
This work 89.62 90.50 (+0.88 ?)
Table 3: UAS on CTB5X test set. ? indicate statistical significance at confidence level of p < 0.01.
Then, we try a variant of self-training with ambiguous labelings following the practice in Ta?ckstro?m
et al. (2013), and use a parse forest composed of dependencies of high probabilities as the syntactic
structure of an instance. We can see that ambiguous labelings help traditional self-training, but still have
no significant improvement over the baseline parser. Results in Table 2 indicate that our syntax projection
method is able to project useful knowledge from source-language parse trees to the target-side forest, and
then helps the target parser to learn effective features.
4.4 Comparisons with Previous Results on Syntax Projection on CTB5X
To make comparison with the recent work of Jiang et al. (2010), We rerun the process of syntax projection
with CTB5X as the target treebank with the DCA pruned & supplemented method (?
p
= 0.1 and ?
s
=
0.6).6 Table 3 shows the results. Jiang et al. (2010) employ the second-order MSTParser of McDonald
and Pereira (2006) with a basic feature set as their base parser. We can see that our baseline parser is
much stronger than theirs. Even though, our approach leads to larger UAS improvement.
This work is different from theirs in a few aspects. First, the purpose of syntax projection in their
work is to produce dependency/non-dependency instances which are used to train local classifiers to
produce auxiliary features for MSTParser. In contrast, the outputs of syntax projection in our work
are partial trees/forests where only reliable dependencies are kept and some words may receive more
than one candidate heads. We directly use these partial structures as extra training data to learn model
parameters. Second, their work measures the reliability of a projected dependencies only from the
perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side
marginal probabilities to throw away bad projections, which turns out effective in handling syntactic
non-isomorphism and errors in word alignments and source-side parses.
5 Related work
Cross-lingual annotation projection has been applied to many different NLP tasks to help processing
resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and
Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous
work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et
al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013).
Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train
delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext
(Zeman and Resnik, 2008; Cohen et al., 2011; S?gaard, 2011). Ta?ckstro?m et al. (2012) derive cross-
lingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively
sharing to better explore multi-source transfer information.
6In the previous draft of this paper, we directly use the projected data with in previous subsection for simplicity, and find
that UAS can reach 91.39% (+1.77). The reason is that the CTB5X test is overlapped with CTB5 train. We correct this mistake
in this version.
790
Our idea of training with ambiguous labelings is originally inspired by the work of Ta?ckstro?m et al.
(2013) on multilingual parser transfer for unsupervised dependency parsing. They use a delexicalized
parser trained on source-language treebank to obtain parse forests for target-language sentences, and re-
train a lexicalized target parser using the sentences with ambiguous labelings. Similar ideas of learning
with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and
sequence labeling problems (Dredze et al., 2009).
6 Conclusions
This paper proposes a simple yet effective framework of soft cross-lingual syntax projection. We
make use of large-scale projected structures as additional training instances to boost performance of
supervised parsing models trained on full-set manually labeled treebank. Compared with previous work,
we make two innovative contributions: 1) using the marginal probabilities of a target-side supervised
parser to control the projection quality with the existence of parsing and aligning errors and cross-lingual
syntax divergences; 2) adopting a new learning technique based ambiguous labelings to make use of
projected incomplete dependency trees for model training. Experimental results on two Chinese datasets
demonstrate the effectiveness of the proposed framework, and show that the bitext-enhanced parser
significantly outperforms all baselines, including supervised parsers, semi-supervised parsers based on
self-training, and previous syntax projection methods.
Our anonymous reviewers present many great comments, especially on the experimental section. We
will improve this work accordingly and release an extended version of this paper at the homepage of
the first author. Such extensions include: 1) further exploring source-language parsing probabilities and
alignment probabilities to help syntax projection; 2) studying the effect of the scale of source/target
treebank and bilingual text.
Acknowledgments
The authors would like to thank Wanxiang Che and Jiang Guo for sharing their bilingual data, and our
anonymous reviewers for their critical and insightful comments, which will certainly help our future
work. This work was supported by National Natural Science Foundation of China (Grant No. 61373095,
61203314, 61373097).
References
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COLING,
pages 89?97.
David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings
EMNLP, pages 877?886.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with
unannotated bilingual text. In Proceedings of CoNLL 2010, pages 46?54.
Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of
EMNLP/CoNLL, pages 141?150.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of NAACL 2013.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa. 2010a. Bitext dependency parsing with bilingual subtree
constraints. In Proceedings of ACL, pages 21?29.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010b. On jointly recognizing and aligning bilingual named
entities. In Proceedings of ACL 2010.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro Torisawa,
and Haizhou Li. 2011. SMT helps bitext dependency parsing. In EMNLP.
791
Shay B. Cohen, Dipanjan Das, , and Noah A. Smith. 2011. Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT 2011, pages 600?609.
Mark Dredze, Partha Pratim Talukdar, and Koby Crammer. 2009. Sequence learning from data with multiple
labels. In ECML/PKDD Workshop on Learning from Multi-Label Data.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency
parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 940?946.
Jason Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages 959?967.
Ruiji Fu, Bing Qin, and Ting Liu. 2011. Generating chinese named entity data from a parallel corpus. In
Proceedings of IJCNLP 2011, pages 264?272.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext
projection constraints. In Proceedings of ACL-IJCNLP 2009, pages 369?377.
Kuzman Ganchev, Jo ao Graca, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured
latent variable models. Journal of Artifical Intellignece Research.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In
Proceedings of EMNLP, pages 1222?1231.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Boostrapping parsers via
syntactic projection across parallel texts. Natural Language Engineering, 11(3):311?325.
Wenbin Jiang, , and Qun Liu. 2010. Dependency parsing and projection based on word-pair classification. In
ACL, pages 897?904.
Rong Jin and Zoubin Ghahramani. 2002. Learning with multiple labels. In Proceedings of NIPS.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In ACL, pages 1?11.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zeng, and Fei Huang. 2012a. Joint bilingual name tagging for parallel
corpora. In Proceedings of CIKM 2012.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012b. A separately passive-aggressive training algorithm
for joint POS tagging and dependency parsing. In COLING 2012, pages 1681?1698.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL.
Kai Liu, Yajuan Lu?, Wenbin Jiang, and Qun Liu. 2013. Bilingually-guided monolingual dependency grammar
induction. In Proceedings of ACL.
Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In
Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.
In Proceedings of EMNLP.
Tahira Naseem, Benjamin Snyder, Jacob Eisentein, and Regina Barzilay. 2009. Multilingual part-of-speech
tagging: two unsupervised approaches. Journal of Artifical Intellignece Research, 36(1):341?385.
Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency
parsing. In Proceedings of ACL.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages
149?160.
792
Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: An introduction. John Wiley & Sons,
Inc., New York.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. In ArXiv:1104.2086.
David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar
features. In Proceedings of EMNLP, pages 822?831.
Anders S?gaard. 2011. Data point selection for cross-language adaptation of dependency parsers. In Proceedings
of ACL 2011, pages 682?686.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven dependency parsing of new languages using incomplete and
noisy training data. In CoNLL, pages 12?20.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer
parsers. In Proceedings of NAACL, pages 1061?1071.
Mengqiu Wang, Wanxiang Che, and Christopher D. Manning. 2013. Joint word alignment and bilingual named
entity recognition using dual decomposition. In Proceedings of ACL 2013.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection
across aligned corpora. In Proceedings of NAACL 2001.
Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In
Proceedings of IJCNLP 2008.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
793
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 816?826, Dublin, Ireland, August 23-29 2014.
Feature Embedding for Dependency Parsing
Wenliang Chen
?
, Yue Zhang
?
, and Min Zhang
??
?
School of Computer Science and Technology, Soochow University, China
?
Singapore University of Technology and Design, Singapore
{wlchen, mzhang}@suda.edu.cn
yue zhang@sutd.edu.sg
Abstract
In this paper, we propose an approach to automatically learning feature embeddings to address
the feature sparseness problem for dependency parsing. Inspired by word embeddings, feature
embeddings are distributed representations of features that are learned from large amounts of
auto-parsed data. Our target is to learn feature embeddings that can not only make full use of
well-established hand-designed features but also benefit from the hidden-class representations
of features. Based on feature embeddings, we present a set of new features for graph-based
dependency parsing models. Experiments on the standard Chinese and English data sets show
that the new parser achieves significant performance improvements over a strong baseline.
1 Introduction
Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007;
Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of
rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and
Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative
power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k
sentences for dependency parsing), the chance of a feature occurring in the training data but not in the
test data can be high.
Another limitation on features is that many are typically derived by (manual) combination of atomic
features. For example, given the head word (w
h
) and part-of-speech tag (p
h
), dependent word (w
d
)
and part-of-speech tag (p
d
), and the label (l) of a dependency arc, state-of-the-art dependency parsers
can have the combined features: [w
h
; p
h
], [w
h
; p
h
;w
d
; p
d
], [w
h
; p
h
;w
d
], and so on, in addition to the
atomic features: [w
h
], [p
h
], etc. Such combination is necessary for high accuracies because the dominant
approach uses linear models, which can not capture complex correlations between atomic features.
We tackle the above issues by borrowing solutions from word representations, which have been in-
tensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of
words have been used for many NLP problems, which represent a word by information from the words
it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov
et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used
to reduce OOV rates in test data. In addition, since the representation of each word carries information
about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used
as additional semantic features (Koo et al., 2008).
In this paper, we show that a distributed representation can be learned for features also. Learned
from large amount of automatically parsed data, the representation of each feature can be defined on the
?
Corresponding author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
816
features it frequently co-occurs with. Similar to words, the feature representation can be used to reduce
the rate of unseen features in test data, and to capture inherent correlations between features. Borrowing
terminologies from word embeddings, we call the feature representation feature embeddings.
Compared with the task of learning word embeddings, the task of learning feature embeddings is
more difficult because the size of features is much larger than the vocabulary size and tree structures
are more complex than word sequences. This requires us to find an effective embedding format and an
efficient inference algorithm. Traditional LSA and RNN (Collobert et al., 2011; Bengio, 2009) models
turn out to be very slow for feature embeddings. Recently, Mikolov et al. (2013a) and Mikolov et al.
(2013b) introduce efficient models to learn high-quality word embeddings from extremely large amounts
of raw text, which offer a possible solution to the efficiency issue of learning feature embeddings. We
adapt their approach for learning feature embeddings, showing how an unordered feature context can
be used to learn the representation of a set of complex features. Using this method, a large number
of embeddings are trained from automatically parsed texts, based on which a set of new features are
designed and incorporated into a graph-based parsing model (McDonald and Nivre, 2007).
We conduct experiments on the standard data sets of the Penn English Treebank and the Chinese Tree-
bank V5.1. The results indicate that our proposed approach significantly improves parsing accuracies.
2 Background
In this section, we introduce the background of dependency parsing and build a baseline parser based on
the graph-based parsing model proposed by McDonald et al. (2005).
2.1 Dependency parsing
Given an input sentence x = (w
0
, w
1
, ..., w
i
, ..., w
m
), where w
0
is ROOT and w
i
(i ?= 0) refers to a
word, the task of dependency parsing is to find y
?
which has the highest score for x,
y
?
= argmax
y?Y (x)
score(x, y)
where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre
and McDonald, 2008): the transition-based model and graph-based model, which showed comparable
accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011;
Bohnet and Nivre, 2012). We apply feature embeddings to a graph-based model in this paper.
2.2 Graph-based parsing model
We use an ordered pair (w
i
, w
j
) ? y to define a dependency relation in tree y from word w
i
to word w
j
(w
i
is the head and w
j
is the dependent), and G
x
to define a graph that consists of a set of nodes V
x
=
{w
0
, w
1
, ..., w
i
, ..., w
m
} and a set of arcs (edges) E
x
= {(w
i
, w
j
)|i ?= j, w
i
? V
x
, w
j
? (V
x
? {w
0
})}.
The parsing model of McDonald et al. (2005) searches for the maximum spanning tree (MST) in G
x
.
We denote Y (G
x
) as the set of all the subgraphs of G
x
that are valid spanning trees (McDonald and
Nivre, 2007). The score of a dependency tree y ? Y (G
x
) is the sum of the scores of its subgraphs,
score(x, y) =
?
g?y
score(x, g) =
?
g?y
f(x, g) ? w (1)
where g is a spanning subgraph of y, which can be a single arc or adjacent arcs, f(x, g) is a high-
dimensional feature vector based on features defined over g and x, and w refers to the weights for the
features. In this paper we assume that a dependency tree is a spanning projective tree.
2.3 Baseline parser
We use the decoding algorithm proposed by Carreras (2007) and use the Margin Infused Relaxed Al-
gorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005) to train feature weights w. We
use the feature templates of Bohnet (2010) as our base feature templates, which produces state-of-the-art
accuracies. We further extend the features by introducing more lexical features to the base features. The
817
First-order
[wp]
h
, [wp]
d
, d(h, d)
[wp]
h
, d(h, d)
w
d
, p
d
, d(h, d)
[wp]
d
, d(h, d)
w
h
, p
h
, w
d
, p
d
, d(h, d)
p
h
, w
h
, p
d
, d(h, d)
w
h
, w
d
, p
d
, d(h, d)
w
h
, p
h
, [wp]
d
, d(h, d)
p
h
, p
b
, p
d
, d(h, d)
p
h
, p
h+1
, p
d?1
, p
d
, d(h, d)
p
h?1
, p
h
, p
d?1
, p
d
, d(h, d)
p
h
, p
h+1
, p
d
, p
d+1
, d(h, d)
p
h?1
, p
h
, p
d
, p
d+1
, d(h, d)
Second-order
p
h
, p
d
, p
c
, d(h, d, c)
w
h
, w
d
, c
w
, d(h, d, c)
p
h
, [wp]
c
, d(h, d, c)
p
d
, [wp]
c
, d(h, d, c)
Second-order (continue)
w
h
, [wp]
c
, d(h, d, c)
w
d
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
h+1
, [wp]
c
, d(h, d, c)
[wp]
h?1
, [wp]
h
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
h?1
, [wp]
h
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
h+1
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
h?1
, [wp]
h
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
h
, [wp]
h+1
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
d
, [wp]
d+1
, [wp]
c
, d(h, d, c)
[wp]
d?1
, [wp]
d
, [wp]
c
, d(h, d, c)
[wp]
d
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
d
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
d
, [wp]
d+1
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
d
, [wp]
d+1
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
d?1
, [wp]
d
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
d?1
, [wp]
d
, [wp]
c
, [wp]
c+1
, d(h, d, c)
Table 1: Base feature templates.
base feature templates are listed in Table 1, where h and d refer to the head, the dependent, respectively,
c refers to d?s sibling or child, b refers to the word between h and d, +1 (?1) refers to the next (previous)
word, w and p refer to the surface word and part-of-speech tag, respectively, [wp] refers to the surface
word or part-of-speech tag, d(h, d) is the direction of the dependency relation between h and d, and
d(h, d, c) is the directions of the relation among h, d, and c.
We train a parser with the base features and use it as the Baseline parser. Defining f
b
(x, g) as the base
features and w
b
as the corresponding weights, the scoring function becomes,
score(x, g) = f
b
(x, g) ? w
b
(2)
3 Feature Embeddings
Our goal is to reduce the sparseness of rich features by learning a distributed representation of features,
which is dense and low dimensional. We call the distributed feature representation feature embeddings.
In the representation, each dimension represents a hidden-class of the features and is expected to capture
a type of similarities or share properties among the features.
The key to learn embeddings is making use of information from a local context, and to this end
various methods have been proposed for learning word embeddings. Lin (1997) and Curran (2005) use
the count of words in a surrounding word window to represent distributed meaning of words. Brown
et al. (1992) uses bigrams to cluster words hierarchically. These methods have been shown effective
on words. However, the number of features is much larger than the vocabulary size, which makes it
infeasible to apply them on features. Another line of research induce word embeddings using neural
language models (Bengio, 2008). However, the training speed of neural language models is too slow for
the high dimensionality of features. Mikolov et al. (2013b) and Mikolov et al. (2013a) introduce efficient
methods to directly learn high-quality word embeddings from large amounts of unstructured raw text.
Since the methods do not involve dense matrix multiplications, the training speed is extremely fast.
We adapt the models of Mikolov et al. (2013b) and Mikolov et al. (2013a) for learning feature embed-
dings from large amounts of automatically parsed dependency trees. Since feature embeddings have a
high computational cost, we also use Negative sampling technique in the learning stage (Mikolov et al.,
2013b). Different from word embeddings, the input of our approach is features rather than words, and
the feature representations are generated from tree structures instead of word sequences. Consequently,
818
Figure 1: An example of one-step context. Figure 2: One-step surrounding features.
we give a definition of unordered feature contexts and adapt the algorithms of Mikolov et al. (2013b) for
feature embeddings.
3.1 Surrounding feature context
The most important difference between features and words is the contextual structure. Given a sentence
x =w
1
, w
2
, ..., w
n
and its dependency tree y, we define the M -step context as a set of relations reachable
within M steps from the current relation. Here one step refers to one dependency arc. For instance, the
one-step context includes the surrounding relations that can be reached in one arc, as shown in Figure 1.
In the figure, for the relation between ?with? and ?fork?, the relation between ?ate? and ?with? is in the
one-step context, while the relation between ?He? and ?ate? is in the two-step context because it can be
reached via the arc between ?ate? and ?with?. A larger M results in more contextual features and thus
might lead to a more accurate embedding, but at the expense of training speed.
Based on the M -step context, we use surrounding features to represent the features on the current
dependency relations. The surrounding features are defined on the relations in the M -step context. Take
1-step context as an example. Figure 2 shows the representations for the current relation between ?with?
and ?fork? in Figure 1. Given the current relation and the relations in its one-step context, we generate
the features based on the base feature templates. In Figure 2 the current feature ?f1:with, fork, R?
can be represented by the surrounding features ?cf1:ate, with, R? and ?cf1: fork, a, L? based on the
template ?T1:w
h
, w
d
, d(h, d)?. Similarly, all the features on the current relation are represented by the
features on the relations in the one-step context. To reduce computational cost, we generate for every
feature its contextual features based on the same feature template. As a result, the embeddings for each
feature template is trained separately. In the experiments, we use one-step context for learning feature
embeddings.
3.2 Feature Embedding model
We adapt the models of Mikolov et al. (2013b) and Mikolov et al. (2013a) to infer feature embeddings
(FE). Based on the representation of surrounding context, the input to the learning models is a set of
features and the output is feature embeddings as shown in Figure 3. For each dependency tree in large
amounts of auto-parsed data, we generate the base features and associate them with their surrounding
contextual features. Then all the base features are put into a set, which is used as the training instances
for learning models.
In the embedding model, we use the features on the current dependency arc to predict the surround-
ing features, as shown in Figure 4. Given sentences and their corresponding dependency trees Y , the
objective of the model is to maximize the conditional log-likelihood of context features,
?
y?Y
?
f?F
y
?
cf?CF
f
log(p(cf |f)) (3)
819
Figure 3: Input feature set. Figure 4: The feature embedding model.
where F
y
is a set of features generated from tree y, CF
f
is the set of surrounding features in the M -step
context of feature f . p(cf |f) can be computed by using the softmax function (Mikolov et al., 2013b),
for which the input is f and the output is cf ,
p(cf |f) =
exp(v
?
cf
T
v
f
)
?
F
i=1
exp(v
?
cf
i
T
v
f
)
(4)
where v
f
and v
?
f
are the input and output vector representations of f , and F is the number of features in
the feature table. The formulation is impractical for large data because the number of features is large
(in the millions) and the computational cost is too high.
To compute the probabilities efficiently, we use the Negative sampling method proposed by Mikolov
et al. (2013b), which approximates the probability by the correct example and K negative samples for
each instance. The formulation to compute log(p(cf |f)) is,
log ?(v
?
cf
T
v
f
) +
K
?
k=1
E
cf
k
?P (cf)
[log ?(?v
?
cf
k
T
v
f
)] (5)
where ?(z) = 1/(1 + exp(?z)) and P (f) is the noise distribution on the data. Following the setting of
Mikolov et al. (2013b), we set K to 5 in our experiments.
We predict the set of features one by one. Stochastic gradient ascent is used to perform the following
iterative update after predicting the i
th
feature,
? ? ? + ?(
?
?
cf
log(p(cf
i
|f)
??
) (6)
where ? is the learning rate and ? includes the parameters of the model and the vector representations
of features. The initial value of ? is 0.025. If the log-likelihood does not improve significantly after one
update, the rate is halved (Mikolov et al., 2009).
3.3 Distributed representation
Based on the proposed surrounding context, we use the feature embedding model with the help of the
Negative sampling method to learn feature embeddings. For each base template T
i
, the distributed rep-
resentations are stored in a matrixM
i
? R
d?|F
i
|
, where d is the number of dimensions (to be chosen
in the experiments) and |F
i
| is the size of the features F
i
for T
i
. For each feature f ? F
i
, its vector is
v
f
= [v
1
, ..., v
d
].
820
< j : T (f) ? ?(v
j
) > for j ? [1, d]
< j : T (f) ? ?(v
j
), w
h
> for j ? [1, d]
Table 2: FE-based templates.
4 Parsing with feature embeddings
In this section, we discuss how to apply the feature embeddings to dependency parsing.
4.1 FE-based feature templates
The base parsing model contains only binary features, while the values in the feature embedding repre-
sentation are real numbers that are not in a bounded range. If the range of the values is too large, they will
exert much more influence than the binary features. To solve this problem, we define a function ?(v
i
)
(details are given in Section 4.3) to convert real values to discrete values. The vector v
f
= [v
1
, ..., v
d
] is
converted into v
N
f
= [?(v
1
), ...,?(v
d
)].
We define a set of new feature templates for the parsing models, capturing feature embedding infor-
mation. Table 2 shows the new templates, where T (f) refers to the base template type of feature f . We
remove any new feature related to the surface form of the head if the word is not one of the Top-N most
frequent words in the training data. We used N=1000 for the experiments, which reduces the size of the
feature sets.
4.2 FE parser
We combine the base features with the new features by a new scoring function,
score(x, g) = f
b
(x, g) ? w
b
+ f
e
(x, g) ? w
e
(7)
where f
b
(x, g) refers to the base features, f
e
(x, g) refers to the FE-based features, and w
b
and w
e
are
their corresponding weights, respectively. The feature weights are learned during training using MIRA
(Crammer and Singer, 2003; McDonald et al., 2005).
We use the same decoding algorithm in the new parser as in the Baseline parser. The new parser is
referred to as the FE Parser.
4.3 Discretization functions
There are various functions to convert the real values in the vectors into discrete values. Here, we use a
simple method. First, for the i
th
base template, the values in the j
th
dimension are sorted in decreasing
order L
ij
. We divide the list into two parts for positive (L
ij+
) and negative (L
ij?
), respectively, and
define two functions. The first function is,
?
1
(v
j
) =
?
?
?
?
?
?
?
+B1 if v
j
is in top 50% in L
ij+
+B2 if v
j
is in bottom 50% in L
ij+
?B1 if v
j
is in top 50% in L
ij?
?B2 if v
j
is in bottom 50% in L
ij?
The second function is,
?
2
(v
j
) =
{
+B1 if v
j
is in top 50% in L
ij+
?B2 if v
j
is in bottom 50% in L
ij?
In ?
2
, we only consider the values (?+B1? and ?-B2?), which have strong values (positive or negative)
on each dimension, and omit the values which are close to zero. We refer the systems with ?
1
as M1
and the ones with ?
2
as M2. We also tried the original continuous values and the scaled values as used
by Turian et al. (2010), but the results were negative.
5 Experiments
We conducted experiments on English and Chinese data, respectively.
821
train dev test
PTB 2-21 22 23
CTB5 001-815 886-931 816-885
1001-1136 1148-1151 1137-1147
Table 3: Data sets of PTB and CTB5.
# of words # of sentences
BLLIP WSJ 43.4M 1.8M
Gigaword Xinhua 272.3M 11.7M
Table 4: Information of raw data.
 89.4
 89.6
 89.8
 90
 90.2
 90.4
 90.6
 90.8
 91
2 5 10 20 50
U
A
S
Sizes of dimensions
BaselineM1M2
Figure 5: Effect of different sizes of embeddings on the development data.
5.1 Data sets
We used the Penn Treebank (PTB) to generate the English data sets, and the Chinese Treebank version 5.1
(CTB5) to generate the Chinese data sets. ?Penn2Malt?
1
was used to convert the data into dependency
structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules
of Zhang and Clark (2008). The details of data splits are listed in Table 3, where the data partition of
Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011b; Hatori et al., 2011).
Following the work of Koo et al. (2008), we used a tagger trained on the training data to provide
part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate
part-of-speech tags for the training set. For English we used the MXPOST (Ratnaparkhi, 1996) tagger
and for Chinese we used a CRF-based tagger with the feature templates defined in Zhang and Clark
(2008). We used gold-standard segmentation in the CTB5 experiments. The accuracies of part-of-speech
tagging are 97.32% for English and 93.61% for Chinese on the test sets, respectively.
To obtain feature contexts, we processed raw data to obtain dependency trees. For English, we used the
BLLIP WSJ Corpus Release 1.
2
For Chinese, we used the Xinhua portion of Chinese Gigaword
3
Version
2.0 (LDC2009T14). The statistical information of raw data sets is listed in Table 4. The MXPOST part-
of-speech tagger and the Baseline dependency parser trained on the training data were used to process
the sentences of the BLLIP WSJ corpus. For Chinese, we need to perform word segmentation and part-
of-speech tagging before parsing. The MMA system (Kruengkrai et al., 2009) trained on the training
data was used to perform word segmentation and tagging, and the Baseline parser was used to parse the
sentences in the Gigaword corpus.
We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens
(excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete depen-
dency tree matches (COMP).
1
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
2
We excluded the texts of PTB from the BLLIP WSJ Corpus.
3
We excluded the texts of CTB5 from the Gigaword data.
822
UAS COMP
Baseline 92.78 48.08
Baseline+BrownClu 93.37 49.26
M2 93.74 50.82
Koo and Collins (2010) 93.04 N/A
Zhang and Nivre (2011) 92.9 48.0
Koo et al. (2008) 93.16 N/A
Suzuki et al. (2009) 93.79 N/A
Chen et al. (2009) 93.16 47.15
Zhou et al. (2011) 92.64 46.61
Suzuki et al. (2011) 94.22 N/A
Chen et al. (2013) 93.77 51.36
Table 5: Results on English data.
N/A=Not Available.
POS UAS COMP
Baseline 93.61 81.04 29.73
M2 93.61 82.94 31.72
Li et al. (2011a) 93.08 80.74 29.11
Hatori et al. (2011) 93.94 81.33 29.90
Li et al. (2012) 94.51 81.21 N/A
Chen et al. (2013) N/A 83.08 32.21
Table 6: Results on Chinese data.
N/A=Not Available.
5.2 Development experiments
In this section, we use the English development data to investigate the effects of different vector sizes
of feature embeddings, and compare the systems with the discretization functions ?
1
(M1) and ?
2
(M2)
(defined in Section 4.3), respectively. To reduce the training time, we used 10% of the labeled training
data to train the parsing models.
Turian et al. (2010) reported that the optimal size of word embedding dimensions was task-specific for
NLP tasks. Here, we investigated the effect of different sizes of embedding dimensions on dependency
parsing. Figure 5 shows the effect on UAS scores as we varied the vector sizes. The systems with FE-
based features always outperformed the Baseline. The curve of M2 was almost flat and we found that M1
performed worse as the sizes increased. Overall, M2 performed better than M1. For M2, 10-dimensional
embeddings achieved the highest score among all the systems. Based on the above observations, we
chose the following settings for further evaluations: 10-dimensional embeddings for M2.
5.3 Final results on English data
We trained the M2 model on the full training data and evaluated it on the English testing data. The
results are shown in Table 5. The parser using the FE-based features outperformed the Baseline. We
obtained absolute improvements of 0.96 UAS points. As for the COMP scores, M2 achieved absolute
improvement of 2.74 over the Baseline. The improvements were significant in McNemar?s Test (p <
10
?7
) (Nivre et al., 2004).
We listed the performance of the related systems in Table 5. We also added the cluster-based features
of Koo et al. (2008) to our baseline system listed as ?Baseline+BrownClu? in Table 5. From the table,
we found that our FE parsers obtained comparable accuracies with the previous state-of-the-art systems.
Suzuki et al. (2011) reported the best result by combining their method with the method of Koo et al.
(2008). We believe that the performance of our parser can be further enhanced by integrating their
methods.
5.4 Final results on Chinese data
We also evaluated the systems on the testing data for Chinese. The results are shown in Table 6. Sim-
ilar to the results on English, the parser using the FE-based features outperformed the Baseline. The
improvements were significant in McNemar?s Test (p < 10
?8
) (Nivre et al., 2004).
We listed the performance of the related systems
4
on Chinese in Table 6. From the table, we found
that the scores of our FE parser was higher than most of the related systems and comparable with the
results of Chen2013, which was the best reported scores so far.
4
We did not include the result (83.96) of Wu et al. (2013) because their part-of-speech tagging accuracy is 97.7%, much
higher than ours and other work. Their tagger includes rich external resources.
823
6 Related work
Learning feature embeddings are related to two lines of research: deep learning models for NLP, and
semi-supervised dependency parsing.
Recent studies used deep learning models in a variety of NLP tasks. Turian et al. (2010) applied
word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (2011) designed
a unified neural network to learn distributed representations that were useful for part-of-speech tagging,
chunking, NER, and semantic role labeling. They tried to avoid task-specific feature engineering. Socher
et al. (2013) proposed a Compositional Vector Grammar, which combined PCFGs with distributed word
representations. Zheng et al. (2013) investigated Chinese character embeddings for Chinese word seg-
mentation and part-of-speech tagging. Wu et al. (2013) directly applied word embeddings to Chinese
dependency parsing. In most cases, words or characters were the inputs to the learning systems and
word/character embeddings were used for the tasks. Our work is different in that we explore distributed
representations at the feature level and we can make full use of well-established hand-designed features.
We use large amounts of raw data to infer feature embeddings. There are several previous studies
relevant to using raw data for dependency parsing. Koo et al. (2008) used the Brown algorithm to learn
word clusters from a large amount of unannotated data and defined a set of word cluster-based features
for dependency parsing models. Suzuki et al. (2009) adapted a Semi-supervised Structured Conditional
Model (SS-SCM) to dependency parsing. Suzuki et al. (2011) reported the best results so far on the
standard test sets of PTB using a condensed feature representation combined with the word cluster-based
features of Koo et al. (2008). Chen et al. (2013) mapped the base features into predefined types using
the information of frequencies counted in large amounts of auto-parsed data. The work of Suzuki et al.
(2011) and Chen et al. (2013) were to perform feature clustering. Ando and Zhang (2005) presented
a semi-supervised learning algorithm named alternating structure optimization for text chunking. They
used a large projection matrix to map sparse base features into a small number of high level features over
a large number of auxiliary problems. One of the advantages of our approach is that it is simpler and
more general than that of Ando and Zhang (2005). Our approach can easily be applied to other tasks by
defining new feature contexts.
7 Conclusion
In this paper, we have presented an approach to learning feature embeddings for dependency parsing from
large amounts of raw data. Based on the feature embeddings, we represented a set of new features, which
was used with the base features in a graph-based model. When tested on both English and Chinese, our
method significantly improved the performance over the baselines and provided comparable accuracy
with the best systems in the literature.
Acknowledgments
Wenliang Chen was supported by the National Natural Science Foundation of China (Grant No.
61203314) and Yue Zhang was supported by MOE grant 2012-T2-2-163. We would also thank the
anonymous reviewers for their detailed comments, which have helped us to improve the quality of this
work.
References
R.K. Ando and T. Zhang. 2005. A high-performance semi-supervised learning method for text chunking. ACL.
Yoshua Bengio. 2008. Neural net language models. In Scholarpedia, page 3881.
Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and trends
R
? in Machine Learning,
2(1):1?127.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of EMNLP-CoNLL 2012, pages 1455?1465. Association
for Computational Linguistics.
824
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd
International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing, China, August.
Coling 2010 Organizing Committee.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, T. J. Watson, Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-Based n-gram Models of Natural Language. Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957?961, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Proceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Min Zhang, and Yue Zhang. 2013. Semi-supervised feature transformation for dependency
parsing. In Proceedings of EMNLP 2013, pages 1303?1313, Seattle, Washington, USA, October. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951?991.
James Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of the
43rd Annual Meeting of the Association for Computational Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan, June. Association for Computational Linguistics.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based chinese dependency parsing.
In Proceedings of ECML/ECPPKDD, Warsaw, Poland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint pos tagging and
dependency parsing in chinese. In Proceedings of 5th International Joint Conference on Natural Language
Processing, pages 1216?1224, Chiang Mai, Thailand, November. Asian Federation of Natural Language Pro-
cessing.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of ACL 2010,
pages 1?11, Uppsala, Sweden, July. Association for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of
ACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara.
2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In
Proceedings of ACL-IJCNLP2009, pages 513?521, Suntec, Singapore, August. Association for Computational
Linguistics.
Zhenghua Li, Wanxiang Che, and Ting Liu. 2011a. Improving chinese pos tagging with dependency parsing. In
Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1447?1451, Chiang
Mai, Thailand, November. Asian Federation of Natural Language Processing.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011b. Joint models for
chinese pos tagging and dependency parsing. In Proceedings of EMNLP 2011, UK, July.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012. A separately passive-aggressive training algo-
rithm for joint pos tagging and dependency parsing. In Proceedings of the 24rd International Conference on
Computational Linguistics (Coling 2012), Mumbai, India. Coling 2012 Organizing Committee.
Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics, pages 64?71, Madrid, Spain, July.
Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Pro-
ceedings of EMNLP-CoNLL, pages 122?131.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98. Association for Computational Linguistics.
825
Tomas Mikolov, Jiri Kopecky, Lukas Burget, Ondrej Glembek, and Jan Cernocky. 2009. Neural network based
language models for highly inflective languages. In Proceedings of ICASSP 2009, pages 4725?4728. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages
3111?3119.
J. Nivre and R. McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proc. of CoNLL 2004, pages 49?56.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared
task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages
915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector
grammars. In Proceedings of ACL 2013. Citeseer.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing. In Proceedings of EMNLP2009, pages 551?560, Singa-
pore, August. Association for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata. 2011. Learning condensed feature representations from large
unsupervised data sets for supervised learning. In Proceedings of ACL2011, pages 636?641, Portland, Oregon,
USA, June. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method
for semi-supervised learning. In Proceedings of ACL 2010, pages 384?394. Association for Computational
Linguistics.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dianhai Yu, Hua Wu, and Haifeng Wang. 2013. Generalization of
words for chinese dependency parsing. In Proceedings of IWPT 2013, pages 73?81.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based
dependency parsing. In Proceedings of EMNLP 2008, pages 562?571, Honolulu, Hawaii, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT2011, pages 188?193, Portland, Oregon, USA, June. Association for Computational Lin-
guistics.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and pos
tagging. In Proceedings of EMNLP 2013, pages 647?657. Association for Computational Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve
statistical dependency parsing. In Proceedings of ACL-HLT2011, pages 1556?1565, Portland, Oregon, USA,
June. Association for Computational Linguistics.
826
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 14?16, Dublin, Ireland, August 23-29 2014.
Dependency Parsing: Past, Present, and Future
Wenliang Chen and Zhenghua Li and Min Zhang
School of Computer Science and Technology,
Soochow University, China
{wlchen, Zhli13, mzhang}@suda.edu.cn
Dependency parsing has gained more and more interest in natural language processing in recent years
due to its simplicity and general applicability for diverse languages. The international conference of
computational natural language learning (CoNLL) has organized shared tasks on multilingual dependen-
cy parsing successively from 2006 to 2009, which leads to extensive progress on dependency parsing
in both theoretical and practical perspectives. Meanwhile, dependency parsing has been successfully
applied to machine translation, question answering, text mining, etc.
To date, research on dependency parsing mainly focuses on data-driven supervised approaches and
results show that the supervised models can achieve reasonable performance on in-domain texts for a
variety of languages when manually labeled data is provided. However, relatively less effort is devoted to
parsing out-domain texts and resource-poor languages, and few successful techniques are bought up for
such scenario. This tutorial will cover all these research topics of dependency parsing and is composed of
four major parts. Especially, we will survey the present progress of semi-supervised dependency parsing,
web data parsing, and multilingual text parsing, and show some directions for future work.
In the first part, we will introduce the fundamentals and supervised approaches for dependency pars-
ing. The fundamentals include examples of dependency trees, annotated treebanks, evaluation metrics,
and comparisons with other syntactic formulations like constituent parsing. Then we will introduce a few
mainstream supervised approaches, i.e., transition-based, graph-based, easy-first, constituent-based de-
pendency parsing. These approaches study dependency parsing from different perspectives, and achieve
comparable and state-of-the-art performance for a wide range of languages. Then we will move to the
hybrid models that combine the advantages of the above approaches. We will also introduce recent
work on efficient parsing techniques, joint lexical analysis and dependency parsing, multiple treebank
exploitation, etc.
In the second part, we will survey the work on semi-supervised dependency parsing techniques. Such
work aims to explore unlabeled data so that the parser can achieve higher performance. This tutorial
will present several successful techniques that utilize information from different levels: whole tree level,
partial tree level, and lexical level. We will discuss the advantages and limitations of these existing
techniques.
In the third part, we will survey the work on dependency parsing techniques for domain adaptation
and web data. To advance research on out-domain parsing, researchers have organized two shared tasks,
i.e., the CoNLL 2007 shared task and the shared task of syntactic analysis of non-canonical languages
(SANCL 2012). Both two shared tasks attracted many participants. These participants tried different
techniques to adapt the parser trained on WSJ texts to out-domain texts with the help of large-scale
unlabeled data. Especially, we will present a brief survey on text normalization, which is proven to be
very useful for parsing web data.
In the fourth part, we will introduce the recent work on exploiting multilingual texts for dependency
parsing, which falls into two lines of research. The first line is to improve supervised dependency parser
with multilingual texts. The intuition behind is that ambiguities in the target language may be unam-
biguous in the source language. The other line is multilingual transfer learning which aims to project the
syntactic knowledge from the source language to the target language.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
14
In the fifth part, we will conclude our talk by discussing some new directions for future work.
Outline
? Part A: Dependency parsing and supervised approaches
? A.1 Introduction to dependency parsing
? A.2 Supervised methods
? A.3 Non-projective dependency parsing
? A.4 Probabilistic and generative models for dependency parsing
? A.5 Other work
? Part B: Semi-supervised dependency parsing
? B.1 Lexical level
? B.2 Partial tree level
? B.3 Whole tree level
? B.4 Other work
? Part C: Parsing the web and domain adaptation
? C.1 CoNLL 2007 shared task (domain adaptation subtask)
? C.2 Works on domain adaptation
? C.3 SANCL 2012 (parsing the web)
? C.4 Text normalization
? C.5 Attempts and challenges for parsing the web
? Part D: Multilingual dependency parsing
? D.1 Dependency parsing on bilingual text
? D.2 Multilingual transfer learning for resource-poor languages
? D.3 Other work
? Part E: Conclusion and open problems
Instructors
Wenliang Chen received his Bachelor degree in Mechanical Engineering and Ph.D. degree in comput-
er science from Northeastern University (Shenyang, China) in 1999 and 2005, respectively. He joined
Soochow University since 2013 and is currently a professor in the university. Prior to joining Soochow
University, he was a research scientist in the Institute for Infocomm Research of Singapore from 2011
to 2013. From 2005 to 2010, he worked as an expert researcher in NICT, Japan. His current research
interests include parsing, machine translation, and machine learning. He is currently working on a syn-
tactic parsing project where he applies semi-supervised learning techniques to explore the information
from large-scale data to improve Dependency parsing. Based on the semi-supervised techniques, he de-
veloped a dependency parser, named DuDuPlus (http://code.google.com/p/duduplus/), for the research
and industry communities.
Zhenghua Li is currently an assistant professor in Soochow University. He received his PhD in Com-
puter Science and Technology from Harbin Institute of Technology (HIT) in April 2013. Zhenghuas
research interests include natural language processing and machine learning. More specifically, his
PhD research focuses on the dependency parsing of the Chinese language using discriminative machine-
learning approaches. He has been working on joint POS tagging and dependency parsing (EMNLP 2011,
COLING 2012), and multiple treebank exploitation for dependency parsing (ACL 2012).
Min Zhang, a distinguished professor and Director of the Research Center of Human Language Tech-
nology at Soochow University (China), received his Bachelor degree and Ph.D. degree in computer
15
science from Harbin Institute of Technology in 1991 and 1997, respectively. From 1997 to 1999, he
worked as a postdoctoral research fellow in Korean Advanced Institute of Science and Technology in
South Korea. He began his academic and industrial career as a researcher at Lernout & Hauspie Asia
Pacific (Singapore) in Sep. 1999. He joined Infotalk Technology (Singapore) as a researcher in 2001
and became a senior research manager in 2002. He joined the Institute for Infocomm Research (Singa-
pore) as a research scientist in Dec. 2003. His current research interests include machine translation,
natural language processing, information extraction, social network computing and Internet intelligence.
He has co-authored more than 150 papers in leading journals and conferences, and co-edited 10 book-
s/proceedings published by Springer and IEEE. He was the recipient of several awards in China and
oversea. He is the vice president of COLIPS (2011-2013), the elected vice chair of SIGHAN/ACL
(2014-2015), a steering committee member of PACLIC (2011-now), an executive member of AFNLP
(2013-2014) and a member of ACL (since 2006). He supervises Ph.D students at National University of
Singapore, Harbin Institute of Technology and Soochow University.
16
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73?83,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chen??, Jun?ichi Kazama?, Min Zhang?, Yoshimasa Tsuruoka??,
Yujie Zhang??, Yiou Wang?, Kentaro Torisawa? and Haizhou Li?
?Human Language Technology, Institute for Infocomm Research, Singapore
?National Institute of Information and Communications Technology (NICT), Japan
?School of Information Science, JAIST, Japan
?Beijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al, 2009; Zhao et al, 2009; Chen
et al, 2010). In bitext parsing, we can use the in-
formation based on ?bilingual constraints? (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al, 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
? We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al, 2009; Chen et al, 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
? We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al
(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
? Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
? We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!!
ROOT H hi hl d d h l f h f i h P Li!! e! g y!commen e !t e!resu ts!!o !!!t e!con erence!!!!w t !! eng
(a)
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!(b)Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle ??(de)/nominalizer? is placed after the
verb compound ???(peiyu)??(qilai)/cultivate?
to modify ???(jiqiao)/skill?. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of ???(jiqiao)/skill?.
The head may be ???(fahui)/demonstrate? or ??
?(peiyu)/cultivate?, as shown in Figure 2-(b) and
-(c), where (b) is correct.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiaoPN!!!!!!!VV!!!!!!!!!DT!!!!!!!!!!!!!!!NN!!!!!!!!!!!!!!!AD!!!!!!!!!!!!!!VV!!!!!!AD!!!!!!!VV!!!!VV DEC NN!!!!CC!!!NN(a)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(b)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(c)Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word ?that? is
a clue indicating the relative clause which shows the
relation between ?skill? and ?cultivate?, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word ?skill? corresponding to ???(jiqiao)/skill?
depends on the word ?demonstrate? corresponding
to ???(fahui)/demonstrate?, while the word ?cul-
tivate? corresponding to ???(peiyu)/cultivate? is a
grandchild of ?skill?. This is a positive evidence for
supporting ???(fahui)/demonstrate? as being the
head of ???(jiqiao)/skill?.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
He!hoped!that!all!the!athletes!would!!fully!demonstrate!the!strength!and!skill!that!they!cultivate!daily
Figure 3: Example of human translation
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word ?skills? corresponding
to ???(jiqiao)/skill? is a grandchild of the word
?play? corresponding to ???(fahui)/demonstrate?.
This is a positive evidence for supporting ??
?(fahui)/demonstrate? as being the head of ??
?(jiqiao)/skill?.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x, y) =
?
g?y
score(w, x, g) =
?
g?y
w ?f(x, g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x, g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the first- and second- order
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al, 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank.?
4.2 Bilingual constraint functions
In this paper, we focus on the first- and second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word ??
?(quanti)? depends on ????(yundongyuan)?
and word ?all? corresponding to ???(quanti)? de-
pends on word ?athletes? corresponding to ???
?(yundongyuan)?. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are ??(ta)?
and ???(xiwang)?, this time their corresponding
words ?He? and ?hope? do not have a direct depen-
dency relation. In this case, Fb2(rs2 : rt2)=?.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al, 2003). Figure 6
shows an example. The source word ???(jiqiao)?
depends on ???(fahui)? while its corresponding
word ?skills? indirectly depends on ?play? which
corresponds to ???(fahui)? via ?to?. In this case,
Fb2(rs2 : rt3)=+.
76
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 5: Example of bilingual constraints (2to2)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words ??
?(liliang)?, ??(he)?, and ???(jiqiao)? form a
parent-sibling structure, while their corresponding
words ?strength?, ?and?, and ?skills? also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3 : rt3)=+.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual reordering function: Fro
Huang et al (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [??(huitan), ??(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=?.
4.4 Original bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
?Fro?
?Fb2, Dir? ?Fb3, Dir?
?Fb2, Dir, Fro? ?Fb3, Dir, Fro?
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature ??Fb2(rs2 : rt3)=+, RIGHT ?? for
the second first-order feature template in Table 1.
5 Verified bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified constraint functions
5.1.1 Monolingual target subtrees
Chen et al (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b ht b h b k
ROOT!!He!!!!!bought!!!!!a!!!!book
e!!!!! oug oug t!! oo !
a book b ht b k!!!!!
(a) (b)
oug !!!a!!!!! oo !
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of ?all? and ?athletes? can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF .
5.1.3 Bilingual subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
? ? ? ?? ? ? ? ??ROOT! ?ta shi yi ming xuesheng
ROOT!!He!!!!!is!!!!!a!!!!!student He!!!!!is is!!!!!student
(a) (b)
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=?.
5.2 Verified bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features ?verified bilingual features?.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
??Fb2(rs2 : rt3) = +, Fvt(rt3) = MF,RIGHT ??
for the third first-order feature template and feature
??Fb2(rs2 : rt3)=+, Fvb(rbi23)=?, RIGHT ?? for
the fifth in Table 2.
First-order features Second-order features
?Fro?
?Fb2, Fvt(rtk)? ?Fb3, Fvt(rtk)?
?Fb2, Fvt(rtk), Dir? ?Fb3, Fvt(rtk), Dir?
?Fb2, Fvb(rbink)? ?Fb3, Fvb(rbink)?
?Fb2, Fvb(rbink), Dir? ?Fb3, Fvb(rbink), Dir?
?Fb2, Fro, Fvb(rbink)?
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al, 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool ?Penn2Malt?2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coder?s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al (2009) and Chen et al (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al, 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}?{?(de),?(le)} following the work
of Huang et al (2009).
To train an English parser, we used the PTB
(Marcus et al, 1993) in our experiments and the
tool ?Penn2Malt? to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al, 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al, 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al,
2009; Chen et al, 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemar?s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
U
A
S
Amount of training data (K)
Baseline1PAG1Baseline2PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the first- and second-order baseline systems,
and PAG1 and PAG2 refer to our first- and second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the first- and second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+STs 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI .
To achieve higher performance, we also added the
source subtree features (Chen et al, 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al (2009) and Chen et al (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877?886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21?29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17?24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222?
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49?56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180?1191,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Joint Models for Chinese POS Tagging and Dependency Parsing
Zhenghua Li?, Min Zhang?, Wanxiang Che?, Ting Liu?, Wenliang Chen? and Haizhou Li?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{lzh,car,tliu}@ir.hit.edu.cn
?Institute for Infocomm Research, Singapore
{mzhang,wechen,hli}@i2r.a-star.edu.sg
Abstract
Part-of-speech (POS) is an indispensable fea-
ture in dependency parsing. Current research
usually models POS tagging and dependency
parsing independently. This may suffer from
error propagation problem. Our experiments
show that parsing accuracy drops by about
6% when using automatic POS tags instead
of gold ones. To solve this issue, this pa-
per proposes a solution by jointly optimiz-
ing POS tagging and dependency parsing in a
unique model. We design several joint models
and their corresponding decoding algorithms
to incorporate different feature sets. We fur-
ther present an effective pruning strategy to re-
duce the search space of candidate POS tags,
leading to significant improvement of parsing
speed. Experimental results on Chinese Penn
Treebank 5 show that our joint models sig-
nificantly improve the state-of-the-art parsing
accuracy by about 1.5%. Detailed analysis
shows that the joint method is able to choose
such POS tags that are more helpful and dis-
criminative from parsing viewpoint. This is
the fundamental reason of parsing accuracy
improvement.
1 Introduction
In dependency parsing, features consisting of part-
of-speech (POS) tags are very effective, since pure
lexical features lead to severe data sparseness prob-
lem. Typically, POS tagging and dependency pars-
ing are modeled in a pipelined way. However, the
pipelined method is prone to error propagation, es-
pecially for Chinese. Due to the lack of morpholog-
ical features, Chinese POS tagging is even harder
than other languages such as English. The state-of-
the-art accuracy of Chinese POS tagging is about
93.5%, which is much lower than that of English
(about 97% (Collins, 2002)). Our experimental re-
sults show that parsing accuracy decreases by about
6% on Chinese when using automatic POS tagging
results instead of gold ones (see Table 3 in Section
5). Recent research on dependency parsing usually
overlooks this issue by simply adopting gold POS
tags for Chinese data (Duan et al, 2007; Zhang and
Clark, 2008b; Huang and Sagae, 2010). In this pa-
per, we address this issue by jointly optimizing POS
tagging and dependency parsing.
Joint modeling has been a popular and effec-
tive approach to simultaneously solve related tasks.
Recently, many successful joint models have been
proposed, such as joint tokenization and POS tag-
ging (Zhang and Clark, 2008a; Jiang et al, 2008;
Kruengkrai et al, 2009), joint lemmatization and
POS tagging (Toutanova and Cherry, 2009), joint
tokenization and parsing (Cohen and Smith, 2007;
Goldberg and Tsarfaty, 2008), joint named en-
tity recognition and parsing (Finkel and Manning,
2009), joint parsing and semantic role labeling
(SRL) (Li et al, 2010), joint word sense disambigua-
tion and SRL (Che and Liu, 2010), joint tokenization
and machine translation (MT) (Dyer, 2009; Xiao et
al., 2010) and joint parsing and MT (Liu and Liu,
2010). Note that the aforementioned ?parsing? all
refer to constituent parsing.
As far as we know, there are few successful mod-
els for jointly solving dependency parsing and other
tasks. Being facilitated by Conference on Com-
putational Natural Language Learning (CoNLL)
2008 and 2009 shared tasks, several joint models
of dependency parsing and SRL have been pro-
posed. Nevertheless, the top-ranked systems all
adopt pipelined approaches (Surdeanu et al, 2008;
1180
Hajic? et al, 2009). Theoretically, joint modeling
of POS tagging and dependency parsing should be
helpful to the two individual tasks. On the one hand,
syntactic information can help resolve some POS
ambiguities which are difficult to handle for the se-
quential POS tagging models. On the other hand,
more accurate POS tags should further improve de-
pendency parsing.
For joint POS tagging and dependency parsing,
the major issue is to design effective decoding algo-
rithms to capture rich features and efficiently search
out the optimal results from a huge hypothesis
space.1 In this paper, we propose several dynamic
programming (DP) based decoding algorithms for
our joint models by extending existing parsing algo-
rithms. We also present effective pruning techniques
to speed up our decoding algorithms. Experimen-
tal results on Chinese Penn Treebank show that our
joint models can significantly improve the state-of-
the-art parsing accuracy by about 1.5%.
The remainder of this paper is organized as fol-
lows. Section 2 describes the pipelined method, in-
cluding the POS tagging and parsing models. Sec-
tion 3 discusses the joint models and the decod-
ing algorithms, while Section 4 presents the pruning
techniques. Section 5 reports the experimental re-
sults and error analysis. We review previous work
closely related to our method in Section 6, and con-
clude this paper in Section 7.
2 The Baseline Pipelined Method
Given an input sentence x = w1...wn, we denote its
POS tag sequence by t = t1...tn, where ti ? T , 1 ?
i ? n, and T is the POS tag set. A dependency tree
is denoted by d = {(h,m) : 0 ? h ? n, 0 < m ?
n}, where (h,m) represents a dependency wh ?
wm whose head word (or father) is wh and modifier
(or child) is wm. w0 is an artificial root token which
is used to simplify the formalization of the problem.
The pipelined method treats POS tagging and de-
pendency parsing as two cascaded problems. First,
1It should be noted that it is straightforward to simultane-
ously do POS tagging and constituent parsing, as POS tags can
be regarded as non-terminals in the constituent structure (Levy
and Manning, 2003). In addition, Rush et al (2010) describes
an efficient and simple inference algorithm based on dual de-
composition and linear programming relaxation to combine a
lexicalized constituent parser and a trigram POS tagger.
an optimal POS tag sequence t? is determined.
t? = arg max
t
Scorepos(x, t)
Then, an optimal dependency tree d? is determined
based on x and t?.
d? = arg max
d
Scoresyn(x, t?,d)
2.1 POS Tagging
POS tagging is a typical sequence labeling prob-
lem. Many models have been successfully applied
to sequence labeling problems, such as maximum-
entropy (Ratnaparkhi, 1996), conditional random
fields (CRF) (Lafferty et al, 2001) and perceptron
(Collins, 2002). We use perceptron to build our POS
tagging baseline for two reasons. Firstly, as a linear
model, perceptron is simple, fast, and effective. It is
competitive to CRF in tagging accuracy but requires
much less training time (Shen et al, 2007). Sec-
ondly, perceptron has been successfully applied to
dependency parsing as well (Koo and Collins, 2010).
In this paper, perceptron is used in all models includ-
ing the POS tagging model, the dependency parsing
models and the joint models.
In a perceptron, the score of a tag sequence is
Scorepos(x, t) = wpos ? fpos(x, t)
where fpos(x, t) refers to the feature vector andwpos
is the corresponding weight vector.
For POS tagging features, we follow the work of
Zhang and Clark (2008a). Three feature sets are
considered: POS unigram, bigram and trigram fea-
tures. For brevity, we will refer to the three sets as
wi ti, ti?1 ti and ti?2 ti?1 ti.
Given wpos, we adopt the Viterbi algorithm to get
the optimal tagging sequence.
2.2 Dependency Parsing
Recently, graph-based dependency parsing has
gained more and more interest due to its state-of-
the-art accuracy. Graph-based dependency parsing
views the problem as finding the highest scoring tree
from a directed graph. Based on dynamic program-
ming decoding, it can efficiently find an optimal tree
in a huge search space. In a graph-based model, the
1181
score of a dependency tree is factored into scores of
small parts (subtrees).
Scoresyn(x, t,d) = wsyn ? fsyn(x, t,d)
=
?
p?d
Scoresyn(x, t, p)
where p is a scoring part which contains one or more
dependencies in the dependency tree d. Figure 1
shows different types of scoring parts used in current
graph-based models.
h m
dependency
h s
sibling
m g h
grandparent
m
h s
tri-sibling
mth s
grand-sibling
mg
Figure 1: Different types of scoring parts used in current
graph-based models (Koo and Collins, 2010).
Eisner (1996) proposes an O(n3) decoding al-
gorithm for dependency parsing. Based on the al-
gorithm, McDonald et al (2005) propose the first-
order model, in which the scoring parts only con-
tains dependencies. The second-order model of Mc-
Donald and Pereira (2006) incorporates sibling parts
and also needs O(n3) parsing time. The second-
order model of Carreras (2007) incorporates both
sibling and grandparent parts, and needs O(n4)
parsing time. However, the grandparent parts are
restricted to those composed of outermost grand-
children. Koo and Collins (2010) propose efficient
decoding algorithms of O(n4) for third-order mod-
els. In their paper, they implement two versions
of third-order models, Model 1 and Model 2 ac-
cording to their naming. Model 1 incorporates only
grand-sibling parts, while Model 2 incorporates both
grand-sibling and tri-sibling parts. Their experi-
ments on English and Czech show that Model 1 and
Model 2 obtain nearly the same parsing accuracy.
Therefore, we use Model 1 as our third-order model
in this paper.
We use three versions of graph-based dependency
parsing models.
? The first-order model (O1): the same with Mc-
Donald et al (2005).
? The second-order model (O2): the same with
Model 1 in Koo and Collins (2010), but without
using grand-sibling features.2
? The third-order model (O3): the same with
Model 1 in Koo and Collins (2010).
We adopt linear models to define the score of a de-
pendency tree. For the third-order model, the score
of a dependency tree is represented as:
Scoresyn(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s)(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
+
?
{(g,h),(h,s),(h,m)}?d
wgsib ? fgsib(x, t, g, h, s,m)
For the first- and second-order models, the above
formula is modified by deactivating extra parts.
For parsing features, we follow standard prac-
tice for graph-based dependency parsing (McDon-
ald, 2006; Carreras, 2007; Koo and Collins, 2010).
Since these features are highly related with our joint
decoding algorithms, we summarize the features as
follows.
? Dependency Features, fdep(x, t, h,m)
? Unigram Features: whth dir, wmtm dir
? Bigram Features: whth wmtm dir dist
? In Between Features: th tb tm dir dist
? Surrounding Features:
th?1 th th+1 tm?1 tm tm+1 dir dist
? Sibling Features, fsib(x, t, h, s,m)
wh th ws ts wm tm dir
? Grandparent Features, fgrd(x, t, g, h,m)
wg tg wh th wm tm dir gdir
? Grand-sibling Features, fgsib(x, t, g, h, s,m)
wg tg wh th ws ts wm tm dir gdir
2This second-order model incorporates grandparent features
composed of all grandchildren rather than just outermost ones,
and outperforms the one of Carreras (2007) according to the
results in Koo and Collins (2010).
1182
where b denotes an index between h and m; dir
and dist are the direction and distance of (h,m);
gdir is the direction of (g, h). We also use back-
off features by generalizing from very specific fea-
tures over word forms, POS tags, directions and dis-
tances to less sparse features over just POS tags or
considering fewer nodes. To avoid producing too
many sparse features, at most two word forms are
used at the same time in sibling, grandparent and
grand-sibling features, while POS tags are used in-
stead for other nodes; meanwhile, at most four POS
tags are considered at the same time for surrounding
features.
3 Joint Models
In the joint method, we aim to simultaneously solve
the two problems.
(t?, d?) = arg max
t,d
Scorejoint(x, t,d)
Under the linear model, the score of a tagged de-
pendency tree is:
Scorejoint(x, t,d) = Scorepos(x, t)
+ Scoresyn(x, t,d)
= wpos?syn ? fpos?syn(x, t,d)
where fpos?syn(.) means the concatenation of fpos(.)
and fsyn(.). Under the joint model, the weights of
POS and syntactic features, wpos?syn, are simulta-
neously learned. We expect that POS and syntactic
features can interact each other to determine an op-
timal joint result.
Similarly to the baseline dependency parsing
models, we define the first-, second-, and third-order
joint models according to the syntactic features con-
tained in fsyn(.).
In the following, we propose two versions of joint
models which can capture different feature sets and
have different complexity.
3.1 Joint Models of Version 1
The crucial problem for the joint method is to de-
sign effective decoding algorithms to capture rich
features and efficiently search out the optimal re-
sults from a huge hypothesis space. Eisner (2000)
describes a preliminary idea to handle polysemy by
extending parsing algorithms. Based on this idea,
we extend decoding algorithms of McDonald et al
(2005) and Koo and Collins (2010), and propose two
DP based decoding algorithms for our joint models
of version 1.
(b)
(a)
i r r j
r+1 ji ri j
i j
Figure 2: The DP structures and derivations of the first-
order decoding algorithm of joint models of version 1.
We omit symmetric right-headed versions for brevity.
Trapezoids denote incomplete spans. Triangles denote
complete spans. Solid circles denote POS tags of the cor-
responding indices.
The decoding algorithm of O1: As shown in
Figure 2, the first-order joint decoding algorithm
utilizes two types of dynamic programming struc-
tures. (1) Incomplete spans consist of a dependency
and the region between the head and modifier; (2)
Complete spans consist of a headword and its de-
scendants on one side. Each span is recursively cre-
ated by combining two smaller and adjacent spans
in a bottom-up fashion.
The pseudo codes are given in Algorithm 1.
I(i,j)(ti,tj) denotes an incomplete span from i to j
whose boundary POS tags are ti and tj . C(i,j)(ti,tj)
refers to a complete span from i to j whose bound-
ary POS tags are ti and tj . Conversely, I(j,i)(tj ,ti)
andC(j,i)(tj ,ti) represent spans of the other direction.
Note that in these notations the first argument index
always refers to the head of the span.
Line 6 corresponds to the derivation in Figure 2-
(a). Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)}) cap-
tures the joint features invented by this combina-
tion, where p = {(i, j)} means that the newly ob-
served scoring part is the dependency (i, j). The
syntactic features, denoted by fsyn(x, ti, tj , i, j), can
only incorporate syntactic unigram and bigram fea-
tures. The surrounding and in between features
are unavailable, because the context POS tags, such
as tb and ti?1, are not contained in the DP struc-
1183
Algorithm 1 The first-order joint decoding algorithm of version 1
1: ?0 ? i ? n, ti ? T C(i,i)(ti,ti) = 0 ? initialization
2: for w = 1..n do ? span width
3: for i = 0..(n? w) do ? span start index
4: j = i + w ? span end index
5: for (ti, tj) ? T 2 do
6: I(i,j)(ti,tj) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)})}7: I(j,i)(tj ,ti) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(j, i)})}8: C(i,j)(ti,tj) = maxi<r?j maxtr?T {I(i,r)(ti,tr) + C(r,j)(tr,tj) + Scorejoint(x, ti, tr, tj , p = ?)}
9: C(j,i)(tj ,ti) = maxi?r<j maxtr?T {C(r,i)(tr,ti) + I(j,r)(tj ,tr) + Scorejoint(x, ti, tr, tj , p = ?)}
10: end for
11: end for
12: end for
tures. Therefore, we adopt pseudo surrounding
and in between features by simply fixing the con-
text POS tags as the single most likely ones (Mc-
Donald, 2006). Taking the in between features
as an example, we use ti t?b tj dir dist instead,
where t?b is the 1-best tag determined by the base-
line POS tagger. The POS features, denoted by
fpos(x, ti, tr, tr+1, tj), can only incorporate all POS
unigram and bigram features.3 Similarly, we use
pseudo POS trigram features such as t?r?1 tr tr+1.
Line 8 corresponds to the derivation in Figure 2-
(b). Since this combination invents no scoring part
(p = ?), Scorejoint(x, ti, tr, tj , p = ?) is only com-
posed of POS features.4
Line 7 and Line 9 create spans in the opposite di-
rection, which can be analogously illustrated. The
space and time complexity of the algorithm are re-
spectively O(n2q2) and O(n3q4), where q = |T |.5
The decoding algorithm of O2 & O3: Figure
3 illustrates the second- and third-order decoding
algorithm of joint models of version 1. A new
kind of span, named the sibling span, is used to
capture sibling structures. Furthermore, each span
is augmented with a grandparent-index to capture
both grandparent and grand-sibling structures. It is
straightforward to derive the pseudo codes of the al-
3? wr tr if i ?= r; ? wr+1 tr+1 if r + 1 ?= j; ? tr tr+1
if r ?= i or r + 1 ?= j; ? ti tr if r ? 1 = i; ? tr+1 tj if
r + 2 = j. Note that wi ti, wj tj and ti tj (if i = j ? 1) are
not incorporated here to avoid double counting.
4? wr tr if r ?= j;? ti tr if i = r?1;? tr tj if r+1 = j.
Pseudo trigram features can be added accordingly.
5We can reduce the time complexity to O(n3q3) by strictly
adopting the DP structures in the parsing algorithm of Eisner
(1996). However, that may make the algorithm harder to com-
prehend.
ig j g i i ji+1
(a)
g i j g i k i k j
(b)
i k j i ik jr+1r
(c)
g i j g i r i r j
(d)
Figure 3: The DP structures and derivations of the
second- and third-order joint decoding algorithm of ver-
sion 1. For brevity, we elide the right-headed and right-
grandparented versions. Rectangles represent sibling
spans.
i j i r r j
i j i r r+1 j
(b)
(a)
Figure 4: The DP structures and derivations of the first-
order joint decoding algorithm of version 2. We omit the
right-headed version for brevity.
1184
gorithm from Figure 3. We omit them due to space
limitation. Pseudo surrounding, in between and POS
trigram features are used due to the same reason as
above. The space and time complexity of the algo-
rithm are respectively O(n3q3) and O(n4q5).
3.2 Joint Models of Version 2
To further incorporate genuine syntactic surround-
ing and POS trigram features in the DP structures,
we extend the algorithms of joint models of version
1, and propose our joint models of version 2.
The decoding algorithm of O1: Figure 4 illus-
trates the first-order joint decoding algorithm of ver-
sion 2. Compared with the structures in Figure 2,
each span is augmented with the POS tags surround-
ing the boundary indices. These context POS tags
enable Scorejoint(.) in line 6-9 of Algorithm 1 to
capture the syntactic surrounding and POS trigram
features, but also require enumeration of POS tags
over more indices. For brevity, we skip the pseudo
codes which can be easily derived from Algorithm
1. The space and time complexity of the algorithm
are respectively O(n2q6) and O(n3q10).
The decoding algorithm of O2 & O3: Using the
same idea as above, the second- and third-order joint
decoding algorithms of version 2 can be derived
based on Figure 3. Again, we omit both its DP struc-
tures and pseudo codes for the sake of brevity. Its
space and time complexity are respectively O(n3q7)
and O(n4q11).
In between features, which should be regarded as
non-local features in the joint situation, still cannot
be incorporated in our joint models of version 2.
Again, we adopt the pseudo version.
3.3 Comparison
Based on the above illustration, we can see that joint
models of version 1 are more efficient with regard
to the number of POS tags for each word, but fail to
incorporate syntactic surrounding features and POS
trigram features in the DP structures. On the con-
trary, joint models of version 2 can incorporate both
aforementioned feature sets, but have higher com-
plexity. These two versions of models will be thor-
oughly compared in the experiments.
4 Pruning Techniques
In this section, we introduce two pruning strategies
to constrain the search space of our models due to
their high complexity.
4.1 POS Tag Pruning
The time complexity of the joint decoding algorithm
is unbearably high with regard to the number of can-
didate POS tags for each word (q = |T |). We
find that it would be extremely time-consuming even
when we only use two most likely POS tags for each
word (q = 2) even for joint models of version 1.
To deal with this problem, we propose a pruning
method that can effectively reduce the POS tag space
based on a probabilistic tagging model.
We adopt a conditional log-linear model (Lafferty
et al, 2001), which defines a conditional distribution
of a POS tag sequence t given x:
P (t|x) = e
wpos?fpos(x,t)
?
t ewpos?fpos(x,t)
We use the same feature set fpos defined in Sec-
tion 2.1, and adopt the exponentiated gradient algo-
rithm to learn the weight vector wpos (Collins et al,
2008).
The marginal probability of tagging a word wi as
t is
P (ti = t|x) =
?
t:t[i]?t
P (t|x)
which can be efficiently computed using the
forward-backward algorithm.
We define pmaxi(x) to be the highest marginal
probability of tagging the word wi:
pmaxi(x) = maxt?T P (ti = t|x)
We then define the allowable candidate POS tags
of the word wi to be
Ti(x) = {t : t ? T , P (ti = t|x) ? ?t?pmaxi(x)}
where ?t is the pruning threshold. Ti(x) is used to
constrain the POS search space by replacing T in
Algorithm 1.
1185
4.2 Dependency Pruning
The parsing time grows quickly for the second- and
third-order models (both baseline and joint) when
the input sentence gets longer (O(n4)). Follow-
ing Koo and Collins (2010), we eliminate unlikely
dependencies using a form of coarse-to-fine prun-
ing (Charniak and Johnson, 2005; Petrov and Klein,
2007). On the development set, 68.87% of the de-
pendencies are pruned, while the oracle dependency
accuracy is 99.77%. We use 10-fold cross validation
to do pruning on the training set.
5 Experiments
We use the Penn Chinese Treebank 5.1 (CTB5) (Xue
et al, 2005). Following the setup of Duan et al
(2007), Zhang and Clark (2008b) and Huang and
Sagae (2010), we split CTB5 into training (secs 001-
815 and 1001-1136), development (secs 886-931
and 1148-1151), and test (secs 816-885 and 1137-
1147) sets. We use the head-finding rules of Zhang
and Clark (2008b) to turn the bracketed sentences
into dependency structures.
We use the standard tagging accuracy to evalu-
ate POS tagging. For dependency parsing, we use
word accuracy (also known as dependency accu-
racy), root accuracy and complete match rate (all
excluding punctuation) .
For the averaged training, we train each model for
15 iterations and select the parameters that perform
best on the development set.
5.1 Results of POS Tag Pruning
Figure 5 shows the distribution of words with dif-
ferent number of candidate POS tags and the k-best
oracle tagging accuracy under different ?t. To avoid
dealing with words that have many candidate POS
tags, we further apply a hard criterion that the decod-
ing algorithms only consider top k candidate POS
tags.
To find the best ?t, we train and evaluate the
second-order joint model of version 1 on the train-
ing and development sets pruned with different ?t
(top k = 5). We adopt the second-order joint model
of version 1 because of its efficiency compared with
the third-order models and its capability of captur-
ing rich features compared with the first-order mod-
els. The results are shown in Table 1. The model
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 >5
pro
por
tion
 of 
wor
ds (
%)
number of candidate POS tags 
0.1
0.01
0.001
93
94
95
96
97
98
99
100
1 2 3 4 5 ?
k-b
est 
ora
cle 
tagg
ing
 acc
ura
cy (
%)
k
0.1
0.01
0.001
Figure 5: Results of POS tag pruning with different prun-
ing threshold ?t on the development set.
?t word root compl. acc. speed
0.1 81.53 76.88 30.00 94.17 2.5
0.01 81.83 76.62 30.62 93.16 1.2
0.001 81.73 77.38 30.50 93.41 0.5
Table 1: Performance of the second-order joint model of
version 1 with different pruning threshold ?t (top k = 5)
on the development set. ?Acc.? means the tagging accu-
racy. ?Speed? refers to the parsing speed (the number of
sentences processed per second).
with ?t = 0.1 obtains the highest tagging accuracy,
which is much higher than that of both ?t = 0.01
and ?t = 0.001. However, its parsing accuracy
is inferior to the other two. ?t = 0.01 produces
slightly better parsing accuracy than ?t = 0.001,
and is twice faster. Finally, we choose ?t = 0.01
due to the efficiency factor and our priority over the
parsing accuracy.
Then we do experiments to find an optimal top
k. Table 2 shows the results. We decide to choose
k = 3 since it leads to best parsing accuracy.
From Table 1 and 2, we can have an interesting
finding: it seems that the harder we filter the POS
tag space, the higher tagging accuracy we get. In
other words, giving the joint model less flexibility
of choosing POS tags leads to better tagging per-
formance.
Due to time limitation, we do not tune ?t and k for
other joint models. Instead, we simply adopt ?t =
0.01 and top k = 3.
5.2 Final Results
Table 3 shows the final results on the test set. We list
a few state-of-the-art results in the bottom. Duan07
refers to the results of Duan et al (2007). They
enhance the transition-based parsing model with
1186
Syntactic Metrics Tagging Accuracy Parsing Speed
word root compl. all-word known unknown Sent/Sec
Joint Models V2
O3 80.79 75.84 29.11 92.80 93.88 76.80 0.3
O2 80.49 75.49 28.24 92.68 93.77 76.27 0.6
O1 77.37 68.64 23.09 92.96 94.05 76.64 2.0
Joint Models V1
O3 80.69 75.90 29.06 92.89 93.96 76.80 0.5
O2 80.74 75.80 28.24 93.08 94.11 77.53 1.7
O1 77.38 69.69 22.62 93.20 94.23 77.76 8.5
Auto POS
O3 79.29 74.65 27.24
93.51 94.36 80.78
2.0
O2 79.03 74.70 27.19 5.8
O1 75.68 68.06 21.10 17.4
MSTParser2 77.95 72.04 25.50 4.1
MSTParser1 75.84 68.55 21.36 5.2
MaltParser 75.24 65.92 23.19 2.6
Gold POS
O3 86.00 77.59 34.02
100.0 100.0 100.0
-
O2 86.18 78.58 34.07 -
O1 82.24 70.10 26.02 -
MSTParser2 85.24 77.41 33.19 -
MSTParser1 83.04 71.49 27.59 -
MaltParser 82.62 69.34 29.06 -
H&S10 85.20 78.32 33.72 -
Z&C08 single 84.33 76.73 32.79 -
Z&C08 hybrid 85.77 76.26 34.41 -
Duan07 83.88 73.70 32.70 -
Table 3: Final results on the test set. ?Gold POS? means that gold POS tags are used as input by the pipelined parsing
models; while ?Auto POS? means that the POS tags are generated by the baseline POS tagging model.
top k word root compl. acc. speed
2 81.46 76.12 30.50 93.51 2.7
3 82.11 76.75 29.75 93.31 1.7
4 81.75 76.62 30.38 93.25 1.4
5 81.83 76.62 30.62 93.16 1.2
Table 2: Performance of the second-order joint model of
version 1 with different top k (?t = 0.01) on the devel-
opment set.
the beam search. H&S10 refers to the results of
Huang and Sagae (2010). They greatly expand the
search space of the transition-based model by merg-
ing equivalent states with dynamic programming.
Z&C08 refers to the results of Zhang and Clark
(2008b). They use a hybrid model to combine the
advantages of both graph-based and transition-based
models. We also do experiments with two publicly
available and widely-used parsers, MSTParser6 and
MaltParser7. MSTParser1 refers to the first-order
6http://sourceforge.net/projects/mstparser/
7http://maltparser.org/
graph-based model of McDonald et al (2005), while
MSTParser2 is the second-order model of McDon-
ald and Pereira (2006). MaltParser is a transition-
based parsing system. It integrates a number of clas-
sification algorithms and transition strategies. We
adopt the support vector machine classifier and the
arc-standard strategy (Nivre, 2008).
We can see that when using gold tags, our
pipelined second- and third-order parsing models
achieve best parsing accuracy, which is even higher
than the hybrid model of Zhang and Clark (2008b).
It is a little surprising that the second-order model
slightly outperforms the third-order one. This may
be possible, since Koo and Collins (2010) shows that
the third-order model outperforms the second-order
one by only 0.32% on English and 0.07% on Czech.
In addition, we only use basic third-order features.
Both joint models of version 1 and 2 can consis-
tently and significantly improve the parsing accu-
racy by about 1.5% for all first-, second- and third-
order cases. Accidentally, the parsing accuracy of
the second-order joint model of version 2 is lower
1187
error pattern # ? error pattern # ?
DEC ? DEG 237 114 NR ? NN 184 100
NN ? VV 389 73 NN ? NR 106 91
DEG ? DEC 170 39 NN ? JJ 95 70
VV ? NN 453 27 VA ? VV 29 41
P ? VV 52 24 JJ ? NN 126 29
P ? CC 39 13 VV ? VA 67 10
Table 4: Error analysis of POS tagging. # means the
error number of the corresponding pattern made by the
baseline tagging model. ? and ? mean the error number
reduced or increased by the joint model.
than that of its counterparts by about 0.3%. More
experiments and further analysis may be needed to
find out the reason. The two versions of joint models
performs nearly the same, which indicates that using
pseudo surrounding and POS trigram features may
be sufficient for the joint method on this data set.
In summary, we can conclude that the joint frame-
work is certainly helpful for dependency parsing.
It is clearly shown in Table 3 that the joint
method surprisingly hurts the tagging accuracy,
which diverges from our discussion in Section 1.
Some insights into this issue will be given in Sec-
tion 5.3. Moreover, it seems that the more syntac-
tic features the joint method incorporates (from
O1 to O3), the more the tagging accuracy drops.
We suspect that this is because the joint models are
dominated by the syntactic features. Take the first-
order joint model as an example. The dimension of
the syntactic features fsyn is about 3.5 million, while
that of fpos is only about 0.5 million. The gap be-
comes much larger for the second- and third-order
cases.
Comparing the parsing speed, we can find that the
pruning of POS tags is very effective. The second-
order joint model of version 1 can parse 1.7 sen-
tences per second, while the pipelined second-order
parsing model can parse 5.8 sentences per second,
which is rather close considering that there is a fac-
tor of q5.
5.3 Error Analysis
To find out the impact of our joint models on the
individual tasks, we conduct detailed error analy-
sis through comparing the results of the pipelined
second-order parsing model and the second-order
joint model of version 1.
Impact on POS tagging: Table 4 shows how the
joint model changes the quantity of POS tagging er-
ror patterns compared with the pipelined model. An
error pattern ?X ? Y? means that the focus word,
whose true tag is ?X?, is assigned a tag ?Y?. We
choose these patterns with largest reduction or in-
crease in the error number, and rank them in de-
scending order of the variation.
From the left part of Table 4, we can see that
the joint method is clearly better at resolving tag-
ging ambiguities like {VV, NN} and {DEG, DEC}.8
One common characteristic of these ambiguous
pairs is that the local or even whole syntactic struc-
ture will be destructed if the wrong tag is chosen. In
other words, resolving these ambiguities is critical
and helpful from the parsing viewpoint. From an-
other perspective, the joint model is capable of pre-
ferring the right tag with the help of syntactic struc-
tures, which is impossible for the baseline sequential
labeling model.
In contrast, pairs like {NN, NR}, {VV, VA} and
{NN, JJ} only slightly influence the syntactic struc-
ture when mis-tagged. The joint method performs
worse on these ambiguous pairs, as shown in the
right part of Table 4.
Impact on parsing: Table 5 studies the change of
parsing error rates between the pipelined and joint
model on different POS tag patterns. We present the
most typical and prominent patterns in the table, and
rank them in descending order of X?s frequency of
occurrence. We also show the change of proportion
of different patterns, which is consistent with the re-
sults in Table 4.
From the table, we can see the joint model can
achieve a large error reduction (0.8?4.0%) for all
the patterns ?X ? X?. In other words, the joint
model can do better given the correct tags than
the pipelined method.
For all the patterns marked by ?, except for the
ambiguous pair {NN, JJ} (which we find is difficult
to explain even after careful result analysis), the joint
model also reduces the error rates (2.2?15.4%). As
8DEG and DEC are the two POS tags for the frequently used
auxiliary word ??? (de?, of) in Chinese. The associative ???
is tagged as DEG, such as ???/father ? ??/eyes (eyes of
the father)?; while the one in a relative clause is tagged as DEC,
such as ??/he ??/made ? ??/progress (progress that he
made)?.
1188
pattern pipelined jointprop (%) error (%) prop (%) error (%)
NN ? NN 94.6 16.8 -1.1 -1.8
? VV ? 2.9 55.5 -0.5 +15.1
? NR ? 0.8 24.5 +0.7 -2.2
? JJ ? 0.7 17.9 +0.5 +2.1
VV ? VV 89.6 34.2 -0.3 ?4.0
? NN ? 6.6 66.4 -0.4 +0.7
? VA ? 1.0 38.8 +0.1 -15.4
NR ? NR 91.7 15.4 -3.7 -0.8
? NN ? 5.9 21.7 +3.2 -3.7
P ? P 92.8 22.6 +3.4 -3.2
? VV ? 3.0 50.0 -1.4 +10.7
? CC ? 2.3 74.4 -0.7 +21.9
JJ ? JJ 80.5 11.2 -2.8 -2.0
? NN ? 9.8 18.3 +2.2 +1.8
DEG ? DEG 86.5 11.1 +2.8 -3.6
? DEC ? 13.5 61.8 -3.1 +37.4
DEC ? DEC 79.7 17.2 +12.1 -4.0
? DEG ? 20.2 56.5 -9.7 +40.2
Table 5: Comparison of parsing error rates on different
POS tag patterns between the pipelined and joint models.
Given a pattern ?X ? Y?, ?prop? means its proportion in
all occurrence of ?X? (Count(X?Y )Count(X) ), and ?error? refers
to its parsing error rate ( Count(wrongly headed X?Y )Count(X?Y ) ).
The last two columns give the absolute reduction (-) or
increase (+) in proportion and error rate made by the joint
model. ? marks the patterns appearing in the left part of
Table 4, while ? marks those in the right part of Table 4.
discussed earlier, these patterns concern ambiguous
tag pairs which usually play similar roles in syn-
tactic structures. This demonstrates that the joint
model can do better on certain tagging error pat-
terns.
For patterns marked by ?, the error rate of the
joint model usually increases by large margin. How-
ever, the proportion of these patterns is substantially
decreased, since the joint model can better resolve
these ambiguities with the help of syntactic knowl-
edge.
In summary, we can conclude that the joint model
is able to choose such POS tags that are more helpful
and discriminative from parsing viewpoint. This is
the fundamental reason of the parsing performance
improvement.
6 Related Work
Theoretically, Eisner (2000) proposes a preliminary
idea of extending the decoding algorithm for de-
pendency parsing to handle polysemy. Here, word
senses can be understood as POS-tagged words.
Koo and Collins (2010) also briefly discuss that their
third-order decoding algorithm can be modified to
handle word senses using the idea of Eisner (2000).
In his PhD thesis, McDonald (2006) extends his
second-order model with the idea of Eisner (2000)
to study the impact of POS tagging errors on pars-
ing accuracy. To make inference tractable, he uses
top 2 candidate POS tags for each word based on
a maximum entropy tagger, and adopts the single
most likely POS tags for the surrounding and in be-
tween features. He conducts primitive experiments
on English Penn Treebank, and shows that parsing
accuracy can be improved from 91.5% to 91.9%.
However, he finds that the model is unbearably time-
consuming.
7 Conclusions
In this paper, we have systematically investigated
the issue of joint POS tagging and dependency pars-
ing. We propose and compare several joint models
and their corresponding decoding algorithms which
can incorporate different feature sets. We also pro-
pose an effective POS tag pruning method which can
greatly improve the decoding efficiency. The experi-
mental results show that our joint models can signif-
icantly improve the state-of-the-art parsing accuracy
by more than 1.5%. Detailed error analysis shows
that the fundamental reason for the parsing accu-
racy improvement is that the joint method is able to
choose POS tags that are helpful and discriminative
from parsing viewpoint.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 60803093, 60975055, the Natural Scientific
Research Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069) and the Fun-
damental Research Funds for the Central Universi-
ties (HIT.KLOF.2010064).
References
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
1189
EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 161?169.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL 2007, pages 208?217.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 406?
414.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING 1996, pages 340?345.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326?334.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, Ohio, June. Association
for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897?904.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 439?446,
Sapporo, Japan, July. Association for Computational
Linguistics.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117.
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 707?715.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. In Computational Lin-
guistics, volume 34, pages 513?553.
1190
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
760?767, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 486?494.
Xinyan Xiao, Yang Liu, YoungSook Hwang, Qun Liu,
and Shouxun Lin. 2010. Joint tokenization and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1200?1208.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
1191
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303?1313,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semi-supervised Feature Transformation for Dependency Parsing
Wenliang Chen?, Min Zhang??, and Yue Zhang?
?School of Computer Science and Technology, Soochow University, China
?Singapore University of Technology and Design, Singapore
{wlchen, mzhang}@suda.edu.cn
yue zhang@sutd.edu.sg
Abstract
In current dependency parsing models, con-
ventional features (i.e. base features) defined
over surface words and part-of-speech tags
in a relatively high-dimensional feature space
may suffer from the data sparseness problem
and thus exhibit less discriminative power on
unseen data. In this paper, we propose a
novel semi-supervised approach to address-
ing the problem by transforming the base fea-
tures into high-level features (i.e. meta fea-
tures) with the help of a large amount of au-
tomatically parsed data. The meta features are
used together with base features in our final
parser. Our studies indicate that our proposed
approach is very effective in processing un-
seen data and features. Experiments on Chi-
nese and English data sets show that the fi-
nal parser achieves the best-reported accuracy
on the Chinese data and comparable accuracy
with the best known parsers on the English
data.
1 Introduction
In recent years, supervised learning models have
achieved lots of progress in the dependency pars-
ing task, as can be found in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al,
2007). The supervised models take annotated data
as training data, utilize features defined over surface
words, part-of-speech tags, and dependency trees,
and learn the preference of features via adjusting
feature weights.
?Corresponding author
In the supervised learning scenarios, many previ-
ous studies explore rich feature representation that
leads to significant improvements. McDonald and
Pereira (2006) and Carreras (2007) define second-
order features over two adjacent arcs in second-
order graph-based models. Koo and Collins (2010)
use third-order features in a third-order graph-based
model. Bohnet (2010) considers information of
more surrounding words for the graph-based mod-
els, while Zhang and Nivre (2011) define a set
of rich features including the word valency and
the third-order context features for transition-based
models. All these models utilize richer and more
complex feature representations and achieve better
performance than the earlier models that utilize the
simpler features (McDonald et al, 2005; Yamada
and Matsumoto, 2003; Nivre and Scholz, 2004).
However, the richer feature representations result in
a high-dimensional feature space. Features in such a
space may suffer from the data sparseness problem
and thus have less discriminative power on unseen
data. If input sentences contain unknown features
that are not included in training data, the parsers can
usually give lower accuracy.
Several methods have been proposed to alleviate
this problem by using large amounts of unannotated
data, ranging from self-training and co-training (Mc-
Closky et al, 2006; Sagae and Tsujii, 2007) to more
complex methods that collect statistical information
from unannotated sentences and use them as addi-
tional features (Koo et al, 2008; Chen et al, 2009).
In this paper, we propose an alternative approach
to semi-supervised dependency parsing via feature
transformation (Ando and Zhang, 2005). More
1303
specifically, we transform base features to a higher-
level space. The base features defined over surface
words, part-of-speech tags, and dependency trees
are high dimensional and have been explored in the
above previous studies. The higher-level features,
which we call meta features, are low dimensional,
and newly defined in this paper. The key idea be-
hind is that we build connections between known
and unknown base features via the meta features.
From another viewpoint, we can also interpret the
meta features as a way of doing feature smoothing.
Our feature transfer method is simpler than that of
Ando and Zhang (2005), which is based on splitting
the original problem into multiple auxiliary prob-
lems. In our approach, the base features are grouped
and each group relates to a meta feature. In the first
step, we use a baseline parser to parse a large amount
of unannotated sentences. Then we collect the base
features from the parse trees. The collected features
are transformed into predefined discrete values via a
transformation function. Based on the transformed
values, we define a set of meta features. Finally, the
meta features are incorporated directly into parsing
models.
To demonstrate the effectiveness of the proposed
approach, we apply it to the graph-based parsing
models (McDonald and Nivre, 2007). We conduct
experiments on the standard data split of the Penn
English Treebank (Marcus et al, 1993) and the Chi-
nese Treebank Version 5.1 (Xue et al, 2005). The
results indicate that the approach significantly im-
proves the accuracy. In summary, we make the fol-
lowing contributions:
? We define a simple yet useful transformation
function to transform base features to meta fea-
tures automatically. The meta features build
connections between known and unknown base
features, and relieve the data sparseness prob-
lem.
? Compared to the base features, the number of
meta features is remarkably small.
? We build semi-supervised dependency parsers
that achieve the best accuracy on the Chinese
data and comparable accuracy with the best
known systems on the English data.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the graph-based parsing model.
Section 3 describes the meta features and meta
parser. Section 4 describes the experiment settings
and reports the experimental results on English and
Chinese data sets. Section 5 discusses related work.
Finally, in Section 6 we summarize the proposed ap-
proach.
2 Baseline parser
In this section, we introduce a graph-based pars-
ing model proposed by McDonald et al (2005) and
build a baseline parser.
2.1 Graph-based parsing model
Given an input sentence, dependency parsing is
to build a dependency tree. We define X as
the set of possible input sentences, Y as the
set of possible dependency trees, and D =
(x1, y1), ..., (xi, yi), ..., (xn, yn) as a training set of
n pairs of xi ? X and yi ? Y . A sentence is de-
noted by x = (w0, w1, ..., wi, ..., wm), where w0 is
ROOT and does not depend on any other word and
wi refers to a word.
In the graph-based model, we define ordered pair
(wi, wj) ? y as a dependency relation in tree y from
word wi to word wj (wi is the head and wj is the
dependent), Gx as a graph that consists of a set of
nodes Vx = {w0, w1, ..., wi, ..., wm} and a set of
arcs (edges) Ex = {(wi, wj)|i ?= j, wi ? Vx, wj ?
(Vx?{w0})}. The parsing model of McDonald et al
(2005) is to search for the maximum spanning tree
(MST) in graph Gx. We denote Y (Gx) as the set
of all the subgraphs of Gx that are valid dependency
trees (McDonald and Nivre, 2007) for sentence x.
We define the score of a dependency tree y ?
Y (Gx) to be the sum of the subgraph scores,
score(x, y) =
?
g?y
score(x, g) (1)
where g is a spanning subgraph of y, which can be a
single arc or adjacent arcs. In this paper we assume
the dependency tree to be a spanning projective tree.
The model scores each subgraph using a linear rep-
resentation. Then scoring function score(x, g) is,
score(x, g) = f(x, g) ? w (2)
where f(x, g) is a high-dimensional feature vector
based on features defined over g and x and w refers
to the weights for the features.
1304
The maximum spanning tree is the highest scoring
tree in Y (Gx). The task of decoding algorithms in
the parsing model for an input sentence x is to find
y?, where
y? = argmax
y?Y (Gx)
score(x, y)
= argmax
y?Y (Gx)
?
g?y
score(x, g)
= argmax
y?Y (Gx)
?
g?y
f(x, g) ? w (3)
In our system, we use the decoding algorithm
proposed by Carreras (2007), which is a second-
order CKY-style algorithm (Eisner, 1996) and fea-
ture weights w are learned during training using the
Margin Infused Relaxed Algorithm (MIRA) (Cram-
mer and Singer, 2003; McDonald et al, 2005).
2.2 Base features
Previous studies have defined different sets of fea-
tures for the graph-based parsing models, such as
the first-order features defined in McDonald et al
(2005), the second-order parent-siblings features de-
fined in McDonald and Pereira (2006), and the
second-order parent-child-grandchild features de-
fined in Carreras (2007). Bohnet (2010) explorers
a richer set of features than the above sets. We fur-
ther extend the features defined by Bohnet (2010)
by introducing more lexical features as the base fea-
tures. The base feature templates are listed in Table
1, where h, d refer to the head, the dependent re-
spectively, c refers to d?s sibling or child, b refers
to the word between h and d, +1 (?1) refers to the
next (previous) word, w and p refer to the surface
word and part-of-speech tag respectively, [wp] refers
to the surface word or part-of-speech tag, d(h, d) is
the direction of the dependency relation between h
and d, and d(h, d, c) is the directions of the relation
among h, d, and c. We generate the base features
based on the above templates.
2.3 Baseline parser
We train a parser with the base features as the Base-
line parser. We define fb(x, g) as the base features
and wb as the corresponding weights. The scoring
function becomes,
score(x, g) = fb(x, g) ? wb (4)
3 Meta features
In this section, we propose a semi-supervised ap-
proach to transform the features in the base feature
space (FB) to features in a higher-level space (FM )
with the following properties:
? The features in FM are able to build connec-
tions between known and unknown features in
FB and therefore should be highly informative.
? The transformation should be learnable based
on a labeled training set and an automatically
parsed data set, and automatically computable
for the test sentences.
The features in FM are referred to as meta fea-
tures. In order to perform the feature transformation,
we choose to define a simple yet effective mapping
function. Based on the mapped values, we define
feature templates for generating the meta features.
Finally, we build a new parser with the base and
meta features.
3.1 Template-based mapping function
We define a template-based function for mapping
the base features to predefined discrete values. We
first put the base features into several groups and
then perform mapping.
We have a set of base feature templates TB . For
each template Ti ? TB , we can generate a set of
base features Fi from dependency trees in the parsed
data, which is automatically parsed by the Baseline
parser. We collect the features and count their fre-
quencies. The collected features are sorted in de-
creasing order of frequencies. The mapping function
for a base feature fb of Fi is defined as follows,
?(fb) =
?
?
?
?
?
?
?
Hi if R(fb) ? TOP10
Mi if TOP10 < R(fb) ? TOP30
Li if TOP30 < R(fb)
Oi Others
where R(fb) is the position number of fb in the
sorted list, ?Others? is defined for the base features
that are not included in the list, and TOP10 and TOP
30 refer to the position numbers of top 10% and top
30% respectively. The numbers, 10% and 30%, are
tuned on the development sets in the experiments.
For a base feature generated from template Ti, we
have four possible values: Hi, Mi, Li, and Oi. In
1305
(a) First-order standard
h[wp], d[wp], d(h,d)
h[wp], d(h,d)
dw, dp, d(h,d)
d[wp], d(h,d)
hw, hp, dw, dp, d(h,d)
hp, hw, dp, d(h,d)
hw, dw, dp, d(h,d)
hw, hp, d[wp], d(h,d)
(b) First-order Linear
hp, bp, dp, d(h,d)
hp, h+1p, d?1p, dp, d(h,d)
h?1p, hp, d?1p, dp, d(h,d)
hp, h+1p, dp, d+1p, d(h,d)
h?1p, hp, dp, d+1p, d(h,d)
(c) Second-order standard
hp, dp, cp, d(h,d,c)
hw, dw, cw, d(h,d,c)
hp, c[wp], d(h,d,c)
dp, c[wp], d(h,d,c)
hw, c[wp], d(h,d,c)
dw, c[wp], d(h,d,c)
(d) Second-order Linear
h[wp], h+1[wp], c[wp], d(h,d,c)
h?1[wp], h[wp], c[wp], d(h,d,c)
h[wp], c?1[wp], c[wp], d(h,d,c)
h[wp], c[wp], c+1[wp], d(h,d,c)
h?1[wp], h[wp], c?1[wp], c[wp], d(h,d,c)
h[wp], h+1[wp], c?1[wp], c[wp], d(h,d,c)
h?1[wp], h[wp], c[wp], c+1[wp], d(h,d,c)
h[wp], h+1[wp], c[wp], c+1[wp], d(h,d,c)
d[wp], d+1[wp], c[wp], d(h,d,c)
d?1[wp], d[wp], c[wp], d(h,d,c)
d[wp], c?1[wp], c[wp], d(h,d,c)
d[wp], c[wp], c+1[wp], d(h,d,c)
d[wp], d+1[wp], c?1[wp], c[wp], d(h,d,c)
d[wp], d+1[wp], c[wp], c+1[wp], d(h,d,c)
d?1[wp], d[wp], c?1[wp], c[wp], d(h,d,c)
d?1[wp], d[wp], c[wp], c+1[wp], d(h,d,c)
Table 1: Base feature templates
total, we have 4?N(TB) possible values for all the
base features, where N(TB) refers to the number of
the base feature templates, which is usually small.
We can obtain the mapped values of all the collected
features via the mapping function.
3.2 Meta feature templates
Based on the mapped values, we define meta fea-
ture templates in FM for dependency parsing. The
meta feature templates are listed in Table 2, where
fb is a base feature of FB , hp refers to the part-
of-speech tag of the head and hw refers to the sur-
face word of the head. Of the table, the first tem-
plate uses the mapped value only, the second and
third templates combine the value with the head in-
formation. The number of the meta features is rel-
atively small. It has 4 ? N(TB) for the first type,
4 ? N(TB) ? N(POS) for the second type, and
4 ?N(TB) ?N(WORD) for the third one, where
N(POS) refers to the number of part-of-speech
tags, N(WORD) refers to the number of words.
We remove any feature related to the surface form
if the word is not one of the Top-N most frequent
words in the training data. We used N=1000 for the
experiments for this paper. This method can reduce
the size of the feature sets. The empirical statistics
of the feature sizes at Section 4.2.2 shows that the
size of meta features is only 1.2% of base features.
[?(fb)]
[?(fb)], hp
[?(fb)], hw
Table 2: Meta feature templates
3.3 Generating meta features
We use an example to demonstrate how to gener-
ate the meta features based on the meta feature tem-
plates in practice. Suppose that we have sentence ?I
ate the meat with a fork.? and want to generate the
meta features for the relation among ?ate?, ?meat?,
and ?with?, where ?ate? is the head, ?meat? is the
dependent, and ?with? is the closest left sibling of
?meat?. Figure 1 shows the example.
We demonstrate the generating procedure using
template Tk = ?hw, dw, cw, d(h, d, c)? (the second
template of Table 1-(c) ), which contains the sur-
face forms of the head, the dependent, its sibling,
and the directions of the dependencies among h,
d, and c. We can have a base feature ?ate, meat,
with, RIGHTSIB?, where ?RIGHTSIB? refers to the
parent-siblings structure with the right direction. In
the auto-parsed data, this feature occurs 200 times
and ranks between TOP10 and TOP30. Accord-
1306
I ate the meat with a fork!!!! !!!! !!!! !!!! !!!! !!!! !!!!.
Tk:!hw,!dw,!cw,!d(h,d,c)
Fb:!ate,!meat,!with,!RIGHTSIB
" (fb)=Mk
[Mk];![Mk],!VV;![Mk],!ate
Figure 1: An example of generating meta features
ing to the mapping function, we obtain the mapped
value Mk. Finally, we have the three meta features
?[Mk]?, ?[Mk], V V ?, and ?[Mk], ate?, where V V is
the part-of-speech tag of word ?ate?. In this way,
we can generate all the meta features for the graph-
based model.
3.4 Meta parser
We combine the base features with the meta features
by a new scoring function,
score(x, g) = fb(x, g) ? wb + fm(x, g) ? wm (5)
where fb(x, g) refers to the base features, fm(x, g)
refers to the meta features, and wb and wm are
their corresponding weights respectively. The fea-
ture weights are learned during training using MIRA
(Crammer and Singer, 2003; McDonald et al,
2005). Note that wb is also retrained here.
We use the same decoding algorithm in the new
parser as in the Baseline parser. The new parser is
referred to as the meta parser.
4 Experiments
We evaluated the effect of the meta features for the
graph-based parsers on English and Chinese data.
4.1 Experimental settings
In our experiments, we used the Penn Treebank
(PTB) (Marcus et al, 1993) for English and the
Chinese Treebank version 5.1 (CTB5) (Xue et al,
2005) for Chinese. The tool ?Penn2Malt?1 was used
1http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
to convert the data into dependency structures with
the English head rules of Yamada and Matsumoto
(2003) and the Chinese head rules of Zhang and
Clark (2008). We followed the standard data splits
as shown in Table 3. Following the work of Koo et
al. (2008), we used a tagger trained on training data
to provide part-of-speech (POS) tags for the devel-
opment and test sets, and used 10-way jackknifing to
generate part-of-speech tags for the training set. We
used the MXPOST (Ratnaparkhi, 1996) tagger for
English and the CRF-based tagger for Chinese. We
used gold standard segmentation in the CTB5. The
data partition of Chinese were chosen to match pre-
vious work (Duan et al, 2007; Li et al, 2011; Hatori
et al, 2011).
train dev test
PTB 2-21 22 23
(sections)
CTB5 001-815 886-931 816-885
(files) 1001-1136 1148-1151 1137-1147
Table 3: Standard data splits
For the unannotated data in English, we used the
BLLIP WSJ corpus (Charniak et al, 2000) contain-
ing about 43 million words.2 We used the MXPOST
tagger trained on the training data to assign part-of-
speech tags and used the Baseline parser to process
the sentences of the Brown corpus. For the unanno-
tated data in Chinese, we used the Xinhua portion
of Chinese Gigaword3 Version 2.0 (LDC2009T14)
(Huang, 2009), which has approximately 311 mil-
lion words. We used the MMA system (Kruengkrai
et al, 2009) trained on the training data to perform
word segmentation and POS tagging and used the
Baseline parser to parse the sentences in the Giga-
word data.
In collecting the base features, we removed the
features which occur only once in the English data
and less than four times in the Chinese data. The
feature occurrences of one time and four times are
based on the development data performance.
We measured the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
2We ensured that the text used for building the meta features
did not include the sentences of the Penn Treebank.
3We excluded the sentences of the CTB data from the Giga-
word data.
1307
kens (excluding all punctuation tokens) with the cor-
rect HEAD.We also reported the scores on complete
dependency trees evaluation (COMP).
4.2 Feature selection on development sets
We evaluated the parsers with different settings on
the development sets to select the meta features.
4.2.1 Different models vs meta features
In this section, we investigated the effect of dif-
ferent types of meta features for the models trained
on different sizes of training data on English.
There are too many base feature templates to test
one by one. We divided the templates into several
categories. Of Table 1, some templates are only re-
lated to part-of-speech tags (P), some are only re-
lated to surface words (W), and the others contain
both part-of-speech tags and surfaces (M). Table 4
shows the categories, where numbers [1?4] refer to
the numbers of words involved in templates. For ex-
ample, the templates of N3WM are related to three
words and contain the templates of W and M. Based
on different categories of base templates, we have
different sets of meta features.4
Category Example
N1P hp, d(h, d)
N1WM hw, d(h, d); hw, hp, d(h, d)
N2P hp, dp, d(h, d)
N2WM hw, dw, d(h, d);
hw, dp, d(h, d)
N3P hp, dp, cp, d(h, d, c)
N3WM hw, dw, cw, d(h, d, c);
dw, d+1p, cp, d(h, d, c)
N4P hp, h+1p, cp, c+1p, d(h, d, c)
N4WM hw, h+1w, cw, c+1w, d(h, d, c);
hw, h+1p, cp, c+1p, d(h, d, c)
Table 4: Categories of base feature templates
We randomly selected 1% and 10% of the sen-
tences respectively from the training data. We
trained the POS taggers and Baseline parsers on
these small training data and used them to process
the unannotated data. Then, we generated the meta
features based on the newly auto-parsed data. The
4We also tested the settings of dividing WM into two sub-
types: W and M. The results showed that both two sub-types
provided positive results. To simplify, we merged W and M
into one category WM.
meta parsers were trained on the different subsets
of the training data with different sets of meta fea-
tures. Finally, we have three meta parsers: MP1,
MP10, MPFULL, which were trained on 1%, 10%
and 100% of the training data.
MP1 MP10 MPFULL
Baseline 82.22 89.50 93.01
+N1P 82.42 89.48 93.08
+N1WM 82.80 89.42 93.19
+N2P 81.29 89.01 93.02
+N2WM 82.69 90.10 93.23
+N3P 83.32 89.73 93.05
+N3WM 84.47 90.75 93.80
+N4P 82.73 89.48 93.01
+N4WM 84.07 90.42 93.67
OURS 85.11 91.14 93.91
Table 5: Effect of different categories of meta features
Table 5 shows the results, where we add each cat-
egory of Table 4 individually. From the table, we
found that the meta features that are only related to
part-of-speech tags did not always help, while the
ones related to the surface words were very helpful.
We also found that MP1 provided the largest relative
improvement among the three settings. These sug-
gested that the more sparse the base features were,
the more effective the corresponding meta features
were. Thus, we built the final parsers by adding
the meta features of N1WM, N2WM, N3WM, and
N4WM. The results showed that OURS achieved
better performance than the systems with individual
sets of meta features.
4.2.2 Different meta feature types
In Table 2, there are three types of meta feature
templates. Here, the results of the parsers with dif-
ferent settings are shown in Table 6, where CORE
refers to the first type, WithPOS refers to the sec-
ond one, and WithWORD refers to the third one.
The results showed that with all the types the parser
(OURS) achieved the best. We also counted the
numbers of the meta features. Only 327,864 (or
1.2%) features were added into OURS. Thus, we
used all the three types of meta features in our final
meta parsers.
1308
System NumOfFeat UAS
Baseline 27,119,354 93.01
+CORE +498 93.84
+WithPOS +14,993 93.82
+WithWORD +312,373 93.27
OURS +327,864 93.91
Table 6: Numbers of meta features
4.3 Main results on test sets
We then evaluated the meta parsers on the English
and Chinese test sets.
4.3.1 English
The results are shown in Table 7, where Meta-
Parser refers to the meta parser. We found that the
meta parser outperformed the baseline with an ab-
solute improvement of 1.01 points (UAS). The im-
provement was significant in McNemar?s Test (p
< 10?7 ).
UAS COMP
Baseline 92.76 48.05
MetaParser 93.77 51.36
Table 7: Main results on English
4.3.2 Chinese
UAS COMP
Baseline 81.01 29.71
MetaParser 83.08 32.21
Table 8: Main results on Chinese
The results are shown in Table 8. As in the ex-
periment on English, the meta parser outperformed
the baseline. We obtained an absolute improvement
of 2.07 points (UAS). The improvement was signif-
icant in McNemar?s Test (p < 10?8 ).
In summary, Tables 7 and 8 convincingly show
the effectiveness of our proposed approach.
4.4 Different sizes of unannotated data
Here, we considered the improvement relative to the
sizes of the unannotated data used to generate the
meta features. We randomly selected the 0.1%, 1%,
and 10% of the sentences from the full data. Table
English Chinese
Baseline 92.76 81.01
TrainData 91.93 80.40
P0.1 92.82 81.58
P1 93.14 82.23
P10 93.48 82.81
FULL 93.77 83.08
Table 9: Effect of different sizes of auto-parsed data
9 shows the results, where P0.1, P1, and P10 corre-
spond to 0.1%, 1%, and 10% respectively. From the
table, we found that the parsers obtained more ben-
efits as we used more raw sentences. We also tried
generating the meta features from the training data
only, shown as TrainData in Table 9. However, the
results shows that the parsers performed worse than
the baselines. This is not surprising because only
the known base features are included in the training
data.
4.5 Comparison with previous work
4.5.1 English
Table 10 shows the performance of the previ-
ous systems that were compared, where McDon-
ald06 refers to the second-order parser of McDon-
ald and Pereira (2006), Koo10 refers to the third-
order parser with model1 of Koo and Collins (2010),
Zhang11 refers to the parser of Zhang and Nivre
(2011), Li12 refers to the unlabeled parser of Li et
al. (2012), Koo08 refers to the parser of Koo et al
(2008), Suzuki09 refers to the parser of Suzuki et al
(2009), Chen09 refers to the parser of Chen et al
(2009), Zhou11 refers to the parser of Zhou et al
(2011), Suzuki11 refers to the parser of Suzuki et al
(2011), and Chen12 refers to the parser of Chen et
al. (2012).
The results showed that our meta parser out-
performed most of the previous systems and ob-
tained the comparable accuracy with the best result
of Suzuki11 (Suzuki et al, 2011) which combined
the clustering-based word representations of Koo et
al. (2008) and a condensed feature representation.
However, our approach is much simpler than theirs
and we believe that our meta parser can be further
improved by combining their methods.
1309
Type System UAS COMP
Sup
McDonald06 91.5
Koo10 93.04 -
Zhang11 92.9 48.0
Li12 93.12 -
Our Baseline 92.76 48.05
Semi
Koo08 93.16
Suzuki09 93.79
Chen09 93.16 47.15
Zhou11 92.64 46.61
Suzuki11 94.22 -
Chen12 92.76 -
MetaParser 93.77 51.36
Table 10: Relevant results for English. Sup denotes the
supervised parsers, Semi denotes the parsers with semi-
supervised methods.
4.5.2 Chinese
Table 11 shows the comparative results, where
Li11 refers to the parser of Li et al (2011), Hatori11
refers to the parser of Hatori et al (2011), and Li12
refers to the unlabeled parser of Li et al (2012). The
reported scores on this data were produced by the
supervised learning methods and our Baseline (su-
pervised) parser provided the comparable accuracy.
We found that the score of our meta parser for this
data was the best reported so far and significantly
higher than the previous scores. Note that we used
the auto-assigned POS tags in the test set to match
the above previous studies.
System UAS COMP
Li11 80.79 29.11
Hatori11 81.33 29.90
Li12 81.21 -
Our Baseline 81.01 29.71
MetaParser 83.08 32.21
Table 11: Relevant results for Chinese
4.6 Analysis
Here, we analyzed the effect of the meta features on
the data sparseness problem.
We first checked the effect of unknown features
on the parsing accuracy. We calculated the number
of unknown features in each sentence and computed
the average number per word. The average num-
bers were used to eliminate the influence of varied
sentence sizes. We sorted the test sentences in in-
creasing orders of these average numbers, and di-
vided equally into five bins. BIN 1 is assigned the
sentences with the smallest numbers and BIN 5 is
with the largest ones. Figure 2 shows the average
accuracy scores of the Baseline parsers against to
the bins. From the figure, we found that for both
two languages the Baseline parsers performed worse
while the sentences contained more unknown fea-
tures.
 70
 75
 80
 85
 90
 95
 100
1 2 3 4 5
A
c
c
u
r
a
c
y
BIN
EnglishChinese
Figure 2: Accuracies relative to numbers of unknown fea-
tures (average per word) by Baseline parsers
Then, we investigated the effect of the meta fea-
tures. We calculated the average number of ac-
tive meta features per word that were transformed
from the unknown features for each sentence. We
sorted the sentences in increasing order of the av-
erage numbers of active meta features and divided
them into five bins. BIN 1 is assigned the sen-
tences with the smallest numbers and BIN 5 is with
the largest ones. Figures 3 and 4 show the results,
where ?Better? is for the sentences where the meta
parsers provided better results than the baselines and
?Worse? is for those where the meta parsers pro-
vided worse results. We found that the gap between
?Better? and ?Worse? became larger while the sen-
tences contain more active meta features for the un-
known features. The gap means performance im-
provement. This indicates that the meta features are
very effective in processing the unknown features.
5 Related work
Our approach is to use unannotated data to generate
the meta features to improve dependency parsing.
1310
 0
 10
 20
 30
 40
 50
1 2 3 4 5
P
e
r
c
e
n
t
a
g
e
BIN
BetterWorse
Figure 3: Improvement relative to numbers of active meta
features on English (average per word)
 0
 10
 20
 30
 40
 50
1 2 3 4 5
P
e
r
c
e
n
t
a
g
e
BIN
BetterWorse
Figure 4: Improvement relative to numbers of active meta
features on Chinese (average per word)
Several previous studies relevant to our approach
have been conducted.
Koo et al (2008) used a word clusters trained on a
large amount of unannotated data and designed a set
of new features based on the clusters for dependency
parsing models. Chen et al (2009) extracted sub-
tree structures from a large amount of data and rep-
resented them as the additional features to improve
dependency parsing. Suzuki et al (2009) extended a
Semi-supervised Structured Conditional Model (SS-
SCM) of Suzuki and Isozaki (2008) to the depen-
dency parsing problem and combined their method
with the word clustering feature representation of
Koo et al (2008). Chen et al (2012) proposed an ap-
proach to representing high-order features for graph-
based dependency parsing models using a depen-
dency language model and beam search. In future
work, we may consider to combine their methods
with ours to improve performance.
Several previous studies used co-training/self-
training methods. McClosky et al (2006) presented
a self-training method combined with a reranking al-
gorithm for constituency parsing. Sagae and Tsujii
(2007) applied the standard co-training method for
dependency parsing. In their approaches, some au-
tomatically parsed sentences were selected as new
training data, which was used together with the orig-
inal labeled data to retrain a new parser. We are able
to use their approaches on top of the output of our
parsers.
With regard to feature transformation, the work
of Ando and Zhang (2005) is similar in spirit to our
work. They studied semi-supervised text chunking
by using a large projection matrix to map sparse base
features into a small number of high level features.
Their project matrix was trained by transforming the
original problem into a large number of auxiliary
problems, obtaining training data for the auxiliary
problems by automatically labeling raw data and us-
ing alternating structure optimization to estimate the
matrix across all auxiliary tasks. In comparison with
their approach, our method is simpler in the sense
that we do not request any intermediate step of split-
ting the prediction problem, and obtain meta fea-
tures directly from self-annotated data. The training
of our meta feature values is highly efficient, requir-
ing the collection of simple statistics over base fea-
tures from huge amount of data. Hence our method
can potentially be useful to other tasks also.
6 Conclusion
In this paper, we have presented a simple but effec-
tive semi-supervised approach to learning the meta
features from the auto-parsed data for dependency
parsing. We build a meta parser by combining the
meta features with the base features in a graph-based
model. The experimental results show that the pro-
posed approach significantly improves the accuracy.
Our meta parser achieves comparable accuracy with
the best known parsers on the English data (Penn
English Treebank) and the best accuracy on the Chi-
nese data (Chinese Treebank Version 5.1) so far.
Further analysis indicate that the meta features are
very effective in processing the unknown features.
The idea described in this paper is general and can
be applied to other NLP applications, such as part-
1311
of-speech tagging and Chinese word segmentation,
in future work.
Acknowledgments
This study was started when Wenliang Chen and
Min Zhang were members of the Department of
Human Language Technology, Institute for Info-
comm Research, Singapore. Wenliang Chen was
funded partially by the National Science Founda-
tion of China (61203314) and Yue Zhang was sup-
ported by MOE grant 2012-T2-2-163. We would
also thank the anonymous reviewers for their de-
tailed comments, which have helped us to improve
the quality of this work.
References
R.K. Ando and T. Zhang. 2005. A high-performance
semi-supervised learning method for text chunking.
ACL.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL-X. SIGNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Uti-
lizing dependency language models for graph-based
dependency parsing models. In Proceedings of ACL
2012, Korea, July.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD, Warsaw,
Poland.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING1996, pages 340?345.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In Proceed-
ings of 5th International Joint Conference on Natu-
ral Language Processing, pages 1216?1224, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Ver-
sion 2.0, LDC2009T14. Linguistic Data Consortium.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of EMNLP 2011, UK, July.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu.
2012. A separately passive-aggressive training algo-
rithm for joint pos tagging and dependency parsing.
In Proceedings of the 24rd International Conference
on Computational Linguistics (Coling 2012), Mumbai,
India. Coling 2012 Organizing Committee.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL, pages 122?131.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
1312
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Association for Computational Linguistics.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of English text. In Proc. of
the 20th Intern. Conf. on Computational Linguistics
(COLING), pages 64?70.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 1044?1050.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using Giga-word
scale unlabeled data. In Proceedings of ACL-08: HLT,
pages 665?673, Columbus, Ohio, June. Association
for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMNLP2009, pages 551?560, Sin-
gapore, August. Association for Computational Lin-
guistics.
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata. 2011.
Learning condensed feature representations from large
unsupervised data sets for supervised learning. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 636?641, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. Building a Large Annotated Chinese
Corpus: the Penn Chinese Treebank. Journal of Natu-
ral Language Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In Proceedings of EMNLP
2008, pages 562?571, Honolulu, Hawaii, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT2011, pages 188?193, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-HLT2011, pages 1556?1565, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
1313
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21?29,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bitext Dependency Parsing with Bilingual Subtree Constraints
Wenliang Chen, Jun?ichi Kazama and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, kazama, torisawa}@nict.go.jp
Abstract
This paper proposes a dependency parsing
method that uses bilingual constraints to
improve the accuracy of parsing bilingual
texts (bitexts). In our method, a target-
side tree fragment that corresponds to a
source-side tree fragment is identified via
word alignment and mapping rules that
are automatically learned. Then it is ver-
ified by checking the subtree list that is
collected from large scale automatically
parsed data on the target side. Our method,
thus, requires gold standard trees only on
the source side of a bilingual corpus in
the training phase, unlike the joint parsing
model, which requires gold standard trees
on the both sides. Compared to the re-
ordering constraint model, which requires
the same training data as ours, our method
achieved higher accuracy because of richer
bilingual constraints. Experiments on the
translated portion of the Chinese Treebank
show that our system outperforms mono-
lingual parsers by 2.93 points for Chinese
and 1.64 points for English.
1 Introduction
Parsing bilingual texts (bitexts) is crucial for train-
ing machine translation systems that rely on syn-
tactic structures on either the source side or the
target side, or the both (Ding and Palmer, 2005;
Nakazawa et al, 2006). Bitexts could provide
more information, which is useful in parsing, than
a usual monolingual texts that can be called ?bilin-
gual constraints?, and we expect to obtain more
accurate parsing results that can be effectively
used in the training of MT systems. With this mo-
tivation, there are several studies aiming at highly
accurate bitext parsing (Smith and Smith, 2004;
Burkett and Klein, 2008; Huang et al, 2009).
This paper proposes a dependency parsing
method, which uses the bilingual constraints that
we call bilingual subtree constraints and statistics
concerning the constraints estimated from large
unlabeled monolingual corpora. Basically, a (can-
didate) dependency subtree in a source-language
sentence is mapped to a subtree in the correspond-
ing target-language sentence by using word align-
ment and mapping rules that are automatically
learned. The target subtree is verified by check-
ing the subtree list that is collected from unla-
beled sentences in the target language parsed by
a usual monolingual parser. The result is used as
additional features for the source side dependency
parser. In this paper, our task is to improve the
source side parser with the help of the translations
on the target side.
Many researchers have investigated the use
of bilingual constraints for parsing (Burkett and
Klein, 2008; Zhao et al, 2009; Huang et al,
2009). For example, Burkett and Klein (2008)
show that parsing with joint models on bitexts im-
proves performance on either or both sides. How-
ever, their methods require that the training data
have tree structures on both sides, which are hard
to obtain. Our method only requires dependency
annotation on the source side and is much sim-
pler and faster. Huang et al (2009) proposes a
method, bilingual-constrained monolingual pars-
ing, in which a source-language parser is extended
to use the re-ordering of words between two sides?
sentences as additional information. The input of
their method is the source trees with their trans-
lation on the target side as ours, which is much
easier to obtain than trees on both sides. However,
their method does not use any tree structures on
21
the target side that might be useful for ambiguity
resolution. Our method achieves much greater im-
provement because it uses the richer subtree con-
straints.
Our approach takes the same input as Huang
et al (2009) and exploits the subtree structure on
the target side to provide the bilingual constraints.
The subtrees are extracted from large-scale auto-
parsed monolingual data on the target side. The
main problem to be addressed is mapping words
on the source side to the target subtree because
there are many to many mappings and reordering
problems that often occur in translation (Koehn et
al., 2003). We use an automatic way for generat-
ing mapping rules to solve the problems. Based
on the mapping rules, we design a set of features
for parsing models. The basic idea is as follows: if
the words form a subtree on one side, their corre-
sponding words on the another side will also prob-
ably form a subtree.
Experiments on the translated portion of the
Chinese Treebank (Xue et al, 2002; Bies et al,
2007) show that our system outperforms state-of-
the-art monolingual parsers by 2.93 points for Chi-
nese and 1.64 points for English. The results also
show that our system provides higher accuracies
than the parser of Huang et al (2009).
The rest of the paper is organized as follows:
Section 2 introduces the motivation of our idea.
Section 3 introduces the background of depen-
dency parsing. Section 4 proposes an approach
of constructing bilingual subtree constraints. Sec-
tion 5 explains the experimental results. Finally, in
Section 6 we draw conclusions and discuss future
work.
2 Motivation
In this section, we use an example to show the
idea of using the bilingual subtree constraints to
improve parsing performance.
Suppose that we have an input sentence pair as
shown in Figure 1, where the source sentence is in
English, the target is in Chinese, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a (candidate) dependency relation.
In the English side, it is difficult for a parser to
determine the head of word ?with? because there
is a PP-attachment problem. However, in Chinese
it is unambiguous. Therefore, we can use the in-
formation on the Chinese side to help disambigua-
He  ate    the    meat with     a    fork    .
?(He) ?(use) ??(fork) ?(eat) ?(meat) ?(.)
Figure 1: Example for disambiguation
tion.
There are two candidates ?ate? and ?meat? to be
the head of ?with? as the dashed directed links in
Figure 1 show. By adding ?fork?, we have two
possible dependency relations, ?meat-with-fork?
and ?ate-with-fork?, to be verified.
First, we check the possible relation of ?meat?,
?with?, and ?fork?. We obtain their corresponding
words ??(meat)?, ??(use)?, and ???(fork)? in
Chinese via the word alignment links. We ver-
ify that the corresponding words form a subtree
by looking up a subtree list in Chinese (described
in Section 4.1). But we can not find a subtree for
them.
Next, we check the possible relation of ?ate?,
?with?, and ?fork?. We obtain their correspond-
ing words ??(ate)?, ??(use)?, and ???(fork)?.
Then we verify that the words form a subtree by
looking up the subtree list. This time we can find
the subtree as shown in Figure 2.
?(use) ??(fork) ?(eat)
Figure 2: Example for a searched subtree
Finally, the parser may assign ?ate? to be the
head of ?with? based on the verification results.
This simple example shows how to use the subtree
information on the target side.
3 Dependency parsing
For dependency parsing, there are two main types
of parsing models (Nivre and McDonald, 2008;
Nivre and Kubler, 2006): transition-based (Nivre,
2003; Yamada and Matsumoto, 2003) and graph-
based (McDonald et al, 2005; Carreras, 2007).
Our approach can be applied to both parsing mod-
els.
In this paper, we employ the graph-based MST
parsing model proposed by McDonald and Pereira
22
(2006), which is an extension of the projec-
tive parsing algorithm of Eisner (1996). To use
richer second-order information, we also imple-
ment parent-child-grandchild features (Carreras,
2007) in the MST parsing algorithm.
3.1 Parsing with monolingual features
Figure 3 shows an example of dependency pars-
ing. In the graph-based parsing model, features are
represented for all the possible relations on single
edges (two words) or adjacent edges (three words).
The parsing algorithm chooses the tree with the
highest score in a bottom-up fashion.
ROOT     He  ate    the    meat   with     a    fork    .
Figure 3: Example of dependency tree
In our systems, the monolingual features in-
clude the first- and second- order features pre-
sented in (McDonald et al, 2005; McDonald
and Pereira, 2006) and the parent-child-grandchild
features used in (Carreras, 2007). We call the
parser with the monolingual features monolingual
parser.
3.2 Parsing with bilingual features
In this paper, we parse source sentences with the
help of their translations. A set of bilingual fea-
tures are designed for the parsing model.
3.2.1 Bilingual subtree features
We design bilingual subtree features, as described
in Section 4, based on the constraints between the
source subtrees and the target subtrees that are ver-
ified by the subtree list on the target side. The
source subtrees are from the possible dependency
relations.
3.2.2 Bilingual reordering feature
Huang et al (2009) propose features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify that the corre-
sponding words form a contiguous span for resolv-
ing shift-reduce conflicts. We also implement sim-
ilar features in our system.
4 Bilingual subtree constraints
In this section, we propose an approach that uses
the bilingual subtree constraints to help parse
source sentences that have translations on the tar-
get side.
We use large-scale auto-parsed data to obtain
subtrees on the target side. Then we generate the
mapping rules to map the source subtrees onto the
extracted target subtrees. Finally, we design the
bilingual subtree features based on the mapping
rules for the parsing model. These features in-
dicate the information of the constraints between
bilingual subtrees, that are called bilingual subtree
constraints.
4.1 Subtree extraction
Chen et al (2009) propose a simple method to ex-
tract subtrees from large-scale monolingual data
and use them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with a monolingual parser and
obtain a set of subtrees (STt) in the target lan-
guage.
We encode the subtrees into string format that is
expressed as st = w : hid(?w : hid)+1, where w
refers to a word in the subtree and hid refers to the
word ID of the word?s head (hid=0 means that this
word is the root of a subtree). Here, word ID refers
to the ID (starting from 1) of a word in the subtree
(words are ordered based on the positions of the
original sentence). For example, ?He? and ?ate?
have a left dependency arc in the sentence shown
in Figure 3. The subtree is encoded as ?He:2-
ate:0?. There is also a parent-child-grandchild re-
lation among ?ate?, ?with?, and ?fork?. So the
subtree is encoded as ?ate:0-with:1-fork:2?. If a
subtree contains two nodes, we call it a bigram-
subtree. If a subtree contains three nodes, we call
it a trigram-subtree.
From the dependency tree of Figure 3, we ob-
tain the subtrees, as shown in Figure 4 and Figure
5. Figure 4 shows the extracted bigram-subtrees
and Figure 5 shows the extracted trigram-subtrees.
After extraction, we obtain a set of subtrees. We
remove the subtrees occurring only once in the
data. Following Chen et al (2009), we also group
the subtrees into different sets based on their fre-
quencies.
1+ refers to matching the preceding element one or more
times and is the same as a regular expression in Perl.
23
ate
He He:1:2-ate:2:0ate
meat ate:1:0-meat:2:1
ate
with ate:1:0-with:2:1
meat
the the:1:2-meat:2:0
with
fork with:1:0-fork:2:1fork
a a:1:2-fork:2:0
Figure 4: Examples of bigram-subtrees
atemeat  with ate:1:0-meat:2:1-with:3:1 atewith   . ate:1:0-with:2:1-.:3:1
(a)
He:1:3-NULL:2:3-ate:3:0ateHe  NULL ateNULL  meat ate:1:0-NULL:2:1-meat:3:1the:1:3-NULL:2:3-meat:3:0a:1:3-NULL:2:3-fork:3:0 with:1:0-NULL:2:1-fork:3:1
ate:1:0-the:2:3-meat:3:1 ate:1:0-with:2:1-fork:3:2with:1:0-a:2:3-fork:3:1 NULL:1:2-He:2:3-ate:3:0He:1:3-NULL:2:1-ate:3:0 ate:1:0-meat:2:1-NULL:3:2ate:1:0-NULL:2:3-with:3:1 with:1:0-fork:2:1-NULL:3:2NULL:1:2-a:2:3-fork:3:0 a:1:3-NULL:2:1-fork:3:0ate:1:0-NULL:2:3-.:3:1 ate:1:0-.:2:1-NULL:3:2
(b)NULL:1:2-the:2:3-meat:3:0 the:1:3-NULL:2:1-meat:3:0
Figure 5: Examples of trigram-subtrees
4.2 Mapping rules
To provide bilingual subtree constraints, we need
to find the characteristics of subtree mapping for
the two given languages. However, subtree map-
ping is not easy. There are two main problems:
MtoN (words) mapping and reordering, which of-
ten occur in translation. MtoN (words) map-
ping means that a source subtree with M words
is mapped onto a target subtree with N words. For
example, 2to3 means that a source bigram-subtree
is mapped onto a target trigram-subtree.
Due to the limitations of the parsing algo-
rithm (McDonald and Pereira, 2006; Carreras,
2007), we only use bigram- and trigram-subtrees
in our approach. We generate the mapping rules
for the 2to2, 2to3, 3to3, and 3to2 cases. For
trigram-subtrees, we only consider the parent-
child-grandchild type. As for the use of other
types of trigram-subtrees, we leave it for future
work.
We first show the MtoN and reordering prob-
lems by using an example in Chinese-English
translation. Then we propose a method to auto-
matically generate mapping rules.
4.2.1 Reordering and MtoN mapping in
translation
Both Chinese and English are classified as SVO
languages because verbs precede objects in simple
sentences. However, Chinese has many character-
istics of such SOV languages as Japanese. The
typical cases are listed below:
1) Prepositional phrases modifying a verb pre-
cede the verb. Figure 6 shows an example. In En-
glish the prepositional phrase ?at the ceremony?
follows the verb ?said?, while its corresponding
prepositional phrase ??(NULL)??(ceremony)
?(at)? precedes the verb ??(say)? in Chinese.
? ?? ? ?
Said at the ceremony
Figure 6: Example for prepositional phrases mod-
ifying a verb
2) Relative clauses precede head noun. Fig-
ure 7 shows an example. In Chinese the relative
clause ???(today) ??(signed)? precedes the
head noun ???(project)?, while its correspond-
ing clause ?signed today? follows the head noun
?projects? in English.
?? ?? ? ? ? ??
The 3 projects signed today
Figure 7: Example for relative clauses preceding
the head noun
3) Genitive constructions precede head noun.
For example, ???(car) ??(wheel)? can be
translated as ?the wheel of the car?.
4) Postposition in many constructions rather
than prepositions. For example, ???(table)
?(on)? can be translated as ?on the table?.
24
We can find the MtoN mapping problem occur-
ring in the above cases. For example, in Figure 6,
trigram-subtree ??(NULL):3-?(at):1-?(say):0?
is mapped onto bigram-subtree ?said:0-at:1?.
Since asking linguists to define the mapping
rules is very expensive, we propose a simple
method to easily obtain the mapping rules.
4.2.2 Bilingual subtree mapping
To solve the mapping problems, we use a bilingual
corpus, which includes sentence pairs, to automat-
ically generate the mapping rules. First, the sen-
tence pairs are parsed by monolingual parsers on
both sides. Then we perform word alignment us-
ing a word-level aligner (Liang et al, 2006; DeN-
ero and Klein, 2007). Figure 8 shows an example
of a processed sentence pair that has tree structures
on both sides and word alignment links.
ROOT    ?? ?? ?? ?? ?
ROOT    They   are   on   the   fringes   of   society   .
Figure 8: Example of auto-parsed bilingual sen-
tence pair
From these sentence pairs, we obtain subtree
pairs. First, we extract a subtree (sts) from a
source sentence. Then through word alignment
links, we obtain the corresponding words of the
words of sts. Because of the MtoN problem, some
words lack of corresponding words in the target
sentence. Here, our approach requires that at least
two words of sts have corresponding words and
nouns and verbs need corresponding words. If not,
it fails to find a subtree pair for sts. If the corre-
sponding words form a subtree (stt) in the target
sentence, sts and stt are a subtree pair. We also
keep the word alignment information in the tar-
get subtree. For example, we extract subtree ??
?(society):2-??(fringe):0? on the Chinese side
and get its corresponding subtree ?fringes(W 2):0-
of:1-society(W 1):2? on the English side, where
W 1 means that the target word is aligned to the
first word of the source subtree, and W 2 means
that the target word is aligned to the second word
of the source subtree. That is, we have a sub-
tree pair: ???(society):2-??(fringe):0? and
?fringe(W 2):0-of:1-society(W 1):2?.
The extracted subtree pairs indicate the trans-
lation characteristics between Chinese and En-
glish. For example, the pair ???(society):2-
? ?(fringe):0? and ?fringes:0-of:1-society:2?
is a case where ?Genitive constructions pre-
cede/follow the head noun?.
4.2.3 Generalized mapping rules
To increase the mapping coverage, we general-
ize the mapping rules from the extracted sub-
tree pairs by using the following procedure. The
rules are divided by ?=>? into two parts: source
(left) and target (right). The source part is
from the source subtree and the target part is
from the target subtree. For the source part,
we replace nouns and verbs using their POS
tags (coarse grained tags). For the target part,
we use the word alignment information to rep-
resent the target words that have correspond-
ing source words. For example, we have the
subtree pair: ???(society):2-??(fringe):0?
and ?fringes(W 2):0-of:1-society(W 1):2?, where
?of? does not have a corresponding word, the POS
tag of ???(society)? is N, and the POS tag of
???(fringe)? is N. The source part of the rule
becomes ?N:2-N:0? and the target part becomes
?W 2:0-of:1-W 1:2?.
Table 1 shows the top five mapping rules of
all four types ordered by their frequencies, where
W 1 means that the target word is aligned to the
first word of the source subtree, W 2 means that
the target word is aligned to the second word, and
W 3 means that the target word is aligned to the
third word. We remove the rules that occur less
than three times. Finally, we obtain 9,134 rules
for 2to2, 5,335 for 2to3, 7,450 for 3to3, and 1,244
for 3to2 from our data. After experiments with dif-
ferent threshold settings on the development data
sets, we use the top 20 rules for each type in our
experiments.
The generalized mapping rules might generate
incorrect target subtrees. However, as described in
Section 4.3.1, the generated subtrees are verified
by looking up list STt before they are used in the
parsing models.
4.3 Bilingual subtree features
Informally, if the words form a subtree on the
source side, then the corresponding words on the
target side will also probably form a subtree. For
25
# rules freq
2to2 mapping
1 N:2 N:0 =>W 1:2 W 2:0 92776
2 V:0 N:1 =>W 1:0 W 2:1 62437
3 V:0 V:1 =>W 1:0 W 2:1 49633
4 N:2 V:0 =>W 1:2 W 2:0 43999
5 ?:2 N:0 =>W 2:0 W 1:2 25301
2to3 mapping
1 N:2-N:0 =>W 2:0-of:1-W 1:2 10361
2 V:0-N:1 =>W 1:0-of:1-W 2:2 4521
3 V:0-N:1 =>W 1:0-to:1-W 2:2 2917
4 N:2-V:0 =>W 2:0-of:1-W 1:2 2578
5 N:2-N:0 =>W 1:2-?:3-W 2:0 2316
3to2 mapping
1 V:2-?/DEC:3-N:0 =>W 1:0-W 3:1 873
2 V:2-?/DEC:3-N:0 =>W 3:2-W 1:0 634
3 N:2-?/DEG:3-N:0 =>W 1:0-W 3:1 319
4 N:2-?/DEG:3-N:0 =>W 3:2-W 1:0 301
5 V:0-?/DEG:3-N:1 =>W 3:0-W 1:1 247
3to3 mapping
1 V:0-V:1-N:2 =>W 1:0-W 2:1-W 3:2 9580
2 N:2-?/DEG:3-N:0 =>W 3:0-W 2:1-W 1:2 7010
3 V:0-N:3-N:1 =>W 1:0-W 2:3-W 3:1 5642
4 V:0-V:1-V:2 =>W 1:0-W 2:1-W 3:2 4563
5 N:2-N:3-N:0 =>W 1:2-W 2:3-W 3:0 3570
Table 1: Top five mapping rules of 2to3 and 3to2
example, in Figure 8, words ???(they)? and
???(be on)? form a subtree , which is mapped
onto the words ?they? and ?are? on the target side.
These two target words form a subtree. We now
develop this idea as bilingual subtree features.
In the parsing process, we build relations for
two or three words on the source side. The con-
ditions of generating bilingual subtree features are
that at least two of these source words must have
corresponding words on the target side and nouns
and verbs must have corresponding words.
At first, we have a possible dependency relation
(represented as a source subtree) of words to be
verified. Then we obtain the corresponding target
subtree based on the mapping rules. Finally, we
verify that the target subtree is included in STt. If
yes, we activate a positive feature to encourage the
dependency relation.
? ? ?? ?? ? ? ? ??
Those are the 3 projects signed today
Figure 9: Example of features for parsing
We consider four types of features based on
2to2, 3to3, 3to2, and 2to3 mappings. In the 2to2,
3to3, and 3to2 cases, the target subtrees do not add
new words. We represent features in a direct way.
For the 2to3 case, we represent features using a
different strategy.
4.3.1 Features for 2to2, 3to3, and 3to2
We design the features based on the mapping
rules of 2to2, 3to3, and 3to2. For example, we
design features for a 3to2 case from Figure 9.
The possible relation to be verified forms source
subtree ???(signed)/VV:2-?(NULL)/DEC:3-
??(project)/NN:0? in which ???(project)?
is aligned to ?projects? and ???(signed)? is
aligned to ?signed? as shown in Figure 9. The
procedure of generating the features is shown in
Figure 10. We explain Steps (1), (2), (3), and (4)
as follows:
??/VV:2-?/DEC:3-??/NN:0
projects(W_3) signed(W_1) 
(1)
V:2-?/DEC:3-N:0
W_3:0-W_1:1W 3:2 W 1:0
(2)
_ - _
(3)
projects:0-signed:1projects:2-signed:0 STt
(4)
3to2:YES
Figure 10: Example of feature generation for 3to2
case
(1) Generate source part from the source
subtree. We obtain ?V:2-?/DEC:3-N:0? from
?? ?(signed)/VV:2-?(NULL)/DEC:3-?
?(project)/NN:0?.
(2) Obtain target parts based on the matched
mapping rules, whose source parts equal
?V:2-?/DEC:3-N:0?. The matched rules are
?V:2-?/DEC:3-N:0 =>W 3:0-W 1:1? and
?V:2-?/DEC:3-N:0 => W 3:2-W 1:0?. Thus,
we have two target parts ?W 3:0-W 1:1? and
?W 3:2-W 1:0?.
(3) Generate possible subtrees by consider-
26
ing the dependency relation indicated in the
target parts. We generate a possible subtree
?projects:0-signed:1? from the target part ?W 3:0-
W 1:1?, where ?projects? is aligned to ??
?(project)(W 3)? and ?signed? is aligned to ??
?(signed)(W 1)?. We also generate another pos-
sible subtree ?projects:2-signed:0? from ?W 3:2-
W 1:0?.
(4) Verify that at least one of the generated
possible subtrees is a target subtree, which is in-
cluded in STt. If yes, we activate this feature. In
the figure, ?projects:0-signed:1? is a target subtree
in STt. So we activate the feature ?3to2:YES?
to encourage dependency relations among ??
?(signed)?, ??(NULL)?, and ???(project)?.
4.3.2 Features for 2to3
In the 2to3 case, a new word is added on the target
side. The first two steps are identical as those in
the previous section. For example, a source part
?N:2-N:0? is generated from ???(car)/NN:2-?
?(wheel)/NN:0?. Then we obtain target parts
such as ?W 2:0-of/IN:1-W 1:2?, ?W 2:0-in/IN:1-
W 1:2?, and so on, according to the matched map-
ping rules.
The third step is different. In the target parts,
there is an added word. We first check if the added
word is in the span of the corresponding words,
which can be obtained through word alignment
links. We can find that ?of? is in the span ?wheel
of the car?, which is the span of the corresponding
words of ???(car)/NN:2-??(wheel)/NN:0?.
Then we choose the target part ?W 2:0-of/IN:1-
W 1:2? to generate a possible subtree. Finally,
we verify that the subtree is a target subtree in-
cluded in STt. If yes, we say feature ?2to3:YES?
to encourage a dependency relation between ??
?(car)? and ???(wheel)?.
4.4 Source subtree features
Chen et al (2009) shows that the source sub-
tree features (Fsrc?st) significantly improve per-
formance. The subtrees are obtained from the
auto-parsed data on the source side. Then they are
used to verify the possible dependency relations
among source words.
In our approach, we also use the same source
subtree features described in Chen et al (2009).
So the possible dependency relations are verified
by the source and target subtrees. Combining two
types of features together provides strong discrim-
ination power. If both types of features are ac-
tive, building relations is very likely among source
words. If both are inactive, this is a strong negative
signal for their relations.
5 Experiments
All the bilingual data were taken from the trans-
lated portion of the Chinese Treebank (CTB)
(Xue et al, 2002; Bies et al, 2007), articles
1-325 of CTB, which have English translations
with gold-standard parse trees. We used the tool
?Penn2Malt?2 to convert the data into dependency
structures. Following the study of Huang et al
(2009), we used the same split of this data: 1-270
for training, 301-325 for development, and 271-
300 for test. Note that some sentence pairs were
removed because they are not one-to-one aligned
at the sentence level (Burkett and Klein, 2008;
Huang et al, 2009). Word alignments were gen-
erated from the Berkeley Aligner (Liang et al,
2006; DeNero and Klein, 2007) trained on a bilin-
gual corpus having approximately 0.8M sentence
pairs. We removed notoriously bad links in {a,
an, the}?{?(DE),?(LE)} following the work of
Huang et al (2009).
For Chinese unannotated data, we used the
XIN CMN portion of Chinese Gigaword Version
2.0 (LDC2009T14) (Huang, 2009), which has ap-
proximately 311 million words whose segmenta-
tion and POS tags are given. To avoid unfair com-
parison, we excluded the sentences of the CTB
data from the Gigaword data. We discarded the an-
notations because there are differences in annota-
tion policy between CTB and this corpus. We used
the MMA system (Kruengkrai et al, 2009) trained
on the training data to perform word segmentation
and POS tagging and used the Baseline Parser to
parse all the sentences in the data. For English
unannotated data, we used the BLLIP corpus that
contains about 43 million words of WSJ text. The
POS tags were assigned by the MXPOST tagger
trained on training data. Then we used the Base-
line Parser to parse all the sentences in the data.
We reported the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens (excluding all punctuation tokens) with cor-
rect HEADs.
5.1 Main results
The results on the Chinese-source side are shown
in Table 2, where ?Baseline? refers to the systems
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
27
with monolingual features, ?Baseline2? refers to
adding the reordering features to the Baseline,
?FBI? refers to adding all the bilingual subtree
features to ?Baseline2?, ?Fsrc?st? refers to the
monolingual parsing systems with source subtree
features, ?Order-1? refers to the first-order mod-
els, and ?Order-2? refers to the second-order mod-
els. The results showed that the reordering fea-
tures yielded an improvement of 0.53 and 0.58
points (UAS) for the first- and second-order mod-
els respectively. Then we added four types of
bilingual constraint features one by one to ?Base-
line2?. Note that the features based on 3to2 and
3to3 can not be applied to the first-order models,
because they only consider single dependencies
(bigram). That is, in the first model, FBI only in-
cludes the features based on 2to2 and 2to3. The
results showed that the systems performed better
and better. In total, we obtained an absolute im-
provement of 0.88 points (UAS) for the first-order
model and 1.36 points for the second-order model
by adding all the bilingual subtree features. Fi-
nally, the system with all the features (OURS) out-
performed the Baseline by an absolute improve-
ment of 3.12 points for the first-order model and
2.93 points for the second-order model. The im-
provements of the final systems (OURS) were sig-
nificant in McNemar?s Test (p < 10?4).
Order-1 Order-2
Baseline 84.35 87.20
Baseline2 84.88 87.78
+2to2 85.08 88.07
+2to3 85.23 88.14
+3to3 ? 88.29
+3to2 ? 88.56
FBI 85.23(+0.88) 88.56(+1.36)
Fsrc?st 86.54(+2.19) 89.49(+2.29)
OURS 87.47(+3.12) 90.13(+2.93)
Table 2: Dependency parsing results of Chinese-
source case
We also conducted experiments on the English-
source side. Table 3 shows the results, where ab-
breviations are the same as in Table 2. As in the
Chinese experiments, the parsers with bilingual
subtree features outperformed the Baselines. Fi-
nally, the systems (OURS) with all the features
outperformed the Baselines by 1.30 points for the
first-order model and 1.64 for the second-order
model. The improvements of the final systems
(OURS) were significant in McNemar?s Test (p <
10?3).
Order-1 Order-2
Baseline 86.41 87.37
Baseline2 86.86 87.66
+2to2 87.23 87.87
+2to3 87.35 87.96
+3to3 ? 88.25
+3to2 ? 88.37
FBI 87.35(+0.94) 88.37(+1.00)
Fsrc?st 87.25(+0.84) 88.57(+1.20)
OURS 87.71(+1.30) 89.01(+1.64)
Table 3: Dependency parsing results of English-
source case
5.2 Comparative results
Table 4 shows the performance of the system we
compared, where Huang2009 refers to the result of
Huang et al (2009). The results showed that our
system performed better than Huang2009. Com-
pared with the approach of Huang et al (2009),
our approach used additional large-scale auto-
parsed data. We did not compare our system with
the joint model of Burkett and Klein (2008) be-
cause they reported the results on phrase struc-
tures.
Chinese English
Huang2009 86.3 87.5
Baseline 87.20 87.37
OURS 90.13 89.01
Table 4: Comparative results
6 Conclusion
We presented an approach using large automati-
cally parsed monolingual data to provide bilingual
subtree constraints to improve bitexts parsing. Our
approach remains the efficiency of monolingual
parsing and exploits the subtree structure on the
target side. The experimental results show that the
proposed approach is simple yet still provides sig-
nificant improvements over the baselines in pars-
ing accuracy. The results also show that our sys-
tems outperform the system of previous work on
the same data.
There are many ways in which this research
could be continued. First, we may attempt to ap-
ply the bilingual subtree constraints to transition-
28
based parsing models (Nivre, 2003; Yamada and
Matsumoto, 2003). Here, we may design new fea-
tures for the models. Second, we may apply the
proposed method for other language pairs such as
Japanese-English and Chinese-Japanese. Third,
larger unannotated data can be used to improve the
performance further.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English Chinese translation treebank
v 1.0. In LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 877?
886, Honolulu, Hawaii, October. Association for
Computational Linguistics.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
WL. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 570?579, Singapore, Au-
gust. Association for Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541?548, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1222?1231, Singapore, August. Associ-
ation for Computational Linguistics.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword
Version 2.0, LDC2009T14. Linguistic Data Con-
sortium.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL,
page 54. Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL-IJCNLP2009,
pages 513?521, Suntec, Singapore, August. Associ-
ation for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL 2005.
T. Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper nlp. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
J. Nivre and S. Kubler. 2006. Dependency parsing:
Tutorial at Coling-ACL 2006. In CoLING-ACL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of IWPT2003,
pages 149?160.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated Chinese cor-
pus. In Coling.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
29
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213?222,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Utilizing Dependency Language Models for Graph-based Dependency
Parsing Models
Wenliang Chen, Min Zhang?, and Haizhou Li
Human Language Technology, Institute for Infocomm Research, Singapore
{wechen, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Most previous graph-based parsing models in-
crease decoding complexity when they use
high-order features due to exact-inference de-
coding. In this paper, we present an approach
to enriching high-order feature representations
for graph-based dependency parsing models
using a dependency language model and beam
search. The dependency language model is
built on a large-amount of additional auto-
parsed data that is processed by a baseline
parser. Based on the dependency language
model, we represent a set of features for the
parsing model. Finally, the features are effi-
ciently integrated into the parsing model dur-
ing decoding using beam search. Our ap-
proach has two advantages. Firstly we utilize
rich high-order features defined over a view
of large scope and additional large raw cor-
pus. Secondly our approach does not increase
the decoding complexity. We evaluate the pro-
posed approach on English and Chinese data.
The experimental results show that our new
parser achieves the best accuracy on the Chi-
nese data and comparable accuracy with the
best known systems on the English data.
1 Introduction
In recent years, there are many data-driven mod-
els that have been proposed for dependency parsing
(McDonald and Nivre, 2007). Among them, graph-
based dependency parsing models have achieved
state-of-the-art performance for a wide range of lan-
guages as shown in recent CoNLL shared tasks
?Corresponding author
(Buchholz and Marsi, 2006; Nivre et al, 2007).
In the graph-based models, dependency parsing is
treated as a structured prediction problem in which
the graphs are usually represented as factored struc-
tures. The information of the factored structures de-
cides the features that the models can utilize. There
are several previous studies that exploit high-order
features that lead to significant improvements.
McDonald et al (2005) and Covington (2001)
develop models that represent first-order features
over a single arc in graphs. By extending the first-
order model, McDonald and Pereira (2006) and Car-
reras (2007) exploit second-order features over two
adjacent arcs in second-order models. Koo and
Collins (2010) further propose a third-order model
that uses third-order features. These models utilize
higher-order feature representations and achieve bet-
ter performance than the first-order models. But this
achievement is at the cost of the higher decoding
complexity, from O(n2) to O(n4), where n is the
length of the input sentence. Thus, it is very hard to
develop higher-order models further in this way.
How to enrich high-order feature representations
without increasing the decoding complexity for
graph-based models becomes a very challenging
problem in the dependency parsing task. In this pa-
per, we solve this issue by enriching the feature rep-
resentations for a graph-based model using a depen-
dency language model (DLM) (Shen et al, 2008).
The N-gram DLM has the ability to predict the next
child based on the N-1 immediate previous children
and their head (Shen et al, 2008). The basic idea
behind is that we use the DLM to evaluate whether a
valid dependency tree (McDonald and Nivre, 2007)
213
is well-formed from a view of large scope. The pars-
ing model searches for the final dependency trees
by considering the original scores and the scores of
DLM.
In our approach, the DLM is built on a large
amount of auto-parsed data, which is processed
by an original first-order parser (McDonald et al,
2005). We represent the features based on the DLM.
The DLM-based features can capture the N-gram in-
formation of the parent-children structures for the
parsing model. Then, they are integrated directly
in the decoding algorithms using beam-search. Our
new parsing model can utilize rich high-order fea-
ture representations but without increasing the com-
plexity.
To demonstrate the effectiveness of the proposed
approach, we conduct experiments on English and
Chinese data. The results indicate that the approach
greatly improves the accuracy. In summary, we
make the following contributions:
? We utilize the dependency language model to
enhance the graph-based parsing model. The
DLM-based features are integrated directly into
the beam-search decoder.
? The new parsing model uses the rich high-order
features defined over a view of large scope and
and additional large raw corpus, but without in-
creasing the decoding complexity.
? Our parser achieves the best accuracy on the
Chinese data and comparable accuracy with the
best known systems on the English data.
2 Dependency language model
Language models play a very important role for sta-
tistical machine translation (SMT). The standard N-
gram based language model predicts the next word
based on the N?1 immediate previous words. How-
ever, the traditional N-gram language model can
not capture long-distance word relations. To over-
come this problem, Shen et al (2008) proposed a
dependency language model (DLM) to exploit long-
distance word relations for SMT. The N-gram DLM
predicts the next child of a head based on the N ? 1
immediate previous children and the head itself. In
this paper, we define a DLM, which is similar to the
one of Shen et al (2008), to score entire dependency
trees.
An input sentence is denoted by x =
(x0, x1, ..., xi, ..., xn), where x0 = ROOT and
does not depend on any other token in x and each
token xi refers to a word. Let y be a depen-
dency tree for x and H(y) be a set that includes the
words that have at least one dependent. For each
xh ? H(y), we have a dependency structure Dh =
(xLk, ...xL1, xh, xR1...xRm), where xLk, ...xL1 are
the children on the left side from the farthest to the
nearest and xR1...xRm are the children on the right
side from the nearest to the farthest. Probability
P (Dh) is defined as follows:
P (Dh) = PL(Dh)? PR(Dh) (1)
Here PL and PR are left and right side generative
probabilities respectively. Suppose, we use a N-
gram dependency language model. PL is defined as
follows:
PL(Dh) ? PLc(xL1|xh)
?PLc(xL2|xL1, xh)
?... (2)
?PLc(xLk|xL(k?1), ..., xL(k?N+1), xh)
where the approximation is based on the nth order
Markov assumption. The right side probability is
similar. For a dependency tree, we calculate the
probability as follows:
P (y) =
?
xh?H(y)
P (Dh) (3)
In this paper, we use a linear model to calculate
the scores for the parsing models (defined in Section
3.1). Accordingly, we reform Equation 3. We define
fDLM as a high-dimensional feature representation
which is based on arbitrary features of PLc, PRc and
x. Then, the DLM score of tree y is in turn computed
as the inner product of fDLM with a corresponding
weight vector wDLM .
scoreDLM (y) = fDLM ? wDLM (4)
3 Parsing with dependency language
model
In this section, we propose a parsing model which
includes the dependency language model by extend-
ing the model of McDonald et al (2005).
214
3.1 Graph-based parsing model
The graph-based parsing model aims to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald et al, 2005). We write (xi, xj) ? y
if there is a dependency in tree y from word xi
to word xj (xi is the head and xj is the depen-
dent). A graph, denoted by Gx, consists of a set
of nodes Vx = {x0, x1, ..., xi, ..., xn} and a set of
arcs (edges) Ex = {(xi, xj)|i 6= j, xi ? Vx, xj ?
(Vx ? x0)}, where the nodes in Vx are the words
in x. Let T (Gx) be the set of all the subgraphs of
Gx that are valid dependency trees (McDonald and
Nivre, 2007) for sentence x.
The formulation defines the score of a depen-
dency tree y ? T (Gx) to be the sum of the edge
scores,
s(x, y) =
?
g?y
score(w, x, g) (5)
where g is a spanning subgraph of y. g can be a
single dependency or adjacent dependencies. Then
y is represented as a set of factors. The model
scores each factor using a weight vector w that con-
tains the weights for the features to be learned dur-
ing training using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003; McDon-
ald and Pereira, 2006). The scoring function is
score(w, x, g) = f(x, g) ? w (6)
where f(x, g) is a high-dimensional feature repre-
sentation which is based on arbitrary features of g
and x.
The parsing model finds a maximum spanning
tree (MST), which is the highest scoring tree in
T (Gx). The task of the decoding algorithm for a
given sentence x is to find y?,
y? = argmax
y?T (Gx)
s(x, y) = argmax
y?T (Gx)
?
g?y
score(w, x, g)
3.2 Add DLM scores
In our approach, we consider the scores of the DLM
when searching for the maximum spanning tree.
Then for a given sentence x, we find y?DLM ,
y?DLM = argmax
y?T (Gx)
(
?
g?y
score(w, x, g)+scoreDLM (y))
After adding the DLM scores, the new parsing
model can capture richer information. Figure 1 illus-
trates the changes. In the original first-order parsing
model, we only utilize the information of single arc
(xh, xL(k?1)) for xL(k?1) as shown in Figure 1-(a).
If we use 3-gram DLM, we can utilize the additional
information of the two previous children (nearer to
xh than xL(k?1)): xL(k?2) and xL(k?3) as shown in
Figure 1-(b).
Figure 1: Adding the DLM scores to the parsing model
3.3 DLM-based feature templates
We define DLM-based features for Dh =
(xLk, ...xL1, xh, xR1...xRm). For each child xch on
the left side, we have PLc(xch|HIS), where HIS
refers to the N ? 1 immediate previous right chil-
dren and head xh. Similarly, we have PRc(xch|HIS)
for each child on the right side. Let Pu(xch|HIS)
(Pu(ch) in short) be one of the above probabilities.
We use the map function ?(Pu(ch)) to obtain the
predefined discrete value (defined in Section 5.3).
The feature templates are outlined in Table 1, where
TYPE refers to one of the types:PL or PR, h pos
refers to the part-of-speech tag of xh, h word refers
to the lexical form of xh, ch pos refers to the part-of-
speech tag of xch, and ch word refers to the lexical
form of xch.
4 Decoding
In this section, we turn to the problem of adding the
DLM in the decoding algorithm. We propose two
ways: (1) Rescoring, in which we rescore the K-
best list with the DLM-based features; (2) Intersect,
215
< ?(Pu(ch)),TYPE >
< ?(Pu(ch)),TYPE, h pos >
< ?(Pu(ch)),TYPE, h word >
< ?(Pu(ch)),TYPE, ch pos >
< ?(Pu(ch)),TYPE, ch word >
< ?(Pu(ch)),TYPE, h pos, ch pos >
< ?(Pu(ch)),TYPE, h word, ch word >
Table 1: DLM-based feature templates
in which we add the DLM-based features in the de-
coding algorithm directly.
4.1 Rescoring
We add the DLM-based features into the decoding
procedure by using the rescoring technique used in
(Shen et al, 2008). We can use an original parser
to produce the K-best list. This method has the po-
tential to be very fast. However, because the perfor-
mance of this method is restricted to the K-best list,
we may have to set K to a high number in order to
find the best parsing tree (with DLM) or a tree ac-
ceptably close to the best (Shen et al, 2008).
4.2 Intersect
Then, we add the DLM-based features in the decod-
ing algorithm directly. The DLM-based features are
generated online during decoding.
For our parser, we use the decoding algorithm
of McDonald et al (2005). The algorithm was ex-
tensions of the parsing algorithm of (Eisner, 1996),
which was a modified version of the CKY chart
parsing algorithm. Here, we describe how to add
the DLM-based features in the first-order algorithm.
The second-order and higher-order algorithms can
be extended by the similar way.
The parsing algorithm independently parses the
left and right dependents of a word and combines
them later. There are two types of chart items (Mc-
Donald and Pereira, 2006): 1) a complete item in
which the words are unable to accept more depen-
dents in a certain direction; and 2) an incomplete
item in which the words can accept more dependents
in a certain direction. In the algorithm, we create
both types of chart items with two directions for all
the word pairs in a given sentence. The direction of
a dependency is from the head to the dependent. The
right (left) direction indicates the dependent is on the
right (left) side of the head. Larger chart items are
created from pairs of smaller ones in a bottom-up
style. In the following figures, complete items are
represented by triangles and incomplete items are
represented by trapezoids. Figure 2 illustrates the
cubic parsing actions of the algorithm (Eisner, 1996)
in the right direction, where s, r, and t refer to the
start and end indices of the chart items. In Figure
2-(a), all the items on the left side are complete and
the algorithm creates the incomplete item (trapezoid
on the right side) of s ? t. This action builds a de-
pendency relation from s to t. In Figure 2-(b), the
item of s ? r is incomplete and the item of r ? t is
complete. Then the algorithm creates the complete
item of s ? t. In this action, all the children of r are
generated. In Figure 2, the longer vertical edge in a
triangle or a trapezoid corresponds to the subroot of
the structure (spanning chart). For example, s is the
subroot of the span s ? t in Figure 2-(a). For the left
direction case, the actions are similar.
Figure 2: Cubic parsing actions of Eisner (Eisner, 1996)
Then, we add the DLM-based features into the
parsing actions. Because the parsing algorithm is
in the bottom-up style, the nearer children are gen-
erated earlier than the farther ones of the same head.
Thus, we calculate the left or right side probabil-
ity for a new child when a new dependency rela-
tion is built. For Figure 2-(a), we add the features of
PRc(xt|HIS). Figure 3 shows the structure, where
cRs refers to the current children (nearer than xt) of
xs. In the figure, HIS includes cRs and xs.
Figure 3: Add DLM-based features in cubic parsing
216
We use beam search to choose the one having the
overall best score as the final parse, where K spans
are built at each step (Zhang and Clark, 2008). At
each step, we perform the parsing actions in the cur-
rent beam and then choose the best K resulting spans
for the next step. The time complexity of the new de-
coding algorithm is O(Kn3) while the original one
is O(n3), where n is the length of the input sentence.
With the rich feature set in Table 1, the running time
of Intersect is longer than the time of Rescoring. But
Intersect considers more combination of spans with
the DLM-based features than Rescoring that is only
given a K-best list.
5 Implementation Details
5.1 Baseline parser
We implement our parsers based on the MSTParser1,
a freely available implementation of the graph-based
model proposed by (McDonald and Pereira, 2006).
We train a first-order parser on the training data (de-
scribed in Section 6.1) with the features defined in
McDonald et al (2005). We call this first-order
parser Baseline parser.
5.2 Build dependency language models
We use a large amount of unannotated data to build
the dependency language model. We first perform
word segmentation (if needed) and part-of-speech
tagging. After that, we obtain the word-segmented
sentences with the part-of-speech tags. Then the
sentences are parsed by the Baseline parser. Finally,
we obtain the auto-parsed data.
Given the dependency trees, we estimate the prob-
ability distribution by relative frequency:
Pu(xch|HIS) =
count(xch,HIS)
?
x?ch
count(x?ch,HIS)
(7)
No smoothing is performed because we use the
mapping function for the feature representations.
5.3 Mapping function
We can define different mapping functions for the
feature representations. Here, we use a simple way.
First, the probabilities are sorted in decreasing order.
Let No(Pu(ch)) be the position number of Pu(ch)
in the sorted list. The mapping function is:
1http://mstparser.sourceforge.net
?(Pu(ch)) =
{ PH if No(Pu(ch)) ? TOP10
PM if TOP10 < No(Pu(ch)) ? TOP30
PL if TOP30 < No(Pu(ch))
PO if Pu(ch)) = 0
where TOP10 and TOP 30 refer to the position num-
bers of top 10% and top 30% respectively. The num-
bers, 10% and 30%, are tuned on the development
sets in the experiments.
6 Experiments
We conducted experiments on English and Chinese
data.
6.1 Data sets
For English, we used the Penn Treebank (Marcus et
al., 1993) in our experiments. We created a stan-
dard data split: sections 2-21 for training, section
22 for development, and section 23 for testing. Tool
?Penn2Malt?2 was used to convert the data into de-
pendency structures using a standard set of head
rules (Yamada and Matsumoto, 2003). Following
the work of (Koo et al, 2008), we used the MX-
POST (Ratnaparkhi, 1996) tagger trained on training
data to provide part-of-speech tags for the develop-
ment and the test set, and used 10-way jackknifing
to generate part-of-speech tags for the training set.
For the unannotated data, we used the BLLIP corpus
(Charniak et al, 2000) that contains about 43 million
words of WSJ text.3 We used the MXPOST tagger
trained on training data to assign part-of-speech tags
and used the Baseline parser to process the sentences
of the BLLIP corpus.
For Chinese, we used the Chinese Treebank
(CTB) version 4.04 in the experiments. We also used
the ?Penn2Malt? tool to convert the data and cre-
ated a data split: files 1-270 and files 400-931 for
training, files 271-300 for testing, and files 301-325
for development. We used gold standard segmenta-
tion and part-of-speech tags in the CTB. The data
partition and part-of-speech settings were chosen to
match previous work (Chen et al, 2008; Yu et al,
2008; Chen et al, 2009). For the unannotated data,
we used the XIN CMN portion of Chinese Giga-
word5 Version 2.0 (LDC2009T14) (Huang, 2009),
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
3We ensured that the text used for extracting subtrees did not
include the sentences of the Penn Treebank.
4http://www.cis.upenn.edu/?chinese/.
5We excluded the sentences of the CTB data from the Giga-
word data
217
which has approximately 311 million words whose
segmentation and POS tags are given. We discarded
the annotations due to the differences in annotation
policy between CTB and this corpus. We used the
MMA system (Kruengkrai et al, 2009) trained on
the training data to perform word segmentation and
POS tagging and used the Baseline parser to parse
all the sentences in the data.
6.2 Features for basic and enhanced parsers
The previous studies have defined four types of
features: (FT1) the first-order features defined in
McDonald et al (2005), (FT2SB) the second-order
parent-siblings features defined in McDonald and
Pereira (2006), (FT2GC) the second-order parent-
child-grandchild features defined in Carreras (2007),
and (FT3) the third-order features defined in (Koo
and Collins, 2010).
We used the first- and second-order parsers of
the MSTParser as the basic parsers. Then we en-
hanced them with other higher-order features us-
ing beam-search. Table 2 shows the feature set-
tings of the systems, where MST1/2 refers to the ba-
sic first-/second-order parser and MSTB1/2 refers to
the enhanced first-/second-order parser. MSTB1 and
MSTB2 used the same feature setting, but used dif-
ferent order models. This resulted in the difference
of using FT2SB (beam-search in MSTB1 vs exact-
inference in MSTB2). We used these four parsers as
the Baselines in the experiments.
System Features
MST1 (FT1)
MSTB1 (FT1)+(FT2SB+FT2GC+FT3)
MST2 (FT1+FT2SB)
MSTB2 (FT1+FT2SB)+(FT2GC+FT3)
Table 2: Baseline parsers
We measured the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens (excluding all punctuation tokens) with the cor-
rect HEAD. In the following experiments, we used
?Inter? to refer to the parser with Intersect, and
?Rescore? to refer to the parser with Rescoring.
6.3 Development experiments
Since the setting of K (for beam search) affects our
parsers, we studied its influence on the development
set for English. We added the DLM-based features
to MST1. Figure 4 shows the UAS curves on the
development set, where K is beam size for Inter-
sect and K-best for Rescoring, the X-axis represents
K, and the Y-axis represents the UAS scores. The
parsing performance generally increased as the K
increased. The parser with Intersect always outper-
formed the one with Rescoring.
 0.912
 0.914
 0.916
 0.918
 0.92
 0.922
 0.924
 0.926
 0.928
1 2 4 8 16
UA
S
K
Rescore
Inter
Figure 4: The influence of K on the development data
K 1 2 4 8 16
English 157.1 247.4 351.9 462.3 578.2
Table 3: The parsing times on the development set (sec-
onds for all the sentences)
Table 3 shows the parsing times of Intersect on
the development set for English. By comparing the
curves of Figure 4, we can see that, while using
larger K reduced the parsing speed, it improved the
performance of our parsers. In the rest of the ex-
periments, we set K=8 in order to obtain the high
accuracy with reasonable speed and used Intersect
to add the DLM-based features.
N 0 1 2 3 4
English 91.30 91.87 92.52 92.72 92.72
Chinese 87.36 87.96 89.33 89.92 90.40
Table 4: Effect of different N-gram DLMs
Then, we studied the effect of adding different N-
gram DLMs to MST1. Table 4 shows the results.
From the table, we found that the parsing perfor-
mance roughly increased as the N increased. When
N=3 and N=4, the parsers obtained the same scores
for English. For Chinese, the parser obtained the
best score when N=4. Note that the size of the Chi-
nese unannotated data was larger than that of En-
glish. In the rest of the experiments, we used 3-gram
for English and 4-gram for Chinese.
218
6.4 Main results on English data
We evaluated the systems on the testing data for En-
glish. The results are shown in Table 5, where -
DLM refers to adding the DLM-based features to the
Baselines. The parsers using the DLM-based fea-
tures consistently outperformed the Baselines. For
the basic models (MST1/2), we obtained absolute
improvements of 0.94 and 0.63 points respectively.
For the enhanced models (MSTB1/2), we found that
there were 0.63 and 0.66 points improvements re-
spectively. The improvements were significant in
McNemar?s Test (p < 10?5)(Nivre et al, 2004).
Order1 UAS Order2 UAS
MST1 90.95 MST2 91.71
MST-DLM1 91.89 MST-DLM2 92.34
MSTB1 91.92 MSTB2 92.10
MSTB-DLM1 92.55 MSTB-DLM2 92.76
Table 5: Main results for English
6.5 Main results on Chinese data
The results are shown in Table 6, where the abbrevi-
ations used are the same as those in Table 5. As in
the English experiments, the parsers using the DLM-
based features consistently outperformed the Base-
lines. For the basic models (MST1/2), we obtained
absolute improvements of 4.28 and 3.51 points re-
spectively. For the enhanced models (MSTB1/2),
we got 3.00 and 2.93 points improvements respec-
tively. We obtained large improvements on the Chi-
nese data. The reasons may be that we use the very
large amount of data and 4-gram DLM that captures
high-order information. The improvements were
significant in McNemar?s Test (p < 10?7).
Order1 UAS Order2 UAS
MST1 86.38 MST2 88.11
MST-DLM1 90.66 MST-DLM2 91.62
MSTB1 88.38 MSTB2 88.66
MSTB-DLM1 91.38 MSTB-DLM2 91.59
Table 6: Main results for Chinese
6.6 Compare with previous work on English
Table 7 shows the performance of the graph-based
systems that were compared, where McDonald06
refers to the second-order parser of McDonald
and Pereira (2006), Koo08-standard refers to the
second-order parser with the features defined in
Koo et al (2008), Koo10-model1 refers to the
third-order parser with model1 of Koo and Collins
(2010), Koo08-dep2c refers to the second-order
parser with cluster-based features of (Koo et al,
2008), Suzuki09 refers to the parser of Suzuki et
al. (2009), Chen09-ord2s refers to the second-order
parser with subtree-based features of Chen et al
(2009), and Zhou11 refers to the second-order parser
with web-derived selectional preference features of
Zhou et al (2011).
The results showed that our MSTB-DLM2 ob-
tained the comparable accuracy with the previous
state-of-the-art systems. Koo10-model1 (Koo and
Collins, 2010) used the third-order features and
achieved the best reported result among the super-
vised parsers. Suzuki2009 (Suzuki et al, 2009) re-
ported the best reported result by combining a Semi-
supervised Structured Conditional Model (Suzuki
and Isozaki, 2008) with the method of (Koo et al,
2008). However, their decoding complexities were
higher than ours and we believe that the performance
of our parser can be further enhanced by integrating
their methods with our parser.
Type System UAS Cost
G
McDonald06 91.5 O(n3)
Koo08-standard 92.02 O(n4)
Koo10-model1 93.04 O(n4)
S
Koo08-dep2c 93.16 O(n4)
Suzuki09 93.79 O(n4)
Chen09-ord2s 92.51 O(n3)
Zhou11 92.64 O(n4)
D MSTB-DLM2 92.76 O(Kn3)
Table 7: Relevant results for English. G denotes the su-
pervised graph-based parsers, S denotes the graph-based
parsers with semi-supervised methods, D denotes our
new parsers
6.7 Compare with previous work on Chinese
Table 8 shows the comparative results, where
Chen08 refers to the parser of (Chen et al, 2008),
Yu08 refers to the parser of (Yu et al, 2008), Zhao09
refers to the parser of (Zhao et al, 2009), and
Chen09-ord2s refers to the second-order parser with
subtree-based features of Chen et al (2009). The
results showed that our score for this data was the
219
best reported so far and significantly higher than the
previous scores.
System UAS
Chen08 86.52
Yu08 87.26
Zhao09 87.0
Chen09-ord2s 89.43
MSTB-DLM2 91.59
Table 8: Relevant results for Chinese
7 Analysis
Dependency parsers tend to perform worse on heads
which have many children. Here, we studied the ef-
fect of DLM-based features for this structure. We
calculated the number of children for each head and
listed the accuracy changes for different numbers.
We compared the MST-DLM1 and MST1 systems
on the English data. The accuracy is the percentage
of heads having all the correct children.
Figure 5 shows the results for English, where the
X-axis represents the number of children, the Y-
axis represents the accuracies, OURS refers to MST-
DLM1, and Baseline refers to MST1. For example,
for heads having two children, Baseline obtained
89.04% accuracy while OURS obtained 89.32%.
From the figure, we found that OURS achieved bet-
ter performance consistently in all cases and when
the larger the number of children became, the more
significant the performance improvement was.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
Number of children
Baseline
OURS
Figure 5: Improvement relative to numbers of children
8 Related work
Several previous studies related to our work have
been conducted.
Koo et al (2008) used a clustering algorithm to
produce word clusters on a large amount of unan-
notated data and represented new features based on
the clusters for dependency parsing models. Chen
et al (2009) proposed an approach that extracted
partial tree structures from a large amount of data
and used them as the additional features to im-
prove dependency parsing. They approaches were
still restricted in a small number of arcs in the
graphs. Suzuki et al (2009) presented a semi-
supervised learning approach. They extended a
Semi-supervised Structured Conditional Model (SS-
SCM)(Suzuki and Isozaki, 2008) to the dependency
parsing problem and combined their method with
the approach of Koo et al (2008). In future work,
we may consider apply their methods on our parsers
to improve further.
Another group of methods are the co-
training/self-training techniques. McClosky et
al. (2006) presented a self-training approach for
phrase structure parsing. Sagae and Tsujii (2007)
used the co-training technique to improve perfor-
mance. First, two parsers were used to parse the
sentences in unannotated data. Then they selected
some sentences which have the same trees produced
by those two parsers. They retrained a parser on
newly parsed sentences and the original labeled
data. We are able to use the output of our systems
for co-training/self-training techniques.
9 Conclusion
We have presented an approach to utilizing the de-
pendency language model to improve graph-based
dependency parsing. We represent new features
based on the dependency language model and in-
tegrate them in the decoding algorithm directly us-
ing beam-search. Our approach enriches the feature
representations but without increasing the decoding
complexity. When tested on both English and Chi-
nese data, our parsers provided very competitive per-
formance compared with the best systems on the En-
glish data and achieved the best performance on the
Chinese data in the literature.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
220
CoNLL-X. SIGNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP 2008.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Michael A. Covington. 2001. A dundamental algorithm
for dependency parsing. In Proceedings of the 39th
Annual ACM Southeast Conference, pages 95?102.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING1996, pages 340?345.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Ver-
sion 2.0, LDC2009T14. Linguistic Data Consortium.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL, pages 122?131.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Association for Computational Linguistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of CoNLL 2004, pages
49?56.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 1044?1050.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-08: HLT,
pages 665?673, Columbus, Ohio, June. Association
for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMNLP2009, pages 551?560, Sin-
gapore, August. Association for Computational Lin-
guistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automati-
cally constructed case structures. In Proceedings of
Coling 2008, pages 1049?1056, Manchester, UK, Au-
gust.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In Proceedings of EMNLP
2008, pages 562?571, Honolulu, Hawaii, October.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
221
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-HLT2011, pages 1556?1565, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
222
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?443,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Accurate Shift-Reduce Constituent Parsing
Muhua Zhu?, Yue Zhang?, Wenliang Chen?, Min Zhang? and Jingbo Zhu?
?Natural Language Processing Lab., Northeastern University, China
?Singapore University of Technology and Design, Singapore
? Soochow University, China and Institute for Infocomm Research, Singapore
zhumuhua@gmail.com yue zhang@sutd.edu.sg
chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Shift-reduce dependency parsers give
comparable accuracies to their chart-
based counterparts, yet the best shift-
reduce constituent parsers still lag behind
the state-of-the-art. One important reason
is the existence of unary nodes in phrase
structure trees, which leads to different
numbers of shift-reduce actions between
different outputs for the same input. This
turns out to have a large empirical impact
on the framework of global training and
beam search. We propose a simple yet
effective extension to the shift-reduce
process, which eliminates size differences
between action sequences in beam-search.
Our parser gives comparable accuracies
to the state-of-the-art chart parsers. With
linear run-time complexity, our parser is
over an order of magnitude faster than the
fastest chart parser.
1 Introduction
Transition-based parsers employ a set of shift-
reduce actions and perform parsing using a se-
quence of state transitions. The pioneering mod-
els rely on a classifier to make local decisions, and
search greedily for a transition sequence to build a
parse tree. Greedy, classifier-based parsers have
been developed for both dependency grammars
(Yamada and Matsumoto, 2003; Nivre et al, 2006)
and phrase-structure grammars (Sagae and Lavie,
2005). With linear run-time complexity, they were
commonly regarded as a faster but less accurate
alternative to graph-based chart parsers (Collins,
1997; Charniak, 2000; McDonald et al, 2005).
Various methods have been proposed to address
the disadvantages of greedy local parsing, among
which a framework of beam-search and global
discriminative training have been shown effective
for dependency parsing (Zhang and Clark, 2008;
Huang and Sagae, 2010). While beam-search
reduces error propagation compared with greedy
search, a discriminative model that is globally op-
timized for whole sequences of transition actions
can avoid local score biases (Lafferty et al, 2001).
This framework preserves the most important ad-
vantage of greedy local parsers, including linear
run-time complexity and the freedom to define ar-
bitrary features. With the use of rich non-local fea-
tures, transition-based dependency parsers achieve
state-of-the-art accuracies that are comparable to
the best-graph-based parsers (Zhang and Nivre,
2011; Bohnet and Nivre, 2012). In addition, pro-
cessing tens of sentences per second (Zhang and
Nivre, 2011), these transition-based parsers can be
a favorable choice for dependency parsing.
The above global-learning and beam-search
framework can be applied to transition-based
phrase-structure (constituent) parsing also (Zhang
and Clark, 2009), maintaining all the afore-
mentioned benefits. However, the effects were
not as significant as for transition-based depen-
dency parsing. The best reported accuracies of
transition-based constituent parsers still lag behind
the state-of-the-art (Sagae and Lavie, 2006; Zhang
and Clark, 2009). One difference between phrase-
structure parsing and dependency parsing is that
for the former, parse trees with different numbers
of unary rules require different numbers of actions
to build. Hence the scoring model needs to disam-
biguate between transitions sequences with differ-
ent sizes. For the same sentence, the largest output
can take twice as many as actions to build as the
434
smallest one. This turns out to have a significant
empirical impact on parsing with beam-search.
We propose an extension to the shift-reduce pro-
cess to address this problem, which gives signifi-
cant improvements to the parsing accuracies. Our
method is conceptually simple, requiring only one
additional transition action to eliminate size dif-
ferences between different candidate outputs. On
standard evaluations using both the Penn Tree-
bank and the Penn Chinese Treebank, our parser
gave higher accuracies than the Berkeley parser
(Petrov and Klein, 2007), a state-of-the-art chart
parser. In addition, our parser runs with over 89
sentences per second, which is 14 times faster than
the Berkeley parser, and is the fastest that we are
aware of for phrase-structure parsing. An open
source release of our parser (version 0.6) is freely
available on the Web. 1
In addition to the above contributions, we apply
a variety of semi-supervised learning techniques to
our transition-based parser. These techniques have
been shown useful to improve chart-based pars-
ing (Koo et al, 2008; Chen et al, 2012), but little
work has been done for transition-based parsers.
We therefore fill a gap in the literature by report-
ing empirical results using these methods. Experi-
mental results show that semi-supervised methods
give a further improvement of 0.9% in F-score on
the English data and 2.4% on the Chinese data.
Our Chinese results are the best that we are aware
of on the standard CTB data.
2 Baseline parser
We adopt the parser of Zhang and Clark (2009) for
our baseline, which is based on the shift-reduce
process of Sagae and Lavie (2005), and employs
global perceptron training and beam search.
2.1 Vanilla Shift-Reduce
Shift-reduce parsing is based on a left-to-right
scan of the input sentence. At each step, a tran-
sition action is applied to consume an input word
or construct a new phrase-structure. A stack
is used to maintain partially constructed phrase-
structures, while the input words are stored in a
buffer. The set of transition actions are
? SHIFT: pop the front word from the buffer,
and push it onto the stack.
1http://sourceforge.net/projects/zpar/
Axioms [?, 0, false,0]
Goal [S, n, true, C]
Inference Rules:
[S, i, false, c]
SHIFT [S|w, i + 1, false, c + cs]
[S|s1s0, i, false, c]
REDUCE-L/R-X [S|X, i, false, c + cr]
[S|s0, i, false, c]
UNARY-X [S|X, i, false, c + cu]
[S, n, false, c]
FINISH [S, n, true, c + cf ]
Figure 1: Deduction system of the baseline shift-
reduce parsing process.
? REDUCE-L/R-X: pop the top two con-
stituents off the stack, combine them into a
new constituent with label X, and push the
new constituent onto the stack.
? UNARY-X: pop the top constituent off the
stack, raise it to a new constituent with la-
bel X, and push the new constituent onto the
stack.
? FINISH: pop the root node off the stack and
ends parsing.
The deduction system for the process is shown
in Figure 1, where the item is formed as ?stack,
buffer front index, completion mark, score?, and
cs, cr , and cu represent the incremental score of
the SHIFT, REDUCE, and UNARY parsing steps,
respectively; these scores are calculated according
to the context features of the parser state item. n
is the number of words in the input.
2.2 Global Discriminative Training and
Beam-Search
For a given input sentence, the initial state has an
empty stack and a buffer that contains all the input
words. An agenda is used to keep the k best state
items at each step. At initialization, the agenda
contains only the initial state. At each step, every
state item in the agenda is popped and expanded
by applying a valid transition action, and the top
k from the newly constructed state items are put
back onto the agenda. The process repeats until
the agenda is empty, and the best completed state
item (recorded as candidate output) is taken for
435
Description Templates
unigrams s0tc, s0wc, s1tc, s1wc, s2tc
s2wc, s3tc, s3wc, q0wt, q1wt
q2wt, q3wt, s0lwc, s0rwc
s0uwc, s1lwc, s1rwc, s1uwc
bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c,
s0wq0w, s0wq0t, s0cq0w, s0cq0t,
q0wq1w, q0wq1t, q0tq1w, q0tq1t,
s1wq0w, s1wq0t, s1cq0w, s1cq0t
trigrams s0cs1cs2c, s0ws1cs2c, s0cs1wq0t
s0cs1cs2w, s0cs1cq0t, s0ws1cq0t
s0cs1wq0t, s0cs1cq0w
Table 1: A summary of baseline feature templates,
where si represents the ith item on the stack S and
qi denotes the ith item in the queue Q. w refers to
the head lexicon, t refers to the head POS, and c
refers to the constituent label.
the output.
The score of a state item is the total score of the
transition actions that have been applied to build
the item:
C(?) =
N?
i=1
?(ai) ? ~?
Here ?(ai) represents the feature vector for the ith
action ai in state item ?. It is computed by apply-
ing the feature templates in Table 1 to the context
of ?. N is the total number of actions in ?.
The model parameter ~? is trained with the aver-
aged perceptron algorithm, applied to state items
(sequence of actions) globally. We apply the early
update strategy (Collins and Roark, 2004), stop-
ping parsing for parameter updates when the gold-
standard state item falls off the agenda.
2.3 Baseline Features
Our baseline features are adopted from Zhang and
Clark (2009), and are shown in Table 1 Here si
represents the ith item on the top of the stack S
and qi denotes the ith item in the front end of the
queue Q. The symbol w denotes the lexical head
of an item; the symbol c denotes the constituent
label of an item; the symbol t is the POS of a lex-
ical head. These features are adapted from Zhang
and Clark (2009). We remove Chinese specific
features and make the baseline parser language-
independent.
3 Improved hypotheses comparison
Unlike dependency parsing, constituent parse
trees for the same sentence can have different
numbers of nodes, mainly due to the existence
of unary nodes. As a result, completed state
NP
NN
address
NNS
issues
VP
VB
address
NP
NNS
issues
Figure 2: Example parse trees of the same sen-
tence with different numbers of actions.
items for the same sentence can have different
numbers of unary actions. Take the phrase ?ad-
dress issues? for example, two possible parses
are shown in Figure 2 (a) and (b), respectively.
The first parse corresponds to the action sequence
[SHIFT, SHIFT, REDUCE-R-NP, FINISH], while
the second parse corresponds to the action se-
quence [SHIFT, SHIFT, UNARY-NP, REDUCE-L-
VP, FINISH], which consists of one more action
than the first case. In practice, variances between
state items can be much larger than the chosen ex-
ample. In the extreme case where a state item does
not contain any unary action, the number of ac-
tions is 2n, where n is the number of words in
the sentence. On the other hand, if the maximum
number of consequent unary actions is 2 (Sagae
and Lavie, 2005; Zhang and Clark, 2009), then the
maximum number of actions a state item can have
is 4n.
The significant variance in the number of ac-
tions N can have an impact on the linear sepa-
rability of state items, for which the feature vec-
tors are
?N
i=1 ? (ai). This turns out to have a sig-
nificant empirical influence on perceptron training
with early-update, where the training of the model
interacts with search (Daume III, 2006).
One way of improving the comparability of
state items is to reduce the differences in their
sizes, and we use a padding method to achieve
this. The idea is to extend the set of actions by
adding an IDLE action, so that completed state
items can be further expanded using the IDLE ac-
tion. The action does not change the state itself,
but simply adds to the number of actions in the
sequence. A feature vector is extracted for the
IDLE action according to the final state context,
in the same way as other actions. Using the IDLE
action, the transition sequence for the two parses
in Figure 2 can be [SHIFT, SHIFT, REDUCE-
NP, FINISH, IDLE] and [SHIFT, SHIFT, UNARY-
NP, REDUCE-L-VP, FINISH], respectively. Their
436
Axioms [?, 0, false, 0, 0]
Goal [S, n, true, m : 2n ? m ? 4n, C]
Inference Rules:
[S, i, false, k,c]
SHIFT [S|w, i + 1, false, k + 1, c + cs]
[S|s1s0, i, false, k, c]
REDUCE-L/R-X [S|X, i, false, k + 1, c + cr]
[S|s0, i, false, k, c]
UNARY-X [S|X, i, false, k + 1, c + cu]
[S, n, false, k, c]
FINISH [S, n, true, k + 1, c + cf ]
[S,n, true, k, c]
IDLE [S, n, true, k + 1, c + ci]
Figure 3: Deductive system of the extended tran-
sition system.
corresponding feature vectors have about the same
sizes, and are more linearly separable. Note that
there can be more than one action that are padded
to a sequence of actions, and the number of IDLE
actions depends on the size difference between the
current action sequence and the largest action se-
quence without IDLE actions.
Given this extension, the deduction system is
shown in Figure 3. We add the number of actions
k to an item. The initial item (Axioms) has k = 0,
while the goal item has 2n ? k ? 4n. Given this
process, beam-search decoding can be made sim-
pler than that of Zhang and Clark (2009). While
they used a candidate output to record the best
completed state item, and finish decoding when
the agenda contains no more items, we can sim-
ply finish decoding when all items in the agenda
are completed, and output the best state item in
the agenda. With this new transition process, we
experimented with several extended features,and
found that the templates in Table 2 are useful to
improve the accuracies further. Here sill denotes
the left child of si?s left child. Other notations can
be explained in a similar way.
4 Semi-supervised Parsing with Large
Data
This section discusses how to extract informa-
tion from unlabeled data or auto-parsed data to
further improve shift-reduce parsing accuracies.
We consider three types of information, including
s0llwc, s0lrwc, s0luwc
s0rlwc, s0rrwc, s0ruwc
s0ulwc, s0urwc, s0uuwc
s1llwc, s1lrwc, s1luwc
s1rlwc, s1rrwc, s1ruwc
Table 2: New features for the extended parser.
paradigmatic relations, dependency relations, and
structural relations. These relations are captured
by word clustering, lexical dependencies, and a
dependency language model, respectively. Based
on the information, we propose a set of novel fea-
tures specifically designed for shift-reduce con-
stituent parsing.
4.1 Paradigmatic Relations: Word
Clustering
Word clusters are regarded as lexical intermedi-
aries for dependency parsing (Koo et al, 2008)
and POS tagging (Sun and Uszkoreit, 2012). We
employ the Brown clustering algorithm (Liang,
2005) on unannotated data (word segmentation is
performed if necessary). In the initial state of clus-
tering, each word in the input corpus is regarded
as a cluster, then the algorithm repeatedly merges
pairs of clusters that cause the least decrease in
the likelihood of the input corpus. The clustering
results are a binary tree with words appearing as
leaves. Each cluster is represented as a bit-string
from the root to the tree node that represents the
cluster. We define a function CLU(w) to return the
cluster ID (a bit string) of an input word w.
4.2 Dependency Relations: Lexical
Dependencies
Lexical dependencies represent linguistic relations
between words: whether a word modifies another
word. The idea of exploiting lexical dependency
information from auto-parsed data has been ex-
plored before for dependency parsing (Chen et al,
2009) and constituent parsing (Zhu et al, 2012).
To extract lexical dependencies, we first run the
baseline parser on unlabeled data. To simplify
the extraction process, we can convert auto-parsed
constituency trees into dependency trees by using
Penn2Malt. 2 From the dependency trees, we ex-
tract bigram lexical dependencies ?w1, w2, L/R?
where the symbol L (R) means that w1 (w2) is the
head of w2 (w1). We also extract trigram lexical
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
437
dependencies ?w1, w2, w3, L/R?, where L means
that w1 is the head of w2 and w3, meanwhile w2
and w3 are required to be siblings.
Following the strategy of Chen et al (2009),
we assign categories to bigram and trigram items
separately according to their frequency counts.
Specifically, top-10% most frequent items are as-
signed to the category of High Frequency (HF);
otherwise if an item is among top 20%, we assign
it to the category of Middle Frequency (MF); oth-
erwise the category of Low Frequency (LF). Here-
after, we refer to the bigram and trigram lexical
dependency lists as BLD and TLD, respectively.
4.3 Structural Relations: Dependency
Language Model
The dependency language model is proposed by
Shen et al (2008) and is used as additional in-
formation for graph-based dependency parsing in
Chen et al (2012). Formally, given a depen-
dency tree y of an input sentence x, we can
denote by H(y) the set of words that have at
least one dependent. For each xh ? H(y), we
have a corresponding dependency structure Dh =
(xLk, . . . xL1, xh, xR1, . . . , xRm). The probability
P (Dh) is defined to be
P (Dh) = PL(Dh) ? PR(Dh)
where PL(Dh) can be in turn defined as:
PL(Dh) ? P (xL1|xh)
?P (xL2|xL1, xh)
? . . .
?P (xLk|xLk?1, . . . , xLk?N+1, xh)
PR(Dh) can be defined in a similar way.
We build dependency language models on auto-
parsed data. Again, we convert constituency trees
into dependency trees for the purpose of simplic-
ity. From the dependency trees, we build a bigram
and a trigram language model, which are denoted
by BLM and TLM, respectively. The following
are the templates of the records of the dependency
language models.
(1) ?xLi, xh, P (xLi|xh)?
(2) ?xRi, xh, P (xRi|xh)?
(3) ?xLi, xLi?1, xh, P (xLi|xLi?1, xh)?
(4) ?xRi, xRi?1, xh, P (xRi|xRi?1, xh)?
Here the templates (1) and (2) belong to BLM
and the templates (3) and (4) belong to TLM. To
Stat Train Dev Test Unlabeled
EN # sent 39.8k 1.7k 2.4k 3,139.1k# word 950.0k 40.1k 56.7k 76,041.4k
CH # sent 18.1k 350 348 11,810.7k# word 493.8k 8.0k 6.8k 269,057.2k
Table 4: Statistics on sentence and word numbers
of the experimental data.
use the dependency language models, we employ
a map function ?(r) to assign a category to each
record r according to its probability, as in Chen et
al. (2012). The following is the map function.
?(r) =
?
??
??
HP if P (r) ? top?10%
MP else if P (r) ? top?30%
LP otherwise
4.4 Semi-supervised Features
We design a set of features based on the infor-
mation extracted from auto-parsed data or unan-
notated data. The features are summarized in Ta-
ble 3. Here CLU returns a cluster ID for a word.
The functions BLDl/r(?), TLDl/r(?), BLMl/r(?),
and TLMl/r(?) check whether a given word com-
bination can be found in the corresponding lists.
For example, BLDl(s1w, s0w) returns a category
tag (HF, MF, or LF) if ?s1w, s0w,L? exits in the
list BLD, else it returns NONE.
5 Experiments
5.1 Set-up
Labeled English data employed in this paper were
derived from the Wall Street Journal (WSJ) corpus
of the Penn Treebank (Marcus et al, 1993). We
used sections 2-21 as labeled training data, section
24 for system development, and section 23 for fi-
nal performance evaluation. For labeled Chinese
data, we used the version 5.1 of the Penn Chinese
Treebank (CTB) (Xue et al, 2005). Articles 001-
270 and 440-1151 were used for training, articles
301-325 were used as development data, and arti-
cles 271-300 were used for evaluation.
For both English and Chinese data, we used ten-
fold jackknifing (Collins, 2000) to automatically
assign POS tags to the training data. We found that
this simple technique could achieve an improve-
ment of 0.4% on English and an improvement of
2.0% on Chinese. For English POS tagging, we
adopted SVMTool, 3 and for Chinese POS tagging
3http://www.lsi.upc.edu/?nlp/SVMTool/
438
Word Cluster Features
CLU(s1w) CLU(s0w) CLU(q0w)
CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0w
Lexical Dependency Features
BLDl(s1w, s0w) BLDl(s1w, s0w)?s1t?s0t BLDr(s1w, s0w)
BLDr(s1w, s0w)?s1t?s0t BLDl(s1w, q0w)?s1t?q0t BLDl(s1w, q0w)
BLDr(s1w, q0w) BLDr(s1w, q0w)?s1t?q0t BLDl(s0w, q0w)
BLDl(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)
TLDl(s1w, s1rdw, s0w) TLDl(s1w, s1rdw, s0w)?s1t?s0t TLDr(s1w, s0ldw, s0w)
TLDr(s1w, s0ldw, s0w)?s1t?s0t TLDl(s0w, s0rdw, q0w)?s0t?q0t TLDl(s0w, s0rdw, q0w)
TLDr(s0w,NONE, q0w) TLDr(s0w,NONE, q0w)?s0t?q0t
Dependency Language Model Features
BLMl(s1w, s0w) BLMl(s1w, s0w)?s1t?s0t BLMr(s1w, s0w)
BLMr(s1w, s0w)?s1t?s0t BLMl(s0w, q0w) BLMl(s0w, q0w)?s0t?q0t
BLMr(s0w, q0w)?s0t?q0t BLMr(s0w, q0w) TLMl(s1w, s1rdw, s0w)
TLMl(s1w, s1rdw, s0w)?s1t?s0t TLMr(s1w, s0ldw, s0w) TLMr(s1w, s0ldw, s0w)?s1t?s0t
Table 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, and
dependency language models. Here the symbol si denotes a stack item, qi denotes a queue item, w
represents a word, and t represents a POS tag.
Lan. System LR LP F1
EN
G Baseline 88.4 88.7 88.6
+padding 88.8 89.5 89.1
+features 89.0 89.7 89.3
CH
N Baseline 85.6 86.3 86.0
+padding 85.5 87.2 86.4
+features 85.5 87.6 86.5
Table 5: Experimental results on the English and
Chinese development sets with the padding tech-
nique and new supervised features added incre-
mentally.
we employed the Stanford POS tagger. 4
We took the WSJ articles from the TIPSTER
corpus (LDC93T3A) as unlabeled English data. In
addition, we removed from the unlabeled English
data the sentences that appear in the WSJ corpus
of the Penn Treebank. For unlabeled Chinese data,
we used Chinese Gigaword (LDC2003T09), on
which we conducted Chinese word segmentation
by using a CRF-based segmenter. Table 4 summa-
rizes data statistics on sentence and word numbers
of the data sets listed above.
We used EVALB to evaluate parser perfor-
mances, including labeled precision (LP), labeled
recall (LR), and bracketing F1. 5 For significance
tests, we employed the randomized permutation-
based tool provided by Daniel Bikel. 6
In both training and decoding, we set the beam
size to 16, which achieves a good tradeoff be-
tween efficiency and accuracy. The optimal iter-
ation number of perceptron learning is determined
4http://nlp.stanford.edu/software/tagger.shtml
5http://nlp.cs.nyu.edu/evalb
6http://www.cis.upenn.edu/?dbikel/software.html#comparator
Lan. Features LR LP F1
EN
G +word cluster 89.3 90.0 89.7
+lexical dependencies 89.7 90.3 90.0
+dependency LM 90.0 90.6 90.3
CH
N +word cluster 85.7 87.5 86.6
+lexical dependencies 87.2 88.6 87.9
+dependency LM 87.2 88.7 88.0
Table 6: Experimental results on the English and
Chinese development sets with different types of
semi-supervised features added incrementally to
the extended parser.
on the development sets. For word clustering, we
set the cluster number to 50 for both the English
and Chinese experiments.
5.2 Results on Development Sets
Table 5 reports the results of the extended parser
(baseline + padding + supervised features) on the
English and Chinese development sets. We inte-
grated the padding method into the baseline parser,
based on which we further incorporated the super-
vised features in Table 2. From the results we find
that the padding method improves the parser accu-
racies by 0.5% and 0.4% on English and Chinese,
respectively. Incorporating the supervised features
in Table 2 gives further improvements of 0.2% on
English and 0.1% on Chinese.
Based on the extended parser, we experimented
different types of semi-supervised features by
adding the features incrementally. The results are
shown in Table 6. By comparing the results in Ta-
ble 5 and the results in Table 6 we can see that the
semi-supervised features achieve an overall im-
provement of 1.0% on the English data and an im-
439
Type Parser LR LP F1
SI
Ratnaparkhi (1997) 86.3 87.5 86.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.5 89.9 89.5
Sagae & Lavie (2005)? 86.1 86.0 86.0
Sagae & Lavie (2006)? 87.8 88.1 87.9
Baseline 90.0 89.9 89.9
Petrov & Klein (2007) 90.1 90.2 90.1
Baseline+Padding 90.2 90.7 90.4
Carreras et al (2008) 90.7 91.4 91.1
RE Charniak & Johnson (2005) 91.2 91.8 91.5Huang (2008) 92.2 91.2 91.7
SE
Zhu et al (2012)? 90.4 90.5 90.4
Baseline+Padding+Semi 91.1 91.5 91.3
Huang & Harper (2009) 91.1 91.6 91.3
Huang et al (2010)? 91.4 91.8 91.6
McClosky et al (2006) 92.1 92.5 92.3
Table 7: Comparison of our parsers and related
work on the English test set. ? Shift-reduce
parsers. ? The results of self-training with a sin-
gle latent annotation grammar.
Type Parser LR LP F1
SI
Charniak (2000)? 79.6 82.1 80.8
Bikel (2004)? 79.3 82.0 80.6
Baseline 82.1 83.1 82.6
Baseline+Padding 82.1 84.3 83.2
Petrov & Klein (2007) 81.9 84.8 83.3
RE Charniak & Johnson (2005)? 80.8 83.8 82.3
SE Zhu et al (2012) 80.6 81.9 81.2Baseline+Padding+Semi 84.4 86.8 85.6
Table 8: Comparison of our parsers and related
work on the test set of CTB5.1.? Huang (2009)
adapted the parsers to Chinese parsing on CTB5.1.
? We run the parser on CTB5.1 to get the results.
provement of 1.5% on the Chinese data.
5.3 Final Results
Here we report the final results on the English and
Chinese test sets. We compared the final results
with a large body of related work. We grouped the
parsers into three categories: single parsers (SI),
discriminative reranking parsers (RE), and semi-
supervised parsers (SE). Table 7 shows the com-
parative results on the English test set and Table 8
reports the comparison on the Chinese test set.
From the results we can see that our extended
parser (baseline + padding + supervised features)
outperforms the Berkeley parser by 0.3% on En-
glish, and is comparable with the Berkeley parser
on Chinese (?0.1% less). Here +padding means
the padding technique and the features in Table 2.
After integrating semi-supervised features, the
parsing accuracy on English is improved to 91.3%.
We note that the performance is on the same level
Parser #Sent/Second
Ratnaparkhi (1997) Unk
Collins (1999) 3.5
Charniak (2000) 5.7
Sagae & Lavie (2005)? 3.7?
Sagae & Lavie (2006)? 2.2?
Petrov & Klein (2007) 6.2
Carreras et al (2008) Unk
This Paper
Baseline 100.7
Baseline+Padding 89.5
Baseline+Padding+Semi 46.8
Table 9: Comparison of running times on the En-
glish test set, where the time for loading models
is excluded. ? The results of SVM-based shift-
reduce parsing with greedy search. ? The results of
MaxEnt-based shift-reduce parser with best-first
search. ? Times reported by authors running on
different hardware.
as the performance of self-trained parsers, except
for McClosky et al (2006), which is based on the
combination of reranking and self-training. On
Chinese, the final parsing accuracy is 85.6%. To
our knowledge, this is by far the best reported per-
formance on this data set.
The padding technique, supervised features,
and semi-supervised features achieve an overall
improvement of 1.4% over the baseline on En-
glish, which is significant on the level of p <
10?5. The overall improvement on Chinese is
3.0%, which is also significant on the level of
p < 10?5.
5.4 Comparison of Running Time
We also compared the running times of our parsers
with the related single parsers. We ran timing tests
on an Intel 2.3GHz processor with 8GB mem-
ory. The comparison is shown in Table 9. From
the table, we can see that incorporating semi-
supervised features decreases parsing speed, but
the semi-supervised parser still has the advantage
of efficiency over other parsers. Specifically, the
semi-supervised parser is 7 times faster than the
Berkeley parser. Note that Sagae & Lavie (2005)
and Sagae & Lavie (2006) are also shift-reduce
parsers, and their running times were evaluated on
different hardwares. In practice, the running times
of the shift-reduce parsers should be much shorter
than the reported times in the table.
5.5 Error Analysis
We conducted error analysis for the three sys-
tems: the baseline parser, the extended parser with
440
 86
 88
 90
 92
 94
1 2 3 4 5 6 7 8
F 
Sc
or
e
Span Length
Baseline
Extended
Semi-supervised
Figure 5: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parsers on spans of different lengths.
the padding technique, and the semi-supervised
parser, focusing on the English test set. The analy-
sis was performed in four dimensions: parsing ac-
curacies on different phrase types, on constituents
of different span lengths, on different sentence
lengths, and on sentences with different numbers
of unknown words.
5.5.1 Different Phrase Types
Table 10 shows the parsing accuracies of the base-
line, extended parser, and semi-supervised parser
on different phrase types. Here we only consider
the nine most frequent phrase types in the English
test set. In the table, the phrase types are ordered
from left to right in the descending order of their
frequencies. We also show the improvements of
the semi-supervised parser over the baseline parser
(the last row in the table). As the results show, the
extended parser achieves improvements on most
of the phrase types with two exceptions: Preposi-
tion Prase (PP) and Quantifier Phrase (QP). Semi-
supervised features further improve parsing accu-
racies over the extended parser (QP is an excep-
tion). From the last row, we can see that improve-
ments of the semi-supervised parser over the base-
line on VP, S, SBAR, ADVP, and ADJP are above
the average improvement (1.4%).
5.5.2 Different Span Lengths
Figure 5 shows a comparison of the three parsers
on spans of different lengths. Here we consider
span lengths up to 8. As the results show, both
the padding extension and semi-supervised fea-
tures are more helpful on relatively large spans:
the performance gaps between the three parsers
are enlarged with increasing span lengths.
 82
 84
 86
 88
 90
 92
 94
10 20 30 40 50 60 70
F 
Sc
or
e
Sentence Length
Baseline
Extended
Semi-supervised
Figure 6: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parser on sentences of different lengths.
5.5.3 Different Sentence Lengths
Figure 6 shows a comparison of parsing accura-
cies of the three parsers on sentences of different
lengths. Each number on the horizontal axis repre-
sents the sentences whose lengths are between the
number and its previous number. For example, the
number 30 refers to the sentences whose lengths
are between 20 and 30. From the results we can
see that semi-supervised features improve parsing
accuracy on both short and long sentences. The
points at 70 are exceptions. In fact, sentences with
lengths between 60 and 70 have only 8 instances,
and the statistics on such a small number of sen-
tences are not reliable.
5.5.4 Different Numbers of Unknown Words
Figure 4 shows a comparison of parsing accura-
cies of the baseline, extended parser, and semi-
supervised parser on sentences with different num-
bers of unknown words. As the results show,
the padding method is not very helpful on sen-
tences with large numbers of unknown words,
while semi-supervised features help significantly
on this aspect. This conforms to the intuition that
semi-supervised methods reduce data sparseness
and improve the performance on unknown words.
6 Conclusion
In this paper, we addressed the problem of dif-
ferent action-sequence lengths for shift-reduce
phrase-structure parsing, and designed a set of
novel non-local features to further improve pars-
ing. The resulting supervised parser outperforms
the Berkeley parser, a state-of-the-art chart parser,
in both accuracies and speeds. In addition, we in-
corporated a set of semi-supervised features. The
441
System NP VP S PP SBAR ADVP ADJP WHNP QP
Baseline 91.9 90.1 89.8 88.1 85.7 84.6 72.1 94.8 89.3
Extended 92.1 90.7 90.2 87.9 86.6 84.5 73.6 95.5 88.6
Semi-supervised 93.2 92.0 91.5 89.3 88.2 86.8 75.1 95.7 89.1
Improvements +1.3 +1.9 +1.7 +1.2 +2.5 +2.2 +3.0 +0.9 -0.2
Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers
on different phrase types.
0 1 2 3 4 5 6 7
70
80
90
100
91
.9
8
89
.7
3
88
.8
7
87
.9
6
85
.9
5
83
.7
81
.4
2
82
.7
4
92
.1
7
90
.5
3
89
.5
1
87
.9
9
88
.6
6
87
.3
3
83
.8
9
80
.4
9
92
.8
8
91
.2
6
90
.4
3
89
.8
8
90
.3
5
86
.3
9 90
.6
8
90
.2
4
F-
sc
o
re
(%
)
Baseline Extended Semi-supervised
Figure 4: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parser
on sentences of different unknown words.
final parser reaches an accuracy of 91.3% on En-
glish and 85.6% on Chinese, by far the best re-
ported accuracies on the CTB data.
Acknowledgements
We thank the anonymous reviewers for their valu-
able comments. Yue Zhang and Muhua Zhu
were supported partially by SRG-ISTD-2012-038
from Singapore University of Technology and De-
sign. Muhua Zhu and Jingbo Zhu were funded
in part by the National Science Foundation of
China (61073140; 61272376), Specialized Re-
search Fund for the Doctoral Program of Higher
Education (20100042110031), and the Fundamen-
tal Research Funds for the Central Universities
(N100204002). Wenliang Chen was funded par-
tially by the National Science Foundation of China
(61203314).
References
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
Ph.D. thesis, University of Pennsylvania.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 12?14, Jeju Island, Ko-
rea.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9?16, Manchester, England.
Eugune Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugune Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, Seattle, Washington, USA.
Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of EMNLP, pages 570?579, Singa-
pore.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012.
Utilizing dependency language models for graph-
based dependency. In Proceedings of ACL, pages
213?222, Jeju, Republic of Korea.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, Stroudsburg, PA, USA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
ACL, Madrid, Spain.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking
for natural language processing. In Proceedings of
ICML, pages 175?182, Stanford, CA, USA.
Hal Daume III. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
USC.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
442
across languages. In Proceedings of EMNLP, pages
832?841, Singapore.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077?1086, Uppsala,
Sweden.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent variable
grammars. In Proceedings of EMNLP, pages 12?22,
Massachusetts, USA.
Liang Huang. 2008. Forest reranking: discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Ohio, USA.
Liang-Ya Huang. 2009. Improve Chinese parsing with
Max-Ent reranking parser. In Master Project Re-
port, Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of ICML, pages 282?289, Massachusetts, USA,
June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewiz. 1993. Building a large anno-
tated corpus of English. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT/NAACL, Main Conference,
pages 152?159, New York City, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: a data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP, Rhode Island, USA.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT/NAACL,
Companion Volume: Short Papers, pages 129?132,
New York, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL, pages 577?585, Ohio, USA.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: to-
wards accurate Chinese part-of-speech tagging. In
Proceedings of ACL, Jeju, Republic of Korea.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206,
Nancy, France.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL/HLT, pages 888?896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193, Portland, Ore-
gon, USA.
Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012.
Exploiting lexical dependencies from large-scale
data for better shift-reduce constituency parsing. In
Proceedings of COLING, pages 3171?3186, Mum-
bai, India.
443
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457?467,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Ambiguity-aware Ensemble Training for Semi-supervised
Dependency Parsing
Zhenghua Li , Min Zhang?, Wenliang Chen
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University
{zhli13,minzhang,wlchen}@suda.edu.cn
Abstract
This paper proposes a simple yet
effective framework for semi-supervised
dependency parsing at entire tree level,
referred to as ambiguity-aware ensemble
training. Instead of only using 1-
best parse trees in previous work, our
core idea is to utilize parse forest
(ambiguous labelings) to combine
multiple 1-best parse trees generated
from diverse parsers on unlabeled data.
With a conditional random field based
probabilistic dependency parser, our
training objective is to maximize mixed
likelihood of labeled data and auto-parsed
unlabeled data with ambiguous labelings.
This framework offers two promising
advantages. 1) ambiguity encoded in
parse forests compromises noise in 1-best
parse trees. During training, the parser is
aware of these ambiguous structures, and
has the flexibility to distribute probability
mass to its preferred parse trees as long
as the likelihood improves. 2) diverse
syntactic structures produced by different
parsers can be naturally compiled into
forest, offering complementary strength
to our single-view parser. Experimental
results on benchmark data show that
our method significantly outperforms
the baseline supervised parser and
other entire-tree based semi-supervised
methods, such as self-training, co-training
and tri-training.
1 Introduction
Supervised dependency parsing has made great
progress during the past decade. However, it
is very difficult to further improve performance
?Correspondence author
of supervised parsers. For example, Koo and
Collins (2010) and Zhang and McDonald (2012)
show that incorporating higher-order features into
a graph-based parser only leads to modest increase
in parsing accuracy. In contrast, semi-supervised
approaches, which can make use of large-scale
unlabeled data, have attracted more and more
interest. Previously, unlabeled data is explored to
derive useful local-context features such as word
clusters (Koo et al, 2008), subtree frequencies
(Chen et al, 2009; Chen et al, 2013), and word
co-occurrence counts (Zhou et al, 2011; Bansal
and Klein, 2011). A few effective learning meth-
ods are also proposed for dependency parsing to
implicitly utilize distributions on unlabeled data
(Smith and Eisner, 2007; Wang et al, 2008;
Suzuki et al, 2009). All above work leads to
significant improvement on parsing accuracy.
Another line of research is to pick up some
high-quality auto-parsed training instances from
unlabeled data using bootstrapping methods, such
as self-training (Yarowsky, 1995), co-training
(Blum and Mitchell, 1998), and tri-training (Zhou
and Li, 2005). However, these methods gain
limited success in dependency parsing. Although
working well on constituent parsing (McClosky et
al., 2006; Huang and Harper, 2009), self-training
is shown unsuccessful for dependency parsing
(Spreyer and Kuhn, 2009). The reason may be that
dependency parsing models are prone to amplify
previous mistakes during training on self-parsed
unlabeled data. Sagae and Tsujii (2007) apply
a variant of co-training to dependency parsing
and report positive results on out-of-domain text.
S?gaard and Rish?j (2010) combine tri-training
and parser ensemble to boost parsing accuracy.
Both work employs two parsers to process the
unlabeled data, and only select as extra training
data sentences on which the 1-best parse trees of
the two parsers are identical. In this way, the auto-
parsed unlabeled data becomes more reliable.
457
w0
He
1
saw
2
a
3
deer
4
riding
5
a
6
bicycle
7
in
8
the
9
park
10
.
11
Figure 1: An example sentence with an ambiguous parse forest.
However, one obvious drawback of these methods
is that they are unable to exploit unlabeled data
with divergent outputs from different parsers.
Our experiments show that unlabeled data with
identical outputs from different parsers tends to be
short (18.25 words per sentence on average), and
only has a small proportion of 40% (see Table 6).
More importantly, we believe that unlabeled data
with divergent outputs is equally (if not more)
useful. Intuitively, an unlabeled sentence with
divergent outputs should contain some ambiguous
syntactic structures (such as preposition phrase
attachment) that are very hard to resolve and
lead to the disagreement of different parsers.
Such sentences can provide more discriminative
instances for training which may be unavailable
in labeled data.
To solve above issues, this paper proposes
a more general and effective framework for
semi-supervised dependency parsing, referred to
as ambiguity-aware ensemble training. Different
from traditional self/co/tri-training which only use
1-best parse trees on unlabeled data, our approach
adopts ambiguous labelings, represented by parse
forest, as gold-standard for unlabeled sentences.
Figure 1 shows an example sentence with an
ambiguous parse forest. The forest is formed by
two parse trees, respectively shown at the upper
and lower sides of the sentence. The differences
between the two parse trees are highlighted
using dashed arcs. The upper tree take ?deer?
as the subject of ?riding?, whereas the lower
one indicates that ?he? rides the bicycle. The
other difference is where the preposition phrase
(PP) ?in the park? should be attached, which
is also known as the PP attachment problem, a
notorious challenge for parsing. Reserving such
uncertainty has three potential advantages. First,
noise in unlabeled data is largely alleviated, since
parse forest encodes only a few highly possible
parse trees with high oracle score. Please note
that the parse forest in Figure 1 contains four
parse trees after combination of the two different
choices. Second, the parser is able to learn useful
features from the unambiguous parts of the parse
forest. Finally, with sufficient unlabeled data, it is
possible that the parser can learn to resolve such
uncertainty by biasing to more reasonable parse
trees.
To construct parse forest on unlabeled data, we
employ three supervised parsers based on different
paradigms, including our baseline graph-based
dependency parser, a transition-based dependency
parser (Zhang and Nivre, 2011), and a generative
constituent parser (Petrov and Klein, 2007). The
1-best parse trees of these three parsers are aggre-
gated in different ways. Evaluation on labeled data
shows the oracle accuracy of parse forest is much
higher than that of 1-best outputs of single parsers
(see Table 3). Finally, using a conditional random
field (CRF) based probabilistic parser, we train
a better model by maximizing mixed likelihood
of labeled data and auto-parsed unlabeled data
with ambiguous labelings. Experimental results
on both English and Chinese datasets demon-
strate that the proposed ambiguity-aware ensem-
ble training outperforms other entire-tree based
methods such as self/co/tri-training. In summary,
we make following contributions.
1. We propose a generalized ambiguity-aware
ensemble training framework for semi-
supervised dependency parsing, which can
458
make better use of unlabeled data, especially
when parsers from different views produce
divergent syntactic structures.
2. We first employ a generative constituent pars-
er for semi-supervised dependency parsing.
Experiments show that the constituent parser
is very helpful since it produces more diver-
gent structures for our semi-supervised parser
than discriminative dependency parsers.
3. We build the first state-of-the-art CRF-based
dependency parser. Using the probabilistic
parser, we benchmark and conduct systemat-
ic comparisons among ours and all previous
bootstrapping methods, including self/co/tri-
training.
2 Supervised Dependency Parsing
Given an input sentence x = w
0
w
1
...w
n
, the goal
of dependency parsing is to build a dependency
tree as depicted in Figure 1, denoted by d =
{(h,m) : 0 ? h ? n, 0 < m ? n}, where (h,m)
indicates a directed arc from the head word w
h
to the modifier w
m
, and w
0
is an artificial node
linking to the root of the sentence.
In parsing community, two mainstream meth-
ods tackle the dependency parsing problem from
different perspectives but achieve comparable ac-
curacy on a variety of languages. The graph-
based method views the problem as finding an
optimal tree from a fully-connected directed graph
(McDonald et al, 2005; McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010),
while the transition-based method tries to find a
highest-scoring transition sequence that leads to
a legal dependency tree (Yamada and Matsumoto,
2003; Nivre, 2003; Zhang and Nivre, 2011).
2.1 Graph-based Dependency Parser
(GParser)
In this work, we adopt the graph-based paradigm
because it allows us to naturally derive conditional
probability of a dependency tree d given a sen-
tence x, which is required to compute likelihood
of both labeled and unlabeled data. Under the
graph-based model, the score of a dependency tree
is factored into the scores of small subtrees p.
Score(x,d;w) = w ? f(x,d)
=
?
p?d
Score(x,p;w)
h m
(a) single dependency
h s
(b) adjacent sibling
m
Figure 2: Two types of scoring subtrees in our
second-order graph-based parsers.
Dependency features f
dep
(x, h,m):
w
h
, w
m
, t
h
, t
m
, t
h?1
, t
m?1
, t
b
, dir(h,m), dist(h,m)
Sibling features f
sib
(x, h,m, s):
w
h
, w
s
, w
m
, t
h
, t
m
, t
s
, t
h?1
, t
m?1
, t
s?1
dir(h,m), dist(h,m)
Table 1: Brief illustration of the syntactic features.
t
i
denotes the POS tag of w
i
. b is an index
between h and m. dir(i, j) and dist(i, j) denote
the direction and distance of the dependency (i, j).
We adopt the second-order graph-based depen-
dency parsing model of McDonald and Pereira
(2006) as our core parser, which incorporates
features from the two kinds of subtrees in Fig. 2.1
Then the score of a dependency tree is:
Score(x,d;w) =
?
{(h,m)}?d
w
dep
? f
dep
(x, h,m)
+
?
{(h,s),(h,m)}?d
w
sib
? f
sib
(x, h, s,m)
where f
dep
(x, h,m) and f
sib
(x, h, s,m) are the
feature vectors of the two subtree in Fig. 2;
w
dep/sib
are feature weight vectors; the dot prod-
uct gives scores contributed by corresponding sub-
trees.
For syntactic features, we adopt those of Bohnet
(2010) which include two categories correspond-
ing to the two types of scoring subtrees in Fig. 2.
We summarize the atomic features used in each
feature category in Table 1. These atomic features
are concatenated in different combinations to com-
pose rich feature sets. Please refer to Table 4 of
Bohnet (2010) for the complete feature list.
2.2 CRF-based GParser
Previous work on graph-based dependency pars-
ing mostly adopts linear models and perceptron
based training procedures, which lack probabilis-
tic explanations of dependency trees and do not
need to compute likelihood of labeled training
1Higher-order models of Carreras (2007) and Koo and
Collins (2010) can achieve higher accuracy, but has much
higher time cost (O(n4)). Our approach is applicable to these
higher-order models, which we leave for future work.
459
data. Instead, we build a log-linear CRF-based
dependency parser, which is similar to the CRF-
based constituent parser of Finkel et al (2008).
Assuming the feature weights w are known, the
probability of a dependency tree d given an input
sentence x is defined as:
p(d|x;w) =
exp{Score(x,d;w)}
Z(x;w)
Z(x;w) =
?
d
?
?Y(x)
exp{Score(x,d
?
;w)}
(1)
where Z(x) is the normalization factor and Y(x)
is the set of all legal dependency trees for x.
Suppose the labeled training data is
D = {(x
i
,d
i
)}
N
i=1
. Then the log likelihood
of D is:
L(D;w) =
N
?
i=1
log p(d
i
|x
i
;w)
The training objective is to maximize the log
likelihood of the training data L(D). The partial
derivative with respect to the feature weights w is:
?L(D;w)
?w
=
N
?
i=1
?
?
?
f(x
i
,d
i
) ?
?
d
?
?Y(x
i
)
p(d
?
|x
i
;w)f(x
i
,d
?
)
?
?
?
(2)
where the first term is the empirical counts and
the second term is the model expectations. Since
Y(x
i
) contains exponentially many dependency
trees, direct calculation of the second term is
prohibitive. Instead, we can use the classic inside-
outside algorithm to efficiently compute the model
expectations within O(n3) time complexity, where
n is the input sentence length.
3 Ambiguity-aware Ensemble Training
In standard entire-tree based semi-supervised
methods such as self/co/tri-training, automatically
parsed unlabeled sentences are used as additional
training data, and noisy 1-best parse trees are
considered as gold-standard. To alleviate the
noise, the tri-training method only uses unlabeled
data on which multiple parsers from different
views produce identical parse trees. However,
unlabeled data with divergent syntactic structures
should be more useful. Intuitively, if several
parsers disagree on an unlabeled sentence, it
implies that the unlabeled sentence contains
some difficult syntactic phenomena which are
not sufficiently covered in manually labeled
data. Therefore, exploiting such unlabeled data
may introduce more discriminative syntactic
knowledge, largely compensating labeled training
data.
To address above issues, we propose ambiguity-
aware ensemble training, which can be interpreted
as a generalized tri-training framework. The key
idea is the use of ambiguous labelings for the
purpose of aggregating multiple 1-best parse trees
produced by several diverse parsers. Here, ?am-
biguous labelings? mean an unlabeled sentence
may have multiple parse trees as gold-standard
reference, represented by parse forest (see Figure
1). The training procedure aims to maximize
mixed likelihood of both manually labeled and
auto-parsed unlabeled data with ambiguous label-
ings. For an unlabeled instance, the model is
updated to maximize the probability of its parse
forest, instead of a single parse tree in traditional
tri-training. In other words, the model is free to
distribute probability mass among the trees in the
parse forest to its liking, as long as the likelihood
improves (Ta?ckstro?m et al, 2013).
3.1 Likelihood of the Unlabeled Data
The auto-parsed unlabeled data with ambiguous
labelings is denoted as D? = {(u
i
,V
i
)}
M
i=1
, where
u
i
is an unlabeled sentence, and V
i
is the corre-
sponding parse forest. Then the log likelihood of
D
? is:
L(D
?
;w) =
M
?
i=1
log
?
?
?
d
?
?V
i
p(d
?
|u
i
;w)
?
?
where p(d?|u
i
;w) is the conditional probability of
d
? given u
i
, as defined in Eq. (1). For an unlabeled
sentence u
i
, the probability of its parse forest V
i
is
the summation of the probabilities of all the parse
trees contained in the forest.
Then we can derive the partial derivative of the
log likelihood with respect to w:
?L(D
?
;w)
?w
=
M
?
i=1
?
?
?
?
d
?
?V
i
p?(d
?
|u
i
,V
i
;w)f(u
i
,d
?
)
?
?
d
?
?Y(u
i
)
p(d
?
|u
i
;w)f(u
i
,d
?
)
?
?
?
(3)
where p?(d?|u
i
,V
i
;w) is the probability of d? un-
460
der the space constrained by the parse forest V
i
.
p?(d
?
|u
i
,V
i
;w) =
exp{Score(u
i
,d
?
;w)}
Z(u
i
,V
i
;w)
Z(u
i
,V
i
;w) =
?
d
?
?V
i
exp{Score(u
i
,d
?
;w)}
The second term in Eq. (3) is the same with the
second term in Eq. (2). The first term in Eq. (3)
can be efficiently computed by running the inside-
outside algorithm in the constrained search space
V
i
.
3.2 Stochastic Gradient Descent (SGD)
Training
We apply L2-norm regularized SGD training to
iteratively learn feature weights w for our CRF-
based baseline and semi-supervised parsers. We
follow the implementation in CRFsuite.2 At each
step, the algorithm approximates a gradient with
a small subset of the training examples, and then
updates the feature weights. Finkel et al (2008)
show that SGD achieves optimal test performance
with far fewer iterations than other optimization
routines such as L-BFGS. Moreover, it is very
convenient to parallel SGD since computations
among examples in the same batch is mutually
independent.
Training with the combined labeled and unla-
beled data, the objective is to maximize the mixed
likelihood:
L(D;D
?
) = L(D) + L(D
?
)
Since D? contains much more instances than D
(1.7M vs. 40K for English, and 4M vs. 16K for
Chinese), it is likely that the unlabeled data may
overwhelm the labeled data during SGD training.
Therefore, we propose a simple corpus-weighting
strategy, as shown in Algorithm 1, where Db
i,k
is the subset of training data used in kth update
and b is the batch size; ?
k
is the update step,
which is adjusted following the simulated anneal-
ing procedure (Finkel et al, 2008). The idea is
to use a fraction of training data (D
i
) at each
iteration, and do corpus weighting by randomly
sampling labeled and unlabeled instances in a
certain proportion (N
1
vs. M
1
).
Once the feature weights w are learnt, we can
2http://www.chokkan.org/software/crfsuite/
Algorithm 1 SGD training with mixed labeled and
unlabeled data.
1: Input: Labeled data D = {(x
i
,d
i
)}
N
i=1
, and unlabeled
data D? = {(u
i
,V
i
)}
M
j=1
; Parameters: I , N
1
, M
1
, b
2: Output: w
3: Initialization: w(0) = 0, k = 0;
4: for i = 1 to I do {iterations}
5: Randomly select N
1
instances from D and M
1
instances from D? to compose a new dataset D
i
, and
shuffle it.
6: Traverse D
i
: a small batch Db
i,k
? D
i
at one step.
7: w
k+1
= w
k
+ ?
k
1
b
?L(D
b
i,k
;w
k
)
8: k = k + 1
9: end for
parse the test data to find the optimal parse tree.
d
?
= arg max
d
?
?Y(x)
p(d
?
|x;w)
= arg max
d
?
?Y(x)
Score(x,d
?
;w)
This can be done with the Viterbi decoding algo-
rithm described in McDonald and Pereira (2006)
in O(n3) parsing time.
3.3 Forest Construction with Diverse Parsers
To construct parse forests for unlabeled data, we
employ three diverse parsers, i.e., our baseline
GParser, a transition-based parser (ZPar3) (Zhang
and Nivre, 2011), and a generative constituen-
t parser (Berkeley Parser4) (Petrov and Klein,
2007). These three parsers are trained on labeled
data and then used to parse each unlabeled sen-
tence. We aggregate the three parsers? outputs on
unlabeled data in different ways and evaluate the
effectiveness through experiments.
4 Experiments and Analysis
To verify the effectiveness of our proposed ap-
proach, we conduct experiments on Penn Tree-
bank (PTB) and Penn Chinese Treebank 5.1 (CT-
B5). For English, we follow the popular practice
to split data into training (sections 2-21), devel-
opment (section 22), and test (section 23). For
CTB5, we adopt the data split of (Duan et al,
2007). We convert original bracketed structures
into dependency structures using Penn2Malt with
its default head-finding rules.
For unlabeled data, we follow Chen et al (2013)
and use the BLLIP WSJ corpus (Charniak et al,
2000) for English and Xinhua portion of Chinese
3http://people.sutd.edu.sg/
?
yue_zhang/doc/
4https://code.google.com/p/berkeleyparser/
461
Train Dev Test Unlabeled
PTB 39,832 1,700 2,416 1.7M
CTB5 16,091 803 1,910 4M
Table 2: Data sets (in sentence number).
Gigaword Version 2.0 (LDC2009T14) (Huang,
2009) for Chinese. We build a CRF-based bigram
part-of-speech (POS) tagger with the features de-
scribed in (Li et al, 2012), and produce POS tags
for all train/development/test/unlabeled sets (10-
way jackknifing for training sets). The tagging ac-
curacy on test sets is 97.3% on English and 94.0%
on Chinese. Table 2 shows the data statistics.
We measure parsing performance using the s-
tandard unlabeled attachment score (UAS), ex-
cluding punctuation marks. For significance test,
we adopt Dan Bikel?s randomized parsing evalua-
tion comparator (Noreen, 1989).5
4.1 Parameter Setting
When training our CRF-based parsers with SGD,
we use the batch size b = 100 for all experiments.
We run SGD for I = 100 iterations and choose
the model that performs best on development
data. For the semi-supervised parsers trained with
Algorithm 1, we use N
1
= 20K and M
1
= 50K
for English, and N
1
= 15K and M
1
= 50K for
Chinese, based on a few preliminary experiments.
To accelerate the training, we adopt parallelized
implementation of SGD and employ 20 threads for
each run. For semi-supervised cases, one iteration
takes about 2 hours on an IBM server having 2.0
GHz Intel Xeon CPUs and 72G memory.
Default parameter settings are used for training
ZPar and Berkeley Parser. We run ZPar for 50
iterations, and choose the model that achieves
highest accuracy on the development data. For
Berkeley Parser, we use the model after 5 split-
merge iterations to avoid over-fitting the train-
ing data according to the manual. The phrase-
structure outputs of Berkeley Parser are converted
into dependency structures using the same head-
finding rules.
4.2 Methodology Study on Development Data
Using three supervised parsers, we have many
options to construct parse forest on unlabeled data.
To examine the effect of different ways for forest
construction, we conduct extensive methodology
study on development data. Table 3 presents the
5http://www.cis.upenn.edu/
?
dbikel/software.html
results. We divide the systems into three types: 1)
supervised single parsers; 2) CRF-based GParser
with conventional self/co/tri-training; 3) CRF-
based GParser with our approach. For the latter
two cases, we also present the oracle accuracy and
averaged head number per word (?Head/Word?)
of parse forest when applying different ways to
construct forests on development datasets.
The first major row presents performance of
the three supervised parsers. We can see that the
three parsers achieve comparable performance on
English, but the performance of ZPar is largely
inferior on Chinese.
The second major row shows the results when
we use single 1-best parse trees on unlabeled
data. When using the outputs of GParser itself
(?Unlabeled ? G?), the experiment reproduces
traditional self-training. The results on both En-
glish and Chinese re-confirm that self-training
may not work for dependency parsing, which
is consistent with previous studies (Spreyer and
Kuhn, 2009). The reason may be that dependency
parsers are prone to amplify previous mistakes on
unlabeled data during training.
The next two experiments in the second ma-
jor row reimplement co-training, where another
parser?s 1-best results are projected into unlabeled
data to help the core parser. Using unlabeled
data with the results of ZPar (?Unlabeled ? Z?)
significantly outperforms the baseline GParser by
0.30% (93.15-82.85) on English. However, the
improvement on Chinese is not significant. Using
unlabeled data with the results of Berkeley Parser
(?Unlabeled? B?) significantly improves parsing
accuracy by 0.55% (93.40-92.85) on English and
1.06% (83.34-82.28) on Chinese. We believe the
reason is that being a generative model designed
for constituent parsing, Berkeley Parser is more
different from discriminative dependency parsers,
and therefore can provide more divergent syntactic
structures. This kind of syntactic divergence is
helpful because it can provide complementary
knowledge from a different perspective. Surdeanu
and Manning (2010) also show that the diversity of
parsers is important for performance improvement
when integrating different parsers in the super-
vised track. Therefore, we can conclude that
co-training helps dependency parsing, especially
when using a more divergent parser.
The last experiment in the second major row
is known as tri-training, which only uses unla-
462
English Chinese
UAS Oracle Head/Word UAS Oracle Head/Word
GParser 92.85
? ?
82.28
? ?Supervised ZPar 92.50 81.04
Berkeley 92.70 82.46
Unlabeled? G (self-train) 92.88 92.85
1.000
82.14 82.28
1.000Semi-supervised GParser Unlabeled? Z (co-train) 93.15 ? 92.50 82.54 81.04
with Single 1-best Trees Unlabeled? B (co-train) 93.40 ? 92.70 83.34 ? 82.46
Unlabeled? B=Z (tri-train) 93.50 ? 97.52 83.10 ? 95.05
Unlabeled? Z+G 93.18 ? 94.97 1.053 82.78 86.66 1.136
Unlabeled? B+G 93.35 ? 96.37 1.080 83.24 ? 89.72 1.188
Semi-supervised GParser Unlabeled? B+Z 93.78 ?? 96.18 1.082 83.86 ?? 89.54 1.199
Ambiguity-aware Ensemble Unlabeled? B+(Z?G) 93.77 ?? 95.60 1.050 84.26 ?? 87.76 1.106
Unlabeled? B+Z+G 93.50 ? 96.95 1.112 83.30 ? 91.50 1.281
Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
? means the corresponding parser significantly outperforms supervised parsers, and ? means the result
significantly outperforms co/tri-training at confidence level of p < 0.01.
beled sentences on which Berkeley Parser and
ZPar produce identical outputs (?Unlabeled ?
B=Z?). We can see that with the verification of
two views, the oracle accuracy is much higher
than using single parsers (97.52% vs. 92.85% on
English, and 95.06% vs. 82.46% on Chinese).
Although using less unlabeled sentences (0.7M
for English and 1.2M for Chinese), tri-training
achieves comparable performance to co-training
(slightly better on English and slightly worse on
Chinese).
The third major row shows the results of
the semi-supervised GParser with our proposed
approach. We experiment with different com-
binations of the 1-best parse trees of the three
supervised parsers. The first three experiments
combine 1-best outputs of two parsers to compose
parse forest on unlabeled data. ?Unlabeled ?
B+(Z?G)? means that the parse forest is initialized
with the Berkeley parse and augmented with the
intersection of dependencies of the 1-best outputs
of ZPar and GParser. In the last setting, the parse
forest contains all three 1-best results.
When the parse forests of the unlabeled data
are the union of the outputs of GParser and ZPar,
denoted as ?Unlabeled ? Z+G?, each word has
1.053 candidate heads on English and 1.136 on
Chinese, and the oracle accuracy is higher than
using 1-best outputs of single parsers (94.97%
vs. 92.85% on English, 86.66% vs. 82.46%
on Chinese). However, we find that although
the parser significantly outperforms the supervised
GParser on English, it does not gain significant im-
provement over co-training with ZPar (?Unlabeled
? Z?) on both English and Chinese.
Combining the outputs of Berkeley Parser and
GParser (?Unlabeled ? B+G?), we get higher
oracle score (96.37% on English and 89.72% on
Chinese) and higher syntactic divergence (1.085
candidate heads per word on English, and 1.188
on Chinese) than ?Unlabeled ? Z+G?, which
verifies our earlier discussion that Berkeley Pars-
er produces more different structures than ZPar.
However, it leads to slightly worse accuracy than
co-training with Berkeley Parser (?Unlabeled ?
B?). This indicates that adding the outputs of
GParser itself does not help the model.
Combining the outputs of Berkeley Parser and
ZPar (?Unlabeled ? B+Z?), we get the best per-
formance on English, which is also significantly
better than both co-training (?Unlabeled ? B?)
and tri-training (?Unlabeled ? B=Z?) on both
English and Chinese. This demonstrates that our
proposed approach can better exploit unlabeled
data than traditional self/co/tri-training. More
analysis and discussions are in Section 4.4.
During experimental trials, we find that ?Unla-
beled?B+(Z?G)? can further boost performance
on Chinese. A possible explanation is that by
using the intersection of the outputs of GParser
and ZPar, the size of the parse forest is better
controlled, which is helpful considering that ZPar
performs worse on this data than both Berkeley
Parser and GParser.
Adding the output of GParser itself (?Unlabeled
? B+Z+G?) leads to accuracy drop, although the
oracle score is higher (96.95% on English and
91.50% on Chinese) than ?Unlabeled ? B+Z?.
We suspect the reason is that the model is likely to
distribute the probability mass to these parse trees
produced by itself instead of those by Berkeley
Parser or ZPar under this setting.
463
Sup Semi
McDonald and Pereira (2006) 91.5
?Koo and Collins (2010) [higher-order] 93.04
Zhang and McDonald (2012) [higher-order] 93.06
Zhang and Nivre (2011) [higher-order] 92.9
Koo et al (2008) [higher-order] 92.02 93.16
Chen et al (2009) [higher-order] 92.40 93.16
Suzuki et al (2009) [higher-order,cluster] 92.70 93.79
Zhou et al (2011) [higher-order] 91.98 92.64
Chen et al (2013) [higher-order] 92.76 93.77
This work 92.34 93.19
Table 4: UAS comparison on English test data.
In summary, we can conclude that our proposed
ambiguity-aware ensemble training is significant-
ly better than both the supervised approaches and
the semi-supervised approaches that use 1-best
parse trees. Appropriately composing the forest
parse, our approach outperforms the best results of
co-training or tri-training by 0.28% (93.78-93.50)
on English and 0.92% (84.26-83.34) on Chinese.
4.3 Comparison with Previous Work
We adopt the best settings on development data
for semi-supervised GParser with our proposed
approach, and make comparison with previous
results on test data. Table 4 shows the results.
The first major row lists several state-of-the-
art supervised methods. McDonald and Pereira
(2006) propose a second-order graph-based parser,
but use a smaller feature set than our work. Koo
and Collins (2010) propose a third-order graph-
based parser. Zhang and McDonald (2012) ex-
plore higher-order features for graph-based de-
pendency parsing, and adopt beam search for
fast decoding. Zhang and Nivre (2011) propose
a feature-rich transition-based parser. All work
in the second major row adopts semi-supervised
methods. The results show that our approach
achieves comparable accuracy with most previous
semi-supervised methods. Both Suzuki et al
(2009) and Chen et al (2013) adopt the higher-
order parsing model of Carreras (2007), and Suzu-
ki et al (2009) also incorporate word cluster
features proposed by Koo et al (2008) in their sys-
tem. We expect our approach may achieve higher
performance with such enhancements, which we
leave for future work. Moreover, our method
may be combined with other semi-supervised ap-
proaches, since they are orthogonal in method-
ology and utilize unlabeled data from different
perspectives.
Table 5 make comparisons with previous results
UAS
Supervised
Li et al (2012) [joint] 82.37
Bohnet and Nivre (2012) [joint] 81.42
Chen et al (2013) [higher-order] 81.01
This work 81.14
Semi Chen et al (2013) [higher-order] 83.08This work 82.89
Table 5: UAS comparison on Chinese test data.
Unlabeled data UAS #Sent Len Head/Word Oracle
NULL 92.34 0 ? ? ?
Consistent (tri-train) 92.94 0.7M 18.25 1.000 97.65
Low divergence 92.94 0.5M 28.19 1.062 96.53
High divergence 93.03 0.5M 27.85 1.211 94.28
ALL 93.19 1.7M 24.15 1.087 96.09
Table 6: Performance of our semi-supervised
GParser with different sets of ?Unlabeled ?
B+Z? on English test set. ?Len? means averaged
sentence length.
on Chinese test data. Li et al (2012) and Bohnet
and Nivre (2012) use joint models for POS tagging
and dependency parsing, significantly outperform-
ing their pipeline counterparts. Our approach can
be combined with their work to utilize unlabeled
data to improve both POS tagging and parsing
simultaneously. Our work achieves comparable
accuracy with Chen et al (2013), although they
adopt the higher-order model of Carreras (2007).
Again, our method may be combined with their
work to achieve higher performance.
4.4 Analysis
To better understand the effectiveness of our pro-
posed approach, we make detailed analysis using
the semi-supervised GParser with ?Unlabeled ?
B+Z? on English datasets.
Contribution of unlabeled data with regard
to syntactic divergence: We divide the unlabeled
data into three sets according to the divergence of
the 1-best outputs of Berkeley Parser and ZPar.
The first set contains those sentences that the two
parsers produce identical parse trees, denoted by
?consistent?, which corresponds to the setting for
tri-training. Other sentences are split into two sets
according to averaged number of heads per word
in parse forests, denoted by ?low divergence? and
?high divergence? respectively. Then we train
semi-supervised GParser using the three sets of
unlabeled data. Table 6 illustrates the results and
statistics. We can see that unlabeled data with
identical outputs from Berkeley Parser and ZPar
tends to be short sentences (18.25 words per sen-
464
tence on average). Results show all the three sets
of unlabeled data can help the parser. Especially,
the unlabeled data with highly divergent struc-
tures leads to slightly higher improvement. This
demonstrates that our approach can better exploit
unlabeled data on which parsers of different views
produce divergent structures.
Impact of unlabeled data size: To under-
stand how our approach performs with regards to
the unlabeled data size, we train semi-supervised
GParser with different sizes of unlabeled data. Fig.
3 shows the accuracy curve on the test set. We
can see that the parser consistently achieves higher
accuracy with more unlabeled data, demonstrating
the effectiveness of our approach. We expect
that our approach has potential to achieve higher
accuracy with more additional data.
 92.3
 92.4
 92.5
 92.6
 92.7
 92.8
 92.9
 93
 93.1
 93.2
0 50K 100K 200K 500K 1M 1.7M
UA
S
Unlabeled Data Size
B+Z Parser
Figure 3: Performance of GParser with different
sizes of ?Unlabeled? B+Z? on English test set.
5 Related Work
Our work is originally inspired by the work of
Ta?ckstro?m et al (2013). They first apply the
idea of ambiguous labelings to multilingual parser
transfer in the unsupervised parsing field, which
aims to build a dependency parser for a resource-
poor target language by making use of source-
language treebanks. Different from their work, we
explore the idea for semi-supervised dependency
parsing where a certain amount of labeled training
data is available. Moreover, we for the first
time build a state-of-the-art CRF-based depen-
dency parser and conduct in-depth comparisons
with previous methods. Similar ideas of learning
with ambiguous labelings are previously explored
for classification (Jin and Ghahramani, 2002) and
sequence labeling problems (Dredze et al, 2009).
Our work is also related with the parser ensem-
ble approaches such as stacked learning and re-
parsing in the supervised track. Stacked learning
uses one parser?s outputs as guide features for
another parser, leading to improved performance
(Nivre and McDonald, 2008; Torres Martins et
al., 2008). Re-parsing merges the outputs of
several parsers into a dependency graph, and then
apply Viterbi decoding to find a better tree (Sagae
and Lavie, 2006; Surdeanu and Manning, 2010).
One possible drawback of parser ensemble is that
several parsers are required to parse the same
sentence during the test phase. Moreover, our
approach can benefit from these methods in that
we can get parse forests of higher quality on
unlabeled data (Zhou, 2009).
6 Conclusions
This paper proposes a generalized training
framework of semi-supervised dependency
parsing based on ambiguous labelings. For
each unlabeled sentence, we combine the 1-best
parse trees of several diverse parsers to compose
ambiguous labelings, represented by a parse
forest. The training objective is to maximize the
mixed likelihood of both the labeled data and
the auto-parsed unlabeled data with ambiguous
labelings. Experiments show that our framework
can make better use of the unlabeled data,
especially those with divergent outputs from
different parsers, than traditional tri-training.
Detailed analysis demonstrates the effectiveness
of our approach. Specifically, we find that our
approach is very effective when using divergent
parsers such as the generative parser, and it is also
helpful to properly balance the size and oracle
accuracy of the parse forest of the unlabeled data.
For future work, among other possible
extensions, we would like to see how our
approach performs when employing more diverse
parsers to compose the parse forest of higher
quality for the unlabeled data, such as the easy-
first non-directional dependency parser (Goldberg
and Elhadad, 2010) and other constituent parsers
(Collins and Koo, 2005; Charniak and Johnson,
2005; Finkel et al, 2008).
Acknowledgments
The authors would like to thank the critical
and insightful comments from our anonymous
reviewers. This work was supported by National
Natural Science Foundation of China (Grant No.
61373095, 61333018).
465
References
Mohit Bansal and Dan Klein. 2011. Web-scale
features for full-scale parsing. In Proceedings of
ACL, pages 693?702.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92?100.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
Proceedings of EMNLP 2012, pages 1455?1465.
Bernd Bohnet. 2010. Top accuracy and fast
dependency parsing is not a contradiction. In
Proceedings of COLING, pages 89?97.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP
1987-89 WSJ Corpus Release 1, LDC2000T43.
Linguistic Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving
dependency parsing with subtrees from auto-parsed
data. In Proceedings of EMNLP, pages 570?579.
Wenliang Chen, Min Zhang, and Yue Zhang. 2013.
Semi-supervised feature transformation for depen-
dency parsing. In Proceedings of EMNLP, pages
1303?1313.
Michael J. Collins and Terry Koo. 2005. Dis-
criminative reranking for natural language parsing.
Computational Linguistics, pages 25?70.
Mark Dredze, Partha Pratim Talukdar, and Koby
Crammer. 2009. Sequence learning from data
with multiple labels. In ECML/PKDD Workshop on
Learning from Multi-Label Data.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD,
pages 559?566.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condition-
al random field parsing. In Proceedings of ACL,
pages 959?967.
Yoav Goldberg and Michael Elhadad. 2010. An
efficient algorithm for easy-first non-directional
dependency parsing. In Proceedings of NAACL.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMNLP 2009,
pages 832?841.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword
Version 2.0, LDC2009T14. Linguistic Data
Consortium.
Rong Jin and Zoubin Ghahramani. 2002. Learning
with multiple labels. In Proceedings of NIPS.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL, pages 1?11.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL, pages 595?603.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting
Liu. 2012. A separately passive-aggressive training
algorithm for joint POS tagging and dependency
parsing. In COLING 2012, pages 1681?1698.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, pages 152?159.
Ryan McDonald and Fernando Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of ACL, pages
91?98.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL, pages 950?958.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT, pages 149?160.
Eric W. Noreen. 1989. Computer-intensive methods
for testing hypotheses: An introduction. John Wiley
& Sons, Inc., New York.
Slav Petrov and Dan Klein. 2007. Improved
inference for unlexicalized parsing. In Proceedings
of NAACL.
Kenji Sagae and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings of
NAACL, pages 129?132.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL,
pages 1044?1050.
David A. Smith and Jason Eisner. 2007. Bootstrap-
ping feature-rich dependency parsers with entropic
priors. In Proceedings of EMNLP, pages 667?677.
466
Anders S?gaard and Christian Rish?j. 2010. Semi-
supervised dependency parsing using generalized
tri-training. In Proceedings of ACL, pages 1065?
1073.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-
driven dependency parsing of new languages using
incomplete and noisy training data. In CoNLL,
pages 12?20.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of NAACL, pages 649?
652.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of
semi-supervised structured conditional models for
dependency parsing. In Proceedings of EMNLP,
pages 551?560.
Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. In Proceedings of NAACL, pages
1061?1071.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking
dependency parsers. In Proceedings of EMNLP,
pages 157?166.
Qin Iris Wang, Dale Schuurmans, and Dekang
Lin. 2008. Semi-supervised convex training for
dependency parsing. In Proceedings of ACL, pages
532?540.
Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support vector
machines. In Proceedings of IWPT, pages 195?206.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of ACL, pages 189?196.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proceedings of EMNLP-CoNLL, pages 320?331.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
Zhi-Hua Zhou and Ming Li. 2005. Tri-training:
Exploiting unlabeled data using three classifiers.
In IEEE Transactions on Knowledge and Data
Engineering, pages 1529?1541.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings of ACL, pages 1556?1565.
Zhi-Hua Zhou. 2009. When semi-supervised learning
meets ensemble learning. In MCS.
467
