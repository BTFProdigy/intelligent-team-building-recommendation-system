Proceedings of NAACL HLT 2007, pages 364?371,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Log-linear Block Transliteration Model based on Bi-Stream HMMs
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel
{bzhao, nbach, ianlane, vogel}@cs.cmu.edu
Language Technologies Institute
School of Computer Science, Carnegie Mellon University
Abstract
We propose a novel HMM-based framework to
accurately transliterate unseen named entities.
The framework leverages features in letter-
alignment and letter n-gram pairs learned from
available bilingual dictionaries. Letter-classes,
such as vowels/non-vowels, are integrated to
further improve transliteration accuracy. The
proposed transliteration system is applied to
out-of-vocabulary named-entities in statistical
machine translation (SMT), and a significant
improvement over traditional transliteration ap-
proach is obtained. Furthermore, by incor-
porating an automatic spell-checker based on
statistics collected from web search engines,
transliteration accuracy is further improved.
The proposed system is implemented within
our SMT system and applied to a real transla-
tion scenario from Arabic to English.
1 Introduction
Cross-lingual natural language applications, such as in-
formation retrieval, question answering, and machine
translation for web-documents (e.g. Google translation),
are becoming increasingly important. However, current
state-of-the-art statistical machine translation (SMT) sys-
tems cannot yet translate named-entities which are not
seen during training. New named-entities, such as per-
son, organization, and location names are continually
emerging on the World-Wide-Web. To realize effective
cross-lingual natural language applications, handling out-
of-vocabulary named-entities is becoming more crucial.
Named entities (NEs) can be translated via transliter-
ation: mapping symbols from one writing system to an-
other. Letters of the source language are typically trans-
formed into the target language with similar pronunci-
ation. Transliteration between languages which share
similar alphabets and sound systems is usually not dif-
ficult, because the majority of letters remain the same.
However, the task is significantly more difficult when the
language pairs are considerably different, for example,
English-Arabic, English-Chinese, and English-Japanese.
In this paper, we focus on forward transliteration from
Arabic to English.
The work in (Arbabi et al, 1994), to our knowledge, is
the first work on machine transliteration of Arabic names
into English, French, and Spanish. The idea is to vow-
elize Arabic names by adding appropriate vowels and uti-
lizing a phonetic look-up table to provide the spelling in
the target language. Their framework is strictly applica-
ble within standard Arabic morphological rules. Knight
and Graehl (1997) introduced finite state transducers that
implement back-transliteration from Japanese to English,
which was then extended to Arabic-English in (Stalls and
Knight, 1998). Al-Onaizan and Knight (2002) translit-
erated named entities in Arabic text to English by com-
bining phonetic-based and spelling-based models, and re-
ranking candidates with full-name web counts, named en-
tities co-reference, and contextual web counts. Huang
(2005) proposed a specific model for Chinese-English
name transliteration with clusterings of names? origins,
and appropriate hypotheses are generated given the ori-
gins. All of these approaches, however, are not based
on a SMT-framework. Technologies developed for SMT
are borrowed in Virga and Khudanpur (2003) and Ab-
dulJaleel and Larkey (2003). Standard SMT alignment
models (Brown et al, 1993) are used to align letter-pairs
within named entity pairs for transliteration. Their ap-
proach are generative models for letter-to-letter transla-
tions, and the letter-alignment is augmented with heuris-
tics. Letter-level contextual information is shown to be
very helpful for transliteration. Oh and Choi (2002)
used conversion units for English-Korean Transliteration;
Goto et al (2003) used conversion units, mapping En-
glish letter-sequence into Japanese Katakana character
string. Li et al (2004) presented a framework allowing
direct orthographical mapping of transliteration units be-
tween English and Chinese, and an extended model is
presented in Ekbal et al (2006).
We propose a block-level transliteration framework, as
shown in Figure 1, to model letter-level context infor-
mation for transliteration at two levels. First, we pro-
pose a bi-stream HMM incorporating letter-clusters to
better model the vowel and non-vowel transliterations
with position-information, i.e., initial and final, to im-
prove the letter-level alignment accuracy. Second, based
on the letter-alignment, we propose letter n-gram (letter-
sequence) alignment models (block) to automatically
learn the mappings from source letter n-grams to target
letter n-grams. A few features specific for transliterations
are explored, and a log-linear model is used to combine
364
Figure 1: Transliteration System Structure. The upper-part is
the two-directional Bi-Stream HMM for letter-alignment; the
lower-part is a log-linear model for combining different feature
functions for block-level transliteration.
these features to learn block-level transliteration-pairs
from training data. The proposed transliteration frame-
work obtained significant improvements over a strong
baseline transliteration approach similar to AbdulJaleel
and Larkey (2003) and Virga and Khudanpur (2003).
The remainder of this paper is organized as follows.
In Section 2, we formulate the transliteration as a general
translation problem; in Section 4, we propose a log-linear
alignment model with a local search algorithm to model
the letter n-gram translation pairs; in Section 5, exper-
iments are presented. Conclusions and discussions are
given in Section 6.
2 Transliteration as Translation
Transliteration can be viewed as a special case of transla-
tion. In this approach, source and target NEs are split into
letter sequences, and each sequence is treated as a pseudo
sentence. The appealing reason of formulating transliter-
ation in this way is to utilize advanced alignment models,
which share ideas applied also within phrase-based sta-
tistical machine translation (Koehn, 2004).
To apply this approach to transliteration, however,
some unique aspects should be considered. First, letters
should be generated from left to right, without any re-
ordering. Thus, the transliteration models can only exe-
cute forward sequential jumps. Second, for unvowelized
languages such as Arabic, a single Arabic letter typically
maps to less than four English letters. Thus, the fertility
for each letter should be recognized to ensure reasonable
length relevance. Third, the position of the letter within
a NE is important. For example, in Arabic, letters such
as ?al? at the beginning of the NE can only be translated
into ?the? or ?al?. Therefore position information should
be considered within the alignment models.
Incorporating the above considerations, transliteration
can be formulated as a noisy channel model. Let fJ1 =
f1f2...fJ denote the source NE with J letters, eI1 =
e1e2...eI be an English transliteration candidate with I
letters. According to Bayesian decision rule:
e?I1=argmax
{eI1}
P (eI1|fJ1 )= argmax
{eI1}
P (fJ1 |eI1)P (eI1), (1)
where P (fJ1 |eI1) is the letter translation model and P (eI1)
is the English letter sequence model corresponding to
the monolingual language models in SMT. In this noisy-
channel scheme, P (fJ1 |eI1) is the key component for
transliteration, in which the transliteration between eI1
and fJ1 can be modeled at either letter-to-letter level, or
letter n-gram transliteration level (block-level).
Our transliteration models are illustrated in Figure 1.
We propose a Bi-Stream HMM of P (fJ1 |eI1) to infer
letter-to-letter alignments in two directions: Arabic-to-
English (F-to-E) and English-to-Arabic (E-to-F), shown
in the upper-part in Figure 1; refined alignment is then
obtained. We propose a log-linear model to extract block-
level transliterations with additional informative features,
as illustrated in the lower-part of Figure 1.
3 Bi-Stream HMMs for Transliteration
Standard IBM translation models (Brown et al, 1993)
can be used to obtain letter-to-letter translations. How-
ever, these models are not directly suitable, because
letter-alignment within NEs is strictly left-to-right. This
sequential property is well suited to HMMs (Vogel et al,
1996), in which the jumps from the current aligned posi-
tion can only be forward.
3.1 Bi-Stream HMMs
We propose a bi-stream HMM for letter-alignment within
NE pairs. For the source NE fJ1 and a target NE eI1, a bi-
stream HMM is defined as follows:
p(fJ1 |eI1)=
?
aJ1
J?
j=1
p(fj |eaj )p(cfj |ceaj )p(aj |aj?1), (2)
where aj maps fj to the English letter eaj at the position
aj in the English named entity. p(aj |aj?1) is the transi-
tion probability distribution assuming first-order Markov
dependency; p(fj |eaj ) is a letter-to-letter translation lex-
icon; cfj is the letter cluster of fj and p(cfj |ceaj ) is a
cluster level translation lexicon. As mentioned in the
above, the vowel/non-vowel linguistic features can be uti-
lized to cluster the letters. The letters from the same clus-
ter tend to share the similar letter transliteration forms.
p(cfj |ceaj ) enables to leverage such letter-correlation in
the transliteration process.
The HMM in Eqn. 2 generates two streams of observa-
tions: the letters together with the letters? classes follow-
ing the distribution of p(fj |eaj ) and p(cfj |ceaj ) at each
365
Figure 2: Block of letters for transliteration. A block is defined
by the left- and right- boundaries in the NE-pair.
state, respectively. To be in accordance with the mono-
tone nature of the NE?s alignment mentioned before, we
enforce the following constraints in Eqn. 3, so that the
transition can only jump forward or stay at the same state:
aj?aj?1?0 ?j ? [1, J ]. (3)
Since the two streams are conditionally independent
given the current state, the extended EM is straight-
forward, with only small modifications of the standard
forward-backward algorithm (Zhao et al, 2005), for pa-
rameter estimation.
3.2 Designing Letter-Classes
Pronunciation is typically highly structured. For in-
stance, in English the pronunciation structure of ?cvc?
(consonant-vowel-consonant) is common. By incorpo-
rating letter classes into the proposed two-stream HMM,
the models? expressiveness and robustness can be im-
proved. In this work, we focus on transliteration of Ara-
bic NEs into English. We define six non-overlapping
letter classes: vowel, consonant, initial, final, noclass,
and unknown. Initial and final classes represent semantic
markers at the beginning or end of NEs such as ?Al? and
?wAl? (in romanization form). Noclass signifies letters
which can be pronounced as both a vowel and a conso-
nant depending on context, for example, the English let-
ter ?y?. The unknown class is reserved for punctuations
and letters that we do not have enough linguistic clues for
mapping them to phonemes.
4 Transliteration Blocks
To further leverage the information from the letter-
context beyond the letter-classes incorporated in our bi-
stream HMM in Eqn. 2, we define letter n-grams, which
consist of n consecutive letters, as the basic transliter-
ation unit. A block is defined as a pair of such letter
n-grams which are transliterations of each other. Dur-
ing decoding of unseen NEs, transliteration is performed
block-by-block, rather than letter-by-letter. The goal of
transliteration model is to learn high-quality translitera-
tion blocks from the training data in a unsupervised fash-
ion.
Specifically, a block X can be represented by its left
and right boundaries in the source and target NEs shown
in Figure 2:
X = (f j+lj , ei+ki ), (4)
where f j+lj is the source letter-ngram with (l+1) letters
in source language, and its projection of ei+ki in the En-
glish NE with left boundary at the position of i, and right
boundary at (i+ k).
We formulate the block extraction as a local search
problem following the work in Zhao and Waibel (2005):
given a source letter n-gram f j+lj , search for the pro-
jected boundaries of candidate target letter n-gram ei+ki
according to a weighted combination of the diverse fea-
tures in a log-linear model detailed in ?4.3. The log-linear
model serves as a performance measure to guide the local
search, which, in our setup, is randomized hill-climbing,
to extract bilingual letter n-gram transliteration pairs.
4.1 Features for Block Transliteration
Three features: fertility, distortion, and lexical transla-
tion are investigated for inferring transliteration blocks
from the NE pairs. Each feature corresponds to one as-
pect of the block within the context of a given NE pair.
4.1.1 Letter n-gram Fertility
The fertility P (?|e) of a target letter e specifies the
probability of generating ? source letters for translitera-
tion. The fertilities can be easily read-off from the letter-
alignment, i.e., the output from the Bi-stream HMM.
Given letter fertility model P (?|ei), a target letter n-gram
eI1, and a source n-gram fJ1 of length J , we compute a
probability of letter n-gram length relevance: P (J |eI1)
via a dynamic programming.
The probability of generating J letters by the English
letter n-gram eI1 is defined:
P (J |eI1) = max{?I1,J=?Ii=1 ?i}
I?
i=1
P (?i|ei). (5)
The recursively updated cost ?[j, i] in dynamic program-
ming is defined as follows:
?[j, i] = max
?
???
???
?[j, i? 1] + logPNull(0|ei)
?[j ? 1, i? 1] + logP?(1|ei)
?[j ? 2, i? 1] + logP?(2|ei)
?[j ? 3, i? 1] + logP?(3|ei)
, (6)
where PNull(0|ei) is the probability of generating a Null
letter from ei; P?(k=1|ei) is the letter-fertility model of
generating one source letter from ei; ?[j, i] is the cost
366
so far for generating j letters from i consecutive English
letters (letter n-gram) ei1 : e1, ? ? ? , ei.
After computing the cost of ?[J, I], the probability
P (J |eI1) is computed for generating the length of the
source NE fJ1 from the English NE eI1 shown in Eqn. 5.
With this letter n-gram fertility model, for every block,
we can compute a fertility score to estimate how relevant
the lengths of the transliteration-pairs are.
4.1.2 Distortion of Centers
When aligning blocks of letters within transliteration
pairs, we expect most of them are close to the diagonal
due to the monotone alignment nature. Thus, a simple
position metric is proposed for each block considering
the relative positions within NE-pairs.
The center ?fj+lj of the source phrase f
j+l
j with a
length of (l + 1) is simply a normalized relative position
in the source entity defined as follows:
?fj+lj =
1
l + 1
j?=j+l?
j?=j
j?
l + 1 . (7)
For the center of English letter-phrase ei+ki , we first
define the expected corresponding relative center for ev-
ery source letter fj? using the lexicalized position score
as follows:
?ei+ki (fj?) =
1
k + 1 ?
?(i+k)
i?=i i? ? P (fj? |ei?)?(i+k)
i?=i P (fj? |ei?)
, (8)
where P (fj? |ei) is the letter translation lexicon estimated
in IBM Models 1?5. i is the position index, which
is weighted by the letter-level translation probabilities;
the term of
?i+k
i?=i P (fj? |ei?) provides a normalization so
that the expected center is within the range of the target
length. The expected center for ei+ki is simply the aver-
age of the ?ei+ki (fj?):
?ei+ki =
1
l + 1
j+l?
j?=j
?ei+ki (fj?) (9)
Given the estimated centers of ?fj+lj and ?ei+ki , we
can compute how close they are via the probability of
P (?fj+lj |?ei+ki ). In our case, because of the mono-
tone alignment nature of transliteration pairs, a simple
gaussian model is employed to enforce that the point
(?ei+ki ,?fj+lj ) is not far away from the diagonal.
4.1.3 Letter Lexical Transliteration
Similar to IBM Model-1 (Brown et al, 1993), we use
a ?bag-of-letter? generative model within a block to ap-
proximate the lexical transliteration equivalence:
P (f j+lj |ei+ki )=
j+l?
j?=j
i+k?
i?=i
P (fj? |ei?)P (ei? |ei+ki ), (10)
where P (ei? |ei+ki ) ' 1/(k+1) is approximated by a bag-
of-word unigram. Since named entities are usually rela-
tively short, this approximation works reasonably well in
practice.
4.2 Extended Feature Functions
Because of the underlying nature of the noisy-channel
model in our proposed transliteration approach in Section
2, the three base feature functions are extended to cover
the directions both from target-to-source and source-to-
target. Therefore, we have in total six feature functions
for inferring transliteration blocks from a named entity
pair.
Besides the above six feature functions, we also com-
pute the average letter-alignment links per block. We
count the number of letter-alignment links within the
block, and normalize the number by the length of the
source letter-ngram. Note that, we can refine the letter-
alignment by growing the intersections of the two di-
rection letter-alignments from Bi-stream HMM via ad-
ditional aligned letter-pairs seen in the union of the two.
In a way, this approach is similar to those of refining the
word-level alignment for SMT in (Och and Ney, 2003).
This step is shown in the upper-part in Figure 1.
Overall, our proposed feature functions cover rela-
tively different aspects for transliteration blocks: the
block level length relevance probability in Eqn. 5, lexical
translation equivalence, and positions? distortion from a
gaussian distribution in Eqn. 8, in both directions; and
the average number of letter-alignment links within the
block. Also, these feature functions are positive and
bounded within [0, 1]. Therefore, it is suitable to apply a
log-linear model (in ?4.3) to combine the weighted indi-
vidual strengths from the proposed feature functions for
better modeling the quality of the candidate translitera-
tion blocks. This log-linear model will serve as a per-
formance measure in a local-search in ?4.4 for inferring
transliteration blocks.
4.3 Log-Linear Transliteration Model
We propose a log-linear model to combine the seven fea-
ture functions in ?4.1 with proper weights as in Eqn. 11:
Pr(X|e, f)= exp(
?M
m=1 ?m?m(X, e, f))?
{X?} exp(
?M
m=1 ?m?m(X ?, e, f))
,
(11)
where ?m(X, e, f) are the real-valued bounded feature
functions corresponding to the seven models introduced
in ?4.1. The log-linear model?s parameters are the
weights {?m} associated with each feature function.
With hand-labeled data, {?m} can be learnt via gen-
eralized iterative scaling algorithm (GIS) (Darroch and
Ratcliff, 1972) or improved iterative scaling (IIS) (Berger
367
et al, 1996). However, as these algorithms are computa-
tionally expensive, we apply an alternative approach us-
ing a simplex down-hill algorithm to optimize the weights
toward better F-measure of block transliterations. Each
feature function corresponds to one dimension in the sim-
plex, and the local optimum only happens at a vertex of
the simplex. Simplex-downhill has several advantages:
it is an efficient approach for optimizing multi-variables
given some performance measure. We compute the F-
measure against a gold-standard block set extracted from
hand-labeled letter-alignment.
To build gold-standard blocks from hand-labeled
letter-alignment, we propose the block transliteration co-
herence in a two-stage fashion. First is the forward pro-
jection: for each candidate source letter-ngram f j+nj ,
search for its left-most el and right-most er projected
positions in the target NE according to the given letter-
alignment. Second is the backward projection: for the
target letter-gram erl , search for its left-most fl? and right-
most fr? projected positions in the source NE. Now if
l??j and r??j+n, i.e. frl is contained within the source
letter-ngram f j+nj , then this block X = (f j+nj , erl ) is de-
fined as coherent for the aligned pairs: (f j+nj , erl ) . We
accept coherent X as gold-standard blocks. This block
transliteration coherence is generally sound for extracting
the gold-blocks mostly because of the the monotone left-
to-right nature of the letter-alignment for transliteration.
A related coherence assumption can be found in (Fox,
2002), where their assumption on phrase-pairs for sta-
tistical machine translation is shown to be somewhat re-
strictive for SMT. This is mainly because the word align-
ment is often non-monotone, especially for langauge-
pairs from different families such as Arabic-English and
Chinese-English.
4.4 Aligning Letter-Blocks: a Local Search
Aligning the blocks within NE pairs can be formulated
as a local search given the heuristic function defined in
Eqn. 11. To be more specific: given a Arabic letter-ngram
f j+lj , our algorithm searches for the best translation can-
didate ei+ki in the target named entities. In our implemen-
tation, we use stochastic hill-climbing with Eqn. 11 as the
performance measure. Down-hill moves are accepted to
allow one or two left and right null letters to be attached
to ei+ki to expand the table of transliteration-blocks.
To make the local search more effective, we normal-
ize the letter translation lexicon p(f |e) within the parallel
entity pair as in:
P? (f |e) = P (f |e)?J
j?=1 P (fj? |e)
. (12)
In this way, the distribution of P? (f |e) is sharper and more
focused in the context of an entity pair.
Overall, given the parallel NE pairs, we can train the
letter level translation models in both directions via the
Bi-stream HMM in Eqn. 2. From the letter-alignment,
we can build the letter translation lexicons and fertility
tables. With these tables, the base feature functions are
then computed for each candidate block, and the features
are combined in the log-linear model in Eqn. 11. Given
a named-entity pair in the training data, we rank all the
transliteration blocks by the scores using the log-linear
model. This step is shown in the lower-part in Figure 1.
4.5 Decoding Unseen NEs
The decoding of NEs is an extension to the noisy-channel
scheme in Eqn. 1. In our configurations for NE translit-
eration, the extracted transliteration blocks are used. Our
letter ngram is a standard letter-ngram model trained us-
ing the SriLM toolkit (Stolcke, 2002). To transliterate the
unseen NEs, the decoder (Hewavitharana et al, 2005) is
configured for monotone decoding. It loads the transliter-
ation blocks and the letter-ngram LM, and it decodes the
unseen Arabic named entities with block-based translit-
eration from left to right.
5 Experiments
5.1 The Data
We have 74,887 bilingual geographic names from
LDC2005G01-NGA, 11,212 bilingual person names
from LDC2005G021, and about 6,000 bilingual names
extracted from the BAMA2 dictionary. In total, there are
92,099 NE pairs. We split them into three parts: 91,459
pairs as the training dataset, 100 pairs as the development
dataset, and 540 unique NE pairs as the held-out dataset.
An additional test set is collected from the TIDES 2003
Arabic-English machine translation evaluation test set.
The 663 sentences contain 286 unique words, which were
not covered by the available training data. From this set
of untranslated words, we manually labeled the entities of
persons, locations and organizations, giving a total of 97
unique un-translated NEs. The BAMA toolkit was used
to romanize the Arabic words. Some names from this test
set are shown in Figure 1.
These untranslated NEs make up only a very small
fraction of all words in the test set. Therefore, having
correct transliterations would give only small improve-
ments in terms of BLEU (Papineni et al, 2002) and NIST
scores. However, successfully translating these unknown
NEs is very crucial for cross-lingual distillation tasks or
question-answering based on the MT-output.
1The corpus is provided as FOUO (for official use only) in
the DARPA-GALE project
2LDC2004L02: Buckwalter Arabic Morphological Ana-
lyzer version 2.0
368
Table 1: Test Set Examples.
To evaluate the transliteration performance, we use
edit-distance between the hypothesis against a reference
set. This is to count the number of insertions, dele-
tions, and substitutions required to correct the hypoth-
esis to match the given reference. An edit-distance of
zero is a perfect match. However, NEs typically have
more than one correct variant. For example, the Arabic
name ?mHmd? (in romanized form) can be transliterated
as Muhammad or Mohammed; both are considered as
correct transliterations. Ideally, we want to have all vari-
ants as reference transliterations. To enable our translit-
eration evaluation to be more informative given only one
reference, edit-distance of one between hypothesis and
reference is considered to be an acceptable match.
5.2 Comparison of Transliteration Models
We compare the performance of three systems within our
proposed framework in Figure.1: the baseline Block sys-
tem, a system in which we use a log-linear combination
of alignment features as described in ?4.3, we call the the
L-Block system, and finally a system, which also uses
the bi-stream HMM alignment model as described in ?3.
This last system will be denoted LCBE system.
The baseline is based on the refined letter-alignment
from the two directions of IBM-Model-4, trained with a
scheme of 15h545 using GIZA++ (Och and Ney, 2004).
The final alignment was obtained by growing the inter-
sections between Arabic-to-English (AE) and English-
to-Arabic (EA) alignments with additional aligned letter-
pairs seen in the union. This is to compensate for the
inherent asymmetry in alignment models. Blocks (letter-
ngram pairs) were collected directly from the refined
letter-alignment, using the same algorithm as described
in ?4.3 for extracting gold-standard letter blocks. There is
no length restrictions to the letter-ngram extracted in our
system. All the blocks were then scored using relative
frequencies and lexical scores in both directions, similar
to the scoring of phrase-pairs in SMT (Koehn, 2004).
In the L-Block system additional feature functions as
defined in ?4.1 were computed on top of the letter-level
alignment obtained from the baseline system. A log-
linear model combining these features was learned with
the gold-blocks described in ?4.3. Transliteration blocks
were extracted using the local-search ?4.4. The other
Table 2: Transliteration accuracy for different translitera-
tion models.
System Accuracy
Baseline 39.18%
L-Block 41.24%
LCBE 46.39%
components remained the same as in the baseline system.
The LCBE system is an extension to both the baseline
and the L-Block system. The key difference in LCBE
is that our proposed bi-stream HMM in Eqn. 2 was ap-
plied in both directions with extended letter-classes. The
resulting combined alignment was used together with all
features of the L-Block system to guide the local-search
for extracting the blocks. The same procedure of decod-
ing was then carried out for the unseen NEs using the
extracted blocks.
To build the letter language model for the decoding
process, we first split the English entities into charac-
ters; additional position indicators ? begin? and ? end?
were added to the begin and end position of the named-
entity; ? middle? was added between the first name and
last name. A letter-trigram language model with SRI LM
toolkit (Stolcke, 2002) was then built using the target side
(English) of NE pairs tagged with the above position in-
formation.
Table 2 shows that the baseline system gives an accu-
racy of 39.18%, while the extended systems L-Block and
LCBE give 41.24% and 46.39%, respectively. These re-
sults show that the additional features besides the letter-
alignment are helpful. The L-Block system, which uses
these features, outperforms the baseline system signifi-
cantly by 2.1% absolute in accuracy. The results also
show that the bi-stream HMM alignment, which uses not
only the letters but also the letter-classes, leads to signif-
icant improvement. It outperforms the L-Block system,
which does not leverage the letter-classes and monotone
alignment, by 4.15% absolute.
5.3 Incorporation of Spell Checking
Our spelling-checker is based on the suggested word-
forms from web search engines for ambiguous candi-
dates. We collected web statistics frequency for both the
proposed transliteration candidates from our system, and
also the suggested candidates from web-search engines.
All the candidates were re-ranked by their frequencies.
Figure 3 shows the performances on the held-out set,
using system LCBE augmented with a spell-checker
(LCBE+Spell), with varying sizes of N-best hypotheses
lists. The held-out set contains 540 unique named entity
pairs. We show accuracy when exact match is requested
and when an edit distances of one is allowed.
369
Figure 3: Transliteration accuracy of LCBE and LCBE+Spell
models for 540 named entity pairs in the held-out set.
Figure 4: Transliteration accuracy of N-best hypotheses for
LCBE and LCBE+Spell models it the MT-03 test set.
Figure 4 shows the performances in the unseen test set
of LCBE and LCBE+Spell, with varying sizes of N-best
hypotheses lists. LCBE+Spell reaches 52% accuracy in
1-best hypothesis. In the 5-best and 10-best cases, the ac-
curacies of LCBE+Spell system archive the highest per-
formances with 66% and 72.16% respectively. The spell-
checker increases the 1-best accuracy by 11.12% and the
10-best accuracy by 7.69%. All these improvements are
statistically significant. These results are also comparable
to other state-of-the-art statistical Arabic name transliter-
ation systems such as (Al-Onaizan and Knight, 2002).
5.4 Comparison with the Google Web Translation
We finally compared our best system with the
state-of-the-art Arabic-English Google Web Translation
(Google). Table 3 shows transliteration examples from
our best system in comparison with Google (as in June
20, 2006)3. The Google system achieved 45.36% accu-
racy for the 1-best hypothesis, which is comparable to
the results when using the LCBE transliteration system,
while LCBE+Spell archived 52%.
3http://www.google.com/translate t
Table 3: Transliteration examples between LCBE+Spell
and Google web translation.
6 Conclusions and Discussions
In this paper we proposed a novel transliteration model.
Viewing transliteration as a translation task we adopt
alignment and decoding techniques used in a phrase-
based statistical machine translation system to work on
letter sequences instead of word sequences. To improve
the performance we extended the HMM alignment model
into a bi-stream HMM alignment by incorporating letter-
classes into the alignment process. We also showed that a
block-extraction approach, which uses a log-linear com-
bination of multiple alignment features, can give signif-
icant improvements in transliteration accuracy. Finally,
spell-checking based on work occurrence statistics ob-
tained from the web gave an additional boost in translit-
eration accuracy.
The goal for this work is to improve the quality of ma-
chine translation, esp. when used in cross-lingual infor-
mation retrieval and distillation tasks, by incorporating
the proposed framework to handle unknown words. Fig-
ure 5 gives an example of the difference named entity
transliteration can make. Shown are the original SMT
system output, the translation when the proposed translit-
eration models are used to translate the unknown named-
entities, and the reference translation. A comparison of
the two SMT outputs indicates that integrating the pro-
posed transliteration model into our machine translation
system can significantly improve translation utility.
Acknowledgment
This work was partially supported by grants from
DARPA (GALE project) and NFS (Str-Dust project).
References
Nasreen AbdulJaleel and Leah Larkey. 2003. Statistical
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of the 12th International
Conference on Information and Knowledge Management,
New Orleans, LA, USA, November.
370
Figure 5: Incorporation of the transliteration model to our
SMT System.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of ACL
Workshop on Computational Approaches to Semitic Lan-
guages, Philadelphia, PA, USA.
Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, and
Elizabeth Bart. 1994. Algorithms for Arabic name translit-
eration. In IBM Journal of Research and Development,
volume 38(2), pages 183?193.
Adam L. Berger, Vincent Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Linguistics,
volume 22 of 1, pages 39?71, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. In Annals of Mathematical
Statistics, volume 43, pages 1470?1480.
Asif Ekbal, S. Naskar, and S. Bandyopadhyay. 2006. A modi-
fied joint source channel model for machine transliteration.
In Proceedings of COLING/ACL, pages 191?198, Australia.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 304?311,
Philadelphia, PA, July 6-7.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context informa-
tion based on the maximum entropy method. In Proceedings
of MT-Summit IX, New Orleans, Louisiana, USA.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand,
Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel.
2005. The CMU statistical machine translation system
for IWSLT2005. In The 2005 International Workshop on
Spoken Language Translation.
Fei Huang. 2005. Cluster-specific name transliteration. In
Proceedings of the HLT-EMNLP 2005, Vancouver, BC,
Canada, October.
Kevin Knight and Jonathan Graehl. 1997. Machine transliter-
ation. In Proceedings of the Conference of the Association
for Computational Linguistics (ACL), Madrid, Spain.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based smt. In Proceedings of the Conference of
the Association for Machine Translation in the Americans
(AMTA), Washington DC, USA.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-
channel model for machine transliteration. In Proceedings
of 42nd ACL, pages 159?166, Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 1:29, pages 19?51.
Franz J. Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. In Computa-
tional Linguistics, volume 30, pages 417?449.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean
transliteration model using pronunciation and contextual
rules. In Proceedings of COLING-2002, pages 1?7, Taipei,
Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Bonnie Stalls and Kevin Knight. 1998. Translating names
and technical terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational Approaches to
Semitic Languages, Montreal, Quebec, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration
of proper names in cross-lingual information retrieval. In
Proceedings of the ACL Workshop on Multi-lingual Named
Entity Recognition, Edmonton, Canada.
Stephan. Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Computational
Lingustics, (COLING-1996), pages 836?841, Copenhagen,
Denmark.
Bing Zhao and Alex Waibel. 2005. Learning a log-linear
model with bilingual phrase-pair features for statistical
machine translation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, Jeju Island,
Korean, October.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual
word spectral clustering for statistical machine translation.
In Proceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 25?32, Ann Arbor, Michigan, June.
371
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520?527,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bilingual-LSA Based LM Adaptation for Spoken Language Translation
Yik-Cheung Tam and Ian Lane and Tanja Schultz
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{yct,ian.lane,tanja}@cs.cmu.edu
Abstract
We propose a novel approach to crosslingual
language model (LM) adaptation based on
bilingual Latent Semantic Analysis (bLSA).
A bLSA model is introduced which enables
latent topic distributions to be efficiently
transferred across languages by enforcing
a one-to-one topic correspondence during
training. Using the proposed bLSA frame-
work crosslingual LM adaptation can be per-
formed by, first, inferring the topic poste-
rior distribution of the source text and then
applying the inferred distribution to the tar-
get language N-gram LM via marginal adap-
tation. The proposed framework also en-
ables rapid bootstrapping of LSA models
for new languages based on a source LSA
model from another language. On Chinese
to English speech and text translation the
proposed bLSA framework successfully re-
duced word perplexity of the English LM by
over 27% for a unigram LM and up to 13.6%
for a 4-gram LM. Furthermore, the pro-
posed approach consistently improved ma-
chine translation quality on both speech and
text based adaptation.
1 Introduction
Language model adaptation is crucial to numerous
speech and translation tasks as it enables higher-
level contextual information to be effectively incor-
porated into a background LM improving recogni-
tion or translation performance. One approach is
to employ Latent Semantic Analysis (LSA) to cap-
ture in-domain word unigram distributions which
are then integrated into the background N-gram
LM. This approach has been successfully applied
in automatic speech recognition (ASR) (Tam and
Schultz, 2006) using the Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003). The LDA model can
be viewed as a Bayesian topic mixture model with
the topic mixture weights drawn from a Dirichlet
distribution. For LM adaptation, the topic mixture
weights are estimated based on in-domain adapta-
tion text (e.g. ASR hypotheses). The adapted mix-
ture weights are then used to interpolate a topic-
dependent unigram LM, which is finally integrated
into the background N-gram LM using marginal
adaptation (Kneser et al, 1997)
In this paper, we propose a framework to per-
form LM adaptation across languages, enabling the
adaptation of a LM from one language based on the
adaptation text of another language. In statistical
machine translation (SMT), one approach is to ap-
ply LM adaptation on the target language based on
an initial translation of input references (Kim and
Khudanpur, 2003; Paulik et al, 2005). This scheme
is limited by the coverage of the translation model,
and overall by the quality of translation. Since this
approach only allows to apply LM adaptation af-
ter translation, available knowledge cannot be ap-
plied to extend the coverage. We propose a bilingual
LSA model (bLSA) for crosslingual LM adaptation
that can be applied before translation. The bLSA
model consists of two LSA models: one for each
side of the language trained on parallel document
corpora. The key property of the bLSA model is that
520
the latent topic of the source and target LSA mod-
els can be assumed to be a one-to-one correspon-
dence and thus share a common latent topic space
since the training corpora consist of bilingual paral-
lel data. For instance, say topic 10 of the Chinese
LSA model is about politics. Then topic 10 of the
English LSA model is set to also correspond to pol-
itics and so forth. During LM adaptation, we first
infer the topic mixture weights from the source text
using the source LSA model. Then we transfer the
inferred mixture weights to the target LSA model
and thus obtain the target LSA marginals. The chal-
lenge is to enforce the one-to-one topic correspon-
dence. Our proposal is to share common variational
Dirichlet posteriors over the topic mixture weights
of a document pair in the LDA-style model. The
beauty of the bLSA framework is that the model
searches for a common latent topic space in an un-
supervised fashion, rather than to require manual in-
teraction. Since the topic space is language indepen-
dent, our approach supports topic transfer in multi-
ple language pairs in O(N) where N is the number of
languages.
Related work includes the Bilingual Topic Ad-
mixture Model (BiTAM) for word alignment pro-
posed by (Zhao and Xing, 2006). Basically, the
BiTAM model consists of topic-dependent transla-
tion lexicons modeling Pr(c|e, k) where c, e and
k denotes the source Chinese word, target English
word and the topic index respectively. On the
other hand, the bLSA framework models Pr(c|k)
and Pr(e|k) which is different from the BiTAM
model. By their different modeling nature, the bLSA
model usually supports more topics than the BiTAM
model. Another work by (Kim and Khudanpur,
2004) employed crosslingual LSA using singular
value decomposition which concatenates bilingual
documents into a single input supervector before
projection.
We organize the paper as follows: In Section 2,
we introduce the bLSA framework including La-
tent Dirichlet-Tree Allocation (LDTA) (Tam and
Schultz, 2007) as a correlated LSA model, bLSA
training and crosslingual LM adaptation. In Sec-
tion 3, we present the effect of LM adaptation on
word perplexity, followed by SMT experiments re-
ported in BLEU on both speech and text input in
Section 3.3. Section 4 describes conclusions and fu-
ASR hypo
Chinese LSA English LSA
Chinese N?gram LM English N?gram LM
Chinese ASR Chinese?>English SMT
Chinese?English
Adapt Adapt
MT hypoTopic distribution
Parallel document corpus
Chinese text English text
Figure 1: Topic transfer in bilingual LSA model.
ture works.
2 Bilingual Latent Semantic Analysis
The goal of a bLSA model is to enforce a one-
to-one topic correspondence between monolingual
LSA models, each of which can be modeled using
an LDA-style model. The role of the bLSA model
is to transfer the inferred latent topic distribution
from the source language to the target language as-
suming that the topic distributions on both sides are
identical. The assumption is reasonable for parallel
document pairs which are faithful translations. Fig-
ure 1 illustrates the idea of topic transfer between
monolingual LSA models followed by LM adapta-
tion. One observation is that the topic transfer can be
bi-directional meaning that the ?flow? of topic can
be from ASR to SMT or vice versa. In this paper,
we only focus on ASR-to-SMT direction. Our tar-
get is to minimize the word perplexity on the target
language through LM adaptation. Before we intro-
duce the heuristic of enforcing a one-to-one topic
correspondence, we describe the Latent Dirichlet-
Tree Allocation (LDTA) for LSA.
2.1 Latent Dirichlet-Tree Allocation
The LDTA model extends the LDA model in which
correlation among latent topics are captured using a
Dirichlet-Tree prior. Figure 2 illustrates a depth-two
Dirichlet-Tree. A tree of depth one simply falls back
to the LDA model. The LDTA model is a generative
model with the following generative process:
1. Sample a vector of branch probabilities bj ?
521
Dir(.)Dir(.) Dir(.)
Dir(.)
topic 1 topic 4Latent topics topic K
j=1
j=2 j=3
Figure 2: Dirichlet-Tree prior of depth two.
Dir(?j) for each node j = 1...J where ?j de-
notes the parameter (aka the pseudo-counts of
its outgoing branches) of the Dirichlet distribu-
tion at node j.
2. Compute the topic proportions as:
?k =
?
jc
b?jc(k)jc (1)
where ?jc(k) is an indicator function which sets
to unity when the c-th branch of the j-th node
leads to the leaf node of topic k and zero other-
wise. The k-th topic proportion ?k is computed
as the product of branch probabilities from the
root node to the leaf node of topic k.
3. Generate a document using the topic multino-
mial for each word wi:
zi ? Mult(?)
wi ? Mult(?.zi)
where ?.zi denotes the topic-dependent uni-
gram LM indexed by zi.
The joint distribution of the latent variables (topic
sequence zn1 and the Dirichlet nodes over child
branches bj) and an observed document wn1 can be
written as follows:
p(wn1 , zn1 , bJ1 ) = p(bJ1 |{?j})
n
?
i
?wizi ? ?zi
where p(bJ1 |{?j}) =
J
?
j
Dir(bj;?j)
?
?
jc
b?jc?1jc
Similar to LDA training, we apply the variational
Bayes approach by optimizing the lower bound of
the marginalized document likelihood:
L(wn1 ; ?,?)=Eq[log
p(wn1 , zn1 , bJ1 ; ?)
q(zn1 , bJ1 ; ?)
]
=Eq[log p(wn1 |zn1 )] + Eq[log
p(zn1 |bJ1 )
q(zn1 )
]
+Eq[log
p(bJ1 ; {?j})
q(bJ1 ; {?j})
]
where q(zn1 , bJ1 ; ?) =
?n
i q(zi) ?
?J
j q(bj) is a fac-
torizable variational posterior distribution over the
latent variables parameterized by ? which are deter-
mined in the E-step. ? is the model parameters for
a Dirichlet-Tree {?j} and the topic-dependent uni-
gram LM {?wk}. The LDTA model has an E-step
similar to the LDA model:
E-Step:
?jc = ?jc +
n
?
i
K
?
k
qik ? ?jc(k) (2)
qik ? ?wik ? eEq[log ?k] (3)
where
Eq[log ?k] =
?
jc
?jc(k)Eq[log bjc]
=
?
jc
?jc(k)
(
?(?jc)??(
?
c
?jc)
)
where qik denotes q(zi = k) meaning the variational
topic posterior of word wi. Eqn 2 and Eqn 3 are
executed iteratively until convergence is reached.
M-Step:
?wk ?
n
?
i
qik ? ?(wi, w) (4)
where ?(wi, w) is a Kronecker Delta function. The
alpha parameters can be estimated with iterative
methods such as Newton-Raphson or simple gradi-
ent ascent procedure.
2.2 Bilingual LSA training
For the following explanations, we assume that our
source and target languages are Chinese and En-
glish respectively. The bLSA model training is a
522
two-stage procedure. At the first stage, we train
a Chinese LSA model using the Chinese docu-
ments in parallel corpora. We applied the varia-
tional EM algorithm (Eqn 2?4) to train a Chinese
LSA model. Then we used the model to compute
the term eEq[log ?k] needed in Eqn 3 for each Chinese
document in parallel corpora. At the second stage,
we apply the same eEq [log ?k] to bootstrap an English
LSA model, which is the key to enforce a one-to-one
topic correspondence. Now the hyper-parameters of
the variational Dirichlet posteriors of each node in
the Dirichlet-Tree are shared among the Chinese and
English model. Precisely, we apply only Eqn 3 with
fixed eEq [log ?k] in the E-step and Eqn 4 in the M-step
on {?wk} to bootstrap an English LSA model. No-
tice that the E-step is non-iterative resulting in rapid
LSA training. In short, given a monolingual LSA
model, we can rapidly bootstrap LSA models of new
languages using parallel document corpora. Notice
that the English and Chinese vocabulary sizes do not
need to be similar. In our setup, the Chinese vo-
cabulary comes from the ASR system while the En-
glish vocabulary comes from the English part of the
parallel corpora. Since the topic transfer can be bi-
directional, we can perform the bLSA training in a
reverse manner, i.e. training an English LSA model
followed by bootstrapping a Chinese LSA model.
2.3 Crosslingual LM adaptation
Given a source text, we apply the E-step to estimate
variational Dirichlet posterior of each node in the
Dirichlet-Tree. We estimate the topic weights on the
source language using the following equation:
??(CH)k ?
?
jc
(
?jc
?
c? ?jc?
)?jc(k)
(5)
Then we apply the topic weights into the target LSA
model to obtain an in-domain LSA marginals:
PrEN (w) =
K
?
k=1
?(EN)wk ? ??
(CH)
k (6)
We integrate the LSA marginal into the target back-
ground LM using marginal adaptation (Kneser et al,
1997) which minimizes the Kullback-Leibler diver-
gence between the adapted LM and the background
LM:
Pra(w|h) ?
(
Prldta(w)
Prbg(w)
)?
? Prbg(w|h) (7)
Likewise, LM adaptation can take place on the
source language as well due to the bi-directional na-
ture of the bLSA framework when target-side adap-
tation text is available. In this paper, we focus on
LM adaptation on the target language for SMT.
3 Experimental Setup
We evaluated our bLSA model using the Chinese?
English parallel document corpora consisting of the
Xinhua news, Hong Kong news and Sina news. The
combined corpora contains 67k parallel documents
with 35M Chinese (CH) words and 43M English
(EN) words. Our spoken language translation sys-
tem translates from Chinese to English. The Chinese
vocabulary comes from the ASR decoder while the
English vocabulary is derived from the English por-
tion of the parallel training corpora. The vocabulary
sizes for Chinese and English are 108k and 69k re-
spectively. Our background English LM is a 4-gram
LM trained with the modified Kneser-Ney smooth-
ing scheme using the SRILM toolkit on the same
training text. We explore the bLSA training in both
directions: EN?CH and CH?EN meaning that an
English LSA model is trained first and a Chinese
LSA model is bootstrapped or vice versa. Exper-
iments explore which bootstrapping direction yield
best results measured in terms of English word per-
plexity. The number of latent topics is set to 200 and
a balanced binary Dirichlet-Tree prior is used.
With an increasing interest in the ASR-SMT cou-
pling for spoken language translation, we also eval-
uated our approach with Chinese ASR hypotheses
and compared with Chinese manual transcriptions.
We are interested to see the impact due to recog-
nition errors on the ASR hypotheses compared to
the manual transcriptions. We employed the CMU-
InterACT ASR system developed for the GALE
2006 evaluation. We trained acoustic models with
over 500 hours of quickly transcribed speech data re-
leased by the GALE program and the LM with over
800M-word Chinese corpora. The character error
rates on the CCTV, RFA and NTDTV shows in the
RT04 test set are 7.4%, 25.5% and 13.1% respec-
tively.
523
Topic index Top words
?CH-40? flying, submarine, aircraft, air, pilot, land, mission, brand-new
?EN-40? air, sea, submarine, aircraft, flight, flying, ship, test
?CH-41? satellite, han-tian, launch, space, china, technology, astronomy
?EN-41? space, satellite, china, technology, satellites, science
?CH-42? fire, airport, services, marine, accident, air
?EN-42? fire, airport, services, department, marine, air, service
Table 1: Parallel topics extracted by the bLSA
model. Top words on the Chinese side are translated
into English for illustration purpose.
-3.05e+08
-3e+08
-2.95e+08
-2.9e+08
-2.85e+08
-2.8e+08
-2.75e+08
-2.7e+08
 2  4  6  8  10  12  14  16  18  20
Tr
ain
ing
 lo
g 
lik
eli
ho
od
# of training iterations
bootstrapped EN LSA
monolingual EN LSA
Figure 3: Comparison of training log likelihood of
English LSA models bootstrapped from a Chinese
LSA and from a flat monolingual English LSA.
3.1 Analysis of the bLSA model
By examining the top-words of the extracted paral-
lel topics, we verify the validity of the heuristic de-
scribed in Section 2.2 which enforces a one-to-one
topic correspondence in the bLSA model. Table 1
shows the latent topics extracted by the CH?EN
bLSA model. We can see that the Chinese-English
topic words have strong correlations. Many of them
are actually translation pairs with similar word rank-
ings. From this viewpoint, we can interpret bLSA as
a crosslingual word trigger model. The result indi-
cates that our heuristic is effective to extract parallel
latent topics. As a sanity check, we also examine the
likelihood of the training data when an English LSA
model is bootstrapped. We can see from Figure 3
that the likelihood increases monotonically with the
number of training iterations. The figure also shows
that by sharing the variational Dirichlet posteriors
from the Chinese LSA model, we can bootstrap an
English LSA model rapidly compared to monolin-
gual English LSA training with both training proce-
dures started from the same flat model.
LM (43M) CCTV RFA NTDTV
BG EN unigram 1065 1220 1549
+CH?EN (CH ref) 755 880 1113
+EN?CH (CH ref) 762 896 1111
+CH?EN (CH hypo) 757 885 1126
+EN?CH (CH hypo) 766 896 1129
+CH?EN (EN ref) 731 838 1075
+EN?CH (EN ref) 747 848 1087
Table 2: English word perplexity (PPL) on the RT04
test set using a unigram LM.
3.2 LM adaptation results
We trained the bLSA models on both CH?EN and
EN?CH directions and compared their LM adapta-
tion performance using the Chinese ASR hypothe-
ses (hypo) and the manual transcriptions (ref) as in-
put. We adapted the English background LM using
the LSA marginals described in Section 2.3 for each
show on the test set.
We first evaluated the English word perplexity us-
ing the EN unigram LM generated by the bLSA
model. Table 2 shows that the bLSA-based LM
adaptation reduces the word perplexity by over 27%
relative compared to an unadapted EN unigram LM.
The results indicate that the bLSA model success-
fully leverages the text from the source language and
improves the word perplexity on the target language.
We observe that there is almost no performance dif-
ference when either the ASR hypotheses or the man-
ual transcriptions are used for adaptation. The result
is encouraging since the bLSA model may be in-
sensitive to moderate recognition errors through the
projection of the input adaptation text into the latent
topic space. We also apply an English translation
reference for adaptation to show an oracle perfor-
mance. The results using the Chinese hypotheses are
not too far off from the oracle performance. Another
observation is that the CH?EN bLSA model seems
to give better performance than the EN?CH bLSA
model. However, their differences are not signifi-
cant. The result may imply that the direction of the
bLSA training is not important since the latent topic
space captured by either language is similar when
parallel training corpora are used. Table 3 shows the
word perplexity when the background 4-gram En-
glish LM is adapted with the tuning parameter ? set
524
LM (43M, ? = 0.7) CCTV RFA NTDTV
BG EN 4-gram 118 212 203
+CH?EN (CH ref) 102 191 179
+EN?CH (CH ref) 102 198 179
+CH?EN (CH hypo) 102 193 180
+EN?CH (CH hypo) 103 198 180
+CH?EN (EN ref) 100 186 176
+EN?CH (EN ref) 101 190 176
Table 3: English word perplexity (PPL) on the RT04
test set using a 4-gram LM.
 100
 105
 110
 115
 120
 125
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
En
gli
sh
 W
or
d 
Pe
rp
lex
ity
Beta
CCTV (CER=7.4%)
BG 4-gram
+bLSA (CH reference)
+bLSA (CH ASR hypo)
+bLSA (EN reference)
Figure 4: Word perplexity with different ? using
manual reference or ASR hypotheses on CCTV.
to 0.7. Figure 4 shows the change of perplexity with
different ?. We see that the adaptation performance
using the ASR hypotheses or the manual transcrip-
tions are almost identical on different ? with an op-
timal value at around 0.7. The results show that the
proposed approach successfully reduces the perplex-
ity in the range of 9?13.6% relative compared to an
unadapted baseline on different shows when ASR
hypotheses are used. Moreover, we observe simi-
lar performance using ASR hypotheses or manual
Chinese transcriptions which is consistent with the
results on Table 2. On the other hand, it is interest-
ing to see that the performance gap from the oracle
adaptation is somewhat related to the degree of mis-
match between the test show and the training condi-
tion. The gap looks wider on the RFA and NTDTV
shows compared to the CCTV show.
3.3 Incorporating bLSA into Spoken Language
Translation
To investigate the effectiveness of bLSA LM adap-
tation for spoken language translation, we incorpo-
rated the proposed approach into our state-of-the-art
phrase-based SMT system. Translation performance
was evaluated on the RT04 broadcast news evalua-
tion set when applied to both the manual transcrip-
tions and 1-best ASR hypotheses. During evalua-
tion two performance metrics, BLEU (Papineni et
al., 2002) and NIST, were computed. In both cases, a
single English reference was used during scoring. In
the transcription case the original English references
were used. For the ASR case, as utterance segmen-
tation was performed automatically, the number of
sentences generated by ASR and SMT differed from
the number of English references. In this case, Lev-
enshtein alignment was used to align the translation
output to the English references before scoring.
3.4 Baseline SMT Setup
The baseline SMT system consisted of a non adap-
tive system trained using the same Chinese-English
parallel document corpora used in the previous ex-
periments (Sections 3.1 and 3.2). For phrase extrac-
tion a cleaned subset of these corpora, consisting of
1M Chinese-English sentence pairs, was used. SMT
decoding parameters were optimized using man-
ual transcriptions and translations of 272 utterances
from the RT04 development set (LDC2006E10).
SMT translation was performed in two stages us-
ing an approach similar to that in (Vogel, 2003).
First, a translation lattice was constructed by match-
ing all possible bilingual phrase-pairs, extracted
from the training corpora, to the input sentence.
Phrase extraction was performed using the ?PESA?
(Phrase Pair Extraction as Sentence Splitting) ap-
proach described in (Vogel, 2005). Next, a search
was performed to find the best path through the lat-
tice, i.e. that with maximum translation-score. Dur-
ing search reordering was allowed on the target lan-
guage side. The final translation result was that
hypothesis with maximum translation-score, which
is a log-linear combination of 10 scores consist-
ing of Target LM probability, Distortion Penalty,
Word-Count Penalty, Phrase-Count and six Phrase-
Alignment scores. Weights for each component
score were optimized to maximize BLEU-score on
the development set using MER optimization as de-
scribed in (Venugopal et al, 2005).
525
Translation Quality - BLEU (NIST)
SMT Target LM CCTV RFA NTDTV ALL
Manual Transcription
Baseline LM: 0.162 (5.212) 0.087 (3.854) 0.140 (4.859) 0.132 (5.146)
bLSA (bLSA-Adapted LM): 0.164 (5.212) 0.087 (3.897) 0.143 (4.864) 0.134 (5.162)
1-best ASR Output
CER (%) 7.4 25.5 13.1 14.9
Baseline LM: 0.129 (4.15) 0.051 (2.77) 0.086 (3.50) 0.095 (3.90)
bLSA (bLSA-Adapted LM): 0.132 (4.16) 0.050 (2.79) 0.089 (3.53) 0.096 (3.91)
Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual
transcriptions and 1-best ASR hypotheses
3.5 Performance of Baseline SMT System
First, the baseline system performance was evalu-
ated by applying the system described above to the
reference transcriptions and 1-best ASR hypotheses
generated by our Mandarin speech recognition sys-
tem. The translation accuracy in terms of BLEU and
NIST for each individual show (?CCTV?, ?RFA?,
and ?NTDTV?), and for the complete test-set, are
shown in Table 4 (Baseline LM). When applied to
the reference transcriptions an overall BLEU score
of 0.132 was obtained. BLEU-scores ranged be-
tween 0.087 and 0.162 for the ?RFA?, ?NTDTV? and
?CCTV? shows, respectively. As the ?RFA? show
contained a large segment of conversational speech,
translation quality was considerably lower for this
show due to genre mismatch with the training cor-
pora of newspaper text.
For the 1-best ASR hypotheses, an overall BLEU
score of 0.095 was achieved. For the ASR case,
the relative reduction in BLEU scores for the RFA
and NTDTV shows is large, due to the significantly
lower recognition accuracies for these shows. BLEU
score is also degraded due to poor alignment of ref-
erences during scoring.
3.6 Incorporation of bLSA Adaptation
Next, the effectiveness of bLSA based LM adapta-
tion was evaluated. For each show the target En-
glish LM was adapted using bLSA-adaptation, as
described in Section 2.3. SMT was then applied us-
ing an identical setup to that used in the baseline ex-
periments.
The translation accuracy when bLSA adaptation
was incorporated is shown in Table 4. When ap-
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
CCTV RFA NTDTV All shows
BLE
U
Baseline-LM bLSA Adapted LM
Figure 5: BLEU score for those 25% utterances
which resulted in different translations after bLSA
adaptation (manual transcriptions)
plied to the manual transcriptions, bLSA adaptation
improved the overall BLEU-score by 1.7% relative
(from 0.132 to 0.134). For all three shows bLSA
adaptation gained higher BLEU and NIST metrics.
A similar trend was also observed when the pro-
posed approach was applied to the 1-best ASR out-
put. On the evaluation set a relative improvement in
BLEU score of 1.0% was gained.
The semantic interpretation of the majority of ut-
terances in broadcast news are not affected by topic
context. In the experimental evaluation it was ob-
served that only 25% of utterances produced differ-
ent translation output when bLSA adaptation was
performed compared to the topic-independent base-
line. Although the improvement in translation qual-
ity (BLEU) was small when evaluated over the en-
tire test set, the improvement in BLEU score for
526
these 25% utterances was significant. The trans-
lation quality for the baseline and bLSA-adaptive
system when evaluated only on these utterances is
shown in Figure 5 for the manual transcription case.
On this subset of utterances an overall improvement
in BLEU of 0.007 (5.7% relative) was gained, with
a gain of 0.012 (10.6% relative) points for the ?NT-
DTV? show. A similar trend was observed when ap-
plied to the 1-best ASR output. In this case a rel-
ative improvement in BLEU of 12.6% was gained
for ?NTDTV?, and for ?All shows? 0.007 (3.7%)
was gained. Current evaluation metrics for trans-
lation, such as ?BLEU?, do not consider the rela-
tive importance of specific words or phrases during
translation and thus are unable to highlight the true
effectiveness of the proposed approach. In future
work, we intend to investigate other evaluation met-
rics which consider the relative informational con-
tent of words.
4 Conclusions
We proposed a bilingual latent semantic model
for crosslingual LM adaptation in spoken language
translation. The bLSA model consists of a set of
monolingual LSA models in which a one-to-one
topic correspondence is enforced between the LSA
models through the sharing of variational Dirich-
let posteriors. Bootstrapping a LSA model for a
new language can be performed rapidly with topic
transfer from a well-trained LSA model of another
language. We transfer the inferred topic distribu-
tion from the input source text to the target lan-
guage effectively to obtain an in-domain target LSA
marginals for LM adaptation. Results showed that
our approach significantly reduces the word per-
plexity on the target language in both cases using
ASR hypotheses and manual transcripts. Interest-
ingly, the adaptation performance is not much af-
fected when ASR hypotheses were used. We eval-
uated the adapted LM on SMT and found that the
evaluation metrics are crucial to reflect the actual
improvement in performance. Future directions in-
clude the exploration of story-dependent LM adap-
tation with automatic story segmentation instead of
show-dependent adaptation due to the possibility of
multiple stories within a show. We will investigate
the incorporation of monolingual documents for po-
tentially better bilingual LSA modeling.
Acknowledgment
This work is partly supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-2-0001. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
Allocation. In Journal of Machine Learning Research,
pages 1107?1135.
W. Kim and S. Khudanpur. 2003. LM adaptation using
cross-lingual information. In Proc. of Eurospeech.
W. Kim and S. Khudanpur. 2004. Cross-lingual latent
semantic analysis for LM. In Proc. of ICASSP.
R. Kneser, J. Peters, and D. Klakow. 1997. Language
model adaptation using dynamic marginals. In Proc.
of Eurospeech, pages 1971?1974.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proc. of ACL.
M. Paulik, C. Fu?gen, T. Schaaf, T. Schultz, S. Stu?ker, and
A. Waibel. 2005. Document driven machine transla-
tion enhanced automatic speech recognition. In Proc.
of Interspeech.
Y. C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proc. of Interspeech.
Y. C. Tam and T. Schultz. 2007. Correlated latent seman-
tic model for unsupervised language model adaptation.
In Proc. of ICASSP.
A. Venugopal, A. Zollmann, and A. Waibel. 2005. Train-
ing and evaluation error minimization rules for statis-
tical machine translation. In Proc. of ACL.
S. Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proc. of ICNLPKE.
S. Vogel. 2005. PESA: Phrase pair extraction as sentence
splitting. In Proc. of the Machine Translation Summit.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual topic
admixture models for word alignment. In Proc. of
ACL.
527
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 184?187,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tools for Collecting Speech Corpora via Mechanical-Turk 
  
Ian Lane1,2, Alex Waibel1,2 
1Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{ianlane,ahw}@cs.cmu.edu 
Matthias Eck2, Kay Rottmann2 
2Mobile Technologies LLC 
Pittsburgh, PA, USA 
matthias.eck@jibbigo.com 
kay.rottmann@jibbigo.com
 
 
Abstract 
To rapidly port speech applications to 
new languages one of the most difficult 
tasks is the initial collection of sufficient 
speech corpora. State-of-the-art automatic 
speech recognition systems are typical 
trained on hundreds of hours of speech 
data. While pre-existing corpora do exist 
for major languages, a sufficient amount 
of quality speech data is not available for 
most world languages. While previous 
works have focused on the collection of 
translations and the transcription of audio 
via Mechanical-Turk mechanisms, in this 
paper we introduce two tools which ena-
ble the collection of speech data remotely. 
We then compare the quality of audio col-
lected from paid part-time staff and unsu-
pervised volunteers, and determine that 
basic user training is critical to obtain us-
able data.  
1 Introduction 
In order to port a spoken language application to a 
new language, first an automatic speech recogni-
tion (ASR) system must be developed. For many 
languages pre-existing corpora do not exist and 
thus speech data must be collected before devel-
opment can begin. The collection of speech corpo-
ra is an expensive undertaking and obtaining this 
data rapidly, for example in response to a disaster, 
cannot be done using the typical methodology in 
which corpora are collected in controlled environ-
ments. 
 
To build an ASR system for a new language, two 
sets of data are required; first, a text corpus con-
sisting of written transcriptions of utterances users 
are likely to speak to the system, this is used to 
train the language model (LM) applied during 
ASR; and second, a corpora of recordings of 
speech, which are used to train an acoustic model 
(AM). Text corpora for a new language can be 
created by manually translating a pre-existing cor-
pus (or a sub-set of that corpus) into the new lan-
guage and crowd-sourcing methodologies can be 
used to rapidly perform this task. Rapidly creating 
corpora of speech data, however, is not trivial. 
Generally speech corpora are collected in con-
trolled environments where speakers are super-
vised by experts to ensure the equipment is setup 
correctly and recordings are performed adequately. 
However, for most languages performing this task 
on-site, where developers are located, is impractic-
al as there may not be a local community of speak-
ers of the required language. An alternative is to 
perform the data collection remotely, allowing 
speakers to record speech on their own PCs or mo-
bile devices in their home country or wherever 
they are located. While previous works have fo-
cused on the generation of translations (Razavian, 
2009) and transcribing of audio (Marge, 2010) via 
Mechanical-Turk, in this paper we focus on the 
collection of speech corpora using a Mechanical-
Turk type framework. 
 
Previous works (Voxforge), (Gruenstein, 2009), 
(Schultz, 2007) have developed solutions for col-
lecting speech data remotely via web-based inter-
faces. A web-based system for the collection of 
open-source speech corpora has been developed by 
the group at www.voxforge.org. Speech recordings 
are collected for ten major European languages and 
speakers can either record audio directly on the 
website or they can call in on a dedicated phone 
line. In (Gruenstein, 2009) spontaneous speech 
(US English) was collected via a web-based mem-
ory game. In this system speech prompts were not 
provided, but rather a voice-based memory game 
was used to gather and partially annotate 
184
             
Figure 1: Screenshots from Speech Collection iPhone App 
 
spontaneous speech. In comparison to the above 
works which focus on the collection of data for 
major languages, the SPICE project (Schultz, 
2007) provides a set of web-based tools to enable 
developers to create voice-based applications for 
less-common languages. In addition to tools for 
defining the phonetic units of a language and creat-
ing pronunciation dictionaries, this system also 
includes tools to create prompts and collect speech 
data from volunteers over the web. 
 
In this paper, we describe two tools we have de-
veloped to collect speech corpora remotely. The 
first, a Mobile smart-phone based system which 
allows speakers to record prompted speech directly 
on their phones and second, a web-based system 
which allows recordings to be collected remotely 
on PCs. We compare the quality of audio collected 
from paid part-time staff and unsupervised volun-
teers and determine that basic user training and 
automatic feedback mechanisms are required to 
obtain usable data. 
2 Collection of Speech on Mobile Devices 
Today?s smart-phones are able to record quality 
audio onboard and generally have the ability to 
connect to the internet via a fast wifi-connection. 
This makes them an ideal platform for collecting 
speech data in the field. Speech data can be col-
lected by a user at any time in any location, and the 
data can be uploaded at a later time when a wire-
less connection is available. At Mobile Technolo-
gies we have developed an iPhone application to 
perform this task. 
 
The collection procedure consists of three steps. 
First, on start-up a small amount of personal in-
formation, namely, gender and age, are requested 
from the user. They then select the language for 
which they intend to provide speech data. The mo-
bile-device ID, personal information and language 
selected is used as an identifier for individual 
speakers. Next, collection of speech data is per-
formed. Collection is performed offline, enabling 
data to be collected in the field where there may 
not be a persistent internet connection. A prompt is 
randomly selected from an onboard database of 
sentences and is presented to the user, who reads 
the sentence aloud holding down a push-to-talk 
button while speaking. During the speech collec-
tion stage, the system automatically proceeds to the 
following prompt when the current recording is 
complete. The user however has the ability to go 
back to previous recordings, listen to it and re-
speak the sentence if any issues are found. Finally, 
the speech data is uploaded using a wireless collec-
tion. Data is uploaded one utterance at a time to an 
FTP server. Uploading each utterance individually 
allows the user to halt the upload and continue it at 
a later time if required. 
  
185
 
Figure 2: Java applet for Web-based recording 
3 Collection via Web-based Recording 
One of the most popular websites for crowd-
sourcing is Amazon Mechanical Turk (AMT). 
?Requesters? post Human Intelligence Tasks 
(HITs) to this website and ?Workers? browse the 
HITs, perform tasks and get paid a predefined 
amount after submitting their work. It has been 
reported that over 100,000 workers from 100 coun-
tries are using AMT (Pontin, 2007). 
 
AMT allows two general types of HITs. A Ques-
tion Form HIT is based on a provided XML tem-
plate and only allows certain elements in the HIT. 
However, it is possible to integrate an external 
JAVA applet within a Question Form HIT which 
allows for some flexibility. Questions can also be 
hosted on an external website which increases flex-
ibility for the HIT developer while remaining 
tightly integrated in the AMT environment. 
 
For collection of audio data Amazon does not offer 
any integrated tools. We thus designed and imple-
mented a Java applet for web based speech collec-
tion. The Java applet can easily be incorporated in 
the AMT Question-Form mechanism and could 
also be used as part of an External-Question HIT.  
Currently the Java applet provides the same basic 
functionality as outlined for the iPhone application. 
The applet sequentially shows a number of 
prompts to record. The user can skip a sentence, 
playback a recording to check the quality and also 
redo the recording for the current sentence (see 
screenshot in Figure 2).  
 
After the user is finished, the recorded sentences 
are uploaded to a web-server using an HTTP Post 
request. An important difference is the necessity to 
be online during the speech recordings. 
4 Evaluation of Recorded Audio 
One issue when collecting speech data remotely is 
the quality of the resulting audio. When collection  
Table 1: Details of Evaluated Corpora 
 
Table 2: Annotations used to label poor quality 
recordings 
is performed in a controlled environment, the de-
veloper can ensure that the recording equipment is 
setup correctly, background noise is kept to a min-
imum and the speaker is adequately trained to use 
the recording equipment. However, the same is not 
guaranteed when collecting speech remotely via 
mechanical-turk frameworks.  
When recording prompted speech there are three 
types of issues that result in unsuitable data: 
? Garbage Audio: recordings that are emp-
ty, clipped, have insufficient power, or are 
incorrectly segmented. 
? Low quality recordings: low Signal-to-
Noise recordings due to poor equipment or 
large background noise 
? Speaker errors: Misspeaking of prompts, 
both accidental and malicious 
To verify the quality of audio recorded in unsuper-
vised environments we compared two sets of 
speech data. First, in an earlier data collection task 
we collected 445 prompted utterances from 10 US-
English speakers. This data collection was per-
formed in a quiet office environment with technic-
al supervision. Speakers were paid a fee for their 
time. As a comparison a similar collection of Hai-
tian Creole was performed. In this case data was 
collected on a volunteer basis and supervision was 
limited. Details of the collected data are shown in 
Table 1.  
  
Paid Employees 
Language English 
Number of Speakers 10  
Utterances Evaluated 445 
  
Volunteers 
Language Haitian Creole 
Number of Speakers 3 
Utterances Evaluated 167 
1 Recorded utterance is empty 
2 Utterance is not segmented correctly 
3 Recording is clipped 
4 Recording contains audible echo 
5 Recording contains audible noise 
186
 Figure 3: Percentage of recorded utterances de-
termined to be inadequate for acoustic model 
training. Annotations limited to five issues 
listed in Table 1. 
To determine the frequency of the quality issues 
listed above, we manually verified the two sets of 
collected speech. The recording of each utterance 
was listened to and if the audio file was determined 
to be of low quality it was annotated with one of 
the tags listed in Table 2. The percentage of utter-
ances labeled with each annotation is shown for the 
English and volunteer Haitian Creole cases in Fig-
ure 3. 
 
Around 10% of the English recordings were found 
to have issues. Clipping occurred in approximately 
5% and a distinct echo was present in the record-
ings for one speaker. For the Haitian Creole case 
the yield of useable audio was significantly lower 
than that obtained for English. For all three speak-
ers clipping was more prevalent and the level of 
background noise was higher. We discovered that 
due to lack of training, one of the volunteers had 
significant issues with the push-to-talk interface in 
our system. This led to many empty or incorrectly 
segmented recordings. In both cases, prompts were 
generally spoken accurately and technical prob-
lems caused poor quality recordings. 
 
We believe the large difference in the yield of high 
quality recordings, 90% for English compared to 
65% for Haitian Creole case, is directly due to the 
lack of training speakers received and the volun-
teer nature of the Haitian Creole task. By incorpo-
rating a basic tutorial when users first start our 
tools and an explicit feedback mechanism which 
automatically detects quality issues and prompts 
users to correct them we expect the yield of high 
quality recordings to increase significantly. In the 
near future we plan to use the tools to collect data 
from large communities of remote users.  
5 Conclusions and Future Work 
In this work, we have described two applications 
that allow speech corpora to be collected remotely, 
either directly on Mobile smart-phones or on a PC 
via a web-based interface. We also investigated the 
quality of recordings made by unsupervised volun-
teers and found that although prompts were gener-
ally read accurately, lack of training led to a 
significantly lower yield of high quality record-
ings. 
 
In the near future we plan to use the tools to collect 
data from large communities of remote users. We 
will also investigate the user of tutorials and feed-
back to improve the yield of high quality data. 
 
Acknowledgements 
 
We would like to thank the Haitian volunteers who 
gave their time to help with this data collection. 
References  
N. S. Razavian, S Vogel, "The Web as a Platform 
to Build Machine Translation Resources", 
IWIC2009 
M. Marge, S. Banerjee and A. Rudnicky, "Using 
the Amazon Mechanical Turk for Transcription 
of Spoken Language", IEEE-ICASSP, 2010 
Voxforge, www.voxforge.org 
A. Gruenstein, I. McGraw, and A. Sutherland, "A 
self-transcribing speech corpus: collecting con-
tinuous speech with an online educational 
game," Submitted to the Speech and Language 
Technology in Education (SLaTE) Workshop, 
2009. 
T. Schultz, et. al, "SPICE: Web-based Tools for 
Rapid Language Adaptation in Speech 
Processing Systems", In the Proceedings of 
INTERSPEECH, Antwerp, Belgium, 2007. 
J. Pontin, ?Artificial Intelligence, With Help From 
the Humans?, The New York Times, 25 March 
2007 
187
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 33?36,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Simulation-based Framework for Spoken Language Understanding and Action Selection in Situated Interaction  David Cohen Ian Lane Carnegie Mellon University Carnegie Mellon University Nasa Research Park Nasa Research Park Moffett Field, CA Moffett Field, CA david.cohen@sv.cmu.edu  lane@cs.cmu.edu  Abstract 
This paper introduces a simulation-based framework for performing action selection and understanding for interactive agents. By simulating the objects and actions relevant to an interaction, an agent can semantically ground natural language and interact consid-erately and on its own initiative in situated environments. The framework proposed in this paper leverages models of the environ-ment, user and system to predict possible fu-ture world states via simulation. It leverages understanding of spoken language and multi-modal input to estimate the state of the ongo-ing interaction and select actions based on the utility of future outcomes in the simulated world. In this paper we introduce this frame-work and demonstrate its effectiveness for in-car navigation. 1 Introduction Speech and multimodal interactive systems have many challenges to overcome before they can ef-fectively interact with users in the real world. The-se challenges include semantically grounding vague and ambiguous natural language utterances, understanding the user?s knowledge and capabili-ties, and acting on their own initiative to plan and take appropriate actions in complex environments. To overcome these challenges, interactive agents require more than just models of the environment, user goals, and attention, they need the ability to infer the consequences of both their and the users? actions ? a capability which simulation provides.   For each given task, an agent must plan the best way to carry it out. In many cases, a simple set of context-dependent behavior templates will not be sufficient. For example, if an in-car navigation as-sistant is trying to direct a driver to his destination, 
it should probably not give directions within the driver?s own neighborhood, with which he is al-ready familiar. However, it should inform the driv-er if there is road construction in the area of which he/she is unaware. Alternatively, if the driver is having an important conversation and the cost of the detour is outweighed by the cost of interrupting the conversation, perhaps the system should re-main quiet. Understanding all the contexts that af-fect interaction is difficult and defining a set of heuristics to choose the appropriate behavior will quickly become unmanageable. An agent in the real world will be faced with complicated situa-tions that will require planning and an understand-ing of the effects its actions will have.  To capture the full context necessary to perform understanding and planning in situated interaction, this paper argues for a unified model of the envi-ronment, the user?s knowledge, attention and goals, and the simulated consequences of different courses of action.  2 Related Work Early work in deep natural language understand-ing (Schank and Abelson, 1977; Wilensky, 1983) formed cognitive theories and developed software to reinforce the idea that understanding an agent?s words requires an understanding of that agent?s plans, goals, and planning mechanisms. Other work (Allen and Perrault, 1980) focused on identi-fying these plans and goals from the partial infor-mation available; interpreting speech acts as primitive actions in a STRIPS planner (Fikes and Nilsson, 1971), and using heuristics to determine an agent?s plan based on their speech acts. Traum (1994) adopted a similar definition of speech acts, and developed a computational theory of ground-ing whereby multiple agents come to understand each other?s plans and meaning.  
33
 Figure 1: Overview of the proposed simulation-based understanding and action selection framework.  Previous work on considerate mixed-initiative systems has placed an emphasis on modeling the user?s mental state, particularly attention and cog-nitive loading. Horvitz et al (2003) treat attention as critical to reasoning about the value of taking action and potentially disrupting users. Multiple modalities such as speech and gesture recognition, as well as mouse and keyboard behavior all con-tribute to their models of attention. Their work also stressed the importance of attention cues in effec-tive collaborative communication. Other work from the same author (Horvitz, 1999) probabilisti-cally tracked a belief in the user?s goal based on attentional cues, specifically trying to determine if a behavior from the system was desired. This work all reinforces the idea that close attention to the user?s mental state must be paid to act considerate-ly with mixed initiative, but never attempts to en-dow a system with the ability to reason about the consequences of its actions. There are several existing paradigms for spoken dialogue systems. RavenClaw (Bohus and Rud-nicky, 2009) uses a human-engineered task tree to guide the logic of an interaction, which allows for well-understood behavior, but does not permit the flexibility of planning needed for complex, dynam-ic interaction. The collaborative agent framework, COLLAGEN (Rich and Sidner, 1996), specifies the data structures for recipes and attention models based off the SharedPlan collaborative discourse framework. The framework proposed by Allen and et al (2002) is built on a collaborative discourse framework similar to SharedPlan, and is similar to our work in its situation theoretic world model and focus on user goal and plan modeling. However, to the best of our knowledge, these frameworks have never been successfully applied to a situated agent in a dynamic environment with many interacting objects and a wealth of multi-modal input as is 
available within an in-car assistant environment. It is in these situations that we believe our framework will demonstrate its applicability compared to prior approaches.  3 A Simulation-based Framework for Un-derstanding Situated Interaction1 In this paper, we propose a framework in which an interactive agent leverages a model of the ongo-ing situated interaction and simulations of possible future scenarios to perform understanding and de-cision-making (Figure 1). The model supports complex inference about natural language as well as other modalities of input, and provides a suita-ble environment for the system to evaluate possible courses of action. As an example, we evaluated the effectiveness of this framework for planning and interacting in an in-car navigation assistant.   Simulated Interaction and Environment The system models its environment in terms of an object-oriented probabilistic model that allows for multiple simultaneous actions. It is assumed that the model is an incomplete view of the world, and there are objects that the model is unaware of. In-cluded in this model is the set of primitive actions all the objects in the world can take, defined by their pre-conditions and post-conditions. Through simulation, the system can project the current world state forward in time in an attempt to predict possible futures. Within each simulated scenario, the system, user, and any number of other actors will interact. At each time step, every object selects a primitive action, which is applied to the world if its pre-conditions have been met.                                                               1 An initial version of the simulator used in the work can be download from: http://speech.sv.cmu.edu/SimInteraction  
34
Programs for Modeling High-Level Actions In order to make inferences about the long-term behavior of objects in the simulator, plans and high-level actions need a representation within the simulator. To do this, programs are defined for several realistic behaviors for each actor. These programs are a specific form of options (Sutton et al, 1999), which in the context of a Markov Deci-sion Process are closed-loop policies for choosing action over an extended period of time. In the current implementation, programs are fi-nite state machines, which are resumed at each time interval, changing state based on the actor?s internal state until a primitive action is selected.   Modeling User Knowledge and Awareness An actor carrying out a program will choose a dif-ferent sequence of actions depending on their in-ternal mental state. That is why the world model must contain this information to make accurate predictions. In particular, a user?s knowledge and attention play critical roles in their decision-making, and thus must be modeled.   Tracking and Parsing The tracker maintains the current world model in-cluding the set of objects that are relevant for simulation and estimated distributions over uncer-tain variables such as the user?s mental state and the programs being run by all relevant objects. The tracker is responsible for initiating simulations to project the situation model into the future. The tracker also manages and interfaces to a set of mini-parsers which interpret input across multiple modalities in various ways. In the proposed framework, the tracker also uses information from the parsers to add new objects to the world model, and modify the parameters of the objects already in the model. Additional parsers can be spawned based on simulation results. For example, if a simulated scenario predicts the car running out of gas, the tracker might spawn a new parser to interpret the driver?s awareness of their gas level based on gaze.  Utility Estimation and Action Selection The desirability of every simulated scenario is de-termined by a utility score, defined by the system designer to maximize the system?s usefulness. The system includes itself and its own possible pro-grams in each simulation it runs, and picks the 
Table 1: Description of three evaluation tasks. TaskID Task Description 1 Destination is a business in downtown area, mostly a straight path as a warm-up task. 2 Destination is a residence in Palo Alto, in-sufficient gas to get to destination. 3 Destination is a residence in Mountain View, retrace much of the path from Task 2.  Table 2: Average number of system turns for base-line and the proposed system. System turns include questions, notifications, and instructions. TaskID Novice Intermediate Expert 1 7.0 7.0 3.0 2 13.0 15.8 9.0 3 12.0 12.8 9.0   program that gives the best expected utility.  4 Demonstration Example We demonstrate the effectiveness of the pro-posed framework for an in-car navigation assistant. We tested this demonstration with ten test subjects each navigating through three the tasks listed in Table 1. The subjects navigated through Mountain View and Palo Alto, California in Google EarthTM while a supervisor observed their progress, entered it into the system and relayed messages between the subject and system. Some subjects had been in the area only a few times and some were current residents of Mountain View and neighboring cities. Based off the subjects? initial self-assessment, the system was given one of three different starting familiarity map estimates - novice, medium, and expert. These initial estimates reflected our intui-tive assessment of the likelihood that a driver would know major streets and neighborhoods. For users with different levels of familiarity we counted the number of system turns, which include questions, notifications, and instructions required to complete the task. These counts are shown in Table 2 show a decrease in the number of system turns across all tasks for users who were more fa-miliar with the area. This is a direct result of the system?s ability to direct these users to waypoints they were familiar with along the route, saving un-necessary directions. Example interactions ob-tained from the experiments are shown in Figure 1. 
35
 Figure 2: Sample interactions from subjects with different starting familiarity estimates.  5 Conclusions This paper introduces a simulation-based frame-work for performing action selection and under-standing in an interactive agent. The framework uses a simulator to predict possible future world states incorporating and updating models of the environment, user and system based on observed input. Understanding of spoken language and mul-timodal input is performed leveraging the past, current and future world states in the simulator. Action selection is performed based on the utility of future world states and the expected user goal. In this paper we introduce this framework and demonstrate its effectiveness for in-car navigation. 
References  James Allen, Nate Blaylock, George Ferguson 2002. A Problem Solving Model for Collaborative Agents. Proc. AAMAS. James F. Allen and C. Raymond Perrault. 1980. Analyz-ing Intention in Utterances. Artificial Intelligence. Dan Bohus and Eric Horvitz. 2011. Multiparty Turn Taking in Situated Dialog: Study, Lessons, Direc-tions Proc. SIGdial.   
 Dan Bohus and Alexander I. Rudnicky. 2009. The RavenClaw Dialog Management Framework: Archi-tecture and Systems. Computer Speech and Lan-guage. Richard E. Fikes and Nils J. Nilsson 1971. STRIPS: A New Approach to the Application of Theorem Prov-ing to Problem Solving. IJCAI. Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces Proc. SIGCHI. Eric Horvitz, Carl Kadie, Tim Paek, David Hovel. 2003. Models of Attention in Computing and Communica-tion: From Principles to Applications. Communica-tions of ACM.  Charles Rich and Candace L. Sidner 1996. COLLAGEN: When Agents Collaborate with Peo-ple. Mitsubishi Electric Research Laboratories Inc. Roger Schank and Robert Abelson. 1977. Scripts Plans Goals and Understanding: an Inquiry into Human Knowledge Structures. Lawrence Erlbaum Associ-ates, Inc., Publishers. Richard S. Sutton, Doina Precup, Satinder Singh. 1999. Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. Artificial Intelligence. David R. Traum. 1994. A Computational Theory of Grounding in Natural Language Conversation Ph.D. Thesis Robert Wilensky. 1983. Planning and Understanding: A Computational Approach to Human Reasoning. The Addison-Wesley series in artificial intelligence. 
36
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 41?44,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
HRItk: The Human-Robot Interaction ToolKit 
Rapid Development of Speech-Centric Interactive Systems in ROS 
 
 
 
 
 
Abstract 
Developing interactive robots is an extremely 
challenging task which requires a broad range 
of expertise across diverse disciplines, includ-
ing, robotic planning, spoken language under-
standing, belief tracking and action 
management. While there has been a boom in 
recent years in the development of reusable 
components for robotic systems within com-
mon architectures, such as the Robot Operat-
ing System (ROS), little emphasis has been 
placed on developing components for Human-
Robot-Interaction. In this paper we introduce 
HRItk (the Human-Robot-Interaction toolkit), 
a framework, consisting of messaging proto-
cols, core-components, and development tools 
for rapidly building speech-centric interactive 
systems within the ROS environment. The 
proposed toolkit was specifically designed for 
extensibility, ease of use, and rapid develop-
ment, allowing developers to quickly incorpo-
rate speech interaction into existing projects. 
1 Introduction 
Robots that operate along and with humans in settings 
such as a home or office are on the verge of becoming a 
natural part of our daily environment (Bohren et al, 
2011, Rosenthal and Veloso 2010, Kanda et al, 2009, 
Srinivasa et al, 2009). To work cooperatively in these 
environments, however, they need the ability to interact 
with people, both known and unknown to them. Natural 
interaction through speech and gestures is a prime can-
didate for such interaction, however, the combination of 
communicative and physical actions, as well as the un-
certainty inherent in audio and visual sensing make such 
systems extremely challenging to create. 
Developing speech and gesture-based interactive 
robots requires a broad range of expertise, including, 
robotic planning, computer vision, acoustic processing, 
speech recognition, natural language understanding, 
belief tracking, as well as dialog management and ac-
tion selection, among others. This complexity makes it 
difficult for all but very large research groups to devel-
op complete systems. While there has been a boom in 
recent years in the development and sharing of reusable 
components, such as path planning, SLAM and object 
recognition, within common architectures, such as the 
Robot Operating System (ROS) (Quigley, 2009), little 
emphasis has been placed on the development of com-
ponents for Human-Robot Interaction although despite 
the growing need for research in this area.  
Prior work in Human-Robot Interaction has gener-
ally resulted in solutions for specific robotic platforms 
(Clodic et al, 2008) or standalone frameworks (Fong et 
al., 2006) that cannot be easily combined with standard 
architectures used by robotics researchers. Earlier work 
(Kanda et al, 2009, Fong et al, 2006) has demonstrated 
the possibilities of multimodal and multiparty interac-
tion on robotic platforms, however, the tasks and inte-
ractions explored until now have been extremely 
limited, due to the complexity of infrastructure required 
to support such interactions and the expertise required to 
effectively implement and optimize individual compo-
nents. To make significant progress, we believe that a 
common, easy to use, and easily extensible infrastruc-
ture, similar to that supported by ROS, is required for 
multi-modal human-robot interaction. Such a frame-
work will allow researchers to rapidly develop initial 
speech and gesture-based interactive systems, enabling 
them to rapidly deploy systems, observe and collect 
interactions in the field and iteratively improve system 
components based on observed deficiencies. By using a 
common architecture and messaging framework, com-
ponents and component models can easily be upgraded 
and extended by a community of researchers, while not 
affecting other components. 
Towards this goal we have developed HRItk1 
(Human-Robot-Interaction toolkit), an infrastructure 
and set of components for developing speech-centric 
interactive systems within the ROS environment. The 
proposed toolkit provides the core components required 
for speech interaction, including, speech recognition, 
natural language understanding and belief tracking. Ad-
ditionally it provides basic components for gesture rec-
ognition and gaze tracking. 
                                                          
1 HRItk is available for download at: 
http://speech.sv.cmu.edu/HRItk 
Ian Lane1, Vinay Prasad1, Gaurav Sinha1, Arlette Umuhoza1,  
Shangyu Luo1, Akshay Chandrashekaran1 and Antoine Raux2 
1 Carnegie Mellon University, NASA Ames Research Park, Moffett Field, California, USA 
2 Honda Research Institute, Mountain View, California, USA 
lane@cs.cmu.edu, ?araux@honda-?ri.com ?
41
 Figure 1: Overview of core understanding and tracking components within HRItk 
 
2 Framework Overview 
An overview of the core components in the toolkit are 
highlighted in Figure 1. We introduce two classes of 
components required for speech and multimodal interac-
tion into the ROS framework, understanding nodes and 
tracking services. Understanding nodes are perceptual 
components that recognize and understand interaction 
events. Using input from sensors, intermediate 
processing nodes or other understanding components, 
these nodes generate hypotheses about current user in-
put. Tracking services monitor the long term and conti-
nuous aspects of interaction, including user dialog goals 
DQG WKH XVHU?V IRFXV RI DWWHQWLRQ. These services are 
leveraged by components including Dialog Manage-
ment and Action Selection to perform interaction. Addi-
tionally, these services provide context to understanding 
nodes enabling them to apply context-specific 
processing during the understanding phase. 
2.1 Data Processing Nodes 
The understanding components implemented in this 
work heavily leverage existing components developed 
in ROS (Quigley et al, 2009). TKHVHLQFOXGHWKH?open-
ni_kinect?  QRGH ZKLFK processes depth-images from 
the Microsoft Kinect sensor, the ?openni_tracker?
which performs skeletal tracking, and ?uvccam? QRGH
which processes color images from external USB cam-
eras. In the near future we also plan to support far-field 
speech recognition using the HARK_ROS toolkit (Na-
kadai et al, 2010). 
2.2 Understanding Nodes 
Understanding nodes recognize and understand events 
observed during interaction. As input they use either 
data obtained directly from sensors, preprocessed data 
from intermediate processing nodes or output from oth-
er understanding components. They either perform 
processing on explicit interaction events, such as speech 
or gesture input, or process continuous input such as 
joint position or gaze direction. The current understand-
ing nodes implemented within HRItk are listed in Table 
1 along with the ROS topics on which they publish.  
Understanding nodes publish two forms of messag-
HV ?state? PHVVDJHV^READY, START and STOP}, in-
dicating the state of the node and whether an interaction 
event has been detected, DQG ?hypothesis?  PHVVDges 
which enumerate the most likely observed events along 
with a likelihood measure for each. The specific struc-
WXUH RI WKH ?hypothesis?  PHVVDJH is dependent on the 
event being observed. 
2.3 State Tracking Services 
In addition to understanding specific events such as 
utterances or gestures, an interactive system needs to 
track longer term and/or continuous aspects of interac-
tion. Such aspects include user goals, which can span 
VHYHUDO XWWHUDQFHV LQ D GLDORJ DQG WKH XVHU?V IRFXV RI
attention (using, e.g., gaze and posture information). 
These can be defined as characterizing the state of the 
world (i.e. the user, the interaction, or the environment) 
at a given time, with possible reference to history.
42
Table 1: ROS nodes, Topics, Services and Messages implemented within HRItk 
ROS Node Topic / Service (* ) Description of Messages  
Speech Detection 
and Recognition 
speech/state 
speech/hypothesis 
speech/hypothesis/best 
speech/hypothesis/final 
speech/context 
State identifying interaction event, each with a unique eventID 
Partial and final hypotheses generated during speech recognition. 
Outputs include 1-best, N-best hypotheses and confusion net-
works. All output contains confidence or component model scores 
Context indicating dialog-state, domain, task of current interaction 
Natural Language 
Understanding 
dialogact/hypothesis 
dialogact/context 
Hypotheses of Concept/Value-pairs generated during NLU 
Context indicating dialog-state, domain, task of current interaction 
Gesture Recognition 
hand/hypothesis 
hand/context 
Hypothesis set of Gesture-Actions with confidence measure 
Context indicating domain or task of current interaction 
Gaze Tracking 
gaze/hypothesis 
hand/context 
Estimate of gaze direction 
Context listing visually salient objects within users field of view 
Dialog State  
Tracking 
dialogstate/state 
belief * 
dialogstate/context 
Receives an UPDATED message when the belief changes 
Belief over the concept set specified in the service request 
Context indicating system actions potentially affecting belief 
 
In addition, states can be significantly larger objects 
than individual event understanding results, which could 
unnecessarily consume significant bandwidth if con-
stantly broadcast. Therefore, state tracking modules use 
ROS services rather than topics to communicate their 
output to other modules. Any module can send a mes-
sage to the tracking service containing a specific query 
and will receive in response the matching state or belief 
over states. 
In order to allow components to react to changes in 
the state, each state-tracking module publishes an 
UPDATED message to its state topic whenever a new 
state is computed. 
2.4 Component Implementations 
Speech Detection and Recognition is performed using 
a ROS node developed around the Julius Speech Rec-
ognition Engine (Lee and Kawahara, 2009). We se-
lected this engine for its compatibility with HARK 
(Nakadai et al 2010), and its support of common model 
formats. A wrapper for Julius was implemented in C++ 
to support the ROS messaging architecture listed in Ta-
ble 1. Partial hypotheses are output during decoding, 
and final hypotheses are provided in 1-best, N-best and 
Confusion Network formats. Context is supported via 
language model switching. 
In order to develop a Speech Recognition compo-
nent for a new task at minimum two component models 
are required, a pronunciation dictionary, and a language 
model (or recognition grammar). Within HRItk we pro-
vide the tools required to generate these models from a 
set of labeled example utterances. We describe the rapid 
model building procedure in Section 4. 
 
Natural Language Understanding is implemented 
using Conditional Random Fields (Lafferty et al 2001) 
similar to the approach described in (Cohn, 2007). For 
example, given WKH LQSXW XWWHUDQFH ?Take this tray to 
the kitchen? listed in Table 3, three concept/value pairs 
are extracted: Action{Carry}, ?Object{tray}, ?
Room{kitchen}. ?Similar to the speech recognition 
component, the NLU component can be rapidly re-
trained using a set of tagged example sentences. 
 
Gesture Recognition of simple hand positions is im-
plemented using a Kinect depth sensor and previous 
work by Fujimura and Xu (2007) for palm/finger seg-
mentation. Currently, the module publishes a hypothesis 
for the number of fingers raised by the user, though 
more complex gestures can be implemented based on 
this model. 
 
Gaze Tracking is implemented using ASEF filters 
(Bolme et al, 2009) and geometric projection. Separate 
ASEF filters were training to locate the pupils of the left 
and right eye as well as their inner and outer corners. 
Filters were trained on hand-labeled images we col-
lected in-house.  
 
Dialog State Tracking is in charge of monitoring as-
pects of dialog that span multiple turns such as user 
goal. Our implementation is based on the Hound dialog 
belief tracking library developed at Honda Research 
Institute USA. Currently, our belief tracking model is 
Dynamic Probabilistic Ontology Trees (Raux and Ma 
2011), which capture the hidden user goal in the form of 
a tree-shaped Bayesian Network. Each node in the Goal 
Network represents a concept that can appear in lan-
guage and gesture understanding results. The structure 
of the network indicates (assumed) conditional indepen-
dence between concepts. With each new input, the net-
work is extended with evidence nodes according to the 
final understanding hypotheses and the system belief is 
estimated as the posterior probability of user goal nodes 
given the evidence so far. 
A request to the dialog state tracking service takes 
the form of a set of concept names, to which the service 
responds with an m-best list of concept value assign-
ments along with the joint posterior probability. 
  
43
 
 
 
3 Rapid System Build Environment 
The models required for the core interaction compo-
nents in the system can be build from a single set of 
labeled examples ?Examples.txt?DORQJZLWKDconcept 
VWUXFWXUH ILOH ?Structure.txt? used by the Dialog State 
Tracker as shown in Figure 2. Running the automatic 
build procedure on these two files will generate 3 new 
models,  
The data LQ WKH?([DPSOHVW[W? ILOH LVused to train 
the language model and pronunciation dictionary used 
by the Speech Detection and Understanding Node and 
the statistical CRF-parser applied in the Natural Lan-
guage Understanding component. Given a set of labeled 
examples, the three models listed above are trained au-
tomatically without any intervention required from the 
user. Once a system has been deployed, speech input is 
logged, and can be transcribed and labeled with seman-
tic concepts to improve the effectiveness of these com-
ponent models. 
As explained in section 3.5, our dialog state tracker 
organizes concepts in a tree structure. For a given do-
main, we specify that structure in a simple text file 
where each line contains a concept followed by the 
name of the parent concept or the keyword ROOT for 
the root of the tree. Based on this file and on the SLU 
data file, the resource building process generates the 
files required by the Hound belief tracker at runtime. 
7KLV ?RII-the-VKHOI? VWUXFWXUH assumes at each node a 
uniform conditional distribution of children values giv-
en the parent value. These distributions are stored in a 
human-readable text file and can thus be manually up-
dated to more informative values. 
Using the above tools, we have developed a sample 
using the proposed framework for robot navigation task. 
The entire system can be build from a single set of la-
beled examples as shown in Figure 3 used to train the 
language model and a component to perform actions on 
the SLU output. 
 
4 Conclusions  
In this paper we introduce HRItk (the Human-Robot-
Interaction toolkit), a framework, consisting of messag-
ing protocols, components, and development tools for 
rapidly building speech-centric interactive systems 
within the ROS environment. The proposed toolkit pro-
vides all the core components required for speech inte-
raction, including, speech recognition, natural language 
understanding and belief tracking and initial implemen-
tations for gesture recognition and gaze tracking. The 
toolkit is specifically designed for extensibility, ease of 
use, and rapid development, allowing developers to 
quickly incorporate speech interaction into existing 
ROS projects. 
References  
Bohren J., Rusu R., Jones E., Marder-Eppstein E., Pantofaru 
C., Wise M., Mosenlechner L., Meeussen W., and Holzer 
S. 2011. Towards autonomous robotic butlers: Lessons 
learned with the PR2, Proc. ICRA 2011 
Bolme, S., Draper, B., and  Beveridge, J. 2009. Average of 
Synthetic Exact Filters, Proc. CVPR 2009. 
Clodic, A., Cao, H., Alili, S., Montreuil, V., Alami, R. and-
Chatila, R. 2008. Shary: A Supervision System Adapted to-
Human-Robot Interaction. In Proc. ISER 2008. 
Cohn, T. 2007. Scaling conditional random fields for natural 
language processing. University of Melbourne. 
Fong T., Kunz C., Hiatt L. and Bugajska M. 2006. The Hu-
man-Robot Interaction Operating System. Proc. HRI 2006.  
Fujimura, K. and Xu, L. 2007. Sign recognition using con-
strained optimization. Proc. ACCV 2007. 
Kanda, T., Shiomi M., Miyashita Z., Ishiguro H., and Hagita 
N. 2009. An affective guide robot in a shopping mall. In 
Proc. HRI 2009 
Lafferty J., McCallum A., and Pereira F.. Conditional random 
fields: Probabilistic models for segmenting and labeling 
sequence data. In Intl. Conf. on Machine Learning, 2001. 
Lee, A. and Kawahara, T. 2009. Recent Development of Open-
Source Speech Recognition Engine Julius. Proc. Asia-
Pacific Signal and Information Processing Association An-
nual Summit and Conference (APSIPA ASC), 2009. 
Nakadai, K., Takahashi, T., Okuno, H.G., Nakajima, H., Ha-
segawa, Y., and Tsujino, H. 2010. Design and Implementa-
tion of Robot Audition System "HARK" . 
Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T. 
Leibs, J., Berger, E., Wheeler, R. and Ng, A. 2009. ROS: 
an open-source robot operating system. Proc. Open-source 
Software Workshop, ICRA 2009. 
Raux, A. and Ma, Y. 2011. Efficient Probabilistic Tracking of 
User Goal and Dialog History for Spoken Dialog Systems. 
Proc. Interspeech 2011. 
Rosenthal S., Veloso M. 2010. Using Symbiotic Relationships 
with Humans to Help Robots Overcome Limitations. In 
Workshop for Collaborative Human/AI Control for Interac-
tive Experiences 2010. 
Srinivasa S., Ferguson D., Helfrich C., Berenson D., Collet A., 
Diankov R., Gallagher G., Hollinger G., Kuffner J., Vande-
Weghe M. 2009. Herb: A Home Exploring Robotic Butler. 
Autonomous Robots, 2009 
Examples.txt 
<Tagged example sentence> <Action> 
 
@Room{kitchen} ? None ?
on ?the ?@Floor{fifth} ?floor ? None ?
take ?this ?@Object{package} ? ?
to ?@Room{room ?123} ? ? Carry ?
Structure.txt 
<Node> <Parent> 
Room ? ? ROOT ?
Floor ? Room ?
Object ? Room 
 
Figure 2: Training examples for robot navigation task 
44
Proceedings of the SIGDIAL 2014 Conference, pages 22?31,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Situated Language Understanding at 25 Miles per Hour
Teruhisa Misu, Antoine Raux
?
, Rakesh Gupta
Honda Research Institute USA
425 National Avenue
Mountain View, CA 94040
tmisu@hra.com
Ian Lane
Carnegie Mellon University
NASA Ames Research Park
Moffett Field, CA 93085
Abstract
In this paper, we address issues in situ-
ated language understanding in a rapidly
changing environment ? a moving car.
Specifically, we propose methods for un-
derstanding user queries about specific tar-
get buildings in their surroundings. Unlike
previous studies on physically situated in-
teractions such as interaction with mobile
robots, the task is very sensitive to tim-
ing because the spatial relation between
the car and the target is changing while
the user is speaking. We collected situated
utterances from drivers using our research
system, Townsurfer, which is embedded
in a real vehicle. Based on this data, we
analyze the timing of user queries, spa-
tial relationships between the car and tar-
gets, head pose of the user, and linguis-
tic cues. Optimized on the data, our al-
gorithms improved the target identification
rate by 24.1% absolute.
1 Introduction
Recent advances in sensing technologies have en-
abled researchers to explore applications that re-
quire a clear awareness of the systems? dynamic
context and physical surroundings. Such appli-
cations include multi-participant conversation sys-
tems (Bohus and Horvitz, 2009) and human-robot
interaction (Tellex et al., 2011; Sugiura et al.,
2011). The general problem of understanding and
interacting with human users in such environments
is referred to as situated interaction.
We address yet another environment, where sit-
uated interactions takes place ? a moving car. In
the previous work, we collected over 60 hours of
in-car human-human interactions, where drivers
interact with an expert co-pilot sitting next to them
in the vehicle (Cohen et al., 2014). One of the
?
Currently with Lenovo.
insights from the analysis on this corpus is that
drivers frequently use referring expressions about
their surroundings. (e.g. What is that big building
on the right?) Based on this insight, we have de-
veloped Townsurfer (Lane et al., 2012; Misu et
al., 2013), a situated in-car intelligent assistant.
Using geo-location information, the system can
answer user queries/questions that contain object
references about points-of-interest (POIs) in their
surroundings. We use driver (user) face orienta-
tion to understand their queries and provide the re-
quested information about the POI they are look-
ing at. We have previously demonstrated and eval-
uated the system in a simulated environment (Lane
et al., 2012). In this paper, we evaluate its utility
in real driving situations.
Compared to conventional situated dialog tasks,
query understanding in our task is expected to be
more time sensitive, due to the rapidly changing
environment while driving. Typically, a car will
move 10 meters in one second while driving at 25
mi/h. So timing can be a crucial factor. In addi-
tion, it is not well understood what kind of linguis-
tic cues are naturally provided by drivers, and their
contributions to situated language understanding
in such an environment. To the best of our knowl-
edge, this is the first study that tackles the issue of
situated language understanding in rapidly moving
vehicles.
In this paper, we first present an overview of the
Townsurfer in-car spoken dialog system (Section
2). Based on our data collection using the sys-
tem, we analyze user behavior while using the sys-
tem focusing on language understanding (Section
3). Specifically, we answer the following research
questions about the task and the system through
data collection and analysis:
1. Is timing an important factor of situated lan-
guage understanding?
2. Does head pose play an important role in lan-
guage understanding? Or is spatial distance
information enough?
22
Speech recognition
Natural language understanding
Gaze (Head-pose) estimation
3) POI Posterior calculation            by Belief tracking
1) (Candidate)     POI look-up
Microphone
Depth sensor
Sensors Sensor signal understanding POI identification (situated understanding)
Speech
Gaze
2) POI score (prior) calculation
Understanding result (POI with maximum posterior)
Geo-location estimation Semantic         geo-spatial database
GPS
IMU
Geo-location
Figure 1: System overview of Townsurfer
Table 1: Example dialog with Townsurfer
U1: What is that place. (POI in gaze)
S1: This is Specialty Cafe, a mid-scale coffee
shop that serves sandwiches.
U2: What is its (POI in dialog history) rating.
S2: The rating of Specialty Cafe is above av-
erage.
U3: How about that one on the left.
(POI located on the left)
S3: This is Roger?s Deli, a low-priced restau-
rant that serves American food.
3. What is the role of linguistic cues in this task?
What kinds of linguistic cues do drivers nat-
urally provide?
Based on the hypothesis obtained from the analy-
sis for these questions, we propose methods to im-
prove situated language understanding (Section 4),
and analyze their contributions based on the col-
lected data (Sections 5 and 6). We then clarify our
research contributions through discussion (Section
7) and comparison with related studies (Section 8).
2 Architecture and Hardware of
Townsurfer
The system uses three main input modalities,
speech, geo-location, and head pose. Speech is
the main input modality of the system. It is used to
trigger interactions with the system. User speech
is recognized, then requested concepts/values are
extracted. Geo-location and head pose informa-
tion are used to understand the target POI of the
user query. An overview of the system with a pro-
cess flow is illustrated in Figure 1 and an exam-
ple dialog with the system is shown in Table 1. A
video of an example dialog is also attached.
In this paper, we address issues in identify-
ing user intended POI, which is a form of ref-
erence resolution using multi-modal information
sources
1
. The POI identification process consists
of the following three steps (cf. Figure 1). This
is similar to but different from our previous work
on landmark-based destination setting (Ma et al.,
2012).
1) The system lists candidate POIs based on geo-
location at the timing of a driver query. Rela-
tive positions of POIs to the car are also cal-
culated based on geo-location and the head-
ing of the car.
2). Based on spatial linguistic cues in the user
utterance (e.g. to my right, on the left), a
2D scoring function is selected to identify ar-
eas where the target POI is likely to be. This
function takes into account the position of the
POI relative to the car, as well as driver head
pose. Scores for all candidate POIs are cal-
culated.
3) Posterior probabilities of each POI are cal-
culated using the score of step 2 as prior,
and non-spatial linguistic information (e.g.
POI categories, building properties) as obser-
vations. This posterior calculation is com-
puted using our Bayesian belief tracker called
DPOT (Raux and Ma, 2011).
The details are explained in Section 4.
System hardware consists of a 3D depth sen-
sor (Primesense Carmine 1.09), a USB GPS (BU-
353S4), an IMU sensor (3DM-GX3-25) and a
close talk microphone (plantronics Voyage Leg-
1
We do not deal with issues in language understanding
related to dialog history and query type. (e.g. General infor-
mation request such as U1 vs request about specific property
of POI such as U2 in Table 1)
23
end UC). These consumer grade sensors are in-
stalled in our Honda Pilot experiment car. We
use Point Cloud Library (PCL) for the face direc-
tion estimation. Geo-location is estimated based
on Extended Kalman filter-based algorithm using
GPS and gyro information as input at 1.5 Hz. The
system is implemented based on the Robot Oper-
ating System ROS (Quigley et al., 2009). Each
component is implemented as a node of ROS, and
communications between the nodes are performed
using the standard message passing mechanisms
in ROS.
3 Data Collection and Analysis
3.1 Collection Setting
We collected data using a test route. The route
passes through downtown Mountain View
2
and
residential area around Honda Research Institute.
We manually constructed our database containing
250 POIs (businesses such as restaurants, compa-
nies) in this area. Each database entry (POI) has
name, geo-location, category and property infor-
mation explained in Section 3.4. POI geo-location
is represented as a latitude-longitude pair (e.g.
37.4010,-122.0539). Size and shape of buildings
are not taken into account. It takes about 30 min-
utes to drive the route. The major difference be-
tween residential area and downtown is the POI
density. While each POI in downtown has on aver-
age 7.2 other POIs within 50 meters, in residential
area POIs have only 1.9 neighbors. Speed limits
also differ between the two (35 mi/h vs 25 mi/h).
We collected data from 14 subjects. They were
asked to drive the test route and make queries
about surrounding businesses. We showed a demo
video
3
of the system to the users before starting the
data collection. We also told them that the objec-
tive is a data collection for a situated spoken dia-
log system, rather than the evaluation of the whole
system. We asked subjects to include the full de-
scription of the target POI within a single utterance
to avoid queries whose understanding requires di-
alog history information
4
. Although the system
answered based on the baseline strategy explained
in Section 4.1, we asked subjects to ignore the sys-
tem responses.
As a result, we collected 399 queries with a
valid target POI. Queries about businesses that do
2
We assumed that a POI is in downtown when it is located
within the rectangle by geo-location coordinates (37.3902, -
122.0827) and (37.3954, -122.0760).
3
not the attached one.
4
Understanding including dialog history information is
our future work.
POI
x
y ?
face direction
target 
direction
Heading 
direction
Figure 2: Parameters used to calculate POI score
(prior)
?  :   rightX:   left+:   no cue
Distance (m)y
RightLeft Distance (m)
x
Figure 3: Target POI positions
not exist on our database (typically a vacant store)
were excluded. The data contains 171 queries in
downtown and 228 in residential area. The queries
were transcribed and the user-intended POIs were
manually annotated by confirming the intended
target POI with the subjects after the data collec-
tion based on a video taken during the drive.
3.2 Analysis of Spatial Relation of POI and
Head Pose
We first analyze the spatial relation between posi-
tion cues (right/left) and the position of the user-
intended target POIs Out of the collected 399
queries, 237 (59.4%) of them contain either right
or left position cue (e.g. What is that on the left?).
The relation between the position cues (cf. Figure
2) and POI positions at start-of-speech timing
5
is
plotted in Figure 3. The X-axis is a lateral distance
(a distance in the direction orthogonal to the head-
ing; a positive value means the right direction) and
the Y-axis is an axial distance (a distance in the
heading direction; a negative value means the POI
is in back of the car. ). The most obvious finding
from the scatter plot is that right and left are pow-
5
Specifically, the latest GPS and face direction informa-
tion at that timing is used.
24
Table 2: Comparison of average and standard deviation of distance (in meter) of POI form the car
ASR result timing Start-of-speech timing
Position cue Site Ave dist. Std dist. Ave dist. Std dist.
Right/left Downtown 17.5 31.0 31.9 28.3
Residential 22.0 36.3 45.2 36.5
No right/left Downtown 17.4 27.8 31.1 26.5
cue Residential 38.3 45.9 52.3 43.4
Distance (m)y
?angular difference (degree)
Figure 4: Relation between POI positions and
head pose
erful cues for the system to identify target POIs.
We can also see that the POI position distribution
has a large standard deviation. This is partly be-
cause the route has multiple sites from downtown
and residential area. Interestingly, while the aver-
age distance to the target POI in downtown is 37.0
meters, that of residential area is 57.4 meters.
We also analyze the relation between face di-
rection and POI positions. Figure 4 plots the re-
lation between the axial distance and the angular
difference ? (between the user face direction and
the target POI direction) (cf. Figure 2). The scat-
ter plot suggests that the angular differences for
distant target POIs is often small. For close target
POIs the angular differences are larger and have a
large variance
6
.
3.3 Analysis of Timing
Referring expressions such as ?the building on the
right? must be resolved with respect to the context
in which the user intended. However, in a moving
car, such a context (i.e. the position of the car and
the situation in the surroundings) can be very dif-
ferent between the time when the user starts speak-
ing the sentence and the time they finish speaking
it. Therefore, situated understanding must be very
time sensitive.
To confirm and investigate this issue, we ana-
lyze the difference in the POI positions between
the time the ASR result is output vs the time the
user actually started speaking. The hypothesis is
6
We will discuss the reason for this in Section 6.2.
Table 3: User-provided linguistic cues
Category of linguistic cue Percentage
used (%)
Relative position to the car (right/left) 59.4
Business category (e.g. restaurant, cafe) 31.8
Color of the POI (e.g. green, yellow) 12.8
Cuisine (e.g. Chinese, Japanese, Mexican) 8.3
Equipments (e.g. awning, outside seating) 7.2
Relative position to the road (e.g. corner) 6.5
that the latter yields a more accurate context in
which to interpret the user sentence. In contrast,
our baseline system uses the more straightforward
approach of resolving expressions using the con-
text at the time of resolution, i.e. whenever the
ASR/NLU has finished processing an utterance
(hereafter ?ASR results timing?).
Specifically, we compare the average axial dis-
tance to the target POIs and its standard deviation
between these two timings. Table 2 lists these fig-
ures broken down by position cue types and sites.
The average axial distance from the car to the tar-
get POIs is often small at the ASR result timing,
but the standard deviation is generally small at the
start-of-speech timing. This indicates that the tar-
get POI positions at the start-of-speech timing is
more consistent across users and sentence lengths
than that at the ASR result timing. This result indi-
cates the presence of a better POI likelihood func-
tion using the context (i.e. car position and orien-
tation) at the start-of-speech timing than using the
ASR result timing.
3.4 Analysis of Linguistic Cues
We then analyze the linguistic cues provided by
the users. Here, we focus on objective and sta-
ble cues. We exclude subjective cues (e.g. big,
beautiful, colorful) and cues that might change in
a short period of time (e.g. with a woman dressed
in green in front). We have categorized the linguis-
tic cues used to describe the target POIs. Table 3
lists the cue types and the percentage of user utter-
ances containing each cue type.
The cues that the users most often provided con-
cern POI position related to the car (right and left).
Nearly 60% of queries included this type of cue
and every subject provided it at least once. The
second most frequent cue is category of business,
especially in downtown. Users also provided col-
25
ors of POIs. Other cues include cuisine, equip-
ments, relative position to the road (e.g. on the
corner).
Another interesting finding from the analysis is
that the users provided more linguistic cues with
increasing candidate POIs in their field of view.
Actually, the users provided 1.51 categories in av-
erage per query in downtown, while they provided
1.03 categories in residential area. (cf. POI den-
sity in Section 3.2: 7.2 vs 1.9) This indicates that
users provide cues considering environment com-
plexity.
4 Methods for Situated Language
Understanding
4.1 Baseline Strategy
We use our previous version (Misu et al., 2013)
as the baseline system for situated language un-
derstanding. The baseline strategy consists of the
following three paragraphs, which correspond to
the process 1)-3) in Section 2 and Figure 1.
The system makes a POI look-up based on the
geo-location information at the time ASR result
is obtained. The search range of candidate POIs
is within the range (relative geo-location of POIs
against the car location) of -50 to 200 meters in
the travelling direction and 100 meters to the left
and 100 meters to the right in the lateral direction.
The ASR result timing is also used to measure the
distances to the candidate POIs.
POI priors are calculated based on the distance
from the car (= axial distance) based on ?the closer
to the car the likely? principle. We use a likelihood
function inversely proportional to the distance. We
use position cues simply to remove POIs from a
list of candidates. For example ?right? position
cue is used to remove candidate POIs that are lo-
cated on< 0 position in the lateral distance. When
no right/left cue is provided, POIs outside of 45
degrees from the face direction are removed from
the list of candidates.
No linguistic cues except right/left are used to
calculate POI posterior probabilities. So, the sys-
tem selects the POI with the highest prior (POI
score) as the language understanding result.
4.2 Strategies Toward Better Situated
Language Understanding
To achieve better situated language understanding
(POI identification) based on the findings of the
analysis in Section 3, we modify steps 1)-3) as fol-
lows:
1. Using start-of-speech timing for the POI
prior calculation
Distance (m)y
?  :   rightX:   left
RightLeft Distance (m)x
Figure 5: Example GMM fitting
2. Gaussian mixture model (GMM)-based POI
probability (prior) calculation
3. Linguistic cues for the posterior calculation.
We use the start-of-speech timing instead of the
time ASR result is output. Because the standard
deviations of the POI distances are small (cf. Sec-
tion 3.2), we expect that a better POI probability
score estimation with the POI positions at this tim-
ing in the subsequent processes than the positions
at the ASR result timing. The POI look-up range
is the same as the baseline.
We apply Gaussian mixture model (GMM) with
diagonal covariance matrices over the input pa-
rameter space. The POI probability (prior) is cal-
culated based on these Gaussians. We use two in-
put parameters of the lateral and axial distances for
queries with right/left cue, and three parameters of
the lateral and axial distances and the difference
in degree between the target and head pose direc-
tions for queries without right/left cue. (The effect
of the parameters is discussed later in Section 6.2.)
We empirically set the number of Gaussian com-
ponents to 2. An example GMM fitting to the POI
positions for queries with right and left cues is il-
lustrated in Figures 5. The center of ellipse is the
mean of the Gaussian.
We use the five linguistic cue categories of Sec-
tion 3.4 for the posterior calculation by the belief
tracker. In the following experiments, we use ei-
ther 1 or 0 as a likelihood of natural language un-
derstanding (NLU) observation. The likelihood
for the category value is 1 if a user query (NLU
result) contains the target value, otherwise 0. This
corresponds to a strategy of simply removing can-
didate POIs that do not have the category values
specified by the user. Here, we assume a clean POI
database with all their properties annotated manu-
ally.
26
Table 4: Comparison of POI identification rate
Method Success
rate (%)
right/left linguistic cues,
the-closer-the-likely likelihood, 43.1
ASR result timing) (Baseline)
1) Start-of-speech timing 42.9
2) GMM-based likelihood 47.9
3) Linguistic cues 54.6
1) + 2) 50.6
1) + 3) 54.4
2) + 3) 62.2
1) + 2) + 3) 67.2
5 Experiments
We use manual transcriptions and natural language
understanding results of the user queries to focus
our evaluations on the issues listed in Section 1.
We evaluate the situated language understanding
(POI identification) performance based on cross
validation. We use the data from 13 users to train
GMM parameters and to define a set of possible
linguistic values, and the data from the remaining
user for evaluation. We train the model parameters
of the GMM using the EM algorithm. Knowledge
about the sites (downtown or residential area) is
not used in the training
7
.
We do not set a threshold for the presentation.
We judge the system successfully understands a
user query when the posterior of the target (user-
intended) POI is the highest. The chance rate,
given by the average of the inverse number of can-
didate POIs in the POI look-up is 10.0%.
6 Analysis of the Results
We first analyze the effect of our three methods
described in Section 4.2. The results are listed in
Table 4.
Simply using the POI positions at the start-of-
speech timing instead of those of the ASR result
timing did not lead to an improvement. This re-
sult is reasonable because the distances to target
POIs are often smaller at the ASR result timing
as we showed in Table 2. However, we achieved
a better improvement (7.5% over the baseline) by
combining it with the GMM-based likelihood cal-
culation. The results supports our Section 3.3 hy-
pothesis that the POI position is less dependent
on users/scenes at the start-of-speech timing. The
linguistic cues were the most powerful informa-
7
The performance was better when the knowledge was not
used.
Confusion
Linguistic cue
Localization error
User error
Figure 6: Breakdown of error causes
tion for this task. The improvement over the base-
line was 11.5%. By using these three methods to-
gether, we obtained more than additive improve-
ment of 24.1% in the POI identification rate over
the baseline
8
. The success rates per site were
60.8% in downtown and 71.9% in residential area.
6.1 Error Analysis
To analyze the causes of the remaining errors, we
have categorized the errors into the following four
categories:
1. Ambiguous references: There were multi-
ple POIs that matched the user query. (e.g.
another yellow building sat next to the target)
2. Linguistic cue: The driver used undefined
linguistic cues such subjective expressions or
dynamic references objects (e.g. optometrist,
across the street, colorful)
3. Localization error: Errors in estimating
geo-location or heading of the car.
4. User error: There were errors in the user
descriptions (e.g. user misunderstood the
neighbor POI?s outside seating as the tar-
get?s)
The distribution of error causes is illustrated in
Figure 6. More than half of the errors are due
to reference ambiguity. These errors are expected
to be resolved through clarification dialogs. (e.g.
asking user ?Did you mean the one in front or
back??) Linguistic errors might be partly resolved
by using a better database with detailed category
information. For dynamic references and subjec-
tive cues, use of image processing techniques will
help. Localization errors can be solved by using
high-quality GPS and IMU sensors. User errors
were rare and only made in downtown.
6.2 Breakdown of Effect of the Spatial
Distance and Head Pose
We then evaluate the features used for the POI
prior calculation to investigate the effect of the in-
put parameters of the lateral and axial distances
8
For reference, the performances of ?1) + 2) + 3)? were
62.9%, 67.2%, 66.1%, 67.2%, and 66.2% when the number
of Gaussian components were 1, 2, 3, 4, and 5.
27
Table 5: Relation between the parameters used for
the POI identification and success rates (%)
query type
parameters used right/left no cue
lateral (x) distance 58.6 51.2
axial (y) distance 59.5 53.7
face direction 43.3 44.4
lateral + axial (x+ y) 73.8 54.3
lateral (x) + face direction 57.8 48.1
axial (y) + face direction 59.1 54.9
lateral + axial + face 68.4 57.4
and the difference in degree between the target
and user face direction angles. Table 5 lists the
relationship between the parameters used for the
GMM-based likelihood calculation and the POI
identification performances
9
.
The results indicate that the axial distance is
the most important parameter. We got a slight
improvement by using the face direction informa-
tion for the queries without right/left cue, but the
improvement was not significant. On the other
hand, use of face direction information for the
right/left queries clearly degraded the POI iden-
tification performance. We think this is because
the users finished looking at the POI and returned
the face to the front when they started speaking,
thus they explicitly provided right/left information
to the system. However, we believe that using a
long-term trajectory of the user face direction will
contribute to an improve in the POI identification
performance.
6.3 Breakdown of the Effect of Linguistic
Cues
We then evaluate the effect of the linguistic cues
per category. Table 6 lists the relationship between
the categories used for the posterior calculation
and the success rates. There is a strong correlation
between the frequency of the cues used (cf. Table
3) and their contributions to the improvement in
success rate. For example, business category in-
formation contributed the most, boosting the per-
formance by 8.5%.
Another point we note is that the contribution of
business category and cuisine categories is large.
Because other categories (e.g. color) are not read-
ily available in a public POI database (e.g. Google
Places API, Yelp API), we can obtain reasonable
performance without using a special database or
9
Note that, we first determine the function to calculate
POI scores (priors) based on the position cues, then calculate
scores with the selected function.
Table 6: Effect of linguistic cues
linguistic cue Success
category used rate (%)
No linguistic cues (*) 50.6
(*) + Business category (e.g. cafe) 59.1
(*) + Color of the POI (e.g. green) 57.6
(*) + Cuisine (e.g. Chinese) 54.1
(*) + Equipments (e.g. awning) 53.9
(*) + Relative position (e.g. corner) 51.4
image processing.
We also found that linguistic cues were espe-
cially effective in downtown. Actually, while the
improvement
10
was 20.0% in downtown that for
residential area was 14.4%. This mainly would be
because the users provided more linguistic cues in
downtown considering the difficulty of the task.
6.4 Using Speech Recognition Results
We evaluate the degradation by using automatic
speech recognition (ASR) results. We use Google
ASR
11
and Julius (Kawahara et al., 2004) speech
recognition system with a language model trained
from 38K example sentences generated from a
grammar. An acoustic model trained from the
WSJ speech corpus is used. Note that they are
not necessarily the best system for this domain.
Google ASR uses a general language model for
dictation and Julius uses a mismatched acoustic
model in terms of the noise condition.
The query success rate was 56.3% for Julius and
60.3% for Google ASR. We got ASR accuracies
of 77.9% and 80.4% respectively. We believe the
performance will improve when N-best hypothe-
ses with confidence scores are used in the posterior
calculating using the belief tracker.
7 Discussion
The main limitation of this work comes from the
small amount of data that we were able to collect.
It is not clear how the results obtained here would
generalize to other sites, POI density, velocities
and sensor performances. Also, results might de-
pend on experimental conditions, such as weather,
hour, season. Hyper-parameters such as the opti-
mal number of Gaussian components might have
to be adapted to different situations. We there-
fore acknowledge that the scenes we experimented
are only a limited cases of daily driving activities.
10
1) + 2) vs 1) + 2) + 3).
11
Although it is not realistic to use cloud-based speech
recognition system considering the current latency, we use
this as a reference system.
28
However, the methods we propose are general and
our findings should be verifiable without loss of
generality by collecting more data and using more
input parameters (e.g. velocity) for the POI prior
calculation.
In addition, much future work remains to realize
a natural interaction with the system, such as tak-
ing into account dialog history and selecting opti-
mal system responses. On the other hand, we be-
lieve this is one of the best platform to investigate
situated interactions. The major topics that we are
going to tackle are:
1. Dialog strategy: Dialog strategy and system
prompt generation for situated environments
are important research topics, especially to
clarify the target when there is ambiguity as
mentioned in Section 6.1. The topic will in-
clude an adaptation of system utterances (en-
trainment) to the user (Hu et al., 2014).
2. Eye tracker: Although we believe head pose
is good enough to estimate user intentions be-
cause we are trained to move the head in driv-
ing schools to look around to confirm safety,
we would like to confirm the difference in
this task between face direction and eye-gaze.
3. POI identification using face direction trajec-
tory: Our analysis showed that the use of face
direction sometimes degrades the POI identi-
fication performance. However, we believe
that using a trajectory of face direction will
change the result.
4. Database: We assumed a clean and perfect
database but we are going to evaluate the per-
formance when noisy database is used. (e.g.
A database based on image recognition re-
sults or user dialog log.)
5. Feedback: Koller et al. (2012) demonstrated
referential resolution is enhanced by giving
gaze information feedback to the user. We
would like to analyze the effect of feedback
with an automotive augmented reality envi-
ronment using our 3D head-up display (Ng-
Thow-Hing et al., 2013).
8 Related Work
The related studies include a landmark-based nav-
igation that handles landmarks as information for
a dialog. Similar system concepts have been
provided for pedestrian navigation situations (Ja-
narthanam et al., 2013; Hu et al., 2014), they do
not handle a rapidly changing environment.
Several works have used timing to enhance
natural interaction with systems. Rose and
Horvitz (2003) and Raux and Eskenazi (2009)
used timing information to detect user barge-ins.
Studies on incremental speech understanding and
generation (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012) have proved that real-time feed-
back actions have potential benefits for users.
Komatani et al. (2012) used user speech timing
against user?s previous and system?s utterances
to understand the intentions of user utterances.
While the above studies have handled timing fo-
cusing on (para-)linguistic aspect, our work han-
dles timing issues in relation to the user?s physical
surroundings.
Recent advancements in gaze and face direction
estimation have led to better user behavior under-
standing. There are a number of studies that have
analyzed relationship between gaze and user in-
tention, such as user focus (Yonetani et al., 2010),
preference (Kayama et al., 2010), and reference
expression understanding (Koller et al., 2012), be-
tween gaze and turn-taking (Jokinen et al., 2010;
Kawahara, 2012). Nakano et al. (2013) used face
direction for addressee identification. The previ-
ous studies most related to ours are reference res-
olution methods by Chai and Prasov (2010), Iida
et al. (2011) and Kennington et al. (2013). They
confirmed that the system?s reference resolution
performance is enhanced by taking the user?s eye
fixation into account. However, their results are
not directly applied to an interaction in a rapidly
changing environment while driving, where eye
fixations are unusual activities.
Marge and Rudnicky (2010) analyzed the effect
of space and distance for spatial language under-
standing for a human-robot communication. Our
task differs with this because we handle a rapidly
changing environment. We believe we can im-
prove our understanding performance based on
their findings.
9 Conclusion
We addressed situated language understanding in
a moving car. We focused on issues in understand-
ing user language of timing, spatial distance, and
linguistic cues. Based on the analysis of the col-
lected user utterances, we proposed methods of us-
ing start-of-speech timing for the POI prior calcu-
lation, GMM-based POI probability (prior) calcu-
lation, and linguistic cues for the posterior calcula-
tion to improve the accuracy of situated language
understanding. The effectiveness of the proposed
methods was confirmed by achieving a significant
improvement in a POI identification task.
29
10 Acknowledgments
The authors would like to thank Yi Ma at Ohio
State University for his contributions to the devel-
opment of HRItk.
References
D. Bohus and E. Horvitz. 2009. Models for Multi-
party Engagement in Open-World Dialog. In Proc.
SIGDIAL, pages 225?234.
J. Chai and Z. Prasov. 2010. Fusing eye gaze with
speech recognition hypotheses to resolve exophoric
reference in situated dialogue. In Proc. EMNLP.
D. Cohen, A. Chandrashekaran, I. Lane, and A. Raux.
2014. The hri-cmu corpus of situated in-car interac-
tions. In Proc. IWSDS, pages 201?212.
N. Dethlefs, H. Hastie, V. Rieser, and O. Lemon. 2012.
Optimising incremental dialogue decisions using in-
formation density for interactive systems. In Proc.
EMNLP, pages 82?93.
Z. Hu, G. Halberg, C. Jimenez, and M. Walker. 2014.
Entrainment in pedestrian direction giving: How
many kinds of entrainment? In Proc. IWSDS, pages
90?101.
R. Iida, M. Yasuhara, and T. Tokunaga. 2011. Multi-
modal reference resolution in situated dialogue by
integrating linguistic and extra-linguistic clues. In
Proc. IJCNLP, pages 84?92.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, and T. Dalmas. 2013. A multithreaded con-
versational interface for pedestrian navigation and
question answering. In Proc. SIGDIAL, pages 151?
153.
K. Jokinen, M. Nishida, and S. Yamamoto. 2010. On
eye-gaze and turn-taking. In Proc. EGIHMI.
T. Kawahara, A. Lee, K. Takeda, K. Itou, and
K. Shikano. 2004. Recent Progress of Open-Source
LVCSR Engine Julius and Japanese Model Reposi-
tory. In Proc. ICSLP, volume IV.
T. Kawahara. 2012. Multi-modal sensing and analysis
of poster conversations toward smart posterboard. In
Proc. SIGDIAL.
K. Kayama, A. Kobayashi, E. Mizukami, T. Misu,
H. Kashioka, H. Kawai, and S. Nakamura. 2010.
Spoken Dialog System on Plasma Display Panel Es-
timating User?s Interest by Image Processing. In
Proc. 1st International Workshop on Human-Centric
Interfaces for Ambient Intelligence (HCIAmi).
C. Kennington, S. Kousidis, and D. Schlangen. 2013.
Interpreting situated dialogue utterances: an update
model that uses speech, gaze, and gesture informa-
tion. In Proc. SIGDIAL.
A. Koller, K. Garoufi, M. Staudte, and M. Crocker.
2012. Enhancing referential success by tracking
hearer gaze. In Proc. SIGDIAL, pages 30?39.
K. Komatani, A. Hirano, and M. Nakano. 2012. De-
tecting system-directed utterances using dialogue-
level features. In Proc. Interspeech.
I. Lane, Y. Ma, and A. Raux. 2012. AIDAS - Immer-
sive Interaction within Vehicles. In Proc. SLT.
Y. Ma, A. Raux, D. Ramachandran, and R. Gupta.
2012. Landmark-based location belief tracking in
a spoken dialog system. In Proc. SIGDIAL, pages
169?178.
M. Marge and A. Rudnicky. 2010. Comparing Spo-
ken Language Route Instructions for Robots across
Environment Representations. In Proc. SIGDIAL,
pages 157?164.
T. Misu, A. Raux, I. Lane, J. Devassy, and R. Gupta.
2013. Situated multi-modal dialog system in vehi-
cles. In Proc. Gaze in Multimodal Interaction, pages
25?28.
Y. Nakano, N. Baba, H. Huang, and Y. Hayashi.
2013. Implementation and evaluation of a multi-
modal addressee identification mechanism for mul-
tiparty conversation systems. In Proc. ICMI, pages
35?42.
V. Ng-Thow-Hing, K. Bark, L. Beckwith, C. Tran,
R. Bhandari, and S. Sridhar. 2013. User-centered
perspectives for automotive augmented reality. In
Proc. ISMAR.
M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote,
J. Leibs, R. Wheeler, and A. Ng. 2009. ROS:
an open-source Robot Operating System. In Proc.
ICRA Workshop on Open Source Software.
A. Raux and M. Eskenazi. 2009. A Finite-state Turn-
taking Model for Spoken Dialog Systems. In Proc.
HLT/NAACL, pages 629?637.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Proc. Interspeech, pages 801?804.
R. Rose and H. Kim. 2003. A hybrid barge-in proce-
dure for more reliable turn-taking in human-machine
dialog systems. In Proc. Automatic Speech Recog-
nition and Understanding Workshop (ASRU), pages
198?203.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proc. SIGDIAL, pages 1?8.
K. Sugiura, N. Iwahashi, H. Kawai, and S. Nakamura.
2011. Situated spoken dialogue with robots using
active learning. Advance Robotics, 25(17):2207?
2232.
30
Table 7: Example user utterances
- What is that blue restaurant on the right?
- How about this building to my right with outside seating?
- What is that Chinese restaurant on the left?
- Orange building to my right.
- What kind of the restaurant is that on the corner?
- The building on my right at the corner of the street.
- What about the building on my right with woman with a jacket in front
- Do you know how good is this restaurant to the left?
- Townsurfer, there is an interesting bakery what is that?
- Is this restaurant on the right any good?
S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Baner-
jee, S. Teller, and N. Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. AAAI.
R. Yonetani, H. Kawashima, T. Hirayama, and T. Mat-
suyama. 2010. Gaze probing: Event-based estima-
tion of objects being focused on. In Proc. ICPR,
pages 101?104.
11 Appendix
Test route:
https://www.google.com/maps/
preview/dir/Honda+Research+
Institute,+425+National+Ave+
%23100,+Mountain+View,+CA+
94043/37.4009909,-122.0518957/
37.4052337,-122.0565795/37.
3973374,-122.0595982/37.4004787,
-122.0730021/Wells+Fargo/37.
4001639,-122.0729708/37.3959193,
-122.0539449/37.4009821,-122.
0540093/@37.3999836,-122.
0792529,14z/data=!4m21!4m20!
1m5!1m1!1s0x808fb713c225003d:
0xcf989a0bb230e5c0!2m2!
1d-122.054006!2d37.401016!
1m0!1m0!1m0!1m0!1m5!1m1!1s0x0:
0x86ca9ba8a2f15150!2m2!1d-122.
082546!2d37.388722!1m0!1m0!1m0!
3e0
31
