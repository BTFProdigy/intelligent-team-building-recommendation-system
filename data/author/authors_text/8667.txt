Bootstrapping Both Product Features and Opinion Words from Chi-
nese Customer Reviews with Cross-Inducing1   
Bo Wang 
Institute of Computational Linguistics 
Peking University 
Beijing, 100871, China 
wangbo@pku.edu.cn  
Houfeng Wang 
Institute of Computational Linguistics 
Peking University 
Beijing, 100871, China 
wanghf@pku.edu.cn  
 
 
Abstract 
We consider the problem of 1  identifying 
product features and opinion words in a 
unified process from Chinese customer re-
views when only a much small seed set of 
opinion words is available. In particular, 
we consider a problem setting motivated by 
the task of identifying product features 
with opinion words and learning opinion 
words through features alternately and it-
eratively. In customer reviews, opinion 
words usually have a close relationship 
with product features, and the association 
between them is measured by a revised 
formula of mutual information in this paper. 
A bootstrapping iterative learning strategy 
is proposed to alternately both of them. A 
linguistic rule is adopted to identify low-
frequent features and opinion words. Fur-
thermore, a mapping function from opinion 
words to features is proposed to identify 
implicit features in sentence. Empirical re-
sults on three kinds of product reviews in-
dicate the effectiveness of our method. 
1 Introduction 
With the rapid expansion of network application, 
more and more customer reviews are available on-
line, which are beneficial for product merchants to 
track the viewpoint of old customers and to assist 
potential customers to purchase products. However, 
                                                 
1 Supported by National Natural Science Foundation of China 
under grant No.60675035 and Beijing Natural Science Foun-
dation under grant No.4072012 
it?s time-consuming to read all reviews in person. 
As a result, it?s significant to mine customer re-
views automatically and to provide users with 
opinion summary. 
In reality, product features and opinion words 
play the most important role in mining opinions of 
customers. One customer review on some cell 
phone is given as follows: 
 
     (a) ?????????????????(The 
appearance is beautiful, the screen is big 
and the photo effect is OK.)  
 
Product features are usually nouns such as ??
?? (appearance) and ???? (screen) or noun 
phrases such as ?????? (photo effect) express-
ing which attributes the customers are mostly con-
cerned. Opinion words (opword is short for ?opin-
ion word?) are generally adjectives used to express 
opinions of customers such as ???? (beautiful), 
??? (big) and ??? (well). As the core part of an 
opinion mining system, this paper is concentrated 
on identifying both product features and opinion 
words in Chinese customer reviews. 
There is much work on feature extraction and 
opinion word identification. Hu and Liu (2004) 
makes use of association rule mining (Agrawal and 
Srikant, 1994) to extract frequent features, the sur-
rounding adjectives of any extracted feature are 
considered as opinion words. Popescu and Etzioni 
(2005) has utilized statistic-based point-wise mu-
tual information (PMI) to extract product features. 
Based on the association of opinion words with 
product features, they take the advantage of the 
syntactic dependencies computed by the MINIPAR 
parser (Lin, 1998) to identify opinion words. Tur-
289
ney (2002) applied a specific unsupervised learn-
ing technique based on the mutual in-formation 
between document phrases and two seed words 
?excellent? and ?poor?.  
Nevertheless, in previous work, identifying 
product features and opinion words are always 
considered two separate tasks. Actually, most 
product features are modified by the surrounding 
opinion words in customer reviews, thus they are 
highly context dependent on each other, which is 
referred to as context-dependency property hence-
forth. With the co-occurrence characteristic, identi-
fying product features and opinion words could be 
combined into a unified process. In particular, it is 
helpful to identify product features by using identi-
fied opinion words and vice versa. That implies 
that such two subtasks can be carried out alter-
nately in a unified process.  Since identifying 
product features are induced by opinion words and 
vice versa, this is called cross-inducing.  
As the most important part of a feature-based 
opinion summary system, this paper focuses on 
learning product features and opinion words from 
Chinese customer reviews. Two sub-tasks are in-
volved as follows: 
Identifying features and opinion words: Resort-
ing to context-dependency property, a bootstrap-
ping iterative learning strategy is proposed to iden-
tify both of them alternately. 
Identifying implicit features: Implicit features 
occur frequently in customer reviews. An implicit 
feature is defined as a feature that does not appear 
in an opinion sentence. The association between 
features and opinion words calculated with the re-
vised mutual information is used to identify im-
plicit features.  
This paper is sketched as follows: Section 2 de-
scribes the approach in detail; Experiment in sec-
tion 3 indicates the effectiveness of our approach. 
Section 4 presents related work and section 5 con-
cludes and presents the future work. 
2 The Approach 
Figure 1 illustrates the framework of an opinion 
summary framework, the principal parts related to 
this paper are shown in bold. The first phase 
?identifying features and opinion words?, works 
iteratively to identify features with the opinion 
words identified and learn opinion words through 
the product features identified alternately. Then, 
one linguistic rule is used to identify low-frequent 
features and opinion words. After that, a mapping 
function is designed to identify implicit features. 
 
 
Figure 1. The framework of an opinion summary 
system 
2.1 Iterative Learning Strategy 
Product features and opinion words are highly con-
text-dependent on each other in customer reviews, 
i.e., the feature ???? (body) for digital camera 
often co-occur with some opinion words such as 
??? (big) or ???? (delicate) while the feature 
????? (the proportion of performance to price)  
often co-occurs with the opinion word ??? (high).  
Product features can be identified resorting to 
the surrounding opinion words identified before 
and vice versa. A bootstrapping method that works 
iteratively is proposed in algorithm 1.  
Algorithm 1 works as follows: given the seed 
opinion words and all the reviews, all noun phrases 
(noun phrases in the form ?noun+?) form CandFe-
aLex (the set of feature candidates) and all adjec-
tives compose of CandOpLex (the set of the candi-
dates of opinion words). The sets ResFeaLex and 
ResOpLex are used to store final features and opin-
ion words. Initially, ResFeaLex is set empty while 
ResOpLex is composed of all the seed opinion 
words. At each iterative step, each feature candi-
date in CandFeaLex is scored by its context-
dependent association with each opword in ResO-
pLex, the candidate whose score is above the pre-
specified threshold Thresholdfeature is added to Res 
290
Algorithm 1. Bootstrap learning product features and opinion words with cross-inducing 
Bootstrap-Learning (ReviewData, SeedOpLex, Thresholdfeature, Thresholdopword) 
1   Parse(ReviewData); 
2   ResFeaLex = {}, ResOpLex = SeedOpLex; 
3   CandFeaLex = all noun phrases in ReviewData; 
4   CandOpLex = all adjectives in ReviewData; 
5   while  (CandFeaLex?{} && CanOpLex?{})  
6        do for each candfea?CandFeaLex 
7              do for each opword?ResOpLex  
8                    do calculate RMI(candfea,opword) with ReviewData; 
9                score(canfea)=?opword?ResOpLexRMI(candfea,opword)/|ResOpLex|; 
10         sort CandFeaLex by score; 
11         for each candfea?CandFeaLex  
12               do  if  (score(candfea)> Thresholdfeature) 
13                       then   ResFeaLex=ResFeaLex+{candfea}; 
14                                CanFeaLex=CandFeaLex ? {candfea}; 
15          for each candop?CandOpLex  
16                 do for each feature?ResFeaLex  
17                      do calculate RMI(candop,feature) with D; 
18                 score(candop)=?feature?ResFeaLexRMI(feature,candop)/|ResFeaLex| ; 
19          sort  CandOpLex by score; 
20          for each candop?CandOpLex 
21       do  if  (score (candop)>Thresholdopword) 
22               then  ResOpLex=ResOpLex+{candop }; 
23                     CanOpLex=CandOpLex ? {candop}; 
24          if  (neither candfea and candop is learned) then break;  
25   return ResFeaLex, ResOpLex; 
 
FeaLex and subtracted from CandFeaLex. Simi-
larly, opinion words are processed in this way, but 
the scores are related to features in ResFeaLex. 
The iterative process continues until neither Res-
FeaLex nor ResOpLex is altered. Any feature can-
didate and opinion word candidate, whose relative 
distance in sentence is less than or equal to the 
specified window size Minimum-Offset, are re-
garded to co-occur with each other. The associa-
tion between them is calculated by the revised mu-
tual information denoted by RMI, which will be 
described in detail in the following section and 
employed to identify implicit features in sentences. 
2.2  Revised Mutual Information 
In customer reviews, features and opinion words 
usually co-occur frequently, features are usually 
modified by the surrounding opinion words. If the 
absolute value of the relative distance in a sentence 
for a feature and an opinion word is less than 
Minimum-Offset, they are considered context-
dependent. 
Many methods have been proposed to measure 
the co-occurrence relation between two words such 
as ?2 (Church and Mercer,1993) , mutual informa-
tion (Church and Hanks, 1989; Pantel and Lin, 
2002), t-test (Church and Hanks, 1989), and log-
likelihood (Dunning,1993). In this paper a revised 
formula of mutual information is used to measure 
the association since mutual information of a low-
frequency word pair tends to be very high.  
Table 1 gives the contingency table for two 
words or phrases w1 and  w2, where A is the num-
ber of reviews where w1 and w2 co-occur; B indi-
cates the number of reviews where w1 occurs but 
does not co-occur with w2; C denotes the number 
of reviews where w2 occurs but does not co-occur 
with w1; D is number of reviews where neither w1 
nor w2 occurs; N = A + B + C + D. 
With the table, the revised formula of mutual in-
formation is designed to calculate the association 
of w1 with w2 as formula (1). 
 
 w2 ~w2
w1 A B 
~w1 C D 
Table 1:  Contingency table 
291
 1 2
1 2 1 2
1 2
( , )
( , ) ( , ) log
( ) ( )
p w w
RMI w w freq w w
p w p w
= ? i                             
log
( ) (
N A
A
)A B A C
?= ? + ? +                           (1) 
 
2.3 Identifying Low-Frequent Features and 
Opinion Words 
In Chinese reviews, one linguistic rule ?noun+ ad-
verb* adjective+? occurs frequently and most of 
the instances of the rule are used to express posi-
tive or negative opinions on some features, i.e., ??
?/noun ??/adverb ??/adjective? (The body is 
rather delicate) , where each Chinese word and its 
part-of-speech is separated by the symbol ?/?.  
Intuitively, this linguistic rule can be used to 
improve the output of the iterative learning. For 
each instance of the rule, if ?noun+? exists in Res-
FeaLex, the ?adjective? part would be added to 
ResOpLex, and if ?adjective+? exists in ResOpLex, 
the noun phrase ?noun+? part will be added to 
ResFeaLex. After that, most low-frequent features 
and opinion words will be recognized. 
2.4 Identifying Implicit Features 
The context-dependency property indicates the 
context association between product features and 
opinion words. As a result, with the revised mutual 
information, the implicit features can be deduced 
from opinion words. A mapping function f: op-
word? feature is used to deduce the mapping fea-
ture for opword , where f(opword) is defined as the 
feature with the largest association with opinion 
word. 
If an opinion sentence contains opinion words, 
but it does not have any explicit features, the map-
ping function f: opword? feature is employed to 
generate the implicit feature for each opinion word 
and the feature is considered as an implicit feature 
in the opinion sentence. Two instances are given in 
(b) and (c), where the implicit features are inserted 
in suitable positions and they are separated in pa-
rentheses. Since f (???? (beautiful)) = ???? 
(appearance) and f (???? (fashionable)) = ??
?? (appearance), ???? (appearance) is an im-
plicit feature in (b). Similarly, the implicit features 
in (c) are ???? (performance) and ???? (pic-
ture). 
 
(b) (??)????(??)???It?s (appear-
ance) beautiful and (appearance) fashion-
able. 
(c) (??)??????(??)????It?s 
(performance) very stable and (picture) 
very clear.  
3 Experiment 
3.1 Data Collection 
We have gathered customer reviews of three kinds 
of electronic products from http://it168.com: digi-
tal camera, cell-phone and tablet. The first 300 re-
views for each kind of them are downloaded. One 
annotator was asked to label each sentence with 
product features (including implicit features) and 
opinion words. The annotation set for features and 
opinion words are shown in table 2. 
 
Product 
Name 
No. of Fea-
tures 
No. of Opin-
ion Words 
digital camera 135 97 
cell-phone 155 125 
tablet 96 83 
Table 2 . Annotation set for product features and 
opinion words   
 
Unlike English, Chinese are not separated by 
any symbol. Therefore, the reviews are tokenized 
and tagged with part-of-speech by a tool 
ICTCLAS2.One example of the output of this tool 
is as (d).  
 
(d) ??/n  ??/n  ?/d  ?/d  ?/a  ?/w  ??
/n  ???/n  ??/v  ?/d  ??/v  ??/v  
??/n  ??/n  ?/w  ??/n  ??/vn  ??
/vn  ?/d  ?/d  ??/a  ?/w 
The seed opinion words employed in the itera-
tive learning are: ???? (clear), ??? (quick),  
??? (white), ???? (weak). ??? (good), ???? 
(good), ??? (high), ??? (little), ??? (many), 
? ? ? (long). Empirically, Thresholdfeature and 
Thresholdopword in Algorithm 1 is set to 0.2, Mini-
mum-Offset is set to 4.  
                                                 
2 http://www.nlp.org.cn   
292
On Set On Sentence  Product Name
Precision Recall F-Score Precision Recall F-Score 
digital camera 64.03% 45.92% 53.49% 46.62% 65.72% 54.55% 
cell-phone 54.43% 43.87% 48.58% 34.17% 55.15% 42.19% 
tablet 51.45% 59.38% 55.13% 41.39% 60.21% 49.06% 
average 56.64% 49.72% 52.40% 40.73% 60.36% 48.60% 
Table 3. Evaluation of apriori algorithm 
 
On Set On Sentence Type Product Name 
Precision Recall F-Score Precision Recall F-score
73.57% 54.81% 62.82% 55.80% 68.69% 61.58%
digital camera 
78.20% 73.33% 75.69% 54.71% 70.80% 63.49%
80.92% 45.81% 58.50% 47.31% 58.59% 52.35%
cell-phone 82.30% 66.46% 73.53% 49.22% 61.63% 54.73%
72.73% 57.29% 64.09% 49.79% 61.03% 54.84%
tablet 77.99% 73.96% 75.92% 52.54% 64.43% 57.88%
75.74% 52.64% 61.80% 50.97% 62.77% 56.26%
feature 
average 79.50% 71.25% 75.05% 52.16% 65.62% 58.70%
89.02% 38.02% 53.28% 72.35% 50.24% 59.30%digital camera 
87.31% 60.94% 71.78% 69.40% 85.28% 76.53%
87.95% 30.80% 45.63% 66.44% 42.84% 52.09%
cell-phone 88.49% 51.90% 65.43% 63.14% 79.51% 70.39%
77.94% 30.64% 43.98% 61.30% 42.69% 50.34%
tablet 80.73% 50.87% 62.41% 63.92% 81.02% 71.46%
84.97% 33.15% 47.63% 66.70% 45.26% 53.91%
opword 
average 85.51% 54.57% 66.54% 65.49% 81.94% 72.79%
Table 4. Evaluation of iterative learning (the upper) and the combination of iterative learning and the 
linguistic rule (the lower). 
3.2 Evaluation Measurement 
As Hu and Liu (2004), the features mined form the 
result set while the features in the manually anno-
tated corpus construct the answer set. With the two 
sets, precision, recall and f-score are used to evalu-
ate the experiment result on set level.  
In our work, the evaluation is also conducted on 
sentence for three factors: Firstly, each feature or 
opinion word may occur many times in reviews but 
it just occurs once in the corresponding answer set; 
Secondly, implicit features should be evaluated on 
sentence; Besides, to generate an opinion summary, 
the features and the opinion words should be iden-
tified for each opinion sentence.  
On sentence, the features and opinion words 
identified for each opinion sentence are compared 
with the annotation result in the corresponding sen-
tence. Precision, recall and f-score are also used to 
measure the performance. 
3.3 Evaluation 
Hu and Liu (2004) have adopted associate rule 
mining to mine opinion features from customer 
reviews in English. Since the original corpus and 
source code is not available for us, in order to 
make comparison with theirs, we have re-
implemented their algorithm, which is denoted as 
apriori method as follows. To be pointed out is that, 
the two pruning techniques proposed in Hu and Liu 
(2004): compactness pruning and redundancy 
pruning, were included in our experiment. The 
evaluation on our test data is listed in table 3. The 
row indexed by average denotes the average per-
formance of the corresponding column and each 
entry in it is bold. 
Table 4 shows our testing result on the same 
data, the upper value in each entry presents the re-
sult for iterative learning strategy while the lower 
values denote that for the combination of iterative 
learning and the linguistic rule. The average row 
293
shows the average performance for the correspond-
ing columns and each entry in the row is shown in 
bold. 
On feature, the average precision, recall and f-
score on set or sentence increase according to the 
order apriori < iterative <  ite+rule, where apriori 
indicates Hu and Liu?s method, iterative represents 
iterative strategy and iterative+rule denotes the 
combination of iterative strategy and the linguistic 
rule. The increase range from apriori to itera-
tive+rule of f-score on set gets to 22.65% while on 
sentence it exceeds 10%. The main reason for the 
poor performance on set for apriori is that many 
common words such as ???? (computer), ???? 
(China) and ???? (time of use) with high fre-
quency are extracted as features. Moreover, the 
poor performance on sentence for apriori method is 
due to that it can?t identify implicit features. Fur-
thermore, the increase in f-score from iterative to 
ite+rule on set and on sentence shows the perform-
ance can be enhanced by the linguistic rule. 
Table 4 also shows that the performance in 
learning opinion words has been improved after 
the linguistic rule has been used. On set, the aver-
age precision increases from 84.97% to 85.51% 
while the average recall from 33.15% to 54.57%. 
Accordingly, the average f-score increase signifi-
cantly by about 18.91%. 
On sentence, although there is a slow decrease 
in the average precision, there is a dramatic in-
crease in the average recall, thus the average f-
score has increased from 53.91% to 72.79%. Fur-
thermore, the best f-score (66.54%) on set and the 
best f-score (72.79%) on sentence indicate the ef-
fectiveness of ite+rule on identifying opinion 
words. 
4 Related Work 
Our work is much related to Hu?s system (Hu and 
Liu,2004), in which association rule mining is used 
to extract frequent review noun phrase as features. 
After that, two pruning techniques: compactness 
pruning and redundancy pruning, are utilized. Fre-
quent features are used to find potential opinion 
words (adjectives) and WordNet syno-
nyms/antonyms in conjunction with a set of seed 
words are used in order to find actual opinion 
words. Finally, opinion words are used to extract 
associated infrequent features. The system only 
extracts explicit features. Our work differs from 
hers at two aspects: (1) their method can?t identify 
implicit features which occur frequently in opinion 
sentences; (2) Product features and opinion words 
are identified on two separate steps in Hu?s system 
but they are learned in a unified process here and 
induced by each other in this paper. 
Popescu and Etzioni (2005) has used web-based 
point-wise mutual information (PMI) to extract 
product features and use the identified features to 
identify potential opinion phrases with co-
occurrence association. They take advantage of the 
syntactic dependencies computed by the MINIPAR 
parser. If an explicit feature is found in a sentence, 
10 extraction rules are applied to find the heads of 
potential opinion phrases. Each head word together 
with its modifier is returned as a potential opinion 
phrase. Our work is different from theirs on two 
aspects: (1) Product features and opinion words are 
identified separately but they are learned simulta-
neously and are boosted by each other here. (2) 
They have utilized a syntactic parser MINIPAR, 
but there?s no syntactic parser available in Chinese, 
thus the requirement of our algorithm is only a 
small seed opinion word lexicon. Although co-
occurrence association is used to derive opinion 
words from explicit features in their work, the way 
how co-occurrence association is represented is 
different. Besides, the two sub-tasks are boosted by 
each other in this paper. 
On identifying opinion words, Morinaga et al
(2002)has utilized information gain to extract clas-
sification features with a supervised method; Hat-
zivassiloglou and Wiebe (1997) used textual  junc-
tions such as ?fair and legitimate? or ?simplistic 
but well-received? to separate similarity- and op-
positely-connoted words; Other methods are pre-
sent in (Riloff et al 2003; Riloff and Wiebe, 2003; 
Gamon and Aue, 2005; Wilson et al 2006) The 
principal difference from previous work is that, 
they have considered extracting opinion words as a 
separate work but we have combined identifying 
features and opinion words in a unified process. 
Besides, the opinion words are identified for sen-
tences but in their work they are identified for re-
views. 
5 Conclusion 
In this paper, identifying product features and 
opinion words are induced by each other and are 
combined in a unified process. An iterative learn-
294
ing strategy based on context-dependence property 
is proposed to learn product features and opinion 
words alternately, where the final feature lexicon 
and opinion word lexicon are identified with very 
few knowledge (only ten seed opinion words) and 
augmented by each other alternately. A revised 
formula of mutual information is used to calculate 
the association between each feature and opinion 
word. A linguistic rule is utilized to recall low-
frequent features and opinion words. Besides, a 
mapping function is designed to identify implicit 
features in sentence. In addition to evaluating the 
result on set, the experiment is evaluated on sen-
tence. Empirical result indicates that the perform-
ance of iterative learning strategy is better than 
apriori method and that features and opinion words 
can be identified with cross-inducing effectively. 
Furthermore, the evaluation on sentence shows the 
effectiveness in identifying implicit features. 
In future, we will learn the semantic orientation 
of each opinion word, calculate the polarity of each 
subjective sentence, and then construct a feature-
based summary system. 
References 
Ana Maria Popescu and Oren Etzioni. 2005. Extracting 
Product Features and Opinions from Reviews. Pro-
ceedings of HLT-EMNLP (2005) 
De-Kang Lin. 1998. Dependency-Based Evaluation of 
MINIPAR. In:Proceedings of the Workshop on the 
Evaluation of Parsing Systems, Granada, Spain, 1998, 
298?312 
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning Subjective Nouns Using Extraction Pattern 
Bootstrapping. Seventh Conference on Natural Lan-
guage Learning (CoNLL-03). ACL SIGNLL. Pages 
25-32. 
Ellen Riloff and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP-03). ACL SIGDAT. 2003, 105-112. 
Kenneth Ward Church and Robert L. Mercer. 1993.  
Introduction to the special issue on computational 
linguistics using large corpora. Computational Lin-
guistics 19:1-24 
Kenneth Ward Church and Patrick Hanks. 1989. Word 
Association Norms, Mutual Information and Lexi-
cography. Proceedings of the 26th Annual Confer-
ence of the Association for Computational Linguis-
tics(1989). 
Michael Gamon and Anthony Aue. 2005. Automatic 
identification of sentiment vocabulary: exploiting low 
association with known sentiment terms. In :ACL 
2005 Workshop on Feature Engineering,2005.  
Minqing Hu and Bing Liu. 2004. Mining Opinion Fea-
tures in Customer Reviews. Proceedings of Nineteeth 
National Conference on Artificial Intellgience 
(AAAI-2004), San Jose, USA, July 2004. 
Patrick Pantel and Dekang Lin. 2002. Document Clus-
tering with Committees. In Proceedings of ACM 
Conference on Research and Development in Infor-
mation Retrieval (SIGIR-02). pp. 199-206. Tampere, 
Finland. 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL 2002: 417-424 
Rakesh Agrawal and Ramakrishan Srikant. 1994. Fast 
algorithm for mining association rules. VLDB?94, 
1994. 
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and 
Toshikazu Fukushima. 2002. Mining Product Repu-
tations on the WEB, Proceedings of 8th ACM 
SIGKDD International Conference on Knowledge. 
Discover and Data Mining, (2002) 341-349 
Ted Dunning. 1993.  Accurate methods for the statistics 
of surprise and coincidence. Computational Linguis-
tics 19:61-74 
Theresa Wilson , Janyce Wiebe, and Rebecca Hwa. 
2006. Recognizing strong and weak opinion clauses.  
Computational Intelligence 22 (2): 73-99. 
295
Chinese Named Entity Recognition and Word Segmentation  
Based on Character 
He Jingzhou, Wang Houfeng 
Institution of Computational Linguistics 
School of Electronics Engineering and Computer Science, 
Peking University, China, 100871 
{hejingzhou, wanghf}@pku.edu.cn 
 
 
 
 
Abstract 
Chinese word segmentation and named 
entity recognition (NER) are both important 
tasks in Chinese information processing. 
This paper presents a character-based 
Conditional Random Fields (CRFs) model 
for such two tasks. In The SIGHAN 
Bakeoff 2007, this model participated in all 
closed tracks for both Chinese NER and 
word segmentation tasks, and turns out to 
perform well. Our system ranks 2nd in the 
closed track on NER of MSRA, and 4th in 
the closed track on word segmentation of 
SXU. 
1 Introduction 
Chinese word segmentation and NER are two of 
the most fundamental problems in Chinese 
information processing and have attracted more 
and more attentions. Many methods have been 
presented, of which, machine learning methods 
have obviously competitive advantage in such 
problems. Maximum Entropy (Ng and Low, 2005) 
and CRFs (Hai Zhao et al 2006, Zhou Junsheng et 
al. 2006) come to good performance in the former 
SIGHAN Bakeoff.  
We consider both tasks as sequence labeling 
problem, and a character-based Conditional 
Random Fields (CRFs) model is applied in this 
Bakeoff. Our system used CRF++ package 
Version 0.49 implemented by Taku Kudo from 
sourceforge1.   
2 System Description 
The system is mainly based on CRFs, while 
different strategies are introduced in word 
segmentation task and NER task.  
2.1 CRFs 
CRFs are undirected graphical models which are 
particularly well suited to sequence labeling tasks, 
such as NER & word segmentation. In these cases, 
CRFs are often referred to as linear chain CRFs.  
CRFs are criminative models, which allow a 
richer feature representation and provide more 
natural modeling.  
                                                             
1
 http://www.sourceforge.net/ 
128
Sixth SIGHAN Workshop on Chinese Language Processing
CRFs define the conditional probability of a 
state sequence given an input sequence as  
 
Where F is a feature function set over its 
arguments, ?k is a learned weight for each feature 
function, and Z is the partition function, which 
ensures that p is appropriately normalized.  
2.2 Word Segmentation Task 
Similar to (Ng and Low, 2005), a Chinese 
character comes into four different tags, as in 
Table 1. 
  
Tag Meaning 
S Character that occurs as a 
single-character word 
B Character that begins a 
multi-character word 
I Character that continues a 
multi-character word 
E Character that ends a 
multi-character word 
Table 1. Word segmentation tag set 
(Ng and Low, 2005) presented feature templates 
as:  
(a) Cn(n = ?2,?1, 0, 1, 2) 
(b) CnCn+1(n = ?2,?1, 0, 1) 
(c) C?1C1 
(d) Pu(C0) 
(e) T(C?2)T(C?1)T(C0)T(C1)T(C2) 
 
In order to find the effect of different features on 
the result, some experiments are conducted on 
these templates, with 90% of the training data 
provided by The SIGHAN Bakeoff 2007 for 
training, leaving 10% for testing. Based on our 
results, some templates are adjusted as follows. 
First, the character-window is reduced to (-1, 0, 
1) in (a).  
Second, feature template (d) is not used. Instead, 
original sentences are split into clauses ended with 
punctuations ???, ???, ???, ???, ??? and ???. 
There are two advantages of this processing: (1) 
the template is simplified but little performance is 
lost; (2) shorter sentences make CRFs training 
quicker.  
Third, template (e) is separate it to three items: 
T(C-1), T(C0) and T(C1), in which five types of 
the characters are considered: N stands for 
numbers, D for dates, E for English letters, S for 
punctuations and C for other characters. Besides, 
this feature template does not always contribute to 
the segmentation result in our experiments, so it 
will be a tradeoff whether to use it or not according 
to experiments.  
Finally, we use the following feature templates: 
(a) Cn(n = ?1, 0, 1) 
(b) CnCn+1(n = ?1, 0) 
(c) C?1C1 
(d) T(Cn) (n = ?1, 0, 1) 
 
We only took part in word segmentation closed 
track, so no additional corpora, dictionary or 
linguistic resources are introduced.  
1.1 NER Task 
Many Chinese NER researches are based on word 
segmentation and even Part-Of-Speech (POS) 
tagging. In fact these steps are not necessary. The 
relationship of them is described in Figure 1.  
129
Sixth SIGHAN Workshop on Chinese Language Processing
  
In closed track of both MSRA and CITYU, a 
character-based CRFs model is used in our system. 
There two reasons as follows: 
First, no word-level information is provided in 
training data of NER tasks in closed track, so it?s 
hard to perform word segmentation with good 
accuracy.  
Second, we had done some experiments on 
Chinese NER, and found that character-based 
method outperformed word segmentation and word 
segmentation + POS, if only character sequence is 
given. Table 2 shows the comparison results.  
 
Feature 
Level 
Integrated 
F-measure 
Character 0.8760 
Word 0.8538 
POS 0.8635 
Table 2. Comparison result among different NER 
models3 
                                                             
3
 Train with Annotation Corpora of People's Daily 
199801 and test with 199806 
In our NER system, a Chinese character can be 
labeled as one of four different tags, as in Table 3. 
 
Tag Meaning 
B First character of a NE 
I Character in a NE but neither 
the first nor the last one 
E Last character of a NE except a 
single-character one 
O Character not in a NE 
Table 3. NER tag set 
It?s similar to the standard of The SIGHAN 
Bakeoff 2007 NER track except for an additional 
tag ?E?. Unlike the tag set used in word 
segmentation task, there is no ?S? tag for 
single-character NEs. This kind of entities is 
usually surname of a Chinese person. In this case, 
the tag ?B? will handle it as well.  
There are actually 3 types of NEs in MSRA and 
CITYU corpora: PER, LOC and ORG, so the tag 
set is further divided into 10 sub tags: B-PER, 
I-PER, E-PER, B-LOC, I-LOC, E-LOC, B-ORG, 
I-ORG, E-ORG and O.  
The feature template is similar to the one used in 
word segmentation task except that here a 
character-window of (-2,-1, 0, 1,2) is applied: 
(a) Cn(n = ?2,?1, 0, 1, 2) 
(b) CnCn+1(n = ?2,?1, 0, 1) 
(c) C?1C1 
(d) T(Cn) (n = ?1, 0, 1) 
 
For CRFs, the precision is usually high while 
recall is low. To solve this problem, a set of feature 
templates (only differ in window size, or 
punctuations) are used to train several different 
models, and finally achieve a group of results. 
Merge them as in Table 4 (for the same Chinese 
character string in result A and B). 
 
Recognition 
Tagging 
Segmentation 
Character 
Word 
POS 
NEs 
Figure 1. NER model achitecture 
130
Sixth SIGHAN Workshop on Chinese Language Processing
A B Result 
Is a NE Isn?t a NE Refer to A 
Isn?t a 
NE 
Is a NE Refer to B 
Isn?t a 
NE 
Isn?t a NE Refer to A or B 
Is a NE Is the same 
NE type as 
A 
Refer to A or B 
Is a NE Is a NE but 
not the same 
type as A 
Choose A or B 
according to 
predefined rules 
Table 4. Merge strategy of results 
With a slight loss of precision, an improvement 
is achieved on recall rate.  
In open track of MSRA, an additional 
segmentation system is used on the corpora and 
some NEs are retrieved based on several 
predefined rules. It was merged with closed track 
result to form open track result.  
3 Evaluation Results 
Our word segmentation system is evaluated in 
closed track on all 5 corpora of CITYU, CKIP, 
CTB, NCC and SXU. Table 5 shows our results on 
the best RunID. Columns R, P, and F show the 
recall, precision, and F measure, respectively. 
Column BEST shows best F-measure of all 
participants in the track. 
Our NER system is evaluated in closed track on 
both MSRA and CITYU corpora, and open track 
on MSRA corpora only. Table 6 shows our official 
results on best RunID. Columns R, P, and F show 
the recall, precision, and F measure, respectively. 
Column BEST shows best F-measure of all 
participants in the track. 
 
4 Conclusion 
In this paper, a character-based CRFs model is 
introduced on both word segmentation and NER. 
Experiments are done to form our feature templates, 
and approaches are used to further improve its 
performance on NER. The evaluation results show 
its competitive performance in The SIGHAN 
Bakeoff 2007. We?ll launch more research and 
experiments on feature picking-up methods and 
combination between character-based model and 
other models in the future. 
 
 
 
 
 
 
 
Table 5. Evaluation results on word segmentation 
 
 
 
 
 
 
 
 
Track R P F BEST 
CITYU 
closed 
0.7608 0.8751 0.814 0.8499 
MSRA 
closed 
0.8862 0.9304 0.9078 0.9281 
MSRA 
open 
0.9135 0.9321 0.9227 0.9988 
Table 6. Evaluation results on NER 
Track 
(all 
closed) 
R P F BEST 
CITYU 0.9421 0.9339 0.938 0.951 
CKIP 0.9369 0.927 0.9319 0.947 
CTB 0.9487 0.9514 0.95 0.9589 
NCC 0.9278 0.925 0.9264 0.9405 
SXU 0.9543 0.9568 0.9556 0.9623 
131
Sixth SIGHAN Workshop on Chinese Language Processing
References 
Hai Zhao, Chang-Ning Huang and Mu Li. An 
Improved Chinese Word Segmentation 
System with Conditional Random Field. 2006. 
Proceedings of the Fifth SIGHAN Workshop 
on Chinese Language Processing. 
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. A 
maximum Entropy Approach to Chinese 
Word Segmentation. 2005. Proceedings of the 
Fourth SIGHAN Workshop on Chinese 
Language Processing. 
Wang Xinhao, Lin Xiaojun, Yu Dianhai, Tian 
Hao,Wu Xihong. Chinese Word 
Segmentation with Maximum Entropy and 
N-gram Language Model. 2006. Proceedings 
of the Fifth SIGHAN Workshop on Chinese 
Language Processing. 
Zhou Junsheng, Dai Xinyu, He Liang, Chen Jiajun. 
Chinese Named Entity Recognition with a 
Multi-Phase Model. 2006. Proceedings of the 
Fifth SIGHAN Workshop on Chinese 
Language Processing. 
Acknowledgement 
This paper is supported by National Natural 
Science Foundation of China (No. 60675035), 
National Social Science Foundation of China (No. 
05BYY043) and Beijing Natural Science 
Foundation (No. 4072012). 
132
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 177?180,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Mining User Reviews: from Specification to Summarization
Xinfan Meng
Key Laboratory of
Computational Linguistics
(Peking University)
Ministry of Education, China
mxf@pku.edu.cn
Houfeng Wang
Key Laboratory of
Computational Linguistics
(Peking University)
Ministry of Education, China
wanghf@pku.edu.cn
Abstract
This paper proposes a method to ex-
tract product features from user reviews
and generate a review summary. This
method only relies on product specifica-
tions, which usually are easy to obtain.
Other resources like segmenter, POS tag-
ger or parser are not required. At fea-
ture extraction stage, multiple specifica-
tions are clustered to extend the vocabu-
lary of product features. Hierarchy struc-
ture information and unit of measurement
information are mined from the specifi-
cation to improve the accuracy of feature
extraction. At summary generation stage,
hierarchy information in specifications is
used to provide a natural conceptual view
of product features.
1 Introduction
Review mining and summarization aims to extract
users? opinions towards specific products from
reviews and provide an easy-to-understand sum-
mary of those opinions for potential buyers or
manufacture companies. The task of mining re-
views usually comprises two subtasks: product
features extraction and summary generation.
Hu and Liu (2004a) use association mining
methods to find frequent product features and use
opinion words to predict infrequent product fea-
tures. A.M. Popescu and O. Etzioni (2005) pro-
poses OPINE, an unsupervised information ex-
traction system, which is built on top of the Kon-
wItAll Web information-extraction system. In or-
der to reduce the features redundancy and pro-
vide a conceptual view of extracted features, G.
Carenini et al (2006a) enhances the earlier work
of Hu and Liu (2004a) by mapping the extracted
features into a hierarchy of features which de-
scribes the entity of interest. M. Gamon et al
(2005) clusters sentences in reviews, then label
each cluster with a keyword and finally provide
a tree map visualization for each product model.
Qi Su et al (2008) describes a system that clus-
ters product features and opinion words simulta-
neously and iteratively.
2 Our Approach
To generate an accurate review summary for a
specific product, product features must be iden-
tified accurately. Since product features are of-
ten domain-dependent, it is desirable that the fea-
tures extraction system is as flexible as possible.
Our approach are unsupervised and relies only on
product specifications.
2.1 Specification Mining
Product specifications can usually be fetched from
web sites like Amazon automatically. Those mate-
rials have several characteristics that are very help-
ful to review mining:
1. Nicely structured, provide a natural concep-
tual view of products;
2. Include only relevant information of the
product and contain few noise words;
3. Except for the product feature itself, usually
also provide a unit to measure this feature.
A typical mobile phone specification is partially
given below:
? Physical features
? Form: Mono block with full keyboard
? Dimensions: 4.49 x 2.24 x 0.39 inch
? Weight: 4.47 oz
? Display and 3D
? Size: 2.36 inch
? Resolution: 320 x 240 pixels (QVGA)
177
2.2 Architecture
The architecture of our approach. is depicted in
Figure 1. We first retrieve multiple specifications
from various sources like websites, user manu-
als etc. Then we run clustering algorithms on
the specifications and generate a specification tree.
And then we use this specification tree to extract
features from product reviews. Finally the ex-
tracted features are presented in a tree form.
Specifications Reviews
            Appearance
            Size
              Thickness
            Price
             ...
Size
Price
Thickness
...
2 Feature
Extraction
           Size: small
                  Thickness: thin
            price: low
1 Clustering
3 Summary
Generation
Figure 1: Architecture Overview
2.3 Specification Clustering
Usually, each product specification describes a
particular product model. Some features are
present in every product specification. But there
are cases that some features are not available in all
specifications. For instance, ?WiFi? features are
only available in a few mobile phones specifica-
tions. Also, different specifications might express
the same features with different words or terms.
So it is necessary to combine multiple specifica-
tions to include all possible features. Clustering
algorithm can be used to combine specifications.
We propose an approach that takes following in-
herent information of specifications into account:
? Hierarchy structure: Positions of features
in hierarchy reflect relationships between fea-
tures. For example, ?length?, ?width? feature
are often placed under ?size? feature.
? Unit of measurement: Similar features are
usually measured in similar units. Though
different specification might refer the same
feature with different terms, the units of mea-
surement used to describe those terms are
usually the same. For example, ?dimension?
and ?size? are different terms, but they share
the same unit ?mm? or ?inch?.
Naturally, a product can be viewed as a tree of
features. The root is the product itself. Each node
in the tree represents a feature in the product. A
complex feature might be conceptually split into
several simple features. In this case, the complex
feature is represented as a parent and the simple
features are represented as its children.
To construct such a product feature tree, we
adopt the following algorithm:
? Parse specifications: We first build a dic-
tionary for common units of measurement.
Then for every specification, we use regular
expression and unit dictionary to parse it to a
tree of (feature, unit) pairs.
? Cluster specification trees: Given multiple
specification trees, we cluster them into a sin-
gle tree. Similarities between features are a
combination of their lexical similarity, unit
similarity and positions in hierarchy:
Sim(f1, f2) =Sim
lex
(f1, f2)
+ Sim
unit
(f1, f2)
+ ? ? Sim
parent
(f1, f2)
+ (1? ?) ? Sim
children
(f1, f2)
The parameter ? is set to 0.7 empirically. If
Sim(f1, f2) is larger than 5, we merge fea-
tures f1 and f2 together.
After clustering, we can get a specification tree
resembles the one in subsection 2.1. However,
this specification tree contains much more features
than any single specification.
2.4 Features Extraction
Features described in reviews can be classified into
two categories: explicit features and implicit fea-
tures (Hu and Liu, 2004a). In the following sec-
tions, we describe methods to extract features in
Chinese product reviews. However, these meth-
ods are designed to be flexible so that they can be
easily adapted to other languages.
178
2.4.1 Explicit Feature Extraction
We generate bi-grams in character level for every
feature in the specification tree, and then match
them to every sentence in the reviews. There might
be cases that some bi-grams would overlap or con-
catenated. In these cases, we join those bi-grams
together to form a longer expression.
2.4.2 Implicit Feature Extraction
Some features are not mentioned directly but can
be inferred from the text. Qi Su et al (2008) in-
vestigates the problem of extracting those kinds
of features. There approach utilizes the associa-
tion between features and opinion words to find
implicit features when opinion words are present
in the text. Our methods consider another kind of
association: the association between features and
units of measurement. For example, in the sen-
tence ?Amobile phone with 8 mega-pixel, not very
common in the market.? feature name is absent in
the sentence, but the unit of measurement ?mega
pixel? indicates that this sentence is describing the
feature ?camera resolution?.
We use regular expression and dictionary of unit
to extract those features.
2.5 Summary Generation
There are many ways to provide a summary. Hu
and Liu (2004b) count the number of positive and
negative review items towards individual feature
and present these statistics to users. G. Carenini
et al (2006b) and M. Gamon et al (2005) both
adopt a tree map visualization to display features
and sentiments associated with features.
We adopt a relatively simple method to generate
a summary. We do not predict the polarities of the
user?s overall attitudes towards product features.
Predicting polarities might entail the construction
of a sentiment dictionary, which is domain depen-
dent. Also, we believe that text descriptions of fea-
tures are more helpful to users. For example, for
feature ?size?, descriptions like ?small? and ?thin?
are more readable than ?positive?.
Usually, the words used to describe a product
feature are short. For each product feature, we re-
port several most frequently occurring uni-grams
and bi-grams as the summary of this feature. In
Figure 2, we present a snippet of a sample sum-
mary output.
? mobile phone: not bad, expensive 
o appearance: cool 
 color: white 
 size: small, thin 
o camera functionality: so-so, acceptable 
 picture quality: good 
 picture resolution: not high 
o entertainment functionality: powerful 
 game: fun, simple 
Figure 2: A Summary Snippet
3 Experiments
In this paper, we mainly focus on Chinese prod-
uct reviews. The experimental data are retrieved
from ZOL websites (www.zol.com.cn). We
collected user reviews on 2 mobile phones, 1 digi-
tal camera and 2 notebook computers. To evaluate
performance of our algorithm on real-world data,
we do not perform noise word filtering on these
data. Then we have a human tagger to tag features
in the user reviews. Both explicit features and im-
plicit features are tagged.
No. of Clustering Mobile Digital Notebook
Specifications Phone Camera Computer
1 153 101 102
5 436 312 211
10 520 508 312
Table 1: No. of Features in Specification Trees.
The specifications for all 3 kinds of products
are retrieved from ZOL, PConline and IT168 web-
sites. We run the clustering algorithm on the spec-
ifications and generate a specification tree for each
kind of product. Table 1 shows that our clustering
method is effective in collecting product features.
The number of features increases rapidly with the
number of specifications input into clustering al-
gorithm. When we use 10 specifications as input,
the clustering methods can collect several hundred
features.
Then we run our algorithm on the data and eval-
uate the precision and recall. We also run the al-
gorithms described in Hu and Liu (2004a) on the
same data as the baseline.
From Table 2, we can see the precision of base-
line system is much lower than its recall. Examin-
ing the features extracted by baseline system, we
find that many mistakenly recognized features are
high-frequency words. Some of those words ap-
pear many times in text. They are related to prod-
179
Product Model
No. of Hu and Liu?s Approach the Proposed Approach
Features Precision Recall F-measure Precision Recall F-measure
Mobile Phone 1 507 0.58 0.74 0.65 0.69 0.78 0.73
Mobile Phone 2 477 0.59 0.65 0.62 0.71 0.77 0.74
Digital camera 86 0.56 0.68 0.61 0.69 0.78 0.73
Notebook Computer 1 139 0.41 0.63 0.50 0.70 0.74 0.72
Notebook Computer 2 95 0.71 0.88 0.79 0.76 0.88 0.82
Table 2: Precision and Recall of Product Extraction.
uct but are not considered to be features. Some
examples of these words are ?advantages?, ?dis-
advantages? and ?good points? etc. And many
other high-frequency words are completely irrel-
evant to product reviews. Those words include
?user?, ?review? and ?comment? etc. In contrast,
our approach recognizes features by matching bi-
grams to the specification tree. Because those
high-frequency words usually are not present in
specifications. They are ignored by our approach.
Thus from Table 2, we can conclude that our ap-
proach could achieve a relatively high precision
while keep a high recall.
Product Model Precision
Mobile Phone 1 0.78
Mobile Phone 2 0.72
Digital camera 0.81
Notebook Computer 1 0.73
Notebook Computer 2 0.74
Table 3: Precision of Summary.
After the summary is given, for each word in
summary, we ask one person to decide whether
this word correctly describe the feature. Table 3
gives the summary precision for each product
model. In general, on-line reviews have several
characteristics in common. The sentences are usu-
ally short. Also, words describing features usu-
ally co-occur with features in the same sentence.
Thus, when the features in a sentence are correctly
recognized, Words describing those features are
likely to be identified by our methods.
4 Conclusion
In this paper, we describe a simple but effective
way to extract product features from user reviews
and provide an easy-to-understand summary. The
proposed approach is based only on product spec-
ifications. The experimental results indicate that
our approach is promising.
In future works, we will try to introduce other
resources and tools into our system. We will also
explore different ways of presenting and visualiz-
ing the summary to improve user experience.
Acknowledgments
This research is supported by National Natural
Science Foundation of Chinese (No.60675035)
and Beijing Natural Science Foundation
(No.4072012).
References
M. Hu and B. Liu. 2004a. Mining and Summariz-
ing Customer Reviews. In Proceedings of the 2004
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168-177.
ACM Press New York, NY, USA.
M. Hu and B. Liu. 2004b. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteenth
National Conference on Artificial Intelligence.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining Customer Opinions from Free
Text. In Proceedings of the 6th International Sym-
posium on Intelligent Data Analysis.
A.M. Popescu and O. Etzioni. 2005. Extracting Prod-
uct Features and Opinions from reviews. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing(EMNLP).
Giuseppe Carenini, Raymond T. Ng, and Adam Pauls.
2006a. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Giuseppe Carenini, Raymond T. Ng, and Adam Pauls.
2006b. Interactive multimedia summaries of evalu-
ative text. In Proceedings of Intelligent User Inter-
faces (IUI), pages 124-131. ACM Press, 2006.
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian Wu,
Xiaoxun Zhang, Bin Swen. 2008. Hidden Senti-
ment Association In Chinese Web Opinion Mining.
In Proceedings of the 17th International Conference
on the World Wide Web, pages 959-968.
180
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88?95,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 Clustering Technique in Multi-Document Personal Name Disambigu-
ation 
 
 
Chen Chen 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China 
chenchen@pku.edu.cn 
Hu Junfeng 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China
hujf@pku.edu.cn 
Wang Houfeng 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China
wanghf@pku.edu.cn 
 
  
Abstract 
 
Focusing on multi-document personal name 
disambiguation, this paper develops an agglo-
merative clustering approach to resolving this 
problem. We start from an analysis of point-
wise mutual information between feature and 
the ambiguous name, which brings about a 
novel weight computing method for feature in 
clustering. Then a trade-off measure between 
within-cluster compactness and among-cluster 
separation is proposed for stopping clustering. 
After that, we apply a labeling method to find 
representative feature for each cluster.  Finally, 
experiments are conducted on word-based 
clustering in Chinese dataset and the result 
shows a good effect. 
1 Introduction 
Multi-document named entity co-reference reso-
lution is the process of determining whether an 
identical name occurring in different texts refers 
to the same entity in the real world. With the rap-
id development of multi-document applications 
like multi-document summarization and informa-
tion fusion, there is an increasing need for multi-
document named entity co-reference resolution. 
This paper focuses on multi-document personal 
name disambiguation, which seeks to determine 
if the same name from different documents refers 
to the same person. 
This paper develops an agglomerative cluster-
ing approach to resolving multi-document per-
sonal name disambiguation. In order to represent 
texts better, a novel weight computing method 
for clustering features is presented. It is based on 
the pointwise mutual information between the 
ambiguous name and features. This paper also 
develops a trade-off point based cluster-stopping 
measure and a labeling algorithm for each clus-
ters. Finally, experiments are conducted on 
word-based clustering in Chinese dataset. The 
dataset contains eleven different personal names 
with varying-sized datasets, and has 1669 texts in 
all. 
The rest of this paper is organized as follows: 
in Section 2 we review the related work; Section 
3 describes the framework; section 4 introduces 
our methodologies including feature weight 
computing with pointwise mutual information, 
cluster-stopping measure based on trade-off 
point, and cluster labeling algorithm. These are 
the main contribution of this paper; Section 5 
discusses our experimental result. Finally, the 
conclusion and suggestions for further extension 
of the work are given in Section 6. 
2 Related Work 
Due to the varying ambiguity of personal names 
in a corpus, existing approaches typically cast it 
as an unsupervised clustering problem based on 
vector space model. The main difference among 
these approaches lies in the features, which are 
used to create a similarity space. Bagga & Bald-
win (1998) first performed within-document co-
reference resolution, and then explored features 
in local context. Mann & Yarowsky (2003) ex-
tracted local biographical information as features. 
Al-Kamha and Embley (2004) clustered search 
results with feature set including attributes, links 
and page similarities. Chen and Martin (2007) 
explored the use of a range of syntactic and se-
mantic features in unsupervised clustering of 
documents. Song (2007) learned the PLSA and 
LDA model as feature sets. Ono et al (2008) 
used mixture features including co-occurrences 
88
of named entities, key compound words, and top-
ic information. Previous works usually focus on 
feature identification and feature selection. The 
method to assign appropriate weight to each fea-
ture has not been discussed widely.  
A major challenge in clustering analysis is de-
termining the number of ?clusters?. Therefore, 
clustering based approaches to this problem still 
require estimating the number of clusters. In Hie-
rarchy clustering, it equates to determine the 
stopping step of clustering. The measure to find 
the ?knee? in the criterion function curve is a 
well known cluster-stopping measure. Pedersen 
and Kulkarni had studied this problem (Pedersen 
and Kulkarni, 2006). They developed cluster-
stopping measures named PK1, PK2, PK3, and 
presented the Adapted Gap Statistics.  
After estimating the number of ?clusters?, we 
obtain the clustering result. In order to label the 
?clusters?, the method that finding representative 
features for each ?cluster? is needed. For example, 
the captain John Smith can be labeled as captain. 
Pedersen and Kulkarni (2006) selected the top N 
non-stopping word features from texts grouped 
in a cluster as label. 
3 Framework 
On the assumption of ?one person per document? 
(i.e. all mentions of an ambiguous personal name 
in one document refer to the same personal enti-
ty), the task of disambiguating personal name in 
text set intends to partition the set into subsets, 
where each subset refer to one particular entity. 
Suppose the set of texts containing the ambi-
guous name is denoted by D= {d1,d2,?,dn}, and  
di (0<i<n+1) stands for one text. The entities 
with the ambiguous name are denoted by a set 
E= {e1,e2,?,em}, where the number of entities ?m? 
is unknown. The ambiguous name in each text di 
indicates only one entity ek. The aim of the work 
is to map an ambiguous name appearing in each 
text to an entity. Therefore, those texts indicating 
the same entity need to be clustered together. 
In determining whether a personal name refers 
to a specific entity, the personal information, so-
cial network information and related topics play 
important roles,  all of which are expressed by 
words in texts,. Extracting words as features, this 
paper applies an agglomerative clustering ap-
proach to resolving name co-reference. The 
framework of our approach consists of the fol-
lowing seven main steps: 
 
Step 1: Pre-process each text with Chinese 
word segmentation tool; 
Step 2: Extract words as features from the 
set of texts D;. 
Step 3: Represent texts d1,?,dn by features 
vectors; 
Step 4: Calculate similarity between texts; 
Step 5: Cluster the set D step by step until 
only one cluster exists;  
Step 6: Estimate the number of entities in 
accordance with cluster-stopping 
measure; 
Step 7: Assign each cluster a discriminating 
label. 
 
This paper focuses on the Step 4, Step 6 and 
Step 7, i.e., feature weight computing method, 
clustering stopping measure and cluster labeling 
method. They will be described in the next sec-
tion in detail.  
Step1 and Step3 are simple, and there is no 
further description here. In Step 2, we use co-
occurrence words of the ambiguous name in 
texts as features. In the process of agglomerative 
clustering (see Step 5), each text is viewed as one 
cluster at first, and the most similar two clusters 
are merged together as a new cluster at each 
round. After replacing the former two clusters 
with the new one, we use average linked method 
to update similarity between clusters. 
4 Methodology  
4.1 Feature weight  
Each text is represented as a feature vector, and 
each item of the vector represents the weight 
value for corresponding feature in the text. Since 
our approach is completely unsupervised we 
cannot use supervised methods to select 
significant features. Since the weight of feature 
will be adjusted well instead of feature selection, 
all words in set D are used as feature in our 
approach. 
The problem of computing feature weight is 
involved in both text clustering and text classifi-
cation. By comparing the supervised text classi-
fication and unsupervised text clustering, we find 
that the former one has a better performance ow-
ing to the selection of features and the computing 
method of feature weight. Firstly, in the applica-
tion of supervised text classification, features can 
be selected by many methods, such as, Mutual 
Information (MI) and Expected Cross Entropy 
(ECE) feature selection methods. Secondly, 
model training methods, such as SVM model, are 
generally adopted by programs when to find the 
89
optimal feature weight. There is no training data 
for unsupervised tasks, so above-mentioned me-
thods are unsuitable for text clustering. 
In addition, we find that the text clustering for 
personal name disambiguation is different from 
common text clustering. System can easily judge 
whether a text contains the ambiguous personal 
name or not. Thus the whole collection of texts 
can be easily divided into two classes: texts  with 
or without the name. As a result, we can easily 
calculate the pointwise mutual information 
between feature words and the personal name. 
To a certain extent, it represents the correlative 
degree between feature words and the underlying 
entity corresponding to the personal name. 
For these reasons, our feature weight 
computing method calculates the pointwise 
mutual information between personal name and 
feature word. And the value of pointwise mutual 
information will be used to expresse feature 
word?s weight by combining the feature?s tf (the 
abbreviation for term-frequency) in text and idf 
(the abbreviation for inverse document frequency) 
in dataset. The formula of feature weight compu-
ting proposed in this paper is as below, and it is 
need both texts containing and not containing the 
ambiguous personal name to form dataset D. For 
each tk in di that contains name, its mi_weight is 
computed as follow: 
))(||log()),MI(1log(
))),(log(1(),,_weight(mi
kk
ikik
tdfDnamet
dttfdnamet
?+?
+=
 
(1) 
And 
)()(
||),(
||/)()(
||/),(
)()(
),(
),MI(
2
k
k
k
k
k
k
k
tdfnamedf
Dtnamedf
Dtdfnamedf
Dtnamedf
tpnamep
tnamep
namet
?
?=
?=
?=
     (2) 
 Where tk is a feature; name is the ambiguous 
name; di is the ith text in dataset; tf(tk,di) 
represents term frequency of feature tk in text di; 
df(tk), df(name) is the number of the texts con-
taining tk or name in dataset D respectively; 
df(tk,name) is the number of texts containing both 
tk and name; |D| is the number of all the texts.  
Formula (2) can be comprehended as: if word 
tk occurs much more times in texts containing the 
ambiguous name than in texts not containing the 
name, it must have some information about the 
name. 
 A widely used approach for computing feature 
weight is tf*idf scheme as formula (3) (Salton 
and Buckley. 1998), which only uses the texts 
containing the ambiguous name. We denote it by 
old_weight . For each tk in di containing name, 
the old_weight is computed as follow: 
)),()(log(
))),(log(1(
),,(old_weight
nametdfnamedf
dttf
dnamet
k
ik
ik
?
+=               (3) 
The first term on the right side is tf, and the 
second term is idf. If the idf scheme is computed 
in the whole dataset D for reducing noise, the 
weight computing formula can be expressed as 
follow, and is denoted by imp_weight: 
))(|D|log())),(log(1(
),_weight(imp
kik
ik
tdfdttf
dt
?+= (4) 
Before clustering, the similarity between texts 
is computed by cosine value of the angle 
between vectors (such as dx, dy in formula (5)):     
yx
yx
yx dd
dd
d,d ?
?=)cos(                               (5) 
Each item of the vector (i.e. dx, dy) represents 
the weight value for corresponding feature in the 
text. 
4.2 Cluster-stopping measure 
The process of clustering will produce n cluster 
results, one for each step. Independent of 
clustering algorithm, the cluster stopping meas-
ure should choose the cluster results which can 
represent the structure of data. 
A fundamental and difficult problem in cluster 
analysis is to measure the structure of clustering 
result. The geometric structure is a representative 
method. It defines that a ?good? clustering re-
sults should make data points from one cluster 
?compact?, while data points from different clus-
ter are ?separate? as far as possible. The indica-
tors should quantify the ?compactness? and ?se-
paration? for clusters, and combine both.  In the 
study of cluster stopping measures by Pedersen 
and Kulkarni (2006), the criterion functions de-
fines text similarity based on cosine value of the 
angle between vectors. Their cluster-stopping 
measures focused on finding the ?knee? of crite-
rion function.  
Our cluster-stopping measure is also based on 
the geometric structure of dataset. The measure 
aims to find the trade-off point between within-
cluster compactness and among-cluster 
separation. Both the within-cluster compactness 
(Internal critical function) and among-cluster 
90
separation (External critical function) are defined 
by Euclidean distance. The hybrid critical 
function (Hybrid critical function) combines 
internal and external criterion functions. 
Suppose that the given dataset contains N ref-
erences, which are denoted as: d1,d2,?,dN; the 
data have been repeatedly clustered into k clus-
ters, where k=N,?,1; and clusters are denoted as 
Cr, r=1,?k; and the number of references in 
each cluster is nr, so nr=|Cr|. We introduce Incrf 
(Internal critical function), Excrf (External 
critical function) and Hycrf (Hybrid critical 
function) to measure it as follows. 
 
??
= ?
?=
k
i
k
1
)Incrf(
iyx Cd,d
2
yx dd                  (6) 
? ? ?
= ?= ??
?=
k
i
k
ijj jinn
k
1 ,1
1
)Excrf(
jyix Cd,Cd
2
yx dd
            (7) 
))Excrf()(Incrf(
M
1
)Hycrf( kkk +?=          (8) 
Where M=Incrf(1)=Excrf(N) 
 
 
 
Figure 1 Hycrf vs. t (N-k) 
 
Chen proved the existence of the minimum 
value between (0,1) in Hycrf(k) (see Chen et al 
2008). The Hycrf value in a typical Hycrf(t) 
curve is shown as Figure 1, where t=N-k. 
Function Hycrf based on Incrf and Excrf is 
used as the Hybrid criterion function. The Hycrf 
curve will rise sharply after the minimum, indi-
cating that the cluster of several optimal parti-
tions? subsets will lead to drastic drop in cluster 
quality. Thus cluster partition can be determined. 
Using the attributes of the Hycrf(k) curve, we put 
forward a new cluster-stopping measure named 
trade-off point based cluster-stopping measure 
(TO_CSM). 
)1Hycrf(
)Hycrf(
)1Hycrf(
1
)TO_CSM( +?+= k
k
k
k  
(9) 
 
Trade-off point based cluster-stopping meas-
ure (TO_CSM) selects the k value which max-
imizes TO_CSM(k), and indicates the number of 
cluster. The first term on the right side of formu-
la (9) is used to minimize the value of Hycrf(k), 
and the second one is used to find the ?knee? ris-
ing sharply. 
4.3 Labeling 
Once the clusters are created, we label each 
entity to represent the underlying entity with 
some important information. A label is 
represented as a list of feature words, which 
summarize the information about cluster?s 
underlying entity. 
The algorithm is outlined as follows: after 
clustering N references into m clusters, for each 
cluster Ck in {C1, C2, ?, Cm}, we calculate the 
score of each feature for Ck and choose features 
as the label of Ck whose scores rank top N. In 
particular, the score caculated in this paper is 
different from Pedersen and Kulkarni?s (2006). 
We combine pointwise mutual information 
computing method with term frequency in cluster 
to compute the score.  
The formula of feature scoring for labeling is 
shown as follows: 
 
))),(log(1(
),(MI),MI(),Score( name
ik
ikkik
Cttf
CtnametCt
+?
?=
 
(10) 
The calculation of MI(tk,name) is shown as 
formula (2) in subsection 4.1. tf(tk,Ci) represents 
the total occurrence frequency of feature tk in 
cluster Ci . The MIname(tk,Ci) is computed as for-
mula (11): 
)()(
||),(
||/)()(
||/),(
)()(
),(
)C,(MI
2
ik
ik
ik
ik
ik
ik
ikname
Cdftdf
DCtdf
DCdftdf
DCtdf
Cptp
Ctp
t
?
?=
?=
?=
 
 (11) 
In formula (10), the weight of stopping words 
can be reduced by the first item. The second item 
can increase the weight of words with high dis-
tinguishing ability for a certain ambiguous name. 
The third item of formula (10) gives higher 
scores to features whose frequency are higher.  
0
0.5
1
1.5
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 10
6
11
3
12
0
Hycrf(t)
91
5 Experiment  
5.1 Data 
The dataset is from WWW, and contains 1,669 
texts with eleven real ambiguous personal names. 
Such raw texts containing ambiguous names are 
collected via search engine1, and most of them 
are news. The eleven person-names are, "??? 
Liu-Yi-si ?Lewis?", "??? Liu-Shu-zhen ", "?
? Li-Qiang", "?? Li-Na", "??? Li-Gui-
ying", "??? Mi-xie-er ?Michelle?", "?? 
Ma-Li ?Mary?", "???  Yue-han-xun ?John-
son?", "?? Wang-Tao", "?? Wang-Gang", "
??? Chen-Zhi-qiang". Names like ?Michelle?, 
?Johnson? are transliterated from English to Chi-
nese, while names like ?Liu ?Shu-zhen?, ?Chen-
Zhi-qiang? are original Chinese personal names. 
Some of these names only have a few persons, 
while others have more persons.  
Table 1 shows our data set. ?#text? presents 
the number of texts with the personal name. 
?#per? presents the number of entities with the 
personal name in text dataset. ?#max? presents 
the maximum of texts for an entity with the per-
sonal name, and ?#min? presents the minimum. 
 
 #text #per #max #min
Lewis 120 6 25 10 
Liu-Shu-zhen 149 15 28 3 
Li-Qiang 122 7 25 9 
Li-Na 149 5 39 21 
Li-Gui-ying 150 7 30 10 
Michelle 144 7 25 12 
Mary 127 7 35 10 
Johnson 279 19 26 1 
Wang-Gang 125 18 26 1 
Wang-Tao 182 10 38 5 
Chen-Zhi-qiang 122 4 52 13 
 
Table 1 Statistics of the test dataset 
 
We first convert all the downloaded docu-
ments into plain text format to facilitate the test 
process, and pre-process them by using the seg-
mentation toolkit ICTCLAS2. 
In testing and evaluating, we adopt B-Cubed 
definition for Precision, Recall and F-Measure 
as indicators (Bagga, Amit and Baldwin. 1998). 
F-Measure is the harmonic mean of Precision 
and Recall. 
The definitions are presented as below: 
                                                 
1 April.2008 
2 http://ictclas.org/ 
? ?= Dd dprecisionNprecision 1              (12) 
? ?= Dd drecallNrecall 1                           (13) 
recallprecision
recallprecision
measureF +
??=? 2      (14) 
where precisiond is the precision for a text d. 
Suppose the text d is in subset A, precisiond is 
the percentage of texts in A which indicates the 
same entity as d. Recalld is the recall ratio for a 
text d. Recalld is the ratio of number of texts 
which indicates the same entity as d in A to that 
in corpus D. n = | D |, D refers to a collection of 
texts containing a particular name (such as Wang 
Tao, e.g. a set of 200 texts, n = 200). Subset A is 
a set formed after clustering (text included in 
class), and d refers to a certain text that contain-
ing "Wang Tao". 
5.2 Result 
All the 1669 texts in the dataset are employed 
during experiment. Each personal name disam-
biguation process only clusters the texts contain-
ing the ambiguous name. After pre-processing, in 
order to verify the mi_weight method for feature 
weight computing, all the words in texts are used 
as features.   
Using formula (1), (3) and (4) as feature 
weight computing formula, we can get the evalu-
ation of cluster result shown as table 2. In this 
step, cluster-stopping measure is not used. In-
stead, the highest F-measure during clustering is 
highlighted to represent the efficiency of the fea-
ture weight computing method.  
Further more, we carry out the experiment on 
the trade-off point based cluster-stopping 
measure, and compare its cluster result with 
highest F-measure and cluster result determined 
by cluster-stopping measure PK3 proposed by 
Pedersen and Kulkarni?s. Based on the 
experiment in Table 2, a structure tree is 
constructed in the clustering process. Cluster-
stopping measures are used to determine where 
to stop cutting the dendrogram. As shown in 
Table 3, the TO-CMS method predicts the 
optimal results of four names in eleven, while 
PK3 method predicts the optimal result of one 
name, which are marked in a bold type. 
 
92
 
 old_weight imp_weight mi_weight 
#pre #rec #F #pre #rec #F #pre #rec #F 
Lewis 0.9488 0.8668. 0.9059 1 1 1 1 1 1 
Liu-Shu-zhen 0.8004 0.7381 0.7680 0.8409 0.8004 0.8201 0.9217 0.7940 0.8531
Li-Qiang 0.8057 0.6886 0.7426 0.9412 0.7968 0.8630 0.8962 0.8208 0.8569
Li-Na 0.9487 0.7719 0.8512 0.9870 0.8865 0.9340 0.9870 0.9870 0.9870
Li-Gui-ying 0.8871 0.9124 0.8996 0.9879 0.8938 0.9385 0.9778 0.8813 0.9271
Michelle 0.9769 0.7205 0.8293 0.9549 0.8146 0.8792 0.9672 0.9498 0.9584
Mary 0.9520 0.6828 0.7953 1 0.9290 0.9632 1 0.9001 0.9474
Johnson 0.9620 0.8120 0.8807 0.9573 0.8083 0.8765 0.9593 0.8595 0.9067
Wang-Gang 0.8130 0.8171 0.8150 0.7804 0.9326 0.8498 0.8143 0.9185 0.8633
Wang-Tao 1 0.9323 0.9650 0.9573 0.9485 0.9529 0.9897 0.9768 0.9832
Chen-Zhi-qiang 0.9732 0.8401 0.9017 0.9891 0.9403 0.9641 0.9891 0.9564 0.9725
Average 0.9153 0.7916 0.8504 0.9451 0.8864 0.9128 0.9548 0.9131 0.9323
 
Table 2 comparison of feature weight computing method (highest F-measure)
 
 Optimal TO-CMS PK3 
#pre #rec #F #pre #rec #F #pre #rec #F 
Lewis 1 1 1 1 1 1 0.8575 1 0.9233
Liu-Shuzhen 0.9217 0.7940 0.8531 0.8466 0.8433 0.8450 0.5451 0.9503 0.6928
Li-Qiang 0.8962 0.8208 0.8569 0.8962 0.8208 0.8569 0.7897 0.9335 0.8556
Li-Na 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9016 0.9424
Li-Gui-ying 0.9778 0.8813 0.9271 0.9778 0.8813 0.9271 0.8750 0.9427 0.9076
Michelle 0.9672 0.9498 0.9584 0.9482 0.9498 0.9490 0.9672 0.9498 0.9584
Mary 1 0.9001 0.9474 0.8545 0.9410 0.8957 0.8698 0.9410 0.9040
Johnson 0.9593 0.8595 0.9067 0.9524 0.8648 0.9066 0.2423 0.9802 0.3885
Wang-Gang 0.8143 0.9185 0.8633 0.9255 0.7102 0.8036 0.5198 0.9550 0.6732
Wang-Tao 0.9897 0.9768 0.9832 0.8594 0.9767 0.9144 0.9700 0.9768 0.9734
Chen-Zhi-qiang 0.9891 0.9564 0.9725 0.8498 1 0.9188 0.8499 1 0.9188
Average 0.9548 0.9131 0.9323 0.9179 0.9068 0.9095 0.7703 0.9574 0.8307
 
Table 3 comparison of cluster-stopping measures? performance
name Entity Created Labels 
Lewis Person-1 ???(Babbitt),???????(Sinclair Lewis),?????(Arrow smith),?
??(Literature Prize),???(Dresser),????(Howells),?????
(Swedish Academy),???????(Sherwood Anderson),???????
(Elmer  Gan Hartley),??(street),??(award),????????(American 
Literature and Arts Association) 
Person-2 ????(Bank of America),????(Bank of America),??(bank),???
(investors),???(credit card),??(Bank of China),??(Citibank),??
(mergers and acquisitions),??(Construction Bank),???(executive officer),
???(banking),??(stock),?????(Ken Lewis) 
Person-3 ??(Single),???(Liana),??(album),???(Liana),???????(Liana 
Lewis),???(Liana),??(airborne),??(sales),???(Music Awards),??
????(Maria Kelly),?(List),??(debut)? 
Person-4 ??????(Carl Lewis),??(long jump),??(Carl),???(Owens),??
(track and field),???(Burrell),?????(the U.S. Olympic Committee),?
?(sprint),???(Taylors),?????(Belgrade),??????(Verde Exxon),
???(Exxon) 
93
Person-5 ??(Tyson),??(King of Boxer),??(knock down),???(heavyweight),?
?(Don King),??(boxing),??(belt),??(Boxing),?(fist),??(bout),??
(Ring),WBC 
Person-6 ???(Daniel),?????(Day Lewis),??(Blood),?????????(Daniel 
Day Lewis),??(There Will Be Blood),??(left crus),??(movie king),??
?????(New York Film Critics Circles),???(the Gold Oscar statues),?
??(Best Actor in a Leading Role),???(Oscar),????(There Will Be 
Blood) 
 
Table 4  Labels for ?Lewis? clusters 
 
On the basis of text clustering result that 
obtained from the Trade-off based cluster-
stopping measure experiment in Table 3, we try 
our labelling method mentioned in subsection 4.3. 
For each cluster, we choose 12 words with 
highest score as its label. The experiment result 
demonstrates that the created label is able to 
represent the category. Take name ???? Liu-
Yi-si ?Lewis?? for example, the labeling result 
shown as Table 4.  
 
5.3 Discussion  
From the test result in table 2, we find that our 
feature weight computing method can improve 
the Chinese personal name clustering disambigu-
ation performance effectively. For each personal 
name in test dataset, the performance is im-
proved obviously. The average value of optimal 
F-measures for eleven names rises from 85.04% 
to 91.28% by using the whole dataset D for cal-
culated idf, and rises from 91.28% to 93.23% by 
using mi_weight. Therefore, in the application of 
Chinese text clustering with constraints, we can 
compute pointwise mutual information between 
constraints and feature, and it can be merged 
with feature weight value to improve the cluster-
ing performance.  
We can see from table 3 that trade-off point 
based cluster-stopping measure (TO_CSM) per-
forms much better than PK3. According to the 
experimental results, PK3 measure is not that 
robust. The optimal number of clusters can be 
determined for certain data. However, we found 
that it did not apply to all cases. For example, it 
obtains the optimal estimation result for data 
?Michelle?, as for ?Liu Shuzhen?, ?Wang Gang? 
and ?Johnson?, the results are extremely bad. 
The better result is achieved by using TO_CSM 
measure, and the selected results are closer to the 
optimal value. The PK3 measure uses the mean 
and the standard deviation to deduce, and its 
processes are more complicated than TO_CSM?s.  
Our cluster labeling method computes the fea-
tures? score with formula (10). From the labeling 
results sample shown in Table 4, we can see that 
all of the labels are representative. Most of them 
are person and organizations? name, and the rest 
are key compound words. Therefore, when the 
clustering performance is good, the quality of 
cluster labels created by our method is also good. 
6 Future Work 
This paper developed a clustering algorithm of 
multi-document personal name disambiguation, 
and put forward a novel feature weight compu-
ting method for vector space model. This method 
computes weight with the pointwise mutual in-
formation between the personal name and feature. 
We also study a hybrid criterion function based 
on trade-off point and put forward the trade-off 
point cluster-stopping measure. At last, we expe-
riment on our score computing method for clus-
ter labeling.  
Unsupervised personal name disambiguation 
techniques can be extended to address the prob-
lem of unsupervised Entity Resolution and unsu-
pervised word sense discrimination. We will at-
tempt to apply the feature weight computing me-
thod to these fields. 
One of the main directions of our future work 
will be how to improve the performance of per-
sonal name disambiguation. Computing weight 
based on a window around names may be helpful. 
Moreover, word-based text features haven?t 
solved two difficult problems of natural language 
problems: Synonym and Polysemy, which se-
riously affect the precision and efficiency of 
clustering algorithms. Text representation based 
on concept and topic may solve the problem.  
 
Acknowledgments 
This research is supported by National Natural 
Science Foundation of Chinese (No.60675035) 
and Beijing Natural Science Foundation 
(No.4072012) 
94
References  
Al-Kamha. R. and D. W. Embley. 2004. Grouping 
search-engine returned citations for person-name 
queries. In Proceedings of WIDM?04, 96-103, 
Washington, DC, USA. 
Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space 
model. In Proceedings of 17th International Con-
ference on Computational Linguistics, 79?85. 
Bagga, Amit and B. Baldwin. 1998. Algorithms for 
scoring co-reference chains. In Proceedings of the 
First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic 
co-reference. 
Chen Ying and James Martin. 2007. Towards Robust 
Unsupervised Personal Name Disambiguation, 
EMNLP 2007. 
Chen Lifei, Jiang Qingshan, and Wang Shengrui. 
2008. A Hierarchical Method for Determining the 
Number of Clusters. Journal of Software, 19(1). [in 
Chinese] 
Chung Heong Gooi and James Allan. 2004. Cross-
document co-reference on a large scale corpus. In S. 
Dumais, D. Marcu, and S. Roukos, editors, HLT-
NAACL 2004: Main Proceedings, 9?16, Boston, 
Massachusetts, USA, May 2 - May 7 2004. Asso-
ciation for Computational Linguistics. 
Gao Huixian. Applied Multivariate Statistical Analy-
sis. Peking Univ. Press. 2004. 
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 
Kulkarni Anagha and Ted Pedersen. 2006. How Many 
Different ?John Smiths?, and Who are They? In 
Proceedings of the Student Abstract and Poster 
Session of the 21st National Conference on Artifi-
cial Intelligence, Boston, Massachusetts. 
Mann G. and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In W. Daelemans and 
M. Osborne, editors, Proceedings of CoNLL-2003, 
33?40, Edmonton, Canada. 
Niu Cheng, Wei Li, and Rohini K. Srihari. 2004.  
Weakly Supervised Learning for Cross-document 
Person Name Disambiguation Supported by Infor-
mation Extraction. In Proceedings of ACL 2004. 
Ono. Shingo, Issei Sato, Minoru Yoshida, and Hiroshi 
Nakagawa2. 2008. Person Name Disambiguation 
in Web Pages Using Social Network, Compound 
Words and Latent Topics. T. Washio et al (Eds.): 
PAKDD 2008, LNAI 5012, 260?271. 
Song Yang, Jian Huang, Isaac G. Councill, Jia Li, and 
C. Lee Giles. 2007. Efficient Topic-based Unsu-
pervised Name Disambiguation. JCDL?07, June 
18?23, 2007, Vancouver, British Columbia, Cana-
da. 
Ted Pedersen and Kulkarni Anagha. 2006. Automatic 
Cluster Stopping with Criterion Functions and the 
Gap Statistic. In Proceedings of the Demonstration 
Session of the Human Language Technology Con-
ference and the Sixth Annual Meeting of the North 
American Chapter of the Association for Computa-
tional Linguistic, New York City, NY. 
 
95
News-Oriented Automatic Chinese Keyword Indexing 
Li Sujian1 
lisujian@pku.
edu.cn 
Wang Houfeng1 
wanghf@pku.edu.
cn 
Yu Shiwen1 
Yusw@pku.edu.
cn 
Xin Chengsheng2 
csxin@peoplemail.
com.cn 
1Institute of Computational Linguistics, Peking University, 100871 
2The Information Center of PEOPLE?S DAILY, 100733 
 
 
Abstract 
In our information era, keywords are very 
useful to information retrieval, text clus-
tering and so on.  News is always a do-
main attracting a large amount of 
attention.  However, the majority of news 
articles come without keywords, and in-
dexing them manually costs highly.  Aim-
ing at news articles? characteristics and 
the resources available, this paper intro-
duces a simple procedure to index key-
words based on the scoring system.  In the 
process of indexing, we make use of some 
relatively mature linguistic techniques and 
tools to filter those meaningless candidate 
items.  Furthermore, according to the hi-
erarchical relations of content words, 
keywords are not restricted to extracting 
from text. These methods have improved 
our system a lot.  At last experimental re-
sults are given and analyzed, showing that 
the quality of extracted keywords are sat-
isfying. 
1 Introduction 
With more and more information flowing into our 
life, it is very important to lead people to gain 
more important information in time as short as 
possible.  Keywords are a good solution, which 
give a brief summary of a document?s content.  
With keywords, people can quickly find what they 
are most interested in and read them carefully.  
That will save us a lot of time.  In addition, key-
words are also useful to the research of information 
retrieval, text clustering, and topic search [Frank 
1999].  Manually indexing keywords will cost 
highly.  Thus, automatically indexing keywords 
from text is of great interests. 
News is always the main domain that people 
pay a large amount of attention to.  Unfortunately, 
only a small fraction of documents in this field 
have keywords.  However, compared to unre-
stricted text, news articles are relatively easy to 
extract keywords from, because they have the fol-
lowing characteristics.  Firstly, a news document is 
always short in length, and usually, only important 
words or phrases repeat.  Secondly, as a rule, the 
purpose of news articles is to illustrate an event or 
a thing for readers.  Then this kind of articles usu-
ally place more emphasis on some name entities 
such as persons, places, organizations and so on.  
Lastly, important content often occurs the first time 
in the title, or in the anterior part of the whole text, 
especially the first paragraph or the first sentence 
in every paragraph.  These characteristics will help 
us in keywords indexing. 
Several methods have been proposed for ex-
tracting English keywords from text.  For example, 
Witten[1999] adopted Na?ve Bayes techniques, and 
Turney[1999] combined decision trees and genetic 
algorithm in his system.  These systems achieved 
satisfying results. However, they need a large 
amount of training documents with keywords, 
which are just what we are in need of now.  For the 
Chinese language, some researchers adopt the 
structure of PAT tree and make use of mutual in-
formation to obtain keywords [Chien 1997, Yang 
2002].  Unfortunately, the construction of PAT tree 
will cost a lot of space and time.  In this paper, 
aiming at the characteristics of news-oriented arti-
cles, resources and techniques of current situation, 
we will introduce a simple procedure to index 
keywords from text. Section 2 will describe the 
architecture of the whole system.  In section 3, we 
will introduce every module in detail, including 
how to obtain candidate keywords, how to filter 
out the meaningless items, and how to score possi-
ble keyword candidates according to their feature 
values.  In section 4, experimental results will be 
given and analyzed.  At last, we will end with the 
conclusion. 
2 
3 
3.1 
System Overview 
Keyword indexing can also be called keyword ex-
traction.  The definition of a keyword is not re-
stricted to one word in our conception.  Here, a 
keyword can be seen as a Chinese character string, 
which might consist of more than one Chinese 
word.  These character strings can summarize the 
content of the document they are in. 
Aiming at the task of keywords indexing, our 
system is designed and composed of three modules.  
As in figure 1, the first module is to recognize 
some Chinese character strings according to their 
frequency, and pick out those named entities in the 
text as the candidate keywords.  The second mod-
ule is a filter to remove all the meaningless charac-
ter strings from the set of candidates. And the third 
module is a selector, which evaluates every candi-
date according to its feature values and choose 
from the candidate set those keywords with higher 
score.  The higher score a character string has, the 
more content it will cover of the article it is in. 
 In our system, there are three kinds of lexicons. 
The lexicon of proper nouns is used to recognize 
named entities.  The general lexicon includes Chi-
nese words in common use, which is adopted for 
the segmentation and POS tagging of the text.  And 
the lexicon of content words is used to expand the 
set of keywords.  They will be introduced in detail 
in the following section. 
System Design 
Recognizer Module 
Character strings Recognizer
Selector
Filter
Segmentation
filter
Keywords
expansion
Feature
computation
Chinese character
strings
POS tagging
filter
Filter of items with
punctuations and
function words
Filter of overlapped
and dependent items
Recognizer through
Frequency Statistics
Named Entities
Recognizer
Original text
Candidate items with
feature values
Fig.1. System Architecture
keywords
Proper Nouns
Lexicon
Content Words
Lexicon
General
Lexicon
It can be seen that one document is composed of a 
set of character strings.  Every character string has 
its frequency in the document.  In general, those 
character strings that occur several times can re-
flect the topic of the document.  So, we take them 
out as keyword candidates. In addition, named en-
tities, such as person names, place names, organi-
zations, translation terms, titles of person and so on, 
are usually very important for the document with-
out reference to their frequency.  They will also be 
picked out from the text by named entities recog-
nizer and input into the filter module with other 
character strings.  
Unlike English, there are no explicit word 
boundaries in Chinese sentences, which makes it 
especially difficult to tell whether a character 
string is composed of one word or more than one 
word.  Due to this characteristic, we don?t use a 
dictionary, but get those character strings only ac-
cording to their frequency statistics.  We set a 
threshold value as 2 for the Chinese character 
strings considering the length of news documents.  
Suppose that a character string is c1c2?cn, and 
f(c1c2?cn) represents its frequency, then we ex-
tract c1c2?cn from text only if f(c1c2?cn) equals to 
or is more than 2.  That is, only a character string 
occurs two or more than two times, it can be se-
lected as a candidate keyword. 
There are two kinds of named entities.  The first 
are those which have rules of composition, mainly 
Chinese names and foreign terms.  They can be 
recognized with statistical and rule-based methods 
combined.  Chinese names are composed of family 
names and first names, whose lengths are respec-
tively 1 or 2 Chinese characters.  Furthermore, 
there is a relatively stable set of family names, 
which often provide the anchor to search a name. 
For foreign terms, there are a relatively set of Chi-
nese characters which are generally used as 
translation characters.  Due to the limitation of the 
paper?s length, we don?t introduce the process of 
recognition in detail here.  The other kind of 
named entities is mainly composed of proper 
nouns which represent names of places, organiza-
tions, person titles, etc.  They often occur in news 
documents, but don?t have rules of composition.  
Thus, we collect such words into our proper nouns 
lexicon.  Then the module can find these named 
entities through looking up in this lexicon. 
3.2 Filter Module 
So far, Chinese character strings are generated only 
through frequency statistics. Thus, some of them 
stand out just because of simple repetition and are 
probably not meaningful units of language.  We 
need to filter out those meaningless items.  As in 
figure 1, we adopt four kinds of filters in filter 
module. They work as follows. 
(1) Filter of Overlapped and Dependent Items  
 
evident that such character strings can?t serve as 
For two character strings S1 and S2, with S1 as a 
substring of S2, and the frequency of S1 is equal to 
that of S2, then S1 is overlapped by S2. In fact, we 
can set a threshold td for f(S1)-f(S2), where the 
function f(.) represents the frequency of some 
character string. If the value of f(S1)-f(S2) is less 
than td, then the string S1 is dependent on S2.  
Here, the overlapped and dependent substring will 
be removed from the candidate set. 
(2) Filter of Items with Punctuations and Func-
tion words 
The recognizer module treats equally all symbols 
in the text, such as Chinese characters and 
punctuations, etc. Thus when conducting the 
process of frequency statistics, for a character 
string, there might exist some punctuations and 
function words such as ???, ???, ???, ???, etc. 
These punctuations and function words usually 
occur in the head or tail of a character string.  It?s 
character strings can?t serve as keywords of an ar-
ticle, and they should be deleted from the candi-
date set. 
(3) Segmentation Filter 
We find the first occurrence position of every can-
didate keyword and get the sentence at the position.  
Then the sentence is segmented.  According to the 
segmented result, we can verify whether the char-
acter string is meaningful.  First of all, we get the 
segmentation result of the character string in the 
segmented sentence.  Suppose the character string 
ci?cj in the original text with the sentence 
c1c2?ci-1ci?cjcj+1?cn as its context, if the 
segmentation tool segments ci-1ci or cjcj+1 into one 
word, then ci?cj will not be regarded as an inte-
grated unit.  That is, this item will be seen as 
meaningless and filtered out from the set of candi-
date keywords.  Here we don?t adopt the method of 
conducting frequency statistics of words after seg-
mentation, but use segmentation tool after fre-
quency statistics of character strings.  There are 
some reasons. Above all, although the segmenta-
tion technique is relatively mature, its precision is 
still not high enough.  Then, for the same character 
string, its segmentation results often differ in dif-
ferent sentences.  Thus, it?s difficult to compute the 
frequency of a character string precisely.  Further-
more, now we only need to segment one sentence 
for a candidate keyword.  That will save us a great 
deal of time. 
(4) POS Filter 
Because keywords provide a brief summary for 
one document, they should be words or phrases 
that represent some meaning units such as nouns 
and noun phrases.  Therefore, a single word whose 
part of speech is preposition, adverb, adjective, or 
conjunctive is filtered out.  At the same time, verb 
phrases, adjective phrases, preposition phrases are 
also excluded from the candidate set.  The same as 
segmentation filter, we only do the POS tagging 
for the sentence where every candidate keyword 
occurs the first time.  If a candidate item is made of 
more than one word, it will have a sequence of 
POS tags according to which we can assign a 
phrase category.  The POS tags or phrase catego-
ries are the basis for POS filtering. 
Only conducting frequency statistics of charac-
ter strings can?t refine the candidate set well, and 
we utilize the relatively mature linguistic segmen-
tation and POS tagging techniques so that we can 
further improve the quality of the candidate key-
words.  Here, the general lexicon with about 
60,000 Chinese words is applied to the processes 
of segmentation and POS tagging. 
3.3 Selector Module 
After several filtering, now we can get a reduced 
set of candidate keywords.  Most character strings 
in the set are meaningful and reflect the content of 
the document to some extent.  For every candidate 
now, we adopt several features to describe it. The 
features include frequency, length, position of the 
first occurrence, part of speech and whether it is a 
proper noun or in a pair of specific punctuations, as 
in table 1.  At the same time, through the process-
ing of several linguistic tools in filter module, we 
can assign a value to every feature in every candi-
date item.   
feature meaning of feature 
freq Frequency of an item 
len Length of an item 
is_noun Whether an item is a noun phrase 
in_title Whether the first occurrence of an item is in the title of one document 
in_seg1 
Whether the first occurrence of an 
item is in the first paragraph of one 
document 
is_proper 
Whether an item is a proper noun, 
for example: person name, organi-
zation, translation term, place 
name, title of a person etc. 
in_sign 
Whether an item is bracketed by a 
pair of specific punctuations such 
as ???? and ????. 
Table 1. Features of candidate keywords 
We can find that the candidate set is still too 
large to select from it the keywords.  Then we will 
conduct feature calculation to refine the candidate 
set.  We have known that every candidate item has 
a feature-value set.  These feature values are our 
basis to evaluate every candidate item.  We com-
pute a score for every candidate keyword through 
the module of feature computation.  The higher the 
score, the more relevant the candidate is to the 
document. 
We compute the percentage how much manually 
indexed keywords of different lengths cover in the 
set of automatically generated candidates.  As in 
figure 2, Length represents the length of keywords 
and percentage denotes the corresponding percent-
age that keywords of this length are in the set.  The 
higher the percentage, the more likely the key-
words of this length are to be selected.  Therefore, 
we can make a conclusion that the score of a can-
didate is directly proportional to the percentage of 
its length.  Then we can acquire the relation be-
tween score and length of a candidate.  At the same 
time, we can also see that the score is directly 
proportional to a candidate?s frequency.  In 
addition, score is relevant to other features in table 
1.  Thus, we get formula 1, as following. 
 
Fig. 2. Relations between Percentage Selected 
and Length of Keywords 
??
???=
???= ??
otherwise0
feature i  thesatisfiesck  if1)(
)1.7)((
100ln)()(
th
)(
Ffi
2
ckf
fw
cklen
ckFreqckscore
i
cki
i
 (1) 
Where ck represents a candidate keyword, the 
function freq(ck) gets the frequency of ck, len(ck) 
represents its length, that is, the number of Chinese 
characters every item includes. F represents all the 
binary features of a candidate keyword as in table 
1. Every feature except the features of freq and len 
are denoted by fi.  fi(ck) is a binary function and its 
value is 0 or 1.  If a candidate item ck satisfies the 
ith feature, then the value is set to 1, otherwise, it?s 
set to 0.  wi is the corresponding weight of feature 
fi.  For features is_noun, in_title, in_seg1, 
is_proper and in_sign, we set their weights to 7, 13, 
5, 11 and 3 respectively by experience.  After each 
candidate keyword gets a score, we choose those 
whose scores rank higher as keywords.   
??
(physical
training)
????
(physical
management)
????
(sports)
??(track
and field)
??
(ball) ...
...
???
(pingpang)
???
(badminton)
??
(football) ...
Fig.3. A Sample Tree Structure of Content Words
 
Now the keywords we get are all selected from 
the original text.  However, some keywords may 
express the content of the document, but they don?t 
occur in the text.  Therefore, we have constructed 
one list of content words with hierarchical relations 
as in figure 3.  That is content words lexicon.  The 
lexicon contains about 1,200 words which are of-
ten used as keywords.  As the content words lexi-
con available now, we can look up in it and expand 
obtained keywords to a higher level, i.e., if a se-
lected keyword has a parent in the lexicon, the par-
ent word will be expanded as a keyword. 
4 Experimental Results and Analysis 
We select 37 news articles from China Daily as our 
testing material from which experts have manually 
extracted keywords.  There are 23 articles about 
national politics, 10 articles of international poli-
tics, and 4 sports news articles.  Here, we auto-
matically extracted keywords from them and 
evaluated the results with the standard measures of 
precision and recall, which are defined as follows: 
Where P represents precision, and R represents 
recall. In general, these two measures in one sys-
tem are opposite to each other.  When precision is 
higher, recall will be lower. Otherwise, when pre-
cision is improved, recall will decrease.  In table 2, 
we illustrate our experimental results.  The first 
three rows give measures for articles about differ-
ent styles and the figures in parentheses represent 
the number of articles.  The fourth row gives the 
average measure of our system.  For comparison, 
we also illustrate the results of Chien?s [1997] 
PAT-tree-based method from his experiments in 
the last row.  From this table, we can see that more 
emphasis is placed on precision in Chien?s system.  
However, we incline to enhancing recall when pre-
cision and recall are assured relatively balanced.  
When precision is lower, perhaps more noise is 
introduced into the set of candidate keywords.  Be-
cause we have adopted segmentation and POS tag-
ging tools which can verify whether a candidate 
character string is a meaningful unit and found that 
the noise introduced now is more or less relevant 
to the content of the article, we don?t have to worry 
more about precision.    Therefore, we hope to 
generate more keywords automatically under the 
condition that the number of noise words is ac-
cepted. 
 
 Recall Precision
National politics (23) 0.452 0.401 
International Politics (10) 0.644 0.594 
Sports news (4) 0.629 0.482 
Average 0.523 0.462 
Chien?s (exact match) 0.30 0.43 
Table 2. Experimental Results 
It has to be pointed out that there are no satis-
factory results in extracting keywords from texts 
[Chien, 1997].  Although some keywords extracted 
are the same as manually extracted ones in mean-
ing, they are often different due to one or two 
characters mismatched.  According to our analysis 
of experimental results, though only 46% of ex-
tracted keywords appear in the set of manual key-
words, the rest are also relevant to the text and 
adapt to the need of information retrieval.  At the 
same time, about 52% of the manual keywords are 
generated by the automatically indexing method, 
however, we can often find a substitute for most of 
the rest in the set of automatically generated key-
words.   
manually indexing keywords ofnumber 
recognized  keywords genuine ofnumber R
llyautomatica indexing keywords ofnumber 
recognized  keywords genuine ofnumber P
=
=
Most of the keywords missed occur only once 
in the text, but they are mostly proper nouns of 
places, organizations or titles of person.  And this 
reveals that we need to further improve the tech-
niques to recognize proper nouns. 
5 Conclusion and Future Work 
We have described a system for automatically in-
dexing keywords from texts.  One document is in-
putted into the recognizer module, the filter 
module and the selector module consecutively, 
with keywords output.  Here we utilize the mature 
techniques available now such as string frequency 
statistics, segmentation and POS tagging tools.  
Then, according to features, we propose our 
method to evaluate directly every candidate key-
word and select those with higher scores as key-
words.  At the same time, we break through the 
tradition of generating keywords only from the 
original text and acquire some keywords through 
looking up in the lexicon of content words with 
hierarchical relations. The experimental results 
show that our system can perform comparably to 
the state of the art. 
Owing to the limit of the training corpus, the 
parameters in scoring formula are set by experi-
ence values.  With our method, we can cumulate 
more and more documents with keywords.  Then 
we can adopt machine-learning methods to conduct 
keyword indexing, which can make parameters 
more objective.  That will be our further work. 
References 
[Chien 1997] Chien, L. F., PAT-Tree-Based Keyword 
Extraction for Chinese Information Retrieval, Pro-
ceedings of the ACM SIGIR International Confer-
ence on Information Retrieval, 1997, pp. 50--59.  
[Frank 1999] Frank E., Paynter G.W., Witten I.H., Gut-
win C., and Nevill-Manning C.G., Domain-specific 
keyphrase extraction, Proc. Sixteenth International 
Joint Conference on Artificial Intelligence, Morgan 
Kaufmann Publishers, San Francisco, CA, 1999, pp. 
668-673. 
[Lai 2002] Yu-Sheng Lai, Chung-Hsien Wu, Meaning-
ful term extraction and discriminative term selection 
in text categorization via unknown-word methodol-
ogy, ACM Transactions on Asian Language Informa-
tion Processing (TALIP), Vol.1, No.1, March 2002, 
pp. 34-64. 
[Liu 1998] Liu Ting, Wu Yan, Wang Kaizhu, An Chi-
nese Word Automatic Segmentation System Based 
on String Frequency Statistics Combined with Word 
Matching, Journal of Chinese Information Processing, 
Vol.12, No.1, 1998, pp. 17-25. 
[Ong 1999] T. Ong and H. Chen,  Updateable PAT-Tree 
Approach to Chinese Key Phrase Extraction Using 
Mutual Information: A Linguistic Foundation for 
Knowledge Management, Proceedings of the Second 
Asian Digital Libaray Conference, Taipei, Taiwan, 
Novemeber 8-9, 1999.  
[Turney 1999] Turney, P.D., Learning to Extract Key-
phrases from Text, NRC Technical Report ERB-1057, 
National Research Council, Canada, 1999. 
[Witten 1999] Witten I.H., Paynter G.W., Frank E., 
Gutwin C., and Nevill-Manning C.G., KEA: Practical 
automatic keyphrase extraction, Proc. DL '99, 1999, 
pp. 254-256.  
[Yang 2002] Wenfeng Yang, Chinese keyword extrac-
tion based on max-duplicated strings of the docu-
ments, Proceedings of the 25th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval, 2002, pp. 439-440. 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1209?1217,
Beijing, August 2010
Build Chinese Emotion Lexicons
Using A Graph-based Algorithm and Multiple Resources
Ge Xu, Xinfan Meng, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University), Ministry of Education
Institute of Computational Linguistics, Peking University
{xuge, mxf, wanghf}@pku.edu.cn
Abstract
For sentiment analysis, lexicons play an
important role in many related tasks. In
this paper, aiming to build Chinese emo-
tion lexicons for public use, we adopted a
graph-based algorithm which ranks words
according to a few seed emotion words.
The ranking algorithm exploits the simi-
larity between words, and uses multiple
similarity metrics which can be derived
from dictionaries, unlabeled corpora or
heuristic rules. To evaluate the adopted
algorithm and resources, two independent
judges were asked to label the top words
of ranking list.
It is observed that noise is almost un-
avoidable due to imprecise similarity met-
rics between words. So, to guarantee
the quality of emotion lexicons, we use
an iterative feedback to combine man-
ual labeling and the automatic ranking al-
gorithm above. We also compared our
newly constructed Chinese emotion lexi-
cons (happiness, anger, sadness, fear and
surprise) with existing counterparts, and
related analysis is offered.
1 Introduction
Emotion lexicons have a great impact on the re-
sults of related tasks. With high-quality emo-
tion lexicons, systems using simple methods
can achieve competitive performance. However,
to manually build an emotion lexicon is time-
consuming. Many research works in building lex-
icons use automatic methods to assist the building
procedure. Such works commonly rank words by
the similarities to a set of seed words, then those
words with high ranking scores are more likely to
be added to the final lexicons or used as additional
seed words.
For Chinese, emotion lexicons are scarce re-
sources. We can get a small set of emotion words
from semantic dictionary (such as CCD, HowNet,
synonym dictionaries) or directly from related pa-
pers (Xu and Tao, 2003) (Chen et al , 2009), but it
is often not sufficient for practical systems. Xu et
al. (2008) constructed a large-scale emotion on-
tology dictionary, but it is not publicly available
yet.
In this paper, we adopted a graph-based algo-
rithm to automatically rank words according to a
few seed words. Similarity between words can be
utilized and multiple resources are used to boost
performance. Combining manual labeling with
automatic ranking through an iterative feedback
framework, we can produce high-quality emotion
lexicons. Our experiments focused on Chinese,
but the method is applicable to any other language
as long as suitable resources exist.
The remainder of this paper is organized as fol-
lows. In Section 2, related works are introduced.
In Section 3, we describe a graph-based algorithm
and how to incorporate multiple resources. Sec-
tion 4 gives the details of applying the algorithm
on five emotions and shows how to evaluate the re-
sults. Section 5 focuses on how to build and evalu-
ate emotion lexicons, linguistic consideration and
instruction for identifying emotions are also in-
cluded. Finally, conclusion is made in Section 6.
1209
2 Related work
Riloff and Shepherd (1997) presented a corpus-
based method that can be used to build seman-
tic lexicons for specific categories. The input to
the system is a small set of seed words for a cat-
egory and a representative text corpus. The out-
put is a ranked list of words that are associated
with the category. An approach proposed by (Tur-
ney, 2002) for the construction of polarity started
with a few positive and negative seeds, then used
a similarity method (pointwise mutual informa-
tion) to grow this seed list from web corpus.
Our experiments are similar with these works, but
we use a different ranking method and incorpo-
rate multiple resources. To perform rating infer-
ence on reviews, Goldberg and Zhu (2006) cre-
ated a graph on both labeled and unlabeled re-
views, and then solved an optimization problem
to obtain a smooth rating function over the whole
graph. Rao and Ravichandran (2009) used three
semi-supervised methods in polarity lexicon in-
duction based on WordNet, and compared them
with corpus-based methods. Encouraging results
show methods using similarity between words can
improve the performance. Wan and Xiao (2009)
presented a method to use two types of similarity
between sentences for document summarization,
namely similarity within a document and simi-
larity between documents. The ranking method
in our paper is similar to the ones used in above
three papers, which fully exploit the relationship
between any pair of sample points (both labeled
and unlabeled). When only limited labeled data
are available, such method achieves significantly
better predictive accuracy over other methods that
ignore the unlabeled examples during training.
Xu et al (2008) at first formed a taxonomy for
emotions, under which an affective lexicon ontol-
ogy exploiting various resources was constructed.
The framework of ontology is filled by the com-
bination of manual classification and automatic
methods?To our best knowledge, this affective
lexicon ontology is the largest Chinese emotion-
oriented dictionary.
3 Our method
3.1 A graph-based algorithm
For our experiments, we chose the graph-based al-
gorithm in (Zhou et al , 2004) which is transduc-
tive learning and formulated as follows:
Given a point set ? = {x1, ..., xl, xl+1, ..., xn},
the first l points xi(i ? l) are labeled and the re-
maining points xu(l+1 ? u ? n) unlabeled. The
goal is to rank the unlabeled points.
Let F denotes an n-dimensional vector whose
elements correspond to ranking scores on the data
set ?. Define another n-dimensional vector Y with
Yi = 1 if xi is labeled and Yi = 0 otherwise. Y
denotes the initial label assignment.
The iterative algorithm is shown in the follow-
ing:
Algorithm 1 A graph-based algorithm
1. Construct the weight matrix W and set Wii to
zero to avoid self-reinforcement. W is domain-
dependent.
2. Construct the similarity matrix S =
D1/2WD1/2 using symmetric normalization. D
is a diagonal matrix with Dii = ?jWij .
3. Iterate F (t + 1) = ?SF (t) + (1 ? ?)Y until
convergence, where ? is a parameter in (0, 1), and
F (0) = Y . We clamp labeled points to 1 after
each iteration.
4. Let F ? denote F (t) when the iteration con-
verges.
In our experiments, labeled points are seed
emotion words, Sij denotes the similarity between
ith word and jth word. In an iteration, each word
absorbs label information from other words. More
similar two words are, more influence they have
on each other. The label information (initially
from seed emotion words) will propagate along S.
The final output F ? contains ranking scores for all
words, and a score indicates how similar the cor-
responding word is to the seed emotion words.
The implementation of the iterative algorithm
is theoretically simple, which only involves ba-
sic matrix operation. Compared with meth-
ods which do not exploit the relationship be-
tween samples, experiments showing advantages
of graph-based learning methods can be found
1210
in (Rao and Ravichandran, 2009),(Goldberg and
Zhu, 2006),(Tong et al , 2005),(Wan and Xiao,
2009),(Zhu and Ghahramani, 2002) etc. When la-
beled data are scarce, such graph-based transduc-
tive learning methods are especially useful.
3.2 Incorporate multiple resources
For building the emotion lexicons, we are faced
with lots of resources, such as semantic dictio-
naries, labeled or unlabeled corpora, and some
linguistic experiences which can be presented as
heuristic rules. Naturally we want to use these
resources together, thus boosting the final perfor-
mance. In graph-base setting, such resources can
be used to construct the emotion-oriented similar-
ity between words, and similarities will be repre-
sented by matrices.
The schemes to fuse similarity matrices are pre-
sented in (Sindhwani et al , 2005), (Zhou and
Burges, 2007), (Wan and Xiao, 2009) and (Tong et
al. , 2005) etc. In our paper, not aiming at compar-
ing different fusion schemes, we used a linear fu-
sion scheme to fuse different similarities matrices
from different resources. The scheme is actually
a convex combination of matrices, with weights
specified empirically.
The fusion of different similarity matrices
falls in the domain of multi-view learning. A
well-known multi-view learning method is Co-
Training, which uses two views (two resources)
to train two interactive classifiers (Blum and
Mitchell, 1998). Since we focus on building emo-
tion lexicons using multiple resources (multiple
views), those who want to see the advantages of
multi-view learning over learning with one view
can refer to (Blum and Mitchell, 1998), (Sind-
hwani et al , 2005), (Zhou and Burges, 2007),
(Wan and Xiao, 2009) and (Tong et al , 2005)
etc.
4 Experiments
We use the method in section 3 to rank for each
emotion with a few seed emotion words. Once we
implement the ranking algorithm 1, the main work
resides in constructing similarity matrices, which
are highly domain-dependent.
4.1 Construct similarity matrices
Here, we introduce how to construct four sim-
ilarity matrices used in building emotion lexi-
cons. Three of them are based on cooccurrence of
words; the fourth matrix is from a heuristic rule.
We use ictclas3.01 to perform word segmenta-
tion and POS tagging.
In our experiments, the number of words in-
volved in ranking is 935062, so theoretically, the
matrices are 93506 ? 93506. If the similarity be-
tween any pair of words is considered, the compu-
tation becomes impractical in both time and space
cost. So we require that each word has at most
500 nearest neighbors.
Four matrices are constructed as follows:
4.1.1 Similarity based on a unlabeled corpus
The unlabeled corpus used is People?s
Daily3(? ? ? ?1997?2004). After word
segmentation and POS tagging, we chose three
POS?s (i,a,l)4. The nouns were not included
to limit the scale of word space. We set the
cooccurrence window to a sentence, and removed
the duplicate occurrences of words. Any pair of
words in a sentence will contribute a unit weight
to the edge which connects the pair of words.
4.1.2 Similarity based on a synonym
dictionary
We used the Chinese synonym dictionary (?
??????????5) for this matrix. In
this dictionary, the words in a synonym set are
presented in one line and separated by spaces, so
there is no need to perform word segmentation
and POS tagging. Any pair of words in one line
will contribute a unit weight to the edge which
connects the pair of words.
4.1.3 Similarity based on a semantic
dictionary
We used The Contemporary Chinese Dictio-
nary (??????) to construct the third simi-
1downloaded from http://www.ictclas.org/
2Words are selected after word segmentation and POS
tagging, see section 4.1.1?4.1.3 for selection of words in de-
tails.
3http://icl.pku.edu.cn/
4i=Chinese idiom, a=adjective, l=Chinese phrase
5http://ir.hit.edu.cn/
1211
larity matrix. Since word segmentation may seg-
ment the entries of the dictionary, we extracted all
the entries in the dictionary and store them in a file
whose words ictclas3.0 was required not to seg-
ment. Furthermore, for an entry in the dictionary,
the example sentences or phrases appearing in its
gloss may contain many irrelevant words in terms
of emotions, so they were removed from the gloss.
After word segmentation and POS tagging6, we
set the cooccurrence window to one line (an en-
try and its gloss without example sentences or
phrases), and removed the duplicate occurrences
of words. An entry and any word in the modi-
fied gloss will contribute a unit weight to the edge
which connects the pair of words. This construct-
ing was a bit different, since we did not consider
the similarity between words in modified gloss.
4.1.4 similarity based on a heuristic rule
In Chinese, a word is composed of one or sev-
eral Chinese characters. A Chinese character is
normally by itself an independent semantic unit,
so the similarity between two words can be in-
ferred from the character(s) that they share. For
example, the Chinese word ? (happy) appears
in the word ?? (readily). Since ?? and ?
share one Chinese character, they are regarded as
similar. Naturally, the larger the proportion that
two words share, the more similar they are. In
this way, the fourth weighted matrix was formed.
To avoid incurring noises, we exclude the cases
where one Chinese character is shared, with the
exception that the Chinese character itself is one
of the two Chinese words.
4.1.5 Fusion of four similarity matrices
After processing all the lines (or sentences), the
weighted matrices are normalized as in algorithm
1, then four similarity matrices are linearly fused
with equal weights (1/4 for each matrix).
4.2 Select seed emotion words
In our experiments, we chose emotions of happi-
ness, sadness, anger, fear and surprise which are
widely accepted as basic emotions7. Empirically,
6since we do not segment entries in this dictionary, all
POS?s are possible
7Guidelines for identifying emotions is in section 5, be-
fore that, we understand emotions through common sense.
we assigned each emotion with seed words given
in Table 1.
Emotion Seed words
?(happiness) ??,??,??,??,?
???,??,??
?(anger) ??,??,??,??,?
?, ??, ??, ??, ?
?,??,??,????,
????,????
?(sadness) ??,??,??,??,?
?,??,??,??,??
??, ??, ????, ?
?,??,??,????,
????
?(fear) ??, ??, ??, ???
?, ??, ??, ??, ?
?,????,????
?(surprise) ??, ???? ,??, ?
?,??,??,?,???
?,??,??
Table 1: Seed emotion words
4.3 Evaluation of our method
We obtained five ranking lists of words using the
method in section 3. Following the work of (Riloff
and Shepherd, 1997), we adopted the following
evaluation setting.
To evaluate the quality of emotion ranking lists,
each list was manually rated by two persons inde-
pendently. For each emotion, we selected the top
200 words of each ranking list and presented them
to judges. We presented the words in random or-
der so that the judges had no idea how our system
had ranked the words. The judges were asked to
rate each word on a scale from 1 to 5 indicating
how strongly it was associated with an emotion, 0
indicating no association. We allowed the judges
to assign -1 to a word if they did not know what
it meant. For the words rated as -1, we manually
assigned ratings that we thought were appropriate.
The results of judges are shown in figures 1-5.
In these figures, horizontal axes are the number of
reviewed words in ranking lists and vertical axes
are number of emotion words found (with 5 dif-
ferent strength). The curve labeled as > x means
that it counts the number of words which are rated
1212
0 50 100 150 2000
20
40
60
80
100
120
140
160
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 1: happiness
0 50 100 150 2000
20
40
60
80
100
120
140
160
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 2: anger
greater than x by either judge.
Curves (> 0, > 1, > 2) display positive slopes
even at the end of the 200 words, which implies
that more emotion words would occur if more
than 200 words are reviewed. By comparison,
curves (> 3, > 4) tend to be flat when they are
close to the right side, which means the cost of
identifying high-quality emotion words will in-
crease greatly as one checks along the ranking list
in descendent order.
It is observed that words which both judges as-
sign 5 are few. In surprise emotion, the number
is even 0. Such results may reflect that emotion
is harder to identify compared with topical cate-
gories in (Riloff and Shepherd, 1997).
0 50 100 150 2000
20
40
60
80
100
120
140
160
180
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 3: sadness
0 50 100 150 2000
20
40
60
80
100
120
140
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 4: fear
0 50 100 150 2000
20
40
60
80
100
120
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 5: surprise
From the semantic dictionary, our method
found many low-frequency emotion words such as
? (pleasant, glad),?? (surprise and happy),?
? (sad), or those used in Chinese dialects such as
?? (fear), ?? (angry). Such emotion words
are necessary for comprehensive emotion lexi-
cons.
Because more POS?s than adjectives and verbs
are included in our experiments, some emotion
words such as the noun ?? (unexpected win-
ner),and the adverb ?? (to one?s surprise) are
also spotted, which to some extent implies the
generality of our method.
5 Construct emotion lexicons
The above section introduced a method to rank
words with a few seed emotion words. How-
ever, to build emotion lexicons requires that we
manually remove the noises incurred by the au-
tomatic ranking method. Accordingly, guide-
lines for identifying emotions are needed, and also
some linguistic consideration in identifying emot-
ing words should be given.
1213
5.1 An iterative feedback to denoise
In our experiments, we observed that noises in-
curred by similarity matrices are almost unavoid-
able. For example, in the unlabeled corpus, ?
??? (state visits) always co-occurred with ?
? (happy) or?? (happy), so in happiness emo-
tion, ???? acquired a high ranking position
(174th); in terms of the heuristic rule, ?? (ex-
pected) shares two Chinese characters with ??
?? (unexpected, surprised), however they have
opposite meaning because?? (exceed, beyond)
is a negative word. ?? unfavorably ranked high
(88th) in surprise emotion; from the semantic dic-
tionary, the gloss of?? (Chinese Spring Festival
pictures) contains?? (happy), thus in happiness
emotion,?? ranked high (158th).
So after each ranking of an emotion, in the de-
scendent order of ranking scores, we manually re-
vised some scores in about top 500. Several crite-
ria (see 5.2 and 5.3) were given to guide if a word
has a specified emotion. For those words surely
bearing the specified emotion, we assigned 1 to
them ,and left others unchanged. Seeing the words
newly revised to be 1 as new seed emotion words,
we run the ranking algorithm again. After such
feedback was repeated 2?3 times, we collected
all the words labeled with 1 to form the final emo-
tion lexicons. In (Zhou et al , 2004), the author
also suggested such iterative feedback to extend
the query (seed) set and improve the ranking out-
put. Commonly, the size of an emotion lexicon is
small, so we do not have to check too many words.
The human revising procedure is sensitive to
annotators? background. To improve the quality
of the emotion lexicons, experts with linguistic or
psychology background will help.
Furthermore, the ranking algorithm used in our
paper is clearly sensitive to the initial seed words,
but since we adopt an iterative feedback frame-
work, the words not appearing in the initial set
of seed words will show up in next iteration with
high ranking scores. We also performed experi-
ments which selected emotion seed words based
on the Chinese synonym dictionary and the emo-
tion words in (Chen et al , 2009), similar results
were found.
5.2 Guidelines for identifying emotions
The same as (Chen et al , 2009), we used the def-
inition that emotion is the felt awareness of bod-
ily reactions to something perceived or thought.
Also, we were highly influenced by the structure
of the affective lexicon presented by (Ortony et
al. , 1987), and used the Affective states and
Affective-Behavioral conditions in the structure to
identify emotion words in our paper8.
With such guidelines,?? (cowardice, relates
more to external evaluation) is not an emotional
word of fear. We also intentionally distinguish be-
tween emotions and expression of emotions. For
example, ?? (laugh), ?? (haw-haw) are seen
as expression of happiness and?? (tremble) as
of fear, but not as emotion words. In addition,
we try to distinguish between an emotion and the
cause of an emotion, see 5.3 for an example.
For each emotion, brief description is given as
below9:
1. Happiness?the emotional reaction to some-
thing that is satisfying.
2. Anger?do not satisfy the current situation
and have a desire to fight or change the situa-
tion. Often there exists a target for this emo-
tion.
3. Sadness?an emotion characterized by feel-
ings of disadvantage, loss, and helplessness.
Sadness often leads to cry.
4. Fear?the emotional response to a perceived
threat. Fear almost always relates to future
events, such as worsening of a situation, or
continuation of a situation that is unaccept-
able.
5. Surprise?the emotional reaction to some-
thing unexpected.
5.3 Linguistic consideration for identifying
emotion words
If a word has multiple senses, we only consider its
emotional one(s). For example,?? (as a verb, it
means be angry, but means vitality or spirits as a
noun) will appear in the emotion lexicon of anger.
8According to (Ortony et al , 1987), surprise should not
be seen as a basic emotion for it relates more to cognition.
However, our paper focuses on the building of emotion lexi-
cons, not the disputable issue of basic emotions
9we mainly referred to http://en.wikipedia.org/wiki
1214
If one sense of a word is the combination of emo-
tions, the word will appear in all related emotions.
We mainly consider four POS?s, namely nouns,
verbs, adjectives and adverb10. If a word has mul-
tiple POS?s, we normally consider its POS with
strongest emotion (Empirically, we think the emo-
tion strength ranks in decedent order as following:
adjectives, verbs, adverbs, nouns.). So we con-
sider the verb of?? (fear) when it can be used
as a noun and a verb in Chinese. The?? exam-
ple above also applies here.
For each of four POS?s, instruction for emotion
identification is given as below:
Nouns: For example,?? (rage, anger),??
(joy or jubilation), ?? (an unexpected winner)
are selected as emotion words. We distinguish be-
tween an emotion and the cause of an emotion.
For example, calamity often leads to sadness, but
does not directly contain the emotion of sadness.
?? appears in the surprise lexicon because we
believe it contains surprise by itself.
Adverbs: The adverbs selected into emotion
lexicons contain the emotions by themselves. For
example,?? (unexpectedly),??? (cheerily),
??? (angrily), ?? (unexpectedly), ???
(sadly) etc.
Verbs: As in (Ortony et al , 1987), Chi-
nese emotion verbs also fall into at least two dis-
tinct classes, causatives and noncausatives. Both
classes are included in our emotion lexicons. For
example, ??? (be angry), ?? (fear) are
noncausative verbs, while ?? (enrage), ??
(to make someone surprised) are causative ones.
Probably due to the abundant usage of ??/?
?/?? (to make someone) etc., causative emo-
tion verbs are few compared to noncausative ones
in Chinese.
Adjective?Quite a lot of emotion words fall in
this POS, since adjectives are the natural expres-
sion of internal states of humans. For example,?
? (happy),?? (surprised),?? (angry) etc.
For any word that it is hard to identify at first
sight, we used a search tool11 to retrieve sentences
10For Chinese idioms, we only considered those used as
these four POS?s, omitted those used as a statement, such
as???? (an army burning with righteous indignation is
bound to win)
11provided by Center for Chinese Linguistics of Peking
University, http://ccl.pku.edu.cn
which contain the word, and then identify if the
word is emotional or not by its usage in the sen-
tences.
5.4 Comparison with existing Chinese
emotion resources
????????????????
????????????????
???????????????
???????????????
???????????????
????????????????
????????????????
????????????????
????????????????
????????????????
???????????????
???????????????
???????????????
????????????????
????????????????
???????????????
????????????????
????????????????
????????????????
????????????????
???????????????
????????????????
????????????????
????????????????
????
Table 2: The emotion lexicon of surprise
Under the guidelines for manually identifying
emotion words, we finally constructed five Chi-
nese emotion lexicons using the iterative feed-
back. The newly constructed emotion lexicons
were also reported as resources together with our
paper. The emotion lexicon of surprise is shown
in Table 2. In this part, we compare our lexicons
with the following counterparts, see Table 3.
Ours1 in the table is the final emotion lexicons,
and Ours2 is the abridged version that excludes
the words of single Chinese character and Chinese
idioms.
Chinese Concept Dictionary (CCD) is a
WordNet-like semantic lexicon(Liu et al , 2003).
1215
? ? ? ? ?
CCD nouns 22 27 38 46 10
(Xu and Tao, 2003) 45 12 28 21 12
(Chen et al , 2009) 28 34 28 17 11
(Xu et al , 2008) 609 187 362 182 47
Ours1 95 118 97 106 99
Ours2 52 77 72 57 65
Table 3: Compare various emotion lexicons
We only considered the noun network which is
richly developed in CCD, as in other semantic dic-
tionaries. For each emotion, we chose its synset
as well as the synsets of its hypernym and hy-
ponym(s). In fact, most of words in the emotion
nouns extracted can be used as verbs or adjectives
in Chinese. However, since CCD is not designed
for emotion analysis, words which are expression
of emotions such as?? (cry) or evaluation such
as?? (cowardice) were included.
Selecting nouns and verbs, Xu and Tao (2003)
offered an emotion taxonomy of 390 emotion
words. The taxonomy contains 24 classes of emo-
tions and excludes Chinese idioms. By our in-
spection to the offered emotion words in this tax-
onomy, the authors tried to exclude expression of
emotions, evaluation and cause of emotions from
emotions, which is similar with our processing12.
Ours2 is intentionally created to compare with this
emotion taxonomy.
Based on (Xu and Tao, 2003), Chen et al
(2009) removed the words of single Chinese char-
acter; let two persons to judge if a word is an
emotional one and only those agreed by the two
persons were seen as emotion words. It is worth
noting that Chen et al (2009) merges? (anger)
and? (fidget) in (Xu and Tao, 2003) to form the
? (anger) lexicon, thus?? (dislike) appears in
anger lexicon. However, we believe?? (dislike)
is different with? (anger), and should be put into
another emotion. Also, we distinguish between?
(hate) and? (anger).
Xu et al (2008) constructed a large-scale affec-
tive lexicon ontology. Given the example words
in their paper, we found that the authors did not
intentionally exclude the expression of emotions
such as???? (literally, red face and ear),?
?? (literally, be smiling). Such criteria of iden-
12Xu and Tao (2003) included words such as ??/??
(be willing to),?? (be careful) in their happiness lexicon,
which we think should not be classified into happiness.
tifying emotion words may partially account for
the large size of their emotion resources.
6 Conclusion and future work
In this paper, aiming to build Chinese emotion lex-
icons, we adopt a graph-based algorithm and in-
corporate multiple resources to improve the qual-
ity of lexicons and save human labor. This is an
initial attempt to build Chinese emotion lexicons,
the quality of constructed emotion lexicons is far
from perfect and is supposed to be improved step
by step.
The method in this paper can be further ex-
tended to subjectivity/polarity classification and
other non-sentimental tasks such as word similar-
ity computing, and can be also adapted to other
languages. The more resources we use, the more
human cost can be saved and the higher the qual-
ity of built emotion lexicons is.
In the future work, we want to construct other
emotion lexicons such as ? (like, love), ? (dis-
like),? (desire) etc. using the same method.
Acknowledgement This research is supported
by National Natural Science Foundation of China
(No.60973053, No.90920011)
References
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training? In Proceed-
ings of the 11th Annual Conference on Computa-
tional Learning Theory, 92-100.
Ying Chen, Sophia Y. M. Lee, and Churen Huang.
2009. A Cognitive-based Annotation System for
Emotion Computing. Proceedings of the Third Lin-
guistic Annotation Workshop (LAW III).
Andrew B. Goldberg, Xiaojin Zhu. 2006. Seeing
stars when there aren?t many stars: graph-based
semi-supervised learning for sentiment categoriza-
tion. Proceedings of TextGraphs: the First Work-
shop on Graph Based Methods for Natural Lan-
guage Processing on the First Workshop on Graph
Based Methods for Natural Language Processing.
Y. Liu and et al 2003. The CCD Construction Model
and Its Auxiliary Tool VACOL. Applied Linguis-
tics, 45(1):83-88.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, 11, 341-364.
1216
Delip Rao and D. Ravichandran. 2009. Semisuper-
vised polarity lexicon induction. Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, 675-682.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons.
In Proceedings of the Second Conference on Em-
pirical Methods in Natural Language Processing,
pages 117-124.
V. Sindhwani, P. Niyogi, and M. Belkin. 2005. A
co-regularization approach to semisupervised learn-
ing with multiple views. Proc. ICML Workshop on
Learning with Multiple views.
H. Tong, J. He, M. Li, C. Zhang, and W. Ma. 2005.
Graph based multi-modality learning. In Proceed-
ings of the 13th Annual ACM international Con-
ference on Multimedia. MULTIMEDIA ?05. ACM,
New York, NY, 862-871.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. ACL 2002, 417-424.
Xiaojun Wan and Jianguo Xiao. 2009. Graph-Based
Multi-Modality Learning for Topic-Focused Mul-
tiDocument Summarization. IJCAI 2009, 1586-
1591.
Linhong Xu, Hongfei Lin, Yu Pan, Hui Ren and Jian-
mei Chen. 2008. Constructing the Afective Lexicon
Ontology. JOURNAL OF THE CHINA SOCIETY
F0R SCIENTIFIC AND TECHNICAL INFORMA-
TION Vo1.27 No.2, 180-185.
X. Y. Xu, and J. H. Tao. 2003. The study of affective
categorization in Chinese. The 1st Chinese Confer-
ence on Affective Computing and Intelligent Inter-
action. Beijing, China.
Hongbo Xu, Tianfang Yao, and Xuanjing Huang.
2009. The second Chinese Opinion Analysis Eval-
uation(in Chinese). COAE 2009.
D. Zhou, O. Bousquet, T. Lal, J. Weston, and B.
Scholkopf. 2004. Learning with local and global
consistency. Advances in Neural Information Pro-
cessing Systems 16. MIT Press, Cambridge, MA. .
D. Zhou and C. J. C. Burges. 2007. Spectral cluster-
ing and transductive learning with multiple views.
Proceedings of the 24th international conference on
Machine learning.
X. Zhu and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical Report CMUCALD02107. CMU.
1217
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 233?244, Dublin, Ireland, August 23-29 2014.
Collaborative Topic Regression with Multiple Graphs Factorization for
Recommendation in Social Media
Qing Zhang
Key Laboratory of Computational
Linguistics (Peking University)
Ministry of Education, China
zqicl@pku.edu.cn
Houfeng Wang
?
Key Laboratory of Computational
Linguistics (Peking University)
Ministry of Education, China
wanghf@pku.edu.cn
Abstract
With a large amount of complex network data available from multiple data sources, how to effec-
tively combine these available data with existing auxiliary information such as item content into
the same recommendation framework for more accurately modeling user preference is an inter-
esting and significant research topic for various recommender systems. In this paper, we propose
a novel hierarchical Bayesian model to integrate multiple social network structures and content
information for item recommendation. The key idea is to formulate a joint optimization frame-
work to learn latent user and item representations, with simultaneously learned social factors
and latent topic variables. The main challenge is how to exploit the shared information among
multiple social graphs in a probabilistic framework. To tackle this challenge, we incorporate
multiple graphs probabilistic factorization with two alternatively designed combination strate-
gies into collaborative topic regression (CTR). Experimental results on real dataset demonstrate
the effectiveness of our approach.
1 Introduction
Many real-life data have representations in the form of multiple views (Liu et al., 2013). For example,
web pages usually consist of both text content and hyperlink information; images on the web have
relevant tags associated with their content. In addition, it is also common that in real networks comprising
multiple types of nodes are connected by multiple types of links, forming heterogeneous information
networks (HIN) (Huang et al., 2012). For example, in scientific community, various types of links are
formed for different types of objects, i.e., author writes paper, venue publishes paper, reader labels tag,
and so on. Therefore, with a large amount of complex network data available from multiple data sources,
how to effectively combine this kind of rich structure with other auxiliary information such as content
information into the same recommendation framework is an interesting and significant research topic
for various recommender systems. This paper aims to model multiple social graphs into a principled
hierarchy Bayesian framework to improve recommending performance.
The basic idea in this paper is inspired by multi-view learning approach (Liu et al., 2013), i.e., leverag-
ing the redundancy and consistency among distinct views (Kumar et al., 2011) to strengthen the overall
performance. We extend this idea (Liu et al., 2013) originally for clustering problem to deal with rat-
ing scarcity problem when modeling user preference for recommendation. Just as in general multi-view
learning, each view of objective function is assumed to be capable of correctly classifying labeled exam-
ples separately. Then, they are smoothed with respect to similarity structures in all views. Similarly, in
this paper, we assume that our individual views of multiple user social relations are similar and comple-
mentary with a shared latent structure.
However, different from multi-view clustering problem, our goal is to recover a sparse rating matrix
with a large number of missing user-item pairs rather than merely exploiting cluster structure with full
task information. Thus, the straightforward multi-view representation of the objective (rating matrix) is
?
Corresponding author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
233
not available. Instead, we use side information (user social graphs) to exploit multi-view learning for
improving collaborative filtering (CF). As a result, collaborative topic regression (CTR) (Wang and Blei,
2011) is employed as our basic learning framework with side information. Recently, CTR has gained
considerable attention due to its well-defined mathematical framework and strong performance on user
behavior prediction for various real-world applications, such as document recommendation (Li et al.,
2013), tag recommendation (Wang et al., 2013), music recommendation (Purushotham et al., 2012),
celebrity recommendation (Ding et al., 2013) and vote prediction (Kang and Lerman, 2013). However,
all the extensions above merely focus on a single view of user or item relation. In reality, a large amount
of diverse social graphs data are widely existed and particularly valuable for mutually reinforcing each
other. Therefore, it should be well considered. Taking this into consideration, we extend CTR with
multiple social graphs factorization for recommender systems.
The main challenge of incorporating multiple relations into CTR is how to exploit the shared infor-
mation among multiple social networks and how to further deal with it to recover sparse rating matrix in
a probabilistic framework. Previous efforts, purely to address the first issue for clustering problem, are
usually to seek a weak consensus (Liu et al., 2013) learnd from data jointly with clustering process. Intu-
itively, consensus can be seen as a latent cluster structure shared by different views. Thus, it means that
learning from different views should be softly regularized towards a common latent structure. However,
it is not easy to directly formulate it in a probabilistic framework, because weak consensus modeling can
not be separated from a joint higher task, i.e., recovering spare rating matrix, in our case.
To tackle this challenge, we propose a novel hierarchical Bayesian model with multiple social graphs
factorization. We exploit two ways of modeling shared information for multi-view based recommen-
dation. One is for heterogeneous network by directly modeling different view specific latent structures
with consensus for user representation. The other is for homogeneous case, which can be used as a trans-
formed version of heterogeneous relations. In contrast with the first strategy, we model the latter using
a shared latent social structure for all views but with different user representations. Thus, we can relax
strong consensus assumption in our heterogeneous case, through linear combination of each sub-latent
user with maintained sharing mechanism. The multiple graphs factorization process in the proposed
model can be seen as a regularization approach on each latent user for better uncovering user-item latent
structures. Although, regularization technique for modeling multiple heterogeneous networks is a hot
research topic, in clustering study from an algebra view (Liu et al., 2013; Kumar et al., 2011), not much
is known on using it for collaborative recommendation problems in a more complex probabilistic setting.
The following sections will discuss those in details and we use the terms network and graph inter-
changeably throughout this paper.
2 Preliminaries
In this section, we briefly review collaborative topic regression (CTR) (Wang and Blei, 2011), as the
foundation of our proposed model. Figure 1 (left) shows the graphical representation of CTR, which
combines the merits of traditional collaborative filtering and probabilistic topic modeling. Specifically,
the key mechanism of CTR is that using topic vectors learned from LDA (Blei et al., 2003) jointly
controls the prior distribution of latent items in original matrix factorization process of CF. The generative
process is described as follows:
? For each user i,
? draw user latent vector u
i
? N(0, ?
?1
u
I), multivariate Gauss distribution with zero mean.
? For each item j,
? draw topic proportions ?
j
? Dirichlet(?), Dirichlet distribution.
? draw item latent offset vector 
j
? N(0, ?
?1
v
I), and set the item latent vector as v
j
= 
j
+ ?
j
.
? For each word w
jn
? draw topic assignment z
jn
?Mult(?), Multinomial distribution.
? draw word w
jn
?Mult(?
z
jn
), Multinomial distribution.
? For each user-item pair (i, j),
234
Figure 1: CTR (left), heterogeneous CTR-MGF (middle), homogeneous CTR-MGF (right).
? draw the response r
ij
? N(u
T
i
v
j
, c
?1
ij
), univariate Gauss distribution, where c
ij
is a confidence
parameter for rating r
ij
, a > b. c
ij
= a (higher confidence), if r
ij
= 1, and c
ij
= b, if r
ij
= 0.
However, CTR does not take the complex social network information, which is available and crucial
in many real-world applications, into consideration.
3 CTR-MGF: Collaborative Topic Regression with Multiple Graphs Factorization
In this section, we discuss our proposed method, called CTR with multiple graphs factorization (CTR-
MGF). Our model is a generalized hierarchical Bayesian model which jointly learns latent user, item and
multiple latent social factor spaces. Different from previous approaches, our method extends CTR to
multiple complex networks setting instead of one particular type of relation for user or item. Moreover,
we consider two real general contexts in various practical applications. One is the context of hetero-
geneous networks. The other is the context of homogeneous networks. It is noted that, for relative
simplicity, in this paper we only consider user oriented complex network. The graphical representation
of our models in Figure 1 (middle and right) takesK = 3 networks as illustration, which can be arbitrary
in our derivation. It is also easy to see that Purushotham et al. (2012) is a special case of our proposed
model, which is not equipped with graph sharing mechanism.
3.1 Model Notations
Each social matrixQ corresponds to a social network structureG = {V,E}, where users and their social
relations are represented as vertex set V and edge setE in network structureG, respectively. The element
q
im
in Q denotes the binary relation between user ?i? and graph specific feature ?m? in heterogeneous
network or the relation between two users ?i? and ?m? in homogeneous network.
3.2 CTR-MGF for Heterogeneous Networks
Heterogeneous network is formed by multiple types of nodes being connected by multiple types of links.
The key characteristic of heterogeneous network is that the sizes of feature dimensions are different
among multiple social graphs. For example, in a social music sharing system such as LastFM, each user
has multiple heterogeneous relations associated with the interested music, i.e., user-artist, user-tag, and
so on. Our model can handle all these relations in the proposed framework, CTR-MGF. Specifically, the
generative process of CTR-MGF for heterogeneous networks is listed as follows:
? For each item j,
? draw topic proportions ?
j
? Dirichlet(?), Dirichlet distribution.
? draw item latent offset 
j
? N(0, ?
?1
v
I), multivariate Gauss distribution and set the item latent
vector as v
j
= 
j
+ ?
j
.
235
? For each word w
jn
? draw topic assignment z
jn
?Mult(?).
? draw word w
jn
?Mult(?
z
jn
).
? For each heterogeneous social graph k,
? For each social graph specific feature m
? draw graph factor-specific latent feature vector s
k
m
? N(0, ?
?1
s
km
I).
? For each user i,
? draw the shared latent user vector among multiple social graphs u
i
? N(0, ?
?1
u
i
I).
? For each heterogeneous social graph k
? For each social graph specific feature m
? draw graph specific user heterogeneous relation pair q
k
im
? N(u
T
i
s
k
m
, c
?1
k,q
im
).
? For each user-item pair (i, j),
? draw the response r
ij
? N(u
T
i
v
j
, c
?1
ij
).
In the above generative process, the joint likelihood of data, i.e., R,Q
k=1,...,K
,W , and the latent
factors U, V, S
k=1,...,K
under the full model is:
p(R,U, V, S
k=1,...,K
, Q
k=1,...,K
,W, ?|?
?
)
= p(R|U, V ) ? p(W, ?|?) ? p(U |?
U
) ? p(V |?
V
) ?
K
?
k
p(Q
k
|U, S
k
, ?
Q
k
) ?
K
?
k
p(S
k
|?
S
k
)
(1)
For learning the parameters, we develop an EM-style algorithm similar to CTR. In our model, finding
the MAP is equivalent to maximizing the following log likelihood obtained by substituting univariate
and multivariate Gaussian pdfs in Eq. 1:
L =
?
j
?
n
log(
?
z
?
jz
?
z
jn
)?
K
?
k=1
?
S
k
2
?
m
(s
k
m
)
T
s
k
m
?
K
?
k=1
?
i
?
m
c
k,q
im
2
(q
k
im
? u
T
i
s
k
m
)
2
?
?
U
2
?
i
u
T
i
u
i
?
?
V
2
?
j
(v
j
? ?
j
)
T
(v
j
? ?
j
)?
?
i
?
j
c
ij
2
(r
ij
? u
T
i
v
j
)
2
(2)
We employ coordinate ascent (CA) approach alternatively optimizing latent factor variables u
i
, v
j
,
s
k=1,2,...,K
m
and the simplex variables ?
j
as topic proportions. Specifically, the following update rules in
CA are obtained by setting the derivative of L with respect to u
i
, v
j
, and s
k=1,2,...,K
m
to zero.
u
i
= (?
U
I + V
T
D
c
i
V +
K
?
k
S
T
k
D
k
q
i
S
k
)
?1
? (V
T
D
c
i
R
i
+
K
?
k
S
T
k
D
k
q
i
Q
k
i
) (3)
v
j
= (?
V
I + U
T
D
c
j
U)
?1
? (?
V
?
j
+ U
T
D
c
j
R
j
) (4)
s
k=1,2,...,K
m
= (?
S
k
I + U
T
D
k
q
m
U)
?1
? (U
T
D
k
q
m
Q
k
m
) (5)
where K is the total number of graphs. I is an identity matrix of the same dimension as that of la-
tent space. U and V are matrices with rows as latent users and latent items, respectively. S
k
is a
matrix with rows as social factor-specific latent feature vectors for graph k. R
i
is a column vector
with values [r
i1
, . . . , r
iJ
]
T
. Similarly, R
j
= [r
1j
, . . . , r
Ij
]
T
. For graph k, Q
k
i
= [q
k
i1
, . . . , q
k
iM
]
T
and Q
k
m
= [q
k
1m
, . . . , q
k
Im
]
T
respectively. Likewise, D
k
q
i
, and D
k
q
m
are similarly defined with diago-
nal elements c
k,q
i.
and c
k,q
.m
, respectively. D
c
i
is a diagonal matrix with values diag(c
i1
, . . . , c
iJ
).
D
c
j
= diag(c
1j
, . . . , c
Ij
). In addition, c
ij
and c
k,q
im
are also seen as the confidence parameters for r
ij
and q
k
im
, respectively. The high confidence value a is set to the observed interactive pairs and the low
confidence value b is set to the unobserved interactive pairs, where a > b > 0.
236
For our brevity, the remaining update rules for ? and ?, can be obtained using the same way as de-
scribed in CTR (Wang and Blei, 2011). Please see that for details.
It is worth noting that through our assumption and the derivation above, we have theoretically proved
that our modeling in this case is equivalent to first concatenating features of different views together and
then applying Purushotham et al. (2012) for recommendation.
3.3 CTR-MGF for Homogeneous Networks
In this section, we further extend the basic CTR to the context of homogeneous networks. In fact, any
user specific homogeneous networks can be obtained through transforming corresponding heterogeneous
networks. For example, in LastFM, we can construct two user-user homogeneous networks by comput-
ing the similarities of user-tag and user-artist from original heterogeneous networks. The goal of this
transformation is to further exploit weak consensus modeling scheme based on Section 3.2. Different
from the graph sharing mechanism presented in last section, we relax the restriction that all users have
the same representation. Specifically, we assume each latent user has multiple sub-graph specific repre-
sentations.
However, it is nontrivial to model the relaxed assumption directly from original perspective. To achieve
this more weaker sharing mechanism, we are towards its transformed perspective, i.e., sacrificing hetero-
geneous characteristic, because we need to exploit shared information from latent graph specific feature
perspective. Thus, we require equal dimensions of different graphs, which motivates us to investigate the
homogeneous case.
The key differences between our model in this section and that in last section are the strategies of
latent user modeling and its social factor modeling. More specifically, we model each latent user as a
linear combination of all sub-latent users associated with multiple homogeneous networks. All these
sub-latent homogeneous users are associated with a shared social factor feature space. Thus, the shared
information among multiple graphs can be exploited and it is more flexible to adjust the contribution
of each sub-latent user to the final latent user representation. The generative process of CTR-MGF for
homogeneous networks is listed as follows:
? For each item j,
? draw topic proportions ?
j
? Dirichlet(?), Dirichlet distribution.
? draw item latent offset 
j
? N(0, ?
?1
v
I), multivariate Gauss distribution and set the item latent
vector as v
j
= 
j
+ ?
j
.
? For each word w
jn
? draw topic assignment z
jn
?Mult(?).
? draw word w
jn
?Mult(?
z
jn
).
? For each social graph specific feature m, regarding to all related homogeneous social graphs
? draw a shared factor-specific latent feature vector across multiple graphs s
m
? N(0, ?
?1
s
m
I).
? For each user i,
? For each homogeneous social graph k
? draw a social graph specific latent user u
k
i
? N(0, (?
k
u
i
)
?1
I).
? For each social graph specific feature m
? draw graph specific user homogeneous relation pair q
k
im
? N((u
k
i
)
T
s
m
, c
?1
k,q
im
).
? draw a final latent user u
i
? N(
?
K
k=1
T
k
u
k
i
, ?
?1
u
i
I).
? For each user-item pair (i, j),
? draw the response r
ij
? N(u
T
i
v
j
, c
?1
ij
).
In the above generative process, the joint likelihood of data, i.e. R, Q
k=1,...,K
and W , and the latent
factors U,U
k=1,...,K
, V and S under the full model is:
p(R,U, V, S, U
k=1,...,K
, Q
k=1,...,K
,W, ?|?
?
) = p(R|U, V ) ? p(W, ?|?)?
(
K
?
k=1
p(Q
k
|U
k
, S, ?
Q
k
)) ? p(S|?
S
) ? (
K
?
k=1
p(U
k
|?
U
k
)) ? p(V |?
V
) ? p(U |?
U
) ? p(U |U
k=1,...,K
, ?
C
)
(6)
237
Similarly to last section, we develop an EM-style algorithm to find the MAP solutions, which is equiva-
lent to maximizing the following log likelihood by substituting univariate and multivariate Gaussian pdfs
in Eq. 6:
L =
?
j
?
n
log(
?
z
?
jz
?
z
jn
)?
K
?
k=1
?
U
k
2
?
i
(u
k
i
)
T
u
k
i
?
?
S
2
?
m
s
T
m
s
m
?
K
?
k=1
?
i
?
m
c
k,q
im
2
(q
k
im
? (u
k
i
)
T
s
m
)
2
?
?
V
2
?
j
(v
j
? ?
j
)
T
(v
j
? ?
j
)
?
?
i
?
j
c
ij
2
(r
ij
? u
T
i
v
j
)
2
?
?
C
2
?
i
(u
i
?
K
?
k=1
T
k
u
k
i
)
T
(u
i
?
K
?
k=1
T
k
u
k
i
)?
?
U
2
?
i
u
T
i
u
i
(7)
We employ coordinate ascent (CA) approach as previous section alternatively optimizing latent factor
variables and simplex variables as topic proportions. Then we acquire the update rules by setting the
derivative of L with respect to the following variables to zero.
u
k=1,2,...,K
i
= (?
U
k
I + ?
C
T
2
k
I + S
T
D
k
q
i
S)
?1
? (?
C
u
i
T
k
? (
K
?
p6=k
T
p
u
p
i
)?
C
T
k
+ S
T
D
k
q
i
Q
k
i
) (8)
u
i
= (?
U
I + ?
C
I + V
T
D
c
i
V )
?1
? (V
T
D
c
i
R
i
+ ?
C
K
?
k
T
k
u
k
i
) (9)
v
j
= (?
V
I + U
T
D
c
j
U)
?1
? (U
T
D
c
j
R
j
+ ?
V
?
j
) (10)
s
m
= (?
S
I +
K
?
k=1
U
T
k
D
k
q
m
U
k
)
?1
? (
K
?
k=1
U
T
k
D
k
q
m
Q
k
m
) (11)
where K is the total number of graphs. I is an identity matrix of the same dimension as that of latent
space. U and V are matrices with rows as latent users and latent items, respectively. S is a matrix with
rows as the shared social factor-specific latent feature vectors for all graphs. T is the graph selection
weight,
?
K
k=1
T
k
= 1, T
k
>= 0. R
i
= [r
i1
, . . . , r
iJ
]
T
and R
j
= [r
1j
, . . . , r
Ij
]
T
. For graph k, Q
k
i
=
[q
k
i1
, . . . , q
k
iM
]
T
, Q
k
m
= [q
k
1m
, . . . , q
k
Im
]
T
and U
k
is a matrix with rows as the social graph k specific latent
user vectors. Likewise, D
k
q
i
, and D
k
q
m
are similarly defined with diagonal elements c
k,q
i.
and c
k,q
.m
,
respectively. D
c
i
= diag(c
i1
, . . . , c
iJ
) and D
c
j
= diag(c
1j
, . . . , c
Ij
). In addition, c
ij
and c
k,q
im
are also
seen as the confidence parameters for r
ij
and q
k
im
, respectively. The high confidence value a is set to the
observed interactive pairs and the low confidence value b is set to the unobserved interactive pairs, where
a > b > 0.
For our brevity, the remaining update rules for ? and ?, can be obtained using the same way as de-
scribed in CTR (Wang and Blei, 2011). Please see that for details.
3.4 Prediction
Using the learned parameters above, we can make in-matrix and out-of-matrix predictions defined in
Wang and Blei (2011). For in-matrix prediction, it refers to the case where those items have been rated
by at least one user in the system. To compute predicted rating, we use
r
?
ij
? (u
?
i
)
T
v
?
j
. (12)
For out-of-matrix prediction, it refers to the case where those items have never been rated by any user in
the system. To compute predicted rating, we use
r
?
ij
? (u
?
i
)
T
?
?
j
, (13)
where the corresponding ?
?
j
is defined as topic proportion in Section 3.2 and 3.3.
238
3.5 Computational Issue
To reduce computational costs when updating u
i
, v
j
and other variables with similar structure in update
rule, we adopt the same strategy of matrix operation shown in Hu et al. (2008). Specifically, directly
computing V
T
D
c
i
V and U
T
D
c
j
U requires time O(L
2
J) and O(L
2
I) for each user and item, where J
and I are the total number of items and users respectively, L is the dimension of latent representation
space. Instead, we rewrite U
T
D
c
j
U = U
T
(D
c
j
? bI)U + bU
T
U . Then, bU
T
U can be pre-computed
and D
c
j
? bI has only I
r
non-zeros elements, where I
r
refers to the number of users who rated item j
and empirically I
r
 I . For other similar structures, i.e., V
T
D
c
i
V , S
T
D
k
q
i
S, and so on, they are similar.
Therefore, we can significantly speed up computation by this sparsity property.
4 Experiments
4.1 Data
We evaluate our proposed method on real life dataset
1
from LastFm. LastFm
2
is an online music
catalogue, powered by social music discovery service for personalized recommendation. This dataset
(Cantador et al., 2011) is challenging. Though it contains 92,834 pairs of observed ratings with 1892
users and 17,632 items, the sparseness is quite low, i.e., merely 0.2783% , which is much lower than
that of the well-known Movielens dataset with the sparseness 4.25%. On average, each user has 44.21
items in the play list, ranging from 0 to 50, and each item appears in 4.95 users libraries, ranging from 0
to 611. For each item, the tag information is used as bag-of-word representation. After text processing,
11,946 distinct words are remained in the corpus. In addition, we further remove noisy users which
have no items. We also construct two additional social graphs for our experiments. One is user-tag
network extracted from user-tag-item relations in original dataset. The other is user-user network through
transforming the constructed user-tag network. The relation in all graphs is binary, i.e., the available
denoted as 1 and the unavailable denoted as 0.
Table 1: Original dataset description
Dataset users items tags user-user relations user-tags-items user-items relations
LastFm 1892 17632 11946 25434 186479 92834
4.2 Metrics
Two metrics for evaluating the recommendation performance are employed, i.e., Recall and NDCG.
Measure for plain relevance:
Recall@k =
]relevance
k
, (14)
where ]relevance denotes the total relevant papers in returned top-k result. Measure for ranking-based
relevance:
NDCG@k =
?
k
i=1
2
rel
i
?1
log
2
(1+i)
IDCG
, (15)
where rel
i
denotes the relevant degree which is binary in our task and IDCG is the optimal score
computed using the same form in numerator but with optimal ranking known in advance.
4.3 Experimental Design
In this paper, we expect the proposed model ?Our-Homo? in Section 3.2 and ?Our-Heter? in Section 3.3
can jointly provide a general and systematic solution to handling the following cases of using multiple
graphs for recommendation:
? Case 1:(Heterogeneous networks with noise) Network data or the extraction process is usually
imprecise or noisy in practice. Transform it into homogeneous case and then use ?Our-Homo?.
1
Data available at http://grouplens.org/datasets/hetrec-2011/
2
http://www.last.fm/
239
? Case 2:(Homogeneous networks) ?Our-Homo? can be directly employed as the tool for case 1.
? Case 3:(Heterogeneous networks with high quality) ?Our-Heter? might be directly employed. It is
not needed to be further transformed into Homogeneous case.
The detail experiments in the following sections are presented to justify the effectiveness of our methods
for the three cases above.
4.4 Experiments for Case 1 and Case 2
In this section, we mainly focus on the most complex and common case 1 with case 2 in practice.
4.4.1 Baselines
We compare our proposed two models, the model in Section 3.2 denoted as Our-Heter and the model in
Section 3.3 denoted as Our-Homo, with some state-of-the-art algorithms.
? CTR: This method, described in Wang and Blei (2011), combines both item content information
and user-item ratings for CF.
? PMF: This method, described in Salakhutdinov and Mnih (2007), is a well-known matrix factor-
ization method for CF, only using interactive rating information.
? SMF-1: This method, described in Purushotham et al. (2012), exploits single user?s social network
structure combined with item?s content information for CF. SMF-1 denotes using our extracted
user-tag relation.
? SMF-2: The same SMF method, described in (Purushotham et al., 2012). SMF-2 denotes using
original user-user relation.
? Our-Heter: Our model for heterogeneous networks, proposed in Section 3.2, uses our extracted
user-tag network and original user-user network.
? Our-Homo: Our model for homogeneous networks, proposed in Section 3.3, uses two homoge-
neous networks, i.e., 1) the transformed user-user network through our extracted user-tag relation,
and 2) original user-user network.
4.4.2 Settings
For a fair comparison, we use the similar settings as prior work in Purushotham et al. (2012). Specifically,
to well judge the influence of multiple social network structures, we fix the effects of content information
to the same level that is optimal in SMF, ?
v
= 0.1. We randomly split the dataset into two parts,
training (90%) and test datasets (10%), with constraint that users in test dataset have more than half
of the average number of rated items, i.e., 20. This expands the range of performance analysis for
our evaluation compared with Purushotham et al. (2012). The optimal parameters are obtained on a
small held-out dataset. For PMF, we set ?
v
= 100, ?
u
= 0.01. For all CTR-based methods, we set
a = 1, b = 0.01, ?
v
= 0.1. Specifically, for CTR, we set ?
u
= 0.01. For SMF-1 and SMF-2, we set
?
u
= 0.01. For Our-Homo, we set ?
u
= 0.01, ?
u1
= ?
u2
= ?
s
= 100, ?
c
= 0.01. For Our-Heter, we
set ?
u
= 0.01, ?
s1
= ?
s2
= 100. The remaining paramters are varied for experiment analysis.
It is noted that the task of out-of-matrix prediction is originally designed for evaluating item content
modeling in CTR rather than user social graphs as in CTR-smf. Thus, we followed the same setting in
baseline CTR-smf (Purushotham et al., 2012), not considering this task.
4.4.3 Performance Comparison with State-of-the-Art Methods
Figure 2 shows the recall and NDCG results of all the methods when the number of latent factor is fixed to
200 (optimal for the baselines). The proposed model ?Our-Homo? consistently outperforms the baselines
and ?Our-Heter? model under both recall and NDCG measures. This finding demonstrates that (1) using
multiple graphs for CTR is a necessary for improving recommendation performance from both ranking
and plain accuracy perspectives. (2) strong consensus for modeling shared information undermines the
performance for multiple graphs factorization as designed in Our-Heter. (3) For heterogeneous case, we
address that through simply transforming the heterogeneous network to homogeneous one and then use
240
Figure 2: Our model comparison with the state-of-the-art methods, for Recall and NDCG.
Figure 3: Performance comparison for different latent factors (K=200,300,400) @ top (50,100,200).
?Our-Homo?. This is natural but the opposite is hard. Thus, our solution ?Our-Homo? for modeling weak
consensus is effective for both homogeneous and heterogeneous cases.
In addition, we can see that CTR-smf (Purushotham et al., 2012) is sensitive to the quality of graph
(SMF-1 with low quality and SMF-2 with high quality as shown in Figure 2). In contrast, we can use the
low quality noisy graph (SMF-1) to improve the overall performance by this transformation process. In
fact, why Our-Heter does not perform well is mainly due to the noisy graph-1. ?Transformation? can be
seen as a ?denoising? process.
4.4.4 Performance Analysis with Different Latent Factors
Figure 3 shows the results of the compared algorithms, with different number of latent factors for varied
top recommended item. It shows that K = 200 factors is optimal for all baselines compared with other
choices of the number of latent factors. This justifies our fire choice of 200 latent factors reported in
Figure 3 (Other factor choices are omitted here due to page constraint, which is not optimal for our
baselines) and suggests that the choice of latent factor number is crucial for all algorithms especially
for PMF. In contrast, the proposed ?Our-Homo? is more stable compared with PMF and outperforms the
other baselines in an overall performance as reported in Figure 2.
241
Figure 4: Parameter analysis of graph selection for our weak consensus modeling, i.e., Our-Homo.
4.4.5 Impact of the Parameter for Graph Selection
Next, we examine how our algorithm ?Our-Homo? is influenced by the graph-selection weights. In Figure
4, the horizontal axis shows the graph proportion weight T
1
for graph-1 (user-tag network). In our case,
two social graphs are considered. The weight for second graph is T
2
= 1 ? T
1
, which is not shown
in Figure 4. Figure 4 clearly proves the effectiveness of our weak consensus modeling. Specifically,
we can see that 0.0 seemingly means that the first graph is not selected due to the weight of the first
graph T
1
= 0.0 and that of second graph T
2
= 1 (fully selected). However, due to our weak consensus
modeling scheme, though the graph-1 weight is 0.0, it does not mean that the first graph is removed. In
fact, the effect of graph-1 is also active through the shared variable ?s? in Figure 1 (right). This further
can be investigated from the Equation 9. Apparently, the case of 0.0 weight for graph-1, u is only relevant
to u
1
, but from Equation 8, we can see that u
1
is also influenced by u
2
via the shared social graph factors
s. Thus, this also explains why 0.0 weight for graph-1 is not equal to the result of SMF-2 as shown in
Figure 2, which only uses graph-2, original user-user graph in SMF-2.
In addition, the ?valley? in Figure 4 might be explained that in the extreme cases (0.0 and 1.0), the
denoising effect of weak consensus is slightly strengthened because only one specific graph (higher
quality smf-2 or lower quality smf-1 shown in Fig.2) is directly associated with final latent user combined
with shared variable. Therefore, the extreme case (1.0) is towards a relative higher performance.
4.5 Experiments for Case 3
Though the proposed ?Our-Homo? is more effective than CTR and CTR-smf, it does not mean that the
proposed another method ?Our-Heter? is useless. In this section, we show how the case 3 will be justified.
4.5.1 Baselines
? Our-Heter(N): Our model for heterogeneous networks, proposed in Section 3.2, uses our modified
high quality user-tag network as described in Section 4.5.2 and original user-user social network as
SMF-2 in Section 4.4.1.
? SMF-1(N): The same SMF method with single social network, described in Purushotham et al.
(2012). SMF-1 (N) denotes using our modified high quality user-tag network as described in Section
4.5.2.
? Our-Homo(O): The result of this method is the same as that reported in Section 4.4.3.
? CTR(O): The result of this method (Wang and Blei, 2011) is the same as that reported in Section
4.4.3.
4.5.2 Settings
We want to investigate whether Our-Heter will outperform Our-Homo in the case where the heteroge-
neous networks with less noise are available, compared with previous results in Figure 2. The settings
242
Figure 5: Our model comparison with the state-of-the-art methods (case 3), for Recall and NDCG.
in case 3 are the same as that in Section 4.4.2 except for the refined user-tag graph. In this case, we con-
struct a less noisy user-tag network by selecting top-10% tags according to tf ? idf value. The optimal
latent factor number is set to 100 for Our-Heter (N) through a small held-out dataset. The remaining
parameters are kept as the same values in Section 4.4.2. For notations in baselines Section, ?O? denotes
old setting and ?N? denotes new setting with updated user-tag network presented in this section.
4.5.3 Performance Comparison with State-of-the-Art Methods
Figure 5 shows Our-Heter (N) can achieve improved performance compared with baselines without trans-
formation, for the case where high quality graphs are available. Specifically, for recall measure, Our-
Heter (N) produces the best result with the increasing number of top recommended items. In addition,
we observe that modeling multiple graphs is necessary to further improve recommending performance,
while multiple high quality heterogeneous graphs are available.
For NDCG measure, Our-Heter (N) is comparable to our baselines. Since recall measure is only
considered for several reasons in previous work (Wang and Blei, 2011; Purushotham et al., 2012), ND-
CG is introduced as a plus compared with primarily focused recall. Therefore, Our-Heter (N) is also
competitive in overall performance in case 3.
In fact, as discussed in Kang and Lerman (2013), CTR-smf (Purushotham et al., 2012) is not always
superior to CTR (Wang and Blei, 2011) and vice versa due to different contexts. Likewise, our model is
under the multi-view assumption as discussed in Section 1 that should be checked in practice.
5 Conclusions
In this paper, we propose a general recommendation framework with multiple data sources based on
CTR. It is a principled hierarchy Bayesian framework with multiple social graphs factorization for rec-
ommender systems. In this framework, two ways of consensus modeling are exploited. Specifically,
the proposed models Our-Homo and Our-Heter can jointly provide a general and systematic solution to
handling three real cases of using multiple graphs with item content information for recommendation:
case 1) Heterogeneous networks with noise; case 2) Homogeneous networks; case 3) Heterogeneous
networks with high quality. Experimental results on real dataset demonstrate the effectiveness of our ap-
proach. While this framework is used for modeling multiple user social graphs, it can be easily extended
to exploiting other side information such as multiple complex relations for items in various applications.
Acknowledgements
We thank the anonymous reviewers for their valuable suggestions. This research was partly supported by
National Natural Science Foundation of China (No.61370117,61333018), Major National Social Science
Fund of China (No.12&ZD227), and National High Technology Research and Development Program of
China (863 Program) (No.2012AA011101).
243
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Iv?an Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. 2nd workshop on information heterogeneity and fusion
in recommender systems (hetrec 2011). In Proceedings of the 5th ACM Conference on Recommender Systems,
RecSys 2011, New York, NY, USA. ACM.
Xuetao Ding, Xiaoming Jin, Yujia Li, and Lianghao Li. 2013. Celebrity recommendation with collaborative social
topic regression. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence,
IJCAI, pages 2612?2618. AAAI Press.
Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In
Eighth IEEE International Conference on Data Mining, ICDM, pages 263?272. IEEE.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo Deng, Dong Wang, Hieu Khac Le, Tarek F Abdelzaher,
Jiawei Han, Alice Leung, John Hancock, et al. 2012. Tweet ranking based on heterogeneous networks. In
Proceedings of International Conference on Computational Linguistics, COLING, pages 1239?1256.
Jeon-Hyung Kang and Kristina Lerman. 2013. La-ctr: A limited attention collaborative topic regression for social
media. In Proceedings of AAAI Conference on Artificial Intelligence, AAAI, pages 119?125.
Abhishek Kumar, Piyush Rai, and Hal Daum?e III. 2011. Co-regularized multi-view spectral clustering. In
Advances in Neural Information Processing Systems, NIPS, pages 1413?1421.
Yingming Li, Ming Yang, and Zhongfei Mark Zhang. 2013. Scientific articles recommendation. In Proceedings
of the 22nd ACM International Conference on Conference on Information & Knowledge Management, CIKM,
pages 1147?1156. ACM.
Jialu Liu, Chi Wang, Jing Gao, and Jiawei Han. 2013. Multi-view clustering via joint nonnegative matrix factor-
ization. In Proceedings of SIAM Data Mining Conference, SDM. SIAM.
Sanjay Purushotham, Yan Liu, and C-c J Kuo. 2012. Collaborative topic regression with social matrix factorization
for recommendation systems. In Proceedings of the 29th International Conference on Machine Learning, ICML,
pages 759?766.
Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic matrix factorization. In Advances in Neural Infor-
mation Processing Systems, NIPS, pages 1257?1264.
Chong Wang and David M Blei. 2011. Collaborative topic modeling for recommending scientific articles. In
Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD, pages 448?456. ACM.
Hao Wang, Binyi Chen, and Wu-Jun Li. 2013. Collaborative topic regression with social regularization for tag
recommendation. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence,
IJCAI, pages 2719?2725. AAAI Press.
244
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 257?268, Dublin, Ireland, August 23-29 2014.
Multi-view Chinese Treebanking
Likun Qiu
1,2,3
, Yue Zhang
1
, Peng Jin
4
and Houfeng Wang
2
1
Singapore University of Technology and Design, Singapore
2
Institute of Computational Linguistics, Peking University, China
3
School of Chinese Language and Literature, Ludong University, China
4
Lab of Intelligent Information Processing and Application, Leshan Normal University, China
{qiulikun,jandp,wanghf}@pku.edu.cn, yue zhang@sutd.edu.sg
Abstract
We present a multi-view annotation framework for Chinese treebanking, which uses dependen-
cy structures as the base view and supports conversion into phrase structures with minimal loss
of information. A multi-view Chinese treebank was built under the proposed framework, and
the first release (PMT 1.0) containing 14,463 sentences is be made freely available. To verify
the effectiveness of the multi-view framework, we implemented an arc-standard transition-based
dependency parser and added phrase structure features produced by the phrase structure view.
Experimental results show the effectiveness of additional features for dependency parsing. Fur-
ther, experiments on dependency-to-string machine translation show that our treebank and parser
could achieve similar results compared to the Stanford Parser trained on CTB 7.0.
1 Introduction
Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms
for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and
Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads
and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers
have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong,
2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al.,
2013), showing that the two types of information complement each other for NLP tasks.
Most existing Chinese and English treebanks fall into the phrase structure category, and much work
has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and
Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on
statistical dependency parsing has frequently used dependency treebanks converted from phrase structure
treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB)
(Xue et al., 2000). However, previous research shows that dependency categories in converted tree-
banks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion
approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue,
2007). The main reason is that the PS treebanks were designed without consideration of DS conversion,
leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks. To
minimize information loss during treebank conversions, a treebank could be designed by considering PS
and DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia et
al., 2009). We develop a multi-view treebank for Chinese, which treats PS and DS as different views of
the same internal structures of a sentence.
We choose the DS view as the base view, from which PS would be derived. Our choice is based on the
effectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt and
Xia, 2012). Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasal
category of a constituent can be derived from the phrasal categories of its immediate subconstituents and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
257
PKU POS Our POS
Ag, a, ad, ia, ja, la a (adjective)
Bg,b, ib, jb, jm, lb b (distinguishing words)
Dg, d, dc, df, id, jd, ld d (adverb)
m, mq m(number)
n, an, in, jn, ln, Ng, vn, nr, kn n (noun)
Qg,q, qb, qc, qd, qe, qj, ql, qr, qt, qv, qz q (measure word)
Rg,r, rr, ry, ryw, rz, rzw r (pronoun)
Tg, t, tt t (temporal noun)
u, ud, ue, ui, ul, uo, us, uz, Ug u (auxiliary word)
v, iv, im, jv, lv, Vg, vd, vi, vl, vq,vu, vx, vt,kv v (verb)
w, wd, wf, wj, wk, wky, wkz, wm,wp, ws, wt, wu, ww, wy, wyy, wyz w (punctuation)
Table 1: Mapping from PKU POS to our POS.
the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal cat-
egories). Consequently, in Chinese, the canonical PS, containing information of constituent hierarchies
and phrasal categories, can be derived naturally from the canonical DS. As Xia et al. (2009) stated, a rich
set of dependency categories should be designed to ensure lossless conversion from DS to PS. When the
information of PS has been represented in DS explicitly or implicitly, we can convert DS to PS without
ambiguity (Rambow et al., 2002).
Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words,
is constructed. This main corpus is based on the Peking University People?s Daily Corpus. We name our
treebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0. To verify the useful-
ness of the treebank for statistical NLP, a transition-based dependency parser is implemented to include
PS features produced in the derivation process of phrasal categories. We perform a set of empirical
evaluations, with experimental results on both dependency parsing and dependency-to-string machine
translation showing the effectiveness of the proposed annotation framework and treebank. We make the
treebank, the DS to PS conversion script and the parser freely available.
2 Annotation Framework
2.1 Part-of-speech Tagset
Our part-of-speech (POS) tagset is based on the Peking University (PKU) People?s Daily corpus, which
consists of over 100 tags (Yu et al., 2003). We simplify the PKU tagset by syntactic distribution. The
simplified tagset contains 33 POS tags. The mapping from the original PKU POS to our simplified POS is
shown in Table 1. For instance, Ag (adjective morpheme), ad (adjective acting as an adverb), ia (adjective
idioms), ja (adjective abbreviation) and la (temporary phrase acting as an adjective) are all mapped to one
tag a (adjective). A set of basic PKU POS tags, including c (conjunction), e (interjection), f (localizer),
g (morpheme), h (prefix), i (idiom), j (abbreviation), k (suffix), l (temporary phrase), nr (personal name),
nrf (family name), nrg (surname), ns (toponym), nt (organization name), nx (non-Chinese noun), nz
(other proper noun), o (onomonopeia), p (preposition), q (measure word), r (pronoun), s (locative), x
(other non-Chinese word), y (sentence final particle), z (state adjective), are left unchanged.
2.2 Dependency Category Tagset
In a DS, the modifier is tagged with a dependency category, which denotes the role the modifier plays
with regard to its head. The root word of a sentence is dependent on a virtual root node R and tagged
with the dependency category HED. Table 2 lists the 32 dependency categories used in our annotation
guideline. These categories are designed in consideration of PS conversion with minimal ambiguities,
and can be classified according to the following criteria:
(1) whether the head dominates a compound clause (i.e. has an IC modifier) in the PS view. Accord-
ing to this, dependency categories can be cross-clause or in-clause. For instance, in Figure 1, the last
punctuation (") is labeled with the cross-clause tag PUS, and its head dominates an IC modifier. (2) the
relative position of the modifier to the head. According to this, dependency categories can be left, right
or free. For instance, the LAD, SBV, ADV, COS, DE and ATT labels in Figure 1 are all left. The VOB label
258
Tag Description Tag Description
ACT action object LAD left additive
ADV adverbial MT modality and time
APP appositive element NUM number
ATT attribute POB propositional object
CMP complement PUN punctuation
COO other coordination element PUS cross-clause punctuation
COS share-right-child coordination element QUC post-positional quantity
DE de (modifier of(special function word)) QUCC non-shared post-positional quantity
DEI dei (modifier ofProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1245?1254, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Learning for Coreference Resolution with Markov Logic
Yang Song1, Jing Jiang2, Wayne Xin Zhao3, Sujian Li1, Houfeng Wang1
1Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
2School of Information Systems, Singapore Management University, Singapore
3School of Electronics Engineering and Computer Science, Peking University, China
{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.com
Abstract
Pairwise coreference resolution models must
merge pairwise coreference decisions to gen-
erate final outputs. Traditional merging meth-
ods adopt different strategies such as the best-
first method and enforcing the transitivity con-
straint, but most of these methods are used
independently of the pairwise learning meth-
ods as an isolated inference procedure at the
end. We propose a joint learning model which
combines pairwise classification and mention
clustering with Markov logic. Experimen-
tal results show that our joint learning sys-
tem outperforms independent learning sys-
tems. Our system gives a better performance
than all the learning-based systems from the
CoNLL-2011 shared task on the same dataset.
Compared with the best system from CoNLL-
2011, which employs a rule-based method,
our system shows competitive performance.
1 Introduction
The task of noun phrase coreference resolution is to
determine which mentions in a text refer to the same
real-world entity. Many methods have been pro-
posed for this problem. Among them the mention-
pair model (McCarthy and Lehnert, 1995) is one of
the most influential ones and can achieve the state-
of-the-art performance (Bengtson and Roth, 2008).
The mention-pair model splits the task into three
parts: mention detection, pairwise classification and
mention clustering. Mention detection aims to iden-
tify anaphoric noun phrases, including proper nouns,
common noun phrases and pronouns. Pairwise clas-
sification takes a pair of detected anaphoric noun
phrase candidates and determines whether they re-
fer to the same entity. Because these classification
decisions are local, they do not guarantee that can-
didate mentions are partitioned into clusters. There-
fore a mention clustering step is needed to resolve
conflicts and generate the final mention clusters.
Much work has been done following the mention-
pair model (Soon et al 2001; Ng and Cardie, 2002).
In most work, pairwise classification and mention
clustering are done sequentially. A major weak-
ness of this approach is that pairwise classification
considers only local information, which may not be
sufficient to make correct decisions. One way to
address this weakness is to jointly learn the pair-
wise classification model and the mention cluster-
ing model. This idea has been explored to some
extent by McCallum and Wellner (2005) using con-
ditional undirected graphical models and by Finley
and Joachims (2005) using an SVM-based super-
vised clustering method.
In this paper, we study how to use a different
learning framework, Markov logic (Richardson and
Domingos, 2006), to learn a joint model for both
pairwise classification and mention clustering un-
der the mention-pair model. We choose Markov
logic because of its appealing properties. Markov
logic is based on first-order logic, which makes
the learned models readily interpretable by humans.
Moreover, joint learning is natural under the Markov
logic framework, with local pairwise classification
and global mention clustering both formulated as
weighted first-order clauses. In fact, Markov logic
has been previously used by Poon and Domingos
(2008) for coreference resolution and achieved good
1245
results, but it was used for unsupervised coreference
resolution and the method was based on a different
model, the entity-mention model.
More specifically, to combine mention cluster-
ing with pairwise classification, we adopt the com-
monly used strategies (such as best-first clustering
and transitivity constraint), and formulate them as
first-order logic formulas under the Markov logic
framework. Best-first clustering has been previously
studied by Ng and Cardie (2002) and Bengtson and
Roth (2008) and found to be effective. Transitivity
constraint has been applied to coreference resolution
by Klenner (2007) and Finkel and Manning (2008),
and also achieved good performance.
We evaluate Markov logic-based method on the
dataset from CoNLL-2011 shared task. Our ex-
periment results demonstrate the advantage of joint
learning of pairwise classification and mention clus-
tering over independent learning. We examine
best-first clustering and transitivity constraint in our
methods, and find that both are very useful for coref-
erence resolution. Compared with the state of the
art, our method outperforms a baseline that repre-
sents a typical system using the mention-pair model.
Our method is also better than all learning systems
from the CoNLL-2011 shared task based on the re-
ported performance. Even with the top system from
CoNLL-2011, our performance is still competitive.
In the rest of this paper, we first describe a stan-
dard pairwise coreference resolution system in Sec-
tion 2. We then present our Markov logic model for
pairwise coreference resolution in Section 3. Exper-
imental results are given in Section 4. Finally we
discuss related work in Section 5 and conclude in
Section 6.
2 Standard Pairwise Coreference
Resolution
In this section, we describe standard learning-based
framework for pairwise coreference resolution. The
major steps include mention detection, pairwise
classification and mention clustering.
2.1 Mention Detection
For mention detection, traditional methods include
learning-based and rule-based methods. Which kind
of method to choose depends on specific dataset. In
this paper, we first consider all the noun phrases
in the given text as candidate mentions. With-
out gold standard mention boundaries, we use a
well-known preprocessing tool from Stanford?s NLP
group1 to extract noun phrases. After obtaining all
the extracted noun phrases, we also use a rule-based
method to remove some erroneous candidates based
on previous studies (e.g. Lee et al(2011), Uryupina
et al(2011)). Some examples of these erroneous
candidates include stop words (e.g. uh, hmm), web
addresses (e.g. http://www.google.com),
numbers (e.g. $9,000) and pleonastic ?it? pronouns.
2.2 Pairwise Classification
For pairwise classification, traditional learning-
based methods usually adopt a classification model
such as maximum entropy models and support vec-
tor machines. Training instances (i.e. positive and
negative mention pairs) are constructed from known
coreference chains, and features are defined to rep-
resent these instances.
In this paper, we build a baseline system that uses
maximum entropy models as the classification algo-
rithm. For generation of training instances, we fol-
low the method of Bengtson and Roth (2008). For
each predicted mention m, we generate a positive
mention pair between m and its closest preceding
antecedent, and negative mention pairs by pairing m
with each of its preceding predicted mentions which
are not coreferential with m. To avoid having too
many negative instances, we impose a maximum
sentence distance between the two mentions when
constructing mention pairs. This is based on the in-
tuition that for each anaphoric mention, its preced-
ing antecedent should appear quite near it, and most
coreferential mention pairs which have a long sen-
tence distance can be resolved using string match-
ing. During the testing phase, we generate men-
tion pairs for each mention candidate with each of
its preceding mention candidates and use the learned
model to make coreference decisions for these men-
tion pairs. We also impose the sentence distance
constraint and use string matching for mention pairs
with a sentence distance exceeding the threshold.
1http://nlp.stanford.edu/software/corenlp.shtml
1246
2.3 Mention Clustering
After obtaining the coreferential results for all men-
tion pairs, some clustering method should be used to
generate the final output. One strategy is the single-
link method, which links all the mention pairs that
have a prediction probability higher than a threshold
value. Two other alternative methods are the best-
first clustering method and clustering with the tran-
sitivity constraint. Best-first clustering means that
for each candidate mention m, we select the best
one from all its preceding candidate mentions based
on the prediction probabilities. A threshold value
is given to filter out those mention pairs that have a
low probability to be coreferential. Transitivity con-
straint means that if a and b are coreferential and
b and c are coreferential, then a and c must also be
coreferential. Previous work has found that best-first
clustering and transitivity constraint-based cluster-
ing are better than the single-link method. Finally
we remove all the singleton mentions.
3 Markov Logic for Pairwise Coreference
Resolution
In this section, we present our method for joint
learning of pairwise classification and mention clus-
tering using Markov logic. For mention detection,
training instance generation and postprocessing, our
method follows the same procedures as described in
Section 2. In what follows, we will first describe
the basic Markov logic networks (MLN) framework,
and then introduce the first-order logic formulas we
use in our MLN including local formulas and global
formulas which perform pairwise classification and
mention clustering respectively. Through this way,
these two isolated parts are combined together, and
joint learning and inference can be performed in a
single framework. Finally we present inference and
parameter learning methods.
3.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic (Richardson and Domingos,
2006; Riedel, 2008). A Markov logic network con-
sists of a set of first-order clauses (which we will re-
fer to as formulas in the rest of the paper) just like in
first-order logic. However, different from first-order
logic where a formula represents a hard constraint,
in an MLN, these constraints are softened and they
can be violated with some penalty. An MLN M
is therefore a set of weighted formulas {(?i, wi)}i,
where ?i is a first order formula andwi is the penalty
(the formula?s weight). These weighted formulas
define a probability distribution over sets of ground
atoms or so-called possible worlds. Let y denote a
possible world, then we define p(y) as follows:
p(y) = 1
Z
exp
(
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
)
. (1)
Here each c is a binding of free variables in ?i to
constants. Each f?ic represents a binary feature func-
tion that returns 1 if the ground formula we get by
replacing the free variables in ?i with the constants
in c under the given possible world y is true, and 0
otherwise. n?i denotes the number of free variables
of a formula ?i. Cn?i is the set of all bindings for the
free variables in ?i. Z is a normalization constant.
This distribution corresponds to a Markov network
where nodes represent ground atoms and factors rep-
resent ground formulas.
Each formula consists of a set of first-order predi-
cates, logical connectors and variables. Take the fol-
lowing formula as one example:
(?i, wi) : headMatch(a, b)?(a ?= b) ? coref (a, b).
The formula above indicates that if two different
candidate mentions a and b have the same head
word, then they are coreferential. Here a and b are
variables which can represent any candidate men-
tion, headMatch and coref are observed predicate
and hidden predicate respectively. An observed
predicate is one whose value is known from the ob-
servations when its free variables are assigned some
constants. A hidden predicate is one whose value is
not known from the observations. From this exam-
ple, we can see that headMatch is an observed pred-
icate because we can check whether two candidate
mentions have the same head word. coref is a hid-
den predicate because this is something we would
like to predict.
3.2 Formulas
We use two kinds of formulas for pairwise classi-
fication and mention clustering, respectively. For
1247
describing the attributes ofmi
mentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).
entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...
genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.
numberType(i,n) mi has number type SINGULAR, PLURAL or UN.
hasHead(i,h) mi has head word h, here h can represent all possible head words.
firstMention(i) mi is the first mention in its sentence.
reflexive(i) mi is reflexive.
possessive(i) mi is possessive.
definite(i) mi is definite noun phrase.
indefinite(i) mi is indefinite noun phrase.
demonstrative(i) mi is demonstrative.
describing the attributes of relations betweenmj andmi
mentionDistance(j,i,m) Distance between mj and mi in mentions.
sentenceDistance(j,i,s) Distance between mj and mi in sentences.
bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NO
and AGREE UN).
closestMatch(j,i,c) mj is the first agreement in number and gender when looking backward
from mi: CAGREE YES, CAGREE NO and CAGREE UN.
exactStrMatch(j,i) Exact strings match between mj and mi.
pronounStrMatch(j,i) Both are pronouns and their strings match.
nopronounStrMatch(j,i) Both are not pronouns and their strings match.
properStrMatch(j,i) Both are proper names and their strings match.
headMatch(j,i) Head word strings match between mj and mi.
subStrMatch(j,i) Sub-word strings match between mj and mi.
animacyMatch(j,i) Animacy types match between mj and mi.
nested(j,i) mj/i is included in mi/j .
c command(j,i) mj/i C-Commands mi/j .
sameSpeaker(j,i) mj and mi have the same speaker.
entityTypeMatch(j,i) Entity types match between mj and mi.
alias(j,i) mj/i is an alias of mi/j .
srlMatch(j,i) mj and mi have the same semantic role.
verbMatch(j,i) mj and mi have semantic role for the same verb.
Table 1: Observed predicates.
pairwise classification, because the decisions are lo-
cal, we use a set of local formulas. For mention
clustering, we use global formulas to implement
best-first clustering or transitivity constraint. We
naturally combine pairwise classification with men-
tion clustering via local and global formulas in the
Markov logic framework, which is the essence of
?joint learning? in our work.
3.2.1 Local Formulas
A local formula relates any observed predicates to
exactly one hidden predicate. For our problem, we
define a list of observed predicates to describe the
properties of individual candidate mentions and the
relations between two candidate mentions, shown in
Table 1. For our problem, we have only one hidden
predicate, i.e. coref. Most of our local formulas are
from existing work (e.g. Soon et al(2001), Ng and
Cardie (2002), Sapena et al(2011)). They are listed
in Table 2, where the symbol ?+? indicates that for
every value of the variable preceding ?+? there is a
separate weight for the corresponding formula.
3.2.2 Global Formulas
Global formulas are designed to add global con-
straints for hidden predicates. Since in our problem
there is only one hidden predicate, i.e. coref, our
global formulas incorporate correlations among dif-
ferent ground atoms of the coref predicates. Next we
will show the best-first and transitivity global con-
straints. Note that we treat them as hard constraints
so we do not set any weights for these global formu-
las.
1248
Lexical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? exactStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? pronounStrMatch (j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? properStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nopronounStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? headMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? subStrMatch(j,i) ? j ?= i ? coref(j,i)
hasHead(j,h1+) ? hasHead(i,h2+) ? j ?= i ? coref(j,i)
Grammatical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? genderType(j,g1+) ? genderType(i,g2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? numberType(j,n1+) ? numberType(i,n2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? bothMatch(j,i,b+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? closestMatch(j,i,c+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? animacyMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nested(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? c command(j,i) ? j ?= i ? coref(j,i)
(mentionType(j,t1+) ? mentionType(i,t2+)) ? j ?= i ? coref(j,i)
(reflexive(j) ? reflexive(i)) ? j ?= i ? coref(j,i)
(possessive(j) ? possessive(i)) ? j ?= i ? coref(j,i)
(definite(j) ? definite(i)) ? j ?= i ? coref(j,i)
(indefinite(j) ? indefinite(i)) ? j ?= i ? coref(j,i)
(demonstrative(j) ? demonstrative(i)) ? j ?= i ? coref(j,i)
Distance and position Features
mentionType(j,t1+) ? mentionType(i,t2+) ? sentenceDistance(j,i,s+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? mentionDistance (j,i,m+) ? j ?= i ? coref(j,i)
(firstMention(j) ? firstMention(i)) ? j ?= i ? coref(j,i)
Semantic Features
mentionType(j,t1+) ? mentionType(i,t2+) ? alias(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? sameSpeaker(j,i) ? j?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? entityTypeMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? srlMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? verbMatch(j,i) ? j ?= i ? coref(j,i)
(entityType(j,e1+) ? entityType(i,e2+)) ? j ?= i ? coref(j,i)
Table 2: Local Formulas.
Best-First constraint:
coref(j, i) ? ?coref(k, i) ?j, k < i(k ?= j) (2)
Here we assume that coref(j,i) returns true if can-
didate mentions j and i are coreferential and false
otherwise. Therefore for each candidate mention i,
we should only select at most one candidate mention
j to return true for the predicate coref(j,i) from all its
preceding candidate mentions.
Transitivity constraint:
coref(j, k)?coref(k, i)?j < k < i ? coref(j, i) (3)
coref(j, k)?coref(j, i)?j < k < i ? coref(k, i) (4)
coref(j, i)?coref(k, i)?j < k < i ? coref(j, k) (5)
With the transitivity constraint, it means for given
mentions j, k and i, if any two pairs of them are
coreferential, then the third pair of them should be
also coreferential.
We use best-first clustering and transitivity con-
straint in our joint learning model respectively. De-
tailed comparisons between them will be shown in
Section 4.
3.3 Inference
We use MAP inference which is implemented by In-
teger Linear Programming (ILP). Its objective is to
maximize a posteriori probability as follows. Here
we use x to represent all the observed ground atoms
and y to represent the hidden ground atoms. For-
mally, we have
y? = argmax
y
p(y|x) ? argmax
y
s(y, x),
where
s(y, x) =
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y, x). (6)
Each hidden ground atom can only takes a value of
either 0 or 1. And global formulas should be satis-
fied as hard constraints when inferring the best y?. So
1249
the problem can be easily solved using ILP. Detailed
introduction about transforming groundMarkov net-
works in Markov logic into an ILP problem can be
found in (Riedel, 2008).
3.4 Parameter Learning
For parameter learning, we employ the online
learner MIRA (Crammer and Singer, 2003), which
establishes a large margin between the score of the
gold solution and all wrong solutions to learn the
weights. This is achieved by solving the quadratic
program as follows
min ? wt ?wt?1 ? . (7)
s.t. s(yi, xi)? s(y?, xi) ? L(yi, y?)
?y? ?= yi, (yi, xi) ? D
Here D = {(yi, xi)}Ni=1 represents N training in-
stances (each instance represents one single docu-
ment in the dataset) and t represents the number of
iterations. In our problem, we adopt 1-best MIRA,
which means that in each iteration we try to find wt
which can guarantee the difference between the right
solution yi and the best solution y? (i.e. the one with
the highest score s(y?, xi), equivalent to y? in Section
3.3)) is at least as big as the loss L(yi, y?), while
changing wt?1 as little as possible. The number of
false ground atoms of coref predicate is selected as
loss function in our experiments. Hard global con-
straints (i.e. best-first clustering or transitivity con-
straint) must be satisfied when inferring the best y?
in each iteration, which can make learned weights
more effective.
4 Experiments
In this section, we will first describe the dataset and
evaluation metrics we use. We will then present the
effect of our joint learning method, and finally dis-
cuss the comparison with the state of the art.
4.1 Data Set
We use the dataset from the CoNLL-2011 shared
task, ?Modeling Unrestricted Coreference in
OntoNotes? (Pradhan et al 2011)2. It uses the En-
glish portion of the OntoNotes v4.0 corpus. There
are three important differences between OntoNotes
2http://conll.cemantix.org/2011/
and another well-known coreference dataset from
ACE. First, OntoNotes does not label any singleton
entity cluster, which has only one reference in the
text. Second, only identity coreference is tagged in
OntoNotes, but not appositives or predicate nomi-
natives. Third, ACE only considers mentions which
belong to ACE entity types, whereas OntoNotes
considers more entity types. The shared task is to
automatically identify both entity coreference and
event coreference, although we only focus on entity
coreference in this paper. We don?t assume that
gold standard mention boundaries are given. So we
develop a heuristic method for mention detection.
See details in Section 2.1.
The training set consists of 1674 documents from
newswire, magazine articles, broadcast news, broad-
cast conversations and webpages, and the develop-
ment set consists of 202 documents from the same
source. For training set, there are 101264 mentions
from 26612 entities. And for development set, there
are 14291 mentions from 3752 entities (Pradhan et
al., 2011).
4.2 Evaluation Metrics
We use the same evaluation metrics as used in
CoNLL-2011. Specifically, for mention detection,
we use precision, recall and the F-measure. A men-
tion is considered to be correct only if it matches
the exact same span of characters in the annotation
key. For coreference resolution, MUC (Vilain et al
1995), B-CUBED (Bagga and Baldwin, 1998) and
CEAF-E (Luo, 2005) are used for evaluation. The
unweighted average F score of them is used to com-
pare different systems.
4.3 The Effect of Joint Learning
To assess the performance of our method, we set up
several variations of our system to compare with the
joint learning system. The MLN-Local system uses
only the local formulas described in Table 2 with-
out any global constraints under the MLN frame-
work. By default, the MLN-Local system uses the
single-link method to generate clustering results.
The MLN-Local+BF system replaces the single-link
method with best-first clustering to infer mention
clustering results after learning the weights for all
the local formulas. The MLN-Local+Trans sys-
tem replaces the best-first clustering with transitivity
1250
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84
MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85
MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97
MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50
MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57
Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.
constraint. The MLN-Joint system is a joint model
for both pairwise classification and mention cluster-
ing. It can combine either best-first clustering or en-
forcing transitivity constraint with pairwise classifi-
cation, and we denote these two variants of MLN-
Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-
spectively.
To compare the performance of the various sys-
tems above, we use 10-fold cross validation on
the training dataset. We empirically find that our
method has a fast convergence rate, to learn the
MLN model, we set the number of iterations to be
10.
The performance of these compared systems is
shown in Table 3. To provide some context for
the performance of this task, we report the median
average F-score of the official results of CoNLL-
2011, which is 50.12 (Pradhan et al 2011). We can
see that MLN-Local achieves an average F-score of
56.84, which is well above the median score. When
adding best-first or transitivity constraint which
is independent of pairwise classification, MLN-
Local+BF and MLN-Local+Trans achieve better re-
sults of 57.85 and 57.97. Most of all, we can see
that the joint learning model (MLN-Joint(BF) or
MLN-Joint(Trans)) significantly outperforms inde-
pendent learning model (MLN-Local+BF or MLN-
Local+Trans) no matter whether best-first clustering
or transitivity constraint is used (based on a paired 2-
tailed t-test with p < 0.05) with the score of 58.50
or 58.57, which shows the effectiveness of our pro-
posed joint learning method.
Best-first clustering and transitivity constraint
are very useful in Markov logic framework, and
both MLN-Local and MLN-Joint benefit from them.
For MLN-Joint, these two clustering methods re-
sult in similar performance. But actually, transi-
tivity is harder than best-first, because it signifi-
cantly increases the number of formulas for con-
straints and slows down the learning process. In
our experiments, we find that MLN-Joint(Trans)3 is
much slower than MLN-Joint(BF). Overall, MLN-
Joint(BF) has a good trade-off between effectiveness
and efficiency.
4.4 Comparison with the State of the Art
In order to compare our method with the state-of-
the-art systems, we consider the following systems.
We implemented a traditional pairwise coreference
system using Maximum Entropy as the base classi-
fier and best-first clustering to link the results. We
used the same set of local features in MLN-Joint.
We refer to this system as MaxEnt+BF. To replace
best-first clustering with transitivity constraint, we
have another system named as MaxEnt+Trans. We
also consider the best 3 systems from CoNLL-2011
shared task. Chang?s system uses ILP to perform
best-first clustering after training a pairwise corefer-
ence model. Sapena?s system uses a relaxation label-
ing method to iteratively perform function optimiza-
tion for labeling each mention?s entity after learning
the weights for features under a C4.5 learner. Lee?s
system is a purely rule-based one. They use a battery
of sieves by precision (from highest to lowest) to it-
eratively choose antecedent for each mention. They
obtained the highest score in CoNLL-2011.
Table 4 shows the comparisons of our system with
the state-of-the-art systems on the development set
of CoNLL-2011. From the results, we can see that
our joint learning systems are obviously better than
3For MLN-Joint(Trans), not all training instances can be
learnt in a reasonable amount of time, so we set up a time out
threshold of 100 seconds. If the model cannot response in 100
seconds for some training instance, we remove it from the train-
ing set.
1251
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60
MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67
MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19
MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31
Lee?s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60
Sapena?s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61
Chang?s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35
Table 4: Comparisons with state-of-the-art systems on the development dataset.
MaxEnt+BF and MaxEnt+Trans. They also out-
perform the learning-based systems of Sapena et al
(2011) and Chang et al(2011), and perform com-
petitively with Lee?s system (Lee et al 2011). Note
that Lee?s system is purely rule-based, while our
methods are developed in a theoretically sound way,
i.e., Markov logic framework.
5 Related Work
Supervised noun phrase coreference resolution has
been extensively studied. Besides the mention-pair
model, two other commonly used models are the
entity-mention model (Luo et al 2004; Yang et al
2008) and ranking models (Denis and Baldridge,
2008; Rahman and Ng, 2009). Interested readers
can refer to the literature review by Ng (2010).
Under the mention-pair model, Klenner (2007)
and Finkel and Manning (2008) applied Integer Lin-
ear Programming (ILP) to enforce transitivity on the
pairwise classification results. Chang et al(2011)
used the same ILP technique to incorporate best-first
clustering and generate the mention clusters. In all
these studies, however, mention clustering is com-
bined with pairwise classification only at the infer-
ence stage but not at the learning stage.
To perform joint learning of pairwise classifi-
cation and mention clustering, in (McCallum and
Wellner, 2005), each mention pair corresponds to
a binary variable indicating whether the two men-
tions are coreferential, and the dependence between
these variables is modeled by conditional undirected
graphical models. Finley and Joachims (2005) pro-
posed a general SVM-based framework for super-
vised clustering that learns item-pair similarity mea-
sures, and applied the framework to noun phrase
coreference resolution. In our work, we take a differ-
ent approach and apply Markov logic. As we have
shown in Section 3, given the flexibility of Markov
logic, it is straightforward to perform joint learning
of pairwise classification and mention clustering.
In recent years, Markov logic has been widely
used in natural language processing problems (Poon
and Domingos, 2009; Yoshikawa et al 2009; Che
and Liu, 2010). For coreference resolution, the most
notable one is unsupervised coreference resolution
by Poon and Domingos (2008). Poon and Domin-
gos (2008) followed the entity-mention model while
we follow the mention-pair model, which are quite
different approaches. To seek good performance in
an unsupervised way, Poon and Domingos (2008)
highly rely on two important strong indicators:
appositives and predicate nominatives. However,
OntoNotes corpus (state-of-art NLP data collection)
on coreference layer for CoNLL-2011 has excluded
these two conditions of annotations (appositives and
predicate nominatives) from their judging guide-
lines. Compared with it, our methods are more ap-
plicable for real dataset. Huang et al(2009) used
Markov logic to predict coreference probabilities
for mention pairs followed by correlation cluster-
ing to generate the final results. Although they also
perform joint learning, at the inference stage, they
still make pairwise coreference decisions and clus-
ter mentions sequentially. Unlike their method, We
formulate the two steps into a single framework.
Besides combining pairwise classification and
mention clustering, there has also been some work
that jointly performs mention detection and coref-
erence resolution. Daume? and Marcu (2005) de-
veloped such a model based on the Learning as
1252
Search Optimization (LaSO) framework. Rahman
and Ng (2009) proposed to learn a cluster-ranker
for discourse-new mention detection jointly with
coreference resolution. Denis and Baldridge (2007)
adopted an Integer Linear Programming (ILP) for-
mulation for coreference resolution which models
anaphoricity and coreference as a joint task.
6 Conclusion
In this paper we present a joint learning method with
Markov logic which naturally combines pairwise
classification and mention clustering. Experimental
results show that the joint learning method signifi-
cantly outperforms baseline methods. Our method
is also better than all the learning-based systems in
CoNLL-2011 and reaches the same level of perfor-
mance with the best system.
In the future we will try to design more global
constraints and explore deeper relations between
training instances generation and mention cluster-
ing. We will also attempt to introduce more predi-
cates and transform structure learning techniques for
MLN into coreference problems.
Acknowledgments
Part of the work was done when the first author
was a visiting student in the Singapore Manage-
ment University. And this work was partially sup-
ported by the National High Technology Research
and Development Program of China(863 Program)
(No.2012AA011101), the National Natural Science
Foundation of China (No.91024009, No.60973053,
No.90920011), and the Specialized Research Fund
for the Doctoral Program of Higher Education of
China (Grant No. 20090001110047).
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference pro-
tocols for coreference resolution. In CoNLL Shared
Task, pages 40?44, Portland, Oregon, USA. Associa-
tion for Computational Linguistics.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 161?169.
Tsinghua University Press.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
III Hal Daume? and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97?104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
EMNLP, pages 660?669.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
ACL (Short Papers), pages 45?48. The Association for
Computer Linguistics.
T. Finley and T. Joachims. 2005. Supervised clustering
with support vector machines. In International Con-
ference on Machine Learning (ICML), pages 217?224.
Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-
jun Chen. 2009. Coreference resolution using markov
logic networks. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing: 10th Interna-
tional Conference, CICLing 2009.
M. Klenner. 2007. Enforcing consistency on coreference
sets. In RANLP.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-
hatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of the ACL, pages 135?142.
1253
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT/EMNLP, pages 25?
32.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems, pages 905?912. MIT Press.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In ACL, pages 1396?
1411. The Association for Computer Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP, pages 650?659.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of map inference for markov logic. In UAI,
pages 468?475. AUAI Press.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. Amodel-
theoretic coreference scoring scheme. In MUC, pages
45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In ACL, pages 843?851. The Association for
Computer Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In ACL/AFNLP,
pages 405?413. The Association for Computer Lin-
guistics.
1254
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311?321,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring Representations from Unlabeled Data with Co-training
for Chinese Word Segmentation
Longkai Zhang Houfeng Wang? Xu Sun Mairgup Mansur
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn, mairgup@gmail.com,
Abstract
Nowadays supervised sequence labeling
models can reach competitive performance
on the task of Chinese word segmenta-
tion. However, the ability of these mod-
els is restricted by the availability of an-
notated data and the design of features.
We propose a scalable semi-supervised fea-
ture engineering approach. In contrast
to previous works using pre-defined task-
specific features with fixed values, we dy-
namically extract representations of label
distributions from both an in-domain cor-
pus and an out-of-domain corpus. We
update the representation values with a
semi-supervised approach. Experiments
on the benchmark datasets show that our
approach achieve good results and reach
an f-score of 0.961. The feature engineer-
ing approach proposed here is a general
iterative semi-supervised method and not
limited to the word segmentation task.
1 Introduction
Chinese is a language without natural word
delimiters. Therefore, Chinese Word Segmen-
tation (CWS) is an essential task required by
further language processing. Previous research
shows that sequence labeling models trained on
labeled data can reach competitive accuracy on
the CWS task, and supervised models are more
accurate than unsupervised models (Xue, 2003;
Low et al, 2005). However, the resource of man-
ually labeled training corpora is limited. There-
fore, semi-supervised learning has become one
?Corresponding author
of the most natural forms of training for CWS.
Traditional semi-supervised methods focus on
adding new unlabeled instances to the training
set by a given criterion. The possible mislabeled
instances, which are introduced from the auto-
matically labeled raw data, can hurt the per-
formance and not easy to exclude by setting a
sound selecting criterion.
In this paper, we propose a simple and scal-
able semi-supervised strategy that works by pro-
viding semi-supervision at the level of represen-
tation. Previous works mainly assume that con-
text features are helpful to decide the potential
label of a character. However, when some of the
context features do not appear in the training
corpus, this assumption may fail. An example is
shown in table 1. Although the context of ???
and ??? is totally different, they share a homo-
geneous structure as ?verb-noun?. Therefore. A
much better way is to map the context informa-
tion to a kind of representation. More precisely,
the mapping should let the similar contexts map
to similar representations, while let the distinct
contexts map to distinct representations.
??? ???
Label B B
Character ? ? ? ? ? ?
Context C-1= ? C-1= ?
Features C0= ? C0= ?
C1= ? C1= ?
Table 1: Example of the context of ??? in ???
? (Eat fruits)? and the context of ??? in ????
(Play basketball)?
We use the label distribution information that
311
is extracted from the unlabeled corpus as this
representation to enhance the supervised model.
We add ?pseudo-labels? by tagging the unla-
beled data with the trained model on the train-
ing corpus. These ?pseudo-labels? are not accu-
rate enough. Therefore, we use the label distri-
bution, which is much more accurate.
To accurately calculate the precise label dis-
tribution, we use a framework similar to the co-
training algorithm to adjust the feature values
iteratively. Generally speaking, unlabeled data
can be classified as in-domain data and out-of-
domain data. In previous works these two kinds
of unlabeled data are used separately for differ-
ent purposes. In-domain data is mainly used to
solve the problem of data sparseness (Sun and
Xu, 2011). On the other hand, out-of domain
data is used for domain adaptation (Chang and
Han, 2010). In our work, we use in-domain and
out-of-domain data together to adjust the labels
of the unlabeled corpus.
We evaluate the performance of CWS on the
benchmark dataset of Peking University in the
second International Chinese Word Segmenta-
tion Bakeoff. Experiment results show that our
approach yields improvements compared with
the state-of-art systems. Even when the la-
beled data is insufficient, our methods can still
work better than traditional methods. Com-
pared to the baseline CWS model, which has
already achieved an f-score above 0.95, we fur-
ther reduce the error rate by 15%.
Our method is not limited to word segmen-
tation. It is also applicable to other problems
which can be solved by sequence labeling mod-
els. We also applied our method to the Chi-
nese Named Entity Recognition task, and also
achieved better results compared to traditional
methods.
The main contributions of our work are as fol-
lows:
? We proposed a general method to utilize
the label distribution given text contexts as
representations in a semi-supervised frame-
work. We let the co-training process ad-
just the representation values from label
distribution instead of using manually pre-
defined feature templates.
? Compared with previous work, our method
achieved a new state-of-art accuracy on the
CWS task as well as on the NER task.
The remaining part of this paper is organized
as follows. Section 2 describes the details of the
problem and our algorithm. Section 3 describes
the experiment and presents the results. Section
4 reviews the related work. Section 5 concludes
this paper.
2 System Architecture
2.1 Sequence Labeling
Nowadays the character-based sequence label-
ing approach is widely used for the Chinese word
segmentation problem. It was first proposed in
Xue (2003), which assigns each character a label
to indicate its position in the word. The most
prevalent tag set is the BMES tag set, which
uses 4 tags to carry word boundary information.
This tag set uses B, M, E and S to represent the
Beginning, the Middle, the End of a word and
a Single character forming a word respectively.
We use this tag set in our method. An example
of the ?BMES? representation is shown in table
2.
Character: ? ? ? ? ? ? ?
Tag: S S B E B M E
Table 2: An example for the ?BMES? representa-
tion. The sentence is ????????? (I love Bei-
jing Tian-an-men square), which consists of 4 Chi-
nese words: ??? (I), ??? (love), ???? (Beijing),
and ????? (Tian-an-men square).
2.2 Unlabeled Data
Unlabeled data can be divided into in-domain
data and out-of-domain data. In previous works,
these two kinds of unlabeled data are used sep-
arately for different purposes. In-domain data
only solves the problem of data sparseness (Sun
and Xu, 2011). Out-of domain data is used
only for domain adaptation (Chang and Han,
2010). These two functionalities are not contra-
dictory but complementary. Our study shows
312
that by correctly designing features and algo-
rithms, both in-domain unlabeled data and out-
of-domain unlabeled data can work together to
help enhancing the segmentation model. In our
algorithm, the dynamic features learned from
one corpus can be adjusted incrementally with
the dynamic features learned from the other cor-
pus.
As for the out-of-domain data, it will be even
better if the corpus is not limited to a specific
domain. We choose a Chinese encyclopedia cor-
pus which meets exactly this requirement. We
use the corpus to learn a large set of informative
features. In our experiment, two different views
of features on unlabeled data are considered:
Static Statistical Features (SSFs): These
features capture statistical information of char-
acters and character n-grams from the unlabeled
corpus. The values of these features are fixed
during the training process once the unlabeled
corpus is given.
Dynamic Statistical Features (DSFs):
These features capture label distribution infor-
mation from the unlabeled corpus given fixed
text contexts. As the training process proceeds,
the value of these features will change, since the
trained tagger at each training iteration may as-
sign different labels to the unlabeled data.
2.3 Framework
Suppose we have labeled data L, two unla-
beled corpora Ua and Ub (one is an in-domain
corpus and the other is an out-of-domain cor-
pus). Our algorithm is shown in Table 3.
During each iteration, we tag the unlabeled
corpus Ua using Tb to get pseudo-labels. Then
we extract features from the pseudo-labels. We
use the label distribution information as dy-
namic features. We add these features to the
training data to train a new tagger Ta. To adjust
the feature values, we extract features from one
corpus and then apply the statistics to the other
corpus. This is similar to the principle of co-
training (Yarowsky, 1995; Blum and Mitchell,
1998; Dasgupta et al, 2002). The difference is
that there are not different views of features, but
different kinds of unlabeled data. Detailed de-
scription of features is given in the next section.
Algorithm
Init:
Using baseline features only:
Train an initial tagger T0 based on L ()
Label Ua and Ub individually using T0
BEGIN LOOP:
Generate DSFs from tagged Ua
Augment L with DSFs to get La
Generate DSFs from tagged Ub
Augment L with DSFs to get Lb
Using baseline features, SSFs and DSFs:
Train new tagger Ta using La
Train new tagger Tb using Lb
Label Ua using Tb
Label Ub using Ta
LOOP until performance does not improve
RETURN the tagger which is trained with
in-domain features.
Table 3: Algorithm description
2.4 Features
2.4.1 Baseline Features
Our baseline feature templates include the
features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). These features are
widely used in the CWS task. To be convenient,
for a character ci with context . . . ci?1cici+1 . . .,
its baseline features are listed below:
? Character uni-grams: ck (i? 3 < k < i+3)
? Character bi-grams: ckck+1 (i ? 3 < k <
i+ 2)
? Whether ck and ck+1 are identical (i? 2 <
k < i + 2)
? Whether ck and ck+2 are identical (i? 4 <
k < i + 2)
The last two feature templates are designed to
detect character reduplication, which is a mor-
phological phenomenon in Chinese language.
An example is ?????? (Perfect), which is
a Chinese idiom with structure ?ABAC?.
313
2.4.2 Static statistical features
Statistical features are statistics that distilled
from the large unlabeled corpus. They are
proved useful in the Chinese word segmenta-
tion task. We define Static Statistical Features
(SSFs) as features whose value do not change
during the training process. The SSFs in our
approach includes Mutual information, Punctu-
ation information and Accessor variety. Previ-
ous works have already explored the functions
of the three static statistics in the Chinese word
segmentation task, e.g. Feng et al (2004); Sun
and Xu (2011). We mainly follow their defini-
tions while considering more details and giving
some modification.
Mutual information
Mutual information (MI) is a quantity that
measures the mutual dependence of two random
variables. Previous works showed that larger MI
of two strings claims higher probability that the
two strings should be combined. Therefore, MI
can show the tendency of two strings forming
one word. However, previous works mainly fo-
cused on the balanced case, i.e., the MI of strings
with the same length. In our study we find that,
in Chinese, there remains large amount of imbal-
anced cases, like a string with length 1 followed
by a string with length 2, and vice versa. We
further considered the MI of these string pairs
to capture more information.
Punctuation information
Punctuations can provide implicit labels for
the characters before and after them. The char-
acter after punctuations must be the first char-
acter of a word. The character before punctua-
tions must be the last character of a word. When
a string appears frequently after punctuations,
it tends to be the beginning of a word. The situ-
ation is similar when a string appears frequently
preceding punctuations. Besides, the probabil-
ity of a string appears in the corpus also affects
this tendency. Considering all these factors,
we propose ?punctuation rate? (PR) to capture
this information. For a string with length len
and probability p in the corpus, we define the
left punctuation rate LPRlen as the number of
times the string appears after punctuations, di-
vided by p. Similarly, the right punctuation
rate RPRlen is defines as the number of times
it appears preceding punctuations divided by its
probability p. The length of string we consider
is from 1 to 4.
Accessor variety
Accessor variety (AV) is also known as letter
successor variety (LSV) (Harris, 1955; Hafer and
Weiss, 1974). If a string appears after or pre-
ceding many different characters, this may pro-
vide some information of the string itself. Pre-
vious work of Feng et al (2004), Sun and Xu
(2011) used AV to represent this statistic. Sim-
ilar to punctuation rate, we also consider both
left AV and right AV. For a string s with length
l, we define the left accessor variety (LAV) as
the types of distinct characters preceding s in
the corpus, and the right accessor variety (RAV)
as the types of distinct characters after s in the
corpus. The length of string we consider is also
from 1 to 4.
2.4.3 Dynamic statistical features
The unlabeled corpus lacks precise labels. We
can use the trained tagger to give the unla-
beled data ?pseudo-labels?. These labels can-
not guarantee an acceptable precision. How-
ever, the label distribution will not be largely
affected by small mistakes. Using the label dis-
tribution information is more accurate than us-
ing the pseudo-labels directly.
Based on this assumption, we propose ?dy-
namic statistical features? (DSFs). The DSFs
are intended to capture label distribution infor-
mation given a text context. The word ?Dy-
namic? is in accordance with the fact that these
feature values will change during the training
process.
We give a formal description of DSFs. Sup-
pose there are K labels in our task. For example,
K = 4 if we take BMES labeling method. We
define the whole character sequence with length
n as X = (x1, x2 ? ? ?xj ? ? ?xn). Given a text con-
text Ci, where i is current character position,
the DSFs can be represented as a list,
DSF (Ci) = (DSF (Ci)1, ? ? ? , DSF (Ci)K)
314
Each element in the list represents the proba-
bility of the corresponding label in the distribu-
tion.
For convenience, we further define function
?count(condition)? as the total number of times
a ?condition? is true in the unlabeled corpus.
For example, count (current=?a?) represents the
times the current character equals ?a?, which is
exactly the number of times character ?a? ap-
pears in the unlabeled corpus.
According to different types of text context
Ci, we can divide DSFs into 3 types:
1.Basic DSF
For Basic DSF of Ci, we define D(Ci):
D(Ci) = (D(Ci)1, . . . , D(Ci)K)
We define Basic DSF with current character po-
sition i, text context Ci and label l (the lth di-
mension in the list) as:
D(Ci)l = P (y = l|Ci = xi)
= count(Ci = xi ? y = l)count(Ci = xi)
In this equation, the numerator counts the num-
ber of times current character is xi with label l.
The denominator counts the number of times
current character is xi.
We use the term ?Basic? because this kind of
DSFs only considers the character of position i
as its context. The text context refers to the cur-
rent character itself. This feature captures the
label distribution information given the charac-
ter itself.
2.BigramDSF
Basic DSF is simple and very easy to imple-
ment. The weakness is that it is less power-
ful to describe word-building features. Although
characters convey context information, charac-
ters themselves in Chinese is sometimes mean-
ingless. Character bi-grams can carry more con-
text information than uni-grams. We modify
Basic DSFs to bi-gram level and propose Bigram
DSFs.
For Bigram DSF of Ci, we define B(Ci):
B(Ci) = (B(Ci)1, . . . , B(Ci)K)
We define Bigram DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
B(Ci)l = P (y = l|Ci = xi?jxi?j+1)
= count(Ci = xi?jxi?j+1 ? y = l)count(Ci = xi?jxi?j+1)
j can take value 0 and 1.
In this equation, the numerator counts the
number of times current context is xi?jxi?j+1
with label l. The denominator counts the num-
ber of times current context is xi?jxi?j+1.
3.WindowDSF
Considering Basic DSF and Bigram DSF only
might cause the over-fitting problem, therefore
we introduce another kind of DSF. We call it
Window DSF, which considers the surrounding
context of a character and omits the character
itself.
For Window DSF, we define W (Ci):
W (Ci) = (W (Ci)1, . . . ,W (Ci)K)
We define Window DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
W (Ci)l = P (y = l|Ci = xi?1xi+1)
= count(Ci = xi?1xi+1 ? y = l)count(Ci = xi?1xi+1)
In this equation, the numerator counts the
number of times current context is xi?1xi+1
with label l. The denominator counts the num-
ber of times current context is xi?1xi+1.
2.4.4 Discrete features VS. Continuous
features
The statistical features may be expressed as
real values. A more natural way is to use dis-
crete values to incorporate them into the se-
quence labeling models . Previous works like
Sun and Xu (2011) solve this problem by set-
ting thresholds and converting the real value
into boolean values. We use a different method
to solve this, which does not need to consider
tuning thresholds. In our method, we process
static and dynamic statistical features using dif-
ferent strategies.
315
For static statistical value:
For mutual information, we round the real
value to their nearest integer. For punctuation
rate and accessor variety, as the values tend to
be large, we first get the log value of the feature
and then use the nearest integer as the corre-
sponding discrete value.
For dynamic statistical value:
Dynamic statistical features are distributions
of a label. The values of DSFs are all percentage
values. We can solve this by multiply the proba-
bility by an integer N and then take the integer
part as the final feature value. We set the value
of N by cross-validation..
2.5 Conditional Random Fields
Our algorithm is not necessarily limited to
a specific baseline tagger. For simplicity and
reliability, we use a simple Conditional Ran-
dom Field (CRF) tagger, although other se-
quence labeling models like Semi-Markov CRF
Gao et al (2007) and Latent-variable CRF Sun
et al (2009) may provide better results than
a single CRF. Detailed definition of CRF can
be found in Lafferty et al (2001); McCallum
(2002); Pinto et al (2003).
3 Experiment
3.1 Data and metrics
We used the benchmark datasets provided by
the second International Chinese Word Segmen-
tation Bakeoff1 to test our approach. We chose
the Peking University (PKU) data in our exper-
iment. Although the benchmark provides an-
other three data sets, two of them are data of
traditional Chinese, which is quite different from
simplified Chinese. Another is the data from Mi-
crosoft Research (MSR). We experimented on
this data and got 97.45% in f-score compared
to the state-of-art 97.4% reported in Sun et al
(2012). However, this corpus is much larger
than the PKU corpus. Using the labeled data
alone can get a relatively good tagger and the
unlabeled data contributes little to the perfor-
mance. For simplicity and efficiency, our further
1http://www.sighan.org/bakeoff2005/
experiments are all conducted on the PKU data.
Details of the PKU data are listed in table 4.
We also used two un-segmented corpora as
unlabeled data. The first one is Chinese Giga-
word2 corpus. It is a comprehensive archive of
newswire data. The second one is articles from
Baike3 of baidu.com. It is a Chinese encyclope-
dia similar to Wikipedia but contains more Chi-
nese items and their descriptions. In the exper-
iment we used about 5 million characters from
each corpus for efficiency. Details of unlabeled
data can be found in table 5.
In our experiment, we did not use any ex-
tra resources such as common surnames, part-
of-speech or other dictionaries.
F-score is used as the accuracy measure. We
define precision P as the percentage of words
in the output that are segmented correctly. We
define recall R as the percentage of the words
in reference that are correctly segmented. Then
F-score is as follows:
F = 2 ? P ?RP +R
The recall of out-of-vocabulary is also taken into
consideration, which measures the ability of the
model to correctly segment out of vocabulary
words.
3.2 Main Results
Table 6 summarizes the segmentation results
on test data with different feature combinations.
We performed incremental evaluation. In this
table, we first present the results of the tagger
only using baseline features. Then we show the
results of adding SSF and DSF individually. In
the end we compare the results of combining
SSF and DSF with baseline features.
Because the baseline features is strong to
reach a relative good result, it is not easy to
largely enhance the performance. Neverthe-
less, there are significant increases in f-score and
OOV-Recall when adding these features. From
table 6 we can see that by adding SSF and DSF
individually, the F-score is improved by +1.1%
2http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
3http://baike.baidu.com/
316
Identical words Total word Identical Character Total character
5.5 ? 104 1.1 ? 106 5 ? 103 1.8 ? 106
Table 4: Details of the PKU data
Corpus Character used
Gigaword 5000193
Baike 5000147
Table 5: Details of the unlabeled data.
P R F OOV
Baseline 0.950 0.943 0.946 0.676
+SSF 0.961 0.953 0.957 0.728
+DSF 0.958 0.953 0.955 0.678
+SSF+DSF 0.965 0.958 0.961 0.731
Table 6: Segmentation results on test data with
different feature combinations. The symbol ?+?
means this feature configuration contains features set
containing the baseline features and all features after
?+?. The size of unlabeled data is fixed as 5 million
characters.
and +0.9%. The OOV-Recall is also improved,
especially after adding SSFs. When considering
SSF and DSF together, the f-score is improved
by +1.5% while the OOV-Recall is improved by
+5.5%.
To compare the contribution of unlabeled
data, we conduct experiments of using differ-
ent sizes of unlabeled data. Note that the SSFs
are still calculated using all the unlabeled data.
However, each iteration in the algorithm uses
unlabeled data with different sizes.
Table 7 shows the results when changing the
size of unlabeled data. We experimented on
three different sizes: 0.5 million, 1 million and 5
million characters.
P R F OOV
DSF(0.5M) 0.962 0.954 0.958 0.727
DSF(1M) 0.963 0.955 0.959 0.728
DSF(5M) 0.965 0.958 0.961 0.731
Table 7: Comparison of results when changing the
size of unlabeled data. (0.5 million, 1 million and 5
million characters).
We further experimented on unlabeled corpus
with larger size (up to 100 million characters).
However the performance did not change signif-
icantly. Besides, because the number of features
in our method is very large, using too large un-
labeled corpus is intractable in real applications
due to the limitation of memory.
Our method can keep working well even when
the labeled data are insufficient. Table 8 shows
the comparison of f-scores when changing the
size of labeled data. We compared the results
of using all labeled data with 3 different situa-
tions: using 1/10, 1/2 and 1/4 of all the labeled
data. In fact, the best system on the Second In-
ternational Chinese Word Segmentation bakeoff
reached 0.95 in f-score by using all labeled data.
From table 8 we can see that our algorithm only
needs 1/4 of all labeled data to achieve the same
f-score.
Baseline +SSF+DSF Improve
1/10 0.934 0.943 +0.96%
1/4 0.946 0.951 +0.53%
1/2 0.952 0.956 +0.42%
All 0.957 0.961 +0.42%
Table 8: Comparison of f-scores when changing the
size of labeled data. (1/10, 1/4, 1/2 and all labeled
data. The size of unlabeled data is fixed as 5 million
characters.)
We also explored how the performance
changes as iteration increases. Figure 1 shows
the change of F-score during the first 10 itera-
tions. From figure 1 we find that f-score has a
fast improvement in the first few iterations, and
then stables at a fixed point. Besides, as the size
of labeled data increases, it converges faster.
Using an in-domain corpus and an out-of-
domain corpus is better than use one corpus
alone. We compared our approach with the
method which uses only one unlabeled corpus.
To use only one corpus, we modify our algorithm
to extract DSFs from the Chinese Giga word
corpus and apply the learned features to itself.
317
Figure 1: Learning curve of using different size of
labeled data
Table 9 shows the result. We can see that our
method outperforms by +0.2% in f-score and
+0.7% in OOV-Recall.
Finally, we compared our method with the
state-of-art systems reported in the previous pa-
pers. Table 10 listed the results. Best05 repre-
sents the best system reported on the Second In-
ternational Chinese Word Segmentation Bake-
off. CRF + Rule system represents a combina-
tion of CRF model and rule based model pre-
sented in Zhang et al (2006). Other three sys-
tems all represent the methods using their cor-
responding model in the corresponding papers.
Note that these state-of-art systems are either
using complicated models with semi-Markov re-
laxations or latent variables, or modifying mod-
els to fit special conditions. Our system uses a
single CRF model. As we can see in table 10,
our method achieved higher F-scores than the
previous best systems.
3.3 Results on NER task
Our method is not limited to the CWS prob-
lem. It is applicable to all sequence labeling
problems. We applied our method on the Chi-
nese NER task. We used the MSR corpus of
the sixth SIGHAN Workshop on Chinese Lan-
guage Processing. It is the only NER corpus
using simplified Chinese in that workshop. We
compared our method with the pure sequence la-
beling approach in He and Wang (2008). We re-
implemented their method to eliminate the dif-
ference of various CRFs implementations. Ex-
periment results are shown in table 11. We can
see that our methods works better, especially
when handling the out-of-vocabulary named en-
tities;
4 Related work
Recent studies show that character sequence
labeling is an effective method of Chinese word
segmentation for machine learning (Xue, 2003;
Low et al, 2005; Zhao et al, 2006a,b). These su-
pervised methods show good results. Unsuper-
vised word segmentation (Maosong et al, 1998;
Peng and Schuurmans, 2001; Feng et al, 2004;
Goldwater et al, 2006; Jin and Tanaka-Ishii,
2006) takes advantage of the huge amount of raw
text to solve Chinese word segmentation prob-
lems. These methods need no annotated corpus,
and most of them use statistics to help model
the problem. However, they usually are less ac-
curate than supervised ones.
Currently ?feature-engineering? methods
have been successfully applied into NLP ap-
plications. Miller et al (2004) applied this
method to named entity recognition. Koo et al
(2008) applied this method to dependency pars-
ing. Turian et al (2010) applied this method to
both named entity recognition and text chunk-
ing. These papers shared the same concept of
word clustering. However, we cannot simply
equal Chinese character to English word because
characters in Chinese carry much less informa-
tion than words in English and the clustering
results is less meaningful.
Features extracted from large unlabeled cor-
pus in previous works mainly focus on statisti-
cal information of characters. Feng et al (2004)
used the accessor variety criterion to extract
word types. Li and Sun (2009) used punctua-
tion information in Chinese word segmentation
by introducing extra labels ?L? and ?R?. Chang
and Han (2010), Sun and Xu (2011) used rich
statistical information as discrete features in
a sequence labeling framework. All these ap-
proaches can be viewed as using static statistics
features in a supervised approach. Our method
is different from theirs. For the static statistics
features in our approach, we not only consider
richer string pairs with the different lengths, but
also consider term frequency when processing
318
P R F OOV
Using one corpus 0.963 0.955 0.959 0.724
Our method 0.965 0.958 0.961 0.731
Table 9: Comparison of our approach with using only the Gigaword corpus
Method P R F-score
Best05 (Chen et al (2005)) 0.953 0.946 0.950
CRF + rule-system (Zhang et al (2006)) 0.947 0.955 0.951
Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945
Latent-variable CRF (Sun et al (2009)) 0.956 0.948 0.952
ADF-CRF (Sun et al (2012)) 0.958 0.949 0.954
Our method 0.965 0.958 0.961
Table 10: Comparison of our approach with the state-of-art systems
P R F OOV
Traditional 0.925 0.872 0.898 0.712
Our method 0.916 0.887 0.902 0.737
Table 11: Comparison of our approach with tradi-
tional NER systems
punctuation features.
There are previous works using features ex-
tracted from label distribution of unlabeled cor-
pus in NLP tasks. Schapire et al (2002) use a
set of features annotated with majority labels
to boost a logistic regression model. We are
different from their approach because there is
no pseudo-example labeling process in our ap-
proach. Qi et al (2009) investigated on large
set of distribution features and used these fea-
tures in a self-training way. They applied the
method on three tasks: named entity recogni-
tion, POS tagging and gene name recognition
and got relatively good results. Our approach is
different from theirs. Although we all consider
label distribution, the way we use features are
different. Besides, our approach uses two unla-
beled corpora which can mutually enhancing to
get better result.
5 Conclusion and Perspectives
In this paper, we presented a semi-supervised
method for Chinese word segmentation. Two
kinds of new features are used for the itera-
tive modeling: static statistical features and dy-
namic statistical features. The dynamic statis-
tical features use label distribution information
for text contexts, and can be adjusted automat-
ically during the co-training process. Experi-
mental results show that the new features can
improve the performance on the Chinese word
segmentation task. We further conducted exper-
iments to show that the performance is largely
improved, especially when the labeled data is
insufficient.
The proposed iterative semi-supervised
method is not limited to the Chinese word
segmentation task. It can be easily extended
to any sequence labeling task. For example, it
works well on the NER task as well. As our
future work, we plan to apply our method to
other natural language processing tasks, such
as text chunking.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Pro-
gram) (No. 2012AA011101) and National Natu-
ral Science Foundation of China (No.91024009).
We also thank Xu Sun and Qiuye Zhao for proof-
reading the paper.
319
References
Blum, A. and Mitchell, T. (1998). Combining
labeled and unlabeled data with co-training.
In Proceedings of the eleventh annual confer-
ence on Computational learning theory, pages
92?100. ACM.
Chang, B. and Han, D. (2010). Enhancing
domain portability of chinese segmentation
model using chi-square statistics and boot-
strapping. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 789?798. Association
for Computational Linguistics.
Chen, A., Zhou, Y., Zhang, A., and Sun, G.
(2005). Unigram language model for chinese
word segmentation. In Proceedings of the
4th SIGHAN Workshop on Chinese Language
Processing, pages 138?141. Association for
Computational Linguistics Jeju Island, Korea.
Dasgupta, S., Littman, M. L., and McAllester,
D. (2002). Pac generalization bounds for co-
training. Advances in neural information pro-
cessing systems, 1:375?382.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Gao, J., Andrew, G., Johnson, M., and
Toutanova, K. (2007). A comparative study
of parameter estimation methods for statisti-
cal natural language processing. In ANNUAL
MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page
824.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Compu-
tational Linguistics and the 44th annual meet-
ing of the Association for Computational Lin-
guistics, pages 673?680. Association for Com-
putational Linguistics.
Hafer, M. A. and Weiss, S. F. (1974). Word seg-
mentation by letter successor varieties. Infor-
mation storage and retrieval, 10(11):371?385.
Harris, Z. S. (1955). From phoneme to mor-
pheme. Language, 31(2):190?222.
He, J. and Wang, H. (2008). Chinese named en-
tity recognition and word segmentation based
on character. In Sixth SIGHAN Workshop on
Chinese Language Processing, page 128.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsu-
pervised segmentation of chinese text by use
of branching entropy. In Proceedings of the
COLING/ACL on Main conference poster
sessions, pages 428?435. Association for Com-
putational Linguistics.
Koo, T., Carreras, X., and Collins, M. (2008).
Simple semi-supervised dependency parsing.
Lafferty, J., McCallum, A., and Pereira, F.
(2001). Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data.
Li, Z. and Sun, M. (2009). Punctuation
as implicit annotations for chinese word
segmentation. Computational Linguistics,
35(4):505?512.
Low, J., Ng, H., and Guo, W. (2005). A
maximum entropy approach to chinese word
segmentation. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language
Processing, volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
McCallum, A. (2002). Efficiently inducing fea-
tures of conditional random fields. In Proceed-
ings of the Nineteenth Conference on Uncer-
tainty in Artificial Intelligence, pages 403?410.
Morgan Kaufmann Publishers Inc.
Miller, S., Guinness, J., and Zamanian, A.
(2004). Name tagging with word clusters
and discriminative training. In Proceedings of
HLT-NAACL, volume 4.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
320
vances in Intelligent Data Analysis, pages
238?247.
Pinto, D., McCallum, A., Wei, X., and Croft,
W. (2003). Table extraction using conditional
random fields. In Proceedings of the 26th an-
nual international ACM SIGIR conference on
Research and development in informaion re-
trieval, pages 235?242. ACM.
Qi, Y., Kuksa, P., Collobert, R., Sadamasa,
K., Kavukcuoglu, K., and Weston, J. (2009).
Semi-supervised sequence labeling with self-
learned features. In Data Mining, 2009.
ICDM?09. Ninth IEEE International Confer-
ence on, pages 428?437. IEEE.
Schapire, R., Rochery, M., Rahim, M., and
Gupta, N. (2002). Incorporating prior
knowledge into boosting. In MACHINE
LEARNING-INTERNATIONAL WORK-
SHOP THEN CONFERENCE-, pages
538?545.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
Sun, X., Zhang, Y., Matsuzaki, T., Tsuruoka,
Y., and Tsujii, J. (2009). A discriminative
latent variable chinese segmenter with hybrid
word/character information. In Proceedings of
Human Language Technologies: The 2009 An-
nual Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 56?64. Association for Compu-
tational Linguistics.
Turian, J., Ratinov, L., and Bengio, Y. (2010).
Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguis-
tics, pages 384?394. Association for Compu-
tational Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Yarowsky, D. (1995). Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd annual meeting
on Association for Computational Linguistics,
pages 189?196. Association for Computational
Linguistics.
Zhang, R., Kikui, G., and Sumita, E. (2006).
Subword-based tagging by conditional ran-
dom fields for chinese word segmentation. In
Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196. Asso-
ciation for Computational Linguistics.
Zhang, Y. and Clark, S. (2007). Chi-
nese segmentation with a word-based percep-
tron algorithm. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL
LINGUISTICS, volume 45, page 840.
Zhao, H., Huang, C., and Li, M. (2006a). An
improved chinese word segmentation system
with conditional random field. In Proceed-
ings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing, volume 117. Syd-
ney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field mod-
eling. In Proceedings of PACLIC, volume 20,
pages 87?94.
321
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 426?435,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Collective Entity Linking with Stacking
Zhengyan He? Shujie Liu? Yang Song? Mu Li? Ming Zhou? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
songyangmagic@gmail.com wanghf@pku.edu.cn
Abstract
Entity disambiguation works by linking am-
biguous mentions in text to their correspond-
ing real-world entities in knowledge base. Re-
cent collective disambiguation methods en-
force coherence among contextual decisions
at the cost of non-trivial inference processes.
We propose a fast collective disambiguation
approach based on stacking. First, we train a
local predictor g0 with learning to rank as base
learner, to generate initial ranking list of can-
didates. Second, top k candidates of related
instances are searched for constructing expres-
sive global coherence features. A global pre-
dictor g1 is trained in the augmented feature
space and stacking is employed to tackle the
train/test mismatch problem. The proposed
method is fast and easy to implement. Exper-
iments show its effectiveness over various al-
gorithms on several public datasets. By learn-
ing a rich semantic relatedness measure be-
tween entity categories and context document,
performance is further improved.
1 Introduction
When extracting knowledge from natural language
text into a machine readable format, ambiguous
names must be resolved in order to tell which real-
world entity the name refers to. The task of linking
names to knowledge base is known as entity linking
or disambiguation (Ji et al, 2011). The resulting text
is populated with semantic rich links to knowledge
base like Wikipedia, and ready for various down-
stream NLP applications.
?Corresponding author
Previous researches have proposed several kinds
of effective approaches for this problem. Learning
to rank (L2R) approaches use hand-crafted features
f(d, e) to describe the similarity or dissimilarity be-
tween contextual document d and entity definition
e. L2R approaches are very flexible and expres-
sive. Features like name matching, context similar-
ity (Li et al, 2009; Zheng et al, 2010; Lehmann et
al., 2010) and category context correlation (Bunescu
and Pasca, 2006) can be incorporated with ease.
Nevertheless, decisions are made independently and
inconsistent results are found from time to time.
Collective approaches utilize dependencies be-
tween different decisions and resolve all ambiguous
mentions within the same context simultaneously
(Han et al, 2011; Hoffart et al, 2011; Kulkarni
et al, 2009; Ratinov et al, 2011). Collective ap-
proaches can improve performance when local ev-
idence is not confident enough. They often utilize
semantic relations across different mentions, and is
why they are called global approaches, while L2R
methods fall into local approaches (Ratinov et al,
2011). However, collective inference processes are
often expensive and involve an exponential search
space.
We propose a collective entity linking method
based on stacking. Stacked generalization (Wolpert,
1992) is a powerful meta learning algorithm that
uses two levels of learners. The predictions of the
first learner are taken as augmented features for the
second learner. The nice property of stacking is that
it does not restrict the form of the base learner. In
this paper, our base learner, an L2R ranker, is first
employed to generate a ranking list of candidates.
426
At the next level, we search for semantic coherent
entities from the top k candidates of neighboring
mentions. The second learner is trained on the aug-
mented feature space to enforce semantic coherence.
Stacking is employed to handle train/test mismatch
problem. Compared with existing collective meth-
ods, the inference process of our method is much
faster because of the simple form of its base learner.
Wikipedians annotate each entity with categories
which provide another source of valuable seman-
tic information. (Bunescu and Pasca, 2006) pro-
pose to generalize beyond context-entity correla-
tion s(d, e) with word-category correlation s(w, c).
However, this method works at word level, and does
not scale well to large number of categories. We
explore a representation learning technique to learn
the category-context association in latent semantic
space, which scales much better to large knowledge
base.
Our contributions are as follows: (1) We pro-
pose a fast and accurate stacking-based collective
entity linking method, which combines the benefits
of both coherence modeling of collective approaches
and expressivity of L2R methods. We show an
effective usage of ranking list as global features,
which is a key improvement for the global predictor.
(2) To overcome problems of scalability and shal-
low word-level comparison, we learn the category-
context correlation with recent advances of repre-
sentation learning, and show that this extra seman-
tic information indeed helps improve entity linking
performance.
2 Related Work
Most popular entity linking systems use the L2R
framework (Bunescu and Pasca, 2006; Li et al,
2009; Zheng et al, 2010; Lehmann et al, 2010).
Its discriminative nature gives the model enough
flexibility and expressivity. It can include any fea-
tures that describe the similarity or dissimilarity of
context d and candidate entity e. They often per-
form well even on small training set, with carefully-
designed features. This category falls into the local
approach as the decision processes for each mention
are made independently (Ratinov et al, 2011).
(Cucerzan, 2007) first suggests to optimize an ob-
jective function that is similar to the collective ap-
proach. However, the author adopts an approxi-
mation method because of the large search space
(which is O(nm) for a document with m mentions,
each with n candidates). Various other methods
like integer linear programming (Kulkarni et al,
2009), personalized PageRank (Han et al, 2011) and
greedy graph cutting (Hoffart et al, 2011) have been
explored in literature. Our method without stacking
resembles the method of (Ratinov et al, 2011) in
that they use the predictions of a local ranker to gen-
erate features for global ranker. The differences are
that we use stacking to train the local ranker to han-
dle the train/test mismatch problem and top k candi-
dates to generate features for the global ranker.
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that uses multiple learners out-
puts to augment the feature space of subsequent
learners. It utilizes a cross-validation strategy to ad-
dress the train set / testset label mismatch problem.
Various applications of stacking in NLP have been
proposed, such as collective document classification
(Kou and Cohen, 2007), stacked dependency parsing
(Martins et al, 2008) and joint Chinese word seg-
mentation and part-of-speech tagging (Sun, 2011).
(Kou and Cohen, 2007) propose stacked graphical
learning which captures dependencies between data
with relational template. Our method is inspired by
their approach. The difference is our base learner is
an L2R model. We search related entity candidates
in a large semantic relatedness graph, based on the
assumption that true candidates are often semanti-
cally correlated while false ones scattered around.
Wikipedians annotate entries in Wikipedia with
category network. This valuable information gener-
alizes entity-context correlation to category-context
correlation. (Bunescu and Pasca, 2006) utilize
category-word as features in their ranking model.
(Kataria et al, 2011) employ a hierarchical topic
model where each inner node in the hierarchy is a
category. Both approaches must rely on pruned cate-
gories because the large number of noisy categories.
We try to address this problem with recent advances
of representation learning (Bai et al, 2009), which
learns the relatedness of category and context in la-
tent continuous space. This method scales well to
potentially large knowledge base.
427
3 Method
In this section, we first introduce our base learner
and local features used; next, the stacking train-
ing strategy is given, followed by an explana-
tion of our global coherence model with aug-
mented feature space; finally we explain how to
learn category-context correlation with representa-
tion learning technique.
3.1 Base learner and local predictor g0
Entity linking is formalized as follows: given
an ambiguous name mention m with its con-
textual document d, a list of candidate entities
e1, e2, . . . , en(m) ? C(m) is generated for m, our
predictor g will generate a ranking score g(ei) for
each candidate ei. The ranking score will be used
to construct augmented features for the next level
learner, or used by our end system to select the an-
swer:
e? = arg max
e?C(m)
g(e) (1)
In an L2R framework, the model is often defined
as a linear combination of features. Here, our fea-
tures f?(d, e) are derived from document d and can-
didate e. The model is defined as g(e) = w?f?(d, e).
In our problem, we are given a list of training data
D = {(di, ei)}. We want to optimize the parameter
w?, such that the correct entity has a higher score over
negative ones. This is done via a preference learning
technique SVM rank, first introduced by (Joachims,
2002). The following margin based loss is mini-
mized w.r.t w?:
L = 1
2
?w??2 + C
?
?d,e? (2)
s.t. w?(f?(d, e)? f?(d, e?)) ? 1? ?d,e? (3)
?d,e? ? 0 (4)
where C is a trade-off between training error and
margin size; ? is slacking variable and loops over
all query documents d and negative candidates e? ?
C(m)? {e}.
This model is expressive enough to include any
form of features describing the similarity and dis-
similarity of d and e. We only include some typical
features seen in literature. The inclusion of these
features is not meant to be exhaustive. Our purpose
is to build a moderate model in which some of the
Surface matching:
1. mention string m exactly matches candidate
e, i.e. m = e
2. neither m is a substring of e nor e is a sub-
string of m
3. m ?= e and m is a substring of e
4. m ?= e and e is a substring of m
5. m ?= e and m is a redirect pointing to e in
Wikipedia
6. m ?= e and e starts with m
7. m ?= e and e ends with m
Context matching:
1. cosine similarity of TF-IDF score between
context and entire Wikipedia page of candidate
2. cosine similarity of TF-IDF score between
context and introduction of Wikipedia page
3. jaccard distance between context and entire
Wikipedia page of candidate
4. jaccard distance between context and intro-
duction of Wikipedia page
Popularity or prominence feature:
percentage of Wikipedia hyperlinks pointing to
e given mention m, i.e. P(e|m)
Category-context coherence model:
cat0 and cat1 (details in Section 3.4)
Table 1: Features for local predictor g0.
useful features like string matching and entity pop-
ularity cannot be easily expressed by collective ap-
proaches like (Hoffart et al, 2011; Han et al, 2011).
The features for level 0 predictor g0 are described
in Table 1. The reader can consult (Li et al, 2009;
Zheng et al, 2010; Lehmann et al, 2010) for further
reference.
3.2 Stacking training for global predictor g1
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that stacks two ?levels? of pre-
dictors. Level 0 includes one or more predictors
h(0)1 , h
(0)
2 , . . . , h
(0)
K : Rd ? R, each one is trained on
the original d-dimensional feature space. The level
1 predictor h(1) : Rd+K ? R is trained in the aug-
mented (d+K)-dimensional feature space, in which
predictions at level 0 are taken as extra features in
h(1).
(Kou and Cohen, 2007) proposed stacked graphi-
428
cal learning for learning and inference on relational
data. In stacked graphical learning, dependencies
among data are captured by relational template, with
which one searches for related instances of the cur-
rent instance. The augmented feature space does
not necessarily to be d + K. Instead, one can con-
struct any declarative feature with the original data
and predictions of related instances. For instance,
in collective document classification (Kou and Co-
hen, 2007) employ relational template to extract
documents that link to this document, then apply a
COUNT aggregator over each category on neighbor-
ing documents as level 1 features.
In our entity linking task, we use a single predic-
tor g0 trained with local features at level 0. Com-
pared with (Kou and Cohen, 2007), both g0 and g1
are L2R models rather than classifier. At level 1, for
each document-candidate entity pair, we use the re-
lational templateN (x) to find related entities for en-
tity x, and construct global features with some func-
tion G({g0(n)|n ? N (x)}) (details in Sec. 3.3).
The global predictor g1 receives as input the origi-
nal features plus G.
One problem is that if we use g0 trained on the en-
tire training set to predict related instances in train-
ing set, the accuracy can be somehow different (typ-
ically lower) for future unseen data. g1 with this pre-
diction as input doesn?t generalize well to test data.
This is known as train/test mismatch problem. To
mimic test time behavior, training is performed in a
cross-validation-like way. Let D be the entire train-
ing set:
1. Split D into L partitions {D1, . . . ,DL}
2. For each split Di:
2.1 Train an instance of g0 on D ?Di
2.2 Predict all related instances inDi with this
predictor g0
2.3 Augment feature space for x ? Di, with G
applied on predictions of N (x)
3. Train level 0 predictor g0 on entire D, for ex-
panding feature space for test data
4. Train level 1 predictor g1 on entire D, in the
augmented feature space.
In the next subsection, we will describe how to
construct global features from the predictions of g0
on neighbors N (x) with G.
3.3 Enforcing coherence with global features G
If one wants to identify the correct entity for an am-
biguous name, he would possibly look for related
entities in its surrounding context. However, sur-
rounding entities can also exhibit some degree of
ambiguity. In ideal cases, most true candidates are
inter-connected with semantic links while negative
candidates are scattered around (Fig. 1). Thus, we
ask the following question: Is there any highly rele-
vant entity to this candidate in context? Or, is there
any mention with highly relevant entity to this can-
didate in the top k ranking list of this mention? And
how many those mentions are? The reason to look
up top k candidates is to improve recall. g0 may not
perfectly rank related entity at the first place, e.g.
?Mitt Romney? in Figure 1.
Assume the ambiguous mention set is M . For
each mention mi ? M , we rank each entity ei,j ?
C(mi) by its score g0(ei,j). Denote its rank as
Rank(ei,j). For each entity e in the candidate set
E = {ei,j |?ei,j ? C(mi), ?mi ? M}, we search
related instances for e as follows:
1. search in E for entities with semantic related-
ness above a threshold ({0.1,0.3,0.5,0.7,0.9});
2. select those entities in step (1) with Rank(e)
less than or equal to k (k ? {1, 3, 5});
3. map entities in step (2) to unique set of men-
tions U , excluding current m, i.e. e ? C(m).
This process is relatively fast. It only involves a
sparse matrix slicing operation on the large pre-
computed semantic relatedness matrix in step (1),
and logical operation in step (2,3). The following
features are fired concerning the unique set U :
- if U is empty;
- if U is not empty;
- if the percentage |U |/|M | is above a threshold
(e.g. 0.3).
The above process generates a total of 45 (5?3?3)
global features.
429
Barack Obama Democratic Party (United States)
Mitt Romney
Republican Party (United States)
Obama, Fukui
Obama, Nagasaki
Democratic Party (Italy)
Democratic Party (Serbia)
Republican Party of Minnesota
Republicanism
Romney, West Virginia
HMS Romney (1694)
... ... ... ...
received national attention during his campaign  ...  with his vectory in the March   [[Obama|Barack Obama]]
[[Democratic Party|Democratic Party (United States)]] primary  ...  He was re-elected president in November
2012, defeating [[Republican|Republican Party (United States)]] nominee [[Romney|Mitt Romney]]
Figure 1: Semantic links for collective entity linking. Annotation [[mention|entity]] follows Wikipedia conventions.
Finally, the semantic relatedness measure of two
entities ei,ej is defined as the common in-links of ei
and ej in Wikipedia (Milne and Witten, 2008; Han
et al, 2011):
SR(ei, ej) = 1?
log(max(|A|, |B|))? log(|A ?B|)
log(|W |)? log(min(|A|, |B|))
(5)
where A and B are the set of in-links for entity ei
and ej respectively, andW is the set of all Wikipedia
pages.
Our method is a trade-off between exact collec-
tive inference and approximating related instance
with top ranked entities produced by g0. Most
collective approaches take all ambiguous mentions
into consideration and disambiguate them simulta-
neously, resulting in difficulty when inference in
large search space (Kulkarni et al, 2009; Hoffart
et al, 2011). Others resolve to some kinds of ap-
proximation. (Cucerzan, 2007) construct features as
the average of all candidates for one mention, in-
troducing considerable noise. (Ratinov et al, 2011)
also employ a two level architecture but only take
top 1 prediction for features. This most resembles
our approach, except we use stacking to tackle the
train/test mismatch problem, and construct different
set of features from top k candidates predicted by
g0. We will show in our experiments that this indeed
helps boost performance.
3.4 Learning category-context coherence
model cat
Entities in Wikipedia are annotated with rich se-
mantic structures. Category network provides us
with another valuable information for entity link-
ing. Take the mention ?Romney? as an exam-
ple, one candidate ?Mitt Romney? with category
?Republican party presidential nominee? co-occurs
frequently with context like ?election? and ?cam-
paign?, while another candidate ?Milton Romney?
with category ?Utah Utes football players? is fre-
quently observed with context like ?quarterback?
and ?backfield?. The category network forms a di-
rected acyclic graph (DAG). Some entities can share
category through the network, e.g. ?Barack Obama?
with category ?Democratic Party presidential nom-
inees? shares the category ?United States presiden-
tial candidates by party? with ?Mitt Romney? when
travelling two levels up the network.
(Bunescu and Pasca, 2006) propose to learn the
category-context correlation at word level through
category-word pair features. This method creates
sparsity problem and does not scale well because
the number of features grows linearly with both the
number of categories and the vocabulary size. More-
over, the category network is somewhat noisy, e.g.
travelling up four levels of the hierarchy can result
in over ten thousand categories, with many irrelevant
ones.
Rather than learning the correlation at word level,
we explore a representation learning method that
learns category-context correlation in the latent se-
mantic space. Supervised Semantic Indexing (SSI)
(Bai et al, 2009) is trained on query-document pairs
to predict their degree of matching. The compar-
ison is performed in the latent semantic space, so
that synonymy and polysemy are implicitly handled
by its inner mechanism. The score function between
query q and document d is defined as:
f(q, d) = qTWd (6)
430
where W is learned with supervision like click-
through data.
Given training data {(qi, di)}, training is done by
randomly sampling a negative target d?. The model
optimizes W such that f(q, d+) > f(q, d?). Thus,
the training objective is to minimize the following
margin-based loss function:
?
q,d+,d?
max(0, 1? f(q, d+) + f(q, d?)) (7)
which is also known as contrastive estimation
(Smith and Eisner, 2005).
W can become very large and inefficient when we
have a big vocabulary size. This is addressed by re-
placing W with its low rank approximation:
W = UTV + I (8)
here, the identity term I is a trade-off between the
latent space model and a vector space model. The
gradient step is performed with Stochastic Gradient
Descent (SGD):
U ?U + ?V (d+ ? d?)qT ,
if 1? f(q, d+) + f(q, d?) > 0 (9)
V ?V + ?Uq(d+ ? d?)T ,
if 1? f(q, d+) + f(q, d?) > 0. (10)
where ? is the learning rate.
The query and document are not necessary real
query and document. In our case, we treat our
problem as: given the occurring context of an en-
tity, retrieving categories corresponding to this en-
tity. Thus, we use context as query q and the cat-
egories of this candidate entity as d. We also treat
the definition page of an entity as its context, and
first train the model with definition pages, because
definition pages exhibit more focused topic. This
considerably accelerates the training process. To
reduce noise, We input the categories directly con-
nected with one entity as a word vector. The input
can be a TF-IDF vector or binary vector. We denote
model trained with normalized TF-IDF and with bi-
nary input as cat0 and cat1 respectively.
4 Experiments
4.1 Datasets
Previous researches have used diverse datasets for
evaluation, which makes it hard for comparison
with others? approaches. TAC-KBP has several
years of data for evaluating entity linking system,
but is not well suited for evaluating collective ap-
proaches. Recently, (Hoffart et al, 2011) anno-
tated a clean and much larger dataset AIDA 1 for
collective approaches evaluation based on CoNLL
2003 NER dataset. (Ratinov et al, 2011) also re-
fined previous work and contribute four publicly
available datasets 2. Thanks to their great works,
we have enough data to evaluate against. Accord-
ing to the setting of (Hoffart et al, 2011), we
split the AIDA dataset for train/development/test
with 946/216/231 documents. We train a separate
model on the Wikipedia training set for evaluating
ACE/QUAINT/WIKI dataset (Ratinov et al, 2011).
Table 2 gives a brief overview of the datasets used.
For knowledge base, we use the Wikipedia XML
dump 3 to extract over 3.3 million entities. We use
annotation from Wikipedia to build a name dictio-
nary from mention string m to entity e for can-
didate generation, including redirects, disambigua-
tion pages and hyperlinks, follows the approach of
(Cucerzan, 2007). For candidate generation, we
keep the top 30 candidates by popularity (Tbl. 1).
Note that our name dictionary is different from
(Ratinov et al, 2011) and has a much higher recall.
Since (Ratinov et al, 2011) evaluate on ?solvable?
mentions and we have no way to recover those men-
tions, we re-implement their global features and the
final scores are not directly comparable to theirs.
4.2 Methods under comparison
We compare our algorithm with several state-of-the-
art collective entity disambiguation systems. The
AIDA system proposed by (Hoffart et al, 2011) use
a greedy graph cutting algorithm that iteratively re-
move entities with low confidence scores. (Han et
al., 2011) employ personalized PageRank to prop-
agate evidence between different decisions. Both
algorithms use simple local features without dis-
criminative training. (Kulkarni et al, 2009) pro-
pose to use integer linear programming (ILP) for
inference. Except our re-implementation of Han?s
1available at http://www.mpi-inf.mpg.de/yago-naga/aida/
2http://cogcomp.cs.illinois.edu/Data, we don?t find the
MSNBC dataset in the zip file.
3available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
431
Dataset ndocs non-
NIL
identified solvable
AIDA dev 216 4791 4791 4707
AIDA test 231 4485 4485 4411
ACE 36 257 238 209(185)
AQUAINT 50 727 697 668(588)
Wikipedia 40 928 918 854(843)
Table 2: Number of mentions in each dataset. ?identi-
fied? means the mention exists in our name dictionary
and ?solvable? means the true entity are among the top 30
candidates by popularity. Number in parenthesis shows
the results of (Ratinov et al, 2011).
method, both AIDA and ILP solution are quite slow
at running time. The online demo of AIDA takes
over 10 sec to process one document with mod-
erate size, while the ILP solution takes around 2-
3 sec/doc. In contrast, our method takes only 0.3
sec/doc, and is easy to implement.
(Ratinov et al, 2011) also utilize a two layer
learner architecture. The difference is that their
method use top 1 candidate generated by local
learner for global feature generation , while we
search the top k candidates. Moreover, stacking is
used to tackle the train/test mismatch problem in
our model. We re-implement the global features of
(Ratinov et al, 2011) and use our local predictor
g0 for level 0 predictor. Note that we only imple-
ment their global features concerning common in-
links and inter-connection (totally 9 features) for fair
comparison because all other models don?t use com-
mon outgoing links for global coherence.
4.3 Settings
We implement SVM rank with an adaptation of lin-
ear SVM in scikit-learn (which is a wrapper of Li-
blinear). The category-context coherence model is
implemented with Numpy configured with Open-
Blas library, and we train this model on the entire
Wikipedia hyperlink annotation. It takes about 1.5d
for one pass over the entire dataset. The learning
rate ? is set to 1e-4 and training cost before update
is below 0.02.
Parameter tuning: there aren?t many parameters
to tune for both g0 and g1. The context document
window size is fixed as 100 for compatibility with
(Ratinov et al, 2011; Hoffart et al, 2011). The num-
ber of candidates is fixed to top 30 ranked by entity?s
popularity. Increase this value will generally boost
recall at the cost of lower precision.
We introduce the following default parameter for
global features in g1. The number of fold for stack-
ing is set to {1,5,10} (see Table 4, default is 10; 1
means no stacking, i.e. training g0 with all training
data and generating level 1 features for training data
directly with this g0). The number k for searching
neighboring entities with relational template is set
to {1,3,5,7} (e.g. in step 2 of Section 3.3 k = 5;
default is 5).
For category-context modeling, the vocabulary
sizes of context and category are set to top 10k and
6k unigrams by frequency. The latent dimension of
low rank approximation is set to 200.
Performance measures: For all non-NIL
queries, we evaluate performance with micro pre-
cision averaged over queries and macro precision
averaged over documents. Mean Reciprocal Rank
(MRR) is an information retrieval measure and is
defined as 1|Q|
?|Q|
i
1
ranki , where ranki is the rank
of correct answer in response to query i. For
ACE/AQUAINT/WIKI we also give the accuracy of
?solvable? mentions, but this is not directly compa-
rable to (Ratinov et al, 2011). Our name dictionary
is different from theirs and ours has a higher recall
rate (Tbl. 2). Hence, the ?solvable? set is different.
k recall k recall
1 78.56 6 96.31
2 89.59 7 97.04
3 93.01 8 97.37
4 94.97 9 97.62
5 95.78 10 97.81
Table 3: Top k recall for local predictor g0.
4.4 Discussions
Table 4 shows the evaluation results on AIDA
dataset and Table 5 shows results on datasets
ACE/AQUAINT/WIKI.
Effect of cat:The first group in Table 4 shows
some baseline features for comparison. We can see
even if the categories only carry incomplete and
noisy information about an entity, it performs much
432
Methods Devset Testset
micro
p@1
macro
p@1
MRR micro
p@1
macro
p@1
MRR
cosine 33.25 28.61 46.03 33.33 28.63 46.54
jaccard 44.71 36.56 57.76 45.66 36.89 57.08
cat0 54.75 47.14 67.70 61.52 54.72 72.55
cat1 60.15 54.64 72.98 65.46 61.04 76.84
popularity 69.21 67.59 79.26 69.07 72.63 79.45
g0 76.04 73.63 84.21 76.16 78.17 84.58
g0+global(Ratinov) 81.30 78.03 88.14 81.45 81.89 88.70
g1+1fold 82.01 78.52 88.90 83.59 83.58 90.05
g1+5fold 81.99 78.42 88.87 83.52 83.37 89.99
g1+10fold 82.01 78.53 88.91 83.59 83.55 90.03
g1+top1 81.65 78.76 88.51 81.81 82.55 89.06
g1+top3 82.20 78.64 88.98 83.52 83.34 89.94
g1+top5 82.01 78.57 88.90 83.63 83.76 90.05
g1+top7 82.05 78.40 88.90 83.75 83.58 90.08
g0+cat 79.36 76.14 86.66 79.64 80.47 87.32
g1+cat 82.24 78.49 89.02 84.88 84.49 90.65
g1+cat+all context 82.99 78.56 89.51 86.49 85.11 91.55
(Hoffart et al, 2011) - - - 82.29 82.02 -
(Shirakawa et al, 2011) - - - 81.40 83.57 -
(Kulkarni et al, 2009) - - - 72.87 76.74 -
(Han et al, 2011) - - - 78.97 75.77 -
Table 4: Performance on AIDA dataset. Maximal value in each group are highlighted with bold font. top k means up
to k candidates are used for searching related instances with relational template.
better than word level features. Group 5 in Table
4 shows cat information generally boosts perfor-
mance for both predictor g0 and g1.
Effect of stacking: Group 3 in Table 4 shows the
results with different fold in stacking training. 1 fold
means training g0 with all training data and directly
augment training data with this g0. Surprisingly, we
do not observe any substantial difference with vari-
ous fold size. We deduce it is possible the way we
fire global features with top k candidates that alle-
viates the problem of train/test mismatch when ex-
tending feature space for g1. Despite the ranking of
true entity can be lower in testset than in training
set, the semantic coherence information can still be
captured with searching over top k candidates.
Effect of top k global features: Group 4 in Table
4 shows the effect of k on g1 performance. Clearly,
increasing k generally improves precision and one
possible reason is the improvement in recall when
searching for related instances. Table 3 shows the
top k recall of local predictor g0. Further increasing
k does not show any improvement.
Our method benefits from such a searching strat-
egy, and consistently outperforms the global fea-
tures of (Ratinov et al, 2011). While their method
is a trade-off between expensive exact search over
all mentions and greedy assigning all mentions
with local predictor, we show this idea can be fur-
ther extended, somewhat like increasing the beam
search size without additional computational over-
head. The only exception is the ACE dataset, since
this dataset is so small, the difference translates to
only one mention. One may notice the improvement
on ACE/AQUAINT datasets is a little inconsistent.
These datasets are much smaller and the results only
differ within 4 mentions. Because these models are
433
Method micro
p@1
macro
p@1
MRR correct
/ solv-
able
ACE
g0 77.43 81.30 79.03 95.22
Ratinov 77.43 80.70 78.81 95.22
g1+5fold 77.04 79.85 78.96 94.74
g0+cat 77.82 81.48 79.31 95.69
g1+cat 77.43 80.16 79.25 95.22
AQUAINT
g0 84.46 84.69 87.49 91.92
Ratinov 85.14 85.29 87.90 92.66
g1+5fold 85.83 85.55 88.27 93.41
g0+cat 85.01 85.00 87.89 92.51
g1+cat 85.28 85.14 88.23 92.81
Wikipedia test
g0 83.19 84.30 86.63 90.40
Ratinov 84.48 85.96 87.62 91.80
g1+5fold 84.81 86.29 88.13 92.15
g0+cat 84.38 86.13 87.51 91.69
g1+cat 85.45 87.16 88.31 92.86
Table 5: Evaluation on ACE/AQUAINT/WIKI datasets.
trained on Wikipedia, the annotation style can be
quite different.
Finally, as we analyze the development set of
AIDA, we discover that some location entities rely
on more distant information across the context, as
we increase the context to the entire contextual doc-
ument, we can gain extra performance boost.
4.5 Error analysis
As we analyze the development set of AIDA, we find
some general problems with location names. Loca-
tion name generally is not part of the main topic
of one document. Thus, comparing context with
its definition is not realistic. Most of the time, we
can find some related location names in context; but
other times, it is not easily distinguished. For in-
stance, in ?France beats Turkey in men?s football...?
France refers to ?France national football team? but
our system links it to the country page ?France? be-
cause it is more popular. This can be addressed by
modeling finer context (Sen, 2012) or local syntac-
tic pattern (Hoffart et al, 2011). In other cases,
our system misclassifies ?New York City? for ?New
York? and ?Netherlands? for ?Holland? and ?Peo-
ple?s Republic of China? for ?China?, because in
all these cases, the latter ones are the most popu-
lar in Wikipedia. It is even hard for us humans to
tell the difference based only on context or global
coherence.
5 Conclusions
We propose a stacking based collective entity link-
ing method, which stacks a global predictor on top
of a local predictor to collect coherence information
from neighboring decisions. It is fast and easy to im-
plement. Our method trades off between inefficient
exact search and greedily assigning mention with lo-
cal predictor. It can be seen as searching related
entities with relational template in stacked graphi-
cal learning, with beam size k. Furthermore, we
adopt recent progress in representation learning to
learn category-context coherence model. It scales
better than existing approaches on large knowledge
base and performs comparison in the latent semantic
space. Combining these two techniques, our model
consistently outperforms all existing more sophisti-
cated collective approaches in our experiments.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Program)
(No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009).
References
B. Bai, J. Weston, D. Grangier, R. Collobert, O. Chapelle,
and K. Weinberger. 2009. Supervised semantic index-
ing. In The 18th ACM Conference on Information and
Knowledge Management (CIKM).
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 6, pages 708?716.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
434
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, pages 765?774. ACM.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 782?792. Association for Computational Lin-
guistics.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the tac 2011
knowledge base population track. In Proceedings of
the Fourth Text Analysis Conference.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
ACM.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hierar-
chical topic models. In Proceedings of KDD.
Zhenzhen Kou and William W Cohen. 2007. Stacked
graphical models for efficient inference in markov ran-
dom fields. In SDM.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
457?466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and Y. Shi.
2010. Lcc approaches to knowledge base population
at tac 2010. In Proc. TAC 2010 Workshop.
F. Li, Z. Zheng, F. Bu, Y. Tang, X. Zhu, and M. Huang.
2009. Thu quanta at tac 2009 kbp and rte track. In
Proceedings of Test Analysis Conference 2009 (TAC
09).
Andre? FT Martins, Dipanjan Das, Noah A Smith, and
Eric P Xing. 2008. Stacking dependency parsers. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 157?166. As-
sociation for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proceedings of the Annual Meeting of
the Association of Computational Linguistics (ACL).
P. Sen. 2012. Collective context-aware topic models
for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages
729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011.
Entity disambiguation based on a probabilistic
taxonomy. Technical report, Technical Report
MSR-TR-2011-125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 354?362. Asso-
ciation for Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In ACL, pages 1385?1394.
David H Wolpert. 1992. Stacked generalization. Neural
networks, 5(2):241?259.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483?491, Los Angeles, California, June. Association
for Computational Linguistics.
435
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 266?277,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Go Climb a Dependency Tree and Correct the Grammatical Errors
Longkai Zhang Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn
Abstract
State-of-art systems for grammar error
correction often correct errors based on
word sequences or phrases. In this paper,
we describe a grammar error correction
system which corrects grammatical errors
at tree level directly. We cluster all error
into two groups and divide our system into
two modules correspondingly: the general
module and the special module. In the
general module, we propose a TreeNode
Language Model to correct errors related
to verbs and nouns. The TreeNode Lan-
guage Model is easy to train and the de-
coding is efficient. In the special module,
two extra classification models are trained
to correct errors related to determiners and
prepositions. Experiments show that our
system outperforms the state-of-art sys-
tems and improves the F
1
score.
1 Introduction
The task of grammar error correction is difficult
yet important. An automatic grammar error cor-
rection system can help second language (L2)
learners improve the quality of their writing. In re-
cent years, there are various competitions devoted
to grammar error correction, such as the HOO-
2011(Dale and Kilgarriff, 2011), HOO-2012(Dale
et al., 2012) and the CoNLL-2013 shared task (Ng
et al., 2013). There has been a lot of work ad-
dressing errors made by L2 learners. A significant
proportion of the systems for grammar error cor-
rection train individual statistical models to cor-
rect each special kind of error word by word and
ignore error interactions. These methods assume
no interactions between different kinds of gram-
matical errors. In real problem settings errors are
correlated, which makes grammar error correction
much more difficult.
Recent research begins to focus on the error
interaction problem. For example, Wu and Ng
(2013) decodes a global optimized result based
on the individual correction confidence of each
kind of errors. The individual correction confi-
dence is still based on the noisy context. Ro-
zovskaya and Roth (2013) uses a joint modeling
approach, which considers corrections in phrase
structures instead of words. For dependencies that
are not covered by the joint learning model, Ro-
zovskaya and Roth (2013) uses the results of Illi-
nois system in the joint inference. These results
are still at word level and are based on the noisy
context. These systems can consider error inter-
actions, however, the systems are complex and
inefficient. In both Wu and Ng (2013) and Ro-
zovskaya and Roth (2013), Integer Linear Pro-
gramming (ILP) is used for decoding a global op-
timized result. In the worst case, the time com-
plexity of ILP can be exponent.
In contrast, we think a better grammar error cor-
rection system should correct grammatical errors
at sentence level directly and efficiently. The sys-
tem should correct as many kinds of errors as pos-
sible in a generalized framework, while allowing
special models for some kinds of errors that we
need to take special care. We cluster all error into
two groups and correspondingly divide our sys-
tem into two modules: the general module and the
special module. In the general module, our sys-
tem views each parsed sentence as a dependency
tree. The system generates correction candidates
for each node on the dependency tree. The cor-
rection can be made on the dependency tree glob-
ally. In this module, nearly all replacement errors
related to verb form, noun form and subject-verb
agreement errors can be considered. In the spe-
cial module, two extra classification models are
used to correct the determiner errors and preposi-
tion errors . The classifiers are also trained at tree
node level. We take special care of these two kinds
266
of errors because these errors not only include re-
placement errors, but also include insertion and
deletion errors. A classification model is more
suitable for handling insertion and deletion errors.
Besides, they are the most common errors made
by English as a Second Language (ESL) learners
and are much easier to be incorporated into a clas-
sification framework.
We propose a TreeNode Language Model
(TNLM) to efficiently measure the correctness of
selecting a correction candidate of a node in the
general module. Similar to the existing statistical
language models which assign a probability to a
linear chain of words, our TNLM assigns correct-
ness scores directly on each node on the depen-
dency tree. We select candidates for each node
to maximize the global correctness score and use
these candidates to form the corrected sentence.
The global optimized inference can be tackled ef-
ficiently using dynamic programming. Because
the decoding is based on the whole sentence, error
interactions can be considered. Our TNLM only
needs to use context words related to each node
on the dependency tree. Training a TreeNode lan-
guage model costs no more than training ordinary
language models on the same corpus. Experiments
show that our system can outperform the state-of-
art systems.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe the
task and give an overview of the system. In section
3 we describe the general module and in section 4
we describe the special module. Experiments are
described in section 5. In section 6 related works
are introduced, and the paper is concluded in the
last section.
2 Task and System Overview
2.1 Task Description
The task of grammar error correction aims to cor-
rect grammatical errors in sentences. There are
various competitions devoted to the grammar er-
ror correction task for L2 learners. The CoNLL-
2013 shared task is one of the most famous, which
focuses on correcting five types of errors that
are commonly made by non-native speakers of
English, including determiner, preposition, noun
number, subject-verb agreement and verb form er-
rors. The training data released by the task orga-
nizers come from the NUCLE corpus(Dahlmeier
et al., 2013). This corpus contains essays writ-
ten by ESL learners, which are then corrected by
English teachers. The test data are 50 student es-
says. Details of the corpus are described in Ng
et al. (2013).
2.2 System Architecture
In our system, lists of correction candidates are
first generated for each word. We generate can-
didates for nouns based on their plurality. We gen-
erate candidates for verbs based on their tenses.
Then we select the correction candidates that max-
imize the overall correctness score. An example
process of correcting figure 1(a) is shown in table
1.
Correcting grammatical errors using local sta-
tistical models on word sequence is insufficient.
The local models can only consider the contexts
in a fixed window. In the example of figure 1(a),
the context of the verb ?is? is ?that boy is on the?,
which sounds reasonable at first glance but is in-
correct when considering the whole sentence. The
limitation of local classifiers is that long distance
syntax information cannot be incorporated within
the local context. In order to effectively use the
syntax information to get a more accurate correct-
ing result, we think a better way is to tackle the
problem directly at tree level to view the sentence
as a whole. From figure 1(a) we can see that the
node ?is? has two children on the dependency tree:
?books? and ?on?. When we consider the node
?is?, its context is ?books is on?, which sounds in-
correct. Therefore, we can make better corrections
using such context information on nodes.
Therefore, our system corrects grammatical er-
rors on dependency trees directly. Because the
correlated of words are more linked on trees than
in a word sequence, the errors are more easier to
be corrected on the trees and the agreement of dif-
ferent error types is guaranteed by the edges. We
follow the strategy of treating different kinds of
errors differently, which is used by lots of gram-
mar error correction systems. We cluster the five
types of errors considered in CoNLL-2013 into
two groups and divide our system into two mod-
ules correspondingly.
? The general module, which is responsible
for the verb form errors, noun number errors
and subject-verb agreement errors. These er-
rors are all replacement errors, which can
be corrected by replacing the wrongly used
word with a reasonable candidate word.
267
Figure 1: Dependency parsing results of (a) the original sentence ?The books of that boy is on the desk
.? (b) the corrected sentence.
Position Original Correction Candidates Corrected
1 The The The
2 books books, book books
3 of of of
4 that that that
5 boy boy, boys boy
6 is is,are,am,was,were,be,being,been are
7 on on on
8 the the the
9 desk desk, desks desk
10 . . .
Table 1: An example of the ?correction candidate generation and candidate selection? framework.
? The special module, where two classifica-
tion models are used to correct the determiner
errors and preposition errors at tree level. We
take special care of these two kinds of errors
because these errors include both replace-
ment errors and insertion/deletion errors. Be-
sides, they are the most common errors made
by ESL learners and is much easier to be in-
corporated into a classification framework.
We should make it clear that we are not the first
to use tree level correction models on ungrammat-
ical sentences. Yoshimoto et al. (2013) uses a
Treelet Language model (Pauls and Klein, 2012)
to correct agreement errors. However, the perfor-
mance of Treelet language model is not that good
compared with the top-ranked system in CoNLL-
2013. The reason is that the production rules in the
Treelet language model are based on complex con-
texts, which will exacerbate the data sparseness
problem. The ?context? in Treelet language model
also include words ahead of treelets, which are
sometimes unrelated to the current node. In con-
trast, our TreeNode Language model only needs to
consider useful context words related to each node
on the dependency tree. To train a TreeNode lan-
guage model costs no more than training ordinary
language models on the same corpus.
2.3 Data Preparation
Our system corrects grammatical errors on de-
pendency trees directly, therefore the sentences
in training and testing data should have been
parsed before being corrected. In our system, we
use the Stanford parser
1
to parse the New York
Times source of the Gigaword corpus
2
, and use the
parsed sentences as our training data. We use the
original training data provided by CoNLL-2013 as
the develop set to tune all parameters.
Some sentences in the news texts use a differ-
ent writing style against the sentences written by
ESL learners. For example, sentences written by
ESL learners seldom include dialogues between
people, while very often news texts include para-
graphs such as ??I am frightened!? cried Tom?. We
use heuristic rules to eliminate the sentences in the
Gigaword corpus that are less likely to appear in
the ESL writing. The heuristic rules include delet-
1
http://nlp.stanford.edu/software/lex-parser.shtml
2
https://catalog.ldc.upenn.edu/LDC2003T05
268
ing sentences that are too short or too long
3
, delet-
ing sentences that contains certain punctuations
such as quotation marks, or deleting sentences that
are not ended with a period.
In total we select and parse 5 million sen-
tences of the New York Times source of English
newswire in the Gigaword corpus. We build the
system and experiment based on these sentences.
3 The General Module
3.1 Overview
The general module aims to correct verb form er-
rors, noun number errors and subject-verb agree-
ment errors. Other replacement errors such as
spelling errors can also be incorporated into the
general module. Here we focus on verb form er-
rors, noun number errors and subject-verb agree-
ment errors only. Our general module views each
sentence as a dependency tree. All words in the
sentence form the nodes of the tree. Nodes are
linked through directed edges, annotated with the
dependency relations.
Before correcting the grammatical errors, the
general module should generate correction candi-
dates for each node first. For each node we use
the word itself as its first candidate. Because the
general module considers errors related to verbs
and nouns, we generate extra correction candi-
dates only for verbs and nouns. For verbs we use
all its verb forms as its extra candidates. For ex-
ample when considering the word ?speaks?, we
use itself and {speak, spoke, spoken, speaking}
as its correction candidates. For nouns we use
its singular form and plural form as its extra cor-
rection candidates. For example when consider-
ing the word ?dog?, we use itself and ?dogs? as
its correction candidate. If the system selects the
original word as the final correction, the sentence
remains unchanged. But for convenience we still
call the newly generated sentence ?the corrected
sentence?.
In a dependency tree, the whole sentence
s can be formulized as a list of production
rules r
1
, ..., r
L
of the form: [r = head ?
modifier
1
,modifier
2
...]. An example of all
production rules of figure 1(a) is shown in table
2. Because the production rules are made up of
words, selecting a different correction candidate
for only one node will result in a list of different
3
In our experiment, no less than 5 words and no more than
30 words.
production rules. For example, figure 1(b) selects
the correction candidate ?is? to replace the origi-
nal ?are?. Therefore the production rules of figure
1(b) include [are ? books, on], instead of [is ?
books, on] in figure 1(a).
books? The, of
of? boy
boy? that
is? books, on
on? desk
desk? the
Table 2: All the production rules in the example of
figure 1(a)
The overall correctness score of s, which
is score(s), can be further decomposed into
?
L
i=0
score(r
i
). A reasonable score function
should score the correct candidate higher than the
incorrect one. Consider the node ?is? in Figure
1(a), the production rule with head ?is? is [is ?
books, on]. Because the correction of ?is? is ?are?,
a reasonable scorer should have score([is ?
books, on]) < score([are? books, on]).
Given the formulation of sentence s =
[r
1
, ..., r
L
] and the candidates for each node, we
are faced with two problems:
1. Score Function. Given a fixed selection of
candidate for each node, how to compute
the overall score of the dependency tree, i.e.,
score(s). Because score(s) is decomposed
into
?
L
i=0
score(r
i
), the problem becomes
finding a score function to measure the cor-
rectness of each r given a fixed selection of
candidates.
2. Decoding. Given each node a list of correc-
tion candidates and a reasonable score func-
tion score(r) for the production rules, how to
find the selection of candidates that maximize
the overall score of the dependency tree.
For the first problem, we propose a TreeNode
Language Model as the correctness measure of a
fixed candidate selection. For the decoding prob-
lem, we use a dynamic programming method to
efficiently find the correction candidates that max-
imize the overall score. We will describe the de-
tails in the following sections.
One concern is whether the automatically
parsed trees are reliable for grammar error cor-
rection. We define ?reliable? as follows. If we
269
change some words in original sentence into their
reasonable correction candidates (e.g. change ?is?
to ?are?) but the structure of the dependency tree
does not change (except the replaced word and
its corresponding POS tag, which are definitely
changed), then we say the dependency tree is reli-
able for this sentence. To verify this we randomly
selected 1000 sentences parsed by the Stanford
Parser. We randomly select the verbs and nouns
and replace them with a wrong form. We parsed
the modified sentences again and asked 2 annota-
tors to examine whether the dependency trees are
reliable for grammar error correction. We find that
99% of the dependency trees are reliable. There-
fore we can see that the dependency tree can be
used as the structure for grammar error correction
directly.
3.2 TreeNode Language Model
In our system we use the score of TreeNode Lan-
guage Model (TNLM) as the scoring function.
Consider a node n on a dependency tree and as-
sume n has K modifiers C
1
, ..., C
K
as its child
nodes. We define Seq(n) = [C
1
, ..., n, ..., C
K
]
as an ordered sub-sequence of nodes that includes
the node n itself and all its child nodes. The or-
der of the sub-sequence in Seq(n) is sorted based
on their position in the sentence. In this formula-
tion, we can score the correctness of a production
rule r by scoring the correctness of Seq(n). Be-
cause Seq(n) is a word sequence, we can use a
language model to measure its correctness. The
sub-sequences are not identical to the original
text. Therefore instead of using ordinary language
models, we should train special language models
using the sub-sequences to measure the correct-
ness of a production rule.
Take the sentence in figure 2 as an example.
When considering the node ?is? in the word se-
quence, it is likely to be corrected into ?are? be-
cause it appear directly after the plural noun ?par-
ents?. However, by the definition above, the sub-
sequence corresponding to the node ?damaged? is
?car is damaged by ?. In such context, the word
?is? is less likely to be changed to ?are?. From
the example we can see that the sub-sequence is
suitable to be used to measure the correctness of
a production rule. From this example we can also
find that the sub-sequences are different with or-
dinary sentences, because ordinary sentences are
less likely to end with ?by?.
Table 3 shows all the sub-sequences in the ex-
ample of figure 2. If we collect all the sub-
sequences in the corpus to form a new sub-
sequence corpus, we can train a language model
based on the new sub-sequence corpus. This is
our TreeNode Language Model. One advantage
of TLM is that once we have generated the sub-
sequences, we can train the TLM in the same
way as we train ordinary language models. Be-
sides, the TLM is not limited to a fixed smoothing
method. Any smoothing methods for ordinary lan-
guage models are applicable for TLM.
Node Sub-sentence
The The
car The car of
of of parents
my my
parents my parents
is is
damaged car is damaged by
by by storm
the the
storm the storm
Table 3: All the sub-sentences in the example of
figure 2
In our system we train the TLM using the same
way as training tri-gram language model. For a
sub-sequence S = w
0
...w
L
, we calculate P (S) =
?
L
i=0
P (w
i
|w
i?1
w
i?2
). The smoothing method
we use is interpolation, which assumes the final
P
?
(w
i
|w
i?1
w
i?2
) of the tri-gram language model
follows the following decomposition:
P
?
(w
i
|w
i?1
w
i?2
) =?
1
P (w
i
|w
i?1
w
i?2
)
+?
2
P (w
i
|w
i?1
)
+?
3
P (w
i
)
(1)
where ?
1
, ?
2
and ?
3
are parameters sum to
1. The parameters ?
1
, ?
2
and ?
3
are estimated
through EM algorithm(Baum et al., 1970; Demp-
ster et al., 1977; Jelinek, 1980).
3.3 Decoding
The decoding problem is to select one correction
candidate for each node that maximizes the over-
all score of the corrected sentence. When the sen-
tence is long and contains many verbs and nouns,
enumerating all possible candidate selections is
time-consuming. We use a bottom-up dynamic
270
Figure 2: A illustrative sentence for TreeNode Language Model.
programming approach to find the maximized cor-
rections within polynomial time complexity.
For a node n with L correction candidates
n
1
, ...n
L
and K child nodes C
1
, ..., C
K
, we define
n.scores[i] as the maximum score if we choose
the ith candidate n
i
for n. Because we decode
from leaves to the root, C
1
.scores, ..., C
K
.scores
have already been calculated before we calculate
n.scores.
We assume the sub-sequence Seq(n
i
) =
[C
1
, ..., C
M
, n
i
, C
M+1
, ..., C
K
] without loss of
generality, where C
1
, .., C
M
are the nodes before
n
i
and C
M+1
, ..., C
K
are the nodes after n
i
.
We define c
i,j
as the jth correction can-
didate of child node C
i
. Given a se-
lection of candidates for each child node
seq = [c
1,j
1
, ..., c
M,j
M
, n
i
, c
M+1,j
M+1
, ..., c
K,j
K
],
we can calculate score(seq) as:
score(seq) = TNLM(seq)
K
?
i=1
C
i
.scores[j
i
]
(2)
where TNLM(seq) is the TreeNode Language
Model score of seq. Then, n.scores[i] is calcu-
lated as:
n.scores[i] = max
?seq
score(seq) (3)
Because seq is a word sequence, the maxi-
mization can be efficiently calculated using Viterbi
algorithm (Forney Jr, 1973). To be specific,
the Viterbi algorithm uses the transition scores
and emission scores as its input. The transition
scores in our model are the tri-gram probabilities
from our tri-gram TNLM. The emission scores
in our model are the candidate scores of each
child: C
1
.scores, ..., C
K
.scores, which have al-
ready been calculated.
After the bottom-up calculation, we only need
to look into the ?ROOT? node to find the maxi-
mum score of the whole tree. Similar to the Viterbi
algorithm, back pointers should be kept to find
which candidate is selected for the final corrected
sentence. Detailed decoding algorithm is shown in
table 4.
Function decode(Node n)
if n is leaf
set n.scores uniformly
return
for each child c of n
decode(c)
calculating n.scores using Viterbi
End Function
BEGIN
decode(ROOT )
find the maximum score for the tree and back-
track all candidates
END
Table 4: The Decoding algorithm
In the real world implementations, we add a
controlling parameter for the confidence of the
correctness of the inputs. We multiply ? on
P (w
0
|w
?2
w
?1
) of the tri-gram TNLM if the cor-
recting candidate w
0
is the same word in the orig-
inal input. ? is larger than 1 to ?emphasis? the
confidence of the original word because the most
of the words in the inputs are correct. The value of
? can be set using the development data.
3.4 The Special Module
The special module is designed for determiner er-
rors and preposition errors. We take special care
of these two kinds of errors because these errors
include insertion and deletion errors, which can-
not be corrected in the general module. Because
there is a fixed number of prepositions and deter-
miners, these two kinds of errors are much easier
to be incorporated into a classification framework.
Besides, they are the most common errors made by
ESL learners and there are lots of previous works
that leave valuable guidance for us to follow.
Similar to many previous state-of-art systems,
we treat the correction of determiner errors and
preposition errors as a classification problem. Al-
though some previous works (e.g. Rozovskaya
et al. (2013)) use NPs and the head of NPs as
271
features, they are basically local classifiers mak-
ing predictions on word sequences. Difference to
the local classifier approaches, we make predic-
tions on the nodes of the dependency tree directly.
In our system we correct determiner errors and
preposition errors separately.
For the determiner errors, we consider the in-
sertion, deletion and replacement of articles (i.e.
?a?, ?an? and ?the?). Because the articles are used
to modify nouns in the dependency trees, we can
classify based on noun nodes. We give each noun
node (node whose POS tag is noun) a label to in-
dicate which article it should take. We use left po-
sition (LP) and right position (RP) to specify the
position of the article. The article therefore lo-
cates between LP and RP. If a noun node already
has an article as its modifier, then LP will be the
position directly ahead of the article. In this case,
RP = LP + 2. If an insertion is needed, the RP
is the position of the first child node of the noun
node. In this case LP = RP ? 1. With this no-
tation, detailed feature templates we use to correct
determiner errors are listed in table 5. In our model
we use 3 labels: ?a?, ?the? and ???. We use ?a?, ?the?
to represent a noun node should be modified with
?a? or ??the? correspondingly. We use ???? to in-
dicate that no article is needed for the noun node.
We use rule-based method to distinguish between
?a? and ?an? as a post-process.
For the preposition errors, we only consider
deletion and replacement of an existing preposi-
tion. The classification framework is similar to
determiner errors. We consider classification on
preposition nodes (nodes whose POS tag is prepo-
sition). We use prepositions as labels to indicate
which preposition should be used. and use ???
to denote that the preposition should be deleted.
We use the same definition of LP and RP as the
correction of determiner errors. Detailed feature
templates we use to correct preposition errors are
listed in table 6. Similar to the previous work(Xing
et al., 2013), we find that adding more preposi-
tions will not improve the performance in our ex-
periments. Thus we only consider a fixed set of
prepositions: {in, for, to, of, on}.
Previous works such as Rozovskaya et al.
(2013) show that Naive Bayes model and averaged
perceptron model show better results than other
classification models. These classifiers can give a
reasonably good performance when there are lim-
ited amount of training data. In our system, we use
large amount of automatically generated training
data based on the parsed Gigaword corpus instead
of the limited training data provided by CoNLL-
2013.
Take generating training data for determiner er-
rors as an example. We generate training data
based on the parsed Gigaword corpus C described
in section 2. Each sentence S in C is a depen-
dency tree T . We use each noun node N on T as
one training instance. If N is modified by ?the?,
its label will be ?THE?. If N is modified by ?a?
or ?an?, its label will be ?A?. Otherwise its label
will be ?NULL?. Then we just omit the determiner
modifier and generate features based on table 5.
Generating training data for preposition errors is
the same, except we use preposition nodes instead
of noun nodes.
By generating training instances in this way, we
can get large amount of training data. Therefore
we think it is a good time to try different classifi-
cation models with enough training data. We ex-
periment on Naive Bayes, Averaged Perceptron,
SVM and Maximum Entropy models (ME) in a 5-
fold cross validation on the training data. We find
ME achieves the highest accuracy. Therefore we
use ME as the classification model in our system.
4 Experiment
4.1 Experiment Settings
In the experiments, we use our parsed Gigaword
corpus as the training data, use the training data
provided by CoNLL-2013 as the develop data, and
use the test data of CoNLL-2013 as test data di-
rectly. In the general module, the training data
is used for the training of TreeNode Language
Model. In the special module, the training data is
used for training individual classification models.
We use the M2 scorer (Dahlmeier and Ng,
2012b) provided by the organizer of CoNLL-2013
for the evaluation of our system. The M2 scorer
is widely used as a standard scorer in previous
systems. Because we make comparison with the
state-of-art systems on the CoNLL-2013 corpus,
we use the same evaluation metric F
1
score of M2
scorer as the evaluation metric.
In reality, some sentences may have more than
one kind of possible correction. As the example in
?The books of that boy is on the desk.?, the cor-
responding correction can be either ?The books of
that boy are on the desk.? or ?The book of that boy
is on the desk.?. The gold test data can only con-
272
Word Features w
LP
, w
LP?1
, w
LP?2
, w
RP
, w
RP+1
, w
RP+2
, w
LP?2
w
LP?1
, w
LP?1
w
LP
,
w
LP
w
RP
, w
RP
w
RP+1
, w
RP+1
w
RP+2
, w
LP?2
w
LP?1
w
LP
, w
LP?1
w
LP
w
RP
,
w
LP
w
RP
w
RP+1
, w
RP
w
RP+1
w
RP+2
Noun Node
Features
NN , w
LP
NN , w
LP?1
w
LP
NN , w
LP?2
w
LP?1
w
LP
NN
Father/Child
Node Features
Fa, w
RP
Fa, w
RP
w
RP+1
Fa, w
RP
w
RP+1
w
RP+2
Fa, Fa&Ch
Table 5: Feature templates for the determiner errors. w
i
is the word at the ith position. NN is the current
noun node. Fa is the father node of the current noun node. Ch is a child node of the current noun node.
Word Features w
LP
, w
LP?1
, w
LP?2
, w
RP
, w
RP+1
, w
RP+2
, w
LP?2
w
LP?1
, w
LP?1
w
LP
,
w
LP
w
RP
, w
RP
w
RP+1
, w
RP+1
w
RP+2
, w
LP?2
w
LP?1
w
LP
, w
LP?1
w
LP
w
RP
,
w
LP
w
RP
w
RP+1
, w
RP
w
RP+1
w
RP+2
Father/Child
Node Features
Fa, w
RP
Fa, w
RP
w
RP+1
Fa, w
RP
w
RP+1
w
RP+2
Fa, Fa&Ch
Table 6: Feature templates for preposition errors. w
i
is the word at the ith position. Fa is the father node
of the current preposition node. Ch is a child node of the current preposition node.
sider a small portion of possible answers. To re-
lieve this, the CoNLL-2013 shared task allows all
participating teams to provide alterative answers
if they believe their system outputs are also cor-
rect. These alterative answers form the ?Revised
Data? in the shared task, which indeed help evalu-
ate the outputs of the participating systems. How-
ever, the revised data only include alterative cor-
rections from the participating teams. Therefore
the evaluation is not that fair for future systems. In
our experiment we only use the original test data
as the evaluation dataset.
4.2 Experiment Results
We first show the performance of each stage of our
system. In our system, the general module and
the special module correct grammar errors conse-
quently. Therefore in table 7 we show the perfor-
mance when each component is added to the sys-
tem.
Method P R F1 score
TNLM 33.96% 17.71% 23.28%
+Det 32.83% 38.28% 35.35%
+Prep 32.64% 39.20% 35.62%
Table 7: Results of each stage in our system.
TNLM is the general module. ?+Det? is the sys-
tem containing the general module and determiner
part of special module.?+Prep? is the final system
We evaluate the effect of using TreeNode lan-
guage model for the general module. We compare
the TNLM with ordinary tri-gram language model.
We use the same amount of training data and the
same smoothing strategy (i.e. interpolation) for
both of them. Table 8 shows the comparison. The
TNLM can improve the F
1
by +2.1%.
Method P R F1 score
Ordinary LM 29.27% 16.68% 21.27%
Our TNLM 33.96% 17.71% 23.28%
Table 8: Comparison for the general module
between TNLM and ordinary tri-gram language
model on the test data.
Based on the result of the general module using
TNLM, we compare our tree level special mod-
ule against the local classification approach. The
special module of our system makes predictions
on the dependency tree directly, while local clas-
sification approaches make predictions on linear
chain of words and decide the article of a noun
Phrase or the preposition of a preposition phrase.
We use the same word level features for the two
approaches except for the local classifiers we do
not add tree level features. Table 9 shows the com-
parison.
When using the parsed Gigaword texts as train-
ing data, the quality of the sentences we select
will influence the result. For comparison, we ran-
domly select the same amount of sentences from
the same source of Gigaword and parse them as
a alterative training set. Table 10 shows the com-
parison between random chosen training data and
273
Method P R F1 score
Local Classifier 26.38% 39.14% 31.51%
Our Tree-based 32.64% 39.20% 35.62%
Table 9: Comparison for the special module on the
test data. The input of the special module is the
sentences corrected by the TNLM in the general
module.
the selected training data of our system. We can
see that the data selection (cleaning) procedure is
important for the improvement of system F1.
Method P R F1 score
Random 31.89% 35.85% 33.75%
Selected 32.64% 39.20% 35.62%
Table 10: Comparison of training using random
chosen sentences and selected sentences.
Method F1 score
Rozovskaya et al. (2013) 31.20%
Kao et al. (2013) 25.01%
Yoshimoto et al. (2013) 22.17%
Rozovskaya and Roth (2013) 35.20%
Our method 35.62%
Table 11: Comparison of F1 of different systems
on the test data .
4.3 Comparison With Other Systems
We also compare our system with the state-of-
art systems. The first two are the top-2 systems
at CoNLL-2013 shared task : Rozovskaya et al.
(2013) and Kao et al. (2013). The third one is
the Treelet Language Model in Yoshimoto et al.
(2013). The fourth one is Rozovskaya and Roth
(2013), which until now shows the best perfor-
mance. The comparison on the test data is shown
in table 11.
In CoNLL-2013 only 5 kinds of errors are con-
sidered. Our system can be slightly modified to
handle the case where other errors such as spelling
errors should be considered. In that case, we can
modify the candidate generation of the general
module. We only need to let the generate cor-
rection candidates be any possible words that are
similar to the original word, and run the same de-
coding algorithm to get the corrected sentence. As
a comparison, the ILP systems should add extra
scoring system to score extra kind of errors.
5 Related Works
Early grammatical error correction systems use
the knowledge engineering approach (Murata and
Nagao, 1994; Bond et al., 1996; Bond and Ikehara,
1996; Heine, 1998). However, manually designed
rules usually have exceptions. Therefore, the ma-
chine learning approach has become the dominant
approach recently. Previous machine learning ap-
proaches typically formulates the task as a clas-
sification problem. Of all the errors, determiner
and preposition errors are the two main research
topics (Knight and Chander, 1994; AEHAN et al.,
2006; Tetreault and Chodorow, 2008; Dahlmeier
and Ng, 2011). Features used in the classifica-
tion models include the context words, POS tags,
language model scores (Gamon, 2010), and tree
level features (Tetreault et al., 2010). Models used
include maximum entropy (AEHAN et al., 2006;
Tetreault and Chodorow, 2008), averaged percep-
tron, Naive Bayes (Rozovskaya and Roth, 2011),
etc. Other errors such as verb form and noun num-
ber errors also attract some attention recently (Liu
et al., 2010; Tajiri et al., 2012).
Recent research efforts have started to deal with
correcting different errors jointly (Gamon, 2011;
Park and Levy, 2011; Dahlmeier and Ng, 2012a;
Wu and Ng, 2013; Rozovskaya and Roth, 2013).
Gamon (2011) uses a high-order sequential label-
ing model to detect various errors. Park and Levy
(2011) models grammatical error correction using
a noisy channel model. Dahlmeier and Ng (2012a)
uses a beam search decoder, which iteratively cor-
rects to produce the best corrected output. Wu and
Ng (2013) and Rozovskaya and Roth (2013) use
ILP to decode a global optimized result. The joint
learning and joint inference are still at word/phrase
level and are based on the noisy context. In the
worst case, the time complexity of ILP can reach
exponent. In contrast, our system corrects gram-
mar errors at tree level directly, and the decoding
is finished with polynomial time complexity.
6 Conclusion and Future work
In this paper we describe our grammar error cor-
rection system which corrects errors at tree level
directly. We propose a TreeNode Language Model
and use it in the general module to correct errors
related to verbs and nouns. The TNLM is easy to
train and the decoding of corrected sentence is ef-
ficient. In the special module, two extra classifica-
tion models are trained to correct errors related to
274
determiners and prepositions at tree level directly.
Because our current method depends on an auto-
matically parsed corpus, future work may include
applying some additional filtering (e.g. Mejer and
Crammer (2012)) of the extended training set ac-
cording to some confidence measure of parsing ac-
curacy.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018), Major National Social
Science Fund of China (No.12&ZD227) and Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101). The contact author of this pa-
per, according to the meaning given to this role
by Key Laboratory of Computational Linguistics,
Ministry of Education, School of Electronics En-
gineering and Computer Science, Peking Univer-
sity, is Houfeng Wang. We thank Longyue Wang
and the reviewers for their comments and sugges-
tions.
References
AEHAN, N., Chodorow, M., and LEACOCK,
C. L. (2006). Detecting errors in english arti-
cle usage by non-native speakers.
Baum, L. E., Petrie, T., Soules, G., and Weiss, N.
(1970). A maximization technique occurring in
the statistical analysis of probabilistic functions
of markov chains. The annals of mathematical
statistics, pages 164?171.
Bond, F. and Ikehara, S. (1996). When and how to
disambiguate??countability in machine trans-
lation?. In International Seminar on Multi-
modal Interactive Disambiguation: MIDDIM-
96, pages 29?40. Citeseer.
Bond, F., Ogura, K., and Kawaoka, T. (1996).
Noun phrase reference in japanese-to-english
machine translation. arXiv preprint cmp-
lg/9601008.
Dahlmeier, D. and Ng, H. T. (2011). Grammatical
error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-
Volume 1, pages 915?923. Association for
Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012a). A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 568?578. Associa-
tion for Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012b). Better eval-
uation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 568?572. Association for Com-
putational Linguistics.
Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).
Building a large annotated corpus of learner en-
glish: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 22?31.
Dale, R., Anisimoff, I., and Narroway, G. (2012).
Hoo 2012: A report on the preposition and de-
terminer error correction shared task. In Pro-
ceedings of the Seventh Workshop on Build-
ing Educational Applications Using NLP, pages
54?62. Association for Computational Linguis-
tics.
Dale, R. and Kilgarriff, A. (2011). Helping our
own: The hoo 2011 pilot shared task. In Pro-
ceedings of the 13th European Workshop on
Natural Language Generation, pages 242?249.
Association for Computational Linguistics.
Dempster, A. P., Laird, N. M., Rubin, D. B., et al.
(1977). Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal
statistical Society, 39(1):1?38.
Forney Jr, G. D. (1973). The viterbi algorithm.
Proceedings of the IEEE, 61(3):268?278.
Gamon, M. (2010). Using mostly native data
to correct errors in learners? writing: a meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 163?171. As-
sociation for Computational Linguistics.
Gamon, M. (2011). High-order sequence model-
ing for language learner error detection. In Pro-
ceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications,
275
pages 180?189. Association for Computational
Linguistics.
Heine, J. E. (1998). Definiteness predictions
for japanese noun phrases. In Proceedings
of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th
International Conference on Computational
Linguistics-Volume 1, pages 519?525. Associ-
ation for Computational Linguistics.
Jelinek, F. (1980). Interpolated estimation of
markov source parameters from sparse data.
Pattern recognition in practice.
Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,
Boisson, J., Wu, J.-c., and Chang, J. S. (2013).
Conll-2013 shared task: Grammatical error cor-
rection nthu system description. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 20?25, Sofia, Bulgaria. Association
for Computational Linguistics.
Knight, K. and Chander, I. (1994). Automated
postediting of documents. In AAAI, volume 94,
pages 779?784.
Liu, X., Han, B., Li, K., Stiller, S. H., and Zhou,
M. (2010). Srl-based verb selection for esl.
In Proceedings of the 2010 conference on em-
pirical methods in natural language process-
ing, pages 1068?1076. Association for Compu-
tational Linguistics.
Mejer, A. and Crammer, K. (2012). Are you
sure? confidence in prediction of dependency
tree edges. In Proceedings of the 2012 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 573?576,
Montr?eal, Canada. Association for Computa-
tional Linguistics.
Murata, M. and Nagao, M. (1994). Determination
of referential property and number of nouns in
japanese sentences for machine translation into
english. arXiv preprint cmp-lg/9405019.
Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., and
Tetreault, J. (2013). The conll-2013 shared task
on grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria. Association
for Computational Linguistics.
Park, Y. A. and Levy, R. (2011). Automated
whole sentence grammar correction using a
noisy channel model. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies-Volume 1, pages 934?944. Asso-
ciation for Computational Linguistics.
Pauls, A. and Klein, D. (2012). Large-scale syn-
tactic language modeling with treelets. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long
Papers-Volume 1, pages 959?968. Association
for Computational Linguistics.
Rozovskaya, A., Chang, K.-W., Sammons, M.,
and Roth, D. (2013). The university of illi-
nois system in the conll-2013 shared task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13?19, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2011). Algorithm
selection and model adaptation for esl correc-
tion tasks. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-
Volume 1, pages 924?933. Association for
Computational Linguistics.
Rozovskaya, A. and Roth, D. (2013). Joint learn-
ing and inference for grammatical error correc-
tion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 791?802, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Tajiri, T., Komachi, M., and Matsumoto, Y.
(2012). Tense and aspect error correction for
esl learners using global context. In Proceed-
ings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short
Papers-Volume 2, pages 198?202. Association
for Computational Linguistics.
Tetreault, J., Foster, J., and Chodorow, M. (2010).
Using parse features for preposition selection
and error detection. In Proceedings of the acl
2010 conference short papers, pages 353?358.
Association for Computational Linguistics.
Tetreault, J. R. and Chodorow, M. (2008). The
ups and downs of preposition error detec-
tion in esl writing. In Proceedings of the
22nd International Conference on Computa-
276
tional Linguistics-Volume 1, pages 865?872.
Association for Computational Linguistics.
Wu, Y. and Ng, H. T. (2013). Grammatical error
correction using integer linear programming. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1456?1465, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Xing, J., Wang, L., Wong, D. F., Chao, L. S., and
Zeng, X. (2013). Um-checker: A hybrid sys-
tem for english grammatical error correction.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 34?42, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-
aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-
machi, M., and Matsumoto, Y. (2013). Naist at
2013 conll grammatical error correction shared
task. CoNLL-2013, 26.
277
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1405?1414,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Chinese Abbreviations with Minimum Semantic Unit and
Global Constraints
Longkai Zhang Li Li Houfeng Wang Xu Sun
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, {li.l,wanghf,xusun}@pku.edu.cn
Abstract
We propose a new Chinese abbreviation
prediction method which can incorporate
rich local information while generating the
abbreviation globally. Different to previ-
ous character tagging methods, we intro-
duce the minimum semantic unit, which is
more fine-grained than character but more
coarse-grained than word, to capture word
level information in the sequence labeling
framework. To solve the ?character dupli-
cation? problem in Chinese abbreviation
prediction, we also use a substring tagging
strategy to generate local substring tagging
candidates. We use an integer linear pro-
gramming (ILP) formulation with various
constraints to globally decode the final ab-
breviation from the generated candidates.
Experiments show that our method outper-
forms the state-of-the-art systems, without
using any extra resource.
1 Introduction
Abbreviation is defined as a shortened description
of the original fully expanded form. For example,
?NLP? is the abbreviation for the corresponding
full form ?Natural Language Processing?. The ex-
istence of abbreviations makes it difficult to iden-
tify the terms conveying the same concept in the
information retrieval (IR) systems and machine
translation (MT) systems. Therefore, it is impor-
tant to maintain a dictionary of the prevalent orig-
inal full forms and the corresponding abbrevia-
tions.
Previous works on Chinese abbreviation gen-
eration focus on the sequence labeling method,
which give each character in the full form an extra
label to indicate whether it is kept in the abbre-
viation. One drawback of the character tagging
strategy is that Chinese characters only contain
limited amount of information. Using character-
based method alone is not enough for Chinese ab-
breviation generation. Intuitively we can think of a
word as the basic tagging unit to incorporate more
information. However, if the basic tagging unit
is word, we need to design lots of tags to repre-
sent which characters are kept for each unit. For a
word with n characters, we should design at least
2
n
labels to cover all possible situations. This re-
duces the generalization ability of the proposed
model. Besides, the Chinese word segmentation
errors may also hurt the performance. Therefore
we propose the idea of ?Minimum Semantic Unit?
(MSU) which is the minimum semantic unit in
Chinese language. Some of the MSUs are words,
while others are more fine-grained than words.
The task of selecting representative characters in
the full form can be further broken down into se-
lecting representative characters in the MSUs. We
model this using the MSU-based tagging method,
which can both utilize semantic information while
keeping the tag set small.
Meanwhile, the sequence labeling method per-
forms badly when the ?character duplication? phe-
nomenon exists. Many Chinese long phrases con-
tain duplicated characters, which we refer to as
the ?character duplication? phenomenon. There is
no sound criterion for the character tagging mod-
els to decide which of the duplicated character
should be kept in the abbreviation and which one
to be skipped. An example is ????????
??(Beijing University of Aeronautics and Astro-
nautics) whose abbreviation is ????. The char-
acter ??? appears twice in the full form and only
one is kept in the abbreviation. In these cases, we
can break the long phase into local substrings. We
can find the representative characters in the sub-
strings instead of the long full form and let the de-
coding phase to integrate useful information glob-
ally. We utilize this sub-string based approach and
obtain this local tagging information by labeling
1405
on the sub-string of the full character sequence.
Given the MSU-based and substring-based
methods mentioned above, we can get a list of
potential abbreviation candidates. Some of these
candidates may not agree on keeping or skipping
of some specific characters. To integrate their ad-
vantages while considering the consistency, we
further propose a global decoding strategy using
Integer Linear Programming(ILP). The constraints
in ILP can naturally incorporate ?non-local? infor-
mation in contrast to probabilistic constraints that
are estimated from training examples. We can also
use linguistic constraints like ?adjacent identical
characters is not allowed? to decode the correct
abbreviation in examples like the previous ????
example.
Experiments show that our Chinese abbrevia-
tion prediction system outperforms the state-of-
the-art systems. In order to reduce the size of
the search space, we further propose pruning con-
straints that are learnt from the training corpus.
Experiment shows that the average number of con-
straints is reduced by about 30%, while the top-1
accuracy is not affected.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
our method, including the MSUs, the substring-
based tagging strategy and the ILP decoding pro-
cess. Experiments are described in section 3. We
also give a detailed analysis of the results in sec-
tion 3. In section 4 related works are introduced,
and the paper is concluded in the last section.
2 System Architecture
2.1 Chinese Abbreviation Prediction
Chinese abbreviations are generated by selecting
representative characters from the full forms. For
example, the abbreviation of ?????? (Peking
University) is ???? which is generated by se-
lecting the first and third characters, see TABLE
1. This can be tackled from the sequence labeling
point of view.
Full form ? ? ? ?
Status Keep Skip Keep Skip
Result ? ?
Table 1: The abbreviation ???? of the full form
?????? (Peking University)
From TABLE 1 we can see that Chinese abbre-
viation prediction is a problem of selecting repre-
sentative characters from the original full form
1
.
Based on this assumption, previous works mainly
focus on this character tagging schema. In these
methods, the basic tagging unit is the Chinese
character. Each character in the full form is la-
beled as ?K? or ?S?, where ?K? means the current
character should be kept in abbreviation and ?S?
means the current character should be skipped.
However, a Chinese character can only contain
limited amount of information. Using character-
based method alone is not enough for Chinese
abbreviation generation. We introduce an MSU-
based method, which models the process of se-
lecting representative characters given local MSU
information.
2.2 MSU Based Tagging
2.2.1 Minimum Semantic Unit
Because using the character-based method is not
enough for Chinese abbreviation generation, we
may think of word as the basic tagging unit to in-
corporate more information intuitively. In English,
the abbreviations (similar to acronyms) are usually
formed by concatenating initial letters or parts of a
series of words. In other words, English abbrevia-
tion generation is based on words in the full form.
However, in Chinese, word is not the most suit-
able abbreviating unit. Firstly, there is no natural
boundary between Chinese words. Errors from the
Chinese word segmentation tools will accumulate
to harm the performance of abbreviation predic-
tion. Second, it is hard to design a reasonable tag
set when the length of a possible Chinese word is
very long. The second column of TABLE 2 shows
different ways of selecting representative charac-
ters of Chinese words with length 3. For a Chi-
nese compound word with 3 characters, there are 6
possible ways to select characters. In this case we
should have at least 6 kinds of tags to cover all pos-
sible situations. The case is even worse for words
with more complicated structures. A suitable ab-
breviating unit should be smaller than word.
We propose the ?Minimum Semantic Unit
(MSU)? as the basic tagging unit. We define MSU
as follows:
1. A word whose length is less or equal to 2 is
an MSU.
1
A small portion of Chinese abbreviations are not gener-
ated from the full form. For example, the abbreviation of ??
??(Shan Dong Province) is ???. However, we can use a
look-up table to get this kind of abbreviations.
1406
Full form SK Label MSUs
???(nursery) ?/K?/S?/S ??+?
???(allowance) ?/S?/K?/S ??+?
???(Credit card) ?/S?/S?/K ??+?
???(Hydropower Station) ?/K?/K?/S ?+?+?
???(Senate) ?/K?/S?/K ?+?+?
???(Music group) ?/S?/K?/K ??+?
Table 2: Representing characters of Chinese words with length 3 (K for keep and S for skip) and the
corresponding MSUs
2. A word whose length is larger than 2, but
does not contain any MSUs with length equal
to 2. For example, ?????(Railway Sta-
tion) is not an MSU because the first two
characters ????(Train) can form an MSU.
By this definition, all 6 strings in TABLE 2 are
often thought as a word, but they are not MSUs
in our view. Their corresponding MSU forms are
shown in TABLE 2.
We collect all the MSUs from the benchmark
datasets provided by the second International Chi-
nese Word Segmentation Bakeoff
2
. We choose the
Peking University (PKU) data because it is more
fine-grained than all other corpora. Suppose we
represent the segmented data as L (In our case L
is the PKU word segmentation data), the MSU se-
lecting algorithm is shown in TABLE 3.
For a given full form, we first segment it us-
ing a standard word segmenter to get a coarse-
grained segmentation result. Here we use the Stan-
ford Chinese Word Segmenter
3
. Then we use the
MSU set to segment each word using the strategy
of ?Maximum Forward Matching?
4
to get the fine-
grained MSU segmentation result.
2.2.2 Labeling strategy
For MSU-based tagging, we use a labeling method
which uses four tags, ?KSFL?. ?K? stands for
?Keep the whole unit?, ?S? stands for ?Skip the
whole unit?, ?F? stands for ?keep the First charac-
ter of the unit?, and Label ?L? stands for ?keep the
Last character of the unit?. An example is shown
in TABLE 4.
The ?KSFL? tag set is also applicable for MSUs
whose length is greater than 2 (an example is ??
??/chocolate?). By examining the corpus we
find that such MSUs are either kept of skipped in
2
http://www.sighan.org/bakeoff2005/
3
http://nlp.stanford.edu/software/
segmenter.shtml
4
In Chinese, ?Forward? means from left to right.
????????????? (The ab-
breviation is ??????)
KSFL ??/K ??/F ??/S ??/S ?
?/F?/S
Table 4: The abbreviation ?????? of ????
??????? (National Linguistics Work Com-
mittee) based on MSU tagging.
the final abbreviations. Therefore, the labels of
these long MSUs are either ?K? or ?S?. Empirically,
this assumption holds for MSUs, but does not hold
for words
5
.
2.2.3 Feature templates
The feature templates we use are as follows. See
TABLE 5.
1. Word X
i
(?2 ? i ? 2)
2. POS tag of word X
i
(?2 ? i ? 2)
3. Word Bigrams (X
i
, X
i+1
) (?2 ? i ? 1)
4. Type of word X
i
(?2 ? i ? 2)
5. Length of word X
i
(?2 ? i ? 2)
Table 5: Feature templates for unit tagging. X
represents the MSU sequence of the full form. X
i
represents the ith MSU in the sequence.
Templates 1, 2 and 3 express word uni-grams
and bi-grams. In MSU-based tagging, we can uti-
lize the POS information, which we get from the
Stanford Chinese POS Tagger
6
. In template 4, the
type of word refers to whether it is a number, an
English word or a Chinese word. Because the ba-
sic tagging unit is MSU, which carries word infor-
mation, we can use many features that are infeasi-
ble in character-based tagging.
5
In table 2, all examples are partly kept.
6
http://nlp.stanford.edu/software/
tagger.shtml
1407
Init:
Let MSUSet = empty set
For each word w in L:
If Length(w) ? 2
Add w to MSUSet
End if
End for
For each word w in L:
If Length(w) > 2 and no word x in MSUSet is a substring of w
Add w to MSUSet
End if
End for
Return MSUSet
Table 3: Algorithm for collecting MSUs from the PKU corpus
2.2.4 Sequence Labeling Model
The MSU-based method gives each MSU an ex-
tra indicative label. Therefore any sequence label-
ing model is appropriate for the method. Previous
works showed that Conditional Random Fields
(CRFs) can outperform other sequence labeling
models like MEMMs in abbreviation generation
tasks (Sun et al., 2009; Tsuruoka et al., 2005). For
this reason we choose CRFs model in our system.
For a given full form?s MSU list, many can-
didate abbreviations are generated by choosing
the k-best results of the CRFs. We can use the
forward-backward algorithm to calculate the prob-
ability of a specified tagging result. To reduce the
searching complexity in the ILP decoding process,
we delete those candidate tagged sequences with
low probability.
2.3 Substring Based Tagging
As mentioned in the introduction, the sequence
labeling method, no matter character-based or
MSU-based, perform badly when the ?character
duplication? phenomenon exists. When the full
form contains duplicated characters, there is no
sound criterion for the sequence tagging strategy
to decide which of the duplicated character should
be kept in the abbreviation and which one to be
skipped. On the other hand, we can tag the sub-
strings of the full form to find the local represen-
tative characters in the substrings of the long full
form. Therefore, we propose the sub-string based
approach to given labeling results on sub-strings.
These results can be integrated into a more accu-
rate result using ILP constraints, which we will de-
scribe in the next section.
Another reason for using the sub-string based
methods is that long full forms contain more char-
acters and are much easier to make mistakes dur-
ing the sequence labeling phase. Zhang et al.
(2012) shows that if the full form contains less
than 5 characters, a simple tagger can reach an ac-
curacy of 70%. Zhang et al. (2012) also shows that
if the full form is longer than 10 characters, the
average accuracy is less than 30%. The numerous
potential candidates make it hard for the tagger to
choose the correct one. For the long full forms,
although the whole sequence is not correctly la-
beled, we find that if we only consider its short
substrings, we may find the correct representative
characters. This information can be integrated into
the decoding model to adjust the final result.
We use the MSU-based tagging method in the
sub-string tagging. The labeling strategy and fea-
ture templates are the same to the MSU-based tag-
ging method. In practice, enumerating all sub-
sequences of a given full form is infeasible if the
full form is very long. For a given full form,
we use the boundary MSUs to reduce the pos-
sible sub-sequence set. For example, ????
???(Chinese Academy of Science) has 5 sub-
sequences: ????, ??????, ????, ???
?? and ???.
2.4 ILP Formulation of Decoding
Given the MSU-based and sub-sequence-based
methods mentioned above as well as the preva-
lent character-based methods, we can get a list
of potential abbreviation candidates and abbrevi-
ated substrings. We should integrate their advan-
tages while keeping the consistency between each
1408
candidate. Therefore we further propose a global
decoding strategy using Integer Linear Program-
ming(ILP). The constraints in ILP can naturally
incorporate ?non-local? information in contrast to
probabilistic constraints that are estimated from
training examples. We can also use linguistic con-
straints like ?adjacent identical characters is not
allowed? to decode the correct abbreviation in ex-
amples like the ???? example in section 1.
Formally, given the character sequence of the
full form c = c
1
...c
l
, we keep Q top-ranked
MSU-based tagging results T=(T
1
, ..., T
Q
) and M
tagged substrings S=(S
1
, ..., S
M
) using the meth-
ods described in previous sections. We also
use N top-ranked character-based tagging results
R=(R
1
, ..., R
N
) based on the previous character-
based works. We also define the setU = S?R?T
as the union of all candidate sequences. Our goal
is to find an optimal binary variable vector solution
~v = ~x~y~z = (x
1
, ..., x
M
, y
1
, ..., y
N
, z
1
, ..., z
Q
) that
maximizes the object function:
?
1
M
?
i=1
score(S
i
) ? x
i
+ ?
2
N
?
i=1
score(R
i
) ? y
i
+?
3
Q
?
i=1
score(T
i
) ? z
i
subject to constrains in TABLE 6. The parame-
ters ?
1
, ?
2
, ?
3
controls the preference of the three
parts, and can be decided using cross-validation.
Constraint 1 indicates that x
i
, y
i
, z
i
are all
boolean variables. They are used as indicator vari-
ables to show whether the corresponding tagged
sequence is in accordance with the final result.
Constraint 2 is used to guarantee that at most
one candidate from the character-based tagging is
preserved. We relax the constraint to allow the
sum to be zero in case that none of the top-ranked
candidate is suitable to be the final result. If the
sum equals zero, then the sub-sequence based tag-
ging method will generate a more suitable result.
Constrain 3 has the same utility for the MSU-
based tagging.
Constraint 4, 5, 6 are inter-method constraints.
We use them to guarantee that the labels of the
preserved sequences of different tagging methods
do not conflict with each other. Constraint 7 is
used to guarantee that the labels of the preserved
sub-strings do not conflict with each other.
Constraint 8 is used to solve the ?character du-
plicate? problem. When two identical characters
are kept adjacently, only one of them will be kept.
Which one will be kept depends on the global de-
coding score. This is the advantage of ILP against
traditional sequence labeling methods.
2.5 Pruning Constraints
The efficiency of solving the ILP decoding prob-
lem depends on the number of candidate tagging
sequences N and Q, as well as the number of sub-
sequences M. Usually, N and Q is less than 10 in
our experiment. Therefore, M influences the time
complexity the most. Because we use the bound-
ary of MSUs instead of enumerating all possible
subsequences, the value of M can be largely re-
duced.
Some characters are always labeled as ?S? or
?K? once the context is given. We can use this
phenomenon to reduce the search space of decod-
ing. Let c
i
denote the i
th
character relative to the
current character c
0
and t
i
denote the tag of c
i
. The
context templates we use are listed in TABLE 7.
Uni-gram Contexts c
0
, c
?1
, c
1
Bi-gram Contexts c
?1
c0, c
?1
c
1
, c
0
c
1
Table 7: Context templates used in pruning
With respect to a training corpus, if a context
C relative to c
0
always assigns a certain tag t to
c
0
, then we can use this constraint in pruning. We
judge the degree of ?always? by checking whether
count(C?t
0
=t)
count(C)
> threshold. The threshold is a
non-negative real number under 1.0.
3 Experiments
3.1 Data and Evaluation Metric
We use the abbreviation corpus provided by Insti-
tute of Computational Linguistics (ICL) of Peking
University in our experiments. The corpus is sim-
ilar to the corpus used in Sun et al. (2008, 2009);
Zhang et al. (2012). It contains 8, 015 Chinese ab-
breviations, including noun phrases, organization
names and some other types. Some examples are
presented in TABLE 8. We use 80% abbreviations
as training data and the rest as testing data. In
some cases, a long phrase may contain more than
one abbreviation. For these cases, the corpus just
keeps their most commonly used abbreviation for
each full form.
The evaluation metric used in our experiment
is the top-K accuracy, which is also used by
Tsuruoka et al. (2005), Sun et al. (2009) and
1409
1. x
i
? {0, 1}, y
i
? {0, 1}, z
i
? {0, 1}
2.
?
N
i=1
y
i
? 1
3.
?
Q
i=1
z
i
? 1
4. ?R
i
? R, S
j
? S, if R
i
and S
j
have a same position but the position gets different labels,
then y
i
+ x
j
? 1
5. ?T
i
? T , S
j
? S, if T
i
and S
j
have a same position but the position gets different labels,
then z
i
+ x
j
? 1
6. ?R
i
? R, T
j
? T , if R
i
and T
j
have a same position but the position gets different labels,
then x
i
+ z
j
? 1
7. ?S
i
, S
j
? S if S
i
and S
j
have a same position but the position gets different labels, then
z
i
+ z
j
? 1
8. ?S
i
, S
j
? S if the last character S
i
keeps is the same as the first character S
j
keeps, then
z
i
+ z
j
? 1
Table 6: Constraints for ILP
Type Full form Abbreviation
Noun Phrase ????(Excellent articles) ??
Organization ????(Writers? Association) ??
Coordinate phrase ????(Injuries and deaths) ??
Proper noun ????(Media) ??
Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun)
Zhang et al. (2012). The top-K accuracy measures
what percentage of the reference abbreviations are
found if we take the top N candidate abbreviations
from all the results. In our experiment, top-10 can-
didates are considered in re-ranking phrase and the
measurement used is top-1 accuracy (which is the
accuracy we usually refer to) because the final aim
of the algorithm is to detect the exact abbreviation.
CRF++
7
, an open source linear chain CRF tool,
is used in the sequence labeling part. For ILP part,
we use lpsolve
8
, which is also an open source tool.
The parameters of these tools are tuned through
cross-validation on the training data.
3.2 Results
TABLE 9 shows the top-K accuracy of the
character-based and MSU-based method. We can
see that the MSU-based tagging method can uti-
lize word information, which can get better perfor-
mance than the character-based method. We can
also figure out that the top-5 candidates include the
reference abbreviation for most full forms. There-
fore reasonable decoding by considering all possi-
ble labeling of sequences may improve the perfor-
mance. Although the MSU-based methods only
outperforms character-based methods by 0.75%
7
http://crfpp.sourceforge.net/
8
http://lpsolve.sourceforge.net/5.5/
for top-1 accuracy, it is much better when consid-
ering top-2 to top-5 accuracy (+2.5%). We further
select the top-ranked candidates for ILP decod-
ing. Therefore the MSU-based method can further
improve the performance in the global decoding
phase.
K char-based MSU-based
1 0.5714 0.5789
2 0.6879 0.7155
3 0.7681 0.7819
4 0.8070 0.8283
5 0.8333 0.8583
Table 9: Top-K (K ? 5) results of character-based
tagging and MSU-based tagging
We then use the top-5 candidates of character-
based method and MSU-based method, as well
as the top-2 results of sub-sequence labeling in
the ILP decoding phase. Then we select the top-
ranked candidate as the final abbreviation of each
instance. TABLE 10 shows the results. We can see
that the accuracy of our method is 61.0%, which
improved by +3.89% compared to the character-
based method, and +3.14% compared to the MSU-
based method.
We find that the ILP decoding phase do play
an important role in generating the right an-
1410
Method Top-1 Accuracy
Char-based 0.5714
MSU-based 0.5789
ILP Result 0.6103
Table 10: Top-1 Accuracy after ILP decoding
swer. Some reference abbreviations which are not
picked out by either tagging method can be found
out after decoding. TABLE 11 shows the exam-
ple of the organization name ?????????
???? (Higher Education Admissions Office).
Neither the character-based method nor the MSU-
based method finds the correct answer ?????,
while after ILP decoding, ????? becomes the
final result. TABLE 12 and TABLE 13 give two
more examples.
True Result ???
Char-based ??
MSU-based ???
ILP Decoding ???
Table 11: Top-1 result of ??????????
??? (Higher Education Admissions Office)
True Result ??
Char-based ??
MSU-based ???
ILP Decoding ??
Table 12: Top-1 result of ?????? (Articles
exceed the value)
True Result ????
Char-based ???
MSU-based ???
ILP Decoding ????
Table 13: Top-1 result of ??????????
(Visual effects of sound and lights)
3.3 Improvements Considering Length
Full forms that are longer than five characters are
long terms. Long terms contain more characters,
which is much easier to make mistakes. Figure
1 shows the top-1 accuracy respect to the term
length using different tagging methods and using
ILP decoding. The x-axis represents the length of
the full form. The y-axis represents top-1 accu-
racy. We find that our method works especially
better than pure character-based or MSU-based
approach when the full form is long. By decod-
ing using ILP, both local and global information
are incorporated. Therefore many of these errors
can be eliminated.
Figure 1: Top-1 accuracy of different methods
considering length
3.4 Effect of pruning
As discussed in previous sections, if we are able
to pre-determine that some characters in a certain
context should be kept or skipped, then the num-
ber of possible boolean variable x can be reduced.
TABLE 14 shows the differences. To guarantee
a high accuracy, we set the threshold to be 0.99.
When the original full form is partially tagged by
the pruning constraints, the number of boolean
variables per full form is reduced from 34.4 to
25.5. By doing this, we can improve the predic-
tion speed over taking the raw input.
From TABLE 14 we can also see that the top-
1 accuracy is not affected by these pruning con-
straints. This is obvious, because CRF itself has
a strong modeling ability. The pruning constraints
cannot improve the model accuracy. But they can
help eliminate those false candidates to make the
ILP decoding faster.
Accuracy Average length Time(s)
raw 0.6103 34.4 12.5
pruned 0.6103 25.5 7.1
Table 14: Comparison of testing time of raw input
and pruned input
3.5 Compare with the State-of-the-art
Systems
We also compare our method with previous meth-
ods, including Sun et al. (2009) and Zhang et al.
(2012). Because we use a different corpus, we
re-implement the system Sun et al. (2009), Zhang
1411
et al. (2012) and Sun et al. (2013), and experi-
ment on our corpus. The first two are CRF+GI
and DPLVM+GI in Sun et al. (2009), which are
reported to outperform the methods in Tsuruoka
et al. (2005) and Sun et al. (2008). For DPLVM
we use the same model in Sun et al. (2009) and
experiment on our own data. We also compare
our approach with the method in Zhang et al.
(2012). However, Zhang et al. (2012) uses dif-
ferent sources of search engine result information
to re-rank the original candidates. We do not use
any extra web resources. Because Zhang et al.
(2012) uses web information only in its second
stage, we use ?BIEP?(the tag set used by Zhang
et al. (2012)) to denote the first stage of Zhang
et al. (2012), which also uses no web information.
TABLE 15 shows the results of the comparisons.
We can see that our method outperforms all other
methods which use no extra resource. Because
Zhang et al. (2012) uses extra web resource, the
top-1 accuracy of Zhang et al. (2012) is slightly
better than ours.
Method Top-1 Accuracy
CRF+GI 0.5850
DPLVM+GI 0.5990
BIEP 0.5812
Zhang et al. (2012) 0.6205
Our Result 0.6103
Table 15: Comparison with the state-of-the-art
systems
4 Related Work
Previous research mainly focuses on ?abbrevia-
tion disambiguation?, and machine learning ap-
proaches are commonly used (Park and Byrd,
2001; HaCohen-Kerner et al., 2008; Yu et al.,
2006; Ao and Takagi, 2005). These ways of link-
ing abbreviation pairs are effective, however, they
cannot solve our problem directly. In many cases
the full form is definite while we don?t know the
corresponding abbreviation.
To solve this problem, some approaches main-
tain a database of abbreviations and their corre-
sponding ?full form? pairs. The major problem
of pure database-building approach is obvious. It
is impossible to cover all abbreviations, and the
building process is quit laborious. To find these
pairs automatically, a powerful approach is to find
the reference for a full form given the context,
which is referred to as ?abbreviation generation?.
There is research on heuristic rules for gen-
erating abbreviations Barrett and Grems (1960);
Bourne and Ford (1961); Taghva and Gilbreth
(1999); Park and Byrd (2001); Wren et al. (2002);
Hearst (2003). Most of them achieved high per-
formance. However, hand-crafted rules are time
consuming to create, and it is not easy to transfer
the knowledge of rules from one language to an-
other.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed a supervised
learning approach by using SVM model. Tsu-
ruoka et al. (2005); Sun et al. (2009) formal-
ized the process of abbreviation generation as a
sequence labeling problem. In Tsuruoka et al.
(2005) each character in the full form is associated
with a binary value label y, which takes the value
S (Skip) if the character is not in the abbreviation,
and value P (Preserve) if the character is in the ab-
breviation. Then a MEMM model is used to model
the generating process. Sun et al. (2009) followed
this schema but used DPLVM model to incor-
porate both local and global information, which
yields better results. Sun et al. (2013) also uses
machine learning based methods, but focuses on
the negative full form problem, which is a little
different from our work.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations.Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method uses no extra resource, but reaches
comparable results.
ILP shows good results in many NLP tasks.
Punyakanok et al. (2004); Roth and Yih (2005)
used it in semantic role labeling (SRL). Martins
et al. (2009) used it in dependency parsing. (Zhao
and Marcus, 2012) used it in Chinese word seg-
mentation. (Riedel and Clarke, 2006) used ILP
1412
in dependency parsing. However, previous works
mainly focus on the constraints of avoiding bound-
ary confliction. For example, in SRL, two argu-
ment of cannot overlap. In CWS, two Chinese
words cannot share a same character. Different to
their methods, we investigate on the conflict of la-
bels of character sub-sequences.
5 Conclusion and Future work
We propose a new Chinese abbreviation predic-
tion method which can incorporate rich local in-
formation while generating the abbreviation glob-
ally. We propose the MSU, which is more coarse-
grained than character but more fine-grained than
word, to capture word information in the se-
quence labeling framework. Besides the MSU-
based method, we use a substring tagging strategy
to generate local substring tagging candidates. We
use an ILP formulation with various constraints
to globally decode the final abbreviation from the
generated candidates. Experiments show that our
method outperforms the state-of-the-art systems,
without using any extra resource. This method
is not limited to Chinese abbreviation generation,
it can also be applied to similar languages like
Japanese.
The results are promising and outperform the
baseline methods. The accuracy can still be im-
proved. Potential future works may include using
semi-supervised methods to incorporate unlabeled
data and design reasonable features from large cor-
pora. We are going to study on these issues in the
future.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
References
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527?533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576?586.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323?324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538?552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61?64. Associa-
tion for Computational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209?214. IEEE.
Martins, A. F., Smith, N. A., and Xing, E. P.
(2009). Concise integer linear programming
formulations for dependency parsing. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1,
pages 342?350. Association for Computational
Linguistics.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126?133.
Punyakanok, V., Roth, D., Yih, W.-t., and Zimak,
D. (2004). Semantic role labeling via integer
linear programming inference. In Proceedings
of the 20th international conference on Compu-
1413
tational Linguistics, page 1346. Association for
Computational Linguistics.
Riedel, S. and Clarke, J. (2006). Incremental in-
teger linear programming for non-projective de-
pendency parsing. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 129?137. Associ-
ation for Computational Linguistics.
Roth, D. and Yih, W.-t. (2005). Integer linear
programming inference for conditional random
fields. In Proceedings of the 22nd international
conference on Machine learning, pages 736?
743. ACM.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641?647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905?913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602?611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191?198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25?31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426?434.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380?404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055?3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhao, Q. and Marcus, M. (2012). Exploring deter-
ministic constraints: from a constrained english
pos tagger to an efficient ilp solution to chinese
word segmentation. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1054?1062, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
1414
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1816?1821,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Muli-label Text Categorization with Hidden Components
Li Li Longkai Zhang Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
li.l@pku.edu.cn, zhlongk@qq.com, wanghf@pku.edu.cn
Abstract
Multi-label text categorization (MTC) is
supervised learning, where a documen-
t may be assigned with multiple categories
(labels) simultaneously. The labels in the
MTC are correlated and the correlation re-
sults in some hidden components, which
represent the ?share? variance of correlat-
ed labels. In this paper, we propose a
method with hidden components for MTC.
The proposed method employs PCA to
capture the hidden components, and incor-
porates them into a joint learning frame-
work to improve the performance. Experi-
ments with real-world data sets and evalu-
ation metrics validate the effectiveness of
the proposed method.
1 Introduction
Many real-world text categorization applications
are multi-label text categorization (Srivastava and
Zane-Ulman, 2005; Katakis et al., 2008; Rubin
et al., 2012; Nam et al., 2013), where a docu-
ments is usually assigned with multiple labels si-
multaneously. For example, as figure 1 shows,
a newspaper article concerning global warming
can be classified into two categories, Environmen-
t, and Science simultaneously. Let X = R
d
be the documents corpus, and Y = {0, 1}
m
be
the label space with m labels. We denote by
{(x
1
, y
1
), (x
2
, y
2
), ..., (x
n
, y
n
)} the training set of
n documents. Each document is denoted by a vec-
tor x
i
= [x
i,1
, x
i,2
, ..., x
i,d
] of d dimensions. The
labeling of the i-th document is denoted by vector
y
i
= [y
i,1
, y
i,2
, ..., y
i,m
], where y
il
is 1 when the
i-th document has the l-th label and 0 otherwise.
The goal is to learn a function f : X ? Y . Gener-
ally, we can assume f consists of m functions, one
for a label.
f = [f
1
, f
2
, ..., f
m
]
Figure 1: A newspaper article concerning global
warming can be classified into two categories, En-
vironment, and Science.
The labels in the MLC are correlated. For ex-
ample, a ?politics? document is likely to be an ?e-
conomic? document simultaneously, but likely not
to be a ?literature? document. According to the
latent variable model (Tabachnick et al., 2001),
the labels with correlation result in some hidden
components, which represent the ?share? variance
of correlated labels. Intuitively, if we can capture
and utilize these hidden components in MTC, the
performance will be improved. To implement this
idea, we propose a multi- label text categorization
method with hidden components, which employ
PCA to capture the hidden components, and then
incorporates these hidden components into a joint
learning framework. Experiments with various da-
ta sets and evaluation metrics validate the values
of our method. The research close to our work is
ML-LOC (Multi-Label learning using LOcal Cor-
relation) in (Huang and Zhou, 2012). The differ-
1816
ences between ours and ML-LOC is that ML-LOC
employs the cluster method to gain the local cor-
relation, but we employ the PCA to obtain the hid-
den code. Meanwhile, ML-LOC uses the linear
programming in learning the local code, but we
employ the gradient descent method since we add
non-linear function to the hidden code.
The rest of this paper is organized as follows.
Section 2 presents the proposed method. We con-
duct experiments to demonstrate the effectiveness
of the proposed method in section 3. Section 4
concludes this paper.
2 Methodology
2.1 Capturing Hidden Component via
Principle Component Analysis
The first step of the proposed method is to capture
hidden components of training instances. Here we
employ Principal component analysis (PCA). This
is because PCA is a well-known statistical tool that
converts a set of observations of possibly correlat-
ed variables into a set of values of linearly uncorre-
lated variables called principle components. These
principle components represent the inner structure
of the correlated variables.
In this paper, we directly employ PCA to con-
vert labels of training instances into their principle
components, and take these principle components
as hidden components of training instances. We
denote by h
i
the hidden components of the i-th in-
stance captured by PCA.
2.2 Joint Learning Framework
We expand the original feature representation of
the instance x
i
by its hidden component code vec-
tor c
i
. For simplicity, we use logistic regression as
the motivating example. Let w
l
denote weights in
the l-th function f
l
, consisting of two parts: 1)w
x
l
is the part involving the instance features. 2) w
c
l
is the part involving the hidden component codes.
Hence f
l
is:
f
l
(x,c) =
1
1 + exp(?x
T
w
x
l
? c
T
w
c
l
)
(1)
where C is the code vectors set of all training in-
stances.
The natural choice of the code vector c is h.
However, when testing an instance, the labeling is
unknown (exactly what we try to predict), conse-
quently we can not capture h with PCA to replace
the code vector c in the prediction function Eq.(1).
Therefore, we assume a linear transformation M
from the training instances to their independent
components, and use Mx as the approximate in-
dependent component. For numerical stability, we
add a non-linear function (e.g., the tanh function)
toMx. This is formulated as follows.
c = tanh(Mx) (2)
Aiming to the discrimination fitting and the in-
dependent components encoding, we optimize the
following optimization problem.
min
W ,C
n
?
i=1
m
?
l=1
`(x
i
, c
i
, y
il
, f
l
) + ?
1
?(f )
+?
2
Z(C) (3)
The first term of Eq.(3) is the loss function. `
is the loss function defined on the training data,
and W denotes all weights in the our model, i.e.,
w
1
, ...,w
l
, ...,w
m
. Since we utilize the logistic re-
gression in our model, the loss function is defined
as follows.
`(x,c, y, f)
= ?ylnf(x,c)? (1? y)ln(1? f(x,c)) (4)
The second term of Eq.(3) ? is to punish the
model complexity, which we use the `
2
regular-
ization term.
?(f ) =
m
?
l=1
||w
l
||
2
. (5)
The third term of Eq.(3) Z is to enforce the code
vector close to the independent component vector.
To obtain the goal, we use the least square error
between the code vector and the independent com-
ponent vector as the third regularized term.
Z(C) =
n
?
i=1
||c
i
? h
i
||
2
. (6)
By substituting the Eq.(5) and Eq.(6) into Eq.(3)
and changing c to tanh(Mx) (Eq.(2)), we obtain
the following optimization problem.
min
W ,M
n
?
i=1
m
?
l=1
`(x
i
, tanh(Mx
i
), y
il
, f )
+?
1
m
?
l=1
||w
l
||
2
+ ?
2
n
?
i=1
||Mx
i
? h
i
||
2
(7)
1817
2.3 Alternative Optimization method
We solve the optimization problem in Eq.(7) by
the alternative optimization method, which opti-
mize one group of the two parameters with the
other fixed. When the M fixed, the third term of
Eq.(7) is a constant and thus can be ignored, then
Eq.(7) can be rewritten as follows.
min
W
n
?
i=1
m
?
l=1
`(x
i
, tanh(Mx
i
), y
il
, f
l
)
+?
1
m
?
l=1
||w
l
||
2
(8)
By decomposing Eq.(8) based on the label, the e-
quation Eq.(8) can be simplified to:
min
w
l
n
?
i=1
`(x
i
, tanh(Mx
i
), y
il
, f
l
) + ?
1
||w
l
||
2
(9)
Eq.(9) is the standard logistic regression, which
has many efficient optimization algorithms.
When W fixed, the second term is constan-
t and can be omitted, then Ep.(7) can rewritten
to Eq.(10). We can apply the gradient descen-
t method to optimize this problem.
min
M
n
?
i=1
m
?
l=1
`(x
i
, tanh(Mx
i
), y
il
, f
l
)
+?
2
n
?
i=1
||Mx
i
? h
i
||
2
(10)
3 Experiments
3.1 Evaluation Metrics
Compared with the single-label classification, the
multi-label setting introduces the additional de-
grees of freedom, so that various multi-label eval-
uation metrics are requisite. We use three differen-
t multi-label evaluation metrics, include the ham-
ming loss evaluation metric.
The hamming loss is defined as the percentage
of the wrong labels to the total number of labels.
Hammingloss =
1
m
|h(x)?y| (11)
where ? denotes the symmetric difference of two
sets, equivalent to XOR operator in Boolean logic.
m denotes the label number.
The multi-label 0/1 loss, also known as subset
accuracy, is the exact match measure as it requires
any predicted set of labels h(x) to match the true
set of labels S exactly. The 0/1 loss is defined as
follows:
0/1loss = I(h(x) 6= y) (12)
Let a
j
and r
j
denote the precision and recall for
the j-th label. The macro-averaged F is a harmon-
ic mean between precision and recall, defined as
follows:
F =
1
m
m
?
i=j
2 ? a
j
? r
j
a
j
+ r
j
(13)
3.2 Datasets
We perform experiments on three MTC data sets:
1) the first data set is slashdot (Read et al., 2011).
The slashdot data set is concerned about science
and technology news categorization, which pre-
dicts multiply labels given article titles and partial
blurbs mined from Slashdot.org. 2) the second da-
ta set is medical (Pestian et al., 2007). This data set
involves the assignment of ICD-9-CM codes to ra-
diology reports. 3) the third data set is tmc2007 (S-
rivastava and Zane-Ulman, 2005). It is concerned
about safety report categorization, which is to la-
bel aviation safety reports with respect to what
types of problems they describe. The characteris-
tics of them are shown in Table 1, where n denotes
the size of the data set, d denotes the dimension of
the document instance, and m denotes the number
of labels.
dataset n d m Lcard
slashdot 3782 1079 22 1.18
medical 978 1449 45 1.245
tmc2007 28596 500 22 2.16
Table 1: Multi-label data sets and associated statis-
tics
The measure label cardinality Lcard, which
is one of the standard measures of ?multi-label-
ness?, defined as follows, introduced in (T-
soumakas and Katakis, 2007).
Lcard(D) =
?
n
i=1
?
m
j=1
y
i
j
n
where D denotes the data set, l
i
j
denotes the j-th
label of the i-th instance in the data set.
1818
3.3 Compared to Baselines
To examine the values of the joint learning frame-
work, we compare our method to two baselines.
The baseline 1 eliminates the PCA, which just
adds an extra set of non-linear features. To im-
plement this baseline, we only need to set ?
2
= 0.
The baseline 2 eliminates the joint learning frame-
work. This baseline captures the hidden compo-
nent codes with PCA, trains a linear regression
model to fit the hidden component codes, and u-
tilizes the outputs of the linear regression model
as features.
For the proposed method, we set ?
1
= 0.001
and ?
2
= 0.1. For the baseline 2, we employ l-
ogistic regression with 0.001 `2 regularization as
the base classifier. Evaluations are done in ten-
fold cross validation. Note that all of them pro-
duce real-valued predictions. A threshold t needs
to be used to determine the final multi-label set y
such that l
j
? y where p
j
? t. We select threshold
t, which makes the Lcard measure of predictions
for the training set is closest to the Lcard mea-
sure of the training set (Read et al., 2011). The
threshold t is determined as follows, where D
t
is
the training set and a multi-label model H
t
pre-
dicts for the training set under threshold t.
t = argmin
t?[0,1]
|Lcard(D
t
)? Lcard(H
t
(D
t
))|
(14)
Table 2 reports our method wins over the base-
lines in terms of different evaluation metrics,
which shows the values of PCA and our join-
t learning framework. The hidden component code
only fits the hidden component in the baseline
method. The hidden component code obtains bal-
ance of fitting hidden component and fitting the
training data in the joint learning framework.
3.4 Compared to Other Methods
We compare the proposed method to BR, C-
C (Read et al., 2011), RAKEL (Tsoumakas and
Vlahavas, 2007) and ML-KNN (Zhang and Zhou,
2007). entropy. ML-kNN is an adaption of kNN
algorithm for multilabel classification. methods.
Binary Revelance (BR) is a simple but effective
method that trains binary classifiers for each label
independently. BR has a low time complexity but
makes an arbitrary assumption that the labels are
independent from each other. CC organizes the
classifiers along a chain and take predictions pro-
duced by the former classifiers as features of the
latter classifiers. ML-kNN uses kNN algorithms
independently for each label with considering pri-
or probabilities. The Label Powerset (LP) method
models independencies among labels by treating
each label combination as a new class. LP con-
sumes too much time, since there are 2
m
label
combinations with m labels. RAndom K labEL
(RAKEL) is an ensemble method of LP. RAKEL
learns several LP models with random subsets of
size k from all labels, and then uses a vote process
to determine the final predictions.
For our proposed method, we employ the set-
up in subsection 3.3. We utilize logistic regression
with 0.001 `2 regularization as the base classifier
for BR, CC and RAKEL. For RAKEL, the num-
ber of ensemble is set to the number of label and
the size of the label subset is set to 3. For MLKN-
N, the number of neighbors used in the k-nearest
neighbor algorithm is set to 10 and the smooth pa-
rameter is set to 1. Evaluations are done in ten-
fold cross validation. We employ the threshold-
selection strategy introduced in subsection 3.3
Table 2 also reports the detailed results in terms
of different evaluation metrics. The mean metric
value and the standard deviation of each method
are listed for each data set. We see our proposed
method shows majorities of wining over the other
state-of-the-art methods nearly at all data sets un-
der hamming loss, 0/1 loss and macro f score. E-
specially, under the macro f score, the advantages
of our proposed method over the other methods are
very clear.
4 CONCLUSION
Many real-world text categorization applications
are multi-label text categorization (MTC), where a
documents is usually assigned with multiple labels
simultaneously. The key challenge of MTC is the
label correlations among labels. In this paper, we
propose a MTC method via hidden components
to capture the label correlations. The proposed
method obtains hidden components via PCA and
incorporates them into a joint learning framework.
Experiments with various data sets and evaluation
metrics validate the effectiveness of the proposed
method.
Acknowledge
We thank anonymous reviewers for their help-
ful comments and suggestions. This research
was partly supported by National High Tech-
1819
hamming?. Lower is better.
Dataset slashdot medical tmc2007
Proposed 0.044? 0.004 0.010? 0.002 0.056? 0.002
Baseline1 0.046? 0.003? 0.010? 0.002 0.056? 0.001
Baseline2 0.047? 0.003? 0.011? 0.001 0.059? 0.001?
BR 0.058? 0.003? 0.010? 0.001 0.060? 0.001?
CC 0.049? 0.003? 0.010? 0.001 0.058? 0.001?
RAKEL 0.039? 0.002? 0.011? 0.002 0.057? 0.001
MLKNN 0.067? 0.003? 0.016? 0.003? 0.070? 0.002?
0/1 loss?. Lower is better.
Dataset slashdot medical tmc2007
Proposed 0.600? 0.042 0.316? 0.071 0.672? 0.010
Baseline1 0.615? 0.034? 0.324? 0.058? 0.672? 0.008
Baseline2 0.669? 0.039? 0.354? 0.062? 0.698? 0.007?
BR 0.803? 0.018? 0.337? 0.063? 0.701? 0.008?
CC 0.657? 0.025? 0.337? 0.064? 0.687? 0.010?
RAKEL 0.686? 0.024? 0.363? 0.064? 0.682? 0.009?
MLKNN 0.776? 0.020? 0.491? 0.083? 0.746? 0.003?
F score?. Larger is better.
Dataset slashdot medical tmc2007
Proposed 0.429? 0.026 0.575? 0.067 0.587? 0.010
Baseline1 0.413? 0.032? 0.547? 0.056? 0.577? 0.011
Baseline2 0.398? 0.032? 0.561? 0.052? 0.506? 0.011?
BR 0.204? 0.011? 0.501? 0.058? 0.453? 0.011?
CC 0.303? 0.022? 0.510? 0.052? 0.505? 0.011?
RAKEL 0.349? 0.023? 0.589? 0.063? 0.555? 0.011?
MLKNN 0.297? 0.031? 0.410? 0.064? 0.431? 0.014?
Table 2: Performance (mean?std.) of our method and baseline in terms of different evaluation metrics.
?/? indicates whether the proposed method is statistically superior/inferior to baseline (pairwise t-test at
5% significance level).
nology Research and Development Program of
China (863 Program) (No.2012AA011101), Na-
tional Natural Science Foundation of China
(No.91024009), Major National Social Science
Fund of China (No. 12&ZD227). The contac-
t author of this paper, according to the meaning
given to this role by Key Laboratory of Computa-
tional Linguistics, Ministry of Education, School
of Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang
References
Sheng-Jun Huang and Zhi-Hua Zhou. 2012. Multi-
label learning by exploiting label correlations local-
ly. In AAAI.
Ioannis Katakis, Grigorios Tsoumakas, and Ioannis
Vlahavas. 2008. Multilabel text classification for
automated tag suggestion. In Proceedings of the
ECML/PKDD.
Jinseok Nam, Jungi Kim, Iryna Gurevych, and Jo-
hannes F?urnkranz. 2013. Large-scale multi-label
text classification-revisiting neural networks. arXiv
preprint arXiv:1312.5419.
John P Pestian, Christopher Brew, Pawe? Matykiewicz,
DJ Hovermale, Neil Johnson, K Bretonnel Cohen,
and W?odzis?aw Duch. 2007. A shared task involv-
ing multi-label classification of clinical free text. In
Proceedings of the Workshop on BioNLP 2007: Bio-
logical, Translational, and Clinical Language Pro-
cessing, pages 97?104. Association for Computa-
tional Linguistics.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and
Eibe Frank. 2011. Classifier chains for multi-label
classification. Machine learning, 85(3):333?359.
Timothy N Rubin, America Chambers, Padhraic S-
myth, and Mark Steyvers. 2012. Statistical topic
models for multi-label document classification. Ma-
chine Learning, 88(1-2):157?208.
Ashok N Srivastava and Brett Zane-Ulman. 2005. Dis-
covering recurring anomalies in text reports regard-
1820
ing complex space systems. In Aerospace Confer-
ence, 2005 IEEE, pages 3853?3862. IEEE.
Barbara G Tabachnick, Linda S Fidell, et al. 2001. Us-
ing multivariate statistics.
Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Interna-
tional Journal of Data Warehousing and Mining (I-
JDWM), 3(3):1?13.
Grigorios Tsoumakas and Ioannis Vlahavas. 2007.
Random k-labelsets: An ensemble method for mul-
tilabel classification. Machine Learning: ECML
2007, pages 406?417.
Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition, 40(7):2038?2048.
1821
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1881?1890,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Coarse-grained Candidate Generation and Fine-grained Re-ranking for
Chinese Abbreviation Prediction
Longkai Zhang Houfeng Wang Xu Sun
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn
Abstract
Correctly predicting abbreviations given
the full forms is important in many natu-
ral language processing systems. In this
paper we propose a two-stage method to
find the corresponding abbreviation given
its full form. We first use the contextual
information given a large corpus to get ab-
breviation candidates for each full form
and get a coarse-grained ranking through
graph random walk. This coarse-grained
rank list fixes the search space inside the
top-ranked candidates. Then we use a sim-
ilarity sensitive re-ranking strategy which
can utilize the features of the candidates
to give a fine-grained re-ranking and se-
lect the final result. Our method achieves
good results and outperforms the state-of-
the-art systems. One advantage of our
method is that it only needs weak super-
vision and can get competitive results with
fewer training data. The candidate genera-
tion and coarse-grained ranking is totally
unsupervised. The re-ranking phase can
use a very small amount of training data
to get a reasonably good result.
1 Introduction
Abbreviation Prediction is defined as finding the
meaningful short subsequence of characters given
the original fully expanded form. As an example,
?HMM? is the abbreviation for the correspond-
ing full form ?Hidden Markov Model?. While
the existence of abbreviations is a common lin-
guistic phenomenon, it causes many problems like
spelling variation (Nenadi?c et al., 2002). The dif-
ferent writing manners make it difficult to identify
the terms conveying the same concept, which will
hurt the performance of many applications, such
as information retrieval (IR) systems and machine
translation (MT) systems.
Previous works mainly treat the Chinese ab-
breviation generation task as a sequence labeling
problem, which gives each character a label to in-
dicate whether the given character in the full form
should be kept in the abbreviation or not. These
methods show acceptable results. However they
rely heavily on the character-based features, which
means it needs lots of training data to learn the
weights of these context features. The perfor-
mance is good on some test sets that are similar to
the training data, however, when it moves to an un-
seen context, this method may fail. This is always
true in real application contexts like the social me-
dia where there are tremendous new abbreviations
burst out every day.
A more intuitive way is to find the full-
abbreviation pairs directly from a large text cor-
pus. A good source of texts is the news texts. In
a news text, the full forms are often mentioned
first. Then in the rest of the news its corresponding
abbreviation is mentioned as an alternative. The
co-occurrence of the full form and the abbrevia-
tion makes it easier for us to mine the abbreviation
pairs from the large amount of news texts. There-
fore, given a long full form, we can generate its
abbreviation candidates from the given corpus, in-
stead of doing the character tagging job.
For the abbreviation prediction task, the candi-
date abbreviation must be a sub-sequence of the
given full form. An intuitive way is to select
all the sub-sequences in the corpus as the can-
didates. This will generate large numbers of ir-
relevant candidates. Instead, we use a contextual
graph random walk method, which can utilize the
contextual information through the graph, to select
a coarse grained list of candidates given the full
form. We only select the top-ranked candidates to
reduce the search space. On the other hand, the
candidate generation process can only use limited
contextual information to give a coarse-grained
ranked list of candidates. During generation, can-
1881
didate level features cannot be included. There-
fore we propose a similarity sensitive re-ranking
method to give a fine-grained ranked list. We then
select the final result based on the rank of each
candidate.
The contribution of our work is two folds.
Firstly we propose an improved method for abbre-
viation generation. Compared to previous work,
our method can perform well with less training
data. This is an advantage in the context of so-
cial media. Secondly, we build a new abbreviation
corpus and make it publicly available for future re-
search on this topic.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
the abbreviation task. In section 3 we describe
the candidate generation part and in section 4 we
describe the re-ranking part. Experiments are de-
scribed in section 5. We also give a detailed anal-
ysis of the results in section 5. In section 6 related
works are introduced, and the paper is concluded
in the last section.
2 Chinese Abbreviation Prediction
System
Chinese Abbreviation Prediction is the task of
selecting representative characters from the long
full form
1
. Previous works mainly use the se-
quence labeling strategies, which views the full
form as a character sequence and give each char-
acter an extra label ?Keep? or ?Skip? to indicate
whether the current character should be kept in
the abbreviation. An example is shown in Table
1. The sequence labeling method assumes that
the character context information is crucial to de-
cide the keep or skip of a character. However,
we can give many counterexamples. An exam-
ple is ??????(Peking University) and ??
????(Tsinghua University), whose abbrevia-
tions correspond to ???? and ???? respec-
tively. Although sharing a similar character con-
text, the third character ??? is kept in the first case
and is skipped in the second case.
We believe that a better way is to extract these
abbreviation-full pairs from a natural text corpus
where the full form and its abbreviation co-exist.
Therefore we propose a two stage method. The
first stage generates a list of candidates given a
large corpus. To reduce the search space, we adopt
1
Details of the difference between English and Chinese
abbreviation prediction can be found in Zhang et al. (2012).
Full form ? ? ? ?
Status Skip Keep Keep Skip
Result ? ?
Table 1: The abbreviation ???? of the full form
?????? (Hong Kong University)
graph random walk to give a coarse-grained rank-
ing and select the top-ranked ones as the can-
didates. Then we use a similarity sensitive re-
ranking method to decide the final result. Detailed
description of the two parts is shown in the follow-
ing sections.
3 Candidate Generation through Graph
Random Walk
3.1 Candidate Generation and Graph
Representation
Chinese abbreviations are sub-sequences of the
full form. We use a brute force method to select
all strings in a given news article that is the sub-
sequence of the full form. The brute force method
is not time consuming compared to using more
complex data structures like trie tree, because in
a given news article there are a limited number of
sub-strings which meet the sub-sequence criteria
for abbreviations. When generating abbreviation
candidates for a given full form, we require the
full form should appear in the given news article
at least once. This is a coarse filter to indicate that
the given news article is related to the full form and
therefore the candidates generated are potentially
meaningful.
The main motivation of the candidate genera-
tion stage in our approach is that the full form and
its abbreviation tend to share similar context in a
given corpus. To be more detailed, given a word
context window w, the words that appear in the
context window of the full form tend to be sim-
ilar to those words in the context window of the
abbreviations.
We use a bipartite graph G(V
word
, V
context
, E)
to represent this phenomena. We build bipartite
graphs for each full form individually. For a given
full form v
full
, we first extract all its candidate
abbreviations V
C
. We have two kinds of nodes
in the bipartite graph: the word nodes and the
context nodes. We construct the word nodes as
V
word
= V
C
? {v
full
}, which is the node set of
the full form and all the candidates. We construct
the context nodes V
context
as the words that appear
1882
in a fixed window of V
word
. To reduce the size of
the graph, we make two extra assumptions: 1) We
only consider the nouns and verbs in the context
and 2) context words should appear in the vocab-
ulary for more than a predefined threshold (i.e. 5
times). Because G is bipartite graph, the edges E
only connect word node and context nodes. We
use the number of co-occurrence of the candidate
and the context word as the weight of each edge
and then form the weight matrix W . Details of the
bipartite graph construction algorithm are shown
in Table 2. An example bipartite graph is shown
in figure 1.
Figure 1: An example of the bipartite graph rep-
resentation. The full form is ??????(Hong
Kong University), which is the first node on the
left. The three candidates are ????, ????,
????, which are the nodes on the left. The
context words in this example are ?????(Tsui
Lap-chee, the headmaster of Hong Kong Uni-
versity), ????(Enrollment), ????(Hold), ??
??(Enact), ????(Subway), which are the nodes
on the right. The edge weight is the co-occurrence
of the left word and the right word.
3.2 Coarse-grained Ranking Using Random
Walks
We perform Markov Random Walk on the con-
structed bipartite graph to give a coarse-grained
ranked list of all candidates. In random walk, a
walker starts from the full form source node S
(in later steps, v
i
) and randomly walks to another
node v
j
with a transition probability p
ij
. In ran-
dom walk we assume the walker do the walking n
times and finally stops at a final node. When the
walking is done, we can get the probability of each
node that the walker stops in the end. Because
the destination of each step is selected based on
transition probabilities, the word node that shares
more similar contexts are more likely to be the fi-
nal stop. The random walk method we use is sim-
ilar to those defined in Norris (1998); Zhu et al.
(2003); Sproat et al. (2006); Hassan and Menezes
(2013); Li et al. (2013).
The transition probability p
ij
is calculated us-
ing the weights in the weight matrix W and then
normalized with respect to the source node v
i
with
the formula p
ij
=
w
ij?
l
w
il
. When the graph ran-
dom walk is done, we get a list of coarse-ranked
candidates, each with a confidence score derived
from the context information. By performing the
graph random walk, we reduce the search space
from exponential to the top-ranked ones. Now we
only need to select the final result from the candi-
dates, which we will describe in the next section.
4 Candidate Re-ranking
Although the coarse-grained ranked list can serve
as a basic reference, it can only use limited in-
formation like co-occurrence. We still need a re-
ranking process to decide the final result. The rea-
son is that we cannot get any candidate-specific
features when the candidate is not fully gener-
ated. Features such as the length of a candidate are
proved to be useful to rank the candidates by pre-
vious work. In this section we describe our second
stage for abbreviation generation, which we use a
similarity sensitive re-ranking method to find the
final result.
4.1 Similarity Sensitive Re-ranking
The basic idea behind our similarity sensitive re-
ranking model is that we penalize the mistakes
based on the similarity of the candidate and the
reference. If the model wrongly selects a less sim-
ilar candidate as the result, then we will attach a
large penalty to this mistake. If the model wrongly
chooses a candidate but the candidate is similar to
the reference, we slightly penalize this mistake.
The similarity between a candidate and the ref-
erence is measured through character similarity,
which we will describe later.
1883
Input: the full form v
full
, news corpus U
Output: bipartite graph G(V
word
, V
context
, E)
Candidate vector V
c
= ?, V
context
= ?
for each document d in U
if d contains v
full
add all words w in the window of v
full
into V
context
for each n-gram s in d
if s is a sub-sequence of v
full
add s into V
c
add all word w in the window of s into V
context
end if
end for
end if
end for
V
word
= V
c
? {v
full
}
for each word v
i
in V
word
for each word v
j
in V
context
calculate edge weight in E based on co-occurrence
end for
end for
Return G(V
word
, V
context
, E)
Table 2: Algorithm for constructing bipartite graphs
We first give some notation of the re-ranking
phase.
1. f(x, y) is a scoring function for a given com-
bination of x and y, where x is the original full
form and y is an abbreviation candidate. For a
given full form x
i
with K candidates, we assume
its corresponding K candidates are y
1
i
,y
2
i
,...,y
K
i
.
2. evaluation function s(x, y) is used to mea-
sure the similarity of the candidate to the refer-
ence, where x is the original full form and y is one
abbreviation candidate. We require that s(x, y)
should be in [0, 1] and s(x, y) = 1 if and only if y
is the reference.
One choice for s(x, y) may be the indicator
function. However, indicator function returns zero
for all false candidates. In the abbreviation predic-
tion task, some false candidates are much closer to
the reference than the rest. Considering this, we
use a Longest Common Subsequence(LCS) based
criterion to calculate s(x, y). Suppose the length
of a candidate is a, the length of the reference is b
and the length of their LCS is c, then we can define
precision P and recall R as:
P =
c
a
,
R =
c
b
,
F =
2 ? P ?R
P +R
(1)
It is easy to see that F is a suitable s(x, y).
Therefore we can use the F-score as the value for
s(x, y).
3. ?(x, y) is a feature function which returns a
m dimension feature vector. m is the number of
features in the re-ranking.
4. ~w is a weight vector with dimension m.
~w
T
?(x, y) is the score after re-ranking. The candi-
date with the highest score will be our final result.
Given these notations, we can now describe our
re-ranking algorithm. Suppose we have the train-
ing set X = {x
1
, x
2
, ..., x
n
}. We should find the
weight vector ~w that can minimize the loss func-
tion:
Loss(~w) =
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
? I(~w
T
?(x
i
, y
j
i
) ? ~w
T
?(x
i
, y
1
i
)))
(2)
1884
I(x) is the indicator function. It equals to 1
if and only if x ? 0. I(j) = 1 means that the
candidate which is less ?similar? to the reference
is ranked higher than the reference. Intuitively,
Loss(~w) is the weighted sum of all the wrongly
ranked candidates.
It is difficult to optimize Loss(~w) because
Loss(~w) is discontinuous. We make a relaxation
here
2
:
L(~w) =
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
?
1
1 + e
?~w
T
(?(x
i
,y
j
i
)??(x
i
,y
1
i
))
)
?
1
2
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
? I(~w
T
?(x
i
, y
j
i
) ? ~w
T
?(x
i
, y
1
i
)))
=
1
2
Loss(~w)
(3)
From the equations above we can see that
2L(~w) is the upper bound of our loss function
Loss(~w). Therefore we can optimize L(~w) to ap-
proximate Loss(~w).
We can use optimization methods like gradient
descent to get the ~w that minimize the loss func-
tion. Because L is not convex, it may go into a lo-
cal minimum. In our experiment we held out 10%
data as the develop set and try random initializa-
tion to decide the initial ~w.
4.2 Features for Re-ranking
One advantage of the re-ranking phase is that it
can now use features related to candidates. There-
fore, we can use a variety of features. We list them
as follows.
1. The coarse-grained ranking score from the
graph random walk phase. From the de-
scription of the previous section we know that
this score is the probability a ?walker? ?walk?
from the full form node to the current candi-
date. This is a coarse-grained score because
it can only use the information of words in-
side the window. However, it is still informa-
tive because in the re-ranking phase we can-
not collect this information directly.
2
To prove this we need the following two inequalities: 1)
when x ? 0, I(x) ?
2
1+e
?x
and 2) s(x
i
, y
1
i
) ? s(x
i
, y
j
i
) ?
0.
2. The character uni-grams and bi-grams in
the candidate. This kind of feature cannot
be used in the traditional character tagging
methods.
3. The language model score of the candi-
date. In our experiment, we train a bi-gram
language model using Laplace smoothing on
the Chinese Gigaword Data
3
.
4. The length of the candidate. Intuitively,
abbreviations tend to be short. Therefore
length can be an important feature for the re-
ranking.
5. The degree of ambiguity of the candidate.
We first define the degree of ambiguity d
i
of a
character c
i
as the number of identical words
that contain the character. We then define the
degree of ambiguity of the candidate as the
sum of all d
i
in the candidates. We need a dic-
tionary to extract this feature. We collect all
words in the PKU data of the second Interna-
tional Chinese Word Segmentation Bakeoff
4
.
6. Whether the candidate is in a word dictio-
nary. We use the PKU dictionary in feature
5.
7. Whether all bi-grams are in a word dictio-
nary. We use the PKU dictionary in feature
5.
8. Adjacent Variety(AV) of the candidate. We
define the left AV of the candidate as the
probability that in a corpus the character in
front of the candidate is a character in the
full form. For example if we consider the full
form ??????(Peking University) and the
candidate ????, then the left AV of ????
is the probability that the character preced-
ing ???? is ??? or ??? or ??? or ??? in
a corpus. We can similarly define the right
AV, with respect to characters follow the can-
didate.
The AV feature is very useful because in some
cases a substring of the full form may have a con-
fusingly high frequency. In the example of ???
???(Peking University), an article in the corpus
may mention ??????(Peking University) and
3
http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
4
http://www.sighan.org/bakeoff2005/
1885
??????(Tokyo University) at the same time.
Then the substring ????? may be included in
the candidate generation phase for ??????
with a high frequency. Because the left AV of ??
??? is high, the re-ranker can easily detect this
false candidate.
In practice, all the features need to be scaled in
order to speed up training. There are many ways
to scale features. We use the most intuitive scal-
ing method. For a feature value x, we scale it as
(x?mean)/(max?min). Note that for language
model score and the score of random walk phase,
we scale based on their log value.
5 Experiments
5.1 Dataset and Evaluation metrics
For the dataset, we collect 3210 abbreviation pairs
from the Chinese Gigaword corpus. The abbre-
viation pairs include noun phrases, organization
names and some other types. The Chinese Gi-
gaword corpus contains news texts from the year
1992 to 2007. We only collect those pairs whose
full form and corresponding abbreviation appear
in the same article for at least one time. For full
forms with more than one reasonable reference,
we keep the most frequently used one as its refer-
ence. We use 80% abbreviation pairs as the train-
ing data and the rest as the testing data.
We use the top-K accuracy as the evaluation
metrics. The top-K accuracy is widely used as the
measurement in previous work (Tsuruoka et al.,
2005; Sun et al., 2008, 2009; Zhang et al., 2012). It
measures what percentage of the reference abbre-
viations are found if we take the top k candidate
abbreviations from all the results. In our experi-
ment, we compare the top-5 accuracy with base-
lines. We choose the top-10 candidates from the
graph random walk are considered in re-ranking
phase and the measurement used is top-1 accuracy
because the final aim of the algorithm is to detect
the exact abbreviation, rather than a list of candi-
dates.
5.2 Candidate List
Table 3 shows examples of the candidates. In our
algorithm we further reduce the search space to
only incorporate 10 candidates from the candidate
generation phase.
K Top-K Accuracy
1 6.84%
2 19.35%
3 49.01%
4 63.70%
5 73.60%
Table 4: Top-5 accuracy of the candidate genera-
tion phase
5.3 Comparison with baselines
We first show the top-5 accuracy of the candidate
generation phase Table 4. We can see that, just
like the case of using other feature alone, using
the score of random walk alone is far from enough.
However, the first 5 candidates contain most of the
correct answers. We use the top-5 candidates plus
another 5 candidates in the re-ranking phase.
We choose the character tagging method as the
baseline method. The character tagging strategy
is widely used in the abbreviation generation task
(Tsuruoka et al., 2005; Sun et al., 2008, 2009;
Zhang et al., 2012). We choose the ?SK? labeling
strategy which is used in Sun et al. (2009); Zhang
et al. (2012). The ?SK? labeling strategy gives each
character a label in the character sequence, with
?S? represents ?Skip? and ?K? represents ?Keep?.
Same with Zhang et al. (2012), we use the Con-
ditional Random Fields (CRFs) model in the se-
quence labeling process.
The baseline method mainly uses the charac-
ter context information to generate the candidate
abbreviation. To be fair we use the same fea-
ture set in Sun et al. (2009); Zhang et al. (2012).
One drawback of the sequence labeling method is
that it relies heavily on the character context in
the full form. With the number of new abbrevi-
ations grows rapidly (especially in social media
like Facebook or twitter), it is impossible to let the
model ?remember? all the character contexts. Our
method is different from theirs, we use a more in-
tuitive way which finds the list of candidates di-
rectly from a natural corpus.
Table 5 shows the comparison of the top-5 accu-
racy. We can see that our method outperforms the
baseline methods. The baseline model performs
well when using character features (Column 3).
However, it performs poorly without the charac-
ter features (Column 2). In contrast, without the
character features, our method (Column 4) works
much better than the sequence labeling method.
1886
Full form Reference Generated Candidates #Enum #Now
????? (Depart-
ment of International
Politics)
??? ???,???,????,??
?,??,??,??
30 7
?????? (Non-
nuclear Countries)
??? ??,??,??,???,??
?,???,????,???
?,????,?????,??
?,??,??
62 13
???? (Drug traf-
ficking)
?? ???,???,??,??,?? 14 5
?????????
????? (Yangtze
Joint River Economic
Development Inc.)
???? ??,??,????,???
?,????,????,???
?,????,??????,?
?????,??????,??
????,????????,?
?????,????,??,?
?,??,??,??
16382 20
Table 3: Generated Candidates. #Enum is the number of candidates generated by enumerating all possi-
ble candidates. #Now is the number of candidates generated by our method.
When we add character features, our method (Col-
umn 5) still outperforms the sequence labeling
method.
K CRF-char Our-char CRF Our
1 38.00% 48.60% 53.27% 55.61%
2 38.16% 70.87% 65.89% 73.10%
3 39.41% 81.78% 72.43% 81.96%
4 55.30% 87.54% 78.97% 87.57%
5 62.31% 89.25% 81.78% 89.27%
Table 5: Comparison of the baseline method and
our method. CRF-char (?-? means minus) is the
baseline method without character features. CRF
is the baseline method. Our-char is our method
without character features. We define character
features as the features that consider the charac-
ters from the original full form as their parts.
5.4 Performance with less training data
One advantage of our method is that it only
requires weak supervision. The baseline
method needs plenty of manually collected
full-abbreviation pairs to learn a good model.
In our method, the candidate generation and
coarse-grained ranking is totally unsupervised.
The re-ranking phase needs training instances
to decide the parameters. However we can use
a very small amount of training data to get a
reasonably good model. Figure 2 shows the result
of using different size of training data. We can
see that the performance of the baseline methods
drops rapidly when there are less training data.
In contrast, when using less training data, our
method does not suffer that much.
Figure 2: Top-1 accuracy when changing the size
of training data. For example, ?50%? means ?us-
ing 50% of all the training data?.
5.5 Comparison with previous work
We compare our method with the method in the
previous work DPLVM+GI in Sun et al. (2009),
which outperforms Tsuruoka et al. (2005); Sun
et al. (2008). We also compare our method with
the web-based method CRF+WEB in Zhang et al.
(2012). Because the comparison is performed on
different corpora, we run the two methods on our
data. Table 6 shows the top-1 accuracy. We
can see that our method outperforms the previous
1887
methods.
System Top-K Accuracy
DPLVM+GI 53.29%
CRF+WEB 54.02%
Our method 55.61%
Table 6: Comparison with previous work. The
search results of CRF+WEB is based on March 9,
2014 version of the Baidu search engine.
5.6 Error Analysis
We perform cross-validation to find the errors and
list the two major errors below:
1. Some full forms may correspond to more
than one acceptable abbreviation. In this
case, our method may choose the one that is
indeed used as the full form?s abbreviation in
news texts, but not the same as the standard
reference abbreviations. The reason for this
phenomenon may lie in the fact that the veri-
fication data we use is news text, which tends
to be formal. Therefore when a reference is
often used colloquially, our method may miss
it. We can relieve this by changing the corpus
we use.
2. Our method may provide biased information
when handling location sensitive phrases.
Not only our system, the system of Sun et al.
(2009); Zhang et al. (2012) also shows this
phenomenon. An example is the case of ??
?????? (Democracy League of Hong
Kong). Because most of the news is about
news in mainland China, it is hard for the
model to tell the difference between the ref-
erence ????? and a false candidate ??
??(Democracy League of China).
Another ambiguity is ??????(Tsinghua
University), which has two abbreviations ??
?? and ????. This happens because the
full form itself is ambiguous. Word sense dis-
ambiguation can be performed first to handle
this kind of problem.
6 Related Work
Abbreviation generation has been studied during
recent years. At first, some approaches maintain
a database of abbreviations and their correspond-
ing ?full form? pairs. The major problem of pure
database-building approach is obvious. It is im-
possible to cover all abbreviations, and the build-
ing process is quite laborious. To find these pairs
automatically, a powerful approach is to find the
reference for a full form given the context, which
is referred to as ?abbreviation generation?.
There is research on using heuristic rules
for generating abbreviations Barrett and Grems
(1960); Bourne and Ford (1961); Taghva and
Gilbreth (1999); Park and Byrd (2001); Wren et al.
(2002); Hearst (2003). Most of them achieved
high performance. However, hand-crafted rules
are time consuming to create, and it is not easy to
transfer the knowledge of rules from one language
to another.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed an SVM ap-
proach. Tsuruoka et al. (2005); Sun et al. (2009)
formalized the process of abbreviation generation
as a sequence labeling problem. The drawback of
the sequence labeling strategies is that they rely
heavily on the character features. This kind of
method cannot fit the need for abbreviation gen-
eration in social media texts where the amount of
abbreviations grows fast.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations. Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method only uses a fixed corpus, instead of us-
ing collective information, which varies from time
to time.
Some of the previous work that relate to ab-
breviations focuses on the task of ?abbreviation
disambiguation?, which aims to find the correct
abbreviation-full pairs. In these works, machine
learning approaches are commonly used (Park and
Byrd, 2001; HaCohen-Kerner et al., 2008; Yu
et al., 2006; Ao and Takagi, 2005). We focus on
another aspect. We want to find the abbreviation
1888
given the full form. Besides, Sun et al. (2013) also
works on abbreviation prediction but focuses on
the negative full form problem, which is a little
different from our work.
One related research field is text normalization,
with many outstanding works (Sproat et al., 2001;
Aw et al., 2006; Hassan and Menezes, 2013; Ling
et al., 2013; Yang and Eisenstein, 2013). While
the two tasks share similarities, abbreviation pre-
diction has its identical characteristics, like the
sub-sequence assumption. This results in different
methods to tackle the two different problems.
7 Conclusion
In this paper, we propose a unified framework for
Chinese abbreviation generation. Our approach
contains two stages: candidate generation and
re-ranking. Given a long term, we first gener-
ate a list of abbreviation candidates using the co-
occurrence information. We give a coarse-grained
rank using graph random walk to reduce the search
space. After we get the candidate lists, we can use
the features related to the candidates. We use a
similarity sensitive re-rank method to get the final
abbreviation. Experiments show that our method
outperforms the previous systems.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
References
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527?533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576?586.
Aw, A., Zhang, M., Xiao, J., and Su, J. (2006). A
phrase-based statistical model for sms text nor-
malization. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 33?
40. Association for Computational Linguistics.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323?324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538?552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61?64. Associa-
tion for Computational Linguistics.
Hassan, H. and Menezes, A. (2013). Social text
normalization using contextual graph random
walks. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1577?
1586, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209?214. IEEE.
Li, J., Ott, M., and Cardie, C. (2013). Identify-
ing manipulated offerings on review portals. In
EMNLP, pages 1933?1942.
Ling, W., Dyer, C., Black, A. W., and Trancoso, I.
(2013). Paraphrasing 4 microblog normaliza-
tion. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language
Processing, pages 73?84, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Nenadi?c, G., Spasi?c, I., and Ananiadou, S. (2002).
Automatic acronym acquisition and term varia-
tion management within domain-specific texts.
1889
In Third International Conference on Language
Resources and Evaluation (LREC2002), pages
2155?2162.
Norris, J. R. (1998). Markov chains. Number
2008. Cambridge university press.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126?133.
Sproat, R., Black, A. W., Chen, S., Kumar, S.,
Ostendorf, M., and Richards, C. (2001). Nor-
malization of non-standard words. Computer
Speech & Language, 15(3):287?333.
Sproat, R., Tao, T., and Zhai, C. (2006). Named
entity transliteration with comparable corpora.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 73?80. Association for
Computational Linguistics.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641?647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905?913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602?611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191?198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25?31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426?434.
Yang, Y. and Eisenstein, J. (2013). A log-linear
model for unsupervised text normalization. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing,
pages 61?72, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380?404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055?3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhu, X., Ghahramani, Z., Lafferty, J., et al. (2003).
Semi-supervised learning using gaussian fields
and harmonic functions. In ICML, volume 3,
pages 912?919.
1890
Feature-Frequency?Adaptive On-line
Training for Fast and Accurate Natural
Language Processing
Xu Sun?
Peking University
Wenjie Li??
Hong Kong Polytechnic University
Houfeng Wang?
Peking University
Qin Lu?
Hong Kong Polytechnic University
Training speed and accuracy are two major concerns of large-scale natural language processing
systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve
the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.
Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time,
which is the target of this work. To reach this target, we present a new training method, feature-
frequency?adaptive on-line training, for fast and accurate training of natural language process-
ing systems. It is based on the core idea that higher frequency features should have a learning rate
that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast
convergence rate. Experiments are conducted based on well-known benchmark tasks, including
named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These
tasks consist of three structured classification tasks and one non-structured classification task,
with binary features and real-valued features, respectively. Experimental results demonstrate
that the proposed method is faster and at the same time more accurate than existing methods,
achieving state-of-the-art scores on the tasks with different characteristics.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: xusun@pku.edu.cn.
?? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: cswjli@comp.polyu.edu.hk.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: wanghf@pku.edu.cn.
? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: csluqin@comp.polyu.edu.hk.
Submission received: 27 December 2012; revised version received: 30 May 2013; accepted for publication:
16 September 2013.
doi:10.1162/COLI a 00193
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Training speed is an important concern of natural language processing (NLP) systems.
Large-scale NLP systems are computationally expensive. In many real-world applica-
tions, we further need to optimize high-dimensional model parameters. For example,
the state-of-the-art word segmentation system uses more than 40 million features (Sun,
Wang, and Li 2012). The heavyNLPmodels together with high-dimensional parameters
lead to a challenging problem onmodel training, whichmay require week-level training
time even with fast computing machines.
Accuracy is another very important concern of NLP systems. Nevertheless, usually
it is quite difficult to build a system that has fast training speed and at the same time
has high accuracy. Typically we need to make a tradeoff between speed and accuracy,
to trade training speed for higher accuracy or vice versa. In this work, we have tried
to overcome this problem: to improve the training speed and the model accuracy at the
same time.
There are twomajor approaches for parameter training: batch and on-line. Standard
gradient descent methods are normally batch training methods, in which the gradient
computed by using all training instances is used to update the parameters of the model.
The batch training methods include, for example, steepest gradient descent, conjugate
gradient descent (CG), and quasi-Newtonmethods like limited-memory BFGS (Nocedal
and Wright 1999). The true gradient is usually the sum of the gradients from each
individual training instance. Therefore, batch gradient descent requires the training
method to go through the entire training set before updating parameters. This is why
batch training methods are typically slow.
On-line learning methods can significantly accelerate the training speed compared
with batch training methods. A representative on-line training method is the stochastic
gradient descent method (SGD) and its extensions (e.g., stochastic meta descent) (Bottou
1998; Vishwanathan et al. 2006). The model parameters are updated more frequently
compared with batch training, and fewer passes are needed before convergence. For
large-scale data sets, on-line training methods can be much faster than batch training
methods.
However, we find that the existing on-line training methods are still not good
enough for training large-scale NLP systems?probably because those methods are
not well-tailored for NLP systems that have massive features. First, the convergence
speed of the existing on-line training methods is not fast enough. Our studies show that
the existing on-line training methods typically require more than 50 training passes
before empirical convergence, which is still slow. For large-scale NLP systems, the
training time per pass is typically long and fast convergence speed is crucial. Second,
the accuracy of the existing on-line training methods is not good enough. We want to
further improve the training accuracy. We try to deal with the two challenges at the
same time. Our goal is to develop a new training method for faster and at the same time
more accurate natural language processing.
In this article, we present a new on-line training method, adaptive on-line gradient
descent based on feature frequency information (ADF),1 for very accurate and fast
on-line training of NLP systems. Other than the high training accuracy and fast train-
ing speed, we further expect that the proposed training method has good theoretical
1 ADF source code and tools can be obtained from http://klcl.pku.edu.cn/member/sunxu/index.htm.
564
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
properties. We want to prove that the proposed method is convergent and has a fast
convergence rate.
In the proposed ADF training method, we use a learning rate vector in the on-line
updating. This learning rate vector is automatically adapted based on feature frequency
information in the training data set. Each model parameter has its own learning rate
adapted on feature frequency information. This proposal is based on the simple intu-
ition that a feature with higher frequency in the training process should have a learning
rate that decays faster. This is because a higher frequency feature is expected to be
well optimized with higher confidence. Thus, a higher frequency feature is expected to
have a lower learning rate. We systematically formalize this intuition into a theoretically
sound training algorithm, ADF.
The main contributions of this work are as follows:
r On the methodology side, we propose a general purpose on-line training
method, ADF. The ADF method is significantly more accurate than
existing on-line and batch training methods, and has faster training speed.
Moreover, theoretical analysis demonstrates that the ADF method is
convergent with a fast convergence rate.
r On the application side, for the three well-known tasks, including named
entity recognition, word segmentation, and phrase chunking, the proposed
simple method achieves equal or even better accuracy than the existing
gold-standard systems, which are complicated and use extra resources.
2. Related Work
Our main focus is on structured classification models with high dimensional features.
For structured classification, the conditional random fields model is widely used. To
illustrate that the proposed method is a general-purpose training method not limited to
a specific classification task or model, we also evaluate the proposal for non-structured
classification tasks like binary classification. For non-structured classification, the max-
imum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996)
is widely used. Here, we review the conditional random fields model and the related
work of on-line training methods.
2.1 Conditional Random Fields
The conditional random field (CRF) model is a representative structured classification
model and it is well known for its high accuracy in real-world applications. The CRF
model is proposed for structured classification by solving ?the label bias problem?
(Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a global feature vector f, the probability
of a label sequence y conditioned on the observation sequence x is modeled as follows
(Lafferty, McCallum, and Pereira 2001):
P(y|x,w) =
exp {w>f (y,x)}
?
?y? exp {w>f (y? ,x)}
(1)
wherew is a parameter vector.
565
Computational Linguistics Volume 40, Number 3
Given a training set consisting of n labeled sequences, zi = (xi,yi), for i = 1 . . .n,
parameter estimation is performed by maximizing the objective function,
L(w) =
n
?
i=1
logP(yi|xi,w)? R(w) (2)
The first term of this equation represents a conditional log-likelihood of training
data. The second term is a regularizer for reducing overfitting. We use an L2 prior,
R(w) = ||w||
2
2?2 . In what follows, we denote the conditional log-likelihood of each sample
as logP(yi|xi,w) as `(zi,w). The final objective function is as follows:
L(w) =
n
?
i=1
`(zi,w)?
||w||2
2?2
(3)
2.2 On-line Training
The most representative on-line training method is the SGD method (Bottou 1998;
Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). The SGD method uses a
randomly selected small subset of the training sample to approximate the gradient of
an objective function. The number of training samples used for this approximation is
called the batch size. By using a smaller batch size, one can update the parameters
more frequently and speed up the convergence. The extreme case is a batch size of 1,
and it gives the maximum frequency of updates, which we adopt in this work. In this
case, the model parameters are updated as follows:
wt+1 = wt + ?t?wtLstoch(zi,wt) (4)
where t is the update counter, ?t is the learning rate or so-called decaying rate, and
Lstoch(zi,wt) is the stochastic loss function based on a training sample zi. (More details
of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and
Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying
rate works the best for natural language processing tasks, and it is adopted in our
implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013).
Other well-known on-line training methods include perceptron training (Freund
and Schapire 1999), averaged perceptron training (Collins 2002), more recent devel-
opment/extensions of stochastic gradient descent (e.g., the second-order stochastic
gradient descent training methods like stochastic meta descent) (Vishwanathan et al.
2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent
method requires the computation or approximation of the inverse of the Hessian matrix
of the objective function, which is typically slow, especially for heavily structured classi-
fication models. Usually the convergence speed based on number of training iterations
is moderately faster, but the time cost per iteration is slower. Thus the overall time cost
is still large.
Compared with the related work on batch and on-line training (Jacobs 1988;
Sperduti and Starita 1993; Dredze, Crammer, and Pereira 2008; Duchi, Hazan, and
Singer 2010; McMahan and Streeter 2010), our work is fundamentally different. The
proposedADF trainingmethod is based on feature frequency adaptation, and to the best
of our knowledge there is no prior work on direct feature-frequency?adaptive on-line
566
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
training. Compared with the confidence-weighted (CW) classification method and its
variation AROW (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze
2009), the proposed method is substantially different. While the feature frequency
information is implicitly modeled via a complicated Gaussian distribution framework
in Dredze, Crammer, and Pereira (2008) and Crammer, Kulesza, and Dredze (2009),
the frequency information is explicitly modeled in our proposal via simple learning
rate adaptation. Our proposal is more straightforward in capturing feature frequency
information, and it has no need to use Gaussian distributions and KL divergence,
which are important in the CW and AROW methods. In addition, our proposal is a
probabilistic learning method for training probabilistic models such as CRFs, whereas
the CW and AROW methods (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza,
and Dredze 2009) are non-probabilistic learning methods extended from perceptron-
style approaches. Thus, the framework is different. This work is a substantial extension
of the conference version (Sun, Wang, and Li 2012). Sun, Wang, and Li (2012) focus on
the specific task of word segmentation, whereas this article focuses on the proposed
training algorithm.
3. Feature-Frequency?Adaptive On-line Learning
In traditional on-line optimization methods such as SGD, no distinction is made for
different parameters in terms of the learning rate, and this may result in slow conver-
gence of the model training. For example, in the on-line training process, suppose the
high frequency feature f1 and the low frequency feature f2 are observed in a training
sample and their corresponding parameters w1 and w2 are to be updated via the same
learning rate ?t. Suppose the high frequency feature f1 has been updated 100 times
and the low frequency feature f2 has only been updated once. Then, it is possible that
the weight w1 is already well optimized and the learning rate ?t is too aggressive for
updating w1. Updating the weight w1 with the learning rate ?t may make w1 be far
from the well-optimized value, and it will require corrections in the future updates. This
causes fluctuations in the on-line training and results in slow convergence speed. On
the other hand, it is possible that the weight w2 is poorly optimized and the same learn-
ing rate ?t is too conservative for updating w2. This also results in slow convergence
speed.
To solve this problem, we propose ADF. In spite of the high accuracy and fast
convergence speed, the proposed method is easy to implement. The proposed method
with feature-frequency?adaptive learning rates can be seen as a learning method with
specific diagonal approximation of the Hessian information based on assumptions of
feature frequency information. In this approximation, the diagonal elements of the
diagonal matrix correspond to the feature-frequency?adaptive learning rates. Accord-
ing to the aforementioned example and analysis, it assumes that a feature with higher
frequency in the training process should have a learning rate that decays faster.
3.1 Algorithm
In the proposed ADF method, we try to use more refined learning rates than traditional
SGD training. Instead of using a single learning rate (a scalar) for all weights, we extend
the learning rate scalar to a learning rate vector, which has the same dimension as the
weight vector w. The learning rate vector is automatically adapted based on feature
567
Computational Linguistics Volume 40, Number 3
frequency information. By doing so, each weight has its own learning rate, and we will
show that this can significantly improve the convergence speed of on-line learning.
In the ADF learning method, the update formula is:
wt+1 = wt +?t ? gt (5)
The update term gt is the gradient term of a randomly sampled instance:
gt = ?wtLstoch(zi,wt) = ?wt
{
`(zi,wt)?
||wt||2
2n?2
}
In addition, ?t ? R
f
+ is a positive vector-valued learning rate and ? denotes the
component-wise (Hadamard) product of two vectors.
The learning rate vector ?t is automatically adapted based on feature frequency
information in the updating process. Intuitively, a feature with higher frequency in the
training process has a learning rate that decays faster. This is because a weight with
higher frequency is expected to be more adequately trained, hence a lower learning
rate is preferable for fast convergence. We assume that a high frequency feature should
have a lower learning rate, and a low frequency feature should have a relatively higher
learning rate in the training process.We systematically formalize this idea into a theoret-
ically sound training algorithm. The proposedmethodwith feature-frequency?adaptive
learning rates can be seen as a learning method with specific diagonal approximation
of the inverse of the Hessian matrix based on feature frequency information.
Given awindow size q (number of samples in awindow), we use a vector v to record
the feature frequency. The kth entry vk corresponds to the frequency of the feature k in
this window. Given a feature k, we use u to record the normalized frequency:
u = vk/q
For each feature, an adaptation factor ? is calculated based on the normalized frequency
information, as follows:
? = ?? u(?? ?)
where ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1. Intu-
itively, the upper bound ? corresponds to the adaptation factor of the lowest frequency
features, and the lower bound ? corresponds to the adaptation factor of the highest
frequency features. The optimal values of ? and ? can be tuned based on specific real-
world tasks, for example, via cross-validation on the training data or using held-out
data. In practice, via cross-validation on the training data of different tasks, we found
that the following setting is sufficient to produce adequate performance for most of the
real-world natural language processing tasks: ? around 0.995, and ? around 0.6. This
indicates that the feature frequency information has similar characteristics across many
different natural language processing tasks.
As we can see, a feature with higher frequency corresponds to a smaller scalar via
linear approximation. Finally, the learning rate is updated as follows:
?k ? ??k
568
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
ADF learning algorithm
1: procedure ADF(Z,w, q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample zi at random from the data set Z
5: . v ? UPDATEFEATUREFREQ(v, zi)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATELEARNRATE(?, v)
8: . . v ? 0
9: . g ??wLstoch(zi,w)
10: . w ? w +? ? g
11: . t? t+ 1
12: returnw
13:
14: procedure UPDATEFEATUREFREQ(v, zi)
15: for k ? features used in sample zi
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATELEARNRATE(?, v)
20: for k ? all features
21: . u? vk/q
22: . ?? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1
The proposed ADF on-line learning algorithm. In the algorithm, Z is the training data set; q, c, ?,
and ? are hyper-parameters; q is an integer representing window size; c is for initializing the
learning rates; and ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1.
With this setting, different features correspond to different adaptation factors based
on feature frequency information. Our ADF algorithm is summarized in Figure 1.
The ADF training method is efficient because the only additional computation
(compared with traditional SGD) is the derivation of the learning rates, which is simple
and efficient. As we know, the regularization of SGD can perform efficiently via the opti-
mization based on sparse features (Shalev-Shwartz, Singer, and Srebro 2007). Similarly,
the derivation of ?t can also perform efficiently via the optimization based on sparse
features. Note that although binary features are common in natural language processing
tasks, the ADF algorithm is not limited to binary features and it can be applied to real-
valued features.
3.2 Convergence Analysis
We want to show that the proposed ADF learning algorithm has good convergence
properties. There are two steps in the convergence analysis. First, we show that the
ADF update rule is a contraction mapping. Then, we show that the ADF training is
asymptotically convergent, and with a fast convergence rate.
To simplify the discussion, our convergence analysis is based on the convex loss
function of traditional classification or regression problems:
L(w) =
n
?
i=1
`(xi, yi,w ? f i)?
||w||2
2?2
569
Computational Linguistics Volume 40, Number 3
where f i is the feature vector generated from the training sample (xi, yi). L(w) is a func-
tion in w ? f i, such as 12 (yi ?w ? f i)2 for regression or log[1+ exp(?yiw ? f i)] for binary
classification.
To make convergence analysis of the proposed ADF training algorithm, we need to
introduce several mathematical definitions. First, we introduce Lipschitz continuity:
Definition 1 (Lipschitz continuity)
A function F : X ? R is Lipschitz continuous with the degree of D if |F(x)? F(y)| ?
D|x? y| for ?x, y ? X . X can be multi-dimensional space, and |x? y| is the distance
between the points x and y.
Based on the definition of Lipschitz continuity, we give the definition of the
Lipschitz constant ||F||Lip as follows:
Definition 2 (Lipschitz constant)
||F||Lip := inf{D where |F(x)? F(y)| ? D|x? y| for ?x, y}
In other words, the Lipschitz constant ||F||Lip is the lower bound of the continuity degree
that makes the function F Lipschitz continuous.
Further, based on the definition of Lipschitz constant, we give the definition of
contraction mapping as follows:
Definition 3 (Contraction mapping)
A function F : X ? X is a contraction mapping if its Lipschitz constant is smaller than
1: ||F||Lip < 1.
Then, we can show that the traditional SGD update is a contraction mapping.
Lemma 1 (SGD update rule is contraction mapping)
Let ? be a fixed low learning rate in SGD updating. If ? ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1,
the SGD update rule is a contraction mapping in Euclidean space with Lipschitz con-
tinuity degree 1? ?/?2.
The proof can be extended from the relatedwork on convergence analysis of parallel
SGD training (Zinkevich et al. 2010). The stochastic training process is a one-following-
one dynamic update process. In this dynamic process, if we use the same update rule F,
we havewt+1 = F(wt) andwt+2 = F(wt+1). It is only necessary to prove that the dynamic
update is a contraction mapping restricted by this one-following-one dynamic process.
That is, for the proposed ADF update rule, it is only necessary to prove it is a dynamic
contraction mapping. We formally define dynamic contraction mapping as follows.
Definition 4 (Dynamic contraction mapping)
Given a function F : X ? X , suppose the function is used in a dynamic one-following-
one process: xt+1 = F(xt) and xt+2 = F(xt+1) for ?xt ? X . Then, the function F is a
dynamic contraction mapping if ?D < 1, |xt+2 ? xt+1| ? D|xt+1 ? xt| for ?xt ? X .
We can see that a contraction mapping is also a dynamic contraction mapping, but
a dynamic contraction mapping is not necessarily a contraction mapping. We first show
570
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
that the ADF update rule with a fixed learning rate vector of different learning rates is
a dynamic contraction mapping.
Theorem 1 (ADF update rule with fixed learning rates)
Let ? be a fixed learning rate vector with different learning rates. Let ?max be the max-
imum learning rate in the learning rate vector ?: ?max := sup{?i where ?i ? ?}. Then
if ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, the ADF update rule is a dynamic contraction
mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Further, we need to prove that the ADF update rule with a decaying learning
rate vector is a dynamic contraction mapping, because the real ADF algorithm has a
decaying learning rate vector. In the decaying case, the condition that ?max ? (||x2i || ?
||?y? `(xi, yi, y?)||Lip)?1 can be easily achieved, because ? continues to decay with an
exponential decaying rate. Even if the ? is initialized with high values of learning rates,
after a number of training passes (denoted as T) ?T is guaranteed to be small enough so
that ?max := sup{?i where ?i ? ?T} and ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Without
losing generality, our convergence analysis starts from the pass T and we take ?T as ?0
in the following analysis. Thus, we can show that the ADF update rule with a decaying
learning rate vector is a dynamic contraction mapping:
Theorem 2 (ADF update rule with decaying learning rates)
Let ?t be a learning rate vector in the ADF learning algorithm, which is decaying
over the time t and with different decaying rates based on feature frequency infor-
mation. Let ?t start from a low enough learning rate vector ?0 such that ?max ?
(||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, where ?max is the maximum element in?0. Then, the ADF
update rule with decaying learning rate vector is a dynamic contraction mapping in
Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Based on the connections between ADF training and contraction mapping, we
demonstrate the convergence properties of the ADF training method. First, we prove
the convergence of the ADF training.
Theorem 3 (ADF convergence)
ADF training is asymptotically convergent.
The proof is sketched in Section 5.
Further, we analyze the convergence rate of the ADF training. When we have the
lowest learning rate?t+1 = ??t, the expectation of the obtainedwt is as follows (Murata
1998; Hsu et al. 2009):
E(wt) = w? +
t
?
m=1
(I ??0?mH(w?))(w0 ?w?)
where w? is the optimal weight vector, and H is the Hessian matrix of the objective
function. The rate of convergence is governed by the largest eigenvalue of the function
Ct =
?t
m=1(I ??0?mH(w?)). Following Murata (1998) and Hsu et al. (2009), we can
derive a bound of rate of convergence, as follows.
571
Computational Linguistics Volume 40, Number 3
Theorem 4 (ADF convergence rate)
Assume ? is the largest eigenvalue of the function Ct =
?t
m=1(I ??0?mH(w?)). For the
proposed ADF training, its convergence rate is bounded by ?, and we have
? ? exp {?0???? 1}
where ? is the minimum eigenvalue ofH(w?).
The proof is sketched in Section 5.
The convergence analysis demonstrates that the proposed method with feature-
frequency-adaptive learning rates is convergent and the bound of convergence rate
is analyzed. It demonstrates that increasing the values of ?0 and ? leads to a lower
bound of the convergence rate. Because the bound of the convergence rate is just an
up-bound rather than the actual convergence rate, we still need to conduct automatic
tuning of the hyper-parameters, including ?0 and ?, for optimal convergence rate in
practice. The ADF training method has a fast convergence rate because the feature-
frequency-adaptive schema can avoid the fluctuations on updating the weights of high
frequency features, and it can avoid the insufficient training on updating the weights of
low frequency features. In the following sections, we perform experiments to confirm
the fast convergence rate of the proposed method.
4. Evaluation
Our main focus is on training heavily structured classification models. We evaluate the
proposal on three NLP structured classification tasks: biomedical named entity recogni-
tion (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking. For the
structured classification tasks, the ADF training is based on the CRF model (Lafferty,
McCallum, and Pereira 2001). Further, to demonstrate that the proposed method is
not limited to structured classification tasks, we also perform experiments on a non-
structured binary classification task: sentiment-based text classification. For the non-
structured classification task, the ADF training is based on themaximum entropymodel
(Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996).
4.1 Biomedical Named Entity Recognition (Structured Classification)
The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004
shared task. The task is to recognize five kinds of biomedical named entities, including
DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining
corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential
labeling task with the BIO encoding.
This data set consists of 20,546 training samples (from 2,000 MEDLINE article
abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data
are summarized in Table 1. State-of-the-art systems for this task include Settles (2004),
Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al.
(2009), and Tsuruoka, Tsujii, and Ananiadou (2009).
Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki,
et al. 2009), we use word token?based features, part-of-speech (POS) based features,
and orthography pattern?based features (prefix, uppercase/lowercase, etc.), as listed in
Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package),
the edges features usually contain only the information of yi?1 and yi, and ignore the
572
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 1
Summary of the Bio-NER data set.
#Abstracts #Sentences #Words
Train 2,000 20,546 (10/abs) 472,006 (23/sen)
Test 404 4,260 (11/abs) 96,780 (23/sen)
Table 2
Feature templates used for the Bio-NER task. wi is the current word token on position i. ti is the
POS tag on position i. oi is the orthography mode on position i. yi is the classification label on
position i. yi?1yi represents label transition. A? B represents a Cartesian product between
two sets.
Word Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
Orthography Pattern?based Features:
{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi, oioi+1, oi+1oi+2}
?{yi, yi?1yi}
information of the observation sequence (i.e., x). The major reason for this simple real-
ization of edge features in traditional CRF implementation is to reduce the dimension
of features. To improve the model accuracy, we utilize rich edge features following Sun,
Wang, and Li (2012), in which local observation information of x is combined in edge
features just like the implementation of node features. A detailed introduction to rich
edge features can be found in Sun, Wang, and Li (2012). Using the feature templates,
we extract a high dimensional feature set, which contains 5.3? 107 features in total.
Following prior studies, the evaluation metric for this task is the balanced F-score
defined as 2PR/(P+ R), where P is precision and R is recall.
4.2 Chinese Word Segmentation (Structured Classification)
Chinese word segmentation aims to automatically segment character sequences into
word sequences. Chinese word segmentation is important because it is the first step
for most Chinese language information processing systems. Our experiments are based
on the Microsoft Research data provided by The Second International Chinese Word
Segmentation Bakeoff. In this data set, there are 8.8? 104 word-types, 2.4? 106 word-
tokens, 5? 103 character-types, and 4.1? 106 character-tokens. State-of-the-art systems
for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and
Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010),
and Zhao and Kit (2011).
The feature engineering follows previous work on word segmentation (Sun, Wang,
and Li 2012). Rich edge features are used. For the classification label yi and the label
transition yi?1yi on position i, we use the feature templates as follows (Sun, Wang, and
Li 2012):
r Character unigrams located at positions i? 2, i? 1, i, i+ 1, and i+ 2.
573
Computational Linguistics Volume 40, Number 3
r Character bigrams located at positions i? 2, i? 1, i and i+ 1.
r Whether xj and xj+1 are identical, for j = i? 2, . . . , i+ 1.
r Whether xj and xj+2 are identical, for j = i? 3, . . . , i+ 1.
r The character sequence xj,i if it matches a word w ? U, with the constraint
i? 6 < j < i. The item xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected from the training data.
r The character sequence xi,k if it matches a word w ? U, with the constraint
i < k < i+ 6.
r The word bigram candidate [xj,i?1, xi,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
B represents the word bigram dictionary collected from the training data.
r The word bigram candidate [xj,i, xi+1,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
All feature templates are instantiated with values that occurred in training samples.
The extracted feature set is large, and there are 2.4? 107 features in total. Our evaluation
is based on a closed test, and we do not use extra resources. Following prior studies, the
evaluation metric for this task is the balanced F-score.
4.3 Phrase Chunking (Structured Classification)
In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs,
are identified. The phrase chunking data is extracted from the data of the CoNLL-2000
shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936
sentences, and the test set consists of 2,012 sentences. We use the feature templates
based on word n-grams and part-of-speech n-grams, and feature templates are shown
in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8? 105
features in total. State-of-the-art systems for this task include Kudo and Matsumoto
(2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al.
(2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior
studies, the evaluation metric for this task is the balanced F-score.
4.4 Sentiment Classification (Non-Structured Classification)
To demonstrate that the proposed method is not limited to structured classification, we
select a well-known sentiment classification task for evaluating the proposed method
on non-structured classification.
Table 3
Feature templates used for the phrase chunking task. wi, ti, and yi are defined as before.
Word-Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?1, ti, ti+1, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
574
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Generally, sentiment classification classifies user review text as a positive or neg-
ative opinion. This task (Blitzer, Dredze, and Pereira 2007) consists of four subtasks
based on user reviews from Amazon.com. Each subtask is a binary sentiment clas-
sification task based on a specific topic. We use the maximum entropy model for
classification. We use the same lexical features as those used in Blitzer, Dredze, and
Pereira (2007), and the total number of features is 9.4? 105. Following prior work, the
evaluation metric is binary classification accuracy.
4.5 Experimental Setting
As for training, we perform gradient descent with the proposed ADF training method.
To compare with existing literature, we choose four popular training methods, a rep-
resentative batch training method, and three representative on-line training methods.
The batch training method is the limited-memory BFGS (LBFGS) method (Nocedal and
Wright 1999), which is considered to be one of the best optimizers for log-linear models
like CRFs. The on-line training methods include the SGD training method, which we
introduced in Section 2.2, the structured perceptron (Perc) training method (Freund
and Schapire 1999; Collins 2002), and the averaged perceptron (Avg-Perc) training
method (Collins 2002). The structured perceptron method and averaged perceptron
method are non-probabilistic training methods that have very fast training speed due
to the avoidance of the computation on gradients (Sun, Matsuzaki, and Li 2013). All
training methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same set
of features.
We also compared the ADF method with the CW method (Dredze, Crammer, and
Pereira 2008) and the AROW method (Crammer, Kulesza, and Dredze 2009). The CW
and AROW methods are implemented based on the Confidence Weighted Learning
Library.2 Because the current implementation of the CW and AROW methods do not
utilize rich edge features, we removed the rich edge features in our systems to make
more fair comparisons. That is, we removed rich edge features in the CRF-ADF setting,
and this simplified method is denoted as ADF-noRich. The second-order stochastic
gradient descent training methods, including the SMD method (Vishwanathan et al.
2006) and the PSA method (Hsu et al. 2009), are not considered in our experiments
because we find those methods are quite slow when running on our data sets with high
dimensional features.
We find that the settings of q, ?, and ? in the ADF training method are not sensitive
among specific tasks and can be generally set. We simply set q = n/10 (n is the number
of training samples). It means that feature frequency information is updated 10 times
per iteration. Via cross-validation only on the training data of different tasks, we find
that the following setting is sufficient to produce adequate performance for most of
the real-world natural language processing tasks: ? around 0.995 and ? around 0.6.
This indicates that the feature frequency information has similar characteristics across
many different natural language processing tasks.
Thus, we simply use the following setting for all tasks: q = n/10, ? = 0.995, and
? = 0.6. This leaves c (the initial value of the learning rates) as the only hyper-parameter
that requires careful tuning. We perform automatic tuning for c based on the training
data via 4-fold cross-validation, testing with c = 0.005, 0.01, 0.05, 0.1, respectively, and
the optimal c is chosen based on the best accuracy of cross-validation. Via this automatic
2 http://webee.technion.ac.il/people/koby/code-index.html.
575
Computational Linguistics Volume 40, Number 3
tuning, we find it is proper to set c = 0.005, 0.1, 0.05, 0.005, for the Bio-NER, word
segmentation, phrase chunking, and sentiment classification tasks, respectively.
To reduce overfitting, we use an L2 Gaussian weight prior (Chen and Rosenfeld
1999) for the ADF, LBFGS, and SGD training methods. We vary the ? with different
values (e.g., 1.0, 2.0, and 5.0) for 4-fold cross validation on the training data of different
tasks, and finally set ? = 5.0 for all training methods in the Bio-NER task; ? = 5.0 for
all training methods in the word segmentation task; ? = 5.0, 1.0, 1.0 for ADF, SGD,
and LBFGS in the phrase chunking task; and ? = 1.0 for all training methods in the
sentiment classification task. Experiments are performed on a computer with an Intel(R)
Xeon(R) 2.0-GHz CPU.
4.6 Structured Classification Results
4.6.1 Comparisons Based on Empirical Convergence. First, we check the experimental re-
sults of different methods on their empirical convergence state. Because the perceptron
training method (Perc) does not achieve empirical convergence even with a very large
number of training passes, we simply report its results based on a large enough number
of training passes (e.g., 200 passes). Experimental results are shown in Table 4.
As we can see, the proposed ADF method is more accurate than other training
methods, either the on-line ones or the batch one. It is a bit surprising that the ADF
method performs even more accurately than the batch training method (LBFGS). We
notice that some previous work also found that on-line training methods could have
Table 4
Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the
number of passes are decided based on empirical convergence (with score deviation of adjacent
five passes less than 0.01). For the non-convergent case, we simply report the results based on a
large enough number of training passes. As we can see, the ADF method achieves the best
accuracy with the fastest convergence speed.
Bio-NER Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 67.69 70.20 68.92 400 152,811.34
SGD (on-line) 70.91 72.69 71.79 91 76,549.21
Perc (on-line) 65.37 66.95 66.15 200 20,436.69
Avg-Perc (on-line) 68.76 72.56 70.61 37 3,928.01
ADF (proposal) 71.71 72.80 72.25 35 27,490.24
Segmentation Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 97.46 96.86 97.16 102 13,550.68
SGD (on-line) 97.58 97.11 97.34 27 6,811.15
Perc (on-line) 96.99 96.03 96.50 200 8,382.606
Avg-Perc (on-line) 97.56 97.05 97.30 16 716.87
ADF (proposal) 97.67 97.31 97.49 15 4,260.08
Chunking Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 94.57 94.09 94.33 105 797.04
SGD (on-line) 94.48 94.04 94.26 56 903.88
Perc (on-line) 93.66 93.31 93.48 200 543.51
Avg-Perc (on-line) 94.34 94.04 94.19 12 33.45
ADF (proposal) 94.66 94.38 94.52 17 282.17
576
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
better performance than batch training methods such as LBFGS (Tsuruoka, Tsujii, and
Ananiadou 2009; Schaul, Zhang, and LeCun 2012). The ADF training method can
achieve better results probably because the feature-frequency?adaptive training schema
can produce more balanced training of features with diversified frequencies. Traditional
SGD training may over-train high frequency features and at the same time may have
insufficient training of low frequency features. The ADF training method can avoid
such problems. It will be interesting to perform further analysis in future work.
We also performed significance tests based on t-tests with a significance level of
0.05. Significance tests demonstrate that the ADF method is significantly more accurate
than the existing training methods in most of the comparisons, whether on-line or
batch. For the Bio-NER task, the differences between ADF and LBFGS, SGD, Perc,
and Avg-Perc are significant. For the word segmentation task, the differences between
ADF and LBFGS, SGD, Perc, and Avg-Perc are significant. For the phrase chunking
task, the differences between ADF and Perc and Avg-Perc are significant; the differences
between ADF and LBFGS and SGD are non-significant.
Moreover, as we can see, the proposed method achieves a convergence state with
the least number of training passes, and with the least wall-clock time. In general, the
ADF method is about one order of magnitude faster than the LBFGS batch training
method and several times faster than the existing on-line training methods.
4.6.2 Comparisons with State-of-the-Art Systems. The three tasks are well-known bench-
mark tasks with standard data sets. There is a large amount of published research on
those three tasks. We compare the proposed method with the state-of-the-art systems.
The comparisons are shown in Table 5.
As we can see, our system is competitive with the best systems for the Bio-NER,
word segmentation, and NP-chunking tasks. Many of the state-of-the-art systems use
extra resources (e.g., linguistic knowledge) or complicated systems (e.g., voting over
Table 5
Comparing our results with some representative state-of-the-art systems.
Bio-NER Method F-score
(Okanohara et al. 2006) Semi-Markov CRF + global features 71.5
(Hsu et al. 2009) CRF + PSA(1) training 69.4
(Tsuruoka, Tsujii, and Ananiadou 2009) CRF + SGD-L1 training 71.6
Our Method CRF + ADF training 72.3
Segmentation Method F-score
(Gao et al. 2007) Semi-Markov CRF 97.2
(Sun, Zhang, et al. 2009) Latent-variable CRF 97.3
(Sun 2010) Multiple segmenters + voting 96.9
Our Method CRF + ADF training 97.5
Chunking Method F-score
(Kudo and Matsumoto 2001) Combination of multiple SVM 94.2
(Vishwanathan et al. 2006) CRF + SMD training 93.6
(Sun et al. 2008) Latent-variable CRF 94.3
Our Method CRF + ADF training 94.5
577
Computational Linguistics Volume 40, Number 3
multiple models). Thus, it is impressive that our single model?based system without
extra resources achieves good performance. This indicates that the proposed ADF
training method can train model parameters with good generality on the test data.
4.6.3 Training Curves. To study the detailed training process and convergence speed, we
show the training curves in Figures 2?4. Figure 2 focuses on the comparisons between
the ADF method and the existing on-line training methods. As we can see, the ADF
method converges faster than other on-line training methods in terms of both training
passes and wall-clock time. The ADF method has roughly the same training speed per
pass compared with traditional SGD training.
Figure 3 (Top Row) focuses on comparing the ADF method with the CW method
(Dredze, Crammer, and Pereira 2008) and the AROW method (Crammer, Kulesza, and
Dredze 2009). Comparisons are based on similar features. As discussed before, the ADF-
noRich method is a simplified system, with rich edge features removed from the CRF-
ADF system. As we can see, the proposed ADF method, whether with or without rich
edge features, outperforms the CWandAROWmethods. Figure 3 (BottomRow) focuses
on the comparisons with different mini-batch (the training samples in each stochastic
update) sizes. Representative results with a mini-batch size of 10 are shown. In general,
we find larger mini-batch sizes will slow down the convergence speed. Results demon-
strate that, compared with the SGD training method, the ADF training method is less
sensitive to mini-batch sizes.
Figure 4 focuses on the comparisons between the ADF method and the batch
training method LBFGS. As we can see, the ADF method converges at least one order
0 20 40 60 80 10066
67
68
69
70
71
72
73
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 1 2 3 4 5 6x 104
66
67
68
69
70
71
72
73
Training Time (sec)
)
 
 
ADFSGDPerc
0 2,000 4,000 6,000 8,000 10,00096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
0 200 400 600 80092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
Figure 2
Comparisons among the ADF method and other on-line training methods. (Top Row)
Comparisons based on training passes. As we can see, the ADF method has the best accuracy
and with the fastest convergence speed based on training passes. (Bottom Row) Comparisons
based on wall-clock time.
578
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
0 20 40 60 80 10060
62
64
66
68
70
72
Bio?NER (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092
93
94
95
96
97
98Segmentation (ADF vs. CW/AROW)
Number of Passes
F?sco
re  (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092.5
93
93.5
94
94.5
95 Chunking (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10068
69
70
71
72
73 Bio?NER (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
0 20 40 60 80 10096.4
96.6
96.8
97
97.2
97.4
97.6
Segmentation (MiniBatch=10)
Number of Passes
F?sco
re  (%)
 
 
ADFSGD
0 20 40 60 80 10093.2
93.4
93.6
93.8
94
94.2
94.4
94.6
Chunking (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
Figure 3
(Top Row) Comparing ADF and ADF-noRich with CW and AROW methods. As we can see,
both the ADF and ADF-noRich methods work better than the CW and AROW methods.
(Bottom Row) Comparing different methods with mini-batch = 10 in the stochastic learning
setting.
magnitude faster than the LBFGS training in terms of both training passes and wall-
clock time. For the LBFGS training, we need to determine the LBFGSmemory parameter
m, which controls the number of prior gradients used to approximate the Hessian
information. A larger value of m will potentially lead to more accurate estimation
of the Hessian information, but at the same time will consume significantly more
memory. Roughly, the LBFGS training consumes m times more memory than the ADF
on-line training method. For most tasks, the default setting of m = 10 is reasonable. We
set m = 10 for the word segmentation and phrase chunking tasks, and m = 6 for the
Bio-NER task due to the shortage of memory for m > 6 cases in this task.
4.6.4 One-Pass Learning Results. Many real-world data sets can only observe the training
data in one pass. For example, some Web-based on-line data streams can only appear
once so that the model parameter learning should be finished in one-pass learning (see
Zinkevich et al. 2010). Hence, it is important to test the performance in the one-pass
learning scenario.
In the one-pass learning scenario, the feature frequency information is computed
?on the fly? during on-line training. As shown in Section 3.1, we only need to have
a real-valued vector v to record the cumulative feature frequency information, which
is updated when observing training instances one by one. Then, the learning rate
vector ? is updated based on the v only and there is no need to observe the training
instances again. This is the same algorithm introduced in Section 3.1 and no change is
required for the one-pass learning scenario. Figure 5 shows the comparisons between
the ADF method and baselines on one-pass learning. As we can see, the ADF method
579
Computational Linguistics Volume 40, Number 3
0 100 200 300 40066
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 100 200 300 40096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. batch)
Number of Passes
F?sco
re  (%)
 
 
ADFLBFGS
0 100 200 300 400
93.6
93.8
94
94.2
94.4
Chunking (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 5 10 15x 104
66
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Training Time (sec)
F?sco
re (%)
 
 
ADFLBFGS
0 1 2 3 4 5x 104
9696.2
96.496.6
96.897
97.297.4
97.6
Segmentation (ADF vs. batch)
Training Time (sec)
F?sco
re  (%)
 
 
ADFLBFGS
0 1,000 2,000 3,00092.5
93
93.5
94
94.5 Chunking (ADF vs. batch)
Training Time (sec)
)
 
 
ADFLBFGS
Figure 4
Comparisons between the ADF method and the batch training method LBFGS. (Top Row)
Comparisons based on training passes. As we can see, the ADF method converges much faster
than the LBFGS method, and with better accuracy on the convergence state. (Bottom Row)
Comparisons based on wall-clock time.
consistently outperforms the baselines. This also reflects the fast convergence speed of
the ADF training method.
4.7 Non-Structured Classification Results
In previous experiments, we showed that the proposed method outperforms existing
baselines on structured classification. Nevertheless, we want to show that the ADF
method also has good performance on non-structured classification. In addition, this
task is based on real-valued features instead of binary features.
Figure 5
Comparisons among different methods based on one-pass learning. As we can see, the ADF
method has the best accuracy on one-pass learning.
580
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 6
Results on sentiment classification (non-structured binary classification).
Accuracy Passes Train-Time (sec)
LBFGS (batch) 87.00 86 72.20
SGD (on-line) 87.13 44 55.88
Perc (on-line) 84.55 25 5.82
Avg-Perc (on-line) 85.04 46 12.22
ADF (proposal) 87.89 30 57.12
Experimental results of different training methods on the convergence state are
shown in Table 6. As we can see, the proposed method outperforms all of the on-line
and batch baselines in terms of binary classification accuracy. Here again we observe
that the ADF and SGD methods outperform the LBFGS baseline.
The training curves are shown in Figure 6. As we can see, the ADF method con-
verges quickly. Because this data set is relatively small and the feature dimension is
much smaller than previous tasks, we find the baseline training methods also have
fast convergence speed. The comparisons on one-pass learning are shown in Fig-
ure 7. Just as for the experiments for structured classification tasks, the ADF method
0 20 40 60 80 10082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Number of Passes
Accur
acy (%
)
 
 
ADFSGDPerc
0 20 40 60 8082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Training Time (sec)
Accur
acy (%
)
 
 
ADFSGDPerc
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Number of Passes
Accur
acy (%
)
 
 
ADFLBFGS
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Training Time (sec)
Accu
racy (%
)
 
 
ADFLBFGS
Figure 6
F-score curves on sentiment classification. (Top Row) Comparisons among the ADF method and
on-line training baselines, based on training passes and wall-clock time, respectively. (Bottom
Row) Comparisons between the ADF method and the batch training method LBFGS, based on
training passes and wall-clock time, respectively. As we can see, the ADF method outperforms
both the on-line training baselines and the batch training baseline, with better accuracy and
faster convergence speed.
581
Computational Linguistics Volume 40, Number 3
Figure 7
One-pass learning results on sentiment classification.
outperforms the baseline methods on one-pass learning, with more than 12.7% error
rate reduction.
5. Proofs
This section gives proofs of Theorems 1?4.
Proof of Theorem 1 Following Equation (5), the ADF update rule is F(wt) := wt+1 =
wt +? ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |F(wt+1)?wt+1|
= |wt+1 +? ? gt+1 ?wt+1|
= |? ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |?maxgt+1|
= |FSGD(wt+1)? FSGD(wt)|
(6)
where ai and bi are the ith elements of the vector ? and gt+1, respectively. FSGD is the
SGD update rule with the fixed learning rate ?max such that ?max := sup{?i where ?i ?
?}. In other words, for the SGD update rule FSGD, the fixed learning rate ?max is derived
from the ADF update rule. According to Lemma 1, the SGD update rule FSGD is a
contraction mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2,
given the condition that ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Hence, it goes to
|FSGD(wt+1)? FSGD(wt)| ? (1? ?max/?2)|wt+1 ?wt| (7)
Combining Equations (6) and (7), it goes to
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
Thus, according to the definition of dynamic contraction mapping, the ADF update rule
is a dynamic contraction mapping in Euclidean space with Lipschitz continuity degree
1? ?max/?2. ut
582
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Proof of Theorem 2 As presented in Equation (5), the ADF update rule is F(wt) :=
wt+1 = wt +?t ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |?t+1 ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |FSGD(wt+1)? FSGD(wt)|
(8)
where ai is the ith element of the vector ?t+1. bi and FSGD are the same as before.
Similar to the analysis of Theorem 1, the third step of Equation (8) is valid because ?max
is the maximum learning rate at the beginning and all learning rates are decreasing
when t is increasing. The proof can be easily derived following the same steps in the
proof of Theorem 1. To avoid redundancy, we do not repeat the derivation. ut
Proof of Theorem 3 Let M be the accumulative change of the ADF weight vectorwt:
Mt :=
?
t?=1,2,...,t
|wt?+1 ?wt? |
To prove the convergence of the ADF, we need to prove the sequence Mt converges as
t??. Following Theorem 2, we have the following formula for the ADF training:
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
where ?max is the maximum learning rate at the beginning. Let d0 := |w2 ?w1| and
q := 1? ?max/?2, then we have:
Mt =
?
t?=1,2,...,t
|wt?+1 ?wt? |
? d0 + d0q+ d0q2 + ? ? ?+ d0qt?1
= d0(1? qt)/(1? q)
(9)
When t??, d0(1? qt)/(1? q) goes to d0/(1? q) because q < 1. Hence, we have:
Mt ? d0/(1? q)
Thus, Mt is upper-bounded. Because we know that Mt is a monotonically increasing
function when t??, it follows that Mt converges when t??. This completes the
proof. ut
583
Computational Linguistics Volume 40, Number 3
Proof of Theorem 4 First, we have
eigen(Ct) =
t
?
m=1
(1??0?m?)
? exp
{
??0?
t
?
m=1
?m
}
Then, we have
0 ?
n
?
j=1
(1? aj) ?
n
?
j=1
e?aj = e?
?n
j=1 aj
This is because 1? aj ? e?aj given 0 ? aj < 1. Finally, because
?t
m=1 ?m ?
?
1?? when
t??, we have
eigen(Ct) ? exp
{
??0?
t
?
m=1
?m
}
? exp
{
??0??
1? ?
}
This completes the proof. ut
6. Conclusions
In this work we tried to simultaneously improve the training speed andmodel accuracy
of natural language processing systems. We proposed the ADF on-line training method,
based on the core idea that high frequency features should result in a learning rate that
decays faster. We demonstrated that the ADF on-line training method is convergent
and has good theoretical properties. Based on empirical experiments, we can state the
following conclusions. First, the ADF method achieved the major target of this work:
faster training speed and higher accuracy at the same time. Second, the ADF method
was robust: It had good performance on several structured and non-structured classifi-
cation tasks with very different characteristics. Third, the ADF method worked well on
both binary features and real-valued features. Fourth, the ADF method outperformed
existing methods in a one-pass learning setting. Finally, our method achieved state-
of-the-art performance on several well-known benchmark tasks. To the best of our
knowledge, our simple method achieved a much better F-score than the existing best
reports on the biomedical named entity recognition task.
Acknowledgments
This work was supported by the National
Natural Science Foundation of China
(no. 61300063, no. 61370117), the Doctoral
Fund of Ministry of Education of China
(no. 20130001120004), a Hong Kong
Polytechnic University internal grant
(4-ZZD5), a Hong Kong RGC Project
(no. PolyU 5230/08E), the National High
Technology Research and Development
Program of China (863 Program,
no. 2012AA011101), and the Major National
Social Science Fund of China (no. 12&ZD227).
This work is a substantial extension of the
conference version presented at ACL 2012
(Sun, Wang, and Li 2012).
584
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
References
Berger, Adam L., Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 440?447, Prague.
Bottou, Le?on. 1998. Online algorithms
and stochastic approximations. In
D. Saad, editor. Online Learning and Neural
Networks. Cambridge University Press,
pages 9?42.
Chen, Stanley F. and Ronald Rosenfeld.
1999. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Carnegie
Mellon University.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of
EMNLP?02, pages 1?8, Philadelphia, PA.
Crammer, Koby, Alex Kulesza, and Mark
Dredze. 2009. Adaptive regularization of
weight vectors. In NIPS?09, pages 414?422,
Vancouver.
Dredze, Mark, Koby Crammer, and
Fernando Pereira. 2008. Confidence-
weighted linear classification. In
Proceedings of ICML?08, pages 264?271,
Helsinki.
Duchi, John, Elad Hazan, and Yoram Singer.
2010. Adaptive subgradient methods
for online learning and stochastic
optimization. Journal of Machine
Learning Research, 12:2,121?2,159.
Finkel, Jenny, Shipra Dingare, Huy Nguyen,
Malvina Nissim, Christopher Manning,
and Gail Sinclair. 2004. Exploiting context
for biomedical entity recognition: From
syntax to the Web. In Proceedings of
BioNLP?04, pages 91?94, Geneva.
Freund, Yoav and Robert Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Gao, Jianfeng, Galen Andrew, Mark Johnson,
and Kristina Toutanova. 2007. A comparative
study of parameter estimation methods for
statistical natural language processing. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics
(ACL?07), pages 824?831, Prague.
Hsu, Chun-Nan, Han-Shen Huang, Yu-Ming
Chang, and Yuh-Jye Lee. 2009. Periodic
step-size adaptation in second-order
gradient descent for single-pass on-line
structured learning. Machine Learning,
77(2-3):195?224.
Jacobs, Robert A. 1988. Increased rates of
convergence through learning rate
adaptation. Neural Networks, 1(4):295?307.
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa
Tsuruoka, and Yuka Tateisi. 2004.
Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of
BioNLP?04, pages 70?75, Geneva.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of NAACL?01, pages 1?8,
Pittsburgh, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence
data. In ICML?01, pages 282?289,
Williamstown, MA.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Flexible text
segmentation with structured multilabel
classification. In Proceedings of HLT/
EMNLP?05, pages 987?994, Vancouver.
McMahan, H. Brendan and Matthew J.
Streeter. 2010. Adaptive bound
optimization for online convex
optimization. In Proceedings of COLT?10,
pages 244?256, Haifa.
Murata, Noboru. 1998. A statistical study
of on-line learning. In D. Saad, editor.
Online Learning in Neural Networks.
Cambridge University Press, pages 63?92.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical optimization. Springer.
Okanohara, Daisuke, Yusuke Miyao,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2006. Improving the scalability of
semi-Markov conditional random
fields for named entity recognition.
In Proceedings of COLING-ACL?06,
pages 465?472, Sydney.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
1996, pages 133?142, Pennsylvania.
Sang, Erik Tjong Kim and Sabine Buchholz.
2000. Introduction to the CoNLL-2000
shared task: Chunking. In Proceedings
of CoNLL?00, pages 127?132, Lisbon.
Schaul, Tom, Sixin Zhang, and Yann LeCun.
2012. No more pesky learning rates. CoRR,
abs/1206.1106.
585
Computational Linguistics Volume 40, Number 3
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random
fields and rich feature sets. In Proceedings
of BioNLP?04, pages 104?107, Geneva.
Shalev-Shwartz, Shai, Yoram Singer, and
Nathan Srebro. 2007. Pegasos: Primal
estimated sub-gradient solver for SVM.
In Proceedings of ICML?07, pages 807?814,
Corvallis, OR.
Sperduti, Alessandro and Antonina Starita.
1993. Speed up learning and network
optimization with extended back
propagation. Neural Networks, 6(3):365?383.
Sun, Weiwei. 2010. Word-based and
character-based word segmentation
models: Comparison and combination.
In COLING?10 (Posters), pages 1,211?1,219,
Beijing.
Sun, Xu, Takuya Matsuzaki, and Wenjie Li.
2013. Latent structured perceptrons
for large-scale learning with hidden
information. IEEE Transactions on
Knowledge and Data Engineering,
25(9):2,063?2,075.
Sun, Xu, Takuya Matsuzaki, Daisuke
Okanohara, and Jun?ichi Tsujii. 2009.
Latent variable perceptron algorithm for
structured classification. In Proceedings
of the 21st International Joint Conference
on Artificial Intelligence (IJCAI 2009),
pages 1,236?1,242, Pasadena, CA.
Sun, Xu, Louis-Philippe Morency, Daisuke
Okanohara, and Jun?ichi Tsujii. 2008.
Modeling latent-dynamic in shallow
parsing: A latent conditional model with
improved inference. In Proceedings of
COLING?08, pages 841?848, Manchester.
Sun, Xu, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-
adaptive learning rates for Chinese word
segmentation and new word detection.
In Proceedings of ACL?12, pages 253?262,
Jeju Island.
Sun, Xu, Yaozhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2009. A discriminative
latent variable Chinese segmenter with
hybrid word/character information.
In Proceedings of NAACL-HLT?09,
pages 56?64, Boulder, CO.
Sun, Xu, Yao Zhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2013. Probabilistic Chinese
word segmentation with non-local
information and stochastic training.
Information Processing & Management,
49(3):626?636.
Tseng, Huihsin, Pichuan Chang, Galen
Andrew, Daniel Jurafsky, and Christopher
Manning. 2005. A conditional random
field word segmenter for SIGHAN bakeoff
2005. In Proceedings of the Fourth SIGHAN
Workshop, pages 168?171, Jeju Island.
Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and
Sophia Ananiadou. 2009. Stochastic
gradient descent training for
l1-regularized log-linear models with
cumulative penalty. In Proceedings of
ACL?09, pages 477?485, Suntec.
Vishwanathan, S. V. N., Nicol N.
Schraudolph, Mark W. Schmidt, and
Kevin P. Murphy. 2006. Accelerated
training of conditional random
fields with stochastic meta-descent.
In Proceedings of ICML?06, pages 969?976,
Pittsburgh, PA.
Zhang, Ruiqiang, Genichiro Kikui, and
Eiichiro Sumita. 2006. Subword-based
tagging by conditional random fields for
Chinese word segmentation. In Proceedings
of the Human Language Technology
Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196,
New York City.
Zhang, Yue and Stephen Clark. 2007.
Chinese segmentation with a word-based
perceptron algorithm. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 840?847,
Prague.
Zhao, Hai, Changning Huang, Mu Li,
and Bao-Liang Lu. 2010. A unified
character-based tagging framework for
Chinese word segmentation. ACM
Transactions on Asian Language Information
Processing, 9(2): Article 5.
Zhao, Hai and Chunyu Kit. 2011. Integrating
unsupervised and supervised word
segmentation: The role of goodness
measures. Information Sciences,
181(1):163?183.
Zinkevich, Martin, Markus Weimer,
Alexander J. Smola, and Lihong Li.
2010. Parallelized stochastic gradient
descent. In Proceedings of NIPS?10,
pages 2,595?2,603, Vancouver.
586
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?262,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast Online Training with Frequency-Adaptive Learning Rates for Chinese
Word Segmentation and New Word Detection
Xu Sun?, Houfeng Wang?, Wenjie Li?
?Department of Computing, The Hong Kong Polytechnic University
?Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, China
{csxsun, cswjli}@comp.polyu.edu.hk wanghf@pku.edu.cn
Abstract
We present a joint model for Chinese
word segmentation and new word detection.
We present high dimensional new features,
including word-based features and enriched
edge (label-transition) features, for the joint
modeling. As we know, training a word
segmentation system on large-scale datasets
is already costly. In our case, adding high
dimensional new features will further slow
down the training speed. To solve this
problem, we propose a new training method,
adaptive online gradient descent based on
feature frequency information, for very fast
online training of the parameters, even given
large-scale datasets with high dimensional
features. Compared with existing training
methods, our training method is an order
magnitude faster in terms of training time, and
can achieve equal or even higher accuracies.
The proposed fast training method is a general
purpose optimization method, and it is not
limited in the specific task discussed in this
paper.
1 Introduction
Since Chinese sentences are written as continuous
sequences of characters, segmenting a character
sequence into words is normally the first step
in the pipeline of Chinese text processing. The
major problem of Chinese word segmentation
is the ambiguity. Chinese character sequences
are normally ambiguous, and new words (out-
of-vocabulary words) are a major source of the
ambiguity. A typical category of new words
is named entities, including organization names,
person names, location names, and so on.
In this paper, we present high dimensional
new features, including word-based features and
enriched edge (label-transition) features, for the
joint modeling of Chinese word segmentation
(CWS) and new word detection (NWD). While most
of the state-of-the-art CWS systems used semi-
Markov conditional random fields or latent variable
conditional random fields, we simply use a single
first-order conditional random fields (CRFs) for
the joint modeling. The semi-Markov CRFs and
latent variable CRFs relax the Markov assumption
of CRFs to express more complicated dependencies,
and therefore to achieve higher disambiguation
power. Alternatively, our plan is not to relax
Markov assumption of CRFs, but to exploit more
complicated dependencies via using refined high-
dimensional features. The advantage of our choice
is the simplicity of our model. As a result, our
CWS model can be more efficient compared with
the heavier systems, and with similar or even higher
accuracy because of using refined features.
As we know, training a word segmentation system
on large-scale datasets is already costly. In our
case, adding high dimensional new features will
further slow down the training speed. To solve this
challenging problem, we propose a new training
method, adaptive online gradient descent based on
feature frequency information (ADF), for very fast
word segmentation with new word detection, even
given large-scale datasets with high dimensional
features. In the proposed training method, we try
to use more refined learning rates. Instead of using
a single learning rate (a scalar) for all weights,
we extend the learning rate scalar to a learning
rate vector based on feature frequency information
in the updating. By doing so, each weight has
253
its own learning rate adapted on feature frequency
information. We will show that this can significantly
improve the convergence speed of online learning.
We approximate the learning rate vector based
on feature frequency information in the updating
process. Our proposal is based on the intuition
that a feature with higher frequency in the training
process should be with a learning rate that is decayed
faster. Based on this intuition, we will show the
formalized training algorithm later. We will show in
experiments that our solution is an order magnitude
faster compared with exiting learning methods, and
can achieve equal or even higher accuracies.
The contribution of this work is as follows:
? We propose a general purpose fast online
training method, ADF. The proposed training
method requires only a few passes to complete
the training.
? We propose a joint model for Chinese word
segmentation and new word detection.
? Compared with prior work, our system
achieves better accuracies on both word
segmentation and new word detection.
2 Related Work
First, we review related work on word segmentation
and new word detection. Then, we review popular
online training methods, in particular stochastic
gradient descent (SGD).
2.1 Word Segmentation and New Word
Detection
Conventional approaches to Chinese word
segmentation treat the problem as a sequential
labeling task (Xue, 2003; Peng et al, 2004; Tseng
et al, 2005; Asahara et al, 2005; Zhao et al,
2010). To achieve high accuracy, most of the state-
of-the-art systems are heavy probabilistic systems
using semi-Markov assumptions or latent variables
(Andrew, 2006; Sun et al, 2009b). For example,
one of the state-of-the-art CWS system is the latent
variable conditional random field (Sun et al, 2008;
Sun and Tsujii, 2009) system presented in Sun et al
(2009b). It is a heavy probabilistic model and it is
slow in training. A few other state-of-the-art CWS
systems are using semi-Markov perceptron methods
or voting systems based on multiple semi-Markov
perceptron segmenters (Zhang and Clark, 2007;
Sun, 2010). Those semi-Markov perceptron systems
are moderately faster than the heavy probabilistic
systems using semi-Markov conditional random
fields or latent variable conditional random fields.
However, a disadvantage of the perceptron style
systems is that they can not provide probabilistic
information.
On the other hand, new word detection is also one
of the important problems in Chinese information
processing. Many statistical approaches have been
proposed (J. Nie and Jin, 1995; Chen and Bai, 1998;
Wu and Jiang, 2000; Peng et al, 2004; Chen and
Ma, 2002; Zhou, 2005; Goh et al, 2003; Fu and
Luke, 2004; Wu et al, 2011). New word detection
is normally considered as a separate process from
segmentation. There were studies trying to solve this
problem jointly with CWS. However, the current
studies are limited. Integrating the two tasks would
benefit both segmentation and new word detection.
Our method provides a convenient framework for
doing this. Our new word detection is not a stand-
alone process, but an integral part of segmentation.
2.2 Online Training
The most representative online training method
is the SGD method. The SGD uses a small
randomly-selected subset of the training samples to
approximate the gradient of an objective function.
The number of training samples used for this
approximation is called the batch size. By using a
smaller batch size, one can update the parameters
more frequently and speed up the convergence. The
extreme case is a batch size of 1, and it gives the
maximum frequency of updates, which we adopt in
this work. Then, the model parameters are updated
in such a way:
wt+1 = wt + ?t?wtLstoch(z i,wt), (1)
where t is the update counter, ?t is the learning rate,
and Lstoch(z i,wt) is the stochastic loss function
based on a training sample z i.
There were accelerated versions of SGD,
including stochastic meta descent (Vishwanathan
et al, 2006) and periodic step-size adaptation
online learning (Hsu et al, 2009). Compared with
those two methods, our proposal is fundamentally
254
different. Those two methods are using 2nd-order
gradient (Hessian) information for accelerated
training, while our accelerated training method
does not need such 2nd-order gradient information,
which is costly and complicated. Our ADF training
method is based on feature frequency adaptation,
and there is no prior work on using feature frequency
information for accelerating online training.
Other online training methods includes averaged
SGD with feedback (Sun et al, 2010; Sun et al,
2011), latent variable perceptron training (Sun et al,
2009a), and so on. Those methods are less related to
this paper.
3 System Architecture
3.1 A Joint Model Based on CRFs
First, we briefly review CRFs. CRFs are proposed
as a method for structured classification by solving
?the label bias problem? (Lafferty et al, 2001).
Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a
global feature vector f , the probability of a label
sequence y conditioned on the observation sequence
x is modeled as follows (Lafferty et al, 2001):
P (y|x,w) =
exp
{
w?f (y,x)
}
?
?y? exp
{
w?f (y? ,x)
} , (2)
wherew is a parameter vector.
Given a training set consisting of n labeled
sequences, z i = (xi, y i), for i = 1 . . . n, parameter
estimation is performed by maximizing the objective
function,
L(w) =
n
?
i=1
logP (y i|xi,w)?R(w). (3)
The first term of this equation represents a
conditional log-likelihood of a training data. The
second term is a regularizer for reducing overfitting.
We employed an L2 prior, R(w) = ||w||
2
2?2 . In what
follows, we denote the conditional log-likelihood of
each sample logP (y i|xi,w) as ?(z i,w). The final
objective function is as follows:
L(w) =
n
?
i=1
?(z i,w)?
||w||2
2?2 . (4)
Since no word list can be complete, new word
identification is an important task in Chinese NLP.
New words in input text are often incorrectly
segmented into single-character or other very short
words (Chen and Bai, 1998). This phenomenon
will also undermine the performance of Chinese
word segmentation. We consider here new word
detection as an integral part of segmentation,
aiming to improve both segmentation and new word
detection: detected new words are added to the
word list lexicon in order to improve segmentation.
Based on our CRF word segmentation system,
we can compute a probability for each segment.
When we find some word segments are of reliable
probabilities yet they are not in the existing word
list, we then treat those ?confident? word segments
as new words and add them into the existing word
list. Based on preliminary experiments, we treat
a word segment as a new word if its probability
is larger than 0.5. Newly detected words are re-
incorporated into word segmentation for improving
segmentation accuracies.
3.2 New Features
Here, we will describe high dimensional new
features for the system.
3.2.1 Word-based Features
There are two ideas in deriving the refined
features. The first idea is to exploit word features
for node features of CRFs. Note that, although our
model is a Markov CRFmodel, we can still use word
features to learn word information in the training
data. To derive word features, first of all, our system
automatically collect a list of word unigrams and
bigrams from the training data. To avoid overfitting,
we only collect the word unigrams and bigrams
whose frequency is larger than 2 in the training set.
This list of word unigrams and bigrams are then used
as a unigram-dictionary and a bigram-dictionary to
generate word-based unigram and bigram features.
The word-based features are indicator functions that
fire when the local character sequence matches a
word unigram or bigram occurred in the training
data. The word-based feature templates derived for
the label yi are as follows:
? unigram1(x, yi) ? [xj,i, yi], if the
character sequence xj,i matches a word w ? U,
255
with the constraint i ? 6 < j < i. The item
xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected
from the training data.
? unigram2(x, yi) ? [xi,k, yi], if the
character sequence xi,k matches a wordw ? U,
with the constraint i < k < i + 6.
? bigram1(x, yi) ? [xj,i?1, xi,k, yi], if
the word bigram candidate [xj,i?1, xi,k] hits
a word bigram [wi, wj ] ? B, and satisfies
the aforementioned constraints on j and k. B
represents the word bigram dictionary collected
from the training data.
? bigram2(x, yi) ? [xj,i, xi+1,k, yi], if
the word bigram candidate [xj,i, xi+1,k] hits a
word bigram [wi, wj ] ? B, and satisfies the
aforementioned constraints on j and k.
We also employ the traditional character-based
features. For each label yi, we use the feature
templates as follows:
? Character unigrams locating at positions i? 2,
i? 1, i, i + 1 and i + 2
? Character bigrams locating at positions i ?
2, i? 1, i and i + 1
? Whether xj and xj+1 are identical, for j = i?
2, . . . , i + 1
? Whether xj and xj+2 are identical, for j = i?
3, . . . , i + 1
The latter two feature templates are designed
to detect character or word reduplication, a
morphological phenomenon that can influence word
segmentation in Chinese.
3.2.2 High Dimensional Edge Features
The node features discussed above are based on
a single label yi. CRFs also have edge features
that are based on label transitions. The second idea
is to incorporate local observation information of
x in edge features. For traditional implementation
of CRF systems (e.g., the HCRF package), usually
the edges features contain only the information
of yi?1 and yi, and without the information of
the observation sequence (i.e., x). The major
reason for this simple realization of edge features
in traditional CRF implementation is for reducing
the dimension of features. Otherwise, there can
be an explosion of edge features in some tasks.
For example, in part-of-speech tagging tasks, there
can be more than 40 labels and more than 1,600
types of label transitions. Therefore, incorporating
local observation information into the edge feature
will result in an explosion of edge features, which
is 1,600 times larger than the number of feature
templates.
Fortunately, for our task, the label set is quite
small, Y = {B,I,E}1. There are only nine possible
label transitions: T = Y ? Y and |T| = 9.2 As
a result, the feature dimension will have nine times
increase over the feature templates, if we incorporate
local observation information of x into the edge
features. In this way, we can effectively combine
observation information of x with label transitions
yi?1yi. We simply used the same templates of
node features for deriving the new edge features.
We found adding new edge features significantly
improves the disambiguation power of our model.
4 Adaptive Online Gradient Descent based
on Feature Frequency Information
As we will show in experiments, the training of the
CRF model with high-dimensional new features is
quite expensive, and the existing training method is
not good enough. To solve this issue, we propose a
fast online training method: adaptive online gradient
descent based on feature frequency information
(ADF). The proposed method is easy to implement.
For high convergence speed of online learning, we
try to use more refined learning rates than the SGD
training. Instead of using a single learning rate (a
scalar) for all weights, we extend the learning rate
scalar to a learning rate vector, which has the same
dimension of the weight vector w. The learning
rate vector is automatically adapted based on feature
frequency information. By doing so, each weight
1B means beginning of a word, I means inside a word, and
E means end of a word. The B,I,E labels have been widely
used in previous work of Chinese word segmentation (Sun et
al., 2009b).
2The operator ? means a Cartesian product between two
sets.
256
ADF learning algorithm
1: procedure ADF(q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample z i at random
5: . v ? UPDATE(v , z i)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATE(? , v)
8: . . v ? 0
9: . g ? ?wLstoch(z i,w)
10: . w ? w + ? ? g
11: . t? t + 1
12: returnw
13:
14: procedure UPDATE(v , z i)
15: for k ? features used in sample z i
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATE(? , v)
20: for k ? all features
21: . u? vk/q
22: . ? ? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1: The proposed ADF online learning algorithm.
q, c, ?, and ? are hyper-parameters. q is an integer
representing window size. c is for initializing the learning
rates. ? and ? are the upper and lower bounds of a scalar,
with 0 < ? < ? < 1.
has its own learning rate, and we will show that this
can significantly improve the convergence speed of
online learning.
In our proposed online learning method, the
update formula is as follows:
wt+1 = wt + ? t ? gt. (5)
The update term gt is the gradient term of a
randomly sampled instance:
gt = ?wtLstoch(z i,wt) = ?wt
{
?(z i,wt)?
||wt||2
2n?2
}
.
In addition, ? t ? Rf+ is a positive vector-
valued learning rate and ? denotes component-wise
(Hadamard) product of two vectors.
We learn the learning rate vector ? t based
on feature frequency information in the updating
process. Our proposal is based on the intuition that a
feature with higher frequency in the training process
should be with a learning rate that decays faster. In
other words, we assume a high frequency feature
observed in the training process should have a small
learning rate, and a low frequency feature should
have a relatively larger learning rate in the training.
Our assumption is based on the intuition that a
weight with higher frequency is more adequately
trained, hence smaller learning rate is preferable for
fast convergence.
Given a window size q (number of samples in a
window), we use a vector v to record the feature
frequency. The k?th entry vk corresponds to the
frequency of the feature k in this window. Given
a feature k, we use u to record the normalized
frequency:
u = vk/q.
For each feature, an adaptation factor ? is calculated
based on the normalized frequency information, as
follows:
? = ?? u(?? ?),
where ? and ? are the upper and lower bounds of
a scalar, with 0 < ? < ? < 1. As we can see,
a feature with higher frequency corresponds to a
smaller scalar via linear approximation. Finally, the
learning rate is updated as follows:
?k ? ??k.
With this setting, different features will correspond
to different adaptation factors based on feature
frequency information. Our ADF algorithm is
summarized in Figure 1.
The ADF training method is efficient, because
the additional computation (compared with SGD) is
only the derivation of the learning rates, which is
simple and efficient. As we know, the regularization
of SGD can perform efficiently via the optimization
based on sparse features (Shalev-Shwartz et al,
2007). Similarly, the derivation of ? t can also
perform efficiently via the optimization based on
sparse features.
4.1 Convergence Analysis
Prior work on convergence analysis of existing
online learning algorithms (Murata, 1998; Hsu et
257
Data Method Passes Train-Time (sec) NWD Rec Pre Rec CWS F-score
MSR Baseline 50 4.7e3 72.6 96.3 95.9 96.1
+ New features 50 1.2e4 75.3 97.2 97.0 97.1
+ New word detection 50 1.2e4 78.2 97.5 96.9 97.2
+ ADF training 10 2.3e3 77.5 97.6 97.2 97.4
CU Baseline 50 2.9e3 68.5 94.0 93.9 93.9
+ New features 50 7.5e3 68.0 94.4 94.5 94.4
+ New word detection 50 7.5e3 68.8 94.8 94.5 94.7
+ ADF training 10 1.5e3 68.8 94.8 94.7 94.8
PKU Baseline 50 2.2e3 77.2 95.0 94.0 94.5
+ New features 50 5.2e3 78.4 95.5 94.9 95.2
+ New word detection 50 5.2e3 79.1 95.8 94.9 95.3
+ ADF training 10 1.2e3 78.4 95.8 94.9 95.4
Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge
features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is
decided by empirical convergence of the training methods.
#W.T. #Word #C.T. #Char
MSR 8.8? 104 2.4? 106 5? 103 4.1? 106
CU 6.9? 104 1.5? 106 5? 103 2.4? 106
PKU 5.5? 104 1.1? 106 5? 103 1.8? 106
Table 1: Details of the datasets. W.T. represents word
types; C.T. represents character types.
al., 2009) can be extended to the proposed ADF
training method. We can show that the proposed
ADF learning algorithm has reasonable convergence
properties.
When we have the smallest learning rate ? t+1 =
?? t, the expectation of the obtainedwt is
E(wt) = w? +
t
?
m=1
(I ? ?0?mH (w?))(w0 ?w?),
wherew? is the optimal weight vector, andH is the
Hessian matrix of the objective function. The rate of
convergence is governed by the largest eigenvalue of
the functionC t =
?t
m=1(I ? ?0?mH (w?)). Then,
we can derive a bound of rate of convergence.
Theorem 1 Assume ? is the largest eigenvalue of
the function C t =
?t
m=1(I ? ?0?mH (w?)). For
the proposed ADF training, its convergence rate is
bounded by ?, and we have
? ? exp
{?0??
? ? 1
}
,
where ? is the minimum eigenvalue ofH (w?).
5 Experiments
5.1 Data and Metrics
We used benchmark datasets provided by the second
International Chinese Word Segmentation Bakeoff
to test our proposals. The datasets are from
Microsoft Research Asia (MSR), City University
of Hongkong (CU), and Peking University (PKU).
Details of the corpora are listed in Table 1. We
did not use any extra resources such as common
surnames, parts-of-speech, and semantics.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the
decoder), precision (P , the percentage of words in
the decoder output that are segmented correctly),
balanced F-score defined by 2PR/(P + R), and
recall of new word detection (NWD recall). For
more detailed information on the corpora, refer to
Emerson (2005).
5.2 Features, Training, and Tuning
We employed the feature templates defined in
Section 3.2. The feature sets are huge. There are
2.4 ? 107 features for the MSR data, 4.1 ? 107
features for the CU data, and 4.7 ? 107 features for
the PKU data. To generate word-based features, we
extracted high-frequency word-based unigram and
bigram lists from the training data.
As for training, we performed gradient descent
258
0 10 20 30 40 5095
95.5
96
96.5
97
97.5
MSR
Number of Passes
F?
sco
re
 
 
ADF
SGD
LBFGS (batch)
0 10 20 30 40 5092
92.5
93
93.5
94
94.5
95
CU
Number of Passes
F?
sco
re
0 10 20 30 40 5094
94.5
95
95.5
PKU
Number of Passes
F?
sco
re
0 2000 4000 600095
95.5
96
96.5
97
97.5
MSR
Training time (sec)
F?
sco
re
 
 
ADF
SGD
LBFGS (batch)
0 1000 2000 3000 400092
92.5
93
93.5
94
94.5
95
CU
Training time (sec)
F?
sco
re
0 1000 2000 3000 400094
94.5
95
95.5
PKU
Training time (sec)
F?
sco
re
Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.
with our proposed training method. To compare
with existing methods, we chose two popular
training methods, a batch training one and an
online training one. The batch training method
is the Limited-Memory BFGS (LBFGS) method
(Nocedal and Wright, 1999). The online baseline
training method is the SGD method, which we have
introduced in Section 2.2.
For the ADF training method, we need to tune the
hyper-parameters q, c, ?, and ?. Based on automatic
tuning within the training data (validation in the
training data), we found it is proper to set q = n/10
(n is the number of training samples), c = 0.1,
? = 0.995, and ? = 0.6. To reduce overfitting,
we employed an L2 Gaussian weight prior (Chen
and Rosenfeld, 1999) for all training methods. We
varied the ? with different values (e.g., 1.0, 2.0, and
5.0), and finally set the value to 1.0 for all training
methods.
5.3 Results and Discussion
First, we performed incremental evaluation in this
order: Baseline (word segmentation model with
SGD training); Baseline + New features; Baseline
+ New features + New word detection; Baseline +
New features + New word detection + ADF training
(replacing SGD training). The results are shown in
Table 2.
As we can see, the new features improved
performance on both word segmentation and new
word detection. However, we also noticed that
the training cost became more expensive via
adding high dimensional new features. Adding
new word detection function further improved the
segmentation quality and the new word recognition
recall. Finally, by using the ADF training method,
the training speed is much faster than the SGD
training method. The ADF method can achieve
empirical optimum in only a few passes, yet
with better segmentation accuracies than the SGD
training with 50 passes.
To get more details of the proposed training
method, we compared it with SGD and LBFGS
training methods based on an identical platform,
by varying the number of passes. The comparison
was based on the same platform: Baseline + New
features + New word detection. The F-score curves
of the training methods are shown in Figure 2.
Impressively, the ADF training method reached
empirical convergence in only a few passes, while
the SGD and LBFGS training converged much
slower, requiring more than 50 passes. The ADF
training is about an order magnitude faster than
the SGD online training and more than an order
magnitude faster than the LBFGS batch training.
Finally, we compared our method with the state-
259
Data Method Prob. Pre Rec F-score
MSR Best05 (Tseng et al, 2005)
?
96.2 96.6 96.4
CRF + rule-system (Zhang et al, 2006)
?
97.2 96.9 97.1
Semi-Markov perceptron (Zhang and Clark, 2007) ? N/A N/A 97.2
Semi-Markov CRF (Gao et al, 2007)
?
N/A N/A 97.2
Latent-variable CRF (Sun et al, 2009b)
?
97.3 97.3 97.3
Our method (A Single CRF)
?
97.6 97.2 97.4
CU Best05 (Tseng et al, 2005)
?
94.1 94.6 94.3
CRF + rule-system (Zhang et al, 2006)
?
95.2 94.9 95.1
Semi-perceptron (Zhang and Clark, 2007) ? N/A N/A 95.1
Latent-variable CRF (Sun et al, 2009b)
?
94.7 94.4 94.6
Our method (A Single CRF)
?
94.8 94.7 94.8
PKU Best05 (Chen et al, 2005) N/A 95.3 94.6 95.0
CRF + rule-system (Zhang et al, 2006)
?
94.7 95.5 95.1
semi-perceptron (Zhang and Clark, 2007) ? N/A N/A 94.5
Latent-variable CRF (Sun et al, 2009b)
?
95.6 94.8 95.2
Our method (A Single CRF)
?
95.8 94.9 95.4
Table 3: Comparing our method with the state-of-the-art CWS systems.
of-the-art systems reported in the previous papers.
The statistics are listed in Table 3. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; CRF + rule-system represents confidence-
based combination of CRF and rule-based models,
presented in Zhang et al (2006). Prob. indicates
whether or not the system can provide probabilistic
information. As we can see, our method achieved
similar or even higher F-scores, compared with the
best systems reported in previous papers. Note that,
our system is a single Markov model, while most of
the state-of-the-art systems are complicated heavy
systems, with model-combinations (e.g., voting of
multiple segmenters), semi-Markov relaxations, or
latent-variables.
6 Conclusions and Future Work
In this paper, we presented a joint model for
Chinese word segmentation and newword detection.
We presented new features, including word-based
features and enriched edge features, for the joint
modeling. We showed that the new features can
improve the performance on the two tasks.
On the other hand, the training of the model,
especially with high-dimensional new features,
became quite expensive. To solve this problem,
we proposed a new training method, ADF training,
for very fast training of CRFs, even given large-
scale datasets with high dimensional features. We
performed experiments and showed that our new
training method is an order magnitude faster than
existing optimization methods. Our final system can
learn highly accurate models with only a few passes
in training. The proposed fast learning method
is a general algorithm that is not limited in this
specific task. As future work, we plan to apply
this fast learning method on other large-scale natural
language processing tasks.
Acknowledgments
We thank Yaozhong Zhang and Weiwei Sun
for helpful discussions on word segmentation
techniques. The work described in this paper was
supported by a Hong Kong RGC Project (No. PolyU
5230/08E), National High Technology Research and
Development Program of China (863 Program) (No.
2012AA011101), and National Natural Science
Foundation of China (No.91024009, No.60973053).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
260
In Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of
machine learning methods for optimum chinese word
segmentation. In Proceedings of The Fourth SIGHAN
Workshop, pages 134?137.
K.J. Chen and M.H. Bai. 1998. Unknown word
detection for chinese by a corpus-based learning
method. Computational Linguistics and Chinese
Language Processing, 3(1):27?44.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word
extraction for chinese documents. In Proceedings of
COLING?02.
Stanley F. Chen and Ronald Rosenfeld. 1999. A
gaussian prior for smoothing maximum entropy
models. Technical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word
segmentation. In Proceedings of the fourth SIGHAN
workshop, pages 138?141.
Thomas Emerson. 2005. The second international
chinese word segmentation bakeoff. In Proceedings
of the fourth SIGHAN workshop, pages 123?133.
Guohong Fu and Kang-Kwong Luke. 2004. Chinese
unknown word identification using class-based lm. In
Proceedings of IJCNLP?04, volume 3248 of Lecture
Notes in Computer Science, pages 704?713. Springer.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL?07), pages 824?831.
Chooi-Ling Goh, Masayuki Asahara, and Yuji
Matsumoto. 2003. Chinese unknown word
identification using character-based tagging and
chunking. In Kotaro Funakoshi, Sandra Kbler, and
Jahna Otterbacher, editors, Proceedings of ACL
(Companion)?03, pages 197?200.
Chun-Nan Hsu, Han-Shen Huang, Yu-Ming Chang, and
Yuh-Jye Lee. 2009. Periodic step-size adaptation in
second-order gradient descent for single-pass on-line
structured learning. Machine Learning, 77(2-3):195?
224.
M. Hannan J. Nie and W. Jin. 1995. Unknown
word detection and segmentation of chinese using
statistical and heuristic knowledge. Communications
of the Chinese and Oriental Languages Information
Processing Society, 5:47C57.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference on
Machine Learning (ICML?01), pages 282?289.
Noboru Murata. 1998. A statistical study of on-line
learning. In On-line learning in neural networks,
Cambridge University Press, pages 63?92.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
Coling 2004, pages 562?568, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML?07.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm
and its efficient approximation. In Proceedings of
EACL?09, pages 772?780, Athens, Greece, March.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: A latent conditional model with
improved inference. In Proceedings of COLING?08,
pages 841?848, Manchester, UK.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun?ichi Tsujii. 2009a. Latent variable perceptron
algorithm for structured classification. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence (IJCAI 2009), pages 1236?1242.
Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, Yoshimasa
Tsuruoka, and Jun?ichi Tsujii. 2009b. A
discriminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of NAACL-HLT?09, pages 56?64, Boulder, Colorado,
June.
Xu Sun, Hisashi Kashima, Takuya Matsuzaki, and
Naonori Ueda. 2010. Averaged stochastic gradient
descent with feedback: An accurate, robust, and
fast training method. In Proceedings of the 10th
International Conference on Data Mining (ICDM?10),
pages 1067?1072.
Xu Sun, Hisashi Kashima, Ryota Tomioka, and Naonori
Ueda. 2011. Large scale real-life action recognition
using conditional random fields with stochastic
training. In Proceedings of the 15th Pacific-Asia
Conf. on Knowledge Discovery and Data Mining
(PAKDD?11).
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Comparison and
combination. In Chu-Ren Huang and Dan Jurafsky,
editors, COLING?10 (Posters), pages 1211?1219.
Chinese Information Processing Society of China.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A
261
conditional random field word segmenter for sighan
bakeoff 2005. In Proceedings of The Fourth SIGHAN
Workshop, pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. In Proceedings of ICML?06, pages 969?
976.
A. Wu and Z. Jiang. 2000. Statistically-enhanced new
word identification in a rule-based chinese system.
In Proceedings of the Second Chinese Language
Processing Workshop, page 46C51, Hong Kong,
China.
Yi-Lun Wu, Chaio-Wen Hsieh, Wei-Hsuan Lin, Chun-
Yi Liu, and Liang-Chih Yu. 2011. Unknown
word extraction from multilingual code-switching
sentences (in chinese). In Proceedings of ROCLING
(Posters)?11, pages 349?360.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. International Journal of
Computational Linguistics and Chinese Language
Processing, 8(1):29?48.
Yue Zhang and Stephen Clark. 2007. Chinese
segmentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 840?
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Companion Volume: Short Papers, pages
193?196, New York City, USA, June. Association for
Computational Linguistics.
Hai Zhao, Changning Huang, Mu Li, and Bao-Liang Lu.
2010. A unified character-based tagging framework
for chinese word segmentation. ACM Trans. Asian
Lang. Inf. Process., 9(2).
Guodong Zhou. 2005. A chunking strategy
towards unknown word detection in chinese word
segmentation. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Yee Kwong, editors, Proceedings
of IJCNLP?05, volume 3651 of Lecture Notes in
Computer Science, pages 530?541. Springer.
262
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572?581,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Lingual Mixture Model for Sentiment Classification
Xinfan Meng? ?Furu Wei? Xiaohua Liu? Ming Zhou? Ge Xu? Houfeng Wang?
?MOE Key Lab of Computational Linguistics, Peking University
?Microsoft Research Asia
?{mxf, xuge, wanghf}@pku.edu.cn
?{fuwei,xiaoliu,mingzhou}@microsoft.com
Abstract
The amount of labeled sentiment data in En-
glish is much larger than that in other lan-
guages. Such a disproportion arouse interest
in cross-lingual sentiment classification, which
aims to conduct sentiment classification in the
target language (e.g. Chinese) using labeled
data in the source language (e.g. English).
Most existing work relies on machine trans-
lation engines to directly adapt labeled data
from the source language to the target lan-
guage. This approach suffers from the limited
coverage of vocabulary in the machine transla-
tion results. In this paper, we propose a gen-
erative cross-lingual mixture model (CLMM)
to leverage unlabeled bilingual parallel data.
By fitting parameters to maximize the likeli-
hood of the bilingual parallel data, the pro-
posed model learns previously unseen senti-
ment words from the large bilingual parallel
data and improves vocabulary coverage signifi-
cantly. Experiments on multiple data sets show
that CLMM is consistently effective in two set-
tings: (1) labeled data in the target language are
unavailable; and (2) labeled data in the target
language are also available.
1 Introduction
Sentiment Analysis (also known as opinion min-
ing), which aims to extract the sentiment informa-
tion from text, has attracted extensive attention in
recent years. Sentiment classification, the task of
determining the sentiment orientation (positive, neg-
ative or neutral) of text, has been the most exten-
sively studied task in sentiment analysis. There is
?Contribution during internship atMicrosoft ResearchAsia.
already a large amount of work on sentiment classi-
fication of text in various genres and in many lan-
guages. For example, Pang et al (2002) focus on
sentiment classification of movie reviews in English,
and Zagibalov and Carroll (2008) study the problem
of classifying product reviews in Chinese. During
the past few years, NTCIR1 organized several pi-
lot tasks for sentiment classification of news articles
written in English, Chinese and Japanese (Seki et
al., 2007; Seki et al, 2008).
For English sentiment classification, there are sev-
eral labeled corpora available (Hu and Liu, 2004;
Pang et al, 2002; Wiebe et al, 2005). However, la-
beled resources in other languages are often insuf-
ficient or even unavailable. Therefore, it is desir-
able to use the English labeled data to improve senti-
ment classification of documents in other languages.
One direct approach to leveraging the labeled data
in English is to use machine translation engines as a
black box to translate the labeled data from English
to the target language (e.g. Chinese), and then us-
ing the translated training data directly for the devel-
opment of the sentiment classifier in the target lan-
guage (Wan, 2009; Pan et al, 2011).
Although the machine-translation-based methods
are intuitive, they have certain limitations. First,
the vocabulary covered by the translated labeled
data is limited, hence many sentiment indicative
words can not be learned from the translated labeled
data. Duh et al (2011) report low overlapping
between vocabulary of natural English documents
and the vocabulary of documents translated to En-
glish from Japanese, and the experiments of Duh
1http://research.nii.ac.jp/ntcir/index-en.html
572
et al (2011) show that vocabulary coverage has a
strong correlation with sentiment classification ac-
curacy. Second, machine translation may change the
sentiment polarity of the original text. For exam-
ple, the negative English sentence ?It is too good to
be true? is translated to a positive sentence in Chi-
nese ?????????? by Google Translate
(http://translate.google.com/), which literally means
?It is good and true?.
In this paper we propose a cross-lingual mixture
model (CLMM) for cross-lingual sentiment classifi-
cation. Instead of relying on the unreliable machine
translated labeled data, CLMM leverages bilingual
parallel data to bridge the language gap between the
source language and the target language. CLMM is
a generative model that treats the source language
and target language words in parallel data as gener-
ated simultaneously by a set of mixture components.
By ?synchronizing? the generation of words in the
source language and the target language in a parallel
corpus, the proposed model can (1) improve vocabu-
lary coverage by learning sentiment words from the
unlabeled parallel corpus; (2) transfer polarity label
information between the source language and target
language using a parallel corpus. Besides, CLMM
can improve the accuracy of cross-lingual sentiment
classification consistently regardless of whether la-
beled data in the target language are present or not.
We evaluate the model on sentiment classification
of Chinese using English labeled data. The exper-
iment results show that CLMM yields 71% in accu-
racy when no Chinese labeled data are used, which
significantly improves Chinese sentiment classifica-
tion and is superior to the SVMand co-training based
methods. When Chinese labeled data are employed,
CLMMyields 83% in accuracy, which is remarkably
better than the SVM and achieve state-of-the-art per-
formance.
This paper makes two contributions: (1) we pro-
pose a model to effectively leverage large bilin-
gual parallel data for improving vocabulary cover-
age; and (2) the proposed model is applicable in both
settings of cross-lingual sentiment classification, ir-
respective of the availability of labeled data in the
target language.
The paper is organized as follows. We review re-
lated work in Section 2, and present the cross-lingual
mixture model in Section 3. Then we present the ex-
perimental studies in Section 4, and finally conclude
the paper and outline the future plan in Section 5.
2 Related Work
In this section, we present a brief review of the re-
lated work on monolingual sentiment classification
and cross-lingual sentiment classification.
2.1 Sentiment Classification
Early work of sentiment classification focuses on
English product reviews or movie reviews (Pang et
al., 2002; Turney, 2002; Hu and Liu, 2004). Since
then, sentiment classification has been investigated
in various domains and different languages (Zag-
ibalov and Carroll, 2008; Seki et al, 2007; Seki et
al., 2008; Davidov et al, 2010). There exist two
main approaches to extracting sentiment orientation
automatically. The Dictionary-based approach (Tur-
ney, 2002; Taboada et al, 2011) aims to aggregate
the sentiment orientation of a sentence (or docu-
ment) from the sentiment orientations of words or
phrases found in the sentence (or document), while
the corpus-based approach (Pang et al, 2002) treats
the sentiment orientation detection as a conventional
classification task and focuses on building classifier
from a set of sentences (or documents) labeled with
sentiment orientations.
Dictionary-based methods involve in creating or
using sentiment lexicons. Turney (2002) derives
sentiment scores for phrases by measuring the mu-
tual information between the given phrase and the
words ?excellent? and ?poor?, and then uses the av-
erage scores of the phrases in a document as the
sentiment of the document. Corpus-based meth-
ods are often built upon machine learning mod-
els. Pang et al (2002) compare the performance
of three commonly used machine learning models
(Naive Bayes, Maximum Entropy and SVM). Ga-
mon (2004) shows that introducing deeper linguistic
features into SVM can help to improve the perfor-
mance. The interested readers are referred to (Pang
and Lee, 2008) for a comprehensive review of senti-
ment classification.
2.2 Cross-Lingual Sentiment Classification
Cross-lingual sentiment classification, which aims
to conduct sentiment classification in the target lan-
guage (e.g. Chinese) with labeled data in the source
573
language (e.g. English), has been extensively stud-
ied in the very recent years. The basic idea is to ex-
plore the abundant labeled sentiment data in source
language to alleviate the shortage of labeled data in
the target language.
Most existing work relies on machine translation
engines to directly adapt labeled data from the source
language to target language. Wan (2009) proposes
to use ensemble method to train better Chinese sen-
timent classification model on English labeled data
and their Chinese translation. English Labeled data
are first translated to Chinese, and then two SVM
classifiers are trained on English andChinese labeled
data respectively. After that, co-training (Blum and
Mitchell, 1998) approach is adopted to leverage Chi-
nese unlabeled data and their English translation to
improve the SVM classifier for Chinese sentiment
classification. The same idea is used in (Wan, 2008),
but the ensemble techniques used are various vot-
ing methods and the individual classifiers used are
dictionary-based classifiers.
Instead of ensemblemethods, Pan et al (2011) use
matrix factorization formulation. They extend Non-
negative Matrix Tri-Factorization model (Li et al,
2009) to bilingual view setting. Their bilingual view
is also constructed by using machine translation en-
gines to translate original documents. Prettenhofer
and Stein (2011) use machine translation engines in
a different way. They generalize Structural Corre-
spondence Learning (Blitzer et al, 2006) to multi-
lingual setting. Instead of using machine translation
engines to translate labeled text, the authors use it to
construct the word translation oracle for pivot words
translation.
Lu et al (2011) focus on the task of jointly im-
proving the performance of sentiment classification
on two languages (e.g. English and Chinese) . the
authors use an unlabeled parallel corpus instead of
machine translation engines. They assume paral-
lel sentences in the corpus should have the same
sentiment polarity. Besides, they assume labeled
data in both language are available. They propose
a method of training two classifiers based on maxi-
mum entropy formulation to maximize their predic-
tion agreement on the parallel corpus. However, this
method requires labeled data in both the source lan-
guage and the target language, which are not always
readily available.
3 Cross-Lingual Mixture Model for
Sentiment Classification
In this section we present the cross-lingual mix-
ture model (CLMM) for sentiment classification.
We first formalize the task of cross-lingual sentiment
classification. Then we describe the CLMM model
and present the parameter estimation algorithm for
CLMM.
3.1 Cross-lingual Sentiment Classification
Formally, the task we are concerned about is to de-
velop a sentiment classifier for the target language T
(e.g. Chinese), given labeled sentiment data DS in
the source language S (e.g. English), unlabeled par-
allel corpus U of the source language and the target
language, and optional labeled dataDT in target lan-
guage T . Aligning with previous work (Wan, 2008;
Wan, 2009), we only consider binary sentiment clas-
sification scheme (positive or negative) in this paper,
but the proposed method can be used in other classi-
fication schemes with minor modifications.
3.2 The Cross-Lingual Mixture Model
The basic idea underlying CLMM is to enlarge
the vocabulary by learning sentiment words from the
parallel corpus. CLMM defines an intuitive genera-
tion process as follows. Suppose we are going to
generate a positive or negative Chinese sentence, we
have two ways of generating words. The first way
is to directly generate a Chinese word according to
the polarity of the sentence. The other way is to first
generate an English word with the same polarity and
meaning, and then translate it to a Chinese word.
More formally, CLMM defines a generative mix-
ture model for generating a parallel corpus. The un-
observed polarities of the unlabeled parallel corpus
are modeled as hidden variables, and the observed
words in parallel corpus are modeled as generated by
a set of words generation distributions conditioned
on the hidden variables. Given a parallel corpus, we
fit CLMM model by maximizing the likelihood of
generating this parallel corpus. By maximizing the
likelihood, CLMM can estimate words generation
probabilities for words unseen in the labeled data but
present in the parallel corpus, hence expand the vo-
cabulary. In addition, CLMM can utilize words in
both the source language and target language for de-
574
termining polarity classes of the parallel sentences.
POS 
NEG 
POS 
NEG 
...? 
Source 
Target 
U 
u wt 
ws 
Figure 1: The generation process of the
cross-lingual mixture model
Figure 1 illustrates the detailed process of gener-
ating words in the source language and target lan-
guage respectively for the parallel corpus U , from
the four mixture components in CLMM. Particu-
larly, for each pair of parallel sentences ui ? U , we
generate the words as follows.
1. Document class generation: Generating the
polarity class.
(a) Generating a polarity class cs from a
Bernoulli distribution Ps(C).
(b) Generating a polarity class ct from a
Bernoulli distribution Pt(C)
2. Words generation: Generating the words
(a) Generating source language wordsws from
a Multinomial distribution P (ws|cs)
(b) Generating target language words wt from
a Multinomial distribution P (wt|ct)
3. Words projection: Projecting the words onto
the other language
(a) Projecting the source language wordsws to
target language words wt by word projec-
tion probability P (wt|ws)
(b) Projecting the target language words wt to
source language words ws by word projec-
tion probability P (ws|wt)
CLMM finds parameters by using MLE (Maxi-
mum Likelihood Estimation). The parameters to be
estimated include conditional probabilities of word
to class, P (ws|c) and P (wt|c), and word projection
probabilities, P (ws|wt) and P (wt|ws). We will de-
scribe the log-likelihood function and then show how
to estimate the parameters in subsection 3.3. The
obtained word-class conditional probability P (wt|c)
can then be used to classify text in the target lan-
guages using Bayes Theorem and the Naive Bayes
independence assumption.
Formally, we have the following log-likelihood
function for a parallel corpus U2.
L(?|U) =
|Us|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
[
Nsi log
(
P (ws|cj) + P (ws|wt)P (wt|cj)
)]
+
|Ut|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
[
Nti log
(
P (wt|cj) + P (wt|ws)P (ws|cj)
)]
(1)
where ? is the model parameters;Nsi (Nti) is the oc-
currences of thewordws (wt) in document di; |Ds| is
the number of documents; |C| is the number of class
labels; Vs and Vt are the vocabulary in the source lan-
guage and the vocabulary in the target language.|Us|
and |Ut| are the number of unlabeled sentences in the
source language and target language.
Meanwhile, we have the following log-likelihood
function for labeled data in the source language Ds.
L(?|Ds) =
|Ds|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
Nsi logP (ws|cj)?ij (2)
where ?ij = 1 if the label of di is cj , and 0 otherwise.
In addition, when labeled data in the target lan-
guage is available, we have the following log-
likelihood function.
L(?|Dt) =
|Dt|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
Nti logP (wt|cj)?ij (3)
Combining the above three likelihood functions
together, we have the following likelihood function.
L(?|Dt, Ds, U) = L(?|U) + L(?|Ds) + L(?|Dt)
(4)
Note that the third term on the right hand side
(L(?|Dt)) is optional.
2For simplicity, we assume the prior distribution P (C) is
uniform and drop it from the formulas.
575
3.3 Parameter Estimation
Instead of estimating word projection probability
(P (ws|wt) and P (wt|ws)) and conditional proba-
bility of word to class (P (wt|c) and P (ws|c)) si-
multaneously in the training procedure, we estimate
them separately since the word projection probabil-
ity stays invariant when estimating other parame-
ters. We estimate word projection probability using
word alignment probability generated by the Berke-
ley aligner (Liang et al, 2006). The word align-
ment probabilities serves two purposes. First, they
connect the corresponding words between the source
language and the target language. Second, they ad-
just the strength of influences between the corre-
sponding words. Figure 2 gives an example of word
alignment probability. As is shown, the three words
?tour de force? altogether express a positive mean-
ing, while in Chinese the same meaning is expressed
with only one word ???? (masterpiece). CLMM
use word alignment probability to decrease the in-
fluences from ???? (masterpiece) to ?tour?, ?de?
and ?force? individually, using the word projection
probability (i.e. word alignment probability), which
is 0.3 in this case.
Herman Melville's Moby Dick was a tour de force.  
 
??? ???? ? ?????? ?? ??? 
1  1  .5  .5  1  1  .3  . 3  . 3  
Figure 2: Word Alignment Probability
We use Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977) to estimate the con-
ditional probability of word ws and wt given class
c, P (ws|c) and P (wt|c) respectively. We derive the
equations for EM algorithm, using notations similar
to (Nigam et al, 2000).
In the E-step, the distribution of hidden variables
(i.e. class label for unlabeled parallel sentences) is
computed according to the following equations.
P (cj |usi) = Z(cusi = cj) =
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
?
cj
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
(5)
P (cj |uti) = Z(cuti = cj) =
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
?
cj
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
(6)
whereZ(cusi = cj)
(
Z(cuti) = cj
)
is the probability
of the source (target) language sentence usi (uti) in
the i-th pair of sentences ui having class label cj .
In the M-step, the parameters are computed by the
following equations.
P (ws|cj) =
1 +
?|Ds|
i=1 ?s(i)NsiP (cj |di)
|V | +
?|Vs|
s=1 ?(i)NsiP (cj |di)
(7)
P (wt|cj) =
1 +
?|Dt|
i=1 ?t(i)NtiP (cj |di)
|V | +
?|Vt|
t=1 ?(i)NtiP (cj |di)
(8)
where ?s(i) and ?t(i) are weighting factor to con-
trol the influence of the unlabeled data. We set ?s(i)
(
?t(i)
)
to ?s
(
?t
)
when di belongs to unlabeled
data, 1 otherwise. When di belongs to labeled data,
P (cj |di) is 1 when its label is cj and 0 otherwise.
When di belongs to unlabeled data, P (cj |di) is com-
puted according to Equation 5 or 6.
4 Experiment
4.1 Experiment Setup and Data Sets
Experiment setup: We conduct experiments on
two common cross-lingual sentiment classification
settings. In the first setting, no labeled data in the
target language are available. This setting has real-
istic significance, since in some situations we need to
quickly develop a sentiment classifier for languages
that we do not have labeled data in hand. In this
case, we classify text in the target language using
only labeled data in the source language. In the sec-
ond setting, labeled data in the target language are
also available. In this case, a more reasonable strat-
egy is to make full use of both labeled data in the
source language and target language to develop the
sentiment classifier for the target language. In our
experiments, we consider English as the source lan-
guage and Chinese as the target language.
Data sets: For Chinese sentiment classification,
we use the same data set described in (Lu et al,
2011). The labeled data sets consist of two English
data sets and one Chinese data set. The English data
set is from the Multi-Perspective Question Answer-
ing (MPQA) corpus (Wiebe et al, 2005) and the NT-
CIR Opinion Analysis Pilot Task data set (Seki et
al., 2008; Seki et al, 2007). The Chinese data set
also comes from the NTCIR Opinion Analysis Pi-
lot Task data set. The unlabeled parallel sentences
576
are selected from ISI Chinese-English parallel cor-
pus (Munteanu and Marcu, 2005). Following the
description in (Lu et al, 2011), we remove neutral
sentences and keep only high confident positive and
negative sentences as predicted by a maximum en-
tropy classifier trained on the labeled data. Table 1
shows the statistics for the data sets used in the ex-
periments. We conduct experiments on two data set-
tings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN
+ NTCIR-CH.
MPQA NTCIR-EN NTCIR-CH
Positive 1,471(30%) 528 (30%) 2,378 (55%)
Negative 3,487(70%) 1,209(70%) 1,916(44%)
Total 4,958 1,737 4,294
Table 1: Statistics about the Data
CLMM includes two hyper-parameters (?s and
?t) controlling the contribution of unlabeled parallel
data. Larger weights indicate larger influence from
the unlabeled data. We set the hyper-parameters
by conducting cross validations on the labeled data.
WhenChinese labeled data are unavailable, we set?t
to 1 and ?s to 0.1, since no Chinese labeled data are
used and the contribution of target language to the
source language is limited. When Chinese labeled
data are available, we set ?s and ?t to 0.2.
To prevent long sentences from dominating the pa-
rameter estimation, we preprocess the data set by
normalizing the length of all sentences to the same
constant (Nigam et al, 2000), the average length of
the sentences.
4.2 Baseline Methods
For the purpose of comparison, we implement the
following baseline methods.
MT-SVM:We translate the English labeled data to
Chinese using Google Translate and use the transla-
tion results to train the SVM classifier for Chinese.
SVM: We train a SVM classifier on the Chinese
labeled data.
MT-Cotrain: This is the co-training based ap-
proach described in (Wan, 2009). We summarize
the main steps as follows. First, two monolingual
SVM classifiers are trained on English labeled data
and Chinese data translated from English labeled
data. Second, the two classifiers make prediction on
Chinese unlabeled data and their English translation,
respectively. Third, the 100 most confidently pre-
dicted English and Chinese sentences are added to
the training set and the twomonolingual SVMclassi-
fiers are re-trained on the expanded training set. The
second and the third steps are repeated for 100 times
to obtain the final classifiers.
Para-Cotrain: The training process is the same as
MT-Cotrain. However, we use a different set of En-
glish unlabeled sentences. Instead of using the corre-
sponding machine translation of Chinese unlabeled
sentences, we use the parallel English sentences of
the Chinese unlabeled sentences.
Joint-Train: This is the state-of-the-art method de-
scribed in (Lu et al, 2011). This model use En-
glish labeled data and Chinese labeled data to obtain
initial parameters for two maximum entropy clas-
sifiers (for English documents and Chinese docu-
ments), and then conduct EM-iterations to update
the parameters to gradually improve the agreement
of the two monolingual classifiers on the unlabeled
parallel sentences.
4.3 Classification Using Only English Labeled
Data
The first set of experiments are conducted on us-
ing only English labeled data to create the sentiment
classifier for Chinese. This is a challenging task,
since we do not use any Chinese labeled data. And
MPQA and NTCIR data sets are compiled by differ-
ent groups using different annotation guidelines.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM N/A N/A
MT-Cotrain 65.13 59.11
Para-Cotrain 67.21 60.71
Joint-Train N/A N/A
CLMM 70.96 71.52
Table 2: Classification Accuracy Using Only
English Labeled Data
Table 2 shows the accuracy of the baseline sys-
tems as well as the proposed model (CLMM). As
is shown, sentiment classification does not bene-
fit much from the direct machine translation. For
NTCIR-EN+NTCIR-CH, the accuracy of MT-SVM
577
is only 62.34%. For MPQA-EN+NTCIR-CH, the
accuracy is 54.33%, even lower than a trivial
method, which achieves 55.4% by predicting all sen-
tences to be positive. The underlying reason is that
the vocabulary coverage in machine translated data
is low, therefore the classifier learned from the la-
beled data is unable to generalize well on the test
data. Meanwhile, the accuracy of MT-SVM on
NTCIR-EN+NTCIR-CH data set is much better than
that on MPQA+NTCIR-CH data set. That is be-
cause NTCIR-EN and NTCIR-CH cover similar top-
ics. The other two methods using machine translated
data, MT-Cotrain and Para-Cotrain also do not per-
form verywell. This result is reasonable, because the
initial Chinese classifier trained on machine trans-
lated data (MT-SVM) is relatively weak. We also
observe that using a parallel corpus instead of ma-
chine translations can improve classification accu-
racy. It should be noted that we do not have the result
for Joint-Train model in this setting, since it requires
both English labeled data and Chinese labeled data.
4.4 Classification Using English and Chinese
Labeled Data
The second set of experiments are conducted on
using both English labeled data and Chinese labeled
data to develop the Chinese sentiment classifier. We
conduct 5-fold cross validations on Chinese labeled
data. We use the same baseline methods as described
in Section 4.2, but we use natural Chinese sentences
instead of translated Chinese sentences as labeled
data in MT-Cotrain and Para-Cotrain. Table 3 shows
the accuracy of baseline systems as well as CLMM.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM 80.58 80.58
MT-Cotrain 82.28 80.93
Para-Cotrain 82.35 82.18
Joint-Train 83.11 83.42
CLMM 82.73 83.02
Table 3: Classification Accuracy Using English and
Chinese Labeled Data
As is seen, SVMperforms significantly better than
MT-SVM. One reason is that we use natural Chi-
nese labeled data instead of translated Chinese la-
beled data. Another reason is that we use 5-fold
cross validations in this setting, while the previous
setting is an open test setting. In this setting, SVM
is a strong baseline with 80.6% accuracy. Never-
theless, all three methods which leverage an unla-
beled parallel corpus, namely Para-Cotrain, Joint-
Train and CLMM, still show big improvements over
the SVM baseline. Their results are comparable and
all achieve state-of-the-art accuracy of about 83%,
but in terms of training speed, CLMM is the fastest
method (Table 4). Similar to the previous setting,We
also have the same observation that using a parallel
corpus is better than using translations.
Method Iterations Total Time
Para-Cotrain 100 6 hours
Joint-Train 10 55 seconds
CLMM 10 30 seconds
Table 4: Training Speed Comparison
4.5 The Influence of Unlabeled Parallel Data
We investigate how the size of the unlabeled par-
allel data affects the sentiment classification in this
subsection. We vary the number of sentences in the
unlabeled parallel from 2,000 to 20,000. We use
only English labeled data in this experiment, since
this more directly reflects the effectiveness of each
model in utilizing unlabeled parallel data. From Fig-
ure 3 and Figure 4, we can see that when more unla-
beled parallel data are added, the accuracy of CLMM
consistently improves. The performance of CLMM
is remarkably superior than Para-Cotrain and MT-
Cotrain. When we have 10,000 parallel sentences,
the accuracy of CLMM on the two data sets quickly
increases to 68.77% and 68.91%, respectively. By
contrast, we observe that the performance of Para-
Cotrain and MT-Cotrain is able to obtain accuracy
improvement only after about 10,000 sentences are
added. The reason is that the two methods use ma-
chine translated labeled data to create initial Chinese
classifiers. As is depicted in Table 2, these classifiers
are relatively weak. As a result, in the initial itera-
tions of co-training based methods, the predictions
made by the Chinese classifiers are inaccurate, and
co-training based methods need to see more parallel
578
Number of Sentences
Accur
acy
62
64
66
68
70
l
l l l
l l
l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 3: Accuracy with different size of
unlabeled data for NTICR-EN+NTCIR-CH
Number of Sentences
Accur
acy
55
60
65
70
l
l
l
l l
l l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 4: Accuracy with different size of
unlabeled data for MPQA+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 5: Accuracy with different size of
labeled data for NTCIR-EN+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 6: Accuracy with different size of
labeled data for MPQA+NTCIR-CH
sentences to refine the initial classifiers.
4.6 The Influence of Chinese Labeled Data
In this subsection, we investigate how the size of
the Chinese labeled data affects the sentiment classi-
fication. As is shown in Figure 5 and Figure 6, when
only 500 labeled sentences are used, CLMM is capa-
ble of achieving 72.52% and 74.48% in accuracy on
the two data sets, obtaining 10% and 8% improve-
ments over the SVM baseline, respectively. This
indicates that our method leverages the unlabeled
data effectively. When more sentences are used,
CLMM consistently shows further improvement in
accuracy. Para-Cotrain and Joint-Train show simi-
lar trends. When 3500 labeled sentences are used,
SVM achieves 80.58%, a relatively high accuracy
for sentiment classification. However, CLMM and
the other two models can still gain improvements.
This further demonstrates the advantages of expand-
ing vocabulary using bilingual parallel data.
5 Conclusion and Future Work
In this paper, we propose a cross-lingual mix-
ture model (CLMM) to tackle the problem of cross-
lingual sentiment classification. This method has
two advantages over the existing methods. First, the
proposed model can learn previously unseen senti-
ment words from large unlabeled data, which are not
covered by the limited vocabulary in machine trans-
lation of the labeled data. Second, CLMM can ef-
fectively utilize unlabeled parallel data regardless of
whether labeled data in the target language are used
or not. Extensive experiments suggest that CLMM
consistently improve classification accuracy in both
settings. In the future, we will work on leverag-
ing parallel sentences and word alignments for other
tasks in sentiment analysis, such as building multi-
lingual sentiment lexicons.
Acknowledgment We thank Bin Lu and Lei Wang for
their help. This research was partly supported by National High
Technology Research and Development Program of China (863
Program) (No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009, No.60973053)
579
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, page 120?128.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, page 92?100.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
page 241?249.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), page 1?38.
Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for Cross-Lingual sentiment
classification? In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, page 429?433,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. InProceedings of the
20th international conference on Computational Lin-
guistics, page 841.
Mingqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge
discovery and data mining, page 168?177.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to sentiment
classification with lexical prior knowledge. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, page 244?252, Suntec, Singapore, August.
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, page 104?111.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies-
Volume 1, page 320?330.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine learning, 39(2):103?134.
Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang.
2011. Cross-lingual sentiment classification via bi-
view non-negative matrix tri-factorization. Advances
in Knowledge Discovery and Data Mining, page
289?300.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, page 79?86.
Peter Prettenhofer and Benno Stein. 2011. Cross-lingual
adaptation using structural correspondence learning.
ACM Transactions on Intelligent Systems and Technol-
ogy (TIST), 3(1):13.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of opinion analysis pilot task at NTCIR-6.
In Proceedings of NTCIR-6 Workshop Meeting, page
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin.
2008. Overview of multilingual opinion analysis task
at NTCIR-7. In Proc. of the Seventh NTCIR Workshop.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based meth-
ods for sentiment analysis. Comput. Linguist., page to
appear.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics,
page 417?424.
Xiaojun Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised chinese sentiment
analysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
?08, page 553?561, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
580
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume 1,
page 235?243.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2):165?210.
Taras Zagibalov and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, page 1073?1080.
581
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30?34,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Entity Representation for Entity Disambiguation
Zhengyan He? Shujie Liu? Mu Li? Ming Zhou? Longkai Zhang? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
zhlongk@qq.com wanghf@pku.edu.cn
Abstract
We propose a novel entity disambigua-
tion model, based on Deep Neural Net-
work (DNN). Instead of utilizing simple
similarity measures and their disjoint com-
binations, our method directly optimizes
document and entity representations for a
given similarity measure. Stacked Denois-
ing Auto-encoders are first employed to
learn an initial document representation in
an unsupervised pre-training stage. A su-
pervised fine-tuning stage follows to opti-
mize the representation towards the simi-
larity measure. Experiment results show
that our method achieves state-of-the-art
performance on two public datasets with-
out any manually designed features, even
beating complex collective approaches.
1 Introduction
Entity linking or disambiguation has recently re-
ceived much attention in natural language process-
ing community (Bunescu and Pasca, 2006; Han
et al, 2011; Kataria et al, 2011; Sen, 2012). It is
an essential first step for succeeding sub-tasks in
knowledge base construction (Ji and Grishman,
2011) like populating attribute to entities. Given
a sentence with four mentions, ?The [[Python]] of
[[Delphi]] was a creature with the body of a snake.
This creature dwelled on [[Mount Parnassus]], in
central [[Greece]].? How can we determine that
Python is an earth-dragon in Greece mythology
and not the popular programming language, Del-
phi is not the auto parts supplier, and Mount Par-
nassus is in Greece, not in Colorado?
A most straightforward method is to compare
the context of the mention and the definition of
candidate entities. Previous work has explored
many ways of measuring the relatedness of context
?Corresponding author
d and entity e, such as dot product, cosine similar-
ity, Kullback-Leibler divergence, Jaccard distance,
or more complicated ones (Zheng et al, 2010;
Kulkarni et al, 2009; Hoffart et al, 2011; Bunescu
and Pasca, 2006; Cucerzan, 2007; Zhang et al,
2011). However, these measures are often dupli-
cate or over-specified, because they are disjointly
combined and their atomic nature determines that
they have no internal structure.
Another line of work focuses on collective dis-
ambiguation (Kulkarni et al, 2009; Han et al,
2011; Ratinov et al, 2011; Hoffart et al, 2011).
Ambiguous mentions within the same context are
resolved simultaneously based on the coherence
among decisions. Collective approaches often un-
dergo a non-trivial decision process. In fact, (Rati-
nov et al, 2011) show that even though global ap-
proaches can be improved, local methods based on
only similarity sim(d, e) of context d and entity e
are hard to beat. This somehow reveals the impor-
tance of a good modeling of sim(d, e).
Rather than learning context entity associa-
tion at word level, topic model based approaches
(Kataria et al, 2011; Sen, 2012) can learn it in
the semantic space. However, the one-topic-per-
entity assumption makes it impossible to scale to
large knowledge base, as every entity has a sepa-
rate word distribution P (w|e); besides, the train-
ing objective does not directly correspond with
disambiguation performances.
To overcome disadvantages of previous ap-
proaches, we propose a novel method to learn con-
text entity association enriched with deep architec-
ture. Deep neural networks (Hinton et al, 2006;
Bengio et al, 2007) are built in a hierarchical man-
ner, and allow us to compare context and entity
at some higher level abstraction; while at lower
levels, general concepts are shared across entities,
resulting in compact models. Moreover, to make
our model highly correlated with disambiguation
performance, our method directly optimizes doc-
30
ument and entity representations for a fixed simi-
larity measure. In fact, the underlying representa-
tions for computing similarity measure add inter-
nal structure to the given similarity measure. Fea-
tures are learned leveraging large scale annotation
of Wikipedia, without any manual design efforts.
Furthermore, the learned model is compact com-
pared with topic model based approaches, and can
be trained discriminatively without relying on ex-
pensive sampling strategy. Despite its simplicity,
it beats all complex collective approaches in our
experiments. The learned similarity measure can
be readily incorporated into any existing collective
approaches, which further boosts performance.
2 Learning Representation for
Contextual Document
Given a mention string m with its context docu-
ment d, a list of candidate entities C(m) are gen-
erated form, for each candidate entity ei ? C(m),
we compute a ranking score sim(dm, ei) indicat-
ing how likely m refers to ei. The linking result is
e = argmaxei sim(dm, ei).
Our algorithm consists of two stages. In the pre-
training stage, Stacked Denoising Auto-encoders
are built in an unsupervised layer-wise fashion to
discover general concepts encoding d and e. In the
supervised fine-tuning stage, the entire network
weights are fine-tuned to optimize the similarity
score sim(d, e).
2.1 Greedy Layer-wise Pre-training
Stacked Auto-encoders (Bengio et al, 2007) is
one of the building blocks of deep learning. As-
sume the input is a vector x, an auto-encoder con-
sists of an encoding process h(x) and a decod-
ing process g(h(x)). The goal is to minimize the
reconstruction error L(x, g(h(x))), thus retaining
maximum information. By repeatedly stacking
new auto-encoder on top of previously learned
h(x), stacked auto-encoders are obtained. This
way we learn multiple levels of representation of
input x.
One problem of auto-encoder is that it treats all
words equally, no matter it is a function word or
a content word. Denoising Auto-encoder (DA)
(Vincent et al, 2008) seeks to reconstruct x given
a random corruption x? of x. DA can capture global
structure while ignoring noise as the author shows
in image processing. In our case, we input each
document as a binary bag-of-words vector (Fig.
1). DA will capture general concepts and ignore
noise like function words. By applying masking
noise (randomly mask 1 with 0), the model also
exhibits a fill-in-the-blank property (Vincent et
al., 2010): the missing components must be re-
covered from partial input. Take ?greece? for ex-
ample, the model must learn to predict it with
?python? ?mount?, through some hidden unit. The
hidden unit may somehow express the concept of
Greece mythology.
h(x)
g(h(x))
pythondragon delphicoding ... greecemountsnake phd
reconstruct input
reconstruct randomzero nodenot reconstruct
inactive
active, but mask out
active
Figure 1: DA and reconstruction sampling.
In order to distinguish between a large num-
ber of entities, the vocabulary size must be large
enough. This adds considerable computational
overhead because the reconstruction process in-
volves expensive dense matrix multiplication. Re-
construction sampling keeps the sparse property
of matrix multiplication by reconstructing a small
subset of original input, with no loss of quality of
the learned representation (Dauphin et al, 2011).
2.2 Supervised Fine-tuning
This stage we optimize the learned representation
(?hidden layer n? in Fig. 2) towards the ranking
score sim(d, e), with large scale Wikipedia an-
notation as supervision. We collect hyperlinks in
Wikipedia as our training set {(di, ei,mi)}, where
mi is the mention string for candidate generation.
The network weights below ?hidden layer n? are
initialized with the pre-training stage.
Next, we stack another layer on top of the
learned representation. The whole network is
tuned by the final supervised objective. The reason
to stack another layer on top of the learned rep-
resentation, is to capture problem specific struc-
tures. Denote the encoding of d and e as d? and
e? respectively, after stacking the problem-specific
layer, the representation for d is given as f(d) =
sigmoid(W ? d? + b), where W and b are weight
and bias term respectively. f(e) follows the same
31
encoding process.
The similarity score of (d, e) pair is defined as
the dot product of f(d) and f(e) (Fig. 2):
sim(d, e) = Dot(f(d), f(e)) (1)
<.,.>
f(d) f(e)
hidden layer n
stacked auto-encoder
sim(d,e)
Figure 2: Network structure of fine-tuning stage.
Our goal is to rank the correct entity higher
than the rest candidates relative to the context of
the mention. For each training instance (d, e), we
contrast it with one of its negative candidate pair
(d, e?). This gives the pairwise ranking criterion:
L(d, e) = max{0, 1? sim(d, e) + sim(d, e?)}
(2)
Alternatively, we can contrast with all its candi-
date pairs (d, ei). That is, we raise the similarity
score of true pair sim(d, e) and penalize all the
rest sim(d, ei). The loss function is defined as
negative log of softmax function:
L(d, e) = ? log exp sim(d, e)?
ei?C(m) exp sim(d, ei)
(3)
Finally, we seek to minimize the following train-
ing objective across all training instances:
L =
?
d,e
L(d, e) (4)
The loss function is closely related to con-
trastive estimation (Smith and Eisner, 2005),
which defines where the positive example takes
probability mass from. We find that by penaliz-
ing more negative examples, convergence speed
can be greatly accelerated. In our experiments, the
softmax loss function consistently outperforms
pairwise ranking loss function, which is taken as
our default setting.
However, the softmax training criterion adds
additional computational overhead when per-
forming mini-batch Stochastic Gradient Descent
(SGD). Although we can use a plain SGD (i.e.
mini-batch size is 1), mini-batch SGD is faster to
converge and more stable. Assume the mini-batch
size ism and the number of candidates is n, a total
of m ? n forward-backward passes over the net-
work are performed to compute a similarity ma-
trix (Fig. 3), while pairwise ranking criterion only
needs 2?m. We address this problem by grouping
training pairs with same mentionm into one mini-
batch {(d, ei)|ei ? C(m)}. Observe that if candi-
date entities overlap, they share the same forward-
backward path. Only m + n forward-backward
passes are needed for each mini-batch now.
Python (programming language)PythonidaePython (mythology)
... ...
... ...
... ...
d0
d1 ...dm ... =sim(d,e)
e0 e1 e2 en
Figure 3: Sharing path within mini-batch.
The re-organization of mini-batch is similar
in spirit to Backpropagation Through Structure
(BTS) (Goller and Kuchler, 1996). BTS is a vari-
ant of the general backpropagation algorithm for
structured neural network. In BTS, parent node
is computed with its child nodes at the forward
pass stage; child node receives gradient as the sum
of derivatives from all its parents. Here (Fig. 2),
parent node is the score node sim(d, e) and child
nodes are f(d) and f(e). In Figure 3, each row
shares forward path of f(d) while each column
shares forward path of f(e). At backpropagation
stage, gradient is summed over each row of score
nodes for f(d) and over each column for f(e).
Till now, our input simply consists of bag-of-
words binary vector. We can incorporate any
handcrafted feature f(d, e) as:
sim(d, e) = Dot(f(d), f(e)) + ~?~f(d, e) (5)
In fact, we find that with only Dot(f(d), f(e))
as ranking score, the performance is sufficiently
good. So we leave this as our future work.
32
3 Experiments and Analysis
Training settings: In pre-training stage, input
layer has 100,000 units, all hidden layers have
1,000 units with rectifier functionmax(0, x). Fol-
lowing (Glorot et al, 2011), for the first recon-
struction layer, we use sigmoid activation func-
tion and cross-entropy error function. For higher
reconstruction layers, we use softplus (log(1 +
exp(x))) as activation function and squared loss
as error function. For corruption process, we use a
masking noise probability in {0.1,0.4,0.7} for the
first layer, a Gaussian noise with standard devi-
ation of 0.1 for higher layers. For reconstruction
sampling, we set the reconstruction rate to 0.01. In
fine-tuning stage, the final layer has 200 units with
sigmoid activation function. The learning rate is
set to 1e-3. The mini-batch size is set to 20.
We run all our experiments on a Linux ma-
chine with 72GB memory 6 core Xeon CPU. The
model is implemented in Python with C exten-
sions, numpy configured with Openblas library.
Thanks to reconstruction sampling and refined
mini-batch arrangement, it takes about 1 day to
converge for pre-training and 3 days for fine-
tuning, which is fast given our training set size.
Datasets: We use half of Wikipedia 1 plain text
(?1.5M articles split into sections) for pre-training.
We collect a total of 40M hyperlinks grouped by
name string m for fine-tuning stage. We holdout
a subset of hyperlinks for model selection, and we
find that 3 layers network with a higher masking
noise rate (0.7) always gives best performance.
We select TAC-KBP 2010 (Ji and Grishman,
2011) dataset for non-collective approaches, and
AIDA 2 dataset for collective approaches. For both
datasets, we evaluate the non-NIL queries. The
TAC-KBP and AIDA testb dataset contains 1020
and 4485 non-NIL queries respectively.
For candidate generation, mention-to-entity dic-
tionary is built by mining Wikipedia structures,
following (Cucerzan, 2007). We keep top 30 can-
didates by prominence P (e|m) for speed consid-
eration. The candidate generation recall are 94.0%
and 98.5% for TAC and AIDA respectively.
Analysis: Table 1 shows evaluation results
across several best performing systems. (Han et
al., 2011) is a collective approach, using Person-
alized PageRank to propagate evidence between
1available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
2available at http://www.mpi-inf.mpg.de/yago-naga/aida/
different decisions. To our surprise, our method
with only local evidence even beats several com-
plex collective methods with simple word similar-
ity. This reveals the importance of context model-
ing in semantic space. Collective approaches can
improve performance only when local evidence is
not confident enough. When embedding our sim-
ilarity measure sim(d, e) into (Han et al, 2011),
we achieve the best results on AIDA.
A close error analysis shows some typical er-
rors due to the lack of prominence feature and
name matching feature. Some queries acciden-
tally link to rare candidates and some link to en-
tities with completely different names. We will
add these features as mentioned in Eq. 5 in future.
We will also add NIL-detection module, which is
required by more realistic application scenarios.
A first thought is to construct pseudo-NIL with
Wikipedia annotations and automatically learn the
threshold and feature weight as in (Bunescu and
Pasca, 2006; Kulkarni et al, 2009).
Methods micro
P@1
macro
P@1
TAC 2010 eval
Lcc (2010) (top1, noweb) 79.22 -
Siel 2010 (top2, noweb) 71.57 -
our best 80.97 -
AIDA dataset (collective approaches)
AIDA (2011) 82.29 82.02
Shirakawa et al (2011) 81.40 83.57
Kulkarni et al (2009) 72.87 76.74
wordsim (cosine) 48.38 37.30
Han (2011) +wordsim 78.97 75.77
our best (non-collective) 84.82 83.37
Han (2011) + our best 85.62 83.95
Table 1: Evaluation on TAC and AIDA dataset.
4 Conclusion
We propose a deep learning approach that auto-
matically learns context-entity similarity measure
for entity disambiguation. The intermediate rep-
resentations are learned leveraging large scale an-
notations of Wikipedia, without any manual effort
of designing features. The learned representation
of entity is compact and can scale to very large
knowledge base. Furthermore, experiment reveals
the importance of context modeling in this field.
By incorporating our learned measure into collec-
tive approach, performance is further improved.
33
Acknowledgments
We thank Nan Yang, Jie Liu and Fei Wang for helpful discus-
sions. This research was partly supported by National High
Technology Research and Development Program of China
(863 Program) (No. 2012AA011101), National Natural Sci-
ence Foundation of China (No.91024009) and Major Na-
tional Social Science Fund of China(No. 12&ZD227).
References
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle.
2007. Greedy layer-wise training of deep networks.
Advances in neural information processing systems,
19:153.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 6, pages 708?716.
Y. Dauphin, X. Glorot, and Y. Bengio. 2011.
Large-scale learning of embeddings with recon-
struction sampling. In Proceedings of the Twenty-
eighth International Conference on Machine Learn-
ing (ICML11).
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In Proceedings of the 28th
International Conference on Machine Learning.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347?352. IEEE.
X. Han, L. Sun, and J. Zhao. 2011. Collective en-
tity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765?774. ACM.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of
named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782?792. Association for Com-
putational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and chal-
lenges. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1148?
1158, Portland, Oregon, USA, June. Association for
Computational Linguistics.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hier-
archical topic models. In Proceedings of KDD.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 457?
466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and
Y. Shi. 2010. Lcc approaches to knowledge base
population at tac 2010. In Proc. TAC 2010 Work-
shop.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
P. Sen. 2012. Collective context-aware topic mod-
els for entity disambiguation. In Proceedings of the
21st international conference on World Wide Web,
pages 729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011. Entity
disambiguation based on a probabilistic taxonomy.
Technical report, Technical Report MSR-TR-2011-
125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 354?
362. Association for Computational Linguistics.
P. Vincent, H. Larochelle, Y. Bengio, and P.A. Man-
zagol. 2008. Extracting and composing robust
features with denoising autoencoders. In Proceed-
ings of the 25th international conference on Ma-
chine learning, pages 1096?1103. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research, 11:3371?3408.
W. Zhang, Y.C. Sim, J. Su, and C.L. Tan. 2011. Entity
linking with effective acronym expansion, instance
selection and topic modeling. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages
1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-
aoyan Zhu. 2010. Learning to link entities with
knowledge base. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 483?491, Los Ange-
les, California, June. Association for Computational
Linguistics.
34
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 177?182,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving Chinese Word Segmentation on Micro-blog Using Rich
Punctuations
Longkai Zhang Li Li Zhengyan He Houfeng Wang? Ni Sun
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, li.l@pku.edu.cn, hezhengyan.hit@gmail.com,
wanghf@pku.edu.cn,sunny.forwork@gmail.com
Abstract
Micro-blog is a new kind of medium
which is short and informal. While no
segmented corpus of micro-blogs is avail-
able to train Chinese word segmentation
model, existing Chinese word segmenta-
tion tools cannot perform equally well
as in ordinary news texts. In this pa-
per we present an effective yet simple ap-
proach to Chinese word segmentation of
micro-blog. In our approach, we incor-
porate punctuation information of unla-
beled micro-blog data by introducing char-
acters behind or ahead of punctuations,
for they indicate the beginning or end of
words. Meanwhile a self-training frame-
work to incorporate confident instances is
also used, which prove to be helpful. Ex-
periments on micro-blog data show that
our approach improves performance, espe-
cially in OOV-recall.
1 INTRODUCTION
Micro-blog (also known as tweets in English) is
a new kind of broadcast medium in the form of
blogging. A micro-blog differs from a traditional
blog in that it is typically smaller in size. Further-
more, texts in micro-blogs tend to be informal and
new words occur more frequently. These new fea-
tures of micro-blogs make the Chinese Word Seg-
mentation (CWS) models trained on the source do-
main, such as news corpus, fail to perform equally
well when transferred to texts from micro-blogs.
For example, the most widely used Chinese seg-
menter ?ICTCLAS? yields 0.95 f-score in news
corpus, only gets 0.82 f-score on micro-blog data.
The poor segmentation results will hurt subse-
quent analysis on micro-blog text.
?Corresponding author
Manually labeling the texts of micro-blog is
time consuming. Luckily, punctuations provide
useful information because they are used as indi-
cators of the end of previous sentence and the be-
ginning of the next one, which also indicate the
start and the end of a word. These ?natural bound-
aries? appear so frequently in micro-blog texts that
we can easily make good use of them. TABLE 1
shows some statistics of the news corpus vs. the
micro-blogs. Besides, English letters and digits
are also more than those in news corpus. They
all are natural delimiters of Chinese characters and
we treat them just the same as punctuations.
We propose a method to enlarge the training
corpus by using punctuation information. We
build a semi-supervised learning (SSL) framework
which can iteratively incorporate newly labeled in-
stances from unlabeled micro-blog data during the
training process. We test our method on micro-
blog texts and experiments show good results.
This paper is organized as follows. In section 1
we introduce the problem. Section 2 gives detailed
description of our approach. We show the experi-
ment and analyze the results in section 3. Section
4 gives the related works and in section 5 we con-
clude the whole work.
2 Our method
2.1 Punctuations
Chinese word segmentation problem might be
treated as a character labeling problem which
gives each character a label indicating its position
in one word. To be simple, one can use label ?B?
to indicate a character is the beginning of a word,
and use ?N? to indicate a character is not the be-
ginning of a word. We also use the 2-tag in our
work. Other tag sets like the ?BIES? tag set are not
suiteable because the puctuation information can-
not decide whether a character after punctuation
should be labeled as ?B? or ?S?(word with Single
177
Chinese English Number Punctuation
News 85.7% 0.6% 0.7% 13.0%
micro-blog 66.3% 11.8% 2.6% 19.3%
Table 1: Percentage of Chinese, English, number, punctuation in the news corpus vs. the micro-blogs.
character).
Punctuations can serve as implicit labels for the
characters before and after them. The character
right after punctuations must be the first character
of a word, meanwhile the character right before
punctuations must be the last character of a word.
An example is given in TABLE 2.
2.2 Algorithm
Our algorithm ?ADD-N? is shown in TABLE 3.
The initially selected character instances are those
right after punctuations. By definition they are all
labeled with ?B?. In this case, the number of train-
ing instances with label ?B? is increased while the
number with label ?N? remains unchanged. Be-
cause of this, the model trained on this unbal-
anced corpus tends to be biased. This problem can
become even worse when there is inexhaustible
supply of texts from the target domain. We as-
sume that labeled corpus of the source domain can
be treated as a balanced reflection of different la-
bels. Therefore we choose to estimate the bal-
anced point by counting characters labeling ?B?
and ?N? and calculate the ratio which we denote
as ?. We assume the enlarged corpus is also bal-
anced if and only if the ratio of ?B? to ?N? is just
the same to? of the source domain.
Our algorithm uses data from source domain to
make the labels balanced. When enlarging corpus
using characters behind punctuations from texts
in target domain, only characters labeling ?B? are
added. We randomly reuse some characters label-
ing ?N? from labeled data until ratio? is reached.
We do not use characters ahead of punctuations,
because the single-character words ahead of punc-
tuations take the label of ?B? instead of ?N?. In
summary our algorithm tackles the problem by du-
plicating labeled data in source domain. We de-
note our algorithm as ?ADD-N?.
We also use baseline feature templates include
the features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). Our algorithm is not
necessarily limited to a specific tagger. For sim-
plicity and reliability, we use a simple Maximum-
Entropy tagger.
3 Experiment
3.1 Data set
We evaluate our method using the data from
weibo.com, which is the biggest micro-blog ser-
vice in China. We use the API provided by
weibo.com1 to crawl 500,000 micro-blog texts of
weibo.com, which contains 24,243,772 charac-
ters. To keep the experiment tractable, we first ran-
domly choose 50,000 of all the texts as unlabeled
data, which contain 2,420,037 characters. We
manually segment 2038 randomly selected micro-
blogs.We follow the segmentation standard as the
PKU corpus.
In micro-blog texts, the user names and URLs
have fixed format. User names start with ?@?, fol-
lowed by Chinese characters, English letters, num-
bers and ? ?, and terminated when meeting punc-
tuations or blanks. URLs also match fixed pat-
terns, which are shortened using ?http://t.
cn/? plus six random English letters or numbers.
Thus user names and URLs can be pre-processed
separately. We follow this principle in following
experiments.
We use the benchmark datasets provided by the
second International Chinese Word Segmentation
Bakeoff2 as the labeled data. We choose the PKU
data in our experiment because our baseline meth-
ods use the same segmentation standard.
We compare our method with three baseline
methods. The first two are both famous Chinese
word segmentation tools: ICTCLAS3 and Stan-
ford Chinese word segmenter4, which are widely
used in NLP related to word segmentation. Stan-
ford Chinese word segmenter is a CRF-based seg-
mentation tool and its segmentation standard is
chosen as the PKU standard, which is the same
to ours. ICTCLAS, on the other hand, is a HMM-
based Chinese word segmenter. Another baseline
is Li and Sun (2009), which also uses punctua-
tion in their semi-supervised framework. F-score
1http://open.weibo.com/wiki
2http://www.sighan.org/bakeoff2005/
3http://ictclas.org/
4http://nlp.stanford.edu/projects/
chinese-nlp.shtml\#cws
178
? ? ? ? ? ? ? ? ? ? ? ?
B - - - - - B - - - - -
B N B B N B B N B B N B
Table 2: The first line represents the original text. The second line indicates whether each character is
the Beginning of sentence. The third line is the tag sequence using ?BN? tag set.
ADD-N algorithm
Input: labeled data {(xi, yi)li?1}, unlabeled data {xj}l+uj=l+1.
1. Initially, let L = {(xi, yi)li?1} and U = {xj}l+uj=l+1.2. Label instances behind punctuations in U as ?B? and add them into
L.
3. Calculate ?B?, ?N? ratio ? in labeled data.
4. Randomly duplicate characters whose labels are ?N? in L to make
?B?/?N?= ?
5. Repeat:
5.1 Train a classifier f from L using supervised learning.
5.2 Apply f to tag the unlabeled instances in U .
5.3 Add confident instances from U to L.
Table 3: ADD-N algorithm.
is used as the accuracy measure. The recall of
out-of-vocabulary is also taken into consideration,
which measures the ability of the model to cor-
rectly segment out of vocabulary words.
3.2 Main results
Method P R F OOV-R
Stanford 0.861 0.853 0.857 0.639
ICTCLAS 0.812 0.861 0.836 0.602
Li-Sun 0.707 0.820 0.760 0.734
Maxent 0.868 0.844 0.856 0.760
No-punc 0.865 0.829 0.846 0.760
No-balance 0.869 0.877 0.873 0.757
Our method 0.875 0.875 0.875 0.773
Table 4: Segmentation performance with different
methods on the development data.
TABLE 4 summarizes the segmentation results.
In TABLE 4, Li-Sun is the method in Li and
Sun (2009). Maxent only uses the PKU data for
training, with neither punctuation information nor
self-training framework incorporated. The next 4
methods all require a 100 iteration of self-training.
No-punc is the method that only uses self-training
while no punctuation information is added. No-
balance is similar to ADD N. The only difference
between No-balance and ADD-N is that the for-
mer does not balance label ?B? and label ?N?.
The comparison of Maxent and No-punctuation
shows that naively adding confident unlabeled in-
stances does not guarantee to improve perfor-
mance. The writing style and word formation of
the source domain is different from target domain.
When segmenting texts of the target domain using
models trained on source domain, the performance
will be hurt with more false segmented instances
added into the training set.
The comparison of Maxent, No-balance and
ADD-N shows that considering punctuation as
well as self-training does improve performance.
Both the f-score and OOV-recall increase. By
comparing No-balance and ADD-N alone we can
find that we achieve relatively high f-score if we
ignore tag balance issue, while slightly hurt the
OOV-Recall. However, considering it will im-
prove OOV-Recall by about +1.6% and the f-
score +0.2%.
We also experimented on different size of un-
labeled data to evaluate the performance when
adding unlabeled target domain data. TABLE 5
shows different f-scores and OOV-Recalls on dif-
ferent unlabeled data set.
We note that when the number of texts changes
from 0 to 50,000, the f-score and OOV both are
improved. However, when unlabeled data changes
to 200,000, the performance is a bit decreased,
while still better than not using unlabeled data.
This result comes from the fact that the method
?ADD-N? only uses characters behind punctua-
179
Size P R F OOV-R
0 0.864 0.846 0.855 0.754
10000 0.872 0.869 0.871 0.765
50000 0.875 0.875 0.875 0.773
100000 0.874 0.879 0.876 0.772
200000 0.865 0.865 0.865 0.759
Table 5: Segmentation performance with different
size of unlabeled data
tions from target domain. Taking more texts into
consideration means selecting more characters la-
beling ?N? from source domain to simulate those
in target domain. If too many ?N?s are introduced,
the training data will be biased against the true dis-
tribution of target domain.
3.3 Characters ahead of punctuations
In the ?BN? tagging method mentioned above,
we incorporate characters after punctuations from
texts in micro-blog to enlarge training set.We also
try an opposite approach, ?EN? tag, which uses
?E? to represent ?End of word?, and ?N? to rep-
resent ?Not the end of word?. In this contrasting
method, we only use characters just ahead of punc-
tuations. We find that the two methods show sim-
ilar results. Experiment results with ADD-N are
shown in TABLE 6 .
Unlabeled ?BN? tag ?EN? tag
Data size F OOV-R F OOV-R
50000 0.875 0.773 0.870 0.763
Table 6: Comparison of BN and EN.
4 Related Work
Recent studies show that character sequence la-
beling is an effective formulation of Chinese
word segmentation (Low et al, 2005; Zhao et al,
2006a,b; Chen et al, 2006; Xue, 2003). These
supervised methods show good results, however,
are unable to incorporate information from new
domain, where OOV problem is a big challenge
for the research community. On the other hand
unsupervised word segmentation Peng and Schu-
urmans (2001); Goldwater et al (2006); Jin and
Tanaka-Ishii (2006); Feng et al (2004); Maosong
et al (1998) takes advantage of the huge amount
of raw text to solve Chinese word segmentation
problems. However, they usually are less accurate
and more complicated than supervised ones.
Meanwhile semi-supervised methods have been
applied into NLP applications. Bickel et al (2007)
learns a scaling factor from data of source domain
and use the distribution to resemble target do-
main distribution. Wu et al (2009) uses a Domain
adaptive bootstrapping (DAB) framework, which
shows good results on Named Entity Recognition.
Similar semi-supervised applications include Shen
et al (2004); Daume? III and Marcu (2006); Jiang
and Zhai (2007); Weinberger et al (2006). Be-
sides, Sun and Xu (2011) uses a sequence labeling
framework, while unsupervised statistics are used
as discrete features in their model, which prove to
be effective in Chinese word segmentation.
There are previous works using punctuations as
implicit annotations. Riley (1989) uses it in sen-
tence boundary detection. Li and Sun (2009) pro-
posed a compromising solution to by using a clas-
sifier to select the most confident characters. We
do not follow this approach because the initial er-
rors will dramatically harm the performance. In-
stead, we only add the characters after punctua-
tions which are sure to be the beginning of words
(which means labeling ?B?) into our training set.
Sun and Xu (2011) uses punctuation information
as discrete feature in a sequence labeling frame-
work, which shows improvement compared to the
pure sequence labeling approach. Our method
is different from theirs. We use characters after
punctuations directly.
5 Conclusion
In this paper we have presented an effective yet
simple approach to Chinese word segmentation on
micro-blog texts. In our approach, punctuation in-
formation of unlabeled micro-blog data is used,
as well as a self-training framework to incorpo-
rate confident instances. Experiments show that
our approach improves performance, especially in
OOV-recall. Both the punctuation information and
the self-training phase contribute to this improve-
ment.
Acknowledgments
This research was partly supported by Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101), National Natural Science Foun-
dation of China (No.91024009) and Major
National Social Science Fund of China(No.
12&ZD227).
180
References
Bickel, S., Bru?ckner, M., and Scheffer, T. (2007).
Discriminative learning for differing training
and test distributions. In Proceedings of the 24th
international conference on Machine learning,
pages 81?88. ACM.
Chen, W., Zhang, Y., and Isahara, H. (2006). Chi-
nese named entity recognition with conditional
random fields. In 5th SIGHAN Workshop on
Chinese Language Processing, Australia.
Daume? III, H. and Marcu, D. (2006). Domain
adaptation for statistical classifiers. Journal of
Artificial Intelligence Research, 26(1):101?126.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, pages 673?680. Association for Computa-
tional Linguistics.
Jiang, J. and Zhai, C. (2007). Instance weight-
ing for domain adaptation in nlp. In Annual
Meeting-Association For Computational Lin-
guistics, volume 45, page 264.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsuper-
vised segmentation of chinese text by use of
branching entropy. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 428?435. Association for Computational
Linguistics.
Li, Z. and Sun, M. (2009). Punctuation as im-
plicit annotations for chinese word segmenta-
tion. Computational Linguistics, 35(4):505?
512.
Low, J., Ng, H., and Guo, W. (2005). A maximum
entropy approach to chinese word segmenta-
tion. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
Pan, S. and Yang, Q. (2010). A survey on trans-
fer learning. Knowledge and Data Engineering,
IEEE Transactions on, 22(10):1345?1359.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
vances in Intelligent Data Analysis, pages 238?
247.
Riley, M. (1989). Some applications of tree-based
modelling to speech and language. In Pro-
ceedings of the workshop on Speech and Nat-
ural Language, pages 339?352. Association for
Computational Linguistics.
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan,
C. (2004). Multi-criteria-based active learning
for named entity recognition. In Proceedings
of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589. Associa-
tion for Computational Linguistics.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Association
for Computational Linguistics.
Weinberger, K., Blitzer, J., and Saul, L. (2006).
Distance metric learning for large margin near-
est neighbor classification. In In NIPS. Citeseer.
Wu, D., Lee, W., Ye, N., and Chieu, H. (2009).
Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009
Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume
3, pages 1523?1532. Association for Computa-
tional Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Zhao, H., Huang, C., and Li, M. (2006a). An im-
proved chinese word segmentation system with
181
conditional random field. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language
Processing, volume 117. Sydney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field model-
ing. In Proceedings of PACLIC, volume 20,
pages 87?94.
182
A Pipeline Approach to Chinese Personal Name
Disambiguation
Yang Song, Zhengyan He, Chen Chen, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{ysong, hezhengyan, chenchen, wanghf}@pku.edu.cn
Abstract
In this paper, we describe our sys-
tem for Chinese personal name dis-
ambiguation task in the first CIPS-
SIGHAN joint conference on Chinese
Language Processing(CLP2010). We
use a pipeline approach, in which pre-
processing, unrelated documents dis-
carding, Chinese personal name exten-
sion and document clustering are per-
formed separately. Chinese personal
name extension is the most important
part of the system. It uses two addi-
tional dictionaries to extract full per-
sonal names in Chinese text. And then
document clustering is performed un-
der different personal names. Exper-
imental results show that our system
can achieve good performances.
1 Introduction
Personal name search is one of the most im-
portant tasks for search engines. When a per-
sonal name query is given to a search engine,
a list of related documents will be shown. But
not all of the returned documents refer to the
same person whom users want to find. For ex-
ample, the query name ?jordan? is submitted
to a search engine, we can get a lot of doc-
uments containing ?jordan?. Some of them
may refer to the computer scientist, others
perhaps refer to the basketball player. For
English, there have been three Web People
Search (WePS1) evaluation campaigns on per-
sonal name disambiguation. But for Chinese,
1http://nlp.uned.es/weps/
this is the first time. It encounters more chal-
lenge for Chinese personal name disambigua-
tion. There are no word boundary in Chinese
text, so it becomes difficult to recognize the
full personal names from Chinese text. For ex-
ample, a query name ???? is given, but the
full personal name from some documents may
be an extension of ????, like ????? or
?????, and so on. Meanwhile, ???? can
also be a common Chinese word. So we need
to discard those documents which are not ref-
ered to any person related to the given query
name.
To solve the above-mentioned problem, we
explore a pipeline approach to Chinese per-
sonal name disambiguation. The overview of
our system is illustrated in Figure 1. We split
this task into four parts: preprocessing, unre-
lated documents discarding, Chinese personal
name extension and document clustering. In
preprocessing and unrelated documents dis-
carding, we use word segmentation and part-
of-speech tagging tools to process the given
dataset and documents are discarded when
the given query name is not tagged as a per-
sonal name or part of a personal name. After
that we perform personal name extension in
the documents for a given query name. When
the query name has only two characters. We
extend it to the left or right for one character.
For example, we can extend ???? to ???
?? or ?????. The purpose of extending
the query name is to obtain the full personal
name. In this way, we can get a lot of full per-
sonal names for a given query name from the
documents. And then document clustering
Figure 1: Overview of the System
is performed under different personal names.
HAC (Hierarchical Agglomerative Clustering)
is selected here. We represent documents with
bag of words and solve the problem in vector
space model, nouns, verbs, bigrams of nouns
or verbs and named entities are selected as
features. The feature weight value takes 0 or
1. In HAC, we use group-average link method
as the distance measure and consine similar-
ity as the similarity computing measure. The
stopping criteria is dependent on a threshold
which is obtained from training data. Our sys-
tem produces pretty good results in the final
evaluation.
The remainder of this paper is organized as
follows. Section 2 introduces related work.
Section 3 gives a detailed description about
our pipeline approach. It includes preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering. Section 4 presents the experimental
results. The conclusions are given in Section
5.
2 Related Work
Several important studies have tried to
solve the task introduced in the previous sec-
tion. Most of them treated it as an cluster-
ing problem. Bagga & Baldwin (1998) first
selected tokens from local context as features
to perform intra-document coreference resolu-
tion. Mann & Yarowsky (2003) extracted lo-
cal biographical information as features. Niu
et al (2004) used relation extraction results
in addition to local context features and get a
perfect results. Al-Kamha and Embley (2004)
clustered search results with feature set in-
cluding attributes, links and page similarities.
In recent years, this problem has attracted
a great deal of attention from many research
institutes. Ying Chen et al (2009) used a
Web 1T 5-gram corpus released by Google
to extract additional features for clustering.
Masaki Ikeda et al (2009) proposed a two-
stage clustering algorithm to improve the low
recall values. In the first stage, some reliable
features (like named entities) are used to con-
nect documents about the same person. Af-
ter that, the connected documents (document
cluster) are used as a source from which new
features (compound keyword features) are ex-
tracted. These new features are used in the
second stage to make additional connections
between documents. Their approach is to im-
prove clusters step by step, where each step
refines clusters conservatively. Han & Zhao
(2009) presented a system named CASIANED
to disambiguate personal names based on pro-
fessional categorization. They first catego-
rize different personal name appearances into
a real world professional taxonomy, and then
the personal name appearances are clustered
into a single cluster. Chen Chen et al (2009)
explored a novel feature weight computing
method in clustering. It is based on the point-
wise mutual information between the ambigu-
ous name and features. In their paper, they
also develop a trade-off point based cluster
stopping criteria which find the trade-off point
between intra-cluster compactness and inter-
cluster separation.
Our approach is based on Chinese per-
sonal name extension. We recognize the full
personal names in Chinese text and perform
document clustering under different personal
names.
3 Methodology
In this section, we will explain preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering in order.
3.1 Preprocessing
We use ltp-service2 to process the given Chi-
nese personal name disambiguation dataset
(a detailed introduction to it will be given
in section 4). Training data in the dataset
contains 32 query names. There are 100-300
documents under every query name. All the
documents are collected from Xinhua News
Agency. They contain the exact same string
as query names. Ltp-service is a web ser-
vice interface for LTP3(Language Technology
Platform). LTP has integrated many Chinese
processing modules, including word segmen-
tation, part-of-speech tagging, named entity
recognition, word sense disambiguation, and
so on. Jun Lang et al (2006) give a detailed
introduction to LTP. Here we only use LTP
to generate word segmentation, part-of-speech
tagging and named entity recognition results
for the given dataset.
3.2 Unrelated documents discarding
Under every query name, there are 100-300
documents. But not all of them are really re-
lated. For example, ???? is a query name in
training data. In corresponding documents,
some are refered to real personal names like
???? or ?????. But others may be a sub-
string of an expression such as ??????
??. These documents are needed to be fil-
tered out. We use the preprocessing tool LTP
to slove this problem. LTP can do word seg-
mentation and part-of-speech tagging for us.
For each document under a given query name,
if the query name in the document is tagged as
a personal name or part of some extended per-
sonal name, the document will be marked as
undiscarded, otherwise the document will be
discarded. Generally speaking, for the query
name containing three characters, we don?t
need to discard any of the corresponding doc-
uments. But in practice, we find that for some
query names, LTP always gives the invariable
2http://code.google.com/p/ltp-service/
3http://ir.hit.edu.cn/ltp/
part-of-speech. For example, no matter what
the context of ???? is, it is always tagged
as a geographic name. So we use another pre-
processing tool ICTCLAS4. Only when both
of them mark one document as discarded, we
discard the corresponding document.
3.3 Chinese personal name extension
After discarding unrelated documents, we
need to recognize the full Chinese personal
names. We hypothesize that the full Chinese
personal name has not more than three char-
acters (We don?t consider the compound sur-
names here). So the query names containing
only two Chinese characters are considered to
extend. In our approach, we use two Chinese
personal names dictionaries. One is a sur-
name dictionary containing 423 one-character
entries. We use it to do left extend for the
query name. For example, the query name
is ???? and its left character in a docu-
ment is ???, we will extend it to full per-
sonal name ?????. The other is a non-
ending Chinese character dictionary contain-
ing 64 characters which could not occur at the
end of personal names. It is constructed by a
personal title dictionary. We use every title?s
first character and some other special charac-
ters (such as numbers or punctuations) to con-
stuct the dictionary. Some manual work has
also been done to filter a few incorrect charac-
ters. Several examples of the two dictionaries
are shown in Table 1.
Through the analysis of Xinhua News arti-
cles, we also find that nearly half of the docu-
ments under given query name actually refer
to the reporters. And they often appear in
the first or last brackets in the body of cor-
responding document. For example, ?(??
????????)? is a sentence containing
query name ????. We use some simple but
efficient rules to get full personal names for
this case.
3.4 Document clustering
For every query name, we can get a list of
full peronal names. For example, when the
4http://ictclas.org/
Table 1: Several Examples of the two Dictionaries
Dictionaries Examples
Surnames ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
Non-ending Chinese characters ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
query name is ????, we can get the per-
sonal names like ?????, ?????, ???
??, ?????. And then document clustering
is performed under different personal names.
3.4.1 Features
We use bag of words to represent docu-
ments. Some representative words need to be
chosen as features. LTP can give us POS tag-
ging and NER results. We select all the nouns,
verbs and named entities which appear in the
same paragraph with given query name as fea-
tures. Meanwhile, the bigrams of nouns or
verbs are also selected. We take 0 or 1 for
feature weight value. 0 represents that the
feature doesn?t appear in corresponding para-
graphs, and 1 represents just the opposite. We
find that this weighting scheme is more effec-
tive than TFIDF.
3.4.2 Clustering
All features are represented in vector space
model. Every document is modeled as a ver-
tex in the vector space. So every document
can be seen as a feature vector. Before cluster-
ing, the similarity between documents is com-
puted by cosine value of the angle between
feature vectors. We use HAC to do document
clustering. It is a bottom-up algorithm which
treats each document as a singleton cluster at
the outset and then successively merges (or
agglomerates) pairs of clusters until all clus-
ters have been merged into a single cluster
that contains all documents. From our ex-
perience, single link and group-average link
method seem to work better than complete
link one. We use group-average link method
in the final submission. The stopping criteria
is a difficult problem for clustering. Here we
use a threshold for terminating condition. So
it is not necessary to determine the number
of clusters beforehand. We select a threshold
which produces the best performance in train-
ing data.
4 Experimental Results
The dataset for Chinese personal name dis-
ambiguation task contains training data and
testing data. The training data contains
32 query names. Every query name folder
contains 100-300 news articles. Given the
query name, all the documents are retrived
by character-based matching from a collection
of Xinhua news documents in a time span of
fourteen years. The testing data contains 25
query names. Two threshold values as termi-
nating conditions are obtained from training
data. They are 0.4 and 0.5. For evaluation,
we use P-IP score and B-cubed score (Bagga
and Baldwin, 1998). Table 2 & Table 3 show
the official evaluation results.
Table 2: Official Results for P-IP score
Threshold P-IP
P IP F score
0.4 88.32 94.9 91.15
0.5 91.3 91.77 91.18
Table 3: Official Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 83.68 92.23 86.94
0.5 87.87 87.49 86.84
Besides the formal evaluation, the organizer
also provide a diagnosis test designed to ex-
plore the relationship between Chinese word
segmentation and personal name disambigua-
tion. That means the query names in the
documents are segmented correctly by manual
work. Table 4 & Table 5 show the diagnosis
results.
Table 4: Diagnosis Results for P-IP score
Threshold P-IP
P IP F score
0.4 89.01 95.83 91.96
0.5 91.85 92.68 91.96
Table 5: Diagnosis Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 84.53 93.42 87.96
0.5 88.59 88.59 87.8
The official results show that our method
performs pretty good. The diagnosis results
show that correct word segmentation can im-
prove the evaluation results. But the improve-
ment is rather limited. That is mainly because
Chinese personal name extension is done well
in our approach. So the diagnosis results don?t
gain much profit from query names? correct
segmentation.
5 Conclusions
We describe our framework in this paper.
First, we use LTP to do preprocessing for orig-
inal dataset which comes from Xinhua news
articles. LTP can produce good results for
Chinese text processing. And then we use
two additional dictionaries(one is Chinese sur-
name dictionary, the other is Non-ending Chi-
nese character dictionary) to do Chinese per-
sonal name extension. After that we perform
document clustering under different personal
names. Official evaluation results show that
our method can achieve good performances.
In the future, we will attempt to use other
features to represent corresponding persons in
the documents. We will also investigate auto-
matic terminating condition.
6 Acknowledgments
This research is supported by National
Natural Science Foundation of Chinese
(No.60973053) and Research Fund for the
Doctoral Program of Higher Education of
China (No.20090001110047).
References
J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS
2 evaluation campaign: overview of the web peo-
ple search clustering task. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Bagga and B. Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 17th Interna-
tional Conference on Computational Linguis-
tics, 79?85.
Mann G. and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings
of CoNLL-2003, 33?40, Edmonton, Canada.
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly
Supervised Learning for Cross-document Person
Name Disambiguation Supported by Informa-
tion Extraction. In Proceedings of ACL 2004.
Al-Kamha. R. and D. W. Embley. 2004. Group-
ing search-engine returned citations for person-
name queries. In Proceedings of WIDM 2004,
96-103, Washington, DC, USA.
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren
Huang. 2009. PolyUHK:A Robust Information
Extraction System for Web Personal Names.
In 2nd Web People Search Evaluation Work-
shop(WePS 2009), 18th WWW Conference.
Masaki Ikeda, Shingo Ono, Issei Sato, Minoru
Yoshida, and Hiroshi Nakagawa. 2009. Person
Name Disambiguation on the Web by Two-Stage
Clustering. In 2nd Web People Search Evalua-
tion Workshop(WePS 2009), 18th WWW Con-
ference.
Xianpei Han and Jun Zhao. 2009. CASIANED:
Web Personal Name Disambiguation Based on
Professional Categorization. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Chen Chen, Junfeng Hu, and Houfeng Wang.
2009. Clustering technique in multi-document
personal name disambiguation. In Proceed-
ings of the ACL-IJCNLP 2009 Student Research
Workshop, pages 88?95.
Jun Lang, Ting Liu, Huipeng Zhang and Sheng Li.
2006. LTP: Language Technology Platform. In
Proceedings of SWCL 2006.
Bagga, Amit and B. Baldwin. 1998. Algorithms
for scoring co-reference chains. In Proceedings
of the First International Conference on Lan-
guage Resources and Evaluation Workshop on
Linguistic co-reference.
Applying Spectral Clustering for Chinese Word Sense Induction
Zhengyan He, Yang Song, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{hezhengyan, ysong, wanghf}@pku.edu.cn
Abstract
Sense Induction is the process of identify-
ing the word sense given its context, often
treated as a clustering task. This paper ex-
plores the use of spectral cluster method
which incorporates word features and n-
gram features to determine which cluster
the word belongs to, each cluster repre-
sents one sense in the given document set.
1 Introduction
Word Sense Induction(WSI) is defined as the
process of identifying different senses of a tar-
get word in a given context in an unsupervised
method. It?s different from word sense disam-
biguation(WSD) in that senses in WSD are as-
sumed to be known. The disadvantage of WSD
is that it derives the senses of word from existing
dictionaries or other corpus and the senses cannot
be extended to other domains. WSI can overcome
this problem as it can automatically derive word
senses from the given document set, or a specific
domain.
Many different approaches based on co-
occurence have been proposed so far. Bordag
(2006) proposes an approach that uses triplets
of co-occurences. The most significant co-
occurences of target word are used to build triplets
that consist of the target word and its two co-
occurences. Then intersection built from the co-
occurence list of each word in the triplet is used
as feature vector. After merging similar triplets
that have more than 80% overlapping words, clus-
tering is performed on the triplets. Triplets with
fewer than 4 intersection words are removed in or-
der to reduce noise.
LDA model has also been applied to WSI
(Brody and Lapata, 2009). Brody proposes a
method that treats document and topics in LDA
as word context and senses respectively. The pro-
cess of generating the context words is as follows:
first generate sense from a multinomial distribu-
tion given context, then generate context words
given sense. They also derive a layered model
to incorporate different kind of features and use
Gibbs sampling method to solve the problem.
Graph-based methods become popular recently.
These methods use the co-occurence graph of
context words to obtain sense clusters based on
sub-graph density. Markov clustering(MCL) has
been used to identify dense regions of graph
(Agirre and Soroa, 2007).
Spectral clustering performs well on problems
in which points cluster based on shape. The
method is that first compute the Laplace matrix
of the affinity matrix, then reform the data points
by stacking the largest eigenvectors of the Laplace
matrix in columns, finally cluster the new data
points using a more simple clustering method like
k-means (Ng et al, 2001).
2 Methodology
Our approach follows a common cluster model
that represents the given context as a word vec-
tor and later uses a spectral clustering method to
group each instance in its own cluster.
Different types of polysemy may arise and the
most significant distinction may be the syntactic
classes of the word and the conceptually differ-
ent senses (Bordag, 2006). Thus we must extract
the features able to distinguish these differences.
They are:
Local tokens: the word occuring in the window
-3 ? +3;
Local bigram feature: bigram within -5 ? +5
Chinese character range;
The above two features model the syntactic us-
age of a specific sense of a Chinese word.
Topical or conceptual feature: the content
words (pos-tagged as noun, verb, adjective) within
the given sentence. As the sentence in the training
set seems generally short, a short window may not
contains enough infomation.
We represent the words in a 0-1 vector accord-
ing to their existence in a given sentence. Then the
similarity measure between two given sentences is
derived from their cosine similarity. We find that it
is difficult to define the relative importance of dif-
ferent types of features in order to combine them
in one vector space, and find that ignoring weight
achieve better result. Brody (2009) achieves this
in LDA model through a layered model with dif-
ferent probability of feature given sense.
Later we use a spectral clustering method from
R kernlab package (Karatzoglou et al, 2004)
which implements the algorithm described in (Ng
et al, 2001). Instead of using the Gaussian kernel
matrix as the similarity matrix we use the cosine
similarity derived above.
One observation is that instances with the same
target word sense often appear in the same con-
text. However, for some verb in Chinese, it is of-
ten the case that one sense relates to a concrete
object while the other relates to a more broad and
abstract concept and the context varies consider-
ably. Simple word co-occurence cannot define a
good similarity measure to group these cases into
one cluster. We must consider semantic related-
ness measures between contexts.
3 Performance
Our system performs well on the training set. Two
methods are used to evaluate the performance un-
der different features.
method precision recall F-score
Purity-based 81.11 83.19 81.99
B-cubed 74.41 76.51 75.33
Table 1: The performance of training set
Our system finally gets a F-score of 0.7598 on
the test set.
4 Conclusion
Our experiment in the Chinese word sense induc-
tion task performs good with respect to the relative
small corpus(only the training set). But only con-
sidering token co-occurence cannot achieve better
result. Moreover, it is difficult to define a simi-
larity measure solely based on lexicon infomation
with no regard to semantic relatedness. Finally,
combining different types of features seems to be
another challenge in our model.
5 Acknowledgments
This research is supported by National Natural
Science Foundation of Chinese (No.9092001).
References
Agirre, Eneko and Aitor Soroa. 2007. Ubc-as: a graph
based unsupervised system for induction and classi-
fication. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 346?349, Morristown, NJ, USA. Association
for Computational Linguistics.
Bordag, Stefan. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In
EACL. The Association for Computer Linguistics.
Brody, Samuel and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103?111.
The Association for Computer Linguistics.
Karatzoglou, Alexandros, Alex Smola, Kurt Hornik,
and Achim Zeileis. 2004. kernlab ? an S4 pack-
age for kernel methods in R. Journal of Statistical
Software, 11(9):1?20.
Ng, Andrew Y., Michael I. Jordan, and Yair Weiss.
2001. On spectral clustering: Analysis and an al-
gorithm. In Advances in Neural Information Pro-
cessing Systems 14, pages 849?856. MIT Press.
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 131?135,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Link Type Based Pre-Cluster Pair Model for Coreference Resolution
Yang Song?, Houfeng Wang? and Jing Jiang?
?Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
?School of Information Systems, Singapore Management University, Singapore
{ysong, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg
Abstract
This paper presents our participation in the
CoNLL-2011 shared task, Modeling Unre-
stricted Coreference in OntoNotes. Corefer-
ence resolution, as a difficult and challenging
problem in NLP, has attracted a lot of atten-
tion in the research community for a long time.
Its objective is to determine whether two men-
tions in a piece of text refer to the same en-
tity. In our system, we implement mention de-
tection and coreference resolution seperately.
For mention detection, a simple classification
based method combined with several effective
features is developed. For coreference resolu-
tion, we propose a link type based pre-cluster
pair model. In this model, pre-clustering of all
the mentions in a single document is first per-
formed. Then for different link types, different
classification models are trained to determine
wheter two pre-clusters refer to the same en-
tity. The final clustering results are generated
by closest-first clustering method. Official test
results for closed track reveal that our method
gives a MUC F-score of 59.95%, a B-cubed
F-score of 63.23%, and a CEAF F-score of
35.96% on development dataset. When using
gold standard mention boundaries, we achieve
MUC F-score of 55.48%, B-cubed F-score of
61.29%, and CEAF F-score of 32.53%.
1 Introduction
The task of coreference resolution is to recognize
all the mentions (also known as noun phrases, in-
cluding names, nominal mentions and pronouns)
in a text and cluster them into equivalence classes
where each quivalence class refers to a real-world
entity or abstract concept. The CoNLL-2011 shared
task1 uses OntoNotes2 as the evaluation corpus. The
coreference layer in OntoNotes constitutes one part
of a multi-layer, integrated annotation of the shal-
low semantic structures in the text with high inter-
annotator agreement. In addition to coreference,
this data set is also tagged with syntactic trees, high
coverage verb and some noun propositions, partial
verb and noun word senses, and 18 named entity
types. The main difference between OntoNotes and
another wellknown coreference dataset ACE is that
the former does not label any singleton entity clus-
ter, which has only one reference in the text. We can
delete all the singleton clusters as a postprocessing
step for the final results. Alternatively, we can also
first train a classifier to separate singleton mentions
from the rest and apply this mention detection step
before coreference resolution. In this work we adopt
the second strategy.
In our paper, we use a traditional learning based
pair-wise model for this task. For mention detec-
tion, we first extract all the noun phrases in the text
and then use a classification model combined with
some effective features to determine whether each
noun phrase is actually a mention. The features in-
clude word features, POS features in the given noun
phrase and its context, string matching feature in
its context, SRL features, and named entity features
among others. More details will be given in Sec-
tion 3. From our in-house experiments, the final F-
scores for coreference resolution can be improved
by this mention detection part. For coreference res-
1http://conll.bbn.com
2http://www.bbn.com/ontonotes/
131
Features describing ci or cj
Words The first and last words of the given NP in ci ( or cj) , also including the words in the
context with a window size 2
POS Tags The part of speech tags corresponding to the words
Pronoun Y if mentions in ci( or cj) are pronouns; else N
Definite Y if mentions in ci( or cj) are definite NP; else N
Demonstrative Y if mentions in ci( or cj) are demonstrative NP; else N
Number Singular or Plural, determined using a data file published by Bergsma and Lin (2006)
Gender Male, Female, Neuter, or Unknown, determined using a data file published by Bergsma
and Lin (2006)
Semantic Class Semantic Classes are given by OntoNotes for named entities
Mentino Type Common Noun Phrases or Pronouns
Table 1: The feature set describing ci or cj .
olution, a traditinal pair-wise model is applied, in
which we first use exact string matching to generate
some pre-clusters. It should be noted that each pro-
noun must be treated as a singleton pre-cluster, be-
cause they are not like names or nominal mentions,
which can be resolved effectively with exact string
matching. We then implement a classification based
pre-cluster pair model combined with several ef-
fective coreference resolution features to determine
whether two pre-clusters refer to the same entity. Fi-
nally, we use closest-first clustering method to link
all the coreferential pre-clusters and generate the fi-
nal cluster results. As mentioned before, mentions
have three types: names, nominal mentions and pro-
nouns. Among them pronouns are very different
from names and nominal mentions, because they can
only supply limited information literally. So we de-
fine three kinds of link types for pre-cluster pairs:
NP-NP link, NP-PRP link and PRP-PRP link. (Here
NP means Noun Phrases and PRP means Pronom-
inal Phrases.) One link represents one pre-cluster
pair. Intuitively, different link types tend to use dif-
ferent features to determine whether this kind of link
is coreferential or not. We implement three kinds
of pre-cluster pair model based on three link types.
Experimental results show that combined with out-
puts from different link type based pre-cluster pair
model can give better results than using an uni-
fied classification model for three different kinds of
link types. For all the classification models, we use
opennlp.maxent3 package.
The rest of this paper is organized as follows. Sec-
tion 2 describes our mention detection method. We
discuss our link type based pre-cluster pair model
for coreference resolution in Section 3, evaluate it in
Section 4, and conclude in Section 5.
2 Mention Detection
We select all the noun phrases tagged by the
OntoNotes corpus as mention candidates and im-
plement a classification-based model combined
with several commonly used features to determine
whether a given noun phrase is a mention. The fea-
tures are given below:
? Word Features - They include the first word and the
last word in each given noun phrase. We also use
words in the context of the noun phrase within a
window size of 2.
? POS Features - We use the part of speech tags of
each word in the word features.
? Position Features - These features indicate where
the given noun phrase appears in its sentence: be-
gining, middle, or end.
? SRL Features - The Semantic Role of the given
noun phrase in its sentence.
? Verb Features - The verb related to the Semantic
Role of the given noun phrase.
3http://incubator.apache.org/opennlp/
132
Features describing the relationship between ci and cj
Distance The minimum distance between mentions in ci and cj
String Match Y if mentions are the same string; else N
Substring Match Y if one mention is a substring of another; else N
Levenshtein Distance Levenshtein Distance between the mentions
Number Agreement Y if the mentions agree in number; else N
Gender Agreement Y if the mentions agree in gender; else N
N & G Agreement Y if mentions agree in both number and gender; else N
Both Pronouns Y if the mentions are both pronouns; else N
Verb Agreement Y if the mentions have the same verb.
SRL Agreement Y if the mentions have the same semantic role
Position Agreement Y if the mentions have the same position (Beginning, Middle or End) in sentences
Table 2: The feature set describing the relationship between ci and cj .
? Entity Type Features - The named entity type for the
given noun phrase.
? String Matching Features - True if there is another
noun phrase wich has the same string as the given
noun phrase in the context.
? Definite NP Features - True if the given noun phrase
is a definite noun phrase.
? Demonstrative NP Features - True if the given noun
phrase is a demonstrative noun phrase.
? Pronoun Features - True if the given noun phrase is
a pronoun.
Intutively, common noun phrases and pronouns
might have different feature preferences. So we train
classification models for them respectively and use
the respective model to predicate for common noun
phrases or pronouns. Our mention detection model
can give 52.9% recall, 80.77% precision and 63.93%
F-score without gold standard mention boundaries
on the development dataset. When gold standard
mention boundaries are used, the results are 53.41%
recall, 80.8% precision and 64.31% F-score. (By us-
ing the gold standard mention boundaries, we mean
we use the gold standard noun phrase boundaries.)
3 Coreference Resolution
After getting the predicated mentions, we use some
heuristic rules to cluster them with the purpose of
generating highly precise pre-clusters. For this task
Metric Recall Precision F-score
MUC 49.64% 67.18% 57.09%
BCUBED 59.42% 70.99% 64.69%
CEAF 45.68% 30.56% 36.63%
AVERAGE 51.58% 56.24% 52.80%
Table 3: Evaluation results on development dataset with-
out gold mention boundaries
Metric Recall Precision F-score
MUC 48.94% 67.72% 56.82%
BCUBED 58.52% 72.61% 64.81%
CEAF 46.49% 30.45% 36.8%
AVERAGE 51.32% 56.93% 52.81%
Table 4: Evaluation results on development dataset with
gold mention boundaries
only identity coreference is considered while attribu-
tive NP and appositive construction are excluded.
That means we cannot use these two important
heuristic rules to generate pre-clusters. In our sys-
tem, we just put all the mentions (names and nomi-
nal mentions, except pronouns) which have the same
string into the identical pre-clusters. With these pre-
clusters and their coreferential results, we imple-
ment a classification based pre-cluster pair model to
determine whether a given pair of pre-clusters re-
fer to the same entity. We follow Rahman and Ng
(2009) to generate most of our features. We also
include some other features which intuitively seem
effective for coreference resolution. These features
133
Metric Recall Precision F-score
MUC 42.66% 53.7% 47.54%
BCUBED 61.05% 74.32% 67.04%
CEAF 40.54% 32.35% 35.99%
AVERAGE 48.08% 53.46% 50.19%
Table 5: Evaluation results on development dataset
with gold mention boundaries using unified classification
model
Metric Recall Precision F-score
MUC 53.73% 67.79% 59.95%
BCUBED 60.65% 66.05% 63.23%
CEAF 43.37% 30.71% 35.96%
AVERAGE 52.58% 54.85% 53.05%
Table 6: Evaluation results on test dataset without gold
mention boundaries
are shown in Table 1 and Table 2. For simplicity, we
use ci and cj to represent pre-clusters i and j. Each
pre-cluster pair can be seen as a link. We have three
kinds of link types: NP-NP link, NP-PRP link and
PRP-PRP link. Different link types may have differ-
ent feature preferences. So we train the classifica-
tion based pre-cluster pair model for each link type
separately and use different models to predicate the
results. With the predicating results for pre-cluster
pairs, we use closest-first clustering to link them and
form the final cluster results.
4 Experimental Results
We present our evaluation results on development
dataset for CoNLL-2011 shared Task in Table 3, Ta-
ble 4 and Table 5. Official test results are given
in Table 6 and Table 7. Three different evaluation
metrics were used: MUC (Vilain et al, 1995), B3
(Bagga and Baldwin, 1998) and CEAF (Luo, 2005).
Finally, the average scores of these three metrics are
used to rank the participating systems. The differ-
ence between Table 3 and Table 4 is whether gold
standard mention boundaries are given. Here ?men-
tion boundaries? means a more broad concept than
the mention definition we gave earlier. We should
also detect real mentions from them. From the ta-
bles, we can see that the scores can be improved litt-
tle by using gold standard mention boundaries. Also
the results from Table 5 tell us that combining differ-
ent link-type based classification models performed
Metric Recall Precision F-score
MUC 46.66% 68.40% 55.48%
BCUBED 54.40% 70.19% 61.29%
CEAF 43.77% 25.88% 32.53%
AVERAGE 48.28% 54.82% 49.77%
Table 7: Evaluation results on test dataset with gold men-
tion boundaries
better than using an unified classification model. For
official test results, our system did not perform as
well as we had expected. Some possible reasons are
as follows. First, verbs that are coreferential with a
noun phrase are also tagged in OntoNotes. For ex-
ample, ?grew ? and ?the strong growth? should be
linked in the following case: ?Sales of passenger
cars grew 22%. The strong growth followed year-
to-year increases.? But we cannot solve this kind
of problem in our system. Second, we should per-
form feature selection to avoid some useless features
harming the scores. Meanwhile, we did not make
full use of the WordNet, PropBank and other back-
ground knowledge sources as features to represent
pre-cluster pairs.
5 Conclusion
In this paper, we present our system for CoNLL-
2011 shared Task, Modeling Unrestricted Corefer-
ence in OntoNotes. First some heuristic rules are
performed to pre-cluster all the mentions. And then
we use a classification based pre-cluster pair model
combined with several cluster level features. We
hypothesize that the main reason why we did not
achieve good results is that we did not carefully ex-
amine the features and dropped the feature selec-
tion procedure. Specially, we did not make full use
of background knowledge like WordNet, PropBank,
etc. In our future work, we will make up for the
weakness and design a more reasonable model to ef-
fectively combine all kinds of features.
Acknowledgments
This research is supported by National Natu-
ral Science Foundation of Chinese (No.60973053,
No.91024009) and Research Fund for the Doc-
toral Program of Higher Education of China
(No.20090001110047).
134
References
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A Model-Theoretic Coreference
Scoring Scheme. In Proceedings of the Sixth Message
Understanding Conference (MUC-6), pages 4552, San
Francisco, CA. Morgan Kaufmann.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the 1st
International Conference on Language Resources and
Evaluation, Granada, Spain, pp. 563566.
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Vancouver, B.C., Canada, pp. 2532.
Vincent Ng. 2008. Unsupervised Models for Corefer-
ence Resolution. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pp. 640?649.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing.
Vincent Ng. 2010. Supervised Noun Phrase Coreference
Research: The First Fifteen Years. In Proceedings of
the 48th Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, pages 1396-1411.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
Path-Based Pronoun Resolution. In COLING?ACL
2006, pages 33?40.
135
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 96?102,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
A Unified Framework for Grammar Error Correction
Longkai Zhang Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn
Abstract
In this paper we describe the PKU system
for the CoNLL-2014 grammar error cor-
rection shared task. We propose a unified
framework for correcting all types of er-
rors. We use unlabeled news texts instead
of large amount of human annotated texts
as training data. Based on these data, a
tri-gram language model is used to cor-
rect the replacement errors while two extra
classification models are trained to correct
errors related to determiners and preposi-
tions. Our system achieves 25.32% in f
0.5
on the original test data and 29.10% on the
revised test data.
1 Introduction
The task of grammar error correction is diffi-
cult yet important. An automatic grammar error
correction system can help second language(L2)
learners improve the quality of their writing. Pre-
vious shared tasks for grammar error correction,
such as the HOO shared task of 2012 (HOO-2012)
and the CoNLL-2013 shared task(CoNLL-2013),
focus on limited types of errors. For example,
HOO-2012 only considers errors related to de-
terminers and prepositions. CoNLL-2013 further
considers errors that are related to noun number,
verb form and subject-object agreement. In the
CoNLL-2014 shared task, all systems should con-
sider all the 28 kinds of errors, including errors
such as spelling errors which cannot be corrected
using a single classifier.
Most of the top-ranked systems in the CoNLL-
2013 shared task(Ng et al., 2013) train individ-
ual classifiers or language models for each kind
of errors independently. Although later systems
such as Wu and Ng (2013); Rozovskaya and Roth
(2013) use Integer Linear Programming (ILP) to
decode a global optimized result, the input scores
for ILP still come from the individual classifica-
tion confidence of each kind of errors. It is hard
to adapt these methods directly into the CoNLL-
2014 shared task. It will be both time-consuming
and impossible to train individual classifiers for all
the 28 kinds of errors.
Besides the classifier and language model
based methods, some systems(Dahlmeier and Ng,
2012a; Yoshimoto et al., 2013; Yuan and Felice,
2013) also use the machine translation approach.
Because there are a limited amount of training
data, this kind of approaches often need to use
other corpora of L2 learners, such as the Cam-
bridge Learner Corpus. Because these corpora use
different annotation criteria, the correction sys-
tems should figure out ways to map the error types
from one corpus to another. Even with these ad-
ditions and transformations, there are still too few
training data available to train a good translation
model.
In contrast, we think the grammar error correc-
tion system should 1) correct most kinds of er-
rors in a unified framework and 2) use as much
unlabeled data as possible instead of using large
amount of human annotated data. To be specific,
our system do not need to train individual clas-
sifiers for each kind of errors, nor do we need
to use manually corrected texts. Following the
observation that a correction can either replace a
wrong word or delete/insert a word, our system
is divided into two parts. Firstly, we use a Lan-
guage Model(LM) to correct errors with respect to
the wrongly used words. The LM only uses the
statistics from a large corpus. All errors related to
wrongly used words can be examined in this uni-
fied model instead of designing individual systems
for each kind of errors. Secondly, we train extra
classifiers for determiner errors and preposition er-
rors. We further consider these two kinds of errors
because many of the deletion and insertion errors
belongs to determiner or preposition errors. The
96
training data of the two classification models also
come from a large unlabeled news corpus there-
fore no human annotation is needed.
Although we try to use a unified framework to
get better performance in the grammar error cor-
rection task, there are still a small portion of errors
we do not consider. The insertion and deletion of
words are not considered if the word is neither a
determiner nor a preposition. Our system is also
incapable of replacing a word sequence into an-
other word sequence. We do not consider these
kinds of errors because we find some of them are
hard to generate correction candidates without fur-
ther understanding of the context, and are not easy
to be corrected even by human beings.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
the task. In section 3 we describe our algorithm.
Experiments are described in section 4. We also
give a detailed analysis of the results in section 4.
In section 5 related works are introduced, and the
paper is concluded in the last section.
2 Task Description
The CoNLL-2014 shared task focuses on correct-
ing all errors that are commonly made by L2 learn-
ers of English. The training data released by
the task organizers come from the NUCLE cor-
pus(Dahlmeier et al., 2013). This corpus contains
essays written by L2 learners of English. These
essays are then corrected by English teachers. De-
tails of the CoNLL-2014 shared task can be found
in Ng et al. (2014).
3 System Overview
3.1 Overview
It is time-consuming to train individual models for
each kind of errors. We believe a better way is to
correct errors in a unified framework. We assume
that each word in the sentence may be involved in
some kinds of errors. We generate a list of cor-
rection candidates for each word. Then a Lan-
guage Model (LM) is used to find the most proba-
ble word sequences based on the original sentence
and the correction candidates for each word. An
illustrative example is shown in figure 1.
Because the LM is designed for the replace-
ment errors rather than insertion and deletion er-
rors, we train two extra classifiers for determiners
and prepositions. The determiner model and the
preposition model can improve the performance in
our experiment.
3.2 Correction Candidate Generation
The correction candidate generation phase aims to
generate a list of correction candidates for each
word in the original sentence. We generate cor-
rection candidates based on the following rules:
1. Words with the same stem
2. Similar words based on edit distance
The first rule includes the words with the
same stem as candidates. These candidates
can be used later to correct the errors re-
lated to word form. For example, candidates
for the word ?time? in the original sentence
?This is a timely rain indeed.? may include
?timed?,?time?,?timed?,?times?,?timings?,?timely?,
?timees? and ?timing?, which all have the stem
?time?. The correct candidate ?timely? is also
included in the candidate list and can be detected
through further processing.
The candidate generated by the second rule are
mainly used for spelling correction. For exam-
ple, a such candidate for ?beleive? may be ?belive?
or ?believe?. To generate meaningful candidates
while guarantee accuracy, we require that the can-
didate and the original word should have the same
initial character. By examining the training data
we experimentally find that very few L2 learn-
ers make spelling errors on the initial characters.
For example, they may spell ?believe? as ?belive?.
However, very few of them may spell ?believe? as
?pelieve? or ?delieve?.
In our system, we generate 10 candidates for
each word. To keep the decoding of the best word
sequence controllable, we do not generate candi-
dates for every word in the original sentence. We
only generate the edit distance based candidates
for the following words:
1. Words that never appear in the English giga-
word corpus
1
2. Words that appear in the gigaword corpus but
with frequency below a threshold (we use 10
in the experiment)
Besides, we do not generate candidates for the
words whose POS tags are ?NNP? or ?NNPS?.
1
http://catalog.ldc.upenn.edu/
LDC2003T05
97
Figure 1: Correction of the original sentence ?Thera is no spaces for Tom?. We use red nodes to represent
the original words in the sentence, and use blue nodes below each word to represent the candidate list of
each word. We use arrows to show the final corrected word sequence with the highest probability.
These words are proper nouns. The correction
of this kind of words should depend on more
contextual information. For the stemming tools
we use the snowball stemmer
2
. To generate
candidates based on edit distance, we use the
org.apache.lucene.search.spell.SpellChecker in
Lucene
3
. Note that unlike other context based
spell checkers such as the one in Microsoft Office,
the SpellChecker class in Lucene is actually not
a spell checker. For an input word w, it can only
suggest words that are similar to w given a pre-
defined dictionary. We build the dictionary using
all words collected from the English Gigaword
corpus.
3.3 Language Model for Candidate Selection
After given each word a list of candidates, we can
now find the word sequence which is most likely to
be the correct sentence. The model we use is the
language model. The probability P (s) of a sen-
tence s = w
0
w
1
...w
n?1
is calculated as:
P (s) =
n?1
?
i=0
P (w
i
|w
0
, ..., w
i?1
) (1)
The transition probability P (w
i
|w
0
, ..., w
i?1
)
is calculated based on language model. In
our system we use a tri-gram language
model trained on the gigaword corpus.
2
http://snowball.tartarus.org/
3
https://lucene.apache.org/
Therefore, P (w
i
|w
0
, ..., w
i?1
) is reduced to
P (w
i
|w
i?2
, w
i?1
). We do not use a fixed smooth-
ing method. We just set the probability of an
unseen string to be a positive decimal which is
very close to zero.
The decoding of the word sequence that max-
imize p(s) can be tackled through dynamic
programming using Viterbi algorithm(Forney Jr,
1973). One useful trick is that to multiply
p(w
i
|w
i?2
, w
i?1
) with a coefficient (4 in our sys-
tem) if w
i?2
, w
i?1
and w
i
are all words in the orig-
inal sentence. This is because most of the original
word sequences are correct. If the system needs to
make a correction, the corrected sequence should
have a much higher score than the original one.
We do not generate candidates for determin-
ers and prepositions. Firstly, they are all frequent
words that are excluded by the rules we men-
tioned in this section. Secondly, the determiner
and preposition errors are the main kinds of errors
made by L2 learners. Some of the errors are re-
lated to the wrong deletions or insertions. There-
fore we choose to take special care of determiners
and prepositions to correct all their replacement,
deletion and insertion errors instead of generating
candidates for them in this stage.
3.4 Determiner Correction
After using LM, the spelling errors as well as ordi-
nary word form errors such as noun numbers, verb
98
forms are supposed to be corrected. As we men-
tioned in the introduction, we should now handle
the deletion and insertion errors. We choose to use
special models for determiner and prepositions be-
cause many of the deletion and insertion errors are
related to determiner errors or preposition errors.
Also, these two kinds of errors have been consid-
ered in HOO-2012 and CoNLL2013. Therefore
it?s easier to make meaningful comparison with
previous works. We use Maximum Entropy (ME)
classifiers to correct the determiner and preposi-
tion errors. In this section we consider the deter-
miner errors. The preposition errors will be con-
sidered in the next section. For both of the two
parts, we use the open source tool MaxEnt
4
as the
implementation of ME.
We consider the determiner correction task as a
multi-class classification task. The input instances
for classification are the space between words. We
consider whether the space should keep empty, or
insert ?a? or ?the?. Therefore, 3 labels are con-
sidered to indicate ?a?, ?the? and ?NULL?. We use
??NULL? to denote that the correct space does not
need an article. We leave the clarification between
?a? and ?an? as a post-process by manually de-
signed rules. We do not consider other determiners
such as ?this? or ??these? because further informa-
tion such as the coreference resolution results is
needed.
Instead of considering all spaces in a sen-
tence, some previous works(AEHAN et al., 2006;
Rozovskaya and Roth, 2010; Rozovskaya et al.,
2013) only consider spaces at the beginning of
noun phrases. Compared to these methods, our
system do not need a POS tagger or a phrase chun-
ker (which is sometimes not accurate enough) to
filter the positions. All the operations are done on
the word level. We list the features we use in ta-
ble 1. Note that for 3-grams and 4-grams we do
not use all combinations of characters because it
will generate more sparse features while the per-
formance is not improved.
Because there are limited amount of training
data, we choose to use the English Gigaword cor-
pus to generate training instances instead of us-
ing the training data of CoNLL-2014. Because the
texts in the Gigaword corpus are all news texts,
most of them are well written by native speakers
and are proofread by the editors. Therefore they
4
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html
1-gram w
?3
, w
?2
, w
?1
, w
1
, w
2
, w
3
2-gram all combinations of w
i
w
j
where
i, j ? {?3,?2,?1, 1, 2, 3}
3-gram w
?3
w
?2
w
?1
,w
?2
w
?1
w
1
,
w
?1
w
1
w
2
, w
1
w
2
w
3
4-gram w
?3
w
?2
w
?1
w
1
,
w
?2
w
?1
w
1
w
2
,w
?1
w
1
w
2
w
3
Table 1: The features used in our system. For a
given blank(space), w
i
means the next ith word
and w
?i
means the previous ith word. For the
example of ?I do not play balls .?, if the current
considered instance is the space between ?play?
and ?balls?, then w
?2
means ?not? and w
1
means
?balls?.
can serve as implicit gold annotations. We gener-
ate the training instances from the sentences in the
Gigaword corpus with the following rules:
1. for each space between words, we treat it as
an instance with label ?NULL?, which means
no article is needed. We use the 3 words be-
fore the space as w
?3
, w
?2
, w
?1
and the 3
words after the space as w
1
, w
2
, w
3
to gener-
ate features. We name this kind of instances
?Space Instance? to indicate we operate on
a space. This kind of training instances can
convey the information that in this context no
article is needed.
2. for each word that is an article, we assume it
as an instance, with the label ?a? or ?the? de-
pending on itself. We use the 3 words before
it as w
?3
, w
?2
, w
?1
and the 3 words after
is as w
1
, w
2
, w
3
. In this case we do not use
the article itself as the context. We name this
kind of instances ?article Instance? to indicate
we operate on an article. This kind of train-
ing instances can convey the information that
in this context a particular article should be
added.
The testing instance are also generated follow-
ing the previously mentioned rules. The decoding
process is as follows. If an instance is a ?space
instance? and is predicted as ?a? or ?the?, we then
add ?a? or ?the? in this space. If an instance is an
?article instance?, the situation is a bit complex. If
it is predicted as another article, we replace it with
the predicted one. If it is predicted as ?NULL?, we
should delete the article to make it a space.
99
To guarantee a certain level of precision, we re-
quire the decoding should only be based on confi-
dent predictions. We use the probability calculated
by the classifier as the confidence score and re-
quire the probability of the considered predictions
should exceed a threshold.
3.5 Preposition Correction
The preposition model is similar to the article
model. We use the same set of features as in ta-
ble 1. The training and testing instance generation
is similar except now we consider prepositions in-
stead of articles. The decoding phase is also iden-
tical to the determiner model.
3.6 Post Processing
The post processing in our system is listed as fol-
lows:
1. Distinguish between ?a? and ?an?. We use
rule based method for this issue.
2. Splitting words. If a word is not in the dic-
tionary but one of its splitting results has a
high frequency, we will split the word into
two words. For example, ?dailylife? is an
out of vocabulary word and the splitting re-
sult ?daily life? is common in English. Then
we split ?dailylife? into ?daily life?.
3. We capitalize the first character of each sen-
tence.
4 Experiment and Analysis
We experiment on the CoNLL-2014 test data. We
evaluate our system based on the M2 scorer which
is provided by the organizers. Details of the M2
scorer can be found in Dahlmeier and Ng (2012b).
We tune the additional parameters like all the
thresholds on the CoNLL-2014 official training
data. We use all the text in the Gigaword corpus to
train the language model. We use 2.5 million sen-
tences in the Gigaword corpus to train the extra
two classifier.
Results of our system are shown in table 2. LM
refers to using language model alone. LM+det
refers to using a determiner classifier after using
a language model. LM+prep refers to using a
preposition classifier after using a language model.
LM+det+preposition refers to using a preposition
classifier after LM+det, which is the method used
in our final system.
Model P R F0.5
LM 29.89% 10.04% 21.42%
LM+det 32.23% 13.64% 25.33%
LM+prep 29.73% 10.04% 21.35%
LM+det+prep(all) 32.21% 13.65% 25.32%
Table 2: The experimental results of our system in
the CoNLL-2014 shared task. The threshold for
determiner model and preposition model is 0.99
and 0.99. Parameters are tuned on the CoNLL-
2014 training data.
Model P R F0.5
LM+det+prep(all) 36.64% 15.96% 29.10%
Table 3: The experimental results of our system
in the CoNLL-2014 shared task on the revised an-
notations. The threshold for determiner model and
preposition model is 0.99 and 0.99. Parameters are
tuned on the CoNLL-2014 training data.
From the results we can see that the main con-
tribution comes from the LM model and deter-
miner model. The preposition model can correct
part of the errors while introduce new errors. The
preposition model may harm the overall perfor-
mance. But considering the fact that the grammar
error correction systems are always used for rec-
ommending errors, we still keep the preposition
model in real applications and suggest the errors
predicted by the preposition model.
One limitation of our system is that we only
use a tri-gram based language model as well as up
to 4-gram features for limited instances. Previous
works(Rozovskaya et al., 2013; Kao et al., 2013)
have shown that other resources like the Google 5-
gram statistics can help improve performance. For
the determiner and preposition models, we exper-
iment on different size of training data, from near
zero to the upper bound of our server?s memory
limit (about 72GB). We find that under this lim-
itation, the performance is still improving when
adding more training instances. We believe the
performance can be further improved.
Scores based on the revised annotations is
shown in table 3.
For the convenience of future meaningful com-
parison, we report the result of our system on the
CoNLL-2013 data set in table 4. We tune the ad-
ditional parameters like all the thresholds on the
CoNLL-2013 official training data. Note that in
CoNLL-2013 the scorer considers F1 score in-
100
Model P R F1
CoNLL13 1st 23.49% 46.45% 31.20 %
CoNLL13 2nd 26.35% 23.80% 25.01 %
LM 18.92% 14.55% 16.45%
LM+det 23.76% 36.15% 28.67%
LM+prep 18.89% 14.55% 16.44%
LM+det+prep 23.74% 36.15% 28.66%
Table 4: The experimental results of our system
on the CoNLL-2013 shared task data. The thresh-
old for determiner model and preposition model
is 0.75 and 0.99. Parameters are tuned on the
CoNLL-2013 training data. CoNLL13 1st is Ro-
zovskaya et al. (2013) and the 2nd is Kao et al.
(2013)
stead of F0.5. Therefore some of the thresholds are
different with the ones in the CoNLL-2014 sys-
tem. Because the CoNLL-2013 shared task only
considers 5 types of errors, it will be much easier
to design components specially for each kind of
errors. Therefore our system is a bit less accurate
than the best system. In this system, we restrict the
candidates to be either noun or verb, and omit the
spell checking model. We also omit some post-
processings like deciding whether a word should
be split into two words, because these kinds of er-
rors are not included.
5 Conclusion
In this paper we describe the PKU system for
the CoNLL-2014 grammar error correction shared
task. We propose a unified framework for correct-
ing all types of errors. A tri-gram language model
is used to correct the replacement errors while two
extra classification models are trained to correct
errors related to determiners and prepositions. Our
system achieves 25.32% in f
0.5
on the original test
data and 29.10% on the revised test data.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117, No.61333018), National High
Technology Research and Development Program
of China (863 Program) (No.2012AA011101)
and Major National Social Science Fund of
China(No.12&ZD227).
References
AEHAN, N., Chodorow, M., and LEACOCK,
C. L. (2006). Detecting errors in english arti-
cle usage by non-native speakers.
Dahlmeier, D. and Ng, H. T. (2012a). A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 568?578. Associa-
tion for Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012b). Better eval-
uation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 568?572. Association for Com-
putational Linguistics.
Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).
Building a large annotated corpus of learner en-
glish: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 22?31.
Forney Jr, G. D. (1973). The viterbi algorithm.
Proceedings of the IEEE, 61(3):268?278.
Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,
Boisson, J., Wu, J.-c., and Chang, J. S. (2013).
Conll-2013 shared task: Grammatical error cor-
rection nthu system description. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 20?25, Sofia, Bulgaria. Association
for Computational Linguistics.
Ng, H. T., Wu, S. M., Briscoe, T., Hadiwinoto,
C., Susanto, R. H., and Bryant, C. (2014). The
conll-2014 shared task on grammatical error
correction. In Proceedings of the Eighteenth
Conference on Computational Natural Lan-
guage Learning: Shared Task (CoNLL-2014
Shared Task), pages 1?12, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., and
Tetreault, J. (2013). The conll-2013 shared task
on grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
101
Task, pages 1?12, Sofia, Bulgaria. Association
for Computational Linguistics.
Rozovskaya, A., Chang, K.-W., Sammons, M.,
and Roth, D. (2013). The university of illi-
nois system in the conll-2013 shared task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13?19, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2010). Train-
ing paradigms for correcting errors in grammar
and usage. In Human language technologies:
The 2010 annual conference of the north amer-
ican chapter of the association for computa-
tional linguistics, pages 154?162. Association
for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2013). Joint learn-
ing and inference for grammatical error correc-
tion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 791?802, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Wu, Y. and Ng, H. T. (2013). Grammatical error
correction using integer linear programming. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1456?1465, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-
aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-
machi, M., and Matsumoto, Y. (2013). Naist at
2013 conll grammatical error correction shared
task. CoNLL-2013, 26.
Yuan, Z. and Felice, M. (2013). Constrained gram-
matical error correction using statistical ma-
chine translation. CoNLL-2013, page 52.
102
