Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820?828,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Integrating Multi-level Linguistic Knowledge with a Unified Framework for
Mandarin Speech Recognition
Xinhao Wang, Jiazhong Nie, Dingsheng Luo, Xihong Wu?
Speech and Hearing Research Center,
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing, 100871, China
{wangxh,niejz,wxh,dsluo}@cis.pku.edu.cn
Abstract
To improve the Mandarin large vocabulary
continuous speech recognition (LVCSR), a
unified framework based approach is intro-
duced to exploit multi-level linguistic knowl-
edge. In this framework, each knowledge
source is represented by a Weighted Finite
State Transducer (WFST), and then they are
combined to obtain a so-called analyzer for in-
tegrating multi-level knowledge sources. Due
to the uniform transducer representation, any
knowledge source can be easily integrated into
the analyzer, as long as it can be encoded
into WFSTs. Moreover, as the knowledge in
each level is modeled independently and the
combination is processed in the model level,
the information inherently in each knowledge
source has a chance to be thoroughly ex-
ploited. By simulations, the effectiveness
of the analyzer is investigated, and then a
LVCSR system embedding the presented ana-
lyzer is evaluated. Experimental results reveal
that this unified framework is an effective ap-
proach which significantly improves the per-
formance of speech recognition with a 9.9%
relative reduction of character error rate on
the HUB-4 test set, a widely used Mandarin
speech recognition task.
1 Introduction
Language modeling is essential for large vocabu-
lary continuous speech recognition (LVCSR), which
aims to determine the prior probability of a supposed
word string W , p(W ). Although the word-based n-
gram language model remains the mainstream for
?Corresponding author: Xihong Wu
most speech recognition systems, the utilization of
linguistic knowledge is too limited in this model.
Consequently, many researchers have focused on
introducing more linguistic knowledge in language
modeling, such as lexical knowledge , syntax and
semantics of language (Wang and Vergyri, 2006;
Wang et al, 2004; Charniak, 2001; Roark, 2001;
Chelba, 2000; Heeman, 1998; Chelba et al, 1997).
Recently, structured language models have been
introduced to make use of syntactic hierarchi-
cal characteristics (Roark, 2001; Charniak, 2001;
Chelba, 2000). Nevertheless, the computational
complexity of decoding will be heavily increased, as
they are parser-based models. In contrast, the class-
based language model groups the words that have
similar functions of syntax or semantics into mean-
ingful classes. As a result, it handles the questions of
data sparsity and generalization of unseen event. In
practice, the part-of-speech (POS) information, cap-
turing the syntactic role of words, has been widely
used in clustering words (Wang and Vergyri, 2006;
Maltese et al, 2001; Samuelsson and Reichl, 1999).
In Heeman?s POS language model (Heeman, 1998),
the joint probability of word sequence and associ-
ated POS sequence was estimated directly, which
has been demonstrated to be superior to the condi-
tional probability previously used in the class-based
models (Johnson, 2001). Moreover, a SuperARV
language model was presented (Wang and Harper,
2002), in which lexical features and syntactic con-
straints were tightly integrated into a linguistic struc-
ture of SuperARV serving as a class in the model.
Thus, these knowledge was integrated in the rep-
resentation level, and then the joint probabilities
820
of words and corresponding SuperARVs were esti-
mated. However, in the class-based language mod-
els, words are taken as the model units, while other
units smaller or larger than words are unfeasible for
modeling simultaneously, such as the Chinese char-
acters for Chinese names.
Usually, speech recognition systems can only rec-
ognize the words within a predefined dictionary.
With the increase of unknown words, i.e., out-of-
vocabulary (OOV) words, the performance will de-
grade dramatically. This is because not only those
unknown words cannot be recognized correctly, but
the words surrounding them will be affected. Thus,
many efforts have been made to deal with the is-
sue of OOV words (Martins et al, 2006; Galescu,
2003; Bazzi and Glass, 2001), and various model
units smaller than words have been examined to rec-
ognize OOVs from speech, such as phonemes (Bazzi
and Glass, 2000a), variable-length phoneme se-
quence (Bazzi and Glass, 2001), syllable (Bazzi and
Glass, 2000b) and sub-word (Galescu, 2003). Since
the proper name is a typical category of OOV words
and usually takes a very large proportion among all
kinds of OOV words, it has been specially addressed
in (Hu et al, 2006; Tanigaki et al, 2000).
All those attempts mentioned above succeed in
utilizing linguistic knowledge in language modeling
in some degree respectively. In this study, a uni-
fied framework based approach, which aims to ex-
ploit information from multi-level linguistic knowl-
edge, is presented. Here, the Weighted Finite State
Transducer (WFST) turns to be an ideal choice for
our purpose. WFSTs were formerly introduced to
simplify the integration of models in speech recog-
nition, including acoustic models, phonetic mod-
els and word n-gram (Mohri, 1997; Mohri et al,
2002). In recent years, the WFST has been suc-
cessfully applied in several state-of-the-art speech
recognition systems, such as systems developed by
the AMI project (Hain et al, 2006), IBM (Saon et
al., 2003) and AT&T (Mohri et al, 1996), and in
various fields of natural language processing, such
as smoothed n-gram model, partial parsing (Abney,
1996), named entities recognition (Friburger and
Maurel, 2004), semantic interpretation (Raymond et
al., 2006) and machine translation (Tsukada and Na-
gata, 2004). In (Takaaki Hori and Minami, 2003),
the WFST has been further used for language model
adaptation, where language models of different vo-
cabularies that represented different styles were in-
tegrated through the framework of speech transla-
tion. In WFST-based systems, all of the models are
represented uniformly by WFSTs, and the general
composition algorithm (Mohri et al, 2000) com-
bines these representations flexibly and efficiently.
Thereby, rather than integrating the models step by
step in decoding stage, a complete search network is
constructed in advance. The combined WFST will
be more efficient by optimizing with determiniza-
tion, minimization and pushing algorithms of WF-
STs (Mohri, 1997). Besides, the researches on opti-
mizing the search space and improving WFST-based
speech recognition has been carried out, especially
on how to perform on-the-fly WFSTs composition
more efficiently (Hori et al, 2007; Diamantino Ca-
seiro, 2002).
In this study, we extend the linguistic knowledge
used in speech recognition. As WFSTs provide a
common and natural representation for lexical con-
straints, n-gram language model, Hidden Markov
Model models and context-dependency, multi-level
knowledge sources can be encoded into WFSTs un-
der the uniform transducer representation. Then this
group of WFSTs is flexibly combined together to
obtain an analyzer representing knowledge of per-
son and location names as well as POS information.
Afterwards, the presented analyzer is incorporated
into LVCSR to evaluate the linguistic correctness of
recognition candidates by an n-best rescoring.
Unlike other methods, this approach holds two
distinct features. Firstly, as all multi-level knowl-
edge sources are modeled independently, the model
units such as character, words, phrase, etc., can be
chosen freely. Meanwhile, the integration of these
information sources is conducted in the model level
rather than the representation level. This setup will
help to model each knowledge source sufficiently
and may promote the accuracy of speech recogni-
tion. Secondly, under this unified framework, it is
easy to combine additional knowledge source into
the framework with the only requirement that the
new knowledge source can be represented by WF-
STs. Moreover, since all knowledge sources are fi-
nally represented by a single WFST, additional ef-
forts are not required for decoding the new knowl-
edge source.
821
The remainder of this paper is structured as fol-
lows. In section 2, we introduce our analyzer in de-
tail, and incorporate it into a Mandarin speech recog-
nition system. In section 3, the simulations are per-
formed to evaluate the analyzer and test its effective-
ness when being applied to LVCSR. The conclusion
appears in section 4.
2 Incorporation of Multi-level linguistic
knowledge in LVCSR
In this section, we start by giving a brief descrip-
tion on WFSTs. Then some special characteristics
of Chinese are investigated, and the model units are
fixed. Afterwards, each knowledge source is rep-
resented with WFSTs, and then they are combined
into a final WFST, so-called analyzer. At last, this
analyzer is incorporated into Mandarin LVCSR.
2.1 Weighted Finite State Transducers
The Weighted Finite State Transducer (WFST) is the
generalization of the finite state automata, in which,
besides of an input label, an output label and a
weight are also placed on each transition. With these
labels, a WFST is capable of realizing a weighted re-
lation between strings. In our system, log probabili-
ties are adopted as transition weights and the relation
between two strings is associated with a weight indi-
cating the probability of the mapping between them.
Given a group of WFSTs, each of which models a
stage of a mapping cascade, the composition opera-
tion provides an efficient approach to combine them
into a single one (Mohri et al, 2002; Mohri et al,
1996). In particular, for two WFSTs R and S, the
composition T = RoS represents the composition
of relations realized by R and S. The combination
is performed strictly on R?s output and S?s input. It
means for each path in T, mapping string r to string
s, there must exist a path mapping r to some string
t in R and a path mapping t to s in S. Decoding on
the combined WFST enables to find the joint opti-
mal results for multi-level weighted relations.
2.2 Model Unit Selection
This study primarily takes the person and location
names as well as the POS information into account.
To deal with Chinese OOV words, different from
the western language in which the phoneme, sylla-
ble or sub-word are used as the model units (Bazzi
and Glass, 2000a; Bazzi and Glass, 2000b; Galescu,
2003), Chinese characters are taken as the basic
units. In general, a person name of Han nation-
ality consists of a surname and a given name usu-
ally with one or two characters. Surnames com-
monly come from a fixed set that has been histori-
cally used. According to a recent investigation on
surnames involving 296 million people, 4100 sur-
names are found, and 129 most used surnames ac-
count for 87% (conducted by the Institute of Genet-
ics and Developmental Biology, Chinese Academy
of Sciences). In contrast, the characters used in
given names can be selected freely, and in many situ-
ations, some commonly used words may also appear
in names, such as ???? (victory) and ???? (the
Changjiang River). Therefore, both Chinese charac-
ters and words are considered as model units in this
study, and a word re-segmentation process on recog-
nition hypotheses is necessary, where an n-gram lan-
guage model based on word classes is adopted.
2.3 Representation and Integration of
Multi-level Knowledge
In this work, we ignore the word boundaries of n-
best hypotheses and perform a word re-segmentation
for names recognition. Given an input Chinese
character, it is encoded by a finite state acceptor
FSAinput. For example, the input ???????
(while synthesizing molecule) is represented as in
Figure 1(a). Then a dictionary is represented by a
50 321
0
?:??:?
?:??
?:?
(a)
(b)
4
?:?
?:? ?:? ?:?
?:??
?:?
?:?
?:?
?:? ?:?
?:??
?:??
?? ? ? ?
1 3
6
5
4
2
10
9
8
7
Figure 1: (a) is an example of the FSA representing a
given input; (b) is the FST representing a toy dictionary.
822
transducer with empty weights, denoted as FSTdict.
Figure 1(b) illustrates a toy dictionary listed in Ta-
ble 1, in which a successful path encodes a mapping
from a Chinese character sequence to some word
in the dictionary. In practice, all Chinese charac-
Chinese Words English Words
?? synthesize
?? element
?? molecule
?? the period of the day from11 p.m.to l a.m.
? together
? present
Table 1: The Toy dictionary
ters should appear in the dictionary for further in-
corporating models of names. Then the combination
of FSAinput and FSTdict, FSTseg = FSAinput ?
FSTdict, will result in a WFST embracing all the
possible candidate segmentations. Afterwards an n-
gram language model based on word classes is used
to weight the candidate segmentations. As in Fig-
ure 2, a toy bigram with three words is depicted by
WFSTn?gram, and the word classes are defined in
Table 2. Here, both in the training and test stages,
0
w1/un(w1)
w2/un(w2)
w3/un(w3)
4
w3/un(w3)
?/back(w1)
w1/un(w1)
?/back(w3)
w2/un(w2)
?/back(w2)
w1/bi(w2,w1)
w2/bi(w3,w2)
w2/bi(w1,w2)
w3/bi(w2,w3)
w1/bi(w3,w1)
w3/bi(w1,w3)
2
3
1
Figure 2: The WFST representing a toy bigram language
model, in which un(w1) denotes the unigram of w1;
bi(w1, w2) and back(w1) respectively denotes the bi-
gram of w2 and the backoff weight given the word history
w1.
the strings of numbers or letters in sentences are ex-
Classes Description
wi Each word wi listed in the dictionary
CNAME Person names of Han nationality
TNAME Translated person names
LOC Location names
NUM Number expressions
LETTER Letter strings
NON Other non Chinese character strings
BEGIN Beginning of sentence
END End of sentence
Table 2: The Definition of word classes
tracted according to the rules, and then substituted
with the class tags, ?NUM? and ?LETTER? respec-
tively. At the same time, the words, such as ????
and ?A??, are replaced with ?NUM?? and ?LET-
TER?? in the dictionary. In addition, name classes,
including ?CNAME?, ?TNAME? and ?LOC?, will
be set according to names recognition.
Hidden Markov Models (HMMs) are adopted
both for names recognition and POS tagging. Here,
each HMM is represented with two WFSTs. Tak-
ing the POS tagging as an example, the toy POS
WFSTs with 3 different tags are illustrated in Fig-
ure 3. The emission probability of a word by a POS,
(P (word/pos)), is represented as in Figure 3(a),
and the bigram transition probabilities between POS
tags are represented as in Figure 3(b), similar to the
word n-gram. In terms of names recognition, the
HMM states correspond to 30 role tags of names,
some for model units of Chinese characters, such as
surname, the first or second character of a given per-
son name with two characters, the first or last charac-
ter of a location name and so on, but others for model
units of words, such as the word before or after a
name, the words in a name and so on. When rec-
ognizing the person names, since there is a big dif-
ference between the translated names and the names
of Han nationality, two types of person names are
modeled separately, and substituted with two differ-
ent class tags in the segmentation language model,
as ?TNAME? and ?CNAME?. Some rules, which
can be encoded into WFSTs, are responsible for the
transformation from a role sequence to correspond-
ing name class (for example, a role sequence might
consist of the surname, the first character of the
823
0pos1/un(pos1)
pos2/un(pos2)
pos3/un(pos3)
pos1/bi(pos2,pos1)
pos3/bi(pos2,pos3)
pos2/bi(pos1,pos2)
pos3/bi(pos1,pos3)
pos1/bi(pos3,pos1)pos2/bi(pos3,pos2)
(a)
(b)
word: pos/p(word/pos)
3
2
1
0
Figure 3: The toy POS WFSTs. (a) is the WFST rep-
resenting the relationship between the word and the pos;
(b) is the WFSA representing the bigram transition prob-
abilities between POS tags
given name, and the second character of the given
name, which will be transformed to ?CNAME? in
FSTseg). Hence, taking names recognition into ac-
count, a WFST, including all possible segmentations
as well as recognized candidates of names, can be
obtained as below, denoted as WFSTwords:
FSAinput ? FSTdict ?WFSTne ?WFSAn?gram
(1)
POS information is integrated as follows.
(? ?WFSTwords) ?WFSTPOS (2)
Consequently, the desired analyzer, a combined
WFST that represents multi-level linguistic knowl-
edge sources, has been obtained.
2.4 Incorporation in LVCSR
The presented analyzer models linguistic knowledge
at different levels, which will be useful to find an
optimal words sequence among a large number of
speech recognition hypotheses. Thus in this re-
search, the analyzer is incorporated after the first
pass recognition, and the n-best hypotheses are
reranked according to the total path scores adjusted
with the analyzer scores as follows.
W? = argmax
W
?
??
log (PAM (O|W ))
+? ? log (PLM (W ))
+? ? log (PAnalyzer (W ))
?
??
(3)
where PAM (O|W ) and PLM (W ) are the acoustic
and language scores produced in first pass decoding,
and PAnalyzer (W ) reflects the linguistic correctness
of one hypothesis scored by the analyzer. Through
the reranking paradigm, a new best sentence hypoth-
esis is obtained.
3 Simulation
Under the unified framework, multi-level linguistic
knowledge is represented by the analyzer as men-
tioned above. To guarantee the effectiveness of
the introduced framework in integrating knowledge
sources, the analyzer is evaluated in this section.
Then the experiments using an LVCSR system in
which the analyzer is embedded are performed.
3.1 Analyzer Evaluation
Considering the function of the analyzer, cascaded
subtasks of word segmentation, names recognition
and POS tagging can be processed jointly, while
they are traditionally handled in a pipeline manner.
Hence, a comparison between the analyzer and the
pipeline system can be used to evaluate the effec-
tiveness of the introduced framework for knowledge
integration. As illustrated in Figure 4, two systems
based on the presented analyzer and the pipeline
manner are constructed respectively.
The evaluation data came from the People?s Daily
of China in 1998 from January to June (annotated by
the Institute of Computational Linguistics of Peking
University1), among which the January to May data
was taken as the training set, and the June data was
taken as the test set (consisted of 21,143 sentences
and about 1.2 million words). The first two thou-
sand sentences from the June data were extracted
as the development set, used to fix the composition
weight ? in equation 2. A dictionary including about
113,000 words was extracted from the training data,
1http://icl.pku.edu.cn/icl res/
824
input d ict ne n gramF SA F ST W F ST W F ST q q q
Decode
The best segmentation
posWFST
CCompose ompose
Decode Decode
Pipeline System Presented Analyzer
output output
Figure 4: The pipeline system vs The analyzer
in which a person or location name was accounted
as a word in vocabulary, only when the number of
its appearances was no less than three.
In Figure 5, the analyzer is compared with the
pipeline system, where the analyzer outperforms the
pipeline manner on all the subtasks in terms of F1-
score metric. Furthermore to detect the differences,
the statistical significance test using approximate
randomization approach (Yeh, 2000) is done on the
word segmentation results. Since there are more
than 21,000 sentences in the test set, which is not
appropriate for approximate randomization test, ten
sets (500 sentences for each) are randomly selected
from the test corpus. For each set, we run 1048576
shuffles twice and calculate the significance level p-
value according to the shuffled results. It has been
shown that all p-value are less than 0.001 on the ten
sets. Accordingly the improvement is statistically
significant. Actually, this significant improvement
is reasonable, since the joint processing avoids error
propagation and provides the opportunity of shar-
ing information between different level knowledge
sources. The superiority of this analyzer also shows
that the integration of multi-level linguistic knowl-
edge under the unified framework is effective, which
may lead to improved LVCSR.
95.9
91.1
89.9
96.8
91.8
88.5
90.9
88
92
96 Pipeline Analyzer
Integrated Analyzer
83.3
80
84
Word Segmentation POS Tagging Person Name Recognition Location Name Recognition
Figure 5: The Performance comparison between the
pipeline system and the analyzer. The system perfor-
mances are measured with the F1-score in the tasks
of word segmentation, POS tagging, the person names
recognition and the location names recognition.
3.2 Experimental Setup for Mandarin Speech
Recognition
In the baseline speech recognition system, the
acoustic models consisted of context-dependent
Initial-Final models, in which the left-to-right model
topology was used to represent each unit. Accord-
ing to the phonetic structures, the number of states
in each model was set to 2 or 3 for initials, and 4
or 5 for tonal finals. Each state was trained to have
32 Gaussian mixtures. The used 39-dimension fea-
ture vector comprised 12 MFCC coefficients, en-
ergy, and their first-order and second-order deltas.
Since in this work we focused on modeling knowl-
edge of language in Mandarin LVCSR, only clean
male acoustic models were trained with a speech
database that contained about 360 hours speech of
over 750 male speakers. This training data was
picked up from three continuous Mandarin speech
corpora: the 863-I, 863-II and Intel corpora. The
brief information about these three speech corpora
was listed in Table 3. As in this work, the eval-
uation data was the 1997 HUB-4 Mandarin broad-
cast news evaluation data (HUB-4 test set), to bet-
ter fit this task, the acoustic models were adapted
by the approach of maximum a posterior (MAP)
adaptation. The adaption data was drawn from the
HUB4 training set, excluding the HUB-4 develop-
825
Corpus Speakers Amount of Speech
(hours)
863-I (male) 83 56.67
863-II(male) 120 78.08
Intel (male) 556 227.30
total 759 362.05
Table 3: The information of the speech training data
ing set, where only the cleaned male speech data
(data under condition f0 defined as (Doddington,
1996)) was used. The partition for the clean data
was done with the acoustic segmentation software
CMUseg 0.52 (Siegler et al, 1997), and finally 8.6
hours adaptation data was obtained.
The language model was a word-based trigram
built on 60,000 words entries and trained with a cor-
pus about 1.5 billion characters. The training set
consisted of broadcast news data from the Xinhua
News Agency released by LDC (Xinhua part of Chi-
nese Gigaword), seven years data of People?s Daily
of China from 1995 to 2002 released by People?s
Daily Online3, and some other data from news web-
sites, such as yahoo, sina and so on.
In addition, the analyzer incorporated in speech
recognition was trained with a larger corpus from
People?s Daily of China, including the data in 1998
from January to June and the data in 2000 from
January to November (annotated by the Institute
of Computational Linguistics of Peking University).
The December data in 2000 was taken as the devel-
opment set used to fix the composition weight ? in
equation 2.
3.3 Experimental Results
In our experiments, the clean male speech data from
the Hub-4 test set was used, and 238 sentences were
finally extracted for testing. The weight of the ana-
lyzer was empirically derived from the development
set, including 649 clean male sentences from the de-
vSet of HUB-4 Evaluation. The recognition results
are shown in Table 4. The baseline system has a
character error rate (CER) of 14.85%. When the an-
alyzer is incorporated, a 9.9% relative reduction is
2Acoustic segmentation software downloaded from
http://www.nist.gov/speech/tools/CMUseg 05targz.htm.
3http://www.people.com.cn
System Err. Sub. Del. Ins.
Baseline 14.85 13.02 0.76 1.07
Analyzer 13.38 11.78 1.00 0.60incorporation
Table 4: The Speech recognition results
achieved. Furthermore, we ran the statistical signif-
icance test to detect the performance improvement,
in which the approximate randomization approach
(Yeh, 2000) was modified to output the significance
level, p-value, for the CER metric. The p-levels pro-
duced through two rounds of 1048576 shuffles are
0.0058 and 0.0057 respectively, both less than 0.01.
Thus the performance improvement imposed by the
utilization of the analyzer is statistically significant.
4 Conclusion
Addressing the challenges of Mandarin large vocab-
ulary continuous speech recognition task, within the
unified framework of WFSTs, this study presents
an analyzer integrating multi-level linguistic knowl-
edge. Unlike other methods, model units, such as
characters and words, can be chosen freely in this
approach since multi-level knowledge sources are
modeled independently. As a consequence, the fi-
nal analyzer can be derived from the combination
of better optimized models based on proper model
units. Along with two level knowledge sources, i.e.,
the person and location names as well as the part-of-
speech information, the analyzer is built and evalu-
ated by a comparative simulation. Further evaluation
is also conducted on an LVCSR system in which the
analyzer is embedded. Experimental results consis-
tently reveal that the approach is effective, and suc-
cessfully improves the performance of speech recog-
nition by a 9.9% relative reduction of character error
rate on the HUB-4 test set. Also, the unified frame-
work based approach provides a property of integrat-
ing additional linguistic knowledge flexibly, such as
organization name and syntactic structure. Further-
more, the presented approach has a benefit of ef-
ficiency that additional efforts are not required for
decoding as new knowledge comes, since all knowl-
edge sources are finally encoded into a single WFST.
826
Acknowledgments
The work was supported in part by the National
Natural Science Foundation of China (60435010;
60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the Na-
tional Key Basic Research Program of China
(2004CB318005), and the New-Century Training
Program Foundation for the Talents by the Ministry
of Education of China.
References
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4):337?344.
Issam Bazzi and James R. Glass. 2000a. Modeling out-
of-vocabulary words for robust speech recognition. In
Proc. of 6th International Conference on Spoken Lan-
guage Processing, pages 401?404, Beijing, China, Oc-
tober.
Issam Bazzi and James Glass. 2000b. Heterogeneous
lexical units for automatic speech recognition: prelim-
inary investigations. In Proc. of ICASSP, pages 1257?
1260, Istanbul, Turkey, June.
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Proc. of EUROSPEECH, pages 61?64, Aal-
borg, Denmark, September.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. of ACL, pages 116?123,
Toulouse, France, July.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, and Dekai Wu. 1997. Structure and performance
of a dependency language model. In Proc. of EU-
ROSPEECH, pages 2775?2778, Rhodes, Greece.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns Hop-
kins University.
Isabel Trancoso Diamantino Caseiro. 2002. Using dy-
namic WFST composition for recognizing broadcast
news. In Proc. of ICSLP, pages 1301?1304, Denver,
Colorado, USA, September.
George Doddington. 1996. The 1996 hub-
4 annotation specification for evaluation of
speech recognition on broadcast news. In
ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.
N. Friburger and D. Maurel. 2004. Finite-state trans-
ducer cascades to extract named entities in texts. The-
oretical Computer Science, 313(1):93?104.
Lucian Galescu. 2003. Recognition of out-of-vocabulary
words with sub-lexical language models. In Proc.
of EUROSPEECH, pages 249?252, Geneva, Switzer-
land, September.
Thomas Hain, Lukas Burget, John Dines, Giulia Garau,
Martin Karafiat, Mike Lincoln, Jithendra Vepa, and
Vincent Wan. 2006. The AMI meeting transcription
system: Progress and performance. In Proc. of Rich
Transcription 2006 Spring Meeting Recognition Eval-
uation.
Peter A. Heeman. 1998. Pos tagging versus classes in
language modeling. In Proc. of the 6th Workshop on
very large corpora, pages 179?187, Montreal, Canada.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring in
extremely large vocabulary continuous speech recog-
nition. IEEE Transactions on audio, speech, and lan-
guage processing, 15(4):1352?1365.
Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, and
Yoshinori Sagisaka. 2006. Language modeling of
chinese personal names based on character units for
continuous chinese speech recognition. In Proc. of
INTERSPEECH, pages 249?252, Pittsburgh, USA,
September.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL, pages
322 ? 329, Toulouse, France.
G. Maltese, P. Bravetti, H. Cr?py, B. J. Grainger, M. Her-
zog, and F. Palou. 2001. Combining word- and
class-based language models: A comparative study in
several languages using automatic and manual word-
clustering techniques. In Proc. of EUROSPEECH,
pages 21?24, Aalborg, Denmark, September.
Ciro Martins, Antonio Texeira, and Joao Neto. 2006.
Dynamic vocabulary adaptation for a daily and real-
time broadcast news transcription system. In Proc. of
Spoken Language Technology Workshop, pages 146?
149, December.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17?32.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69?88.
Mehrya Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
827
Christan Raymond, Fre de ric Be chet, Renato D. Mori,
and Ge raldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48(3-4):288?304.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Christer Samuelsson and Wolfgang Reichl. 1999. A
class-based language model for large-vocabulary
speechrecognition extracted from part-of-speech
statistics. In Proc. of ICASSP, pages 537?540,
Phoenix, Arizona, USA, March.
George Saon, Geoffrey Zweig, Brain KingsBury, Lidia
Mangu, and Upendra Canudhari. 2003. An architec-
ture for rapid decoding of large vocabulary conversa-
tional speech. In Proc. of Eurospeech, pages 1977?
1980, Geneva, Switzerland, September.
Matthew A. Siegler, Uday Jain, Bhiksha Raj, and
Richard M. Stern. 1997. Automatic segmentation,
classification and clustering of broadcast news audio.
In Proc. of DARPA Speech Recognition Workshop,
pages 97?99, Chantilly, Virginia, February.
Daniel Willett Takaaki Hori and Yasuhiro Minami.
2003. Language model adaptation using WFST-based
speaking-style translation. In Proc. of ICASSP, pages
I.228?I.231, Hong Kong, April.
Koichi Tanigaki, Hirofumi Yamamoto, and Yoshinori
Sagisaka. 2000. A hierarchical language model incor-
porating class-dependent word models for oov words
recognition. In Proc. of 6th International Conference
on Spoken Language Processing, pages 123?126, Bei-
jing, China, October.
Hajime Tsukada and Masaaki Nagata. 2004. Efficient
decoding for statistical machine translation with a fully
expanded WFST model. In Proc. of EMNLP, pages
427?433, Barcelona, Spain, July.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proc. of
EMNLP, pages 238?247, Philadelphia, USA, July.
Wen Wang and Dimitra Vergyri. 2006. The use of word
n-grams and parts of speech for hierarchical cluster
language modeling. In Proc. of ICASSP, pages 1057?
1060, Toulouse, France, May.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pages 261?264, Montreal, Canada, May.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proc. of
COLING, pages 947?953, Saarbr?cken, August.
828
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1298?1307,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Refining Grammars for Parsing with Hierarchical Semantic Knowledge
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu
?
, Huisheng Chi
Speech and Hearing Research Center
Key Laboratory of Machine Perception (Ministry of Education)
School of Electronics Engineering and Computer Science
Peking University, Beijing, 100871, China
{linxj, fanyang, zhangm, wxh}@cis.pku.edu.cn, chi@pku.edu.cn
Abstract
This paper proposes a novel method to
refine the grammars in parsing by utiliz-
ing semantic knowledge from HowNet.
Based on the hierarchical state-split ap-
proach, which can refine grammars au-
tomatically in a data-driven manner, this
study introduces semantic knowledge into
the splitting process at two steps. Firstly,
each part-of-speech node will be anno-
tated with a semantic tag of its termi-
nal word. These new tags generated in
this step are semantic-related, which can
provide a good start for splitting. Sec-
ondly, a knowledge-based criterion is used
to supervise the hierarchical splitting of
these semantic-related tags, which can al-
leviate overfitting. The experiments are
carried out on both Chinese and English
Penn Treebank show that the refined gram-
mars with semantic knowledge can im-
prove parsing performance significantly.
Especially with respect to Chinese, our
parser achieves an F
1
score of 87.5%,
which is the best published result we are
aware of.
1 Introduction
At present, most high-performance parsers are
based on probabilistic context-free grammars
(PCFGs) in one way or another (Collins, 1999;
Charniak and Johnson, 2005; Petrov and Klein,
2007). However, restricted by the strong context-
free assumptions, the original PCFG model which
simply takes the grammars and probabilities off a
treebank, does not perform well. Therefore, a va-
riety of techniques have been developed to enrich
and generalize the original grammar, ranging from
lexicalization to symbol annotation.
?
Corresponding author: Xihong Wu.
Lexicalized PCFGs use the structural features
on the lexical head of phrasal node in a tree, and
get significant improvements for parsing (Collins,
1997; Charniak, 1997; Collins, 1999; Charniak,
2000). However, they suffer from the problem of
fundamental sparseness of the lexical dependency
information. (Klein and Manning, 2003).
In order to deal with this limitation, a variety
of unlexicalized parsing techniques have been pro-
posed. Johnson (1998) annotates each node by
its parent category in a tree, and gets significant
improvements compared with the original PCFGs
on the Penn Treebank. Then, some manual and
automatic symbol splitting methods are presented,
which get comparable performance with lexical-
ized parsers (Klein and Manning, 2003; Matsuzaki
et al, 2005). Recently, Petrov et al (2006) in-
troduces an automatic hierarchical state-split ap-
proach to refine the grammars, which can alter-
nately split and merge the basic nonterminals by
the Expectation-Maximization (EM) algorithm. In
this method, the nonterminals are split to differ-
ent degrees, as appropriate to the actual complex-
ity in the data. The grammars refined in this way
are proved to be much more accurate and compact
than previous work on automatic annotation. This
data-driven method still suffers from the overfit-
ting problem, which may be improved by integrat-
ing other external information.
In this paper, we propose a novel method that
combines the strengths of both data-driven and
knowledge-driven strategies to refine grammars.
Based on the work proposed by Petrov et al
(2006), we use the semantic knowledge from
HowNet (Dong and Dong, 2000) to supervise
the hierarchical state-split process at the part-of-
speech(POS) level. At first, we define the most
general hypernym in HowNet as the semantic class
of a word, and then use this semantic class to ini-
tialize the tag of each POS node. In this way, a
new set of semantic-related tags is generated, and
1298
a good starting annotation is provided to reduce
the search space for the EM algorithm in the split-
ting process. Then, in order to mitigate the overfit-
ting risk, the hierarchical hypernym-hyponym re-
lation between hypernyms in HowNet is utilized
to supervise the splitting of these new semantic-
related tags. By introducing a knowledge-based
criterion, these new tags are decided whether or
not to split into subcategories from a semantic per-
spective. To investigate the effectiveness of the
presented approach, several experiments are con-
duced on both Chinese and English. They reveal
that the semantic knowledge is potentially useful
to parsing.
The remainder of this paper is organized as
follows. Section 2 reviews some closely related
works, including the lexical semantic related pars-
ing and the hierarchical state-split unlexicalized
parsing. In section 3, the presented method for
grammar refining is described in detail, and sev-
eral experiments are carried out for evaluation in
Section 4. Conclusions are drawn in Section 5.
2 Background
This paper tries to refine the grammars through
an improved hierarchical state-split process in-
tegrated with semantic knowledge. The related
works are reviewed as follows.
2.1 Lexical Semantic Related Parsing
Semantic knowledge is useful to resolving syntac-
tic ambiguities, and a variety of researches focus
on how to utilize it. Especially in recent years,
a conviction arose that semantic knowledge could
be incorporated into the lexicalized parsing.
Based on the lexicalized grammars, Bikel
(2000) attempts at combining parsing and word
sense disambiguation in a unified model, using a
subset of SemCor (Miller et al, 1994). Bikel
(2000) evaluates this model in a parsing context
with sense information from WordNet, but does
not get improvements on parsing performance.
Xiong et al (2005) combines word sense from
CiLin and HowNet (two Chinese semantic re-
sources) in a generative parsing model, which gen-
eralizes standard bilexical dependencies to word-
class dependencies, and indeed help to tackle the
sparseness problem in lexicalized parsing. The
experiments show that the parse model combined
with word sense and the most special hypernyms
achieves a significant improvement on Penn Chi-
nese Treebank. This work only considers the most
special hypernym of a word, rather than other
hypernyms at different levels of the hypernym-
hyponym hierarchy.
Then, Fujita et al (2007) uses the Hinoki tree-
bank as training data to train a discriminative parse
selection model combining syntactic features and
word sense information. Instead of utilizing the
most special hypernym, the word sense informa-
tion in this model is embodied with more general
concepts. Based on the hand-craft sense informa-
tion, this model is proved to be effective for parse
selection.
Recently, Agirre et al (2008) train two lexical-
ized models (Charniak, 2000; Bikel, 2004) on pre-
processed inputs, where content words are substi-
tuted with semantic classes from WordNet. By in-
tegrating the word semantic classes into the pro-
cess of parser training directly, these two models
obtain significant improvements in both parsing
and prepositional phrase attachment tasks. Zhang
(2008) does preliminary work on integrating POS
with semantic class of words directly, which can
not only alleviate the confusion in parsing, but also
infer syntax and semantic information at the same
time.
2.2 The Hierarchical State-split Parsing
In order to alleviate the context-free assumptions,
Petrov et al (2006) proposes a hierarchical state-
split approach to refine and generalize the orig-
inal grammars, and achieves state-of-the-art per-
formance. Starting with the basic nonterminals,
this method repeats the split-merge (SM) cycle to
increase the complexity of grammars. That is, it
splits every symbol into two, and then re-merges
some new subcategories based on the likelihood
computation.
Splitting
In each splitting stage, the previous syntactic sym-
bol is split into two subcategories, and the EM al-
gorithm is adopted to learn probability of the rules
for these latent annotations to maximize the like-
lihood of trees in the training data. Finally, each
symbol generates a series of new subcategories in
a hierarchical fashion. With this method, the split-
ting strategy introduces more context information,
and the refined grammars cover more linguistic in-
formation which helps resolve the syntactic ambi-
guities.
1299
However, it is worth noting that the EM algo-
rithm does not guarantee a global optimal solution,
and often gets stuck in a suboptimal configuration.
Therefore, a good starting annotation is expected
to help alleviate this problem, as well as reduce the
search space for EM.
Merging
It is obvious that using more derived subcategories
can increase accuracy, but the refined grammars fit
tighter to the training data, and may lead to over-
fitting to some extent. In addition, different sym-
bols should have their specific numbers of subcat-
egories. For example, the comma POS tag should
have only one subcategory, as it always produces
the terminal comma. On the contrary, the noun
POS tag and the verb POS tag are expected to have
much more subcategories to express their context
dependencies. Therefore, it is not reasonable to
split them in the same way.
The symbol merging stage is introduced to al-
leviate this defect. This approach splits symbols
only where needed, and it is implemented by split-
ting each symbol first and then measure the loss in
likelihood incurred when removing this subcate-
gory. If the loss is small, it means that this subcate-
gory does not take enough information and should
be removed. In general it is hard to decide the
threshold of the likelihood loss, and this merging
stage is often executed by removing a certain pro-
portion of subcategories, as well as giving priority
to the most informative subcategories.
By splitting and merging alternately, this
method can refine the grammars step by step to
mitigate the overfitting risk to some extent. How-
ever, this data-driven method can not solve this
problem completely, and we need to find other ex-
ternal information to improve it.
Analysis
The hierarchical state-split approach is used to
split all the symbols in the same way. Table 1 cites
the subcategories for several POS tags, along with
their two most frequent words. Results show that
the words in the same subcategory of POS tags are
semantic consistent in some cases. Therefore, it
is expected to optimize the splitting and merging
process at the POS level with semantic knowledge.
NR
NR-0 ???(Daja river) ???(Nepal)
NR-1 ??(Sony) ???(Bole Co.)
NR-2 ??(C. Hua) ???(T. Wen)
NR-3 ???(S. Yue) ?(Shang)
LC
LC-0 ??(middle) ??(right)
LC-1 ??(before) ??(since)
LC-2 ??(start) ?(end)
LC-3 ??(till) ?(end)
P
P-0 ??(whenever) ??(as for)
P-1 ??(like) ??(as)
P-2 ??(look to) ??(according to)
P-3 ?(be close to) ??(contrast)
Table 1: The two most frequent words in the sub-
categories of several POS tag.
3 Integration with Semantic Knowledge
In this paper, the semantic knowledge is used to re-
fine grammars by improving the automatic hierar-
chical state-split approach. At first, in order to pro-
vide good starting annotations to reduce the search
space for the EM algorithm, we try to annotate the
tag of each POS node with the most general hyper-
nym of its terminal word. In this way, we generate
a new set of semantic-related tags. And then, in-
stead of splitting and merging all symbols together
automatically, we propose a knowledge-based cri-
terion with hierarchical semantic knowledge to su-
pervise the splitting of these new semantic-related
tags.
3.1 HowNet
The semantic knowledge resource we use is
HowNet, which is a common sense knowledge
base unveiling concepts and inter-conceptual re-
lations in Chinese and English.
As a knowledge base of graph structure,
HowNet is devoted to demonstrating the proper-
ties of concepts through sememes and relations
between sememes. Broadly speaking, a sememe
refers to the smallest basic semantic unit that can-
not be reduced further, which can be represented
in English and their Chinese equivalents, such as
the sememe institution|??. The relations expli-
cated in HowNet include hypernym-hyponym re-
lations, location-event relations, time-event rela-
tions and so on. In this work, we mainly focus on
1300
.vitality
is full ofThe goveronmentThe goveronment is full of
.
IP
NP VP PU
NN VV NP ?
?? ?? NN
vitality
a.
??
IP
NP VP PU
NN-Entity VV-Event NP ?
?? ?? NN-Attribute
b.
??
Figure 1: The two syntax trees of the sentence "The government is full of vitality". a. is the original
syntax tree, b. is the syntax tree in which each tag of the POS node is annotated with the most general
hypernym of its terminal word.
the hypernym-hyponym relations. Take the word
??(government) as an example, its hypernyms with
the hierarchical hypernym-hyponym relations are
listed below from speciality to generality, which
we call hierarchical semantic information in this
paper.
institution|???group|???thing|???entity|??
It is clear that this word ??(government) has hy-
pernyms from the most special hypernym institu-
tion|?? to the most general hypernym entity|??
in a hierarchical way.
In HowNet(Update 2008), there are 173535
concepts, with 2085 sememes. The sememes are
categorized into entity, event, attribute, attribute
value, etc., each corresponding to a sememe hi-
erarchy tree.
3.2 Annotating the Training Data
One of the original motivations for the grammar
refinement is that the original symbols, especially
the POS tags, are usually too general to distin-
guish the context dependencies. Take the sentence
in Figure 1 for example, the word ??(government)
should have different context dependencies com-
pared with the word ??(vitality), although both of
them have the same POS tag "NN". In fact, the
two words are defined in HowNet with different
hypernyms. The word ??(government) is defined
as a kind of objective things, while the word ?
?(vitality) is defined as a property that is often used
to describe things. It is obvious that the different
senses can represent their different syntax struc-
tures, and we expect to refine the POS tags with
semantic knowledge.
In the automatic hierarchical state-split ap-
proach introduced above, the EM algorithm is
used to search for the maximum of the likelihood
during the splitting process, which can generate
subcategories for POS tags to express the context
dependencies. However, this method often gets
stuck in a suboptimal configuration, which varies
depending on the start point. Therefore, a good
start of the annotations is very important. As it is
displayed in Figure 1, we annotate the tag of each
POS node with the hypernym of its terminal word
as the starting annotation. There are two problems
that we have to consider in this process: a) how to
choose the appropriate semantic granularity, and
b) how to deal with the polysemous words.
As mentioned above, the semantic information
of each word can be represented as hierarchi-
cal hypernym-hyponym relations among its hyper-
nyms. In general, it is hard to decide the appro-
priate level of granularity to represent the word.
The semantic class is only used as the starting an-
notations of POS tags to reduce the search space
for EM in our method. It is followed by the hi-
erarchical state-split process to further refine the
starting annotations based on the structural infor-
mation. If more special kinds of semantic classes
are chosen, it will make the structural information
weaker. As annotations with the special hyper-
nym always defeat some of the advantage of au-
tomatically latent annotations learning, we anno-
tate the training data with the most general hyper-
nym. For example, as shown in Figure 1, the POS
tag "NN" of ??(government) is annotated as "NN-
Entity", and "NN" of ??(energy) is annotated as
"NN-Attribute".
Another problem is how to deal with the polyse-
mous words in HowNet. In fact, when we choose
the most general hypernym as the word?s semantic
1301
??(beast)
??(insect)
??(banana) 
??(orange)
??(noon) 
??(forenoon)
??(north)
??(south)
noon forenoon 
north south
noon forenoon north southbeast insect banana orange
entity| ??
thing| ?? time| ?? direction| ??
animal| ? fruit| ??
Continue Splitting...
Having hyponyms...
NN-Entity HowNet
beast insect
banana orange
Figure 2: A schematic figure for the hierarchical state-split process of the semantic-related tag "NN-
Entity". Each subcategory of this tag has its own word set, and corresponds to one hypernym at the
appropriate level in HowNet.
representation, this problem has been alleviated to
a large extent. In this paper we adopt the first sense
option as our word sense disambiguation (WSD)
strategy to determine the sense of each token in-
stance of a target word. That is to say, all token in-
stances of a given word are tagged with the sense
that occurs most frequently in HowNet. In addi-
tion, we keep the tag of the POS node whose ter-
minal word is not defined in HowNet unchanged.
3.3 Supervising the Hierarchical State-split
Process
With the method proposed above, we can produce
a good starting annotation with semantic knowl-
edge, which is of great use to constraining the au-
tomatic splitting process. Our parser is trained on
the good starting annotations with the automatic
hierarchical state-split process, and gets improve-
ments compared with the original training data.
However, during this process, only the most gen-
eral hypernyms are used as the semantic repre-
sentation of words, and the hierarchical semantic
knowledge is not explored. In addition, the auto-
matic process tries to refine all symbols together
through a data-driven manner, which suffers the
overfitting risk.
After annotating the training data with hyper-
nyms, a new set of semantic-related tags such as
"NN-Entity" is produced. We treat the refining
process of these semantic-related tags as the spe-
cializing process of hypernym with hierarchical
semantic knowledge. Each subcategory of these
tags corresponds to a appropriate special level of
hypernym in the HowNet. For example, every sub-
category of "NN-Entity" could corresponds to a
appropriate hyponym of entity|??.
We integrate the hierarchical semantic knowl-
edge into the original hierarchical state-split pro-
cess to refine these semantic-related tags. First
of all, it is necessary to establish the mapping
from each subcategory of these semantic-related
tags to the hypernym at the appropriate level in
HowNet. Then, instead of likelihood judgment, a
knowledge-based criterion is proposed, to decide
whether or not to remove the new subcategories
of these tags. That is to say, once the parent tag
of this new subcategory is mapped onto the most
special hypernym without any hyponym, it should
be removed immediately.
The schematic Figure 2 demonstrates this se-
mantically supervised splitting process. The left
part of this figure is the subcategories of the
semantic-related tag "NN-Entity", which is split
hierarchically. As expressed by the dashed line,
each subcategory corresponds to one hypernym in
the right part of this figure. If the hypernym node
has no hyponym, the corresponding subcategory
will stop splitting.
The mapping from each subcategory of these
semantic-related tags to the hypernym at the ap-
propriate level is implemented with the word set
related to this subcategory. As it is shown in Fig-
1302
DataSet
Chinese English
Xue et al (2002) Marcus et al (1993)
TrainSet Art. 1-270,400-1151 Sections 2-21
DevSet Articles 301-325 Section 22
TestSet Articles 271-300 Section 23
Table 2: Experimental setup.
ure 2, the original tag "NN-Entity" treats all the
words it products as its word set. Once the orig-
inal category is split into two subcategories, its
word set is also split, through forcedly dividing
each word in the word set into one subcategory
which is most frequent with this word. And then,
each subcategory is mapped onto the most specific
hypernym that contains its related word set en-
tirely in HowNet. On this basis, a new knowledge-
based criterion is introduced to enrich and gener-
alize these semantic-related tags, with purpose of
fitting to the hierarchical semantic structure rather
than the training data.
4 Experiments
In this section, we designed several experiments to
investigate the validity of refining grammars with
semantic knowledge.
4.1 Experimental Setup
We did experiments on Chinese and English. In
order to make a fair comparison with previous
works, we split the standard corpora as shown
in Table 2. Our parsers were evaluated by the
EVALB parseval reference implementation
1
. The
Berkeley parser
2
was used to train the models with
the original automatic hierarchical state-split pro-
cess. The semantic resource we used to improve
parsing was HowNet, which has been introduced
in Subsection 3.1. Statistical significance was
checked using Dan Bikel?s randomized parsing
evaluation comparator with the default setting of
10,000 iterations
3
.
4.2 Semantic Representation Experiments
First of all, we ran experiments with different se-
mantic representation methods on Chinese. The
polysemous words in the training set were anno-
tated with the WSD strategy of first sense option,
1
http://nlp.cs.nyu.edu/evalb/.
2
http://code.google.com/p/berkeleyparser/.
3
http://www.cis.upenn.edu/ dbikel/software.html.
which was proved to be useful in Agirre et al
(2008).
As mentioned in Subsection 3.1, the semantic
information of each word can be represented as
a hierarchical relation among its hypernyms from
specialty to generalization in HowNet. In order to
choose the appropriate level of granularity to rep-
resent words, we annotated the training set with
different levels of granularity as semantic repre-
sentation. In our experiments, the automatic hier-
archical state-split process is used to train models
on these training sets with different level of seman-
tic representation.
We tried two kinds of semantic representations,
one is using the most general hypernym, and the
other is using the most special hypernym. Results
in Figure 3 proved the effectiveness of our method
in Subsection 3.2. When we annotated the tag of
each POS node with the most general hypernym of
its terminal word, the parser performs much bet-
ter than both the baseline and the one annotated
with the most special hypernym. Moreover, the F
1
score starts dropping after 3 training iterations on
the training set annotated with the most special hy-
pernyms, while it is still improving with the most
general one, indicating overfitting.
1 2 3 4
68
70
72
74
76
78
80
82
84
86
88
 
 
P
a
r
s
i
n
g
 
a
c
c
u
r
a
c
y
 
(
F
1
)
Times of split-merge iteration
 Baseline
 Most Special Hypernym
 Most General Hypernym
Figure 3: Performances on Chinese with different
semantic representations: the training set without
semantic representation, the training set annotated
with the most special hypernyms, and the training
set annotated with the most general hypernyms.
When the training set was annotated with the
most general hypernyms, there were only 57 new
semantic-related tags such as "NN-Entity", "NN-
Attribute" and so on. However, when the train-
ing set was annotated with the most special hyper-
nyms, 4313 new tags would be introduced. Ob-
1303
viously, it introduces too many tags at once and is
difficult to refine appropriate grammars in the sub-
sequent step starting with this over-splitting train-
ing set.
4.3 Grammar Refinement Experiments
Several experiments were carried out on Chinese
and English to verify the effectiveness of refining
grammars with semantic knowledge. We took the
most general hypernym as the semantic represen-
tation, and the polysemous words in the training
set were annotated with the WSD strategy of first
sense option.
In our experiments, three kinds of method were
compared. The baseline was trained on the raw
training set with the automatic hierarchical state-
split approach. Then, we improved it with the se-
mantic annotation, which annotated the raw train-
ing set with the most general hypernyms as se-
mantic representations, while keeping the train-
ing approach used in the baseline unchanged.
Further, our knowledge-based criterion was in-
troduced to supervise the automatic hierarchical
state-split process with semantic knowledge.
In this section, since most of the parsers (includ-
ing the baseline parser and our advanced parsers)
had the same behavior on development set that the
accuracy continued increasing in the five begin-
ning iterations and then dropped at the sixth iter-
ation, we chose the results at the fifth iteration as
our final test set parsing performance.
Performances on Chinese
Figure 4 shows that refining grammars with se-
mantic knowledge can help improve parsing per-
formance significantly on Chinese (sentences of
length 40 or less). Benefitting from the good start-
ing annotations, our parser achieved significant
improvements compared with the baseline (86.8%
vs. 86.1%, p<.08). It proved that the good start-
ing annotations with semantic knowledge were ef-
fective in the splitting process. Further, we su-
pervised the splitting of the new semantic-related
tags from the semantic annotations, and achieved
the best results at the fifth iteration. The best F
1
score reached 87.5%, with an error rate reduction
of 10.1%, relative to the baseline (p<.004).
Table 3 compared our methods with the best
previous works on Chinese. The result showed
that refining grammars integrated with semantic
knowledge could resolve syntactic ambiguities re-
LP LR F1
82
83
84
85
86
87
88
89
90
91
92
 
  
87.5
86.8
86.1
86.0
85.7
84.9
88.9
88.0
Evaluation Criterion
 Baseline
 Semantic Annotation
 Semantic Annotation & Knowledge-based Criterion
87.3
Figure 4: Performances at the fifth iteration on
Chinese (sentences of length 40 or less) with three
methods: the baseline, the parser trained on the
semantic annotations with automatic method, and
the parser trained on the semantic annotations with
knowledge-based criterion.
Parser
? 40 words all
LP LR F
1
LP LR F
1
Chiang and
81.1 78.8 79.9 78.0 75.2 76.6
Bikel (2002)
Petrov and
86.9 85.7 86.3 84.8 81.9 83.3
Klein (2007)
This Paper 88.9 86.0 87.5 86.0 83.1 84.5
Table 3: Our final parsing performance compared
with the best previous works on Chinese.
markably and achieved the state-of-the-art perfor-
mance on Chinese.
Performances on English
In order to verify the effectiveness of our method
on other languages, we carried out some experi-
ments on English. HowNet is a common sense
knowledge base in Chinese and English, there-
fore, it was still utilized as the knowledge source
in these experiments.
The same three methods were compared on En-
glish (sentences of length 40 or less), and the re-
sults were showed in Table 4. Compared with the
baseline (90.1%), the parsers trained with the se-
mantic annotation, while using different splitting
methods introduced in Section 3, achieved an F
1
score of 90.2% and 90.3% respectively. The re-
sults showed that our methods could get a small
but stable improvements on English (p<.08).
1304
Subcategory Refined from the Original Training Set
PN-0
??(aid foreign),??(aunt),??(self),?(you),?(donate),??(those),??(appearence),
???(self),??(we),?(that),??(the above),??(there),??(other),??(below)
Subcategories Fefined from the Good Starting Annotations
PN-0 ??(aunt),??(self),???(self),??(we),?(you)
PN-Event-0 ??(aid foreign),?(donate)
PN-AttributeValue-2 ??(the above),??(those),?(that),??(other),??(below)
Table 5: Several subcategories that generated from the original training set and the good starting annota-
tions respectively.
Method F
1
Baseline 90.1
Semantic Annotations 90.2
Semantic Annotations &
90.3
Knowledge-based Criterion
Table 4: Performances at the fifth iteration on En-
glish (sentences of length 40 or less) with three
methods: the baseline, the parser trained on the
semantic annotations with automatic method, and
the parser trained on the semantic annotations with
knowledge-based criterion.
These results on English were preliminary, and
we did not introduce any language dependent op-
eration such as morphological processing. Since
only the lemma of English words can be found
in HowNet, we just annotated two kinds of POS
tags "VB"(Verb, base form) and "NN"(Noun, sin-
gular or mass) with semantic knowledge, on the
contrary, we annotated almost all POS tags whose
corresponding words could be found in HowNet
on Chinese. This might be the reason that the
improvement on the English Treebank was much
smaller than that of Chinese. It is expected to
achieve more improvements through some mor-
phological analysis in the future.
4.4 Results and Analysis
So far, a new strategy has been introduced to re-
fine the grammars in two steps, and achieved sig-
nificant improvements on parsing performance. In
this section, we analyze the grammars learned at
different steps, attempting to explain how the se-
mantic knowledge works.
It is hard to inspect all the grammars by hand.
Since the semantic knowledge is mainly used for
generating and splitting new semantic-related tags
in our method, we focus on the refined subcate-
gories of these tags.
First, we examine the refined subcategories of
POS tags, which are generated from the original
training set and the good starting annotations re-
spectively. Several subcategories are listed and
compared in Table 5, along with their frequent
words. It can be seen that the subcategories refined
with semantic knowledge are more consistent than
the previous one. For example, the subcategory
"PN-0", which is refined from the original training
set, produces a lot of words without semantic con-
sistence. In contrast, we refine the subcategories
"PN-0", "PN-Event-0" and "PN-AttributeValue-2"
from the good starting annotations. Each of them
produces a small but semantic consistent word set.
In order to inspect the difference between the
automatic splitting process and the semantic based
one, we compare the numbers of subcategories re-
fined in these two processes. Since it is hard to list
all the semantic-related tags here, three parts of
the semantic-related tags were selected and listed
in Table 6, along with the number of their subcat-
egories. The first part is the noun and verb related
tags, which are most heavily split in both two pro-
cesses. It is clear that the semantic based split-
ting process can generate more subcategories than
the automatic one, because the semantic structures
of noun and verb are sophisticated. The second
part lists the tags that have much more subcate-
gories (? 4) from the automatic splitting process
than the semantic based one, and the third part
vice verse. It can be seen that most of the sub-
categories in the second part are functional cate-
gories, while most of the subcategories in the third
part are content categories. It means that the se-
mantic based splitting process is prone to generat-
ing less subcategory for the functional categories,
but more subcategories for the content categories.
This tendency is in accordance with the linguis-
tic intuition. We believe that it is the main effect
1305
Semantic-related
Automatic split Semantic based
tag number split numebr
NN-Attribute 30 30
NN-AttributeValue 25 27
NN-Entity 32 32
NN-Event 31 30
VV-Attribute 2 2
VV-AttributeValue 27 27
VV-Entity 22 26
VV-Event 29 32
BA-event 13 5
CS-AttributeValue 29 16
CS-entity 22 15
OD-Attribute 13 7
PN-Attribute 26 22
AS-AttributeValue 2 7
JJ-event 4 8
NR-AttributeValue 9 13
NT-event 12 18
VA-AttributeValue 22 27
VA-event 7 11
Table 6: The number of subcategories learned
from two approaches: the automatic hierarchical
state-splitting, and the semantic based splitting.
of our knowledge-based criterion, because it ad-
justs the splitting results dynamically with seman-
tic knowledge, which can alleviate the overfitting
risk.
5 Conclusions
In this paper, we present a novel approach to in-
tegrate semantic knowledge into the hierarchical
state-split process for grammar refinement, which
yields better accuracies on Chinese than previ-
ous methods. The improvements are mainly ow-
ing to two aspects. Firstly, the original treebank
is initialized by annotating the tag of each POS
node with the most general hypernym of its ter-
minal word, which reduces the search space for
the EM algorithm and brings an initial restrict to
the following splitting step. Secondly, the splitting
process is supervised by a knowledge-based crite-
rion with the new semantic-related tags. Benefit-
ting from the hierarchical semantic knowledge, the
proposed approach alleviates the overfitting risk in
a knowledge-driven manner. Experimental results
reveal that the semantic knowledge is of great use
to syntactic disambiguation. The further analysis
on the refined grammars shows that, our method
tends to split the content categories more often
than the baseline method and the function classes
less often.
Acknowledgments
We thank Yaozhong Zhang for the enlighten-
ing discussions. We also thank the anony-
mous reviewers who gave very helpful com-
ments. The work was supported in part by the
National Natural Science Foundation of China
(60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA010103), the National Key Ba-
sic Research Program of China (2004CB318005,
2004CB318105).
References
E. Agirre, T. Baldwin and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with
sense information. In Proc. of ACL?08, pages 317-
325.
D. Bikel. 2000. A statistical model for pars-
ing and word-sense disambiguation. In Proc. of
EMNLP/VLC?2000, pages 155-163.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479-511.?
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. of
AAAI?97, pages 598-603.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL?00, pages 132-139.
E Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxEnt discriminative reranking.
In Proc. of ACL?05, pages 173-180.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In Proc. of COLING?02, pages
183-189.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. of ACL?97, pages 16-
23.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, U. of Penn-
sylvania.
Z. Dong and Q. Dong. 2000. HowNet Chinese-
English conceptual database. Technical Re-
port Online Software Database, Released at ACL.
http://www.keenage.com.
1306
S. Fujita, F. Bond, S. Oepen and T. Tanaka 2007. Ex-
ploiting semantic information for HPSG parse se-
lection. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 25-32.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613-
631.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL?03, pages 423-430.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Prob-
abilistic CFG with latent annotations. In Proc. of
ACL?05, pages 75-82.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Proc. of ARPA-HLT Workshop., pages 240-243.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proc. of COLING-ACL?06, pages
443?440.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL?07,
pages 404-411.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005.
Parsing the Penn Chinese treebank with semantic
knowledge. In Proc. of IJCNLP?05, pages 70-81.
N. Xue, F.-D. Chiou, and M. Palmer. 2002. Building
a large scale annotated Chinese corpus. In Proc. of
COLING?02, pages 1-8.
Y. Zhang. 2008. The Study and Realization of Chinese
Parsing with Semantic and Sentence Type Informa-
tion. Master thesis, Peking University.
1307
An Improved CRF based Chinese Language Processing System for SIGHAN
Bakeoff 2007
Xihong Wu, Xiaojun Lin, Xinhao Wang, Chunyao Wu, Yaozhong Zhang and Dianhai Yu
Speech and Hearing Research Center
State Key Laboratory of Machine Perception,
Peking University, China, 100871
{wxh,linxj,wangxh,wucy,zhangyaoz,yudh}@cis.pku.edu.cn
Abstract
This paper describes three systems: the
Chinese word segmentation (WS) system,
the named entity recognition (NER) sys-
tem and the Part-of-Speech tagging (POS)
system, which are submitted to the Fourth
International Chinese Language Processing
Bakeoff. Here, Conditional Random Fields
(CRFs) are employed as the primary mod-
els. For the WS and NER tracks, the n-
gram language model is incorporated in our
CRFs based systems in order to take into ac-
count the higher level language information.
Furthermore, to improve the performances
of our submitted systems, a transformation-
based learning (TBL) technique is adopted
for post-processing.
1 Introduction
Among 24 closed and open tracks in this bakeoff, we
participated in 23 tracks, except the open NER track
of MSRA. Our systems are ranked 1st in 6 tracks,
and get close to the top level in several other tracks.
Recently, Maximum Entropy model(ME) and
CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai
Zhao et al, 2006) turned out to be promising in natu-
ral language processing tracks, and obtain excellent
performances on most of the test corpora of Bake-
off 2005 and Bakeoff 2006. Compared to the gen-
erative models, like HMM, the primary advantage
of CRFs is that it relaxes the independence assump-
tions, which makes it able to handle multiple inter-
acting features between observation elements (Wal-
lach et al, 2004).
However, the ME and CRFs emphasize the rela-
tion of the basic units of sequence, like the Chinese
characters in these tracks. While, the higher level
information, like the relationship of the words is ig-
nored. From this point of view, the n-gram language
model is incorporated in our CRFs based systems in
order to cover the word level language information.
Based on several pilot-experimental results, we
found that the tagging errors always follow some
patterns. In order to find those error patterns and cor-
rect the similar errors, we integrated the TBL post-
processor in our systems. In addition, extra train-
ing data, which is transformed from People Daily
Corpus (Shiwen Yu et al, 2000) with some auto-
extracted transition rules, is used in each corpus for
the open tracks of WS.
The remainder of this paper is organized as fol-
lows. The scheme of our three developed systems
are described in section 2, 3 and 4, respectively. In
section 5, evaluation results based on these systems
are enumerated and discussed. Finally some conclu-
sions are drawn in section 6.
2 Word Segmentation
The WS system mainly consists of three compo-
nents, CRFs, n-gram language model and post-
processing strategies.
2.1 Conditional Random Fields
Conditional Random Fields, as the statistical se-
quence labeling models, achieve great success in
natural language processing, such as chunking (Fei
Sha et al, 2003) and word segmentation (Hai Zhao
et al, 2006). Different from traditional generative
155
Sixth SIGHAN Workshop on Chinese Language Processing
model, CRFs relax the constraint of the indepen-
dence assumptions, and therefore turn out to be more
suitable for natural language tasks.
CRFs model the conditional distribution p(Y |X)
of the labels Y given the observations X directly
with the formulation:
P?(Y |X) = 1Z(X)exp{
?
c?C
?
k
?kfk(Yc, X, c)}
(1)
Y is the label sequence, X is the observation se-
quence, Z(X) is a normalization term, fk is a fea-
ture function, and c is the set of cliques in Graphic.
In our tasks, C = {(yi?1, yi)}, X is the Chinese
character sequence of a sentence.
To label a Chinese character, we need to define
the label tags. Here we have six types of tags ac-
cording to character position in a word (Hai Zhao et
al., 2006):
tag = {B1, B2, B3, I, E, S}
?B1, B2, B3, I, E? represent the first, second, third,
continue, and end character positions in a multi-
character word, and ?S? is the single-character word
tag.
The unigram feature templates used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0)
CnCn+1Cn+2 (n = ?1)
Where C0 refers to the current character and
C?n(Cn) is the nth character to the left(right) of the
current character. We also use the basic bigram fea-
ture template which denotes the dependency on the
previous tag and current tag.
2.2 Multi-Model Integration
In order to integrate multi-model information, we
use a log-linear model(Och et al, 2002) to compute
the posterior probability:
Pr (W |C) = p?M1 (W |C)
= exp[
?M
m=1 ?mhm(W,C)]
?
W ? exp[
?M
m=1 ?mhm(W ?, C)]
(2)
Where W is the word sequence, and C is the char-
acter sequence. The decision rule here is:
W0 = argmaxW {Pr(W |C)}
= argmaxW {
M
?
m=1
?mhm(W,C)} (3)
The parameters ?M1 of this model can be opti-
mized by standard approaches, such as the Mini-
mum Error Rate Training used in machine transla-
tion (Och, 2003). In fact, the CRFs approach is
a special case of this framework when we define
M = 1 and use the following feature function:
h1(W,C) = logP?(Y |X) (4)
In our approach, the logarithms of the scores gen-
erated by the two kinds of models are used as feature
functions:
h1(W,C) = logPcrf (W,C)
= log
?
w
i
P?(wi|C) (5)
h2(W,C) = logPlm(W ) (6)
The first feature function(Eq.5) comes from CRFs.
Instead of computing the score of the whole la-
bel sequence Y with character sequence X through
P?(Y |X) directly, we try to get the posterior prob-
ability of a sub-sequence to be tagged as one whole
word P?(wi|C). Then we combine all the score of
words together. The second feature function(Eq.6)
comes from n-gram language model, which aims to
catch the words information.
The log-linear model with the feature functions
described above allows the dynamic programming
search algorithm for efficient decoding. The system
generates the word lattice with posterior probability
P?(wi|C). Then the best word sequence is searched
on the word lattice with the decision rule(Eq.3).
Since arbitrary sub-sequence can be viewed as a
candidate word in word lattice, we need to deal with
the problem of OOV words. The unigram of an OOV
word is estimated as:
Unigram(OOV Word) = pl (7)
where p is the minimal value of unigram scores in
the language model; l is the length of the OOV
word, which is used as a punishment factor to
avoid overemphasizing the long OOV words (Xin-
hao Wang et al, 2006).
2.3 Post-Processing Strategies
The division and combination rule, which has been
proved to be useful in our system of Bakeoff 2006
(Xinhao Wang et al, 2006), is adopted for the post-
processing in the system.
156
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Training Data Transition
For the WS open tracks, the unique difference from
closed tracks is that the additional training data is
supplemented for model refinement.
For the Simplified Chinese tracks, the additional
training data are collected from People Daily Cor-
pus with a set of auto-extracted transition rules. This
process is performed in a heuristic strategy and con-
tains five steps as follows:
(1) Segment the raw People Daily texts with the cor-
responding system for the closed track of each cor-
pus.
(2) Compare the result of step 1 with People Daily
Corpus to get the conflict pairs. For example,
{pair1: ??? vs. ???}
(Zhemin Jiang)
{pair2: ??? vs. ???}
(catch with two hands)
In each pair, the left phrase follows the People Daily
Corpus segmentation guideline, while the right one
is the phrase obtained from step 1.
(3) Divide the pairs into two sets: the first set con-
tains the pairs with right phrase appearing in the tar-
get training data; the other pairs are in the second
set.
(4) Select sentences which contain the left phrase of
the pairs in the second set from People Daily Cor-
pus.
(5) Transform these selected sentences by replacing
their phrase in the left side of the pair in the first set
to the right one. This is used as our transition rules.
3 Named Entity Recognition
The named entity recognition track is viewed as a
character sequence tagging problem in our NER sys-
tem and the log-linear model mentioned above is
employed again to integrate multi-model informa-
tion. To find the error patterns and correct them,
a TBL strategy is then used in the post-processing
module.
3.1 Model Description
In this NER track, we employe the log-linear model
and use the logarithms of the scores generated by the
two types of models as feature functions. Besides
CRFs, another model is the class-based n-gram lan-
guage model:
h1(Y, X) = logPcrf (Y, X)
= logP?(Y |X) (8)
h2(Y, X) = logPclm(Y, X) (9)
Y is the label sequence and X is the character se-
quence.
CRFs are used to generate the N-best tagging re-
sults with the scores of whole label sequence Y on
character sequence X by P?(Y |X). And then, the
log-linear model is used to reorder the N-best tag-
ging results by integrating the CRFs score and the
class-based n-gram language model score together.
CRFs
In this track, one Chinese character is labeled by
a tag of ten classes, which denoting the beginning,
continue, ending character of a specified named en-
tity or a non-entity character. There are three types
of named entities in these tracks, including person
name, location name and organization name.
In CRFs, the basic features used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0, 1)
CnCn+2 (n = ?1)
Besides basic unigram features, the bigram transi-
tion features considering the previous tag is adopted
with template Cn (n = ?2,?1, 0, 1, 2).
Class-Based N-gram Language Model
For the class-based n-gram language model, we
define that each character is a single class, while
each type of named entity is viewed as a single class.
With the character sequence and label sequence, the
class sequence can be generated. Take this sentence
for instance:
???????????
(But Ibrahimov is not satisfied)
Table 1 shows its class sequence. Class-based n-
gram language model can be trained with class se-
quence.
3.2 TBL
Since the analysis on our experiments shows that the
tagging errors always follow some patterns in NER
track, TBL strategy is adopted in our system to find
these patterns and correct the similar errors.
157
Sixth SIGHAN Workshop on Chinese Language Processing
character sequence ? ? ? ? ? ? ? ? ? ? ?
label sequence N Per-B Per-C Per-C Per-C Per-C Per-E N N N N
class sequence ? PERSON ? ? ? ?
Table 1: A class sequence example
Transformation-based learning is a symbolic ma-
chine learning method, introduced by (Eric Brill,
1995). The main idea in TBL is to generate a set of
transformation rules that can correct tagging errors
produced by the initial process.
There are four main procedures in our TBL
framework: An initial state assignment which is op-
erated by the system we described above; a set of al-
lowable templates for rules, ranging from words in
a 3 positions windows and name entity information
in a 3-word window with their combinations consid-
ered, and rules which are learned according to the
tagging differences between training data and results
generated by our system, at last, those rules are in-
troduced to correct similar errors.
4 POS Tagging
The POS tagging track is to assign the part-of-
speech sequence for the correctly segmented word
sequence. In our system, for the CTB corpus, the
CRFs are adopted; however for the other four cor-
pora, considering the limitations of resources and
time, the ME model is adopted. To improve the per-
formance of ME model, the POS tag of the previous
word is taken as a feature and the dynamic program-
ming strategy is used in decoding.
In the closed track, the features include the basic
features and their combined features. Firstly the pre-
vious and next words of the current word are taken
as the basic features. Secondly, based on the anal-
ysis of the OOV words, the first and last characters
of the current word, as well as the length of the cur-
rent word are proven to be effective features for the
OOV POS. Furthermore since the long distance con-
straint word may impact the POS of current word
(Yan Zhao et al, 2006), in the open track, a Chi-
nese parser is imported and the word depended on
the current word is extracted as feature.
5 Experiments and Results
We have participated in 23 tracks, except the open
NER track of MSRA. CRFs, ME model and n-gram
language model are adopted in these systems. Our
implementation uses the CRF++ package1 provided
by Taku Kudo, the Maximum Entropy Toolkit2 pro-
vided by Zhang Le, and the SRILM Toolkit provided
by Andreas Stolcke (Andreas Stolcke et al, 2002).
5.1 Chinese Word Segmentation
In the closed tracks, CRFs and bigram language
model are trained on the given training data for each
corpus. In order to integrate these two models, it is
necessary to train the corresponding parameter ?M1
with Minimum Error Rate Training approache based
on a development data. Since the development data
is not provided in this bakeoff, a ten-fold cross val-
idation approach is employed to implement the pa-
rameter training. A set of parameters can be trained
independently, and then the mean value is calculated
as the estimation of each parameter.
Table 2 gives the results of our WS system for
closed tracks.
baseline +LM +LM+Post
CTB 94.7 94.7 94.8
NCC 92.6 92.4 92.9
SXU 94.7 95.7 95.8
CITYU 92.9 93.7 93.9
CKIP 93.2 93.7 93.7
Table 2: Word segmentation performance on F-
value with different approach for the closed tracks
In the open tracks, as we do not have enough time
to finish the parameter estimation on the new data,
our system adopt the same parameters ?M1 used in
closed tracks. The unique difference from closed
1http://chasen.org/taku/software/CRF++
2http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
158
Sixth SIGHAN Workshop on Chinese Language Processing
tracks is that extra training data is added for each
corpus to improve the performance. For the Sim-
plified Chinese tracks, additional data comes from
People Daily Corpus which is transformed by our
transition strategy. At the same time, for the Tra-
ditional Chinese tracks, additional data comes from
the training and testing data used in the early Bake-
off. However, we implement two systems for the
CTB open track. The system (a) takes the training
and testing data used in the early Bakeoff as addi-
tional data, and System (b) takes the translated Peo-
ple Daily Corpus as additional data. Table 3 gives
the results of our open WS system.
baseline +LM +LM+Post
CTB(a) 99.2 99.2 99.3
CTB(b) 95.6 95.1 97.0
NCC 93.7 93.0 92.9
SXU 96.4 87.0 95.8
CITYU 95.8 90.6 91.0
CKIP 94.5 94.8 95.1
Table 3: Word segmentation performance on F-
value with different approach for the open tracks
The result shows that the system performance is
sensitive to the parameters ?M1 . Although we train
the useful parameter for closed tracks, it plays a bad
role in open tracks as we do not adapt it for the ad-
ditional training data.
5.2 Named Entity Recognition
In the closed NER tracks, CRFs and class-based tri-
gram language model are trained on the given train-
ing data for each corpus. The same approach em-
ployed in the WS tracks is adopted to train the corre-
sponding parameter ?M1 in our NER systems. Mean-
while, the TBL rules trained via five-fold cross val-
idation approach are also used in post-processing
procedure. Table 4 reports the results of our closed
NER system.
5.3 POS Tagging
The experiments show that the CRFs/ME method is
superior to the TBL method, and the concurrent er-
rors for these two methods are less than 60%. There-
fore we adopted TBL to correct the output results
of CRFs/ME: If the output tags of CRFs/ME and
baseline +LM +LM+Post
MSRA 89.3 89.7 89.9
CITYU 79.3 80.6 80.5
Table 4: Named entity recognition F-value through
different approaches for the closed tracks
TBL are not consistent and the output probability
of CRFs/ME is below a certain threshold, the TBL
results are fixed. Here the 90% of the training set
is taken as the training data and remained 10% is
separated as the development data to get the thresh-
old, which is 0.60 for the CRFs, and 0.90 for the
ME. In addition, the POS tagged corpus of the Chi-
nese Treebank 5.0 from LDC is added to the training
data for CTB open track. In our system, the Berke-
ley Parser (Slav Petrov et al, 2006) is adopted to
obtain the long distance constraint words. The per-
formance achieved by the methods described above
on each corpus are reported in Table 5.
CRFs/ME CRFs/ME
CRFs/ME TBL +TBL +TBL
+Syntax
CTIYU 88.7 87.7 89.1 89.0
CKIP 91.8 91.4 92.2 92.1
CTB 94.0 92.7 94.3 96.5
NCC 94.6 94.3 94.9 95.0
PKU 93.5 93.2 94.0 94.1
Table 5: POS tagging performance on total-accuracy
with different approach
6 Conclusion
In this paper, we have briefly described our systems
participating in the Bakeoff 2007. In the WS and
NER systems, the log-linear model is adopted to in-
tegrate CRFs and language model, which improves
the system performances effectively. At the same
time, system integration approach used in the POS
system also proves its validity. In addition, a heuris-
tic strategy is imported to generate additional train-
ing data for the open WS tracks. Finally, several
post-processing strategies are used to further im-
prove our systems.
159
Sixth SIGHAN Workshop on Chinese Language Processing
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word Seg-
mentation. Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing. pp. 161-164.
Jeju Island, Korea.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, Christopher Manning. 2005. A Conditional
Random Field Word Segmenter for Sighan Bakeoff
2005. Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing. pp. 168-171. Jeju
Island, Korea.
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing.
pp. 162-165. Sydney, Australia.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction. Technical Report, UPenn CIS TR
MS-CIS-04-21.
Shiwen Yu, Xuefeng Zhu and Huiming Duan. 2000.
Specification of large-scale modern Chinese corpus.
Proceedings of ICMLP?2001. pp. 18-24. Urumqi,
China.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. Proceedings of Hu-
man Language Technology/NAACL. pp. 213-220. Ed-
monton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL). pp. 295-302. Philadelphia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proceedings of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL). pp. 160-167. Sapporo,
Japan.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, Xi-
hong Wu. 2006. Chinese Word Segmentation with
Maximum Entropy and N-gram Language Model. the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing. pp. 138-141. Sydney, Australia.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in Part-of-Speech tagging. Computational Lingusitics.
21(4).
Yan Zhao, Xiaolong Wang, Bingquan Liu, and Yi Guan.
2006. Fusion of Clustering Trigger-Pair Features for
POS Tagging Based on Maximum Entropy Model.
Journal of Computer Research and Development.
43(2). pp. 268-274.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings of International
Conference on Spoken Language Processing. pp. 901-
904. Denver, Colorado.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the ACL. pp. 433-440.
Sydney, Australia.
160
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 269?272,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Text Segmentation with LDA-Based Fisher Kernel
Qi Sun, Runxin Li, Dingsheng Luo and Xihong Wu
Speech and Hearing Research Center, and
Key Laboratory of Machine Perception (Ministry of Education)
Peking University
100871, Beijing, China
{sunq,lirx,dsluo,wxh}@cis.pku.edu.cn
Abstract
In this paper we propose a domain-
independent text segmentation method,
which consists of three components. Latent
Dirichlet alocation (LDA) is employed to
compute words semantic distribution, and we
measure semantic similarity by the Fisher
kernel. Finally global best segmentation is
achieved by dynamic programming. Experi-
ments on Chinese data sets with the technique
show it can be effective. Introducing latent
semantic information, our algorithm is robust
on irregular-sized segments.
1 Introduction
The aim of text segmentation is to partition a doc-
ument into a set of segments, each of which is co-
herent about a specific topic. This task is inspired
by problems in information retrieval, summariza-
tion, and language modeling, in which the ability
to provide access to smaller, coherent segments in
a document is desired.
A lot of research has been done on text seg-
mentation. Some of them utilize linguistic criteria
(Beeferman et al, 1999; Mochizuki et al, 1998),
while others use statistical similarity measures to
uncover lexical cohesion. Lexical cohesion meth-
ods believe a coherent topic segment contains parts
with similar vocabularies. For example, the Text-
Tiling algorithm, introduced by (Hearst, 1994), as-
sumes that the local minima of the word similarity
curve are the points of low lexical cohesion and thus
the natural boundary candidates. (Reynar, 1998)
has proposed a method called dotplotting depending
on the distribution of word repetitions to find tight
regions of topic similarity graphically. One of the
problems with those works is that they treat terms
uncorrelated, assigning them orthogonal directions
in the feature space. But in reality words are corre-
lated, and sometimes even synonymous, so that texts
with very few common terms can potentially be on
closely related topics. So (Choi et al, 2001; Brants
et al, 2002) utilize semantic similarity to identify
cohesion. Unsupervised models of texts that capture
semantic information would be useful, particularly
if they could be achieved with a ?semantic kernel?
(Cristianini et al, 2001) , which computes the simi-
larity between texts by also considering relations be-
tween different terms. A Fisher kernel is a function
that measures the similarity between two data items
not in isolation, but rather in the context provided
by a probability distribution. In this paper, we use
the Fisher kernel to describe semantic information
similarity. In addition, (Fragkou et al, 2004; Ji and
Zha, 2004) has treated this task as an optimization
problem with global cost function and used dynamic
programming for segments selection.
The remainder of the paper is organized as fol-
lows. In section 2, after a brief overview of our
method, some key aspects of the algorithm are de-
scribed. In section 3, some experiments are pre-
sented. Finally conclusion and future research di-
rections are drawn in section 4.
2 Methodology
This paper considers the sentence to be the smallest
unit, and a block b is the segment candidate which
consists of one or more sentences. We employ LDA
269
model (Blei et al, 2003) in order to find out latent
semantic topics in blocks, and LDA-based Fisher
kernel is used to measure the similarity of adjacent
blocks. Each block is then given a final score based
on its length and semantic similarity with its previ-
ous block. Finally the segmentation points are de-
cided by dynamic programming.
2.1 LDA Model
We adopt LDA framework, which regards the cor-
pus as mixture of latent topics and uses document as
the unit of topic mixtures. In our method, the blocks
defined in previous paragraph are regarded as ?doc-
uments? in LDA model.
The LDA model defines two corpus-level parame-
ters ? and ?. In its generative process, the marginal
distribution of a document p(d|?, ?) is given by the
following formula:
?
p(?|?)(
N?
n=1
?
k
p(zk|?d)p(wn|zk, ?))d?
where d is a word sequence (w1, w2, ...wN ) of
length N . ? parameterizes a Dirichlet distribution
and derives the document-related random variable
?d, then we choose a topic zk, k ? {1...K} from the
multinomial distribution of ?d. Word probabilities
are parameterized by a k?V matrix ? with V being
the size of vocabulary and ?vk = P (w = v|zk). We
use variational EM (Blei et al, 2003) to estimate the
parameters.
2.2 LDA-Based Fisher Kernel
In general, a kernel function k(x, y) is a way of mea-
suring the resemblance between two data items x
and y. The Fisher kernel?s key idea is to derive a ker-
nel function from a generative probability model. In
this paper we follow (Hofmann, 2000) to consider
the average log-probability of a block, utilizing the
LDA model. The likelihood of b is given by:
l(b) =
N?
i=1
P? (wi|b) log
K?
k=1
?wik?(k)b
where the empirical distribution of words in the
block P? (wi|b) can be obtained from the number of
word-block co-occurrence n(b, wi), normalized by
the length of the block.
The Fisher kernel is defined as
K(b1, b2) = 5T? l(b1)I?1 5? l(b2)
which engenders a measure of similarity between
any two blocks b1 and b2. The derivation of the
kernel is quite straightforward and following (Hof-
mann, 2000) we finally have the result:
K(b1, b2) = K1(b1, b2) +K2(b1, b2), with
K1(b1, b2) =
?
k
?(k)b1 ?
(k)
b2 /?(k)corpus
K2(b1, b2) =
?
i P? (wi|b1)P? (wi|b2)
?
k
P (zk|b1,wi)P (zk|b2,wi)
P (wi|zk)
where K1(b1, b2) is a measure of how much b1 and
b2 share the same latent topic, taking synonymy
into account. And K2(b1, b2) is the traditional inner
product of common term frequencies, but weighted
by the degree to which these terms belong to the
same latent topic, taking polysemy into account.
2.3 Cost Function and Dynamic Programming
The local minima of LDA-based Fisher kernel sim-
ilarities indicate low semantic cohesion and seg-
mentation candidates, which is not enough to get
reasonably-sized segments. The lengths of segmen-
tation candidates have to be considered, thus we
build a cost function including two parts of infor-
mation. Segmentation points can be given in terms
of a vector ~t = (t0, ..., tm, ..., tM ), where tm is the
sentence label with m indicating the mth block. We
define a cost function as follows:
J(~t;?) =
M?
m=1
?F (ltm+1,tm+1)
+ K(btm?1+1,tm , btm+1,tm+1)
where F (ltm+1,tm+1) is equal to
(ltm+1,tm+1??)2
2?2 and
ltm+1,tm+1 is equal to tm+1?tm indicating the num-
ber of sentences in block m. The LDA-based ker-
nel function measures similarity of block m? 1 and
block m, where block m?1 spans sentence tm?1+1
to tm and block m spans sentence tm + 1 to tm+1
The cost function is the sum of the costs of as-
sumed unknown M segments, each of which is
made up of the length probability of block m and the
similarity score of block m with its previous block
m ? 1. The optimal segmentation ~t gives a global
minimum of J(~t;?).
270
3 Experiments
3.1 Preparation
In our experiments, we evaluate the performance of
our algorithms on Chinese corpus. With news docu-
ments from Chinese websites, collected from 10 dif-
ferent categories, we design an artificial test corpus
in the similar way of (Choi, 2000), in which we
take each n-sentence document as a coherent topic
segment, randomly choose ten such segments and
concatenate them as a sample. Three data sets, Set
3-5, Set 13-15 and Set 5-20, are prepared in our ex-
periments, each of which contains 100 samples. The
data sets? names are represented by a range number
n of sentences in a segment.
Due to generality, we take three indices to eval-
uate our algorithm: precision, recall and error rate
metric (Beeferman et al, 1999) . And all exper-
imental results are averaged scores generated from
the individual results of different samples. In order
to determine appropriate parameters, some hold-out
data are used.
We compare the performance of our methods with
the algorithm in (Fragkou et al, 2004) on our test
set. In particular, the similarity representation is a
main difference between those two methods. While
we pay attention to latent topic information behind
words of adjacent blocks, (Fragkou et al, 2004) cal-
culates word density as the similarity score function.
3.2 Results
In order to demonstrate the improvement of LDA-
based Fisher kernel technique in text similarity eval-
uation, we omit the length probability part in the cost
function and compare the LDA-based Fisher kernel
and the word-frequency cosine similarity by the er-
ror rate Pk of segmenting texts. Figure 1 shows
the error rates for different sets of data. On av-
erage, the error rates are reduced by as much as
about 30% over word-frequency cosine similarity
with our methods, which shows Fisher kernel sim-
ilarity measure,with latent topic information added
by LDA, outperforms traditional word similarity
measure. The performance comparisons drawn from
Set 3-5 and Set 13-15 indicates that our similarity al-
gorithm can uncover more descriptive statistics than
traditional one especially for segments with less sen-
tences due to its prediction on latent topics.
set 3-5 set  13-15 set 5-20
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
P
k
 LDA-based Fisher kernel
 Word-Frequency Cosine Similarity
Figure 1: Error Rate Pk on different data sets with differ-
ent similarity metrics.
In the cost function, there are three parameters ?
, ? and ?. We determine appropriate ? and ? with
hold-out data. For the value of ?, we take it between
0 and 1 because the length part is less important than
the similarity part according to our preliminary ex-
periments. We design the experiment to study ??s
impact on segmentation by varying it over a certain
range. Experimental results in Figure 2 show that
the reduce of error rate achieved by our algorithm
is in a range from 14.71% to 53.93%. Set 13-15
achieves best segmentation performance, which in-
dicates the importance of text structure: it is easier
to segment the topic with regular length and more
sentences. The performance on Set 5-20 obtains the
best improvement with our methods, which illus-
trates that LDA-based Fisher kernel can express text
similarity more exactly than word density similarity
on irregular-sized segments.
Table 1: Evaluation against different algorithms on Set
5-20.
Algo. Pk Recall Precision
TextTiling 0.226 66.00% 60.72 %
P. Fragkou Algo. 0.344 69.00% 37.92 %
Our Algo. 0.205 59.00% 62.27 %
While most experiments of other authors were
taken on short regular-sized segments which was
firstly presented by (Choi, 2000), we use compar-
atively long range of segments, Set 5-20, to evaluate
different algorithms. Table 1 shows that, in terms of
271
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
lambda
Pk
Set 3?5Set 13?15Set 5?20Set 3?5Set 13?15Set 5?20
Figure 2: Error Rate Pk when the ? changes. There are
two groups of lines, the solid lines representing algorithm
of (Fragkou et al, 2004) while the dash ones indicate
performance of our algorithm, and each line in a group
shows error rates in different data sets.
Pk, our algorithm employing dynamic programming
as P. Fragkou Algo. achieves the best performance
among those three. As for long irregular-sized text
segmentation, although local even-sized blocks sim-
ilarity provides more exact information than the sim-
ilarity between global irregular-sized texts, with the
consideration of latent topic information, the latter
will perform better in the task of text segmentation.
Though the performance of the proposed method is
not superior to TextTiling method, it avoids thresh-
olds selection, which makes it robust in applications.
4 Conclusions and Future Work
We present a new method for topic-based text seg-
mentation that yields better results than previously
methods. The method introduces a LDA-based
Fisher kernel to exploit text semantic similarities and
employs dynamic programming to obtain global op-
timization. Our algorithm is robust and insensitive
to the variation of segment length. In the future,
we plan to investigate more other similarity mea-
sures based on semantic information and to deal
with more complicated segmentation tasks. Also,
we want to exam the factor importance of similar-
ity and length in this text segmentation task.
Acknowledgments
The authors would like to thank Jiazhong Nie for his help
and constructive suggestions. The work was supported
in part by the National Natural Science Foundation of
China (60435010; 60535030; 60605016), the National
High Technology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the National
Key Basic Research Program of China (2004CB318005),
and the New-Century Training Program Foundation for
the Talents by the Ministry of Education of China.
References
Doug Beeferman, Adam Berger and John D. Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34(1-3):177?210.
David M. Blei and Andrew Y. Ng and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of machine
Learning Research 3: 993?1022.
Thorsten Brants, Francine Chen and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic Analysis. CIKM
?02211?218
Freddy Choi, Peter Wiemer-Hastings and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. Proceedings of 6th EMNLP, 109?117.
Freddy Y. Y. Choi. 2000. Advances in Domain Inde-
pendent Linear Text Segmentation. Proceedings of
NAACL-00.
Nello Cristianini, John Shawe-Taylor and Huma Lodhi.
2001. Latent Semantic Kernels. Proceedings of
ICML-01, 18th International Conference on Machine
Learning 66?73.
Pavlina Fragkou, Petridis Vassilios and Kehagias Athana-
sios. 2004. A Dynamic Programming Algorithm for
Linear Text Segmentation. J. Intell. Inf. Syst., 23(2):
179?197.
Marti Hearst. 1994. Multi-Paragraph Segmentation of
Expository Text. Proceedings of the 32nd. Annual
Meeting of the ACL, 9?16.
Thomas Hofmann. 2000. Learning the Similarity of
Documents: An Information-Geometric Approach to
Document Retrieval and Categorization. Advances in
Neural Information Processing Systems 12: 914?920.
Xiang Ji and Hongyuan Zha. 2003. Domain-
Independent Text Segmentation Using Anisotropic
Diffusion and Dynamic Programming. Proceedings
of the 26th annual international ACM SIGIR Confer-
ence on Research and Development in Informaion Re-
trieval, 322?329.
Hajime Mochizuki, Takeo Honda and Manabu Okumura.
1998. Text Segmentation with Multiple Surface Lin-
guistic Cues. Proceedings of the COLING-ACL?98,
881-885.
Jeffrey C. Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. PhD thesis. University of
Pennsylvania.
272
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 138?141,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation with Maximum Entropy
and N-gram Language Model
Wang Xinhao, Lin Xiaojun, Yu Dianhai, Tian Hao, Wu Xihong
National Laboratory on Machine Perception,
School of Electronics Engineering and Computer Science,
Peking University, China, 100871
{wangxh,linxj,yudh,tianhao,wxh}@cis.pku.edu.cn
Abstract
This paper presents the Chinese word seg-
mentation systems developed by Speech
and Hearing Research Group of Na-
tional Laboratory on Machine Perception
(NLMP) at Peking University, which were
evaluated in the third International Chi-
nese Word Segmentation Bakeoff held by
SIGHAN. The Chinese character-based
maximum entropy model, which switches
the word segmentation task to a classi-
fication task, is adopted in system de-
veloping. To integrate more linguistics
information, an n-gram language model
as well as several post processing strate-
gies are also employed. Both the closed
and open tracks regarding to all four cor-
pora MSRA, UPUC, CITYU, CKIP are
involved in our systems? evaluation, and
good performance are achieved. Espe-
cially, in the closed track on MSRA, our
system ranks 1st.
1 Introduction
Chinese word segmentation is one of the core tech-
niques in Chinese language processing and attracts
lots of research interests in recent years. Sev-
eral promising methods are proposed by previous
researchers, in which Maximum Entropy (ME)
model has turned out to be a successful way for
this task (Hwee Tou Ng et al, 2004; Jin Kiat
Low et al, 2005). By employing Maximum En-
tropy (ME) model, the Chinese word segmentation
task is regarded as a classification problem, where
each character will be classified to one of the four
classes, i.e., the beginning, middle, end of a multi-
character word and a single-character word.
However, in a high degree, ME model pays its
emphasis on Chinese characters while debases the
consideration on the relationship of the context
words. Motivated by this view, several strategies
used for reflecting the context words? relationship
and integrating more linguistics information, are
employed in our systems.
As known, an n-gram language model could ex-
press the relationship of the context words well, it
therefore as a desirable choice is imported in our
system to modify the scoring of the ME model.
An analysis on our preliminary experiments shows
the combination ambiguity is another issue that
should be specially tackled, and a division and
combination strategy is then adopted in our sys-
tem. To handle the numeral words, we also intro-
duce a number conjunction strategy. In addition,
to deal with the long organization names problem
in MSRA corpus, a post processing strategy for
organization name is presented.
The remainder of this paper is organized as fol-
lows. Section 2 describes our system in detail.
Section 3 presents the experiments and results.
And in last section, we draw our conclusions.
2 System Description
With the ME model, n-gram language model, and
several post processing strategies, our systems are
established. And detailed description on these
components are given in following subsections.
2.1 Maximum Entropy Model
The ME model used in our system is based on the
previous works (Jin Kiat Low et al, 2005; Hwee
Tou Ng et al, 2004). As mentioned above, the
ME model based word segmentation is a 4-classes
learning process. Here, we remarked four classes,
i.e. the beginning, middle, end of a multi-character
138
word and a single-character word, as b, m, e and s
respectively.
In ME model, the following features (Jin Kiat
Low et al, 2005) are selected:
a) cn (n = ?2,?1, 0, 1, 2)
b) cncn+1 (n = ?2,?1, 0, 1)
c) c?1c+1
where cn indicates the character in the left or right
position n relative to the current character c0.
For the open track especially, three extended
features are extracted with the help of an external
dictionary as follows:
d) Pu (c0)
e) L and t0
f) cnt0 (n = ?1, 0, 1)
where Pu(c0) denotes whether the current charac-
ter is a punctuation, L is the length of word W that
conjoined from the character and its context which
matching a word in the external dictionary as long
as possible. t0 is the boundary tag of the character
in W.
With the features, a ME model is trained which
could output four scores for each character with
regard to four classes. Based on scores of all char-
acters, a completely segmented semiangle matrix
can be constructed. Each element wji in this ma-
trix represents a word that starts at the ith charac-
ter and ends at jth character, and its value ME(j, i),
the score for these (j ? i+1) characters to form a
word, is calculated as follow:
ME[j, i] = ? log p(w = ci...cj)
= ? log[p(bci)p(mci+1)...
p(mcj?1)p(ecj )]
(1)
As a consequence, the optimal segmentation re-
sults corresponding to the best path with the low-
est overall score could be reached via a dynamic
programming algorithm. For example:
@?c????(I was 19 years old that year)
Table 1 shows its corresponding matrix. In this
example, the ultimate segmented result is:
@ ?c ? ???
2.2 Language Model
N-gram language model, a widely used method
in natural language processing, can represent the
context relation of words. In our systems, a bi-
gram model is integrated with ME model in the
phase of calculating the path score. In detail, the
score of a path will be modified by adding the bi-
gram of words with a weight ? at the word bound-
aries. The approach used for modifying path score
is based on the following formula.
V [j, i] = ME[j, i]
+mini?1k=1{[(V [i ? 1, k]
+?Bigram(wk,i?1, wi,j)}
(2)
where V[j,i] is the score of local best path which
ends at the jth character and the last word on the
path is wi,j = ci...cj , the parameter ? is optimized
by the test set used in the 2nd International Chi-
nese Word Segmentation Bakeoff. When scoring
the path, if one of the words wk,i?1 and wi,j is out
of the vocabulary, their bigram will backoff to the
unigram. And the unigram of the OOV word will
be calculated as:
Unigram(OOV Word) = pl (3)
where p is the minimal unigram value of words in
vocabulary; l is the length of the word acting as
a punishment factor to avoid overemphasizing the
long OOV words.
2.3 Post Processing Strategies
The analysis on preliminary experiments, where
the ME model and n-gram language model are in-
volved, lead to several post processing strategies
in developing our final systems.
2.3.1 Division and Combination Strategy
To handle the combination ambiguity issue,
we introduce a division and combination strategy
which take in use of unigram and bigram. For
each two words A and B, if their bigrams does
not exist while there exists the unigram of word
AB, then they can be conjoined as one word. For
example, ??ff(August)? and ???(revolution)?
are two segmented words, and in training set the
bigram of ??ff? and ???? is absent, while
the word ??ff??(the August Revolution)? ap-
peares, then the character string ??ff??? is
conjoined as one word. On the other hand, for a
word C which can be divided as AB, if its uni-
gram does not exit in training set, while the bigram
of its subwords A and B exits, then it will be re-
segmented. For example, Taking the word ??L
N?U?(economic system reform)? for instance,
if its corresponding unigram is absent in training
set, while the bigram of two subwords ??LN
139
@ ? c ? ? ? ?
1 2 3 4 5 6 7
@ 1 6.3180e-07
? 2 33.159 7.5801
c 3 26.401 0.0056708 5.2704
? 4 71.617 45.221 49.934 3.1001e-07
? 5 83.129 56.734 61.446 33.869 7.0559
? 6 90.021 63.625 68.337 40.760 12.525 12.534
? 7 77.497 51.101 55.813 28.236 0.0012012 10.077 10.055
Table 1: A completely segmented matrix
?(economic system)? and ?U?(reform)? exists,
as a consequence, it will be segmented into two
words ??LN?? and ?U??.
2.3.2 Numeral Word Processing Strategy
The ME model always segment a numeral
word into several words. For instance, the word
?4.34(RMB Yuan 4.34)?, may be segmented
into two words ?4.? and ?34?. To tackle this
problem, a numeral word processing strategy is
used. Under this strategy, those words that contain
Arabic numerals are manually marked in the train-
ing set firstly, then a list of high frequency charac-
ters which always appear alone between the num-
bers in the training set can be extracted, based on
which numeral word issue can be tackled as fol-
lows. When segmenting one sentence, if two con-
joint words are numeral words, and the last char-
acter of the former word is in the list, then they are
combined as one word.
2.3.3 Long Organization Name Processing
Strategy
Since an organization name is usually an OOV,
it always will be segmented as several words, es-
pecially for a long one, while in MSRA corpus, it
is required to be recognized as one word. In our
systems, a corresponding strategy is presented to
deal with this problem. Firstly a list of organiza-
tion names is manually selected from the training
set and stored in the prefix-tree based on charac-
ters. Then a list of prefixes is extracted by scan-
ning the prefix-tree, that is, for each node, if the
frequencies of its child nodes are all lower than the
predefined threshold k and half of the frequency of
the current node, the string of the current node will
be extracted as a prefix; otherwise, if there exists
a child node whose frequency is higher than the
threshold k, scan the corresponding subtree. In the
same way, the suffixes can also be extracted. The
only difference is that the order of characters is in-
verse in the lexical tree.
During recognizing phase, to a successive
words string that may include 2-5 words, will be
combined as one word, if all of the following con-
ditions are satisfied.
a) Does not include numbers, full stop or comma.
b) Includes some OOV words.
c) Has a tail substring matching some suffix.
d) Appears more than twice in the test data.
e) Has a higher frequency than any of its substring which
is an OOV word or combined by multiple words.
f) Satisfy the condition that for any two successive words
w1 w2 in the strings, freq(w1w2)/freq(w1)?0.1, unless w1
contains some prefix in its right.
3 Experiments and Results
We have participated in both the closed and open
tracks of all the four corpora. For MSRA corpus
and other three corpora, we build System I and
System II respectively. Both systems are based on
the ME model and the Maximum Entropy Toolkit
1, provided by Zhang Le, is adopted.
Four systems are derived from System I with re-
gard to whether or not the n-gram language model
and three post processing strategies are used on the
closed track of MSRA corpus. Table 2 shows the
results of four derived systems.
System R P F ROOV RIV
IA 95.0 95.7 95.3 66.0 96.0
IB 96.0 95.6 95.8 60.3 97.3
IC 96.4 96.0 96.2 60.3 97.7
ID 96.4 96.1 96.3 61.2 97.6
Table 2: The effect of MEmodel, n-gram language
model and three post processing strategies on the
closed track of MSRA corpus.
System IA only adopts the ME model. System
IB integrates the ME model and the bigram lan-
guage model. System IC integrates the division
and combination strategy and the numeral words
1http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html
140
processing strategy. System ID adds the long or-
ganization name processing strategy.
For the open track of MSRA, an external dictio-
nary is utilized to extract the e and f features. The
external dictionary is built from six sources, in-
cluding the Chinese Concept Dictionary from In-
stitute of Computational Linguistics, Peking Uni-
versity(72,716 words), the LDC dictionary(43,120
words), the Noun Cyclopedia(111,633), the word
segmentation dictionary from Institute of Com-
puting Technology, Chinese Academy of Sci-
ences(84,763 words), the dictionary from Insti-
tute of Acoustics, and the dictionary from Insti-
tute of Computational Linguistics, Peking Univer-
sity(68,200 words) and a dictionary collected by
ourselves(63,470 words).
The union of the six dictionaries forms a big
dictionary, and those words appearing in five or
six dictionaries are extracted to form a core dic-
tionary. If a word belongs to one of the following
dictionaries or word sets, it is added into the exter-
nal dictionary.
a) The core dictionary.
b) The intersection of the big dictionary and the training
data.
c) The words appearing in the training data twice or more
times.
Those words in the external dictionaries will be
eliminated, if in most cases they are divided in
the training data. Table 3 shows the effect of ME
model, n-gram language model, three post pro-
cessing strategies on the open track of MSRA.
Here System IO only adopts the basic features,
while the external dictionary based features are
used in four derived systems related to open track:
IA, IB, IC, ID.
System R P F ROOV RIV
IO 96.0 96.5 96.3 71.1 96.9
IA 97.5 96.9 97.2 65.9 98.6
IB 97.6 96.8 97.2 64.8 98.7
IC 97.7 97.0 97.4 66.8 98.8
ID 97.7 97.1 97.4 67.5 98.8
Table 3: The effect of MEmodel, n-gram language
model, three post processing strategies on the open
track of MSRA.
System II only adopts ME model, the division
and combination strategy and the numeral word
processing strategy. In the open track of the cor-
pora CKIP and CITYU, the training set and test set
from the 2nd Chinese Word Segmentation Backoff
are used for training. For the corpora UPUC and
CITYU, the external dictionaries are used, which
is constructed in the same way as that in the open
track of MSRA Corpus. Table 4 shows the official
results of system II on UPUC, CKIP and CITYU.
Corpus R P F ROOV RIV
UPUC-C 93.6 92.3 93.0 68.3 96.1
UPUC-O 94.0 90.7 92.3 56.1 97.6
CKIP-C 95.8 94.8 95.3 64.6 97.2
CKIP-O 95.8 94.8 95.3 64.7 97.2
CITYU-C 96.9 97.0 97.0 77.3 97.8
CITYU-O 97.9 97.6 97.7 81.3 98.5
Table 4: Official results of our systems on UPUC
CKIP and CITYU
On the UPUC corpus, an interesting observation
is that the performance of the open track is worse
than the closed track. The investigation and analy-
sis lead to a possible explanation. That is, the seg-
mentation standard of the dictionaries, which are
used to construct the external dictionary, is differ-
ent from that of the UPUC corpus.
4 Conclusion
In this paper, a detailed description on several Chi-
nese word segmentation systems are presented,
where ME model, n-gram language model as well
as three post processing strategies are involved. In
the closed track of MSRA, the integration of bi-
gram language model greatly improves the recall
ratio of the words in vocabulary, although it will
impairs the performance of system in recognizing
the words out of vocabulary. In addition, three
strategies are introduced to deal with combination
ambiguity, numeral word, long organization name
issues. And the evaluation results reveal the valid-
ity and effectivity of our approaches.
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
A maximum Entropy Approach to Chinese Word
Segmentation. 2005. Preceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pp. 161-164.
Hwee Tou Ng and Jin Kiat Low. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? 2004. Preceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing(EMNLP), pp. 277-284.
Zhang Huaping and Liu Qun. Model of Chinese
Words Rough Segmentation Based on N-Shortest-
Paths Method. 2002. Journal of Chinese Informa-
tion Processing, 28(1):pp. 1-7.
141
A New Approach to Automatic Document Summarization 
Xiaofeng Wu 
National Laboratory of Pattern Recognition,  
Institute of Automation, 
Chinese Academy of Sciences 
Beijing, China 
xfwu@nlpr.ia.ac.cn 
Chengqing Zong 
National Laboratory of Pattern Recognition, 
Institute of Automation, 
Chinese Academy of Sciences 
Beijing, China 
cqzong@nlpr.ia.ac.cn 
 
 
Abstract 
In this paper we propose a new approach 
based on Sequence Segmentation Models 
(SSM) to the extractive document summa-
rization, in which summarizing is regarded 
as a segment labeling problem. Comparing 
with the previous work, the difference of 
our approach is that the employed features 
are obtained not only from the sentence 
level, but also from the segment level. In 
our approach, the semi-Markov CRF model 
is employed for segment labeling. The pre-
liminary experiments have shown that the 
approach does outperform all other tradi-
tional supervised and unsupervised ap-
proaches to document summarization. 
1 Introduction 
Document summarization has been a rapidly 
evolving subfield of Information Retrieval (IR) 
since (Luhn, 1958). A summary can be loosely 
defined as a text that is produced from one or more 
texts and conveys important information of the 
original text(s). Usually it is no longer than half of 
the original text(s) or, significantly less (Radev et 
al., 2002). Recently, many evaluation competitions 
(like the Document Understanding Conference 
DUC ?http://duc.nist.gov?, in the style of NIST?s 
TREC), provided some sets of training corpus. It is 
obvious that, in the age of information explosion, 
document summarization will be greatly helpful to 
the internet users; besides, the techniques it uses 
can also find their applications in speech tech-
niques and multimedia document retrieval, etc. 
The approach to summarizing can be catego-
rized in many ways. Some of them are: 1) indica-
tive, informative and evaluative, according to func-
tionality; 2) single-document and multi-document, 
according to the amount of input documents; 3) 
generic and query-oriented, according to applica-
tions. Yet the taxonomy currently widely em-
ployed is to categorize summarization into abstrac-
tive and extractive. 
According to (Radev et al, 2002), all methods 
that are not explicitly extractive are categorized as 
abstractive. These approaches include ontological 
information, information fusion, and compression. 
Abstract-based summarization never goes beyond 
conceptual stage, though ever since the dawn of 
summarization it has been argued as an alternative 
for its extract-based counterpart. On the other hand, 
extractive summarization is still attracting a lot of 
researchers (Yeh et al, 2005) (Daum?e III and 
Marcu, 2006) and many practical systems, say, 
MEAD ?http://www.summarization.com/mead/?, 
have been produced. Using supervised or unsuper-
vised machine learning algorithms to extract sen-
tences is currently the mainstream of the extractive 
summarization. However, all pervious methods 
focus on obtaining features from the sentence gra-
nularity. 
In this paper we focus on generating summariza-
tion by using a supervised extractive approach in 
which the features are obtained from a larger gra-
nularity, namely segment. The remainder of the 
paper is organized as follows: Section 2 introduces 
the related work concerning the extract-based 
summarization. Section 3 describes our motiva-
tions. Our experiments and results are given in 
Section 4, and Section 5 draws the conclusion and 
mentions the future work. 
126
2 Related Work 
Early researchers approached the summarization 
problem by scoring each sentence with a combina-
tion of the features like word frequency and distri-
bution, some proper names (Luhn, 1958), sentence 
positions in a paragraph (Baxendale, 1958), and 
sentence similarity (Gong, 2001) etc. The results 
were comparatively good. Most supervised extrac-
tive methods nowadays focus on finding powerful 
machine learning algorithms that can properly 
combine these features. 
Bayesian classifier was first applied to summari-
zation by (Pedersen and Chen, 1995), the authors 
claimed that the corpus-trained feature weights 
were in agreement with (Edmundson, 1969), which 
employed a subjective combination of weighted 
features. Another usage of the na?ve Bayesian 
model in summarization can be found in (Aone et 
al., 1997). Bayesian model treats each sentence 
individually, and misses the intrinsic connection 
between the sentences. (Yeh et al, 2005) employed 
genetic algorithm to calculate the belief or score of 
each sentence belonging to the summary, but it 
also bears this shortcoming. 
To overcome this independence defect, (Conroy 
and O?leary, 2001) pioneered in deeming this prob-
lem as a sequence labeling problem. The authors 
used HMM, which has fewer independent assump-
tions. However, HMM can not handle the rich lin-
guistic features among the sentences either. Re-
cently, as CRF (Lafferty and McCallum, 2001) has 
been proved to be successful in part-of-speech tag-
ging and other sequence labeling problems, (Shen 
et al, 2007) attempted to employ this model in 
document summarization. CRF can leverage all 
those features despite their dependencies, and ab-
sorb other summary system?s outcome. By intro-
ducing proper features and making a comparison 
with SVM, HMM, etc., (Shen et al, 2007) claimed 
that CRF could achieve the best performance. 
All these approaches above share the same 
viewpoint that features should be obtained at sen-
tence level. Nevertheless, it can be easily seen that 
the non-summary or summary sentences tend to 
appear in a consecutive manner, namely, in seg-
ments. These rich features of segments can surely 
not be managed by those traditional methods.  
Recently, Sequence Segmentation Model (SSM) 
has attracted more and more attention in some 
traditional sequence learning tasks. SSM builds a 
direct path to encapsulate the rich segmental 
features (e.g., entity length and the similarity with 
other entities, etc., in entity recognition). Semi-
CRF (Sarawagi and Cohen, 2004) is one of the 
SSMs, and generally outperforms CRF. 
3 Motivations 
According to the analysis in Section 2, our basic 
idea is clear that we regard the supervised summa-
rizing as a problem of sequence segmentation. 
However, in our approach, the features are not only 
obtained on the sentence level but also on the seg-
ment level.  
Here a segment means one or more sentences 
sharing the same label (namely, non-summary or 
summary), and a text is regarded as a sequence of 
segments. Semi-CRF is a qualified model to ac-
complish the task of segment labeling, besides it 
shares all the virtues of CRF. Using semi-CRF, we 
can easily leverage the features both in traditional 
sentence level and in the segment level. Some fea-
tures, like Log Likelihood or Similarity, if obtained 
from each sentence, are inclined to give unex-
pected results due to the small granularity. Fur-
thermore, semi-CRF is a generalized version of 
CRF. The features designed for CRF can be used 
in semi-CRF directly, and it has been proved that 
semi-CRF outperforms CRF in some Natural Lan-
guage Processing (NLP) problems (Sarawagi and 
Cohen, 2004).  
In the subsections below, we first introduce 
semi-CRF then describe the features we used in 
our approach. 
3.1 Semi-CRF 
CRF was first introduced in (Lafferty and 
McCallum, 2001). It is a conditional model P(Y|X), 
and here both X and Y may have complex structure. 
The most prominent merits of CRF are that it 
offers relaxation of the strong independence 
assumptions made in HMM or Maximum Entropy 
Markov Models (MEMM) (McCallum, 2000) and 
it is no victim of the label bias problem. Semi-CRF 
is a generalization version of sequential CRF. It 
extends CRF by allowing each state to persist for a 
non-unit length of time. After this time has elapsed, 
the system might transmit to a new state, which 
only depends on its previous one. When the system 
is in the ?segment of time?, it is allowed to behave 
non-Markovianly. 
127
3.1.1 CRF vs. Semi-CRF 
Given an observed sentence sequence 
X=(x1,x2,?,xM). The corresponding output labels 
are Y=(y1,y2,?,yM), where yi gets its value from a 
fixed set ?. For document summarization, 
?={0,1}. Here 1 for summary and 0 for non-
summary. The goal of CRF is to find a sequence of 
Y, that maximize the probability: 
      
1
( | , ) exp( ( , ))
( )
P Y X W W F X Y
Z X
= ?          (1) 
Here?  is a vertical vector of 
size T. The vertical vector 
1
( , ) f ( , , )
M
i
F X Y i X Y==?
1 2
'f ( , , ..., )Tf f f= means 
there are T feature functions, and each of them can 
be written as ft(i,X,Y)?R,t?(1,?,T),i?(1,?,M). 
For example, in our experiment the 10th feature 
function is expressed as: [if the length of current 
sentence is bigger than the predefined threshold 
value]&[if the current sentence is a summary]. 
When this feature function is acting upon the third 
sentence in text_1 with label_sequence_1, the fol-
lowing feature equation f10(3,text_1, la-
bel_sequence_1) means: in text_1 with la-
bel_sequence_1, [if the length of the third sentence 
is bigger than the predefined threshold value]&[if 
the third sentence is a summary]. W is a horizontal 
vector of size T that represents the weights of these 
features respectively. Equation (2) gives the defini-
tion of Z(X), which is a normalization constant that 
makes the probabilities of all state sequences sum 
to 1. 
'( ) exp( ( , '))YZ X W F X= ?? Y
|
     (2) 
If we change the sequence vector X to 
S=<s1,s2,?,sN>, which means one way to split X 
into N segments, we have the semi-CRF. Each 
element in S is a triple: Sj=<tj,uj,yj>, which de-
notes the jth segment in this way of segmentation. 
In the triple, tj denotes the start point of the jth seg-
ment, uj denotes its end position, and yj is the out-
put label of the segment (recall the example at the 
beginning of this subsection that there is only one 
output for a segment). Under this definition, seg-
ments should have no overlapping, and satisfy the 
following conditions: 
1
| | |
N
j
js X=
=?                                    (3) 
    (4) 1 11, | |,1 | |, 1N j j jt u X t u X t u+= = ? ? ? = +
Here, |?| denotes the length of?. 
 
Figure 1  A 10-sentences text with label sequence 
 
For example, one way to segment a text of 10 sen-
tences in Figure 1 is S=<(1,1,1),(2,4,0),(5,5,1), 
(6,9,0),(10,10,1)> . The circles in the second row 
represent sentences, and actually are only some 
properties of the corresponding sentences. 
Consequently, the feature function f in CRF 
converts to the segmental feature function 
g=(g1,g2,?,g T?). Like f, gt(i,x,s) ?R also maps a 
triple (i,x,s) to a real number. Similarly, we may 
define . Now we give the 
final equation used to estimate the probability of S.           
Given a sequence X and feature weight W, we have 
1
( , ) g( , , )
N
i
G X S i X S== ?
1
( | , ) exp( ( , ))
( )
P S X W W G X S
Z X
= ?          (5) 
Here,  
'
( ) exp( ( , '))
S
Z X W G X
??
= ? S?                 (6) 
Where, { }all segmentations allowed? = ? ? . 
3.1.2 Inference 
The inference or the testing problem of semi-CRF 
is to find the best S that maximizes Equation (5). 
We use the following Viterbi-like algorithm to cal-
culate the optimum path. 
Suppose the longest segment in corpus is K, let 
S1:i,y represent all possible segmentations starting 
from 1 to i , and the output of the last segment is y. 
V(i,y) denotes the biggest value of P(S?|X,W). Note 
that it?s also the largest value of W?G(X,S?), 
S??S1:i,y. 
Compared with the traditional Viterbi algorithm 
used in CRF, the inference for semi-CRF is more 
time-consuming. But by studying Algorithm 1, we 
can easily find out that the cost is only linear in K. 
j
128
      
3.1.3 Parameter Estimation 
Define the following function 
log ( | , )
( ( , ) log (    
lW l l
l l l l
L P S X W
W G X S Z X
= ?
= ? ?? ))   (8) 
In this approach, the problem of parameter estima-
tion is to find the best weight W that maximizes LW. 
According to (Bishop, 2006), the Equation (8) is 
convex. So it can be optimized by gradient ascent. 
Various methods can be used to do this work (Pie-
tra et al 1997). In our system, we use L-BFGS, a 
quasi-Newton algorithm (Liu and Nocedal. 1989), 
because it has the fast converging speed and effi-
cient memory usage. APIs we used for estimation 
and inference can be found in website 
?http:\\crf.sourcefourge.net?. 
3.2 Features 
(Shen et al 2007) has made a thorough investiga-
tion of the performances of CRF, HMM, and SVM. 
So, in order to simplify our work and make it com-
parable to the previous work, we shape our desig-
nation of features mainly under their framework.  
The mid column in Table 1 lists all of the fea-
tures we used in our semi-CRF approach. For the 
convenience of comparison, we also list the name 
of the features used in (Shen et al 2007) in the 
right column, and name them Regular Features. 
The features in bold-face in the mid column are the 
corresponding features tuned to fit for the usage of 
semi-CRF. We name them Extended Features. 
There are some features that are not in bold-face in 
the mid column. These features are the same as the 
Regular Features in the right column. We also 
used them in our approach. The mark star denotes 
that there is no counterpart. We number these fea-
tures in the left column.  Algorithm 1: 
Step1. Initialization: 
 Let V i  ( , ) 0,  0
 
No. semi-CRF CRF 
1 Ex_Position        Position 
2 Ex_Length         Length 
3 Ex_Log_Likelihood  Log Likeli-hood 
 
4 
Ex_Similarity_to_ 
Neighboring_   
Segments           
Similarity to 
Neighboring 
Sentences 
5 Ex_Segment_ 
Length     * 
6 Thematic           Thematic  
7 Indicator           Indicator  
8 Upper Case         Upper Case  
y for i= =
Step2. Induction: 
 0for i >   
', 1,...,( , ) max ( , ')
           g( , ', , 1, )
y k KV i y V i k y
W y y x i d i
== ?
+ ? ? +         (7)  
Step3. Termination and path readout: 
       max (| |, )ybestSegment V X y=
               Table 1. Features List 
The details of the features we used in semi-
CRF are explained as follow. 
Extended Features: 
Ex_Position: is an extended version of the Po-
sition feature. It gives the description of the po-
sition of a segment in the current segmentation. 
If the sentences in the current segment contain 
the beginning sentence of a paragraph, the value 
of this feature will be 1, 2 if it contains the end 
of a paragraph; and 3 otherwise; 
Ex_Length: the number of words in the cur-
rent segment after removing some stop-words. 
Ex_Log_Likelihood: the log likelihood of the 
current segment being generated by the docu-
ment. We use Equation (9) below to calculate 
this feature. N(wj,si) denotes the number of oc-
currences of the word wj in the segment si, and 
we use ( , ) / ( , )
k
j w k
N w D N w D?  to estimate the 
probability of a word being generated by a doc-
ument. 
log ( | ) ( , ) log ( | )
j
i j iw j
P s D N w s p w D=?      (9) 
Ex_Similarity_to_Neighboring_Segments: 
we define the cosine similarity based on the 
TF*IDF (Frakes &Baeza-Yates, 1992) between 
a segment and its neighbors. But unlike (Shen et 
al. 2007), in our work only the adjacent neighbors 
of the segment in our work are considered. 
EX_Segment_Length: this feature describes 
the number of sentences contained in a segment. 
129
All these features above are actually an ex-
tended version used in the regular CRF (or in 
other supervised model). It is easy to see that, if 
the segment length is equal to 1, then the fea-
tures will degrade to their normal forms.  
There are some features that are also used in 
semi-CRF but we don?t extend them like those 
features above. Because the extended version of 
these features leads to no improvement of our 
result. These features are: 
Regular features we used: 
Thematic: with removing of stop words, we 
define the words with the highest frequency in 
the document to be the thematic words. And this 
feature gives the count of these words in each 
sentence. 
Indicator: indicative words such as ?conclu-
sion? and ?briefly speaking? are very likely to be 
included in summary sentences, so we define 
this feature to signal if there are such words in a 
sentence. 
Upper Case: some words with upper case are 
of high probability to be a name, and sentences 
with such words together with other words 
which the author might want to emphasize are 
likely to be appeared in a summary sentence. So 
we use this feature to indicate whether there are 
such words in a sentence. 
It should be noted that theoretically the num-
ber of extended features obtained from the cor-
pus goes linearly with K in Equation (7). 
 
4 Experiments 
4.1 Corpus & Evaluation Criteria 
To evaluate our approach, we applied the widely 
used test corpus of (DUC2001), which is spon-
sored by ARDA and run by NIST 
?http://www.nist.gov?. The basic aim of DUC 
2001 is to further progress of summarization and 
enable researchers to participate into large-scale 
experiments. The corpus DUC2001 we used con-
tains 147 news texts, each of which has been la-
beled manually whether a sentence belongs to a 
summary or not. Because in (Shen et al 2007) all 
the experiments were conducted upon DUC2001, 
we may make a comparison between the sequence 
labeling models and the sequence segmentation 
modes we used. The only preprocessing we did is 
to remove some stop words according to a stop 
word list.  
We use F1 score as the evaluation criteria which is 
defined as: 
2*Precesion*Recall
1
Precesion+Recall
F =                 (10) 
We used 10-fold cross validation in order to reduce 
the uncertainty of the model we trained. The final 
F1 score reported is the average of all these 10 ex-
periments. 
All those steps above are strictly identical to the 
work in (Shen et al 2007), and its result is taken as 
our baseline. 
4.2 Results & Analysis 
As we mentioned in Sub-Section 3.2, those ex-
tended version of features only work when seg-
ment length is bigger than one. So, each of these 
extended version of features or their combination 
can be used together with all the other regular fea-
tures listed in the right column in Table 1. In order 
to give a complete test of the capacity of all these 
extended features and their combinations, we do 
the experiments according to the power set of {1, 2, 
3, 4, 5} (the numbers are the IDs of these extended 
features as listed in Table 1), that is we need to do 
the test 25-1 times with different combinations of 
the extended features. The results are given in Ta-
ble 2. The rows with italic fonts (1, 3, 5, 7, 9, 11, 
13), in Table 2 denote the extended features used. 
For example, ?1+2? means that the features 
Ex_Positon and the Ex_Length are together used 
with all other regular features are used.  
Table 2. Experiment results. 
 1 2 3 4 5 
F1 0.395 0.391 0.398 0.394 0.392 
 1+2 1+3 1+4 1+5 2+3 
F1 0.395 0.396 0.396 0.395 0.382 
 2+4 2+5 3+4 3+5 4+5 
F1 0.389 0.384 0.398 0.399 0.380 
 1+2+3 1+2+4 1+2+5 1+3+4 1+3+5
F1 0.398 0.397 0.393 0.403 0.402 
 1+4+5 2+3+4 2+3+5 2+4+5 3+4+5
F1 0.402 0.403 0.401 0.403 0.404 
 1+2 +3+4 
1+2 
+3+5 
1+2 
+4+5 
1+3 
+4+5 
2+3 
+4+5 
F1 0.407 0.404 0.406 0.402 0.404 
 All CRF 
F1 0.406 0.389 
130
Other rows (2, 4, 6, 8, 10, 12, 14) give F1 scores 
corresponding to the features used. 
     In Table 3 we compare our approach with some 
of the most popular unsupervised methods, includ-
ing LSA (Frakes & Baeza-Yates, 1992) and HITS 
(Mihalcea 2005). The experiments were conducted 
by (Shen et al 2007). 
Table 3 Comparison with unsupervised methods 
 
From the results in Table 2 we can see that indi-
vidually applying these extended features can im-
prove the performance somewhat. The best one of 
these extended features is feature 3, as listed in the 
2nd row, the 5th column. The highest improvement, 
1.8%, is obtained by combining the features 1, 2, 3 
and 4. Although a few of the combinations hurt the 
performance, most of them are helpful. This veri-
fies our hypothesis that the extended features under 
SSM have greater power than the regular features. 
The results in Table 3 demonstrate that our ap-
proach significantly outperforms the traditional 
unsupervised methods. 8.3% and 4.9% improve-
ments are respectively gained comparing to LSA 
and HITS models 
Currently, the main problem of our method is 
that the searching space goes large by using the 
extended features and semi-CRF, so the training 
procedure is time-consuming. However, it is not so 
unbearable, as it has been proved in (Sarawagi and 
Cohen, 2004). 
5 Conclusion and Future Work 
In this paper, we exploit the capacity of semi-CRF , 
we also make a test of most of the common fea-
tures and their extended version designed for doc-
ument summarization. We have compared our ap-
proach with that of the regular CRF and some of 
the traditional unsupervised methods. The com-
parison proves that, because summary sentences 
and non-summary sentences are very likely to 
show in a consecutive manner, it is more nature to 
obtain features from a lager granularity than sen-
tence.  
In our future work, we will test this approach on 
some other well known corpus, try the complex 
features used in (Shen et al 2007), and reduce the 
time for training. 
 
Acknowledgements 
The research work described in this paper has been 
funded by the Natural Science Foundation of Chi-
na under Grant No. 60375018 and 60121302. 
     
References 
C.Aone, N. Charocopos, J. Gorlinsky. 1997. An 
Intelligent Multilingual Information Browsing and 
Retrieval System Using Information Extraction. In 
ANLP, 332-339. 
P.B. Baxendale. 1958. Man-made Index for Tech-
nical Literature -An Experiment. IBM Journal of 
Research and Development, 2(4):354-361.  
C.M. Bishop. 2006. Linear Models for Classifica-
tion, Pattern Recognition and Machine Learning, 
chapter 4, Springer. 
J. M. Conroy and D. P. O?leary. 2001. Text Sum-
marization via Hidden Markov Models. In SIGIR, 
406-407. 
Hal Daum?e III, and D. Marcu.  2006. Bayesian 
Query- Focused Summarization, In ACL 
H. P. Edmundson. 1969. New Methods in Auto-
matic Extracting. Journal of the Association for 
Computing Machinery, 16(2):264-285. 
W. B. Frakes, R. Baeza-Yates, 1992, Information 
Retrieval Data Structures & Algorithms. Prentice 
Hall PTR, New Jersey  
Y. H. Gong and X. Liu. 2001. Generic text summa-
rization using relevance measure and latent seman-
tic analysis. In SIGIR, 19-25 
J. Kupiec, J. Pedersen, and F. Chen. 1995. A 
Trainable Document Summarizer. Research and 
Development in Information Retrieval, 68-73 
J. D. Lafferty, A. McCallum and F. C. N. Pereira. 
2001. Conditional random fields: probabilistic 
models for segmenting and labeling sequence data. 
ICML, 282-289. 
D. C. Liu and J. Nocedal. 1989. On the limited 
memory BFGS method for large-scale optimiza-
tion. Mathematic Programming, 45:503-528. 
H. P. Luhn. 1958. The Automatic Creation of Lit-
erature Abstracts. IBM Journal of Research and 
Development, 2(2): 159 -165. 
 LSA HITS Seim-CRF 
F1 0.324 0.368 0.407 
131
A. McCallum, D. Freitag, and F. Pereira. 2000. 
Maximum entropy Markov models for information 
extraction and segmentation. In ICML, 591-598 
Mihalcea R. Mihalcea. 2005. Language independ-
ent extractive summarization. In AAAI, 1688-1689 
S. D. Pietra, V. D. Pietra, and J. D. Lafferty. 1997. 
Inducing features of random fields. IEEE Tran. on 
Pattern Analysis and Machine Intelligence, 
19(:)380?393. 
D. R. Radev, E. Hovy and K. McKeown. 2002. 
Introduction to the Special Issue on Summarization. 
Computational Linguistics, 28(4): 399-408.  
S. Sarawagi and W.W. Cohen. 2004. Semi-markov 
conditional random fields for information extrac-
tion.In NIPS 
D. Shen, J. T. Sun, H. Li, Q. Yang, Z. Chen. 2007. 
Document Summarization using Conditional Ran-
dom Fields? In IJCAI, 1805-1813 
J. Y. Yeh, H. R. Ke, W. P. Yang and I. H. Meng. 
2005. Text summarization using trainable summar-
izer and latent semantic analysis. IPM, 41(1): 75?
95  
 
132
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 79?86,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Realization of the Chinese BA-construction in an English-Chinese 
Machine Translation System 
 
Xiaohong Wu 
Centre Tesni?re, Facult? des Lettres 
Universit? de Franche-Comt? 
Besan?on, France 
wuxaiohong@voila.fr 
Sylviane Cardey 
Centre Tesni?re, Facult? des Lettres 
Universit? de Franche-Comt? 
Besan?on, France 
Sylviane.cardey@univ-fcomte.fr 
 
Peter Greenfield 
Centre Tesni?re, Facult? des Lettres 
Universit? de Franche-Comt? 
Besan?on, France 
Peter.greenfield@univ-fcomte.fr 
 
Abstract 
The BA-construction refers to a special 
grammatical structure in Mandarin 
Chinese. It is an extremely important 
syntactic structure in Chinese, which is 
frequently used in daily life. The study of 
the BA-construction has attracted the 
attention of almost all linguists who are 
interested in this language. Yet it is a 
quite complex and difficult linguistic 
phenomenon and it is hard to analyze it 
satisfactorily to cope with the syntactic 
structure(s) of another language which 
does not possess this kind of construction 
(e.g. in machine translation). This paper 
discusses a few methods on how some of 
the English imperative sentences are 
realized by the Chinese BA-construction 
which is mandatory in transferring 
certain source language (SL) information 
into target language (TL) in an 
experimental machine translation (MT) 
system.  We also introduce the basic 
syntactic structures of the BA-
construction and explain how we 
formalize and control these structures to 
satisfy our need. Some features related to 
the BA-construction, such as 
obligatoriness versus the optionality, the 
semantics as well as the properties of the 
elements preceding and following the BA 
are also discussed. Finally we suggest 
that by constraining the variations of the 
formalized patterns of the BA-
construction, a better MT could be 
reached. 
1 Introduction 
BA-construction (? ? ) is a special 
syntactic structure in the Chinese language. It is 
so frequently used in everyday conversations that 
its usage can not be simply ignored. In fact, the 
BA-construction has been greatly drawing the 
attention of almost all linguists who are 
interested in the Chinese language. The reason 
for this concentrates not only on the fact that it is 
a quite special Chinese linguistic phenomenon 
but also that until now no consensus has been 
reached among linguists on whether its 
grammatical category belongs to that of a verb or 
that of a preposition. Historically speaking, much 
evidence shows that it was used more as a verb 
than as a preposition. However, recent research 
tends to classify the BA-construction to the 
category of the prepositional phrase (PP), which 
characterizes the pre-posed object (usually a 
noun phrase ? NP) of a transitive verb (Zhou and 
PU, 1985). In the following sections we will first 
introduce very briefly the different points of 
view held by these linguists and then we will 
demonstrate our choice for the study of the BA-
construction in our experimental English-
Chinese machine translation system, which is 
based on the controlled language technique. We 
will particularly stress the problems we face 
when transferring certain English imperative 
sentences into Chinese sentences containing the 
BA-construction which is mandatory in some 
cases, while this is optional in other cases, or can 
be used as one of the other alternatives (between 
79
a normal syntactic structure (V + NP + X1 ) and 
the BA-construction (BA + NP + V + X). 
2 The BA-Construction: a verb phrase 
or a prepositional phrase? 
    It is important to note that we do not pretend 
to give an overview of all kinds of points of view 
on the study of the BA-construction here, nor do 
we claim to justify all the different conceptions 
held in the literature in this short paper. Instead, 
we just try to verify how our practice with this 
construction can be better formulated for our 
specific purpose: to be well adapted to serve for 
an English-Chinese MT system.  
     Whether the word BA ( ) in the BA-
construction is a verb or a preposition is an open 
question in Chinese linguistics. Due to the 
difficulty of having sufficient and strong 
evidence to distinguish the BA-construction 
between a verb and a preposition, some linguists 
also call the BA and some other words which 
possess the same property, such as BEI ( ) etc., 
a ?coverb? ( , literally: a sub-
verb) which share the properties of both a verb 
and a preposition. As a result of no consensus 
among linguists, the analysis of this construction 
is divided into two separate schools: that of a 
verb phrase (VP) and that of prepositional phrase 
(PP) or one that is more inclined to one of the 
schools than the other. The first school of 
linguists states that the BA-construction should 
be considered as a VP whose surface structure 
resembles a lot the serial-verb constructions (
) (Subj + V + (NP1+2 ) + V2 + (NP2) ?), 
(see example 1 b). Like a serial verb 
construction, the first V can be represented by 
the word BA and form a BA-construction. In 
their opinion, the BA shows the characteristics of 
the other parallel verbs which are used in the 
serial-verb construction (refers to any surface 
string with more than one verb in a sentence). 
Furthermore, some features of the BA indicate 
that the elements following the BA make up a 
constituent in which the BA looks more like a 
verbal head taking a complement (Bender, 2002), 
(Hashimoto, 1971), (Ma, 1985), and (Her, 1990), 
for example: 
1 a)  
                                                          
1
 X: a non-null variable, usually an adverb or a PP 
2
 
+: refers to the possibility of more than one NP. 
(literal translation: Zhang San BA (V1) Li Si 
hit (V2) LE3  (ASP) a punch, Wang Wu kick 
(V3) LE (ASP) two foot) 
     Zhang San gave Li Si a punch and Wang 
Wu two kicks. 
b)  
(literal translation: I open (V1) door come 
(V2) in take (V3) book) 
I opened the door and went in to take a book.    
One of their supporting points is that unlike a 
prepositional phrase, the BA-construction can 
not be moved to the beginning of the sentence, 
for example: 
c) *4 , 
Compare this with the following example 
(with a prepositional phrase): 
2 a)  
     (literal translation: He in Beijing buy LE 
(ASP) a BEN (CLS5 ) book;) 
     He bought a book in Beijing. 
   b)  
     (literal translation: In Beijing, he buy 
LE(ASP) a BEN(CL) book) 
     In Beijing, he bought a book. 
Furthermore, like the other verbs, the BA can 
be negated by MEIYOU ( ), for example (1 
a): 
 
(literal translation: Zhang San, MEIYOU6 , 
BA Li Si hit a punch, Wang Wu kick two foot) 
Zhang San did not give Li Si a punch and 
Wang WU two kicks. 
  In addition, like other monosyllable verbs, 
the BA as a verb can be used as the attributive of 
a noun by adding a structural word ?DE ( ?
(STR7 ) between it and the noun, for example, ?
? (read, DE, book; the book to read); ?
?, (listen, DE, song; a song to listen to); ?
? (BA, DE checks; the checks to do/the pass 
to guard) 
                                                          
3
 LE : Aspectual particle indicating a past action 
4
 *: ungrammatical 
5
 CLS: classifier 
6
 MEIYOU (   ): negation = no, not or do not 
7
 STR: structural word usually connects a constituent 
to a NP 
80
 A basic structural analysis of the first school 
is illustrated in Figure 1 ?BA as a Verb? from the 
example cited from (LIN, 2004): 
3)  
(literal translation: Zhang San BA Li Si hit 
LE) 
Zhang San hit Li Si. 
  
 
 
Figure 1 BA as a Verb 
The second school of linguists claims that the 
BA-construction is actually a prepositional 
phrase with its head word followed by a NP 
complement which is moved in front of the 
transitive main verb in the sentence (See 
example 4 a) below). Furthermore, though the 
BA possesses the categorical features of a verb, 
it is hard to qualify the BA to function alone as 
the main verb or predicate in a sentence. In 
addition, in Mandarin Chinese the aspect 
attachments can be used as one of the conditions 
to test the verbhood of a word. The fact is that in 
most cases, if an aspect attachment, such as LE (
), GUO ( ) (expressing past actions) and 
ZHE ( ) (expressing continuous actions), is 
attached to the BA, the whole sentence will look 
strange and become ungrammatical (see below in 
b) and c)).  
4 a) (literal 
translation: He, BA, just now, DE (STR), talks, 
again, speak, LE, one BIAN (CLS)) 
   He repeated what he had said just now. 
Compare the following with aspect 
attachments: 
   b) * (LE) 
   c) * 
(GUO) 
   d) *
          (ZHE) 
Compare with other verb: 
5 a) (LE) 
     (literal translation: he, look, LE, this book) 
  He has read the book. 
b) (GUO) 
  (literal translation: he, look, GUO, this book) 
   He read the book. 
c) 
   He is looking at the book.  
Their point of view concerning this 
construction is also supported by some 
grammatical criteria to test the verbhood of a 
word. For instance, most monosyllable verbs can 
be duplicated as independent ?AA? or ?A A? 
structures in Chinese, for example ?  (see, 
look)? as ? ? or ? ?  ?  (read)? as ?
? or ? ?; ?  (eat)?as ? ? or ?
?; and ?  (go or walk)? as ? ? or ?
?; but never ? ? as ?* ? or ?* ? 
(some transitive verbs can be used this way 
without objects, but the duplicated ? ? or ?
? as a verb must have its object following it, 
e.g. ?  (make checks; to guard a pass, 
etc.)? or ? ?). Furthermore the verb 
following the BA-construction is a transitive 
verb which in fact subcategorizes for (or still 
governs) the pre-posed logical object (the 
complement of the preposition BA) and the main 
verb is usually accompanied by other auxiliary 
constituents following or immediately preceding 
it. In other words, the verb can not stand alone 
after its object is moved in front of it (see in 6 a), 
7 a) and 7 c)) in italics and in blue and the 
ungrammatical sentences 6 c) and 7 d)). Besides, 
Chinese is a thematic language, and the theme is 
often placed in front of the other constituents in 
the sentences accordingly. In many cases, we can 
see that the BA-construction does have an effect 
of emphasis on the semantic content that this 
structure carries (see the comparisons between 6 
a) and 6 b), and between 7 a) and 7 b)). We take 
again the example (4), ?He repeated what he had 
said just now.?, and show it in (6) (HU, 1991).  
Compare: 
6 a)8  
                                                          
8
 The underlined part refers to the BA-construction; 
the italic refers to the auxiliary constituents; and the 
word in bold font refers to the verb. 
? VP 
NP                    V? 
V                               VP 
NP                       V 
Zhang San 
ba 
Li Si da 
hit 
81
(Subj + BA-structure + V + auxiliary 
constituent) 
   b) (Subj + V + Obj) 
   c) *  
7 a) (Subj + BA-structure 
V + LE + auxiliary constituent) 
(literal translation: I BA letter read one BIAN 
(CLS))    I read the letter once. 
   b) (Subj + V + Obj) 
      I read the letter. 
   c) (Subj + BA-
structure + auxiliary constituent + V + LE) 
(literal translation: I BA letter carefully read 
LE)   I have carefully read the letter. 
      d)  *  
As shown in example (6 a, c) and (7 a, c, d), if 
we leave out the auxiliary constituents ? ? in 
(6 a), and ? ?, ? ? and ? ?in (7 a, c), 
both sentences (6 c and 7 d) become 
ungrammatical. Therefore, the syntactic structure 
of the second school can be analyzed as shown in 
Figure 2 ?BA as a Preposition?: 
 
     
Figure 2 BA as a Preposition 
 
Schematically, a BA-construction always has 
the following linear configurations: 
     a) NP* + BA + NP + V + X 
     b) NP* + BA + NP + X + V 
where the sentence can have an optional (in 
many cases) NP* as subject, followed by BA and 
its NP complement, then followed  by a 
transitive V and another constituent X (which 
might precede the verb as shown in (b), and 
usually is an adverb or a prepositional phrase). 
Concerning our own view, we adopt the idea 
that the BA is a preposition with which the 
patient object is shifted to the front of the main 
verb and the BA structure functions as an adjunct 
of the verb like many other adjuncts that are 
often placed between the subject and the 
predicate verb (HU, 1991). The reason for this 
choice is that considering the BA-construction as 
a PP is easier for the syntactic analysis and 
formulation than taking it as a VP in a serial verb 
construction.  
Against this background, we will demonstrate 
in the following section how we formalize the 
BA-construction to cope with its English 
counterpart imperative sentences in our work and 
how these English sentences are finally 
constructed into grammatical target Chinese 
sentences containing the BA-structure. 
3 Formalization of the BA-construction 
The MT system we work with is oriented to 
the automatic translation of medical protocols 
selected from two sub-domains: echinococcosis 
(clinical practice) and molecular cloning 
(laboratory practice), where the predominant 
sentence type is the imperative sentence. Due to 
the fact that the BA-construction is mandatory in 
transferring some of the information conveyed in 
these SL sentences, we have formalized some 
English sentences into Chinese counterpart 
sentences containing the BA-construction. To do 
this, we compare carefully each of the sentence 
pairs in both languages from a parallel bilingual 
corpus which we have constructed for our 
research. In this way, we obtained enough 
evidence to support the formalization of this 
special Chinese construction for our MT system. 
Though the BA-construction is a very productive 
structure from which we can derive many 
varieties in Mandarin Chinese, our observation 
of the corpus reveals that the variations are 
limited but nevertheless indispensable for 
formulation. 
As we have mentioned in the above paragraph, 
we have constructed a parallel bilingual corpus 
for an experimental MT system for the purpose 
of automatic translation of medical protocols 
which are from two different sources: one is on 
echinococcosis, a kind of transmissible disease 
shared by humans and animals, and the other is 
on molecular cloning. Like many other scientific 
documents, the medical texts we collected show 
a high degree of homogeneity in respect of the 
text structure and lexical usage, but often we find 
very long and structurally complicated sentences 
which are difficult to analyze or to be formally 
S 
NP                    VP 
PP                              V? 
V 
Zhang San 
ba 
Li Si 
Da        LE 
hit 
P        NP 
82
represented.  To narrow down the linguistic 
difficulty, we adopt the controlled language 
technique as a supporting method (CARDEY, et 
al. 2004), (WU, 2005). In other words, we first 
make the raw text materials simpler and easier 
for the computer to process, for example, to 
standardize the general structure of the text, the 
terminology, and to constrain the lexical usages 
and the sentence structures, which allows us to 
avoid many complex linguistic phenomena and 
which helps us to design practical controlled 
writing rules. Controlled language has been 
proved to be very feasible in machine translation 
by many systems, e.g. KANT (NYBERG & 
TERUKO, 1996). With the simpler and clearer 
input source sentences, the machine can 
generally produce better output target sentences.  
We finally work with our already well- 
controlled final texts for linguistic analysis which 
is based on unification-based grammar. 
According to our observation, the English 
sentences which have to be transferred into 
Chinese sentences containing the BA-
constructions are of two types, of which one is 
obligatory and the other is optional (with the BA-
construction or no). The typical feature of these 
kinds of sentences is that the main verb in the 
sentence often indicates a kind of change or 
movement; therefore, in both the source and 
target sentence the goal or location of this change 
or movement is represented by a prepositional 
phrase, for example: 
8) Insert a catheter in the cyst. 
          
9) Store the tube on the ice. 
          
The syntactic structure of this kind of sentence 
in the SL can be represented as: 
       S  VP 
       VP  V NP PP 
and we get two basic formulae by applying 
predicate-argument generation for example 8 and 
9: 
      Insert (_, Compl1, in_Compl2) 
      Store (_, Compl1, on_Compl2) 
?_? refers to the position of the verb which may 
vary accordingly.  
From the aligned TL sentence, we can 
formulate the TL sentence as: 
     S  VP 
     VP  PP1 V PP2  
in which the first PP is the BA-structure and the 
second PP corresponds to the PP in the SL. 
Therefore we get two corresponding formulae for 
example 8 and 9 in the TL respectively: 
     (BA_Compl1, _, _Compl2_ ) 
     (BA_Compl1, _,  _Compl2_ ) 
In fact, for example 8 the Chinese translation 
can leave out the second preposition ? ... ( )?, 
for the reason that it is more convenient if we 
lexicalize a Chinese equivalent for the English 
preposition ?in? in the Chinese translation at the 
cost that it is a bit redundant in the TL sometimes, 
but completely grammatical and acceptable. Our 
principle here is that every word should have its 
status in the sentence. So whenever it is possible 
and, in particular acceptable in the TL, we assign 
a correspondence to the SL preposition (or other 
words like adverbs or NP as adjunct) in the TL. 
By doing so, the machine can have a better 
performance in most cases. It is particularly 
beneficial for bi-directional MT. The 
correspondence of a SL preposition is mostly 
composed of two Chinese characters in the 
structure of ?X ? Y?, of which ??? is the 
position of the complement of the preposition in 
question. The second element ?Y? is usually 
considered as a noun indicating the direction or 
location in Chinese. However, in our case, we 
consider it as a disjoint part of the first 
preposition ?X?. In other words, the ?X?Y? 
structure is considered as one language unit in 
our practice. The lexicalization of a prepositional 
phrase in the TL is also one of our criteria to test 
if a sentence has to be constructed with the BA-
structure or not. Most importantly this practice 
can reduce the workload of writing too many 
grammatical rules for the system, for example 
when a preposition has to be translated into 
Chinese and when it needs not to, etc. 
Like most of the English imperative sentences, 
the Chinese counterpart sentences start with 
verbs. However, in some cases, the BA-
construction is also employed. Generally 
speaking, many of the sentences can be used in 
both ways: to start with a verb or start with the 
BA-construction. They do not make big 
differences in general. However, semantically 
the sentences starting with a verb tend to be more 
narrative while the BA-construction is more firm 
and authoritative in expressing the ideas, for 
example: 
10) Store the tube on the ice. 
83
  a. (BA + N + V + PP)9  
11) Aspirate the contrast medium from the 
cyst. 
  a.  
  b.  
The protocols we work with are instructions of 
certain step-by-step procedures of either clinical 
practice or laboratory practice, just like product 
use instructions, recipes and user?s manuals. The 
semantic contents of these sentences should be 
firmly expressed as kinds of orders. Though both 
pairs of the Chinese sentences (10 and 11) are 
transferring the same idea, the BA-construction 
is more expressive and natural in this case 
(example 10 a) and 11 a).  
In our corpus, we have observed that some of 
the English imperative sentences can be 
transferred into two kinds of BA-construction, 
that of obligatory and that of optional. 
Obligatoriness: 
In our work, some sentences must be 
constructed into Chinese BA-structure, otherwise, 
the whole sentence sounds either ungrammatical 
(see in c below) or unnatural or especially 
unacceptable (see in b below). The 
grammaticality of the sentence can be tested by 
moving the translated SL PP to the front of the 
sentence in the TL (see in c)), for example: 
12 a) Inject contrast medium into the cyst. 
         
10
  
    b)   
 (unacceptable) 
    c)  *  
As is shown in (c), if the whole sentence 
becomes ungrammatical after moving the PP in 
front of the sentence, we classify the sentence as 
obligatory to be transferred into to a TL sentence 
containing the BA-structure. We then constrain 
the syntactic structure to the first one as the legal 
structure while excluding the other two, thus the 
formulations are: 
     insert (_, Compl1, into_Compl2) 
      (BA_Compl1, _, _Compl2_ ) 
The other two are excluded: 
      (_, Compl1, _Compl2_ )     
(unacceptable) 
                                                          
9
 Note: the BA is underlined; the verb is in bold font; 
and the object (logical) is in italic. 
10
 Red: refer the translated SL PP in TL. 
     *  ( _Compl2_ , _, Compl1)  
Notice that though the first excluded 
formulation in the TL shares the same structure 
as that of the SL, they are unacceptable in the TL. 
The same situation applies to the following two 
examples: 
13 a) Leave the contrast medium in the cyst as 
a substitute of protoscolicide agent.        
,  
 
     b) * ,  
(ungrammatical) 
     c) *  
(strange and ungrammatical) 
The final formulation is based on (a): 
     Leave (_, Compl1, in_Compl2, X) 
      (X, BA_Compl1, _, _Compl2_ ) 
The other two are excluded: 
     *  (X, _, Compl1, _Compl2_ ) 
     *  (X, _Compl2_ , _, Compl1,) 
14 a) Leave the inserted catheter in the cyst for 
1-3 days. 
     1 3  
Alternative: 
     1 3  
     b) 1 3  
(unacceptable) 
     c) *1 3  
(ungrammatical) 
Note: for (b) a better alternative should be: 
     1 3 (an 
acceptable sentence)    
The final legal formulations are: 
     Leave (_, Compl1, in_Compl2, T) 
     (BA_Compl1, _, _Compl2_ , T) 
The alternatives (in a) and b)) will be excluded 
as long as the first one (a) is a perfectly 
acceptable sentence. Unlike the ?X? in example 
(13 and 14), here the ?T? refers to adjuncts 
which refers to TIME and which usually 
occupies a different position in the sentence in 
our case.  
Therefore our criterion to test the 
obligatoriness is to see what kind of grammatical 
performance a sentence will exhibit when it is 
used in the form shown in the above (b?s and c?s, 
especially in (c?s)). If the sentence looks 
84
unacceptable or is in particular ungrammatical, 
then it must be constructed into the TL sentence 
containing the BA-structure. This phenomenon is 
in fact closely related with the semantic contents 
of the verb and as well as the preposition (a goal 
or a location) in question (we will not discuss 
this aspect in this paper). 
Optionality 
Some sentences that we have observed can be 
used optionally. That is to say, we can transfer 
the SL sentences without employing the BA-
construction, or with the BA-construction in the 
TL. In doing so, no significant loss of the 
sentence meaning will occur (except that in some 
cases there still exist the semantic differences 
where a BA-construction exhibits firmness and 
authority), for example:     
15 a) Dissolve the nucleic acids in 50 ?l of TE 
that contains 20 ?g/ml DNase-free RNase A.  
      b) 20 ?g/ml DNase RNase A
50 ?l TE  
Final formulations: 
   Dissolve (_, Compl1, in_Compl2) 
    (BA_Compl1, _, _Compl2_ ) 
Or: 
      ( _Compl2_ , _, Compl1) 
16 a) Store the tube on the ice for three  
minutes. 
        
(linear sequence of the literal translation: BA 
tube, on ice, store, three minute)  
     b)  
Alternative: 
          
Final formulations: 
     Store (_, Compl1, on_Compl2, T) 
      (BA_Compl1, _, _Compl2_ , T) 
Or: 
     ( _Compl2_ , _, Compl1, T) 
16 a) Vortex the solution gently for a few  
seconds. 
        
(linear sequence: BA solution, gently, vortex, 
a few seconds) 
      b)  
Final formulations : 
     Vortex (_, Compl1, Y, T) 
      (BA_Compl1, Y, _, T) 
Or: 
      (Y, _, Compl1, T) 
Here ?Y? refers to adverbs.   
However, if the transitive verb (e.g. ?vortex?) 
is used intransitively as is often the case in our 
corpus, the BA-construction has to be changed to 
the normal sentence structure (V + (X) + PP), for 
example:  
17) Vortex gently for a few seconds.  
     
Formulation for this becomes: 
     Vortex (_, Y, T) 
      (Y, _, T) 
The reason why we allow the alternative 
formulations in the second case is that these 
sentences are actually subcategorized for by the 
verbs and will not be confused with other similar 
syntactic structures (e.g. V + NP + PP) which do 
not employ the BA-construction in the TL while 
transferring the intended information. We 
demonstrate this with an example: 
18 a) Puncture the cyst with the needle. 
          
While the machine is searching the 
information concerning this sentence, two major 
supported sources of information (lexicon and 
grammar rules) will help it find the correct 
structure for transferring the sentence into the 
correct TL correspondence. Therefore, the 
machine will not mismatch the syntactic 
structure for this sentence by wrongly employing 
the BA-construction, for example the following 
translation will be excluded by both the 
information stored in the lexicon and grammar as 
a legal instruction: 
b) *
This is an understandable but very unnatural 
sentence and can be regarded as ungrammatical 
in the target language. Though it possesses the 
same structure as that of the other BA-
construction, the problem of this 
ungrammaticality is caused by the semantic 
content conveyed by both the verb and the 
preposition. Usually a BA-construction expresses 
the resultative or directional effect of the verb. 
However, what the PP ?with the needle? 
expresses is the manner of the verb, that is, how 
the action is done. Semantically, it is not within 
the semantic scope of the BA-construction 
(though we can find few contradictory examples) 
85
and thus can not be translated into to the target 
language by incorrectly employing the BA-
construction.  
In our system prepositional phrases like, ?with 
the needle? is subcategorized by the verb 
?puncture? and the syntactic rules for this verb. 
To demonstrate this, we simplify the lexical and 
syntactic information as shown in the formula 
below: 
     Puncture (_, Compl1, with_Compl2) 
     ( _Compl2, _, Compl1) 
The above information tells us that the verb 
?puncture? of the source language, like the other 
verbs mentioned in the previous paragraphs, can 
have two complements, of which one has a 
preposition as the head of the second linear 
complement. The correspondence in the target 
language for this verb is ? ? which take two 
complements too. One corresponds to the first 
complement of the SL and is placed after the 
verb ? ?, and the other complement 
corresponds to the second complement but is 
placed in front of the verb with a preposition as 
its head ? ?. The simplified syntactic structures 
for both sentences are: 
SL: V_311 (_, A, P_B) 
TL: V_3 (P_B, _, A) 
4 Conclusion 
In this paper we have discussed a special 
Chinese syntactic structure: the BA-construction 
which is quite controversial in the literature but 
nevertheless less problematic in our work. After 
comparing with other syntactic structures, we 
finally adopt the idea that the BA-construction 
shows more characteristics of a PP which is still 
governed by the verb which follows it, in 
particular in our work. We thus treat this 
structure as a PP rather than a VP. This is 
supported by the relatively simpler sentence 
structures found in our corpus. While 
constructing our grammar and formulating the 
BA-structure, we lay focus on the syntactic 
performance and semantic contents that the BA-
construction exhibits. Based on the verb types 
and the semantic content of the preposition 
following the verb, we finally formulate two 
kinds of sentence types concerning the BA-
construction in the target language which can 
well satisfy our purpose. Of course, like many 
                                                          
11
 V_3: refers to the syntactic pattern of the verb. 
other language-specific syntactic structures, our 
analysis and practice can not satisfy all situations. 
However, as we work on a relatively narrow 
domain where the sentence types by themselves 
do not vary greatly. We can find a better solution 
by controlling the syntactic types to tackle the 
problems concerning the BA-construction and 
the alike. 
References 
BENDER, Emily. 2002 The Syntax of Madarin Ba: 
Reconsidering the Verbal Analysis, Journal of East 
Asian Linguistics, 2002 
CARDEY, Sylviane, GREENFIELD, Peter, WU 
Xiaohong. 2004. Desinging a Controlled Language 
for the Machine Translation of Medical Protocols: 
the Case of English to Chinese. In Proceedings of 
the AMTA 2004, LNAI 3265, Springer-Verlag, pp. 
37-47 
HASHIMOTO, Anne Yue. 1971. Descriptive 
adverbials and the passive construction, Unicorn, 
No. 7. 
HER, One-Soon. 1990. Grammatical Functions and 
Verb Subcategorization in Madarin Chinese. PhD 
dissertation, University of Hawaii. 
HU Yushu et al 1991 Modern Chinese 	


,   ISBN 7- 5320-0547-
X/G.456 
LIN, Tzong-Hong Jonah. 2004. Grammar of Chinese, 
Lecture note, ?The Ba construction and Bei 
Construction 12/21/2004, National Tsing Hua 
University, Taiwan, 	Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1886?1896, Dublin, Ireland, August 23-29 2014.
Learning the Taxonomy of Function Words for Parsing
Dongchen Li, Xiantao Zhang, Dingsheng Luo and Xihong Wu
Key Laboratory of Machine Perception and Intelligence
Speech and Hearing Research Center
Peking University, Beijing, China
{lidc,zhangxt,dsluo,wxh}@cis.pku.edu.cn
Abstract
Completely data-driven grammar training is prone to over-fitting. Human-defined word class
knowledge is useful to address this issue. However, the manual word class taxonomy may be
unreliable and irrational for statistical natural language processing, aside from its insufficient
linguistic phenomena coverage and domain adaptivity. In this paper, a formalized representation
of function word subcategorization is developed for parsing in an automatic manner. The function
word classification representing intrinsic features of syntactic usages is used to supervise the
grammar induction, and the structure of the taxonomy is learned simultaneously. The grammar
learning process is no longer a unilaterally supervised training by hierarchical knowledge, but
an interactive process between the knowledge structure learning and the grammar training. The
established taxonomy implies the stochastic significance of the diversified syntactic features.
The experiments on both Penn Chinese Treebank and Tsinghua Treebank show that the proposed
method improves parsing performance by 1.6% and 7.6% respectively over the baseline.
1 Introduction
Probabilistic context-free grammar (PCFG) is widely used in the fields of speech recognition, machine
translation, information retrieval, etc. It takes the empirical rules and probabilities from a Treebank.
However, due to the context-free assumption, PCFG does not always perform well (Klein and Man-
ning, 2003). For instance, it assumes adverbs, including temporal adverbs, degree adverbs and negation
adverbs, to share the same distribution, whereas the distinction would provide useful indication for dis-
ambiguating the syntactic structure of the context.
It arose that the manual word classification in linguistic research was used to enrich PCFG and im-
prove the performance. However, from the point of view of statistical natural language processing, there
are some drawbacks for manual classification. Firstly, Linguistic phenomena covered by the manual
refinement may be limited by the linguistic observations of human. Secondly, the evidence of manual
refinement is often based on a particular corpus or specific sources of knowledge acquisition. As a result,
its adaptivity to different domains or genres may be insufficient. As for function words, due to the ambi-
guity and complexity in syntactic grammar, it is more difficult to develop formalized representation than
for content words. There are diversified standards for grammar refinement. Consequently, the word clas-
sification or category refinement can be conducted in distinct manners, while each of them is reasonable
in some sense. A delicate hierarchical classification inevitably involves in multiple dividing standards.
However, the word sets under distinct dividing standards may be overlapping. The problems come up
that how to choose the set of the multiple standards to cooperate to build the taxonomy, and how to de-
cide the priority of each standard. Regarding that the manual method is hard to overcome critical issues,
manual taxonomy for function words may not be reliable for statistical natural language processing.
This article attempts to address these issues in a data-driven manner. we first manually construct a
cursory and flat classification of function words. A hierarchical split-merge approach is employed to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1886
introduce our classification, and the PCFG training procedure is supervised to alleviate the over-fitting
issue. The priorities of the subcategorization standards are determined by the measurement of effec-
tiveness for parsing in a greedy manner in the hierarchical classification. And the hierarchical structure
of the classification is learned by data-driven approach in the course of grammar induction, so as to fit
the practical usages in the Treebank. Accordingly, the grammar learning process is no longer a unilat-
erally supervised training by hierarchical knowledge, but an interactive process between the knowledge
representation induction and the grammar training. That is, the grammar induction is supervised by the
knowledge and the structure of the taxonomy is learned simultaneously. These two processes are iterated
for several rounds and the hierarchical structure of the function word taxonomy is constructed. In each
round, the induced grammar could benefit from the optimized taxonomy during the learning process. The
category split in the early rounds take more priorities than in the late ones. Thus, the learned taxonomy
implies the stochastic significance of the series of the syntactic features.
Experiments on Penn Chinese Treebank Fifth Edition (CTB5.0) (Xue et al., 2002) and Tsinghua Chi-
nese Treebank (TCT) (Zhou, 2004) are performed. The results show that the induced grammars with
refined conjunction categories gain parsing performance improvement by 1.6% on CTB and by 7.6% on
TCT. During the training process, a taxonomy of function words is learned, which reflects their practical
usages in the corpus.
The rest of this paper is organized as follows. We first review related work on category refinement
for parsing. Then we describe our manually defined categories of function words in Section 3. The
hierarchical state-split approach for introducing the the categories are presented in Section 4, and our
taxonomy learning method is described in Section 5. In Section 7, experimental comparison is conducted
among various methods on granularity choosing. And conclusions of this research are drawn in last
section.
2 Related Work
A variety of techniques have been proposed to enrich PCFG in either manual (Klein and Manning, 2003;
Zhang and Clark, 2011) or automatic (Petrov, 2009; Cohen et al., 2012) manner.
2.1 Automatic Refinement of Function Words for Parsing
One way of grammar refinement is data-driven state-split methods (Matsuzaki et al., 2005; Prescher,
2005). The part-of-speech and syntactic tags in the grammar are automatically split to encode the kinds
of linguistic distinctions exhibited in the Treebank. The hierarchical state-split approach (Petrov et al.,
2006) started from a bare-bones Treebank derived grammar, and iteratively refined it in a split-merge-
smooth cycle with the EM-based parameter re-estimation. It achieved state of the art accuracies for many
languages including English, Chinese and German.
One tag is usually heterogeneous, in the sense that its word set can be of multiple different types.
Nevertheless, the automatic process tries to split the tags through a greedy data-driven manner, where
multiple distinctive information is used simultaneously when dividing tags. Thus the refined tags are
not intuitively interpretable. Meanwhile, considering that the EM algorithm usually gets stuck at a sub-
optimal configuration, this data-driven method suffers from the risk of over-fitting. As shown in their
experiments, there is little to be gained from splitting the closed part-of-speech classes (e.g. DT, CC, IN)
or the nonterminal ADJP.
To alleviate the risk of over-fitting, we employ the human-defined knowledge to constrain the splitting
process in this research. Based on the state-split model, our approach aims to reach a compromise
between manual and automatic refinement approaches.
2.2 Manual Refinement of Function Words for Parsing
The other way to refine the annotation for training a parser is incorporating knowledge base. Semantic
knowledge of content words has been proved to be effective in alleviate the data sparsity. Some re-
searches utilized semantic knowledge in WordNet (Miller, 1995; Fellbaum, 1999) for English parsing
(Fujita et al., 2010; Agirre et al., 2008), and Xiong et al. (2005; Lin et al. (2009) improved Chinese pars-
1887
ing by incorporating semantic knowledge in HowNet (Dong and Dong, 2003; Dong and Dong, 2006).
While WordNet and Hownet contain word classification for content words, Li et al. (2014b; Li et al.
(2014a) have focused on exploiting manual classification for conjunction in parsing.
Klein and Manning (2003) examined the annotation in Penn English Treebank, manually split the ma-
jority of the part-of-speech (POS) tags. For the function words, they split the tag ?IN? into subordinating
conjunctions, complementizers and prepositions, and appended
?
BE to all forms of ?be? and
?
HAVE to
all forms of ?have?. Conjunction tags are also marked to indicate whether they were ?But?, ?but? or
?&?. The experimental results showed that the split tags of function words surprisingly make much con-
tribution to the overall improved parsing accuracy. Levy and Manning (2003) transferred this work to
Penn Chinese Treebank. They found that, in some cases, certain adverbs such as ?however (,)? and
?especially (c??)? preferred IP modification and could help disambiguate IP coordination from VP
coordination. To capture this point, they marked those adverbs possessing an IP grandparent. However,
these manual refinement methods seems to split the tags in a rough way, which might account for a mod-
est accuracy achieved. Some existing work used heuristic rules to simply split the tags of function words
(Klein and Manning, 2003; Levy and Manning, 2003). They demonstrated that many function words
stood out to be helpful in predicting the syntactic structure and syntactic label.
3 Manual Tabular Subcategories of Function Words
When subcategorizing function words, in this section, we manually list various grammatical distinctions
that are commonly made in traditional and generative grammar in a fairly flat taxonomy. The grammar
training procedure learns by using our manual taxonomy as a starting point, and constructs a reasonable
and subtle hierarchical strucutre based on the distribution of function words usages in the corpus.
Based on some existing knowledge base (Xia, 2000; Xue et al., 2000; Zhu et al., 1995; Wang and
Yu, 2003) and previous research work (Li et al., 2014b), we investigate and summarize the usage of
function words, and come up with a hierarchical subcategories. The taxonomy of the function words is
represented in a tree structure, where each subcategory of a function word corresponds to a node in the
taxonomy, the nonterminals are subcategories and the terminals are words.
For the convenience and consistence, our manual classification just gives a rough and broad taxonomy.
It is labor-intensive and error-prone of classifying the function words manually to produce a consistent
output. Fine-grained hierarchical structure is not obligatory, but would be harmful if inappropriately clas-
sified, as it may mislead the learning process. To avoid this kind of risk, the elaboration is saved, rather
than introducing unnecessary bias. The learning process would perform the hierarchical classification
according to the distribution in the corpus.
For instance, the distinction within conjunctions is intricate. Conjunctions are the words that are
called ?connective words? in traditional Chinese grammar books. In Penn Chinese Treebank, they are
tagged as coordinating conjunctions (CC), subordinating conjunctions (CS), or adverbs (AD) according
to their syntactic distribution. CC conjoins two equivalent constituents (noun phrases, clauses, etc.),
each of which has approximately the same function as the whole construction. CS precedes a subordi-
nating clause, whereas conjunctive adverbs often appear in the main clause and pair with a subordinating
conjunction (e.g., if (XJ)/CS ... then (?)/AD). However, in Chinese, it is often hard to tell the sub-
ordinating clause from the main clause in the compound statement. As a result, in the prospective of
linguistic computing, the confusion is that, CS and conjunctive adverbs both precedes the subordinating
clauses or main clauses, while CC connects two phrases or precedes the main clause. In our scheme,
we simply conflates the CC, CS and conjunctive adverbs together. This result in a general ?conjunction?
category, within which we just enumerate all the possible uses of the conjunctions. As a result, the struc-
ture of our human-defined taxonomy is fairly flat, as briefly shown in Figure 1 and Figure 2. Our scheme
releases our hands from the confusing situations, by leaving them to our data-driven method described in
the following section. Figure 1 and Figure 2 abbreviate the manual classification and their corresponding
examples.
Many prepositions in Chinese are evolved from verbs, thus the linguistic characteristics of preposi-
tions are somewhat similar to verbs. Therefore, this paper divides the preposition word set according to
1888
Subordinating Conjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Coordination: bothQ
Progression: not only?=
Transition: although?,
Preference: rather than??
Cause: becausedu
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Assumption: ifXJ
Universalization: whatever??
Unnecessary Condition: sinceQ,
Insufficient Condition: although=?
Sufficient Condition: as long as??
Necessary Condition: only if?k
Equality: unless??
... ...
Figure 1: Abbreviated Hierarchical subcategories of subordinating conjunctions with examples.
Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Conjunctive Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
Preference: would rather?X
In case: lest??
Otherwise: or else?K
However: but%
Result
?
?
?
Therefore: so??
Then, As a result: as soon?
So that: so that?B
Progression
?
?
?
?
?
?
?
Furthermore: but also?
In addition: moreover,	
Later: subsequently? 
As well: likewise?
...
Adjunct Adverb
?
?
?
Frequency Adverbs: for many times?g
Degree Adverbs: very4?
...
...
Figure 2: Abbreviated Hierarchical subcategories of adverbs with examples.
the types of their associated arguments: ?benefactive?, such as ??(for)? and ??(to)?, marks the ben-
eficiary of an action; ?locative?, such as ?3(in)?, marks adverbials that indicate the place of the event;
?direction?, such as ? ?(towards)? and ? d(from)?, marks adverbials that answer the questions ?from
where?? and ?to where??; ?temporal?, such as ?3(on)?, marks temporal or aspectual adverbials that
answer the question ?when??, and so on.
4 Refining Grammar with Hierarchical Category Refinement
In this section, we choose the appropriate granularity in a data-driven manner based on the split-merge
learning method in Section 2.1. Our approach first initializes the categories with the most general sub-
categories in the taxonomy and then splits the categories through the hypernym-hyponym relation in the
1889
taxonomy. Data-driven method is used to merge the overly refined subcategories.
The top category in the taxonomy is used as the starting annotations of POS tags. As we cannot
predict which layer should be the most adequate one, we try to avoid applying any priori restriction on
the refinement granularity, and start with the most general tags.
With the hierarchical knowledge, it turns out to be a critical issue that which granularity should be
used to refine the tags for parsing. We intend to take neither too coarse subcategories nor too fine ones in
the hierarchical knowledge for parsing. Instead, it would be our advantage to split the tags with the very
granularity where needed, rather than splitting them all to one specific granularity in the taxonomy.
For example, ?Conjunctive Adverbs? are divided into three subcategories in our taxonomy as shown
in Figure 2. The evidence for the refinement may occur in very rare case, and certainly some of the
context of the different subcategories are quite the same. Splitting symbols with the same context is
not only unnecessary, but potentially harmful, since it unreasonably fragments observations of other
symbols.behavior.
In this paper, the hierarchical subcategory knowledge is used to refine grammars by supervising the
automatic hierarchical state-split approach. In the split stage in each cycle, the function word subcategory
is split along the hierarchy of the knowledge, instead of being randomly split and classified automatically.
In this way, we try to alleviate the over-fitting of the greedy data-driven approach, and a new set of
knowledge-related tags are generated. In the following step, we retreat some of the split subcategories to
their more general layer according to its likelihood loss of merging them. In this way, we try to avoid the
excessive refinement in our hierarchical knowledge without sufficient data support.
There are two issues that we have to consider in this process: a) how to deal with the polysemous
words, and b) how to deal with the multi-branch situation other than binary branch in the taxonomy.
Regarding to the polysemous words, they occur mostly in two situation for function words. Some are the
polysemous words which can be taken as conjunctions or auxiliary words, while the others can be taken
as preposition or adverbs. Fortunately there is no ambiguity for a word given its POS tag, so we could
neglect this situation in the split process when training. We demonstrated the solution for the multiple
branches in the Section 5.
5 Learning the Taxonomy of Function Words
There are multiple subcategorization criterions for building function word taxonomy, and it is diffi-
culty for human to rank the ordering in the classification process. This section represents the method
of learning the taxonomy of the function words in data-driven manner. Based on the manual tabular
classification, the similar word classes are conflated to express the data distribution.
The multiple branches in the taxonomy are intractable for the original split-merge method, because it
splits every category into two and merges half of them for efficiency. If we follow this scheme in our
training process, it would be difficult to deal with the multi-branch situation in the taxonomy, because
how to choose the first two to split among the multiple branches is another challenge. It is an equally
difficult problem for us to binarize the taxonomy by hand comparing to directly choosing the granularity.
It would be our advantage to binarize the taxonomy by a data-driven method. For automatic binariza-
tion, a straightforward approach is to measure the utility of traversing all the plausible ways of cutting all
the branches into two sets individually and use the best one. Then we can deal with the divided two sets
in the same manner recursively. However, not only is this impractical, requiring an entire training phase
for each possible binarization scheme which is exponentially expensive, but it assumes the contributions
of multiple binarizations in different branches are independent. In fact, extra sub-symbols may need to
be added to several nonterminals before they can cooperate to pass information along the parse tree.
Therefore, we go in the opposite direction, and propose an extended version of split-merge learning
to handle the multiple branches in the taxonomy. That is, we split each state into all the subcategories in
the lower layer in the taxonomy even if it has multiple branches, train, and then measure for every two
sibling subcategories in the same layer the loss in likelihood incurred when merging them. If this loss is
small, the new division of these two subcategories does not carry enough useful information and can be
merged back. Contrary to the gain in likelihood for splitting, the loss in likelihood for merging can be
1890
efficiently approximated (Petrov et al., 2006).
More specifically, we assume transitivity in merging multiple subcategories in one layer. Figure 3
gives an illustration. After the split stage, the category A has been split into subcategories A-1, A-2, ...
to A-7. Then we compute the loss in likelihood of the training data by merging back each pair of two
subcategories through A-1 to A-7. If the loss is lower than a certain threshold
1
set for each round of
merge, this pair of newly split subcategories will be merged. We only show the sibling ones for brevity in
this example. Assume the losses of merging these pairs (A-1, A-2), (A-2, A-3), (A-3, A-4) and (A-4, A-
5) are below the threshold ?. Thus, A-1, A-2, A-3, A-4 and A-5 are merged to X-1 due to the transitivity
of the connected points, where X-1 is the automatically generated subcategory which contains the five
conflated subcategories as its descendants. At the meantime, A-6 and A-7 still remain. This scheme is an
approximation because it merges subcategories that should be merged with the same subcategory. But it
will leave the split of this instances to the next round when more evidence on interaction with other more
refined subcategories is given.
(a) Refined subcategories before the merge stage (b) Refined subcategories after the merge stage
Figure 3: Illustration of merging the subcategories for multiple branches in the taxonomy. Where ? is
a certain threshold below which this pair of subcategories will be merged, and X is the automatically
generated subcategory which contains the conflated subcategories as its descendants.
After merging in each round, the hierarchical knowledge is reshaped to fit the practical usage in the
Treebank. The split-merge cycles allow us to progressively increase the complexity of the hierarchical
knowledge, and the more useful distinctions are represented as the higher level in the taxonomy, which
gives priority to the most useful distinctions in return by supervising the grammar induction. Figure 4
demonstrates the transformation of the hierarchical structure from the tabular classification. Along this
road, the training scheme is not a unilateral training, but an interactive process between the knowledge
representation learning and the grammar training. Our learning process exerts a mutual effect to both the
induced grammar and the optimized structure of the hierarchical knowledge. In this way, the set of di-
viding standards are chosen iteratively according to their syntactic features. The more effective divisions
are conducted in the early stages. In the following stages, the divisions which interact with previous
divisions to give the most effective disambiguating information are adopted. The final taxonomy are
built based on manual classification in data-driven approach, and the hierarchical structure are optimized
and rational in the perspective of actual data distribution. Figure 4 illustrates a concrete instance of the
procedure of learning the taxonomy. On one hand, this procedure provides a more rational hierarchical
subcategorization structure according to data distribution. On the other hand, the order of the division
criterions represents the priorities the grammar induction takes for each criterion. The structure in the
higher levels of the taxonomy are determined by the dominant syntactic characteristics. And the division
in the later iterations are on the basis of minor distinctive characteristics.
6 Experiments and Results
6.1 Data Set
We present experimental results on both CTB5.0 (All traces and functional tags were stripped.) and TCT.
We ran experiments on CTB5.0 using the standard data allocation: files from CHTB 001.fid to
CHTB 270.fid, and files from CHTB 400.fid to CHTB 1151.fid were used as training set. The develop-
ment set includes files from CHTB 301.fid to CHTB 325.fid, and the test set includes files CHTB 271.fid
1
In practice, instead of setting a predefined threshold for merging, we merge a specific number of the newly split subcate-
gories.
1891
AA-7A-6A-5A-4A-3A-2A-1
(a) First round of category split
A
A-7A-6X-1
X-1
A-5A-4A-3A-2A-1
(b) First round of category merge
A
A-7A-6X-1
A-5A-4A-3A-2A-1
(c) Second round of category split
A
A-7A-6X-1
X-3X-2
X-2
A-2A-1
X-3
A-5A-4A-3
(d) Second round of category merge
A
A-7A-6X-1
X-3
A-5A-4A-3
X-2
A-2A-1
(e) Third round of category split
A
A-7A-6X-1
X-3
X-4A-3
X-2
A-2A-1
X-4
A-5A-4
(f) Third round of category merge
Figure 4: Iteration of grammar induction and taxonomy structure learning
to CHTB 300.fid. Experiments on TCT use the data set as in CIPS-SIGHAN-ParsEval-2012 (Zhou,
2012). We have parsed on the segmented text in the Treebank, namely, no use of gold POS-tags, use
of gold segmentations, and full-length sentences. This is the same as for other 5 parsers in Table 1 for
comparison. All the experiments were carried out after six cycles of split-merge.
1892
6.2 Final Results
The final results are shown in Table 1. Our final parsing performance is higher than both the manual
annotation method (Levy and Manning, 2003) and the data-driven method (Petrov, 2009).
Parser Precision Recall F
1
Levy(2003) 78.40 79.20 78.80
Petrov(2009) 84.82 81.93 83.33
Lin(2009) 86.00 83.10 84.50
Qian(2012) 84.57 83.68 84.13
Zhang(2013) 84.42 84.43 84.43
This paper 86.55 83.41 84.95
Table 1: Our final parsing performance compared with the best previous work on CTB5.0.
On test set TCT, the method achieves the best precision, recall and F-measure in the CIPS-SIGHAN-
ParsEval-2012 competition, and table 2 compares our results with the system of Beijing Information
Science and Technology University (BISTU) which got the second place in the competition.
Parser Precision Recall F
1
BISTU 70.10 68.08 69.08
This paper 76.81 76.66 76.74
Table 2: Our final parsing performance compared with the best previous works on TCT.
Given the manual labor required for generating the taxonomy (and in languages where there is a
taxonomy, determining whether it is suitable), this first study focuses on a language where there is quite
a bit of under- and over-specification in the Treebanks? tag sets. So this work is only implemented on
Chinese. We regard it as future work to transfer this approach to other languages.
6.3 Analysis
The outline of constructing the taxonomy of function words are as follows. Firstly, the function words are
manually subcategorized in a rough and cursory way. When dealing with subcategories hard to resolve
their relation of subordination, we simply treat them as siblings in the tree in a rather flat stricture, and
leave the elaboration of exquisite clustering to the algorithms. The data-driven approach in Section 4 au-
tomatically choose the appropriate granularity of refinement for our grammar. Moreover, the split-merge
learning for multiple branches in the hierarchical subcategories in Section 5 exploits the relationship be-
tween the sibling nodes in the same layer, making use of the Treebank data to adjust and optimize the
hierarchy.
During the split-merge process, the hierarchical subcategories are learned to fit the data, which is
a transformation of our manually defined hierarchy. The transformed hierarchy is just the route map
of subcategories employed in our model. As abbreviated in Figure 5 and Figure 6, many distinctions
between word sets of the subcategories have been exploited by our approach, and the learned taxonomy
is interpretable. For instance, It shows that the learned structure of the taxonomy is reasonable.
6.4 Comparison with Previous Work
Although the taxonomy of function words are learned in the grammar training process, the grammar is
trained on the Treebank in supervised manner. Thus, this work is not directly relevant with unsupervised
grammar induction literature (Headden III et al., 2009; Berant et al., 2007; Mare?cek and
?
Zabokrtsk`y,
2014).
1893
Subordinating Conjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Coordination: bothQ
X
{
Progression: not only?=
Transition: although?,
X
{
Preference: rather than??
Cause: becausedu
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Universalization: ??
Equality: ??
X
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
X
?
?
?
?
?
X
{
Assumption: XJ
Sufficient Condition: ??
Necessary Condition: ?k
X
{
Unnecessary Condition: Q,
Insufficient Condition: =?
...
Figure 5: Abbreviated automatically learned hierarchical subcategories of subordinating conjunctions
with examples. Where ?X? represents the automatically generated subcategory.
Conjunctive Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
?
?
Preference: would rather?X
X
{
In case: lest??
Otherwise: or else?K
However: but%
Result
?
?
?
?
?
X
{
Therefore: so??
So that: so that?B
Then, As a result: as soon?
Progression
?
?
?
?
?
?
?
?
?
X
?
?
?
?
?
X
{
Furthermore: but also?
In addition: moreover,	
Later: subsequently? 
As well: likewise?
...
Figure 6: Abbreviated automatically learned hierarchical subcategories of adverbs with examples.
Lin et al. (2009) and Li et al. (2014b) presented ideas of using either hierarchical semantic knowledge
from HowNet for content words or grammar knowledge for subordinating conjunctions. They introduced
hierarchical subcategory knowledge in a different stage. They split the original Treebank categories
in split-merge process according to the data, and then find a method to map the subcategories to the
node in the taxonomy, and constrain their further splitting. Comparing to their work, our approach is
more delicate, which is splitting the categories according to the knowledge, and learning the knowledge
structure according to data during the training course. Lin et al. (2009) incorporated semantic knowledge
of content words into the data-driven method. It would be promising if this work stacks with the content
word knowledge. However, the work with content word knowledge have to handle the polysemous words
in the semantic taxonomy, so they split the categories according to the data, and then find a way to map
the subcategories to the node in the taxonomy, and constrain their further splitting. It is our goal to make
these two methods compatible with each other.
Incorporating word formation knowledge achieved higher parsing accuracy according to Zhang and
1894
Clark (2011). However, they ran their experiment on gold POS-tags and a different data set split, which
is different form the setup of work in Table 1 including this work. They also presented their result on
automatically assigned POS-tags and the same data set split as in the work in Table 1 to facilitate the
performance comparison. It gave F
1
score of 81.45% for sentences with less than 40 words and 78.3%
for all sentences, significantly lower than Petrov and Klein (2007).
Zhang et al. (2013) exhaustively exploited character-level syntactic structures for words, and achieved
84.43% on F
1
measure. They placed more emphasis on the word-formation of content words, which
our model highlights the value of the function words. The complementary intuitions make it possible to
integrate these approaches together in the future work.
7 Conclusion
This paper presents an approach for inducing finer syntactic categories while learning the taxonomy for
function words. It used linguistic insight to guide the state-split process, and the hierarchical structure
representing syntactic features of function word usages was established during the grammar training
process. Empirical evidence has been provided that automatically subcategorizing function words con-
tributes to high parsing performance. The induced grammar supervised by the taxonomy outperformed
pervious approaches, which benefited from both the knowledge and the data-driven method. The pro-
posed approach for learning the structure of the taxonomy could be generalized to construct semantic
knowledge base.
Acknowledgments
This work was supported in part by the National Basic Research Program of China (973 Program) under
grant 2013CB329304, the Research Special Fund for Public Welfare Industry of Health under grant
201202001, the Key National Social Science Foundation of China under grant 12&ZD119, the National
Natural Science Foundation of China under grant 91120001.
References
Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and pp attachment performance
with sense information. Proceedings of ACL-08: HLT, pages 317?325.
Jonathan Berant, Yaron Gross, Matan Mussel, Ben Sandbank, Eytan Ruppin, and Shimon Edelman. 2007. Boost-
ing unsupervised grammar induction by splitting complex sentences on function words. In Proceedings of the
Boston University Conference on Language Development.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-
variable pcfgs. In Proceedings of the 50th annual meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 223?231. Association for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2003. Hownet-a hybrid language and knowledge resource. In Proceedings
of the international conference on natural language processing and knowledge engineering, pages 820?824.
IEEE.
Zhengdong Dong and Qiang Dong. 2006. HowNet and the computation of meaning. World Scientific Publishing
Co. Pte. Ltd.
Christiane Fellbaum. 1999. WordNet. Wiley Online Library.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2010. Exploiting semantic information for hpsg
parse selection. Research on language and computation, 8(1):1?22.
William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109.
Association for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st annual
meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computational
Linguistics.
1895
Roger Levy and Christopher D Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1, pages 439?
446. Association for Computational Linguistics.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2014a. Improved parsing with taxonomy of conjunctions. In 2014
IEEE China Summit & International Conference on Signal and Information Processing. IEEE.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2014b. Learning grammar with explicit annotations for subordi-
nating conjunctions in chinese. In Proceedings of the 52th annual meeting of the Association for Computational
Linguistics Student Research Workshop. Association for Computational Linguistics.
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu, and Huisheng Chi. 2009. Refining grammars for parsing
with hierarchical semantic knowledge. In Proceedings of the 2009 conference on empirical methods in natural
language processing: Volume 3-Volume 3, pages 1298?1307. Association for Computational Linguistics.
David Mare?cek and Zden?ek
?
Zabokrtsk`y. 2014. Dealing with function words in unsupervised dependency parsing.
In Computational Linguistics and Intelligent Text Processing, pages 250?261. Springer.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Pro-
ceedings of the 43rd annual meeting on Association for Computational Linguistics, pages 75?82. Association
for Computational Linguistics.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human language technologies
2007: the conference of the North American chapter of the Association for Computational Linguistics, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st international conference on computational linguistics and the 44th
annual meeting of the Association for Computational Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
Slav Orlinov Petrov. 2009. Coarse-to-Fine natural language processing. Ph.D. thesis, University of California.
Detlef Prescher. 2005. Inducing head-driven pcfgs with latent heads: Refining a tree-bank grammar for parsing.
In Machine Learning: ECML 2005, pages 292?304. Springer.
Hui Wang and Shiwen Yu. 2003. The semantic knowledge-base of contemporary chinese and its applications
in wsd. In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17, pages
112?118. Association for Computational Linguistics.
Fei Xia. 2000. The part-of-speech tagging guidelines for the penn chinese treebank (3.0). Technical report.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the penn chinese treebank
with semantic knowledge. In Natural language processing?IJCNLP 2005, pages 70?81. Springer.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch. 2000. The bracketing guidelines for the penn chinese
treebank (3.0). Technical report.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In
Proceedings of the 19th international conference on computational linguistics-Volume 1, pages 1?8. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational linguistics, 37(1):105?151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. 51st
annual meeting of the Association for Computational Linguistics.
Qiang Zhou. 2004. Annotation scheme for chinese treebank. Journal of Chinese information processing, 18(4):1?
8.
Qiang Zhou. 2012. Evaluation report of the third chinese parsing evaluation: Cips-sighan-parseval-2012. In
Proceedings of the second CIPS-SIGHAN joint conference on Chinese language processing, pages 159?167.
Xuefeng Zhu, Shiwen Yu, and Hui Wang. 1995. The development of contemporary chinese grammatical knowl-
edge base and its applications. International journal of asian language processing, 5(1,2):39?41.
1896
Proceedings of the ACL 2014 Student Research Workshop, pages 48?55,
Baltimore, Maryland USA, June 22-27 2014.
c?2014 Association for Computational Linguistics
Learning Grammar with Explicit Annotations for Subordinating
Conjunctions
Dongchen Li, Xiantao Zhang and Xihong Wu
Key Laboratory of Machine Perception and Intelligence
Speech and Hearing Research Center
Peking University, Beijing, China
{lidc,zhangxt,wxh}@cis.pku.edu.cn
Abstract
Data-driven approach for parsing may suf-
fer from data sparsity when entirely un-
supervised. External knowledge has been
shown to be an effective way to alleviate
this problem. Subordinating conjunctions
impose important constraints on Chinese
syntactic structures. This paper proposes a
method to develop a grammar with hierar-
chical category knowledge of subordinat-
ing conjunctions as explicit annotations.
Firstly, each part-of-speech tag of the sub-
ordinating conjunctions is annotated with
the most general category in the hierar-
chical knowledge. Those categories are
human-defined to represent distinct syn-
tactic constraints, and provide an appropri-
ate starting point for splitting. Secondly,
based on the data-driven state-split ap-
proach, we establish a mapping from each
automatic refined subcategory to the one
in the hierarchical knowledge. Then the
data-driven splitting of these categories is
restricted by the knowledge to avoid over
refinement. Experiments demonstrate that
constraining the grammar learning by the
hierarchical knowledge improves parsing
performance significantly over the base-
line.
1 Introduction
Probabilistic context-free grammars (PCFGs) un-
derlie most of the high-performance parsers
(Collins, 1999; Charniak, 2000; Charniak and
Johnson, 2005; Zhang and Clark, 2009; Chen and
Kit, 2012; Zhang et al, 2013). However, a naive
PCFG which simply takes the empirical rules and
probabilities off of a Treebank does not perform
well (Klein and Manning, 2003; Levy and Man-
ning, 2003; Bansal and Klein, 2012), because
its context-freedom assumptions are too strong in
some cases (e.g. it assumes that subject and ob-
ject NPs share the same distribution). Therefore,
a variety of techniques have been developed to en-
rich PCFG (Klein and Manning, 2005; Matsuzaki
et al, 2005; Zhang and Clark, 2011; Shindo et al,
2012).
Hierarchical state-split approach (Petrov et al,
2006; Petrov and Klein, 2007; Petrov and Klein,
2008a; Petrov and Klein, 2008b; Petrov, 2009)
refines and generalizes the original grammars in
a data-driven manner, and achieves state-of-the-
art performance. Starting from a completely
markovized X-Bar grammar, each category is split
into two subcategories. EM is initialized with this
starting point and used to climb the highly non-
convex objective function of computing the joint
likelihood of the observed parse trees. Then a
merging step applies a likelihood ratio test to re-
verse the least useful half part of the splits. Learn-
ing proceeds by iterating between those two steps
for six rounds. Spectral learning of latent-variable
PCFGs (Cohen et al, 2012; Bailly et al, ; Co-
hen et al, 2013b; Cohen et al, 2013a) is an-
other effective manner of state-split approach that
provides accurate and consistent parameter esti-
mates. However, this two complete data-driven
approaches are likely to be hindered by the over-
fitting issue.
Incorporating knowledge (Zhang et al, 2013;
Wu et al, 2011) to refine the categories in train-
ing a parser has been proved to remedy the
weaknesses of probabilistic context-free grammar
(PCFG). The knowledge contains content words
semantic resources base (Fujita et al, 2010; Agirre
et al, 2008; Lin et al, 2009), named entity cues
(Li et al, 2013) and so on. However, they are
limited in that they do not take into account the
knowledge about subordinating conjunctions.
Subordinating conjunctions are important in-
dications for different syntactic structure, espe-
48
cially for Chinese. For example, the subordinating
conjunction ???? (no matter what) is typically
ahead of a sentence with pros and cons of the sit-
uation; on the contrary, a sufficient condition of-
ten occurs after the subordinating conjunction ?X
J? (if). Those two cases are of distinct syntac-
tic structure. Figure 1 demonstrates that although
the sequences of the part-of-speech of the input
words are similar, these two subordinating con-
junctions exert quite different syntactic constraints
to the following clauses.
IP
VP
VP
VA
??
succeed
ADVP
AD
?
not
CC
??
or
VP
VA
??
succeed
ADVP
CS
??
No
matter
(a) ???? (no matter what) is typically ahead of a sentence
with pros and cons of the situation.
IP
IP
VP
VP
VP
VA
??
succeed
ADVP
AD
?
don?t
ADVP
AD
??
still
NP
PN
\
you
ADVP
CS
XJ
if
(b) ?XJ? (if) often precedes a sufficient condition.
Figure 1: Different types of subordinating con-
junctions indicate distinct syntactic structure.
Based on the hierarchical state-split approach,
this paper proposes a data-oriented model super-
vised by our hierarchical subcategories of subordi-
nating conjunctions. In order to constrain the auto-
matic subcategory refinement, we firstly establish
the mapping between the automatic clustered sub-
categories and the predefined subcategories. Then
we employ a knowledge criterion to supervise the
hierarchical splitting of these subordinating con-
junction subcategories by the automatic state-split
approach, which can alleviate over-fitting. The ex-
periments are carried out on Penn Chinese Tree-
bank and Tsinghua Treebank, which verify that
the refined grammars with refined subordinating
conjunction categories can improve parsing per-
formance significantly.
The rest of this paper is organized as follows.
We first describe our hierarchical subcategories of
subordinating conjunction. Section 3 illustrates
the constrained grammar learning process in de-
tails. Section 4 presents the experimental evalua-
tion and the comparison with other approaches.
2 Hierarchical Subcategories of
Subordinating Conjunction
The only tag ?CS? for all the various subordinat-
ing conjunctions is too coarse to indicate the in-
tricate subordinating relationship. The words in-
dicating different grammatical features share the
same tag ?CS?, such as transition relationship,
progression relationship, preference relationship,
purpose relationship and condition relationship. In
each case, the context is different, and the subor-
dinating conjunction is an obvious indication for
the parse disambiguation for the context. The ex-
isting resources for computational linguistic, like
HowNet (Dong and Dong, 2003) and Cilin (Mei
et al, 1983), have classified all subordinating con-
junctions as one category, which is too coarse to
capture the syntactic implication.
To make use of the indication, we subdivide the
subordinating conjunctions according to its gram-
matical features in our scheme. Subordinating
conjunctions indicating each relationship is further
subdivided into two subcategories: one is used be-
fore the principal clause, the other is before the
subordinate clause. For example, the conjunc-
tions representing cause and effect contains ?be-
cause? and ?so?, where ?because? should mod-
ify the cause, and ?so? should modify the effect.
In addition, we found that there are several cases
in the conditional clause. Accordingly, we sub-
divide the conditional subordinating conjunctions
into seven types: assumption, universalization,
49
SubordinatingConjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
LatterOf?Transition
?? %
Formerof?Transition
?? ?,
Progression
{
FormerOf?Progression
?? ?
LatterOf?Progression
?? $?
Preference
{
LatterOf?Preference
?? ?X
FormerOf?Preference
?? ??
LogicCoordination
?
?
?
LatterOfTheCoordinationq
Logic?And
?? ??
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Assumption XJ
Universalization??
UnnecessaryConditionQ,
InsufficientCondition=?
SufficientCondition??
NecessaryCondition?k
Equality??
Purpose
{
LatterOf?Purpose
?? ?B
FormerOf?Purpose
?? ??
CauseAndEffect
{
Causedu
EffectJ
Figure 2: Hierarchical subcategories of subordinating conjunctions with examples.
equality, sufficient condition, necessary condition,
sufficient but unnecessary condition and necessary
but insufficient condition (concession). The de-
tailed hierarchical subcategories of subordinating
conjunctions are displayed in Figure 2.
3 Parsing with Hierarchical Categories
The automatic state-split approach is designed to
refine all symbols together through a data-driven
manner, which takes the over-fitting risk. Instead
of splitting and merging all symbols together auto-
matically, we employ a knowledge-based criterion
with hierarchical refinement knowledge to con-
straint the splitting of these new refined tags for
subordinating conjunctions.
At the beginning, we produce a good starting
annotation with the top subcategories in the hi-
erarchical subcategories, which is of great use to
constraining the automatic splitting process. As
demonstrated in Figure 4, our parser is trained on
the good initialization with the automatic hierar-
chical state-split process, and gets improvements
compared with the original training data. For ex-
ample, as shown in Figure 2, the category for
%(but) and ?Cause? for du(because) is anno-
tated as the top category ?Transition? and ?Cause
And Effect? respectively.
However, during this process, only the most
general hypernyms are used as the semantic rep-
resentation of words, and the lower subcategory
knowledge in the hierarchy is not explored. Thus,
we further constraint the split of the subordinating
conjunctions subcategories to be consistent with
the hierarchical subcategories to alleviate the over-
fitting issue. The top class is only used as the start-
ing annotations of POS tags to reduce the search
space for EM in our method. It is followed by the
hierarchical state-split process to further refine the
starting annotations based on the hierarchical sub-
categories.
3.1 Mapping from Automatic Subcategories
to Predefined Subcategories
With the initialization proposed above, the auto-
matically split-merge approach produces a series
of refined categories for each tag. We restrict each
automatically refined subcategory of subordinat-
ing conjunctions to correspond to a special node
50
Figure 3: A schematic figure for the hierarchical state-split process of the tag ?CS?. Each subcategory
of this tag has its own word set, and corresponds to one layer at the appropriate level in the hierarchical
subcategories.
in the hierarchical subcategories, as a hyponym
of ?CS?. The hierarchical subcategories are em-
ployed in the hierarchical state-split process to im-
pose restrictions on the subcategory refinement.
First of all, it is necessary to establish the map-
ping from each subcategory in the data-driven hi-
erarchical subcategories to the subcategory in the
predefined hierarchical subcategories. We trans-
fer the method for semantic-related labels (Lin et
al., 2009) to our case here. The mapping is imple-
mented with the word set related to each automati-
cally refined granularity of clustered subordinating
conjunctions and the node at the special level in
the subcategory knowledge. The schematic in Fig-
ure 3 demonstrates this supervised splitting pro-
cess for CS. The left part of this figure is the word
sets of automatic clustered subcategories of the
CS, which is split hierarchically. As expressed
by the lines, each subcategory corresponds to one
node in the right part of this figure, which is our hi-
erarchical subcategory knowledge of subordinat-
ing conjunctions.
As it is shown in Figure 3, the original tag ?CS?
treats all the words it produces as its word set.
Upon splitting each coarse category into two more
specific subcategories, its word set is also cut into
two subsets accordingly, through forcedly divid-
ing each word in the word set into one subcategory
which is most probable for this word in the lex-
ical grammar. And each automatic refinement is
mapped to the most specific subcategory (that is to
say, the lowest node) that contains the entirely cor-
responding word set in the human-defined knowl-
edge. On this basis, the new knowledge-based cri-
terion is introduced to enrich and generalize these
subcategories, with the purpose of fitting the re-
finement to the subcategory knowledge rather than
the training data.
3.2 Knowledge-based Criterion for
Subordinating Conjunctions Refinement
With the mapping between the automatic refined
subcategories and the human-defined hierarchical
subcategory knowledge, we could supervise the
automatic state refinement by the knowledge.
Instead of being merged by likelihood, a
knowledge-based criterion is employed, to decide
whether or not to go back to the upper layer in
the hierarchical subcategories and thus remove the
new subcategories of these tags. The criterion is
that, we assume that the bottom layer in the hi-
erarchical subcategories is special enough to ex-
press the distinction of the subordinating conjunc-
tions. If the subcategories of the subordinating
conjunctions has gone beyond the bottom layer,
then the new split subcategories are deemed to be
unnecessary and should be merged back. That is
to say, once the parent layer of this new subcate-
gory is mapped onto the most special subcategory,
it should be removed immediately. As illustrated
51
Treebank Train Dataset Develop Dataset Test Dataset
CTB5 Articles 1-270 Articles 400-1151, 301-325 Articles 271-300
TCT 16000 sentences 800 sentences 758 sentences
Table 1: Data allocation of our experiment.
in Figure 3, if the node has no hyponym, this sub-
category has been specialized enough according to
the knowledge, and thus the corresponding subcat-
egory will stop splitting.
By introducing a knowledge-based criterion,
the issue is settled whether or not to further split
subcategories from the perspective of predefined
knowledge. To investigate the effectiveness of the
presented approach, several experiments are con-
ducted on both Penn Chinese Treebank and Ts-
inghua Treebank. They reveal that the subcategory
knowledge of subordinating conjunctions is effec-
tive for parsing.
4 Experiments
4.1 Experimental Setup
We present experimental results on both Chinese
Treebank (CTB) 5.0 (Xue et al, 2002) (All traces
and functional tags were stripped.) and Tsinghua
Treebank (TCT) (Zhou, 2004). All the experi-
ments were carried out after six cycles of split-
merge.
The data set alocation is described in Table 1.
We use the EVALB parseval reference imple-
mentation (Sekine, 1997) for scoring. Statistical
significance was checked by Bikel?s randomized
parsing evaluation comparator (Bikel, 2000).
4.2 Parsing Performance with Hierarchical
Subcategories
We presented a flexible approach which refines
the subordinating conjunctions in a hierarchy fash-
ion where the hierarchical layers provide different
granularity of specificity. To facilitate the compar-
isons, we set up 6 experiments on CTB5.0 with
different strategies of choosing the subcategory
layers in the hierarchical subcategory knowledge:
? baseline: Training without hierarchical sub-
category knowledge
? top: Choosing the top layer in hierarchi-
cal subcategories (using ?Transition?, ?Con-
dition? , ?Purpose? and so on)
? bottom: Choosing the bottom layer in hierar-
chical subcategories (the most specified sub-
categories)
? word: Substituting POS tag with the word it-
self
? knowledge criterion: Automatically choos-
ing the appropriate layer through the knowl-
edge criterion
Figure 4: Comparison of parsing performance for
each model in the split-merge cycles.
Figure 4 shows the F
1
scores of the last 4 cy-
cles in the 6 split-merge cycles. The results are
just as expectation, through which we can tell that
the ?top? model performs slightly better than the
baseline owing to a better start point of the state-
splitting. This result confirms the value of our
initial explicit annotations. While the ?bottom?
model doesn?t improve the performance due to
excessive refinement and causes over-fitting, the
?word? model behaves even worse for the same
reason. In the 5th split-merge cycle, the ?knowl-
edge criterion? model picks the appropriate layer
52
in hierarchical subcategories and achieves the best
result.
We also test our method on TCT. Table 2 com-
pares the accuracies of the baseline, initialization
with top subcategories and the ?knowledge cri-
terion? model, and confirms that the subcategory
knowledge helps parse disambiguation.
Parser P R F
1
baseline 74.40 74.28 74.34
top 75.12 75.17 75.14
knowledge criterion 76.18 76.27 76.22
Table 2: Our parsing performance with different
criterions on TCT.
4.3 Final Results
Our final results are achieved using the ?knowl-
edge criterion? model. As we can see from the
table 3, our final parsing performance is higher
than the unlexicalized parser (Levy and Manning,
2003; Petrov, 2009) and the parsing system in
Qian and Liu (2012), but falls short of the systems
using semantic knowledge of Lin et al (2009) and
exhaustive word formation knowledge of Zhang et
al. (2013).
Parser P R F
1
Levy(2003) 78.40 79.20 78.80
Petrov(2009) 84.82 81.93 83.33
Qian(2012) 84.57 83.68 84.13
Zhang(2013) 84.42 84.43 84.43
Lin(2009) 86.00 83.10 84.50
This paper 85.93 82.87 84.32
Table 3: Our final parsing performance compared
with the best previous works on CTB5.0.
The improvement on the hierarchical state-split
approach verifies the effectiveness of the subcat-
egory knowledge of subordinating conjunctions
for alleviating over-fitting. And the subcategory
knowledge could be integrated with the knowl-
edge base employed in Lin et al (2009) and Zhang
et al (2013) to contribute more on parsing accu-
racy improvement.
5 Conclusion
In this paper, we present an approach to constrain
the data-driven state-split method by hierarchi-
cal subcategories of subordinating conjunctions,
which appear as explicit annotations in the gram-
mar. The parsing accuracy is improved by this
method owing to two reasons. Firstly, the most
general hypernym of subordinating conjunctions
exerts an initial restrict to the following splitting
step. Secondly, the splitting process is confined
by a knowledge-based criterion with the human-
defined hierarchical subcategories to avoid over
refinement.
Acknowledgments
We thank Baidu for travel and conference sup-
port for this paper. We thank Meng Zhang and
Dingsheng Luo for their valuable advice. This
work was supported in part by the National Ba-
sic Research Program of China (973 Program) un-
der grant 2013CB329304, the Research Special
Fund for Public Welfare Industry of Health under
grant 201202001, the Key National Social Science
Foundation of China under grant 12&ZD119, the
National Natural Science Foundation of China un-
der grant 91120001.
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and pp attachment perfor-
mance with sense information. Proceedings of ACL-
08: HLT, pages 317?325.
Rapha?el Bailly, Xavier Carreras P?erez, Franco M
Luque, and Ariadna Julieta Quattoni. Unsupervised
spectral learning of wcfg as low-rank matrix com-
pletion. Association for Computational Linguistics.
Mohit Bansal and Daniel Klein. 2012. An all-
fragments grammar for simple and accurate parsing.
Technical report, DTIC Document.
Bikel. 2000. Dan bikel?s random-
ized parsing evaluation comparator. In
http://www.cis.upenn.edu/dbikel/software.html.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd annual meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
53
American chapter of the association for computa-
tional Linguistics conference, pages 132?139. Asso-
ciation for Computational Linguistics.
Xiao Chen and Chunyu Kit. 2012. Higher-order con-
stituent parsing and parser combination. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics: Short papers-
Volume 2, pages 1?5. Association for Computational
Linguistics.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th an-
nual meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 223?231.
Association for Computational Linguistics.
Shay B Cohen, Giorgio Satta, and Michael Collins.
2013a. Approximate pcfg parsing using tensor de-
composition. In Proceedings of NAACL-HLT, pages
487?496.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2013b. Experiments with
spectral learning of latent-variable pcfgs. In Pro-
ceedings of NAACL-HLT, pages 148?157.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Zhendong Dong and Qiang Dong. 2003. Hownet-a hy-
brid language and knowledge resource. In Proceed-
ings of the international conference on natural lan-
guage processing and knowledge engineering, pages
820?824. IEEE.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on lan-
guage and computation, 8(1):1?22.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st annual meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Dan Klein and Christopher D Manning. 2005. Parsing
and hypergraphs. In New developments in parsing
technology, pages 351?372. Springer.
Roger Levy and Christopher D Manning. 2003. Is
it harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st annual meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439?446. Association for Computational Lin-
guistics.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2013.
Improved chinese parsing using named entity cue.
In Proceeding of the 13th international conference
on parsing technology, pages 45?53.
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu,
and Huisheng Chi. 2009. Refining grammars for
parsing with hierarchical semantic knowledge. In
Proceedings of the 2009 conference on empirical
methods in natural language processing: Volume 3-
Volume 3, pages 1298?1307. Association for Com-
putational Linguistics.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd annual meeting on Associ-
ation for Computational Linguistics, pages 75?82.
Association for Computational Linguistics.
Jia-Ju Mei, YM Li, YQ Gao, et al 1983. Chinese
thesaurus (tong-yi-ci-ci-lin).
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North Amer-
ican chapter of the Association for Computational
Linguistics, pages 404?411.
Slav Petrov and Dan Klein. 2008a. Discriminative
log-linear grammars with latent variables. Advances
in neural information processing systems, 20:1153?
1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the conference on empirical meth-
ods in natural language processing, pages 867?876.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
international conference on computational linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433?440. As-
sociation for Computational Linguistics.
Slav Orlinov Petrov. 2009. Coarse-to-Fine natural
language processing. Ph.D. thesis, University of
California.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 501?
511. Association for Computational Linguistics.
Collins Sekine. 1997. Evalb bracket scoring program.
In http://nlp.cs.nyu.edu/evalb/.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing.
In Proceedings of the 50th annual meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 440?448. Association for
Computational Linguistics.
54
Xihong Wu, Meng Zhang, and Xiaojun Lin. 2011.
Parsing-based chinese word segmentation integrat-
ing morphological and syntactic information. In
Proceedings of 7th international conference on nat-
ural language processing and knowledge engineer-
ing (NLP-KE), pages 114?121. IEEE.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of the 19th international confer-
ence on computational linguistics-Volume 1, pages
1?8. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162?171. Association for Computational Linguis-
tics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational linguistics, 37(1):105?151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
51st annual meeting of the Association for Compu-
tational Linguistics.
Qiang Zhou. 2004. Annotation scheme for chinese
treebank. Journal of Chinese information process-
ing, 18(4):1?8.
55
