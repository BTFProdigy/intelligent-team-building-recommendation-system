Understanding Students? Explanations in Geometry Tutoring
Octav Popescu, Vincent Aleven, and Kenneth Koedinger
Human-Computer Interaction Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
octav@cmu.edu, aleven@cs.cmu.edu, koedinger@cmu.edu
Abstract
Precise Natural Language Understanding is
needed in Geometry Tutoring to accurately
determine the semantic content of students?
explanations. The paper presents an NLU
system developed in the context of the
Geometry Explanation Tutor. The system
combines unification-based syntactic
processing with description logics based
semantics to achieve the necessary accuracy
level. Solutions to specific semantic problems
dealing with equivalence of semantic
representations are described. Experimental
results on classification accuracy are also
presented.
1 Introduction
The Geometry Cognitive Tutor is designed to
help high school and middle school students
learn geometry. As a kind of Cognitive Tutor
(Anderson et al 1995), the system is based on an
underlying cognitive model, implemented as an
ACT-R production system (Anderson and
Lebiere 1998), of both novice and ideal student
knowledge. This model is used to monitor
student performance and to provide assistance
just when students need it and in a context that
demands it. Currently the Geometry Cognitive
Tutor is in regular use (two days per week) in
about 350 schools around the US.
The tutor proposes problems to students and
checks their solutions step by step. It can also
provide context-sensitive hints at each step in
solving the problem, as needed. The students are
asked to compute various elements of the
problem (mostly angle measures) by applying the
theorems and definitions they have learned. In
case the results are correct, the students are also
asked to provide a justification for their results.
In the version of the tutor that is currently in use
in schools, the justification consists in choosing
the right theorem or definition out of a menu.
The choice is accepted or rejected by the tutor,
accordingly.
The use of this tutor in combination with
classroom instruction has been shown to lead to
improvements in students? test scores over
traditional classroom instruction alone
(Koedinger et al 1997). Experiments have also
shown that the use of menu-based justifications
help students learn with greater understanding
over a tutor that does not ask for justifications
(Aleven and Koedinger 2002).
However there is room for improvement.
Classroom observation shows that some students
try to game the system by choosing each item in
the menu in turn, until one is accepted. Even
when this is not the case, it is possible that
simply recognizing the name of the correct
theorem or definition out of a menu does not
imply that the student is fully knowledgeable
about the actual content of the theorem involved.
Thus it is plausible that asking the students to
express the content of these theorems and
definitions in their own words as a form of self-
explanation could lead to a deeper level of their
understanding.
To verify this hypothesis we are currently
developing the Geometry Explanation Tutor.
This version is based on the Geometry Cognitive
Tutor, with the modification that the justification
consists of a natural language sentence that
expresses the content of the theorem or definition
used to derive the result, in free form. The tutor
checks the semantic content of the explanation
and provides feedback on its correctness and
completeness. In case the explanation quality is
deemed not good enough, the student is allowed
to refine it, until it becomes acceptable.
Thus, one of the main problems that the
Geometry Explanation Tutor faces is to
determine with accuracy the semantic content of
students? utterances. There are many different
ways to express the same semantic content,
which have to be recognized as being equivalent.
The determination of equivalence relations has to
work reliably over variation of syntactic
structure, variation of content words, or a
combination of both. For example, the sentences
below all express the same geometry theorem,
about the measures of angles formed by other
angles.
An angle formed by adjacent angles is equal to the
sum of these angles.
The measure of an angle formed by other angles is
equal to the sum of the measures of those adjacent
angles.
An angle's measure is equal to the sum of the two
adjacent angles that form it.
The sum of the measures of two adjacent angles is
equal to the measure of the angle formed by the two
angles.
The measure of an angle formed by two adjacent
angles is equal to the sum of the measures of the two
angles.
If adjacent angles form an angle, its measure is their
sum.
When an angle is formed by adjacent angles, its
measure is equal to the sum of those angles.
The process has also to be consistent, so no
unwarranted conclusions are derived from the
text, and robust, in an environment of imprecise
or ungrammatical language, as uttered more often
than not by high school students. Many times this
content equivalence relies on inferences specific
to the domain of discourse. Our hypothesis is that
such a high-precision recognition process needs
to be based on contextual information about the
domain of discourse modeled in a logic system.
2 The System?s Architecture
The system?s overall architecture is presented in
Figure 1 below. The interface module takes the
input sentence from the tutor, word by word, in
real time, and after some preprocessing and
spelling checking, it passes it to the chart parser.
It also passes the results back to the tutor. The
chart parser is the main engine of the system. It
uses linguistic knowledge about the target natural
language from the unification grammar and the
lexicon. The parser used currently is LCFlex, a
left-corner active-chart parser developed at the
University of Pittsburgh (Ros? and Lavie 1999).
The parser calls the feature structure unifier in
order to process restrictions attached to grammar
rules and build feature structures for each phrase
successfully recognized. These feature structures
store lexical, syntactic, and semantic properties
of corresponding words and phrases. The parser
uses an active chart that serves as a storage area
for all valid phrases that could be built from the
word sequence it received up to each point in the
process.
Figure 1. System Architecture
Some of the restrictions in the grammar are
directives to the description logic system,
currently Loom (MacGregor 1991). The logics
system relies on a model of the domain of
discourse, encoded as concepts, relations, and
production rules, in the two knowledge bases.
Concepts and relations stand for predicates in the
underlying logic. Production rules perform
additional inferences that are harder to encode
into concepts and/or relations.
The linguistic inference module mediates the
interaction between the feature structure unifier
and the description logics system. This module is
responsible for performing semantic processing
that is specific to natural language understanding,
like compositional semantics, resolving
metonymies and references, and performing
semantic repairs.
Based on this knowledge base, the logic
system builds compositionally a model-theoretic
semantic representation for the sentence, as a set
of instances of various concepts connected
through various relations. An instance
corresponds to a discourse referent in the
sentence. The logic system performs forward-
chaining classification of resulting instances, and
also ensures semantic coherence of the semantic
representation.
The logic system then uses a classifier to
evaluate the semantic representation against a
classification hierarchy of valid representations
TUTOR SYNTACTIC PROCESSING
SEMANTIC PROCESSING
UPPER MODEL
KNOWLEDGE
BASE
STUDENT
MODEL
CHART
PARSER
(LCFLEX)
UNIFICATION
GRAMMAR
LEXICON
FEATURE
STRUCTURE
UNIFIER
FEATURE
STRUCTURES
ACTIVE
CHART
INTERFACE
MODULE
PRODUCTION
ENGINE
CLASSIFICATION
HIERARCHY
SEMANTIC
REPRESENTATION
COGNITIVE
MODEL
LINGUISTIC
INFERENCE
DESCRIPTION
LOGIC (LOOM)
GEOMETRY
KNOWLEDGE
BASE
for geometry theorems. The results of the
classification are passed back to the tutor.
1.1 Example of Compositional Build of
the Semantic Representation
To see how the compositional building of
semantic representations works, let?s consider the
last step in parsing the sentence:
The measure of a right angle is 90 degrees.
A set of simplified knowledge base definitions
necessary for building its representation, in
Loom?s definitional language, is:
(defconcept Configuration
  :is-primitive (:and Thing (:all participant Thing)))
(defconcept Being&Having
  :is-primitive (:and Configuration
                              (:at-most 2 participant)))
(defconcept Ascription
  :is (:and Being&Having (:exactly 1 attribuend)
                                        (:exactly 1 attribute))
(defproduction ascription-production
  :when (:detects (Ascription x))
  :do ((combine-instances (x attribute)
                                         (x attribuend))))
(defconcept Geometry-Unit
  :is (:and Unit (:one-of ?degree ?radian ?meter ?foot)))
(defconcept Angle-Unit
  :is (:and Geometry-Unit (:one-of ?degree ?radian)))
(defconcept Geometry-Measure
  :is (:and Measure (:the unit Geometry-Unit)))
(defconcept Angle-Measure
  :is (:and Geometry-Measure (:the unit Angle-Unit)))
(defconcept Geometry-Object
  :is-primitive (:and Spatial
                            (:all measure Geometry-Measure)))
(defproperty Right :domain Geometry-Object)
(defconcept Angle
  :is-primitive (:and Geometry-Object
                              (:the measure Angle-Measure)))
(defconcept Right-Angle :is (:and Right Angle))
Figure 2. Example of Semantic Representation
Based on these definitions and the rules of the
grammar, the system builds the representation
below for the subject of the example above,
expressed in Loom?s assertional language:
(tell (:about measure-1 (:create Angle-Measure)))
(tell (:about angle-1 (:create Right-Angle)
          (measure measure-1)))
The system also builds this structure for the verb
phrase:
(tell (:about measure-2 (:create Angle-Measure)
          (unit ?degree) (value 90)))
(tell (:about being&having-1 (:create Being&Having)
          (attribute measure-2)))
The two structures are illustrated in Figure 2.
Then the parser applies the grammar rule for
clauses, given below in simplified form. Connect-
s eman t i c s  will assert an attribuend relation
between instances being&having-1 and measure-1,
relation specified in the lexicon as the semantic
role of the verb?s subject.
(<Cl> ==> (<NP> <VP>)
         ((x0 = x2)
          ((x0 subject) = x1)
          ((x0 semantics) <=
             (connect-semantics (x2 semantics)
                (x2 subject sem-role) (x1 semantics)))))
Loom then classifies being&having-1 as an
instance of the more specific concept Ascription,
and this classification triggers production
ascription-production. The production will combine
the two measure instances, measure-1 and
measure-2, into a single instance, resulting in the
structure below:
(tell (:about measure-1 (:create Angle-Measure)
          (unit ?degree) (value 90)))
(tell (:about angle-1 (:create Right-Angle)
          (measure measure-1)))
(tell (:about being&having-1 (:create Ascription)
          (attribute measure-1) (attribuend measure-1)))
The structure is shown in Figure 3.
Figure 3. Resulting Semantic Representation
This structure is then classified against a
hierarchy of concept definitions representing
classes of possible explanations. A few of them
are shown in Figure 4.
WS-8
VALUE
UNITMEASURE
ATTRIBUEND ATTRIBUTE
RIGHT-
ANGLE
ASCRIPTION
ANGLE-
MEASURE
NUMBER
ANGLE-
UNIT
ANGLE-1 MEASURE-1 DEGREE
90
BEING&
HAVING-1
WS-7
UNIT
ATTRIBUTE
VALUE
BEING&
HAVING-1
WS-4
MEASURE-1ANGLE-1
MEASURE
RIGHT-
ANGLE
ANGLE-
MEASURE
BEING&
HAVING
ANGLE-
MEASURE
ANGLE-
UNIT
NUMBER
MEASURE-2 DEGREE
90
Figure 4. Partial Classification Hierarchy
3  Specific Problems in Students?
Explanations
The basic approach we take to the content
equivalence problem is to provide the right
inference rules to make the logic system derive
the same semantic representation for all
sentences that are semantically equivalent. Below
we present how this is done in some specific
cases.
1.2 Variation of Syntactic Structure
Even when the choice of content words is the
same, the same meaning can be conveyed
through a variety of syntactic structures. Some
cases, like that of passive versus active
constructs, can be taken care of in the grammar.
Other cases require specific knowledge about the
domain of discourse. One such situation is that of
prepositional phrases attached in different places
in the sentence, without changing the meaning,
like in these examples:
In a triangle angles opposite to congruent sides are
congruent.
Angles opposite to congruent sides in a triangle are
congruent.
Angles in a triangle opposite to congruent sides are
congruent.
Angles opposite to congruent sides are congruent in
a triangle.
The solution in our system comes as a concept
definition that identifies the container relation at
the assertion level, and percolates it down to
involved objects.
(defconcept Ascription-Location
  :is (:and Ascription (:at-least 1 belongs-to))
  :implies
     (:and (:relates belongs-to attribuend belongs-to)
              (:relates belongs-to attribute belongs-to))))
A similar case is that of using constructs specific
to the domain of discourse.
The measures of these two angles are equal.
These two angles are equal in measure.
Knowledge about the semantics of ?equal? and
?measure? is involved in determining that ?equal
in measure? means the same thing as ?measures ?
are equal?. We can model this knowledge by
defining a rule that will identify cases of ?equal in
some measurable quantity? and will generate a
structure with the meaning of ?equal quantity?.
(defconcept Equal-in
  :is (:and Equal (:some belongs-to Measure)))
(defproduction equal-in-production
  :when (:detects (Equal-in ?object))
  :do (combine-semantics ?object
                                         (?object belongs-to)))
The use of relative and subordinate clauses can
also lead to a large variety of syntactic structures
without a significant change in meaning:
The sum of the measures of complementary angles
is 90 degrees.
If angles are complementary, then the sum of their
measures is 90 degrees.
The measures of the angles sum to 90 degrees,
because they are complementary angles.
Complementary angles are angles whose measures
sum to 90 degrees.
These sentences all express the same theorem
about complementary angles using respectively a
single clause sentence, a conditional clause, a
subordinate clause, or a relative clause. Because
the semantic representation we build does not
keep any trace of the original syntactic structure,
such variations are automatically ignored. For
example, the structure built for the first sentence
is:
(tell (:about measure-1 (:create Geometry-Measure)))
(tell (:about angle-1 (:create Angle)
          Complementary (measure measure-1))
(tell (:about sum-1 (:create Sum)
          (value 90) (unit 'degree) (term measure-1))
(tell (:about being&having-1 (:create Ascription)
          (attribute sum-1) (attribuend sum-1))
Ignoring the conditionality, the structures for the
two clauses in the second sentence are:
(tell (:about angle-1 (:create Angle) Complementary))
(tell (:about being&having-1 (:create Ascription)
          (attribute angle-1) (attribuend angle-1))
(tell (:about thing-1 (:create Thing)
          (measure measure-1)))
(tell (:about measure-1 (:create Geometry-Measure)))
(tell (:about sum-1 (:create Sum)
          (value 90) (unit 'degree) (term measure-1))
COMPLEMENTARY-
ANGLES
?The angles are
complementary.?
ANGLES-90
?The measure of this
angle is 90 degrees.?
UNKNOWN
ANGLE-SUM-90
?These angles add
up to 90.?
RIGHT-ANGLES-90
?The measure of a right
angle is 90 degrees.?
COMPLEMENTARY-
ANGLES-SUM-90
?Complementary angles
sum to 90.?
RIGHT-ANGLES
?This is a right angle.?
(tell (:about being&having-2 (:create Ascription)
          (attribute sum-1) (attribuend sum-1))
All that is needed to achieve semantic
equivalence is a reference resolution mechanism
that identifies referents at the semantic level with
their antecedents. In the example above the
system would solve thing-1  to angle-1.
1.3 Variation of Content Words
Many times differences in the content words
used in the sentence do not make any difference
at the meaning level. An obvious case is that of
synonyms. However there are cases when
different words are used as synonyms only in
certain contexts. For instance:
Angles ABC and BAC are equal.
Angles ABC and BAC are congruent.
Versus:
The measures of angles ABC and BAC are equal.
*The measures of angles ABC and BAC are
congruent.
Here the synonymy holds only when the objects
involved in the relation are geometry objects, and
it is not allowed when they are measures. We can
make this distinction by defining ?congruent? as a
specialized case of ?equal?:
(defrelation equal-to
  :is-primitive (:and relation (:domain Thing)
                                           (:range Thing))
  :characteristics :symmetric)
(defrelation congruent-to
  :is (:and equal-to (:domain Geometry-Object)
                              (:range Geometry-Object)))
Moreover, we can add a production rule that will
perform the inference that if the measures of
some objects are equal, then the objects
themselves are congruent. This rule will make
the third sentence above be recognized as
equivalent to the first two sentences.
(defconcept Equal-Measure
  :is (:and Measure (:at-least 1 equal-to)))
(defproduction equal-measure-production
  :when (:detects (Equal-Measure ?measure))
  :do (connect-semantics
            (?measure measure-of) congruent-to
            (?measure equal-to measure-of)))
A related phenomenon is that of using very
generic functional words in usual language to
denote specific relations among the concepts of
the domain.
The angles of a linear pair sum to 180.
The angles that form a linear pair sum to 180.
The angles that are elements of a linear pair sum to
180.
In these examples the angles are actually the
elements of the linear pair. However in the first
two sentences the relation is expressed either
through a preposition, or through a generic verb
like ?form?. Recovering the explicit relation and
thus being able to determine that the three
examples above are semantically equivalent
requires once again a model of the domain of
discourse. We can model this first by defining
the element-of relation as a more specific version
of the generic relation belongs-to expressed by
?of?. This definition will make the system build
the same representation for the first sentence as
for the third one.
(defconcept Set :is-primitive Thing)
(defrelation element-of
  :is (:and belongs-to (:domain Thing) (:range Set)))
Second, we can define a production rule that
recognizes a ?form? configuration and asserts a
?belongs-to? relation between the arguments, thus
generating for the second sentence the same
representation as for the first one:
(defconcept Form-Configuration
  :is (:and Generalized-Possession
               (:the part Thing)
               (:the whole Thing)))
(defproduction form-configuration-production
  :when (:detects
                 (Form-Configuration ?configuration))
  :do (connect-instances (?configuration part)
                                       belongs-to
                                       (?configuration whole)))
Another similar situation is that when students
use the definition of a concept expressed in terms
of more generic concepts, instead of its name.
Adjacent angles on a line sum to 180 degrees.
Linear angles sum to 180 degrees.
The ability to recognize such examples as being
semantically equivalent, with the right degree of
generality, is conditioned by the possibility to
model the definitions of those specific concepts
within the framework of the system. This case
can be dealt with by defining ? linear angles? as
?adjacent angles on a line?:
(defrelation adjacent-to
  :is-primitive (:and relation
                              (:domain Geometry-Object)
                              (:range Geometry-Object)))
(defconcept Adjacent-Angle
  :is (:and Angle (:at-least 1 adjacent-to)))
(defconcept Angle-on-Line
  :is (:and Angle (:some location Line)))
(defconcept Linear-Angle
  :is (:and Adjacent-Angle Angle-on-Line))
1.4 Syntactic Ambiguity
Syntactic ambiguity in many cases does not
reflect semantic ambiguity. One such possibility
is prepositional phrase attachment. That is,
following only the grammar rules, many times a
prepositional phrase could be an adjunct/
argument of several preceding components. A
deeper look at those alternative attachments
reveals that most of them can be discarded
because they do not result in a meaningful
sentence. However, in absence of detailed
knowledge about the meaning of the words in the
sentence and their possible interactions, an NLU
approach would not be able to disambiguate
among them.
The sum of the measures of the three interior angles
in a triangle is equal to 180 degrees.
The subject in this example contains three
prepositional phrases: ?of the measures?, ?of the
three interior angles?, and ?in a triangle?. While the
first one can only be attached to one place: the
noun ?sum?, the second one already can be
attached to two places: ?sum? or ?measures?, and
the third one can be attached to three places:
?sum?, ?measures?, or ?angles?, resulting in a
total of 6 different valid parses. By adding
appropriate restrictions to the definitions of the
concepts involved, our approach can make some
of these combinations invalid during the parsing
process. In our example we can restrict sums to
only apply to elements that are measures, and
thus eliminate the attachment of prepositional
phrase ?of the three interior angles? to ?the sum?.
And then we can restrict the containment relation
to have geometry objects on both sides, and thus
eliminate the attachment of ? in a triangle? to either
?the sum? or ?the measures?.
(defconcept Sum
  :is-primitive (:and Measure (:all term Measure)))
(defconcept Object-in-Location
  :is (:and Object (:some location Geometry-Object))
  :implies Geometry-Object)
1.5 Reference Resolution
Disambiguation
The presence of anaphora in students?
explanations results in cases where sentences
with different sets of words are semantically
equivalent. Recognizing the semantic
equivalence of such cases leads to the necessity
to have an accurate reference resolution
mechanism, which allows us to build the right
semantic representation for the sentence.
The resolution of referents to antecedents is
done in our system at the semantic level. That is
we simply try to merge the semantic
representation of the referent with that of the
antecedent. This mechanism has the advantage
that the logic system will make sure that all
semantic constraints associated with the two
discourse referents are enforced, so that elements
that are incompatible will fail the merge. This
takes care both of the number restrictions, as well
as all other semantic features, like taxonomic
compatibility between the concepts involved.
Finding the right referent for an anaphor is not
always easy. Syntactic criteria can help with
disambiguation among candidates, but there are
cases where they cannot lead to a unique
antecedent. Adding semantic constraints to the
solution can increase the accuracy considerably.
If the lengths of two sides of triangles are equal, then
the measures of the angles opposite them will also
be equal.
In this example there are five possible candidates
as antecedent for the pronoun ?them?: ?the
lengths?, ?two sides?, ?a triangle?, ?the measures?,
and ? the angles?. Constraints of the Binding
Theory implemented in our system eliminate ?the
angles?, since ?them? is a personal pronoun that
has to be free within its local domain. Constraints
on number eliminate ?a triangle?, as being
singular, while ?them? is plural. Then semantic
constraints attached to the definition of relation
?opposite? can eliminate both ?the lengths? and
?the measures?, by asking that geometry objects
can oppose only other geometry objects:
(defconcept Object-opposite
  :is (:and Geometry-Object
                (:some opposite-to Thing))
  :implies (:all opposite-to Geometry-Object))
4 Performance Evaluation
As a measure of the system?s ability to
understand students? explanations we evaluated
the accuracy of the classification of these
sentences with respect to the hierarchy of
explanation classes. The evaluation used a set of
700 sentences representing actual explanations
provided by high school students during an
experimental study in 2003. The classification
task consists in associating each sentence with
one or more of 200 fine-grained categories, a
difficult task even for humans. We used the
kappa statistic (Cohen 1960) to measure the
inter-rater reliability between the system and two
human raters. We used three different measures.
First, a ?set equality? measure, where two sets of
classes match only if they are identical. Second,
an ?overlap? measure, where two sets are
considered to partially match if they share some
subset. And third, a ?weighted overlap?, which
takes into account the relative semantic distance
between different classes in assessing the match
between two sets of categories. The results in
Table 1 show the system to work reasonably
well, although not at human level.
?
Actual
Agreement
Chance
Agreement s
?
Set equality
Human-
Human
0.84 0.84 0.034 0.014
System-
Human
0.65 0.66 0.025 0.018
Overlap
Human-
Human
0.87 0.88 0.040 0.012
System-
Human
0.73 0.74 0.033 0.016
Weighted
overlap
Human-
Human
0.92 0.94 0.30 0.0087
System-
Human
0.81 0.87 0.30 0.012
Table 1. Agreement between the system and
human raters.
Regarding the hypothesis question of whether
replacing menu-based justifications with natural
language justifications helps students have a
better understanding of geometry, we do not have
a definitive answer yet. Some experimental
results based on the same study seem to show
(Aleven et al 2004) that while students? ability
to express their knowledge was improved
considerably, students? performance on actual
problem solving was not affected significantly.
There are a number of possible causes for that, so
further studies are needed.
5 Conclusions
We present a natural language understanding
system that combines unification-based syntactic
processing with logic-based semantics. The
system is used in conjunction with a Geometry
Cognitive Tutor to help students better
understand geometry. The approach we take
allows for an elegant solution to the problem of
determining equivalence between various ways
to express the same meaning. Study results show
that the system works reasonably well on
classifying students? explanations on a grid of
about 200 fine-grained categories, although there
is space for further improvement. One particular
problem is robustness in the face of
ungrammaticality. Also the question of whether
natural language explanations improve students?
understanding of geometry still waits for a
definitive answer.
Acknowledgements
This work was supported by NSF ITR/IPE, NSF
grant No. EIA-0113864, ?Tutoring explanation
and discovery learning: Achieving deep
understanding through tutorial dialog.?
References
Vincent Aleven and Kenneth R. Koedinger 2002. An
Effective Meta-cognitive Strategy: Learning by
Doing and Explaining with a Computer-Based
Cognitive Tutor. Cognitive Science, 26(2), 147-
179.
Vincent Aleven, Amy Ogan, Octav Popescu, Cristen
Torrey, Kenneth R. Koedinger 2004. Evaluating
the Effectiveness of a Tutorial Dialogue System for
Self-Explanation. In Proceedings of the 7
th
International Conference on Intelligent Tutoring
Systems, Macelo, Brasil.
John R. Anderson, Albert T. Corbett, Kenneth R.
Koedinger, and Ray Pelletier 1995. Cognitive
tutors: Lessons learned. In The Journal of the
Learning Sciences, 4:167-207.
John R. Anderson and Christian Lebiere 1998. The
Atomic Components of Thought. Hillsdale, NJ:
Erlbaum.
Jacob Cohen 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement, 20, 37-46.
Kenneth R. Koedinger, John J. Anderson, William H.
Hadley and Mary A. Mark 1997. Intelligent
tutoring goes to school in the big city. In
International Journal of Artificial Intelligence in
Education, 8:30-43.
Robert MacGregor 1991. Using a description
classifier to enhance deductive inference. In
Proceedings of the Seventh IEEE Conference on AI
Applications, 141-147, Miami, FL.
Carolyn Penstein Ros? and Alon Lavie 1999. LCFlex:
An efficient robust left-corner parser, User?s
manual, University of Pittsburgh.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 195?198,
Prague, June 2007. c?2007 Association for Computational Linguistics
IRST-BP: Web People Search Using Name Entities 
 
Octavian Popescu 
FBK-irst, Trento (Italy) 
popescu@itc.it 
Bernardo Magnini 
FBK-irst, Trento (Italy) 
magnini@itc.it 
 
 
Abstract 
In this paper we describe a person clus-
tering system for web pages and report 
the results we have obtained on the test 
set of the Semeval 2007 Web Person 
Search task. Deciding which particular 
person a name refers to within a text 
document depends mainly on the capac-
ity to extract the relevant information 
out of texts when it is present. We con-
sider ?relevant? here to stand primarily 
for two properties: (1) uniqueness and 
(2) appropriateness. In order to address 
both (1) and (2) our method gives pri-
mary importance to Name Entities 
(NEs), defined according to the ACE 
specifications. The common nouns not 
referring to entities are considered fur-
ther as coreference clues only if they are 
found within already coreferred docu-
ments. 
1 Introduction 
Names are ambiguous items (Artiles, Gonzalo 
and  Sekine 2007). As reported on an experiment 
carried out on an Italian news corpus (Magnini 
et al 2006) within a 4 consecutive days from a 
local newspaper the perplexity is 56% and 14% 
for first and last name respectively. Deciding 
which particular person a name refers to within a 
text document depends mainly on the capacity to 
extract the relevant information out of texts 
when it is present1. We consider ?relevant? here 
to stand primarily for two properties: (1) 
uniqueness and (2) appropriateness. A feature is 
unique as long as it appears only with one per-
son. Consider a cluster of web pages that charac-
terizes only one person. Many of the N-grams in 
this cluster are unique compared to other cluster. 
Yet the uniqueness may come simply from the 
sparseness. Appropriateness is the property of an 
N-gram to characterize that person. 
Uniqueness may be assured by ontological 
properties (for example, ?There is a unique 
president of a republic at a definite moment of 
time?, ?Alberta University is in Canada). How-
ever, the range of ontological information we are 
able to handle is quite restricted and we are not 
able to realize the coreference solely relying on 
them. Uniqueness may be assured by estimating 
a very unlike probability of the occurrence of 
certain N-grams for different persons (as, for 
example, ?Dekang Lin professor Alberta Canada 
Google?).  
Appropriateness is a difficult issue because of 
two reasons: (a) it is a dynamic feature (b) it is 
hard to be localized and extracted from text. The 
greatest help comes from the name of the page, 
when it happens to be a suggestive name such as 
?homepage?, ?CV?, ?resume? or ?about?. Gene-
                                                 
1
 It is very difficult to evaluate whether the informa-
tion allowing the coreference of two instances of a 
(same) name is present in a web page or news. A 
crude estimation on our news corpus for the names 
occurring between 6-20 times, which represent 8% of 
the names inventory for the whole collection, is that 
in much more than 50% of the news, the relevant 
information is not present. 
195
alogy pages are very useful, to the extent that the 
information could be accurately extracted and 
that the same information occurs in some other 
pages as well. However, in general, for plain 
web pages, we rely on paragraphs in which a 
single person is mentioned and consequently, the 
search space for similarity is also within this 
type of paragraphs. 
Our proposal is to rely on special N-grams for 
coreference and it is a variant of agglomerative 
clustering based on social net-
works(Bagga&Baldwin 1998, Malin 2005) . The 
terms the N-grams contain are crucial. Suppose 
we have the same name shared by two different 
persons who happen to also have the same pro-
fession, let?s say, ?lawyer?, and who also prac-
tice in the same state. While all three words ? 
(name, profession, state) - might be rare words 
for the whole corpus, their probability computed 
as chance to be seen in the same document is 
low, their three-gram fails to cluster correctly the 
documents referring to the two persons2. Know-
ing that the ?lawyer? is a profession that has dif-
ferent specializations, which are likely to be 
found as determiners, we may address this prob-
lem more accurately considering the same three-
gram by changing ?lawyer? with a word more 
specific denoting her specialization. 
The present method for clustering people web 
pages containing names according addresses 
both uniqueness and appropiateness. We rely on 
a procedure that firstly identifies the surest cases 
of coreference and then recursively discover new 
cases. It is not necessarily the case that the latest 
found coreferences are more doubtful, but rather 
that the evidence required for their coreference 
is harder to achieve. 
The cluster metrics gives a primary impor-
tance to words denoting entities which are de-
fined according to ACE definitions: PER, LOC, 
ORG, GPE.  
In Section 2 we present in detail the architec-
ture of our system and in Section 3 we present 
its behavior and the results we obtained on the 
test set of Semeval 2007 Web Person Search 
task. In section 4 we present our conclusions and 
future directions for improvement. 
                                                 
2
 The traditional idf methods used in document clus-
tering must be further refined in order to be effective 
in person coreference. 
2 System Architecture 
First, the text is split into paragraphs, based 
mainly on the html structure of the page. We 
have a Perl script which decides weather the 
name of interest is present within a paragraph. If 
the test is positive the paragraph is marked as a 
person-paragraph, and our initial assumption is 
that each person-paragraph refers to a different 
person.  
The second step is considered the first proce-
dure of the feature extraction module. To each 
paragraph person we associate a set of NEs, rare 
words and temporal expressions, each of them 
counting as independent items. For all of these 
items which are inside of the same dependency 
path we also consider the N-grams made out of 
the respective items preserving the order. For 
each person-paragraph we compute the list of 
above items and consider them as features for 
clustering. This set is called the association set. 
The first step in making the coreference is the 
most important one and consists in two opera-
tions: (1) the most similar pages are clustered 
together and (2) for each cluster, we make a list 
of the pages which most likely do not refer to the 
same person. Starting with this initial estimation, 
the next steps are repeated till no new corefer-
ence is made.  
For each cluster of pages, a new set of items 
is computed starting from the association sets. 
Only the ones which are specific to the respec-
tive cluster - comparing against all other clusters 
and against the list of pages not related (see (2) 
above) ? are kept in the new association set. 
These are the features we use further for cluster-
ing. The clustering score of two person-
paragraphs is given by summing up the individ-
ual score of common features in their association 
sets. The score of a feature is determined based 
on its type - (NE, distinctive words, temporal 
expressions) - , its length in terms of words 
compounding it, and the number of its occur-
rences inside the cluster and inside the whole 
corpus, considering only the web pages relative 
to that name and the absolute frequency of the 
words. The feature score is finally weighed with 
a factor which expresses the distance between 
the name and the respective feature. An empiri-
cal threshold has been chosen. 
196
Each of the above paragraphs representing a 
module in our system is explained in one of the 
next subsections respectively. 
2.1 Preprocessing 
Web pages contain a lot of information outside 
the raw text. We wrote Perl scripts for identify-
ing the e-mail addresses, phone and fax numbers 
and extract them if they were in the same para-
graph with the name of interest. It seems that a 
lot can be gained considering the web addresses, 
the type of page, the links outside the pages and 
so on. However, we have not exploited up to 
now these extra clues for coreference. The whole 
corpus associated with a name is searched only 
once. If the respective items are found in two 
different pages, these two pages are clustered.  
In web pages, the visual structure plays an 
important role, and many times the graphics de-
sign substitutes for linguistics features. Using a 
normal html parser, such as lynx, the text may 
lack its usual grammatical structure which may 
drastically decrease the performances of sen-
tence splitters, Name Entity Recognizers and 
parsers. To alleviate this problem, the text is first 
tagged with PoS. If a paragraph, ?\n?, does not 
have a main verb, then it is treated separately. If 
the text contains only nouns and determiners and 
if the paragraph is within a paragraph containing 
the name of interest, the phrase ?You are talking 
about? is added in front of it to make it a normal 
sentence. 
The text is split into person-paragraphs, and 
each person-paragraph is split into sentences, 
lemmatized, the NEs are recognized 3  and the 
text is parsed using MiniPar (Dekang Lin 1998). 
We are interested only in dependency paths that 
are rooted in NEs ? the NP which are included in 
bigger XP, or sister of NPs, or contain time ex-
pressions. 
The person-paragraphs are checked for the in-
terest names. We write rules for recognizing the 
valid names. If a page does not have a valid 
name of interest, it is discarded. A page is also 
discarded when a valid name of interest has its 
entity type ?ORG?. 
                                                 
3
 We thank to the Textec group at IRST for making it 
possible for everyone to pre process the text very 
easily with state of the art performances. 
2.2 Feature Extraction 
The association set contains a set of features. 
The features are NEs or part of NEs, because the 
closed class words, the very frequent words ? 
computed on the set of all web pages for all per-
sons ? are deleted from the NEs4. When we refer 
to the length of a feature we mean the number of 
words it is made of, after deletion. 
We consider words (phrases) which are not 
NEs as features but only if they are frequent in 
already coreferred person-paragraphs. That is, 
initially the coreference is determined solely on 
NEs. If there is enough evidence, i.e. when a 
word is frequent within the cluster and not pre-
sent within other clusters, then the respective 
word (phrase) is taken into account for corefer-
ence. 
Time expressions are relevant indicators for 
coreference if they are appropriately linked to a 
person. We consider them always, just like a 
NE, but when they appear in particular depend-
ency trees they have a special value. If they are 
dominated by a name of interest and/or by the 
lemma ?birth?, ?born? we consider them as a 
sure factor for coreference.  
For all composed features we also consider 
the order preserved combinations of their parts 
obtaining new features. 
The association sets increase their cardinality 
by coreference. At each step, the new added fea-
tures are checked against the ones from the other 
clusters. The common features are kept in sepa-
rate sets. The coreference is not decided on their 
basis, but these features are used to identify the 
paragraph persons that do not refer to a particu-
lar person, and therefore should not be included 
in the same cluster. We do not explicitly weigh 
differently the features (apart of the cases men-
tioned above) but they are actually weighed dif-
ferently implicitly. The words within a com-
posed feature are repeated, a feature of length n 
produces n(n-1) new features, n> 2. Besides, as 
we will see in the next section, the similarity 
score uses the length of a feature. 
                                                 
4
 Sometimes, correctly or not,  the SVM base NER 
we use includes, especially inside of LOC and GPE 
name entities, common words. In order to remain as 
precise as possible, we choose not to consider these 
words when we compute the similarity score. 
197
2.3 Similarity Measure 
Our similarity score for two person-paragraphs 
is the sum of the individual scores of the com-
mon features which are weighed according to 
the maximum of distances between the name of 
interest and the feature. 
There are three parameters on which we rely 
for computing similarity: the length, the number 
of occurrences, and the absolute frequency of a 
feature. The score considers the cube of the fea-
ture length (which means that the one word fea-
tures do not score). We compute the ratio be-
tween the number of occurrences within the 
cluster and the number of occurrences in the web 
pages relative to that name. The third parameter 
is the absolute frequency of the words. As usu-
ally, if the word is a rare word it counts as more 
evidence for coreference. We regard these pa-
rameters as independent, in spite of their relative 
dependency, and we simply multiply them. 
We define the distance between a feature and 
a name as a discrete measure. If the name and 
the feature are sisters of the same head then their 
distance is minimum, therefore their importance 
for similarity is the highest. The second lower 
distance value is given within the same sentence 
and the distance increases with the number of 
sentences. If there are no other names mentioned 
in the paragraph, the distance is divided by half. 
We have established an empirical threshold 
which initially is very high, as the features are 
not checked among the clusters in the first run. 
After the first run, it is relaxed and the common 
and individual sets are computed as we have 
described in the previous section. 
3 Evaluation 
The system performance on the test set of Seme-
val 2007 Web Person Search task is F?=0.5 = 
0.75, harmonic means of purity, and F=0.2 = 0.80 
- the inverse purity mean. The data set has been 
divided in three sets: SET1 ACL people, SET2 
Wikipedia people, and SET3 census people. The 
results are presented in table 1. The fact that the 
system is less accurate on SET2 may be due to 
the fact that larger person paragraph are consid-
ered and therefore more inappropriate similarity 
are declared. 
 
 
Test 
Set 
Purity Inverse 
Purity 
F?=0.5 
SET1 0,75 0,80 0,77 
SET2 0,83 0,71 0,77 
SET3 0,81 0,75 0,78 
4 Conclusion and Further Research 
Our method is greedy and it depends a lot on the 
accuracy of coreference as the system propa-
gates the errors from step to step. 
One of the big problems of our system is the 
preprocessing step and further improvement is 
required. That is because we rely on the per-
formances of NER and parsers. We also hope 
that by the inclusion of extra textual information 
the html carries, we will have better results. 
A second direction for us is to exactly under-
stand the role of ontological information. For the 
moment, we recognized some of the words de-
noting professions and we tried to guess their 
determinators. We think that having hierarchical 
relationships among LOC, GPE and also for 
ORG may make a difference in results especially 
for massive corpora. 
References 
Artiles, J., Gonzalo, J. and Sekine, S. (2007). 
Establishing a benchmark for the Web People 
Search Task: The Semeval 2007 WePS Track. 
In Proceedings of Semeval 2007, Association 
for Computational Linguistics. 
Bagga A., Baldwin B.,(1998) Entity-Based 
cross-document-referencing using vector 
space model, In proceedings of 17th  Interna-
tional Conference on Computational Linguis-
tics 
Magnini B., Pianta E., Popescu O. and Speranza 
M. (2006). Ontology Population from Textual 
Mentions: Task Definition and Benchmark. 
Proceedings of the OLP2 workshop on Ontol-
ogy Population and Learning, Sidney, Austra-
lia,. Joint with ACL/Coling  
Malin. B., (2005): Unsupervised Name Disam-
biguation via Network Similarity, In proceed-
ings SIAM Conference on Data Mining 2005 
Zanolli R., Pianta E. (2006) Technical report, 
ITC IRST 
198
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 26?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ontology Population from Textual Mentions:
Task Definition and Benchmark
Bernardo Magnini, Emanuele Pianta, Octavian Popescu and
Manuela Speranza
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
Via Sommarive 18, 38050 Povo (TN), Italy
{magnini, pianta, popescu, manspera}@itc.it
Abstract
In this paper we propose and investigate
Ontology Population from Textual Mentions
(OPTM), a sub-task of Ontology Population
from text where we assume that mentions for
several kinds of entities (e.g. PERSON,
O R G A N I Z A T I O N , LO C A T I O N , GEO-
POLITICAL_ ENTITY) are already extracted
from a document collection. On the one
hand, OPTM simplifies the general Ontology
Population task, limiting the input textual
material; on the other hand, it introduces
challenging extensions to Ontology Popula-
tion restricted to named entities, being open
to a wider spectrum of linguistic phenomena.
We describe a manually created benchmark
for OPTM and discuss several factors which
determine the difficulty of the task.
1 Introduction
Mentions are portions of text which refer to enti-
ties
1
. As an example, given a particular textual
context, both the mentions ?George W. Bush?
and ?the U.S. President.? refer to the same entity,
i.e. a particular instance of Person whose first
name is ?George?, whose middle initial is ?W.?,
whose family name is ?Bush? and whose role is
?U.S. President?.
In this paper we propose and investigate Ontol-
ogy Population from Textual Mentions (OPTM),
a sub-task of Ontology Learning and Population
                                                 
1
 The terms ?mention? and ?entity? have been intro-
duced within the ACE Program (Linguistic Data Con-
sortium, 2004). ?Mentions? are equivalent to ?refer-
ring expressions? and ?entities? are equivalent to
?referents?, as widely used in computational linguis-
tics. In this paper, we use italics for ?mentions? and
small caps for ENTITY and ENTITY_ATTRIBUTE.
(OLP) from text where we assume that mentions
for several kinds of entities (e.g. PERSON,
ORGANIZATION, LO C A T I O N , GEO-POLITICAL
_ENTITY) are already extracted from a document
collection.
We assume an ontology with a set of classes
C={c
1
, ?, c
n
} with each class c
1
 being described
by a set of attribute value pairs [a
1
, v
1
]. Given a
set of mentions M={m
1,c1
, ?,  m
n,cn
}, where each
mention m
j
 is classified into a class c
i
 in C, the
OPTM task is defined in three steps: Recognition
and Classification of Entity Attributes, Normali-
zation, and Resolution of inter-text Entity Co-
reference.
(i) Recognition and Classification of Entity
Attributes (RCEA). The textual material
expressed in a mention is extracted and dis-
tributed along the attribute-value pairs al-
ready defined for the class c
i
 of the mention;
as an example, given the PERSON mention
?U.S. President Bush?, we expect that the
attribute LAST_NAME is filled with the value
?Bush? and the attribute ROLE is filled with
the value ?U.S. President?. Note that fillers,
at this step, are still portions of text.
(ii) Normalization. The textual material ex-
tracted at step (i) is assigned to concepts and
relations already defined in the ontology; for
example, the entity BUSH is created as an in-
stance of COUNTRY_PRESIDENT, and an in-
stance of the relation PRESIDENT_OF is cre-
ated between BUSH and U.S.A. At this step
different instances are created for co-
referring mentions.
(iii) Resolution of inter-text Entity Co-
reference (REC). Each mention m
j
 has to be
assigned to a single individual entity be-
longing to a class in C . For example, we
recognize that the instances created at step
(i) for ?U.S. President Bush? and ?George
W. Bush? actually refer to the same entity.
26
In this paper we address steps (i) and (iii),
while step (ii) is work in progress. The input of
the OPTM task consists of classified mentions
and the output consists of individual entities
filled with textual material (i.e. there is no nor-
malization) with their co-reference relations. The
focus is on the definition of the task and on an
empirical analysis of the aspects that determine
its complexity, rather than on approaches and
methods for the automatic solution of OPTM.
There are several advantages of OPTM which
make it appealing for OLP. First, mentions pro-
vide an obvious simplification with respect to the
more general Ontology Population from text (cf.
Buitelaar et al 2005); in particular, mentions are
well defined and there are systems for automatic
mention recognition. Although there is no univo-
cally accepted definition for the OP task, a useful
approximation has been suggested by
(Bontcheva and Cunningham, 2005) as Ontology
Driven Information Extraction with the goal of
extracting and classifying instances of concepts
and relations defined in a Ontology, in place of
filling a template. A similar task has been ap-
proached in a variety of perspectives, including
term clustering (Lin, 1998 and Almuhareb and
Poesio, 2004) and term categorization (Avancini
et al 2003). A rather different task is Ontology
Learning, where new concepts and relations are
supposed to be acquired, with the consequence of
changing the definition of the Ontology itself
(Velardi et al 2005). However, since mentions
have been introduced as an evolution of the tra-
ditional Named Entity Recognition task (see
Tanev and Magnini, 2006), they guarantee a rea-
sonable level of difficulty, which makes OPTM
challenging both for the Computational Linguis-
tic side and the Knowledge Representation
community. Second, there already exist anno-
tated data with mentions, delivered under the
ACE (Automatic Content Extraction) initiative
(Ferro et al 2005, Linguistic Data Consortium
2004), which makes the exploitation of machine
learning based approaches possible. Finally,
having a limited scope with respect to OLP, the
OPTM task allows for a better estimation of per-
formance; in particular, it is possible to evaluate
more easily the recall of the task, i.e. the propor-
tion of information correctly assigned to an en-
tity out of the total amount of information pro-
vided by a certain mention.
In the paper we both define the OPTM task
and describe an OPTM benchmark, i.e. a docu-
ment collection annotated with mentions as well
as an ontology where information from mentions
has been manually extracted. The general archi-
tecture of the OPTM task has been sketched
above, considering three sub tasks. The docu-
ment collection we use consists of about 500
Italian news items. Currently, mentions referring
to PE R S O N , ORGANIZATION and GEO-
POLITICAL_ ENTITY have been annotated and co-
references among such mentions have been es-
tablished. As for the RCEA sub task, we have
considered mentions referring to PERSON and
have built a knowledge base of instances, each
described with a number of attribute-value pairs.
The paper is structured as follows. Section 2
provides the useful background as far as men-
tions and entities are concerned. Section 3 de-
fines the OPTM task and introduces the dataset
we have used, as well as the annotation proce-
dures and guidelines we have defined for the re-
alization of the OPTM benchmark corpus. Sec-
tion 4 reports on a number of quantitative and
qualitative analyses of the OPTM benchmark
aimed at determining the difficulty of the task.
Finally, Section 5 proposes future extensions and
developments of our work.
2 Mentions and Entities
As indicated in the ACE Entity Detection
task, the annotation of entities (e.g. PERSON,
ORGANIZAT I O N , LOCAT I O N  a n d  GEO-
POLITICAL_ENTITY) requires that the entities
mentioned in a text be detected, their syntactic
head marked, their sense disambiguated, and that
selected attributes of these entities be extracted
and merged into a unified representation for each
entity.
As it often happens that the same entity is
mentioned more than once in the same text, two
inter-connected levels of annotation have been
defined: the level of the entity, which provides a
representation of an object in the world, and the
level of the entity mention, which provides in-
formation about the textual references to that
object.  For instance, if  the entity
GEORGE_W._BUSH (e.g. the individual in the
world who is the current president of the U.S.) is
mentioned in two different sentences of a text as
?the U.S. president? and as ?the president?, these
two expressions are considered as two co-
referring entity mentions.
The kinds of reference made by entities to
something in the world are described by the fol-
lowing four classes:
? specific referential entities are those where
the entity being referred to is a unique object
27
or set of objects (e.g. ?The president of
thecompany is here?)
 2
;
? generic referential entities refer to a kind or
type of entity and not to a particular object (or
set of objects) in the world (e.g. ?The presi-
dent is elected every 5 years?);
? under-specified referential entities are non-
generic non-specific references, including im-
precise quantifications (e.g. ?everyone?) and
estimates (e.g. ?more than 10.000 people?);
? negatively quantified entities refer to the
empty set of the mentioned type of object (e.g.
?No lawyer?).
The textual extent of mentions is defined as
the entire nominal phrase used to refer to an en-
tity, thus including modifiers (e.g. ?a big fam-
ily?), prepositional phrases (e.g. ?the President of
the Republic?) and dependent clauses (e.g. ?the
girl who is working in the garden?).
The classification of entity mentions is based
on syntactic features; among the most significant
categories defined by LDD (Linguistic Data
Consortium 2004) there are:
- NAM: proper names (e.g. ?Ciampi?, ?the
UN?);
- NOM: nominal constructions (e.g. ?good chil-
dren?, ?the company?);
- PRO: pronouns, e.g. personal (?you?) and in-
definite (?someone?);
- WHQ: wh-words, such as relatives and inter-
rogatives (e.g. ?Who?s there??);
- PTV: partitive constructions (e.g. ?some of
them?, ?one of the schools?);
- APP: appositive constructions (e.g. ?Dante,
famous poet? , ?Juventus, Italian football
club?).
Since the dataset presented in this paper has
been developed for Italian, some new types of
mentions have been added to those listed in the
LDC guidelines; for instance, we have created a
specific tag, ENCLIT, to annotate the clitics
whose extension can not be identified at word-
level (e.g. ?veder[lo]?/?to see him?). Some types
of mentions, on the other hand, have been elimi-
nated; this is the case for pre-modifiers, due to
syntactic differences between English, where
both adjectives and nouns can be used as pre-
modifiers, and Italian, which only admits adjec-
tives in that position.
In extending the annotation guidelines, we
have decided to annotate all conjunctions of en-
tities, not only those which share the same modi-
fiers as indicated in the ACE guidelines, and to
mark them using a specific new tag, CONJ (e.g.
                                                 
2
 Notice that the corpus is in Italian, but we present English
examples for the sake of readability.
?mother and child?)
3
.
According to the ACE standards, each dis-
tinct person or set of people mentioned in a
document refers to an entity of type PERSON. For
example, people may be specified by name
(?John Smith?), occupation (?the butcher?),
family relation (?dad?), pronoun (?he?), etc., or
by some combination of these.
PERSON (PE), the class we have considered
for the Ontology Population from Textual Men-
tion task, is further classified with the following
subtypes:
? INDIVIDUAL_PERSON: PES which refer to a
single person (e.g. ?George W. Bush?);
? GROUP_PERSON: PES which refer to more than
one person (e.g. ?my parents?, ?your family?,
etc.);
? INDEFINITE_PERSON: a PE is classified as in-
definite when it is not possible to judge from
the context whether it refers to one or more
persons (e.g. ?I wonder who came to see me?).
3 Task definition
In Section 3.1 we first describe the document
collection we have used for the creation of the
OPTM benchmark. Then, Section 3.2 provides
details about RCEA, the first step in OPTM.
3.1 Document collection
The OPTM benchmark is built on top of a
document collection (I-CAB, Italian Content
Annotated Bank)
4
 annotated with entity men-
tions. I-CAB (Magnini et al 2006) consists of
525 news documents taken from the local news-
paper ?L?Adige?
5
. The selected news stories be-
long to four different days (September, 7th and
8th 2004 and October, 7th and 8th 2004) and are
grouped into five categories: News Stories, Cul-
tural News, Economic News, Sports News and
Local News (see Table 1).
09/07 09/08 10/07 10/08 Total
News 23 25 18 21 87
Culture 20 18 16 18 72
Economy 13 15 12 14 54
Sport 29 41 27 26 123
Local 46 43 49 51 189
TOTAL 131 142 122 130 525
Table 1: Number of news stories per category.
                                                 
3
 Appositive and conjoined mentions are complex construc-
tions. Although LDC does not identify heads for complex
constructions, we have decided to annotate all the extent as
head.
4
 A demo is available at http://ontotext.itc.it/webicab
5
 http://www.ladige.it/
28
I-CAB is further divided into training and
test sections, which contain 335 and 190 docu-
ments respectively. In total, I-CAB consists of
around 182,500 words: 113,500 and 69,000
words in the training and the test sections re-
spectively (the average length of a news story is
around 339 words in the training section and 363
words in the test section).
The annotation of I-CAB is being carried out
manually, as we intend I-CAB to become a
benchmark for various automatic Information
Extraction tasks, including recognition and nor-
malization of temporal expressions, entities, and
relations between entities (e.g. the relation af-
filiation connecting a person to the organization
to which he or she is affiliated).
3.2 Recognition and Classification
As stated in Section 1, we assume that for
each type of entity there is a set of attribute-value
pairs, which typically are used for mentioning
that entity type. The same entity may have dif-
ferent values for the same attribute and, at this
point no normalization of the data is made, so
there is no way to differentiate between different
values of the same attribute, e.g. there is no
stipulation regarding the relationship between
?politician? and ?political leader?. Finally, we
currently assume a totally flat structure among
the possible values for the attributes.
The work we describe in this Section and in
the next one concerns a pilot study on entities of
type PERSON. After an empirical investigation on
the dataset described in Section 3.1 we have as-
sumed that the attributes listed in the first column
of Table 2 constitute a proper set for this type of
entity. The second column lists some possible
values for each attribute.
The textual extent of a value is defined as the
maximal extent containing pertinent information.
For instance, if we have a person mentioned as
?the thirty-year-old sport journalist?, we will
select ?sport journalist? as value for the attribute
ACTIVITY. In fact, the age of the journalist in not
pertinent to the activity attribute and is left out,
whereas ?sport? contributes to specifying the
activity performed.
As there are always less paradigmatic values
for a given attribute, we shortly present further
the guidelines in making a decision in those
cases. Generally, articles and prepositions are not
admitted at the beginning of the textual extent of
a value, an exception being made in the case of
articles in nicknames.
Attributes Possible values
FIRST_NAME Ralph, Greg
MIDDLE_NAME J., W.
LAST_NAME McCarthy, Newton
NICKNAME Spider, Enigmista
TITLE prof., Mr.
SEX actress
ACTIVITY
AFFILIATION
ROLE
journalist, doctor
The New York Times
director, president
PROVENIENCE South American
FAMILY_RELATION father, cousin
AGE_CATEGORY boy, girl
MISCELLANEA The men with red shoes
Table 2. Attributes for PERSON.
Typical examples for the TITLE attribute are
?Mister?, ?Miss?, ?Professor?, etc. We consider
as TITLE the words which are used to address
people with special status, but which do not refer
specifically to their activity. In Italian, profes-
sions are often used to address people (e.g. ?av-
vocato/lawyer?, ?ingegnere/engineer?). In order
to avoid a possible overlapping between the
TITLE attribute and the ACTIVITY attribute, pro-
fessions are considered values for title only if
they appear in abbreviated forms (?avv.?, ?ing.?
etc.) before a proper name.
With respect to the SEX attribute, we con-
sider as values all the portions of text carrying
this information. In most cases, first and middle
names are relevant. In addition, the values of the
SEX attribute can be gendered words (e.g. ?Mis-
ter? vs. ?Mrs.?, ?husband? vs. ?wife?) and words
from grammatical categories carrying informa-
tion about gender (e.g. adjectives).
The attributes A CTIVITY, RO L E , AF -
FILIATION are three strictly connected attributes.
ACTIVITY refers to the actual activity performed
by a person, while ROLE refers to the position
they occupy. So, for instance, ?politician? is a
possible value for ACTIVITY, while ?leader of the
Labour Party? refers to a ROLE. Each group of
these three attributes is associated with a mention
and all the information within a group has to be
derived from the same mention. If different
pieces of information derive from distinct men-
tions, we will have two separate groups. Con-
sider the following three mentions of the same
entity:
29
(1) ?the journalist of Radio Liberty?
(2) ?the redactor of breaking news?
(3) ?a spare time astronomer?
These three mentions lead to three different
groups of ACTIVITY, ROLE and AFFILIATION.
The obvious inference that the first two mentions
conceptually belong to the same group is not
drawn. This step is to be taken at a further stage.
The PROVENIENCE attribute can have as
values all phrases denoting geographical/racial
origin or provenience and religious affiliation.
The attribute AGE_CATEGORY can have either
numerical values, such as ?three years old?, or
words indicating age, such as ?middle-aged?, etc.
In the next section we will analyze the occur-
rences of the values of these attributes in a news
corpus.
4 Data analysis
The difficulty of the OPTM task is directly cor-
related to four factors: (i) the extent to which the
linguistic form of mentions varies; (ii) the per-
plexity of the values of the attributes; (iii) the
size of the set of the potential co-references and
(iv) the number of different mentions per entity.
In this section we present the work we have un-
dertaken so far and the results we have obtained
regarding the above four factors.
We started with a set of 175 documents be-
longing to the I-CAB corpus (see Section 3.1).
Each document has been manually annotated
observing the specifications described in Section
3.2. We focused on mentions referring to
INDIVIDUAL PERSON (Mentions in Table 3), ex-
cluding from the dataset both mentions referring
to different entity types (e.g. ORGANIZATION)
and PERSON GROUP. In addition, for the pur-
poses of this work we decided to filter out the
following mentions: (i) mentions consisting of a
single pronoun; (ii) nested mentions, (in particu-
lar in the case where a larger mention, e.g.
?President Ciampi?, contained a smaller one, e.g.
?Ciampi?, only the larger mention was consid-
ered). The total number of remaining mentions
(Meaningful mentions in Table 3) is 2343. Fi-
nally, we filtered out repetitions of mentions (i.e.
string equal) that co-refer inside the same docu-
ment, obtaining a set of 1139 distinct mentions.
The average number of mentions for an entity
in a document is 2.09, while the mentions/entity
proportion within the whole collection is 2.68.
The detailed distribution of mentions with re-
spect to document entities is presented in Table
4. Columns 1 and 3 list the number of mentions
and columns 2 and 4 list the number of entities
which are mentioned for the respective number
of times (from 1 to 9 and more than 10). For in-
stance, in the dataset there are 741 entities which,
within a single document, have just one mention,
while there are 27 entities which are mentioned
more than 10 times in the same document. As an
indication of variability, only 14% of document
entities have been mentioned in two different
ways.
Documents 175
Words 57 033
Words in mentions 8116
Mentions 3157
Meaningful mentions 2343
Distinct mentions 1139
Document entities 1117
Collection entities 873
Table 3. Documents, mentions and entities in the
OPTM dataset.
#M/E #occ #M/E #occ
1 741 6 15
2 164 7 11
3 64 8 12
4 47 9 5
5 31 ?10 27
Table 4. Distribution of mentions per entity.
4.1 Co-reference density
We can estimate the a priori probability that two
entities selected from different documents co-
refer. Actually, this is the estimate of the prob-
ability that two entities co-refer conditioned by
the fact that they have been correctly identified
inside the documents. We can compute such
probability as the complement of the ratio be-
tween the number of different entities and the
number of the document entities in the collec-
tion.
entitiesdocument
entitiescollection
corefcrossP
?
?
?=?
#
#
1)(
From Table 3 we read these values as 873
and 1117 respectively, therefore, for this corpus,
the probability of intra-document co-reference is
approximately 0.22.
30
A cumulative factor in estimating the diffi-
culty of the co-reference task is the ratio between
the number of different entities and the number
of mentions. We call this ratio the co-reference
density and it shows the a priori expectation that
a correct identified mention refers to a new en-
tity.
mentions
entitiescollection
densitycoref
#
# ?
=?
The co-reference density takes values in the
interval with limits [0-1]. The case where the co-
reference density tends to 0 means that all the
mentions refer to the same entity, while where
the value tends to 1 it means that each mention in
the collection refers to a different entity. Both
limits render the co-reference task superfluous.
The figure for co-reference density we found in
our corpus is 873/2343 ? 0.37, and it is far from
being close to one of the extremes.
A last measure we introduce is the ratio
between the number of different entities and the
number of distinct mentions. Let?s call it pseudo
co-reference density. In fact it shows the value of
co-reference density conditioned by the fact that
one knows in advance whether two mentions that
are identical also co-refer.
mentionsdistinct
entitiescollection
densitypcoref
?
?
=?
#
#
The pseudo co-reference for our corpus is
873/1139 ? 0.76. This information is not directly
expressed in the collection, so it should be ap-
proximated. The difference between co-reference
density and pseudo co-reference density (see Ta-
ble 5) shows the increase in recall, if one consid-
ers that two identical mentions refer to the same
entity with probability 1. On the other hand, the
loss in accuracy might be too large (consider for
example the case when two different people hap-
pen to have the same first name).
co-reference density 0.37
pseudo co-reference density 0.76
cross co-reference 0.22
Table 5. A priori estimation of difficulty of co-
reference
4.2 Attribute variability
The estimation of the variability of the values for
a certain attribute is given in Table 6. The first
column indicates the attribute under considera-
tion; the second column lists the total number of
mentions of the attribute found in the corpus; the
third column lists the number of different values
that the attribute actually takes and, between pa-
rentheses, its proportion over the total number of
values; the fourth column indicates the propor-
tion of the occurrences of the attribute with re-
spect to the total number of mentions (distinct
mentions are considered).
Table 6. Variability of values for attributes.
In Table 7 we show the distribution of the at-
tributes inside one mention. That is, we calculate
how many times one entity contains more than
one attribute. Columns 1 and 3 list the number of
attributes found in a mention, and columns 2 and
4 list the number of mentions that actually con-
tain that number of values for attributes.
#attributes #mentions #attributes #mentions
1 398 5 55
2 220 6 25
3 312 7 8
4 117 8 4
Table 7. Number of attributes inside a mention.
An example of a mention from our dataset that
includes values for eight attributes is the follow-
ing:
The correspondent of Al Jazira, Amr Abdel
Hamid, an Egyptian of Russian nationality?
We conclude this section with a statistic re-
garding the coverage of attributes (miscellanea
excluded). There are 7275 words used in 1139
Attributes total
occ.
distinct
occ. (%)
occ.
prob.
FIRST_NAME 535 303 (44%) 27,0%
MIDDLE_NAME 25 25 (100%) 2,1%
LAST_NAME 772 690 (11%) 61,0%
NICKNAME 14 14 (100%) 1,2%
TITLE 12 10 (17%) 0,8%
SEX 795 573 (23%) 51,0%
ACTIVITY 145 88 (40%) 7,0%
AFFILIATION 134 121 (10%) 11,0%
ROLE 155 92 (42%) 8,0%
PROVENIENCE 120 80 (34%) 7,3%
FAMILY_REL. 17 17(100%) 1,4%
AGE_CATEGORY 31 31(100%) 2,7%
MISCELLANEA 106 106 (100%) 9,3%
31
distinct mentions, out of which 3606, approxi-
mately 49%, are included in the values of the
attributes.
5 Conclusion and future work
We have presented work in progress aiming at
a better definition of the general OLP task. In
particular we have introduced Ontology Popula-
tion from Textual Mentions (OPTM) as a simpli-
fication of OLP, where the source textual mate-
rial are already classified mentions of entities.
An analysis of the data has been conducted over
a OPTM benchmark manually built from a cor-
pus of Italian news. As a result a number of indi-
cators have been extracted that suggest the com-
plexity of the task for systems aiming at auto-
matic resolution of OPTM.
Our future work is related to the definition and
extension of the OPTM benchmark for the nor-
malization step (see Introduction). For this step it
is crucial the construction and use of a large-
scale ontology, including the concepts and rela-
tions referred by mentions. A number of inter-
esting relations between mentions and ontology
are likely to emerge.
The work presented in this paper is part of the
ONTOTEXT project, a larger initiative aimed at
developing text mining technologies to be ex-
ploited in the perspective of the Semantic Web.
The project focuses on the study and develop-
ment of innovative knowledge extraction tech-
niques for producing new or less noisy informa-
tion to be made available to the Semantic Web.
ONTOTEXT addresses three key research as-
pects: annotating documents with semantic and
relational information, providing an adequate
degree of interoperability of such relational in-
formation, and updating and extending the on-
tologies used for Semantic Web annotation. The
concrete evaluation scenario in which algorithms
will be tested with a number of large-scale ex-
periments is the automatic acquisition of infor-
mation about people from newspaper articles.
6 Acknowledgements
This work was partially funded the three-
year project ONTOTEXT
6
 funded by the Provin-
cia Autonoma di Trento. We would like to thank
Nicola Tovazzi for his contribution to the anno-
tation of the dataset.
                                                 
6
 http://tcc.itc.it/projects/ontotext
References
Almuhareb, A. and Poesio, M.. 2004. Attribute-
based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP 2004, pages
158--165, Barcelona, Spain.
Avancini, H., Lavelli, A., Magnini, B.,
Sebastiani, F., Zanoli, R. (2003). Expanding
Domain-Specific Lexicons by Term Categori-
zation. In: Proceedings of SAC 2003, 793-79.
Cunningham, H. and Bontcheva, K. Knowledge
Management and Human Language: Crossing
the Chasm. Journal of Knowledge Manage-
ment, 9(5), 2005.
Buitelaar, P., Cimiano, P. and Magnini, B. (Eds.)
Ontology Learning from Text: Methods,
Evaluation and Applications. IOS Press, 2005.
Ferro, L., Gerber, L., Mani, I., Sundheim, B. and
Wilson, G. (2005). TIDES 2005 Standard for
the Annotation of Temporal Expressions.
Technical report, MITRE.
Lavelli, A., Magnini, B., Negri, M., Pianta, E.,
Speranza, M. and Sprugnoli, R. (2005). Italian
Content Annotation Bank (I-CAB): Temporal
Expressions (V. 1.0.). Technical Report T-
0505-12. ITC-irst, Trento.
Lin, D. (1998). Automatic Retrieval and Clus-
tering of Similar Words. In: Proceedings of
COLING-ACL98, Montreal, Canada, 1998.
Linguistic Data Consortium (2004). ACE
(Automatic Content Extraction) English An-
notation Guidelines for Entities, version 5.6.1
2005.05.23.
http://projects.ldc.upenn.edu/ace/docs/English-
Entities-Guidelines_v5.6.1.pdf
Magnini, B., Pianta, E., Girardi, C., Negri, M.,
Romano, L., Speranza, M., Bartalesi Lenzi, V.
and Sprugnoli, R. (2006). I-CAB: the Italian
Content Annotation Bank. Proceedings of
LREC-2006, Genova, Italy, 22-28 May, 2006.
Tanev, H. and Magnini, B. Weakly Supervised
Approaches for Ontology Population. Pro-
ceedings of EACL-2006, Trento, Italy, 3-7
April, 2006.
Velardi, P., Navigli, R., Cuchiarelli, A., Neri, F.
(2004). Evaluation of Ontolearn, a Methodol-
ogy for Automatic Population of Domain On-
tologies. In: Buitelaar, P., Cimiano, P.,
Magnini, B. (eds.): Ontology Learning from
Text: Methods, Evaluation and Applications,
IOS Press, Amsterdam, 2005.
32
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 191?194,
Prague, June 2007. c?2007 Association for Computational Linguistics
IRST-BP: Preposition Disambiguation  
 based on  
 Chain Clarifying Relationships Contexts 
 
Octavian Popescu 
FBK-IRST, Trento (Italy) 
popescu@itc.it  
Sara Tonelli 
FBK-IRST, Trento (Italy) 
satonelli@itc.it 
Emanuele Pianta 
FBK-IRST, Trento (Italy) 
pianta@itc.it 
 
 
Abstract 
We are going to present a technique of 
preposition disambiguation based on 
sense discriminative patterns, which are 
acquired using a variant of Angluin?s al-
gorithm. They represent the essential in-
formation extracted from a particular 
type of local contexts we call Chain 
Clarifying Relationship contexts. The 
data set and the results we present are 
from the Semeval task, WSD of Preposi-
tion (Litkowski 2007). 
1 Introduction 
Word Sense Disambiguation (WSD) is a prob-
lem of finding the relevant clues in a surround-
ing context. Context is used with a wide scope in 
the NLP literature. However, there is a dichot-
omy among two types of contexts, local and 
topical contexts (Leacock et. all 1993), that is 
general enough to encompass the whole notion 
and at the same to represent a relevant distinc-
tion. 
The local context is formed by information on 
word order, distance and syntactic structure and 
it is not restricted to open-class words. A topical 
context is formed by the list of those words that 
are likely to co-occur with a particular sense of a 
word. Generally, the WSD methods have a 
marked predilection for topical context, with the 
consequence that structural clues are rarely, if 
ever, taken into account. However, it has been 
suggested (Stetina&Nagao 1997, Dekang 1997) 
that structural words, especially prepositions and 
particles, play an important role in computing 
the lexical preferences considered to be the most 
important clues for disambiguation. 
Closed class words, prepositions in particular, 
are ambiguous (Litkowski&Hargraves2006). 
Their disambiguation is essential for the correct 
processing of the meaning of a whole phrase. A 
wrong PP-attachment may render the sense of 
the whole sentence unintelligible. Consider for 
example: 
 
(1) Joe heard the gossip about you and me. 
(2) Bob rowed about his old car and his 
mother. 
 
A probabilistic context free grammar most 
likely will parse both (1) and (2) wrongly1. It 
would attach ?about? to ?to hear? in (1) and 
would consider the ?his old car and his mother? 
the object of ?about? in (2).  
The information needed for disambiguation of 
open class words is spread at all linguistics lev-
els, from lexicon to pragmatics, and can be lo-
cated within all discourse levels, from immedi-
ate collocation to paragraphs (Stevenson&Wilks 
1999). Intuitively, prepositions have a different 
behavior. Most likely, their senses are deter-
mined within the government category of their 
                                                 
1
 Indeed, Charniak?s parser, considered to be among 
the most accurate ones for English, parses wrongly 
both of them. 
191
heads. We expect the local context to play the 
most important role in the disambiguation of 
prepositions. 
We are going to present a technique of prepo-
sition disambiguation based on sense discrimina-
tive patterns, which are acquired using a variant 
of Angluin?s algorithm. These patterns represent 
the essential information extracted from a par-
ticular type of local contexts we call Chain 
Clarifying Relationship contexts. The data set 
and the results we present are from the Semeval 
task, WSD of Preposition (Litkowski 2007). 
In Section 2 we introduce the Chain Clarify-
ing Relationships, which represent particular 
types of local contexts. In Section 3 we present 
the main ideas of the Angluin algorithm. We 
show in Section 4 how it can be adapted to ac-
commodate the preposition disambiguation task. 
Section 5 is dedicated to further research. 
2 Chain Clarifying Relationships 
We think of ambiguity of natural language as a 
net - like relationship. Under certain circum-
stances, a string of words represents a unique 
collection of senses. If a different sense for one 
of these words is chosen, the result is an un-
grammatical sentence. Consider (3) below: 
 
(3) Most people do not live in a state of 
high intellectual awareness about their 
every action. 
 
Suppose one chooses the sense of ?to live? to 
be ?to populate?. Then, its complement, ?state?, 
should be synonym with location. The analysis 
crashes when ?awareness? is considered. There 
are two things we notice here: (a) the relation-
ship between ?live? and ?state? ? the only two 
acceptable sense combination out of four are 
(populate, location) and (experience, entity) ? 
and (b) the chain like relationship between 
?awareness?, ?state?, ?live? where the sense of 
any of them determines the sense of all the oth-
ers in a cascade effect, or results in ungrammati-
cality. A third thing, not directly observable in 
(3) is that the syntactic configuration is crucial in 
order for (a) and (b) to arise. Example (4) shows 
that in a different syntactic configuration the 
above sense relationship simply disappears: 
 
(4) The awareness of people about the state insti-
tutions is arguably the first condition to live 
in a democratic state. 
 
We call the relationship between ?live?, 
?state?, ?awareness? a Chain Clarifying Rela-
tionship (CCR). In that specific syntactic con-
figuration their senses are interdependent and 
independent of the rest of the sentence. To each 
CCR corresponds a sense discriminative pattern. 
Our goal is to learn which local contexts are 
CCRs. Each CCR is a pattern of words on a syn-
tactic configuration. Each slot can be filled only 
by words defined by certain lexical features. To 
learn a CCR means to discover the syntactic 
configuration and the respective features. For 
example consider (5) and (6) with their CCRs in 
(CCR5) and (CCR6) respectively:  
 
(5) Some people lived in the same state of 
disappointment/ optimism/ happiness. 
 (CCR5) (vb=live_sense_2, prep1=in_1, 
prep1_obj=state_sense_1,prep2=of_sense_1
a,prep2_obj=[State_of_Spirit])  
 (6) Some people lived in the same state of 
Africa/ Latin America/ Asia. 
(CCR6) (vb=live_sense_1, prep1=in_1, 
prep1_obj=state_sense_1,prep2=of_1b,prep
2_obj = [Location]) 
 
The lexical features of the open class words in 
a specific syntactic configuration trigger the 
senses of each word, if the context is a CCR. In 
(CCR5) any word that has the same lexical trait 
as the one required by prep2_obj slot will deter-
mine a unique sense for all the other words, in-
cluding the preposition. The same holds for 
(CCR6). The difference between (CCR5) and 
(CCR6) is part of the linguistic knowledge 
(which can be clearly shown: ?how? (5) vs. 
?where? (6)). 
The CCR approach proposes a deterministic 
approach to WSD. There are two features of 
CCRs which are interesting from a strictly prac-
tical point of view. Firstly, CCR proposal is a 
way to determine the size of the window where 
the disambiguation clues are searched for (many 
WSD algorithms arbitrarily set it apriori). Sec-
ondly, within a CCR, by construction, the sense 
of one word determines the senses of all the oth-
ers. 
192
3 Angluin Learning Algorithm  
Our working hypothesis is that we can learn the 
CCRs contexts by inferring differences via a 
regular language learning algorithm. What we 
want to learn is which features fulfil each syn-
tactic slot. First we introduce the original An-
gluin?s algorithm and then we mention a variant 
of it admitting unspecified values.  
Angluin proved that a regular set can be 
learned in polynomial time by assuming the ex-
istence of an oracle which can gives ?yes/no? 
answers and counterexamples to two types of 
queries: membership queries and conjecture que-
ries (queries about the form of the regular lan-
guage) (Angluin 1998). 
The algorithm employs an observation table 
built on prefix /suffix closed classes. To each 
word a {1, 0} value is associated, ?1? meaning 
that the word belongs to the target regular lan-
guage. Initially the table is empty and is filled 
incrementally. The table is closed if all prefixes 
of the already seen examples are in the table and 
is consistent if two rows dominated by the same 
prefix have the same value, ?0? or ?1?. 
If the table is not consistent or closed then a 
set of membership queries is made. If the table is 
consistent and closed then a conjecture query is 
made. If the oracle responds ?no?, it has to pro-
vide a counterexample and the previous steps are 
cycled till ?yes? is obtained. 
The role of the oracle for conjecture questions 
can be substituted by a stochastic process. If 
strict equality is not requested, then a probably 
approximately correct identification of language 
can be obtained (PAC identification), which 
guarantees that the two languages (the identified 
one, Li, and the target one, Lt) are equal up to a 
certain extent. The approximation is constrained 
by two parameters ? ? accuracy and ? ? confi-
dence, and the constraint is P(d(Li, Lt) ? ?) ? ?), 
where the distance between two languages is the 
probability to see a word in just one of them. 
The algorithm can be further generalized to 
work with unspecified values. The examples 
may have three values (?yes?, ?no?, ???), as in 
many domains one has to deal with partial 
knowledge The main result is that a variant of 
the above algorithm successfully halts if the 
number of counterexamples provided by the ora-
cle have O(log n) missing attributes, where n is 
the number of attributes (Goldmann et al 2003). 
4 Preposition Disambiguation Task 
The CCR extraction algorithm is supervised. 
Consider that you have a sense annotated cor-
pora. Extract the dependency paths and filter out 
the ones which are not sense discriminative. Try 
to generalize each slot and retain the minimal 
ones. What is left are CCRs. 
Unfortunately, for the preposition disam-
biguation task the training set is sense annotated 
only for prepositions. We have undertaken a dif-
ferent strategy. The training corpus can be used 
as an oracle. The main idea is to start with a set 
of few examples for each sense from the training 
set which are considered to be the most repre-
sentative ones. We try to generalize each of 
them independently and to tackle down the bor-
der cases (the cases that may correspond to two 
different senses) which are considered unspeci-
fied examples. The process stops when the ora-
cle does not bring any new information (the 
training cases have been learned). Below we 
explain this process step by step. 
Step 1. Get the seed examples. For each 
preposition and sense get the seed examples. 
This operation is performed by a human expert. 
It may be the case that the glosses or the diction-
ary definition are a good starting point (with the 
advantage that the intervention of a human is no 
more required). However, we preferred do to it 
manually for better precision. 
Besides the most frequent sense, we have con-
sidered, in average, another two senses. There is 
a practical reason for this limitation: the number 
of examples for the rest of the senses is insuffi-
cient. In total we have considered 149 senses out 
of the 241 senses present in the training set. For 
each an average of three examples has been cho-
sen. 
Step 2. Get the CCRs. For each example we 
read the lex units associated with its frame from 
FrameNet. Our goal is to identify the relevant 
syntactic and lexical features associated with 
each slot. We have undertaken two simplifying 
assumptions. Firstly, only the government cate-
gory of the head of the PP is considered (which 
can be a verb, a noun or an adjective). Secondly, 
193
the lexical features are identified with synsets 
from WordNet.  
We have used the Charniak?s parser to extract 
the structure of the PP-phrases and further we 
have used Collin?s algorithm to implement a 
head recogniser. 
A head can have many synsets. In order to 
understand which sense the word has in the re-
spective construction we look for the synset 
common to the elements extracted from lex. If 
the proposed synset uniquely identifies just one 
sense then it is considered a CCR. If not, we are 
looking for the next synset. This step corre-
sponds to membership queries in Angluin?s al-
gorithm. 
Step 3. Generalize the CCRs. At the end of 
step 2 we have a set of CCRs for each sense. We 
obtained 395 initial CCRs. We tried to extend 
the coverage by taking into account the hypero-
nyms of each synsets. Only approximately 10% 
of these new patterns have received an answer 
from the oracle. Consequently, for our ap-
proach ,a part of the training corpus has not been 
used. It serves only 15 examples in average to 
get a correct CCR. All the instances of the same 
CCR do not bring any new information to our 
approach. 
Posteriori, we have noticed that the initial pat-
terns have an almost 50% (48.57%) coverage in 
the test data. The generalized patterns obtained 
after the third step have 82% test corpus cover-
age. For the rest 18%, which are totally un-
known cases, we have chosen the most frequent 
sense. 
In table 1 we present the performances of our 
system. It achieves 0.65 (FF-score), which com-
pares favourably against baseline ? the most fre-
quent -of 0.53. On the first column of Table 1 
we write the FF score interval - more than 0.75, 
between 0.75 and 0.5, and less than 0.5 respec-
tively, - on the second column we present the 
number of cases within that interval the system 
solved and on the third column we include the 
corresponding number for baseline. 
Table 1 
 
Interval System Baseline 
1.00 - 0.75 18 8 
0.75 - 0.50 15 6 
0.00 ? 0.50 2 20 
5 Conclusion and Further Research 
Our system did not perform very well (third po-
sition out of three). Analyzing the errors, we 
have noticed that our system systematically con-
found two senses in some cases (for example 
?by? 5(2) vs. 15(3), for ?on? 4(1c) vs. 1(1) etc.). 
We would like to see whether these errors are 
due to a misclassification in training. 
 
References 
 
Angluin, D. (1987): ?Learning Regular Sets 
from Queries and Counterexamples?, Infor-
mation and Computation Volume 75 ,  Issue 2 
Goldman, S., Kwek, S., Scott, S. (2003): ?Learn-
ing from examples with unspecified attribute 
values?, Information and Computation, Vol-
ume 180 
Leacock, C., Towell, G., Voorhes, E. (1993): 
?Towards Building Contextual Representa-
tions of Word Senses Using Statistical Mod-
els?, In Proceedings, SIGLEX workshop: Ac-
quisition of Lexical Knowledge from Text 
Lin, D. (1997): ?Using syntactic dependency as 
local context to resolve word sense ambigu-
ity?.ACL/EACL-97,  Madrid 
Litkowski, K. C. (2007):?Word Sense Disam-
biguation of Prepositions? , The Semeval 
2007 WePS Track. In Proceedings of Semeval 
2007, ACL 
Litkowski, K. C., Hargraves O. (2006): ?Cover-
age and Inheritance in the Preposition Project", 
Proceedings of the Third ACL-SIGSEM 
Workshop on Prepositions, Trento, 
Stetina J, Nagao M (1997): ?Corpus based PP 
attachment ambiguity resolution with a se-
mantic dictionary.?, Proc. of the 5th Work-
shop on very large corpora, Beijing and 
Hongkong, pp 66-80 
Stevenson K., Wilks, Y.,(2001): ?The interaction 
of knowledge sources in word sense disam-
biguation?, Computational Linguistics, 
27(3):321?349. 
 
194
Coling 2010: Poster Volume, pages 988?996,
Beijing, August 2010
Dynamic Parameters for Cross Document Coreferece 
 
 
Octavian Popescu 
papsi@racai.ro 
Racai, Romanina Academy 
 
 
 
 
  
 
Abstract 
 
In this paper we present a new algorithm for 
the Person Cross Document Coreference task. 
We show that accurate results require a way to 
adapt the parameters of the similarity function 
? metrics and threshold ? to the ontological 
constraints obeyed by individuals. The tech-
nique we propose dynamically changes the ini-
tial weights computed when the context is ana-
lyzed. The weight recomputation is necessary 
in order to resolve clusters borders, which are 
inevitably blurred by a static approach. The re-
sults show a significant gain in accuracy. 
1 Introduction 
The Person Cross Document Coreference, CDC, 
task requires that all the personal name mentions, 
PNMs, in a corpus be clustered together accord-
ing to the individuals they refer to (Grishman 
1994). The coreference between two PNMs is 
decided on the basis of the local contexts. In this 
paper we consider a news corpus, and the local 
context is the piece of news to which a particular 
PNM belongs. We work on a seven year Italian 
local newspaper corpus, Adige 500K (Magnini 
et. al. 2006). 
While there are certain similarities between a 
disambiguation task and the CDC task, we main-
tain that there is a significant difference which 
sets the CDC task apart. Unlike in other disam-
biguation tasks, in the CDC tasks the relevant 
coreference context depends on the corpus itself. 
In word sense disambiguation, for instance, the 
distribution of the relevant context is mainly re-
gulated by strong syntactic and semantic rules. 
The existence of such rules allows for disambig-
uation decisions which are made by considering 
the local context only. On the other hand, the 
distribution of the PNMs in a corpus is rather 
random and the relevant coreference context is a 
dynamic variable which depends on the diversity 
of the corpus, that is, on how many different per-
sons with the same name share a similar context. 
Unlike the word senses which are subject to 
strong linguistic constraints, the name distribu-
tion is more or less random. To exemplify, con-
sider the name ?John Smith? and an organiza-
tion, say ?U.N.?.  The extent to which ?works for 
U.N.? in ?John Smith works for U.N.? is a rele-
vant coreference context depends on the diversity 
of the corpus itself. If in that corpus, among all 
the ?John Smiths? there is only one person who 
works for ?U.N.? then ?works for U.N.? is a re-
levant coreference context, but if there are many 
?John Smiths? working for U.N., then ?works for 
U.N.? is not a relevant coreference system. 
In this paper we present a method to exactly 
determine the relevance of a piece of context for 
the coreference. As above, the exactness is un-
derstood in relationship with the whole system of 
clusters. The relevance of a piece of context is 
computed by means of a weighting procedure. 
The classic weighting procedures are static, each 
piece of context receives an initial value that is 
also a final one and the clustering proceeds on 
the basis of these values. We demonstrate that 
this approach has serious drawbacks and we ar-
gue that in order to obtain accurate results, a dy-
namic weighting procedure is necessary, which 
outputs new values depending on the cluster con-
figuration. 
In Section 2 we review the relevant literature. 
In Section 3 we present the problems related to 
the classical approach to the CDC task and we 
present evidence that the data distribution in a 
news corpus requires a proper treatment of these 
988
problems. In Section 4 we present the technique 
that permits to overcome the problems identified 
in Section 3. In Section 5 we present the context 
extraction technique that supports the method 
developed in Section 4. In Section 6 we present 
the results of an evaluation experiment. The pa-
per ends with Conclusion and Further Work sec-
tion. 
2 Related Work 
In a classical paper (Bagga and Baldwin 1998), a 
PCDC system based on the vector space model 
(VSM) is proposed. While there are many advan-
tages in representing the context as vectors on 
which a similarity function is applied, it has been 
shown that there are inherent limitations asso-
ciated with the vectorial model (Popescu 2008). 
These problems, related to the density in the vec-
torial space (superposition) and to the discri-
minative power of the similarity power (mask-
ing), become visible as more cases are consi-
dered.  
Testing the system on many names, (Gooi and 
Allan, 2004), it has been noted empirically that 
the accuracy of the results varies significantly 
from name to name. Indeed, by considering just 
the sentence level context, which is a strong re-
quirement for establishing coreference, a PCDC 
system obtains a good score for ?John Smith?. 
This happens because the prior probability of 
coreference of any two ?John Smiths? mentions 
is low, as this is a very common name and none 
of the ?John Smiths? has an overwhelming num-
ber of mentions. But for other types of names the 
same system is not accurate. If it considers, for 
instance, ?Barack Obama?, the same system ob-
tains a very low recall, as the probability of any 
two ?Barack Obama? mentions to corefer is very 
high and the relevant coreference context is very 
often found beyond the sentence level. Without 
further adjustments, a vectorial model cannot 
resolve the problem of considering too much or 
too little contextual evidence in order to obtain a 
good precision for ?John Smith? and simulta-
neously a good recall for ?Barack Obama?. 
These types of name have different cluster sys-
tems 
In an experiment using bigrams (Pederson et 
al. 2005) on a news corpus, it has been observed 
that the relationship between the amount of in-
formation given to a CDC system and the per-
formances is not linear. If the system has re-
ceived in input the correct number of persons 
with the same name, the accuracy of the system 
has dropped. A typical case for this situation is 
when there is a person that is very often men-
tioned, and few other persons that have few men-
tions. When the number of clusters is passed in 
the input, the clusters representing the persons 
who are rarely mentioned are wrongly enriched. 
However, this situation can be avoided if there is 
a measure of how big the threshold should be. 
The system of clusters is not developed unrealis-
tically if we are able to handle the fact that indi-
viduals obey different constraints which are de-
rived directly from the ontological properties. 
These constraints are determined directly from 
the context and adequate weights can be set.  
Recently, there has been a major interest in the 
CDC systems, and, in the last two years, two im-
portant evaluation campaigns have been orga-
nized: Web People Search-1 (Artiles et al 2007)  
and ACE 2008 (www.nist.gov/speech/tests/ace/). 
It has been noted that the data variance between 
training and test is very high (Lefever 2007). Ra-
ther than being a particularity of those corpora, 
the problem is general. The performances of a 
bag of words VSM depends to a very high extent 
on the corpus diversity (see Section 3.2). For re-
liable results, a CDC system must have access to 
global information regarding the coreference 
space. 
Rich biographic facts have been shown to im-
prove the accuracy of CDC (Mann and Ya-
rowsky 2003). Indeed, when available, the birth 
date, the occupation etc. represent a relevant co-
reference context because the probability that 
two different persons have the same name, the 
same birth date and the same occupation is neg-
ligible. However, it is equally unlikely to find 
this information in a news corpus a sufficient 
number of times. Even for a web corpus, where 
the amount of this kind of information is higher 
than in a news corpus, the extended biographic 
facts, including e-mail address, phones, etc., con-
tribute only with approximately 3% to the total 
number of coreferences (Elmacioglu et al 2007).  
In order to improve the performances of the CDC 
systems based on VSM, the special importance 
of pieces of context has been exploited by im-
plementing a cascade clustering technique (Wei 
2006). Other authors have relied on advanced 
clustering techniques (among others Han et al 
2005, Chen 2006). However, these techniques 
rely on the precise analysis of the context, which 
is a time consuming process. It has been also 
noted that, in spite of deep analysis, the relevant 
coreference context is hard to find (Vu 2007). 
 
989
3 Coreference Based on Association Sets 
The coreference of two PNMs is realized on the 
basis of the context. In a news corpus, the con-
text surrounding each PNM, which is relevant for 
coreference, is extracted into a set, called associ-
ation set. In Table 1 we present an example of 
association sets related to the same name.  
Name Associated Sets 
 
Paolo Rossi 
TV, comedian, , satire 
research, conference  
politics, meeting 
Table 1: Associated Sets 
A weighting schema, a global metrics and 
threshold are set, and the distance between two 
association sets is computed. The decision of 
coreferencing two PNMs is made on comparing 
the distance to the threshold and clustering the 
PNMs representing the same individual into a 
unique cluster. The accuracy of a CDC system 
based on association sets depends on two factors: 
(1) the ability to extract the relevant elements for 
the association sets from the news context and 
(2) the adequacy of the similarity formula - me-
trics and threshold. 
Regarding the first factor, the ability to extract 
the relevant pieces of context, the right heuristics 
must be found, because the exact syntax-
semantics analysis of text is unfortunately very 
hard or impossible to implement. A strong limi-
tation comes from the fact that even a shallow 
parsing requires too much time in order to be 
practical. However, it has been shown that accu-
rate parings of PNMs and co-occurring special 
words can be found by employing relaxed extrac-
tion techniques (Buitelaar&Magnini 2005). The 
association sets built in this way are effective in 
solving the CDC task (Sekine 2008, Popescu 
2008). We make use of these findings in order to 
build the association sets, which mainly include 
named entities and certain special words, which 
are bound to an ontology. The details of these 
particular association sets are given in Section 5. 
As straightforward as the classical approach 
based on the distance between association sets 
may seem, there are actually a series of problems 
related to the second requirement, namely the 
adequacy of similarity formula. We make these 
problems explicit below. 
3.1 Masking, Superposition and Border 
Proximity  
In order to introduce the first problem we start 
with an intuitive example. Suppose that we want 
to individuate the persons with the name Michael 
Jackson in a news corpus. A simplistic solution is 
to cluster together all such PNMs and declare 
that than there is just one person mentioned in 
the whole corpus with this name. This solution 
has the advantage of being very simple and of 
obtaining a very high score in terms of precision 
and recall. This is because most of such PNMs 
refer to only one person indeed ? the pop star. 
However, the above method fails short when it 
comes to presenting the evidence for its corefe-
rence decision. Actually, it turns out that this is a 
very hard task, because the number of PNMs, 
which do not refer to the pop star, is extremely 
small. Thus, the prior chances of correctly find-
ing two PNMs which do not refer to this person 
are quite small. Unfortunately, the classical me-
trics are too coarse to capture the difference in 
such cases, even if the association sets are 100% 
correct. To support this statement, let us consider 
three classes under the same name, with each 
class corresponding to a different individual. Let 
us further suppose that two classes contain the 
great majority of the PNMs, and the third class 
only has a small number of PNMs. A linear deci-
sion is likely to confound the elements of the 
third class to the ones of the first two1. This hap-
pens because the elements of the third class are 
transparent to the hyper plane that separates the 
two well-represented classes. This situation is 
called masking, and is a direct effect of applying 
an inaccurate weighting schema and metrics 
(Hastie&Tibshirani 2001). The effects of mask-
ing on the CDC task have been empirically no-
ticed in (Pederson 2005). The main obstacle in 
dealing with masking is the correct treatment of 
the border elements. ?ij, the discrimant function 
between two classes, i and  j respectively, must 
assign zero to all border elements. In Section 4, 
we directly address this problem. 
The second problem that needs to be solved by 
the CDC systems based on associated sets may 
be regarded as the negative effect of counter ba-
lancing the sparseness problem. In general, the 
association sets  are too sparse to permit pair to 
pair comparison. Rather, the information must be 
interpolated from a set of corefered association 
sets. For example, in Figure 1, any two associa-
tion sets chosen from the three ones on the left, 
AS1, AS2 and AS3 respectively, are similar 
                                               
1
 In fact any decision functions that can be bijectively 
transformed into a linear function, like most exponen-
tial kernel functions for example, are similarly prone 
to masking.  
990
enough to one another to corefer. However, none 
of these association sets is similar enough to the 
one on the right ? AS4. But accepting the corefe-
rence of any initial pair, in this particular case, 
we implicitly accept the coreference with the 
fourth one. 
Figure 1. Interpolating 
By interpolating the information in the set of 
the initial three association sets, the coreference 
becomes possible between all four association 
sets. In general, by interpolating from a set of the 
association sets, one wants to find the right core-
ferences and to avoid the false ones accurately. 
In a vector space, the interpolation is safe if the 
initial vectors are orthogonal to each other, be-
cause the sum of orthogonal vectors is also or-
thogonal to any other vector that is not part of the 
sum. Therefore the right coreferences have a big 
dot product with the sum, while the false ones 
have a dot product with the sum close to zero. 
This property of the sum of the orthogonal vec-
tors is called superposition (Gallant 1993). By 
representing the association sets as vectors, 
where each set of vectors is associated exclusive-
ly with a certain individual, the sum of these vec-
tors has the superposition property. 
However, if the vectors representing the asso-
ciation sets are not orthogonal, then the interpo-
lated vectors are prone to false coreferences. In 
this case, the accidental coincidences ? which are 
responsible for the original vectors not being or-
thogonal ? biases the dot product and introduces 
false coreferences. Consequently the superposi-
tion affects negatively the overall accuracy. The 
aggravating effect of superposition in conjunc-
tion with an agglomerative clustering procedure 
has been empirically noted in Gooi&Allan.  
The third problem is directly related to the fact 
that in the most ambiguous cases the association 
sets lead to high-dimensional, very sparse vec-
tors. The basic fact is that inside a cluster of cor-
rectly corefered PNMs that refer to the same in-
dividual, the distance from most of these PNMS 
to the center of the cluster is smaller than the dis-
tance from these PNMs to the border. Let us con-
sider that all the m PNMs representing the same 
individual are points in an n dimensional vector 
space and their cluster is normalized to the unit 
sphere. The distance from the center of the 
sphere to the closest point is an exponentially 
growing formula both in 1/n and 1/m. Even for 
small values, the distance from the center to the 
closest point is larger than ?. The points 
representing the PNMs in the same cluster are 
closer to the border, and not to the center of the 
sphere. This is a secondary effect of the curse of 
dimensionality problem in the vector space2. 
3.2 Data Distribution 
Let us consider the corpus, focusing on the dis-
tribution of PNMs. Many PNMs are the mentions 
of the same name, considered as a string. We are 
interested in the frequency with which a certain 
name appears. We have noticed that there is a 
strict relationship between the names, their fre-
quencies and the number of mentions; see Table 
2. 
Freq  PNM # PNM
1 317,245 317,245
2 ? 5 166,029 467,560
6 ? 20 61,570 634,309
21 ? 100 25,651 1,090,836
101 ? 1000 7,750 2,053,994
1001 ? 2000 4,25 569,627
2001 ? 4000 157 422,585
4001 ? 5000 17 73,860
5001 ? 31091 22 190,373
Table 2 Frequency of Names and PNMs in Adige500k 
The names have a very unbalanced distribu-
tion. A name which has a frequency over 20 and 
is ambiguous represents a difficult case. The 
measure we use in order to evaluate the difficulty 
is the Gini?s mean difference. Let X1, X2, ?, Xn 
be the individuals that are named with the same 
name and let S be the set of the PNMs of this 
name PNMS, S1, S2, ? Sn. The Gini?s mean dif-
ference is a measure of the spread of the informa-
tion in the set S: 





	 
 
	


  
 
(1) 
The uniform distribution makes Gini?s factor 
null. A value of this factor close to 1 shows a 
skewed distribution. In the first case, G ? 0, the 
superposition effect is likely to be responsible for 
false coreferences, while in the latter case, G ? 1, 
                                               
2
 The curse of dimensionality refers to the fact that the 
number of sample points required to state confident 
values for a statistics grows exponentially with the 
dimension of vector space. 
991
the masking effect is predominant. However, 
there is a close relationship between all the three 
problems above. As the most ambiguous cases 
are near the border, it is likely that the vectors are 
not orthogonal and consequently the false corefe-
rences are introduced in the system, which ulti-
mately leads to masking. 
4 Resolving the Border Condition 
We are going to present a technique developed to 
deal with the problems identified in the previous 
section. The bottom line is that the weights and 
the threshold required by the similarity function 
of two association sets should be dynamically 
computed. In this way the border between any 
pair of clusters can be accurately set.   
We present the procedure of adjusting the 
weights and the threshold for a given group of 
clusters in order to maximize the probability of 
the correct coreferences. The first step is to 
present the construction of the association sets, 
with initial weight values. The second step is to 
show how these initial weight values are recom-
puted for a set of given clusters.  
Initialization  
As mentioned in the first paragraph of Section 
3, the association sets are built out of the sur-
rounding context by considering the named enti-
ties, and special words. The named entities are 
clearly marked in the input, the corpus having 
being tagged by a Named Entities Recognition 
tool. The words considered special are identified 
using an ontology and the procedure is given in 
Section 5. The construction of the association set 
is a search procedure starting from the PNM. The 
first search space is the longest nominal group 
which is headed  by a PNM: 
uno dei falchi dell' amministrazione di Stati Uniti 
guidata dal presidente George W.Bush 
one of the falcons of the U.S. administration lead 
by the president Georg W. Bush 
All the special words that are present in this no-
minal group are included in the association set of 
this PNM. In this example, these special words 
are ?president? and ?administration? respective-
ly. The named entity ?U.S.? is also included. 
These elements receive the highest weights. The 
search space is extended to the sentence level 
and new named entities/special words are in-
cluded. However, unlike in the first phase, the 
weight of these words is determined on the basis 
of a second parameter, namely the number of 
different names interfering between the PNM 
and these words. We take into consideration 
three values 0, 1 and 2 or more. After the sen-
tence, the next search domain is the whole news. 
Basically, the significance of an element de-
creases linearly with the distance and the number 
of other interfering PNMs. In Table 3 we present 
the linear kernel weighting schema described 
above. The series ?ij is decreasing linearly over 
both indexes. 
 
 Interfering PNMs 
Domain 0 1 ?2 
PNM Group ?11 ?12 ?13 
IN Sentence ?21 ?22 ?23 
Out Sentence ?31 ?32 ?33 
Table 3. Linear Kernel for Initial Weights 
Recomputation 
The association set is basically a pair of two 
vectors: X = (x1, ?, xn) the set of words and W = 
(w1, ?, wn) the set of the initial weights. Two 
PNMs corefer or not depending on whether the 
sum of their common part is bigger, respectively 
lesser than a threshold. 
  	 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 997?1006,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Person Cross Document Coreference with Name Perplexity Estimates 
  Octavian Popescu popescu@racai.ro  .      Abstract 
The Person Cross Document Coreference sys-tems depend on the context for making deci-sions on the possible coreferences between person name mentions. The amount of context required is a parameter that varies from cor-pora to corpora, which makes it difficult for usual disambiguation methods. In this paper we show that the amount of context required can be dynamically controlled on the basis of the prior probabilities of coreference and we present a new statistical model for the compu-tation of these probabilities. The experiment we carried on a news corpus proves that the prior probabilities of coreference are an impor-tant factor for maintaining a good balance be-tween precision and recall for cross document coreference systems. 1 Introduction The Person Cross Document Coreference (Grishman 1994) task, which requires that all and only the textual mentions of an entity of type Person be individuated in a large collection of text documents, is one of the challenging tasks for natural language processing systems. In the most general case the corpus itself is the only available source of information regarding the persons mentioned and we consider that this is the case in this paper. A PCDC system must be able to use the information existing in the corpus in order to assign to each personal name mention (PNM) a piece of context. The coreference of any two PNMs is decided mainly on the basis of the similarity of the pieces of contexts associated with them. A successful PCDC must accurately extract the relevant context for coreference. However, the context relevance is not abso-lute. Whether the contextual information uniquely individuates a person is a matter of 
probability. This paper presents a statistical tech-nique developed to provide a PCDC system with more information regarding the probability of a correct coreference. The reason for developing this technique is twofold: (i) the relevant corefer-ence context depends on the corpus itself and (ii) valid coreferences require a large amount of in-formation, which is unavailable in the majority of cases. The first reason is linked to a particularity of the CDC task that makes it more complex than other NLP tasks. Unlike in other disambiguation tasks, in the CDC tasks the relevant coreference context depends on the corpus itself. In word sense disambiguation, for instance, the distribu-tion of the relevant context is mainly regulated by strong syntactic and semantic rules. The exis-tence of such rules makes it possible for the dis-ambiguation decisions to be made considering the local context. On the other hand, the distribu-tion of the PNMs in a corpus is rather random and the relevant coreference context is a dynamic variable depending on the diversity of the corpus, that is, on how many different persons with the same name share a similar context. To exem-plify, consider the name ?John Smith? and an organization, say ?U.N.?.  The extent to which ?works for U.N.? in ?John Smith works for U.N.? is a relevant coreference context depends on the diversity of the corpus itself. If in that corpus, among all the ?John Smiths? there is only one person who works for ?U.N.? then ?works for U.N.? is a relevant coreference con-text, but if there are many ?John Smiths? work-ing for U.N., then ?works for U.N.? is not a rele-vant coreference system; in this last case, more contextual evidence is needed in order to cor-rectly corefer the ?John Smith? PNMs. The rele-vance of a context for coreference also depends on the corpus, not only on the specific relation-ship that exists between ?John Smith? and 
997
?works for U.N.?. Thus, A PCDC system must have access to global information regarding the PNMs. The second reason comes from practical con-siderations. The amount of information required to correctly infer PNMs coreferences is not pre-sent in corpus in a computationally friendly way. In many cases the relevant coreference informa-tion is embedded in semantic and ontological deep inferences, which are difficult to program In as much as 60% of the cases, two documents containing the same name, from a news corpus, lack contexts which are directly similar and big enough to correctly decide on the coreference.  We propose a new method to control the amount of contextual coreference required for correct coreferences. Rather than having fixed rules deciding on the size of the context sur-rounding a PNM, we propose a probabilistic ap-proach that requires contextual evidence for coreference differentially, by considering the prior probability of the coreference of two PNMs; the higher this probability is, the less their correct coreference depends on the context and vice versa. We present a statistical model where the prior coreference probabilities are computed considering only the corpus itself, and we show how these probabilities are used by a PCDC system that dynamically revises the amount of context relevant for coreference.  In Section 2 we review the CDC relevant lit-erature. In section 3 we analyze the data from annotated coreference corpora and we individu-ate a specific problem, setting up a working hy-pothesis. In Section 4 we develop a statistical model for computing the prior coreference prob-abilities and in Section 5 we present the results obtained by applying it to a large news corpus. In section 6 a direct evaluation on CDC is carried on a test corpus. In Section 7 we show how the proposed techniques extends naturally to a strat-egy of construction relevant test corpora for CDC task. The paper ends with the Conclusion and the Future Research section.  2 Related Work  In a classical paper (Bagga and Baldwin 1998), a PCDC system based on the vector space model (VSM) is proposed. While there are many advan-tages in representing the context as vectors on which a similarity function is applied, it has been shown that there are inherent limitations associ-ated with the vectorial model (Popescu 2008). These problems, related to the density in the vec-
torial space (superposition) and to the discrimi-native power of the similarity power (masking), become visible as more cases are considered.  Testing the system on many names, (Gooi and Allan, 2004), it has been noted empirically that the accuracy of the results varies significantly from name to name. Indeed, considering just the sentence level context, which is a strong re-quirement for establishing coreference, a PCDC system obtains a good score for ?John Smith?. This happens because the prior probability of coreference of any two ?John Smiths? mentions is low, as this is a very common name and none of the ?John Smith? has an overwhelming num-ber of mentions. But for other types of names the same system is not accurate. If it considers, for instance, ?Barack Obama?, the same system ob-tains a very low recall, as the probability of any two ?Barack Obama? mentions to corefer is very high and the relevant coreference context is found very often beyond the sentence level. Without further adjustments, a vectorial model cannot resolve the problem of considering too much or too little contextual evidence in order to obtain a good precision for ?John Smith? and simultaneously a good recall for ?Barack Obama?.  In an experiment using bigrams (Pederson et al 2005) on a news corpus, it has been observed that the relationship between the amount of in-formation given to a PCDC system and the per-formances is not linear. If the system has re-ceived in input the correct number of persons with the same name, the accuracy of the system has dropped. A typical case for this situation is when there is a person that is very often men-tioned, and few other persons having few men-tions; when the number of clusters is passed in the input, the clusters representing the persons who are rarely mentioned are wrongly enriched. However, this situation can be avoided if there is a measure of how probable it is to have a certain number of different persons with the same name, each being mentioned very often in a newspaper. Recently, there has been a major interest in the PCDC systems, and, in the last two years, three important evaluation campaigns have been orga-nized: Web People Search-1 (Artiles et al 2007), ACE 2008 (www.nist.gov/speech/tests/ace/). It has been noted that the data variance between training and test is very high (Lefever 2007). Rather than being a particularity of those cor-pora, the problem is general. The performances of a bag of words VSM depends to a very high extent on the corpus diversity (see Section 3). 
998
For reliable results, a PCDC system must have access to global information regarding the coreference space. Rich biographic facts have been shown to im-prove the accuracy of PCDC (Mann and Yarowsky 2003). Indeed, when available, the birth date, the occupation etc. represent a rele-vant coreference context because the probability that two different persons have the same name, the same birth date and the same occupation is negligible. However, it is equally unlikely to find this information in a news corpus a sufficient number of times. Even for a web corpus, where the amount of this kind of information is higher than in a news corpus, the extended biographic facts, including e-mail address, phones, etc., con-tribute only with approximately 3% to the total number of coreferences (Elmacioglu et al 2007).  In order to improve the performances of the PCDC systems based on VSM, some authors have focused on methods that allow a better analysis of the context by extracting the depend-ency chains (Ng 2007).  The special importance of pieces of context has been exploited by im-plementing a cascade clustering technique (Wei 2006). Other authors have relied on advanced clustering techniques (among others Han et al 2005, Chen 2006). However, these techniques rely on the precise analysis of the context, which is a time consuming process. It has been also noted that, in spite of deep analysis, the relevant coreference context is hard to find (Vu 2007).  The technique we present in the next sections is complementary to these approaches. We pro-pose a statistical model designed to offer to the PCDC systems information regarding the distri-bution of PNMs in the corpus. This information is used to reduce the contextual data variation and to attain a good balance between precision and recall.  3 Data Analysis In this Section we present the data analysis of the PNMs. We are interested in establishing a rela-tionship between the distribution of the PNMs and the relevant context for coreference. As men-tioned in the preceding sections, the amount of the relevant context for coreference cannot be decided prior to the investigation of that particu-lar corpus. The performances of a bag of words VSM with a prior defined context approach will vary greatly from corpus to corpus. We have run the following experiment: we have considered the training and test corpora used in Web People 
Search-1 (WePS-1), which are web page corpora, and we have implemented a bag of word ap-proach with two variants of clustering: agglom-erative (A), and hierarchic (H). We have ran-domly chosen a set of seven names from training and test (14 names in total) and we have com-pared the results applying the two systems, A and H, on each set of names. In Figure 1 we pre-sent the results obtained. The figures on the ver-tical axes are computed using F?=0.5 formula.
 Figure 1. Variation between training and test We have noticed a great variation in the be-havior of the two systems. In order to search for an explanation for this difference we have looked at the distribution in the two corpora of the Named Entities, of the words denoting profes-sions and of the meta-contextual information - e-mails, urls, phones, and addresses. It turns out that these types of contextual information are distributed between training and test approxi-mately evenly. (see Table 1a,b). 
Profession training occ. test occ. Doctor 543 668 Lawyer 277 385 Professor 523 490 Researcher 340 166 Teacher 617 569 Coach 467 471 Actor 998 790 Table 1a. Profession words in training and test  Address training occ. test occ. Phone 1,109 1,169 Fax 606 426 e-mail 3,134 2,186 Table 1b. Meta-Context in training and test By manually investigating the training and test set of our experiment we have reached the con-clusion that the reason for the difference is two fold: firstly, while the distribution of the words denoting profession is similar, in the test set the modifiers, for example ?internist?, ?neurosur-geon? for ?doctor?, are more frequent. Secondly, the number of different persons having the same 
999
name is, on average, higher in test than in train-ing. The results plotted in Figure 1 show that it is not a question of which algorithm is better, but rather that there are different cases where one approach is preferred over the other. The prob-lem we face is deciding when it is appropriate to use one or the other. To induce from the corpus itself when a piece of context is or is not a relevant context requires deep ontological inferences and a very powerful tool of semantic analysis of the context. Consider for example two words denoting profession, ?doctor? and ?researcher?, and their possible modifiers ?internist?, ?neurosurgeon?, and ?pro-fessor? and ?PhD?. In the first case it is certain that the coreference is not possible, while in the second the coreference is very probable. To find out such relationships is computationally very hard. However, the analysis carried out further shows that we can avoid making such computa-tions in most of the cases.  The number of different persons is a parame-ter that cannot be known beforehand. However, not all the names behave alike with respect to coreference. There are noticeable differences between names; for example less than 5 000 first names cover approximately 96% of the total of first names, while for the same percentage of coverage more than 70 000 of last names must be considered (Popescu et al 2007). Let us call per-plexity of a name the number of different persons that carry it. The search space depends directly on the name perplexity. The bigger the perplex-ity, the larger the amount of information required for the correct coreference must be. It seems natural that the amount of contextual evidence required by a PCDC depends on the name per-plexity. In order to evaluate the relationships between the context and the name perplexity, we need an annotated corpus. We have used the I-CAB cor-pus (Magnini et al 2006), which is a four-day news corpus fully annotated, coreference rela-tionships included. The documents in this corpus are entire pieces of news. For each PNM we have counted how many contexts containing specific information about the person carrying the respec-tive name is present in that particular document. There are many types of contexts that refer to a person, but some of these types are very infre-quent. We considered only those types of infor-mation that are present at least 5% of the times in the context surrounding a PNM. Table 2 presents the results of this investigation. 
 occ. diff occ entities First Names 2299 676 1592 Last Names 4173 1906 2191 Middle Name 110 44 41 Activity 973 322 569 Affiliation 566 399 409 Role 531 211 317 Family Relation 133 46 94 Table 2. Name perplexity and context  On the second column the total number of oc-currences is listed, on the third column how many of these occurrences have different values (no case sensitive string match), and on the fourth column the number of different persons (Entities) having that information. The entries ?activity?, ?affiliation?, and ?role? represent pieces of context where the respective informa-tion is directly expressed (no inferences). We call this type of context professional context and for approximately 30% of the PNMs, one of the above three types of professional contexts is pre-sent. The perplexity of the first names, computed as the ratio between the fourth column and the third column is two times bigger than the perplexity of the last names. The lowest name perplexity is obtained by the names having a middle name - a name with at least three tokens ? and it is very close to 1 (1.07). Comparatively, the highest per-plexity of two tokens name is 3. The relationship between the number of tokens of a name and its perplexity is straightforward: for names with more than four tokens the perplexity is 1 in 99,6% of the cases (the name by itself is a rele-vant context for coreference). In approximately 74% of the cases there is just one entity corresponding to a two-token name. Considering any two PNMs of the same name the similarity of two of the professional contexts guarantees the correct coreference. However, two professional contexts are present in only 4% of the cases. There are just four cases when con-sidering just one professional attribute was mis-leading, and all these cases are high perplexity names. Moreover, in the case of many low per-plexity names, the contexts could be minimally similar in order to correctly corefer any two PNMs of that respective name.  This analysis shows that there is a direct rela-tionship between the name perplexity and the relevant coreference context. However, the aver-age figures are not very informative, as the vari-ance of perplexity is very high. Rather than fo-
1000
cusing on the exact figure for name perplexity, we will try to partition the names according to their perplexity and to link each partition to a specific behavior with respect to coreference. The partitioning technique should ensure that the variance of the name perplexity within the same partition is low and that a specific amount of context should lead to the correct coreference decision for the great majority of names within that partition.  Our working hypothesis is that we can esti-mate the name perplexity within each partition and use this information to control the amount of contextual evidence required. Let us recall the ?John Smith? and ?Barack Obama? example from the previous section. Both ?John? and ?Smith? are American common first and last names. The chance that many different persons carry this name is high. On the other hand, as both ?Barack? and ?Obama? are rare American first and last names respectively, almost surely many mentions of this name refer only to one person. The argument above does not depend on the context, but just on the prior estimation of the usage of those names. Having an estimation of a name?s perplexity, we may decrease/increase the amount of contextual evidence needed.  4 (p, ?) Statistical Model Let D be the set of all PNMs from a given corpus C and let DN be the set of corresponding names. We want to find a partition P of DN such that within each partition the name perplexity varies only within predicted margins. Let X be a ran-dom variable with uniform distribution over DN and let Y be the random variable defined by X?s name perplexity. Let us suppose that we want P = {p1, p2, ?, pm} to be a partition of DN, where the percentage of each partition class is pi: the first partition class contains p1 percentage of the name population, the second partition class con-tains p2 percentage of the name population and the last partition class contains pm = 1 - ?pi per-centage of the name population.  If we knew the distribution function of Y, let?s call it F, we would simply determine ?i from equation 1, where Pi =?pk , k ? i : ?i = F-1(Pi)? F(Y<?i) = Pi       (1) and we would know that in each partition pi the name perplexity is between ?i-1 and ?i, with ?0 = 0. However we do not know F. Fortunately, we can estimate ?i.  
There is no restriction that may impose a par-ticular form for F; for example, the normal dis-tribution hypothesis of name perplexity is ruled out by a ?2 test with 96.5% confidence for the 14 names chosen from WePS-1 (see Section 3, first paragraph). We are going to present a distributional free method for constructing the partition P. The ad-vantage of this method is that it does not depend on any assumption about the PNMs distribution.  Let us consider X1, X2, ?, Xn a sample of in-dependent and identical distributed names from DN. By rearranging the indexes, without losing the generality, let us suppose that Y1, Y2, ?, Yn is ordered, that is Y1 ? Y2 ? ? ? Yn. Even if we do not know what form F has, we can still use equa-tion (1) in order to estimate ?i. The expected value of F(Yi) is (Hogg, Mckean, Craig 2006): E[F(Yi)] = i/(n+1)       (2) which is an estimation of how much mass prob-ability is on the left of Yi. In our terms, we esti-mate that E[F(Yi)] percentage of the name popu-lation has a name perplexity lower than Yi.  For a given number ?, the percentage of the name population having the name perplexity at most ? is determined by finding the smallest Yi greater than ? and use the equation (2) to esti-mate E[F(Yi)].  In order to build the partition P we are inter-ested in the percentage of the name population that has the perplexity between two given values. Let (Yi, Yj) be the smallest interval that includes these two values. We can estimate the percentage of the name population that has a perplexity be-tween Yi and Yj. This estimate is simply F(Yj) ?F(Yi). We can use directly equation (2) to esti-mate this difference. However, it is more impor-tant to have a confidence interval for this esti-mate, that is we want to know what the probabil-ity is that the interval (F(Yi), F(Yj)) contains at least a given percentage of the population, p. The optimal partition P is the one that maximizes the confidence in the fact that within each of the par-tition classes as many names as possible have the name perplexity in a given interval. Let p be a given real number between (0,1) representing the mass probability that goes into the interval (F(Yi), F(Yj)). Let ? = P(F(Yj) ? F(Yi) ? p). Fortunately ? has a distribution that does not depend on F. More precisely, ? has a beta distribution given by the function in formula (3): 
1001
? = P(F(Yj) ? F(Yi) ? p) =    ?p1  ?(n+1)/( ?(j-i)) ?(n-j+i+1)xj-i-1(1-x)n-j+idx   (3) The ?, called the gamma function, is the ex-tension of the factorial, ?(x) = ?0? tx-1e-tdt. The gamma function has the property that ?(x) = ?(x-1) ?(x-2)?. ?(1); for x an integer, as the argu-ments in the formula (3) are, ?(x) = (x-1)! The formula (3) gives us a method of building the partition P. Let us start with a set of perplex-ity intervals: (?0, ?1], (?1,?2], ?(?m-1, ?m]. We partition the names in DN such that we maximize the confidence ? that at least pi percentage of the name population has a name perplexity in (?i-1, ?i]. We chose an independent and identical dis-tributed sample X of names to which the ordered sample Y of name perplexity values corresponds. We start with the lowest perplexity interval and determine p1,?1 and Y0, Yi1, such that Y0 ? ?0 ? ?1 ?Yi1 and ?1 = P(F(Yi1) ? F(Y0) ? p1). The ith in-dex varies according to the desired ?1, when p1 is given, and vice-versa. We can choose i1 with m-1 liberty grades. Once we are satisfied with the values (p1,?1), we search for the i2th index such that Yi1 ? ?1 ? ?2 ?Yi2 and (pi2,?i2) have the de-sired value. The process continues till the penul-timate (pm-1,?m-1). We have no liberty in choosing the (pm,?m).  We can compute the size of the sample needed for guaranteeing a minimum ? and p.  Let us give an example. Suppose that (?0, ?1] = (0,2]. Thus we are interested in finding p, the percentage of the name population such that we can be ? sure that at least p names have a per-plexity between 1 and 2 inclusive. We take a random sample of n = 30 and suppose the small-est index i1 such that Yi ? 3 for all i > i1 is 17 .  We want to compute the confidence ? that at least p = 60% of the name population has the name perplexity within (0,2]: ? = 1 - ?0 0.6 30!/(16!15!)x15(1-x14)dx = = 1 ? k(?0 0.6 x15dx + ?0 0.6 x29dx) =  = 1 ? k[(1/16)(6/10)16 + (1/30)(6/10)30] ? .965 In practice we want to have optimal values for p and ?; a large p implies a small ? and vice-versa. The optimality is determined by the accu-racy of the CDC system: we want to have the 
largest possible percentage of names into each partition such that our confidence that the names inside each partition have the same perplexity.  It is useful to work the equation (1) back-wards. Suppose that we established the first par-tition class of P - we have found the i1th index, p1, and ?1. Now we refer only to the names in the partition class. We can compute the probability that a certain percentage of the names within that particular partition class have a given name per-plexity. That is, we consider a random sample inside the partition class, X, and its correspon-dent random variable Y, as above. The confi-dence that p1inside percentage of names have the name perplexity ?p within the interval (Y0, Yith) is: P(Y0 < ?p <Yi1) = ?k   (kn) pk(1-p)n-k      (4) (kn) represents the k-combinations of size n.  By taking advantage of the bootstrapping method (Efron and Tibshirani 1993) we do not have to resample inside the partition class. We use the Y0, .., Yi1 values with replacement. Using (4) we obtain p1inside which shows us which per-centage inside the partition class has the name perplexity within (Y0, Yi1]. And consequently we can compute ?1inside. Finally we are able to formu-late the following statement about each partition class: In the ith partition class enter pi percent-age of the name population with a confi-dence ?i. Inside this partition class we are ?iinside confident that piinside percentage of the names have a name perplexity within (Yi1-1, Yi1]. The p and ? indicate the theoretical values that define the partition. In practice the exact distribution of the names into the subset is unknown, therefore each heuristics that computes the perplexity creates its own dis-tribution. The values ?iinside and piinside control how much a certain heuristics departs from the theoretical values. The optimal heuristics have very big figures for ?iinside and piinside. In the next section we present an experi-ment carried on a news corpus. We show how the above model leads to a stable parti-tion of names and that inside each partition class reliable (p,?) values can be computed.  
1002
5 Name Perplexity Partition For the experiment described in this section, we have used a two-year part of the seven-years Ital-ian local newspaper corpus called Adige500k Corpus (Magnini 2006).  We describe below how we compute the per-plexity class for the one-token names and two-tokens names respectively. As mentioned in sec-tion 3, the name perplexity decreases rapidly for tree-token or more names. If desired, the same technique could also be applied for those names. In Adige500k there are 106, 187 different one-token names; 429, 243 two-token names; 36, 773 three-token names; 5, 152 four-token names, 940 four token names and less than 300 different four-token or more names.  An estimate of the name perplexity of the one-token names is the size of the different one-token names with which it forms a complete PNM in the corpus. For example for the first name ?John? the estimation of its perplexity is the size of the one-token last names it combines with in forming PNMs, like ?Smith, Travolta, Kennedy? etc. The bigger the size of its complementary names, the higher is its name perplexity. In Table 3 we present the figures of these estimates. occurrences (interval) average perplexity 1-5 4.13 6-20 8.34 21-100 17.44 101-1,000 68.54 1,000-5,000 683.95 5,000-31,091 478.23 Table 3. Average perplexity one-token names We start with a five name perplexity classes: ?very low? (VL) , ?low? (L) , ?medium?, (M) ?high? (H) and ?very high? (VH). The name per-plexity of a two-token name is interpolated from the name perplexity of its components. We used the following heuristics: the name perplexity class is the average name perplexity classes of its one-token name. If the name perplexity classes are the same then the name perplexity class of the whole name is one class less (if possible). In order to compute the borderline be-tween two consecutive classes we apply the (p, ?) method. We selected 25 two-tokens names and we manually investigate their oc-currence in order to know their real name perplexity. The perplexity classes obtained 
after applying the (p, ?) technique are listed in Tables 4a and 4b respectively. 
perplexity class percentage very high (VH) 5.3% high (H) 8.7% medium (M) 20.9% low (L) 27.6% very low (VL) 37.5% Table 4a. First Name perplexity classes perplexity class percentage very high (VH) 1.8% high (H) 3.36% medium (M) 17.51% low (L) 20.31% very low (VL) 57.02% Table 4b. Last Name perplexity classes Tables 4a and 4b fully describe the partition for one-token names. Ordering the one token names according to their perplexity we  chose the first ones according to the  percentage listed above. The same process applies to the one-token last names. The values computed for two-token names are listed.  P ? pinside ?inside VH 0.04% 70% 70% 80% H 2.53% 76% 70% 80% M 10.08% 87% 80% 82% L 27.97% 90% 99% 90% VL 59.38% 96.5% 99% 96.5% Table 5. (p, ?, pinside, ?inside) values 6 CDC with Name Perplexity Estimates The working hypothesis is that using the name partition obtained with the (p, ?) procedure we can effectively improve the accuracy of a CDC system by reducing/increasing the amount of contextual evidence required for coreferencing according to the perplexity class to each the name belong. To construct a test corpus we have adopted the following strategy: we chose 20 two-token names such that both sets of one token-names, the first names and the last names respectively, cover the whole space in the perplexity partition. In Table 6a and 6b we present 5 first and last names used in test. As not all the 25 names formed by combin-
1003
ing the names in 6a and 6b are found in the corpus, we consider 11 other two-tokens names having the same distribution. On the first column the names are listed, on the sec-ond column the computed perplexity (P), on the third column the number of occurrences as one-token name (O), on the fourth the number of occurrences in a two-token name (T) and on the last column the computed perplexity class (PC). Name P O T PC Dellai 7 31091 10722 VL Parolari 171 1,619 2207 H Prodi 52 9184 3382 M Ruini 15 554 203 L Rossi 753 7506 8356 VH Table 6a. Test Last Names  Name P O T PC Camillo 276 664 1731 L Lorenzo 2088 2167 2198 H Paolo 5255 4001 51244 VH Romano 14 886 6414 M Varena 5 10 85 VH Table 6b. Test First Names We compare the results obtained by our CDC system using the name perplexity parti-tion (S) against two baselines: one that con-siders only the context at the sentence level and (BLS) one that considers the whole news (BLN). We obtain the following figures us-ing the B-CUBED measure: S scores .72, BLS .59 and BLN .61. The gain in accuracy of more than 10% is due to the use of name perplexity classes. The great advantage of using the (p,?) es-timates can be seen in those case where the ratio between the number of mentions and the rank of the name is close to extremes: either big number of mentions and low name perplexity, or low number of mentions and high name perplexity. In the first case the contextual evidence for coreference may be very scarce and in the second case, the re-quirement for strong contextual evidence is the best decision. Our results suggest that loosening the contextual requirements in the first case leads to an important gain in recall, 
up to 40%, while the lose in precision is less than 1.5%. The situation is best described by four panels of the five-number-summary plots of the test corpus. Panel A shows the distribution of the main five quantilies con-sidering all the names together. Panel B shows the distribution for very low perplex-ity class, Panel C for medium perplexity class and Panel D for the very high perplex-ity class. The number of outliers in Panel A is high, which makes it difficult for any CDC system, but inside each perplexity class the variation is reduced. 
 7 Constructing an Evaluation Corpus The (p, ?) technique could be used for construct-ing a test corpus for the CDC task. The main problem faced in the construction of the test cor-pus is data variation. The number of different entities mentioned with the same name is a ran-dom variable with a big variance. The distribu-tion of the number of entities is very skew. The average perplexity is 2.01%, but less than 18% of the total number of names have a perplexity greater than 3. In Figure 2 we plot a modified Lorenz curve (the vertical axis is not divided in percentage, as the values are discrete). 
 Figure 2. Lorenz Curve names/no. entities The direct consequence of this situation is the fact that constructing an evaluation corpus by taking random names will result with a great probability in a very skew test corpus. Indeed, 
1004
the expectation is that in such corpus, the aver-age perplexity is very low, and consequently, the great majority of cases can be coreferenced by a simple algorithm. Therefore, this test corpus may be largely ineffective in ranking the algorithms. In fact, we want to construct an evaluation cor-pus that is able to promote the most effective algorithms. The discriminative power of a test corpus is directly related to the variance of the data. Moreover, if only certain names are consid-ered for a test corpus, the variance can be very low; in particular, when the test corpus contains just one name the variance is zero. It is difficult to see the merits of different algorithms when tested on such corpora. In order to make more informative statements we need to construct an evaluation corpus that is less dependent on the data variance. A possible solution is to form a partition of the set of the PNMs, that is, to split the whole set of PNMs in mutual disjunctive groups. This type of method-ology is called stratified sampling, mainly be-cause each group is a stratum. The sampling strategy, the number of sampling elements, the variance and the sampling error can be calculated independently for each strata. The main advantages of stratified sampling are that we can concentrate on the special groups, that in general this strategy improves the accuracy of the estimation, and that the number of elements in each stratum can be conveniently chosen. The main disadvantages are related to the difficulty in finding a suitable partition of the population. The strata should be chosen prior to the sampling time, but the homogeneity inside the stratum should be guaranteed.  Our proposal is to use the name perplexity in-tervals. We argue that this proposal is four-fold sustainable. Firstly, the name perplexity is di-rectly connected to the random variable whose distribution we estimate, namely the number of entities. Secondly, for free names it can be com-puted off - line. Thirdly, it gives us an independ-ent and formally correct way to make a partition. Fourthly, it easily allows a separation between the important and unimportant cases.  To begin with, let us suppose we have a name that has n occurrences in the Adige 500K. If n is relatively large, than we can be sure that there are some dominant entities that may be repre-sented by the majority of PNMs that have this name as value. However, it is unknown whether the n comes from the fact that there are indeed some dominant entities or whether the name by itself is a frequently used name. 
In order to deal with the differences between frequency vs. perplexity, we propose to build a matrix defined by the frequency classes as rows and perplexity classes as columns. In Figure 3 we present this matrix. 
 Figure 3. Frequency/Commonness strata ma-trix. The number of different names in each of the cells of the matrix may differ according to the departure of the normal distribution of each stra-tum. In general, if the real distribution is normal, then as much as ten examples are sufficient. Oth-erwise, for not very skew distributions, which  we expect most of the strata to have, an average of 30 examples should suffice. In same cases, as the normal distribution can be appropriately sampled when both Np and N(1-p) are grater than five ? where p is the ratio perplexity/frequency and N the sample dimension ? the number of elements in the cell may be around 200, by a maximal rough estimation. 8 Conclusion and Further Research We have presented a distributional free statistical method to design a name perplexity system, such that each perplexity class maximizes the number of names for which the prior coreference prob-ability belongs to the same interval. This infor-mation helps the PCDC systems lower/increase adequately the amount of contextual evidence required for coreference. The approach presented here is effective in dealing with the problems raised by using a simi-larity metrics on contextual vectors improving the overall accuracy with more than 10%. We would like to increase the number of cases considered in the sample required to delimit the perplexity classes. Equation (3) may be devel-oped further in order to obtain exactly the num-ber of required cases. The (p, ?) procedure is effective is dealing with the problems regarding the construction of an evaluation corpus. The technique presented in the last section could be extended further and  we are already working on a new series of experi-ments whose results will be made available in the near future.  
1005
Acknowledgments  The corpus used in this paper is Adige500k, a seven-year news corpus from an Italian local newspaper. The author thanks to all the people involved in the construction of Adige500k. References  J. Artiles, Gonzalo, J., S. Sekine. 2007. Establishing a benchmark for WePS. In Proceedings of Se-mEval. A. Bagga, B. Baldwin. 1998. Entity-based Cross-Document Co-referencing using the Vector Space Model. In Proceedings of ACL. J. Chen, D. Ji, C. Tan, Z. Niu. 2006. Unsupervised Relation Disambiguation Using Spectral Cluster-ing. In Proceedings of COLING C. Gooi, J. Allan. 2004. Cross-Document Corefer-ence on a Large Scale Corpus. In Proceedings of  ACL.  R. Grishman. 1994. Whither Written Language Evaluation? In Proceedings of Human Language Technology Workshop, pp. 120-125. San Mateor. E. Elmacioglu, Y. M. F. M.Y.Khan, D. Lee. 2007. PSNUS: Web People Name Disambiguation by Simple Clustering with Rich Features, in Proceed-ings of SemEval H. Han, W. Xu. 2005. A Hierarchical Bayes Mix-ture Model for Name Disambiguation in Author Citations, in Proceedings of SAC?05 R. Hog, J. McKean, A. Craig, 2006. Introduction of Mathematical Statistics, ed. Prentice Hall E. Lefever, V. Hoste, F. Timur. 2007. AUG: A Com-bined Classification and Clustering Approach for Web People Disambiguation, In Proceedings of SemEval B. Magnini, M. Speranza, M. Negri, L. Romano, R. Sprugnoli. 2006. I-CAB ? the Italian Content An-notation Bank. LREC 2006 V., Ng. 2007. Shallow Semantics for Coreference Resolution, In Proceedings of IJCAI T. Pedersen, A. Purandare, A. Kulkarni. 2005. Name Discrimination by Clustering Similar Contexts, in Proceeding of CICLING O. Popescu, C. Girardi. 2008. Improving Cross Document Coreference, in Proceedings of JADT O. Popescu, B. Magnini. 2007. Inferring Corefer-ence among Person Names in a Large Corpus of News Collection, in Proceedings of AIIA 
Y. Wei, M. Lin, H. Chen. 2006. Name Disambigua-tion in Person Information Mining, in Proceedings of IEEE Q. Vu, T. Massada, A. Takasu, J. Adachi. 2007. Us-ing Knowledge Base to Disambiguate Personal names in Web Search Results, In Proceedings of SAC 
1006
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1634?1642,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Fast and Accurate Misspelling Correction in Large Corpora
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Ngoc Phuoc An Vo
University of Trento
Fondazione Bruno Kessler
Trento, Italy
ngoc@fbk.eu
Abstract
There are several NLP systems whose ac-
curacy depends crucially on finding mis-
spellings fast. However, the classical ap-
proach is based on a quadratic time algo-
rithm with 80% coverage. We present a
novel algorithm for misspelling detection,
which runs in constant time and improves
the coverage to more than 96%. We use
this algorithm together with a cross docu-
ment coreference system in order to find
proper name misspellings. The experi-
ments confirmed significant improvement
over the state of the art.
1 Introduction
The problem of finding the misspelled words in a
corpus is an important issue for many NLP sys-
tems which have to process large collections of
text documents, like news or tweets corpora, dig-
italized libraries etc. Any accurate systems, such
as the ones developed for cross document corefer-
ence, text similarity, semantic search or digital hu-
manities, should be able to handle the misspellings
in corpora. However, the issue is not easy and
the required processing time, memory or the de-
pendence on external resources grow fast with the
size of the analyzed corpus; consequently, most of
the existing algorithms are inefficient. In this pa-
per, we present a novel algorithm for misspelling
detection which overcomes the drawbacks of the
previous approaches and we show that this algo-
rithm is instrumental in improving the state of the
art of a cross document coreference system.
Many spelling errors in a corpus are acciden-
tal and usually just one or two letters in a word
are affected, like existnece vs. the dictionary form
existence. Such misspellings are rather a unique
phenomenon occurring randomly in a text. For an
automatic speller which has access to a dictionary,
finding and compiling a list of correct candidates
for the misspelled words like the one above is not
very difficult. However, not all misspellings are in
this category. To begin with, proper nouns, espe-
cially foreign proper names, are not present in the
dictionary and their misspelling may affect more
than one or two characters. Moreover, the mis-
spelling of proper names may not be random, for
example there might be different spellings of the
same Chinese or Russian name in English, the in-
correct ones occurring with some frequency. Also,
especially if the corpus contains documents writ-
ten by non native speakers, the number of char-
acters varying between the correct and the actual
written form may be more than two. In this case,
finding and compiling the list of correct candidates
is computationally challenging for traditional al-
gorithms, as the distance between the source string
and the words in the candidates list is high.
The Levenshtein distance has been used to com-
pile a list of correct form candidates for a mis-
spelled word. The Levenshtein distance between
two strings counts the number of changes needed
to transform one string into the other, where a
change is one of the basic edit operations: dele-
tion, insertion, substitution of a character and the
transposition of two characters. The Edit Dis-
tance algorithm, (ED) computes the similarity be-
tween two strings according to the Levenshtein
distance. Most of the random misspellings which
are produced by a native speaker are within one
or maximum two basic edit operations (Damerau,
1964). For this reason the ED algorithm is the
most common way to detect and correct the mis-
spellings. However, there is a major inconve-
nience associated with the use of ED, namely, ED
1634
runs in quadratic time considering the length of
the strings, O(n
2
). The computation time for more
than a few thousands pairs is up to several tens of
seconds, which is impracticably large for most of
large scale applications. By comparison, the num-
ber of proper names occurring in a medium sized
English news corpus is around 200, 000, which
means that there are some 200, 000, 000 pairs.
In order to cope with the need for a lower com-
putation time, on the basis of ED, a series of algo-
rithms have been developed that run in linear time
(Navaro 2001). Unfortunately, this improvement
is not enough for practical applications which in-
volve a large amount of data coming from large
corpora. The reason is two-fold: firstly, the linear
time is still too slow (Mihov and Schulz, 2004)
and secondly, the required memory depends both
on the strings? length and on the number of differ-
ent characters between the source string and the
correct word, and may well exceed several GBs.
Another solution is to index the corpus using struc-
tures like trie trees, or large finite state automata.
However, this solution may require large amounts
of memory and is inefficient when the number of
characters that differ between the source string and
the candidate words is more than two characters
(Boytsov, 2011).
We focus specifically on misspellings for which
there is no dictionary containing the correct form
and/or for which the Levenshtein distance to the
correct word may be higher than two characters.
For this purpose, we developed a novel approach
to misspelling correction based on a non indexing
algorithm, which we call the prime mapping algo-
rithm, PM. PM runs in constant time, O(1), with
insignificant memory consumption. The running
time of the PM algorithm does not depend either
on the strings? length or on the number of different
characters between the source string and the can-
didate word. It requires a static amount of mem-
ory, ranging from a few KBs to a maximum of a
few MBs, irrespective of the size of the corpus or
the number of pairs for which the misspelling rela-
tionship is tested. We run a series of experiments
using PM on various corpora in English and Ital-
ian. The results confirm that PM is practical for
large corpora. It successfully finds the candidate
words for misspellings even for large Levenshtein
distances, being more than 30 times faster than a
linear algorithm, and several hundred times faster
than ED. The running time difference is due to the
fact that PM maps the strings into numbers and
performs only one arithmetic operation in order to
decide whether the two strings may be in a mis-
spelling relationship. Instead of a quadratic num-
ber of characters comparisons, PM executes only
one arithmetic operation with integers.
We also report here the results obtained when
using PM inside a cross document coreference
system for proper nouns. Correcting a proper
name misspelling is actually a more complex task
than correcting a misspelled common word. Some
misspellings may not be random and in order to
cope with repetitive misspellings, as the ones re-
sulting from the transliteration of foreign names,
the PM is combined with a statistical learning al-
gorithm which estimates the probability of a cer-
tain type of misspelling considering the surround-
ing characters in the source string. Unlike with
common words, where a misspelling is obvious,
in the case of proper names, John vs. Jon for ex-
ample, it is unclear whether we are looking at two
different names or a misspelling. The string sim-
ilarity evidence is combined with contextual evi-
dence provided by a CDC system to disambiguate.
To evaluate the PM algorithm we use publicly
available misspelling annotated corpora contain-
ing documents created by both native and non-
native speakers. The PM within a CDC system for
proper names is evaluated using CRIPCO (Ben-
tivogli et al., 2008). The experiments confirm that
PM is a competitive algorithm and that the CDC
system gains in accuracy by using a module of
misspelling correction.
The rest of the paper is organized as follows. In
Section 2 we review the relevant literature. In Sec-
tion 3 we introduce the PM algorithm and com-
pare it against other algorithms. In Section 4 we
present the CDC system with misspelling correc-
tion for proper names. In Section 5 we present the
results obtained on English and Italian corpora.
2 Related Work
In a seminal paper (Damerau, 1964) introduced
the ED algorithm. The rationale for this algorithm
was the empirical observation that about 80% of
the misspelled words produced by native speakers
have distance 1 to the correct word. ED cannot be
extended to increase the accuracy, because for k =
2, k being the maximal admissible distance to the
correct word, the running time is too high. Most of
the techniques developed further use ED together
with indexing methods and/or parallel processing.
In (San Segundo et al., 2001) an M-best can-
1635
didate HMM recognizer for 10,000 Spanish city
names is built for speech documents. An N-gram
language model is incorporated to minimize the
search spaces. A 90% recognition rate is reported.
The model is not easily generalizable to the situ-
ation in which the names are unknown - as it is
the case with the personal proper names in a large
corpus. The N-gram model is memory demanding
and for 200,000 different names the dimension of
the requested memory is impracticably big.
The problem related to personal proper names
was discussed in (Allan and Raghavan, 2002).
However, the paper addresses only the problem of
clustering together the names which ?sound alike?
and no cross document coreference check was car-
ried out. The technique to find similar names
is based on a noisy channel model. The condi-
tional probabilities for each two names to be sim-
ilarly spelled are computed. The time complex-
ity is quadratic, which renders this technique un-
feasible for big data. In fact, the results are re-
ported for a 100 word set. A different approach
comes from considering search queries databases
(Bassil and Alwani, 2012). These techniques are
similar to the model based on the noisy channel,
as they compute the conditional probabilities of
misspellings based on their frequencies in similar
queries. Unfortunately, large numbers of queries
for proper names are not available. A similar tech-
nique, but using morphological features, was pre-
sented in (Veronis, 1988). The method can man-
age complex combinations of typographical and
phonographic errors.
It has been noted in many works dedicated to
error correction, see among others (Mihov and
Schulz, 2004), that the ED algorithm is imprac-
ticably slow when the number of pairs is large. A
solution is to build a large tries tree. While this
solution improves the searching time drastically,
the memory consumption may be large. Automata
indexing was used in (Oflazer, 1996). While the
memory consumption is much less than for the
tries tree approaches, it is still high. For Turk-
ish, the author reported 28,825 states and 118,352
transitions labeled with surface symbols. The re-
covery error rate is 80%. In (Boytsov, 2011) a
review of indexing methods is given. Testing on
5,000 strings for k=1,2,3 is reported and the paper
shows the problem the systems run into for bigger
values of k. In (Huld?en, 2009) a solution employ-
ing approximations via an A* strategy with finite
automata is presented. The method is much faster
for k bigger than the one presented in (Chodorow
and Leacock, 2000). However, the usage of A*
for proper names may be less accurate than the
one reported in the paper, because unlike the com-
mon words in a given language, the names may
have unpredictable forms, especially the foreign
names. The results reported show how the time
and memory vary for indexing methods according
to the length of the words for k=1,2,3.
A method that uses mapping from strings to
numbers is presented in (Reynaert, 2004). This
method uses sum of exponentials. The value of
the exponential was empirically found. However,
the mapping is only approximative. Our mapping
is precise and does not use exponential operations
which are time consuming.
The study in (Navarro, 2001) is focused on non
indexing approximate string search methods, in
particular on the simple ED distance. The non-
indexing methods may reach linear running time,
but it is not always the case that they are scalable
to big data. In (Nagata et al., 2006) a study on the
type of errors produced by non-native speakers of
English is carried out, but the long distance mis-
spellings are not considered.
3 Prime Mapping Misspeling Algorithm
The algorithms based on the Levenshtein dis-
tance use the dynamic programming technique to
build a table of character to character comparisons.
We present here a novel approach to misspelling
which does not build this table, skipping the need
to compare characters. In a nutshell, the prime
mapping algorithm, PM, replaces the characters
compare operations to a unique arithmetic oper-
ation.This can be done by associating to any letter
of the alphabet a unique prime number. For ex-
ample we can associate 2 to a, 3 to b, 5 to c ...
97 to z. Any string will be mapped into a unique
number which is the product of the prime numbers
corresponding to its letters. For example the name
abba is mapped to 2 ? 3 ? 3 ? 2 = 36. By computing
the ratio between any two words we can detect the
different letters with just one operation. For exam-
ple, the difference between abba and aba is 36/12
= 3, which corresponds uniquely to b because the
product/ratio of prime numbers is unique.
Unlike the ED algorithm, the prime mapping
does not find the number of edit operations needed
to transform one string into another. In fact, two
words that have just one letter in the mutual dif-
ference set may be quite distinct: all the strings
1636
aba, aab, baa differ by one letter when compared
with abba. In order to be in a misspelling relation-
ship, the two strings should also have a common
part, like prefix or middle, or suffix. The com-
plete Prime Mapping (PM) algorithm consists of
two successive steps: (1) find all the candidate
words that differ from the target word by at most
k characters and (2) check weather the target word
and the candidate word have a common part, suf-
fix, prefix or middle part. Both steps above are
executed in constant time, therefore they do not
depend either on the length of the strings or on k,
the maximal number of different characters. Nor-
mally, k = 3, because the probability of a mis-
spelled word having more than three distinct let-
ters is insignificant, but unlike in the case of ED,
the choice of k has no influence on the running
time. The first step takes an integer ratio and a
hash table key check, both being O(1). The sec-
ond step checks if the first k letters at the begin-
ning or at the end of the word are the same, and it
requires 2k character comparisons, which is also
an O(1) process, as k is fixed. The pseudo code
and detailed description of the PM algorithm are
given below.
Algorithm 1 Prime Mapping
Require: charList wordsList, primeList, k
Ensure: misspList
1: misspList? ?
2: foreach ? in charList: p(?)? p
i
, p
i
in primeList
3: foreach w in wordsList: p(w)?
?
p(?) , ? in w
4: primeKTable?
(
n
k
)
of prime arithmetics
5: for w in wordsList do
6: for w? in wordsList, w 6= w? do
7: r?
p(w)
p(w
?
)
8: if r in primeKTable then
9: if commonPart (w, w?) 6= ? then
10: misspList?misspList + (w, w?)
11: end if
12: end if
13: end for
14: end for
map letters to prime numbers. A helpful way
to assign primes to letters is according to their fre-
quency; on average, the numbers corresponding to
names are smaller and the operation gets less time.
compute a hash table with prime arithmetics
of K primes. In the hash table primeKTable we
record all the combinations that can result from di-
viding two products which have less than k primes:
1/p
i
, p
i
, p
i
/p
j
etc. If the ratio between two map-
pings is in the hash table, then the corresponding
words have all the letters in common, except for
at most k. The number of all the combination is
k letter difference #combination Memory
1 60 480B
2 435 8K
5 142,506 0.9MB
6 593, 775 3.8MB
10 30, 045, 015 180MB
Table 1: The PM algorithm memory needs
(
n
k
)
. The memory consumption for different val-
ues for k is given in Table 1. The figures compare
extremely favorably with the ones of ED based ap-
proaches (gigs magnitude) . (line 7-8)
find misspelling candidates by ratio. By com-
puting the ratio and by checking the hash table, we
found the pairs which use the same letters, except
for at most k. The procedure commonpart checks
whether the two strings also have a common part
by looking at the start and end k. If this is the case,
the pair is in a misspelling relationship.
Figure 1: PM vs. the fastest ED type algorithm
The PM is much faster than ED. The fastest
variant of ED, which does not compare strings
having length difference bigger than 1, theoret-
ically finds only 80% of the misspellings. In
practice, only around 60% of the misspellings are
found because of proper names and words mis-
spelled by non-native speakers. The PM algorithm
considers all possible pairs, finds more than 99%
of misspellings and is 35 times faster. To obtain
the same coverage, the ED algorithm must run for
more than 100 days. The time comparison for mil-
lions of pairs is plotted in Figure 1. The experi-
ments were performed on an i5, 2.8 GHz proces-
sor.
There is an immediate improvement we can
bring to the basic variant of PM. The figures re-
ported above are obtained by doing the whole set
of possible pairs. By taking into account the fact
that two words differing by k+1 letters cannot be
k similar, we can organize the number represent-
ing the names into an array which reduced drasti-
1637
cally the number of comparisons. For example, all
the words containing the letters x, y, z cannot be
k = 2 similar with the words not containing any of
these letters. By dividing the mapping of a word to
the primes associated with the letters of an k-gram,
we know if the words containing the k-gram can
be misspelling candidates with at most k differ-
ence, and there is no more need to carry out all the
ratios. We arrange the mappings of all words into
an array such that on the first indexes we have the
words containing the less frequent k + 1 gram, on
the next indexes we have the words containing the
second less frequent k+1 gram and do not contain
the first k+1 gram, on the next indexes the words
containing the third less frequent k + 1 gram and
do not contain the first two k+1 gram, etc. In this
way, even the most frequent k + 1 gram has only
a few words assigned and consequently the num-
ber of direct comparisons is reduced to the mini-
mum. The mapping corresponding to a k+1 gram
are ordered in this array according to the length of
the words. The number of trigrams is theoretically
large, the k + 1 power of the size of the alpha-
bet. However, the number of actually occurring
k-trigrams is only a small fraction of it. For exam-
ple, for k = 2, the number of trigrams is a few hun-
dred, out of the 2, 700 possible ones. PM2gram
runs in almost a quarter of the time needed by the
basic PM. For the same set of names we obtained
the results reported in Table 2. The last column
indicates how many times the algorithm is slower
than the PM in its basic form.
algorithm time coverage times slower
basicED 132 days 99% 310
ED1 14 days 80% 35
PM 9 hours 99% 1
PM2gram 2 hours 42min 96% 0.26
Table 2: ED variants versus MP
4 Correcting Proper Names Misspellings
In this section we focus on a class of words which
do not occur in a priorly given dictionary and for
which the misspelled variants may not be random.
Proper names are representative for this class. For
example, the same Russian name occurs in corpus
as Berezovski, Berezovsky or Berezovschi because
of inaccurate transliteration. By convention, we
consider the most frequent form as the canonical
one, and all the other forms as misspelled variants.
Many times, the difference between a canonical
form and a misspelled variant follows a pattern: a
Pattern Context Example
dj?dji ovic djiukanovic djukanovic
k?kh aler kaler khaler, taler thaler
ki?ky ovsk berezovski berezovsky
n?ng chan chan-hee chang-hee
dl?del abd abdelkarim abdlkrim
Table 3: Name misspellings patterns
particular group of letters substitutes another one
in the context created by the other characters in
the name. A misspelling pattern specifies the con-
text, as prefix or suffix of a string, where a particu-
lar group of characters is a misspelling of another.
See Table 3 for examples of such patterns.
Finding and learning such patterns, along with
their probability of indicating a true misspelling,
bring an important gain to CDC systems both in
running time and in alleviating the data-sparseness
problem. The CDC system computes the prob-
ability of coreference for two mentions t and t?
using a similarity metrics into a vectorial space,
where vectors are made out of contextual features
occurring with t and t? respectively (Grishman,
1994). However, the information extracted from
documents is often too sparse to decide on coref-
erence (Popescu, 2009). Coreference has a global
effect, as the CDC systems generally improve the
coverage creating new vectors by interpolating the
information resulting from the documents which
were coreferred (Hastie et al., 2005). This infor-
mation is used to find further coreferences that no
single pair of documents would allow. Thus, miss-
ing a coreference pair may result in losing the pos-
sibility of realizing further coreferences. However,
for two mentions matching a misspelling pattern
which is highly accurate, the threshold for contex-
tual evidence is lowered. Thus, correcting a mis-
spelling is not beneficial for a single mention only,
but for the accuracy of the whole.
The strategy we adopt for finding patterns is
to work in a bootstrapping manner, enlarging the
valid patterns list while maintaining a high accu-
racy of the coreference, over 90%. Initially, we
start with an empty base of patterns. Considering
only the very high precision threshold for coref-
erence, above 98% certainty, we obtain a set of
misspelling pairs. This set is used to extract pat-
terns of misspellings via a parameter estimation
found using the EM-algorithm. The pattern is con-
sidered valid only if it also has more than a given
number of occurrences. The recursion of the pre-
vious steps is carried out by lowering with an ?
the threshold for accuracy of coreference for pat-
1638
tern candidates. The details and the pseudo code
are given below.
Algorithm 2 Misspelling Pattern Extraction
Require: thCoref , ?, minO, thAcc
Require: thCDC
Ensure: pattList
1: pattList, candPattList? ?
2: while there is a pair (t, t?) to test for coreference do
3: if (t, t?) matches p, p in pattList then
4: prob? corefProb(p)
5: else
6: use PM algorithm on pair (t, t?)
7: prob? thCoref
8: end if
9: if pair (t, t?) coref with prob then
10: candPattList? candPattList + (t, t?)
11: end if
12: extractPatterns from candPattList
13: for cp in new extracted patterns do
14: if #cp>minO and corefProb(cp)>thAcc then
15: pattList? pattList + (t, t?)
16: end if
17: end for
18: if prob>thCDC then
19: corefer (t, t?)
20: end if
21: end while
22: thCoref ? thCoref - ?
23: goto line 2
1. Compile a list of misspelling candidates
For each source string, t, try to match t against the
list of patterns (initially empty). If there is a pat-
tern matching (t, t?) then their prior probability of
coreference is the probability associated with that
pattern (line 4).
2. CDC coreference evidence For each pair (t
,t?) in the canonical candidates list use the CDC
system to compute the probability of coreference
between t and t?. If the probability of coreference
of t and t? is higher than thCoref , the default
value is 98%, then consider t as a misspelling of t?
and put (t, t?) in a candidate pattern list (line 10).
3. Extract misspelling patterns Find patterns
in the candidate pattern list. Consider only pat-
terns with more than minO occurrences, whose
default value is 10, and which have the probability
of coreference higher than thAcc, whose default
value is 90% (line 15).
4. CDC and pattern evidence For each (t,t?)
pair matching a pattern and the CDC probabil-
ity of coreference more then thCDC, whose de-
fault value is 80%, then corefer t and t? (line
21). The fact that the pair (t,t?) matches a pattern
of misspelling is considered supporting evidence
for coreference and in this way it plays a direct
role in enhancing the system coverage. Decrease
thCoref by ?,whose default is value 0.5, and re-
peat the process of finding patterns (goto line 2).
To extract the pattern from a given list of pairs,
procedure extractPatterns at line 12 above, we
generate all the suffixes and prefixes of the strings.
We compute the probability that a group of char-
acters represents a spelling error, given a certain
suffix and/or prefix. We use the EM algorithm to
compute these probabilities. For a pair (P, S) of
a prefix and a suffix, the tuples (p(P)=p, p(S)=s,
pi) are the quantities to be estimated via EM, with
pi being the coreference probability. A corefer-
ence event is directly observable, without know-
ing, however, which prefix or suffix contribute to
the coreference. The EM equations are given be-
low, where X is the observed data; Z are the hid-
den variable, p and s respectively; ? the parame-
ters (p,s, pi); Q(?,?
(t)
) the expected log likelihood
at iteration t.
E? step ?
(t)
i
?
(t)
i
= E[z
i
|x
i
, ?
(t)
]
=
p(x
i
|z
i
,?
(t)
) p(z
i
=P |?
(t)
)
p(x
i
|?
(t)
)
=
pi
(t)
[p
(t)
]
x
i
[(1?p
(t)
]
(1?x
i
)
pi
(t)
[p
(t)
]
x
i
[(1?p
(t)
]
(
1?x
i
)+(1?pi
(t)
)[s
(t)
]
x
i
[(1?s
(t)
]
(1?x
i
)
(1)
M? step ?
(t+1)
?Q(?|?
t)
?pi
= 0 pi
(t+1)
=
?
i
?
(t)
i
n
?Q(?|?
t)
?p
= 0 p
(t+1)
=
?
i
?
(t)
i
x
i
?
i
?
(t)
i
?Q(?|?
t)
?s
= 0 s
(t+1)
=
?
i
(1??
(t)
i
)x
i
?
i
(1??
(t)
i
)
(2)
5 Experiments
We performed a set of experiments on different
corpora in order to evaluate: (1) the performances
of the PM algorithm for misspelling detection, (2)
the accuracy of proper name misspelling pattern
acquisition from large corpora, and (3) the im-
provements of a CDC system, employing a cor-
rection module for proper name misspellings.
In Section 5.1 the accuracy of the PM algorithm
is tested on various corpora containing annotated
misspellings of English words. In particular, we
were interested to see the results when the edit dis-
tance between the misspelled pair is bigger than 3,
because handling bigger values for k is crucial for
finding misspelling errors produced by non-native
speakers. The evaluation is directly relevant for
the correction of the spelling of foreign names.
1639
In Section 5.2 the proper name misspelling pat-
terns were extracted from two large news cor-
pora. One corpus is part of the English Gigawords,
LDC2009T13 (Parker et al., 2009) and the sec-
ond corpus is Adige500k in Italian (Magnini et al.,
2006). We use a Named Entity Recognizer which
has an accuracy above 90% for proper names. We
evaluated the accuracy of the patterns by random
sampling.
In Section 5.3 the accuracy of the CDC system
with the correction module for proper name mis-
spellings was tested against a gold standard.
5.1 PM Evaluation
We consider the following publicly available En-
glish corpora containing the annotation of the mis-
spelled words: Birkbeck, Aspell, Holbrook, Wiki-
pidia. Birkbeck is a heterogeneous collection of
documents, so in the experiments below we re-
fer to each document separately. In particular we
distinguish between misspellings of native speak-
ers vs. misspelling of non-native speakers. Fig-
ure 2 shows that there are two types of corpora.
For the first type, the misspellings found within
two characters are between 80% and 100% of
the whole number of misspellings. For the sec-
ond type, less than 50% of the misspellings are
within two characters.The second category is rep-
resented by the misspellings of non native speak-
ers. The misspellings are far from the correct
forms and they represent chunks of phonetically
similar phonemes, like boiz vs. boys. The situa-
tion of the foreign name misspellings is likely to
be similar to the misspellings found in the sec-
ond type of corpora. For those cases, handling
a k value bigger than 2 is crucial. Not only the
Figure 2: k = 1, 2
non-indexing methods, but also indexing ones are
rather inefficient for k values bigger than 2 for
large corpora. The PM algorithm does not have
this drawback, and we tested the coverage of the
errors we found for values of k ranging from 3 to
10. In Figure 3 we plot the distributions for the
Figure 3: Foreign Misspellings
corpora which are problematic for k=2. Values of
k are plotted on the OX axis, and the percentage of
the misspellings within the respective k on the OY
axis. The results showed PM is also able to find
the phonemically similar misspellings. We can see
that for k bigger than 9 the number of misspellings
is not significant.
The PM algorithm performed very well, being
able to find the misspellings even for large k val-
ues. There were 47, 837 words in Aspell, Holbrrok
and Wikipedia, and 30, 671 in Birkbeck, and PM
found all the misspelling pairs in a running time of
25 minutes. This is a very competitive time, even
for indexing methods. For k above 8 the access to
the hash table containing the prime combinations
was slower, but not significantly so.
5.2 Pattern Extraction Evaluation
We extracted the set of names using a NER from
the two corpora, LDC2009T13 and Adige500k.
The set of proper names is rather large in both cor-
pora - 160, 869 names from the English corpus and
185, 508 from the Italian corpus. Apparently, the
quasi-similar names, which are considered as mis-
spelled name candidates, is very high. In Figure
4 we plot this data. The English Cand and Italian
Cand are absolute values, while the English True
and Italian True represent percentages. For exam-
ple, a name of length 5 is likely to have around 23
misspelling candidates, but only 17% of them are
likely to be true misspellings, the rest being differ-
ent names.
Figure 4: Candidates vs. True Misspellings
1640
The numbers are estimated considering samples
having the size between 30 and 50, for each name
length. The percentages change rapidly with the
length of the string. For names with the length
bigger than 11, the probability that a misspelling
candidate is a true misspelling is more than 98%.
This fact suggests a strategy for pattern extrac-
tion: start from the higher name length towards the
lower length names. The patterns found by the al-
gorithm described in Section 4 have between 900
and 20 occurrences. There are 12 patterns having
more than 400 occurrences, 20 having between 20
and 50 occurrences, see Fig. 5.
Figure 5: Distribution of the patterns:
5.3 CDC and Misspelling correction
The CRIPCO corpus (Bentivogli et al., 2008)
is a gold standard for CDC in Italian, contain-
ing pieces of news extracted from Adige500k.
There are 107 names, the majority being Ital-
ian names. We scrambled the names to cre-
ate misspelling candidates. For example the
name leonardo was scrambled like teonardo,
lionaldo, loenarod etc. We considered the top 15
frequency letters and maximum 4 letters for each
scrambling. We randomly selected 70% of the
original CRIPCO making no modifications, and
called this corpus CRwCR. 30% of the original
documents were assigned to the invented pseudo-
names, and we called this corpus CRwSC (cor-
rect documents with scrambled names). From
Adige500k we randomly chose 20, 000 documents
and assigned them to the scrambled names as
well, calling this corpus NCRwSC. From these
pieces we created a new corpus: 70% of the initial
CRIPCO documents with the original names, 30%
of the CRIPCO documents with scrambled names
and 20, 000 documents with the same scrambled
names. For the names occurring in CRwCR, the
scrambled names are valid name misspellings in
the CRwSC corpus, and invalid in NCRwSC.
As expected, the PM algorithm found all the
Figure 6: Proper Names CRIPCO Evaluation
misspelling candidates and some others as well.
We let the threshold confidence of coreference to
vary from 90% to 98%. The number in Figure
6 refers to the precision and recall for the name
misspellings in the CRIPCO corpus created via
random scrambling. We were also interested to
see how the pattern finding procedure works, but
scrambling randomly produced too many contexts.
Therefore, we chose to modify the names in a non
random way, by replacing the final o to ino, ex.
paolo goes to paolino, and modifying one letter in
the word for half of the occurrences, ex. paorino.
The idea is that ino is a very common suffix for
names in Italian. The system was able to learn the
pseudo alternatives created in the context ino. The
noise introduced was relatively low, see Fig. 6.
6 Conclusion and Further Research
In this paper we described a system able to correct
misspellings, including proper name misspellings,
fast and accurately. The algorithm introduced,
PM, overcomes the time/memory limitations of
the approaches based on the edit distance.
The system is built on a novel string compare
algorithm which runs in constant time indepen-
dently of the length of the names or the number of
different letters allowed, with no auxiliary mem-
ory request. As such, the algorithm is much faster
than any other non-indexing algorithms. Because
it is independent of k, it can be used even for large
k, where even the indexing methods have limita-
tions. We also used an EM based technique to find
misspelling patterns. The results obtained are very
accurate.
The system makes a first selection of the docu-
ments, drastically reducing the human work load.
Another line of future research is to use the PM
algorithm in other NLP tasks, where finding the
pairs having some particular elements in common
is necessary: for example, comparing parsing trees
or dependency trees. We think that PM can be
used in other NLP tasks as well and we hope the
community can take advantage of it.
1641
References
James Allan and Hema Raghavan. 2002. Using Part-
of-Speech Patterns to Reduce Query Ambiguity. In
Proceedings of the 25th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 307?314. ACM.
Youssef Bassil and Mohammad Alwani. 2012. OCR
Post-Processing Error Correction Algorithm Using
Google?s Online Spelling Suggestion. Journal of
Emerging Trends in Computing and Information Sci-
ences, ISSN 2079-8407, Vol. 3, No. 1.
Luisa Bentivogli, Christian Girardi, and Emanuele Pi-
anta. 2008. Creating a Gold Standard for Person
Cross-Document Coreference Resolution in Italian
News. In The Workshop Programme, page 19.
Leonid Boytsov. 2011. Indexing Methods for Approx-
imate Dictionary Searching: Comparative Analysis.
Journal of Experimental Algorithmics (JEA), 16:1?
1.
Martin Chodorow and Claudia Leacock. 2000. An Un-
supervised Method for Detecting Grammatical Er-
rors. In Proceedings of the 1st North American
chapter of the Association for Computational Lin-
guistics conference, pages 140?147. Association for
Computational Linguistics.
Fred J Damerau. 1964. A Technique for Computer
Detection and Correction of Spelling Errors. Com-
munications of the ACM, 7(3):171?176.
Ralph Grishman. 1994. Whither Written Language
Evaluation? In Proceedings of the workshop on Hu-
man Language Technology, pages 120?125. Associ-
ation for Computational Linguistics.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
and James Franklin. 2005. The Elements of Statis-
tical Learning: Data Mining, Inference and Predic-
tion. The Mathematical Intelligencer, 27(2):83?85.
M?ans Huld?en. 2009. Fast Approximate String
Matching with Finite Automata. Procesamiento del
lenguaje natural, 43:57?64.
Bernardo Magnini, Emanuele Pianta, Christian Girardi,
Matteo Negri, Lorenza Romano, Manuela Speranza,
Valentina Bartalesi Lenzi, and Rachele Sprugnoli.
2006. I-CAB: The Italian Content Annotation Bank.
In Proceedings of LREC, pages 963?968.
Stoyan Mihov and Klaus U Schulz. 2004. Fast Ap-
proximate Search in Large Dictionaries. Computa-
tional Linguistics, 30(4):451?477.
Ryo Nagata, Koichiro Morihiro, Atsuo Kawai, and
Naoki Isu. 2006. A Feedback-Augmented Method
for Detecting Errors in The Writing of Learners of
English. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 241?248. Association for
Computational Linguistics.
Gonzalo Navarro. 2001. A Guided Tour to Approx-
imate String Matching. ACM computing surveys
(CSUR), 33(1):31?88.
Kemal Oflazer. 1996. Error-tolerant Finite-state
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, 22(1):73?89.
Robert Parker, Linguistic Data Consortium, et al.
2009. English Gigaword Fourth Edition. Linguistic
Data Consortium.
Octavian Popescu. 2009. Person Cross Document
Coreference with Name Perplexity Estimates. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 997?1006. Association for Com-
putational Linguistics.
Martin Reynaert. 2004. Text Induced Spelling Cor-
rection. In Proceedings of the 20th international
conference on Computational Linguistics, page 834.
Association for Computational Linguistics.
Rub?en San Segundo, Javier Mac??as Guarasa, Javier
Ferreiros, P Martin, and Jos?e Manuel Pardo. 2001.
Detection of Recognition Errors and Out of the
Spelling Dictionary Names in a Spelled Name Rec-
ognizer for Spanish. In INTERSPEECH, pages
2553?2556.
Jean Veronis. 1988. Morphosyntactic Correction in
Natural Language Interfaces. In Proceedings of
the 12th conference on Computational linguistics-
Volume 2, pages 708?713. Association for Compu-
tational Linguistics.
1642
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 58?67,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Inducing Example-based Semantic Frames
from a Massive Amount of Verb Uses
Daisuke Kawahara
?
Daniel W. Peterson
?
Octavian Popescu
?
Martha Palmer
?
?
Kyoto University, Kyoto, Japan
?
University of Colorado at Boulder, Boulder, CO, USA
?
Fondazione Bruno Kessler, Trento, Italy
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu
Abstract
We present an unsupervised method for in-
ducing semantic frames from verb uses in
giga-word corpora. Our semantic frames
are verb-specific example-based frames
that are distinguished according to their
senses. We use the Chinese Restau-
rant Process to automatically induce these
frames from a massive amount of verb in-
stances. In our experiments, we acquire
broad-coverage semantic frames from two
giga-word corpora, the larger comprising
20 billion words. Our experimental results
indicate the effectiveness of our approach.
1 Introduction
Semantic frames are indispensable knowledge for
semantic analysis or text understanding. In the
last decade, semantic frames, such as FrameNet
(Baker et al., 1998) and PropBank (Palmer et al.,
2005), have been manually elaborated. These
resources are effectively exploited in many nat-
ural language processing (NLP) tasks, includ-
ing not only semantic parsing but also ma-
chine translation (Boas, 2002), information ex-
traction (Surdeanu et al., 2003), question answer-
ing (Narayanan and Harabagiu, 2004), paraphrase
acquisition (Ellsworth and Janin, 2007) and recog-
nition of textual entailment (Burchardt and Frank,
2006).
There have been many attempts to automati-
cally acquire frame knowledge from raw corpora
with the goal of either adding frequency informa-
tion to an existing resource or of inducing simi-
lar frames for other languages. Most of these ap-
proaches, however, focus on syntactic frames, i.e.,
subcategorization frames (e.g., (Manning, 1993;
Briscoe and Carroll, 1997; Korhonen et al., 2006;
Lippincott et al., 2012; Reichart and Korhonen,
2013)). Since subcategorization frames represent
argument patterns of verbs and are purely syn-
tactic, expressions that have the same subcatego-
rization frame can have different meanings (e.g.,
metaphors). Semantics-oriented NLP applications
based on frames, such as paraphrase acquisition
and machine translation, require consistency in the
meaning of each frame, and thus these subcatego-
rization frames are not suitable for these semantic
tasks.
Recently, there have been a few studies on au-
tomatically acquiring semantic frames (Materna,
2012; Materna, 2013). Materna induced seman-
tic frames (called LDA-Frames) from triples of
(subject, verb, object) in the British National
Corpus (BNC) based on Latent Dirichlet Allo-
cation (LDA) and the Dirichlet Process. LDA-
Frames capture limited linguistic phenomena of
these triples, and are defined across verbs based
on probabilistic topic distributions.
This paper presents a method for automati-
cally building verb-specific semantic frames from
a large raw corpus. Our semantic frames are verb-
specific like PropBank and semantically distin-
guished. A frame has several syntactic case slots,
each of which consists of words that are eligible to
fill the slot. For example, let us show three seman-
tic frames of the verb ?observe?:
1
observe:1
nsubj:{we, author, ...} dobj:{effect, result, ...}
prep in:{study, case, ...} ...
observe:2
nsubj:{teacher, we, ...} dobj:{child, student, ...}
prep in:{classroom, school, ...} ...
observe:3
nsubj:{child, people, ...} dobj:{bird, animal, ...}
prep at:{range, time, ...} ...
1
In this paper, we use the dependency relation names
of the Stanford collapsed dependencies (de Marneffe et al.,
2006) as the notations of case slots. For instance, ?nsubj?
means a nominal subject, ?dobj? means a direct object, ?iboj?
means an indirect object, ?ccomp? means a clausal comple-
ment and ?prep *? means a preposition.
58
Frequencies, which are not shown in the above ex-
amples, are attached to each semantic frame, case
slot and word, and can be effectively exploited for
the applications of these semantic frames. The fre-
quencies of words in each case slot become good
sources of selectional preferences.
Our novel contributions are summarized as fol-
lows:
? induction of semantic frames based on the
Chinese Restaurant Process (Aldous, 1985)
from only automatic parses of a web-scale
corpus,
? exploitation of the assumption of one sense
per collocation (Yarowsky, 1993) to make the
computation feasible,
? providing broad-coverage knowledge for se-
lectional preferences, and
? evaluating induced semantic frames by us-
ing an existing annotated corpus with verb
classes.
2 Related Work
The most closely related work to our semantic
frames are LDA-Frames, which are probabilistic
semantic frames automatically induced from a raw
corpus (Materna, 2012; Materna, 2013). He used a
model based on LDA and the Dirichlet Process to
cluster verb instances of a triple (subject, verb, ob-
ject) to produce semantic frames and slots. Both
of these are represented as a probabilistic distri-
bution of words across verbs. He applied this
method to the BNC and acquired 427 frames and
144 slots (Materna, 2013). These frames are over-
generalized across verbs and might be difficult
to provide with fine-grained selectional prefer-
ences. In addition, Grenager and Manning (2006)
proposed a method for inducing PropBank-style
frames from Stanford typed dependencies ex-
tracted from raw corpora. Although these frames
are based on typed dependencies and more seman-
tic than subcategorization frames, they are not dis-
tinguished in terms of the senses of words filling a
case slot.
There are hand-crafted semantic frames in the
lexicons of FrameNet (Baker et al., 1998) and
PropBank (Palmer et al., 2005). Corpus Pattern
Analysis (CPA) frames (Hanks, 2012) are another
manually created repository of patterns for verbs.
Each pattern represents a prototypical word usage
as extracted by lexicographers from the BNC. Cre-
ating CPA is time consuming, but our proposed
method may be employed to assist in the creation
of this type of resource, as shown in Section 4.4.
Our task can be regarded as clustering of verb
instances. In this respect, the models of Parisien
and Stevenson are related to our method (Parisien
and Stevenson, 2009; Parisien and Stevenson,
2010). Parisien and Stevenson (2009) proposed
a Dirichlet Process model for clustering usages
of the verb ?get.? Later, Parisien and Stevenson
(2010) proposed a Hierarchical Dirichlet Process
model for jointly clustering argument structures
(i.e., subcategorization frames) and verb classes.
However, their argument structures are not seman-
tic but syntactic, and also they did not evaluate the
resulting frames. There have also been related ap-
proaches to clustering verb types (Vlachos et al.,
2009; Sun and Korhonen, 2009; Falk et al., 2012;
Reichart and Korhonen, 2013). These methods in-
duce verb clusters in which multiple verbs partic-
ipate, and do not consider the polysemy of verbs.
Our objective is different from theirs.
Another line of related work is unsupervised
semantic parsing or semantic role labeling (Poon
and Domingos, 2009; Lang and Lapata, 2010;
Lang and Lapata, 2011a; Lang and Lapata, 2011b;
Titov and Klementiev, 2011; Titov and Klemen-
tiev, 2012). These approaches basically clus-
ter predicates and their arguments to distinguish
predicate senses and semantic roles of arguments.
Modi et al. (2012) extended the model of Titov and
Klementiev (2012) to jointly induce semantic roles
and frames using the Chinese Restaurant Process,
which is also used in our approach. However,
they did not aim at building a lexicon of semantic
frames, but at distinguishing verbs that have dif-
ferent senses in a relatively small annotated cor-
pus. Applying this method to a large corpus could
produce a frame lexicon, but its scalability would
be a big problem.
For other languages than English, Kawahara
and Kurohashi (2006a) proposed a method for au-
tomatically compiling Japanese semantic frames
from a large web corpus. They applied con-
ventional agglomerative clustering to predicate-
argument structures using word/frame similarity
based on a manually-crafted thesaurus. Since
Japanese is head-final and has case-marking post-
positions, it seems easier to build semantic frames
with it than with other languages such as English.
They also achieved an improvement in depen-
dency parsing and predicate-argument structure
59
analysis by using their resulting frames (Kawahara
and Kurohashi, 2006b).
3 Method for Inducing Semantic Frames
Our objective is to automatically induce verb-
specific example-based semantic frames. Each se-
mantic frame consists of a partial set of syntactic
slots: nsubj, dobj, iobj, ccomp and prep *. Each
slot consists of words with frequencies, which
could provide broad-coverage selectional prefer-
ences.
Frames for a verb should be semantically distin-
guished. That is to say, each frame should consist
of predicate-argument structures that have consis-
tent usages or meanings.
Our procedure to automatically generate seman-
tic frames from verb usages is as follows:
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based on
the assumption of one sense per collocation
to get a set of initial frames, and
3. apply clustering to the initial frames based
on the Chinese Restaurant Process to produce
the final semantic frames.
Each of these steps is described in the following
sections in detail.
3.1 Extracting Predicate-argument
Structures from a Raw Corpus
We first apply dependency parsing to a large raw
corpus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al., 2006).
2
Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep ?
Here, we do not distinguish adjuncts from argu-
ments. All extracted dependents of a verb are han-
dled as arguments. This distinction is left for fu-
ture work, but this will be performed using slot
2
http://nlp.stanford.edu/software/lex-parser.shtml
Sentences:
They observed the effects of ...
This statistical ability to observe an effect ...
We did not observe a residual effect of ...
He could observe the results at the same time ...
My first opportunity to observe the results of ...
You can observe beautiful birds ...
Children may then observe birds ...
.
.
.
Predicate-argument structures:
nsubj:they observe dobj:effect
observe dobj:effect
nsubj:we observe dobj:effect
nsubj:he observe dobj:result prep at:time
observe dobj:result
nsubj:you observe dobj:bird
nsubj:child observe dobj:bird
.
.
.
Initial frames:
nsubj:{they, we, ...} observe dobj:{effect}
nsubj:{he, ...} observe dobj:{result} prep at:{time}
nsubj:{you, child, ...} observe dobj:{bird}
.
.
.
Figure 1: Examples of predicate-argument struc-
tures and initial frames for the verb ?observe.?
frequencies in the applications of semantic frames
or the method proposed by Abend and Rappoport
(2010).
We apply the following processes to extracted
predicate-argument structures:
? A verb and an argument are lemmatized, and
only the head of an argument is preserved for
compound nouns.
? Phrasal verbs are also distinguished from
non-phrasal verbs. For example, ?look up?
has independent frames from ?look.?
? The passive voice of a verb is distinguished
from the active voice, and thus these have in-
dependent frames. Passive voice is detected
using the part-of-speech tag ?VBN? (past
participle). The alignment between frames of
active and passive voices will be done after
the induction of frames using the model of
Sasano et al. (2013) in the future.
? ?xcomp? (open clausal complement) is re-
named to ?ccomp? (clausal complement) and
?xsubj? (controlling subject) is renamed to
?nsubj? (nominal subject). This is because
60
these usages as predicate-argument structures
are not different.
? A capitalized argument with the part-of
speech ?NNP? (singular proper noun) or
?NNPS? (plural proper noun) is general-
ized to ?name?. Similarly, an argument of
?ccomp? is generalized to ?comp? since the
content of a clausal complement is not impor-
tant.
Extracted predicate-argument structures are
collected for each verb and the subsequent pro-
cesses are applied to the predicate-argument struc-
tures of each verb. Figure 1 shows examples of
predicate-argument structures for ?observe.?
3.2 Constructing Initial Frames from
Predicate-argument Structures
A straightforward way to produce semantic frames
is to cluster the extracted predicate-argument
structures directly. Since our objective is to com-
pile broad-coverage semantic frames, a massive
amount of predicate-argument structures should
be fed into the clustering. It would take prohibitive
computational costs to conduct the sampling pro-
cedure, which is described in the next section.
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:
3
dobj, ccomp, nsubj, prep ?, iobj.
This selection of a predominant argument order
above is justified by relative comparisons of the
discriminative power of the different slots for CPA
frames (Popescu, 2013). If a predicate-argument
structure does not have any of the above slots, it is
discarded.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
3
If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
word, e.g., ?dobj:effect?) are merged into an ini-
tial frame (Figure 1). After this process, we dis-
card minor initial frames that occur fewer than 10
times.
For example, we have 732,292 instances
(predicate-argument structures) for the verb ?ob-
serve? in the web corpus that is used in our exper-
iment (its details are described in Section 4.1). As
the result of this merging process, we obtain 6,530
initial frames, which become an input for the clus-
tering. This means that this process accelerates the
speed of clustering more than 100 times.
The precision of this process will be evaluated
in Section 4.3.
3.3 Clustering using Chinese Restaurant
Process
We cluster initial frames for each verb to produce
final semantic frames using the Chinese Restau-
rant Process (Aldous, 1985). We regard each ini-
tial frame as an instance in the usual clustering of
the Chinese Restaurant Process.
We calculate the posterior probability of a se-
mantic frame f
j
given an initial frame v
i
as fol-
lows:
P (f
j
|v
i
) ?
{
n(f
j
)
N+?
? P (v
i
|f
j
) f
j
?= new
?
N+?
? P (v
i
|f
j
) f
j
= new,
(1)
where N is the number of initial frames for the
target verb and n(f
j
) is the current number of ini-
tial frames assigned to the semantic frame f
j
. ?
is a hyper-parameter that determines how likely
it is for a new semantic frame to be created. In
this equation, the first term is the Dirichlet process
prior and the second term is the likelihood of v
i
.
P (v
i
|f
j
) is defined based on the Dirichlet-
Multinomial distribution as follows:
P (v
i
|f
j
) =
?
w?V
P (w|f
j
)
count(v
i
,w)
, (2)
where V is the vocabulary in all case slots cooc-
curring with the verb. It is distinguished by
the case slot, and thus consists of pairs of slots
and words, e.g., ?nsubj:child? and ?dobj:bird.?
count(v
i
, w) is the number of w in the initial
frame v
i
.
P (w|f
j
) is defined as follows:
P (w|f
j
) =
count(f
j
, w) + ?
?
t?V
count(f
j
, t) + |V | ? ?
, (3)
61
where count(f
j
, w) is the current number of w in
the frame f
j
, and ? is a hyper-parameter of Dirich-
let distribution. For a new semantic frame, this
probability is uniform (1/|V |).
We use Gibbs sampling to realize this cluster-
ing.
4 Experiments and Evaluations
4.1 Experimental Settings
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we first crawled the
web. We extracted sentences from each web
page that seems to be written in English based
on the encoding information. Then, we selected
sentences that consist of at most 40 words, and
removed duplicated sentences. From this pro-
cess, we obtained a corpus of one billion sen-
tences, totaling approximately 20 billion words.
We focused on verbs whose frequency was more
than 1,000. There were 19,649 verbs, includ-
ing phrasal verbs, and separating passive and ac-
tive constructions. We extracted 2,032,774,982
predicate-argument structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition)
to induce semantic frames. This corpus consists
of approximately 180 million sentences, which to-
taling four billion words. There were 7,356 verbs
after applying the same frequency threshold as the
web corpus. We extracted 423,778,278 predicate-
argument structures from this corpus.
We set the hyper-parameters ? in (1) and ? in
(3) to 1.0. The frame assignments for all the com-
ponents were initialized randomly. We took 100
samples for each initial frame and selected the
frame assignment that has the highest probability.
These parameters were determined according to a
preliminary experiment to manually examine the
quality of resulting frames.
4.2 Experimental Results
We executed the per-verb clustering tasks on a PC
cluster. It finished within a few hours for most
verbs, but it took a couple of days for very frequent
verbs, such as ?get? and ?say.? The clustering pro-
duced an average number of semantic frames per
verb of 15.2 for the web corpus and 18.5 for the
Gigaword corpus. Examples of induced semantic
frames from the web corpus are shown in Table 1.
slot instances
nsubj i:5850, we:5201, he:3796, you:3669, ...
dobj what:7091, people:2272, this:2262, ...
observe:1
prep in way:254, world:204, life:194, ...
.
.
.
nsubj we:11135, you:1321, i:1317, ...
dobj change:5091, difference:2719, ...
observe:2
prep in study:622, case:382, cell:362, ...
.
.
.
nsubj student:3921, i:2240, we:2174, ...
dobj child:2323, class:2184, student:2025, ...
observe:3
prep in classroom:555, action:509, ...
.
.
.
nsubj we:44833, i:6873, order:4051, ...
dobj card:28835, payment:22569, ...
accept:1
prep for payment:1166, convenience:1147, ...
.
.
.
nsubj i:10568, we:9300, you:5106, ...
dobj that:14180, this:12061, it:7756, ...
accept:2
prep as part:1879, fact:1085, truth:926, ...
.
.
.
nsubj people:7459, he:6696, we:5515, ...
dobj christ:13766, jesus:6528, it:5612, ...
accept:3
prep as savior:5591, lord:597, one:469, ...
.
.
.
Table 1: Examples of resulting frames for the verb
?observe? and ?accept? induced from the web cor-
pus. The number following an instance word rep-
resents its frequency.
4.3 Evaluation of Induced Semantic Frames
We evaluate precision and coverage of induced se-
mantic frames. To measure the precision of in-
duced semantic frames, we adopt the purity met-
ric, which is usually used to evaluate clustering re-
sults. However, the problem is that it is impossible
to assign gold-standard classes to the huge num-
ber of instances. To automatically measure the
purity of the induced semantic frames, we make
use of the SemLink corpus (Loper et al., 2007), in
which VerbNet classes (Kipper-Schuler, 2005) and
PropBank/FrameNet frames are assigned to each
instance. We make a test set that contains 157 pol-
ysemous verbs that occur 10 or more times in the
SemLink corpus (sections 02-21 of the Wall Street
Journal). We first add these instances to the in-
stances from a raw corpus and apply clustering to
these merged instances. Then, we compare the in-
duced semantic frames of the SemLink instances
with their gold-standard classes. We adopt Verb-
Net classes and PropBank frames as gold-standard
classes.
For each group of verb-specific semantic
frames, we measure the purity of the frames as the
percentage of SemLink instances belonging to the
majority gold class in their respective cluster. Let
62
PU CO F
1
Mac Mic Mac Mic Mac Mic
against One frame 0.799 0.802 0.917 0.952 0.854 0.870
VerbNet Initial frames 0.985 0.982 0.755 0.812 0.855 0.889
Induced sem frames 0.900 0.901 0.886 0.928 0.893 0.914
against One frame 0.901 0.872 ? ? 0.909 0.910
PropBank Initial frames 0.994 0.993 ? ? 0.858 0.893
Induced sem frames 0.965 0.949 ? ? 0.924 0.939
Table 2: Evaluation results of semantic frames from the web corpus against VerbNet classes and Prop-
Bank frames. ?Mac? means a macro average and ?Mic? means a micro average.
PU CO F
1
Mac Mic Mac Mic Mac Mic
against One frame 0.799 0.804 0.855 0.920 0.826 0.858
VerbNet Initial frames 0.985 0.981 0.666 0.758 0.795 0.855
Induced sem frames 0.916 0.909 0.796 0.880 0.852 0.894
against One frame 0.901 0.874 ? ? 0.877 0.896
PropBank Initial frames 0.994 0.993 ? ? 0.798 0.859
Induced sem frames 0.968 0.953 ? ? 0.874 0.915
Table 3: Evaluation results of semantic frames from the Gigaword corpus against VerbNet classes and
PropBank frames. ?Mac? means a macro average and ?Mic? means a micro average.
N denote the total number of SemLink instances
of the target verb, G
j
the set of instances belong-
ing to the j-th gold class and F
i
the set of instances
belonging to the i-th frame. The purity (PU) can
then be written as follows:
PU =
1
N
?
i
max
j
|G
j
? F
i
|. (4)
For example, a frame of the verb ?observe? con-
tains 11 SemLink instances, and eight out of them
belong to the class SAY-37.7, which is the ma-
jority class among these 11 instances. PU is cal-
culated by summing up such counts over all the
frames of this verb.
Usually, inverse purity or collocation is used
to measure the recall of normal clustering tasks.
However, these recall measures do not fit our task.
This is because it is not a real error to have similar
separate frames. Instead, we want to avoid hav-
ing so many frames that we cannot provide broad-
coverage selectional preferences due to sparsity.
To judge this aspect, we measure coverage.
The coverage (CO) measures to what extent
predicate-argument structures of the target verb in
a test set are included in one of frames of the verb.
We use the predicate-argument structures of the
above 157 verbs from the SemLink corpus, which
are the same ones used in the evaluation of PU.
We judge a predicate-argument structure as cor-
rect if all of its argument words (of the target slot
described in Section 3.1) are included in the corre-
sponding slot of a frame. If the clustering gets bet-
ter, the value of CO will get higher, because merg-
ing instances by clustering alleviates data sparsity.
These per-verb scores are aggregated into an
overall score by averaging over all verbs. We use
two ways of averaging: a macro average and a mi-
cro average. The macro average is a simple av-
erage of scores for individual verbs. The micro
average is obtained by weighting the scores for in-
dividual verbs proportional to the number of in-
stances for that verb. Finally, we use the harmonic
mean (F
1
) of purity and coverage as a single mea-
sure of clustering quality.
For comparison, we adopt the following two
baseline methods:
One frame a frame into which all the instances
for a verb are merged
Initial frames the initial frames without cluster-
ing (described in Section 3.2)
Table 2 and Table 3 list evaluation results for
semantic frames induced from the web corpus and
the Gigaword corpus, respectively.
4
Note that CO
does not consider gold-standard classes, and thus
the values of CO are the same for the VerbNet
4
We did not adopt inverse purity, but its values for the
induced semantic frames range from 0.42 to 0.49.
63
and PropBank evaluations. The induced frames
outperformed the two baseline methods in terms
of F
1
in most cases. While the coverage of the
web frames was higher than that of the Giga-
word frames, as expected, the purity of the web
frames was slightly lower than that of the Giga-
word frames. This degradation might be caused
by the noise in the web corpus.
The purity of the initial frames was around
98%-99%, which means that there were few cases
that the one-sense-per-collocation assumption was
violated.
Modi et al. (2012) reported a purity of 77.9%
for the assignment of FrameNet frames to the
FrameNet corpus. We also conducted the above
purity evaluation against FrameNet frames for 140
verbs.
5
We obtained a macro average of 92.9%
and a micro average of 89.2% for the web frames,
and a macro average of 93.2% and a micro average
of 89.8% for the Gigaword frames. It is difficult
to directly compare these results with Modi et al.
(2012), but our frame assignments seem to have
higher accuracy.
4.4 Evaluation against CPA Frames
Corpus Pattern Analysis (CPA) is a technique for
linking word usage to prototypical syntagmatic
patterns.
6
The resource was built manually by in-
vestigating examples in the BNC, and the set of
corpus examples used to induce each pattern is
given. For example, the following three patterns
describe the usage of the verb ?accommodate.?
[Human 1] accommodate [Human 2]
[Building] accommodate [Eventuality]
[Human] accommodate [Self] to [Eventuality]
In this paper, we use CPA to evaluate the quality
of the automatically induced frames. By compar-
ing the induced frames to CPA patterns, we can
evaluate the correctness and relevance of this ap-
proach from a human point of view. To do that,
we associate semantic features to the set of words
in each slot in the frames, using SUMO (Niles
and Pease, 2001). For example, take the follow-
ing frame for the verb ?accomplish?:
accomplish:1
nsubj:{you, leader, employee, ...}
dobj:{developing, progress, objective, ...}.
5
Since FrameNet frames are not assigned to all the verbs
of SemLink, the number of verbs is different from the evalu-
ations against VerbNet and PropBank.
6
http://deb.fi.muni.cz/pdev/
all K-means
Entropy (E) 0.790 0.516
Recovery Rate (RC) 0.347 0.630
Purity (P ) 0.462 0.696
Table 4: CPA Evaluation.
Using SUMO, we map this frame to the following:
nsubj: [Human]
dobj: [SubjectiveAssessmentAttribute],
which corresponds to pattern 3 for ?accomplish?
in CPA.
We also associate SUMO attributes to the CPA
patterns with more than 10 examples (716 verbs).
There are many patterns of SUMO attributes for
any CPA frame or induced frame, since each
filler word in a particular slot can have more
than one SUMO attribute. We filter out the
non-discriminative SUMO attributes following the
technique described in Popescu (2013). Using
this, we obtain SUMO attributes for both CPA
clusters and induced frames, and we can use the
standard entropy-based measures to evaluate the
match between the two types of patterns: E ? en-
tropy, RC ? recovery rate, and P ? purity (Li et
al., 2004):
E =
K
?
j=1
m
j
m
? e
j
, RC = 1 ?
K,L
?
j,i=1
p
ij
m
i
, (5)
P =
K
?
j=1
m
j
m
? p
j
, p
j
= max
i
p
ij
, (6)
e
j
=
L
?
i=1
p
ij
log
2
p
ij
, p
ij
=
m
ij
m
i
, (7)
where m
j
is the number of induced frames corre-
sponding to topic j, m
ij
is the number of induced
frames in cluster j and annotated with the CPA
pattern i, m is the total number of induced frames,
L is the number of CPA patterns, and K is the
number of induced frames.
We also consider a K-means clustering process,
with K set as 2 or 3 depending on the number of
SUMO-attributed patterns. The K-means evalu-
ation is carried out considering only the centroid
of the cluster, which corresponds to the prototypi-
cal induced semantic frame with SUMO attributes.
We compute E, RC and P using formulae (5) -
(7) for each verb and then compute the macro av-
erage, considering all the frames and only the K-
means centroids, respectively. The results for the
induced web frames are displayed in Table 4.
64
The evaluation method presented here over-
comes some of the drawbacks of the previous ap-
proaches (Materna, 2012; Materna, 2013). First,
we did not limit the evaluation to the most frequent
patterns. Second, the mapping was carried out au-
tomatically and not by hand. The results above
compare favorably with the previous approaches,
especially considering that no filtering procedures
were applied to the induced frames. We anticipate
that the results based on the prototypical induced
frames with SUMO attributes would be competi-
tive. Our post-analysis revealed that the entropy
can be lowered further if an automatic filtering
based on frequencies is applied.
4.5 Evaluation of the Quality of Selectional
Preferences
We also investigated the quality of selectional
preferences within the induced semantic frames.
The only publicly available test data for selectional
preferences, to our knowledge, is from Chambers
and Jurafsky (2010). This data consists of quadru-
ples (verb, relation, word, confounder) and does
not contain their context.
7
A typical way for using our semantic frames is
to select an appropriate frame for an input sen-
tence and judge the eligibility of the word uses
against the selected frame. However, due to the
lack of context for the above data, it is difficult to
select a corresponding semantic frame for a test
quadruple and thus the induced semantic frames
cannot be naturally applied to this data. To in-
vestigate the potential for selectional preferences
of the semantic frames, we approximately match
a quadruple with each of the semantic frames of
the verb and select the frame that has the highest
probability as follows:
P (w) = max
i
P (w|v, rel, f
i
), (8)
where w is the word or confounder, v is the verb,
rel is the relation and f
i
is a semantic frame. By
comparing the probabilities of the word and the
confounder, we select either of them according to
the higher probability. For tie breaking in the case
that no frames are found for the verb or both the
word and confounder are not found in the case slot,
we randomly select either of them in the same way
as Chambers and Jurafsky (2010).
We use the ?neighbor frequency? set, which is
the most difficult among the three sets included
7
A document ID of the English Gigaword corpus is avail-
able, but it is difficult to recover the context of each instance
from this information.
in the data. It contains 6,767 quadruples and the
relations consist of three classes: subject, object
and preposition, which has no distinction of ac-
tual prepositions. To link these relations with our
case slots, we manually aligned the subject with
the nsubj (nominal subject) slot, the object with
the dobj (direct object) slot and the preposition
with prep * (all the prepositions) slots. For the
preposition relation, we choose the highest prob-
ability among all the preposition slots in a frame.
To match the generalized ?name? with the word in
a quadruple, we change the word to ?name? if it is
capitalized and not a capitalized personal pronoun.
Our semantic frames from the Gigaword corpus
achieved an accuracy of 81.7%
8
and those from
the web corpus achieved an accuracy of 80.2%.
This slight deterioration seems to come from the
noise in the web corpus. The best performance
in Chambers and Jurafsky (2010) is 81.7% on
this ?neighbor frequency? set, which was achieved
by conditional probabilities with the Erk (2007)?s
smoothing method calculated from the English Gi-
gaword corpus. Our approach for selectional pref-
erences does not use smoothing like Erk (2007),
but it achieved equivalent performance to the pre-
vious work. If we applied our semantic frames to a
verb instance with its context, a more precise judg-
ment of selectional preferences would be possible
with appropriate frame selection.
5 Conclusion
This paper has described an unsupervised method
for inducing semantic frames from instances of
each verb in giga-word corpora. This method is
clustering based on the Chinese Restaurant Pro-
cess. The resulting frame data are open to the pub-
lic and also can be searched by inputting a verb via
our web interface.
9
As applications of the resulting frames, we plan
to integrate them into syntactic parsing, semantic
role labeling and verb sense disambiguation. For
instance, Kawahara and Kurohashi (2006b) im-
proved accuracy of dependency parsing based on
Japanese semantic frames automatically induced
from a large raw corpus. It is valuable and promis-
ing to apply our semantic frames to these NLP
tasks.
8
Since the dataset was created from the NYT 2001 portion
of the English Gigaword Corpus, we built semantic frames
again from the Gigaword corpus except this part.
9
http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
65
Acknowledgments
This work was supported by Kyoto University
John Mung Program and JST CREST. We grate-
fully acknowledge the support of the National Sci-
ence Foundation Grant NSF 1116782 - RI: Small:
A Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 226?236.
David Aldous. 1985. Exchangeability and related top-
ics.
?
Ecole d?
?
Et?e de Probabilit?es de Saint-Flour XIII
?1983, pages 1?198.
Collin Baker, Charles J. Fillmore, and John Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 86?90.
Hans C. Boas. 2002. Bilingual framenet dictionaries
for machine translation. In Proceedings of the 3rd
International Conference on Language Resources
and Evaluation, pages 1364?1371.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing, pages 356?363.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating textual entailment with LFG and FrameNet
frames. In Proceedings of the 2nd PASCAL Recog-
nizing Textual Entailment Workshop, pages 92?97.
Nathanael Chambers and Daniel Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 445?453.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Michael Ellsworth and Adam Janin. 2007. Mu-
taphrase: Paraphrasing with framenet. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 143?150.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216?223.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying french verbs using french and en-
glish lexical resources. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 854?863.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 1?
8.
Patrick Hanks. 2012. How people use words to make
meanings: Semantic types meet valencies. Input,
Process and Product: Developments in Teaching
and Language Corpora, pages 54?69.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation, pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176?183.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345?352.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 939?947.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1117?1126.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1320?1331.
Tao Li, Sheng Ma, and Mitsunori Ogihara. 2004.
Entropy-based criterion in categorical clustering. In
Proceedings of the 21st International Conference on
Machine Learning, volume 4, pages 536?543.
66
Thomas Lippincott, Anna Korhonen, and Diarmuid
?
O S?eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420?429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Christopher Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, pages 235?
242.
Ji?r?? Materna. 2012. LDA-Frames: An unsupervised
approach to generating semantic frames. In Alexan-
der Gelbukh, editor, Proceedings of the 13th Inter-
national Conference CICLing 2012, Part I, volume
7181 of Lecture Notes in Computer Science, pages
376?387. Springer Berlin / Heidelberg.
Ji?r?? Materna. 2013. Parameter estimation for LDA-
Frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482?486.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1?7.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693?701.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Proceedings of the International
Conference on Formal Ontology in Information Sys-
tems, pages 2?9.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Christopher Parisien and Suzanne Stevenson. 2009.
Modelling the acquisition of verb polysemy in chil-
dren. In Proceedings of the CogSci2009 Workshop
on Distributional Semantics beyond Concrete Con-
cepts, pages 17?22.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd annual
meeting of the Cognitive Science Society.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Octavian Popescu. 2013. Learning corpus patterns us-
ing finite state automata. In Proceedings of the 10th
International Conference on Computational Seman-
tics, pages 191?203.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862?872.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi,
and Manabu Okumura. 2013. Automatic knowl-
edge acquisition for case alternation between the
passive and active voices in Japanese. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1213?1223.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1445?1455.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
74?82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266?271.
67
Proceedings of NAACL HLT 2009: Short Papers, pages 153?156,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Name Perplexity  Octavian Popescu ?    ?? Abstract?
The accuracy of a Cross Document Corefer-ence system depends on the amount of context available, which is a parameter that varies greatly from corpora to corpora. This paper presents a statistical model for computing name perplexity classes. For each perplexity class, the prior probability of coreference is estimated. The amount of context required for coreference is controlled by the prior corefer-ence probability. We show that the prior prob-ability coreference is an important factor for maintaining a good balance between precision and recall for cross document coreference sys-tems. 1 Introduction?The Person Cross Document Coreference (PCDC) task which requires that all and only the textual mentions of an entity of type Person be individu-ated in a large collection of text documents, is a challenging tasks for natural language processing systems (Grishman 1994). A PCDC system must be able to use the information existing in the cor-pus in order to assign to each person name mention (PNM) a piece of context relevant for coreference. In many cases, the contextual information relevant for coreference is very scarce or embedded in se-mantic and ontological deep inferences, which are difficult to program, anyway.  Unlike in other disambiguation tasks, like word sense disambiguation for instance, where the dis-tribution of relevant contexts is mainly regulated by strong syntactic rules, in PCDC the relevance of contexts is a matter of interdependency. To exem-plify, consider the name ?John Smith? and an or-ganization, say ?U.N.?. The context ?works for U.N.? is a relevant coreference context for ?John Smith? if there is just one person named John 
Smith working for U.N.; if there are two or more John Smiths working for U.N., then  ?works for U.N.? is no longer a relevant context for corefer-ence. For the PCDC task, the relevance of the con-text depends to a great extent on the diversity of the corpus itself, rather than on the specific rela-tionship that exists between ?John Smith? and ?works for U.N.?.  Valid coreference can be realized when a large amount of information is available. However, the requirement that only contextually provable coreferences be realized is too strong; the required relevant context is not actually explicitly found in the text in at least 60% of the times (Popescu 2007).   This paper presents a statistical technique devel-oped to give a PCDC system more information regarding the probability of a correct coreference, without performing deep semantic and ontological analyses. If a PCDC system knows that the prior probability for two PNMs to corefer is high, then the amount of contextual evidence required can be lowered and vice-versa. Our goal is to precisely define a statistical model in which the prior coreference probabilities can be computed and, consequently, to design a PCDC system that dy-namically revises the context relevance accord-ingly. We review the PCDC literature relevant for our purposes, present the statistical model and show the preliminary results. The paper ends with the Conclusion and Further Research section. 2 Related?Work??In a classical paper (Bagga 1998), a PCDC system based on the vector space model (VSM) is pro-posed. While there are many advantages in repre-senting the context as vectors on which a similarity function is applied, it has been shown that there are 
153
inherent limitations associated with the vectorial model (Popescu 2008). These problems, related to the density in the vectorial space (superposition) and to the discriminative power of the similarity power (masking), become visible as more cases are considered. (Gooi, 2004), testing the system on many names, empirically observes the variance in the results obtained by the same PCDC system. Indeed, considering just the sentence level context, which is a strong requirement for establishing coreference, a PCDC system obtains a good score for ?John Smith?. This is because the probability of coreference of any two ?John Smith? mentions is low. But, as the relevant context is often outside the sentence containing the mention, for other types of names the same system is not accurate. If it considers, for instance, ?Barack Obama?, the same system obtains a very low recall, as the prob-ability of any two ?Barack Obama? mentions to corefer is very high. Without further adjustments, a vectorial model cannot resolve the problem of con-sidering too much or too little contextual evidence in order to obtain a good precision for ?John Smith? and simultaneously a good recall for ?Ba-rack Obama?.   The relationship between the prior probabilities and the accuracy of a system is also empirically noted in (Pederson 2005). In their experiment, the authors note that having in the input of the system the correct number of persons carrying the same name is likely to hurt the results of a system based on bigrams. This happens because the amount of context is statically considered. The variance in the results obtained by a PCDC system has been noted also in (Lefever 2007, Popescu 2007).  In order to improve the performances of PCDC systems based on VSM, some authors have fo-cused on methods that allow a better analysis of the context (Ng 2007) combined with a cascade clustering technique (Wei 2006), or have relied on advanced clustering techniques (Chen 2006).  The technique we present in the next section is complementary to these approaches. We propose a statistical model designed to offer to the PCDC systems information regarding the distribution of PNMs in the corpus. This information is used to reduce the contextual data variation and to attain a good balance between precision and recall. 
3 Name Perplexity Classes  The amount of contextual information required for the coreference of two or more PNMs depends on several factors. Our working hypothesis is that we can compute a prior probability of coreference for each name and use this probability to control the amount of contextual evidence required. Let us recall the ?John Smith? and ?Barack Obama? ex-ample from the previous section. Both ?John? and ?Smith? are American common first and last names. The chance that many different persons carry this name is high. On the other hand, as both ?Barack? and ?Obama? are rare American first and last names respectively, almost surely many men-tions of this name refer only to one person. The argument above does not depend on the context, but just on the prior estimation of the usage of those names. Computing an estimation of a name?s frequency class, we may decrease or increase the amount of contextual evidence needed accordingly.   To each one-token name we associate the num-ber of different tokens with which it forms a PNM in the corpus. For example, for ?John? we can have the set ?Smith?, ?F. Kennedy?, ?Travolta? etc. We call this number the perplexity of a one-token name. The perplexity gives a direct estimation of the ambiguity of a name in the corpus. In Table 1 we present the relationship between the number of occurrences (in intervals, in the first column) and the average perplexity (second column). The fig-ures reported here, as well as those in the next Sec-tion, come from the investigation of the Adige500k, an Italian news corpus (Magnini 2006).  occurrences (interval) average perplexity 1-5 4.13 6-20 8.34 21-100 17.44 101-1,000 68.54 1,000-5,000 683.95 5,000-31,091 478.23 Table 1. Average perplexity one-token names We divide the class of one-token names in 5 categories according to their perplexity: very low, low, medium, high and very high. It is useful to keep separate the first and the last names. It has been shown that the average perplexity is three times lower for last names than for first names 
154
(Popescu 2007). Therefore, the first and last names perplexities play different roles in establishing the prior probability of coreference. The perplexity class of two-token names is computed using the following heuristics: the perplexity class of two-token names is the average class of the perplexity of the one-token names composing it. If the per-plexity classes of the one-token names are the same, then the perplexity of the whole name is one class less (if possible). The perplexity classes represent a partition of the name population; each name belongs to one and only one class. In establishing the border be-tween two consecutive perplexity classes, we want to maximize the confidence that inside each stra-tum the prior coreference probability has a low variance.  The relationship between the perplexity classes and the prior coreference probability is straight-forward. The lower the perplexity, the greater the coreference probability, and, therefore, the lower the amount of relevant context required for coreference.  In order to decide the percentage of the name population that goes into each of the perplexity classes, we use a distributional free statistics method. In this way we can compute the confi-dence of the prior conference probability estimates.  We introduce two random variables: X, a ran-dom variable defined over the name population and Y, which represents the number of different persons carrying the same name. Let X1,?,Xn be a random sample of names from one perplexity class, and let Y1,?,Yn be the corresponding values denoting the number of persons that carry the names X1,?,Xn. The indices have been chosen such that Y1?,Yn is an ordered statistics: Y1?Y2???Yn. Let F be the distribution function of Y. And let p be a given probability. If F(Yj)-F(Yi) ? p, then at least 100p percent of the probability distribution is between Yi and Yj; it means that  ? = P[F(Yj)-F(Yi)] ? p                  (1) is the probability that the interval (Yi, Yj) contains 100p percent of the Y values.  In our case, ? is the confidence of the estimation that 100p percent of names from a certain perplex-ity class have the expected prior coreference prob-ability in a given interval. 
The ? probability is computed with the formula: ? = P(F(Yj) ? F(Yi) < p) =    1-?0p  ?(n+1)/( ?(j-i)) ?(n-j+i+1)xj-i-1(1-x)n-j+idx   (2) where ? is the extension of the factorial function, ?(x) = ?0?  tx-1e-tdt.  In practice, we start with an interval that repre-sents the prior coreference probability desired for that perplexity class. For example we want to be ? = 80% sure that p = 90% of the two-token names in the ?very low? perplexity class are names car-ried by a maximum of 2 persons. We choose a ran-dom sample of two-token names from that perplexity class, the size of the random sample be-ing determined by ? and p ? see equation (2). If the random sample satisfies (1) then we have the de-sired perplexity class. If not, the one-token names that have the highest perplexity and were consid-ered ?very low? are excluded ? they are assigned to the next perplexity class - and the computation is re made.  In a preliminary experiment, using a sample of 25 two-token names from a part of the Adige500k corpus spanning two years, we have obtained the perplexity classes listed in Tables 2 and 3. In Adige 500k there are 106, 192 different one-token names, which combine into 429, 251 different two-token names and 36, 773 three-token names. perplexity class percentage very high 5.3% High 8.7% Medium 20.9% Low 27.6% very low 37.5% Table 2. First Name perplexity classes perplexity class percentage very high 1.8% High 3.36% Medium 17.51% Low 20.31% very low 57.02% Table 3. Last Name perplexity classes The perplexity class of two-token names is computed as specified in the first paragraph of this page. In approximately 60% of the cases, a two-token name has a ?low?, or ?very low? perplexity class. If a PCDC system computes the context 
155
similarity based on words with special properties or on named entities, in general at least four simi-larities must be detected between two contexts in order to have a safe coreference. Our preliminary results show that coreferring on the basis of just one special word and one named entity for those names in ?low? or ?very low? does not lose more than 1,5% in precision, while it gains up to 40% in recall for these cases. On the other hand, for ?very high? perplexity two-token names we were able to increase precision by requiring a stronger similar-ity between contexts.  The gain of using prior coreference probabilities determined by the perplexity classes is important, especially for those names that are situated at the extreme: ?very low? perplexity with a big number of occurrences and ?very high? with a small num-ber of occurrences. These cases establish the inter-val for the amount of contextual similarity required for coreference. However, the problematic cases remain when the perplexity class is ?very high? and the number of occurrences is very big.  4 Conclusion?and?Further?Research??We have presented a distributional free statistical method to design a name perplexity system, such that each perplexity class maximizes the number of names for which the prior coreference belongs to the same interval. This information helps the PCDC systems to lower/increase adequately the amount of contextual evidence required for coreference. In our preliminary experiment we have observed that we can adequately reduce the amount of con-textual evidence required for the coreference of ?low? and ?very low? perplexity class. For the top perplexity class names the requirement for extra contextual evidence has increased the precision.  The approach presented here is effective in deal-ing with the problems raised by using a similarity metrics on contextual vectors. It gives a direct way of identifying the most problematic cases for coreference. Solving these cases represents our first objective for the future. We plan to increase the number of cases consid-ered in the sample required to delimit the perplex-ity classes. The equation (2) may be developed further in order to obtain exactly the number of required cases for each perplexity class. 
References??A. Bagga, B. Baldwin.1998. Entity-based Cross-Document Co-referencing using the Vector Space Model, In Proceedings ACL. J.? Chen,? D.? Ji,? C.? Tan,? Z.? Niu.2006.?Unsupervised?Relation?Disambiguation?Using? Spectral? Cluster?ing,?In?Proceedings?of?COLING C. Gooi, J. Allan.2004. Cross-Document Corefer-ence on a Large Scale Corpus, in Proceeding ACL.  R. Grishman.1994. Whither Written Language Evaluation? In proceedings Human Language Technology Workshop, 120-125. San Mateor. E. Lefever, V. Hoste, F. Timur.2007. AUG: A Combined Classification and Clustering Approach for Web People Disambiguation, In Proceedings of SemEval B. Magnini, M. Speranza, M. Negri, L. Romano, R. Sprugnoli. 2006.I-CAB ? the Italian Content Annotation Bank. LREC 2006 V.,?Ng.2007.?Shallow?Semantics?for?Coreference?Resolution,?In?Proceedings?of?IJCAI T. Pedersen, A. Purandare, A. Kulkarni. 2005. Name Discrimination by Clustering Similar Con-texts, in Proceeding of CICLING O. Popescu, C. Girardi, 2008, Improving Cross Document Coreference, in Proceedings of JADT O. Popescu, B. Magnini.2007, Inferring Corefer-ence among Person Names in a Large Corpus of News Collection, in Proceedings of AIIA O. Popescu, B. Magnini.2007. Irst-bp: WePS using Named Entities, In Proceedings of SEMEVAL O. Popescu, M. Magnini, L. Serafini, A. Tamilin, M. Speranza.2006. From Mention to Ontology: a Pilot Study, in Proceedings of SWAP Y.?Wei,?M.?Lin,?H.?Chen.2006.?Name?Disambigua?tion? in? Person? Information? Mining,? In? Proceed?ings?of?IEEE? 
156
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 284?288,
Dublin, Ireland, August 23-24, 2014.
FBK-TR: Applying SVM with Multiple Linguistic Features for
Cross-Level Semantic Similarity
Ngoc Phuoc An Vo
Fondazione Bruno Kessler
University of Trento
Trento, Italy
ngoc@fbk.eu
Tommaso Caselli
TrentoRISE
Trento, Italy
t.caselli@trentorise.eu
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Abstract
Recently, the task of measuring seman-
tic similarity between given texts has
drawn much attention from the Natural
Language Processing community. Espe-
cially, the task becomes more interesting
when it comes to measuring the seman-
tic similarity between different-sized texts,
e.g paragraph-sentence, sentence-phrase,
phrase-word, etc. In this paper, we, the
FBK-TR team, describe our system par-
ticipating in Task 3 "Cross-Level Seman-
tic Similarity", at SemEval 2014. We also
report the results obtained by our system,
compared to the baseline and other partic-
ipating systems in this task.
1 Introduction
Measuring semantic text similarity has become a
hot trend in NLP as it can be applied to other
tasks, e.g. Information Retrieval, Paraphrasing,
Machine Translation Evaluation, Text Summariza-
tion, Question and Answering, and others. Several
approaches proposed to measure the semantic sim-
ilarity between given texts. The first approach is
based on vector space models (VSMs) (Meadow,
1992). A VSM transforms given texts into "bag-
of-words" and presents them as vectors. Then, it
deploys different distance metrics to compute the
closeness between vectors, which will return as
the distance or similarity between given texts. The
next well-known approach is using text-alignment.
By assuming that two given texts are semantically
similar, they could be aligned on word or phrase
levels. The alignment quality can serve as a simi-
larity measure. "It typically pairs words from the
two texts by maximizing the summation of the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
word similarity of the resulting pairs" (Mihalcea
et al., 2006). In contrast, the third approach uses
machine learning techniques to learn models built
from different lexical, semantic and syntactic fea-
tures and then give predictions on degree of simi-
larity between given texts (?ari
?
c et al., 2012).
At SemEval 2014, the Task 3 "Cross-Level Se-
mantic Similarity" (Jurgens et al., 2014) is to eval-
uate the semantic similarity across different sizes
of texts, in particular, a larger-sized text is com-
pared to a smaller-sized one. The task consists
of four types of semantic similarity comparison:
paragraph to sentence, sentence to phrase, phrase
to word, and word to sense. The degree of similar-
ity ranges from 0 (different meanings) to 4 (simi-
lar meanings). For evaluation, systems were eval-
uated, first, within comparison type and second,
across all comparison types. Two methods are
used to evaluate between system outputs and gold
standard (human annotation), which are Pearson
correlation and Spearman?s rank correlation (rho).
The FBK-TR team participated in this task with
three different runs. In this paper, we present a
clear and comprehensive description of our sys-
tem which obtained competitive results. Our main
approach is using machine learning technique to
learn models from different lexical and semantic
features from train corpora to make prediction on
the test corpora. We used support vector machine
(SVM) regression model to solve the task.
The remainder of the paper is organized as fol-
lows. Section 2 presents the system overview.
Sections 3, 4 and 5 describe the Semantic Word
Similarity, String Similarity and other features, re-
spectively. Section 6 discusses about SVM ap-
proach. Section 7 presents the experiment settings
for each subtask. Finally, Sections 8 and 9 present
the evaluation and conclusion.
284
Figure 1: System Overview.
2 System Overview
Our system was built on different linguistic fea-
tures as shown in Figure 1. By constructing a
pipeline system, each linguistic feature can be
used independently or together with others to mea-
sure the semantic similarity of given texts as well
as to evaluate the significance of each feature to
the accuracy of system?s predictions. On top of
this, the system is expandable and scalable for
adopting more useful features aiming for improv-
ing the accuracy.
3 Semantic Word Similarity Measures
At the lexical level, we built a simple, yet effec-
tive Semantic Word Similarity model consisting of
three components: WordNet similarity, Wikipedia
relatedness and Latent Semantic Analysis (LSA).
These components played important and compli-
mentary roles to each other.
3.1 Data Processing
We used the TreeTagger tool (Schmid, 1994) to
extract Part-of-Speech (POS) from each given
text, then tokenize and lemmatize it. On the basis
of the POS tags, we only picked lemmas of con-
tent words (Nouns and Verbs) from the given texts
and then paired them up regarding to similar POS
tags.
3.2 WordNet Similarity and Levenshtein
Distance
WordNet (Fellbaum, 1999) is a lexical database
for the English language in which words are
grouped into sets of synonyms (namely synsets,
each expressing a distinct concept) to provide
short, general definitions, and record the vari-
ous semantic relations between synsets. We used
Perdersen?s package WordNet:Similarity (Peder-
sen et al., 2004) to obtain similarity scores for
the lexical items covered in WordNet. Similarity
scores have been computed by means of the Lin
measure (Lin, 1998). The Lin measure is built on
Resnik?s measure of similarity (Resnik, 1995):
Sim
lin
=
2 ? IC(LCS)
IC(concept
1
) + IC(concept
2
)
(1)
where IC(LCS) is the information content (IC) of
the least common subsumer (LCS) of two con-
cepts.
To overcome the limit in coverage of WordNet,
we applied the Levenshtein distance (Levenshtein,
1966). The distance between two words is defined
by the minimum number of operations (insertions,
deletions and substitutions) needed to transform
one word into the other.
3.3 Wikipedia Relatedness
Wikipedia Miner (Milne and Witten, 2013) is a
Java-based package developed for extracting se-
mantic information from Wikipedia. Through our
experiments, we observed that Wikipedia related-
ness plays an important role for providing extra
information to measure the semantic similarity be-
tween words. We used the package Wikipedia
Miner from University of Waikato (New Zealand)
to extract additional relatedness scores between
words.
3.4 Latent Semantic Analysis (LSA)
We also took advantage from corpus-based ap-
proaches to measure the semantic similarity be-
tween words by using Latent Semantic Analysis
(LSA) technique (Landauer et al., 1998). LSA as-
sumes that similar and/or related words in terms
of meaning will occur in similar text contexts. In
general, a LSA matrix is built from a large cor-
pus. Rows in the matrix represent unique words
and columns represent paragraphs or documents.
The content of the matrix corresponds to the word
count per paragraph/document. Matrix size is then
reduced by means of Single Value Decomposition
(SVD) technique. Once the matrix has been ob-
tained, similarity and/or relatedness between the
words is computed by means of cosine values
(scaled between 0 and 1) for each word vector
in the matrix. Values close to 1 are assumed to
285
be very similar/related, otherwise dissimilar. We
trained our LSA model on the British National
Corpus (BNC)
1
and Wikipedia
2
corpora.
4 String Similarity Measures
The Longest Common Substring (LCS) is the
longest string in common between two or more
strings. Two given texts are considered similar if
they are overlapping/covering each other (e.g sen-
tence 1 covers a part of sentence 2, or otherwise).
We implemented a simple algorithm to extract the
LCS between two given texts. Then we divided the
LCS length by the product of normalized lengths
of two given texts and used it as a feature.
4.1 Analysis Before and After LCS
After extracting the LCS between two given texts,
we also considered the similarity for the parts be-
fore and after the LCS. The similarity between the
text portions before and after the LSC has been ob-
tained by means of the Lin measure and the Lev-
enshtein distance.
5 Other Features
To take into account other levels of analysis for se-
mantic similarity between texts, we extended our
features by means of topic modeling and Named
Entities.
5.1 Topic Modeling (Latent Dirichlet
Allocation - LDA)
Topic modeling is a generative model of docu-
ments which allows to discover topics embedded
in a document collection and their balance in each
document. If two given texts are expressing the
same topic, they should be considered highly sim-
ilar. We applied topic modeling, particularly, La-
tent Dirichlet allocation (LDA) (Blei et al., 2003)
to predict the topics expressed by given texts.
The MALLET topic model package (McCal-
lum, 2002) is a Java-based tool used for inferring
hidden "topics" in new document collections us-
ing trained models. We used Mallet topic model-
ing tool to build different models using BNC and
Wikipedia corpora.
We noticed that, in LDA, the number of top-
ics plays an important role to fine grained predic-
tions. Hence, we built different models for differ-
ent numbers of topics, from minimum 20 topics to
1
http://www.natcorp.ox.ac.uk
2
http://en.wikipedia.org/wiki/Wikipedia:Database_download
maximum 500 topics (20, 50, 100, 150, 200, 250,
300, 350, 400, 450 and 500). From the proportion
vectors (distribution of documents over topics) of
given texts, we applied three different measures to
compute the distance between each pair of texts,
which are Cosine similarity, Kullback-Leibler and
Jensen-Shannon divergences (Gella et al., 2013).
5.2 Named-Entity Recognition (NER)
NER aims at identifying and classifying entities
in a text with respect to a predefined set of cate-
gories such as person names, organizations, loca-
tions, time expressions, quantities, monetary val-
ues, percentages, etc. By exploring the training
set, we observed that there are lot of texts in this
task containing named entities. We deployed the
Stanford Named Entity Recognizer tool (Finkel et
al., 2005) to extract the similar and overlapping
named entities between two given texts. Then we
divided the number of similar/overlapping named
entities by the sum length of two given texts.
6 Support Vector Machines (SVMs)
Support vector machine (SVM) (Cortes and Vap-
nik, 1995) is a type of supervised learning ap-
proaches. We used the LibSVM package (Chang
and Lin, 2011) to learn models from the different
linguistic features described above. However, in
SVM the problem of finding optimal kernel pa-
rameters is critical and important for the learning
process. Hence, we used practical advice (Hsu et
al., 2003) for data scaling and a grid-search pro-
cess for finding the optimal parameters (C and
gamma) for building models. We trained the SVM
models in a regression framework.
7 Experiment Settings
For subtasks paragraph-to-sentence and sentence-
to-phrase, since the length between two units is
completely different, we decided, first to apply
topic model to identify if two given texts are ex-
pressing a same topic. Furthermore, named enti-
ties play an important role in these subtasks. How-
ever, as there are many named entities which are
not English words and cannot be identified by the
NER tool, we developed a program to detect and
identify common words occurring in both given
texts. Then we continued to extract other lexical
and semantic features to measure the similarity be-
tween the two texts.
286
Team Para2Sent Para2Sent
(Pearson) (Spearman)
UNAL-NLP, run2 (ranked 1st) 0.837 0.820
ECNU, run1(ranked 1st) 0.834 0.821
FBK-TR, run2 0.77 0.775
FBK-TR, run3 0.759 0.770
FBK-TR, run1 0.751 0.759
Baseline (LCS) 0.527 0.613
Table 1: Results for paragraph-to-sentence.
Team Sent2Phr Sent2Phr
(Pearson) (Spearman)
Meerkat_Mafia, 0.777 0.760
SuperSaiyan (ranked 1st)
FBK-TR, run3 0.702 0.695
FBK-TR, run1 0.685 0.681
FBK-TR, run2 0.648 0.642
Baseline (LCS) 0.562 0.626
Table 2: Results for sentence-to-phrase.
For the subtask word-to-sense, we used the Se-
mantic Word Similarity model which consists of
three components: WordNet similarity, Wikipedia
relatedness and LSA similarity (described in sec-
tion 3). For phrase-to-word, we extracted all
glosses of the given word, then computed the simi-
larity between the given phrase and each extracted
gloss. Finally, we selected the highest similarity
score for result.
8 Evaluations
As a result, we report our performance in the four
subtasks as follows.
8.1 Subtasks: Paragraph-to-Sentence and
Sentence-to-Phrase
The evaluation results using Pearson and Spear-
man correlations show the difference between our
system and best system in these two subtasks in
the Tables 1 and 2.
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.811 0.742 0.415 0.356 2.324
(ranked 1st)
FBK-TR 0.759 0.702 0.305 0.155 1.95
Baseline 0.527 0.562 0.165 0.109 1.363
Table 3: Overall result using Pearson.
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.801 0.728 0.424 0.344 2.297
(ranked 1st)
FBK-TR 0.770 0.695 0.298 0.150 1.913
Baseline 0.613 0.626 0.162 0.130 1.528
Table 4: Overall result using Spearman.
8.2 Subtasks: Phrase-to-Word and
Word-to-Sense
Even though we did not submit the results as
they looked very low, we report the scores for
the phrase-to-word and word-to-sense subtasks. In
the phrase-to-word subtask, we obtained a Pearson
score of 0.305 and Spearman value of 0.298. As
for the word-to-sense subtask, we scored 0.155 for
Pearson and 0.150 for Spearman.
Overall, with the submitted results for two sub-
tasks described in Section 8.1, our system?s runs
ranked 20th, 21st and 22nd among 38 participat-
ing systems. However, by taking into account the
un-submitted results for the two other subtasks,
our best run obtained 1.95 (Pearson correlation)
and 1.913 (Spearman correlation), which can be
ranked in the top 10 among 38 systems (figures
are reported in Table 3 and 4).
9 Conclusions and Future Work
In this paper, we describe our system participating
in the Task 3, at SemEval 2014. We present a com-
pact system using machine learning approach (par-
ticularly, SVMs) to learn models from a set of lex-
ical and semantic features to predict the degree of
similarity between different-sized texts. Although
we only submitted the results for two out of four
subtasks, we obtained competitive results among
the other participants. For future work, we are
planning to increase the number of topics in LDA,
as more fine-grained topics should allow predict-
ing better similarity scores. Finally, we will inves-
tigate more on the use of syntactic features.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
287
Vector Networks. Machine learning, 20(3):273?
297.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370.
Spandana Gella, Bahar Salehi, Marco Lui, Karl
Grieser, Paul Cook, and Timothy Baldwin. 2013.
Unimelb_nlp-core: Integrating predictions from
multiple domains and feature sets for estimating se-
mantic textual similarity. Atlanta, Georgia, USA,
page 207.
Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.
2003. A Practical Guide to Support Vector Classifi-
cation.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014) August 23-24, 2014, Dublin,
Ireland.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259?284.
Vladimir I Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions and Reversals.
In Soviet physics doklady, volume 10, page 707.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In ICML, volume 98, pages 296?
304.
Andrew Kachites McCallum. 2002. Mallet: A Ma-
chine Learning for Language Toolkit.
Charles T Meadow. 1992. Text Information Retrieval
Systems. Academic Press, Inc., Orlando, FL, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In AAAI, vol-
ume 6, pages 775?780.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222?239.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - Measuring the Re-
latedness of Concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38?41.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. arXiv
preprint cmp-lg/9511007.
Frane ?ari?c, Goran Glava?, Mladen Karan, Jan ?najder,
and Bojana Dalbelo Ba?i?c. 2012. Takelab: Sys-
tems for Measuring Semantic Text Similarity. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 441?448.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44?49. Manch-
ester, UK.
288
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 289?293,
Dublin, Ireland, August 23-24, 2014.
FBK-TR: SVM for Semantic Relatedness and Corpus Patterns for RTE
Ngoc Phuoc An Vo
Fondazione Bruno Kessler
University of Trento
Trento, Italy
ngoc@fbk.eu
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Tommaso Caselli
TrentoRISE
Trento, Italy
t.caselli@trentorise.eu
Abstract
This paper reports the description and
scores of our system, FBK-TR, which
participated at the SemEval 2014 task
#1 "Evaluation of Compositional Distribu-
tional Semantic Models on Full Sentences
through Semantic Relatedness and Entail-
ment". The system consists of two parts:
one for computing semantic relatedness,
based on SVM, and the other for identi-
fying the entailment values on the basis
of both semantic relatedness scores and
entailment patterns based on verb-specific
semantic frames. The system ranked 11
th
on both tasks with competitive results.
1 Introduction
In the Natural Language Processing community,
meaning related tasks have gained an increasing
popularity. These tasks focus, in general, on a
couple of short pieces of text, like pair of sen-
tences, and the systems are required to infer a cer-
tain meaning relationship that exists between these
texts. Two of the most popular meaning related
tasks are the identification of Semantic Text Sim-
ilarity (STS) and Recognizing Textual Entailment
(RTE). The STS tasks require to identify the de-
gree of similarity (or relatedness) that exists be-
tween two text fragments (sentences, paragraphs,
. . . ), where similarity is a broad concept and its
value is normally obtained by averaging the opin-
ion of several annotators. The RTE task requires
the identification of a directional relation between
a pair of text fragments, namely a text (T) and a
hypothesis (H). The relation (T? H) holds when-
ever the truth of H follows from T.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
At SemEval 2014, the Task #1 "Evaluation
of Compositional Distributional Semantic Models
on Full Sentences through Semantic Relatedness
and Entailment" (Marelli et al., 2014a) primarily
aimed at evaluating Compositional Distributional
Semantic Models (CDSMs) of meaning over two
subtasks, namely semantic relatedness and tex-
tual entailment (ENTAILMENT, CONTRADIC-
TION and NEUTRAL), over pairs of sentences
(Marelli et al., 2014b). Concerning the relatedness
subtask, the system outputs are evaluated against
gold standard ratings in two ways, using Pearson
correlation and Spearman?s rank correlation (rho).
The Pearson correlation is used for evaluating and
ranking the participating systems. Similarly, for
the textual entailment subtask, system outputs are
evaluated against a gold standard rating with re-
spect to accuracy.
Our team, FBK-TR, participated in both sub-
tasks with five different runs. In this paper, we
present a comprehensive description of our system
which obtained competitive results in both tasks
and which is not based on CDSMs. Our approach
for the relatedness task is based on machine learn-
ing techniques to learn models from different lexi-
cal and semantic features from the train corpus and
then to make prediction on the test corpus. Par-
ticularly, we used support vector machine (SVM)
(Chang and Lin, 2011), regression model to solve
this subtask. On the other hand, the textual en-
tailment task uses a methodology mainly based on
corpus patterns automatically extracted from an-
notated text corpora.
The remainder of the paper is organized as
follows: Section 2 presents the SVM system
for semantic relatedness. Section 3 describes
the methodology used for extracting patterns and
computing the textual entailment values. Finally,
Section 4 discusses about the evaluations and Sec-
tion 5 presents conclusions and future work.
289
Figure 1: Schema of the system for computing entailment.
2 System Overview for Semantic
Relatedness Subtask
Concerning the Semantic Relatedness subtask our
SVM system is built on different linguistic fea-
tures, ranging from relatedness at the lexical level
(WordNet based measures, Wikipedia relatedness
and Latent Semantic Analysis), to sentence level,
including topic modeling based on Latent Dirich-
let allocation (LDA) and string similarity (Longest
Common Substring).
2.1 Lexical Features
At the lexical level, we built a simple, yet effective
Semantic Word Relatedness model, which con-
sists of 3 components: WordNet similarity (based
on the Lin measure as implemented in Pedersen
package WordNet:Similarity (Pedersen et
al., 2004), Wikipedia relatedness (as provided by
the Wikipedia Miner package (Milne and Witten,
2013)), and Latent Semantic Analysis (Landauer
et al., 1998), with a model trained on the British
National Corpus (BNC)
1
and Wikipedia. At this
level of analysis, we concentrated only on the
best matched (lemma) pairs of content words, i.e.
Noun-Noun, Verb-Verb, extracted from each sen-
tence pair. The content words have been automati-
cally extracted by means of part-of-speech tagging
(TreeTagger (Schmid, 1994)) and lemmatization.
For words which are not present in WordNet,
the relatedness score has been obtained by means
of the Levenshtein distance (Levenshtein, 1966).
1
http://www.natcorp.ox.ac.uk
2.2 Topic Modeling
We have applied topic modeling based on Latent
Dirichlet allocation (LDA) (Blei et al., 2003) as
implemented in the MALLET package (McCal-
lum, 2002). The topic model was developed us-
ing the BNC and Wikipedia (with the numbers
of topics varying from 20 to 500 topics). From
the proportion vectors (distribution of documents
over topics) of the given texts, we apply 3 differ-
ent measures (Cosine similarity, Kullback-Leibler
and Jensen-Shannon divergences) to compute the
distances between each pair of sentences.
2.3 String Similarity: Longest Common
Substring
As for the string level, two given sentences are
considered similar/related if they are overlap-
ping/covering each other (e.g sentence 1 covers
a part of sentence 2, or otherwise). Hence, we
considered the text overlapping between two
given texts as a feature for our system. The
extraction of the features at the string level was
computed in two steps: first, we obtained Longest
Common Substring between two given sentences.
After this, we also considered measuring the
similarity for the parts before and after the LCS
between two given texts, by means of the Lin
measure and the Levenshtein distance.
3 System Overview for RTE Subtask
The system for the identification of the entailment
values is illustrated in Figure 1. Entailment values
290
are computed starting from a baseline (only EN-
TAILMENT and NEUTRAL values) which relies
on the output (i.e. scores) of the semantic related-
ness system. After this step, two groups of entail-
ment patterns are applied whether the surface form
of a sentence pair is affirmative (i.e. absence of
negation words) or negative. Each type of pattern
provides in output an associated entailment value
which corresponds to the final value assigned by
the system.
The entailment patterns are based on verb-
specific semantic frames that include both syn-
tactic and semantic information. Hence, we have
explicit access to the information that individual
words have and to the process of combining them
in bigger units, namely phrases, which carry out
meanings. The patterns have two properties: i.)
the senses of the words inside the pattern are sta-
ble, they do not change whatever context is added
to the left, right or inside the phrase matching the
pattern, and ii.) the replacement of a word with an-
other word belonging to a certain class changes the
senses of the words. Patterns with these properties
are called Sense Discriminative Patterns (SDPs).
It has been noted (Popescu et al., 2011) that we can
associate to a phrase that is matched by an SDP a
set of phrases for which an entailment relationship
is decidable showing that there is a direct relation-
ship between SDPs and entailment .
SDP patterns have been obtained from large
parsed corpora. To maximize the accuracy of the
corpus we have chosen sentences containing at
maximum two finite verbs from BNC and Anno-
tated English Gigaword. We parsed this corpus
with the Stanford parser, discarding the sentences
from the Annotated English Gigaword which have
a different parsing. Each words is replaced with
their possible SUMO attributes (Niles and Pease,
2003). Only the following Stanford dependen-
cies are retained as valid [n, nsub]sbj, [d,i,p]obj,
prep, [x,c]comp. We considered only the most fre-
quent occurrences of such patterns for each verb.
To cluster into a single SDP pattern, all patterns
that are sense auto-determinative, we used the
OntoNotes (Hovy et al., 2006) and CPA (Hanks,
2008) lexica. Inside each cluster, we searched
for the most general hypernyms for each syntac-
tic slot such that there are no common patterns
between clusters (Popescu, 2013). However, the
patterns thus obtained are not sufficient enough
for the task. Some expressions may be the para-
phrasis a word in the context of an SDP. To ex-
tract this information, we considered all the pairs
in training that are in an ENTAILMENT relation-
ship, with a high relatedness score (4 to 5), and we
extracted the parts that are different for each gram-
matical slot. In this way, we compiled a list of
quasi synonym phrases that can be replaced inside
an SDP without affecting the replacement. This
is the only component that depends on the train-
ing corpus. Figure 2 describes the algorithm for
computing entailment on the basis of the SDPs.
The following subsections illustrate the identifi-
cation of entailment relation for affirmative sen-
tences and negated sentences.
Figure 2: Algorithm for computing entailment.
3.1 Entailment on Affirmative Sentences
Affirmative sentences use three types of entail-
ment patterns. The switch baseline and hyponym
patterns works in this way: If two sentences are
matched by the same SDP, and the difference be-
tween them is that the second one contains a hy-
pernym on the same syntactic position, then the
first one is entailed by the second (i.e. ENTAIL-
MENT). If the two SDPs are such that the dif-
ference between them is that the second contains
a word which is not synonym, hypernym or hy-
ponym on the same syntactic position, then there is
no entailment between the two phrases (i.e. NEU-
TRAL). The entailment direction is from the sen-
tence that contains the hyponym toward the other
291
sentence. The antonym patterns check if the two
SDPs are the same, with the only difference be-
ing in the verb of the second sentence being an
antonym of the verb in the first sentence (i.e.
CONTRADICTION).
3.2 Entailment on Negative Sentences
As for negated sentences, we distinguish between
existential negative phrases (i.e. there is no or
there are no) and factual negative ones (presence
of a negative polarity word). An assumption re-
lated to each SDP is that it entails the existence
of any of the component of the pattern which can
be expressed by means of dedicated phrases. A
SDP of the kind "[Human] beat [Animal]", en-
tails both phrases, namely there is a [Human] and
there is a [Animal]. We call this set of associ-
ated existential phrases, Existential Assumptions
(EAs). This type of existential entailment obtained
through the usage of SDP has a direct consequence
for handling the ENTAILMENT, CONTRADIC-
TION and NEUTRAL types of entailment when
one of the phrases is negated. If the first phrase
belongs to the EA of the second one, then the
first phrase is entailed by the second phrase; if the
first phrase is an existential negation of a phrase
belonging to the EA set of the second phrase,
meaning that it contains the string there is/are no,
then the first one is a contradiction of the second
phrase; if neither the first phrase, nor its negation
belong to the EA set of the second phrase, then the
two sentences are neutral with respect to the en-
tailment. The general rule described in 3.1 applies
to these types of phrases as well: replacing a word
on the same syntactic slot inside a phrase that is
matched by a SDP leads to a CONTRADICTION
type of entailment, if the replacement is a hyper-
nym of the original word. Similarly, the approach
can be applied to factual negative phrases. The
scope of negation is considered to be the extension
of the SDP and thus the negative set of EAs.
4 Evaluation and Ranking
Table 1 illustrates the results for Pearson and
Spearman correlations for the relatedness subtask
on the test set. Table 2 reports the Accuracy values
for the entailment subtask on the test set.
Concerning the relatedness results our systems
ranked 11
th
out of 17 participating systems. Best
score of our system is reported in Table 1. One
of the main reason for the relatively low results
Team Pearson Spearman
ECNU_run1 (ranked 1
st
) 0.82795 0.76892
FBK-TR_run3 0.70892 0.64430
Table 1: Results for semantic relatedness subtask.
Team Accuracy
Illinois-LH_run1 (ranked 1
st
) 84.575
FBK-TR_run3 75.401
?FBK-TR_baseline 64.080
?FBK-TR_new 85.082
Table 2: Results for entailment subtask.
of the systems for this subtask concerns the fact
that it is designed for a general-level of texts (i.e.
compositionality is not taken into account).
As for the entailment subtask, our system
ranked 11
th
out of 18 participating systems. The
submitted results of the system are illustrated in
Table 2 and are compared against the best system,
our baseline system (?FBK-TR_baseline) as de-
scribed in Figure 1, and a new version of the par-
ticipating system after fixing some bugs in the sub-
mitted version due to the processing of the parser?s
output (?FBK-TR_new). The new version of the
system scores in the top provides a new state of the
art result, with an improvement of 10 points with
respect to our submitted system.
5 Conclusion and Future Work
This paper reports the description of our system,
FBK-TR, which implements a general SVM se-
mantic relatedness system based on distributional
features (LSA, LDA), knowledge-based related
features (WordNet and Wikipedia) and string over-
lap (LCS). On top of that, we added structural in-
formation at both semantic and syntactic level by
using SDP patterns. The system reached compet-
itive results in both subtasks. By correcting some
bugs in the entailment scripts, we obtained an im-
provement over our submitted systems as well as
for the best ranking system. We plan to improve
and extend the relatedness system by means of
compositional methods. Finally, the entailment
system can be improved by taking into account
additional linguistic evidences, such as the alter-
nation between indefinite and definite determiners,
noun modifiers and semantically empty heads.
292
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Patrick Hanks. 2008. Mapping meaning onto use: a
Pattern Dictionary of English Verbs. In Proceedings
of the AACL 2008.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
the human language technology conference of the
NAACL, Companion Volume: Short Papers, pages
57?60.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259?284.
Vladimir I Levenshtein. 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions and Rever-
sals. In Soviet Physics Doklady, volume 10, page
707.
M Marelli, L Bentivogli, M Baroni, R Bernardi,
S Menini, and R Zamparelli. 2014a. Semeval-2014
Task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation, August 23-24, 2014, Dublin,
Ireland.
M Marelli, S Menini, M Baroni, L Bentivogli,
R Bernardi, and R Zamparelli. 2014b. A SICK
cure for the evaluation of compositional distribu-
tional semantic models. In Proceedings of LREC
2014, Reykjavik (Iceland): ELRA.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222?239.
Ian Niles and Adam Pease. 2003. Mapping Word-
Net to the SUMO Ontology. In Proceedings of the
IEEE International Knowledge Engineering Confer-
ence, pages 23?26.
Ted Pedersen, Patwardhan Siddharth, and Michelizzi
Jason. 2004. Wordnet::Similarity: Measuring the
Relatedness of Concepts. In Proceedings of the
HLT-NAACL 2004.
Octavian Popescu, Elena Cabrio, and Bernardo
Magnini. 2011. Textual Entailment Using Chain
Clarifying Relationships. In Proceedings of the IJ-
CAI Workshop Learning by Reasoning and its Appli-
cations in Intelligent Question-Answering.
Octavian Popescu. 2013. Learning Corpus Patterns
Using Finite State Automata. In Proceedings of the
ICSC 2013.
Helmut Schmid. 1994. Probabilistic Part-of-Sspeech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44?49. Manch-
ester, UK.
293
