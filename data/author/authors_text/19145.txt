Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 578?589,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatically Classifying Edit Categories in Wikipedia Revisions
Johannes Daxenberger? and Iryna Gurevych??
? Ubiquitous Knowledge Processing Lab
Department of Computer Science, Technische Universita?t Darmstadt
? Information Center for Education
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we analyze a novel set of fea-
tures for the task of automatic edit category
classification. Edit category classification as-
signs categories such as spelling error correc-
tion, paraphrase or vandalism to edits in a doc-
ument. Our features are based on differences
between two versions of a document includ-
ing meta data, textual and language properties
and markup. In a supervised machine learning
experiment, we achieve a micro-averaged F1
score of .62 on a corpus of edits from the En-
glish Wikipedia. In this corpus, each edit has
been multi-labeled according to a 21-category
taxonomy. A model trained on the same
data achieves state-of-the-art performance on
the related task of fluency edit classification.
We apply pattern mining to automatically la-
beled edits in the revision histories of different
Wikipedia articles. Our results suggest that
high-quality articles show a higher degree of
homogeneity with respect to their collabora-
tion patterns as compared to random articles.
1 Introduction
Due to its ever-evolving and collaboratively built
content, Wikipedia has been the subject of many
NLP studies. While the number of newly created
articles in the online encyclopedia declined in the
last few years (Suh et al, 2009), the number of edits
in existing articles is rather stable.1 It is reasonable
to assume that the latter will not change in the near
1http://stats.wikimedia.org/EN/
TablesDatabaseEdits.htm
future. One of the major reasons for the popular-
ity of Wikipedia is its up-to-dateness (Keegan et al,
2013), which in turn requires constant editing activ-
ity. Wikipedia?s revision history stores all changes
made to any page in the encyclopedia in separate
revisions. Previous studies have exploited revision
history data in tasks such as preposition error cor-
rection (Cahill et al, 2013), spelling error correc-
tion (Zesch, 2012) or paraphrasing (Max and Wis-
niewski, 2010). However, they all use different ap-
proaches to extract the information needed for their
task. Ferschke et al (2013) outline several appli-
cations benefiting from revision history data. They
argue for a unified approach to extract and classify
edits from revision histories based on a predefined
edit category taxonomy.
In this work, we show how the extraction and
automatic multi-label classification of any edit in
Wikipedia can be handled with a single approach.
Therefore, we use the 21-category edit classification
taxonomy developed in previous work (Daxenberger
and Gurevych, 2012). This taxonomy enables a fine-
grained analysis of edit activity in revision histories.
We present the results from an automatic classifica-
tion experiment, based on an annotated corpus of ed-
its in the English Wikipedia. Additional information
necessary to reproduce our results, including word
lists and training, development and test data, is re-
leased online.2 To the best of our knowledge, this
is the first approach allowing to classify each single
edit in Wikipedia into one or more of 21 different
edit categories using a supervised machine learning
2http://www.ukp.tu-darmstadt.de/data/
edit-classification
578
approach.
We define our task as edit category classification.
An edit is a coherent, local change which modifies a
document and which can be related to certain meta
data (e.g. its author, time stamp etc.). In edit ca-
tegory classification, we aim to detect all n edits
ekv?1,v with 0 ? k < n in adjacent versions rv?1, rv
of a document (we refer to the older revision as rv?1
and to the newer as rv) and assign each of them
to one or more edit categories. There exist at least
two main applications of edit category classification:
First, a fine-grained classification of edits in collab-
oratively created documents such as Wikipedia ar-
ticles, scientific papers or research proposals, would
help us to better understand the collaborative writing
process. This includes answers to questions about
the kind of contribution of individual authors (Who
has added substantial contents?, Who has improved
stylistic issues?) and about the kind of collabora-
tion which characterizes different articles (Liu and
Ram, 2011). Second, automatic classification of ed-
its generates huge amounts of training data for the
above mentioned NLP systems.
Edit category classification is related to the bet-
ter known task of document pair classification.
In document pair classification, a pair of docu-
ments has to be assigned to one or more categories
(e.g. paraphrase/non-paraphrase, plagiarism/non-
plagiarism). Here, the document may be a very short
text, such as a sentence or a single word. Appli-
cations of document pair classification include pla-
giarism detection (Potthast et al, 2012), paraphrase
detection (Madnani et al, 2012) or text similarity
detection (Ba?r et al, 2012). In edit category clas-
sification, we also have two documents. However,
these documents are different versions of the same
text. This scenario implies certain characteristics for
a well-designed feature set as we will demonstrate in
this study.
The main contributions of this paper are: First,
we introduce a novel feature set for edit category
classification. Second, we evaluate the performance
of this feature set on different tasks within a cor-
pus of Wikipedia edits. We propose the new task of
edit category classification and show that our model
is able to classify edits from a 21-category taxon-
omy. Furthermore, our model achieves state-of-the-
art performance in a fluency edit classification task
(Bronner and Monz, 2012). Third, we analyze col-
laboration patterns based on edit categories on two
subsets of Wikipedia articles, namely featured and
non-featured articles. We detect correlations be-
tween collaboration patterns and high-quality arti-
cles. This is demonstrated by the fact that featured
articles have a higher degree of homogeneity with
respect to their collaboration patterns as compared
to random articles.
The rest of this paper is structured as follows. In
Section 2, we motivate our experiments based on
previous work. Section 3 explains our training data
and the features we use for the machine learning ex-
periments. In Section 4, we present and discuss the
results of our experiments. We also demonstrate an
application of our classifier model in Section 5 by
mining frequent collaboration patterns in the revi-
sion histories of different articles. Finally, we draw
a conclusion in Section 6.
2 Related Work
Wikipedia is a huge data source for generating train-
ing data for edit category classification, as all pre-
vious versions of each page in the encyclopedia
are stored in its revision history. Unsurprisingly,
the number of studies extracting certain kinds of
Wikipedia edits keeps growing. Most of these use
manually defined rules or filters find the right kind
of edits. Among the latter, there are NLP applica-
tions such as the detection of lexical errors (Nelken
and Yamangil, 2008), spelling error correction (Max
and Wisniewski, 2010; Zesch, 2012), preposition er-
ror correction (Cahill et al, 2013), sentence com-
pression (Nelken and Yamangil, 2008; Yamangil
and Nelken, 2008), summarization (Nelken and Ya-
mangil, 2008), simplification (Yatskar et al, 2010;
Woodsend and Lapata, 2011), paraphrasing (Max
and Wisniewski, 2010; Dutrey et al, 2011), tex-
tual entailment (Zanzotto and Pennacchiotti, 2010;
Cabrio et al, 2012), information retrieval (Aji et al,
2010; Nunes et al, 2011) and bias detection (Re-
casens et al, 2013).
Bronner and Monz (2012) define features for the
supervised classification of factual and fluency edits.
Their features are calculated both on character- and
word-level. Furthermore, they use features based
on POS tags, named entities, acronyms, and a lan-
579
Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia?s diff page tool.
guage model (word n-grams). In their experiments,
character-level features and named entity features
show the highest improvement over the baseline.
Vandalism detection in Wikipedia has mostly
been defined as a binary machine learning task,
where the goal is to classify a pair of adjacent re-
visions as vandalized or not-vandalized based on
edit category features. In Adler et al (2011), the
authors group these features into meta data (au-
thor, comment and time stamp of a revision), rep-
utation (author and article reputation), textual (lan-
guage independent, i.e. token- and character-based)
and language features (language dependent, mostly
dictionary-based). They carry out cross-validation
experiments on the PAN-WVC-10 corpus (Potthast
and Holfeld, 2011). Classifiers based on reputa-
tion and text performed best. Adler et al (2011)
use Random Forests as classifier (Breiman, 2001)
in their experiments. This classifier was also used
in the vandalism detection study of Javanmardi et al
(2011) where it outperformed the classifiers based
on Logistic Regression and Naive Bayes.
Different to the approach of Bronner and Monz
(2012) and previous vandalism classification stud-
ies, we built a model which accounts for multi-
labeling and a fine-grained edit category system.
Our feature set builds upon existing work while
adding a substantial number of new features.
3 Experiments
3.1 Wikipedia Edit Category Corpus
For our experiments, we used the freely avail-
able Wikipedia Edit Category Corpus (WPEC) com-
piled in previous work (Daxenberger and Gurevych,
2012). In this corpus, each pair of adjacent revisions
is segmented into one or more edits. This enables
an accurate picture of the editing process, as an au-
thor may perform several independent edits in the
same revision. Furthermore, edits are multi-labeled,
i.e. each edit is assigned one or more categories.
This is important for a precise description of major
edits, e.g. when an entire new paragraph including
text, references and markup is added. There are four
basic types of edits, namely Insertions, Deletions,
Modifications and Relocations. These are calculated
via a line-based diff comparison on the source text
(including wiki markup). As previously suggested
(Daxenberger and Gurevych, 2012), inside modified
lines, only the span of text which has actually been
changed is marked as edit (either Insertion, Dele-
tion or Modification), not the entire line. We ex-
tracted the data which is not contained in WPEC
(meta data and plain text of rv?1 and rv) using the
Java Wikipedia Library (JWPL) with the Revision
Toolkit (Ferschke et al, 2011).
In Daxenberger and Gurevych (2012), we divide
the 21-category taxonomy into text-base (meaning-
changing edits), surface (non meaning-changing ed-
its) and Wikipedia policy (VANDALISM and RE-
VERT) edits. Among the text-base edits, we include
categories for templates, references (internal and ex-
ternal links), files and information, each of which
is further divided into an insertion (I), deletion (D)
and modification (M) category. Surface edits con-
sist of paraphrases, spelling and grammar correc-
tions, relocations and markup edits. The latter cate-
gory contains all edits which affect markup elements
that are not covered by any of the other categories
and is divided into insertions, deletions and modifi-
cations. This includes, for example, apostrophes in
'''bold text'''. We also suggested an OTHER category,
which is intended for edits which cannot be labeled
due to segmentation errors. Figure 1 shows an exam-
ple edit from WPEC, labeled with the REFERENCE-
580
Feature Value Explanation
M
et
a
D
at
a
Author group user Wikimedia user group of the author
Author is registered* true Author is registered (otherwise: IP user)
Same author* false Authors of rv and rv?1 are the same
Comment length* 0 Number of characters in the comment
Vulgarism in comment false Comment contains a word from in the vulgarism word list
Comment is auto-generated false Entire comment has been auto-generated
Auto-generated comment ratio 0 Auto-generated part of comment divided by length of the comment
Incorrect comment ratio 0 Out-of-dictionary word count divided by word count in the comment
Comment n-grams1 ? Presence or absence of token n-grams in the comment
Is revert* false Comment contains a word from in the revert word list
Is minor false Revision has been marked as minor change
Time difference* 505 Time difference between rv?1 and rv (in minutes)
Number of edits 1 Absolute number of edits in the (rv?1, rv)-pair
Te
xt
ua
l
Diff capitals* 0 Difference in the number of capitals
Diff digits* 0 Difference in the number of digits
Diff special characters* 2 Difference in the number of non-alphanumeric characters
Diff whitespace characters 1 Difference in the number of whitespace characters
Diff characters* 9 Difference in the number of characters
Diff tokens* 1 Difference in the number of whitespace-separated tokens
Diff repeated characters 0 Difference in the number of repeated characters
Diff repeated tokens 0 Difference in the number of repeated white-space separated tokens
Cosine similarity 0 Cosine similarity
Levenshtein distance* 9 Levenshtein distance
Optimal string alignment distance 9 Optimal string alignment distance (Damerau-Levenshtein distance)
Ratio diff to paragraph characters 0.02 Diff characters divided by the length of the edited paragraph
Ratio diff to revision characters 0.0005 Diff characters divided by the length of rv?1
Ratio diff to paragraph tokens 0.04 Diff tokens divided by the length of the edited paragraph
Ratio diff to revision tokens 0.0003 Diff tokens divided by the length of rv?1
Ratio old to new paragraph 0 Difference in the number of characters in the edited paragraph
Character n-grams1 p,o,e,t,r,y2 Presence or absence of n-grams of edited characters
Token n-grams1 poetry2 Presence or absence of n-grams of edited tokens
Simple edit type Insertion Modification, Insertion, Deletion or Relocation
M
ar
ku
p
Diff number m 0 Difference in the number of m
Diff type m false Different types of m
Diff type context m true3 Different types of m within the immediate context of the edit
Is covered by m true3 Edit is covered by m in rv?1
Covers m false Edit covers m in rv?1
L
an
gu
ag
e Diff spelling errors* 0 Difference in the number of out-of-dictionary words
Diff vulgar words* 0 Difference in the number of tokens contained in vandalism word list
Semantic similarity -1 Explicit Semantic Analysis with vector indexes from Wiktionary
Diff POS tags* false POS tag sets are symmetrically different
Diff type POS tags* 0 Number of distinct POS tags
1 N-gram features are represented as boolean features.
2 In this example, n = 1 (unigrams).
3 True if m corresponds to internal link, false otherwise.
Table 1: List of edit category classification features with explanations. The values correspond to the the example edit
from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with
* have previously been mentioned in Adler et al (2011), Javanmardi et al (2011) or Bronner and Monz (2012).
581
M category. WPEC was created in a manual anno-
tation study with three annotators. The overall inter-
annotator agreement measured as Krippendorf?s ? is
.67 (Daxenberger and Gurevych, 2012). The exper-
iments in this study are based on the gold standard
annotations in WPEC, which have been derived by
means of a majority vote for each edit.
WPEC consists of 981 revision pairs, segmented
into 1,995 edits. We define edit category classifica-
tion as a multi-label classification task. For the sake
of readability, in the following we will refer to an
edit ekv?1,v as ei, with ei ? E, where 0 ? i < 1995
and E is the set of all edits. An edit ei is the basic
classification unit in our task. Each ei has to be la-
beled with a set of categories y ? C, where C is the
set of all edit categories, |C| = 21.
3.2 Features for Edit Category Classification
We grouped our features into meta data, textual,
markup and language features. An overview and
explanation of all features can be found in Table 1.
The scheme we apply to group edit category clas-
sification features is similar to the system used by
Adler et al (2011). We re-use some of the features
suggested by Adler et al (2011), Javanmardi et al
(2011) and Bronner and Monz (2012), as marked in
Table 1. Features are calculated on edited text spans.
We label the edited text span corresponding to ei in
rv?1 as tv?1 and the edited text span in rv as tv. In
edits which are insertions, we consider tv?1 to be
empty, while tv is considered empty for deletions.
For Relocations, tv?1 = tv.
Table 1 includes the value of each feature for the
example edit from Figure 1. This edit modifies the
link [[Dactyl|Dactylic]] by adding a speci-
fication to the target of that link. For spell-checking,
we use British and US-American English Jazzy dic-
tionaries.3 Markup elements are detected by the
Sweble Wikitext parser (Dohrn and Riehle, 2011).
Meta data features We consider the comment,
author, time stamp or any other flag (?minor
change?) of rv as meta data. The Wikimedia user
group4 of an author specifies the edit permissions
3http://sourceforge.net/projects/
jazzydicts
4http://meta.wikimedia.org/wiki/User_
classes
of this user (e.g. bot, administrator, blocked user).
We indicate whether the revision comments or parts
of it have been auto-generated. This happens when
a page is blanked, i.e. all of its content has been
deleted or replaced or when a new page or redirect is
created (denoted by the Comment is auto-generated
feature). Furthermore, edits within a specific sec-
tion of an article are automatically marked by adding
a prefix with the name of this section to the com-
ment of the revision (denoted by the Auto-generated
comment ratio feature). Meta data features have the
same value for all edits in a (rv?1, rv)-pair.
Textual features Textual features are calculated
based on a certain property of the changed text. In
a preprocessing step, any wiki markup inside tv?1
and tv is deleted. As for the example edit from Fig-
ure 1, tv?1 would correspond to an empty string and
tv would be represented as ? (poetry)?. The n-gram
feature spaces are composed of n-grams that are
present either in tv?1 but not tv, or vice verse. Char-
acter n-grams only contain English alphabet charac-
ters, token n-grams consist of words excluding spe-
cial characters.
Markup features As opposed to textual features,
wiki markup features account for the Wikimedia
specific markup elements. Markup features are cal-
culated based on the number and type of a markup
element m and the surrounding context of an edit.
Here, m can be a template, an external or internal
link, an image or any other element used to describe
markup including HTML tags. The type of m is
defined by the link target for internal and external
links and images, by the name of the template for
templates and by the wiki markup element name for
markup elements. Markup features are calculated on
text spans tv?1 and tv. Naturally, wiki markup is not
deleted beforehand. The edited text spans tv?1 and
tv may be located inside a markup elementm (e.g. a
link or a template). In such cases, our diff algorithm
will not label the entire element m, but rather the
actually modified text. However, such an edit may
change the name of a template or the target of a link
(as in the example edit from Figure 1). We there-
fore include the immediate context sv?1 and sv of
each edit and compare the type of potential markup
elements m in sv?1 and sv. Here, sv (sv?1) is de-
fined as tv (tv?1) including all preceding and follow-
582
Revisions Edits Cardinality
Train 713 1,597 1.20
Test 89 229 1.24
Dev 89 169 1.21
Table 2: Statistics of the training, test and development
set. Cardinality is the average number of edit categories
assigned to an edit.
ing characters in rv (rv?1) which are not separated
from tv (tv?1) by a boundary character (whitespace
or line break). The above described features model
what is actually edited in the text. A number of fea-
tures are calculated on tv?1 only. These features are
more likely to inform about where an edit is con-
ducted. They specify whether tv?1 covers (i.e. con-
tains) a certain wiki markup element and vice versa,
i.e. whether tv?1 is located inside a text span that
belongs to a markup element.
Language Language features are calculated on the
context sv?1 and sv of edits, any wiki markup is
deleted. For the Explicit Semantic Analysis, we use
Wiktionary (Zesch et al, 2008) and not Wikipedia
assuming that the former has a better coverage with
respect to different lexical classes. POS tagging was
carried out using the OpenNLP POS tagger.5 The
vandalism word list contains a hand-crafted set of
around 100 vandalism and spam words from various
places in the web.
3.3 Experimental Setup
We extract features with the help of ClearTK (Ogren
et al, 2008). For the machine learning part, we use
Weka (Hall et al, 2009) with the Meka6 and Mu-
lan (Tsoumakas et al, 2010) extensions for multi-
label classification. We use DKPro Lab (Eckart de
Castilho et al, 2011) to test different parameter com-
binations. We randomly split the gold standard data
from WPEC into 80% training, 10% test and 10%
development set, as shown in Table 2.
Multi-label Classification We report the perfor-
mance of various machine learning algorithms. A
comprehensive overview of multi-label classifica-
tion algorithms and evaluation measures can be
5Maxent model for English, http://opennlp.
apache.org
6http://meka.sourceforge.net
R
an
do
m
M
aj
or
ity
B
R
H
O
M
E
R
R
A
K
E
L
Threshold ? ? .10 .25 .33
Example
Accuracy .09 .13 .50 .44 .53
Exact Match .06 .13 .35 .36 .44
F1 .09 .13 .55 .47 .56
Precision .10 .13 .54 .46 .56
Recall .10 .13 .61 .50 .60
Label
Macro-F1 .10 .06 .49 .35 .51
Micro-F1 .10 .12 .59 .49 .62
Ranking One Error .90 .87 .42 .48 .34
Table 3: Overall classification results with 3 multi-label
classifiers and a C4.5 decision tree base classifier, as com-
pared to random and majority category baselines.
found in Madjarov et al (2012). Multi-label classi-
fication problems are solved by either transforming
the multi-label classification task into one or more
single-label classification tasks (problem transfor-
mation method) or by adapting single-label clas-
sification algorithms (algorithm adaption method).
Several algorithms have been developed on top of
the former methods and use ensembles of such clas-
sifiers (ensemble methods). We applied the Bi-
nary Relevance approach (BR), a simple transfor-
mation method which converts the multi-label prob-
lem into |C| binary single-label problems, where |C|
is the number of categories. Hence, this method
trains a classifier for each category in the corpus
(one-against-all). It is the most straightforward ap-
proach when dealing with multi-labeled data. How-
ever, it does not consider possible relationships or
dependencies between categories. Therefore, we
tested two more sophisticated methods. Hierar-
chy of multi-label classifiers HOMER (Tsoumakas
et al, 2008) is a problem transformation method.
It accounts for possibly hierarchical relationships
among categories by dividing the overall category
set into a tree-like structure with nodes of small ca-
tegory sets of size k and leaves of single categories.
Subsequently, a multi-label classifier is applied to
each node in the tree. Random k-labelsets RAKEL
(Tsoumakas et al, 2011) is an ensemble method,
which randomly chooses l typically small subsets
with k categories from the overall set of catego-
ries. Subsequently, all k-labelsets which are found
in the multi-labeled data set are converted into new
categories in a single-labeled data set using the la-
583
bel powerset transformation (Trohidis et al, 2008).
HOMER and BR are among the multi-label clas-
sifiers, which Madjarov et al (2012) recommend
as benchmark methods. As underlying single-label
classification algorithm, we used a C4.5 decision
tree classifier (Quinlan, 1993), as decision tree clas-
sifiers yield state-of-the-art performance in the re-
lated work.
Multi-label Evaluation We denote the set of rel-
evant categories for each edit ei ? E as yi ?
C and the set of predicted categories as h(ei).
Evaluation measures for multi-label classification
systems are based on either bipartitions or rank-
ings. Among the former, we report example-
based (weighting each edit equally) and label-based
(weighting each edit category equally) measures.
The accuracy of a multi-label classifier is defined
as 1|E|
?|E|
i=1
|h(ei)?yi|
|h(ei)?yi|
, which corresponds to the Jac-
card similarity of h(ei) and yi averaged over all ed-
its. We report subset accuracy (exact match), cal-
culated as 1|E|
?|E|
i=1 I , with I = 1 if h(ei) =
yi and I = 0 otherwise. Example-based pre-
cision is defined as 1|E|
?|E|
i=1
|h(ei)?yi|
|h(ei)|
, recall as
1
|E|
?|E|
i=1
|h(ei)?yi|
|yi|
, and F1 as 1|E|
?|E|
i=1
2?|h(ei)?yi|
|h(ei)|+|yi|
.
For the label-based measures, we report macro-
and micro-averaged F1 scores. As a ranking-based
measure, we report one error, which is defined as
1
|E|
?|E|
i=1J[arg max
c?C
f(ei, c)] /? yiK, JexprK = 1 if
expr is true and JexprK = 0 otherwise. f(ei, c) de-
notes the rank of category c ? C as predicted by
the classifier. The one error measure evaluates the
number of edits where the highest ranked category
in the predictions is not in the set of relevant cate-
gories. It becomes smaller when the performance of
the classifier increases.
Table 3 shows the overall classification scores.
We calculated a random baseline, which multi-labels
edits at random considering the label powerset fre-
quencies it has learned from the training data. Fur-
thermore, we calculated a majority category base-
line, which labels all edits with the most frequent
edit category in the training data. In Figure 2, we
list the results for each category, together with the
average pair-wise inter-rater agreement (F1 scores).
The F1 scores are calculated based on the study we
carried out in Daxenberger and Gurevych (2012).
Parameters and Feature selection All parame-
ters have been adjusted on the development set us-
ing the RAKEL classifier, aiming to optimize accu-
racy. With respect to the n-gram features, we tested
values for n = 1, 2 and 3. For comment n-grams,
unigrams turned out to yield the best overall per-
formance, and bigrams for character and token n-
grams. The word and character n-gram spaces are
limited to the 500 most frequent items, the comment
n-gram space is limited to the 1,500 most frequent
items. To transform ranked output into bipartitions,
it is necessary to set a threshold. This threshold is
reported in Table 3 and has been optimized for each
classifier with respect to label cardinality (average
number of labels assigned to edits) on the develop-
ment set. Since most of the traditional feature se-
lection methods cannot be applied directly to multi-
labeled data, we used the label powerset approach to
transform the multi-labeled data into single-labeled
data and subsequently applied ?2. Feature reduc-
tion to the highest-ranked features clearly improved
the classifier performance on the development set.
We therefore limited the feature space to the 150
highest-ranked features in our experiments.
For the RAKEL classifier, we set l = 42 (twice
the size of the category set) and k = 3. In HOMER,
we used BR as transformation method, random dis-
tribution of categories to the children nodes and
k = 3. For all other classifier parameters, we used
the default settings as configured in Meka respective
Mulan.
4 Discussion
The classifiers significantly outperformed both base-
lines. RAKEL shows best performance for almost
all measures in Table 3. The simpler BR approach,
which assumes no dependencies between categories,
still outperforms HOMER.
We trained and tested the classifier with different
feature groups (see Table 1), to analyze the impor-
tance of single types of features. As shown in Fig-
ure 2, textual features had the highest impact on clas-
sification performance. On the opposite, language
features played a minor role in our experiments.
Among the highest ranked individual features for the
entire set of categories, we find textual (Levenshtein
distance, Simple edit type), markup (Diff number
584
Re
ve
rt
(3
0)
Va
nd
ali
sm
(1
9)
Pa
ra
ph
ra
se
(5
)
Re
loc
at
ion
(2
)
M
ar
ku
p-
I (
27
)
M
ar
ku
p-
M
(2
)
M
ar
ku
p-
D
(1
0)
Sp
ell
in
g/G
r.
(2
0)
Re
fer
en
ce
-I
(3
1)
Re
fer
en
ce
-M
(1
2)
Re
fer
en
ce
-D
(1
2)
In
for
m
at
ion
-I
(3
5)
In
for
m
at
ion
-M
(2
8)
In
for
m
at
ion
-D
(1
8)
Te
m
pl
at
e-I
(1
0)
Te
m
pl
at
e-D
(2
)
Fi
le-
I (
2)
Fi
le-
D
(1
)
Ot
he
r (
16
)
0
0.2
0.4
0.6
0.8
1
F
1
sc
or
e
Human Classifier Textual Markup Meta Data Language
Figure 2: F1 scores of RAKEL with C4.5 as base classifier for individual categories. We add human inter-annotator
agreement as average pair-wise F1 scores as well as F1 scores for classifiers trained and tested on single feature
groups, cf. Table 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M
and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set.
markup elements) and meta data (Number of edits)
features.
Bronner and Monz (2012) report an accuracy
of .88 for their best performing system on the bi-
nary classification task of distinguishing fluency and
factual edits. The best performing classifier in
their study was Random Forests (Breiman, 2001).
To compare our features with their approach, we
mapped the 21 edit categories from Daxenberger and
Gurevych (2012) to the binary category set (factual
vs. fluency) of Bronner and Monz (2012). Edits la-
beled as SPELLING/GRAMMAR, MARKUP, RELO-
CATION and PARAPHRASE are considered fluency
edits, the remaining categories factual edits. We re-
moved all edits labeled as OTHER, REVERT or VAN-
DALISM from WPEC. After applying the category
mapping, we deleted all edits which were labeled
with both the fluency and factual category. The lat-
ter may happen due to multi-labeling. This resulted
in 1,262 edits labeled as either fluency or factual.
On the 80% training split from Table 2, we trained
a Random Forests classifier with the optimized fea-
ture set and feature reduction as described in Sec-
tion 3.3. The number of trees was set to 100, with
unlimited depth. On the remaining data (test and
development split), we achieved an accuracy of .90.
Although we did not use the same data set as Bron-
ner and Monz (2012), this result suggests that our
feature set is suited for related tasks such as fluency
detection.
With respect to vandalism detection in Wikipedia,
state-of-the-art systems have a performance of
around .82 to .85 AUC-PR on the English Wikipedia
(Adler et al, 2011). We suspect that the low perfor-
mance of our system for Vandalism edits is mostly
due to a lower amount of training data, a higher skew
in the training and test data and the fact that we did
not include features which inform about future ac-
tions (e.g. whether a revision is reverted).
Error Analysis Sparseness is a major problem
for some of the 21 categories, as shown in Fig-
ure 2 by categories such as FILE-D, TEMPLATE-
D, MARKUP-M or PARAPHRASE which have only
very few examples in training, development and
test set. Categories with low inter-annotator agree-
ment in WPEC such as MARKUP-M, PARAPHRASE
or OTHER also yielded low classification accuracy.
We analyzed frequent errors of the classifier with
the help of a confusion matrix. PARAPHRASE ed-
its have been confused with INFORMATION-M by
the classifier. Furthermore, the classifier had prob-
lems to distinguish between VANDALISM and RE-
VERT as well as INFORMATION-I. Generally, modi-
fications as compared to insertions or deletions per-
form worse. All of the classifiers we tested, build
585
their predictions by thresholding over a ranking, cf.
Table 3. This generates a source of errors, because
the classifier is not able to make a prediction, if it
does not have enough confidence for any of the cate-
gories. The imbalance of the data, because of the
high skew in the category distribution, is another
reason for classification errors. In ambiguous cases,
the classifier will be biased toward the category with
more examples in the training data.
5 A closer look at edit sequences: Mining
collaboration patterns
An edit category classifier allows us to label en-
tire article revision histories. We applied the best-
performing model from Section 3.3 trained on the
entire WPEC to automatically classify all edits in the
Wikipedia Quality Assessment Corpus (WPQAC)
as presented in previous work (Daxenberger and
Gurevych, 2012). WPQAC consists of 10 fea-
tured and 10 non-featured articles7, with an over-
all number of 21,578 revisions (9,986 revisions
from featured articles and 11,592 from non-featured
articles), extracted from the April 2011 English
Wikipedia dump. The articles in WPQAC are care-
fully chosen to form comparable pairs of featured
and non-featured articles, which should reduce the
noise of external influences on edit activity such
as popularity or visibility. In Daxenberger and
Gurevych (2012), we have shown significant dif-
ferences in the edit category distribution of arti-
cles with featured status before and after the articles
were featured. We concluded that articles become
more stable after being featured, as shown by the
higher number of surface edits and lower number of
meaning-changing edits.
Different to our previous approach which is based
on the mere distribution of edit categories, in the
present study we include the chronological order of
edits and use a 10 times larger amount of data for our
experiments. We segmented all adjacent revisions
in WPQAC into edits, following the approach ex-
plained in Daxenberger and Gurevych (2012). Dur-
ing the classification process, we discarded revisions
where the classifier could not assign any of the 21
edit categories with a confidence higher than the
7http://en.wikipedia.org/wiki/Wikipedia:
FA
threshold, cf. Table 3. This resulted in 17,640 re-
maining revisions. We applied a sequential pattern
mining algorithm with time constraints (Hirate and
Yamana, 2006; Fournier-Viger et al, 2008) to the
data. The latter is based on the PrefixSpan algorithm
(Pei et al, 2004). Calculations have been carried out
within the open-source SPMF Java data mining plat-
form.8
We created one time-extended sequence database
for the 10 featured articles and one for the 10 non-
featured articles. The sequence databases consist
of one row per article. Each row is a chronologi-
cally ordered list of revisions. Each revision is rep-
resented by the itemset of all edit categories for all
edits in that revision (in alphabetical order).
The output of the algorithm are sequential pat-
terns with time constraints. To obtain meaningful
results, we constrained the output with the follow-
ing parameters:
? Minimum support: 1 (the patterns have to be
present in each article)
? Time interval allowed between two successive
itemsets in the patterns: 1 (patterns are ex-
tracted only from adjacent revisions)
? Minimum time interval between the first item-
set and the last itemset in the patterns: 1 (the
length of the patterns is 2 or higher)
As this output reflects recurring sequences of ad-
jacent revisions labeled with edit categories, we re-
fer to it as collaboration patterns. With these pa-
rameters, the algorithm discovered 1,358 sequen-
tial patterns for featured articles and 968 for non-
featured articles. The number of shared patterns in
featured and non-featured articles is 427, this corre-
sponds to the number of frequent patterns in a se-
quence database which contains all 20 featured and
non-featured articles. The maximum length of pat-
terns which were found was 6 for featured articles,
and 5 for non-featured articles. These numbers show
that the defined collaboration patterns seem to have
discriminative power for different kinds of articles.
Featured articles can be characterized by a higher
8http://www.philippe-fournier-viger.
com/spmf
586
Featured
1 INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 INFORMATION-I 5 INFORMATION-I
1 INFORMATION-D, INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 REFERENCE-I
1 TEMPLATE-D 2 REFERENCE-I
Non-
Featured
1 INFORMATION-I 2 INFORMATION-I, REFERENCE-I 3 INFORMATION-I 4 REFERENCE-I 5 MARKUP-I
1 MARKUP-I 2 REFERENCE-D 3 MARKUP-I
1 VANDALISM 2 REVERT
Table 4: Examples of collaboration patterns which have been found in either all featured or all non-featured articles of
WPQAC.
degree of homogeneity with respect to their collab-
orative patterns due to a higher number and length
of frequent sequential patterns in featured articles as
compared to non-featured articles.
In Table 4, we list some examples of collabora-
tion patterns with a minimum support of 1 which
we found in featured, but not non-featured arti-
cles, or vice verse. Unsurprisingly, patterns which
contain combinations of the most frequent catego-
ries (INFORMATION-I, REFERENCE-I), have a high
overall frequency. The diversity inside collaboration
patterns measured by the number of different edit
categories was higher in non-featured articles. For
example, the VANDALISM - REVERT pattern was
only found in non-featured articles. Patterns in fea-
tured articles tended to be more homogeneous, as
shown by the first pattern in Table 4, a repetition
of additions of information. We conclude that dis-
tinguished, high-quality articles, show a higher de-
gree of homogeneity as compared to a subset of non-
featured articles and the overall corpus.
6 Conclusion
In this study, we evaluated a novel feature set
for building a model to automatically classify
Wikipedia edits. Using a freely available cor-
pus (Daxenberger and Gurevych, 2012), our model
achieved a micro-averaged F1 score of .62 classify-
ing edits within a range of 21 categories. Textual
features had the highest impact on classifier perfor-
mance, whereas language features play a minor role.
The same classifier model obtained state-of-the-art
performance on the related task of fluency edit clas-
sification. Applications which potentially benefit
from our work include the analysis of the writing
process in collaboratively created documents, such
as wikis or research papers. We have demonstrated
how our model can be used to detect collaboration
patterns in article revision histories. On a subset
of articles from the English Wikipedia, we found
that high-quality articles show a higher degree of
homogeneity in their collaborative patterns as com-
pared to random articles. Furthermore, automatic
edit category classification allows to generate huge
amounts of category-filtered training data for NLP
tasks, e.g. spelling and grammar correction or van-
dalism detection. With respect to future work, we
plan to include more resources, e.g. the PAN-WVC-
10 (Potthast and Holfeld, 2011) or WiCoPaCo (Max
and Wisniewski, 2010) to increase the size of train-
ing data. A larger amount of labeled data would
certainly help to improve the classifier performance
for weak categories (e.g. VANDALISM and PARA-
PHRASE) and sparse categories (e.g. TEMPLATE-D,
MARKUP-M). Based on our trained classifier, anno-
tating more examples can be alleviated with the help
of active learning.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806, and by the
Hessian research excellence program ?Landes-
Offensive zur Entwicklung Wissenschaftlich-
o?konomischer Exzellenz? (LOEWE) as part of the
research center ?Digital Humanities?. We thank the
anonymous reviewers for their valuable feedback.
References
B Thomas Adler, Luca Alfaro, Santiago M Mola-
Velasco, Paolo Rosso, and Andrew G West. 2011.
Wikipedia Vandalism Detection: Combining Natural
Language, Metadata, and Reputation Features. In
Alexander Gelbukh, editor, Computational Linguistics
587
and Intelligent Text Processing, Lecture Notes in Com-
puter Science, pages 277?288. Springer.
Ablimit Aji, Yu Wang, and Eugene Agichtein. 2010. Us-
ing the Past To Score the Present: Extending Term
Weighting Models Through Revision History Analy-
sis. ReCALL, pages 629?638.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation, held in conjunction
with the 1st Joint Conference on Lexical and Compu-
tational Semantics, pages 435?40, Montreal, Canada,
USA.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45(1):5?32.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories. In
European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 356?366, Avi-
gnon, France.
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on The People?s Web meets NLP, pages 34?
43, Jeju Island, Republic of Korea.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane
Napolitano. 2013. Robust Systems for Preposition
Error Correction Using Wikipedia Revisions. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 507?
517, Atlanta, GA, USA.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In Proceed-
ings of the 24th International Conference on Compu-
tational Linguistics, pages 711?726, Mumbai, India.
Hannes Dohrn and Dirk Riehle. 2011. Design and imple-
mentation of the Sweble Wikitext parser. In Proceed-
ings of the 7th International Symposium on Wikis and
Open Collaboration, pages 72?81, Mountain View,
CA, USA.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in Wikipedia?s revision history. Proce-
samiento del Lenguaje Natural, 46:51?58.
Richard Eckart de Castilho, Iryna Gurevych, and
Richard Eckart de Castilho. 2011. A Lightweight
Framework for Reproducible Parameter Sweeping in
Information Retrieval. In Proceedings of the Work-
shop on Data Infrastructures for Supporting Informa-
tion Retrieval Evaluation, pages 7?10, Glasgow, UK.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia?s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations, pages 97?102, Portland, OR,
USA.
Oliver Ferschke, Johannes Daxenberger, and Iryna
Gurevych. 2013. A Survey of NLP Methods and Re-
sources for Analyzing the Collaborative Writing Pro-
cess in Wikipedia. In Iryna Gurevych and Jungi Kim,
editors, The Peoples Web Meets NLP: Collaboratively
Constructed Language Resources, Theory and Appli-
cations of Natural Language Processing, chapter 5.
Springer.
Philippe Fournier-Viger, Roger Nkambou, and Engel-
bert Mephu Nguifo. 2008. A Knowledge Discovery
Framework for Learning Task Models from User Inter-
actions in Intelligent Tutoring Systems. In Alexander
Gelbukh and Eduardo F. Morales, editors, Proceedings
of the 7th Mexican International Conference on Artifi-
cial Intelligence, Lecture Notes in Computer Science,
pages 765?778. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10?18.
Yu Hirate and Hayato Yamana. 2006. Generalized Se-
quential Pattern Mining with Item Intervals. Journal
of Computers, 1(3):51?60.
Sara Javanmardi, David W. McDonald, and Cristina V.
Lopes. 2011. Vandalism Detection in Wikipedia: A
High-Performing, Feature-Rich Model and its Reduc-
tion Through Lasso. In Proceedings of the 7th Interna-
tional Symposium on Wikis and Open Collaboration,
pages 82?90, Mountain View, CA, USA.
Brian Keegan, Darren Gergle, and Noshir Contractor.
2013. Hot Off the Wiki: Structures and Dynamics
of Wikipedia?s Coverage of Breaking News Events.
American Behavioral Scientist, 57(5):595?622, May.
Jun Liu and Sudha Ram. 2011. Who does what: Col-
laboration patterns in the wikipedia and their impact
on article quality. ACM Trans. Management Inf. Syst.,
2(2):11.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Sas?o Dz?eroski. 2012. An extensive experimental
comparison of methods for multi-label learning. Pat-
tern Recognition, 45(9):3084?3104.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
588
Language Technologies, pages 182?190, Montre?al,
Canada.
Aure?lien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedias Revision History. In Proceedings of the
7th Conference on International Language Resources
and Evaluation, Valletta, Malta.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedia?s Article Revision History for Training
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31?36, Chicago, IL, USA.
Se?rgio Nunes, Cristina Ribeiro, and Gabriel David. 2011.
Term weighting based on document revision history.
Journal of the American Society for Information Sci-
ence and Technology, 62(12):2471?2478.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC), pages 32?38, Marrakech, Morocco.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal,
and Mei-Chun Hsu. 2004. Mining sequential patterns
by pattern-growth: the PrefixSpan approach. IEEE
Transactions on Knowledge and Data Engineering,
16(11):1424?1440.
Martin Potthast and Teresa Holfeld. 2011. Overview of
the 2nd International Competition on Wikipedia Van-
dalism Detection. In Notebook Papers of CLEF 2011
Labs and Workshops, Amsterdam, Netherlands.
Martin Potthast, Tim Gollub, Matthias Hagen, Johannes
Kiesel, Maximilian Michel, Arnd Oberla?nder, Mar-
tin Tippmann, Alberto Barro?n-Ceden?o, Parth Gupta,
Paolo Rosso, and Benno Stein. 2012. Overview of
the 4th International Competition on Plagiarism De-
tection. In CLEF 2012 Evaluation Labs and Workshop
Working Notes Papers, Rome, Italy.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2013. Linguistic Models for Analyz-
ing and Detecting Biased Language. In Proceedings of
the 51st Annual Meeting on Association for Computa-
tional Linguistics, pages 1650?1659, Sofia, Bulgaria.
Bongwon Suh, Gregorio Convertino, Ed H. Chi, and Pe-
ter Pirolli. 2009. The singularity is not near: slow-
ing growth of Wikipedia. In Proceedings of the 5th
International Symposium on Wikis and Open Collabo-
ration, Orlando, FL, USA.
Konstantinos Trohidis, Grigorios Tsoumakas, George
Kalliris, and Ioannis Vlahavas. 2008. Multi-label
classification of music into emotions. In 9th Inter-
national Conference on Music Information Retrieval,
pages 325?330, Philadelphia, PA, USA.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2008. Effective and Efficient Multilabel Classi-
fication in Domains with Large Number of Labels. In
Proceedings of the ECML/PKDD 2008 Workshop on
Mining Multidimensional Data, Antwerp, Belgium.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining and
Knowledge Discovery Handbook, chapter 34, pages
667?685. Springer.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2011. Random k-Labelsets for Multi-Label
Classification. IEEE Transactions on Knowledge and
Data Engineering, 23(7):1079?1089.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to Simplify Sentences with Quasi-Synchronous Gram-
mar and Integer Programming. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 409?420, Edinburgh, Scot-
land, UK.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Short Papers,
pages 137?140, Columbus, OH, USA.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 365?368, Los Angeles, CA, USA.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of
the COLING-Workshop on The People?s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 28?36, Beijing, China.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic Re-
latedness. In Proceedings of the Twenty-Third AAAI
Conference on Artificial Intelligence, pages 861?866,
Chicago, IL, USA.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529?538, Avignon, France.
589
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 187?192,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatically Detecting Corresponding Edit-Turn-Pairs in Wikipedia
Johannes Daxenberger
?
and Iryna Gurevych
??
? Ubiquitous Knowledge Processing Lab
Department of Computer Science, Technische Universit?at Darmstadt
? Information Center for Education
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
In this study, we analyze links between
edits in Wikipedia articles and turns from
their discussion page. Our motivation is
to better understand implicit details about
the writing process and knowledge flow in
collaboratively created resources. Based
on properties of the involved edit and
turn, we have defined constraints for corre-
sponding edit-turn-pairs. We manually an-
notated a corpus of 636 corresponding and
non-corresponding edit-turn-pairs. Fur-
thermore, we show how our data can be
used to automatically identify correspond-
ing edit-turn-pairs. With the help of su-
pervised machine learning, we achieve an
accuracy of .87 for this task.
1 Introduction
The process of user interaction in collaborative
writing has been the topic of many studies in re-
cent years (Erkens et al, 2005). Most of the re-
sources used for collaborative writing do not ex-
plicitly allow their users to interact directly, so that
the implicit effort of coordination behind the ac-
tual writing is not documented. Wikipedia, as one
of the most prominent collaboratively created re-
sources, offers its users a platform to coordinate
their writing, the so called talk or discussion pages
(Vi?egas et al, 2007). In addition to that, Wikipedia
stores all edits made to any of its pages in a revi-
sion history, which makes the actual writing pro-
cess explicit. We argue that linking these two re-
sources helps to get a better picture of the collabo-
rative writing process. To enable such interaction,
we extract segments from discussion pages, called
turns, and connect them to corresponding edits in
the respective article. Consider the following snip-
pet from the discussion page of the article ?Boron?
in the English Wikipedia. On February 16th of
2011, user JCM83 added the turn:
Shouldn?t borax be wikilinked in the
?etymology? paragraph?
Roughly five hours after that turn was issued
on the discussion page, user Sbharris added
a wikilink to the ?History and etymology? sec-
tion of the article by performing the following
edit:
'' borax''? [[borax]]
This is what we define as a corresponding edit-
turn-pair. More details follow in Section 2. To
the best of our knowledge, this study is the first
attempt to detect corresponding edit-turn-pairs in
the English Wikipedia fully automatically.
Our motivation for this task is two-fold. First,
an automatic detection of corresponding edit-turn-
pairs in Wikipedia pages might help users of the
encyclopedia to better understand the development
of the article they are reading. Instead of having to
read through all of the discussion page which can
be an exhausting task for many of the larger arti-
cles in the English Wikipedia, users could focus
on those discussions that actually had an impact
on the article they are reading. Second, assuming
that edits often introduce new knowledge to an ar-
ticle, it might be interesting to analyze how much
of this knowledge was actually generated within
the discourse on the discussion page.
The detection of correspondence between edits
and turns is also relevant beyond Wikipedia. Many
companies use Wikis to store internal information
and documentation (Arazy et al, 2009). An align-
ment between edits in the company Wiki and is-
sues discussed in email conversations, on mailing
lists, or other forums, can be helpful to track the
flow or generation of knowledge within the com-
pany. This information can be useful to improve
communication and knowledge sharing.
187
In the limited scope of this paper, we will fo-
cus on two research questions. First, we want to
understand the nature of correspondence between
Wikipedia article edits and discussion page turns.
Second, we want to know the distinctive properties
of corresponding edit-turn-pairs and how to use
these to automatically detect corresponding pairs.
2 Edit-Turn-Pairs
In this section, we will define the basic units of our
task, namely edits and turns. Furthermore, we will
explain the kind of correspondence between edits
and turns we are interested in.
Edits To capture a fine-grained picture of
changes to Wikipedia article pages, we rely on the
notion of edits defined in our previous work (Dax-
enberger and Gurevych, 2012). Edits are coherent
modifications based on a pair of adjacent revisions
from Wikipedia article pages. To calculate edits,
a line-based diff comparison between the old re-
vision and the new revision is made, followed by
several post-processing steps. Each pair of adja-
cent revisions found in the edit history of an arti-
cle consists of one or more edits, which describe
either inserted, deleted, changed or relocated text.
Edits are associated with metadata from the revi-
sion they belong to, this includes the comment (if
present), the user name and the time stamp.
Turns Turns are segments from Wikipedia dis-
cussion pages. To segment discussion pages into
turns, we follow a procedure proposed by Fer-
schke et al (2012). With the help of the Java
Wikipedia Library (Zesch et al, 2008), we ac-
cess discussion pages from a database. Discus-
sion pages are then segmented into topics based
upon the structure of the page. Individual turns
are retrieved from topics by considering the revi-
sion history of the discussion page. This proce-
dure successfully segmented 94 % of all turns in
a corpus from the Simple English Wikipedia (Fer-
schke et al, 2012). Along with each turn, we store
the name of its user, the time stamp, and the name
of the topic to which the turn belongs.
Corresponding Edit-Turn-Pairs An edit-turn-
pair is defined as a pair of an edit from a Wikipedia
article?s revision history and a turn from the dis-
cussion page bound to the same article. If an arti-
cle has no discussion page, there are no edit-turn-
pairs for this article.
A definition of correspondence is not straight-
forward in the context of edit-turn-pairs. Ferschke
et al (2012) suggest four types of explicit perfor-
matives in their annotation scheme for dialog acts
of Wikipedia turns. Due to their performative na-
ture, we assume that these dialog acts make the
turn they belong to a good candidate for a cor-
responding edit-turn-pair. We therefore define an
edit-turn-pair as corresponding, if: i) The turn is
an explicit suggestion, recommendation or request
and the edit performs this suggestion, recommen-
dation or request, ii) the turn is an explicit refer-
ence or pointer and the edit adds or modifies this
reference or pointer, iii) the turn is a commitment
to an action in the future and the edit performs this
action, and iv) the turn is a report of a performed
action and the edit performs this action. We define
all edit-turn-pairs which do not conform to the up-
per classification as non-corresponding.
3 Corpus
With the help of Amazon Mechanical Turk
1
, we
crowdsourced annotations on a corpus of edit-
turn-pairs from 26 random English Wikipedia ar-
ticles in various thematic categories. The search
space for corresponding edit-turn-pairs is quite
big, as any edit to an article may correspond to any
turn from the article?s discussion page. Assuming
that most edit-turn-pairs are non-corresponding,
we expect a heavy imbalance in the class distribu-
tion. It was important to find a reasonable amount
of corresponding edit-turn-pairs before the actual
annotation could take place, as we needed a cer-
tain amount of positive seeds to keep turkers from
simply labeling pairs as non-corresponding all the
time. In the following, we explain the step-by-step
approach we chose to create a suitable corpus for
the annotation study.
Filtering We applied various filters to avoid an-
notating trivial content. Based on an automatic
classification using the model presented in our pre-
vious work (Daxenberger and Gurevych, 2013),
we excluded edits classified as Vandalism, Revert
or Other. Furthermore, we removed all edits which
are part of a revision created by bots, based on the
Wikimedia user group
2
scheme. To keep the class
imbalance within reasonable margins, we limited
the time span between edits and turns to 86,000
1
www.mturk.com
2
http://meta.wikimedia.org/wiki/User_
classes
188
seconds (about 24 hours). The result is a set of
13,331 edit-turn-pairs, referred to as ETP-all.
Preliminary Annotation Study From ETP-all,
a set of 262 edit-turn-pairs have been annotated
as corresponding as part of a preliminary annota-
tion study with one human annotator. This step is
intended to make sure that we have a substantial
number of corresponding pairs in the data for the
final annotation study. However, we still expect
a certain amount of non-corresponding edit-turn-
pairs in this data, as the annotator judged the cor-
respondence based on the entire revision and not
the individual edit. We refer to this 262 edit-turn-
pairs as ETP-unconfirmed.
Mechanical Turk Annotation Study Finally,
for the Mechanical Turk annotation study, we se-
lected 500 random edit-turn-pairs from ETP-all
excluding ETP-unconfirmed. Among these, we
expect to find mostly non-corresponding pairs.
From ETP-unconfirmed, we selected 250 ran-
dom edit-turn-pairs. The resulting 750 pairs have
each been annotated by five turkers. The turk-
ers were presented the turn text, the turn topic
name, the edit in its context, and the edit comment
(if present). The context of an edit is defined as
one preceding and one following paragraph of the
edited paragraph. Each edit-turn-pair could be la-
beled as ?corresponding?, ?non-corresponding? or
?can?t tell?. To select good turkers and to block
spammers, we carried out a pilot study on a small
portion of manually confirmed corresponding and
non-corresponding pairs, and required turkers to
pass a qualification test.
The average pairwise percentage agreement
over all pairs is 0.66. This was calculated as
1
N
?
N
i=1
?
C
c=1
v
c
i
C
, where N = 750 is the overall
number of annotated edit-turn-pairs, C =
R
2
?R
2
is
the number of pairwise comparisons, R = 5 is the
number of raters per edit-turn-pair, and v
c
i
= 1 if a
pair of raters c labeled edit-turn-pair i equally, and
0 otherwise. The moderate pairwise agreement re-
flects the complexity of this task for non-experts.
Gold Standard To rule out ambiguous cases,
we created the Gold Standard corpus with the help
of majority voting. We counted an edit-turn-pair
as corresponding, if it was annotated as ?corre-
sponding? by least three out of five annotators,
and likewise for non-corresponding pairs. Further-
more, we deleted 21 pairs for which the turn seg-
?1 2-6 7-11 12-16 17-21
0
20
40
60
time span in hours
%
o
f
p
a
i
r
s
corresponding non-corresponding
Figure 1: Percentage of (non-)corresponding edit-
turn-pairs for various time intervals in ETP-gold.
mentation algorithm clearly failed (e.g. when the
turn text was empty). This resulted in 128 corre-
sponding and 508 non-corresponsing pairs, or 636
pairs in total. We refer to this dataset as ETP-gold.
To assess the reliability of these annotations, one
of the co-authors manually annotated a random
subset of 100 edit-turn-pairs contained in ETP-
gold as corresponding or non-corresponding. The
inter-rater agreement between ETP-gold (major-
ity votes over Mechanical Turk annotations) and
our expert annotations on this subset is Cohen?s
? = .72. We consider this agreement high enough
to draw conclusions from the annotations (Artstein
and Poesio, 2008).
Obviously, this is a fairly small dataset which
does not cover a representative sample of articles
from the English Wikpedia. However, given the
high price for a new corresponding edit-turn-pair
(due to the high class imbalance in random data),
we consider it as a useful starting point for re-
search on edit-turn-pairs in Wikipedia. We make
ETP-gold freely available.
3
As shown in Figure 1, more than 50% of all
corresponding edit-turn-pairs in ETP-gold occur
within a time span of less than one hour. In our
24 hours search space, the probability to find a
corresponding edit-turn-pair drops steeply for time
spans of more than 6 hours. We therefore expect
to cover the vast majority of corresponding edit-
turn-pairs within a search space of 24 hours.
4 Machine Learning with
Edit-Turn-Pairs
We used DKPro TC (Daxenberger et al, 2014)
to carry out the machine learning experiments on
edit-turn-pairs. For each edit, we stored both the
edited paragraph and its context from the old re-
vision as well as the edited paragraph and con-
text from the new revision. We used Apache
3
http://www.ukp.tu-darmstadt.de/data/
edit-turn-pairs
189
OpenNLP
4
for the segmentation of edit and turn
text. Training and testing the classifier has been
carried out with the help of the Weka Data Mining
Software (Hall et al, 2009). We used the Sweble
parser (Dohrn and Riehle, 2011) to remove Wiki
markup.
4.1 Features
In the following, we list the features extracted
from preprocessed edits and turns. The edit text
is composed of any inserted, deleted or relocated
text from both the old and the new revision. The
edit context includes the edited paragraph and one
preceding and one following paragraph. The turn
text includes the entire text from the turn.
Similarity between turn and edit text We pro-
pose a number of features which are purely based
on the textual similarity between the text of the
turn, and the edited text and context. We used the
cosine similarity, longest common subsequence,
and word n-gram similarity measures. Cosine sim-
ilarity was applied on binary weighted term vec-
tors (L
2
norm). The word n-gram measure (Lyon
et al, 2004) calculates a Jaccard similarity coeffi-
cient on trigrams. Similarity has been calculated
between i) the plain edit text and the turn text, ii)
the edit and turn text after any wiki markup has
been removed, iii) the plain edit context and turn
text, and iv) the edit context and turn text after any
wiki markup has been removed.
Based on metadata of edit and turn Several of
our features are based on metadata from both the
edit and the turn. We recorded whether the name
of the edit user and the turn user are equal, the
absolute time difference between the turn and the
edit, and whether the edit occurred before the turn.
Cosine similarity, longest common subsequence,
and word n-gram similarity were also applied to
measure the similarity between the edit comment
and the turn text as well as the similarity between
the edit comment and the turn topic name.
Based on either edit or turn Some features are
based on the edit or the turn alone and do not take
into account the pair itself. We recorded whether
the edit is an insertion, deletion, modification or
relocation. Furthermore, we measured the length
of the edit text and the length of the turn text. The
1,000 most frequent uni-, bi- and trigrams from the
turn text are represented as binary features.
4
http://opennlp.apache.org
Baseline R. Forest SVM
Accuracy .799 ?.031 .866 ?.026? .858 ?.027?
F1
mac.
NaN .789 ?.032 .763 ?.033
Precision
mac.
NaN .794 ?.031 .791 ?.032
Recall
mac.
.500 ?.039 .785 ?.032? .736 ?.034?
F1
non-corr.
.888 ?.025 .917 ?.021 .914 ?.022
F1
corr.
NaN .661 ?.037 .602 ?.038
Table 1: Classification results from a 10-fold
cross-validation experiment on ETP-gold with
95% confidence intervals. Non-overlapping inter-
vals w.r.t. the majority baseline are marked by ?.
4.2 Classification Experiments
We treat the automatic classification of edit-turn-
pairs as a binary classification problem. Given the
small size of ETP-gold, we did not assign a fixed
train/test split to the data. For the same reason, we
did not further divide the data into train/test and
development data. Rather, hyperparameters were
optimized using grid-search over multiple cross-
validation experiments, aiming to maximize accu-
racy. To deal with the class imbalance problem,
we applied cost-sensitive classification. In corre-
spondence with the distribution of class sizes in
the training data, the cost for false negatives was
set to 4, and for false positives to 1. A reduction of
the feature set as judged by a ?
2
ranker improved
the results for both Random Forest as well as the
SVM, so we limited our feature set to the 100 best
features.
In a 10-fold cross-validation experiment, we
tested a Random Forest classifier (Breiman, 2001)
and an SVM (Platt, 1998) with polynomial ker-
nel. Previous work (Ferschke et al, 2012; Bronner
and Monz, 2012) has shown that these algorithms
work well for edit and turn classification. As base-
line, we defined a majority class classifier, which
labels all edit-turn-pairs as non-corresponding.
4.3 Discussion and Error Analysis
The classification results for the above configura-
tion are displayed in Table 1. Due to the high
class imbalance in the data, the majority class
baseline sets a challenging accuracy score of .80.
Both classifiers performed significantly better than
the baseline (non-overlapping confidence inter-
vals, see Table 1). With an overall macro-averaged
F1 of .79, Random Forest yielded the best results,
both with respect to precision as well as recall.
The low F1 on corresponding pairs is likely due
to the small number of training examples.
190
To understand the mistakes of the classifier, we
manually assessed error patterns within the model
of the Random Forest classifier. Some of the false
positives (i.e. non-corresponding pairs classified
as corresponding) were caused by pairs where the
revision (as judged by its comment or the edit con-
text) is related to the turn text, however the specific
edit in this pair is not. This might happen, when
somebody corrects a spelling error in a paragraph
that is heavily disputed on the discussion page.
Among the false negatives, we found errors caused
by a missing direct textual overlap between edit
and turn text. In these cases, the correspondence
was indicated only (if at all) by some relationship
between turn text and edit comment.
5 Related Work
Besides the work by Ferschke et al (2012) which
is the basis for our turn segmentation, there are
several studies dedicated to discourse structure in
Wikipedia. Vi?egas et al (2007) propose 11 di-
mensions to classify discussion page turns. The
most frequent dimensions in their sample are re-
quests for coordination and requests for informa-
tion. Both of these may be part of a corresponding
edit-turn-pair, according to our definition in Sec-
tion 2. A subsequent study (Schneider et al, 2010)
adds more dimensions, among these an explicit ca-
tegory for references to article edits. This dimen-
sion accounts for roughly 5 to 10% of all turns.
Kittur and Kraut (2008) analyze correspondence
between article quality and activity on the discus-
sion page. Their study shows that both implicit
coordination (on the article itself) and explicit co-
ordination (on the discussion page of the article)
play important roles for the improvement of arti-
cle quality. In the present study, we have analyzed
cases where explicit coordination lead to implicit
coordination and vice versa.
Kaltenbrunner and Laniado (2012) analyze the
development of discussion pages in Wikipedia
with respect to time and compare dependences be-
tween edit peaks in the revision history of the arti-
cle itself and the respective discussion page. They
find that the development of a discussion page is
often bound to the topic of the article, i.e. arti-
cles on time-specific topics such as events grow
much faster than discussions about timeless, ency-
clopedic content. Furthermore, they observed that
the edit peaks in articles and their discussion pages
are mostly independent. This partially explains the
high number of non-corresponding edit-turn-pairs
and the consequent class imbalance.
While there are several studies which analyze
the high-level relationship between discussion and
edit activity in Wikipedia articles, very few have
investigated the correspondence between edits and
turns on the textual level. Among the latter, Fer-
ron and Massa (2014) analyze 88 articles and their
discussion pages related to traumatic events. In
particular, they find a correlation between the arti-
cle edits and their discussions around the anniver-
saries of the events.
6 Conclusion
The novelty of this paper is a computational analy-
sis of the relationship between the edit history and
the discussion of a Wikipedia article. As far as
we are aware, this is the first study to automati-
cally analyze this relationship involving the tex-
tual content of edits and turns. Based on the types
of turn and edit in an edit-turn-pair, we have oper-
ationalized the notion of corresponding and non-
corresponding edit-turn-pairs. The basic assump-
tion is that in a corresponding pair, the turn con-
tains an explicit performative and the edit corre-
sponds to this performative. We have presented
a machine learning system to automatically detect
corresponding edit-turn-pairs. To test this system,
we manually annotated a corpus of corresponding
and non-corresponding edit-turn-pairs. Trained
and tested on this data, our system shows a sig-
nificant improvement over the baseline.
With regard to future work, an extension of the
manually annotated corpus is the most important
issue. Our classifier can be used to bootstrap the
annotation of additional edit-turn-pairs.
Acknowledgments
The authors would like to give special thanks to
Viswanathan Arunachalam and Dat Quoc Nguyen,
who carried out initial experiments and the pre-
liminary annotation study, and to Emily Jamison,
who set up the Mechanical Turk task. This work
has been supported by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Pro-
gram under grant No. I/82806, and by the Hessian
research excellence program ?Landes-Offensive
zur Entwicklung Wissenschaftlich-?okonomischer
Exzellenz? (LOEWE) as part of the research cen-
ter ?Digital Humanities?. We thank the anony-
mous reviewers for their helpful suggestions.
191
References
Ofer Arazy, Ian Gellatly, Soobaek Jang, and Raymond
Patterson. 2009. Wiki deployment in corporate
settings. IEEE Technology and Society Magazine,
28(2):57?64.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45(1):5?32.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories.
In European Chapter of the Association for Compu-
tational Linguistics (EACL 2012), pages 356?366,
Avignon, France.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics, pages 711?726, Mumbai, In-
dia.
Johannes Daxenberger and Iryna Gurevych. 2013.
Automatically Classifying Edit Categories in
Wikipedia Revisions. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 578?589, Seattle, WA, USA.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
A Java-based Framework for Supervised Learning
Experiments on Textual Data. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics. System Demonstrations,
page (to appear), Baltimore, MD, USA.
Hannes Dohrn and Dirk Riehle. 2011. Design and im-
plementation of the Sweble Wikitext parser. In Pro-
ceedings of the International Symposium on Wikis
and Open Collaboration (WikiSym ?11), pages 72?
81, Mountain View, CA, USA.
Gijsbert Erkens, Jos Jaspers, Maaike Prangsma, and
Gellof Kanselaar. 2005. Coordination processes in
computer supported collaborative writing. Comput-
ers in Human Behavior, 21(3):463?486.
Michela Ferron and Paolo Massa. 2014. Beyond the
encyclopedia: Collective memories in Wikipedia.
Memory Studies, 7(1):22?45.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-
tar. 2012. Behind the Article: Recognizing Dialog
Acts in Wikipedia Talk Pages. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 777?
786, Avignon, France.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian Witten. 2009.
The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10?18.
Andreas Kaltenbrunner and David Laniado. 2012.
There is No Deadline - Time Evolution of Wikipedia
Discussions. In Proceedings of the Annual Interna-
tional Symposium on Wikis and Open Collaboration,
Linz, Austria.
Aniket Kittur and Robert E. Kraut. 2008. Harnessing
the wisdom of crowds in wikipedia: quality through
coordination. In Proceedings of the 2008 ACM Con-
ference on Computer Supported Cooperative Work,
pages 37?46, San Diego, CA, USA.
C. Lyon, R. Barrett, and J. Malcolm. 2004. A theoret-
ical basis to the automated detection of copying be-
tween texts, and its practical implementation in the
Ferret plagiarism and collusion detector. In Plagia-
rism: Prevention, Practice and Policy Conference,
Newcastle, UK.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In Bernhard Sch?olkopf, Christopher J. C. Burges,
and Alexander J. Smola, editors, Advances in Kernel
Methods: Support Vector Learning, pages 185?208.
MIT Press.
Jodi Schneider, Alexandre Passant, and John G. Bres-
lin. 2010. A Content Analysis: How Wikipedia
Talk Pages Are Used. In Proceedings of the 2nd In-
ternational Conference of Web Science, pages 1?7,
Raleigh, NC, USA.
Fernanda B. Vi?egas, Martin Wattenberg, Jesse Kriss,
and Frank Ham. 2007. Talk Before You Type: Co-
ordination in Wikipedia. In Proceedings of the 40th
Annual Hawaii International Conference on System
Sciences, pages 78?78, Big Island, HI, USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco.
192
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 61?66,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
DKPro TC: A Java-based Framework for Supervised Learning
Experiments on Textual Data
Johannes Daxenberger
?
, Oliver Ferschke
??
, Iryna Gurevych
??
and Torsten Zesch
??
? UKP Lab, Technische Universit?t Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
We present DKPro TC, a framework for
supervised learning experiments on tex-
tual data. The main goal of DKPro TC is
to enable researchers to focus on the actual
research task behind the learning problem
and let the framework handle the rest. It
enables rapid prototyping of experiments
by relying on an easy-to-use workflow en-
gine and standardized document prepro-
cessing based on the Apache Unstruc-
tured Information Management Architec-
ture (Ferrucci and Lally, 2004). It ships
with standard feature extraction modules,
while at the same time allowing the user
to add customized extractors. The exten-
sive reporting and logging facilities make
DKPro TC experiments fully replicable.
1 Introduction
Supervised learning on textual data is a ubiquitous
challenge in Natural Language Processing (NLP).
Applying a machine learning classifier has be-
come the standard procedure, as soon as there is
annotated data available. Before a classifier can
be applied, relevant information (referred to as
features) needs to be extracted from the data. A
wide range of tasks have been tackled in this way
including language identification, part-of-speech
(POS) tagging, word sense disambiguation, sen-
timent detection, and semantic similarity.
In order to solve a supervised learning task,
each researcher needs to perform the same set of
steps in a predefined order: reading input data,
preprocessing, feature extraction, machine learn-
ing, and evaluation. Standardizing this process
is quite challenging, as each of these steps might
vary a lot depending on the task at hand. To com-
plicate matters further, the experimental process
is usually embedded in a series of configuration
changes. For example, introducing a new fea-
ture often requires additional preprocessing. Re-
searchers should not need to think too much about
such details, but focus on the actual research task.
DKPro TC is our take on the standardization of
an inherently complex problem, namely the imple-
mentation of supervised learning experiments for
new datasets or new learning tasks.
We will make some simplifying assumptions
wherever they do not harm our goal that the frame-
work should be applicable to the widest possible
range of supervised learning tasks. For example,
DKPro TC only supports a limited set of machine
learning frameworks, as we argue that differences
between frameworks will mainly influence run-
time, but will have little influence on the final con-
clusions to be drawn from the experiment. The
main goal of DKPro TC is to enable the researcher
to quickly find an optimal experimental configura-
tion. One of the major contributions of DKPro TC
is the modular architecture for preprocessing and
feature extraction, as we believe that the focus of
research should be on a meaningful and expressive
feature set. DKPro TC has already been applied to
a wide range of different supervised learning tasks,
which makes us confident that it will be of use to
the research community.
DKPro TC is mostly written in Java and freely
available under an open source license.
1
2 Requirements
In the following, we give a more detailed overview
of the requirements and goals we have identified
for a general-purpose text classification system.
These requirements have guided the development
of the DKPro TC system architecture.
1
http://dkpro-tc.googlecode.com
61
Single-label Multi-label Regression
Document Mode
? Spam Detection
? Sentiment Detection
? Text Categorization
? Keyphrase Assignment
? Text Readability
Unit/Sequence Mode
? Named Entity Recognition
? Part-of-Speech Tagging
? Dialogue Act Tagging ? Word Difficulty
Pair Mode
? Paraphrase Identification
? Textual Entailment
? Relation Extraction ? Text Similarity
Table 1: Supervised learning scenarios and feature modes supported in DKPro TC, with example NLP
applications.
Flexibility Users of a system for supervised
learning on textual data should be able to choose
between different machine learning approaches
depending on the task at hand. In supervised ma-
chine learning, we have to distinguish between ap-
proaches based on classification and approaches
based on regression. In classification, given a
document d ? D and a set of labels C =
{c
1
, c
2
, ..., c
n
}, we want to label each document
d with L ? C, where L is the set of relevant
or true labels. In single-label classification, each
document d is labeled with exactly one label, i.e.
|L| = 1, whereas in multi-label classification, a
set of labels is assigned, i.e. |L| ? 1. Single-
label classification can further be divided into bi-
nary classification (|C| = 2) and multi-class clas-
sification (|C| > 2). In regression, real numbers
instead of labels are assigned.
Feature extraction should follow a modular de-
sign in order to facilitate reuse and to allow seam-
less integration of new features. However, the way
in which features need to be extracted from the in-
put documents depends on the the task at hand.
We have identified several typical scenarios in su-
pervised learning on textual data and propose the
following feature modes:
? In document mode, each input document will
be used as its own entity to be classified, e.g.
an email classified as wanted or unwanted
(spam).
? In unit/sequence mode, each input document
contains several units to be classified. The
units in the input document cannot be divided
into separate documents, either because the
context of each unit needs to be preserved
(e.g. to disambiguate named entities) or be-
cause they form a sequence which needs to
be kept (in sequence tagging).
? The pair mode is intended for problems
which require a pair of texts as input, e.g.
a pair of sentences to be classified as para-
phrase or non-paraphrase. It represents a
special case of multi-instance learning (Sur-
deanu et al., 2012), in which a document con-
tains exactly two instances.
Considering the outlined learning approaches and
feature modes, we have summarized typical sce-
narios in supervised learning on textual data in Ta-
ble 1 and added example applications in NLP.
Replicability and Reusability As it has been
recently noted by Fokkens et al. (2013), NLP ex-
periments are not replicable in most cases. The
problem already starts with undocumented pre-
processing steps such as tokenization or sentence
boundary detection that might have heavy impact
on experimental results. In a supervised learning
setting, this situation is even worse, as e.g. fea-
ture extraction is usually only partially described
in the limited space of a research paper. For ex-
ample, a paper might state that ?n-gram features?
were used, which encompasses a very broad range
of possible implementations.
In order to make NLP experiments replicable, a
text classification framework should (i) encourage
the user to reuse existing components which they
can refer to in research papers rather than writ-
ing their own components, (ii) document all per-
formed steps, and (iii) make it possible to re-run
experiments with minimal effort.
Apart from helping the replicability of experi-
ments, reusing components allows the user to con-
centrate on the new functionality that is specific
to the planned experiment instead of having to
reinvent the wheel. The parts of a text classifi-
cation system which can typically be reused are
62
preprocessing components, generic feature extrac-
tors, machine learning algorithms, and evaluation.
3 Architecture
We now give an overview of the DKPro TC archi-
tecture that was designed to take into account the
requirements outlined above. A core design deci-
sion is to model each of the typical steps in text
classification (reading input data and preprocess-
ing, feature extraction, machine learning and eval-
uation) as separate tasks. This modular architec-
ture helps the user to focus on the main problem,
i.e. developing and selecting good features.
In the following, we describe each module in
more detail, starting with the workflow engine that
is used to assemble the tasks into an experiment.
3.1 Configuration and Workflow Engine
We rely on the DKPro Lab (Eckart de Castilho
and Gurevych, 2011) workflow engine, which al-
lows fine-grained control over the dependencies
between single tasks, e.g. the pre-processing of a
document obviously needs to happen before the
feature extraction. In order to shield the user
from the complex ?wiring? of tasks, DKPro TC
currently provides three pre-defined workflows:
Train/Test, Cross-Validation, and Prediction (on
unseen data). Each workflow supports the feature
modes described above: document, unit/sequence,
and pair.
The user is still able to control the behavior of
the workflow by setting parameters, most impor-
tantly the sources of input data, the set of feature
extractors, and the classifier to be used. Internally,
each parameter is treated as a single dimension
in the global parameter space. Users may pro-
vide more than one value for a certain parame-
ter, e.g. specific feature sets or several classifiers.
The workflow engine will automatically run all
possible parameter value combinations (a process
called parameter sweeping).
3.2 Reading Input Data
Input data for supervised learning tasks comes in
myriad different formats which implies that read-
ing data cannot be standardized, but needs to be
handled individually for each data set. However,
the internal processing should not be dependent on
the input format. We therefore use the Common
Analysis Structure (CAS), provided by the Apache
Unstructured Information Management Architec-
ture (UIMA), to represent input documents and
annotations in a standardized way.
Under the UIMA model, reading input data
means to transform arbitrary input data into a
CAS representation. DKPro TC already provides
a wide range of readers from UIMA component
repositories such as DKPro Core.
2
The reader
also needs to assign to each classification unit an
outcome attribute that represents the relevant label
(single-label), labels (multi-label), or a real value
(regression). In unit/sequence mode, the reader
additionally needs to mark the units in the CAS.
In pair mode, a pair of texts (instead of a single
document) is stored within one CAS.
3.3 Preprocessing
In this step, additional information about the docu-
ment is added to the CAS, which efficiently stores
large numbers of stand-off annotations. In pair
mode, the preprocessing is automatically applied
to both documents.
DKPro TC allows the user to run arbitrary
UIMA-based preprocessing components as long
as they are compatible with the DKPro type sys-
tem that is currently used by DKPro Core and
EOP.
3
Thus, a large set of ready-to-use prepro-
cessing components for more than ten languages
is available, containing e.g. sentence boundary de-
tection, lemmatization, POS-tagging, or parsing.
3.4 Feature Extraction
DKPro TC ships a constantly growing number of
feature extractors. Feature extractors have access
to the document text as well as all the additional
information that has been added in the form of
UIMA stand-off annotations during the prepro-
cessing step. Users of DKPro TC can add cus-
tomized feature extractors for particular use cases
on demand.
Among the ready-to-use feature extractors con-
tained in DKPro TC, there are several ones ex-
tracting grammatical information, e.g. the plural-
singular ratio or the ratio of modal to all verbs.
Other features collect information about stylistic
cues of a document, e.g. the number of exclama-
tions or the type-token-ratio. DKPro TC is able to
extract n-grams or skip n-grams of tokens, charac-
ters, and POS tags.
Some feature extractors need access to informa-
tion about the entire document collection, e.g. in
2
http://dkpro-core-asl.googlecode.com
3
http://hltfbk.github.io/Excitement-Open-Platform/
63
order to weigh lexical features with tf.idf scores.
Such extractors have to declare that they depend
on collection level information and DKPro TC
will automatically include a special task that is
executed before the actual features are extracted.
Depending on the feature mode which has been
configured, DKPro TC will extract information
on document level, unit- and/or sequence-level, or
document pair level.
DKPro TC stores extracted features in its inter-
nal feature store. When the extraction process is
finished, a configurable data writer converts the
content from the feature store into a format which
can be handled by the utilized machine learning
tool. DKPro TC currently ships data writers for
the Weka (Hall et al., 2009), Meka
4
, and Mallet
(McCallum, 2002) frameworks. Users can also
add dedicated data writers that output features in
the format used by the machine learning frame-
work of their choice.
3.5 Supervised Learning
For the actual machine learning, DKPro TC cur-
rently relies on Weka (single-label and regres-
sion), Meka (multi-label), and Mallet (sequence
labeling). It contains a task which trains a freely
configurable classifier on the training data and
evaluates the learned model on the test data.
Before training and evaluation, the user may ap-
ply dimensionality reduction to the feature set, i.e.
select a limited number of (expectedly meaning-
ful) features to be included for training and eval-
uating the classifier. DKPro TC uses the feature
selection capabilities of Weka (single-label and re-
gression) and Mulan (multi-label) (Tsoumakas et
al., 2010).
DKPro TC can also predict labels on unseen
(i.e. unlabeled) data, using a trained classifier. In
that case, no evaluation will be carried out, but the
classifier?s prediction for each document will be
written to a file.
3.6 Evaluation and Reporting
DKPro TC calculates common evaluation scores
including accuracy, precision, recall, and F
1
-
score. Whenever sensible, scores are reported for
each individual label as well as aggregated over
all labels. To support users in further analyz-
ing the performance of a classification workflow,
DKPro TC outputs the confusion matrix, the ac-
4
http://meka.sourceforge.net
tual predictions assigned to each document, and a
ranking of the most useful features based on the
configured feature selection algorithm. Additional
task-specific reporting can be added by the user.
As mentioned before, a major goal of
DKPro TC is to increase the replicability of NLP
experiments. Thus, for each experiment, all con-
figuration parameters are stored and will be re-
ported together with the classification results.
4 Tweet Classification: A Use Case
We now give a brief summary of what a supervised
learning task might look like in DKPro TC using
a simple Twitter sentiment classification example.
Assuming that we want to classify a set of tweets
either as ?emotional? or ?neutral?, we can use the
setup shown in Listing 1. The example uses the
Groovy programming language which yields bet-
ter readable code, but pure Java is also supported.
Likewise, a DKPro TC experiment can also be set
up with the help of a configuration file, e.g. in
JSON or via Groovy scripts.
First, we create a workflow as a BatchTask-
CrossValidation which can be used to run
a cross-validation experiment on the data (using
10 folds as configured by the corresponding pa-
rameter). The workflow uses LabeledTweet-
Reader in order to import the experiment data
from source text files into the internal document
representation (one document per tweet). This
reader adds a UIMA annotation that specifies the
gold standard classification outcome, i.e. the rel-
evant label for the tweet. In this use case, pre-
processing consists of a single step: running the
ArkTweetTagger (Gimpel et al., 2011), a spe-
cialized Twitter tokenizer and POS-tagger that is
integrated in DKPro Core. The feature mode is set
to document (one tweet per CAS), and the learning
mode to single-label (each tweet is labeled with
exactly one label), cf. Table 1.
Two feature extractors are configured: One for
returning the number of hashtags and another one
returning the ratio of emoticons to tokens in the
tweet. Listing 2 shows the Java code for the sec-
ond extractor. Two things are noteworthy: (i) doc-
ument text and UIMA annotations are readily
available through the JCas object, and (ii) this is
really all that the user needs to write in order to
add a new feature extractor.
The next item to be configured is the Weka-
DataWriter which converts the internal fea-
64
BatchTaskCrossValidation batchTask = [
experimentName: "Twitter-Sentiment",
preprocessingPipeline: createEngineDescription(ArkTweetTagger), // Preprocessing
parameterSpace: [ // multi-valued parameters in the parameter space will be swept
Dimension.createBundle("reader", [
readerTrain: LabeledTweetReader,
readerTrainParams: [LabeledTweetReader.PARAM_CORPUS_PATH, "src/main/resources/tweets.txt"]]),
Dimension.create("featureMode", "document"),
Dimension.create("learningMode", "singleLabel"),
Dimension.create("featureSet", [EmoticonRatioExtractor.name, NumberOfHashTagsExtractor.name]),
Dimension.create("dataWriter", WekaDataWriter.name),
Dimension.create("classificationArguments", [NaiveBayes.name, RandomForest.name])],
reports: [BatchCrossValidationReport], // collects results from folds
numFolds: 10];
Listing 1: Groovy code to configure a DKPro TC cross-validation BatchTask on Twitter data.
public class EmoticonRatioFeatureExtractor
extends FeatureExtractorResource_ImplBase implements DocumentFeatureExtractor
{
@Override
public List<Feature> extract(JCas annoDb) throws TextClassificationException {
int nrOfEmoticons = JCasUtil.select(annoDb, EMO.class).size();
int nrOfTokens = JCasUtil.select(annoDb, Token.class).size();
double ratio = (double) nrOfEmoticons / nrOfTokens;
return new Feature("EmoticonRatio", ratio).asList();
}
}
Listing 2: A DKPro TC document mode feature extractor measuring the ratio of emoticons to tokens.
ture representation into the Weka ARFF format.
For the classification, two machine learning algo-
rithms will be iteratively tested: a Naive Bayes
classifier and a Random Forest classifier. Pass-
ing a list of parameters into the parameter space
will automatically make DKPro TC test all pos-
sible parameter combinations. The classification
task automatically trains a model on the training
data and stores the results of the evaluation on
the test data for each fold on the disk. Finally,
the evaluation scores for each fold are collected
by the BatchCrossValidationReport and
written to a single file using a tabulated format.
5 Related Work
This section will give a brief overview about tools
with a scope similar to DKPro TC. We only list
freely available software, most of which is open-
source. Unless otherwise indicated, all of the tools
are written in Java.
ClearTK (Ogren et al., 2008) is conceptually
closest to DKPro TC and shares many of its dis-
tinguishing features like the modular feature ex-
tractors. It provides interfaces to machine learn-
ing libraries such as Mallet or libsvm, offers wrap-
pers for basic NLP components, and comes with
a feature extraction library that facilitates the de-
velopment of custom feature extractors within the
UIMA framework. In contrast to DKPro TC, it is
rather designed as a programming library than a
customizable research environment for quick ex-
periments and does not provide predefined text
classification setups. Furthermore, it does not sup-
port parameter sweeping and has no explicit sup-
port for creating experiment reports.
Argo (Rak et al., 2013) is a web-based work-
bench with support for manual annotation and au-
tomatic analysis of mainly bio-medical data. Like
DKPro TC, Argo is based on UIMA, but focuses
on sequence tagging, and it lacks DKPro TC?s pa-
rameter sweeping capabilities.
NLTK (Bird et al., 2009) is a general-purpose
NLP toolkit written in Python. It offers com-
ponents for a wide range of preprocessing tasks
and also supports feature extraction and machine
learning for supervised text classification. Like
DKPro TC, it can be used to quickly setup baseline
experiments. As opposed to DKPro TC, NLTK
lacks a modular structure with respect to prepro-
cessing and feature extraction and does not sup-
port parameter sweeping.
Weka (Hall et al., 2009) is a machine learning
framework that covers only the last two steps of
DKPro TC?s experimental process, i.e. machine
learning and evaluation. However, it offers no ded-
icated support for preprocessing and feature gener-
ation. Weka is one of the machine learning frame-
works that can be used within DKPro TC for ac-
tual machine learning.
Mallet (McCallum, 2002) is another machine
65
learning framework implementing several super-
vised and unsupervised learning algorithms. As
opposed to Weka, is also supports sequence tag-
ging, including Conditional Random Fields, as
well as topic modeling. Mallet can be used as ma-
chine learning framework within DKPro TC.
Scikit-learn (Pedregosa et al., 2011) is a ma-
chine learning framework written in Python. It
offers basic functionality for preprocessing, fea-
ture selection, and parameter tuning. It provides
some methods for preprocessing such as convert-
ing documents to tf.idf vectors, but does not offer
sophisticated and customizable feature extractors
for textual data like DKPro TC.
6 Summary and Future Work
We have presented DKPro TC, a comprehensive
and flexible framework for supervised learning on
textual data. DKPro TC makes setting up exper-
iments and creating new features fast and simple,
and can therefore be applied for rapid prototyp-
ing. Its extensive logging capabilities emphasize
the replicability of results. In our own research
lab, DKPro TC has successfully been applied to a
wide range of tasks including author identification,
text quality assessment, and sentiment detection.
There are some limitations to DKPro TC which
we plan to address in future work. To reduce the
runtime of experiments with very large document
collections, we want to add support for parallel
processing of documents. While the current main
goal of DKPro TC is to bootstrap experiments on
new data sets or new applications, we also plan to
make DKPro TC workflows available as resources
to other applications, so that a model trained with
DKPro TC can be used to automatically label tex-
tual data in different environments.
Acknowledgments
This work has been supported by the Volks-
wagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Hessian research excellence pro-
gram ?Landes-Offensive zur Entwicklung
Wissenschaftlich-?konomischer Exzellenz?
(LOEWE) as part of the research center ?Digital
Humanities?. The authors would like give special
thanks to Richard Eckhart de Castilho, Nicolai
Erbs, Lucie Flekova, Emily Jamison, Krish
Perumal, and Artem Vovk for their contributions
to the DKPro TC framework.
References
S. Bird, E. Loper, and E. Klein. 2009. Natural Lan-
guage Processing with Python. O?Reilly Media Inc.
R. Eckart de Castilho and I. Gurevych. 2011. A
Lightweight Framework for Reproducible Parame-
ter Sweeping in Information Retrieval. In Proc. of
the Workshop on Data Infrastructures for Support-
ing Information Retrieval Evaluation, pages 7?10.
D. Ferrucci and A. Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
A. Fokkens, M. van Erp, M. Postma, T. Pedersen,
P. Vossen, and N. Freire. 2013. Offspring from
Reproduction Problems: What Replication Failure
Teaches Us. In Proc. ACL, pages 1691?1701.
K. Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. Smith. 2011. Part-of-speech
tagging for Twitter: annotation, features, and exper-
iments. In Proc. ACL, pages 42?47.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. Witten. 2009. The WEKA Data Min-
ing Software: An Update. SIGKDD Explorations,
11(1):10?18.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
P. Ogren, P. Wetzler, and S. Bethard. 2008. ClearTK:
A UIMA toolkit for statistical natural language pro-
cessing. In Towards Enhanced Interoperability for
Large HLT Systems: UIMA for NLP workshop at
LREC, pages 32?38.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
R. Rak, A. Rowley, J. Carter, and S. Ananiadou.
2013. Development and Analysis of NLP Pipelines
in Argo. In Proc. ACL, pages 115?120.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Proc. EMNLP-CoNLL, pages
455?465.
G. Tsoumakas, I. Katakis, and I. Vlahavas. 2010. Min-
ing Multi-label Data. Transformation, 135(2):1?20.
66
