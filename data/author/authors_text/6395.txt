Parmenides: an opportunity for ISO TC37 SC4?
Fabio Rinaldi
1
, James Dowdall
1
, Michael Hess
1
, Kaarel Kaljurand
1
, Andreas Persidis
2
,
Babis Theodoulidis
3
, Bill Black
3
, John McNaught
3
, Haralampos Karanikas
3
, Argyris Vasilakopoulos
3
,
Kelly Zervanou
3
, Luc Bernard
3
, Gian Piero Zarri
4
, Hilbert Bruins Slot
5
, Chris van der Touw
5
,
Margaret Daniel-King
6
, Nancy Underwood
6
, Agnes Lisowska
6
, Lonneke van der Plas
6
,
Veronique Sauron
6
, Myra Spiliopoulou
7
, Marko Brunzel
7
, Jeremy Ellman
8
,
Giorgos Orphanos
9
, Thomas Mavroudakis
10
, Spiros Taraviras
10
.
Abstract
Despite the many initiatives in recent years
aimed at creating Language Engineering
standards, it is often the case that dierent
projects use dierent approaches and often
dene their own standards. Even within the
same project it often happens that dierent
tools will require dierent ways to represent
their linguistic data.
In a recently started EU project focusing
on the integration of Information Extrac-
tion and Data Mining techniques, we aim
at avoiding the problem of incompatibility
among dierent tools by dening a Com-
mon Annotation Scheme internal to the
project. However, when the project was
started (Sep 2002) we were unaware of the
standardization eort of ISO TC37/SC4,
and so we commenced once again trying to
dene our own schema. Fortunately, as this
work is still at an early stage (the project
will last till 2005) it is still possible to redi-
rect it in a way that it will be compati-
ble with the standardization work of ISO.
In this paper we describe the status of the
work in the project and explore possible
synergies with the work in ISO TC37 SC4.
1 1
Institute of Computational Linguistics, Uni-
versity of Zurich, Switzerland;
2
Biovista, Athens,
Greece;
3
Centre for Research in Information Manage-
ment, UMIST, Manchester, UK;
4
CNRS, Paris, France;
5
Unilever Research and Development, Vlaardingen,
The Netherlands;
6
TIM/ISSCO, University of Geneva,
Switzerland;
7
Uni Magdeburg, Germany;
8
Wordmap
Ltd., Bath, UK;
9
Neurosoft, Athens, Greece;
10
The
Greek Ministry of National Defense, Athens, Greece
1 Introduction
It is by now widely accepted that some W3C stan-
dards (such as XML and RDF) provide a con-
venient and practical framework for the creation
of eld-specic markup languages (e.g. MathML,
VoiceXML). However XML provides only a common
\alphabet" for interchange among tools, the steps
that need to be taken before there is any real shar-
ing are still many (just as many human languages
share the same alphabets, that does not mean that
they can be mutually intelligible). The necessary
step to achieve mutual understanding in Language
Resources is to create a common data model.
The existence of a standard brings many other
advantages, like the ability to automatically com-
pare the results of dierent tools which provide the
same functionality, from the very basic (e.g. tok-
enization) to the most complex (e.g. discourse rep-
resentation). Some of the NIST-supported competi-
tive evaluations (e.g. MUC) greatly beneted by the
existence of scoring tools, which could automatically
compare the results of each participant against a gold
standard. The creation of such tools (and their ef-
fectiveness) was possible only because the organizing
institute had pre-dened and \imposed" upon the
participants the annotation scheme. However, that
sort of \brute force" approach might not always pro-
duce the best results. It is important to involve the
community in the denition of such standards at an
early stage, so that all the possible concerns can be
met and a wider acceptance can be achieved.
Another clear benet of agreed standards is that
they will increase interoperability among dierent
tools. It is not enough to have publicly available
APIs to ensure that dierent tools can be integrated.
In fact, if their representation languages (their \data
vocabulary") are too divergent, no integration will
be possible (or at least it will require a considerable
mapping eort). For all the above reasons we enthu-
siastically support any concertation work, aimed at
establishing common foundations for the eld.
In a recently started EU project (\Parmenides")
focusing on the integration of Information Extrac-
tion and Data Mining techniques (for Text Mining)
we aim at avoiding the problem of incompatibility
among dierent tools by dening a Common Annota-
tion Scheme internal to the project. However, when
the project was started (Sep 2002) we were unaware
of the standardization eort of ISO TC37 SC4, and
so we commenced once again trying to dene our own
schema. Fortunately, as this work is still at an early
stage (the project will last till 2005) it is still possible
to redirect it in a way that it will be compatible with
the standardization work of ISO.
In this paper we will describe the approach fol-
lowed so far in the denition of the Parmenides Com-
mon Annotation Scheme, even if its relation with ISO
is still only supercial. In the forthcoming months
our intention is to explore possible synergies between
our work and the current initiatives in ISO TC37
SC4, with the aim to get at a Parmenides annota-
tion scheme which is conformant to the approach cur-
rently discussed in the standardization committee.
2 The Parmenides Lingua Franca
In this section we will describe the XML-based anno-
tation scheme proposed for the Parmenides project.
In general terms the project is concerned with or-
ganisational knowledge management, specically, by
developing an ontology driven systematic approach
to integrating the entire process of information gath-
ering, processing and analysis.
The annotation scheme is intended to work as the
projects' lingua franca: all the modules will be re-
quired to be able to accept as input and generate
as output documents conformant to the (agreed) an-
notation scheme. The specication will be used to
create data-level compatibility among all the tools
involved in the project.
Each tool might choose to use or ignore part of
the information dened by the markup: some infor-
mation might not yet be available at a given stage
of processing or might not be required by the next
module. Facilities will need to be provided for lter-
ing annotations according to a simple conguration
le. This is in fact one of the advantages of using
XML: many readily available o-the-shelf tools can
be used for parsing and ltering the XML annota-
tions, according to the needs of each module.
The annotation scheme will be formally dened by
a DTD and an equivalent XML schema denition.
Ideally the schema should remain exible enough to
allow later additional entities when and if they are
needed. However the present document has only an
illustrative purpose, in particular the set of anno-
tation elements introduced needs to be further ex-
panded and the attributes of all elements need to be
veried.
There are a number of simplications which have
been taken in this document with the purpose of
keeping the annotation scheme as simple as possible,
however they might be put into question and more
complex approaches might be required. For instance
we assume that we will be able to identify a unique
set of tags, suitable for all the applications. If this
proves to be incorrect, a possible way to deal with
the problem is the use of XML namespaces. Our
assumptions allow us (for the moment) to keep all
XML elements in the same namespace (and there-
fore ignore the issue altogether).
2.1 Corpus Development
The annotation scheme will be used to create a de-
velopment corpus - a representative sample of the
domain, provided by the users as typical of the doc-
uments they manually process daily. In this phase,
the documents are annotated by domain experts for
the information of interest. This provides the bench-
mark against which algorithms can be developed and
tested to automate extraction as far as possible.
Of primary importance to the annotation process
is the consolidation of the \information of interest",
the text determined as the target of the Information
Extraction modules. Given the projects' goals, this
target will be both diverse and complex necessitating
clarity and consensus.
2.2 Sources Used for this Document
Parmenides aims at using consolidated Information
Extraction techniques, such as Named Entity Ex-
traction, and therefore this work builds upon well-
known approaches, such as the Named Entity anno-
tation scheme from MUC7 (Chinchor, 1997). Cru-
cially, attention will be paid to temporal annota-
tions, with the aim of using extracted temporal in-
formation for detection of trends (using Data Min-
ing techniques). Therefore we have investigated all
the recently developed approaches to such a problem,
and have decided for the adoption of the TERQAS
tagset (Ingria and Pustejovsky, 2002; Pustejovsky et
al., 2002).
Other sources that have been considered include
the GENIA tagset (GENIA, 2003), TEI (TEI Con-
sortium, 2003) and the GDA
1
tagset. The list of
entities introduced so far is by no means complete
1
http://www.i-content.org/GDA/tagset.html
but serves as the starting point, upon which to build
a picture of the domains from information types they
contain. The domain of interests (e.g. Biotechnol-
ogy) are also expected to be terminology-rich and
therefore require proper treatment of terminology.
To supplement the examples presented, a com-
plete document has been annotated according to the
outlined specication.
2
There are currently three
methods of viewing the document which oer dif-
fering ways to visualize the annotations. These
are all based on transformation of the same XML
source document, using XSLT and CSS (and some
Javascript for visualization of attributes). For exam-
ple, the basic view can be seen in gure (1).
3 Levels of Annotation
The set of Parmenides annotations is organized into
three levels:
 Structural Annotations
Used to dene the physical structure of the doc-
ument, it's organization into head and body,
into sections, paragraphs and sentences.
3
 Lexical Annotations
Associated to a short span of text (smaller than
a sentence), and identify lexical units that have
some relevance for the Parmenides project.
 Semantic Annotations
Not associated with any specic piece of text
and as such could be free-oating within the
document, however for the sake of clarity, they
will be grouped into a special unit at the end
of the document. They refer to lexical anno-
tations via co-referential Ids. They (partially)
correspond to what in MUC7 was termed `Tem-
plate Elements' and `Template Relations'.
Structural annotations apply to large text spans,
lexical annotations to smaller text spans (sub-
sentence). Semantic annotations are not directly
linked to a specic text span, however, they are
linked to text units by co-referential identiers.
All annotations are required to have an unique ID
and thus will be individually addressable, this allows
semantic annotations to point to the lexical annota-
tions to which they correspond. Semantic Annota-
tions themselves are given a unique ID, and therefore
can be elements of more complex annotations (\Sce-
nario Template" in MUC parlance).
2
available at http://www.ifi.unizh.ch/Parmenides
3
Apparently the term 'structure' is used with a dif-
ferent meaning in the ISO documentation, referring
to morpho-syntactical structure rather than document
structure.
Structural Annotations The structure of the
documents will be marked using an intuitively appro-
priate scheme which may require further adaptations
to specic documents. For the moment, the root
node is <ParDoc> (Parmenides Document) which
can contain <docinfo>, <body>, <ParAnn>. The
<docinfo> might include a title, abstract or sum-
mary of the documents contents, author informa-
tion and creation/release time. The main body
of the documents (<body>) will be split into sec-
tions (<sec>) which can themselves contain sec-
tions as well as paragraphs (<para>). Within the
paragraphs all sentences will be identied by the
<sentence> tag. The Lexical Annotations will
(normally) be contained within sentences. The -
nal section of all documents will be <ParAnn> (Par-
menides Annotations) where all of the semantic an-
notations that subsume no text are placed. Figure
(2) demonstrates the annotation visualization tool
displaying the documents structure (using nested
boxes).
Lexical Annotations Lexical Annotations are
used to mark any text unit (smaller than a sentence),
which can be of interest in Parmenides. They include
(but are not limited to):
1. Named Entities in the classical MUC sense
2. New domain-specic Named Entities
3. Terms
4. Temporal Expressions
5. Events
6. Descriptive phrases (chunks)
The set of Lexical Annotations described in this
document will need to be further expanded to cover
all the requirements of the project, e.g. names of
products (Acme Arms International's KryoZap (TM)
tear gas riot control gun), including e.g. names of
drugs (Glycocortex's Siderocephalos).
When visualizating the set of Lexical Tags in a
given annotated document, clicking on specic tags
displays the attribute values (see gure (3)).
Semantic Annotations The relations that exist
between lexical entities are expressed through the
semantic annotations. So lexically identied peo-
ple can be linked to their organisation and job ti-
tle, if this information is contained in the document
(see gure (4)). In terms of temporal annotations, it
is the explicit time references and events which are
identied lexically, the temporal relations are then
captured through the range of semantic tags.
Figure 1: Basic Annotation Viewing
3.1 Example
While the structural annotations and lexical annota-
tions should be easy to grasp as they correspond to
accepted notions of document structure and of con-
ventional span-based annotations, an example might
help to illustrate the role of semantic annotations.
(1) The recent ATP award is
<ENAMEX id="e8" type="ORGANIZATION">
Dyax
</ENAMEX>
's second, and follows a
<NUMEX id="n5" type="MONEY">
$4.3 million
</NUMEX>
<ENAMEX id="e9" type="ORGANIZATION">
NIST
</ENAMEX>
grant to
<ENAMEX id="e10" type="ORGANIZATION">
Dyax
</ENAMEX>
and
<ENAMEX id="e11" type="ORGANIZATION">
CropTech Development Corporation
</ENAMEX>
in
<TIMEX3 tid="t4" type="DATE" value="1997">
1997
</TMEX3>
There are two occurrences of Dyax in this short
text: the two Lexical Entities e8 and e10, but clearly
they correspond to the same Semantic Entity. To
capture this equivalence, we could use the syntactic
notion of co-reference (i.e. Identify the two as co-
referent). Another possible approach is to make a
step towards the conceptual level, and create a se-
mantic entity, of which both e8 and e10 are lexical
expressions (which could be dierent, e.g. \Dyax",
\Dyax Corp.", \The Dyax Corporation"). The sec-
ond approach can be implemented using an empty
XML element, created whenever a new entity is men-
tioned in text. For instance, in (2) we can use the tag
Figure 2: Visualization of Structural Annotations
<PEntity> (which stands for Parmenides Entity).
(2) <PEntity peid="obj1" type="ORGANIZATION"
mnem="Dyax" refid="e1 e3 e6 e8 e10 e12"/>
The new element is assigned (as usual) a unique
identication number and a type. The attribute mnem
contains just one of the possible ways to refer to the
semantic entity (a mnemonic name, possibly chosen
randomly). However, it also takes as the value of
the refid attribute as many coreferent ids as are
warranted by the document. In this way all lexical
manifestations of a single entity are identied. All
the lexical entities which refer to this semantic entity,
are possible ways to `name' it (see also g. 4).
Notice that the value of the `type' attribute has
been represented here as a string for readability pur-
poses, in the actual specication it will be a pointer
to a concept in a domain-specic Ontology.
Other semantic entities from (1) are:
(3) <PEntity peid="obj2" type="ORGANIZATION"
mnem="NIST" refid="e2 e4 e7 e9"/>
<PEntity peid="obj3" type="ORGANIZATION"
mnem="CropTech" refid="e11"/>
The newly introduced semantic entities can then
be used to tie together people, titles and organiza-
tions on the semantic level. Consider for example
the text fragment (4), which contains only Lexical
Annotations.
(4) ... said
<ENAMEX id="e17" type="PERSON">
Charles R. Wescott
</ENAMEX>
, Ph.D.,
<ROLE type='x' id="x5">
Senior Scientist
</ROLE>
at
<ENAMEX id="e60" type="ORGANIZATION">
Dyax Corp
</ENAMEX>
The Lexical Entity e17 requires the introduction
of a new semantic entity, which is given the arbitrary
identier `obj5':
(5) <PEntity peid="obj5" type="PERSON"
mnem="Charles R. Wescott" refid="e17"/>
Figure 3: Visualization of Lexical Annotations and their attributes
In turn, this entity is linked to the entity obj1
from (1) by a relation of type `workFor' (PRelation
stands for Parmenides Relation):
(6) <PRelation prid="rel2" source="obj5"
target="obj1" type="worksFor" role="Senior
Scientist" evidence="x5"/>
4 Discussion
As the status of the Parmenides annotation scheme
is still preliminary, we aim in this section to pro-
vide some justication for the choices done so far
and some comparison with existing alternatives.
4.1 Named Entities
One of the purposes of Named Entities is to instanti-
ate frames or templates representing facts involving
these elements. A minor reason to preserve the clas-
sic named entities is so that we can test an IE system
against the MUC evaluation suites and know how
it is doing compared to the competition and where
there may be lacunae. As such, the MUC-7 speci-
cation (Chinchor, 1997) is adopted with the minor
extension of a non-optional identication attribute
on each tag.
4.2 Terminology
A term is a means of referring to a concept of a spe-
cial subject language; it can be a single wordform,
a multiword form or a phrase, this does not matter.
The only thing that matters is that it has special
reference: the term is restricted to refer to its con-
cept of the special domain. The act of (analytically)
dening xes the special reference of a term to a con-
cept. Thus, it makes no sense to talk of a term not
having a denition. A concept is described by den-
ing it (using other certain specialised linguistic forms
(terms) and ordinary words), by relating it to other
concepts, and by assigning a linguistic form (term)
to it.
If we are interested in fact extraction from densely
terminological texts with few named entities apart
from perhaps names of authors, names of laborato-
ries, and probably many instances of amounts and
measures, then we would need to rely much more on
prior identication of terms in the texts, especially
where these are made up of several word forms.
A term can have many variants: even standard-
ised terms have variants e.g. singular, plural forms
of a noun. Thus we should perhaps more correctly
refer to a termform, at least when dealing with text.
Among variants one can also include acronyms and
reduced forms. You therefore nd a set of variants,
typically, all referring to the same concept in a special
domain: they are all terms (or termforms). Again
this problem pinpoints the need for a separation of
the lexical annotations (the surface variants within
the document) and semantic annotations (pointing
abstractly to the underlying concept).
4.3 Approaches to Temporal Annotations
TIDES (Ferro et al, 2001) is a temporal annota-
tion scheme that was developed at the MITRE Cor-
poration and it can be considered as an extension
of the MUC7 Named Entity Recognition (Tempo-
ral Entity Recognition - TIMEX Recognition) (Chin-
chor, 1997). It aims at annotating and normalizing
explicit temporal references. STAG (Setzer, 2001)
is an annotation scheme developed at the University
of She?eld. It has a wider focus than TIDES in
the sense that it combines explicit time annotation,
event annotation and the ability to annotate tempo-
ral relations between events and times.
TimeML (Ingria and Pustejovsky, 2002) stands for
\Time Markup Language" and represents the inte-
gration and consolidation of both TIDES and STAG.
It was created at the TERQAS Workshop
4
and is
designed to combine the advantages of the previous
temporal annotations schemes. It contains a set of
tags which are used to annotate events, time expres-
sions and various types of event-event, event-time
and time-time relations. TimeML is specically tar-
geted at the temporal attributes of events (time of
occurrence, duration etc.).
As the most complete and recent, TimeML should
be adopted for the temporal annotations. Broadly,
its organization follows the Parmenides distinction
between lexical/semantic annotations. Explicit tem-
poral expressions and events receive an appropriate
(text subsuming) lexical tag. The temporal rela-
tions existing between these entities are then cap-
tured through a range of semantic (non-text subsum-
ing) tags.
For example, each event introduces a correspond-
ing semantic tag. There is a distinction be-
tween event \tokens" and event \instances" moti-
vated by predicates that represent more than one
event. Accordingly, each event creates a semantic
<MAKEINSTANCE> tag that subsumes no text. Ei-
ther, one tag for each realised event or a single tag
with the number of events expressed as the value of
the cardinality attribute. The tag is introduced and
the event or to which it refers is determined by the
attributes eventID.
5 Conclusion
We believe that ISO TC37/SC4 provides a very in-
teresting framework within which specic research
concerns can be addressed without the risk of rein-
venting the wheel or creating another totally new
4
http://www.cs.brandeis.edu/~jamesp/arda/time
and incompatible annotation format. The set of an-
notations that we have been targeting so far in Par-
menides is probably a small subset of what is tar-
geted by ISO TC37/SC4. Although we had only lim-
ited access to the documentation available, we think
our approach is compatible with the work being done
in ISO.
It is, we believe, extremely important for a project
like ours, to be involved directly in the ongoing dis-
cussion. Moreover we are at precisely the right stage
for a more direct `exposure' to the ISO TC37/SC4
discussion, as we have completed the exploratory
work but no irrevocable modeling commitment has
so far been taken. Therefore we would hope to be-
come more involved in order to make our proposal
t exactly into that framework. The end result of
this process might be that Parmenides could become
a sort of \Guinea Pig" for at least a subset of ISO
TC37 SC4.
Acknowledgments
The Parmenides project is funded by the European
Commission (contract No. IST-2001-39023) and
by the Swiss Federal O?ce for Education and Sci-
ence (BBW/OFES). All the authors listed have con-
tributed to the (ongoing) work described in this pa-
per. Any remaining errors are the sole responsibility
of the rst author.
References
Nancy Chinchor. 1997. MUC-7 Named Entity Task Denition, Version
3.5. http://www.itl.nist.gov/iaui/894.02/
related projects/muc/proceedings/ne task.html.
Lisa Ferro, Inderjeet Mani, Beth Sundheim, and George Wilson. 2001.
Tides temporal annotation guidelines, version 1.0.2. Technical re-
port, The MITRE Corporation.
GENIA. 2003. Genia project home page. http://www-tsujii.is.s.u-
tokyo.ac.jp/~genia.
Bob Ingria and James Pustejovsky. 2002. TimeML
Specication 1.0 (internal version 3.0.9), July.
http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
James Pustejovsky, Roser Sauri, Andrea Setzer, Rob Giazauskas, and
Bob Ingria. 2002. TimeML Annotation Guideline 1.00 (internal
version 0.4.0), July. http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
Andrea Setzer. 2001. Temporal Information in Newswire Articles: An
Annotation Scheme and Corpus Study. Ph.D. thesis, University of
She?eld.
TEI Consortium. 2003. The text encoding initiative. http://www.tei-
c.org/.
Figure 4: Visualization of Semantic Annotations
Answering Questions in the Genomics Domain
Fabio Rinaldi, James Dowdall, Gerold Schneider
Institute of Computational Linguistics,
University of Zurich, CH-8057 Zurich
Switzerland
{rinaldi, dowdall, gschneid}@cl.unizh.ch
Andreas Persidis
Biovista, 34 Rodopoleos Str.,
Ellinikon, GR-16777 Athens,
Greece
andreasp@biovista.com
Abstract
In this paper we describe current efforts aimed at
adapting an existing Question Answering system to
a new document set, namely research papers in the
genomics domain. The system has been originally
developed for another restricted domain, however it
has already proved its portability. Nevertheless, the
process is not painless, and the specific purpose of
this paper is to describe the problems encountered.
1 Introduction
One of the core problems in exploiting scientific
papers in research and clinical settings is that the
knowledge that they contain is not easily acces-
sible. Although various resources which attempt
to consolidate such knowledge are being created
(e.g. UMLS1, SWISS-PROT, OMIM, GeneOntol-
ogy, GenBank, LocusLink), the amount of informa-
tion available keeps growing exponentially (Stapley
and Benoit, 2000).
There is accordingly a pressing need for intelli-
gent systems capable of accessing that information
in an efficient and user-friendly way. Question An-
swering systems aim at providing a focused way
to access the information contained in a document
collection. Specific research in the area of Ques-
tion Answering has been prompted in the last few
years in particular by the Question Answering track
of the Text REtrieval Conference (TREC-QA) com-
petitions (Voorhees, 2001). The TREC-QA compe-
titions focus on open-domain systems, i.e. systems
that can (potentially) answer any generic question.
As these competitions are based on large volumes
of text, the competing systems (normally) resort to a
relatively shallow text analysis.2 In contrast a ques-
tion answering system working on a restricted do-
main can take advantage of the formatting and style
1
http://www.nlm.nih.gov/research/umls/
2With some notable exception, e.g. (Harabagiu et al, 2001).
conventions in the text, can make use of the specific
domain-dependent terminology, and of full parsing.
In many restricted domains, including technical
documentation and research papers, terminology
plays a pivotal role. This is in fact one of the
major differences between restricted domains and
open domain texts. While in open domain systems
Named Entities play a major role, in technical doc-
umentation, as well as in research papers, they have
a secondary role, by contrast a far greater role is
played by domain terminology. Terminology is a
major obstacle for processing research papers and
at the same time a key access path to the knowledge
encoded in those papers. Terminology provides the
means to name and access domain-specific concepts
and objects.
Restricted domains present the additional prob-
lem of ?domain navigation?. Users of the system
cannot always be expected to be completely fa-
miliar with the domain terminology. Unfamiliar-
ity with domain terminology might lead to ques-
tions which contain imperfect formulations of do-
main terms. It becomes therefore essential to be
able to detect terminological variants and exploit the
relations between terms (like synonymy, meronymy,
antonymy). The process of variation is well in-
vestigated in terminological research (Daille et al,
1996). In the Biomedical domain, an example of a
system that deals with terminological variants (also
called ?aliases?) can be found in (Pustejovsky et al,
2002).
In the rest of this paper we will first briefly de-
scribe our existing Question Answering system, Ex-
trAns (section 2). In the following section (3) we
detail the specific problems encountered in the new
domain and the steps that we have taken to solve
them. We conclude the paper with an overview of
related research (section 4).
Figure 1: Example of document to be analyzed
2 The original Question Answering system
ExtrAns is a Question Answering system aimed at
restricted domains, in particular terminology-rich
domains. While open domain Question Answering
systems typically are targeted at large text collec-
tions and use relatively little linguistic information,
ExtrAns answers questions over such domains by
exploiting linguistic knowledge from the documents
and terminological knowledge about a specific do-
main. Various applications of the ExtrAns system
have been developed, from the original prototype
aimed at the Unix documentation files (Molla? et al,
2000) to a version targeting the Aircraft Mainte-
nance Manuals (AMM) of the Airbus A320 (Molla?
et al, 2003; Rinaldi et al, 2004). In the present pa-
per we describe current work in applying the system
to a different domain and text type: research papers
in the genomics area.
Our approach to Question Answering is particu-
larly computationally intensive; this allows a deeper
linguistic analysis to be performed, at the cost of
higher processing time. The documents are an-
alyzed in an off-line stage and transformed in a
semantic representation (called ?Minimal Logical
Forms? or MLFs), which is stored in a Knowledge
Base (KB). In an on-line phase (see fig. 2) the user
queries are analyzed using the same basic machin-
ery (however the cost of processing them is neg-
ligible, so that there is no visible delay) and their
semantic representation is matched in the KB. If a
match is encountered, the sentences that gave origin
to the match are presented as possible answer to the
question.
Documents (and queries) are first tokenized, then
they go through a terminology-processing module.
If a term belonging to a synset in the terminolog-
ical knowledge base is detected, then the term is
replaced by a synset identifier in the logical form.
This results in a canonical form, where the synset
identifier denotes the concept that each of the terms
in the synset names. In this way any term contained
in a user query is automatically mapped to all its
variants. This approach amounts to an implicit ?ter-
minological normalization? for the domain, where
the synset identifier can be taken as a reference to
SemanticMatching
DocumentKB
document logicalform
AnswersinDocument
DocumentLinguisticProcessingQUERY QueryFiltering
Thesaurus
QUERY+Synset
Figure 2: Schematic representation of the core QA engine
the ?concept? that each of the terms in the synset de-
scribes (Kageura, 2002).
ExtrAns depends heavily on its use of logical
forms, which are designed so that they are easy to
build and to use, yet expressive enough for the task
at hand (Molla?, 2001). The logical forms and asso-
ciated semantic interpretation methods are designed
to cope with problematic sentences, which include
very long sentences, even sentences with spelling
mistakes, and structures that are not recognized by
the syntactic analyzer. An advantage of ExtrAns?
Minimal Logical Forms (MLFs) is that they can be
produced with minimal domain knowledge. This
makes our technology easily portable to different
domains. The only true impact of the domain is
during the preprocessing stage of the input text and
during the creation of a thesaurus that reflects the
specific terms used in the chosen domain, their lex-
ical relations and their word senses.
Unlike sentences in documents, user queries
are processed on-line and the resulting MLFs are
proved by deduction over the MLFs of document
sentences stored in the KB. When no direct answer
for a user query can be found, the system is able to
relax the proof criteria in a stepwise manner. First,
hyponyms are added to the query terms. This makes
the query more general but maintains its logical cor-
rectness. If no answers can be found or the user
determines that they are not good answers, the sys-
tem will attempt approximate matching, in which
the sentence that has the highest overlap of predi-
cates with the query is retrieved. The matching sen-
tences are scored and the best matches are returned.
The MLFs contain pointers to the original text
which allow ExtrAns to identify and highlight those
words in the retrieved sentence that contribute most
to a particular answer. An example of the output of
ExtrAns can be seen in fig. 3. When the user clicks
on one of the answers provided, the corresponding
document will be displayed with the relevant pas-
sages highlighted. Another click displays the an-
swer in the context of the document and allows the
user to verify the justification of the answer.
3 Moving to the new domain
The first step in adapting the system to a new do-
main is identifying the specific set of documents to
be analyzed. We have experimented with two dif-
ferent collections in the genomics domain. The first
collection (here called the ?Biovista? corpus) has
been generated from Medline using two seed term
lists of genes and pathways (biological process) to
extract an initial corpus of research papers (full ar-
ticles). The second collection is constituted by the
GENIA corpus (Kim et al, 2003)3, which contains
2000 abstracts from Medline (a total of 18546 sen-
tences). The advantage of the latter is that domain-
specific terminology is already manually annotated.
However focusing only on that case would mean
disregarding a number of real-world problems (in
particular terminology detection).
3.1 Formatting information
An XML based filtering tool has been used to select
zones of the documents that need to be processed
in a specific fashion. Consider for instance the case
of bibliography. The initial structure of the docu-
ment allows to identify easily each bibliographical
item. Isolating the authors, titles and publication in-
formation is then trivial (because it follows a regular
structure). The name of the authors (together with
the html cross-references) can then be used to iden-
tify the citations within the main body of the paper.
If a preliminary zone identification (as described) is
not performed, the names of the authors used in the
citations would appear as spurious elements within
sentences, making their analysis very difficult.
Another common case is that of titles. Normally
they are Nominal Phrases rather than sentences. If
3
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
Figure 3: Example of interaction with the system
the parser was expecting to find a sentence it would
fail. However using the knowledge that a title is
being processed, we can modify the configuration
of the parser so that it accepts an NP as a correct
parse.
3.2 Terminology
The high frequency of terminology in technical text
produces various problems when locating answers.
A primary problem is the increased difficulty of
parsing text in a technical domain due to domain-
specific sublanguage. Various types of multi-word
terms characterize these domains, in particular re-
ferring to specific concepts (e.g. genome sequences,
proteins). These multi-word expressions might in-
clude lexical items which are either unknown to a
generic lexicon (e.g. ?argentine methylation?), have
a specific meaning unique to this domain or dever-
bal adjectives (and nouns) are often mistagged as
verbs (e.g. ?mediated activation?, ?cell killing?).
Abbreviations and acronyms, often complex (e.g.
bracketed inside NPs, like ?adenovirus (ad) infec-
tion?) are another common source of inconsisten-
cies. In such cases the parser might either fail to
identify the compound as a phrase and consequently
fail to parse the sentence including such items. Al-
ternatively a parser might attempt to ?guess? their
lexical category (in the set of open class categories),
leading to an exponential growth of the number of
possible syntactic parses and often incorrect deci-
sions. Not only the internal structure of the com-
pound can be multi-way ambiguous, also the bound-
aries of the compounds are difficult to detect and the
parsers may try odd combinations of the tokens be-
longing to the compounds with neighboring tokens.
We have described in (Rinaldi et al, 2002) some
approaches that might be taken towards terminology
extraction for a specific domain. The GENIA cor-
pus removes these problems completely by provid-
ing pre-annotated terminological units. This allows
attention to be focused on other challenges of the
QA task, rather than getting ?bogged down? with
terminology extraction and organization.
In the case of the Biovista corpus, we had to
perform a phase of terminology discovery, which
was facilitated by the existence of the seed lists of
genes and pathways. We first marked up those terms
which appear in the corpus using additional xml
tags. This identified 900 genes and 218 pathways
that occur in the corpus - represented as boxed to-
kens in fig. 4. Next the entire corpus is chunked into
nominal and verbal chunks using LT Chunk (Finch
and Mikheev, 1997). Ignoring prepositions and
gerunds the chunks are a minimal phrasal group -
represented as the square braces in fig. 4. The cor-
pus terms are then expanded to the boundary of the
phrasal chunk they appear in. For example, NP3 in
fig. 4 contains two terms of interest producing the
new term ?IFN-induced transcription?. The 1118
corpus terms were expanded into 6697 new candi-
date terms. 1060 involve a pathway in head position
and 1154 a gene. The remaining 4483 candidate
terms involve a novel head with at least one gene or
pathway as a modifier.
Once the terminology is available, it is necessary
to detect relations among terms in order to exploit
Argentine methylation of  STAT1  modulates   IFN -induced  transcription
NP1 VBZ
subj
NP2 NP3
prepmodpp
obj
Figure 4: An example of syntactic analysis
it. We have focused our attention in particular to
the relations of synonymy and hyponymy, which
are detected as described in (Dowdall et al, 2003)
and gathered in a Thesaurus. The organizing unit is
the WordNet style synset which includes strict syn-
onymy as well as three weaker synonymy relations.
These sets are further organized into a isa hierarchy
based on two definitions of hyponymy.
One of the most serious problems that we have
encountered in working in restricted domains is
the syntactic ambiguity generated by multi-word
units, in particular technical terms. Any generic
parser, unless developed specifically for the do-
main at hand, will have serious problems dealing
with those multi-words. The solution that we have
adopted is to parse multi-word terms as single syn-
tactic units. The tokenizer detects the terms (pre-
viously collected in the Thesaurus) as they appear in
the input stream, and packs them into single lexical
tokens prior to syntactical analysis, assigning them
the syntactic properties of their head word. In previ-
ous work this approach has proved to be particularly
effective, bringing a reduction in the complexity of
parsing of 46% (Rinaldi et al, 2002).
3.3 Parsing
The deep syntactic analysis builds upon the chunks
to identify sentence level syntactic relations be-
tween the heads of the chunks. The output is a
hierarchical structure of syntactic relations - func-
tional dependency structures - represented as the di-
rected arrows in fig. 4. The parser (Pro3Gres) uses
hand-written declarative rules to encode acknowl-
edged facts, such as verbs typically take one but
never two subjects, combined with a statistical lan-
guage model that calculates lexicalized attachment
probabilities, similar to (Collins, 1999). Parsing is
seen as a decision process, the probability of a total
parse is the product of probabilities of the individual
decisions at each ambiguous point in the derivation.
Probabilistic parsers generally have the advan-
tage that they are fast and robust, and that they
resolve syntactic ambiguities with high accuracy.
Both of these points are prerequisites for a statistical
analysis that is feasible over large amounts of text
and beneficial to the Q&A system?s performance.
In comparison to shallow processing methods,
parsing has the advantage that relations spanning
long stretches of text can still be recognized, and
that the parsing context largely contributes to the
disambiguation. In comparison to deep linguistic,
formal grammar-based parsers, however, the output
of probabilistic parsers is relatively shallow, pure
context-free grammar (CFG) constituency output,
tree structures that do not include grammatical func-
tion annotation nor co-indexation and empty nodes
annotation expressing long-distance dependencies
(LDD). In a simple example sentence ?John wants
to leave?, a deep-linguistic syntactic analysis ex-
presses the identity of the explicit matrix clause
subject and implicit subordinate clause subject by
means of co-indexing the explicit and the empty im-
plicit subject trace t: ?[John1 wants [t1 to leave]]?.
A parser that fails to recognize these implicit sub-
jects, so-called control subjects, misses very impor-
tant information, quantitatively about 3 % of all sub-
jects.
Although LDD annotation is actually provided in
Treebanks such as the Penn Treebank (Marcus et al,
1993) over which they are typically trained, most
probabilistic parsers largely or fully ignore this in-
formation. This means that the extraction of LDDs
and the mapping to shallow semantic representa-
tions such as MLF is not always possible, because
first co-indexation information is not available, sec-
ond a single parsing error across a tree fragment
containing an LDD makes its extraction impossible,
third some syntactic relations cannot be recovered
Figure 5: Dependency Tree output of the SWI Prolog graphical implementation of the parser
on configurational grounds only.
We therefore adapt ExtrAns to use a new statis-
tical broad-coverage parser that is as fast as a prob-
abilistic parser but more deep-linguistic because it
delivers grammatical relation structures which are
closer to predicate-argument structures and shallow
semantic structures like MLF, and more informative
if non-local dependencies are involved (Schneider,
2003). It has been evaluated and shown to have
state-of-the-art performance.
The parser expresses distinctions that are es-
pecially important for a predicate-argument based
shallow semantic representation, as far as they
are expressed in the Penn Treebank training data,
such as PP-attachment, most LDDs, relative clause
anaphora, participles, gerunds, and the argu-
ment/adjunct distinction for NPs.
In some cases functional relations distinctions
that are not expressed in the Penn Treebank are
made. Commas are e.g. disambiguated between
apposition and conjunction, or the Penn tag IN is
disambiguated between preposition and subordinat-
ing conjunction. Other distinctions that are less rel-
evant or not clearly expressed in the Treebank are
left underspecified, such as the distinction between
PP arguments and adjuncts, or a number of types of
subordinate clauses. The parser is robust in that it
returns the most promising set of partial structures
when it fails to find a complete parse for a sentence.
For sentences syntactically more complex than this
illustrative example, as many hierarchical relations
are returned as possible. A screenshot of its graphi-
cal interface can be seen in fig. 5. Its parsing speed
is about 300,000 words per hour.
Fig. 4 displays the three levels of analysis that are
performed on a simple sentence. Term expansion
yields NP3 as a complete candidate term. However,
NP1 and NP2 form two distinct, fully expanded
noun phrase chunks. Their formation into a noun
phrase with an embedded prepositional phrase is re-
covered from the parser?s syntactic relations giv-
ing the maximally projected noun phrase involv-
ing a term: ?Argentine methylation of STAT1? (or
juxtaposed ?STAT1 Argentine methylation?). Fi-
nally, the highest level syntactic relations (subj
and obj) identifies a transitive predicate relation
between these two candidate terms.
3.4 MLFs
The deep-linguistic dependency based parser partly
simplifies the construction of MLF. First, the map-
ping between labeled dependencies and a surface
semantic representation is often more direct than
across a complex constituency subtree (Schneider,
2003), and often more accurate (Johnson, 2002).
Dedicated labels can directly express complex re-
lations, the lexical participants needed for the con-
struction are more locally available.
Let us look at the example sentence ?Aden-
ovirus infection and transfection were used to model
changes in susceptibility to cell killing caused by
E1A expression?. The control relation (infection
is the implicit subject of model) and the PP rela-
tion (including the description noun) are available
locally. The reduced relative clause killing caused
by is expressed by a local dedicated label (modpart).
Only the conjunction infection and transfection, ex-
pressed here by bracketing, needs to be searched
across the syntactic hierarchy.
This leads to the following MLFs:
object(infection, o1, [o1]).
object(transfection, o2, [o2]).
object(change, o3, [o3]).
object(susceptibility, o4, [o4]).
object(killing, o5, [o5]).
object(expression, o6, [o6]).
object(cell, o7, [o7]).
evt(cause, e3, [o6]).
evt(model, e1, [(o1,o2), o3]).
evt(use, e2, [(o1,o2), e1]).
by(e3, o6).
in(o5, o7).
to(o4, o5).
in(o3, o4).
4 Related Work
Question Answering in Biomedicine is surveyed in
detail in (Zweigenbaum, 2003), in particular regard-
ing clinical questions. An example of a system ap-
plied to such questions is presented in (Niu et al,
2003), where it is applied in a setting for Evidence-
Based Medicine. This system identifies specific
?roles? within the document sentences and the ques-
tions, determining the answers is then a matter of
comparing the roles in each. To this aim, natural
language questions are translated into the PICO for-
mat (Sackett et al, 2000).
Automatic knowledge extraction (or strategies for
improving these methods) over Medline articles are
numerous. For example, (Craven and Kumlien,
1999) identifies possible drug-interaction relations
(predicates) between proteins and chemicals using
a ?bag of words? approach applied to the sentence
level. This produces inferences of the type: drug-
interactions (protein, pharmacologic-agent) where
an agent has been reported to interact with a pro-
tein.
(Sekimizu et al, 1998) uses frequently occurring
predicates and identifies the subject and object ar-
guments in the predication, in contrast (Rindflesch
et al, 2000) uses named entity recognition tech-
niques to identify drugs and genes, then identifies
the predicates which connect them. This type of
?object-relation-object? inference may also be im-
plied (Cimino and Barnet, 1993). This method
uses ?if then? rules to extract semantic relationships
between the medical entities depending on which
MeSH headings these entities appear under. For
example, if a citation has ?Electrocardiography?
with the subheading ?Methods? and has ?Myocar-
dial Infarction? with the subheading ?Diagnosis?
then ?Electrocardiography? diagnoses ?Myocardial
Infarction?.
(Spasic? et al, 2003) uses domain-relevant verbs
to improve on terminology extraction. The co-
occurrence in sentences of selected verbs and can-
didate terms reinforces their termhood. But where
such linguistic inferences are stored in a KB as facts,
statistical inferences are only used to visualize pos-
sible relations between objects for further investiga-
tion. (Stapley and Benoit, 2000) measures statistical
gene name co-occurrence and graphically displays
the results for an expert to investigate the dominant
patterns. The PubMed4 system uses the UMLS to
relate metathesaurus concepts against a controlled
vocabulary used to index the abstracts. This allows
efficient retrieval of abstracts from medical journals,
but it makes use of hyponymy and lexical synonymy
to organize the terms. It collects terminologies from
differing sub-domains in a metathesaurus of con-
cepts.
All such inferences (especially statistical) need to
be verified by an expert to ensure their validity. Syn-
tactic parsing, if any, is reserved to shallow NP iden-
tifying strategies (Sekimizu et al, 1998), or possi-
bly supplemented with PP information (Rindflesch
et al, 2000). Semantic interpretation of the docu-
ments is only attempted through their MeSH head-
ings (Mendonca and Cimino, 1999).
5 Conclusion
This paper documents our approach towards QA in
the genomics domain. Although some aspects of
the work described in this paper are still experimen-
tal, we think that the description of the problems
that we have encountered and the specific solutions
adopted or planned will provide an interesting con-
tribution to the workshop. We conclude by observ-
ing that Question Answering is currently seen as an
?advanced? topic in the Genomics Track of TREC5,
due to be targeted for the first time in Year 2 (2005).
Acknowledgments
The authors wish to thank the organizers of the workshop
and the anonymous reviewers for their helpful comments
and suggestions.
References
J.J. Cimino and G.O. Barnet. 1993. Automatic Knowl-
edge Acquisition from Medline. Methods of Informa-
tion in Medicine, 32(2):120?130.
Michael Collins. 1999. Head-Statistical Models for Nat-
ural Language Processing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, USA.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from
4
http://www.ncbi.nlm.nih.gov/pubmed/
5
http://medir.ohsu.edu/?genomics/roadmap.html
text sources. Proceedings of the 8th International
Conference on Intelligent Systems for Molecular Bi-
ology (ISMB-99).
B. Daille, B. Habert, C. Jacquemin, and J. Roy-
aute?. 1996. Empirical observation of term varia-
tions and principles for their description. Termino-
logy, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-Sanjuan,
and Eric Sanjuan. 2003. Complex Structuring of
Term Variants for Question Answering. In Proc. of the
ACL 03, Workshop on Multiword Expression: Analy-
sis, Acquisition and Treatment, Sapporo, Japan, July.
Steve Finch and Andrei Mikheev. 1997. A Workbench
for Finding Structure in Texts. In Proceedings of Ap-
plied Natural Language Processing, Washington, DC,
April.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Rox-
ana G??rju, Vasile Rus, and Paul Morarescu. 2001.
FALCON: Boosting knowledge for answer engines.
In Ellen M. Voorhees and Donna Harman, editors,
Proceedings of the Ninth Text REtrieval Conference
(TREC-9).
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, University of Pennsylvania, Philadelphia.
Kyo Kageura. 2002. The Dynamics of Terminology, A
descriptive theory of term formation and terminologi-
cal growth. Terminology and Lexicography, Research
and Practice. John Benjamins Publishing.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(1):180?182.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics, 19:313?
330.
E. A. Mendonca and J. J. Cimino. 1999. Automated
Knowledge Extraction from Medline Citations. Med-
ical Informatics.
Diego Molla?, Rolf Schwitter, Michael Hess, and Rachel
Fournier. 2000. ExtrAns, an answer extraction sys-
tem. T.A.L. special issue on Information Retrieval ori-
ented Natural Language Processing, pages 495?522.
Diego Molla?, Fabio Rinaldi, Rolf Schwitter, James Dow-
dall, and Michael Hess. 2003. Answer Extraction
from Technical Texts. IEEE Intelligent Systems.
Diego Molla?. 2001. Ontologically promiscuous flat log-
ical forms for NLP. In Harry Bunt, Ielka van der Sluis,
and Elias Thijsse, editors, Proceedings of IWCS-4,
pages 249?265. Tilburg University.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia
Rodriguez-Gianolli. 2003. Answering clinical ques-
tions with role identification. In Sophia Ananiadou
and Jun?ichi Tsujii, editors, Proceedings of the ACL
2003 Workshop on Natural Language Processing in
Biomedicine, pages 73?80.
J. Pustejovsky, J. Castan?o, R. Saur?i, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creating
Large-scale Information Servers for Biomedical Li-
braries. In ACL 2002 Workshop on Natural Language
Processing in the Biomedical Domain. Philadel-
phia, PA. Available at http://www.medstract.
org/publications.html.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Inter-
national Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?30
August.
Fabio Rinaldi, Michael Hess, James Dowdall, Diego
Molla?, and Rolf Schwitter. 2004. Question answering
in terminology-rich technical domains. In Mark May-
bury, editor, New Directions in Question Answering.
AAAI Press.
T.C. Rindflesch, L. Tanabe, J. N. Weinstein, and
L. Hunter. 2000. Edgar: Extraction of drugs, genes
and relations from the biomedical literature. In Pacific
Symposium on Biocomputing, pages 514?25.
D. L. Sackett, S. E. Straus, W. S. Richardson,
W. Rosenberg, and R. B. Haynes. 2000. Evidence
Based Medicine: How to Practice and Teach EBM.
Churchill Livingstone.
Gerold Schneider. 2003. Extracting and Using Trace-
Free Functional Dependencies from the Penn Tree-
bank to Reduce Parsing Complexity. In Proceedings
of The Second Workshop on Treebanks and Linguis-
tic Theories (TLT 2003), Va?xjo?, Sweden, November
14-15.
T. Sekimizu, H. Park, and J Tsujii. 1998. Identifying the
interaction between genes and gene products based on
frequently seen verbs in Medline abstracts. Genome
Informatics, Universal Academy Press.
Irena Spasic?, Goran Nenadic?, and Sophia Ananiadou.
2003. Using domain-specific verbs for term classifi-
cation. In Sophia Ananiadou and Jun?ichi Tsujii, edi-
tors, Proceedings of the ACL 2003 Workshop on Nat-
ural Language Processing in Biomedicine, pages 17?
24.
B.J. Stapley and G. Benoit. 2000. Bibliometrics: infor-
mation retrieval and visualization from co-occurrence
of gene names in medline abstracts. In Proceedings
of the Pacific Symposium on Biocomputing (Oahu,
Hawaii), pages 529?540.
Ellen M. Voorhees. 2001. The TREC question answer-
ing track. Natural Language Engineering, 7(4):361?
378.
Pierre Zweigenbaum. 2003. Question answering in
biomedicine. In Proc. of EACL 03 Workshop: Natu-
ral Language Processing for Question Answering, Bu-
dapest.
