Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1897?1907, Dublin, Ireland, August 23-29 2014.
A Neural Reordering Model for Phrase-based Translation
Peng Li
?
Yang Liu
?
Maosong Sun
?
Tatsuya Izuha
?
Dakun Zhang
?
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
?
Toshiba (China) R&D Center
zhangdakun@toshiba.com.cn
Abstract
While lexicalized reordering models have been widely used in phrase-based translation systems,
they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a
neural reordering model that conditions reordering probabilities on the words of both the current
and previous phrase pairs. Including the words of previous phrase pairs significantly improves
context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we
build one classifier for all phrase pairs, which are represented as continuous space vectors. Ex-
periments on the NIST Chinese-English datasets show that our neural reordering model achieves
significant improvements over state-of-the-art lexicalized reordering models.
1 Introduction
Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004).
While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still
remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete
(Knight, 1999; Zaslavskiy et al., 2009).
The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al.,
2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et
al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012;
Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn
et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Un-
like the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements
in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabil-
ities conditioned on the words of each phrase pair. They often distinguish between three orientations
with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering
models capture the phenomenon that some words are far more likely to be displaced than others, they
outperform unlexicalized reordering models substantially.
Despite their apparent success in statistical machine translation, lexicalized reordering models suffer
from the following three drawbacks:
1. Context insensitivity. Lexicalized reordering models determine the orientations only depending on
the words of current phrase pairs. In fact, a phrase pair usually has different orientations in different
contexts. It is important to include more contexts to improve the expressive power of reordering
models.
2. Ambiguity. Short phrase pairs, which are observed in the training data more frequently, usually have
multiple orientations. We observe that about 92.4% of one-word Chinese-English phrase pairs are
ambiguous. This makes it hard to decide which orientation should be properly used in decoding.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1897
Figure 1: Ambiguity in phrase reordering. The phrase pair ??yingyun?, ?business?? is labeled with
different orientations in different contexts: (a) monotone, (b) swap, and (c) discontinuous. Lexicalized
reordering models use fixed probability distributions (e.g., 17.50% for M, 1.59% for S, and 80.92% for
D) in decoding even though the surrounding contexts keep changing.
3. Sparsity. Lexicalized reordering models maintain a reordering probability distribution for each
phrase pair. As most long phrase pairs that are capable of memorizing local word selection and
reordering only occur once in the training data, maximum likelihood estimation can hardly train the
models accurately.
In this work, we propose a neural reordering model for phrase-based translation. The contribution is
twofold. Firstly, unlike conventional lexicalized reordering models, the neural reordering model condi-
tions reordering probabilities on the words of both the current and previous phrase pairs. Including the
words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambi-
guity. Secondly, to alleviate the data sparsity problem, we build a neural classifier for all phrase pairs,
which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets
show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized
models.
2 Lexicalized Reordering Models
The lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have
become the de facto standard in modern phrase-based systems. These models are called lexicalized
because they condition reordering probabilities on the words of each phrase pair. Depending on the
relationship between the current and previous phrase pairs, lexicalized reordering models often define
orientations to classify different reordering patterns.
More formally, we use f = {
?
f
1
, . . . ,
?
f
n
} to denote a sequence of source phrases, e = {e?
1
, . . . , e?
n
}
to denote the phrase sequence on the target side, and a = {a
1
, . . . , a
n
} to denote the alignment be-
tween source and target phrases. A source phrase
?
f
a
i
and a target phrase e?
i
form a phrase pair. Lex-
icalized reordering models aim to estimate the conditional probability of a sequence of orientations
o = {o
1
, . . . , o
n
}:
P (o|f , e,a) =
n
?
i=1
P (o
i
|f , e?
1
, . . . , e?
i
, a
1
, . . . , a
i
) (1)
where each o
i
takes values over a set of predefined orientations. For simplicity, current lexicalized
1898
model
source phrase length
1 2 3 4 5 6 7
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) 92.74 54.01 24.09 14.40 10.78 8.47 6.95
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
a
i?1
, a
i?1
, a
i
) 21.72 5.22 2.63 1.48 0.98 0.67 0.54
Table 1: Percentages of phrase pairs that have multiple orientations. Including previous phrase pairs in
modeling significantly reduces the reordering ambiguity for the M/S/D orientations. For example, while
92.74% of 1-word Chinese-English phrase pairs have multiple orientations observed in the training data,
the ratio dramatically drops to 21.72% if the orientations are conditioned on both the current and previous
phrase pairs.
reordering models use orientations conditioned only on a
i?1
and a
i
:
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) (2)
The most widely used orientations are monotone (M), swap (S), and discontinuous (D):
1
o
i
=
?
?
?
M if a
i
? a
i?1
= 1
S if a
i
? a
i?1
= ?1
D if |a
i
? a
i?1
| =? 1
(3)
As lexicalized reordering models maintain a reordering probability distribution for each phrase pair,
it is hard to accurately learn reordering probabilities for long phrase pairs that are usually observed only
once in the training data. On the contrary, short phrase pairs that occur in the training data for many times
tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ??yingyun?,
?business?? is observed to have different orientations in different contexts.
It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding
contexts keep changing. Previous study shows that considering more contexts into reordering modeling
improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful
mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity.
3 A Neural Reordering Model
3.1 The Model
Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase
pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering
model is given by
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) (4)
where ?
?
f
a
i?1
, e?
i?1
? is the previous phrase pair.
Including the previous phrase pairs improves the context sensitivity. For example, given a phrase pair
??yingyun?, ?business??, its orientation is more likely to be monotone if it is preceded by a noun phrase
pair such as ??xinyongka?, ?credit card??. On the contrary, the probability of the discontinuous orienta-
tion is higher if the previous phrase pairs contain verbs such as ??gaishan?, ?improve??. Therefore, the
new model is capable of capturing the phenomenon that the orientation of a phrase pair depends on its
surrounding contexts.
Another advantage of including previous phrase pairs is the reduction of reordering ambiguity. As
shown in Table 1, 92.74% of 1-word Chinese-English phrase pairs have multiple orientations (i.e., M, S,
1
There are many variants of lexicalized reordering models depending on the model type, orientation, directionality, lan-
guage, and collapsing. See http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel for more details.
1899
and D) observed in the training data. The ratio decreases with the increase of phrase length. In contrast,
the new model is much less ambiguous (e.g., the ratio of ambiguous one-word phrase pairs dramatically
drops to 21.72%) as it is conditioned on both the current and previous phrase pairs.
Unfortunately, including more contexts in modeling also increases the data sparsity. We observe that
about 90% of reordering examples (i.e., the current and previous phrase pairs) are observed only once in
the training data. As a result, it is more difficult to train lexicalized reordering models accurately using
maximum likelihood estimation.
To alleviate the data sparsity problem, we use the following two strategies:
1. Reordering as classification. Instead of maintaining a reordering probability distribution for each
phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013).
This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as
training examples. We find that 500, 000 reordering examples suffice to train a robust classifier
(Section 4.5).
2. Continuous space representation. Instead of using a symbolic representation of phrases, we use
a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al.,
2011b; Li et al., 2013). Consider two phrases ?in London? and ?in Centara Grand?. It is usually
easy to predict the orientations of ?in London? because it might be observed in the training data for
many times. This is not the case for ?in Centara Grand? as it might occur only once. However, if
the two phrases happen to have very similar continuous space representations, ?in Centara Grand?
is likely to have a similar reordering probability distribution with ?in London?.
To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive
autoencoders. Given two words w
1
and w
2
, suppose their vector space representations are c
1
and c
2
.
The vector space representation p of the two-word phrase {w
1
, w
2
} can be computed using a two-layer
neural network:
p = g
(1)
(W
(1)
[c
1
; c
2
] + b
(1)
) (5)
where [c
1
; c
2
] ? R
2n
is the concatenation of c
1
and c
2
, W
(1)
? R
n?2n
is a weight matrix, b
(1)
is a bias
vector, and g
(1)
is an element-wise activation function.
In order to measure how well p represents c
1
and c
2
, they can be reconstructed using another two-layer
neural network:
[c
?
1
; c
?
2
] = g
(2)
(W
(2)
p + b
(2)
) (6)
where c
?
1
? R
n
and c
?
2
? R
n
are reconstructed vectors of c
1
and c
2
, W
(2)
? R
2n?n
is a weight matrix,
b
(2)
? R
n
is a bias vector, and g
(2)
is an element-wise activation function. The reconstruction error can
be measured by comparing c
1
and c
2
with c
?
1
and c
?
2
. This process runs recursively in a bottom-up style
to obtain the vector space representation of a multi-word phrase (Socher et al., 2011a). Socher et al.
(2011a) find that minimizing the norms of hidden layers leads to the reduction of reconstruction error in
an undesirable way. Therefore, we normalize p such that ||p||
2
= 1.
Treating phrase reordering as a classification problem, we propose a neural reordering classifier that
takes the current and previous phrase pairs as input. The neural network consists of four recursive
autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current
phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e.,
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
)
into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that
the recursive autoencoders for the same language share with the same parameters. Our neural network is
similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector
space representation for variable-sized blocks ranging from words to sentences on the fly both in training
and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training
phase, which makes our approach simpler and more scalable to large data.
1900
Formally, given the previous phrase pair ?
?
f
a
i?1
, e?
i?1
?, the current phrase pair ?
?
f
i
, e?
i
? and the orienta-
tion o
i
, the reordering probability is computed as
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) = g(W
o
c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) + b
o
), (7)
where W
o
is a weight matrix, b
o
is a bias vector, c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) is the concatenation of the vectors
of the four phrases.
2
Following Och (2003), we use a linear model in our decoder with conventional features (e.g., trans-
lation probabilities and n-gram language model). The neural reordering model is incorporated into the
discriminative framework as an additional feature.
3.2 Training
Training the neural reordering model involves minimizing the following two kinds of errors:
? Reconstruction error: It measures how well the computed vector space representations represent
the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees
formed during computing the vector space representation for all the phrases in the training data.
? Classification error: It measures how well the resulting classifier predicts the reordering orienta-
tions. It is defined as the average cross-entropy errors of all the training examples.
In our experiments, the objective function is a linear interpolation of the reconstruction error and the
classification error.
Following Socher et al. (2011b), we use L-BFGS (Liu and Nocedal, 1989) to optimize the parameters.
At the beginning of each iteration, a binary tree for each phrase is constructed using a greedy algorithm
(Socher et al., 2011b).
3
With these trees fixed, the partial derivatives with respect to parameters are
computed via the backpropagation through structures algorithm (Goller and Kuchler, 1996).
When optimizing the parameters of the softmax layer, the training procedure keeps the parameters of
the recursive autoencoders and word embedding matrices fixed. The corresponding error function is the
classification error as described above. We also use L-BFGS to optimize the parameters and the standard
error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives.
3.3 Decoding
As the vector space representation of a phrase is calculated based on all the words in the phrase, using
the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et
al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model
is directly integrated in decoding, making the decoder to only explore in a much smaller search space.
4
Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and
Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model.
4 Experiments
4.1 Data Preparation
We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M
sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was
trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which
contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set,
and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used
2
In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive
autoencoders to the softmax layer. Taking ?resident population? as an example, there are three vectors in the binary tree used
by the corresponding recursive autoencoder, denoted as x?
1
, x?
2
and x?
3
. The average vector is computed as x? =
1
3
?
3
i=1
x?
i
.
3
As phrases in phrase-based translation are not necessarily syntactic constituents, we do not use parse trees in this work.
4
Experimental results show that we can only achieve comparable performance with Moses by integrating neural reordering
model directly in decoding.
1901
Model Orientation MT06 MT02 MT03 MT04 MT05 MT08
distance N/A 29.56 31.40 31.27 31.34 29.98 23.87
word
M/S/D 30.19 32.03 31.86 32.09 30.55 24.20
left/right 30.17 31.98 31.52 31.98 30.19 24.30
phrase
M/S/D 30.24 32.35 31.85 32.00 30.78 24.33
left/right 29.57 32.64 31.53 31.90 30.70 24.28
hierarchical
M/S/D 30.46 32.52 31.89 32.09 30.39 24.11
left/right 30.03 32.13 31.59 31.91 30.21 24.41
neural
M/S/D 30.68 32.19 31.94 32.20 30.81 24.71
left/right 31.03** 33.03** 32.48** 32.52** 31.11* 25.20**
Table 2: Comparison of distance-based, lexicalized, and neural reordering models in terms of case-
insensitive BLEU-4 scores. ?distance? denotes the distance-based reordering model (Koehn et al., 2003),
?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes the phrase-based
lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-based reordering
model (Galley and Manning, 2008), and ?neural? denotes our model. The ?left? and ?right? orientations
only considers whether the current source phrase is on the left of the previous source phrase or not. We
use ?*? to highlight the result that is significantly better than the best baseline (highlighted in italic)
at p < 0.05 level and ?**? at p < 0.01 level. The neural model does not work well for the M/S/D
orientations due to the non-separability problem (Section 4.3).
as the evaluation metric. As a trade-off between expressive power and computational cost, we set the
dimension of the word embedding vectors to 25.
5
Both g
(1)
and g
(2)
are set to tanh(?). The other
hyperparameters are optimized via random search (Bergstra and Bengio, 2012).
4.2 Comparison of Distance-based, Lexicalized, and Neural Reordering Models
We compare three kinds of reordering models with increasing expressive power:
1. distance-based model: penalizing phrase displacements proportionally to the amount of nonmono-
tonicity (Koehn et al., 2003);
2. lexicalized models: conditioning the reordering probabilities on the current phrase pairs. The ori-
entations can be determined with respect to words (Tillman, 2004), phrases (Koehn et al., 2007), or
hierarchical phrases (Galley and Manning, 2008);
3. neural model: conditioning the reordering probabilities on both the current and previous phrase
pairs.
For lexicalized and neural models, we further distinguish between two kinds of orientation sets:
{monotone, swap, discontinuous} and {left, right}. The left/right orientations only consider whether
the current source phrase is on the left of the previous source phrase or not. Therefore, swap and
discontinuous-left are merged into left while monotone and discontinuous-right into right.
All these reordering models are tested using Moses (Koehn et al., 2007), except that the neural model
needs an additional hypergraph reranking procedure (Section 3.3). Implemented using Java, it takes the
reranker 0.748 second to rerank a hypergraph on average.
Table 2 shows the case-insensitive BLEU-scores of distance-based, lexicalized, and neural reordering
models on the NIST Chinese-English datasets. ?distance? denotes the distance-based reordering model
(Koehn et al., 2003), ?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes
the phrase-based lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-
based reordering model (Galley and Manning, 2008), and ?neural? denotes our model.
5
We find that the dimensions of vectors do not have a significant impact on translation performance. For efficiency, we set
the dimension to 25.
1902
Figure 2: The non-separability problem for the neural reordering model. Given an aligned Chinese-
English sentence pair, the unaligned Chinese word ?de? makes a big difference in determining M/S/D
orientations. In (a), ?de? is included in the previous source phrase and thus the orientation is monotone.
In (b), however, it is not included in the previous source phrase and the orientation is discontinuous. In
our neural reordering model, ?liu wan de? and ?liu wan? have very similar vector space representations
yet different orientations (i.e., M and D). In other words, training examples labeled with M, S, D are
prone to be mixed with each other in the vector space. Therefore, it is difficult to find a hyperplane to
separate M, S and D examples in the high-dimensional space.
We find that lexicalized reordering models obtain significant improvements over the distance-based
model, which indicates that conditioning reordering probabilities on the words of the current phrase
pairs does improve the expressive power. Our neural model using left/right orientations significantly
outperforms all variants of lexicalized models. We use ?*? to highlight the result that is significantly
better than the best baseline (highlighted in italic) at p < 0.05 level and ?**? at p < 0.01 level. This
suggests that conditioning reordering probabilities on the words of current and previous phrase pairs is
helpful for resolving reordering ambiguities and reducing context insensitivity.
4.3 The Non-Separability Problem
In Table 2, the neural model using the M/S/D orientations fails to outperform lexicalized models signifi-
cantly. One possible reason is that the neural model suffers from the non-separability problem due to the
M/S/D orientations.
As shown in Figure 2, given an aligned Chinese-English sentence pair, the unaligned Chinese function
word ?de? makes a big difference in determining M/S/D orientations. In (a), ?de? is included in the
previous source phrase and thus the orientation is monotone. In (b), however, ?de? is not included in the
previous source phrase and the orientation is discontinuous. In our neural reordering model, ?liu wan
de? and ?liu wan? have very similar vector space representations yet different orientations (i.e., M and
D). In other words, training examples labeled with M, S, D are prone to be mixed with each other in
the vector space. Therefore, it is difficult to find a hyperplane to separate M, S and D examples in the
high-dimensional space.
Fortunately, we find that using the left/right orientations can alleviate this problem. As the left/right
orientations only consider whether the current source phrase is on the left of the previous source phrase
or not, unaligned source words will not change orientations. For example, both Figure 2(a) and 2(b) are
identified as the right orientation.
As a result, using left/right orientations in the neural reordering model not only has a higher classifi-
cation accuracy (85%) over using the M/S/D orientations (69%), but also achieves higher BLEU scores
on all NIST datasets systematically.
4.4 The Effect of Distortion Limit
Figure 3 shows the performance of the lexicalized model and our neural model with various distortion
limits. The lexicalized model is the word-based model with M/S/D orientations. The neural model uses
left/right orientations. The neural model consistently outperforms the lexicalized model, especially for
large distortion limits. This finding suggests that the neural model is superior to lexicalized models in
predicting long-distance reordering.
1903
2 4 6 822
23
24
25
Distortion Limit
BLEU
 
 
neurallexicalized
Figure 3: BLEU with various distortion limits.
# examples Accuracy BLEU
100,000 83.55 30.92
200,000 84.40 31.03
300,000 84.55 31.01
400,000 84.95 30.93
500,000 85.25 31.27
3,000,000 85.55 31.03
Table 3: Effect of training corpus size.
Vectors MT06 MT02 MT03 MT04 MT05 MT08
ours 31.03 33.03 32.48 32.52 31.11 25.20
word2vec 30.44 32.28 32.00 32.07 30.24 24.54
Table 4: Comparison of neural reordering models trained based on word vectors produced by our model
(ours) and word2vec (Mikolov et al., 2013).
4.5 The Effect of Training Corpus Size
Table 3 shows the classification accuracy and translation performance with various number of randomly
sampled reordering examples for training the neural classifier. The classification accuracy and transla-
tion performance generally rise as the number of reordering example increases.
6
Surprisingly, both the
classification accuracy and translation performance of using 500,000 reordering examples are close to
using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples
are enough for training a robust classifier.
4.6 Learned Vector Space Representations
We randomly sampled 200,000 English phrases and found 999 clusters according to the vector space
representations computed by recursive autoencoders using the k-means algorithm (MacQueen, 1967).
The distance between two phrases is calculated by the Euclidean distance between their vector space
representations.
Figure 4 shows 10 of the 999 clusters. An interesting finding is that phrase pairs that are close in the
vector space share with similar reordering patterns rather than semantic similarity. For example, ?by
june 1? and ?within the agencies? have similar distributions on the left/right orientations but are totally
unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled
data hardly helps in training the neural reordering model. Table 4 shows the results when we replace
the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive
autoencoders and the classifier are retrained. The performance of the neural reordering model trained in
this way drops significantly, which confirms our analysis.
5 Related Work
Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006)
use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order
in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering
models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finer-
grained distance bins instead. Another direction is to learn sparse reordering features and create more
flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major
challenge. In contrast, our neural reordering model is capable of learning features automatically.
6
The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated
with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the
performance. We leave this for future work.
1904
but is willing toeconomy is required to
range of services to said his visit is to
is making use of
june 18, 2001late 2011
as detention centergroup all togethertake care of oldby june 1
and complete by end 1998
or other economicand for other
within the agencies
Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases
that have similar reordering probability distributions rather than similar semantic similarity fall into one
cluster.
Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al.,
2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et
al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and
target side contexts, these approaches still face the data sparsity problem.
Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to
compute vector space representation for variable-sized blocks ranging from words to sentences on the
fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7
words in the training phase, which makes our approach simpler and more scalable to large data.
6 Conclusion
We have shown that surrounding context is effective for resolving reordering ambiguities in phrase-based
models. As the data sparseness problem is the major challenge for using context in reordering models,
we propose to use a single classifier based on recursive autoencoders to predict reordering orientations.
Experimental results show that our neural reordering model outperforms the state-of-the-art lexicalized
reordering models significantly and consistently across all the NIST datasets under various settings.
There are a few future directions we plan to explore. First, as the machine translation system and neu-
ral classifier are trained separately, the neural network training only has an indirect effect on translation
quality. Jointly training the machine translation system and neural classifier is an interesting topic. Sec-
ond, it is interesting to develop more efficient models to leverage larger contexts to resolve reordering
ambiguities. Third, we plan to extend our work to other translation models such as syntax-based and
n-gram based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013). Finally, as we cast
phrase reordering as two-category classification problem (i.e, left vs. right), it is interesting to intersect
structured SVM (Tsochantaridis et al., 2005) with neural networks to develop a large margin training
algorithm for our neural reordering model.
Acknowledgements
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013), the 863 Program (No. 2012AA011102), Toshiba Corporation
Corporate Research & Development Center, and the Singapore National Research Foundation under its
International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme.
1905
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?536.
James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine
Learning Research, 13(1):281?305.
Arianna Bisazza and Marcello Federico. 2012. Modified distortion matrices for phrase-based statistical machine
translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 478?487.
Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrat-
ed reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013. Can Markov models
over minimal translation units help phrase-based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 399?405.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, pages 285?293.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Proceedings of 1996 IEEE International Conference on Neural Networks (Vol-
ume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statis-
tical machine translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics, pages 867?875.
Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011
Sixth Workshop on Statistical Machine Translation, pages 187?197.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151.
Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
586?594.
Maxim Khalilov and Khalil Simaan. 2010. Source reordering using maxent classifiers and supertags. In Proceed-
ings of The 14th Annual Conference of the European Association for Machine Translation, pages 292?299.
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577.
1906
DongC. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math-
ematical Programming, 45(1-3):503?528.
James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281?297.
Jos?e B. Mari`no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527?549.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving a lexicalized
hierarchical reordering model using maximum entropy. In Proceedings of The twelfth Machine Translation
Summit (MT Summit XII).
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Main
Proceedings, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics, pages 160?167.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning internal representations by
error propagation. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1:
Foundations, pages 318?362.
Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of Advances in Neural
Information Processing Systems 24, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Processing, pages 151?161.
Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the
Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004: Short Papers, pages 101?104.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453?1484.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational Linguistics, pages 521?528.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic distortion in a discriminative reordering model for statistical
machine translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages
353?360.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation
as a traveling salesman problem. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 333?341.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-
based statistical machine translation. In Proceedings of the 20th International Conference on Computational
Linguistics, pages 205?211.
1907
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2031?2041, Dublin, Ireland, August 23-29 2014.
Query Lattice for Translation Retrieval
Meiping Dong
?
, Yong Cheng
?
, Yang Liu
?
, Jia Xu
?
, Maosong Sun
?
,
Tatsuya Izuha

, Jie Hao
#
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
hellodmp@163.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
chengyong3001@gmail.com, xu@tsinghua.edu.cn

Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
#
Toshiba (China) R&D Center
haojie@toshiba.com.cn
Abstract
Translation retrieval aims to find the most likely translation among a set of target-language strings
for a given source-language string. Previous studies consider the single-best translation as a query
for information retrieval, which may result in translation error propagation. To alleviate this
problem, we propose to use the query lattice, which is a compact representation of exponentially
many queries containing translation alternatives. We verified the effectiveness of query lattice
through experiments, where our method explores a much larger search space (from 1 query to
1.24 ? 10
62
queries), runs much faster (from 0.75 to 0.13 second per sentence), and retrieves
more accurately (from 83.76% to 93.16% in precision) than the standard method based on the
query single-best. In addition, we show that query lattice significantly outperforms the method
of (Munteanu and Marcu, 2005) on the task of parallel sentence mining from comparable corpora.
1 Introduction
Translation retrieval aims to search for the most probable translation candidate from a set of target-
language strings for a given source-language string. Early translation retrieval methods were widely
used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al.,
1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records
that are pairs of source-language and target-language strings. Given an input source string, the retrieval
system returns a translation record of maximum similarity to the input on the source side. Although these
methods prove to be effective in example-based and memory-based translation systems, they heavily rely
on parallel corpora that are limited both in size and domain.
More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends
only on monolingual corpora. Given an input source string, their system retrieves translation candidates
from a set of target-language sentences. This can be done by combining machine translation (MT) and
information retrieval (IR): machine translation is used to transform the input source string to a coarse
translation, which serves as a query to retrieve the most probable translation in the monolingual corpus.
Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora
that are readily available on the Web.
However, the MT + IR pipeline suffers from the translation error propagation problem. Liu et al.
(2012) use 1-best translations, which are inevitably erroneous due to the ambiguity and structural di-
vergence of natural languages, as queries to the IR module. As a result, translation mistakes will be
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Corresponding author: Jia Xu. Tel: +86-10-62781693 Ext 1683. Homepage: iiis.tsinghua.edu.cn/?xu
2031
propagated to the retrieval process. This situation aggravates when high-accuracy MT systems are not
available for resource-scarce languages.
In this work, we propose to use query lattice in translation retrieval to alleviate the translation error
propagation problem. A query lattice is a compact representation of exponentially many queries. We
design a retrieval algorithm that takes the query lattice as input to search for the most probable translation
candidate from a set of target-language sentences. As compared with Liu et al. (2012), our approach
explores a much larger search space (from 1 query to 1.24? 10
62
queries), runs much faster (from 0.75
second per sentence to 0.13), and retrieves more accurately (from 83.76% to 93.16%). We also evaluate
our approach on extracting parallel sentences from comparable corpora. Experiments show that our
translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system.
2 Related Work
Our work is inspired by three research topics: retrieving translation candidates from parallel corpus,
using lattice to compactly represent exponentially many alternatives, and using lattice as query in infor-
mation retrieval.
1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from
existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990;
Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use
a parallel corpus (e.g., translation records that are pairs of source-language and target-language
strings), they focus on calculating the similarity between two source-language strings. In contrast,
we evaluate the translational equivalence of a given source string and a target string in a large
monolingual corpus.
2. Lattice in Machine Translation. Lattices have been widely used in machine translation: consider-
ing Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Mat-
soukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes
risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system
combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a
lattice that encodes exponentially many translation candidates as a single query to retrieve similar
target sentences via an information retrieval system.
3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to
Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies
or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document
retrieval, however, lattices are used as a compact representation of multiple speech recognition
transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004;
Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that
uses the bag-of-words model because translation retrieval must take structure and dependencies in
text into account to ensure translational equivalence.
3 Query Lattice for Translation Retrieval
3.1 Translation Retrieval
Let f be a source-language string, E be a set of target-language strings, the problem is how to find the
most probable translation
?
e from E. Note that E is a monolingual corpus rather than a parallel corpus.
Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin
and Tanaka, 2000; Baldwin, 2001) does not apply here.
We use P (e|f) to denote the probability that a target-language sentence e is the translation of a source-
language sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by
2032
introducing a coarse translation q as a hidden variable:
P (e|f) =
?
q?Q(f)
P (q, e|f) (1)
=
?
q?Q(f)
P (q|f)? P (e|q, f) (2)
where P (q|f) is a translation sub-model, P (e|q, f) is a retrieval sub-model, and Q(f) is the set of all
possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model.
To take advantage of various translation and retrieval information sources, we use a log-linear model
(Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned
on a source sentence f parameterized by a real-valued vector ?:
P (q, e|f ;?) =
exp(? ? h(q, e, f))
?
q
?
?Q(f)
?
e
?
?E
exp(? ? h(q
?
, e
?
, f))
(3)
where h(?) is a vector of feature functions and ? is the corresponding feature weight vector.
Accordingly, the decision rule for the latent variable model is given by
?
e = argmax
e?E
{
?
q?Q(f)
exp(? ? h(q, e, f))
}
(4)
As there are exponentially many queries, it is efficient to approximate the summation over all possible
queries by using maximization instead:
?
e ? argmax
e?E
{
max
q?Q(f)
{
? ? h(q, e, f)
}
}
(5)
Unfortunately, the search space is still prohibitively large since we need to enumerate all possible
queries. Liu et al. (2012) split Eq. (5) into two steps. In the first step, a translation module runs to
produce the 1-best translation
?
q of the input string f as a query:
?
q ? argmax
q?Q(f)
{
?
t
? h
t
(q, e, f)
}
(6)
where h
t
(?) is a vector of translation features and ?
t
is the corresponding feature weight vector. In the
second step, a monolingual retrieval module takes the 1-best translation
?
q as a query to search for the
target string
?
e with the highest score:
?
e ? argmax
e?E
{
?
r
? h
r
(
?
q, e, f)
}
(7)
where h
r
(?) is a vector of retrieval features and ?
r
is the corresponding feature weight vector.
Due to the ambiguity of translation, however, state-of-the-art MT systems are still far from producing
high-quality translations, especially for distantly-related languages. As a result, the 1-best translations
are usually erroneous and potentially introduce retrieval mistakes.
A natural solution is to use n-best lists as queries:
?
e ? argmax
e?E
{
max
q?N(f)
{
? ? h(q, e, f)
}
}
(8)
where N(f) ? T(f) is the n-best translations of the input source sentence f .
2033
Figure 1: Two kinds of query lattices: (a) search graph that is generated after phrase-based decoding and
(b) translation option graph that is generated before decoding. Translation option graph is more compact
and encodes more translation candidates.
Although using n-best lists apparently improves the retrieval accuracy over using 1-best lists, there
are two disadvantages. First, the decision rule in Eq. (8) requires to enumerate all the n translations and
retrieve for n times. In other words, the time complexity increases linearly. Second, an n-best list only
accounts for a tiny fraction of the exponential search space of translation. To make things worse, there
are usually very few variations in n-best translations because of spurious ambiguity - a situation where
multiple derivations give similar or even identical translations.
Therefore, we need to find a more elegant way to enable the retrieval module to explore exponentially
many queries without sacrificing efficiency.
3.2 Query Lattice
We propose to use query lattice to compactly represent exponentially many queries. For example, given
a source sentence ?bushi yu shalong juxing huitan?, we can use the search graph produced by a phrase-
based translation system (Koehn et al., 2007) as a lattice to encode exponentially many derivations.
Figure 1(a) shows a search graph for the example source sentence. Each edge is labeled with an
English phrase as well as the corresponding translation feature value vector. Node 0 denotes the starting
node. Node 7 and node 8 are two ending nodes. Each path from the starting node to an ending node
denotes a query. Paths that reach the same node in the lattice correspond to recombined hypotheses
that have equivalent feature histories (e.g., coverage, last generated target words, the end of last covered
source phrase, etc) in phrase-based decoding.
However, there are two problems with using search graph as query lattice. First, it is computationally
expensive to run a phrase-based system to generate search graphs. The time complexity for phrase-based
decoding with beam search is O(n
2
b) (Koehn et al., 2007), where n is the length of source string and b is
the beam width. Moreover, the memory requirement is usually very high due to language models. As a
result, translation is often two orders of magnitude slower than retrieval. Second, a search graph has too
many ?duplicate? edges due to different reordering, which increase the time complexity of retrieval (see
Section 3.3). For example, in Figure 1(a), the English phrase ?Sharon? occurs two times due to different
reordering.
Alternatively, we propose to use translation option graph as query lattice. In a phrase-based trans-
lation system, translation options that are phrase pairs matching a substring in the input source string
are collected before decoding. These translation options form a query lattice with monotonic reorder-
ing. Figure 1(b) shows an example translation option graph, in which nodes are sorted according to the
positions of source words. Each edge is labeled with an English phrase as well as the corresponding
translation feature value vector.
We believe that translation option graph has three advantages over search graph:
1. Improved efficiency in translation. Translation option graph requires no decoding.
2. Improved efficiency in retrieval. Translation option graph has no duplicate edges.
2034
Algorithm 1 Retrieval with lattice as query.
1: procedure LATTICERETRIEVE(L(f),E, k)
2: Q? GETWORDS(L(f)) . Get distinct words in the lattice to form a coarse query
3: E
k
? RETRIEVE(E, Q, k) . Retrieve top-k target sentences using the coarse query
4: for all e ? E
k
do
5: FINDPATH(L(f), e) . Find a path with the highest score
6: end for
7: SORT(E
k
) . Sort retrieved sentences according the scores
8: return E
k
9: end procedure
Algorithm 2 Find a path with the highest score.
1: procedure FINDPATH(L(f), e)
2: for v ? L(f) in topological order do
3: path(v)? ? . Initialize the Viterbi path at node v
4: score(v)? 0 . Initialize the Viterbi score at node v
5: for u ? IN(v) do . Enumerate all antecedents
6: p? path(u) ? {e
u?v
} . Generate a new path
7: s? score(u) + COMPUTESCORE(e
u?v
) . Compute the path score
8: if s > score(v) then
9: path(v)? p . Update the Viterbi path
10: score(v)? s . Update the Viterbi score
11: end if
12: end for
13: end for
14: end procedure
3. Enlarged search space. Translation option graph represents the entire search space of monotonic
decoding while search graph prunes many translation candidates.
In Figure 1, the search graph has 9 nodes, 10 edges, 4 paths, and 3 distinct translations. In contrast,
the translation option graph has 6 nodes, 9 edges, 10 paths, and 10 distinct translations. Therefore,
translation option graph is more compact and encodes more translation candidates.
Although translation option graph ignores language model and lexcialized reordering models, which
prove to be critical information sources in machine translation, we find that it achieves comparable or
even better retrieval accuracy than search graph (Section 4). This confirms the finding of Liu et al. (2012)
that language model and lexicalized reordering models only have modest effects on translation retrieval.
3.3 Retrieval with Query Lattice
Given a target corpus E and a query lattice L(f) ? Q(f), our goal is to find the target sentence
?
e with
the highest score ? ? h(q, e, f):
?
e ? argmax
e?E
{
max
q?L(f)
{
? ? h(q, e, f)
}
}
(9)
Due to the exponentially large search space, we use a coarse-to-fine algorithm to search for the target
sentence with the highest score, as shown in Algorithm 1. We use an example to illustrate the basic idea.
Given an input source sentence ?bushi yu shalong juxing le huitan?, our system first generates a query
lattice like Figure 1(a). It is non-trivial to directly feed the query lattice to a retrieval system. Instead, we
would like to first collect all distinct words in the lattice: {?Bush?, ?and? , ?Sharon?, ?held?, ?a?, ?talk?,
?talks?, ?with?}. This set serves as a coarse single query and the retrieval system returns a list of target
sentences that contain these words:
2035
Chinese English
Training 1.21M 1.21M
Dev in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Test in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based
translation model and language model for Moses (Koehn et al., 2007). The development set is used
to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set
consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To
examine the effect of domains on retrieval performance, we used two development and test sets: in-
domain and out-domain.
President Bush gave a talk at a meeting
Bush held a meeting with Sharon
Sharon and Bush attended a meeting held at London
Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences
(scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we
can match each retrieved sentence against the query lattice to find a path with the highest score using
additional translation features. For example, the Viterbi path for ?Bush held a meeting with Sharon? in
Figure 1(a) is ?Bush held talks with Sharon?. The translation features of matched arcs in the path are
collected to compute the overall score according to Eq. (9). Finally, the algorithm returns a sorted list:
Bush held a meeting with Sharon
President Bush gave a talk at a meeting
Sharon and Bush attended a meeting held at London
More formally, the input of Algorithm 1 are a query lattice L(f), a target corpus E, and a parameter
k (line 1). The function GETWORDS simply collects all the distinct words appearing in the lattice (line
2), which are used for constructing a coarse boolean query Q. Then, the function RETRIEVE runs to
retrieve the top-k target sentences E
k
in the target corpus E only using standard IR features according
to the query Q (line 3). These first two steps eliminate most unlikely candidates and return a coarse set
of target sentence candidates efficiently.
1
Then, a procedure FINDPATH(L(f), e) runs to search for the
translation with the highest score for each candidate (lines 4-6). Finally, the algorithm returns the sorted
list of target sentences (lines 7-9).
Algorithm 2 shows the procedure FINDPATH(L(f), e), which searches for the path with higher score
using a Viterbi-style algorithm. The function COMPUTESCORE scores an edge according to the Eq. (9)
which linearly combines the translation and retrieval features.
Generally, the lattice-based retrieval algorithm has a time complexity of O(k|E|), where |E| is the
number of edges in the lattice.
4 Experiments
In this section, we try to answer two questions:
1. Does using query lattices improve translation retrieval accuracy over using n-best lists?
2. How does translation retrieval benefit other end-to-end NLP tasks such as machine translation?
1
In our experiments, we set the parameter k to 500 as a larger value of k does not give significant improvements but introduce
more noises.
2036
Accordingly, we evaluated our system in two tasks: translation retrieval (Section 4.1) and parallel
corpus mining (Section 4.2).
4.1 Evaluation on Translation Retrieval
4.1.1 Experimental Setup
In this section, we evaluate the accuracy of translation retrieval: given a query set (i.e., source sentences),
our system returns a sorted list of target sentences. The evaluation metrics include precision@n and
recall.
The datasets for the retrieval evaluation are summarized in Table 1. The training set, which is used to
train the phrase-based translation model and language model for the-state-of-the-art phrase-based system
Moses (Koehn et al., 2007), contains 1.21M Chinese-English sentences with 32.0M Chinese words and
35.2M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on
the English side of the training corpus. The development set, which is used to optimize feature weights
using the minimum-error-rate algorithm (Och, 2003), consists of query set and a document set. We
sampled 5K parallel sentences randomly, in which 5K Chinese sentences are used as queries and half
of their parallelled English sentences(2.5K) mixed with other English sentences(2.3M) as the retrieval
document set. As a result, we can compute precision and recall in a noisy setting. The test set is used
to compute retrieval evaluation metrics. To examine the effect of domains on retrieval performance, we
used two data sets: in-domain and out-domain. The in-domain development and test sets are close to
the training set while the out-domain data sets are not.
We compare three variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice,
we further distinguish between search graph and translation option graph. They are generated by Moses
with the default setting.
We use both translation and retrieval features in the experiments. The translation features include
phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, lan-
guage models, and word penalty. Besides the conventional IR features such as term frequency and
inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002):
the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity
penalty. These features impose structural constraints on retrieval and ensure translation closeness of re-
trieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss
function we used in our experiment is 1?P@n. Note that using translation option graph as query lattice
does not include language models and distance-based lexicalized reordering models as features.
4.1.2 Evaluation Results
Table 2 shows the results on the in-domain test set. The ?# candidates? column gives the number of
translation candidates explored by the retrieval module for each source sentence on average. The lattices,
either generated by search graph or by translation options, contain exponentially many candidates. We
find that using lattices dramatically improves the precisions over using 1-best and n-best lists. All the
improvements over 1-best and n-best lists are significant statistically. The 1-best, n-best, and the search
graph lattice share with the same translation time: 5,640 seconds for translating 5,000 queries. Note
that the translation time is zero for the translation option graph because it does not need phrase-based
decoding. For retrieval, the time cost for the n-best list method generally increases linearly. As the search
graph lattice contains many edges, the retrieval time increases by an order of magnitude as compared
with 100-best list. An interesting finding is that using translation options as a lattice contains more
candidates and consumes much less time for retrieval than using search graph as a lattice. One possible
reason is that a search graph generated by Moses usually contains many redundant edges. For example,
Figure 1 is actually a search graph and many phrases occurs multiple times in the lattice (e.g., ?and?
and ?Sharon?). In contrast, a lattice built by translation options hardly has any redundant edges but
still represents exponentially many possible translations. We can also see that the lattice constructed by
search graph considering language model can benefit the precision much, especially when n is little. But
this advantage decreases with n increasing and the time consumed by translation options as lattice is
much less than the search graph as lattice. Besides, the margin between them is not too large so we can
2037
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 87.40 91.40 92.24 92.88 93.64 5,640 82
10-best 10 89.84 93.20 93.96 94.36 95.56 5,640 757
100-best 100 90.76 94.32 95.00 95.76 96.76 5,640 7,421
lattice (graph) 1.20? 10
54
93.60 96.08 96.28 96.52 96.80 5,640 89,795
lattice (options) 4.14? 10
62
93.28 95.84 95.96 96.16 96.84 0 307
Table 2: Results on the in-domain test set. We use the minimum-error-rate training algorithm (Och,
2003) to optimize the feature with the respect to 1?P@n.
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 67.32 76.60 79.40 81.80 83.76 3,660 92
10-best 10 72.68 80.96 83.36 85.84 88.76 3,660 863
100-best 100 78.60 85.76 87.76 89.64 92.16 3,660 8,418
lattice (graph) 1.51? 10
61
84.32 89.40 90.68 91.56 92.44 3,660 67,205
lattice (options) 1.24? 10
65
81.92 88.00 89.80 91.24 93.16 0 645
Table 3: Results on the out-of-domain test set.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 2: In-domain Precision-Recall curves.
0.3 0.4 0.5 0.6 0.7 0.8 0.90.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 3: Out-domain Precision-Recall curves.
abandon some little precision for obtain the large time reducing. Therefore, using translation options as
lattices seems to be both effective and efficient.
Table 3 shows the results on the out-of-domain test set. While the precisions for all methods drop, the
margins between lattice-based retrieval and n-best list retrieval increase, suggesting that lattice-based
methods are more robust when dealing with noisy datasets.
Figures 2 and 3 show the Precision-Recall curves on the in-domain and out-of-domain test sets. As
the query set is derived from parallel sentences, recall can be computed in our experiments. The curves
show that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the
out-of-domain test set.
4.2 Evaluation on Parallel Corpus Mining
In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel
corpus from a comparable corpus.
4.2.1 Experimental Setup
The comparable corpus for extracting parallel sentences contains news articles published by Xinhua
News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and
1.7M English articles.
We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system.
2038
language articles sentences words vocabulary
Chinese 1.2M 18.5M 441.2M 2.1M
English 1.7M 17.8M 440.2M 3.4M
Table 4: The Xinhua News Comparable Corpus from 1995 to 2010
Munteanu and Marcu (2005) this work
English words Chinese words BLEU English words Chinese Words BLEU
5.00M 4.12M 22.84 5.00M 3.98M 25.44
10.00M 8.20M 25.10 10.00M 8.17M 26.62
15.00M 12.26M 25.41 15.00M 12.49M 26.49
20.00M 16.30M 25.56 20.00M 16.90M 26.87
Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system.
Given a comparable corpus (see Table 4), both systems extract parallel corpora that are used for training
phrase-base models (Koehn et al., 2007). The baseline system is a re-implementation of the method
described in (Munteanu and Marcu, 2005). Our system uses translation option graph as query lattice.
Our system significantly outperforms the baseline for various sizes.
It assigned a score to each sentence pair using a classifier. Our system used translation option graph as
query lattices due to its simplicity and effectiveness. For each source sentence in the comparable corpus,
our system retrieved the top target sentence together with a score.
To evaluate the quality of extracted parallel corpus, we trained phrase-based models on it and ran
Moses on NIST datasets. The development set is the NIST 2005 test set and the test set is the NIST 2006
test set. The final evaluation metric is case-insensitive BLEU-4.
4.2.2 Evaluation Results
Table 5 shows the comparison of BLEU scores using parallel corpora extracted by the baseline and our
system. We find that our system significantly outperforms the baseline for various parallel corpus sizes.
This finding suggests that using lattice to compactly represent exponentially many alternatives does help
to alleviate the translation error propagation problem and identify parallel sentences of high translational
equivalence.
5 Conclusion
In this work, we propose to use query lattice to address the translation error propagation problem in
translation retrieval. Two kinds of query lattices are used in our experiments: search graph and translation
option graph. We show that translation option graph is more compact and represents a much larger
search space. Our experiments on Chinese-English datasets show that using query lattices significantly
outperforms using n-best lists in the retrieval task. Moreover, we show that translation retrieval is capable
of extracting high-quality parallel corpora from a comparable corpus. In the future, we plan to apply
our approach to retrieving translation candidates directly from the Web, which can be seen as a huge
monolingual corpus.
Acknowledgments
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013 and No. 61033001), the 863 Program (No. 2012AA011102),
Toshiba Corporation Corporate Research & Development Center, and the Singapore National Research
Foundation under its International Research Centre @ Singapore Funding Initiative and administered by
the IDM Programme.
2039
References
T. Baldwin and H. Tanaka. 2000. The effects of word order and segmentation on translation retrieval performance.
In Proceedings of COLING.
Timothy Baldwin. 2001. Low-cost, high-performance translation retrieval: Dumber is better. In Proceedings of
ACL, pages 18?25, Toulouse, France, July. Association for Computational Linguistics.
Karen Cheung and Douglas Vogel. 2005. Complexity reduction in lattice-based information retrieval. Information
Retrieval, pages 285?299.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou Ng. 2010. Statistical lattice-based spoken document
retrieval. ACM Transactions on Information Systems, 28(1).
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020, Columbus, Ohio, June. Association for Computational Linguistics.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan L?u. 2009. Lattice-based system combination for statis-
tical machine translation. In Proceedings of EMNLP, pages 1105?1113, Singapore, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL - Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic, June. Association for Computational Linguistics.
Chunyang Liu, Qi Liu, Yang Liu, and Maosong Sun. 2012. THUTR: A translation retrieval system. In Proceed-
ings of COLING - Demo and Poster Sessions, pages 321?328, Mumbai, India, December. The COLING 2012
Organizing Committee.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate
training for statistical machine translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Spyros Matsoukas, Ivan Bulyko, Bing Xiang, Kham Nguyen, Richard Schwartz, and John Makhoul. 2007. In-
tegrating speech recognition and machine translation. In Proceedings of ICASSP, volume 4, pages IV?1281.
IEEE.
C.N. Moore. 1958. A mathematical theory of the use of language symbols in retrieval. In ICSI 1958.
Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 31(4):477?1504.
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993. Two approaches to matching in example-based machine
translation. In TMI 1993.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL, pages 295?302, Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan, July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Uta Priss. 2000. Lattice-based information retrieval. Knwoledge Organization, 27(3):132?142.
Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-NAACL, pages 129?136, Boston, Massachusetts, USA, May.
Association for Computational Linguistics.
S. Sato and M. Nagao. 1990. Toward memory-based translation. In Proceedings of COLING.
Andreas Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of ICSLP.
2040
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding
for statistical machine translation. In Proceedings of EMNLP, pages 620?629, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In Proceedings of IWSLT 2005, pages 141?147, Pittsburgh, PA, October.
Zheng-Yu Zhou, Peng Yu, Ciprian Chelba, and Frank Seide. 2006. Towards spoken-document retrieval for the
internet: Lattice indexing for large-scale web-search architectures. In Proceedings of HLT-NAACL, pages 415?
422, New York City, USA, June. Association for Computational Linguistics.
2041
