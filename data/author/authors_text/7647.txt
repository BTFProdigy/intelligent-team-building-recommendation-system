Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 11?20,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
It?s a Contradiction?No, it?s Not:
A Case Study using Functional Relations
Alan Ritter, Doug Downey, Stephen Soderland and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
{aritter,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Contradiction Detection (CD) in text is a
difficult NLP task. We investigate CD
over functions (e.g., BornIn(Person)=Place),
and present a domain-independent algorithm
that automatically discovers phrases denoting
functions with high precision. Previous work
on CD has investigated hand-chosen sentence
pairs. In contrast, we automatically harvested
from the Web pairs of sentences that appear
contradictory, but were surprised to find that
most pairs are in fact consistent. For example,
?Mozart was born in Salzburg? does not con-
tradict ?Mozart was born in Austria? despite
the functional nature of the phrase ?was born
in?. We show that background knowledge
about meronyms (e.g., Salzburg is in Austria),
synonyms, functions, and more is essential for
success in the CD task.
1 Introduction and Motivation
Detecting contradictory statements is an important
and challenging NLP task with a wide range of
potential applications including analysis of politi-
cal discourse, of scientific literature, and more (de
Marneffe et al, 2008; Condoravdi et al, 2003;
Harabagiu et al, 2006). De Marneffe et al present a
model of CD that defines the task, analyzes different
types of contradictions, and reports on a CD system.
They report 23% precision and 19% recall at detect-
ing contradictions in the RTE-3 data set (Voorhees,
2008). Although RTE-3 contains a wide variety of
contradictions, it does not reflect the prevalence of
seeming contradictions and the paucity of genuine
contradictions, which we have found in our corpus.
1.1 Contradictions and World Knowledge
Our paper is motivated in part by de Marneffe et al?s
work, but with some important differences. First,
we introduce a simple logical foundation for the CD
task, which suggests that extensive world knowl-
edge is essential for building a domain-independent
CD system. Second, we automatically generate a
large corpus of apparent contradictions found in ar-
bitrary Web text. We show that most of these appar-
ent contradictions are actually consistent statements
due to meronyms (Alan Turing was born in London
and in England), synonyms (George Bush is mar-
ried to both Mrs. Bush and Laura Bush), hypernyms
(Mozart died of both renal failure and kidney dis-
ease), and reference ambiguity (one John Smith was
born in 1997 and a different John Smith in 1883).
Next, we show how background knowledge enables
a CD system to discard seeming contradictions and
focus on genuine ones.
De Marneffe et al introduced a typology of con-
tradiction in text, but focused primarily on contra-
dictions that can be detected from linguistic evi-
dence (e.g. negation, antonymy, and structural or
lexical disagreements). We extend their analysis to
a class of contradictions that can only be detected
utilizing background knowledge. Consider for ex-
ample the following sentences:
1) ?Mozart was born in Salzburg.?
2) ?Mozart was born in Vienna.?
3) ?Mozart visited Salzburg.?
4) ?Mozart visited Vienna.?
Sentences 1 & 2 are contradictory, but 3 & 4 are
not. Why is that? The distinction is not syntactic.
Rather, sentences 1 and 2 are contradictory because
11
the relation expressed by the phrase ?was born in?
can be characterized here as a function from peo-
ple?s names to their unique birthplaces. In contrast,
?visited? does not denote a functional relation.1
We cannot assume that a CD system knows, in
advance, all the functional relations that might ap-
pear in a corpus. Thus, a central challenge for a
function-based CD system is to determine which re-
lations are functional based on a corpus. Intuitively,
we might expect that ?functional phrases? such as
?was born in? would typically map person names
to unique place names, making function detection
easy. But, in fact, function detection is surprisingly
difficult because name ambiguity (e.g., John Smith),
common nouns (e.g., ?dad? or ?mom?), definite de-
scriptions (e.g., ?the president?), and other linguistic
phenomena can mask functions in text. For example,
the two sentences ?John Smith was born in 1997.?
and ?John Smith was born in 1883.? can be viewed
as either evidence that ?was born in? does not de-
note a function or, alternatively, that ?John Smith?
is ambiguous.
1.2 A CD System Based on Functions
We report on the AUCONTRAIRE CD system, which
addresses each of the above challenges. First, AU-
CONTRAIRE identifies ?functional phrases? statis-
tically (Section 3). Second, AUCONTRAIRE uses
these phrases to automatically create a large cor-
pus of apparent contradictions (Section 4.2). Fi-
nally, AUCONTRAIRE sifts through this corpus to
find genuine contradictions using knowledge about
synonymy, meronymy, argument types, and ambi-
guity (Section 4.3).
Instead of analyzing sentences directly, AUCON-
TRAIRE relies on the TEXTRUNNER Open Informa-
tion Extraction system (Banko et al, 2007; Banko
and Etzioni, 2008) to map each sentence to one or
more tuples that represent the entities in the sen-
tences and the relationships between them (e.g.,
was born in(Mozart,Salzburg)). Using extracted tu-
ples greatly simplifies the CD task, because nu-
merous syntactic problems (e.g., anaphora, rela-
tive clauses) and semantic challenges (e.g., quantifi-
cation, counterfactuals, temporal qualification) are
1Although we focus on function-based CD in our case study,
we believe that our observations apply to other types of CD as
well.
delegated to TEXTRUNNER or simply ignored. Nev-
ertheless, extracted tuples are a convenient approxi-
mation of sentence content, which enables us to fo-
cus on function detection and function-based CD.
Our contributions are the following:
? We present a novel model of the Contradiction
Detection (CD) task, which offers a simple log-
ical foundation for the task and emphasizes the
central role of background knowledge.
? We introduce and evaluate a new EM-style al-
gorithm for detecting whether phrases denote
functional relations and whether nouns (e.g.,
?dad?) are ambiguous, which enables a CD sys-
tem to identify functions in arbitrary domains.
? We automatically generate a corpus of seem-
ing contradictions from Web text, and report
on a set of experiments over this corpus, which
provide a baseline for future work on statistical
function identification and CD. 2
2 A Logical Foundation for CD
On what basis can a CD system conclude that two
statements T and H are contradictory? Logically,
contradiction holds when T |= ?H . As de Marneffe
et al point out, this occurs when T and H contain
antonyms, negation, or other lexical elements that
suggest that T and H are directly contradictory. But
other types of contradictions can only be detected
with the help of a body of background knowledge
K: In these cases, T and H alone are mutually con-
sistent. That is,
T |=\ ?H ?H |=\ ?T
A contradiction between T and H arises only in
the context of K. That is:
((K ? T ) |= ?H) ? ((K ?H) |= ?T )
Consider the example of Mozart?s birthplace in
the introduction. To detect a contradiction, a CD
system must know that A) ?Mozart? refers to the
same entity in both sentences, that B) ?was born in?
denotes a functional relation, and that C) Vienna and
Salzburg are inconsistent locations.
2The corpus is available at http://www.cs.
washington.edu/research/aucontraire/
12
Of course, world knowledge, and reasoning about
text, are often uncertain, which leads us to associate
probabilities with a CD system?s conclusions. Nev-
ertheless, the knowledge base K is essential for CD.
We now turn to a probabilistic model that helps
us simultaneously estimate the functionality of re-
lations (B in the above example) and ambiguity of
argument values (A above). Section 4 describes the
remaining components of AUCONTRAIRE.
3 Detecting Functionality and Ambiguity
This section introduces a formal model for comput-
ing the probability that a phrase denotes a function
based on a set of extracted tuples. An extracted tuple
takes the form R(x, y) where (roughly) x is the sub-
ject of a sentence, y is the object, and R is a phrase
denoting the relationship between them. If the re-
lation denoted by R is functional, then typically the
object y is a function of the subject x. Thus, our dis-
cussion focuses on this possibility, though the anal-
ysis is easily extended to the symmetric case.
Logically, a relation R is functional in a vari-
able x if it maps it to a unique variable y:
?x, y1, y2 R(x, y1) ? R(x, y2) ? y1 = y2. Thus,
given a large random sample of ground instances of
R, we could detect with high confidence whether R
is functional. In text, the situation is far more com-
plex due to ambiguity, polysemy, synonymy, and
other linguistic phenomena. Deciding whether R is
functional becomes a probabilistic assessment based
on aggregated textual evidence.
The main evidence that a relation R(x, y) is func-
tional comes from the distribution of y values for
a given x value. If R denotes a function and x is
unambiguous, then we expect the extractions to be
predominantly a single y value, with a few outliers
due to noise. We aggregate the evidence that R is
locally functional for a particular x value to assess
whether R is globally functional for all x.
We refer to a set of extractions with the same
relation R and argument x as a contradiction set
R(x, ?). Figure 1 shows three example contradic-
tion sets. Each example illustrates a situation com-
monly found in our data. Example A in Figure 1
shows strong evidence for a functional relation. 66
out of 70 TEXTRUNNER extractions for was born in
(Mozart, PLACE) have the same y value. An am-
biguous x argument, however, can make a func-
tional relation appear non-functional. Example B
depicts a distribution of y values that appears less
functional due to the fact that ?John Adams? refers
to multiple, distinct real-world individuals with that
name. Finally, example C exhibits evidence for a
non-functional relation.
A. was born in(Mozart, PLACE):
Salzburg(66), Germany(3), Vienna(1)
B. was born in(John Adams, PLACE):
Braintree(12), Quincy(10), Worcester(8)
C. lived in(Mozart, PLACE):
Vienna(20), Prague(13), Salzburg(5)
Figure 1: Functional relations such as example A have a
different distribution of y values than non-functional rela-
tions such as C. However, an ambiguous x argument as in
B, can make a functional relation appear non-functional.
3.1 Formal Model of Functions in Text
To decide whether R is functional in x for all x,
we first consider how to detect whether R is lo-
cally functional for a particular value of x. The local
functionality of R with respect to x is the probabil-
ity that R is functional estimated solely on evidence
from the distribution of y values in a contradiction
set R(x, ?).
To decide the probability that R is a function, we
define global functionality as the average local func-
tionality score for each x, weighted by the probabil-
ity that x is unambiguous. Below, we outline an EM-
style algorithm that alternately estimates the proba-
bility that R is functional and the probability that x
is ambiguous.
Let R?x indicate the event that the relation R is
locally functional for the argument x, and that x is
locally unambiguous for R. Also, let D indicate
the set of observed tuples, and define DR(x,?) as the
multi-set containing the frequencies for extractions
of the form R(x, ?). For example the distribution of
extractions from Figure 1 for example A is
Dwas born in(Mozart,?) = {66, 3, 1}.
Let ?fR be the probability that R(x, ?) is locally
functional for a random x, and let ?f be the vector
of these parameters across all relations R. Likewise,
?ux represents the probability that x is locally unam-
biguous for random R, and ?u the vector for all x.
13
We wish to determine the maximum a pos-
teriori (MAP) functionality and ambiguity pa-
rameters given the observed data D, that is
arg max?f ,?u P (?
f ,?u|D). By Bayes Rule:
P (?f ,?u|D) =
P (D|?f ,?u)P (?f ,?u)
P (D)
(1)
We outline a generative model for the data,
P (D|?f ,?u). Let us assume that the event R?x de-
pends only on ?fR and ?
u
x , and further assume that
given these two parameters, local ambiguity and lo-
cal functionality are conditionally independent. We
obtain the following expression for the probability
of R?x given the parameters:
P (R?x|?
f ,?u) = ?fR?
u
x
We assume each set of data DR(x,?) is gener-
ated independently of all other data and parameters,
given R?x. From this and the above we have:
P (D|?f ,?u) =
?
R,x
(
P (DR(x,?)|R
?
x)?
f
R?
u
x
+P (DR(x,?)|?R
?
x)(1? ?
f
R?
u
x)
)
(2)
These independence assumptions allow us to ex-
press P (D|?f ,?u) in terms of distributions over
DR(x,?) given whether or not R
?
x holds. We use the
URNS model as described in (Downey et al, 2005)
to estimate these probabilities based on binomial
distributions. In the single-urn URNS model that we
utilize, the extraction process is modeled as draws of
labeled balls from an urn, where the labels are either
correct extractions or errors, and different labels can
be repeated on varying numbers of balls in the urn.
Let k = maxDR(x,?), and let n =
?
DR(x,?);
we will approximate the distribution over DR(x,?)
in terms of k and n. If R(x, ?) is locally func-
tional and unambiguous, there is exactly one cor-
rect extraction label in the urn (potentially repeated
multiple times). Because the probability of correct-
ness tends to increase with extraction frequency, we
make the simplifying assumption that the most fre-
quently extracted element is correct.3 In this case, k
is the number of correct extractions, which by the
3As this assumption is invalid when there is not a unique
maximal element, we default to the prior P (R?x) in that case.
URNS model has a binomial distribution with pa-
rameters n and p, where p is the precision of the ex-
traction process. If R(x, ?) is not locally functional
and unambiguous, then we expect k to typically take
on smaller values. Empirically, the underlying fre-
quency of the most frequent element in the?R?x case
tends to follow a Beta distribution.
Under the model, the probability of the evidence
given R?x is:
P (DR(x,?)|R
?
x) ? P (k, n|R
?
x) =
(
n
k
)
pk(1? p)n?k
And the probability of the evidence given ?R?x is:
P (DR(x,?)|?R
?
x) ? P (k, n|?R
?
x)
=
(n
k
) ? 1
0
p?k+?f?1(1?p?)n+?f?1?k
B(?f ,?f )
dp?
=
(n
k
)
?(n? k + ?f )?(?f + k)
B(?f , ?f )?(?f + ?f + n)
(3)
where n is the sum over DR(x,?), ? is the Gamma
function and B is the Beta function. ?f and ?f are
the parameters of the Beta distribution for the ?R?x
case. These parameters and the prior distributions
are estimated empirically, based on a sample of the
data set of relations described in Section 5.1.
3.2 Estimating Functionality and Ambiguity
Substituting Equation 3 into Equation 2 and apply-
ing an appropriate prior gives the probability of pa-
rameters ?f and ?u given the observed data D.
However, Equation 2 contains a large product of
sums?with two independent vectors of coefficients,
?f and ?u?making it difficult to optimize analyti-
cally.
If we knew which arguments were ambiguous,
we would ignore them in computing the function-
ality of a relation. Likewise, if we knew which rela-
tions were non-functional, we would ignore them in
computing the ambiguity of an argument. Instead,
we initialize the ?f and ?u arrays randomly, and
then execute an algorithm similar to Expectation-
Maximization (EM) (Dempster et al, 1977) to arrive
at a high-probability setting of the parameters.
Note that if ?u is fixed, we can compute the ex-
pected fraction of locally unambiguous arguments x
for which R is locally functional, using DR(x?,?) and
14
Equation 3. Likewise, for fixed ?f , for any given
x we can compute the expected fraction of locally
functional relations R that are locally unambiguous
for x.
Specifically, we repeat until convergence:
1. Set ?fR =
1
sR
?
x P (R
?
x|DR(x,?))?
u
x for all R.
2. Set ?ux =
1
sx
?
R P (R
?
x|DR(x,?))?
f
R for all x.
In both steps above, the sums are taken over only
those x or R for which DR(x,?) is non-empty. Also,
the normalizer sR =
?
x ?
u
x and likewise sx =?
R ?
f
R.
As in standard EM, we iteratively update our pa-
rameter values based on an expectation computed
over the unknown variables. However, we alter-
nately optimize two disjoint sets of parameters (the
functionality and ambiguity parameters), rather than
just a single set of parameters as in standard EM.
Investigating the optimality guarantees and conver-
gence properties of our algorithm is an item of future
work.
By iteratively setting the parameters to the expec-
tations in steps 1 and 2, we arrive at a good setting
of the parameters. Section 5.2 reports on the perfor-
mance of this algorithm in practice.
4 System Overview
AUCONTRAIRE identifies phrases denoting func-
tional relations and utilizes these to find contradic-
tory assertions in a massive, open-domain corpus of
text.
AUCONTRAIRE begins by finding extractions of
the form R(x, y), and identifies a set of relations
R that have a high probability of being functional.
Next, AUCONTRAIRE identifies contradiction sets
of the form R(x, ?). In practice, most contradiction
sets turned out to consist overwhelmingly of seem-
ing contradictions?assertions that do not actually
contradict each other for a variety of reasons that
we enumerate in section 4.3. Thus, a major chal-
lenge for AUCONTRAIRE is to tease apart which
pairs of assertions in R(x, ?) represent genuine con-
tradictions.
Here are the main components of AUCONTRAIRE
as illustrated in Figure 2:
Extractor: Create a set of extracted assertions E
from a large corpus of Web pages or other docu-
ments. Each extraction R(x, y) has a probability p
Figure 2: AUCONTRAIRE architecture
of being correct.
Function Learner: Discover a set of functional re-
lations F from among the relations in E . Assign to
each relation in F a probability pf that it is func-
tional.
Contradiction Detector: Query E for assertions
with a relation R in F , and identify sets C of po-
tentially contradictory assertions. Filter out seeming
contradictions in C by reasoning about synonymy,
meronymy, argument types, and argument ambigu-
ity. Assign to each potential contradiction a proba-
bility pc that it is a genuine contradiction.
4.1 Extracting Factual Assertions
AUCONTRAIRE needs to explore a large set of
factual assertions, since genuine contradictions are
quite rare (see Section 5). We used a set of extrac-
tions E from the Open Information Extraction sys-
tem, TEXTRUNNER (Banko et al, 2007), which was
run on a set of 117 million Web pages.
TEXTRUNNER does not require a pre-defined set
of relations, but instead uses shallow linguistic anal-
ysis and a domain-independent model to identify
phrases from the text that serve as relations and
phrases that serve as arguments to that relation.
TEXTRUNNER creates a set of extractions in a sin-
gle pass over the Web page collection and provides
an index to query the vast set of extractions.
Although its extractions are noisy, TEXTRUNNER
provides a probability that the extractions are cor-
15
rect, based in part on corroboration of facts from
different Web pages (Downey et al, 2005).
4.2 Finding Potential Contradictions
The next step of AUCONTRAIRE is to find contra-
diction sets in E .
We used the methods described in Section 3 to
estimate the functionality of the most frequent rela-
tions in E . For each relation R that AUCONTRAIRE
has judged to be functional, we identify contradic-
tion sets R(x, ?), where a relation R and domain ar-
gument x have multiple range arguments y.
4.3 Handling Seeming Contradictions
For a variety of reasons, a pair of extractions
R(x, y1) and R(x, y2) may not be actually contra-
dictory. The following is a list of the major sources
of false positives?pairs of extractions that are not
genuine contradictions, and how they are handled
by AUCONTRAIRE. The features indicative of each
condition are combined using Logistic Regression,
in order to estimate the probability that a given pair,
{R(x, y1), R(x, y2)} is a genuine contradiction.
Synonyms: The set of potential contradictions
died from(Mozart,?) may contain assertions that
Mozart died from renal failure and that he died from
kidney failure. These are distinct values of y, but
do not contradict each other, as the two terms are
synonyms. AUCONTRAIRE uses a variety of knowl-
edge sources to handle synonyms. WordNet is a re-
liable source of synonyms, particularly for common
nouns, but has limited recall. AUCONTRAIRE also
utilizes synonyms generated by RESOLVER (Yates
and Etzioni, 2007)? a system that identifies syn-
onyms from TEXTRUNNER extractions. Addition-
ally, AUCONTRAIRE uses edit-distance and token-
based string similarity (Cohen et al, 2003) between
apparently contradictory values of y to identify syn-
onyms.
Meronyms: For some relations, there is no con-
tradiction when y1 and y2 share a meronym,
i.e. ?part of? relation. For example, in the set
born in(Mozart,?) there is no contradiction be-
tween the y values ?Salzburg? and ?Austria?, but
?Salzburg? conflicts with ?Vienna?. Although this
is only true in cases where y occurs in an up-
ward monotone context (MacCartney and Manning,
2007), in practice genuine contradictions between
y-values sharing a meronym relationship are ex-
tremely rare. We therefore simply assigned contra-
dictions between meronyms a probability close to
zero. We used the Tipster Gazetteer4 and WordNet
to identify meronyms, both of which have high pre-
cision but low coverage.
Argument Typing: Two y values are not contra-
dictory if they are of different argument types. For
example, the relation born in can take a date or a
location for the y value. While a person can be
born in only one year and in only one city, a per-
son can be born in both a year and a city. To avoid
such false positives, AUCONTRAIRE uses a sim-
ple named-entity tagger5 in combination with large
dictionaries of person and location names to as-
sign high-level types (person, location, date, other)
to each argument. AUCONTRAIRE filters out ex-
tractions from a contradiction set that do not have
matching argument types.
Ambiguity: As pointed out in Section 3, false con-
tradictions arise when a single x value refers to mul-
tiple real-world entities. For example, if the con-
tradiction set born in(John Sutherland, ?) includes
birth years of both 1827 and 1878, is one of these a
mistake, or do we have a grandfather and grandson
with the same name? AUCONTRAIRE computes the
probability that an x value is unambiguous as part
of its Function Learner (see Section 3). An x value
can be identified as ambiguous if its distribution of
y values is non-functional for multiple functional re-
lations.
If a pair of extractions, {R(x, y1), R(x, y2)}, does
not fall into any of the above categories and R is
functional, then it is likely that the sentences under-
lying the extractions are indeed contradictory. We
combined the various knowledge sources described
above using Logistic Regression, and used 10-fold
cross-validation to automatically tune the weights
associated with each knowledge source. In addi-
tion, the learning algorithm also utilizes the follow-
ing features:
? Global functionality of the relation, ?fR
? Global unambiguity of x, ?ux
4http://crl.nmsu.edu/cgi-bin/Tools/CLR/
clrcat
5http://search.cpan.org/?simon/
Lingua-EN-NamedEntity-1.1/NamedEntity.pm
16
? Local functionality of R(x, ?)
? String similarity (a combination of token-based
similarity and edit-distance) between y1 and y2
? The argument types (person, location, date, or
other)
The learned model is then used to estimate how
likely a potential contradiction {R(x, y1), R(x, y2)}
is to be genuine.
5 Experimental Results
We evaluated several aspects of AUCONTRAIRE:
its ability to detect functional relations and to de-
tect ambiguous arguments (Section 5.2); its preci-
sion and recall in contradiction detection (Section
5.3); and the contribution of AUCONTRAIRE?s key
knowledge sources (Section 5.4).
5.1 Data Set
To evaluate AUCONTRAIRE we used TEXTRUN-
NER?s extractions from a corpus of 117 million Web
pages. We restricted our data set to the 1,000 most
frequent relations, in part to keep the experiments
tractable and also to ensure sufficient statistical sup-
port for identifying functional relations.
We labeled each relation as functional or not,
and computed an estimate of the probability it is
functional as described in section 3.2. Section 5.2
presents the results of the Function Learner on this
set of relations. We took the top 2% (20 relations)
as F , the set of functional relations in our exper-
iments. Out of these, 75% are indeed functional.
Some examples include: was born in, died in, and
was founded by.
There were 1.2 million extractions for all thou-
sand relations, and about 20,000 extractions in 6,000
contradiction sets for all relations in F .
We hand-tagged 10% of the contradiction sets
R(x, ?) where R ? F , discarding any sets with over
20 distinct y values since the x argument for that
set is almost certainly ambiguous. This resulted in a
data set of 567 contradiction sets containing a total
of 2,564 extractions and 8,844 potentially contradic-
tory pairs of extractions.
We labeled each of these 8,844 pairs as contradic-
tory or not. In each case, we inspected the original
sentences, and if the distinction was unclear, con-
sulted the original source Web pages, Wikipedia ar-
ticles, and Web search engine results.
In our data set, genuine contradictions over func-
tional relations are surprisingly rare. We found only
110 genuine contradictions in the hand-tagged sam-
ple, only 1.2% of the potential contradiction pairs.
5.2 Detecting Functionality and Ambiguity
We ran AUCONTRAIRE?s EM algorithm on the
thousand most frequent relations. Performance con-
verged after 5 iterations resulting in estimates of the
probability that each relation is functional and each
x argument is unambiguous. We used these proba-
bilities to generate the precision-recall curves shown
in Figure 3.
The graph on the left shows results for function-
ality, while the graph on the right shows precision at
finding unambiguous arguments. The solid lines are
results after 5 iterations of EM, and the dashed lines
are from computing functionality or ambiguity with-
out EM (i.e. assuming uniform values of ?c when
computing ?f and vice versa). The EM algorithm
improved results for both functionality and ambigu-
ity, increasing area under curve (AUC) by 19% for
functionality and by 31% for ambiguity.
Of course, the ultimate test of how well AUCON-
TRAIRE can identify functional relations is how well
the Contradiction Detector performs on automati-
cally identified functional relations.
5.3 Detecting Contradictions
We conducted experiments to evaluate how well
AUCONTRAIRE distinguishes genuine contradic-
tions from false positives.
The bold line in Figure 4 depicts AUCONTRAIRE
performance on the distribution of contradictions
and seeming contradictions found in actual Web
data. The dashed line shows the performance of AU-
CONTRAIRE on an artificially ?balanced? data set
that we constructed to contain 50% genuine contra-
dictions and 50% seeming ones.
Previous research in CD presented results on
manually selected data sets with a relatively bal-
anced mix of positive and negative instances. As
Figure 4 suggests, this is a much easier problem than
CD ?in the wild?. The data gathered from the Web
is badly skewed, containing only 1.2% genuine con-
tradictions.
17
Functionality
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Ambiguity
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Figure 3: After 5 iterations of EM, AUCONTRAIRE achieves a 19% boost to area under the precision-recall curve
(AUC) for functionality detection, and a 31% boost to AUC for ambiguity detection.
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Web DistributionBalanced Data
Figure 4: Performance of AUCONTRAIRE at distinguish-
ing genuine contradictions from false positives. The bold
line is results on the actual distribution of data from the
Web. The dashed line is from a data set constructed to
have 50% positive and 50% negative instances.
5.4 Contribution of Knowledge Sources
We carried out an ablation study to quantify how
much each knowledge source contributes to AU-
CONTRAIRE?s performance. Since most of the
knowledge sources do not apply to numeric argu-
ment values, we excluded the extractions where y
is a number in this study. As shown in Figure 5,
performance of AUCONTRAIRE degrades with no
knowledge of synonyms (NS), with no knowledge
of meronyms (NM), and especially without argu-
ment typing (NT). Conversely, improvements to any
of these three components would likely improve the
performance of AUCONTRAIRE.
The relatively small drop in performance from
no meronyms does not indicate that meronyms are
not essential to our task, only that our knowledge
sources for meronyms were not as useful as we
hoped. The Tipster Gazetteer has surprisingly low
coverage for our data set. It contains only 41% of
the y values that are locations. Many of these are
matches on a different location with the same name,
which results in incorrect meronym information. We
estimate that a gazetteer with complete coverage
would increase area under the curve by approxi-
mately 40% compared to a system with meronyms
from the Tipster Gazetteer and WordNet.
AuContraire NS NM NT
Percentage AUC
0
20
40
60
80
100
Figure 5: Area under the precision-recall curve for the
full AUCONTRAIRE and for AUCONTRAIRE with knowl-
edge removed. NS has no synonym knowledge; NM has
no meronym knowledge; NT has no argument typing.
To analyze the errors made by AUCONTRAIRE,
we hand-labeled all false-positives at the point of
maximum F-score: 29% Recall and 48% Precision.
18
Figure 6 reveals the central importance of world
knowledge for the CD task. About half of the errors
(49%) are due to ambiguous x-arguments, which we
found to be one of the most persistent obstacles to
discovering genuine contradictions. A sizable por-
tion is due to missing meronyms (34%) and missing
synonyms (14%), suggesting that lexical resources
with broader coverage than WordNet and the Tipster
Gazetteer would substantially improve performance.
Surprisingly, only 3% are due to errors in the extrac-
tion process.
Extraction Errors (3%)
Missing Synonyms (14%)
Missing Meronyms (34%)
Ambiguity (49%)
Figure 6: Sources of errors in contradiction detection.
All of our experimental results are based on the
automatically discovered set of functions F . We
would expect AUCONTRAIRE?s performance to im-
prove substantially if it were given a large set of
functional relations as input.
6 Related Work
Condoravdi et al (2003) first proposed contradiction
detection as an important NLP task, and Harabagiu
et al (2006) were the first to report results on con-
tradiction detection using negation, although their
evaluation corpus was a balanced data set built
by manually negating entailments in a data set
from the Recognizing Textual Entailment confer-
ences (RTE) (Dagan et al, 2005). De Marneffe et
al. (2008) reported experimental results on a contra-
diction corpus created by annotating the RTE data
sets.
RTE-3 included an optional task, requiring sys-
tems to make a 3-way distinction: {entails, contra-
dicts, neither} (Voorhees, 2008). The average per-
formance for contradictions on the RTE-3 was preci-
sion 0.11 at recall 0.12, and the best system had pre-
cision 0.23 at recall 0.19. We did not run AUCON-
TRAIRE on the RTE data sets because they contained
relatively few of the ?functional contradictions? that
AUCONTRAIRE tackles. On our Web-based data
sets, we achieved a precision of 0.62 at recall 0.19,
and precision 0.92 at recall 0.51 on the balanced data
set. Of course, comparisons across very different
data sets are not meaningful, but merely serve to un-
derscore the difficulty of the CD task.
In contrast to previous work, AUCONTRAIRE is
the first to do CD on data automatically extracted
from the Web. This is a much harder problem than
using an artificially balanced data set, as shown in
Figure 4.
Automatic discovery of functional relations has
been addressed in the database literature as Func-
tional Dependency Mining (Huhtala et al, 1999;
Yao and Hamilton, 2008). This focuses on dis-
covering functional relationships between sets of at-
tributes, and does not address the ambiguity inherent
in natural language.
7 Conclusions and Future Work
We have described a case study of contradiction de-
tection (CD) based on functional relations. In this
context, we introduced and evaluated the AUCON-
TRAIRE system and its novel EM-style algorithm
for determining whether an arbitrary phrase is func-
tional. We also created a unique ?natural? data set
of seeming contradictions based on sentences drawn
from a Web corpus, which we make available to the
research community.
We have drawn two key lessons from our case
study. First, many seeming contradictions (approx-
imately 99% in our experiments) are not genuine
contradictions. Thus, the CD task may be much
harder on natural data than on RTE data as sug-
gested by Figure 4. Second, extensive background
knowledge is necessary to tease apart seeming con-
tradictions from genuine ones. We believe that these
lessons are broadly applicable, but verification of
this claim is a topic for future work.
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, ONR grant N00014-
08-1-0431 as well as gifts from the Utilika Founda-
tion and Google, and was carried out at the Univer-
sity of Washington?s Turing Center.
19
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
traditional and open relation extraction. In Proceed-
ings of ACL.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In IIWeb.
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. In Proceedings
of the HLT-NAACL 2003 workshop on Text meaning,
pages 38?45, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL 2008.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society Se-
ries B, 39(1):1?38.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI.
Yka? Huhtala, Juha Ka?rkka?inen, Pasi Porkka, and Hannu
Toivonen. 1999. TANE: An efficient algorithm for
discovering functional and approximate dependencies.
The Computer Journal, 42(2):100?111.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of ACL-08: HLT, pages 63?71, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hong Yao and Howard J. Hamilton. 2008. Mining func-
tional dependencies from data. Data Min. Knowl. Dis-
cov., 16(2):197?219.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
20
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 563?570, Vancouver, October 2005. c?2005 Association for Computational Linguistics
KnowItNow: Fast, Scalable Information Extraction from the Web
Michael J. Cafarella, Doug Downey, Stephen Soderland, Oren Etzioni
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
{mjc,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Numerous NLP applications rely on
search-engine queries, both to ex-
tract information from and to com-
pute statistics over the Web corpus.
But search engines often limit the
number of available queries. As a
result, query-intensive NLP applica-
tions such as Information Extraction
(IE) distribute their query load over
several days, making IE a slow, off-
line process.
This paper introduces a novel archi-
tecture for IE that obviates queries to
commercial search engines. The ar-
chitecture is embodied in a system
called KNOWITNOW that performs
high-precision IE in minutes instead
of days. We compare KNOWITNOW
experimentally with the previously-
published KNOWITALL system, and
quantify the tradeoff between re-
call and speed. KNOWITNOW?s ex-
traction rate is two to three orders
of magnitude higher than KNOW-
ITALL?s.
1 Background and Motivation
Numerous modern NLP applications use the Web as their
corpus and rely on queries to commercial search engines
to support their computation (Turney, 2001; Etzioni et al,
2005; Brill et al, 2001). Search engines are extremely
helpful for several linguistic tasks, such as computing us-
age statistics or finding a subset of web documents to an-
alyze in depth; however, these engines were not designed
as building blocks for NLP applications. As a result,
the applications are forced to issue literally millions of
queries to search engines, which limits the speed, scope,
and scalability of the applications. Further, the applica-
tions must often then fetch some web documents, which
at scale can be very time-consuming.
In response to heavy programmatic search engine use,
Google has created the ?Google API? to shunt program-
matic queries away from Google.com and has placed hard
quotas on the number of daily queries a program can is-
sue to the API. Other search engines have also introduced
mechanisms to limit programmatic queries, forcing ap-
plications to introduce ?courtesy waits? between queries
and to limit the number of queries they issue.
To understand these efficiency problems in more detail,
consider the KNOWITALL information extraction sys-
tem (Etzioni et al, 2005). KNOWITALL has a generate-
and-test architecture that extracts information in two
stages. First, KNOWITALL utilizes a small set of domain-
independent extraction patterns to generate candidate
facts (cf. (Hearst, 1992)). For example, the generic pat-
tern ?NP1 such as NPList2? indicates that the head of
each simple noun phrase (NP) in NPList2 is a member of
the class named in NP1. By instantiating the pattern for
class City, KNOWITALL extracts three candidate cities
from the sentence: ?We provide tours to cities such as
Paris, London, and Berlin.? Note that it must also fetch
each document that contains a potential candidate.
Next, extending the PMI-IR algorithm (Turney, 2001),
KNOWITALL automatically tests the plausibility of the
candidate facts it extracts using pointwise mutual in-
formation (PMI) statistics computed from search-engine
hit counts. For example, to assess the likelihood that
?Yakima? is a city, KNOWITALL will compute the PMI
between Yakima and a set of k discriminator phrases that
tend to have high mutual information with city names
(e.g., the simple phrase ?city?). Thus, KNOWITALL re-
quires at least k search-engine queries for every candidate
extraction it assesses.
Due to KNOWITALL?s dependence on search-engine
queries, large-scale experiments utilizing KNOWITALL
take days and even weeks to complete, which makes re-
search using KNOWITALL slow and cumbersome. Pri-
vate access to Google-scale infrastructure would provide
563
sufficient access to search queries, but at prohibitive cost,
and the problem of fetching documents (even if from a
cached copy) would remain (as we discuss in Section
2.1). Is there a feasible alternative Web-based IE system?
If so, what size Web index and how many machines are
required to achieve reasonable levels of precision/recall?
What would the architecture of this IE system look like,
and how fast would it run?
To address these questions, this paper introduces a
novel architecture for web information extraction. It
consists of two components that supplant the generate-
and-test mechanisms in KNOWITALL. To generate ex-
tractions rapidly we utilize our own specialized search
engine, called the Bindings Engine (or BE), which ef-
ficiently returns bindings in response to variabilized
queries. For example, in response to the query ?Cities
such as ProperNoun(Head(?NounPhrase?))?, BE will
return a list of proper nouns likely to be city names. To
assess these extractions, we use URNS, a combinatorial
model, which estimates the probability that each extrac-
tion is correct without using any additional search engine
queries.1 For further efficiency, we introduce an approx-
imation to URNS, based on frequency of extractions? oc-
currence in the output of BE, and show that it achieves
comparable precision/recall to URNS.
Our contributions are as follows:
1. We present a novel architecture for Information Ex-
traction (IE), embodied in the KNOWITNOW sys-
tem, which does not depend on Web search-engine
queries.
2. We demonstrate experimentally that KNOWITNOW
is the first system able to extract tens of thousands
of facts from the Web in minutes instead of days.
3. We show that KNOWITNOW?s extraction rate is two
to three orders of magnitude greater than KNOW-
ITALL?s, but this increased efficiency comes at the
cost of reduced recall. We quantify this tradeoff for
KNOWITNOW?s 60,000,000 page index and extrap-
olate how the tradeoff would change with larger in-
dices.
Our recent work has described the BE search engine
in detail (Cafarella and Etzioni, 2005), and also analyzed
the URNS model?s ability to compute accurate probability
estimates for extractions (Downey et al, 2005). However,
this is the first paper to investigate the composition of
these components to create a fast IE system, and to com-
pare it experimentally to KNOWITALL in terms of time,
1In contrast, PMI-IR, which is built into KNOWITALL, re-
quires multiple search engine queries to assess each potential
extraction.
recall, precision, and extraction rate. The frequency-
based approximation to URNS and the demonstration of
its success are also new.
The remainder of the paper is organized as follows.
Section 2 provides an overview of BE?s design. Sec-
tion 3 describes the URNS model and introduces an ef-
ficient approximation to URNS that achieves similar pre-
cision/recall. Section 4 presents experimental results. We
conclude with related and future work in Sections 5 and
6.
2 The Bindings Engine
This section explains how relying on standard search en-
gines leads to a bottleneck for NLP applications, and pro-
vides a brief overview of the Bindings Engine (BE)?our
solution to this problem. A comprehensive description of
BE appears in (Cafarella and Etzioni, 2005).
Standard search engines are computationally expen-
sive for IE and other NLP tasks. IE systems issue multiple
queries, downloading all pages that potentially match an
extraction rule, and performing expensive processing on
each page. For example, such systems operate roughly as
follows on the query (?cities such as ?NounPhrase??):
1. Perform a traditional search engine query to find
all URLs containing the non-variable terms (e.g.,
?cities such as?)
2. For each such URL:
(a) obtain the document contents,
(b) find the searched-for terms (?cities such as?) in
the document text,
(c) run the noun phrase recognizer to determine
whether text following ?cities such as? satisfies
the linguistic type requirement,
(d) and if so, return the string
We can divide the algorithm into two stages: obtaining
the list of URLs from a search engine, and then process-
ing them to find the ?NounPhrase? bindings. Each stage
poses its own scalability and speed challenges. The first
stage makes a query to a commercial search engine; while
the number of available queries may be limited, a single
one executes relatively quickly. The second stage fetches
a large number of documents, each fetch likely resulting
in a random disk seek; this stage executes slowly. Nat-
urally, this disk access is slow regardless of whether it
happens on a locally-cached copy or on a remote doc-
ument server. The observation that the second stage is
slow, even if it is executed locally, is important because
it shows that merely operating a ?private? search engine
does not solve the problem (see Section 2.1).
The Bindings Engine supports queries contain-
ing typed variables (such as NounPhrase) and
564
string-processing functions (such as ?head(X)? or
?ProperNoun(X)?) as well as standard query terms. BE
processes a variable by returning every possible string
in the corpus that has a matching type, and that can be
substituted for the variable and still satisfy the user?s
query. If there are multiple variables in a query, then all
of them must simultaneously have valid substitutions.
(So, for example, the query ?<NounPhrase> is located
in <NounPhrase>? only returns strings when noun
phrases are found on both sides of ?is located in?.) We
call a string that meets these requirements a binding for
the variable in question. These queries, and the bindings
they elicit, can usefully serve as part of an information
extraction system or other common NLP tasks (such as
gathering usage statistics). Figure 1 illustrates some of
the queries that BE can handle.
president Bush <Verb>
cities such as ProperNoun(Head(<NounPhrase>))
<NounPhrase> is the CEO of <NounPhrase>
Figure 1: Examples of queries that can be handled by
BE. Queries that include typed variables and string-
processing functions allow NLP tasks to be done ef-
ficiently without downloading the original document
during query processing.
BE?s novel neighborhood index enables it to process
these queries with O(k) random disk seeks and O(k) se-
rial disk reads, where k is the number of non-variable
terms in its query. As a result, BE can yield orders of
magnitude speedup as shown in the asymptotic analysis
later in this section. The neighborhood index is an aug-
mented inverted index structure. For each term in the cor-
pus, the index keeps a list of documents in which the term
appears and a list of positions where the term occurs, just
as in a standard inverted index (Baeza-Yates and Ribeiro-
Neto, 1999). In addition, the neighborhood index keeps
a list of left-hand and right-hand neighbors at each posi-
tion. These are adjacent text strings that satisfy a recog-
nizer for one of the target types, such as NounPhrase.
As with a standard inverted index, a term?s list is pro-
cessed from start to finish, and can be kept on disk as a
contiguous piece. The relevant string for a variable bind-
ing is included directly in the index, so there is no need
to fetch the source document (thus causing a disk seek).
Expensive processing such as part-of-speech tagging or
shallow syntactic parsing is performed only once, while
building the index, and is not needed at query time. It
is important to note that simply preprocessing the corpus
and placing the results in a database would not avoid disk
seeks, as we would still have to explicitly fetch these re-
sults. The run-time efficiency of the neighborhood index
Query Time Index Space
BE O(k) O(N)
Standard engine O(k + B) O(N)
Table 1: BE yields considerable savings in query time
over a standard search engine. k is the number of con-
crete terms in the query, B is the number of variable
bindings found in the corpus, and N is the number of
documents in the corpus. N and B are typically ex-
tremely large, while k is small.
comes from integrating the results of corpus processing
with the inverted index (which determines which of those
results are relevant).
The neighborhood index avoids the need to return to
the original corpus, but it can consume a large amount
of disk space, as parts of the corpus text are folded into
the index several times. To conserve space, we perform
simple dictionary-lookup compression of strings in the
index. The storage penalty will, of course, depend on the
exact number of different types added to the index. In our
experiments, we created a useful IE system with a small
number of types (including NounPhrase) and found that
the neighborhood index increased disk space only four
times that of a standard inverted index.
Asymptotic Analysis:
In our asymptotic analysis of BE?s behavior, we count
query time as a function of the number of random disk
seeks, since these seeks dominate all other processing
tasks. Index space is simply the number of bytes needed
to store the index (not including the corpus itself).
Table 1 shows that BE requires only O(k) random disk
seeks to process queries with an arbitrary number of vari-
ables whereas a standard engine takes O(k + B), where
k is the number of concrete query terms, and B is the
number of bindings found in a corpus of N documents.
Thus, BE?s performance is the same as that of a standard
search engine for queries containing only concrete terms.
For variabilized queries, N may be in the billions and B
will tend to grow with N . In our experiments, eliminating
the B term from our query processing time has resulted
in speedups of two to three orders of magnitude over a
standard search engine. The speedup is at the price of a
small constant multiplier to index size.
2.1 Discussion
While BE has some attractive properties for NLP compu-
tations, is it necessary? Could fast, large-scale informa-
tion extraction be achieved merely by operating a ?pri-
vate? search engine?
The release of open-source search engines such as
Nutch2, coupled with the dropping price of CPUs and
2http://lucene.apache.org/nutch/
565
8.16
0.06
0
1
2
3
4
5
6
7
8
9
10
BE Nutch
El
ap
se
d 
m
in
u
te
s
Figure 2: Average time to return the relevant bindings
in response to a set of queries was 0.06 CPU minutes
for BE, compared to 8.16 CPU minutes for the com-
parable processing on Nutch. This is a 134-fold speed
up. The CPU resources, network, and index size were
the same for both systems.
disks, makes it feasible for NLP researchers to operate
their own large-scale search engines. For example, Tur-
ney operates a search engine with a terabyte-sized index
of Web pages, running on a local eight-machine Beowulf
cluster (Turney, 2004). Private search engines have two
advantages. First, there is no query quota or need for
?courtesy waits? between queries. Second, since the en-
gine is local, network latency is minimal.
However, to support IE, we must also execute the sec-
ond stage of the algorithm (see the beginning of this sec-
tion). In this stage, each document that matches a query
has to be retrieved from an arbitrary location on a disk.3
Thus, the number of random disk seeks scales linearly
with the number of documents retrieved. Moreover, many
NLP applications require the extraction of strings match-
ing particular syntactic or semantic types from each page.
The lack of linguistic data in the search engine?s index
means that many pages are fetched only to be discarded
as irrelevant.
To quantify the speedup due to BE, we compared it to a
standard search index built on the open-source Nutch en-
gine. All of our Nutch and BE experiments were carried
out on the same corpus of 60 million Web pages and were
run on a cluster of 23 dual-Xeon machines, each with two
local 140 Gb disks and 4 Gb of RAM. We set al config-
uration values to be exactly the same for both Nutch and
BE. BE gave a 134-fold speed up on average query pro-
cessing time when compared to the same queries with the
Nutch index, as shown in Figure 2.
3Moving the disk head to an arbitrary location on the disk
is a mechanical operation that takes about 5 milliseconds on
average.
3 The URNS Model
To realize the speedup from BE, KNOWITNOW must also
avoid issuing search engine queries to validate the cor-
rectness of each extraction, as required by PMI compu-
tation. We have developed a probabilistic model obviat-
ing search-engine queries for assessment. The intuition
behind this model is that correct instances of a class or
relation are likely to be extracted repeatedly, while ran-
dom errors by an IE system tend to have low frequency
for each distinct incorrect extraction.
Our probabilistic model, which we call URNS, takes the
form of a classic ?balls-and-urns? model from combina-
torics. We think of IE abstractly as a generative process
that maps text to extractions. Each extraction is modeled
as a labeled ball in an urn. A label represents either an
instance of the target class or relation, or represents an
error. The information extraction process is modeled as
repeated draws from the urn, with replacement.
Formally, the parameters that characterize an urn are:
? C ? the set of unique target labels; |C| is the number
of unique target labels in the urn.
? E ? the set of unique error labels; |E| is the number
of unique error labels in the urn.
? num(b) ? the function giving the number of balls
labeled by b where b ? C ? E. num(B) is the
multi-set giving the number of balls for each label
b ? B.
The goal of an IE system is to discern which of the
labels it extracts are in fact elements of C, based on re-
peated draws from the urn. Thus, the central question we
are investigating is: given that a particular label x was
extracted k times in a set of n draws from the urn, what
is the probability that x ? C? We can express the prob-
ability that an element extracted k of n times is of the
target relation as follows.
P (x ? C|x appears k times in n draws) =
?
r?num(C)( rs )k(1 ? rs )n?k
?
r??num(C?E)( r
?
s )k(1 ? r
?
s )n?k
(1)
where s is the total number of balls in the urn, and the
sum is taken over possible repetition rates r.
A few numerical examples illustrate the behavior of
this equation. Let |C| = |E| = 2, 000 and assume
for simplicity that all labels are repeated on the same
number of balls (num(ci) = RC for all ci ? C, and
num(ei) = RE for all ei ? E). Assume that the ex-
traction rules have precision p = 0.9, which means that
RC = 9 ? RE ? target balls are nine times as common
in the urn as error balls. Now, for k = 3 and n = 10, 000
we have P (x ? C) = 93.0%. Thus, we see that a small
number of repetitions can yield high confidence in an ex-
traction. However, when the sample size increases so that
566
n = 20, 000, and the other parameters are unchanged,
then P (x ? C) drops to 19.6%. On the other hand, if
C balls repeat much more frequently than E balls, say
RC = 90?RE (with |E| set to 20,000, so that p remains
unchanged), then P (x ? C) rises to 99.9%.
The above examples enable us to illustrate the advan-
tages of URNS over the noisy-or model used in previous
work. The noisy-or model assumes that each extraction is
an independent assertion that the extracted label is ?true,?
an assertion that is correct a fraction p of the time. The
noisy-or model assigns the following probability to ex-
tractions:
Pnoisy?or(x ? C|x appears k times) = 1 ? (1 ? p)k
Therefore, the noisy-or model will assign the same
probability ? 99.9% ? in all three of the above exam-
ples, although this is only correct in the case for which
n = 10, 000 and RC = 90?RE . As the other two exam-
ples show, for different sample sizes or repetition rates,
the noisy-or model can be highly inaccurate. This is not
surprising given that the noisy-or model ignores the sam-
ple size and the repetition rates.
URNS uses an EM algorithm to estimate its parameters,
and currently the algorithm takes roughly three minutes
to terminate.4 Fortunately, we determined experimen-
tally that we can approximate URNS?s precision and recall
using a far simpler frequency-based assessment method.
This is true because good precision and recall merely re-
quire an appropriate ordering of the extractions for each
relation, and not accurate probabilities for each extrac-
tion. For unary relations, we use the simple approxima-
tion that items extracted more often are more likely to
be true, and order the extractions from most to least ex-
tracted. For binary relations like CapitalOf(X,y),
in which we extract several different candidate capitals y
for each known country X, we use a smoothed frequency
estimate to order the extractions. Let freq(R(X, y)) de-
note the number of times that the binary relation R(X, y)
is extracted; we define:
smoothed freq(R(X, y)) = freq(R(X, y))maxy? freq(R(X, y?)) + 1
We found that sorting by smoothed frequency (in de-
scending order) performed better than simply sorting by
freq for relations R(X, y) in which different known X val-
ues may have widely varying Web presence.
Unlike URNS, our frequency-based assessment does
not yield accurate probabilities to associate with each ex-
traction, but for the purpose of returning a ranked list of
high-quality extractions it is comparable to URNS (see
4This code has not been optimized at all. We believe that
we can easily reduce its running time to less than a minute on
average, and perhaps substantially more.
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150 200 250
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 3: Country: KNOWITALL maintains some-
what higher precision than KNOWITNOW throughout
the recall-precision curve.
Figures 3 through 6), and it has the advantage of being
much faster. Thus, in the experiments reported on below,
we use frequency-based assessment as part of KNOWIT-
NOW.
4 Experimental Results
This section contrasts the performance of KNOWITNOW
and KNOWITALL experimentally. Before considering the
experiments in detail, we note that a key advantage of
KNOWITNOW is that it does not make any queries to Web
search engines. As a result, KNOWITNOW?s scale is not
limited by a query quota, though it is limited by the size
of its index.
We report on the following metrics:
? Recall: how many distinct extractions does each
system return at high precision?5
? Time: how long did each system take to produce
and rank its extractions?
? Extraction Rate: how many distinct high-quality
extractions does the system return per minute? The
extraction rate is simply recall divided by time.
We contrast KNOWITALL and KNOWITNOW?s preci-
sion/recall curves in Figures 3 through 6. We com-
pared KNOWITNOW with KNOWITALL on four rela-
tions: Corp, Country, CeoOf(Corp,Ceo), and
CapitalOf(Country,City). The unary relations
were chosen to examine the difference between a relation
with a small number of correct instances (Country) and
one with a large number of extractions (Corp). The bi-
nary relations were chosen to cover both functional rela-
tions (CapitalOf) and set-valued relations (CeoOf?
we treat former CEOs as correct instances of the relation).
5Since we cannot compute ?true recall? for most relations
on the Web, the paper uses the term ?recall? to refer to the size
of the set of facts extracted.
567
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150 200
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 4: CapitalOf: KNOWITNOW does nearly as
well as KNOWITALL, but has more difficulty than
KNOWITALL with sparse data for capitals of more ob-
scure countries.
For the two unary relations, both systems created ex-
traction rules from eight generic patterns. These are hy-
ponym patterns like ?NP1 {,} such as NPList2? or ?NP2
{,} and other NP1?, which extract members of NPList2
or NP2 as instances of NP1. For the binary relations,
the systems instantiated rules from four generic patterns.
These are patterns for a generic ?of? relation. They are
?NP1 , rel of NP2?, ?NP1 the rel of NP2?, ?rel of NP2
, NP1?, and ?NP2 rel NP1?. When rel is instantiated for
CeoOf, these patterns become ?NP1 , CEO of NP2? and
so forth.
Both KNOWITNOW and KNOWITALL merge extrac-
tions with slight variants in the name, such as those dif-
fering only in punctuation or whitespace, or in the pres-
ence or absence of a corporate designator. For binary
extractions, CEOs with the same last name and same
company were also merged. Both systems rely on the
OpenNlp maximum-entropy part-of-speech tagger and
chunker (Ratnaparkhi, 1996), but KNOWITALL applies
them to pages downloaded from the Web based on the re-
sults of Google queries, whereas KNOWITNOW applies
them once to crawled and indexed pages.6 Overall, each
of the above elements of KNOWITALL and KNOWIT-
NOW are the same to allow for controlled experiments.
Whereas KNOWITNOW runs a small number of vari-
abilized queries (one for each extraction pattern, for
each relation), KNOWITALL requires a stopping crite-
rion. Otherwise, KNOWITALL will continue to query
Google and download URLs found in its result pages over
many days and even weeks. We allowed a total of 6 days
of search time for KNOWITALL, allocating more search
for the relations that continued to be most productive. For
CeoOf KNOWITNOW returned all pairs of Corp,Ceo
6Our time measurements for KNOWITALL are not affected
by the tagging and chunking time because it is dominated
by time required to query Google, waiting a second between
queries.
0.75
0.8
0.85
0.9
0.95
1
0 5,000 10,000 15,000 20,000 25,000
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 5: Corp: KNOWITALL?s PMI assessment main-
tains high precision. KNOWITNOW has low recall up
to precision 0.85, then catches up with KNOWITALL.
in its corpus; KNOWITALL searched for CEOs of a ran-
dom selection of 10% of the corporations it found, and
we projected the total extractions and search effort for all
corporations. For CapitalOf, both KNOWITNOW and
KNOWITALL looked for capitals of a set of 195 coun-
tries.
Table 2 shows the number of queries, search time, dis-
tinct correct extractions at precision 0.8, and extraction
rate for each relation. Search time for KNOWITNOW is
measured in seconds and search time for KNOWITALL
is measured in hours. The number of extractions per
minute counts the distinct correct extractions. Since we
limit KNOWITALL to one Google query per second, the
time for KNOWITALL is proportional to the number of
queries. KNOWITNOW?s extraction rate is from 275 to
4,707 times that of KNOWITALL at this level of preci-
sion.
While the number of distinct correct extractions from
KNOWITNOW at precision 0.8 is roughly comparable to
that of 6 days search effort from KNOWITALL, the sit-
uation is different at precision 0.9. KNOWITALL?s PMI
assessor is able to maintain higher precision than KNOW-
ITNOW?s frequency-based assessor. The number of cor-
rect corporations for KNOWITNOW drops from 23,128 at
precision 0.8 to 1,116 at precision 0.9. KNOWITALL is
able to identify 17,620 correct corporations at precision
0.9. Even with the drop in recall, KNOWITNOW?s ex-
traction rate is still 305 times higher than KNOWITALL?s.
The reason for KNOWITNOW?s difficulty at precision 0.9
is due to extraction errors that occur with high frequency,
particularly generic references to companies (?the Seller
is a corporation ...?, ?corporations such as Banks?, etc.)
and truncation of certain company names by the extrac-
tion rules. The more expensive PMI-based assessment
was not fooled by these systematic extraction errors.
Figures 3 through 6 show the recall-precision curves
for KNOWITNOW with URNS assessment, KNOWIT-
NOW with the simpler frequency-based assessment, and
568
Google Queries Time Extractions Extractions per minute
NOW ALL NOW (sec) ALL (hrs) NOW ALL NOW ALL ratio
Corp 0 (16) 201,878 42 56.1 23,128 23,617 33,040 7.02 4,707
Country 0 (16) 35,480 42 9.9 161 203 230 0.34 672
CeoOf 0 (6) 263,646 51 73.2 2,402 5,823 2,836 1.33 2,132
CapitalOf 0 (6) 17,216 55 4.8 169 192 184 0.67 275
Table 2: Comparison of KNOWITNOW with KNOWITALL for four relations, showing number of Google queries
(local BE queries in parentheses), search time, correct extractions at precision 0.8, and extraction rate (the
number of correct extractions at precision 0.8 per minute of search). Overall, KNOWITNOW took a total of
slightly over 3 minutes as compared to a total of 6 days of search for KNOWITALL.
0.75
0.8
0.85
0.9
0.95
1
0 2,000 4,000 6,000
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowitNow-URNS
KnowItAll-PMI
Figure 6: CeoOf: KNOWITNOW has difficulty dis-
tinguishing low frequency correct extractions from
noise. KNOWITALL is able to cope with the sparse
data more effectively.
KNOWITALL with PMI-based assessment. For each of
the four relations, PMI is able to maintain a higher pre-
cision than either frequency-based or URNS assessment.
URNS and frequency-based assessment give roughly the
same levels of precision.
For the relations with a small number of correct in-
stances, Country and CapitalOf, KNOWITNOW is
able to identify 70-80% as many instances as KNOW-
ITALL at precision 0.9. In contrast, Corp and CeoOf
have a huge number of correct instances and a long tail
of low frequency extractions that KNOWITNOW has dif-
ficulty distinguishing from noise. Over one fourth of
the corporations found by KNOWITALL had Google hit
counts less than 10,500, a sparseness problem that was
exacerbated by KNOWITNOW?s limited index size.
Figure 7 shows projected recall from larger KNOW-
ITNOW indices, fitting a sigmoid curve to the recall
from index size of 10M, 20M, up to 60M pages. The
curve was fitted using logistic regression, and is restricted
to asymptote at the level reported for Google-based
KNOWITALL for each relation. We report re-
call at precision 0.9 for capitals of 195 coun-
tries and CEOs of a random selection of the
top 5,000 corporations as ranked by PMI.
Recall is defined as the percent of countries with a
0
0.2
0.4
0.6
0.8
1
0 100 200 300 400
KnowItNow index size (millions)
R
ec
al
l a
t 0
.
9 
pr
ec
is
io
n
KnowItNow CeoOf
Google CeoOf
KnowItNow CapitalOf
Google CapitalOf
Figure 7: Projections of recall (at precision 0.9) as a
function of KNOWITNOW index size. At 400 million
pages, KNOWITNOW?s recall rapidly approaches the
recall achieved by KNOWITALL using roughly 300,000
Google queries.
correct capital or the number of correct CEOs divided by
the number of corporations.
The curve for CeoOf is rising steeply enough that a
400 million page KNOWITNOW index may approach the
same level of recall yielded by KNOWITALL when it uses
300,000 Google queries. As shown in Table 2, KNOW-
ITALL takes slightly more than three days to generate
these results. KNOWITNOW would operate over a cor-
pus 6.7 times its current one, but the number of required
random disk seeks (and the asymptotic run time analy-
sis) would remain the same. We thus expect that with a
larger corpus we can construct a KNOWITNOW system
that reproduces KNOWITALL levels of precision and re-
call while still executing in the order of a few minutes.
5 Related Work
There has been very little work published on how to make
NLP computations such as PMI-IR and IE fast for large
corpora. Indeed, extraction rate is not a metric typically
used to evaluate IE systems, but we believe it is an im-
portant metric if IE is to scale.
Hobbs et al point out the advantage of fast text
processing for rapid system development (Hobbs et al,
1992). They could test each change to system parameters
569
and domain-specific patterns on a large sample of docu-
ments, having moved from a system that took 36 hours to
process 100 documents to FASTUS, which took only 11
minutes. This allowed them to develop one of the highest
performing MUC-4 systems in only one month.
While there has been extensive work in the IR and
Web communities on improvements to the standard in-
verted index scheme, there has been little work on effi-
cient large-scale search to support natural language ap-
plications. One exception is Resnik?s Linguist?s Search
Engine (Elkiss and Resnik, 2004), a tool for searching
large corpora of parse trees. There is little published in-
formation about its indexing system, but the user man-
ual suggests its corpus is a combination of indexed sen-
tences and user-specific document collections driven by
the user?s AltaVista queries. In contrast, the BE system
has a single index, constructed just once, that serves all
queries. There is no published performance data avail-
able for Resnik?s system.
6 Conclusions and Future Directions
In previous work, statistical NLP computation over large
corpora has been a slow, offline process, as in KNOW-
ITALL (Etzioni et al, 2005) and also in PMI-IR appli-
cations such as sentiment classification (Turney, 2002).
Technology trends, and open source search engines such
as Nutch, have made it feasible to create ?private? search
engines that index large collections of documents; but as
shown in Figure 2, firing large numbers of queries at pri-
vate search engines is still slow.
This paper described a novel and practical approach
towards substantially speeding up IE. We described
KNOWITNOW, which extracts thousands of facts in min-
utes instead of days. Furthermore, we sketched URNS,
a probabilistic model that both obviates the need for
search-engine queries and outputs more accurate prob-
abilities than PMI-IR. Finally, we introduced a simple,
efficient approximation to URNS, whose probability esti-
mates are not as good, but which has comparable preci-
sion/recall to URNS, making it an appropriate assessor for
KNOWITNOW.
The speed and massively improved extraction rate of
KNOWITNOW come at the cost of reduced recall. We
quantified this tradeoff in Table 2, and also argued that as
KNOWITNOW?s index size increases from 60 million to
400 million pages, KNOWITNOW would achieve in min-
utes the same precision/recall that takes KNOWITALL
days to obtain. Of course, a hybrid approach is possi-
ble where KNOWITNOW has, say, a 100 million page
index and, when necessary, augments its results with a
limited number of queries to Google. Investigating the
extraction-rate/recall tradeoff in such a hybrid system is
a natural next step.
While our experiments have used the Web corpus, our
approach transfers readily to other large corpora; exper-
imentation with other corpora is another topic for future
work. In conclusion, we believe that our techniques trans-
form IE from a slow, offline process to an online one.
They could open the door to a new class of interactive IE
applications, of which KNOWITNOW is merely the first.
7 Acknowledgments
This research was supported in part by NSF grant IIS-
0312988, DARPA contract NBCHD030010, ONR grant
N00014-02-1-0324, and gifts from Google and the Tur-
ing Center.
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern Informa-
tion Retrieval. Addison Wesley.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng. 2001.
Data-intensive question answering. In Procs. of Text RE-
trieval Conference (TREC-10), pages 393?400.
M. Cafarella and O. Etzioni. 2005. A Search Engine for Nat-
ural Language Applications. In Procs. of the 14th Interna-
tional World Wide Web Conference (WWW 2005).
D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabilistic
Model of Redundancy in Information Extraction. In Procs.
of the 19th International Joint Conference on Artificial Intel-
ligence (IJCAI 2005).
E. Elkiss and P. Resnik, 2004. The Linguist?s Search Engine
User?s Guide. University of Maryland.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from
Large Text Corpora. In Procs. of the 14th International
Conference on Computational Linguistics, pages 539?545,
Nantes, France.
J.R. Hobbs, D. Appelt, M. Tyson, J. Bear, and D. Israel. 1992.
Description of the FASTUS system used for MUC-4. In
Procs. of the Fourth Message Understanding Conference,
pages 268?275.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Procs. of the Empirical Methods in Natural Language
Processing Conference, Univ. of Pennsylvania.
P. D. Turney. 2001. Mining the Web for Synonyms: PMI-IR
versus LSA on TOEFL. In Procs. of the Twelfth European
Conference on Machine Learning (ECML-2001), pages 491?
502, Freiburg, Germany.
P. D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Procs. of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 417?424.
P. D. Turney, 2004. Waterloo MultiText System. Institute for
Information Technology, Nat?l Research Council of Canada.
570
NAACL HLT Demonstration Program, pages 25?26,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
TextRunner: Open Information Extraction on the Web
Alexander Yates
Michael Cafarella
Michele Banko
Oren Etzioni
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates,banko,hastur,mjc,etzioni,soderlan}@cs.washington.edu
Matthew Broadhead
Stephen Soderland
1 Introduction
Traditional information extraction systems
have focused on satisfying precise, narrow,
pre-specified requests from small, homoge-
neous corpora. In contrast, the TextRunner
system demonstrates a new kind of informa-
tion extraction, called Open Information Ex-
traction (OIE), in which the system makes a
single, data-driven pass over the entire cor-
pus and extracts a large set of relational
tuples, without requiring any human input.
(Banko et al, 2007) TextRunner is a fully-
implemented, highly scalable example of OIE.
TextRunner?s extractions are indexed, al-
lowing a fast query mechanism.
Our first public demonstration of the Text-
Runner system shows the results of perform-
ing OIE on a set of 117 million web pages. It
demonstrates the power of TextRunner in
terms of the raw number of facts it has ex-
tracted, as well as its precision using our novel
assessment mechanism. And it shows the abil-
ity to automatically determine synonymous re-
lations and objects using large sets of extrac-
tions. We have built a fast user interface for
querying the results.
2 Previous Work
The bulk of previous information extraction
work uses hand-labeled data or hand-crafted
patterns to enable relation-specific extraction
(e.g., (Culotta et al, 2006)). OIE seeks to
avoid these requirements for human input.
Shinyama and Sekine (Shinyama and
Sekine, 2006) describe an approach to ?un-
restricted relation discovery? that does away
with many of the requirements for human in-
put. However, it requires clustering of the doc-
uments used for extraction, and thus scales in
quadratic time in the number of documents.
It does not scale to the size of the Web.
For a full discussion of previous work, please
see (Banko et al, 2007), or see (Yates and Et-
zioni, 2007) for work relating to synonym res-
olution.
3 Open IE in TextRunner
OIE presents significant new challenges for in-
formation extraction systems, including
Automation of relation extraction, which in
traditional information extraction uses hand-
labeled inputs.
Corpus Heterogeneity on the Web, which
makes tools like parsers and named-entity tag-
gers less accurate because the corpus is differ-
ent from the data used to train the tools.
Scalability and efficiency of the system.
Open IE systems are effectively restricted to
a single, fast pass over the data so that they
can scale to huge document collections.
In response to these challenges, Text-
Runner includes several novel components,
which we now summarize (see (Banko et al,
2007) for details).
1. Single Pass Extractor
The TextRunner extractor makes a sin-
gle pass over all documents, tagging sen-
tences with part-of-speech tags and noun-
phrase chunks as it goes. For each pair of noun
phrases that are not too far apart, and subject
to several other constraints, it applies a clas-
sifier described below to determine whether or
not to extract a relationship. If the classifier
25
deems the relationship trustworthy, a tuple of
the form t = (ei, rj , ek) is extracted, where
ei, ek are entities and rj is the relation between
them. For example, TextRunner might ex-
tract the tuple (Edison, invented, light bulbs).
On our test corpus (a 9 million document sub-
set of our full corpus), it took less than 68
CPU hours to process the 133 million sen-
tences. The process is easily parallelized, and
took only 4 hours to run on our cluster.
2. Self-Supervised Classifier
While full parsing is too expensive to apply to
the Web, we use a parser to generate training
examples for extraction. Using several heuris-
tic constraints, we automatically label a set
of parsed sentences as trustworthy or untrust-
worthy extractions (positive and negative ex-
amples, respectively). The classifier is trained
on these examples, using features such as the
part of speech tags on the words in the re-
lation. The classifier is then able to decide
whether a sequence of POS-tagged words is a
correct extraction with high accuracy.
3. Synonym Resolution
Because TextRunner has no pre-defined re-
lations, it may extract many different strings
representing the same relation. Also, as with
all information extraction systems, it can ex-
tract multiple names for the same object. The
Resolver system performs an unsupervised
clustering of TextRunner?s extractions to
create sets of synonymous entities and rela-
tions. Resolver uses a novel, unsupervised
probabilistic model to determine the probabil-
ity that any pair of strings is co-referential,
given the tuples that each string was extracted
with. (Yates and Etzioni, 2007)
4. Query Interface
TextRunner builds an inverted index of
the extracted tuples, and spreads it across a
cluster of machines. This architecture sup-
ports fast, interactive, and powerful relational
queries. Users may enter words in a relation or
entity, and TextRunner quickly returns the
entire set of extractions matching the query.
For example, a query for ?Newton? will return
tuples like (Newton, invented, calculus). Users
may opt to query for all tuples matching syn-
onyms of the keyword input, and may also opt
to merge all tuples returned by a query into
sets of tuples that are deemed synonymous.
4 Experimental Results
On our test corpus of 9 million Web doc-
uments, TextRunner extracted 7.8 million
well-formed tuples. On a randomly selected
subset of 400 tuples, 80.4% were deemed cor-
rect by human reviewers.
We performed a head-to-head compari-
son with a state-of-the-art traditional in-
formation extraction system, called Know-
ItAll. (Etzioni et al, 2005) On a set of ten
high-frequency relations, TextRunner found
nearly as many correct extractions as Know-
ItAll (11,631 to 11,476), while reducing the
error rate of KnowItAll by 33% (18% to
12%).
Acknowledgements
This research was supported in part by NSF
grants IIS-0535284 and IIS-0312988, DARPA
contract NBCHD030010, ONR grant N00014-
05-1-0185 as well as gifts from Google, and
carried out at the University of Washington?s
Turing Center.
References
M. Banko, M. J. Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007. Open In-
formation Extraction from the Web. In IJCAI.
A. Culotta, A. McCallum, and J. Betz. 2006. Inte-
grating Probabilistic Extraction Models and Re-
lational Data Mining to Discover Relations and
Patterns in Text. In HLT-NAACL.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,
A. Popescu, T. Shaked, S. Soderland, D. Weld,
and A. Yates. 2005. Unsupervised Named-
Entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91?
134.
Y. Shinyama and S. Sekine. 2006. Preemptive
Information Extraction Using Unrestricted Re-
lation Discovery. In HLT-NAACL.
A. Yates and O. Etzioni. 2007. Unsupervised Res-
olution of Objects and Relations on the Web. In
NAACL-HLT.
26
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Compiling a Massive, Multilingual Dictionary via Probabilistic Inference
Mausam Stephen Soderland Oren Etzioni
Daniel S. Weld Michael Skinner* Jeff Bilmes
University of Washington, Seattle *Google, Seattle
{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.com
Abstract
Can we automatically compose a large set
of Wiktionaries and translation dictionar-
ies to yield a massive, multilingual dic-
tionary whose coverage is substantially
greater than that of any of its constituent
dictionaries?
The composition of multiple translation
dictionaries leads to a transitive inference
problem: if word A translates to word
B which in turn translates to word C,
what is the probability that C is a trans-
lation of A? The paper introduces a
novel algorithm that solves this problem
for 10,000,000 words in more than 1,000
languages. The algorithm yields PANDIC-
TIONARY, a novel multilingual dictionary.
PANDICTIONARY contains more than four
times as many translations than in the
largest Wiktionary at precision 0.90 and
over 200,000,000 pairwise translations in
over 200,000 language pairs at precision
0.8.
1 Introduction and Motivation
In the era of globalization, inter-lingual com-
munication is becoming increasingly important.
Although nearly 7,000 languages are in use to-
day (Gordon, 2005), most language resources are
mono-lingual, or bi-lingual.1 This paper investi-
gates whether Wiktionaries and other translation
dictionaries available over the Web can be auto-
matically composed to yield a massive, multilin-
gual dictionary with superior coverage at compa-
rable precision.
We describe the automatic construction of a
massive multilingual translation dictionary, called
1The English Wiktionary, a lexical resource developed by
volunteers over the Internet is one notable exception that con-
tains translations of English words in about 500 languages.
Figure 1: A fragment of the translation graph for two senses
of the English word ?spring?. Edges labeled ?1? and ?3? are
for spring in the sense of a season, and ?2? and ?4? are for
the flexible coil sense. The graph shows translation entries
from an English dictionary merged with ones from a French
dictionary.
PANDICTIONARY, that could serve as a resource
for translation systems operating over a very
broad set of language pairs. The most immedi-
ate application of PANDICTIONARY is to lexical
translation?the translation of individual words or
simple phrases (e.g., ?sweet potato?). Because
lexical translation does not require aligned cor-
pora as input, it is feasible for a much broader
set of languages than statistical Machine Transla-
tion (SMT). Of course, lexical translation cannot
replace SMT, but it is useful for several applica-
tions including translating search-engine queries,
library classifications, meta-data tags,2 and recent
applications like cross-lingual image search (Et-
zioni et al, 2007), and enhancing multi-lingual
Wikipedias (Adar et al, 2009). Furthermore,
lexical translation is a valuable component in
knowledge-based Machine Translation systems,
e.g., (Bond et al, 2005; Carbonell et al, 2006).
PANDICTIONARY currently contains over 200
million pairwise translations in over 200,000 lan-
guage pairs at precision 0.8. It is constructed from
information harvested from 631 online dictionar-
ies and Wiktionaries. This necessitates match-
2Meta-data tags appear in community Web sites such as
flickr.com and del.icio.us.
262
ing word senses across multiple, independently-
authored dictionaries. Because of the millions of
translations in the dictionaries, a feasible solution
to this sense matching problem has to be scalable;
because sense matches are imperfect and uncer-
tain, the solution has to be probabilistic.
The core contribution of this paper is a princi-
pled method for probabilistic sense matching to in-
fer lexical translations between two languages that
do not share a translation dictionary. For exam-
ple, our algorithm can conclude that Basque word
?udaherri? is a translation of Maori word ?koanga?
in Figure 1. Our contributions are as follows:
1. We describe the design and construction of
PANDICTIONARY?a novel lexical resource
that spans over 200 million pairwise transla-
tions in over 200,000 language pairs at 0.8
precision, a four-fold increase when com-
pared to the union of its input translation dic-
tionaries.
2. We introduce SenseUniformPaths, a scal-
able probabilistic method, based on graph
sampling, for inferring lexical translations,
which finds 3.5 times more inferred transla-
tions at precison 0.9 than the previous best
method.
3. We experimentally contrast PANDIC-
TIONARY with the English Wiktionary and
show that PANDICTIONARY is from 4.5 to
24 times larger depending on the desired
precision.
The remainder of this paper is organized as fol-
lows. Section 2 describes our earlier work on
sense matching (Etzioni et al, 2007). Section 3
describes how the PANDICTIONARY builds on and
improves on their approach. Section 4 reports on
our experimental results. Section 5 considers re-
lated work on lexical translation. The paper con-
cludes in Section 6 with directions for future work.
2 Building a Translation Graph
In previous work (Etzioni et al, 2007) we intro-
duced an approach to sense matching that is based
on translation graphs (see Figure 1 for an exam-
ple). Each vertex v ? V in the graph is an or-
dered pair (w, l) where w is a word in a language
l. Undirected edges in the graph denote transla-
tions between words: an edge e ? E between (w1,
l1) and (w2, l2) represents the belief that w1 and
w2 share at least one word sense.
Construction: The Web hosts a large num-
ber of bilingual dictionaries in different languages
and several Wiktionaries. Bilingual dictionaries
translate words from one language to another, of-
ten without distinguishing the intended sense. For
example, an Indonesian-English dictionary gives
?light? as a translation of the Indonesian word ?en-
teng?, but does not indicate whether this means il-
lumination, light weight, light color, or the action
of lighting fire.
The Wiktionaries (wiktionary.org) are sense-
distinguished, multilingual dictionaries created by
volunteers collaborating over the Web. A transla-
tion graph is constructed by locating these dictio-
naries, parsing them into a common XML format,
and adding the nodes and edges to the graph.
Figure 1 shows a fragment of a translation
graph, which was constructed from two sets of
translations for the word ?spring? from an English
Wiktionary, and two corresponding entries from
a French Wiktionary for ?printemps? (spring sea-
son) and ?ressort? (flexible spring). Translations of
the season ?spring? have edges labeled with sense
ID=1, the flexible coil sense has ID=2, translations
of ?printemps? have ID=3, and so forth.3
For clarity, we show only a few of the actual
vertices and edges; e.g., the figure doesn?t show
the edge (ID=1) between ?udaherri? and ?primav-
era?.
Inference: In our previous system we had
a simple inference procedure over translation
graphs, called TRANSGRAPH, to find translations
beyond those provided by any source dictionary.
TRANSGRAPH searched for paths in the graph be-
tween two vertices and estimated the probability
that the path maintains the same word sense along
all edges in the path, even when the edges come
from different dictionaries. For example, there are
several paths between ?udaherri? and ?koanga? in
Figure 1, but all shift from sense ID 1 to 3. The
probability that the two words are translations is
equivalent to the probability that IDs 1 and 3 rep-
resent the same sense.
TRANSGRAPH used two formulae to estimate
these probabilities. One formula estimates the
probability that two multi-lingual dictionary en-
tries represent the same word sense, based on the
proportion of overlapping translations for the two
entries. For example, most of the translations of
3Sense-distinguished multi-lingual entries give rise to
cliques all of which share a common sense ID.
263
French ?printemps? are also translations of the sea-
son sense of ?spring?. A second formula is based
on triangles in the graph (useful for bilingual dic-
tionaries): a clique of 3 nodes with an edge be-
tween each pair of nodes. In such cases, there is
a high probability that all 3 nodes share a word
sense.
Critique: While TRANSGRAPH was the first
to present a scalable inference method for lexical
translation, it suffers from several drawbacks. Its
formulae operate only on local information: pairs
of senses that are adjacent in the graph or triangles.
It does not incorporate evidence from longer paths
when an explicit triangle is not present. Moreover,
the probabilities from different paths are com-
bined conservatively (either taking the max over
all paths, or using ?noisy or? on paths that are
completely disjoint, except end points), thus lead-
ing to suboptimal precision/recall.
In response to this critique, the next section
presents an inference algorithm, called SenseUni-
formPaths (SP), with substantially improved recall
at equivalent precision.
3 Translation Inference Algorithms
In essence, inference over a translation graph
amounts to transitive sense matching: if word A
translates to word B, which translates in turn to
word C, what is the probability that C is a trans-
lation of A? If B is polysemous then C may not
share a sense with A. For example, in Figure 2(a)
if A is the French word ?ressort? (the flexible-
coil sense of spring) and B is the English word
?spring?, then Slovenian word ?vzmet? may or may
not be a correct translation of ?ressort? depending
on whether the edge (B,C) denotes the flexible-
coil sense of spring, the season sense, or another
sense. Indeed, given only the knowledge of the
path A ? B ? C we cannot claim anything with
certainty regarding A to C.
However, if A, B, and C are on a circuit that
starts at A, passes through B and C and re-
turns to A, there is a high probability that all
nodes on that circuit share a common word sense,
given certain restrictions that we enumerate later.
Where TRANSGRAPH used evidence from circuits
of length 3, we extend this to paths of arbitrary
lengths.
To see how this works, let us begin with the sim-
plest circuit, a triangle of three nodes as shown in
Figure 2(b). We can be quite certain that ?vzmet?
shares the sense of coil with both ?spring? and
?ressort?. Our reasoning is as follows: even
though both ?ressort? and ?spring? are polysemous
they share only one sense. For a triangle to form
we have two choices ? (1) either ?vzmet? means
spring coil, or (2) ?vzmet? means both the spring
season and jurisdiction, but not spring coil. The
latter is possible but such a coincidence is very un-
likely, which is why a triangle is strong evidence
for the three words to share a sense.
As an example of longer paths, our inference
algorithms can conclude that in Figure 2(c), both
?molla? and ?vzmet? have the sense coil, even
though no explicit triangle is present. To show
this, let us define a translation circuit as follows:
Definition 1 A translation circuit from v?1 with
sense s? is a cycle that starts and ends at v?1 with
no repeated vertices (other than v?1 at end points).
Moreover, the path includes an edge between v?1
and another vertex v?2 that also has sense s
?.
All vertices on a translation circuit are mutual
translations with high probability, as in Figure
2(c). The edge from ?spring? indicates that ?vzmet?
means either coil or season, while the edge from
?ressort? indicates that ?molla? means either coil
or jurisdiction. The edge from ?vzmet? to ?molla?
indicates that they share a sense, which will hap-
pen if all nodes share the sense season or if either
?vzmet? has the unlikely combination of coil and
jurisdiction (or ?molla? has coil and season).
We also develop a mathematical model of
sense-assignment to words that lets us formally
prove these insights. For more details on the the-
ory please refer to our extended version. This pa-
per reports on our novel algorithm and experimen-
tal results.
These insights suggest a basic version of our al-
gorithm: ?given two vertices, v?1 and v
?
2 , that share
a sense (say s?) compute all translation circuits
from v?1 in the sense s
?; mark all vertices in the
circuits as translations of the sense s??.
To implement this algorithm we need to decide
whether a vertex lies on a translation circuit, which
is trickier than it seems. Notice that knowing
that v is connected independently to v?1 and v
?
2
doesn?t imply that there exists a translation circuit
through v, because both paths may go through a
common node, thus violating of the definition of
translation circuit. For example, in Figure 2(d) the
Catalan word ?ploma? has paths to both spring and
ressort, but there is no translation circuit through
264
spring
English
ressort
French
vzmet
Slovenian
spring
English
ressort
French
vzmet
Slovenian
spring
English
vzmet
Slovenian
ressort
French
molla
Italian
spring
English
ressort
French
ploma
Catalan
Feder
German
???? 
Russian
spring
English
ressort
French
fj?der
Swedish
penna
Italian
Feder
German
(a)                         (b)                                   (c)                                (d)                     (e)
season
coil
jurisdiction
coil
s* s*
s* s*
s*
? ? ?
? ?
feather
coil
?
?
Figure 2: Snippets of translation graphs illustrating various inference scenarios. The nodes in question mark represent the
nodes in focus for each illustration. For all cases we are trying to infer translations of the flexible coil sense of spring.
it. Hence, it will not be considered a transla-
tion. This example also illustrates potential errors
avoided by our algorithm ? here, German word
?Feder? mean feather and spring coil, but ?ploma?
means feather and not the coil.
An exhaustive search to find translation circuits
would be too slow, so we approximate the solution
by a random walk scheme. We start the random
walk from v?1 (or v
?
2) and choose random edges
without repeating any vertices in the current path.
At each step we check if the current node has an
edge to v?2 (or v
?
1). If it does, then all the ver-
tices in the current path form a translation circuit
and, thus, are valid translations. We repeat this
random walk many times and keep marking the
nodes. In our experiments for each inference task
we performed a total of 2,000 random walks (NR
in pseudo-code) of max circuit length 7. We chose
these parameters based on a development set of 50
inference tasks.
Our first experiments with this basic algorithm
resulted in a much higher recall than TRANS-
GRAPH, albeit, at a significantly lower precision.
A closer examination of the results revealed two
sources of error ? (1) errors in source dictionary
data, and (2) correlated sense shifts in translation
circuits. Below we add two new features to our
algorithm to deal with each of these error sources,
respectively.
3.1 Errors in Source Dictionaries
In practice, source dictionaries contain mistakes
and errors occur in processing the dictionaries to
create the translation graph. Thus, existence of a
single translation circuit is only limited evidence
for a vertex as a translation. We wish to exploit
the insight that more translation circuits constitute
stronger evidence. However, the different circuits
may share some edges, and thus the evidence can-
not be simply the number of translation circuits.
We model the errors in dictionaries by assigning
a probability less than 1.0 to each edge4 (pe in the
4In our experiments we used a flat value of 0.6, chosen by
pseudo-code). We assume that the probability of
an edge being erroneous is independent of the rest
of the graph. Thus, a translation graph with pos-
sible data errors converts into a distribution over
accurate translation graphs.
Under this distribution, we can use the proba-
bility of existence of a translation circuit through a
vertex as the probability that the vertex is a trans-
lation. This value captures our insights, since a
larger number of translation circuits gives a higher
probability value.
We sample different graph topologies from our
given distribution. Some translation circuits will
exist in some of the sampled graphs, but not in
others. This, in turn, means that a given vertex v
will only be on a circuit for a fraction of the sam-
pled graphs. We take the proportion of samples in
which v is on a circuit to be the probability that v
is in the translation set. We refer to this algorithm
as Unpruned SenseUniformPaths (uSP).
3.2 Avoiding Correlated Sense-shifts
The second source of errors are circuits that in-
clude a pair of nodes sharing the same polysemy,
i.e., having the same pair of senses. A circuit
might maintain sense s? until it reaches a node that
has both s? and a distinct si. The next edge may
lead to a node with si, but not s?, causing an ex-
traction error. The path later shifts back to sense
s? at a second node that also has s? and si. An ex-
ample for this is illustrated in Figure 2(e), where
both the German and Swedish words mean feather
and spring coil. Here, Italian ?penna? means only
the feather and not the coil.
Two nodes that share the same two senses oc-
cur frequently in practice. For example, many
languages use the same word for ?heart? (the or-
gan) and center; similarly, it is common for lan-
guages to use the same word for ?silver?, the metal
and the color. These correlations stem from com-
parameter tuning on a development set of 50 inference tasks.
In future we can use different values for different dictionaries
based on our confidence in their accuracy.
265
Figure 3: The set {B, C} has a shared ambiguity - each
node has both sense 1 (from the lower clique) and sense 2
(from the upper clique). A circuit that contains two nodes
from the same ambiguity set with an intervening node not in
that set is likely to create translation errors.
mon metaphor and the shared evolutionary roots
of some languages.
We are able to avoid circuits with this type of
correlated sense-shift by automatically identifying
ambiguity sets, sets of nodes known to share mul-
tiple senses. For instance, in Figure 2(e) ?Feder?
and ?fj?der? form an ambiguity set (shown within
dashed lines), as they both mean feather and coil.
Definition 2 An ambiguity set A is a set of ver-
tices that all share the same two senses. I.e.,
?s1, s2, with s1 6= s2 s.t. ?v ? A, sense(v, s1)?
sense(v, s2), where sense(v, s) denotes that v has
sense s.
To increase the precision of our algorithm we
prune the circuits that contain two nodes in the
same ambiguity set and also have one or more in-
tervening nodes that are not in the ambiguity set.
There is a strong likelihood that the intervening
nodes will represent a translation error.
Ambiguity sets can be detected from the graph
topology as follows. Each clique in the graph rep-
resents a set of vertices that share a common word
sense. When two cliques intersect in two or more
vertices, the intersecting vertices share the word
sense of both cliques. This may either mean that
both cliques represent the same word sense, or that
the intersecting vertices form an ambiguity set. A
large overlap between two cliques makes the for-
mer case more likely; a small overlap makes it
more likely that we have found an ambiguity set.
Figure 3 illustrates one such computation.
All nodes of the clique V1, V2, A,B,C,D share
a word sense, and all nodes of the clique
B,C,E, F,G,H also share a word sense. The set
{B,C} has nodes that have both senses, forming
an ambiguity set. We denote the set of ambiguity
sets by A in the pseudo-code.
Having identified these ambiguity sets, we mod-
ify our random walk scheme by keeping track of
whether we are entering or leaving an ambiguity
set. We prune away all paths that enter the same
ambiguity set twice. We name the resulting algo-
rithm SenseUniformPaths (SP), summarized at a
high level in Algorithm 1.
Comparing Inference Algorithms Our evalua-
tion demonstrated that SP outperforms uSP. Both
these algorithms have significantly higher recall
than TRANSGRAPH algorithm. The detailed re-
sults are presented in Section 4.2. We choose SP
as our inference algorithm for all further research,
in particular to create PANDICTIONARY.
3.3 Compiling PanDictionary
Our goal is to automatically compile PANDIC-
TIONARY, a sense-distinguished lexical transla-
tion resource, where each entry is a distinct word
sense. Associated with each word sense is a list of
translations in multiple languages.
We use Wiktionary senses as the base senses
for PANDICTIONARY. Recall that SP requires two
nodes (v?1 and v
?
2) for inference. We use the Wik-
tionary source word as v?1 and automatically pick
the second word from the set of Wiktionary trans-
lations of that sense by choosing a word that is
well connected, and, which does not appear in
other senses of v?1 (i.e., is expected to share only
one sense with v?1).
We first run SenseUniformPaths to expand the
approximately 50,000 senses in the English Wik-
tionary. We further expand any senses from the
other Wiktionaries that are not yet covered by
PANDICTIONARY, and add these to PANDIC-
TIONARY. This results in the creation of the
world?s largest multilingual, sense-distinguished
translation resource, PANDICTIONARY. It con-
tains a little over 80,000 senses. Its construction
takes about three weeks on a 3.4 GHz processor
with a 2 GB memory.
Algorithm 1 S.P.(G, v?1, v
?
2,A)
1: parameters NG: no. of graph samples, NR: no. of ran-
dom walks, pe: prob. of sampling an edge
2: createNG versions ofG by sampling each edge indepen-
dently with probability pe
3: for all i = 1..NG do
4: for all vertices v : rp[v][i] = 0
5: perform NR random walks starting at v?1 (or v
?
2 ) and
pruning any walk that enters (or exits) an ambiguity
set in A twice. All walks that connect to v?2 (or v
?
1 )
form a translation circuit.
6: for all vertices v do
7: if(v is on a translation circuit) rp[v][i] = 1
8: return
?
i
rp[v][i]
NG
as the prob. that v is a translation
266
4 Empirical Evaluation
In our experiments we investigate three key ques-
tions: (1) which of the three algorithms (TG, uSP
and SP) is superior for translation inference (Sec-
tion 4.2)? (2) how does the coverage of PANDIC-
TIONARY compare with the largest existing mul-
tilingual dictionary, the English Wiktionary (Sec-
tion 4.3)? (3) what is the benefit of inference over
the mere aggregation of 631 dictionaries (Section
4.4)? Additionally, we evaluate the inference algo-
rithm on two other dimensions ? variation with the
degree of polysemy of source word, and variation
with original size of the seed translation set.
4.1 Experimental Methodology
Ideally, we would like to evaluate a random sam-
ple of the more than 1,000 languages represented
in PANDICTIONARY.5 However, a high-quality
evaluation of translation between two languages
requires a person who is fluent in both languages.
Such people are hard to find and may not even
exist for many language pairs (e.g., Basque and
Maori). Thus, our evaluation was guided by our
ability to recruit volunteer evaluators. Since we
are based in an English speaking country we were
able to recruit local volunteers who are fluent in
a range of languages and language families, and
who are also bilingual in English.6
The experiments in Sections 4.2 and 4.3 test
whether translations in a PANDICTIONARY have
accurate word senses. We provided our evalua-
tors with a random sample of translations into their
native language. For each translation we showed
the English source word and gloss of the intended
sense. For example, a Dutch evaluator was shown
the sense ?free (not imprisoned)? together with the
Dutch word ?loslopende?. The instructions were
to mark a word as correct if it could be used to ex-
press the intended sense in a sentence in their na-
tive language. For experiments in Section 4.4 we
tested precision of pairwise translations, by having
informants in several pairs of languages discuss
whether the words in their respective languages
can be used for the same sense.
We use the tags of correct or incorrect to com-
pute the precision: the percentage of correct trans-
5The distribution of words in PANDICTIONARY is highly
non-uniform ranging from 182,988 words in English to 6,154
words in Luxembourgish and 189 words in Tuvalu.
6The languages used was based on the availability of na-
tive speakers. This varied between the different experiments,
which were conducted at different times.
Figure 4: The SenseUniformPaths algorithm (SP) more
than doubles the number of correct translations at precision
0.95, compared to a baseline of translations that can be found
without inference.
lations divided by correct plus incorrect transla-
tions. We then order the translations by probabil-
ity and compute the precision at various probabil-
ity thresholds.
4.2 Comparing Inference Algorithms
Our first evaluation compares our SenseUniform-
Paths (SP) algorithm (before and after pruning)
with TRANSGRAPH on both precision and num-
ber of translations.
To carry out this comparison, we randomly sam-
pled 1,000 senses from English Wiktionary and
ran the three algorithms over them. We evalu-
ated the results on 7 languages ? Chinese, Danish,
German, Hindi, Japanese, Russian, and Turkish.
Each informant tagged 60 random translations in-
ferred by each algorithm, which resulted in 360-
400 tags per algorithm7. The precision over these
was taken as a surrogate for the precision across
all the senses.
We compare the number of translations for each
algorithm at comparable precisions. The baseline
is the set of translations (for these 1000 senses)
found in the source dictionaries without inference,
which has a precision 0.95 (as evaluated by our
informants).8
Our results are shown in Figure 4. At this high
precision, SP more than doubles the number of
baseline translations, finding 5 times as many in-
ferred translations (in black) as TG.
Indeed, both uSP and SP massively outperform
TG. SP is consistently better than uSP, since it
performs better for polysemous words, due to its
pruning based on ambiguity sets. We conclude
7Some translations were marked as ?Don?t know?.
8Our informants tended to underestimate precision, often
marking correct translations in minor senses of a word as in-
correct.
267
0.5
0.6
0.7
0.8
0.9
1
0.0 4.0 8.0 12.0 16.0
Pr
ec
is
io
n
Translations in Millions
PanDictionary
English Wiktionary
Figure 5: Precision vs. coverage curve for PANDIC-
TIONARY. It quadruples the size of the English Wiktionary at
precision 0.90, is more than 8 times larger at precision 0.85
and is almost 24 times the size at precision 0.7.
that SP is the best inference algorithm and employ
it for PANDICTIONARY construction.
4.3 Comparison with English Wiktionary
We now compare the coverage of PANDIC-
TIONARY with the English Wiktionary at varying
levels of precision. The English Wiktionary is the
largest Wiktionary with a total of 403,413 transla-
tions. It is also more reliable than some other Wik-
tionaries in making word sense distinctions. In this
study we use only the subset of PANDICTIONARY
that was computed starting from the English Wik-
tionary senses. Thus, this subsection under-reports
PANDICTIONARY?s coverage.
To evaluate a huge resource such as PANDIC-
TIONARY we recruited native speakers of 14 lan-
guages ? Arabic, Bulgarian, Danish, Dutch, Ger-
man, Hebrew, Hindi, Indonesian, Japanese, Ko-
rean, Spanish, Turkish, Urdu, and Vietnamese. We
randomly sampled 200 translations per language,
which resulted in about 2,500 tags. Figure 5
shows the total number of translations in PANDIC-
TIONARY in senses from the English Wiktionary.
At precision 0.90, PANDICTIONARY has 1.8 mil-
lion translations, 4.5 times as many as the English
Wiktionary.
We also compare the coverage of PANDIC-
TIONARY with that of the English Wiktionary in
terms of languages covered. Table 1 reports, for
each resource, the number of languages that have
a minimum number of distinct words in the re-
source. PANDICTIONARY has 1.4 times as many
languages with at least 1,000 translations at pre-
cision 0.90 and more than twice at precision 0.7.
These observations reaffirm our faith in the pan-
lingual nature of the resource.
PANDICTIONARY?s ability to expand the lists
of translations provided by the EnglishWiktionary
is most pronounced for senses with a small num-
0.75
0.8
0.85
0.9
0.95
1 2 3,4 >4
Pre
cis
ion
Avg precision 0.90
Avg precision 0.85
Polysemy of the English source word
3-4
Figure 6: Variation of precision with the degree of poly-
semy of the source English word. The precision decreases as
polysemy increases, still maintaining reasonably high values.
ber of translations. For example, at precision 0.90,
senses that originally had 3 to 6 translations are in-
creased 5.3 times in size. The increase is 2.2 times
when the original sense size is greater than 20.
For closer analysis we divided the English
source words (v?1) into different bins based on the
number of senses that English Wiktionary lists for
them. Figure 6 plots the variation of precision with
this degree of polysemy. We find that translation
quality decreases as degree of polysemy increases,
but this decline is gradual, which suggests that SP
algorithm is able to hold its ground well in difficult
inference tasks.
4.4 Comparison with All Source Dictionaries
We have shown that PANDICTIONARY has much
broader coverage than the English Wiktionary, but
how much of this increase is due to the inference
algorithm versus the mere aggregation of hundreds
of translation dictionaries in PANDICTIONARY?
Since most bilingual dictionaries are not sense-
distinguished, we ignore the word senses and
count the number of distinct (word1, word2) trans-
lation pairs.
We evaluated the precision of word-word trans-
lations by a collaborative tagging scheme, with
two native speakers of different languages, who
are both bi-lingual in English. For each sug-
gested translation they discussed the various
senses of words in their respective languages
and tag a translation correct if they found some
sense that is shared by both words. For this
study we tagged 7 language pairs: Hindi-Hebrew,
# languages with distinct words
? 1000 ? 100 ? 1
English Wiktionary 49 107 505
PanDictionary (0.90) 67 146 608
PanDictionary (0.85) 75 175 794
PanDictionary (0.70) 107 607 1066
Table 1: PANDICTIONARY covers substantially more lan-
guages than the English Wiktionary.
268
050
100
150
200
250
EW 631D PD(0.9) PD(0.85) PD(0.8)
Inferred transl. Direct transl.
Tra
nsl
ati
on
s(i
n m
illio
ns)
Figure 7: The number of distinct word-word translation
pairs from PANDICTIONARY is several times higher than the
number of translation pairs in the English Wiktionary (EW)
or in all 631 source dictionaries combined (631 D). A major-
ity of PANDICTIONARY translations are inferred by combin-
ing entries from multiple dictionaries.
Japanese-Russian, Chinese-Turkish, Japanese-
German, Chinese-Russian, Bengali-German, and
Hindi-Turkish.
Figure 7 compares the number of word-word
translation pairs in the English Wiktionary (EW),
in all 631 source dictionaries (631 D), and in PAN-
DICTIONARY at precisions 0.90, 0.85, and 0.80.
PANDICTIONARY increases the number of word-
word translations by 73% over the source dictio-
nary translations at precision 0.90 and increases it
by 2.7 times at precision 0.85. PANDICTIONARY
also adds value by identifying the word sense of
the translation, which is not given in most of the
source dictionaries.
5 Related Work
Because we are considering a relatively new prob-
lem (automatically building a panlingual transla-
tion resource) there is little work that is directly re-
lated to our own. The closest research is our previ-
ous work on TRANSGRAPH algorithm (Etzioni et
al., 2007). Our current algorithm outperforms the
previous state of the art by 3.5 times at precision
0.9 (see Figure 4). Moreover, we compile this in a
dictionary format, thus considerably reducing the
response time compared to TRANSGRAPH, which
performed inference at query time.
There has been considerable research on meth-
ods to acquire translation lexicons from either
MRDs (Neff and McCord, 1990; Helmreich et
al., 1993; Copestake et al, 1994) or from par-
allel text (Gale and Church, 1991; Fung, 1995;
Melamed, 1997; Franz et al, 2001), but this has
generally been limited to a small number of lan-
guages. Manually engineered dictionaries such as
EuroWordNet (Vossen, 1998) are also limited to
a relatively small set of languages. There is some
recent work on compiling dictionaries from mono-
lingual corpora, which may scale to several lan-
guage pairs in future (Haghighi et al, 2008).
Little work has been done in combining mul-
tiple dictionaries in a way that maintains word
senses across dictionaries. Gollins and Sanderson
(2001) explored using triangulation between alter-
nate pivot languages in cross-lingual information
retrieval. Their triangulation essentially mixes
together circuits for all word senses, hence, is un-
able to achieve high precision.
Dyvik?s ?semantic mirrors? uses translation
paths to tease apart distinct word senses from
inputs that are not sense-distinguished (Dyvik,
2004). However, its expensive processing and
reliance on parallel corpora would not scale to
large numbers of languages. Earlier (Knight and
Luk, 1994) discovered senses of Spanish words by
matching several English translations to a Word-
Net synset. This approach applies only to specific
kinds of bilingual dictionaries, and also requires a
taxonomy of synsets in the target language.
Random walks, graph sampling and Monte
Carlo simulations are popular in literature, though,
to our knowledge, none have applied these to our
specific problems (Henzinger et al, 1999; Andrieu
et al, 2003; Karger, 1999).
6 Conclusions
We have described the automatic construction of
a unique multilingual translation resource, called
PANDICTIONARY, by performing probabilistic in-
ference over the translation graph. Overall, the
construction process consists of large scale in-
formation extraction over the Web (parsing dic-
tionaries), combining it into a single resource (a
translation graph), and then performing automated
reasoning over the graph (SenseUniformPaths) to
yield a much more extensive and useful knowl-
edge base.
We have shown that PANDICTIONARY has
more coverage than any other existing bilingual
or multilingual dictionary. Even at the high preci-
sion of 0.90, PANDICTIONARY more than quadru-
ples the size of the English Wiktionary, the largest
available multilingual resource today.
We plan to make PANDICTIONARY available
to the research community, and also to the Wik-
tionary community in an effort to bolster their ef-
forts. PANDICTIONARY entries can suggest new
translations for volunteers to add to Wiktionary
entries, particularly if combined with an intelli-
gent editing tool (e.g., (Hoffmann et al, 2009)).
269
Acknowledgments
This research was supported by a gift from the
Utilika Foundation to the Turing Center at Uni-
versity of Washington. We acknowledge Paul
Beame, Nilesh Dalvi, Pedro Domingos, Rohit
Khandekar, Daniel Lowd, Parag, Jonathan Pool,
Hoifung Poon, Vibhor Rastogi, Gyanit Singh for
fruitful discussions and insightful comments on
the research. We thank the language experts who
donated their time and language expertise to eval-
uate our systems. We also thank the anynomous
reviewers of the previous drafts of this paper for
their valuable suggestions in improving the evalu-
ation and presentation.
References
E. Adar, M. Skinner, and D. Weld. 2009. Information
arbitrage in multi-lingual Wikipedia. In Procs. of
Web Search and Data Mining(WSDM 2009).
C. Andrieu, N. De Freitas, A. Doucet, and M. Jor-
dan. 2003. An Introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
F. Bond, S. Oepen, M. Siegel, A. Copestake, and
D D. Flickinger. 2005. Open source machine trans-
lation with DELPH-IN. In Open-Source Machine
Translation Workshop at MT Summit X.
J. Carbonell, S. Klein, D. Miller, M. Steinbaum,
T. Grassiany, and J. Frey. 2006. Context-based ma-
chine translation. In AMTA.
A. Copestake, T. Briscoe, P. Vossen, A. Ageno,
I. Castellon, F. Ribas, G. Rigau, H. Rodriquez, and
A. Samiotou. 1994. Acquisition of lexical trans-
lation relations from MRDs. Machine Translation,
3(3?4):183?219.
H. Dyvik. 2004. Translation as semantic mirrors: from
parallel corpus to WordNet. Language and Comput-
ers, 49(1):311?326.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007. Lexical translation with application to image
search on the Web. In Machine Translation Summit
XI.
M. Franz, S. McCarly, and W. Zhu. 2001. English-
Chinese information retrieval at IBM. In Proceed-
ings of TREC 2001.
P. Fung. 1995. A pattern matching method for finding
noun and proper noun translations from noisy paral-
lel corpora. In Proceedings of ACL-1995.
W. Gale and K.W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-1991.
T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
SIGIR.
Raymond G. Gordon, Jr., editor. 2005. Ethnologue:
Languages of the World (Fifteenth Edition). SIL In-
ternational.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL.
S. Helmreich, L. Guthrie, and Y. Wilks. 1993. The
use of machine readable dictionaries in the Pangloss
project. In AAAI Spring Symposium on Building
Lexicons for Machine Translation.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 1999. Measuring index
quality using random walks on the web. In WWW.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-
rty, and D. S. Weld. 2009. Amplifying commu-
nity content creation with mixed-initiative informa-
tion extraction. In ACM SIGCHI (CHI2009).
D. R. Karger. 1999. A randomized fully polynomial
approximation scheme for the all-terminal network
reliability problem. SIAM Journal of Computation,
29(2):492?514.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In AAAI.
I.D. Melamed. 1997. A Word-to-Word Model of
Translational Equivalence. In Proceedings of ACL-
1997 and EACL-1997, pages 490?497.
M. Neff and M. McCord. 1990. Acquiring lexical data
from machine-readable dictionary resources for ma-
chine translation. In 3rd Intl Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion of Natural Language.
P. Vossen, editor. 1998. EuroWordNet: A multilingual
database with lexical semantic networds. Kluwer
Academic Publishers.
270
Expanding the Recall of Relation Extraction by Bootstrapping
Junji Tomita
NTT Cyber Solutions Laboratories,
NTT Corporation
1-1 Hikarinooka Yokosuka-Shi,
Kanagawa 239-0847, Japan
tomita.junji@lab.ntt.co.jp
Stephen Soderland Oren Etzioni
Department of Computer Science
& Engineering
University of Washington
Seattle, WA 98195-2350
 soderlan,etzioni
@cs.washington.edu
Abstract
Most works on relation extraction assume
considerable human effort for making an
annotated corpus or for knowledge engi-
neering. Generic patterns employed in
KnowItAll achieve unsupervised, high-
precision extraction, but often result in low
recall. This paper compares two boot-
strapping methods to expand recall that
start with automatically extracted seeds
by KnowItAll. The first method is string
pattern learning, which learns string con-
texts adjacent to a seed tuple. The second
method learns less restrictive patterns that
include bags of words and relation-specific
named entity tags. Both methods improve
the recall of the generic pattern method. In
particular, the less restrictive pattern learn-
ing method can achieve a 250% increase
in recall at 0.87 precision, compared to the
generic pattern method.
1 Introduction
Relation extraction is a task to extract tu-
ples of entities that satisfy a given relation
from textual documents. Examples of rela-
tions include CeoOf(Company, Ceo) and Acquisi-
tion(Organization, Organization). There has been
much work on relation extraction; most of it em-
ploys knowledge engineering or supervised ma-
chine learning approaches (Feldman et al, 2002;
Zhao and Grishman, 2005). Both approaches are
labor intensive.
We begin with a baseline information extraction
system, KnowItAll (Etzioni et al, 2005), that does
unsupervised information extraction at Web scale.
KnowItAll uses a set of generic extraction pat-
terns, and automatically instantiates rules by com-
bining these patterns with user supplied relation
labels. For example, KnowItAll has patterns for a
generic ?of? relation:
NP1 ?s  relation , NP2
NP2 ,  relation of NP1
where NP1 and NP2 are simple noun phrases that
extract values of argument1 and argument2 of a
relation, and  relation is a user-supplied string
associated with the relation. The rules may also
constrain NP1 and NP2 to be proper nouns.
If a user supplies the relation labels ?ceo?
and ?chief executive officer? for the relation
CeoOf(Company, Ceo), KnowItAll inserts these
labels into the generic patterns shown above, to
create 4 extraction rules:
NP1 ?s ceo , NP2
NP1 ?s chief executive officer , NP2
NP2 , ceo of NP1
NP2 , chief executive officer of NP1
The same generic patterns with different la-
bels can also produce extraction rules for a May-
orOf relation or an InventorOf relation. These
rules have alternating context strings (exact string
match) and extraction slots (typically an NP or
head of an NP). This can produce rules with high
precision, but low recall, due to the wide variety
of contexts describing a relation. This paper looks
at ways to enhance recall over this baseline system
while maintaining high precision.
To enhance recall, we employ bootstrapping
techniques which start with seed tuples, i.e. the
most frequently extracted tuples by the baseline
system. The first method represents rules with
three context strings of tokens immediately adja-
cent to the extracted arguments: a left context,
56
middle context, and right context. These are in-
duced from context strings found adjacent to seed
tuples.
The second method uses a less restrictive pat-
tern representation such as bag of words, similar
to that of SnowBall(Agichtein, 2005). SnowBall is
a semi-supervised relation extraction system. The
input of Snowball is a few hand labeled correct
seed tuples for a relation (e.g. <Microsoft, Steve
Ballmer> for CeoOf relation). SnowBall clusters
the bag of words representations generated from
the context strings adjacent to each seed tuple, and
generates rules from them. It calculates the confi-
dence of candidate tuples and the rules iteratively
by using an EM-algorithm. Because it can extract
any tuple whose entities co-occur within a win-
dow, the recall can be higher than the string pat-
tern learning method. The main disadvantage of
SnowBall or a method which employs less restric-
tive patterns is that it requires Named Entity Rec-
ognizer (NER).
We introduce Relation-dependent NER (Rela-
tion NER), which trains an off-the-shelf super-
vised NER based on CRF(Lafferty et al, 2001)
with bootstrapping. This learns relation-specific
NE tags, and we present a method to use these tags
for relation extraction.
This paper compares the following two boot-
strapping strategies.
SPL: a simple string pattern learning method. It
learns string patterns adjacent to a seed tuple.
LRPL: a less restrictive pattern learning method.
It learns a variety of bag of words patterns,
after training a Relation NER.
Both methods are completely self-supervised ex-
tensions to the unsupervised KnowItAll. A user
supplies KnowItAll with one or more relation la-
bels to be applied to one or more generic extrac-
tion patterns. No further tagging or manual selec-
tion of seeds is required. Each of the bootstrapping
methods uses seeds that are automatically selected
from the output of the baseline KnowItAll system.
The results show that both bootstrapping meth-
ods improve the recall of the baseline system. The
two methods have comparable results, with LRPL
outperforms SPL for some relations and SPL out-
performs LRPL for other relations.
The rest of the paper is organized as follows.
Section 2 and 3 describe SPL and LRPL respec-
tively. Section 4 reports on our experiments, and
section 5 and 6 describe related works and conclu-
sions.
2 String Pattern Learning (SPL)
Both SPL and LRPL start with seed tuples that
were extracted by the baseline KnowItAll system,
with extraction frequency at or above a threshold
(set to 2 in these experiments). In these experi-
ments, we downloaded a set of sentences from the
Web that contained an occurrence of at least one
relation label and used this as our reservoir of un-
labeled training and test sentences. We created a
set of positive training sentences from those sen-
tences that contained both argument values of a
seed tuple.
SPL employs a method similar to that of
(Downey et al, 2004). It generates candidate ex-
traction rules with a prefix context, a middle con-
text, and a right context. The prefix is zero to 
 
tokens immediately to the left of extracted argu-
ment1, the middle context is all tokens between
argument1 and argument2, and the right context of
zero to 
 
tokens immediately to the right of ar-
gument2. It discards patterns with more than 

intervening tokens or without a relation label.
SPL tabulates the occurrence of such patterns
in the set of positive training sentences (all sen-
tences from the reservoir that contain both argu-
ment values from a seed tuple in either order), and
also tabulates their occurrence in negative training
sentences. The negative training are sentences that
have one argument value from a seed tuple and a
nearest simple NP in place of the other argument
value. This idea is based on that of (Ravichan-
dran and Hovy, 2002) for a QA system. SPL
learns a possibly large set of strict extraction rules
that have alternating context strings and extraction
slots, with no gaps or wildcards in the rules.
SPL selects the best patterns as follows:
1. Groups the context strings that have the exact
same middle string.
2. Selects the best pattern having the largest pat-
tern score, , for each group of context
strings having the same middle string.
  
 
 
    
 
 
     
	

    
(1)
3. Selects the patterns having  greater than

	
.
57
Figure 1: The architecture of LRPL (Less Restric-
tive Pattern Learning).
where 
 
  is a set of sentences that match
pattern  and include both argument values of a
seed tuple. 
	

  is a set of sentences that
match  and include just one argument value of
a seed tuple (e.g. just a company or a person for
CeoOf).  is a constant for smoothing.
3 Less Restrictive Pattern Learning
(LRPL)
LRPL uses a more flexible rule representation than
SPL. As before, the rules are based on a window of
tokens to the left of the first argument, a window
of middle tokens, and a window of tokens to the
right of the second argument. Rather than using
exact string match on a simple sequence of tokens,
LRPL uses a combination of bag of words and im-
mediately adjacent token. The left context is based
on a window of 
 
tokens immediately to the
left of argument1. It has two sets of tokens: the
token immediately to the left and a bag of words
for the remaining tokens. Each of these sets may
have zero or more tokens. The middle and right
contexts are similarly defined. We call this repre-
sentation extended bag of words.
Here is an example of how LRPL represents
the context of a training sentence with win-
dow size set to 4. ?Yesterday ,  Arg2Steve
Ballmer /Arg2, the Chief Executive Officer of
 Arg1Microsoft /Arg1 said that he is ...?.
order: arg2_arg1
values: Steve Ballmer, Microsoft
L: {yesterday} {,}
M: {,} {chief executive officer the} {of}
R: {said} {he is that}
Some of the tokens in these bags of words may
be dropped in merging this with patterns from
other training sentences. Each rule also has a con-
fidence score, learned from EM-estimation.
We experimented with simply using three bags
of words as in SnowBall, but found that precision
was increased when we distinguished the tokens
immediately adjacent to argument values from the
other tokens in the left, middle, and right bag of
words.
Less restrictive patterns require a Named Entity
Recognizer (NER), because the patterns can not
extract candidate entities by themselves1. LRPL
trains a supervised NER in bootstrapping for ex-
tracting candidate entities.
Figure 1 overviews LRPL. It consists of two
bootstrapping modules: Relation NER and Rela-
tion Assessor. LRPL trains the Relational NER
from seed tuples provided by the baseline Know-
ItAll system and unlabeled sentences in the reser-
voir. Then it does NE tagging on the sentences to
learn the less restrictive rules and to extract can-
didate tuples. The learning and extraction steps at
Relation Assessor are similar to that of SnowBall;
it generates a set of rules and uses EM-estimation
to compute a confidence in each rule. When these
rules are applied, the system computes a probabil-
ity for each tuple based on the rule confidence, the
degree of match between a sentence and the rule,
and the extraction frequency.
3.1 Relation dependent Named Entity
Recognizer
Relation NER leverages an off-the-shelf super-
vised NER, based on Conditional Random Fields
(CRF). In Figure 1, TrainSentenceGenerator auto-
matically generates training sentences from seeds
and unlabeled sentences in the reservoir. TrainEn-
tityRecognizer trains a CRF on the training sen-
tences and then EntityRecognizer applies the
trained CRF to all the unlabeled sentences, creat-
ing entity annotated sentences.
It can extract entities whose type matches an ar-
gument type of a particular relation. The type is
not explicitly specified by a user, but is automati-
cally determined according to the seed tuples. For
example, it can extract ?City? and ?Mayor? type en-
tities for MayorOf(City, Mayor) relation. We de-
scribe CRF in brief, and then how to train it in
bootstrapping.
1Although using all noun phrases in a sentence may be
possible, it apparently results in low precision.
58
3.1.1 Supervised Named Entity Recognizer
Several state-of-the-art supervised NERs are
based on a feature-rich probabilistic conditional
classifier such as Conditional Random Fields
(CRF) for sequential learning tasks(Lafferty et al,
2001; Rosenfeld et al, 2005). The input of CRF is
a feature sequence  of features 	

, and outputs a
tag sequence 
 of tags 

. In the training phrase, a
set of  

 


 is provided, and outputs a model


. In the applying phase, given  , it outputs a
tag sequence 
 by using 

. In the case of NE
tagging, given a sequence of tokens, it automat-
ically generates a sequence of feature sets; each
set is corresponding to a token. It can incorporate
any properties that can be represented as a binary
feature into the model, such as words, capitalized
patterns, part-of-speech tags and the existence of
the word in a dictionary. It works quite well on
NE tagging tasks (McCallum and Li, 2003).
3.1.2 How to Train Supervised NER in
Bootstrapping
We use bootstrapping to train CRF for relation-
specific NE tagging as follows: 1) select the sen-
tences that include all the entity values of a seed
tuple, 2) automatically mark the argument values
in each sentence, and 3)train CRF on the seed
marked sentences. An example of a seed marked
sentence is the following:
seed tuple: <Microsoft, Steve Ballmer>
seed marked sentence:
"Yesterday, <Arg2>Steve Ballmer</Arg2>,
CEO of <Arg1>Microsoft</Arg1>
announced that ..."
Because of redundancy, we can expect to gen-
erate a fairly large number of seed marked sen-
tences by using a few highly frequent seed tuples.
To avoid overfitting on terms from these seed tu-
ples, we substitute the actual argument values with
random characters for each training sentence, pre-
serving capitalization patterns and number of char-
acters in each token.
3.2 Relation Assessor
Relation Assessor employs several SnowBall-like
techniques including making rules by clustering
and EM-estimation for the confidence of the rules
and tuples.
In Figure 1, ContextRepresentationGenerator
generates extended bag of words contexts, from
entity annotated sentences, and classifies the con-
texts into two classes: training contexts 
	
(if
their entity values and their orders match a seed
tuple) and test contexts 
 
(otherwise). Train-
ConfidenceEstimator clusters 
	
based on the
match score between contexts, and generates a
rule from each cluster, that has average vectors
over contexts belonging to the cluster. Given a set
of generated rules and test contexts 
 
, Confi-
denceEstimator estimates each tuple confidence in

 
by using an EM algorithm. It also estimates
the confidence of the tuples extracted by the base-
line system, and outputs the merged result tuples
with confidence.
We describe the match score calculation
method, the EM-algorithm, and the merging
method in the following sub sections.
3.2.1 Match Score Calculation
The match score (or similarity)  of two ex-
tended bag of words contexts 

, 

is calculated
as the linear combination of the cosine values be-
tween the corresponding vectors.
 

 

 
 
 

 
 
 
 
 
 (2)
where,  is the index of left, middle, or right con-
texts.  is the index of left adjacent, right adjacent,
or other tokens. 
 
is the weight corresponding
to the context vector indexed by  and .
To achieve high precision, Relation Assessor
uses only the entity annotated sentences that have
just one entity for each argument (two entities
in total) and where those entities co-occur within


tokens window, and it uses at most 
 
left
and right tokens. It discards patterns without a re-
lation label.
3.2.2 EM-estimation for tuple and rule
confidence
Several rules generated from only positive ev-
idence result in low precision (e.g. rule ?of? for
MayorOf relation generated from ?Rudolph Giu-
liani of New York?). This problem can be im-
proved by estimating the rule confidence by the
following EM-algorithm.
1. For each 

in 
 
, identifies the best match
rule   

, based on the match score be-
tween 

and each rule . 

is the th con-
text that includes tuple 

.
 
 
 
 
  argmax

   
 
 (3)
59
2. Initializes seed tuple confidence, 
 
 
  
for all 
 
, where 
 
is a seed tuple.
3. Calculates tuple confidence, 
 , and rule
confidence,  , by using EM-algorithm. E
and M stages are iterated several times.
E stage:
  

 


 

  

  
	
  

  
(4)
M stage:
 
 
  (5)
  


    
 
 
 
  
 
 
 
 
 
(6)
where
 

  

 

 

   

  
 
 

 
  
 is a constant for smoothing.
This algorithm assigns a high confidence to the
rules that frequently co-occur with only high con-
fident tuples. It also assigns a high confidence to
the tuples that frequently co-occur with the con-
texts that match high confidence rules.
When it merges the tuples extracted by the base-
line system, the algorithm uses the following con-
stant value for any context that matches a baseline
pattern.
 

 

 

  

 
 
  
 
  
(7)
where 

denotes the context of tuple 

that
matches a baseline pattern, and 

is any baseline
pattern. With this calculation, the confidence of
any tuple extracted by a baseline pattern is always
greater than or equal to that of any tuple that is
extracted by the learned rules and has the same
frequency.
4 Evaluation
The focus of this paper is the comparison be-
tween bootstrapping strategies for extraction, i.e.,
string pattern learning and less restrictive pattern
learning having Relation NER. Therefore, we first
compare these two bootstrapping methods with
the baseline system. Furthermore, we also com-
pare Relation NER with a generic NER, which is
trained on a pre-existing hand annotated corpus.
Table 1: Weights corresponding to a context vector
(
 
).
adjacency
left other right total
left 0.067 0.133 0.2
context middle 0.24 0.12 0.24 0.6
right 0.133 0.067 0.2
4.1 Relation Extraction Task
We compare SPL and LRPL with the baseline sys-
tem on 5 relations: Acquisition, Merger, CeoOf,
MayorOf, and InventorOf. We downloaded about
from 100,000 to 220,000 sentences for each of
these relations from the Web, which contained a
relation label (e.g. ?acquisition?, ?acquired?, ?ac-
quiring? or ?merger?, ?merged?, ?merging?). We
used all the tuples that co-occur with baseline pat-
terns at least twice as seeds. The numbers of seeds
are between 33 (Acquisition) and 289 (CeoOf).
For consistency, SPL employs the same assess-
ment methods with LRPL. It uses the EM algo-
rithm in Section 3.2.2 and merges the tuples ex-
tracted by the baseline system. In the EM algo-
rithm, the match score    between a learned
pattern  and a tuple  is set to a constant 

.
LRPL uses MinorThird (Cohen, 2004) imple-
mentation of CRF for Relation NER. The features
used in the experiments are the lower-case word,
capitalize pattern, part of speech tag of the cur-
rent and +-2 tokens, and the previous state (tag)
referring to (Minkov et al, 2005; Rosenfeld et al,
2005). The parameters used for SPL and LRPL
are experimentally set as follows: 
	
 	,


 
, 

 	, 
 
 ,   ,
   and the context weights for LRPL shown in
Table 1.
Figure 2-6 show the recall-precision curves. We
use the number of correct extractions to serve as
a surrogate for recall, since computing actual re-
call would require extensive manual inspection of
the large data sets. Compared to the the baseline
system, both bootstrapping methods increases the
number of correct extractions for almost all the re-
lations at around 80% precision. For MayorOf re-
lation, LRPL achieves 250% increase in recall at
0.87 precision, while SPL?s precision is less than
the baseline system. This is because SPL can not
distinguish correct tuples from the error tuples that
60
Figure 2: The recall-precision curve of CeoOf re-
lation.
Figure 3: The recall-precision curve of MayorOf
relation.
co-occur with a short strict pattern, and that have a
wrong entity type value. An example of the error
tuples extracted by SPL is the following:
Learned Pattern: NP1 Mayor NP2
Sentence:
"When Lord Mayor Clover Moore spoke,..."
Tuple: <Lord, Clover Moore>
The improvement of Acquisition and Merger re-
lations is small for both methods; the rules learned
for Merger and Acquisition made erroneous ex-
tractions of mergers of geo-political entities, ac-
quisition of data, ball players, languages or dis-
eases. For InventorOf relation, LRPL does not
work well. This is because ?Invention? is not a
proper noun phrase, but a noun phrase. A noun
phrase includes not only nouns, but a particle,
a determiner, and adjectives in addition to non-
capitalized nouns. Our Relation NER was unable
to detect regularities in the capitalization pattern
and word length of invention phrases.
At around 60% precision, SPL achieves higher
recall for CeoOf and MayorOf relations, in con-
Figure 4: The recall-precision curve of Acquisi-
tion relation.
Figure 5: The recall-precision curve of Merger re-
lation.
trast, LRPL achieves higher recall for Acquisition
and Merger. The reason can be that nominal style
relations (CeoOf and MayorOf) have a smaller
syntactic variety for describing them. Therefore,
learned string patterns are enough generic to ex-
tract many candidate tuples.
4.2 Entity Recognition Task
Generic types such as person, organization, and
location cover many useful relations. One might
expect that NER trained for these generic types,
can be used for different relations without mod-
ifications, instead of creating a Relation NER.
To show the effectiveness of Relation NER, we
compare Relation NER with a generic NER
trained on a pre-existent hand annotated corpus
for generic types; we used MUC7 train, dry-run
test, and formal-test documents(Table 2) (Chin-
chor, 1997). We also incorporate the following
additional knowledge into the CRF?s features re-
ferring to (Minkov et al, 2005; Rosenfeld et al,
61
Figure 6: The recall-precision curve of InventorOf
relation.
Table 2: The number of entities and unique entities
in MUC7 corpus. The number of documents is
225.
entity all uniq
Organization 3704 993
Person 2120 1088
Location 2912 692
2005): first and last names, city names, corp des-
ignators, company words (such as ?technology?),
and small size lists of person title (such as ?mr.?)
and capitalized common words (such as ?Mon-
day?). The base features for both methods are the
same as the ones described in Section 4.1.
The ideal entity recognizer for relation extrac-
tion is recognizing only entities that have an ar-
gument type for a particular relation. Therefore,
a generic test set such as MUC7 Named Entity
Recognition Task can not be used for our evalu-
ation. We randomly selected 200 test sentences
from our dataset that had a pair of correct enti-
ties for CeoOf or MayorOf relations, and were not
used as training for the Relation NER. We mea-
sured the accuracy as follows.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535?1545,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identifying Relations for Open Information Extraction
Anthony Fader, Stephen Soderland, and Oren Etzioni
University of Washington, Seattle
{afader,soderlan,etzioni}@cs.washington.edu
Abstract
Open Information Extraction (IE) is the task
of extracting assertions from massive corpora
without requiring a pre-specified vocabulary.
This paper shows that the output of state-of-
the-art Open IE systems is rife with uninfor-
mative and incoherent extractions. To over-
come these problems, we introduce two sim-
ple syntactic and lexical constraints on bi-
nary relations expressed by verbs. We im-
plemented the constraints in the REVERB
Open IE system, which more than doubles the
area under the precision-recall curve relative
to previous extractors such as TEXTRUNNER
and WOEpos. More than 30% of REVERB?s
extractions are at precision 0.8 or higher?
compared to virtually none for earlier systems.
The paper concludes with a detailed analysis
of REVERB?s errors, suggesting directions for
future work.1
1 Introduction and Motivation
Typically, Information Extraction (IE) systems learn
an extractor for each target relation from la-
beled training examples (Kim and Moldovan, 1993;
Riloff, 1996; Soderland, 1999). This approach to IE
does not scale to corpora where the number of target
relations is very large, or where the target relations
cannot be specified in advance. Open IE solves this
problem by identifying relation phrases?phrases
that denote relations in English sentences (Banko
et al, 2007). The automatic identification of rela-
1The source code for REVERB is available at http://
reverb.cs.washington.edu/
tion phrases enables the extraction of arbitrary re-
lations from sentences, obviating the restriction to a
pre-specified vocabulary.
Open IE systems have achieved a notable measure
of success on massive, open-domain corpora drawn
from the Web, Wikipedia, and elsewhere. (Banko et
al., 2007; Wu and Weld, 2010; Zhu et al, 2009). The
output of Open IE systems has been used to support
tasks like learning selectional preferences (Ritter et
al., 2010), acquiring common sense knowledge (Lin
et al, 2010), and recognizing entailment (Schoen-
mackers et al, 2010; Berant et al, 2011). In ad-
dition, Open IE extractions have been mapped onto
existing ontologies (Soderland et al, 2010).
We have observed that two types of errors are fre-
quent in the output of Open IE systems such as TEX-
TRUNNER and WOE: incoherent extractions and un-
informative extractions.
Incoherent extractions are cases where the ex-
tracted relation phrase has no meaningful interpre-
tation (see Table 1 for examples). Incoherent ex-
tractions arise because the learned extractor makes a
sequence of decisions about whether to include each
word in the relation phrase, often resulting in incom-
prehensible predictions. To solve this problem, we
introduce a syntactic constraint: every multi-word
relation phrase must begin with a verb, end with a
preposition, and be a contiguous sequence of words
in the sentence. Thus, the identification of a relation
phrase is made in one fell swoop instead of on the
basis of multiple, word-by-word decisions.
Uninformative extractions are extractions that
omit critical information. For example, consider the
sentence ?Faust made a deal with the devil.? Previ-
1535
ous Open IE systems return the uninformative
(Faust, made, a deal)
instead of
(Faust, made a deal with, the devil).
This type of error is caused by improper handling
of relation phrases that are expressed by a combi-
nation of a verb with a noun, such as light verb
constructions (LVCs). An LVC is a multi-word ex-
pression composed of a verb and a noun, with the
noun carrying the semantic content of the predi-
cate (Grefenstette and Teufel, 1995; Stevenson et al,
2004; Allerton, 2002). Table 2 illustrates the wide
range of relations expressed this way, which are not
captured by existing open extractors. Our syntactic
constraint leads the extractor to include nouns in the
relation phrase, solving this problem.
Although the syntactic constraint significantly re-
duces incoherent and uninformative extractions, it
allows overly-specific relation phrases such as is of-
fering only modest greenhouse gas reduction targets
at. To avoid overly-specific relation phrases, we in-
troduce an intuitive lexical constraint: a binary rela-
tion phrase ought to appear with at least a minimal
number of distinct argument pairs in a large corpus.
In summary, this paper articulates two simple but
surprisingly powerful constraints on how binary re-
lationships are expressed via verbs in English sen-
tences, and implements them in the REVERB Open
IE system. We release REVERB and the data used in
our experiments to the research community.
The rest of the paper is organized as follows. Sec-
tion 2 analyzes previous work. Section 3 defines our
constraints precisely. Section 4 describes REVERB,
our implementation of the constraints. Section 5 re-
ports on our experimental results. Section 6 con-
cludes with a summary and discussion of future
work.
2 Previous Work
Open IE systems like TEXTRUNNER (Banko et al,
2007), WOEpos, and WOEparse (Wu and Weld, 2010)
focus on extracting binary relations of the form
(arg1, relation phrase, arg2) from text. These sys-
tems all use the following three-step method:
1. Label: Sentences are automatically labeled
with extractions using heuristics or distant su-
pervision.
Sentence Incoherent Relation
The guide contains dead links
and omits sites.
contains omits
The Mark 14 was central to the
torpedo scandal of the fleet.
was central torpedo
They recalled that Nungesser
began his career as a precinct
leader.
recalled began
Table 1: Examples of incoherent extractions. In-
coherent extractions make up approximately 13% of
TEXTRUNNER?s output, 15% of WOEpos?s output, and
30% of WOEparse?s output.
is is an album by, is the author of, is a city in
has has a population of, has a Ph.D. in, has a cameo in
made made a deal with, made a promise to
took took place in, took control over, took advantage of
gave gave birth to, gave a talk at, gave new meaning to
got got tickets to, got a deal on, got funding from
Table 2: Examples of uninformative relations (left) and
their completions (right). Uninformative relations oc-
cur in approximately 4% of WOEparse?s output, 6% of
WOEpos?s output, and 7% of TEXTRUNNER?s output.
2. Learn: A relation phrase extractor is learned
using a sequence-labeling graphical model
(e.g., CRF).
3. Extract: the system takes a sentence as in-
put, identifies a candidate pair of NP arguments
(arg1, arg2) from the sentence, and then uses
the learned extractor to label each word be-
tween the two arguments as part of the relation
phrase or not.
The extractor is applied to the successive sentences
in the corpus, and the resulting extractions are col-
lected.
This method faces several challenges. First,
the training phase requires a large number of la-
beled training examples (e.g., 200, 000 heuristically-
labeled sentences for TEXTRUNNER and 300, 000
for WOE). Heuristic labeling of examples obviates
hand labeling but results in noisy labels and distorts
the distribution of examples. Second, the extrac-
tion step is posed as a sequence-labeling problem,
where each word is assigned its own label. Because
each assignment is uncertain, the likelihood that the
extracted relation phrase is flawed increases with
the length of the sequence. Finally, the extractor
1536
chooses an extraction?s arguments heuristically, and
cannot backtrack over this choice. This is problem-
atic when a word that belongs in the relation phrase
is chosen as an argument (for example, deal from
the ?made a deal with? sentence).
Because of the feature sets utilized in previous
work, the learned extractors ignore both ?holistic?
aspects of the relation phrase (e.g., is it contiguous?)
as well as lexical aspects (e.g., how many instances
of this relation are there?). Thus, as we show in Sec-
tion 5, systems such as TEXTRUNNER are unable
to learn the constraints embedded in REVERB. Of
course, a learning system, utilizing a different hy-
pothesis space, and an appropriate set of training ex-
amples, could potentially learn and refine the con-
straints in REVERB. This is a topic for future work,
which we consider in Section 6.
The first Open IE system was TEXTRUNNER
(Banko et al, 2007), which used a Naive Bayes
model with unlexicalized POS and NP-chunk fea-
tures, trained using examples heuristically generated
from the Penn Treebank. Subsequent work showed
that utilizing a linear-chain CRF (Banko and Et-
zioni, 2008) or Markov Logic Network (Zhu et al,
2009) can lead to improved extraction. The WOE
systems introduced by Wu and Weld make use of
Wikipedia as a source of training data for their ex-
tractors, which leads to further improvements over
TEXTRUNNER (Wu and Weld, 2010). Wu and Weld
also show that dependency parse features result in a
dramatic increase in precision and recall over shal-
low linguistic features, but at the cost of extraction
speed.
Other approaches to large-scale IE have included
Preemptive IE (Shinyama and Sekine, 2006), On-
Demand IE (Sekine, 2006), and weak supervision
for IE (Mintz et al, 2009; Hoffmann et al, 2010).
Preemptive IE and On-Demand IE avoid relation-
specific extractors, but rely on document and en-
tity clustering, which is too costly for Web-scale IE.
Weakly supervised methods use an existing ontol-
ogy to generate training data for learning relation-
specific extractors. While this allows for learn-
ing relation-specific extractors at a larger scale than
what was previously possible, the extractions are
still restricted to a specific ontology.
Many systems have used syntactic patterns based
on verbs to extract relation phrases, usually rely-
ing on a full dependency parse of the input sentence
(Lin and Pantel, 2001; Stevenson, 2004; Specia and
Motta, 2006; Kathrin Eichler and Neumann, 2008).
Our work differs from these approaches by focus-
ing on relation phrase patterns expressed in terms
of POS tags and NP chunks, instead of full parse
trees. Banko and Etzioni (Banko and Etzioni, 2008)
showed that a small set of POS-tag patterns cover a
large fraction of relationships in English, but never
incorporated the patterns into an extractor. This pa-
per reports on a substantially improved model of bi-
nary relation phrases, which increases the recall of
the Banko-Etzioni model (see Section 3.3). Further,
while previous work in Open IE has mainly focused
on syntactic patterns for relation extraction, we in-
troduce a lexical constraint that boosts precision and
recall.
Finally, Open IE is closely related to semantic role
labeling (SRL) (Punyakanok et al, 2008; Toutanova
et al, 2008) in that both tasks extract relations and
arguments from sentences. However, SRL systems
traditionally rely on syntactic parsers, which makes
them susceptible to parser errors and substantially
slower than Open IE systems such as REVERB. This
difference is particularly important when operating
on the Web corpus due to its size and heterogeneity.
Finally, SRL requires hand-constructed semantic re-
sources like Propbank and Framenet (Martha and
Palmer, 2002; Baker et al, 1998) as input. In con-
trast, Open IE systems require no relation-specific
training data. ReVerb, in particular, relies on its ex-
plicit lexical and syntactic constraints, which have
no correlate in SRL systems. For a more detailed
comparison of SRL and Open IE, see (Christensen
et al, 2010).
3 Constraints on Relation Phrases
In this section we introduce two constraints on re-
lation phrases: a syntactic constraint and a lexical
constraint.
3.1 Syntactic Constraint
The syntactic constraint serves two purposes. First,
it eliminates incoherent extractions, and second, it
reduces uninformative extractions by capturing rela-
tion phrases expressed by a verb-noun combination,
including light verb constructions.
1537
V | V P | VW ?P
V = verb particle? adv?
W = (noun | adj | adv | pron | det)
P = (prep | particle | inf. marker)
Figure 1: A simple part-of-speech-based regular expres-
sion reduces the number of incoherent extractions like
was central torpedo and covers relations expressed via
light verb constructions like gave a talk at.
The syntactic constraint requires the relation
phrase to match the POS tag pattern shown in Fig-
ure 1. The pattern limits relation phrases to be either
a verb (e.g., invented), a verb followed immediately
by a preposition (e.g., located in), or a verb followed
by nouns, adjectives, or adverbs ending in a preposi-
tion (e.g., has atomic weight of). If there are multiple
possible matches in a sentence for a single verb, the
longest possible match is chosen. Finally, if the pat-
tern matches multiple adjacent sequences, we merge
them into a single relation phrase (e.g., wants to ex-
tend). This refinement enables the model to readily
handle relation phrases containing multiple verbs. A
consequence of this pattern is that the relation phrase
must be a contiguous span of words in the sentence.
The syntactic constraint eliminates the incoherent
relation phrases returned by existing systems. For
example, given the sentence
Extendicare agreed to buy Arbor Health Care for
about US $432 million in cash and assumed debt.
TEXTRUNNER returns the extraction
(Arbor Health Care, for assumed, debt).
The phrase for assumed is clearly not a valid rela-
tion phrase: it begins with a preposition and splices
together two distant words in the sentence. The syn-
tactic constraint prevents this type of error by sim-
ply restricting relation phrases to match the pattern
in Figure 1.
The syntactic constraint reduces uninformative
extractions by capturing relation phrases expressed
via LVCs. For example, the POS pattern matched
against the sentence ?Faust made a deal with the
Devil,? would result in the relation phrase made a
deal with, instead of the uninformative made.
Finally, we require the relation phrase to appear
between its two arguments in the sentence. This is a
common constraint that has been implicitly enforced
in other open extractors.
3.2 Lexical Constraint
While the syntactic constraint greatly reduces unin-
formative extractions, it can sometimes match rela-
tion phrases that are so specific that they have only a
few possible instances, even in a Web-scale corpus.
Consider the sentence:
The Obama administration is offering only modest
greenhouse gas reduction targets at the conference.
The POS pattern will match the phrase:
is offering only modest greenhouse gas reduction targets at
(1)
Thus, there are phrases that satisfy the syntactic con-
straint, but are not relational.
To overcome this limitation, we introduce a lexi-
cal constraint that is used to separate valid relation
phrases from overspecified relation phrases, like the
example in (1). The constraint is based on the in-
tuition that a valid relation phrase should take many
distinct arguments in a large corpus. The phrase in
(1) is specific to the argument pair (Obama admin-
istration, conference), so it is unlikely to represent a
bona fide relation. We describe the implementation
details of the lexical constraint in Section 4.
3.3 Limitations
Our constraints represent an idealized model of re-
lation phrases in English. This raises the question:
How much recall is lost due to the constraints?
To address this question, we analyzed Wu and
Weld?s set of 300 sentences from a set of random
Web pages, manually identifying all verb-based re-
lationships between noun phrase pairs. This resulted
in a set of 327 relation phrases. For each rela-
tion phrase, we checked whether it satisfies our con-
straints. We found that 85% of the relation phrases
do satisfy the constraints. Of the remaining 15%,
we identified some of the common cases where the
constraints were violated, summarized in Table 3.
Many of the example relation phrases shown in
Table 3 involve long-range dependencies between
words in the sentence. These types of dependen-
cies are not easily representable using a pattern over
POS tags. A deeper syntactic analysis of the input
sentence would provide a much more general lan-
guage for modeling relation phrases. For example,
one could create a model of relations expressed in
1538
Binary Verbal Relation Phrases
85% Satisfy Constraints
8% Non-Contiguous Phrase Structure
Coordination: X is produced and maintained by Y
Multiple Args: X was founded in 1995 by Y
Phrasal Verbs: X turned Y off
4% Relation Phrase Not Between Arguments
Intro. Phrases: Discovered by Y, X . . .
Relative Clauses: . . . the Y that X discovered
3% Do Not Match POS Pattern
Interrupting Modifiers: X has a lot of faith in Y
Infinitives: X to attack Y
Table 3: Approximately 85% of the binary verbal relation
phrases in a sample of Web sentences satisfy our con-
straints.
terms of dependency parse features that would cap-
ture the non-contiguous relation phrases in Table 3.
Previous work has shown that dependency paths do
indeed boost the recall of relation extraction systems
(Wu and Weld, 2010; Mintz et al, 2009). While us-
ing dependency path features allows for a more flex-
ible model of relations, it significantly increases pro-
cessing time, which is problematic for Web-scale ex-
traction. Further, we have found that this increased
recall comes at the cost of lower precision on Web
text (see Section 5).
The results in Table 3 are similar to Banko and Et-
zioni?s findings that a set of eight POS patterns cover
a large fraction of binary verbal relation phrases.
However, their analysis was based on a set of sen-
tences known to contain either a company acquisi-
tion or birthplace relationship, while our results are
on a random sample of Web sentences. We applied
Banko and Etzioni?s verbal patterns to our random
sample of 300 Web sentences, and found that they
cover approximately 69% of the relation phrases in
the corpus. The gap in recall between this and the
85% shown in Table 3 is largely due to LVC relation
phrases (made a deal with) and phrases containing
multiple verbs (refuses to return to), which their pat-
terns do not cover.
In sum, our model is by no means complete.
However, we have empirically shown that the ma-
jority of binary verbal relation phrases in a sample
of Web sentences are captured by our model. By
focusing on this subset of language, our model can
be used to perform Open IE at significantly higher
precision than before.
4 REVERB
This section introduces REVERB, a novel open ex-
tractor based on the constraints defined in the previ-
ous section. REVERB first identifies relation phrases
that satisfy the syntactic and lexical constraints, and
then finds a pair of NP arguments for each identified
relation phrase. The resulting extractions are then
assigned a confidence score using a logistic regres-
sion classifier.
This algorithm differs in three important ways
from previous methods (Section 2). First, the re-
lation phrase is identified ?holistically? rather than
word-by-word. Second, potential phrases are fil-
tered based on statistics over a large corpus (the
implementation of our lexical constraint). Finally,
REVERB is ?relation first? rather than ?arguments
first?, which enables it to avoid a common error
made by previous methods?confusing a noun in the
relation phrase for an argument, e.g. the noun deal in
made a deal with.
4.1 Extraction Algorithm
REVERB takes as input a POS-tagged and NP-
chunked sentence and returns a set of (x, r, y)
extraction triples.2 Given an input sentence s,
REVERB uses the following extraction algorithm:
1. Relation Extraction: For each verb v in s,
find the longest sequence of words rv such that
(1) rv starts at v, (2) rv satisfies the syntactic
constraint, and (3) rv satisfies the lexical con-
straint. If any pair of matches are adjacent or
overlap in s, merge them into a single match.
2. Argument Extraction: For each relation
phrase r identified in Step 1, find the nearest
noun phrase x to the left of r in s such that x is
not a relative pronoun, WHO-adverb, or exis-
tential ?there?. Find the nearest noun phrase y
to the right of r in s. If such an (x, y) pair could
be found, return (x, r, y) as an extraction.
We check whether a candidate relation phrase
rv satisfies the syntactic constraint by matching it
against the regular expression in Figure 1.
2REVERB uses OpenNLP for POS tagging and NP chunk-
ing: http://opennlp.sourceforge.net/
1539
To determine whether rv satisfies the lexical con-
straint, we use a large dictionary D of relation
phrases that are known to take many distinct argu-
ments. In an offline step, we construct D by find-
ing all matches of the POS pattern in a corpus of
500 million Web sentences. For each matching re-
lation phrase, we heuristically identify its arguments
(as in Step 2 above). We set D to be the set of all
relation phrases that take at least k distinct argument
pairs in the set of extractions. In order to allow for
minor variations in relation phrases, we normalize
each relation phrase by removing inflection, auxil-
iary verbs, adjectives, and adverbs. Based on ex-
periments on a held-out set of sentences, we found
that a value of k = 20 works well for filtering out
overspecified relations. This results in a set of ap-
proximately 1.7 million distinct normalized relation
phrases, which are stored in memory at extraction
time.
As an example of the extraction algorithm in ac-
tion, consider the following input sentence:
Hudson was born in Hampstead, which is a
suburb of London.
Step 1 of the algorithm identifies three relation
phrases that satisfy the syntactic and lexical con-
straints: was, born in, and is a suburb of. The first
two phrases are adjacent in the sentence, so they are
merged into the single relation phrase was born in.
Step 2 then finds an argument pair for each relation
phrase. For was born in, the nearest NPs are (Hud-
son, Hampstead). For is a suburb of, the extractor
skips over the NP which and chooses the argument
pair (Hampstead, London). The final output is
e1: (Hudson, was born in, Hampstead)
e2: (Hampstead, is a suburb of, London).
4.2 Confidence Function
The extraction algorithm in the previous section has
high recall, but low precision. Like with previous
open extractors, we want way to trade recall for pre-
cision by tuning a confidence threshold. We use a
logistic regression classifier to assign a confidence
score to each extraction, which uses the features
shown in Table 4. All of these features are efficiently
computable and relation independent. We trained
the confidence function by manually labeling the ex-
tractions from a set of 1, 000 sentences from the Web
and Wikipedia as correct or incorrect.
Weight Feature
1.16 (x, r, y) covers all words in s
0.50 The last preposition in r is for
0.49 The last preposition in r is on
0.46 The last preposition in r is of
0.43 len(s) ? 10 words
0.43 There is a WH-word to the left of r
0.42 r matches VW*P from Figure 1
0.39 The last preposition in r is to
0.25 The last preposition in r is in
0.23 10 words < len(s) ? 20 words
0.21 s begins with x
0.16 y is a proper noun
0.01 x is a proper noun
-0.30 There is an NP to the left of x in s
-0.43 20 words < len(s)
-0.61 r matches V from Figure 1
-0.65 There is a preposition to the left of x in s
-0.81 There is an NP to the right of y in s
-0.93 Coord. conjunction to the left of r in s
Table 4: REVERB uses these features to assign a confi-
dence score to an extraction (x, r, y) from a sentence s
using a logistic regression classifier.
Previous open extractors require labeled training
data to learn a model of relations, which is then used
to extract relation phrases from text. In contrast,
REVERB uses a specified model of relations for ex-
traction, and requires labeled data only for assigning
confidence scores to its extractions. Learning a con-
fidence function is a much simpler task than learning
a full model of relations, using two orders of magni-
tude fewer training examples than TEXTRUNNER or
WOE.
4.3 TEXTRUNNER-R
The model of relation phrases used by REVERB
is specified, but could a TEXTRUNNER-like sys-
tem learn this model from training data? While
it is difficult to answer such a question for all
possible permutations of features sets, training ex-
amples, and learning biases, we demonstrate that
TEXTRUNNER itself cannot learn REVERB?s model
even when re-trained using the output of REVERB
as labeled training data. The resulting system,
TEXTRUNNER-R, uses the same feature representa-
tion as TEXTRUNNER, but different parameters, and
a different set of training examples.
To generate positive instances, we ran REVERB
1540
on the Penn Treebank, which is the same dataset
that TEXTRUNNER is trained on. To generate neg-
ative instances from a sentence, we took each noun
phrase pair in the sentence that does not appear as
arguments in a REVERB extraction. This process
resulted in a set of 67, 562 positive instances, and
356, 834 negative instances. We then passed these
labeled examples to TEXTRUNNER?s training proce-
dure, which learns a linear-chain CRF using closed-
class features like POS tags, capitalization, punctu-
ation, etc.TEXTRUNNER-R uses the argument-first
extraction algorithm described in Section 2.
5 Experiments
We compare REVERB to the following systems:
? REVERB?lex - The REVERB system described
in the previous section, but without the lexical
constraint. REVERB?lex uses the same confi-
dence function as REVERB.
? TEXTRUNNER - Banko and Etzioni?s 2008 ex-
tractor, which uses a second order linear-chain
CRF trained on extractions heuristically gener-
ated from the Penn Treebank. TEXTRUNNER
uses shallow linguistic features in its CRF,
which come from the same POS tagger and NP-
chunker that REVERB uses.
? TEXTRUNNER-R - Our modification to
TEXTRUNNER, which uses the same extrac-
tion code, but with a model of relations trained
on REVERB extractions.
? WOEpos - Wu and Weld?s modification to
TEXTRUNNER, which uses a model of re-
lations learned from extractions heuristically
generated from Wikipedia.
? WOEparse - Wu and Weld?s parser-based ex-
tractor, which uses a large dictionary of depen-
dency path patterns learned from heuristic ex-
tractions generated from Wikipedia.
Each system is given a set of sentences as input,
and returns a set of binary extractions as output. We
created a test set of 500 sentences sampled from the
Web, using Yahoo?s random link service.3 After run-
3http://random.yahoo.com/bin/ryl
REVERB REVERB WOE TEXT- WOE TEXT-
0.0
0.1
0.2
0.3
0.4
0.5
A
r
e
a
U
n
d
e
r
P
R
C
u
r
v
e
?lex
parse
RUNNER-R
pos
RUNNER
A
r
e
a
U
n
d
e
r
P
R
C
u
r
v
e
Figure 2: REVERB outperforms state-of-the-art open
extractors, with an AUC more than twice that of
TEXTRUNNER or WOEpos, and 38% higher than
WOEparse.
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Comparison of REVERB-Based Systems
REVERB
REVERB
?lex
TEXTRUNNER-R
P
r
e
c
i
s
i
o
n
Figure 3: The lexical constraint gives REVERB
a boost in precision and recall over REVERB?lex.
TEXTRUNNER-R is unable to learn the model used by
REVERB, which results in lower precision and recall.
ning each extractor over the input sentences, two hu-
man judges independently evaluated each extraction
as correct or incorrect. The judges reached agree-
ment on 86% of the extractions, with an agreement
score of ? = 0.68. We report results on the subset
of the data where the two judges concur.
The judges labeled uninformative extractions con-
servatively. That is, if critical information was
dropped from the relation phrase but included in the
second argument, it is labeled correct. For example,
both the extractions (Ackerman, is a professor of, bi-
ology) and (Ackerman, is, a professor of biology) are
considered correct.
Each system returns confidence scores for its ex-
tractions. For a given threshold, we can measure
the precision and recall of the output. Precision
is the fraction of returned extractions that are cor-
rect. Recall is the fraction of correct extractions in
1541
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Extractions
REVERB
WOE
parse
WOE
pos
TEXTRUNNER
P
r
e
c
i
s
i
o
n
Figure 4: REVERB achieves significantly higher preci-
sion than state-of-the-art Open IE systems, and compara-
ble recall to WOEparse.
the corpus that are returned. We use the total num-
ber of extractions labeled as correct by the judges
as our measure of recall for the corpus. In order to
avoid double-counting, we treat extractions that dif-
fer superficially (e.g., different punctuation or drop-
ping inessential modifiers) as a single extraction. We
compute a precision-recall curve by varying the con-
fidence threshold, and then compute the area under
the curve (AUC).
5.1 Results
Figure 2 shows the AUC of each system. REVERB
achieves an AUC that is 30% higher than WOEparse
and is more than double the AUC of WOEpos or
TEXTRUNNER. The lexical constraint provides a
significant boost in performance, with REVERB
achieving an AUC 23% higher than REVERB?lex.
REVERB proves to be a useful source of train-
ing data, with TEXTRUNNER-R having an AUC
71% higher than TEXTRUNNER and performing
on par with WOEpos. From the training data,
TEXTRUNNER-R was able to learn a model that
predicts contiguous relation phrases, but still re-
turned incoherent relation phrases (e.g., starting with
a preposition) and overspecified relation phrases.
These errors are due to TEXTRUNNER-R overfitting
the training data and not having access to the lexical
constraint.
Figure 3 shows the precision-recall curves of the
systems introduced in this paper. TEXTRUNNER-R
has much lower precision than REVERB and
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Relations Only
REVERB
WOE
parse
WOE
pos
TEXTRUNNER
P
r
e
c
i
s
i
o
n
Figure 5: On the subtask of identifying relations phrases,
REVERB is able to achieve even higher precision and re-
call than other systems.
REVERB?lex at all levels of recall. The lexi-
cal constraint gives REVERB a boost in precision
over REVERB?lex, reducing overspecified extrac-
tions from 20% of REVERB?lex?s output to 1% of
REVERB?s. The lexical constraint also boosts recall
over REVERB?lex, since REVERB is able to find a
correct relation phrase where REVERB?lex finds an
overspecified one.
Figure 4 shows the precision-recall curves of
REVERB and the external systems. REVERB has
much higher precision than the other systems at
nearly all levels of recall. In particular, more than
30% of REVERB?s extractions are at precision 0.8
or higher, compared to virtually none for the other
systems. WOEparse achieves a slightly higher recall
than REVERB (0.62 versus 0.64), but at the cost of
lower precision.
In order to highlight the role of the relational
model of each system, we also evaluate their per-
formance on the subtask of extracting just the rela-
tion phrases from the input text. Figure 5 shows the
precision-recall curves for each system on the rela-
tion phrase-only evaluation. In this case, REVERB
has both higher precision and recall than the other
systems.
REVERB?s biggest improvement came from the
elimination of incoherent extractions. Incoher-
ent extractions were a large fraction of the errors
made by previous systems, accounting for approxi-
mately 13% of TEXTRUNNER?s extractions, 15% of
WOEpos?s, and 30% of WOEparse?s. Uninformative
1542
REVERB - Incorrect Extractions
65% Correct relation phrase, incorrect arguments
16% N-ary relation
8% Non-contiguous relation phrase
2% Imperative verb
2% Overspecified relation phrase
7% Other, including POS/chunking errors
Table 5: The majority of the incorrect extractions re-
turned by REVERB are due to errors in argument extrac-
tion.
extractions had a smaller effect on other systems?
precision, accounting for 4% of WOEparse?s extrac-
tions, 5% of WOEpos?s, and 7% of TEXTRUNNER?s,
while only appearing in 1% of REVERB?s extrac-
tions. REVERB?s reduction in uninformative extrac-
tions resulted in a boost in recall, capturing many
LVC relation phrases missed by other systems (like
those shown in Table 2).
To test the systems? speed, we ran each extrac-
tor on a set of 100, 000 sentences using a Pen-
tium 4 machine with 4GB of RAM. The process-
ing times were 16 minutes for REVERB, 21 min-
utes for TEXTRUNNER, 21 minutes for WOEpos, and
11 hours for WOEparse. The times for REVERB,
TEXTRUNNER, and WOEpos are all approximately
the same, since they all use the same POS-tagging
and NP-chunking software. WOEparse processes
each sentence with a dependency parser, resulting
in much longer processing time.
5.2 REVERB Error Analysis
To better understand the limitations of REVERB, we
performed a detailed analysis of its errors in pre-
cision (incorrect extractions returned by REVERB)
and its errors in recall (correct extractions that
REVERB missed).
Table 5 summarizes the types of incorrect extrac-
tions that REVERB returns. We found that 65% of
the incorrect extractions returned by REVERB were
cases where a relation phrase was correctly identi-
fied, but the argument-finding heuristics failed. The
remaining errors were cases where REVERB ex-
tracted an incorrect relation phrase. One common
mistake that REVERB made was extracting a rela-
tion phrase that expresses an n-ary relationship via
a ditransitive verb. For example, given the sentence
REVERB - Missed Extractions
52% Could not identify correct arguments
23% Relation filtered out by lexical constraint
17% Identified a more specific relation
8% POS/chunking error
Table 6: The majority of extractions that were missed by
REVERB were cases where the correct relation phrase
was found, but the arguments were not correctly identi-
fied.
?I gave him 15 photographs,? REVERB extracts (I,
gave, him). These errors are due to the fact that
REVERB only models binary relations.
Table 6 summarizes the correct extractions that
were extracted by other systems and were not ex-
tracted by REVERB. As with the false positive ex-
tractions, the majority of false negatives (52%) were
due to the argument-finding heuristics choosing the
wrong arguments, or failing to extract all possible ar-
guments (in the case of coordinating conjunctions).
Other sources of failure were due to the lexical con-
straint either failing to filter out an overspecified re-
lation phrase or filtering out a valid relation phrase.
These errors hurt both precision and recall, since
each case results in the extractor overlooking a cor-
rect relation phrase and choosing another.
5.3 Evaluation At Scale
Section 5.1 shows that REVERB outperforms ex-
isting Open IE systems when evaluated on a sam-
ple of sentences. Previous work has shown that
the frequency of an extraction in a large corpus is
useful for assessing the correctness of extractions
(Downey et al, 2005). Thus, it is possible a pri-
ori that REVERB?s gains over previous systems will
diminish when extraction frequency is taken into ac-
count.
In fact, we found that REVERB?s advantage over
TEXTRUNNER when run at scale is qualitatively
similar to its advantage on single sentences. We ran
both REVERB and TEXTRUNNER on Banko and Et-
zioni?s corpus of 500 million Web sentences and ex-
amined the effect of redundancy on precision.
As Downey?s work predicts, precision increased
in both systems for extractions found multiple
times, compared with extractions found only once.
However, REVERB had higher precision than
1543
TEXTRUNNER at all frequency thresholds. In fact,
REVERB?s frequency 1 extractions had a precision
of 0.75, which TEXTRUNNER could not approach
even with frequency 10 extractions, which had a
precision of 0.34. Thus, REVERB is able to return
more correct extractions at a higher precision than
TEXTRUNNER, even when redundancy is taken into
account.
6 Conclusions and Future Work
The paper?s contributions are as follows:
? We have identified and analyzed the problems
of incoherent and uninformative extractions for
Open IE systems, and shown their prevalence
for systems such as TEXTRUNNER and WOE.
? We articulated general, easy-to-enforce con-
straints on binary, verb-based relation phrases
in English that ameliorate these problems and
yield richer and more informative relations
(see, for example, Table 2).
? Based on these constraints, we designed, im-
plemented, and evaluated the REVERB extrac-
tor, which substantially outperforms previous
Open IE systems in both recall and precision.
? We make REVERB and the data used in our
experiments available to the research commu-
nity.4
In future work, we plan to explore utilizing our
constraints to improve the performance of learned
CRF models. Roth et al have shown how to incor-
porate constraints into CRF learners (Roth and Yih,
2005). It is natural, then, to consider whether the
combination of heuristically labeled training exam-
ples, CRF learning, and our constraints will result
in superior performance. The error analysis in Sec-
tion 5.2 also suggests natural directions for future
work. For instance, since many of REVERB?s errors
are due to incorrect arguments, improved methods
for argument extraction are in order.
Acknowledgments
We would like to thank Mausam, Dan Weld, Yoav
Artzi, Luke Zettlemoyer, members of the KnowItAll
4http://reverb.cs.washington.edu
group, and the anonymous reviewers for their help-
ful comments. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, and DARPA contract FA8750-09-C-0179,
and carried out at the University of Washington?s
Turing Center.
References
David J. Allerton. 2002. Stretched Verb Constructions in
English. Routledge Studies in Germanic Linguistics.
Routledge (Taylor and Francis), New York.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In In the Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, January.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, FAM-LbR ?10, pages 52?60, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034?1041.
Gregory Grefenstette and Simone Teufel. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proceedings of the sev-
enth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 98?103, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 286?
295, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
1544
Holmer Hemsen Kathrin Eichler and Gnter Neu-
mann. 2008. Unsupervised relation extraction
from web documents. In LREC. http://www.lrec-
conf.org/proceedings/lrec2008/.
J. Kim and D. Moldovan. 1993. Acquisition of semantic
patterns for information extraction from corpora. In
Procs. of Ninth IEEE Conference on Artificial Intelli-
gence for Applications, pages 171?176.
Dekang Lin and Patrick Pantel. 2001. DIRT-Discovery
of Inference Rules from Text. In Proceedings of
ACM Conference on Knowledge Discovery and Data
Mining(KDD-01), pages pp. 323?328.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing Functional Relations in Web Text. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1266?1276, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ?09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003?1011, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
E. Riloff. 1996. Automatically constructing extraction
patterns from untagged text. In Procs. of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044?1049.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In ACL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ?05, pages 736?743, New
York, NY, USA. ACM.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731?738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 304?311, New York City, USA,
June. Association for Computational Linguistics.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open in-
formation extraction to domain-specific relations. AI
Magazine, 31(3):93?102.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272.
Lucia Specia and Enrico Motta. 2006. M.: A hybrid
approach for extracting semantic relations from texts.
In In. Proceedings of the 2 nd Workshop on Ontology
Learning and Population, pages 57?64.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity
of light verb constructions. In 2nd ACL Workshop on
Multiword Expressions, pages 1?8.
M. Stevenson. 2004. An unsupervised WordNet-based
algorithm for relation extraction. In Proceedings of
the ?Beyond Named Entity? workshop at the Fourth
International Conference on Language Resources and
Evalutaion (LREC?04).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 118?127, Morristown,
NJ, USA. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW ?09:
Proceedings of the 18th international conference on
World wide web, pages 101?110, New York, NY, USA.
ACM.
1545
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 523?534, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Open Language Learning for Information Extraction
Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington, Seattle
{mausam,schmmd,rbart,soderlan,etzioni}@cs.washington.edu
Abstract
Open Information Extraction (IE) systems ex-
tract relational tuples from text, without re-
quiring a pre-specified vocabulary, by iden-
tifying relation phrases and associated argu-
ments in arbitrary sentences. However, state-
of-the-art Open IE systems such as REVERB
and WOE share two important weaknesses ?
(1) they extract only relations that are medi-
ated by verbs, and (2) they ignore context,
thus extracting tuples that are not asserted as
factual. This paper presents OLLIE, a sub-
stantially improved Open IE system that ad-
dresses both these limitations. First, OLLIE
achieves high yield by extracting relations me-
diated by nouns, adjectives, and more. Sec-
ond, a context-analysis step increases preci-
sion by including contextual information from
the sentence in the extractions. OLLIE obtains
2.7 times the area under precision-yield curve
(AUC) compared to REVERB and 1.9 times
the AUC of WOEparse.
1 Introduction
While traditional Information Extraction (IE)
(ARPA, 1991; ARPA, 1998) focused on identifying
and extracting specific relations of interest, there
has been great interest in scaling IE to a broader
set of relations and to far larger corpora (Banko et
al., 2007; Hoffmann et al2010; Mintz et al2009;
Carlson et al2010; Fader et al2011). However,
the requirement of having pre-specified relations of
interest is a significant obstacle. Imagine an intel-
ligence analyst who recently acquired a terrorist?s
laptop or a news reader who wishes to keep abreast
of important events. The substantial endeavor in
1. ?After winning the Superbowl, the Saints are now
the top dogs of the NFL.?
O: (the Saints; win; the Superbowl)
2. ?There are plenty of taxis available at Bali airport.?
O: (taxis; be available at; Bali airport)
3. ?Microsoft co-founder Bill Gates spoke at ...?
O: (Bill Gates; be co-founder of; Microsoft)
4. ?Early astronomers believed that the earth is the
center of the universe.?
R: (the earth; be the center of; the universe)
W: (the earth; be; the center of the universe)
O: ((the earth; be the center of; the universe)
AttributedTo believe; Early astronomers)
5. ?If he wins five key states, Romney will be elected
President.?
R,W: (Romney; will be elected; President)
O: ((Romney; will be elected; President)
ClausalModifier if; he wins five key states)
Figure 1: OLLIE (O) has a wider syntactic range and finds
extractions for the first three sentences where REVERB
(R) and WOEparse (W) find none. For sentences #4,5,
REVERB and WOEparse have an incorrect extraction by
ignoring the context that OLLIE explicitly represents.
analyzing their corpus is the discovery of important
relations, which are likely not pre-specified. Open
IE (Banko et al2007) is the state-of-the-art
approach for such scenarios.
However, the state-of-the-art Open IE systems,
REVERB (Fader et al2011; Etzioni et al2011)
and WOEparse (Wu and Weld, 2010) suffer from two
key drawbacks. Firstly, they handle a limited sub-
set of sentence constructions for expressing relation-
ships. Both extract only relations that are mediated
by verbs, and REVERB further restricts this to a sub-
set of verbal patterns. This misses important infor-
mation mediated via other syntactic entities such as
nouns and adjectives, as well as a wider range of
verbal structures (examples #1-3 in Figure 1).
523
Secondly, REVERB and WOEparse perform only
a local analysis of a sentence, so they often extract
relations that are not asserted as factual in the sen-
tence (examples #4,5). This often occurs when the
relation is within a belief, attribution, hypothetical
or other conditional context.
In this paper we present OLLIE (Open Language
Learning for Information Extraction), 1 our novel
Open IE system that overcomes the limitations of
previous Open IE by (1) expanding the syntactic
scope of relation phrases to cover a much larger
number of relation expressions, and (2) expand-
ing the Open IE representation to allow additional
context information such as attribution and clausal
modifiers. OLLIE extractions obtain a dramatically
higher yield at higher or comparable precision rela-
tive to existing systems.
The outline of the paper is as follows. First, we
provide background on Open IE and how it relates
to Semantic Role Labeling (SRL). Section 3 de-
scribes the syntactic scope expansion component,
which is based on a novel approach that learns open
pattern templates. These are relation-independent
dependency parse-tree patterns that are automati-
cally learned using a novel bootstrapped training set.
Section 4 discusses the context analysis component,
which is based on supervised training with linguistic
and lexical features.
Section 5 compares OLLIE with REVERB and
WOEparse on a dataset from three domains: News,
Wikipedia, and a Biology textbook. We find that
OLLIE obtains 2.7 times the area in precision-yield
curves (AUC) as REVERB and 1.9 times the AUC
as WOEparse. Moreover, for specific relations com-
monly mediated by nouns (e.g., ?is the president
of?) OLLIE obtains two order of magnitude higher
yield. We also compare OLLIE to a state-of-the-art
SRL system (Johansson and Nugues, 2008) on an
IE-related end task and find that they both have com-
parable performance at argument identification and
have complimentary strengths in sentence analysis.
In Section 6 we discuss related work on pattern-
based relation extraction.
2 Background
Open IE systems extract tuples consisting of argu-
ment phrases from the input sentence and a phrase
1Available for download at http://openie.cs.washington.edu
from the sentence that expresses a relation between
the arguments, in the format (arg1; rel; arg2). This is
done without a pre-specified set of relations and with
no domain-specific knowledge engineering. We
compare OLLIE to two state-of-the-art Open IE sys-
tems: (1) REVERB (Fader et al2011), which
uses shallow syntactic processing to identify rela-
tion phrases that begin with a verb and occur be-
tween the argument phrases;2 (2) WOEparse (Wu
and Weld, 2010), which uses bootstrapping from en-
tries in Wikipedia info-boxes to learn extraction pat-
terns in dependency parses. Like REVERB, the
relation phrases begin with verbs, but can handle
long-range dependencies and relation phrases that
do not come between the arguments. Unlike RE-
VERB, WOE does not include nouns within the re-
lation phrases (e.g., cannot represent ?is the presi-
dent of? relation phrase). Both systems ignore con-
text around the extracted relations that may indi-
cate whether it is a supposition or conditionally true
rather than asserted as factual (see #4-5 in Figure 1).
The task of Semantic role labeling is to identify
arguments of verbs in a sentence, and then to clas-
sify the arguments by mapping the verb to a se-
mantic frame and mapping the argument phrases to
roles in that frame, such as agent, patient, instru-
ment, or benefactive. SRL systems can also identify
and classify arguments of relations that are mediated
by nouns when trained on NomBank annotations.
Where SRL begins with a verb or noun and then
looks for arguments that play roles with respect to
that verb or noun, Open IE looks for a phrase that ex-
presses a relation between a pair of arguments. That
phrase is often more than simply a single verb, such
as the phrase ?plays a role in?, or ?is the CEO of?.
3 Relational Extraction in OLLIE
Figure 2 illustrates OLLIE?s architecture for learning
and applying binary extraction patterns. First, it uses
a set of high precision seed tuples from REVERB to
bootstrap a large training set. Second, it learns open
pattern templates over this training set. Next, OLLIE
applies these pattern templates at extraction time.
This section describes these three steps in detail. Fi-
nally, OLLIE analyzes the context around the tuple
(Section 4) to add information (attribution, clausal
modifiers) and a confidence function.
2Available for download at http://reverb.cs.washington.edu/
524
ReVerb 
Seed Tuples 
Training Data 
Open Pattern Learning Bootstrapper 
Pattern Templates 
Pattern Matching Context Analysis Sentence Tuples Ext. Tuples 
Extraction 
Learning 
Figure 2: System architecture: OLLIE begins with seed
tuples from REVERB, uses them to build a bootstrap
training set, and learns open pattern templates. These are
applied to individual sentences at extraction time.
3.1 Constructing a Bootstrapping Set
Our goal is to automatically create a large training
set, which encapsulates the multitudes of ways in
which information is expressed in text. The key ob-
servation is that almost every relation can also be ex-
pressed via a REVERB-style verb-based expression.
So, bootstrapping sentences based on REVERB?s tu-
ples will likely capture all relation expressions.
We start with over 110,000 seed tuples ? these are
high confidence REVERB extractions from a large
Web corpus (ClueWeb)3 that are asserted at least
twice and contain only proper nouns in the argu-
ments. These restrictions reduce ambiguity while
still covering a broad range of relations. For ex-
ample, a seed tuple may be (Paul Annacone; is the
coach of; Federer) that REVERB extracts from the
sentence ?Paul Annacone is the coach of Federer.?
For each seed tuple, we retrieve all sentences in a
Web corpus that contains all content words in the
tuple. We obtain a total of 18 million sentences.
For our example, we will retrieve all sentences that
contain ?Federer?, ?Paul?, ?Annacone? and some syn-
tactic variation of ?coach?. We may find sentences
like ?Now coached by Annacone, Federer is win-
ning more titles than ever.?
Our bootstrapping hypothesis assumes that all
these sentences express the information of the orig-
inal seed tuple. This hypothesis is not always true.
As an example, for a seed tuple (Boyle; is born in;
Ireland) we may retrieve a sentence ?Felix G. Whar-
ton was born in Donegal, in the northwest of Ireland,
a county where the Boyles did their schooling.?
3http://lemurproject.org/clueweb09.php/
To reduce bootstrapping errors we enforce addi-
tional dependency restrictions on the sentences. We
only allow sentences where the content words from
arguments and relation can be linked to each other
via a linear path of size four in the dependency parse.
To implement this restriction, we only use the sub-
set of content words that are headwords in the parse
tree. In the above sentence ?Ireland?, ?Boyle? and
?born? connect via a dependency path of length six,
and hence this sentence is rejected from the training
set. This reduces our set to 4 million (seed tuple,
sentence) pairs.
In our implementation, we use Malt Dependency
Parser (Nivre and Nilsson, 2004) for dependency
parsing, since it is fast and hence, easily applica-
ble to a large corpus of sentences. We post-process
the parses using Stanford?s CCprocessed algorithm,
which compacts the parse structure for easier extrac-
tion (de Marneffe et al2006).
We randomly sampled 100 sentences from our
bootstrapping set and found that 90 of them sat-
isfy our bootstrapping hypothesis (64 without de-
pendency constraints). We find this quality to be sat-
isfactory for our needs of learning general patterns.
Bootstrapped data has been previously used to
generate positive training data for IE (Hoffmann et
al., 2010; Mintz et al2009). However, previous
systems retrieved sentences that only matched the
two arguments, which is error-prone, since multiple
relations can hold between a pair of entities (e.g.,
Bill Gates is the CEO of, a co-founder of, and has a
high stake in Microsoft).
Alternatively, researchers have developed sophis-
ticated probabilistic models to alleviate the effect
of noisy data (Riedel et al2010; Hoffmann et al
2011). In our case, by enforcing that a sentence ad-
ditionally contains some syntactic form of the rela-
tion content words, our bootstrapping set is naturally
much cleaner.
Moreover, this form of bootstrapping is better
suited for Open IE?s needs, as we will use this data
to generalize to other unseen relations. Since the
relation words in the sentence and seed match, we
can learn general pattern templates that may apply
to other relations too. We discuss this process next.
3.2 Open Pattern Learning
OLLIE?s next step is to learn general patterns that
encode various ways of expressing relations. OL-
525
Extraction Template Open Pattern
1. (arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass? {rel:postag=VBN} ?{prep ?}? {arg2}
2. (arg1; {rel}; arg2) {arg1} ?nsubj? {rel:postag=VBD} ?dobj? {arg2}
3. (arg1; be {rel} by; arg2) {arg1} ?nsubjpass? {rel:postag=VBN} ?agent? {arg2}
4. (arg1; be {rel} of; arg2) {rel:postag=NN;type=Person} ?nn? {arg1} ?nn? {arg2}
5. (arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass? {slot:postag=VBN;lex ?announce|name|choose...}
?dobj? {rel:postag=NN} ?{prep ?}? {arg2}
Figure 3: Sample open pattern templates. Notice that some patterns (1-3) are purely syntactic, and others are seman-
tic/lexically constrained (in bold font). A dependency parse that matches pattern #1 is shown in Figure 4.
LIE learns open pattern templates ? a mapping from
a dependency path to an open extraction, i.e., one
that identifies both the arguments and the exact
(REVERB-style) relation phrase. Figure 3 gives ex-
amples of high-frequency pattern templates learned
by OLLIE. Note that some of the dependency
paths are completely unlexicalized (#1-3), whereas
in other cases some nodes have lexical or semantic
restrictions (#4, 5).
Open pattern templates encode the ways in
which a relation (in the first column) may
be expressed in a sentence (second column).
For example, a relation (Godse; kill; Gandhi)
may be expressed with a dependency path (#2)
{Godse}?nsubj?{kill:postag=VBD}?dobj?{Gandhi}.
To learn the pattern templates, we first extract the
dependency path connecting the arguments and re-
lation words for each seed tuple and the associated
sentence. We annotate the relation node in the path
with the exact relation word (as a lexical constraint)
and the POS (postag constraint). We create a re-
lation template from the seed tuple by normalizing
?is?/?was?/?will be? to ?be?, and replacing the rela-
tion content word with {rel}.4
If the dependency path has a node that is not part
of the seed tuple, we call it a slot node. Intuitively,
if slot words do not negate the tuple they can be
skipped over. As an example, ?hired? is a slot word
for the tuple (Annacone; is the coach of; Federer) in
the sentence ?Federer hired Annacone as a coach?.
We associate postag and lexical constraints with the
slot node as well. (see #5 in Figure 3).
Next, we perform several syntactic checks on
each candidate pattern. These checks are the con-
straints that we found to hold in very general pat-
terns, which we can safely generalize to other un-
seen relations. The checks are: (1) There are no slot
4Our current implementation only allows a single relation
content word; extending to multiple words is straightforward ?
the templates will require rel1, rel2,. . .
nodes in the path. (2) The relation node is in the
middle of arg1 and arg2. (3) The preposition edge
(if any) in the pattern matches the preposition in the
relation. (4) The path has no nn or amod edges.
If the checks hold true we accept it as a purely
syntactic pattern with no lexical constraints. Oth-
ers are semantic/lexical patterns and require further
constraints to be reliable as extraction patterns.
3.2.1 Purely Syntactic Patterns
For syntactic patterns, we aggressively general-
ize to unseen relations and prepositions. We remove
all lexical restrictions from the relation nodes. We
convert all preposition edges to an abstract {prep ?}
edge. We also replace the specific prepositions in
extraction templates with {prep}.
As an example, consider the sentences, ?Michael
Webb appeared on Oprah...? and ?...when Alexan-
der the Great advanced to Babylon.? and associ-
ated seed tuples (Michael Webb; appear on; Oprah)
and (Alexander; advance to; Babylon). Both these
data points return the same open pattern after gen-
eralization: ?{arg1} ?nsubj? {rel:postag=VBD}
?{prep ?}? {arg2}? with the extraction template
(arg1, {rel} {prep}, arg2). Other examples of syn-
tactic pattern templates are #1-3 in Figure 3.
3.2.2 Semantic/Lexical Patterns
Patterns that do not satisfy the checks are not as
general as those that do, but are still important. Con-
structions like ?Microsoft co-founder Bill Gates...?
work for some relation words (e.g., founder, CEO,
director, president, etc.) but would not work for
other nouns; for instance, from ?Chicago Symphony
Orchestra? we should not conclude that (Orchestra;
is the Symphony of; Chicago).
Similarly, we may conclude (Annacone; is the
coach of; Federer) from the sentence ?Federer hired
Annacone as a coach.?, but this depends on the se-
mantics of the slot word, ?hired?. If we replaced
526
?hired? by ?fired? or ?considered? then the extraction
would be false.
To enable such patterns we retain the lexical con-
straints on the relation words and slot words.5 We
collect all patterns together based only on the syn-
tactic restrictions and convert the lexical constraint
into a list of words with which the pattern was seen.
Example #5 in Figure 3 shows one such lexical list.
Can we generalize these lexically-annotated pat-
terns further? Our insight is that we can generalize
a list of lexical items to other similar words. For
example, if we see a list like {CEO, director, presi-
dent, founder}, then we should be able to generalize
to ?chairman? or ?minister?.
Several ways to compute semantically similar
words have been suggested in the literature like
Wordnet-based, distributional similarity, etc. (e.g.,
(Resnik, 1996; Dagan et al1999; Ritter et al
2010)). For our proof of concept, we use a simple
overlap metric with two important Wordnet classes
? Person and Location. We generalize to these types
when our list has a high overlap (> 75%) with hy-
ponyms of these classes. If not, we simply retain the
original lexical list without generalization. Example
#4 in Figure 3 is a type-generalized pattern.
We combine all syntactic and semantic patterns
and sort in descending order based on frequency of
occurrence in the training set. This imposes a natural
ranking on the patterns ? more frequent patterns are
likely to give higher precision extractions.
3.3 Pattern Matching for Extraction
We now describe how these open patterns are used
to extract binary relations from a new sentence. We
first match the open patterns with the dependency
parse of the sentence and identify the base nodes for
arguments and relations. We then expand these to
convey all the information relevant to the extraction.
As an example, consider the sentence: ?I learned
that the 2012 Sasquatch music festival is scheduled
for May 25th until May 28th.? Figure 4 illustrates the
dependency parse. To apply pattern #1 from Figure
3 we first match arg1 to ?festival?, rel to ?scheduled?
and arg2 to ?25th? with prep ?for?. However, (festi-
val, be scheduled for, 25th) is not a very meaningful
extraction. We need to expand this further.
5For highest precision extractions, we may also need seman-
tic constraints on the arguments. In this work, we increase our
yield by ignoring the argument-type constraints.
learned_VBD 
I_PRP scheduled_VBN 
that_IN festival_NN is_VBZ 25th_NNP 28th_NNP 
the_DET Sasquatch_NNP music_NN May_NNP_11 May_NNP_14 2012_CD 
nsubj ccomp 
complm nsubjpass auxpass prep_for prep_until 
det num nn nn nn nn 
Figure 4: A sample dependency parse. The col-
ored/greyed nodes represent all words that are extracted
from the pattern {arg1} ?nsubjpass? {rel:postag=VBN}
?{prep ?}? {arg2}. The extraction is (the 2012
Sasquatch Music Festival; is scheduled for; May 25th).
For the arguments we expand on amod, nn, det,
neg, prep of, num, quantmod edges to build the
noun-phrase. When the base noun is not a proper
noun, we also expand on rcmod, infmod, partmod,
ref, prepc of edges, since these are relative clauses
that convey important information. For relation
phrases, we expand on advmod, mod, aux, auxpass,
cop, prt edges. We also include dobj and iobj in the
case that they are not in an argument. After identi-
fying the words in arg/relation we choose their order
as in the original sentence. For example, these rules
will result in the extraction (the Sasquatch music fes-
tival; be scheduled for; May 25th).
3.4 Comparison with WOEparse
OLLIE?s algorithm is similar to that of WOEparse
? both systems follow the basic structure of boot-
strap learning of patterns based on dependency parse
paths. However, there are three significant differ-
ences. WOE uses Wikipedia-based bootstrapping,
finding a sentence in a Wikipedia article that con-
tains the infobox values. Since WOE does not have
access to a seed relation phrase, it heuristically as-
signs all intervening words between the arguments
in the parse as the relation phrase. This often results
in under-specified or nonsensical relation phrases.
For example, from the sentence ?David Miscavige
learned that after Tom Cruise divorced Mimi Rogers,
he was pursuing Nicole Kidman.? WOE?s heuristics
will extract the relation divorced was pursuing be-
tween ?Tom Cruise? and ?Nicole Kidman?. OLLIE,
in contrast, produces well-formed relation phrases
by basing its templates on REVERB relation phrases.
Secondly, WOE does not assign semantic/lexical
restrictions to its patterns, and thus, has lower preci-
sion due to aggressive syntactic generalization. Fi-
nally, WOE is designed to have verb-mediated rela-
527
tion phrases that do not include nouns, thus missing
important relations such as ?is the president of?. In
our experiments (see Figure 5) we find WOEparse to
have lower precision and yield than OLLIE.
4 Context Analysis in OLLIE
We now turn to the context analysis component,
which handles the problem of extractions that are not
asserted as factual in the text. In some cases, OLLIE
can handle this by extending the tuple representation
with an extra field that turns an otherwise incorrect
tuple into a correct one. In other cases, there is no re-
liable way to salvage the extraction, and OLLIE can
avoid an error by giving the tuple a low confidence.
Cases where OLLIE extends the tuple representa-
tion include conditional truth and attribution. Con-
sider sentence #4 in Figure 1. It is not asserting that
the earth is the center of the universe. OLLIE adds
an AttributedTo field, which makes the final extrac-
tion valid (see OLLIE extraction in Figure 1). This
field indicates who said, suggested, believes, hopes,
or doubts the information in the main extraction.
Another case is when the extraction is only condi-
tionally true. Sentence #5 in Figure 1 does not assert
as factual that (Romney; will be elected; President),
so it is an incorrect extraction. However, adding
a condition (?if he wins five states?) can turn this
into a correct extraction. We extend OLLIE to have
a ClausalModifier field when there is a dependent
clause that modifies the main extraction.
Our approach for extracting these additional fields
makes use of the dependency parse structure. We
find that attributions are marked by a ccomp (clausal
complement) edge. For example, in the parse of sen-
tence #4 there is a ccomp edge between ?believe?
and ?center?. Our algorithm first checks for the pres-
ence of a ccomp edge to the relation node. However,
not all ccomp edges are attributions. We match the
context verb (e.g., ?believe?) with a list of commu-
nication and cognition verbs from VerbNet (Schuler,
2006) to detect attributions. The context verb and its
subject then populate the AttributedTo field.
Similarly, the clausal modifiers are marked by ad-
vcl (adverbial clause) edge. We filter these lexically,
and add a ClausalModifier field when the first word
of the clause matches a list of 16 terms created using
a training set: {if, when, although, because, ...}.
OLLIE has high precision for AttributedTo and
ClausalModifier fields, nearly 98% on a develop-
ment set, however, these two fields do not cover all
the cases where an extraction is not asserted as fac-
tual. To handle others, we train OLLIE?s confidence
function to reduce the confidence of an extraction if
its context indicates it is likely to be non-factual.
We use a supervised logistic regression classifier
for the confidence function. Features include the
frequency of the extraction pattern, the presence of
AttributedTo or ClausalModifier fields, and the po-
sition of certain words in the extraction?s context,
such as function words or the communication and
cognition verbs used for the AttributedTo field. For
example, one highly predictive feature tests whether
or not the word ?if? comes before the extraction
when no ClausalModifier fields are attached. Our
training set was 1000 extractions drawn evenly from
Wikipedia, News, and Biology sentences.
5 Experiments
Our experiments evaluate three main questions. (1)
How does OLLIE?s performance compare with exis-
ting state-of-the-art open extractors? (2) What are
the contributions of the different sub-components
within OLLIE? (3) How do OLLIE?s extractions com-
pare with semantic role labeling argument identifi-
cation?
5.1 Comparison of Open IE Systems
Since Open IE is designed to handle a variety of
domains, we create a dataset of 300 random sen-
tences from three sources: News, Wikipedia and Bi-
ology textbook. The News and Wikipedia test sets
are a random subset of Wu and Weld?s test set for
WOEparse. We ran three systems, OLLIE, REVERB
and WOEparse on this dataset resulting in a total of
1,945 extractions from all three systems. Two an-
notators tagged the extractions as correct if the sen-
tence asserted or implied that the relation was true.
Inter-annotator agreement was 0.96, and we retained
the subset of extractions on which the two annotators
agree for further analysis.
All systems associate a confidence value with an
extraction ? ranking with these confidence values
generates a precision-yield curve for this dataset.
Figure 5 reports the curves for the three systems.
We find that OLLIE has a higher performance, ow-
ing primarily to its higher yield at comparable preci-
528
0.5  
0.6  
0.7  
0.8  
0.9  
1  
0  100  200  300  400  500  600  
OLLIE  
ReVerb 
WOE  
Yield 
Pre
cis
ion
 
parse 
Figure 5: Comparison of different Open IE systems. OL-
LIE achieves substantially larger area under the curve
than other Open IE systems.
sion. OLLIE finds 4.4 times more correct extractions
than REVERB and 4.8 times more than WOEparse at
a precision of about 0.75. Overall, OLLIE has 2.7
times larger area under the curve than REVERB and
1.9 times larger than WOEparse.6 We use the Boot-
strap test (Cohen, 1995) to find that OLLIE?s better
performance compared to the two systems is highly
statistically significant.
We perform further analysis to understand the rea-
sons behind the high yield from OLLIE. We find that
40% of the OLLIE extractions that REVERB misses
are due to OLLIE?s use of parsers ? REVERB misses
those because its shallow syntactic analysis cannot
skip over the intervening clauses or prepositional
phrases between the relation phrase and the argu-
ments. About 30% of the additional yield is those
extractions where the relation is not between its ar-
guments (see instance #1 in Figure 1). The rest are
due to other causes such as OLLIE?s ability to handle
relationships mediated by nouns and adjectives, or
REVERB?s shallow syntactic analysis, etc. In con-
trast, OLLIE misses very few extractions returned by
REVERB, mostly due to parser errors.
We find that WOEparse misses extractions found
by OLLIE for a variety of reasons. The primary
cause is that WOEparse does not include nouns in re-
lation phrases. It also misses some verb-based pat-
terns, probably due to training noise. In other cases,
WOEparse misses extractions due to ill-formed rela-
tion phrases (as in the example of Section 3.4: ?di-
vorced was pursuing? instead of the correct relation
?was pursuing?).
While the bulk of OLLIE?s extractions in our test
6Evaluating recall is difficult at this scale ? however, since
yield is proportional to recall, the area differences also hold for
the equivalent precision-recall curves.
Relation OLLIE REVERB incr.
is capital of 8,566 146 59x
is president of 21,306 1,970 11x
is professor at 8,334 400 21x
is scientist of 730 5 146x
Figure 6: OLLIE finds many more correct extractions for
relations that are typically expressed by noun phrases ?
up to 146 times that of REVERB. WOEparse outputs no
instances of these, because it does not allow nouns in the
relation. These results are at point of maximum yield
(with comparable precisions around 0.66).
set were verb-mediated, our intuition suggests that
there exist many relationships that are most natu-
rally expressed via noun phrases. To demonstrate
this effect, we chose four such relations ? is capi-
tal of, is president of, is professor at, and is scientist
of. We ran our systems on 100 million random sen-
tences from the ClueWeb corpus. Figure 6 reports
the yields of these four relations.7
OLLIE found up to 146 times as many extrac-
tions for these relations than REVERB. Because
WOEparse does not include nouns in relation phrases,
it is unable to extract any instance of these relations.
We examine a sample of the extractions to verify that
noun-mediated extractions are the main reason for
this large yield boost over REVERB (73% of OLLIE
extractions were noun-mediated). High-frequency
noun patterns like ?Obama, the president of the US?,
?Obama, the US president?, ?US President Obama?
far outnumber sentences of the form ?Obama is the
president of the US?. These relations are seldom the
primary information in a sentence, and are typically
mentioned in passing in noun phrases that express
the relation.
For some applications, noun-mediated relations
are important, as they associate people with work
places and job titles. Overall, we think of the results
in Figure 6 as a ?best case analysis? that illustrates
the dramatic increase in yield for certain relations,
due to syntactic scope expansion in Open IE.
5.2 Analysis of OLLIE
We perform two control experiments to understand
the value of semantic/lexical restrictions in pattern
learning and precision boost due to context analysis
component.
7We multiply the total number of extractions with precision
on a sample for that relation to estimate the yield.
529
0  
0.2  
0.4  
0.6  
0.8  
1  
0  10  20  30  40  50  60  
OLLIE  
OLLIE[Lex]  
OLLIE[syn]  
Yield 
Pre
cis
ion
 
Figure 7: Results on the subset of extractions from pat-
terns with semantic/lexical restrictions. Ablation study
on patterns with semantic/lexical restrictions. These pat-
terns without restrictions (OLLIE[syn]) result in low pre-
cision. Type generalization improves yield compared to
patterns with only lexical constraints (OLLIE[lex]).
Are semantic restrictions important for open pat-
tern learning? How much does type generalization
help? To answer these questions we compare three
systems ? OLLIE without semantic or lexical restric-
tions (OLLIE[syn]), OLLIE with lexical restrictions
but no type generalization (OLLIE[lex]) and the full
system (OLLIE). We restrict this experiment to the
patterns where OLLIE adds semantic/lexical restric-
tions, rather than dilute the result with patterns that
would be unchanged by these variants.
Figure 7 shows the results of this experiment on
our dataset from three domains. As the curves
show, OLLIE was correct to add lexical/semantic
constraints to these patterns ? precision is quite low
without the restrictions. This matches our intuition,
since these are not completely general patterns and
generalizing to all unseen relations results in a large
number of errors. OLLIE[lex] performs well though
at lower yield. The type generalization helps the
yield somewhat, without hurting the precision. We
believe that a more data-driven type generalization
that uses distributional similarity (e.g., (Ritter et al
2010)) may help much more. Also, notice that over-
all precision numbers are lower, since these are the
more difficult relations to extract reliably. We con-
clude that lexical/semantic restrictions are valuable
for good performance of OLLIE.
We also compare our full system to a version that
does not use the context analysis of Section 4. Fig-
ure 8 compares OLLIE to a version (OLLIE[pat]) that
does not add the AttributedTo and ClausalModifier
fields, and, instead of context-sensitive confidence
function, uses the pattern frequency in the training
0.5  
0.6  
0.7  
0.8  
0.9  
1  
0  100  200  300  400  500  600  
OLLIE  
OLLIE[pat]  
Yield 
Pre
cis
ion
 
Figure 8: Context analysis increases precision, raising the
area under the curve by 19%.
set as a ranking function. 10% of the sentences have
an OLLIE extraction with ClausalModifier and 6%
have AttributedTo fields. Adding ClausalModifier
corrects errors for 21% of extractions that have a
ClausalModifier and does not introduce any new er-
rors. Adding AttributedTo corrects errors for 55%
of the extractions with AttributedTo and introduces
an error for 3% of the extractions. Overall, we find
that OLLIE gives a significant boost to precision over
OLLIE[pat] and obtains 19% additional AUC.
Finally, we analyze the errors made by OLLIE.
Unsurprisingly, because of OLLIE?s heavy reliance
on the parsers, parser errors account for a large part
of OLLIE?s errors (32%). 18% of the errors are due
to aggressive generalization of a pattern to all un-
seen relations and 12% due to incorrect application
of lexically annotated patterns. About 14% of the er-
rors are due to important context missed by OLLIE.
Another 12% of the errors are because of the limita-
tions of binary representation, which misses impor-
tant information that can only be expressed in n-ary
tuples.
We believe that as parsers become more robust
OLLIE?s performance will improve even further. The
presence of context-related errors suggests that there
is more to investigate in context analysis. Finally, in
the future we wish to extend the representation to
include n-ary extractions.
5.3 Comparison with SRL
Our final evaluation suggests answers to two im-
portant questions. First, how does a state-of-the-art
Open IE system do in terms of absolute recall? Sec-
ond, how do Open IE systems compare against state-
of-the-art SRL systems?
SRL, as discussed in Section 2, has a very dif-
ferent goal ? analyzing verbs and nouns to identify
530
their arguments, then mapping the verb or noun to
a semantic frame and determining the role that each
argument plays in that frame. These verbs and nouns
need not make the full relation phrase, although, re-
cent work has shown that they may be converted
to Open IE style extractions with additional post-
processing (Christensen et al2011).
While a direct comparison between OLLIE and
a full SRL system is problematic, we can compare
performance of OLLIE and the argument identifica-
tion step of an SRL system. We set each system the
following task ? ?based on a sentence, find all noun-
pairs that have an asserted relationship.? This task is
permissive for both systems, as it does not require
finding an exact relation phrase or argument bound-
ary, or determining the argument roles in a relation.
We create a gold standard by tagging a random
50 sentences of our test set to identify all pairs of
NPs that have an asserted relation. We only counted
relation expressed by a verb or noun in the text, and
did not include relations expressed simply with ?of?
or apostrophe-s. Where a verb mediates between an
argument and multiple NPs, we represent this as a
binary relation for all pairs of NPs.
For example the sentence, ?Macromolecules
translocated through the phloem include proteins
and various types of RNA that enter the sieve tubes
through plasmodesmata.? has five binary relations.
arg1: arg2: relation term
Macromolecules phloem translocated
Macromolecules proteins include
Macromolecules types of RNA include
types of RNA sieve tubes enter
types of RNA plasmodesmata enter
We find an average of 4.0 verb-mediated relations
and 0.3 noun-mediated relations per sentence. Eval-
uating OLLIE against this gold standard helps to an-
swer the question of absolute recall: what percent-
age of binary relations expressed in a sentence can
our systems identify.
For comparison, we use a state-of-the-art SRL
system from Lund University (Johansson and
Nugues, 2008), which is trained on PropBank
(Martha and Palmer, 2002) for its verb-frames and
NomBank (Meyers et al2004) for noun-frames.
The PropBank version of the system won the very
competitive 2008 CONLL SRL evaluation.
We conduct this experiment by manually compar-
LUND OLLIE union
Verb relations 0.58 (0.69) 0.49 (0.55) 0.71 (0.83)
Noun relations 0.07 (0.33) 0.13 (0.13) 0.20 (0.33)
All relations 0.54 (0.67) 0.47 (0.52) 0.67 (0.80)
Figure 9: Recall of LUND and OLLIE on binary relations.
In parentheses is recall with oracle co-reference. Both
systems identify approximately half of all argument pairs,
but have lower recall on noun-mediated relations.
ing the outputs of LUND and OLLIE against the gold
standard. For each pair of NPs in the gold standard
we determine whether the systems find a relation
with that pair of NPs as arguments. Recall is based
on the percentage of NP pairs where the head nouns
matches head nouns of two different arguments in an
extraction or semantic frame. If the argument value
is conjunctive, we count a match against the head
noun of each item in the list. We also count cases
where system output would match the gold standard,
given perfect co-reference.
Figure 9 shows the recall for OLLIE and LUND,
with recall based on oracle co-referential matches
in parentheses. Our analysis shows strong recall
for both systems for verb-mediated relations: LUND
finding about two thirds of the argument pairs and
OLLIE finding over half. Both systems have low
recall for noun-mediated relations, with most of
LUND?s recall requiring co-reference. We observe
that a union of the two systems raises recall to
0.71 for verb-mediated relations and 0.83 with co-
reference, demonstrating that each system is identi-
fying argument pairs that the other missed.
It is not surprising that OLLIE has recall of ap-
proximately 0.5, since it is tuned for high precision
extraction, and avoids less reliable extractions from
constructions such as reduced relative clauses and
gerunds, or from noun-mediated relations with long-
range dependencies. In contrast, SRL is tuned to
identify the argument structure for nearly all verbs
and nouns in a sentence. The missing recall from
SRL is primarily where it does not identify both ar-
guments of a binary relation, or where the correct
argument is buried in a long argument phrase, but is
not its head noun.
It is surprising that LUND, trained on Nom-
Bank, identifies so few noun-mediated argument
pairs without co-reference. An example will make
this clear. For the sentence, ?Clarcor, a maker of
packaging and filtration products, said ...?, the tar-
531
get relation is between Clarcor and the products it
makes. LUND identifies a frame maker.01 in which
argument A0 has head noun ?maker? and A1 is a PP
headed by ?products?, missing the actual name of the
maker without co-reference post-processing. OLLIE
finds the extraction (Clarcor; be a maker of; packag-
ing and filtration products) where the heads of both
arguments matched those of the target. In another
example, LUND identifies ?his? and ?brother? as the
arguments of the frame brother.01, rather than the
actual names of the two brothers.
We can draw several conclusions from this exper-
iment. First, nouns, although less frequently mediat-
ing relations, are much harder and both systems are
failing significantly on those ? OLLIE is somewhat
better. Two, neither systems dominates the other;
in fact, recall is increased significantly by a union
of the two. Three, and probably most importantly,
significant information is still being missed by both
systems, and more research is warranted.
6 Related Work
There is a long history of bootstrapping and pat-
tern learning approaches in traditional informa-
tion extraction, e.g., DIPRE (Brin, 1998), Snow-
Ball (Agichtein and Gravano, 2000), Espresso (Pan-
tel and Pennacchiotti, 2006), PORE (Wang et al
2007), SOFIE (Suchanek et al2009), NELL (Carl-
son et al2010), and PROSPERA (Nakashole et
al., 2011). All these approaches first bootstrap data
based on seed instances of a relation (or seed data
from existing resources such as Wikipedia) and then
learn lexical or lexico-POS patterns to create an ex-
tractor. Other approaches have extended these to
learning patterns based on full syntactic analysis of
a sentence (Bunescu and Mooney, 2005; Suchanek
et al2006; Zhao and Grishman, 2005).
OLLIE has significant differences from the previ-
ous work in pattern learning. First, and most impor-
tantly, these previous systems learn an extractor for
each relation of interest, whereas OLLIE is an open
extractor. OLLIE?s strength is its ability to gener-
alize from one relation to many other relations that
are expressed in similar forms. This happens both
via syntactic generalization and type generalization
of relation words (sections 3.2.1 and 3.2.2). This ca-
pability is essential as many relations in the test set
are not even seen in the training set ? in early exper-
iments we found that non-generalized pattern learn-
ing (equivalent to traditional IE) had significantly
less yield at a slightly higher precision.
Secondly, previous systems begin with seeds that
consist of a pair of entities, whereas we also in-
clude the content words from REVERB relations in
our training seeds. This results in a much higher
precision bootstrapping set and high rule preci-
sion while still allowing morphological variants that
cover noun-mediated relations. A third difference is
in the scale of the training ? REVERB yields millions
of training seeds, where previous systems had orders
of magnitude less. This enables OLLIE to learn pat-
terns with greater coverage.
The closest to our work is the pattern learning
based open extractor WOEparse. Section 3.4 de-
tails the differences between the two extractors. An-
other extractor, StatSnowBall (Zhu et al2009), has
an Open IE version, which learns general but shal-
low patterns. Preemptive IE (Shinyama and Sekine,
2006) is a paradigm related to Open IE that first
groups documents based on pairwise vector cluster-
ing, then applies additional clustering to group en-
tities based on document clusters. The clustering
steps make it difficult for it to scale to large corpora.
7 Conclusions
Our work describes OLLIE, a novel Open IE ex-
tractor that makes two significant advances over
the existing Open IE systems. First, it expands
the syntactic scope of Open IE systems by identi-
fying relationships mediated by nouns and adjec-
tives. Our experiments found that for some rela-
tions this increases the number of correct extrac-
tions by two orders of magnitude. Second, by an-
alyzing the context around an extraction, OLLIE is
able to identify cases where the relation is not as-
serted as factual, but is hypothetical or conditionally
true. OLLIE increases precision by reducing con-
fidence in those extractions or by associating addi-
tional context in the extractions, in the form of at-
tribution and clausal modifiers. Overall, OLLIE ob-
tains 1.9 to 2.7 times more area under precision-
yield curves compared to existing state-of-the-art
open extractors. OLLIE is available for download at
http://openie.cs.washington.edu.
532
Acknowledgments
This research was supported in part by NSF grant IIS-0803481,
ONR grant N00014-08-1-0431, DARPA contract FA8750-09-
C-0179 and the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Air Force Research Laboratory (AFRL) con-
tract number FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or im-
plied, of IARPA, AFRL, or the U.S. Government. This research
is carried out at the University of Washington?s Turing Center.
We thank Fei Wu and Dan Weld for providing WOE?s code
and Anthony Fader for releasing REVERB?s code. Peter Clark,
Alan Ritter, and Luke Zettlemoyer provided valuable feedback
on the research and Dipanjan Das helped us with state-of-the-
art SRL systems. We also thank the anonymous reviewers for
their comments on an earlier draft.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Procs. of the Fifth ACM International Conference on
Digital Libraries.
ARPA. 1991. Proc. 3rd Message Understanding Conf.
Morgan Kaufmann.
ARPA. 1998. Proc. 7th Message Understanding Conf.
Morgan Kaufmann.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
S. Brin. 1998. Extracting Patterns and Relations from the
World Wide Web. In WebDB Workshop at 6th Interna-
tional Conference on Extending Database Technology,
EDBT?98, pages 172?183, Valencia, Spain.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. of HLT/EMNLP.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Procs. of AAAI.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the 6th International Conference on
Knowledge Capture (K-CAP ?11).
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Lan-
guage Resources and Evaluation (LREC 2006).
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open infor-
mation extraction: the second generation. In Proceed-
ings of the International Joint Conference on Artificial
Intelligence (IJCAI ?11).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 286?
295.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL, pages 541?550.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING 08),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC 02).
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ?09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003?1011.
Ndapandula Nakashole, Martin Theobald, and Gerhard
Weikum. 2011. Scalable knowledge harvesting with
high precision and high recall. In Proceedings of the
Fourth International Conference on Web Search and
Web Data Mining (WSDM 2011), pages 227?236.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
533
44th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?06).
P. Resnik. 1996. Selectional constraints: an information-
theoretic model and its computational realization.
Cognition.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In ECML/PKDD (3), pages 148?163.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?10).
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Procs. of KDD, pages 712?717.
Fabian M. Suchanek, Mauro Sozio, and Gerhard
Weikum. 2009. Sofie: a self-organizing framework
for information extraction. In Proceedings of WWW,
pages 631?640.
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC?07), pages 580?594.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ?10).
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Procs. of ACL.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW
?09: Proceedings of the 18th international conference
on World Wide Web, pages 101?110, New York, NY,
USA. ACM.
534
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1721?1731,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Generating Coherent Event Schemas at Scale
Niranjan Balasubramanian, Stephen Soderland, Mausam, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{niranjan,ssoderlan, mausam, etzioni}@cs.washington.edu
Abstract
Chambers and Jurafsky (2009) demonstrated
that event schemas can be automatically in-
duced from text corpora. However, our analy-
sis of their schemas identifies several weak-
nesses, e.g., some schemas lack a common
topic and distinct roles are incorrectly mixed
into a single actor. It is due in part to their
pair-wise representation that treats subject-
verb independently from verb-object. This of-
ten leads to subject-verb-object triples that are
not meaningful in the real-world.
We present a novel approach to inducing
open-domain event schemas that overcomes
these limitations. Our approach uses co-
occurrence statistics of semantically typed re-
lational triples, which we call Rel-grams (re-
lational n-grams). In a human evaluation, our
schemas outperform Chambers?s schemas by
wide margins on several evaluation criteria.
Both Rel-grams and event schemas are freely
available to the research community.
1 Introduction
Event schemas (also known as templates or frames)
have been widely used in information extraction.
An event schema is a set of actors (also known as
slots) that play different roles in an event, such as
the perpetrator, victim, and instrument in a bomb-
ing event. They provide essential guidance in ex-
tracting information related to events from free text
(Patwardhan and Riloff, 2009), and can also aid in
other NLP tasks, such as coreference (Irwin et al,
2011), summarization (Owczarzak and Dang, 2010),
and inference about temporal ordering and causality.
Actor Rel Actor
A1:<person> failed A2:test
A1:<person> was suspended for A3:<time period>
A1:<person> used A4:<substance, drug>
A1:<person> was suspended for A5:<game, activity>
A1:<person> was in A6:<location>
A1:<person> was suspended by A7:<org, person>
Actor Instances:
A1: {Murray, Morgan, Governor Bush, Martin, Nelson}
A2: {test}
A3: {season, year, week, month, night}
A4: {cocaine, drug, gasoline, vodka, sedative}
A5: {violation, game, abuse, misfeasance, riding}
A6: {desert, Simsbury, Albany, Damascus, Akron}
A7: {Fitch, NBA, Bud Selig, NFL, Gov Jeb Bush}
Table 1: An event schema produced by our system, rep-
resented as a set of (Actor,Rel, Actor) triples, and a set
of instances for each actor A1, A2, etc. For clarity we
show unstemmed verbs.
Until recently, all event schemas in use in NLP
were hand-engineered, e.g., the MUC templates and
ACE event relations (ARPA, 1991; ARPA, 1998;
Doddington et al, 2004). This led to technology that
could only focus on specific domains of interest and
has not been applicable more broadly.
The seminal work of Chambers and Jurafsky
(2009) showed that event schemas can also be in-
duced automatically from text corpora. Instead of
labeled roles these schemas have a set of relations
and actors that serve as arguments.1 Their system
is fully automatic, domain-independent, and scales
to large text corpora.
However, we identify several limitations in the
schemas produced by their system.2 Their schemas
1In the rest of this paper we use event schemas to refer to
these automatically induced schemas with actors and relations.
2Available at http://www.usna.edu/Users/cs/
nchamber/data/schemas/acl09
1721
Actor Rel Actor
A1 caused A2
A2 spread A1
A2 burned A1
- extinguished A1
A1 broke out -
- put out A1
Actor Instances:
A1: {fire, aids, infection, disease}
A2: {virus, bacteria, disease, urushiol, drug}
Table 2: An event schema from Chambers? system that
mixes the events of fire spreading and disease spreading.
often lack coherence: mixing unrelated events and
having actors whose entities do not play the same
role in the schema. Table 2 shows an event schema
from Chambers that mixes the events of fire spread-
ing and disease spreading.
Much of the incoherence of Chambers? schemas
can be traced to their representation that uses pairs
of elements from an assertion, thus, treating subject-
verb and verb-object separately. This often leads to
subject-verb-object triples that do not make sense in
the real world. For example, the assertions ?fire
caused virus? and ?bacteria burned AIDS? are im-
plicit in Table 2.
Another limitation in schemas Chambers released
is that they restrict schemas to two actors, which can
result in combining different actors. Table 4 shows
an example of combining perpeterators and victims
into a single actor.
1.1 Contributions
We present an event schema induction algorithm that
overcomes these weaknesses. Our basic represen-
tation is triples of the form (Arg1, Relation, Arg2),
extracted from a text corpus using Open Information
Extraction (Mausam et al, 2012). The use of triples
aids in agreement between subject and object of a
relation. The use of Open IE leads to more expres-
sive relation phrases (e.g., with prepositions). We
also assign semantic types to arguments, both to al-
leviate data sparsity and to produce coherent actors
for our schemas.
Table 1 shows an event schema generated by our
system. It has six relations and seven actors. The
schema makes several related assertions about a per-
son using a drug, failing a test, and getting sus-
pended. The main actors in the schema include the
person who failed the test, the drug used, and the
agent that suspended the person.
Our first step in creating event schemas is to tab-
ulate co-occurrence of tuples in a database that we
call Rel-grams (relational n-grams) (Sections 3, 5.1).
We then perform analysis on a graph induced from
the Rel-grams database and use this to create event
schemas (Section 4).
We compared our event schemas with those of
Chambers on several metrics including whether the
schema pertains to a coherent topic or event and
whether the actors play a coherent role in that event
(Section 5.2). Amazon Mechanical Turk workers
judged that our schemas have significantly better co-
herence ? 92% versus 82% have coherent topic and
81% versus 59% have coherent actors.
We release our open domain event schemas and
the Rel-grams database3 for further use by the NLP
community.
2 System Overview
Our approach to schema generation is based on the
idea that frequently co-occurring relations in text
capture relatedness of assertions about real-world
events. We begin by extracting a set of relational tu-
ples from a large text corpus and tabulate occurrence
of pairs of tuples in a database.
We then construct a graph from this database and
identify high-connectivity nodes (relational tuples)
in this graph as a starting point for constructing event
schemas. We use graph analysis to rank the tu-
ples and merge arguments to form the actors in the
schema.
3 Modeling Relational Co-occurrence
In order to tabulate pairwise occurences of relational
tuples we need a suitable relation-based represen-
tation. We now describe the extraction and rep-
resentation of relations, a database for storing co-
occurrence information, and our probabilistic model
for the co-occurrence. We call this model Rel-
grams, as it can be seen as a relational analog to the
n-grams language model.
3.1 Relations Extraction and Representation
We extract relational triples from each sentence in
a large corpus using the OLLIE Open IE system
3Available at http://relgrams.cs.washington.edu
1722
Tuples Table
Id Arg1 Rel Arg2 Count
... ... ... ... ...
13 bomb explode in <loc> 547
14 bomb explode in Baghdad 22
15 bomb explode in market 7
... ... ... ... ...
87 bomb kill <per> 173
... ... ... ... ...
92 <loc> be suburb of <loc> 1023
... ... ... ... ...
BigramCounts Table
T1 T2 Dist. Count E11 E12 E21 E22
... ... ... ... ... ... ... ...
13 87 1 27 25 0 0 0
13 87 2 35 33 0 0 0
... ... ... ... ... ... ... ...
13 87 10 62 59 0 0 0
87 13 1 6 0 0 0 0
... ... ... ... ... ... ... ...
92 13 1 12 0 0 12 0
... ... ... ... ... ... ... ...
Figure 1: Tables in the Rel-grams Database: Tuples maps tuples to unique identifiers, BigramCounts provides the
co-occurrence counts (Count) within various distances (Dist.), and four types of argument equality counts (E11-E22).
E11 is the number of times when T1.Arg1 = T2.Arg1, E12 is when T1.Arg1 = T2.Arg2 and so on.
(Mausam et al, 2012).4 This provides relational tu-
ples in the format (Arg1, Relation, Arg2) where each
tuple element is a phrase from the sentence. The
sentence ?He cited a new study that was released by
UCLA in 2008.? produces three tuples:
1. (He, cited, a new study)
2. (a new study, was released by, UCLA)
3. (a new study, was released in, 2008)
Relational triples provide a more specific repre-
sentation which is less ambiguous when compared
to (subj, verb) or (verb, obj) pairs. However, using
relational triples also increases sparsity. To reduce
sparsity and to improve generalization, we represent
the relation phrase by its stemmed head verb plus
any prepositions. The relation phrase may include
embedded nouns, in which case these are stemmed
as well. Moreover, tuple arguments are represented
as stemmed head nouns, and we also record seman-
tic types of the arguments.
We selected 29 semantic types from WordNet, ex-
amining the set of instances on a small development
set to ensure that the types are useful, but not overly
specific. The set of types are: person, organization,
location, time unit, number, amount, group, busi-
ness, executive, leader, effect, activity, game, sport,
device, equipment, structure, building, substance,
nutrient, drug, illness, organ, animal, bird, fish, art,
book, and publication.
To assign types to arguments, we apply Stanford
Named Entity Recognizer (Finkel et al, 2005)5, and
also look up the argument in WordNet 2.1 and record
4Available at: http://knowitall.github.io/
ollie/
5We used the system downloaded from: http://nlp.
stanford.edu/software/CRF-NER.shtml and used
the seven class CRF model distributed with it.
the first three senses if they map to our target se-
mantic types. We use regular expressions to recog-
nize dates and numeric expressions, and map per-
sonal pronouns to <person>. We associate all types
found by this mechanism with each argument. The
tuples in the example above are normalized to the
following:
1. (He, cite, study)
2. (He, cite, <activity>)
3. (<person>, cite, study)
4. (<person>, cite, <activity>)
5. (study, be release by, UCLA)
6. (study, be release by, <organization>)
7. (study, be release in, 2008)
8. (study, be release in, <time unit>)
9. (<activity>, be release by, UCLA)
...
In our preliminary experiments, we found that us-
ing normalized relation strings and semantic classes
for arguments results in a ten-fold increase in the
number of Rel-grams with a minimum support.
3.2 Co-occurrence Tabulation
We construct a database to hold co-occurrence
statistics for pairs of tuples found in each document.
Figure 1 shows examples for the types of statistics
contained in the database. The database consists
of two tables: 1) Tuples ? Maps each tuple to a
unique identifier and tabulates tuple counts. 2) Bi-
gramCounts ? Stores the directional co-occurrence
frequency, a count for tuple T followed by T ? at a
distance of k, and tabulates the number of times the
same argument was present in the pair of tuples.
Equality Constraints: Along with the co-
occurrence counts, we record the equality of argu-
ments in Rel-grams pairs. We assert an argument
1723
Table 3: Given a source tuple, the Rel-grams language
model estimates the probability of encountering other re-
lational tuples in a document. For clarity, we show the
unstemmed version.
Top tuples related to
(<person>, convicted of, murder)
1. (<person>, convicted in, <time unit>)
2. (<person>, sentenced to, death)
3. (<person>, sentenced to, year)
4. (<person>, convicted in, <location>)
5. (<person>, sentenced to, life)
6. (<person>, convicted in, <person>)
7. (<person>, convicted after, trial)
8. (<person>, sent to, prison)
pair is equal if they are from the same token se-
quence in the source sentence or one argument is a
co-referent mention of the other. We use the Stan-
ford Co-reference system (Lee et al, 2013)6 to de-
tect co-referring mentions. There are four possible
equalities depending on the specific pair of argu-
ments in the tuples are the same, shown as E11, E12,
E21 and E22 in Figure 1. For example, the E21 col-
umn has counts for the number of times the Arg2 of
T1 was determined to be the same as the Arg1 of T2.
Implementation and Query Language: We pop-
ulated the Rel-grams database using OLLIE extrac-
tions from a set of 1.8 Million New York Times arti-
cles drawn from the Gigaword corpus. The database
consisted of approximately 320K tuples that have
frequency ? 3 and 1.1M entries in the bigram table.
The Rel-grams database allows for powerful
querying using SQL. For example, Table 3 shows the
most frequent rel-grams associated with the query
tuple (<person>, convicted of, murder).
3.3 Rel-grams Language Model
From the tabulated co-occurrence statistics, we esti-
mate bi-gram conditional probabilities of tuples that
occur within a window of k tuples from each other.
Formally, we use Pk(T ?|T ) to denote the conditional
probability that T ? follows T within a window of k
tuples. To discount estimates from low-frequency
tuples, we use a ?-smoothed estimate:
6Available for download at: http://nlp.stanford.
edu/software/dcoref.shtml
Pk(T
?|T ) =
#(T, T ?, k) + ?
?
T ???V
#(T, T ??, k) + ? ? |V |
(1)
where, #(T, T ?, k) is the number of times T ? fol-
lows T within a window of k tuples. k = 1 in-
dicates adjacent tuples in the document. |V | is the
number of unique tuples in the corpus. For experi-
ments in this paper, we set ? to 0.05.
Co-occurrence within a small window is usu-
ally more reliable but is also sparse, whereas co-
occurrence within larger windows addresses sparsity
but may lead to topic drift. To leverage the bene-
fits of different window sizes, we also define a met-
ric with a weighted average of window sizes from 1
to 10, where the weight decays as window size in-
creases. For example, with ? set to 0.5 in equation
2, a window of k+1 has half the weight of a window
of k.
P (T ?|T ) =
?10
k=1 ?
kPk(T ?|T )
?10
k=1 ?
k
(2)
We believe that Rel-grams is a valuable source
of common-sense knowledge and may be useful for
several downstream tasks such as improving infor-
mation extractors, inference of implicit information,
etc. We assess its usefulness in the context of gener-
ating event schemas.
4 Schema Generation
We now use Rel-grams to identify relations and ac-
tors pertaining to a particular event. Our schema
generation consists of three steps. First, we build a
relation graph of tuples (G) using connections iden-
tified by Rel-grams. Second, we identify a set of
seed tuples as starting points for schemas. We use
graph analysis to find the tuples most related to each
seed. Finally, we merge the arguments in these tu-
ples to create actors and output the final schema.
Next we describe each of these steps in detail.
4.1 Rel-graph construction
We define a Rel-graph as an undirected weighted
graph G = (V,E), whose vertices (V ) are relation
tuples with edges (E), where an edge between ver-
tices T and T ? is weighted by the symmetric condi-
tional probability SCP (T, T ?) defined as
1724
SCP (T, T ?) = P (T |T ?)? P (T ?|T ) (3)
Both conditional probabilities are computed in
Equation 2. Figure 2 shows a portion of a Rel-graph
where the thickness of the edge indicates symmetric
conditional probability.
(bomb, explode at, <location>) 
(bomb, explode on,          <time_unit>) (bomb, kill,      <person>) 
(bomb, wound,       <person>) (<person>, plant,          bomb) 
(<organization>, claim, responsibility) 
Figure 2: Part of a Rel-graph showing tuples strongly
associated with (bomb, explode at, <location>). Undi-
rected edges are weighted by symmetric conditional
probability with line thickness indicating weight.
4.2 Finding Related Tuples
Our goal is to find closely related tuples that per-
tain to an event or topic. First, we locate high-
connectivity nodes in the Rel-graph to use as seeds.
We sort nodes by the sum of their top 25 edge
weights7 and take the top portion of this list after
filtering out redundant views of the same relation.
For each seed (Q), we find related tuples by ex-
tracting the sub-graph (GQ) from Q?s neighbors
(within two hops from Q) in the Rel-graph. Graph
analysis can detect the strongly connected nodes
within this sub-graph, representing tuples that fre-
quently co-occur in the context of the seed tuple.
Page rank is a well-known graph analysis algo-
rithm that uses graph connectivity to identify impor-
tant nodes within a graph (Brin and Page, 1998). We
are interested in connectivity within a subgraph with
respect to a designated query node (the seed). Con-
nection to a query node can help minimize concept
drift and ensure that the selected tuples are closely
related to the main topic of the sub-graph.
7Limiting to the top 25 edges avoids overly general tuples
that occur in many topics, which tend to have a large number of
weak edges.
In this work, we adapt the Personalized PageR-
ank algorithm (Haveliwala, 2002). The personalized
version of PageRank returns ranks of various nodes
with respect to a given query node and hence is more
appropriate for our task than the basic PageRank al-
gorithm. Within the subgraph GQ for a given seed
Q, we compute a solution to the following set of
PageRank Equations:
PRQ(T )
= (1? d) + d
X
T ?
SCP (T, T ?)PRQ(T
?) if T = Q
= d
X
T ?
SCP (T, T ?)PRQ(T
?) otherwise
Here PRQ(T ) denotes the page rank of a tuple T
personalized for the query tuple Q. It is a sum of
all its neighbors? page ranks, weighted by the edge
weights; d is the damping probability, which we set
to be 0.85 in our implementation.
The solution is computed iteratively by initializ-
ing the page rank of Q to 1 and all others to 0, then
recomputing page rank values until they converge to
within a small . This computation remains scalable,
since we restrict it to subgraphs a small number of
hops away from the query node. This is a standard
practice to handle large graphs (Agirre and Soroa,
2009; Mausam et al, 2010).
4.3 Creating Actors and Relations
We take the top n tuples from GQ according to
their Page rank scores. From each tuple T :
(Arg1, Rel, Arg2) in GQ, we record two actors
(A1, A2) corresponding to Arg1 and Arg2, and add
Rel to the list of relations that they participate in.
Then, we merge actors in two steps. First, we col-
lect the equality constraints for the tuples in GQ. If
the arguments corresponding to any pair of actors
have a non-zero equality constraint then we merge
them. Second, we merge actors that perform simi-
lar actions. A1 and A2 are merged if they are con-
nected to the same actor A3 through the same rela-
tion. For example, A1 and A2 in (A1:lawsuit, file by,
A3:company) and (A2:suit, file by, A3:company),
will be merged into a single actor. To avoid merging
distinct actors, we use a small list of rules that spec-
ify the semantic type pairs that cannot be merged
(e.g., location-date). Also, we do not merge two ac-
tors, if it can result in a relation where the same actor
1725
System A1 Rel A2
Relgrams {bomb, missile, grenade, device} explode in {city, Bubqua, neighborhood}
{bomb, missile, grenade, device} explode kill {people, civilian, lawmaker, owner, soldier}
{bomb, missile, grenade, device} explode on {Feb., Fri., Tues., Sun., Sept.}
{bomb, missile, grenade, device} explode wound {civilian, person, people, soldier, officer}
{bomb, missile, grenade, device} explode in {Feb., Beirut Monday, Sept., Aug.}
{bomb, missile, grenade, device} explode injure {woman, people, immigrant, policeman}
Chambers {bomb, explosion, blast, bomber, mine} explode {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} set off {bomb, explosion, blast, bomber, mine, bombing}
{bomb, explosion, blast, bomber, mine} kill {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} detonate {bomb, explosion, blast, bomber, mine, bombing}
{bomb, explosion, blast, bomber, mine} injure {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} plant {bomb, explosion, blast, bomber, mine, bombing}
Relgrams {Carey, John Anthony Volpe, Chavez, She } veto {legislation, bill, law, measure, version}
{legislation, bill, law, measure, version} be sign by {Carey, John Anthony Volpe, Chavez, She }
{legislation, bill, law, measure, version} be pass by {State Senate, State Assembly, House, Senate, Parliament}
{Carey, John Anthony Volpe, Chavez, She } sign into {law}
{Carey, John Anthony Volpe, Chavez, She } to sign {bill}
{Carey, John Anthony Volpe, Chavez, She } be governor of {Massachusetts, state, South Carolina, Texas, California}
Chambers {clinton, bush, bill, president, house} oppose {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} sign {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} approve {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house veto {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} support {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} pass {bill, measure, legislation, plan, law}
Table 4: ?Bombing? and ?legislation? schema examples from Rel-grams and Chambers represented as a set of
(A1, Rel, A2) tuples, where the schema provides a set of instances for each actor A1 and A2. Relations and argu-
ments are in the stemmed form, e.g., ?explode kill? refers to ?exploded killing?. Instances in bold produce tuples that
are not valid in the real world.
is both the Arg1 and Arg2.
Finally, we generate an ordered list of tuples using
the final set of actors and their relations. The out-
put tuples are sorted by the average page rank of the
original tuples, thereby reflecting their importance
within the sub-graph GQ.
5 Evaluation
We present experiments to explore two main ques-
tions: How well do Rel-grams capture real world
knowledge, and what is the quality of event schemas
built using Rel-grams.
5.1 Evaluating Rel-grams
What sort of common-sense knowledge is encap-
sulated in Rel-grams? How often does it indicate
an implication between a pair of statements, and
how often does it indicate a common real-world
event or topic? To answer these questions, we con-
ducted an experiment to identify a subset of our Rel-
grams database with high precision for two forms of
common-sense knowledge:
? Implication: The Rel-grams express an im-
plication from T to T? or from T? to T, a
bi-directional form of the Recognizing Tex-
tual Entailment (RTE) guidelines (Dagan et al,
2005).
? Common Topic: Is there an underlying com-
mon topic or event to which both T and T? are
relevant?
We also evaluated whether both T and T? are valid
tuples that are well-formed and make sense in the
real world, a necessary pre-condition for either im-
plication or common topic.
We are particularly interested in the highest pre-
cision portion of our Rel-grams database. The
database has 1.1M entries with support of at least
three instances for each tuple. To find the highest
precision subset of these, we identified tuples that
have at least 25 Rel-grams, giving us 12,600 seed
tuples with a total of over 280K Rel-grams. Finally,
we sorted this subset by the total symmetrical con-
ditional probability of the top 25 Rel-grams for each
seed tuple.
We tagged a sample of this 280K set of Rel-grams
for valid tuples, implication between T and T?, and
common topic. We found that in the top 10% of this
set, 87% of the seed tuples were valid and 74% of the
Rel-grams had both tuples valid. Of the Rel-grams
1726
System Id A1 Rel A2
Relgrams R1 bomb explode in city
bomb explode kill people
bomb explode on Fri.
... ... ...
Chambers C1 blast explode child
child detonate blast
child plant bomb
... ... ...
Table 5: A grounded instantiation of the schemas from
Table 4, where each actor is represented as a randomly
selected instance.
with both tuples valid, 83% expressed an implication
between the tuples, and 90% had a common topic.
There were several reasons for invalid tuples ?
parsing errors; binary projections of inherently n-ary
relations, for example (<person>, put, <person>);
head-noun only representation omitting essential in-
formation; and incorrect semantic types, primarily
due to NER tagging errors.
While the Rel-grams suffer from noise in the tu-
ple validity, there is clearly strong signal in the data
about common topic and implication between tuples
in the Rel-grams. As we demonstrate in the follow-
ing section, an end task can use graph analysis tech-
niques to amplify this strong signal, producing high-
quality relational schemas.
5.2 Schemas Evaluation
In our schema evaluation, we are interested in
assessing how well the schemas correspond to
common-sense knowledge about real world events.
To this end, we focus on three measures, topical co-
herence, tuple validity, and actor coherence.
A good schema must be topically coherent, i.e.,
the relations and actors should relate to some real
world topic or event. The tuples that comprise a
schema should be valid assertions that make sense
in the real world. Finally, each actor in the schema
should belong to a cohesive set that plays a consis-
tent role in the relations. Since there are no good
automated ways to make such judgments, we per-
form a human evaluation using workers from Ama-
zon?s Mechanical Turk (AMT).
We compare Rel-grams schemas against the state-
of-the-art narrative schemas released by Cham-
bers (Chambers and Jurafsky, 2009).8 Chambers?
8Available at http://www.usna.edu/Users/cs/
System Id A1 Rel A2
Relgrams R11 bomb explode in city
missile explode in city
grenade explode in city
... ... ...
Relgrams R21 missile explode in city
missile explode in neighborhood
missile explode in front
... ... ...
Table 6: A schema instantiation used to test for actor co-
herence. Each of the top instances for A1 or A2 is pre-
sented, holding the relation and the other actor fixed.
schemas are less expressive than ours ? they do not
associate types with actors and each schema has a
constant pre-specified number of relations. For a
fair comparison we use a similarly expressive ver-
sion of our schemas that strips off argument types
and has the same number of relations per schema
(six) as their highest quality output set.
5.2.1 Evaluation Design
We created two tasks for AMT annotators. The
first task tests the coherence and validity of rela-
tions in a schema and the second does the same
for the schema actors. In order to make the tasks
understandable to unskilled AMT workers, we fol-
lowed the accepted practice of presenting them with
grounded instances of the schemas (Wang et al,
2013), e.g., instantiating a schema with a specific ar-
gument instead of showing the various possibilities
for an actor.
First, we collect the information in schemas as a
set of tuples: S = {T1, T2, ? ? ? , Tn}, where each tu-
ple is of the form T : (X,Rel, Y ), which conveys
a relationship Rel between actors X and Y . Each
actor is represented by its highest frequency exam-
ples (instances). Table 4 shows examples of schemas
from Chambers and Rel-grams represented in this
format. Then, we create grounded tuples by ran-
domly sampling from top instances for each actor.
Task I: Topical Coherence To test whether the re-
lations in a schema form a coherent topic or event,
we presented the AMT annotators with a schema as
a set of grounded tuples, showing each relation in
the schema, but randomly selecting one of the top 5
instances from each actor. We generated five such
nchamber/data/schemas/acl09
1727
Figure 3: (a) Has Topic: Percentage of schema instanti-
ations with a coherent topic. (b) Valid Tuples: Percent-
age of grounded statements that assert valid real-world
relations. (c) Valid + On Topic: Percentage of grounded
statements where 1) the instantiation has a coherent topic,
2) the tuple is valid and 3) the relation belongs to the
common topic. All differences are statistically significant
with a p-value < 0.01.
instantiations for each schema. An example instan-
tiation is shown in Table 5.
We ask three kinds of questions on each grounded
schema: (1) is each of the grounded tuples valid (i.e.
meaningful in the real world); (2) do the majority of
relations form a coherent topic; and (3) does each
tuple belong to the common topic. Similar to pre-
vious AMT studies we get judgments from multiple
(five) annotators on each task and use the majority
labels (Snow et al, 2008).
Our instructions specified that the annotators
should ignore grammar and focus on whether a tuple
may be interpreted as a real world statement. For ex-
ample, the first tuple in R1 in Table 5 is a valid state-
ment ? ?a bomb exploded in a city?, but the tuples
in C1 ?a blast exploded a child?, ?a child detonated
a blast?, and ?a child planted a blast? don?t make
sense.
Task II: Actor Coherence To test whether the in-
stances of an actor form a coherent set, we held the
relation and one actor fixed and presented the AMT
annotators with the top 5 instances for the other ac-
tor. The first example R11 in Table 6 holds the
relation ?explode in? fixed, and A2 is grounded to
the randomly selected instance ?city?. We present
grounded tuples by varying A1 and ask annotators to
judge whether these instances form a coherent topic
and whether each instance belongs to that common
topic. As with Task I, we create five random instan-
tiations for each schema.
Figure 4: Actor Coherence: Has Role bars compare the
percentage of tuples where the tested actors have a co-
herent role. Fits Role compares the percentage of top
instances that fit the specified role for the tested actors.
All differences are statistically significant with a p-value
< 0.01.
5.2.2 Results
We obtained a test set of 100 schemas per system
by randomly sampling from the top 500 schemas
from each system. We evaluate this test set using
Task I and II as described above. For both tasks we
obtained ratings from five turkers and use the major-
ity labels as the final annotation.
Does the schema belong to a single topic? The
Has Topic bars in Figure 3 show results for schema
coherence. Rel-grams has a higher proportion of
schemas with a coherent topic, 91% compared to
82% for Chambers?. This is a 53% reduction in in-
coherent schemas.
Do tuples assert valid real-world relations? The
Valid Tuples bars in Figure 3 compare the percent-
age of valid grounded tuples in the schema instan-
tiations. A tuple was labeled valid if a majority of
the annotators labeled it to be meaningful in the real
world. Here we see a dramatic difference ? Rel-
grams have 92% valid tuples, compared with Cham-
bers? 61%.
What proportion of tuples belong? The Valid +
On Topic bars in Figure 3 compare the percentage
of tuples that are both valid and on topic, i.e., fits
the main topic of the schema. Tuples from schema
instantiations that did not have a coherent topic were
labeled incorrect.
Rel-grams have a higher proportion of valid tu-
ples belonging to a common topic, 82% compared to
1728
58% for Chambers? schemas, a 56% error reduction.
This is the strictest of the experiments described thus
far ? 1) the schema must have a topic, 2) the tuple
must be valid, and 3) the tuple must belong to the
topic.
Do actors represent a coherent set of argu-
ments? We evaluated schema actors from the top
25 schemas in Chambers? and Rel-grams schemas,
using grounded instances such as those in Table 6.
Figure 4 compares the percentage of tuples where
the actors play a coherent role (Has Role), and the
percentage of instances that fit that role for the actor
(Fits Role). Rel-grams has much higher actor co-
herence than Chambers?, with 97% judged to have a
topic compared to 81%, and 81% of instances fitting
the common role compared with Chambers? 59%.
5.2.3 Error Analysis
The errors in both our schemas and those of
Chambers are primarily due to mismatched actors
and from extraction errors, although Chambers?
schemas have a larger number of actor mismatch er-
rors and the cause of the errors is different for each
system.
Examining the data published by Chambers, the
main source of invalid tuples are mismatch of sub-
ject and object for a given relation, which accounts
for 80% of the invalid tuples. We hypothesize that
this is due to the pair-wise representation that treats
subject-verb and verb-object separately, causing in-
consistent s-v-o tuples. An example is (boiler, light,
candle) where (boiler, light) and (light, candle) are
well-formed, yet the entire tuple is not. In addition,
43% of the invalid tuples seem to be from errors by
the dependency parser.
Our schemas also suffer from mismatched actors,
despite the semantic typing of the actors ? we found
a mismatch in 56% of the invalid tuples (5% of
all tuples). A general type such as <person> or
<organization> may still have an instance that does
not play the same role as other instances. For exam-
ple a relation (A1, graduated from, A2) has A2 that
is mostly school names, but also includes ?church?
which leads to an invalid tuple.
Extraction errors account for 47% of the invalid
tuples in our schemas, primarily errors that truncate
an n-ary relation as a binary tuple. For example, the
sentence ?Mr. Diehl spends more time ... than the
commissioner? is misanalysed by the Open IE ex-
tractor as (Mr. Diehl, spend than, commissioner).
6 Related Work
Prior work by Chambers and Jurafsky (2008;
2009; 2010) showed that event sequences (narrative
chains) mined from text can be used to induce event
schemas in a domain-independent fashion. How-
ever, our manual evaluation of their output showed
key limitations which may limit applicability.
As pointed out earlier, a major weakness in
Chambers? approach is the pair-wise representation
of subject-verb and verb-object. Also, their released
a set of schemas are limited to two actors, although
this number can be increased by setting a chain split-
ting parameter.
Chambers and Jurafsky (2011) extended schema
generation to learn domain-specific event templates
and associated extractors. In work parallel to ours,
Cheung et al (2013), developed a probabilistic so-
lution for template generation. However, their ap-
proach requires performing joint probability estima-
tion using EM, which can limit scaling to large cor-
pora.
In this work we developed an Open IE based
solution to generate schemas. Following prior
work (Balasubramanian et al, 2012), we use Open
IE triples for modeling relation co-occurrence. We
extend the triple representation with semantic types
for arguments to alleviate sparisty and to improve
coherence. We developed a page rank based schema
induction algorithm which results in more coherent
schemas with several actors. Unlike Chambers? ap-
proach this method does not require explicit param-
eter tuning for controlling the number of actors.
While our event schemas are close to being tem-
plates (because of associated types, and actor clus-
tering), they do not have associated extractors. Our
future work will focus on building extractors for
these. It will also be interesting to compare with
Cheung?s system on smaller focused corpora.
Defining representations for events is a topic of
active interest (Fokkens et al, 2013). In this work,
we use a simpler representation, defining event
schemas as a set of actors with associated types and
a set of roles they play.
1729
7 Conclusions
We present a system for inducing event schemas
from text corpora based on Rel-grams, a language
model derived from co-occurrence statistics of re-
lational triples (Arg1, Relation, Arg2) extracted by
a state-of-the-art Open IE system. By using triples
rather than a pair-wise representation of subject-verb
and verb-object, we achieve more coherent schemas
than Chambers and Jurafsky (2009). In particular,
our schemas have higher topic coherence (92% com-
pared to Chambers? 82%; make a higher percentage
of valid assertions (94% compared with 61%); and
have greater actor coherence (81% compared with
59%).
Our schemas are also more expressive than those
published by Chambers ? we have semantic typing
for the actors, we are not limited to two actors per
schema, and our relation phrases include preposi-
tions and are thus more precise and have higher cov-
erage of actors involved in the event.
Our future plans are to build upon our event
schemas to create an open-domain event extractor.
This will extend each induced schema to have asso-
ciated extractors. These extractors will operate on a
document and instantiate an instance of the schema.
We have created a Rel-grams database with 1.1M
entries and a set of over 2K event schemas from a
corpus of 1.8M New York Times articles. Both are
freely available to the research community9 and may
prove useful for a wide range of NLP applications.
Acknowledgments
We thank the anonymous reviewers, Tony Fader, and
Janara Christensen for their valuable feedback. This
paper was supported by Office of Naval Research
(ONR) grant number N00014-11-1-0294, Army Re-
search Office (ARO) grant number W911NF-13-
1-0246, Intelligence Advanced Research Projects
Activity (IARPA) via Air Force Research Lab-
oratory (AFRL) contract number FA8650-10-C-
7058, and Defense Advanced Research Projects
Agency (DARPA) via AFRL contract number AFRL
FA8750-13-2-0019. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copyright
annotation thereon. The views and conclusions con-
tained herein are those of the authors and should
9available at http://relgrams.cs.washington.edu
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of ONR, ARO, IARPA, AFRL, or the U.S.
Government.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 33?41.
ARPA. 1991. Proc. 3rd Message Understanding Conf.
Morgan Kaufmann.
ARPA. 1998. Proc. 7th Message Understanding Conf.
Morgan Kaufmann.
Niranjan Balasubramanian, Stephen Soderland, Oren Et-
zioni, et al 2012. Rel-grams: a probabilistic model
of relations in text. In Proceedings of the Joint Work-
shop on Automatic Knowledge Base Construction and
Web-scale Knowledge Extraction, pages 101?105. As-
sociation for Computational Linguistics.
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks, 30(1-7):107?117.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Proceedings of
ACL-08: HLT.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proceedings of ACL.
N. Chambers and D. Jurafsky. 2010. A database of nar-
rative schemas. In Proceedings of LREC.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Pro-
ceedings of ACL.
J. Cheung, H. Poon, and L. Vandervende. 2013. Prob-
abilistic frame induction. In Proceedings of NAACL
HLT.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, , and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program-tasks, data,
and evaluation. In Procs. of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
1730
Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, BV SynerScope,
Luciano Serafini, Rachele Sprugnoli, and Jesper Hoek-
sema. 2013. Gaf: A grounded annotation framework
for events. NAACL HLT 2013, page 11.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517?526.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92. Associ-
ation for Computational Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, (Just Accepted):1?54.
Mausam, Stephen Soderland, Oren Etzioni, Daniel Weld,
Kobi Reiter, Michael Skinner, Marcus Sammer, and
Jeff Bilmes. 2010. Panlingual lexical translation via
probabilistic inference. Artificial Intelligence Journal
(AIJ).
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP.
K. Owczarzak and H.T. Dang. 2010. Overview of the tac
2010 summarization track.
S. Patwardhan and E. Riloff. 2009. A unified model of
phrasal and sentential evidence for information extrac-
tion. In Proceedings of EMNLP 2009.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?
263. Association for Computational Linguistics.
A. Wang, C.D.V. Hoang, and M-Y. Kan. 2013. Perspec-
tives on crowdsourcing annotations for Natural Lan-
guage Processing. Language Resources and Evalua-
tion, 47:9?31.
1731
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1891?1901,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Type-Aware Distantly Supervised Relation Extraction
with Linked Arguments
Mitchell Koch John Gilmer Stephen Soderland Daniel S. Weld
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{mkoch,jgilme1,soderlan,weld}@cs.washington.edu
Abstract
Distant supervision has become the lead-
ing method for training large-scale rela-
tion extractors, with nearly universal adop-
tion in recent TAC knowledge-base pop-
ulation competitions. However, there are
still many questions about the best way
to learn such extractors. In this paper we
investigate four orthogonal improvements:
integrating named entity linking (NEL)
and coreference resolution into argument
identification for training and extraction,
enforcing type constraints of linked argu-
ments, and partitioning the model by rela-
tion type signature.
We evaluate sentential extraction perfor-
mance on two datasets: the popular set of
NY Times articles partially annotated by
Hoffmann et al. (2011) and a new dataset,
called GORECO, that is comprehensively
annotated for 48 common relations. We
find that using NEL for argument identi-
fication boosts performance over the tra-
ditional approach (named entity recogni-
tion with string match), and there is further
improvement from using argument types.
Our best system boosts precision by 44%
and recall by 70%.
1 Introduction
Relation extractors are commonly trained by dis-
tant supervision (also known as knowledge-based
weak supervision (Hoffmann et al., 2011)), an au-
tonomous technique that creates a labeled train-
ing set by heuristically matching the contents of a
knowledge base (KB) to mentions (substrings) in
a textual corpus. For example, if a KB contained
the ground tuple BornIn(Albert Einstein, Ulm) then
Training
Extraction
KB
Argument 
Identification
Matching
Train Extractor Extractor
Argument 
Identification
Figure 1: Distantly supervised extraction pipeline.
a distant supervision system might label the sen-
tence ?While [Einstein]
1
was born in [Ulm]
2
, he
moved to Munich at an early age.? as a positive
training instance of the BornIn relation. Although
distant supervision is a simple idea and often cre-
ates data with false positives, it has become ubiq-
uitous; for example, all top-performing systems in
recent TAC-KBP slot filling competitions used the
method.
Surprisingly, however, many aspects of distant
supervision are poorly studied. In response we
perform an extensive search of ways to improve
distant supervision and the extraction process, in-
cluding using named entity linking (NEL) and
coreference to identify arguments for distant su-
pervision and extraction, as well as using type con-
straints and partitioning the trained model by rela-
tion type signatures.
The first step in the distant supervision process
is argument identification (Figure 1) ? finding
textual mentions referring to entities that might be
in some relation. Next comes matching, where KB
facts, e.g. tuples such as R(e
1
, e
2
), are associated
with sentences mentioning entities e
1
and e
2
in
the assumption that many of these sentences de-
scribe the relation R. Most previous systems per-
form these steps by first using named entity recog-
nition (NER) to identify possible arguments and
then using a simple string match, but this crude
1891
approach misses many possible instances. Since
the separately-studied task of named entity linking
(NEL) is precisely what is needed to perform dis-
tant supervision, it is interesting to see if today?s
optimized linkers lead to improved performance
when used to train extractors.
Coreference, the task of clustering mentions
that describe the same entity, may also be use-
ful for increasing the number of candidate argu-
ments. Consider the following variant of our pre-
vious example: ?While [he]
1
was born in [Ulm]
2
,
[Einstein]
3
moved to Munich at an early age.?
Since mentions 1 and 3 corefer, one could con-
sider using either the pair ?1, 2? or ?3, 2? (or both)
for training. Intuitively, it seems that ?1, 2? is more
representative of BornIn and might generalize bet-
ter, so we consider the use of coreference at both
training and extraction time.
Semantic relations often have selectional prefer-
ences (also known as type signatures); for exam-
ple, BornIn holds between people and locations.
Therefore, it seems promising to include entity
types, whether coarse or fine grained in the dis-
tantly supervised relation extraction process. We
consider two ways of adding this information. By
using NEL to get linked entities, we can impose
type constraints on the relation extraction system
to only allow relations over appropriately typed
mentions. We also investigate using coarse types
from NER to learn separate models for different
relation type signatures in order to make the mod-
els more effective.
In summary, this paper represents the following
contributions:
? We explore several dimensions for improv-
ing distantly supervised relation extraction,
including better argument identification dur-
ing training and extraction using both NEL
and coreference, partitioning the model by
relation type signatures, and enforcing type
constraints of linked arguments as a post-
processing step. While some of these ideas
may seem straightforward, to our knowledge
they have not been systematically studied.
And, as we show, they lead to dramatic im-
provements.
? Since previous datasets are incapable of mea-
suring an extractor?s true recall, we intro-
duce GORECO, a new exhaustively-labeled
dataset with gold annotations for sentential
instances of 48 relations across 128 newswire
documents from the ACE 2004 corpus (Dod-
dington et al., 2004).
? We demonstrate that NEL argument identifi-
cation boosts both precision and recall, and
using type constraints with linked arguments
further boosts precision, yielding a 43% in-
crease in precision and 27% boost to re-
call. Using coreference during training ar-
gument identification gives an additional 7%
improvement to precision and further boosts
recall by 9%. Partitioning the model by rela-
tion type signature offers further benefits, so
our best system yields a total boost of 44% to
precision and 70% to recall.
2 Distantly Supervised Extraction
At a sentence-level, the goal for relation extrac-
tion is to determine for each sentence, what facts
are expressed. We describe these as relation an-
notations of the form s?R(m
1
,m
2
), where s is
a sentence, R ? R is a relation name, R is our
finite set of target relations, and m
1
and m
2
are
grounded entity mentions of the form (s, t
1
, t
2
, e),
where t
1
and t
2
delimit a text span in the sentence,
and e is a grounded entity.
2.1 Training
During training, the contents of the KB are heuris-
tically matched to the training corpus according
to the distant supervision hypothesis: if a relation
holds between two entities, any sentence contain-
ing those two entities is likely to express that rela-
tion.
The training KB ? contains fact tuples of form
R(e
1
, e
2
), where R ? R is a relation name, R is
our finite set of target relations, and e
1
and e
2
are
ground entities. The training text corpus ? con-
tains documents, which contain sentences. Argu-
ment identification is performed over the text cor-
pus to get grounded mentionsm. Then during sen-
tential instance generation, sentential instances of
the form (s,m
1
,m
2
) are generated representing
a sentence with two grounded mentions. At this
point, these sentential instances can be matched
to the seed KB, yielding candidate relation anno-
tations of the form s?R(m
1
,m
2
) by finding all
relations that hold over the entities in a sentential
instance. These candidate relation annotations are
all positive instances to use for training. Negative
instance generation is also performed, generating
1892
negative examples of the form s?NA(m
1
,m
2
) in-
dicating that no relation holds between m
1
and
m
2
. There are several heuristics for generating
negative instances, and the number of negative ex-
amples and how they are treated can greatly affect
performance (Min et al., 2013).
Because the distant supervision hypothesis of-
ten does not hold, this training data is noisy. That
a fact is in the KB does not imply that the sen-
tence in question is expressing the relation. There
has been much work in combating noise in dis-
tant supervision training data, but one of the most
successful ideas is to train a multi-instance classi-
fier which assumes at-least-one relation holds for
positive bags. We use Hoffmann et al. (2011)?s
MULTIR system, which uses a probabilistic graph-
ical model to jointly reason at the corpus-level
and sentence-level, handles overlapping relations
in the KB so that multiple relations can hold over
an entity pair, and scales to large datasets.
2.2 Extraction
The trained relation extractor can assign a most
likely relation and a confidence score to a senten-
tial instance (s,m
1
,m
2
). To get these sentential
instances, argument identification and sentential
instance generation are applied to new documents.
Then the relation extractor potentially yields a re-
lation annotation of the form s?R(m
1
,m
2
), or
potentially no relation. At extraction time a men-
tion m might have a NIL link if a correspond-
ing ground entity was not found during argument
identification (meaning the entity is not in the KB).
The relation annotations have associated confi-
dence scores, so a threshold can be chosen to only
use high-confidence relation annotations.
3 Argument Identification
An important piece of relation extraction is deter-
mining what can be an argument, and how to form
a semantic representation of it. We define an argu-
ment identification function ArgIdent
?
(D), which
takes a document D, finds potential arguments,
and links them to entities in ? if possible, yield-
ing m, a set of grounded mentions in D. Pre-
vious relation extraction systems have based this
on NER. We evaluate NER-based argument iden-
tification against argument identification based on
NEL, as well as NEL with coreference.
3.1 Named Entity Recognition
Named entity recognition (NER) tags spans of to-
kens with basic types such as PERSON, ORGANI-
ZATION, LOCATION, and MISC. This is a high
accuracy tagging task often performed using a
sequence classifier (Finkel et al., 2005). Rela-
tion extraction systems can base their argument
identification on NER, by using NER to identify
text spans indicating entities and then find corre-
sponding entities in the KB through exact string
match (Riedel et al., 2010). Some downsides of
using NER with exact string match for relation ex-
traction is that it does not allow for overlapping
mentions, it can only capture arguments with full
names, and it can only capture arguments with
types of the NER system, e.g., ?politician? might
not be captured.
3.2 Named Entity Linking
Named entity linking (NEL) is the task of ground-
ing textual mentions to entities in a KB, such as
Wikipedia. Thus ?named entity? here, has a some-
what broader definition than in NER ? these are
any entities in the KB, not just those expressed
with proper names. Hachey et al. (2013) define
three stages that NEL systems take to perform
this task: extraction (mention detection), search
(generating candidate KB entities for a mention),
and disambiguation (selecting the best entity for a
mention). There has been much work on the task
of NEL in recent years (Milne and Witten, 2008;
Kulkarni et al., 2009; Ratinov et al., 2011; Cheng
and Roth, 2013).
Our definition of a function ArgIdent(D) is
completely served by an NEL system. It can
find any entity in the KB, and those entities are
grounded. Additionally, NEL can have overlap-
ping mentions as well as support for abbreviated
mentions like ?Obama?, or acronyms like ?US?.
NEL does not seek to capture anaphoric mentions,
however.
3.3 Coreference Resolution
Coreference resolution is the task of clustering
mentions of entities together, typically within a
single document. Using coreference, we can find
even more mentions than NEL, since it can find
pronouns and anaphoric mentions. We seek to use
coreference to add additional arguments to those
found by NEL, and we refer to this combined ar-
gument identification method as NEL+Coref. Tak-
1893
ing in arguments from NEL argument identifica-
tion and coreference clusters, we ground the clus-
ters by picking the most common grounded entity
from NEL mentions that occur in a coreference
cluster. A difficulty is that mentions from NEL
and coreference can have small differences in text
spans, such as whether determiners are included.
We try to assign each NEL argument to a corefer-
ence cluster, first looking for an exact span match,
then by looking for the shortest coreference men-
tion that contains it. If the coreference cluster al-
ready has matched an NEL argument through ex-
act span match that is different from the one found
by looking for the shortest containing coreference
mention, the new NEL argument is not added.
This gives for each coreference cluster a possible
grounding to an entity in the KB. What is provided
as final arguments for NEL+Coref argument iden-
tification are, in order, grounded NEL arguments,
grounded coreference arguments that do not over-
lap with previous arguments, NIL arguments from
NEL that do not overlap with previous arguments,
and NIL arguments from coreference that do not
overlap with previous arguments.
4 Type-Awareness
Relations have expected types for each argument.
Entity types, whether coarse-grained, such as from
NER, or fine-grained, such as from Freebase enti-
ties, are an important source of information that
can be useful for making decisions in relation ex-
traction. We bring type-awareness into the system
through partitioning the model, as well as by en-
forcing type constraints on output relation annota-
tions.
Model Partitioning Instead of building a single
relation extractor that can generate sentential in-
stances and then relation annotations with argu-
ments of any type, we can instead build separate
relation extractors for each possible coarse type
signature, e.g., (PERSON, PERSON), (PERSON, LO-
CATION), etc., and combine the extractions from
the extractor for each type signature. This modi-
fication allows each trained model to only handle
instances of specific types, and thus relations of
that type signature, allowing each to do a better job
of choosing relations within the type signature.
Type Constraints We can additionally reject re-
lation annotations where the types of the argu-
ments do not agree with the expected types of the
relation. That is, we only accept a relation annota-
tion s?R(m
1
,m
2
) when EntityTypes(e
1
) ? ?
1
6=
? and EntityTypes(e
2
)??
2
6= ?, wherem
1
is linked
to e
1
, m
2
is linked to e
2
, EntityTypes provides the
set of valid types for an entity, ?
1
is the set of al-
lowed types for the first argument of target relation
r, and ?
2
for the second argument.
5 Evaluation Setups
Relation extraction is often evaluated from a
macro-reading perspective (Mitchell et al., 2009),
in which the extracted facts, R(e
1
, e
2
), are judged
true or false independent of any supporting sen-
tence. For these experiments, however, we take a
micro-reading approach in order to strictly eval-
uate whether a relation extractor is able to extract
every fact expressed by a sentence s?R(m
1
,m
2
).
Micro-reading is more difficult, but it provides
fully semantic information at the sentence and
document level allowing detailed justifications,
and, for our purposes, allows us to better under-
stand the effects of our modifications. In order
to fairly evaluate different systems, even those us-
ing different methods of argument identification,
we want to use gold evaluation data allowing for
varying mention types. We additionally use Hoff-
mann et al. (2011)?s sentential evaluation as-is in
order to better compare with prior work. For our
training corpus, we use the TAC-KBP 2009 (Mc-
Namee and Dang, 2009) English newswire corpus
containing one million documents with 27 million
sentences.
5.1 Hoffmann et al. Sentential Evaluation
Hoffmann et al. (2011) generated their gold data
by taking the union of sentential instances where
some system being evaluated extracted a relation
as well as the sentential instances matching ar-
guments in the KB. They took a random sample
of these sentential instances and manually labeled
them with either a single relation or NA. Although
this process provides good coverage, since is is
sampled from extractions over a large corpus, it
does not allow one to measure true recall. Indeed,
Hoffmann?s method significantly overestimates re-
call, since the random sample is only over senten-
tial instances where a program detected an extrac-
tion or a KB match was found. Furthermore, this
test set only contains sentential instances in which
arguments are marked using NER, which makes
it impossible to determine if the use of NEL or
1894
coreference confers any benefit.
Finally, it does not allow for the possibility that
there may be multiple relations that should be ex-
tracted for a pair of arguments. For example, a
CeoOf relation, and an EmployedBy relation might
both be present for (Larry Page, Google). To ad-
dress these issues, we manually annotate a full set
of documents with relation annotations. Because
we are evaluating changing various aspects of the
distant supervision process, we cannot use Riedel
et al. (2010)?s distant supervision data as-is as oth-
ers did on the Hoffmann et al. (2011) sentential
evaluation. Instead, we use the TAC-KBP data de-
scribed above.
5.2 GoReCo Evaluation
In order to allow for variations on mentions (NER,
NEL, and coreference each has its own definition
of what a mention boundary should be), we want
gold relation annotations over coreference clus-
ters broadly defined to allow mentions obtained
from NER and NEL, as well as gold coreference
mentions. So as long as a relation extraction sys-
tem extracts a relation annotation s?R(m
1
,m
2
)
where m
1
and m
2
are allowed options (based on
text spans), it will get credit for extracting the
relation annotation. We introduce the GORECO
(gold relations and coreference) evaluation to sat-
isfy these constraints.
We start with an existing gold coreference
dataset, ACE 2004 (Doddington et al., 2004)
newswire, consisting of 128 documents. To get
relation annotations over coreference clusters, we
define two human annotation tasks and use the
BRAT (Stenetorp et al., 2012) tool for visualization
and relation and coreference annotations.
Relation Annotation The annotator is pre-
sented with a document with gold mentions indi-
cated and asked to determine for each sentence,
what facts involving target relations are expressed
by the sentence. They draw an arrow for each fact
and label it with the relation. They also have the
ability to add mentions not present (ReAnn men-
tions).
Supplemental Coreference Mentions from
NER and NEL are displayed along with ACE and
ReAnn mentions from the previous task. The
annotator draws coreference links from NER or
NEL mentions to an ACE or ReAnn mention if
they are coreferent.
We randomly shuffle the 128 ACE 2004
newswire documents and use 64 as a development
set and 64 as a test set. To complete annotations
of these documents, we only used one original hu-
man annotator (hired using the oDesk crowdsourc-
ing platform) and found mistakes by having others
check the work, as well as checking false positives
of relation extractors on the development set to
find patterns of annotation mistakes. On average,
there are 7 relation annotations per document.
For the GORECO evaluation, we define our
train/test split (with the separate TAC-KBP corpus
used for training) such that each has a different set
of documents and entities, in order to evaluate how
well the system performs on unseen entities. To do
this, we remove entities found in the gold evalua-
tion set from the training KB. (We do not remove
entities for the Hoffmann et al. (2011) evaluation,
since they do not.) We choose the threshold con-
fidence score for each system using the develop-
ment set to optimize for F1 and report results on
the test set.
5.2.1 Target Relations
Since we use a different evaluation, we also seek to
choose a more comprehensive and interesting set
of relations than prior work. Riedel et al. (2010),
whose train and test data is also used by Hoff-
mann et al. (2011) and Surdeanu et al. (2012), use
Freebase properties under domains /people, /busi-
ness, and /location. Since /location relations such
as /location/location/contains dominate the results
(and are relatively uninteresting in that they rarely
change), we do not use any /location relations, and
instead use the domains /people, /business, and
/organization (Google, 2012).
Since many Freebase properties are between
an entity and a table instead of another
entity, we also use joined relations, such
as /people/person/employment_history ./ /busi-
ness/employment_tenure/company , in this case
representing employment. We bring in an addi-
tional 20 relations of this form, also under /person,
/business, and /organization. Additionally we use
NELL (Carlson et al., 2010a) relations mapped to
Freebase by Zhang et al. (2012).
We only include a relation in our set of target
relations if both of its entity arguments are con-
tained in the set of entities found via NER with
exact string match or NEL over the training cor-
pus. We also remove inverse relations, since they
represent needless duplication. This gives us a set
1895
R of 105 target relations based on joins and unions
of Freebase properties. Of the 105 target relations,
48 were used at least once in the GORECO data.
6 Experiments and Results
We conduct experiments to determine how chang-
ing distantly supervised relation extraction along
various dimensions affects performance. We ex-
amine the choice of argument identification dur-
ing training and extraction, as well as the effects
of model type partitioning, and type constraints.
We consider the space of all combinations of these
dimensions, but focus on specific combinations
where we find improvements.
6.1 Relation Extraction Setup
We use and modify Hoffmann et al. (2011)?s sys-
tem MULTIR to control our experiments and as
a baseline. For NER argument identification as
well as for the use of NER in the features, we use
use Stanford NER (Finkel et al., 2005). For NEL
argument identification we use Wikipedia Miner
with the default threshold 0.5, and allowing re-
peated mentions (Milne and Witten, 2008). Since
Wikipedia Miner does not support NIL links, we
use non-overlapping NER mentions as NIL links.
For coreference, we use Stanford?s sieve-based de-
terministic coreference system (Lee et al., 2013).
For sentential instance generation, we take all
pairs of non-overlapping arguments in a sentence
(in either order). If the arguments have KB links,
we do not allow sentential instances where both
arguments represent the same entity. We use the
same lexical and syntactic features as MULTIR,
based on the features of Mintz et al. (2009). As
required for features, we use Stanford CoreNLP?s
tokenizer, part of speech tagger (Toutanova et al.,
2003), and dependency parser (de Marneffe and
Manning, 2008), and use the Charniak Johnson
constiuent parser (Charniak and Johnson, 2005).
For negative training generation, we take a simi-
lar approach to Riedel et al. (2010) and define a
percentage parameter n of the number of nega-
tive instances divided by the number of total in-
stances. Experimenting with n ? {0, 20%, 80%},
we find that n = 20% works best for our evalua-
tions, optimizing for F1, although using 80% neg-
ative training gives high precision at lower recall.
We use frequency-based feature selection to elimi-
nate features that appear less than 10 times, which
is helpful both for reducing overfitting as well as
0.60 0.1 0.2 0.3 0.4 0.5
1
0.7
0.8
0.9
Relative Recall
Prec
ision
Hoffmann et al. (2011)
NER+LTNER
NER+LT+CT
Figure 2: Methods evaluated in the context of
Hoffmann et al. (2011)?s sentential extraction
evaluation. NER: our NER baseline used for
training and extraction, LT: use NEL for train-
ing only, CT: use coreference for training only.
(NER+LT+CT means we use NER for extraction,
and NEL+Coref for training.)
constraining memory usage. Since the perceptron
learning of MULTIR is sensitive to instance order-
ing, we perform 10 random shuffles and average
the models.
For model type partitioning, when training with
NER, we ensure that the NER types match the
coarse relation type signatures. For NEL, we at-
tempt to use NER for coarse types of arguments,
but if an NER type is not present, we map the Free-
base type to its FIGER type (Ling and Weld, 2012)
to its coarse type. For type constraints, we use
Freebase?s expected input and output types for re-
lations. For NIL links, we use the NER type of
PERSON, ORGANIZATION, or LOCATION, if avail-
able, mapping it to appropriate Freebase types.
6.2 NER Baseline
As a result of a larger training set, as well as model
averaging, our baseline, which is otherwise equiv-
alent to the methods of Hoffmann et al. (2011)
and uses their MULTIR system, has slightly higher
precision as shown in Figure 2, curve NER. It is
also higher than that of Xu et al. (2013), who
achieved higher performance than Hoffmann et
al. (2011); our baseline gets 89.9% precision and
59.6% relative recall, while Xu et al. (2013)?s sys-
tem gets 84.6% precision and 56.1% relative re-
call. See Figure 3 and Table 1 for results on
GORECO.
6.3 NEL and Type Constraints
On GORECO, using NEL argument identification
increases recall and gives higher precision over the
entire curve. We further find that filtering results
using type constraints gives a large boost in pre-
1896
cision at a small cost to recall. Note the increase
in performance from NER to NEL to NEL+TC in
Figure 3a, as well as in Table 1. Using NEL gives
more recall, since it is able to capture arguments
that NER cannot, such as professions like ?pa-
leontologist?. The decrease in recall from type
constraints comes from false positives in the type
constraints process including from non-ideal links,
e.g., ?paleontologist? might get linked to the entity
Paleontology, so will not have the type required for
the Profession relation.
On the Hoffmann et al. (2011) sentential evalu-
ation, we were not able to use NEL argument iden-
tification at extraction time, because the instances
in the test set are from NER argument identifica-
tion. We tried using NEL only at training time
and found that it got similar performance to using
NER (Figure 2, curve NER+LT). Doing the same
on GORECO yielded slightly lower recall, because
of the mismatch of features learned from NEL ar-
guments (Figure 3b, curve NER+LT).
6.4 NEL+Coref Argument Identification
Using NEL+Coref for both training and extrac-
tion (see Table 1) introduces noise from arguments
not encountered during training time, but using
NEL+Coref just for training results in a decrease
in recall but similar precision (Figures 2 and 3b).
We found using NEL+Coref at test time unhelp-
ful for this dataset, because there were no exam-
ples we could find where coreference could re-
cover arguments that NEL could not. There were
three true positives from NEL+Coref involving
pronouns in the GORECO development set, but
there were also proper name versions of the ar-
guments nearby in the same sentences, making
coreference unnecessary. Additionally, corefer-
ence brings in many mentions such as times like
?Friday? or ?1954? that do not have corresponding
KB matches during training time. These sentential
instances have similar features to others involv-
ing coreference mentions, and there are not neg-
ative instances to weigh against these, since these
types do not appear in the training data. Better fea-
tures more suited to coreference mentions could be
helpful here.
At both training and extraction time, corefer-
ence can cluster together mentions that can be con-
sidered to be separate, such as in ?Brian Kain, a
33-year-old accountant?, ?Brian Kain? and ?ac-
countant? are coreferent in the gold ACE 2004
770 10 20 30 40 50 60 70
1
0
0.2
0.4
0.6
0.8
True Positives Count
Prec
ision
NER
NEL+TC
NELNER+TP
NEL+TC+TP
(a)
770 10 20 30 40 50 60 70
1
0
0.2
0.4
0.6
0.8
True Positives Count
Prec
ision
NER
NER+LT
NER+LT+CT
NEL+TC+CT+TP
NEL+TP
NEL+TC
NEL+TC+TP
NEL+CT
NEL+TC+CT
(b)
Figure 3: Precision versus true positives count
curves for different versions of the system evalu-
ated on the GORECO test set, containing 470 gold
instances. NER/NEL: argument identification used
in training and extraction, LT: use NEL for train-
ing only, CT: use coreference for training only, TC:
type constraints, TP: model type partitioning.
dataset. This means that type constraints will
disregard a Profession annotation between these
when it should not, because ?Brian Kain? (which
would have been a NIL link) gets the link of ?ac-
countant?. This effect contributes to the decrease
in recall.
6.5 Model Type Partitioning
Using type partitioning helps both NER and NEL
based models as shown with the +TP curves in
Figure 3). Partitioning by type signature results in
each model being able to better choose relations
for sentential instances of that type signature. In
the Partitioned columns of Table 1, removing type
partitioning from the best system (NEL training
1897
Single Partitioned
R P F1 R P F1
NER training
NER extraction 7.9 21.8 11.6 11.3 21.0 14.7
NEL extraction 8.5 21.4 12.2 9.8 19.7 13.1
NEL training
NER extraction 9.6 21.1 13.2 8.9 25.1 13.2
NEL extraction 10.0 30.5 15.1 15.3 16.7 16.0
NEL w/TC extraction 11.7 31.1 17.0 13.4 31.3 18.8
NEL+Coref training
NER extraction 9.4 19.2 12.6 6.8 28.3 11.0
NEL extraction 12.1 27.5 16.8 11.1 21.6 14.6
NEL w/TC extraction 12.8 33.3 18.5 12.1 34.1 17.9
NEL+Coref extraction 10.6 20.4 14.0 10.0 12.9 11.3
NEL+Coref w/TC extraction 9.4 22.7 13.3 7.9 19.1 11.1
Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For
nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either
training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1
except with coreference at extraction time and when combined with type partitioning.
and extraction, with type constraints, Partitioned)
results in a decrease in F1 from 18.8% to 17.0%.
Table 2 shows by-relation performance results for
the best system (curve NEL+TC+TP in Figure 3a).
6.6 Other Dimensions Explored
We also experimented with adding generalized
features that replaced lexemes with WordNet
classes (Fellbaum, 1998), which had uneven re-
sults. We observed a small but consistent improve-
ment on the NER baseline (11.6% F1 to 12.7%
F1 on GORECO), but after introducing NEL argu-
ment identification and partitioning, we no longer
observed the improvement. For some relations,
there was a small gain in recall that was offset by
a loss in precision, but for others, the gain in recall
outweighed the loss of precision.
We experimented with a negative instance feed-
back loop that ran a trained extractor over the
training corpus and tested whether each extrac-
tion made was in fact a negative example. Even
though the training corpus contains one million
documents, this method only yielded a few thou-
sand new negative instances due to the difficulty
of being certain an extraction should be negative.
A na?ve approach would simply ensure that both
entities participate in a relation in the KB; this is
troublesome, because of KB incompleteness and
because of type errors. For example Freebase con-
tains BornIn(Barack Obama, Honolulu), but our ex-
tractor extracted BornIn(Barack Obama, Hawaii).
To avoid labeling this true extraction as a nega-
tive instance we have to be robust about location
semantics. We selected new negative instances
NA(e
1
, e
2
) from our initial extractor that had e
1
in the knowledge base, with e
1
participating as the
first argument in the extracted relation but with-
out e
2
as the second argument. The results were
promising for some relations but overall inconclu-
sive as identifying true negatives is quite difficult.
Relation #Extractions #TP #FP
Nationality 50 11 38
Profession 43 23 20
EmployedBy 27 17 10
Spouse 22 2 20
LivedIn 6 4 2
OrgInCitytown 4 3 1
AthletePlaysForTeam 2 2 0
OrgType 1 1 0
Table 2: By-relation evaluation of the best system
(NEL with type constraints and type partitioning)
on the GORECO test set. The true positives (TP)
are the number of gold relations over coreference
clusters that matched, so multiple extractions can
match a single true positive.
7 Related Work
There has been much recent work on distantly su-
pervised relation extraction. Mintz et al. (2009)
use Freebase to train relation extractors over
Wikipedia without labeled data using multi-class
logistic regression and lexical and syntactic fea-
tures. Hoffmann et al. (2011) use a probabilis-
tic graphical model for multi-instance, multi-label
1898
learning and extract over newswire text using
Freebase relations. Surdeanu et al. (2012) take a
similar approach and use soft constraints and lo-
gistic regression. Riedel et al. (2013) integrate
open information extraction with schema-based,
proposing a universal schema approach, including
using features based on latent types. There has
also been recent work on reducing noise in dis-
tantly supervised relation extraction (Nguyen and
Moschitti, 2011; Takamatsu et al., 2012; Roth et
al., 2013; Ritter et al., 2013). Xu et al. (2013) and
Min et al. (2013) improve the quality of distant su-
pervision training data by reducing false negative
examples.
Distant supervision is related to semi-
supervised bootstrap learning work such as
Carlson et al. (2010b) and many others. Note that
distant supervision can be viewed as a subroutine
of bootstrap learning; bootstrap learning can
continue the process of distant supervision by
taking the new tuples found and then training on
those again, and repeating the process.
There has also been work on performing NEL
and coreference jointly (Cucerzan, 2007; Ha-
jishirzi et al., 2013), however these systems do not
perform relation extraction. Singh et al. (2013)
performs joint relation extraction, NER, and coref-
erence in a fully-supervised manner. They get
slight improvement by adding coreference, but do
not use NEL. Ling and Weld (2013) extend MUL-
TIR to find meronym relations in a biology text-
book. They get slight improvement over NER by
using coreference to pick the best mention of an
entity in the sentence for the meronym relation at
training and extraction time.
8 Conclusions and Future Work
Given the growing importance of distant supervi-
sion, a comprehensive understanding of its vari-
ants is crucial. While some of the optimizations
we propose may seem intuitive, they have not pre-
viously been systematically explored. Our experi-
ments show that NEL, type constraints, and type
partitioning are extremely important in order to
best take advantage of the seed KB during training
as well as known information at extraction time.
Our best system results in a 44% increase in pre-
cision, and a 70% increase in recall over our NER
baseline using GORECO. While we were not able
to evaluate all our methods on Hoffmann et al.
(2011)?s sentential evaluation, our baseline per-
forms significantly better than previous methods,
especially in precision, and training-only modifi-
cations perform similarly in both evaluations.
Future work will explore the use of NEL in dis-
tantly supervised relation extraction further, tun-
ing a confidence parameter for the NEL system,
and determining whether different confidence pa-
rameters should be used for training and extrac-
tion. Another possible direction is interleaving
NEL with relation extraction by using newly ex-
tracted facts to try to improve NEL performance.
We freely distribute GORECO a new gold stan-
dard evaluation for relation extraction consisting
of exhaustive annotations of the 128 documents
from ACE 2004 newswire for 48 relations. The
source code of our system, its output, as well as
our gold data are available at
http://cs.uw.edu/homes/mkoch/re.
Acknowledgements
We thank Raphael Hoffmann, Luke Zettlemoyer,
Mausam, Xiao Ling, Congle Zhang, Hannaneh
Hajishirzi, Leila Zilles, and the anonymous re-
viewers for helpful feedback. Additionally, we
thank Anand Mohan and Graeme Britz for annota-
tions and revisions of the GORECO dataset. This
work was supported by Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-
0181, ONR grant N00014-12-1-0211, a gift from
Google, a grant from Vulcan, and the WRF / TJ
Cable Professorship. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship under Grant
No. DGE-1256082.
References
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010a. Toward an architecture for never-
ending language learning. In Proceedings of the
AAAI Conference on Artificial Intelligence (AAAI-
10).
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka, Jr., and Tom M. Mitchell.
2010b. Coupled semi-supervised learning for infor-
mation extraction. In Proceedings of the Third ACM
International Conference on Web Search and Data
Mining, WSDM ?10, pages 101?110, New York,
NY, USA. ACM.
1899
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Xiao Cheng and Dan Roth. 2013. Relational inference
for wikification. In EMNLP.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708?716.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 1?8,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 363?370, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Google. 2012. Freebase data dumps.
https://developers.google.com/
freebase/data.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evalu-
ating entity linking with wikipedia. Artif. Intell.,
194:130?150, January.
Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and
Luke S. Zettlemoyer. 2013. Joint coreference res-
olution and named-entity linking with multi-pass
sieves. In EMNLP, pages 289?299. ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In ACL-HLT,
pages 541?550.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
KDD ?09, pages 457?466, New York, NY, USA.
ACM.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Comput. Linguist., 39(4):885?916, December.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In Proceedings of the 26th Con-
ference on Artificial Intelligence (AAAI).
Xiao Ling and Daniel S. Weld. 2013. Extracting
meronyms for a biology knowledge base using dis-
tant supervision. In Automated Knowledge Base
Construction (AKBC) 2013: The 3rd Workshop on
Knowledge Extraction at CIKM.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the tac 2009 knowledge base population track. In
Text Analysis Conference (TAC), volume 17, pages
111?113.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th ACM
Conference on Information and Knowledge Man-
agement, CIKM ?08, pages 509?518, New York,
NY, USA. ACM.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of NAACL-HLT, pages 777?
782.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2009), pages 1003?1011.
Tom M. Mitchell, Justin Betteridge, Andrew Carlson,
Estevam Hruschka, and Richard Wang. 2009. Pop-
ulating the semantic web by macro-reading internet
text. In The Semantic Web-ISWC 2009, pages 998?
1002. Springer.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, HLT
?11, pages 277?282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1375?1384,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In ECML/PKDD (3), pages 148?
163.
1900
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant super-
vision for information extraction. TACL, 1:367?378.
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow. 2013. A survey of noise reduction
methods for distant supervision. In Proceedings of
the 2013 Workshop on Automated Knowledge Base
Construction, AKBC ?13, pages 73?78, New York,
NY, USA. ACM.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In CIKM
Workshop on Automated Knowledge Base Construc-
tion (AKBC).
Pontus Stenetorp, Sampo Pyysalo, Goran Topi
?
c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Stroudsburg, PA, USA, April. Association for Com-
putational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wei Xu, Zhao Le, Raphael Hoffmann, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 2013 Conference of the Association for
Computational Linguistics (ACL 2013), Sofia, Bul-
garia, July. Association for Computational Linguis-
tics.
Congle Zhang, Raphael Hoffmann, and Daniel S.
Weld. 2012. Ontological smoothing for relation ex-
traction with minimal supervision. In AAAI.
1901
Proceedings of NAACL-HLT 2013, pages 1163?1173,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Coherent Multi-Document Summarization
Janara Christensen, Mausam, Stephen Soderland, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{janara,mausam,soderlan,etzioni}@cs.washington.edu
Abstract
This paper presents G-FLOW, a novel system
for coherent extractive multi-document sum-
marization (MDS).1 Where previous work on
MDS considered sentence selection and or-
dering separately, G-FLOW introduces a joint
model for selection and ordering that balances
coherence and salience. G-FLOW?s core rep-
resentation is a graph that approximates the
discourse relations across sentences based on
indicators including discourse cues, deverbal
nouns, co-reference, and more. This graph en-
ables G-FLOW to estimate the coherence of a
candidate summary.
We evaluate G-FLOW on Mechanical Turk,
and find that it generates dramatically bet-
ter summaries than an extractive summarizer
based on a pipeline of state-of-the-art sentence
selection and reordering components, under-
scoring the value of our joint model.
1 Introduction
The goal of multi-document summarization (MDS)
is to produce high quality summaries of collections
of related documents. Most previous work in ex-
tractive MDS has studied the problems of sentence
selection (e.g., (Radev, 2004; Haghighi and Vander-
wende, 2009)) and sentence ordering (e.g., (Lapata,
2003; Barzilay and Lapata, 2008)) separately, but
we believe that a joint model is necessary to produce
coherent summaries. The intuition is simple: if the
sentences in a summary are first selected?without
regard to coherence?then a satisfactory ordering of
the selected sentences may not exist.
1System and data at http://knowitall.cs.washington.edu/gflow/
doc1: Bomb-
ing in
Jerusalem
doc1: Anger
from Israelis
doc1: Suspen-
sion of peace
accord due to
bombing
doc2: Hamas
claims respon-
sibility
doc5: Pales-
tinians con-
demn attack
doc4: Mubarak
urges peace
accord
doc5: Pales-
tinians urge
peace accord
doc3: Clinton
urges peace
accord
Figure 1: An example of a discourse graph covering a
bombing and its aftermath, indicating the source docu-
ment for each node. A coherent summary should begin
with the bombing and then describe the reactions. Sen-
tences are abbreviated for compactness.
An extractive summary is a subset of the sen-
tences in the input documents, ordered in some
way.2 Of course, most possible summaries are in-
coherent. Now, consider a directed graph where the
nodes are sentences in the collection, and each edge
represents a pairwise ordering constraint necessary
for a coherent summary (see Figure 1 for a sample
graph). By definition, any coherent summary must
obey the constraints in this graph.
Previous work has constructed similar graphs au-
tomatically for single document summarization and
manually for MDS (see Section 2). Our system,
G-FLOW extends this research in two important
ways. First, it tackles automatic graph construction
for MDS, which requires novel methods for identi-
fying inter-document edges (Section 3). It uses this
2We focus exclusively on extractive summaries, so we drop
the word ?extractive? henceforth.
1163
State-of-the-art MDS system G-FLOW
? The attack took place Tuesday near Cailaco in East Timor, a
former Portuguese colony, according to a statement issued by the
pro-independence Christian Democratic Union of East Timor.
? The United Nations does not recognize Indonesian claims to East
Timor.
? In a decision welcomed as a landmark by Portugal, European Union
leaders Saturday backed calls for a referendum to decide the fate of East
Timor, the former Portuguese colony occupied by Indonesia since 1975.
? Indonesia invaded East Timor in 1975 and annexed it the following
year.
? Bhichai Rattakul, deputy prime minister and president of the
Bangkok Asian Games Organizing Committee, asked the Foreign
Ministry to urge the Saudi government to reconsider withdrawing
its 105-strong team.
? The games will be a success.
? Thailand won host rights for the quadrennial games in 1995, but
setbacks in preparations led officials of the Olympic Council of Asia late
last year to threaten to move the games to another country.
? Thailand showed its nearly complete facilities for the Asian Games to
a tough jury Thursday - the heads of the organizing committees from the
43 nations competing in the December event.
Table 1: Pairs of sentences produced by a pipeline of a state-of-the-art sentence extractor (Lin and Bilmes, 2011) and
sentence orderer (Li et al, 2011a), and by G-FLOW.
graph to estimate coherence of a candidate summary.
Second, G-FLOW introduces a novel methodology
for joint sentence selection and ordering (Section 4).
It casts MDS as a constraint optimization problem
where salience and coherence are soft constraints,
and redundancy and summary length are hard con-
straints. Because this optimization problem is NP-
hard, G-FLOW uses local search to approximate it.
We report on a Mechanical Turk evaluation that
directly compares G-FLOW to state-of-the-art MDS
systems. Using DUC?04 as our test set, we com-
pare G-FLOW against a combination of an extractive
summarization system with state-of-the-art ROUGE
scores (Lin and Bilmes, 2011) followed by a state-
of-the-art sentence reordering scheme (Li et al,
2011a). We also compare G-FLOW to a combina-
tion of an extractive system with state-of-the-art co-
herence scores (Nobata and Sekine, 2004) followed
by the reordering system. In both cases participants
substantially preferred G-FLOW. Participants chose
G-FLOW 54% of the time when compared to Lin,
and chose Lin?s system 22% of the time. When com-
pared to Nobata, participants chose G-FLOW 60%
of the time, and chose Nobata only 20% of the time.
The remainder of the cases were judged equivalent.
A further analysis shows that G-FLOW?s sum-
maries are judged superior along several dimensions
suggested in the DUC?04 evaluation (including co-
herence, repetitive text, and referents). A compar-
ison against manually written, gold standard sum-
maries, reveals that while the gold standard sum-
maries are preferred in direct comparisons, G-FLOW
has nearly equivalent scores on almost all dimen-
sions suggested in the DUC?04 evaluation.
The paper makes the following contributions:
? We present G-FLOW, a novel MDS system that
jointly solves the sentence selection and order-
ing problems to produce coherent summaries.
? G-FLOW automatically constructs a domain-
independent graph of ordering constraints over
sentences in a document collection, based on
syntactic cues and redundancy across docu-
ments. This graph is the backbone for estimat-
ing the coherence of a summary.
? We perform human evaluation on blind test
sets and find that G-FLOW dramatically outper-
forms state-of-the-art MDS systems.
2 Related Work
Most existing research in multi-document summa-
rization (MDS) focuses on sentence selection for in-
creasing coverage and does not consider coherence
of the summary (Section 2.1). Although coherence
has been used in ordering of summary sentences
(Section 2.2), this work is limited by the quality of
summary sentences given as input. In contrast, G-
FLOW incorporates coherence in both selection and
ordering of summary sentences.
G-FLOW can be seen as an instance of discourse-
driven summarization (Section 2.3). There is prior
work in this area, but primarily for summarization of
single documents. There is some preliminary work
on the use of manually-created discourse models in
MDS. Our approach is fully automated.
2.1 Subset Selection in MDS
Most extractive summarization research aims to in-
crease the coverage of concepts and entities while
reducing redundancy. Approaches include the use of
maximum marginal relevance (Carbonell and Gold-
stein, 1998), centroid-based summarization (Sag-
gion and Gaizauskas, 2004; Radev et al, 2004), cov-
1164
ering weighted scores of concepts (Takamura and
Okumura, 2009; Qazvinian et al, 2010), formula-
tion as minimum dominating set problem (Shen and
Li, 2010), and use of submodularity in sentence se-
lection (Lin and Bilmes, 2011). Graph centrality has
also been used to estimate the salience of a sentence
(Erkan and Radev, 2004). Approaches to content
analysis include generative topic models (Haghighi
and Vanderwende, 2009; Celikyilmaz and Hakkani-
Tur, 2010; Li et al, 2011b), and discriminative mod-
els (Aker et al, 2010).
These approaches do not consider coherence as
one of the desiderata in sentence selection. More-
over, they do not attempt to organize the selected
sentences into an intelligible summary. They are
often evaluted by ROUGE (Lin, 2004), which is
coherence-insensitive. In practice, these approaches
often result in incoherent summaries.
2.2 Sentence Reordering
A parallel thread of research has investigated taking
a set of summary sentences as input and reordering
them to make the summary fluent. Various algo-
rithms use some combination of topic-relatedness,
chronology, precedence, succession, and entity co-
herence for reordering sentences (Barzilay et al,
2001; Okazaki et al, 2004; Barzilay and Lapata,
2008; Bollegala et al, 2010). Recent work has also
used event-based models (Zhang et al, 2010) and
context analysis (Li et al, 2011a).
The hypothesis in this research is that a pipelined
combination of subset selection and reordering will
produce high-quality summaries. Unfortunately,
this is not true in practice, because sentences are se-
lected primarily for coverage without regard to co-
herence. This methodology often leads to an inad-
vertent selection of a set of disconnected sentences,
which cannot be put together in a coherent sum-
mary, irrespective of how the succeeding algorithm
reorders them. In our evaluation, reordering had lim-
ited impact on the quality of the summaries.
2.3 Coherence Models and Summarization
Research on discourse analysis of documents pro-
vides a basis for modeling coherence in a docu-
ment. Several theories have been developed for
modeling discourse, e.g., Centering Theory, Rhetor-
ical Structure Theory (RST), Penn Discourse Tree-
Bank (Grosz and Sidner, 1986; Mann and Thomp-
son, 1988; Wolf and Gibson, 2005; Prasad et al,
2008). Numerous discourse-guided summariza-
tion algorithms have been developed (Marcu, 1997;
Mani, 2001; Taboada and Mann, 2006; Barzilay and
Elhadad, 1997; Louis et al, 2010). However, these
approaches have been applied to single document
summarization and not to MDS.
Discourse models have seen some application to
summary generation in MDS, for example, using a
detailed semantic representation of the source texts
(McKeown and Radev, 1995; Radev and McKe-
own, 1998). A multi-document extension of RST
is Cross-document Structure Theory (CST), which
has been applied to MDS (Zhang et al, 2002; Jorge
and Pardo, 2010). However, these systems require
a stronger input, such as a manual CST-annotation
of the set of documents. Our work can be seen as
an instance of summarization based on lightweight
CST. However, a key difference is that our proposed
algorithm is completely automated and does not re-
quire any additional human annotation. Addition-
ally, while incorporating coherence into selection,
this work does not attempt to order the sentences
coherently, while our approach performs joint selec-
tion and ordering.
Discourse models have also been used for evalu-
ating summary quality (Barzilay and Lapata, 2008;
Louis and Nenkova, 2009; Pitler et al, 2010). Fi-
nally, there is work on generating coherent sum-
maries in specific domains, such as scientific articles
(Saggion and Lapalme, 2002; Abu-Jbara and Radev,
2011) using domain-specific cues like citations. In
contrast, our work generates summaries without any
domain-specific knowledge. Other research has fo-
cused on identifying coherent threads of documents
rather than sentences (Shahaf and Guestrin, 2010).
3 Discourse Graph
As described in Section 1, our goal is to identify
pairwise ordering constraints over a set of input sen-
tences. These constraints specify a multi-document
discourse graph, which is used by G-FLOW to eval-
uate the coherence of a candidate summary.
In this graph G, each vertex is a sentence and an
edge from si to sj indicates that sj can be placed
right after si in a coherent summary. In other words,
the two share a discourse relationship. In the fol-
1165
lowing three sentences (from possibly different doc-
uments) there should be an edge from s1 to s2, but
not between s3 and the other sentences:
s1 Militants attacked a market in Jerusalem.
s2 Arafat condemned the bombing.
s3 The Wye River Accord was signed in Oct.
Discourse theories have proposed a variety of re-
lationships between sentences such as background
and interpretation. RST has 17 such relations (Mann
and Thompson, 1988) and PDTB has 16 (Prasad et
al., 2008). While we seek to identify pairs of sen-
tences that have a relationship, we do not attempt to
label the edges with the exact relation.
We use textual cues from the discourse literature
in combination with the redundancy inherent in re-
lated documents to generate edges. Because this
methodology is noisy, the graph used by G-FLOW is
an approximation, which we refer to as an approx-
imate discourse graph (ADG). We first describe the
construction of this graph, and then discuss the use
of the graph for summary generation (Section 4).
3.1 Deverbal Noun Reference
Often, the main description of an event is mentioned
in a verbal phrase and subsequent references use
deverbal nouns (nominalization of verbs) (e.g., ?at-
tacked? and ?the attack?). In this example, the noun
is derivationally related to the verb, but that is not al-
ways the case. For example, ?bombing? in s2 above
refers to ?attacked? in s1.
We identify verb-noun pairs with this relationship
as follows. First, we locate a set of candidate pairs
from WordNet: for each verb v, we determine po-
tential noun references n using a path length of up to
two in WordNet (moving from verb to noun is pos-
sible via WordNet?s ?derivationally related? links).
This set captures verb-noun pairs such as (?to at-
tack?, ?bombing?), but also includes generic pairs
such as (?to act?, ?attack?). To filter such errors
we score the candidate references. Our goal is to
emphasize common pairs and to deemphasize pairs
with common verbs or verbs that map to many
nouns. To this end, we score pairs by (c/p) ? (c/q),
where c is the number of times the pair (v, n) ap-
pears in adjacent sentences, p is the number of times
the verb appears, and q is the number of times that
v appears with a different noun. We generate these
statistics over a background corpus of 60,000 arti-
cles from the New York Times and Reuters, and
filter out candidate pairs scoring below a threshold
identified over a small training set.
We construct edges in the ADG between pairs of
sentences containing these verb to noun mappings.
To our knowledge, we are the first to use deverbal
nouns for summarization.
3.2 Event/Entity Continuation
Our second indicator is related to lexical chains
(Barzilay and Lapata, 2008). We add an edge in
the ADG from a sentence si to sj if they contain
the same event or entity and the timestamp of si is
less than or equal to the timestamp of sj (timestamps
generated with (Chang and Manning, 2012)).
3.3 Discourse Markers
We use 36 explicit discourse markers (e.g., ?but?,
?however?, ?moreover?) to identify edges between
two adjacent sentences of a document (Marcu and
Echihabi, 2002). This indicator lets us learn an edge
from s4 to s5 below:
s4 Arafat condemned the bombing.
s5 However, Netanyahu suspended peace talks.
3.4 Inferred Edges
We exploit the redundancy of information in MDS
documents to infer edges to related sentences. An
edge (s, s??) can be inferred if there is an existing
edge (s, s?) and s? and s?? express similar informa-
tion. As an example, the edge (s6, s7) can be in-
ferred based on edge (s4, s5):
s6 Arafat condemned the attack.
s7 Netanyahu has suspended the talks.
To infer edges we need an algorithm to identify
sentences expressing similar information. To iden-
tify these pairs, we extract Open Information Extrac-
tion (Banko et al, 2007) relational tuples for each
sentence, and we mark any pair of sentences with
an equivalent relational tuple as redundant (see Sec-
tion 4.3). The inferred edges allow us to propagate
within-document discourse information to sentences
from other documents.
3.5 Co-referent Mentions
A sentence sj will not be clearly understood in iso-
lation and may need another sentence si in its con-
text, if sj has a general reference (e.g., ?the presi-
1166
dent?) pointing to a specific entity or event in si (e.g.,
?President Bill Clinton?). We construct edges based
on coreference mentions, as predicted by Stanford?s
coreference system (Lee et al, 2011). We are able
to identify syntactic edge (s8, s9):
s8 Pres. Clinton expressed sympathy for Israel.
s9 He said the attack should not derail the deal.
3.6 Edge Weights
We weight each edge in the ADG by adding the
number of distinct indicators used to construct that
edge ? if sentences s and s? have an edge because
of a discourse marker and a deverbal reference, the
edge weight wG(s, s?) will be two. We also include
negative edges in the ADG. wG(s, s?) is negative if
s? contains a deverbal noun reference, a discourse
marker, or a co-reference mention that is not fulfilled
by s. For example, if s? contains a discourse marker,
and s is neither the sentence directly preceding s?
and there is no inferred discourse link between s and
s?, then we will add a negative edge wG(s, s?).
3.7 Preliminary Graph Evaluation
We evaluated the quality of the ADG used by G-
FLOW, which is important not only for its use in
MDS, but also because the ADG may be used for
other applications like topic tracking and decompos-
ing an event into sub-events. One author randomly
chose 750 edges and labeled an edge correct if the
pair of sentences did have a discourse relationship
between them and incorrect otherwise. 62% of the
edges accurately reflected a discourse relationship.
Our ADG has on average 31 edges per sentence for
a dataset in which each document cluster has on av-
erage 253 sentences. This evaluation includes only
the positive edges.
4 Summary Generation
We denote a candidate summary X to be a sequence
of sentences ?x1, x2, . . . , x|X|?. G-FLOW?s summa-
rization algorithm searches through the space of or-
dered summaries and scores each candidate sum-
mary along the dimensions of coherence (Section
4.1), salience (Section 4.2) and redundancy (Section
4.3). G-FLOW returns the summary that maximizes
a joint objective function (Section 4.4).
weight feature
-0.037 position in document
0.033 from first three sentences
-0.035 number of people mentions
0.111 contains money
0.038 sentence length > 20
0.137 length of sentence
0.109 #sentences verbs appear in (any form)
0.349 #sentences common nouns appear in
0.355 #sentences proper nouns appear in
Table 2: Linear regression features for salience.
4.1 Coherence
G-FLOW estimates coherence of a candidate sum-
mary via the ADG. We define coherence as the sum
of edge weights between successive summary sen-
tences. For disconnected sentence pairs, the edge
weight is zero.
Coh(X) =
?
i=1..|X|?1
wG+(xi, xi+1) + ?wG?(xi, xi+1)
wG+ represents positive edges and wG? represents
negative edge weights. ? is a tradeoff coefficient for
positive and negative weights, which is tuned using
the methodology described in Section 4.4.
4.2 Salience
Salience is the inherent value of each sentence to
the documents. We compute salience of a summary
(Sal(X)) as the sum of the saliences of individual
sentences (
?
i Sal(xi)).
To estimate salience of a sentence, G-FLOW uses
a linear regression classifier trained on ROUGE
scores over the DUC?03 dataset. The classifier uses
surface features designed to identify sentences that
cover important concepts. The complete list of fea-
tures and learned weights is in Table 2. The clas-
sifier finds a sentence more salient if it mentions
nouns or verbs that are present in more sentences
across the documents. The highest ranked features
are the last three ? number of other sentences that
mention a noun or a verb in the given sentence. We
use the same procedure as in deverbal nouns for de-
tecting verb mentions that appear as nouns in other
sentences (Section 3.1).
4.3 Redundancy
We also wish to avoid redundancy. G-FLOW first
processes each sentence with a state-of-the-art Open
Information extractor OLLIE (Mausam et al, 2012),
which converts a sentence into its component re-
lational tuples of the form (arg1, relational phrase,
1167
arg2).3 For example, it finds (Militants, bombed, a
marketplace) as a tuple from sentence s12.
Two sentences will express redundant information
if they both contain the same or synonymous com-
ponent fact(s). Unfortunately, detecting synonymy
even at relational tuple level is very hard. G-FLOW
approximates this synonymy by considering two re-
lational tuples synonymous if the relation phrases
contain verbs that are synonyms of each other, have
at least one synonymous argument, and are times-
tamped within a day of each other. Because the in-
put documents cover related events, these relatively
weak rules provide good performance. The same
algorithm is used for inferring edges for the ADG
(Section 3.4). This algorithm can detect that the fol-
lowing sentences express redundant information:
s12 Militants bombed a marketplace in Jerusalem.
s13 He alerted Arafat after assailants attacked the
busy streets of Mahane Yehuda.
4.4 Objective Function
The objective function needs to balance coherence,
salience and redundancy and also honor the given
budget, i.e., maximum summary lengthB. G-FLOW
treats redundancy and budget as hard constraints and
coherence and salience as soft. Coherence is neces-
sarily soft as the graph is approximate. While previ-
ous MDS systems specifically maximized coverage,
in preliminary experiments on a development set, we
found that adding a coverage term did not improve
G-FLOW?s performance. We optimize:
maximize: F (x) , Sal(X) + ?Coh(X)? ?|X|
s.t.
?
i=1..|X| len(xi) < B
?xi, xj ? X : redundant(xi, xj) = 0
Here len refers to the sentence length. We add |X|
term (the number of sentences in the summary) to
avoid picking many short sentences, which may in-
crease coherence and salience scores at the cost of
overall summary quality.
The parameters ?, ? and ? (see Section 4.1) are
tuned automatically using a grid search over a de-
velopment set as follows. We manually generate ex-
tractive summaries for each document cluster in our
development set (DUC?03) and choose the parame-
ter setting that minimizes |F (XG-FLOW) ? F (X?)|
3Available from http://ollie.cs.washington.edu
summed over all document clusters. F is the objec-
tive function, XG-FLOW is the summary produced by
G-FLOW and X? is the manual summary.
This constraint optimization problem is NP hard,
which can be shown by using a reduction of the
longest path problem. For this reason, G-FLOW uses
local search to reach an approximation of the opti-
mum. G-FLOW employs stochastic hill climbing
with random restarts as the base search algorithm.
At each step, the search either adds a sentence, re-
moves a sentence, replaces a sentence by another, or
reorders a pair of sentences. The initial summary for
random restarts is constructed as follows. We first
pick the highest salience sentence with no incoming
negative edges as the first sentence. The following
sentences are probabilistically added one at a time
based on the summary score up to that sentence. The
initial summary is complete when there are no possi-
ble sentences left to fit within the budget. Intuitively,
this heuristic chooses a good starting point by se-
lecting a first sentence that does not rely on context
and subsequent sentences that build a high scoring
summary. As with all local search algorithms, this
algorithm is highly scalable and can easily apply to
large collections of related documents, but does not
guarantee global optima.
5 Experiments
Because summaries are intended for human con-
sumption we focused on human evaluations. We
hired workers on Amazon Mechanical Turk (AMT)
to evaluate the summaries. Our evaluation addresses
the following questions: (1) how do G-FLOW sum-
maries compare against the state-of-the-art in MDS
(Section 5.2)? (2) what is G-FLOW?s performance
along important summarization dimensions such as
coherence and redundancy (Section 5.3)? (3) how
does G-FLOW perform on coverage as measured
by ROUGE (Section 5.3.1)? (4) how much do the
components of G-FLOW?s objective function con-
tribute to performance (Section 5.4)? (5) how do G-
FLOW?s summaries compare to human summaries?
5.1 Data and Systems
We evaluated the systems on the Task 2 DUC?04
multi-document summarization dataset. This dataset
consists of 50 clusters of related documents, each of
which contains 10 documents. Each cluster of doc-
1168
uments also includes four gold standard summaries
used for evaluation. As in the DUC?04 competition,
we allowed 665 bytes for each summary including
spaces and punctuation. We used DUC?03 as our
development set, which contains 30 document clus-
ters, again with approximately 10 documents each.
We compared G-FLOW against four systems. The
first is a recent MDS extractive summarizer, which
we choose for its state-of-the-art ROUGE scores
(Lin and Bilmes, 2011).4 The second is a pipeline
of Lin?s system followed by a reimplementation of
a state-of-the-art sentence reordering system (Li et
al., 2011a). We refer to these systems as LIN and
LIN-LI, respectively. This second baseline allows
us to quantify the advantage of using coherence as a
factor in both sentence extraction and ordering.
We also compare against the system that had the
highest coherence ratings at DUC?04 (Nobata and
Sekine, 2004), which we refer to as NOBATA. As
this system did not preform sentence ordering on its
output, we also compare against a pipeline of No-
bata?s system and the sentence reordering system.
We refer to this system as NOBATA-LI.
Lastly, to evaluate how well the system performs
against human generated summaries, we compare
against the gold standard summaries provided by
DUC.
5.2 Overall Summary Quality
Following (Haghighi and Vanderwende, 2009) and
(Celikyilmaz and Hakkani-Tur, 2010), to compare
overall summary quality, we asked AMT workers
to compare two candidate system summaries. The
workers first read a gold standard summary, fol-
lowed by the two system summaries, and were then
asked to choose the better summary from the pair.
The system summaries were shown in a random or-
der to remove any bias.
To ensure that workers provided high quality data
we added two quality checks. First, we restricted
to workers who have an overall approval rating of
over 95% on AMT. Second, we asked the workers
to briefly describe the main events of the summary.
We manually filtered out work where this descrip-
tion was incorrect.
4We thank Lin and Bilmes for providing us with their code.
Unfortunately, we were unable to obtain other recent MDS sys-
tems from their authors.
Six workers compared each pair of summaries.
We recorded the scores for each cluster, and report
three numbers: the percentages of clusters where a
system is more often preferred over the other and the
percentage where the two systems are tied. G-FLOW
is preferred almost three times as often as LIN:
G-FLOW Indifferent LIN
56% 24% 20%
Next, we compared G-FLOW and LIN-LI. Sen-
tence reordering improves performance, but G-
FLOW is still overwhelmingly preferred:
G-FLOW Indifferent LIN-LI
54% 24% 22%
These results suggest that incorporating coher-
ence in sentence extraction adds significant value to
a summarization system. In these experiments, LIN
and LIN-LI are preferred in some cases. We an-
alyzed those summaries more carefully, and found
that occasionally, G-FLOW will sacrifice a small
amount of coverage for coherence, resulting in lower
performance in those cases (see Section 5.3.1).
We also compared LIN and LIN-LI, and found
that reordering does not improve performance by
much.
LIN-LI Indifferent LIN
32% 38% 30%
While the scores presented above represent com-
parisons between G-FLOW and a summarization
system with state-of-the-art ROUGE scores, we
also compared against a summarization system with
state-of-the-art coherence scores ? the system with
the highest coherence scores from DUC?04, (No-
bata and Sekine, 2004). We found that G-FLOW was
again preferred:
G-FLOW Indifferent NOBATA
68% 10% 22%
Adding in sentence ordering again improved the
scores for the comparison system somewhat:
G-FLOW Indifferent NOBATA-LI
60% 20% 20%
While these scores show a significant improve-
ment over previous sytems, they do not convey how
well G-FLOW compares to the gold standard ? man-
ually generated summaries. As a final experiment,
we compared G-FLOW and a second, manually gen-
erated summary:
1169
G-FLOW Indifferent Gold
14% 18% 68%
While we were pleased that in 32% of the cases,
Turkers either preferred G-FLOW or were indiffer-
ent, there is clearly a lot of room for improvement
despite the gains reported over previous sytems.
5.3 Comparison along Summary Dimensions
A high quality summary needs to be good along sev-
eral dimensions. We asked AMT workers to rate
summaries using the quality questions enumerated
in DUC?04 evaluation scheme.5 These questions
concern: (1) coherence, (2) useless, confusing, or
repetitive text, (3) redundancy, (4) nouns, pronouns,
and personal names that are not well-specified (5)
entities rementioned in an overly explicit way, (6)
ungrammatical sentences, and (7) formatting errors.
We evaluated G-FLOW LIN-LI and NOBATA-LI
against the gold standard summaries, using the same
AMT scheme as in the previous section. To assess
automated performance with respect to the standards
set by human summaries, we also evaluated a (dif-
ferent) gold standard summary for each document
cluster, using the same Mechanical Turk scheme as
in the previous section. The 50 summaries produced
by each system were evaluated by four workers. The
results are shown in Figure 2.
G-FLOW was rated significantly better than LIN-
LI in all categories except ?Redundancy? and signif-
icant better than NOBATA-LI on ?Coherence? and
?Referents?. The ratings for ?Coherence?, ?Refer-
ents?, and ?OverlyExplicit? are not surprising given
G-FLOW?s focus on coherence. The results for
?UselessText? may also be due to G-FLOW?s focus
on coherence which ideally prevents it from getting
off topic. Lastly, G-FLOW may perform better on
?Grammatical? and ?Formatting? because it tends to
choose longer sentences than other systems, which
are less likely to be sentence segmentation errors.
There may also be some bleeding from one dimen-
sion to the other ? if a worker likes one summary she
may score it highly for many dimensions.
Finally, somewhat surprisingly, we find G-
FLOW?s performance to be nearly that of human
summaries. G-FLOW is rated statistically signifi-
cantly lower than the gold summaries on only ?Re-
5http://duc.nist.gov/duc2004/quality.questions.txt
System R F
NOBATA 30.44 34.36
Best system in DUC-04 38.28 37.94
Takamura and Okumura (2009) 38.50 -
LIN 39.35 38.90
G-FLOW 37.33 37.43
Gold Standard Summaries 40.03 40.03
Table 3: ROUGE-1 recall and F-measure results (%) on
DUC-04. Some values are missing because not all sys-
tems reported both F-measure and recall.
dundancy?. Given the results from the previous sec-
tion, G-FLOW is likely performing worse on cate-
gories not conveyed in these scores, such as Cover-
age, which we examine next.
5.3.1 Coverage Evaluation using ROUGE
Most recent research has focused on the ROUGE
evaluation, and thus implicitly on coverage of in-
formation in a summary. To estimate the coverage
of G-FLOW, we compared the systems on ROUGE
(Lin, 2004). We calculated ROUGE-1 scores for
G-FLOW, LIN, and NOBATA.6 As sentence order-
ing does not matter for ROUGE, we do not include
LIN-LI or NOBATA-LI in this evaluation. Because
our algorithm does not explicitly maximize coverage
while LIN does, we expected G-FLOW to perform
slightly worse than LIN.
The ROUGE-1 scores for G-FLOW, LIN, NO-
BATA and other recent MDS systems are listed in Ta-
ble 3. We also include the ROUGE-1 scores for the
gold summaries (compared to the other gold sum-
maries). G-FLOW has slightly lower scores than
LIN and the gold standard summaries, but much
higher scores than NOBATA. G-FLOW only scores
significantly lower than LIN and the gold standard
summaries.
We can conclude that good summaries have both
the characteristics listed in the quality dimensions,
and good coverage. The gold standard summaries
outperform G-FLOW on both ROUGE scores and
the quality dimension scores, and therefore, out-
perform G-FLOW on overall comparison. How-
ever, G-FLOW is preferred to LIN-LI in addition to
NOBATA-LI indicating that its quality scores out-
weigh its ROUGE scores in that comparison. An
improvement to G-FLOW may focus on increasing
6ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n
4 -w 1.2
1170
Coherence UselessText Redundancy Referents OverlyExplicit Grammatical Formatting
Rat
ing
0
1
2
3
4
GoldG?FlowNobata?LiLin?Li
Figure 2: Ratings for the systems. 0 is the lowest possible score and 4 is the highest possible score. G-FLOW is rated
significantly higher than LIN-LI on all categories, except for ?Redundancy?, and significantly higher than NOBATA-LI
on ?Coherence? and ?Referents?. G-FLOW is only significantly lower than the gold standard on ?Redundancy?.
coverage while retaining strengths such as coher-
ence.
5.4 Ablation Experiments
In this ablation study, we evaluated the contribution
of the main components of G-FLOW ? coherence
and salience. The details of the experiments are the
same as in the experiment described in Section 5.2.
We first measured the importance of coherence in
summary generation. This system G-FLOW-SAL is
identical to the full system except that it does not
include the coherence term in the objective function
(see Section 4.4). The results show that coherence is
very important to G-FLOW?s performance:
G-FLOW Indifferent G-FLOW-SAL
54% 26% 20%
Similarly, we evaluated the contribution of
salience. This system G-FLOW-COH does not in-
clude the salience term in the objective function:
G-FLOW Indifferent G-FLOW-COH
60% 20% 20%
Without salience, the system produces readable,
but highly irrelevant summaries.
5.5 Agreement of Expert & AMT Workers
Because summary evaluation is a relatively complex
task, we compared AMT workers? annotations with
expert annotations from DUC?04. We randomly
selected ten summaries from each of the seven
DUC?04 annotators, and asked four Turk workers
to annotate them on the DUC?04 quality questions.
For each DUC?04 annotator, we selected all pairs
of summaries where one summary was judged more
than one point better than the other summary. We
compared whether the workers (voting as in Sec-
tion 5.2) likewise judged that summary better than
the second summary. We found that the annotations
agreed in 75% of cases. When we looked only at
pairs more than two points different, the agreement
was 80%. Thus, given the subjective nature of the
task, we feel reasonably confident that the AMT an-
notations are informative, and that the dramatic pref-
erence of G-FLOW over the baseline systems is due
to a substantial improvement in its summaries.
6 Conclusion
In this paper, we present G-FLOW, a multi-
document summarization system aimed at generat-
ing coherent summaries. While previous MDS sys-
tems have focused primarily on salience and cov-
erage but not coherence, G-FLOW generates an or-
dered summary by jointly optimizing coherence and
salience. G-FLOW estimates coherence by using
an approximate discourse graph, where each node
is a sentence from the input documents and each
edge represents a discourse relationship between
two sentences. Manual evaluations demonstrate that
G-FLOW generates substantially better summaries
than a pipeline of state-of-the-art sentence selec-
tion and reordering components. ROUGE scores,
which measure summary coverage, show that G-
FLOW sacrifices a small amount of coverage for
overall readability and coherence. Comparisons to
gold standard summaries show that G-FLOW must
improve in coverage to equal the quality of manu-
ally written summaries. We believe this research has
applications to other areas of summarization such as
update summarization and query based summariza-
tion, and we are interested in investigating these top-
ics in future work.
1171
Acknowledgements
We thank Luke Zettlemoyer, Lucy Vanderwende, Hal
Daume III, Pushpak Bhattacharyya, Chris Quirk, Erik
Frey, Tony Fader, Michael Schmitz, Alan Ritter, Melissa
Winstanley, and the three anonymous reviewers for help-
ful conversations and feedback on earlier drafts. We also
thank Lin and Bilmes for providing us with the code for
their system. This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-11-1-0294, and
DARPA contract FA8750-13-2-0019, and carried out at
the University of Washington?s Turing Center. This pa-
per was also supported in part by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via Air Force
Research Laboratory (AFRL) contract number FA8650-
10-C-7058. The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. The
views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, either ex-
pressed or implied, of IARPA, AFRL, or the U.S. Gov-
ernment.
References
Amjad Abu-Jbara and Dragomir R. Radev. 2011. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of ACL 2011, pages 500?509.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using A * search and
discriminative training. In Proceedings of EMNLP
2010.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI 2007, pages 68?74.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay, Noemie Elhadad, and Kathleen R McK-
eown. 2001. Sentence ordering in multidocument
summarization. In Proceedings of HLT 2001, pages
1?7.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. Informa-
tion Process Management, 46(1):89?109.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?824.
Angel Chang and Christopher Manning. 2012. SU-
TIME: A library for recognizing and normalizing time
expressions. In Proceedings of LREC 2012.
Gunes Erkan and Dragomir R Radev. 2004. LexRank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research,
22(1):457?479.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Maria Lucia Castro Jorge and Thiago Alexan-
dre Salgueiro Pardo. 2010. Multi-Document
Summarization: Content Selection based on CST
Model (Cross-document Structure Theory). Ph.D.
thesis, Nu?cleo Interinstitucional de Lingu???stica
Computacional (NILC).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL 2003, pages 545?552.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In CoNLL 2011
Shared Task.
Peifeng Li, Guangxi Deng, and Qiaoming Zhu. 2011a.
Using context inference to improve sentence ordering
for multi-document summarization. In Proceedings of
IJCNLP 2011, pages 1055?1061.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011b.
Generating aspect-oriented multi-document summa-
rization with event-aspect model. In Proceedings of
EMNLP 2011, pages 1137?1146.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81.
Annie Louis and Ani Nenkova. 2009. Automatic sum-
mary evaluation without using human models. In Pro-
ceedings of EMNLP 2009, pages 306?314.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit discourse relations. In Proceedings of SIGDIAL
2010, pages 59?62.
1172
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Publishing Co, Amsterdam/Philadelphia.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL 2002, pages 368?375.
Daniel Marcu. 1997. From discourse structures to text
summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, pages 82?88.
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP
2012, pages 523?534.
Kathleen McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of SIGIR 1995, pages 74?82.
Chikashi Nobata and Satoshi Sekine. 2004. Crl/nyu
summarization system at duc-2004. In Proceedings of
DUC 2004.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.
2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of COLING 2004,
pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of ACL
2010, pages 544?554.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of LREC 2008.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of COLING
2010, pages 895?903.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):469?500.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919?938.
Dragomir R. Radev. 2004. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. Jour-
nal of Artificial Intelligence Research, 22(1):457?479.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile relevance
and redundancy removal. In Proceedings of DUC
2004.
Horacio Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497?526.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting the
dots between news articles. In Proceedings of KDD
2010, pages 623?632.
Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proceed-
ings of Coling 2010, pages 984?992.
Maite Taboada and William C. Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse Studies,
8(4):567?588.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL 2009,
pages 781?789.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?288.
Zhu Zhang, Sasha Blair-Goldensohn, and Dragomir R.
Radev. 2002. Towards CST-enhanced summarization.
In Proceedings of AAAI 2002, pages 439?445.
Renxian Zhang, Li Wenjie, and Lu Qin. 2010. Sen-
tence ordering with event-enriched semantics and two-
layered clustering for multi-document news summa-
rization. In Proceedings of COLING 2010, pages
1489?1497.
1173
Proceedings of the ACL 2010 Conference Short Papers, pages 286?290,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extracting Sequences from the Web
Anthony Fader, Stephen Soderland, and Oren Etzioni
University of Washington, Seattle
{afader,soderlan,etzioni}@cs.washington.edu
Abstract
Classical Information Extraction (IE) sys-
tems fill slots in domain-specific frames.
This paper reports on SEQ, a novel
open IE system that leverages a domain-
independent frame to extract ordered se-
quences such as presidents of the United
States or the most common causes of death
in the U.S. SEQ leverages regularities
about sequences to extract a coherent set
of sequences from Web text. SEQ nearly
doubles the area under the precision-recall
curve compared to an extractor that does
not exploit these regularities.
1 Introduction
Classical IE systems fill slots in domain-specific
frames such as the time and location slots in sem-
inar announcements (Freitag, 2000) or the terror-
ist organization slot in news stories (Chieu et al,
2003). In contrast, open IE systems are domain-
independent, but extract ?flat? sets of assertions
that are not organized into frames and slots
(Sekine, 2006; Banko et al, 2007). This paper
reports on SEQ?an open IE system that leverages
a domain-independent frame to extract ordered se-
quences of objects from Web text. We show that
the novel, domain-independent sequence frame in
SEQ substantially boosts the precision and recall
of the system and yields coherent sequences fil-
tered from low-precision extractions (Table 1).
Sequence extraction is distinct from set expan-
sion (Etzioni et al, 2004; Wang and Cohen, 2007)
because sequences are ordered and because the ex-
traction process does not require seeds or HTML
lists as input.
The domain-independent sequence frame con-
sists of a sequence name s (e.g., presidents of the
United States), and a set of ordered pairs (x, k)
where x is a string naming a member of the se-
quence with name s, and k is an integer indicating
Most common cause of death in the United States:
1. heart disease, 2. cancer, 3. stroke, 4. COPD,
5. pneumonia, 6. cirrhosis, 7. AIDS, 8. chronic liver
disease, 9. sepsis, 10. suicide, 11. septic shock.
Largest tobacco company in the world:
1. Philip Morris, 2. BAT, 3. Japan Tobacco,
4. Imperial Tobacco, 5. Altadis.
Largest rodent in the world:
1. Capybara, 2. Beaver, 3. Patagonian Cavies. 4. Maras.
Sign of the zodiac:
1. Aries, 2. Taurus, 3. Gemini, 4. Cancer, 5. Leo,
6. Virgo, 7. Libra, 8. Scorpio, 9. Sagittarius,
10. Capricorn, 11. Aquarius, 12. Pisces, 13. Ophiuchus.
Table 1: Examples of sequences extracted by SEQ
from unstructured Web text.
its position (e.g., (Washington, 1) and (JFK, 35)).
The task of sequence extraction is to automatically
instantiate sequence frames given a corpus of un-
structured text.
By definition, sequences have two properties
that we can leverage in creating a sequence ex-
tractor: functionality and density. Functionality
means position k in a sequence is occupied by a
single real-world entity x. Density means that if
a value has been observed at position k then there
must exist values for all i < k, and possibly more
after it.
2 The SEQ System
Sequence extraction has two parts: identify-
ing possible extractions (x, k, s) from text, and
then classifying those extractions as either cor-
rect or incorrect. In the following section, we
describe a way to identify candidate extractions
from text using a set of lexico-syntactic patterns.
We then show that classifying extractions based
on sentence-level features and redundancy alone
yields low precision, which is improved by lever-
aging the functionality and density properties of
sequences as done in our SEQ system.
286
Pattern Example
the ORD the fifth
the RB ORD the very first
the JJS the best
the RB JJS the very best
the ORD JJS the third biggest
the RBS JJ the most popular
the ORD RBS JJ the second least likely
Table 2: The patterns used by SEQ to detect ordi-
nal phrases are noun phrases that begin with one
of the part-of-speech patterns listed above.
2.1 Generating Sequence Extractions
To obtain candidate sequence extractions (x, k, s)
from text, the SEQ system finds sentences in its
input corpus that contain an ordinal phrase (OP).
Table 2 lists the lexico-syntactic patterns SEQ uses
to detect ordinal phrases. The value of k is set to
the integer corresponding to the ordinal number in
the OP.1
Next, SEQ takes each sentence that contains an
ordinal phrase o, and finds candidate items of the
form (x, k) for the sequence with name s. SEQ
constrains x to be an NP that is disjoint from o, and
s to be an NP (which may have post-modifying
PPs or clauses) following the ordinal number in o.
For example, given the sentence ?With help
from his father, JFK was elected as the 35th Pres-
ident of the United States in 1960?, SEQ finds
the candidate sequences with names ?President?,
?President of the United States?, and ?President of
the United States in 1960?, each of which has can-
didate extractions (JFK, 35), (his father, 35), and
(help, 35). We use heuristics to filter out many of
the candidate values (e.g., no value should cross a
sentence-like boundary, and x should be at most
some distance from the OP).
This process of generating candidate ex-
tractions has high coverage, but low preci-
sion. The first step in identifying correct ex-
tractions is to compute a confidence measure
localConf(x, k, s|sentence), which measures
how likely (x, k, s) is given the sentence it came
from. We do this using domain-independent syn-
tactic features based on POS tags and the pattern-
based features ?x {is,are,was,were} the kth s? and
?the kth s {is,are,was,were} x?. The features are
then combined using a Naive Bayes classifier.
In addition to the local, sentence-based features,
1Sequences often use a superlative for the first item (k =
1) such as ?the deepest lake in Africa?, ?the second deepest
lake in Africa? (or ?the 2nd deepest ...?), etc.
we define the measure totalConf that takes into
account redundancy in an input corpus C. As
Downey et al observed (2005), extractions that
occur more frequently in multiple distinct sen-
tences are more likely to be correct.
totalConf(x, k, s|C) =
?
sentence?C
localConf(x, k, s|sentence) (1)
2.2 Challenges
The scores localConf and totalConf are not suffi-
cient to identify valid sequence extractions. They
tend to give high scores to extractions where the
sequence scope is too general or too specific. In
our running example, the sequence name ?Presi-
dent? is too general ? many countries and orga-
nizations have a president. The sequence name
?President of the United States in 1960? is too spe-
cific ? there were not multiple U.S. presidents in
1960.
These errors can be explained as violations of
functionality and density. The sequence with
name ?President? will have many distinct candi-
date extractions in its positions, which is a vio-
lation of functionality. The sequence with name
?President of the United States in 1960? will not
satisfy density, since it will have extractions for
only one position.
In the next section, we present the details of how
SEQ incorporates functionality and density into its
assessment of a candidate extraction.
Given an extraction (x, k, s), SEQ must clas-
sify it as either correct or incorrect. SEQ breaks
this problem down into two parts: (1) determining
whether s is a correct sequence name, and (2) de-
termining whether (x, k) is an item in s, assuming
s is correct.
A joint probabilistic model of these two deci-
sions would require a significant amount of la-
beled data. To get around this problem, we repre-
sent each (x, k, s) as a vector of features and train
two Naive Bayes classifiers: one for classifying s
and one for classifying (x, k). We then rank ex-
tractions by taking the product of the two classi-
fiers? confidence scores.
We now describe the features used in the two
classifiers and how the classifiers are trained.
Classifying Sequences To classify a sequence
name s, SEQ uses features to measure the func-
tionality and density of s. Functionality means
287
that a correct sequence with name s has one cor-
rect value x at each position k, possibly with ad-
ditional noise due to extraction errors and synony-
mous values of x. For a fixed sequence name s
and position k, we can weight each of the candi-
date x values in that position by their normalized
total confidence:
w(x|k, s, C) =
totalConf(x, k, s|C)
?
x
? totalConf(x?, k, s|C)
For overly general sequences, the distribution of
weights for a position will tend to be more flat,
since there are many equally-likely candidate x
values. To measure this property, we use a func-
tion analogous to information entropy:
H(k, s|C) = ?
?
x
w(x|k, s, C) log
2
w(x|k, s, C)
Sequences s that are too general will tend to have
high values of H(k, s|C) for many values of k.
We found that a good measure of the overall non-
functionality of s is the average value of H(k, s|C)
for k = 1, 2, 3, 4.
For a sequence name s that is too specific, we
would expect that there are only a few filled-in po-
sitions. We model the density of s with two met-
rics. The first is numFilledPos(s|C), the num-
ber of distinct values of k such that there is some
extraction (x, k) for s in the corpus. The second
is totalSeqConf(s|C), which is the sum of the
scores of most confident x in each position:
totalSeqConf(s|C) =
?
k
max
x
totalConf(x, k, s|C) (2)
The functionality and density features are com-
bined using a Naive Bayes classifier. To train the
classifier, we use a set of sequence names s labeled
as either correct or incorrect, which we describe in
Section 3.
Classifying Sequence Items To classify (x, k)
given s, SEQ uses two features: the total con-
fidence totalConf(x, k, s|C) and the same total
confidence normalized to sum to 1 over all x, hold-
ing k and s constant. To train the classifier, we use
a set of extractions (x, k, s) where s is known to
be a correct sequence name.
3 Experimental Results
This section reports on two experiments. First, we
measured how the density and functionality fea-
tures improve performance on the sequence name
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Both Feature Sets
Only Density
Only Functionality
Max localConf
Figure 1: Using density or functionality features
alone is effective in identifying correct sequence
names. Combining both types of features outper-
forms either by a statistically significant margin
(paired t-test, p < 0.05).
classification sub-task (Figure 1). Second, we
report on SEQ?s performance on the sequence-
extraction task (Figure 2).
To create a test set, we selected all sentences
containing ordinal phrases from Banko?s 500M
Web page corpus (2008). To enrich this set O,
we obtained additional sentences from Bing.com
as follows. For each sequence name s satis-
fying localConf(x, k, s|sentence) ? 0.5 for
some sentence in O, we queried Bing.com for
?the kth s? for k = 1, 2, . . . until no more hits
were returned.2 For each query, we downloaded
the search snippets and added them to our cor-
pus. This procedure resulted in making 95, 611
search engine queries. The final corpus contained
3, 716, 745 distinct sentences containing an OP.
Generating candidate extractions using the
method from Section 2.1 resulted in a set of over
40 million distinct extractions, the vast majority
of which are incorrect. To get a sample with
a significant number of correct extractions, we
filtered this set to include only extractions with
totalConf(x, k, s|C) ? 0.8 for some sentence,
resulting in a set of 2, 409, 211 extractions.
We then randomly sampled and manually la-
beled 2, 000 of these extractions for evaluation.
We did a Web search to verify the correctness of
the sequence name s and that x is the kth item in
the sequence. In some cases, the ordering rela-
tion of the sequence name was ambiguous (e.g.,
2We queried for both the numeric form of the ordinal and
the number spelled out (e.g ?the 2nd ...? and ?the second ...?).
We took up to 100 results per query.
288
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
SEQ
REDUND
LOCAL
Figure 2: SEQ outperforms the baseline systems,
increasing the area under the curve by 247% rela-
tive to LOCAL and by 90% relative to REDUND.
?largest state in the US? could refer to land area or
population), which could lead to merging two dis-
tinct sequences. In practice, we found that most
ordering relations were used in a consistent way
(e.g., ?largest city in? always means largest by
population) and only about 5% of the sequence
names in our sample have an ambiguous ordering
relation.
We compute precision-recall curves relative to
this random sample by changing a confidence
threshold. Precision is the percentage of correct
extractions above a threshold, while recall is the
percentage correct above a threshold divided by
the total number of correct extractions. Because
SEQ requires training data, we used 15-fold cross
validation on the labeled sample.
The functionality and density features boost
SEQ?s ability to correctly identify sequence
names. Figure 1 shows how well SEQ can iden-
tify correct sequence names using only functional-
ity, only density, and using functionality and den-
sity in concert. The baseline used is the maximum
value of localConf(x, k, s) over all (x, k). Both
the density features and the functionality features
are effective at this task, but using both types of
features resulted in a statistically significant im-
provement over using either type of feature in-
dividually (paired t-test of area under the curve,
p < 0.05).
We measure SEQ?s efficacy on the complete
sequence-extraction task by contrasting it with two
baseline systems. The first is LOCAL, which
ranks extractions by localConf .3 The second is
3If an extraction arises from multiple sentences, we use
REDUND, which ranks extractions by totalConf .
Figure 2 shows the precision-recall curves for each
system on the test data. The area under the curves
for SEQ, REDUND, and LOCAL are 0.59, 0.31,
and 0.17, respectively. The low precision and flat
curve for LOCAL suggests that localConf is not
informative for classifying extractions on its own.
REDUND outperformed LOCAL, especially at
the high-precision part of the curve. On the subset
of extractions with correct s, REDUND can iden-
tify x as the kth item with precision of 0.85 at re-
call 0.80. This is consistent with previous work on
redundancy-based extractors on the Web. How-
ever, REDUND still suffered from the problems
of over-specification and over-generalization de-
scribed in Section 2. SEQ reduces the negative ef-
fects of these problems by decreasing the scores
of sequence names that appear too general or too
specific.
4 Related Work
There has been extensive work in extracting lists
or sets of entities from the Web. These extrac-
tors rely on either (1) HTML features (Cohen
et al, 2002; Wang and Cohen, 2007) to extract
from structured text or (2) lexico-syntactic pat-
terns (Hearst, 1992; Etzioni et al, 2005) to ex-
tract from unstructured text. SEQ is most similar
to this second type of extractor, but additionally
leverages the sequence regularities of functionality
and density. These regularities allow the system to
overcome the poor performance of the purely syn-
tactic extractor LOCAL and the redundancy-based
extractor REDUND.
5 Conclusions
We have demonstrated that an extractor leveraging
sequence regularities can greatly outperform ex-
tractors without this knowledge. Identifying likely
sequence names and then filling in sequence items
proved to be an effective approach to sequence ex-
traction.
One line of future research is to investigate
other types of domain-independent frames that ex-
hibit useful regularities. Other examples include
events (with regularities about actor, location, and
time) and a generic organization-role frame (with
regularities about person, organization, and role
played).
the maximal localConf .
289
6 Acknowledgements
This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-08-1-
0431, DARPA contract FA8750-09-C-0179, and
an NSF Graduate Research Fellowship, and was
carried out at the University of Washington?s Tur-
ing Center.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, pages 2670?2676.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
gap: Learning-based information extraction rival-
ing knowledge-engineering methods. In ACL, pages
216?223.
William W. Cohen, Matthew Hurst, and Lee S. Jensen.
2002. A flexible learning system for wrapping ta-
bles and lists in html documents. In In International
World Wide Web Conference, pages 232?241.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034?1041.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2004. Methods for domain-independent informa-
tion extraction from the Web: An experimental com-
parison. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence (AAAI-2004),
pages 391?398.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Er Yates. 2005. Unsupervised
named-entity extraction from the web: An experi-
mental study. Artificial Intelligence, 165:91?134.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2-3):169?202.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, pages
539?545.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731?738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
290
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 902?912,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hierarchical Summarization:
Scaling Up Multi-Document Summarization
Janara Christensen Stephen Soderland
Computer Science & Engineering
University of Washington
Seattle, USA
janara@cs.washington.edu
soderlan@cs.washington.edu
Gagan Bansal Mausam
Computer Science & Engineering
Indian Institute of Technology
Delhi, India
gaganbansal1993@gmail.com
mausam@cse.iitd.ac.in
Abstract
Multi-document summarization (MDS)
systems have been designed for short, un-
structured summaries of 10-15 documents,
and are inadequate for larger document
collections. We propose a new approach
to scaling up summarization called hierar-
chical summarization, and present the first
implemented system, SUMMA.
SUMMA produces a hierarchy of relatively
short summaries, in which the top level
provides a general overview and users can
navigate the hierarchy to drill down for
more details on topics of interest. SUMMA
optimizes for coherence as well as cover-
age of salient information. In an Amazon
Mechanical Turk evaluation, users pref-
ered SUMMA ten times as often as flat
MDS and three times as often as timelines.
1 Introduction
The explosion in the number of documents
on the Web necessitates automated approaches
that organize and summarize large document col-
lections on a complex topic. Existing methods
for multi-document summarization (MDS) are de-
signed to produce short summaries of 10-15 doc-
uments.
1
MDS systems do not scale to data sets
ten times larger and proportionately longer sum-
maries: they either cannot run on large input or
produce a disorganized summary that is difficult
to understand.
We present a novel MDS paradigm, hierarchi-
cal summarization, which operates on large doc-
ument collections, creating summaries that orga-
nize the information coherently. It mimics how
someone with a general interest in a complex topic
would learn about it from an expert ? first, the ex-
pert would provide an overview, and then more
1
In the DUC evaluations, summaries have a budget of 665
bytes and cover 10 documents.
Hierarchical Summarization: Scaling Up Multi-Document
Summarization
bstract
For topics that cover large amounts of in-
formation, simple, short summaries are in-
sufficient ? complex topics require more
information and more structure to under-
stand. We propose a new approach to scal-
ing up summarization called hierarchical
summarization, and present the first imple-
mented system, SUMMA.
SUMMA produces a hierarchy of relatively
short summaries, where the top level pro-
vides a general overview and users can
navigate the hierarchy to drill down for
more details on topics of interest. Com-
pared to flat multi-document summaries,
users prefer SUMMA ten times as often
and learn just as much, and compared to
timelines, users prefer SUMMA three t mes
as often and learn more in twice as many
cases.
1 Introduction
The explosion in the number of documents over
the Web necessitates automated approaches that
organize and summarize large document collec-
tions on a complex topic. Existing methods for
multi-document summarization (MDS) can handle
10-15 documents and create a short flat summary,
but are insufficient for large-scale summarization.
For large-scale summarization, we need summa-
rizers that organize the information coherently and
enable personalized interaction with the summary
so that users can explore the various aspects of in-
formation in different levels of detail based on in-
dividual interest.
To this end, we present a novel MDS paradigm,
hierarchical summarization. Hierarchical summa-
rization is designed to operate on large document
collections. It mimics how someone with a gen-
eral interest in a complex topic would learn about
x1,1
x1,2
x1,3
x4,1
x4,2
x4,3
x9,1
x9,2
x8,1
x8,2
x8,3
x7,1
x7,2
x3,1
x3,2
x3,3
x2,1
x2,2 x6,1
x6,2
x5,1
x5,2
x5,3
On Aug 7 1998, car bombs
exploded outside US em-
bassies in Kenya and Tanza-
nia. Several days later, the
US began investigations into
bombings. The US retali-
ated with missile strikes on
suspected terrorist camps
in Afghanistan and Sudan
on Aug 20.
Some questioned the timing
of Clinton?s decision to launch
strikes. On Aug 22, with bin
Laden having survived the
strikes, the US outlined other
efforts to damage his net-
work. Russia, Sudan, Pakistan,
and Afghanistan condemned the
strikes.
Clinton proposed meth-
ods to inflict financial
damage on bin Laden.
Another possibility is
for the United States to
negotiate with the Tal-
iban to surrender bin
Laden. But diplomats
who have dealt with
the Taliban doubt that
anything could come of
such negotiations.
Figure 1: An example of a hierarchical summary for the 1998
embassy bombings, with one branch of the hierarchy high-
lighted. Each rectangle represents a summary and each x
i,j
represents a sentence within a summary. The root summary
provides an overview of the events of August 1998. When the
last sentence is selected, a more detailed summary of the mis-
sile strikes is produced, and when the middle sentence of that
summary is selected, a more detailed summary bin Laden?s
escape is produced.
it from an expert ? first, the expert would give
an overview, and then more specific information
about various aspects. It has the following novel
characteristics:
i r A hierarchical summary of the 1998 embassy
bombings. Each rectangle represents a summary and each
x
i,j
is a sentenc within a summary. The root summary pro-
vides an overview of the events of August 1998. When the
third sentence is selected, a more detailed summary of the
missile strikes is displayed. Selecting the second sentence of
that summary produces a more detailed summary of the US?
options.
specific information about various aspects. Hi-
erarchical summarization has the following novel
characteristics:
? The summary is hierarchically organized
along one or more organizational principles
such as time, location, entities, or events.
? Each non-leaf summary is associated with a
set of child summaries where each gives de-
tails of an element (e.g. sentence) in the par-
ent summary.
? A user can navigate within the hierarchical
summary by clicking on an element of a par-
ent summary to view the associated child
summary.
For example, given the topic, ?1998 embassy
bombings,? the first summary (Figure 1) might
902
mention that the US retaliated by striking
Afghanistan and Sudan. The user can click on this
information to learn more about these attacks. In
this way, the system can present large amounts of
information without overwhelming the user, and
the user can tailor the output to their interests.
In this paper, we describe SUMMA, the first
hierarchical summarization system for multi-
document summarization.
2
It operates on a corpus
of related news articles. SUMMA hierarchically
clusters the sentences by time, and then summa-
rizes the clusters using an objective function that
optimizes salience and coherence.
We conducted an Amazon Mechanical Turk
(AMT) evaluation where AMT workers compared
the output of SUMMA to that of timelines and flat
summaries. SUMMA output was judged superior
more than three times as often as timelines, and
users learned more in twice as many cases. Users
overwhelmingly preferred hierarchical summaries
to flat summaries (92%) and learned just as much.
Our main contributions are as follows:
? We introduce and formalize the novel task of
hierarchical summarization.
? We present SUMMA, the first hierarchical
summarization system, which operates on
news corpora and summarizes over an or-
der of magnitude more documents than tra-
ditional MDS systems, producing summaries
an order of magnitude larger.
? We present a user study which demonstrates
the value of hierarchical summarization over
timelines and flat multi-document summaries
in learning about a complex topic.
In the next section, we formalize hierarchical
summarization. We then describe our methodol-
ogy to implement the SUMMA hierarchical sum-
marization system: hierarchical clustering in Sec-
tion 3 and creating summaries based on that clus-
tering in Section 4. We discuss our experiments in
Section 5, related work in Section 6, and conclu-
sions in Section 7.
2 Hierarchical Summarization
We propose a new task for large-scale summariza-
tion called hierarchical summarization. Input to a
hierarchical summarization system is a set of re-
lated documents D and a budget b for each sum-
mary within the hierarchy (in bytes, words, or sen-
tences). The output is the hierarchical summary
H , which we define formally as follows.
2
http://knowitall.cs.washington.edu/summa/
Definition A hierarchical summary H of a docu-
ment collection D is a set of summaries X orga-
nized into a hierarchy. The top of the hierarchy
is a summary X
1
representing all of D, and each
summary X
i
consists of summary units x
i,j
(e.g.
the jth sentence of summary i) that point to a child
summary, except at the leaf nodes of the hierarchy.
A child summary adds more detail to the infor-
mation in its parent summary unit. The child sum-
mary may include sub-events or background and
reactions to the event or topic in the parent.
We define several metrics in Section 4 for
a well-constructed hierarchical summary. Each
summary should maximize coverage of salient in-
formation; it should minimize redundancy; and
it should have intra-cluster coherence as well as
parent-to-child coherence.
Hierarchical summarization has two important
strengths in the context of large-scale summariza-
tion. First, the information presented at the start
is small and grows only as the user directs it, so
as not to overwhelm the user. Second, each user
directs his or her own experience, so a user inter-
ested in one aspect need only explore that section
of the data without having to view or understand
the entire summary. The parent-to-child links pro-
vide a means for a user to navigate, drilling down
for more details on topics of interest.
There are several possible organizing principles
for the hierarchy ? by date, by entities, by loca-
tions, or by events. Some organizing principles
will fit the data in a document collection better
than others. A system may select different orga-
nization for different portions of the hierarchy, for
example, organizing first by location or prominent
entity and then by date for the next level.
3 Hierarchical Clustering
Having defined the task, we now describe
the methodology behind our implementation,
SUMMA. In future work we intend to design a
system that dynamically selects the best organiz-
ing principle for each level of the hierarchy. In
this first implementation, we have opted for tem-
poral organization, since this is generally the most
appropriate for news events.
The problem of hierarchical summarization as
described in Section 2 has all of the requirements
of MDS, and additional complexities of inducing a
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
903
Hierarchical Clustering H
s1
. . .
sN
sj+1
. . .
sN
sl+1
. . .
sN
sj+1
. . .
sl
si+1
. . .
sj
s1
. . .
si
sk+1
. . .
si
s1
. . .
sk
Hierarchical Summary X
x1,1
x1,2
x1,3
x4,1
x4,2
x8,1
x8,2
x7,1
x7,2
x7,3
x3,1
x3,2
x3,3
x2,1
x2,2
x6,1
x6,2
x5,1
x5,2
x5,3
Figure 2: Examples of a hierarchical clustering and a hier-
archical summary, where the input sentences are s 2 S, the
number of input sentences is N , and the summary sentences
are x 2 X . The hierarchical clustering determines the struc-
ture of the hierarchical summary.
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
child summaries.
We simplify the problem by decomposing it into
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
ple). A hierarchical clustering is a tree in which if
a cluster g
p
is the parent of cluster g
c
, then each
sentence in g
c
is also in g
p
. This organizes the
information into manageable, semantically-related
sections and induces a hierarchical structure over
the input.
The hierarchical clustering serves as input to the
second step ? summarizing given the hierarchy.
The hierarchical summary follows the hierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizes the sentences in that cluster. More-
over, the number of sentences in a flat summary is
exactly equal to the number of child clusters of the
node, since the user will click a sentence to get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Because we are interested in temporal hierar-
chical summarization, we hierarchically cluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assume a binary
split at each node (Berkhin, 2006). The number of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
to determine if the timestamps in the sentence re-
fer to the root verb. If no timestamp is given, we
use the article date.
3.1 Temporal Clustering
After acquiring the timestamps, we must hierar-
chically cluster the sentences into sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lem reduces to identifying the best dates at which
to split a cluster into subclusters. We identify these
dates by looking for bursts of activity.
News tends to be bursty ? many articles on a
topic appear at once and then taper out (Kleinberg,
2002). For example, Figure 3 shows the number of
articles per day related to 1998 embassy bombings
published in the New York Times (identified using
a key word search). There were two main events
? on the 7th, the embassies were bombed and
on the 20th, US retaliated through missile strikes.
The figure shows a correspondence between these
events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters evenly spaced across time. We thus
choose clusters C = {c
1
, . . . , c
k
} as follows:
maximize
C
B(C) + ?E(C)
(1)
where C is a clustering, B(C) is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and ? is the tradeoff parameter.
B(C) =
X
c2C
burst(c) (2)
burst(c) is the difference in the number of sen-
tences published the day before the first date in c
and the average number of sentences published on
the first and second date of c:
burst(c) =
pub(d
i
) + pub(d
i+1
)
2
  pub(d
i 1
) (3)
where d is a date indexed over time, such that d
j
is a day before d
j+1
, and d
i
is the first date in c.
Figure 2: Examples of input and output o hierarchical sum-
marization. The input sentences ar s ? S, the number of
input sentences is N , and the summary sentences are x ? X .
child summaries.
We simplify the problem by decomposing it into
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
ple). A hierarchical clustering is a tree in which if
a cluster g
p
is the parent of cluster g
c
, then each
sentence in g
c
is also in g
p
. This organizes the
information into manageable, semantically-related
sections and induces a hierarchical structure over
the input.
The hierarchical clustering serves as input to the
second step ? summarizing given the hierarchy.
The hierarchical summary follows the hierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizes the senten in that cluster. More-
over, the number of sent es in a flat summary is
exactly equal to the number of child clusters of the
node, since the user will click a sentence to get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Because we are inter st d in temporal hierar-
chical summarization, we hierarchically cluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assume a binary
split at each no e (Berkhin, 2006). The number of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
to determine if the timestamps in the sentence re-
fer to the root verb. If no timestamp is given, we
use the article date.
3.1 Temporal Clustering
After acquiring the timestamps, we must hierar-
chically cluster the sentences into sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lem reduces to identifying the best dates at which
to split a cluster into subclusters. We identify these
dates by looking for bursts of activity.
News tends to be bursty ? many articles on a
topic appear at once and then taper out (Kleinberg,
2002). For exampl , Figure 3 show the number of
articles per day related to the 1998 embassy bomb-
ings published in the New York Times (identified
using a key word search). There were two main
events ? on the 7th, the embassies were bombed
an on the 20th, the US retaliated through mis-
sile strikes Th figure shows a correspondence
between these events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters e enly spaced across time. We thus
choose clusters C = {c
1
, . . . , c
k
} as follows:
maximize
C
B(C) + ?E(C)
(1)
where is a clustering, B(C) is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and ? is the trad off parameter.
B(C) =
?
c?C
burst(c) (2)
burst(c) is the difference in the number of sen-
tences published the day before the first date in c
and the average number of sentences published on
the first a d sec nd date of c:
burst(c) =
b(d
i
) + pub(d
i+1
)
2
? pub(d
i?1
) (3)
where d is a date indexed over time, such that d
j
is a day before d
j+1
, and d
i
is the first date in c.
pub(d
i
) is the number of sentences published on
d
i
. The evenness of the split is measured by:
E(C) = min
c?C
size(c) (4)
where size(c) is the number of dates in cluster c.
We perform hierarchical clustering top-down, at
each point solving for Equation 1. ? was set using
a grid-search over a development set.
904
6 8 10 12 14 16 18 20 22 24
0
20
40
Day of Month
N
u
m
b
e
r
o
f
A
r
t
i
c
l
e
s
1
Figure 3: News coverage by date for the embassy bombings
in Tanzania and Kenya. There are spikes in the number of
articles published at the two major events.
3.2 Choosing the number of clusters
We cannot know a priori the number of clusters
for a given topic. However, when the number of
clusters is too large for the given summary budget,
the sentences will have to be too short, and when
the number of clusters is too small, we will not use
enough of the budget. We set the maximum num-
ber of clusters k
max
and minimum number of clus-
ters k
min
to be a function of the budget b and the
average sentence length in the cluster s
avg
, such
that k
max
? s
avg
? b and k
min
? s
avg
? b/2.
Given a maximum and minimum number of
clusters, we must determine the appropriate num-
ber of clusters. At each level, we cluster the sen-
tences by the method described above and choose
the number of clusters k according to the gap
statistic (Tibshirani et al, 2000). Specifically, for
each level, the algorithm will cluster repeatedly
with k varying from the minimum to the maxi-
mum. The algorithm will return the k that max-
imizes the gap statistic:
Gap
n
(k) = E
?
n
{log(W
k
)} ? log(W
k
) (5)
where W
k
is the score for the clusters computed
with Equation 1, and E
?
n
is the expectation under
a sample of size n from a reference distribution.
Ideally, the maximum depth of the clustering
would be a function of the number of sentences
in each cluster, but in our implementation, we set
the maximum depth to three, which works well for
the size of the datasets we use (300 articles).
4 Summarizing within the Hierarchy
After the sentences are clustered, we have a struc-
ture for the hierarchical summary that dictates the
number of summaries and the number of sentences
in each summary. We also have the set of sen-
tences from which each summary is drawn.
Intuitively, each cluster summary in the hierar-
chical summary should convey the most salient
information in that cluster. Furthermore, the hier-
archical summary should not include redundant
sentences. A hierarchical summary that is only
salient and nonredundant may still not be suitable
if the sentences within a cluster summary are dis-
connected or if the parent sentence for a summary
does not relate to the child summary. Thus, a hi-
erarchical summary must also have intra-cluster
coherence and parent-to-child coherence.
4.1 Salience
Salience is the value of each sentence to the topic
from which the documents are drawn. We measure
salience of a summary (Sal(X)) as the sum of the
saliences of individual sentences (
?
i
Sal(x
i
)).
Following previous research in MDS, we com-
puted individual saliences using a linear regres-
sion classifier trained on ROUGE scores over the
DUC?03 dataset (Lin, 2004; Christensen et al,
2013). This method finds those sentences more
salient that mention nouns or verbs that occur fre-
quently in the cluster.
In preliminary experiments, we noticed that
many sentences that were reaction sentences were
given a higher salience than action sentences. For
example, the reaction sentence, ?President Clinton
vowed to track down the perpetrators behind the
bombs that exploded outside the embassies in Tan-
zania and Kenya on Friday,? would have a higher
score than the action sentence, ?Bombs exploded
outside the embassies in Tanzania and Kenya on
Friday.? This problem occurs because the first sen-
tence has a higher ROUGE score (it covers more
important words than the second sentence). To ad-
just for this problem, we use only words identified
in the main clause (heuristically identified via the
parse tree) to compute our salience scores.
4.2 Redundancy
We identify redundant sentences using a linear
regression classifier trained on a manually la-
beled subset of the DUC?03 sentences. The fea-
tures include shared noun counts, sentence length,
TF*IDF cosine similarity, timestamp difference,
and features drawn from information extraction
such as number of shared tuples in Open IE
(Mausam et al, 2012).
905
4.3 Summary Coherence
We require two types of coherence: coherence be-
tween the parent and child summaries and coher-
ence within each summary X
i
.
We rely on the approximate discourse graph
(ADG) that was proposed in (Christensen et al,
2013) as the basis for measuring coherence. Each
node in the ADG is a sentence from the dataset.
An edge from sentence s
i
to s
j
with positive
weight indicates that s
j
may follow s
i
in a coher-
ent summary, e.g. continued mention of an event
or entity, or coreference link between s
i
and s
j
.
A negative edge indicates an unfulfilled discourse
cue or co-reference mention.
Parent-to-Child Coherence: Users navigate the
hierarchical summary from parent sentence to
child summary, so if the parent sentence bears no
relation to the child summary, the user will be un-
derstandably confused. The parent sentence must
have positive evidence of coherence with the sen-
tences in its child summary.
We estimate parent to child coherence as the co-
herence between a parent sentence and each sen-
tence in its child summary as:
PCoh(X) =
?
c?C
?
i=1..|X
c
|
w
G+
(x
p
c
, x
c,i
)) (6)
where x
p
c
is the parent sentence for cluster c and
w
G+
(x
p
c
, x
c,i
) is the sum of the positive edge
weights from x
p
c
to x
c,i
in the ADG G.
Intra-cluster Coherence: In traditional MDS, the
documents are usually quite focused, allowing for
highly focused summaries. In hierarchical sum-
marization, however, a cluster summary may span
hundreds of documents and a wide range of infor-
mation. For this reason, we may consider a sum-
mary acceptable even if it has limited positive evi-
dence of coherence in the ADG, as long as there
is no negative evidence in the form of negative
edges. For example, the following is a reasonable
summary for events spanning two weeks:
s
1
Bombs exploded at two US embassies.
s
2
US missiles struck in Afghanistan and Sudan.
Our measure of intra-cluster coherence mini-
mizes the number of missing references. These
are coreference mentions or discourse cues where
none of the sentences read before (either in an an-
cestor summary or in the current summary) con-
tain an antecedent:
CCoh(X) = ?
?
c?C
?
i=1..|X
c
|
#missingRef(x
c,i
) (7)
4.4 Objective Function
Having estimated salience, redundancy, and two
forms of coherence, we can now put this informa-
tion together into a single objective function that
measures the quality of a candidate hierarchical
summary.
Intuitively, the objective function should bal-
ance salience and coherence. Furthermore, the
summary should not contain redundant informa-
tion and each cluster summary should honor the
given budget, i.e., maximum summary length b.
We treat redundancy and budget as hard con-
straints and coherence and salience as soft con-
straints. Lastly, we require that sentences are
drawn from the cluster that they represent and that
the number of sentences in the summary corre-
sponding to each non-leaf cluster c is equivalent
to the number of child clusters of c. We optimize:
maximize: F (x) , Sal(X) + ?PCoh(X) + ?CCoh(X)
s.t. ?c ? C :
?
i=1..|X
c
|
len(x
c,i
) < b
?x
i
, x
j
? X : redundant(x
i
, x
j
) = 0
?c ? C, ?x
c
? X
c
: x
c
? c
?c ? C : |X
c
| = #children(c)
The tradeoff parameters ? and ? were set based
on a development set.
4.5 Algorithm
Optimizing this objective function is NP-hard, so
we approximate a solution by using beam search
over the space of partial hierarchical summaries.
Notice the contribution from a sentence depends
on individual salience, coherence (CCoh) based
on sentences visible on the user path down the hi-
erarchy to this sentence, and coherence (PCoh)
based on its parent sentence and its child sum-
mary. Since most of the sentence contributions de-
pend on the path from the root to the sentence, we
build our partial summary by incrementally adding
a sentence top-down in the hierarchy and from first
sentence to last within a cluster summary.
To account for PCoh, we estimate the contribu-
tion of the sentence by jointly identifying its best
child summary. However, we do not fix the child
summary at this time ? we simply use it to estimate
PCohwhen using that sentence. Since computing
the best child summary is also intractable we ap-
proximate a solution by a local search algorithm
over the child cluster.
Overall, our algorithm is a two level nested
search algorithm ? beam search in the outer loop to
906
search through the space of partial summaries and
local search (hill climbing with random restarts) in
the inner loop to pick the best sentence to add to
the existing partial summary. We use a beam of
size ten in our implementation.
5 Experiments
Our experiments are designed to evaluate how ef-
fective hierarchical summarization is in summa-
rizing a large, complex topic and how well this
helps users learn about the topic. Our evaluation
addresses the following questions:
? Do users prefer hierarchical summaries for
topic exploration? (Section 5.1)
? Are hierarchical summaries more effective
than other methods for learning about com-
plex events? (Section 5.2)
? How informative are the hierarchical sum-
maries compared to the other methods? (Sec-
tion 5.3)
? How coherent is the hierarchical structure in
the summaries? (Section 5.4)
We compared SUMMA against two baseline sys-
tems which represent the main NLP methods for
large-scale summarization: an algorithm for cre-
ating timelines over sentences (Chieu and Lee,
2004),
3
and a state-of-the-art flat MDS system
(Lin and Bilmes, 2011).
4
Each system was given
the same budget (over 10 times the traditional
MDS budget, which is 665 bytes).
We evaluated the questions on ten news topics,
representing a range of tasks: (1) Pope John Paul
II?s death and the 2005 Papal Conclave, (2) Bush v.
Gore, (3) the Tulip Revolution, (4) Daniel Pearl?s
kidnapping, (5) the Lockerbie bombing handover
of suspects, (6) the Kargil War, (7) NATO?s bomb-
ing of Yugoslavia in 1999, (8) Pinochet?s arrest in
London, (9) the 2005 London bombings, and (10)
the crash and investigation of SwissAir Flight 111.
We chose topics containing a set of related events
that unfolded over several months and were promi-
nent enough to be reported in at least 300 articles.
We drew our articles from the Gigaword corpus,
which contains articles from the New York Times
and other major newspapers. For each topic, we
used the 300 documents that best matched a key
3
Unfortunately, we were unable to obtain more recent
timeline systems from authors of the systems.
4
(Christensen et al, 2013) is a state-of-the-art coherent
MDS system, but does not scale to 300 documents.
word search. We selected topics which were be-
tween five and fifteen years old so that evaluators
would have relatively less pre-existing knowledge
about the topic.
5.1 User Preference
In our first experiment, we simply wished to eval-
uate which system users most prefer. We hired
Amazon Mechanical Turk (AMT) workers and as-
signed two topics to each worker. We paired up
workers such that one worker would see output
from SUMMA for the first topic and a competing
system for the second and the other worker would
see the reverse. For quality control, we asked
workers to complete a qualification task first, in
which they were required to write a short summary
of a news article. We also manually removed spam
from our results. Previous work has used AMT
workers for summary evaluations and has shown
high correlations with expert ratings (Christensen
et al, 2013). Five workers were hired to view each
topic-system pair.
We asked the workers to choose which format
they preferred and to explain why. The results are
as follows:
SUMMA 76% TIMELINE 24%
SUMMA 92% FLAT-MDS 8%
Users preferred the hierarchical summaries
three times more often than timelines and over
ten times more often than flat summaries. When
we examined the reasons given by the users, we
found that the people who preferred the hierar-
chical summaries liked that they gave a big pic-
ture overview and were then allowed to drill down
deeper. Some also explained that it was eas-
ier to remember information when presented with
the overview first. Typical responses included,
?Could gather and absorb the information at my
own pace,? and, ?Easier to follow and understand.?
When users preferred the timelines, they usually
remarked that it was more familiar, i.e. ?I liked
the familiarity of the format. I am used to these
timelines and they feel comfortable.? Users com-
plained that the flat summaries were disjointed,
confusing, and very frustrating to read.
5.2 Knowledge Acquisition
Evaluating how much a user learned is inherently
difficult, more so when the goal is to allow the user
the freedom to explore information based on indi-
vidual interest. For this reason, instead of asking a
set of predefined questions, we assess the knowl-
907
edge gain by following the methodology of (Sha-
haf et al, 2012) ? asking users to write a paragraph
summarizing the information learned.
Using the same setup as in the previous exper-
iment, for each topic, five AMT workers spent
three minutes reading through a timeline or sum-
mary and were then asked to write a description
of what they had learned. Workers were not al-
lowed to see the timeline or summary while writ-
ing. We collected five descriptions for each topic-
system combination. We then asked other AMT
workers to read and compare the descriptions writ-
ten by the first set of workers. Each evaluator was
presented with a corresponding Wikipedia article
and descriptions from a pair of users (timeline vs.
SUMMA or flat MDS vs. SUMMA). The descrip-
tions were randomly ordered to remove bias. The
workers were asked which user appeared to have
learned more and why. For each pair of descrip-
tions, four workers evaluated the pair. Standard
checks such as approval rating, location filtering,
etc. were used for removing spam. The results of
this experiment are as follows:
Prefer Indiff. Prefer
SUMMA 58% 17% TIMELINE 25%
SUMMA 40% 22% FLAT-MDS 38%
Descriptions written by workers using SUMMA
were preferred over twice as often as those from
timelines. We looked more closely at those cases
where the participants either preferred the time-
lines or were indifferent and found that this pref-
erence was most common when the topic was not
dominated by a few major events, but was instead
a series of similarly important events. For exam-
ple, in the kidnapping and beheading of Daniel
Pearl there were two or three obviously major
events, whereas in the Kargil War there were many
smaller important events. In latter cases, the hier-
archical summaries provided little advantage over
the timelines because it was more difficult to ar-
range the sentences hierarchically.
Since SUMMA was judged to be so much supe-
rior to flat MDS systems in Section 5.1, it is sur-
prising that users descriptions from flat MDS were
preferred nearly as often as those from SUMMA.
While the flat summaries were disjointed, they
were good at including salient information, with
the most salient tending to be near the start of the
summary. Thus, descriptions from both SUMMA
and flat MDS generally covered the most salient
information.
5.3 Informativeness
In this experiment, we assess the salience of the
information captured by the different systems, and
the ability of SUMMA to organize the information
so that more important information is placed at
higher levels.
ROUGE Evaluation: We first automatically
assessed informativeness by calculating the
ROUGE-1 scores of the output of each of the sys-
tems. For the gold standard comparison summary,
we use the Wikipedia articles for the topics.
5
Note that there is no good translation of ROUGE
for hierarchical summarization. Thus, we simply
use the traditional ROUGE metric, which will
not capture any of the hierarchical format. This
score will essentially serve as a rough measure of
coverage of the entire summary to the Wikipedia
article. The scores for each of the systems are as
follows:
P R F1
SUMMA 0.25 0.67 0.31
TIMELINE 0.28 0.65 0.33
FLAT-MDS 0.30 0.64 0.34
None of the differences are significant. From
this evaluation, one can gather that the systems
have similar coverage of the Wikipedia articles.
Manual Evaluation: While ROUGE serves as a
rough measure of coverage, we were interested in
gathering more fine-grained information on the in-
formativeness of each system. We performed an
additional manual evaluation that assesses the re-
call of important events for each system.
We first identified which events were most im-
portant in a news story. Because reading 300 arti-
cles per topic is impractical, we asked AMT work-
ers to read a Wikipedia article on the same topic
and then identify the three most important events
and the five most important secondary events. We
aggregated responses from ten workers per topic
and chose the three most common primary and five
most common secondary events.
One of the authors then manually identified the
presence of these events in the hierarchical sum-
maries, the timelines and the flat MDS summaries.
Below we show event recall (the percentage of the
events that were mentioned).
5
We excluded one topic (the handover of the Lockerbie
bombing suspects) because the corresponding Wikipedia ar-
ticle had insufficient information.
908
Events SUMMA TIMELINE FLAT-MDS
Prim. 96% 74% 93%
Sec. 76% 53% 64%
The difference in recall between SUMMA and
TIMELINE was significant in both cases, and the
difference between SUMMA and FLAT-MDS was
not. In general, the flat summaries were quite re-
dundant, which contributed to the slightly lower
event recall. The timelines, on the other hand,
were both incoherent and at the same time re-
ported less important facts.
We also evaluated at what level in the hierar-
chy the events were identified for the hierarchical
summaries. The event recall shows the percentage
of events mentioned at that level or above in the
hierarchical summary:
Events Level 1 Level 2 Level 3
Prim. 63% 81% 96%
Sec. 27% 51% 76%
81% of the primary events are present in the first
or second level, and 76% of the secondary events
are mentioned by the third level. While recog-
nizing primary events is relatively simple because
they are repeated frequently, identification of im-
portant secondary events often requires external
knowledge.
5.4 Parent-to-Child Coherence
We next tested the hierarchical coherence. One of
the authors graded how much each non-leaf sen-
tence in a summary was coherent with its child
summary on a scale of one to five, with one be-
ing incoherent and five being perfectly coherent.
We used the coherence scale from DUC?04.
6
Level 1 Level 2
Coherence 3.8 3.4
We found that for the top level of the summary,
the parent sentence generally represented the most
important event in the cluster and the child sum-
mary usually expressed details or reactions of the
event. The lower coherence scores were often the
result of too few lexical connections or lack of a
theme or story. While the facts of the sentences
made sense together, the summaries sometimes
did not read as if they were written by a human,
but as a series of disparate sentences.
For the second level, the problems were more
basic. The parent sentence occasionally expressed
a less important fact that the child summary did
6
http://duc.nist.gov/duc2004/quality.questions.txt
not then expand on or, more commonly, the child
summary was not focused enough. This result
stems from two problems in our algorithm. First,
summarizing sentences are rare, making good
choices for parent sentences difficult to find. The
second problem relates to the difficulty in identify-
ing whether two sentences are on the same topic.
For example, suppose the parent sentence is, ?A
Swissair plane Wednesday night crashed off Nova
Scotia, Canada.? A very good child sentence is,
?The airline confirmed that all passengers died.?
However, based on their surface features, the sen-
tence, ?A plane made an unscheduled landing after
a Swissair plane crashed off the coast of Canada,?
appears to be a better choice.
Even though there is scope for improvement, we
find these coherence scores encouraging for a first
algorithm for the task.
6 Related Work
Traditional approaches to large-scale summariza-
tion have included flat summaries and timelines.
There are two primary shortcomings to these ap-
proaches: first, they require the user to sort
through large amounts of potentially overwhelm-
ing information, and second, the output is static
? users with different interests will see the same
information. Below we describe related work on
traditional MDS, structured summaries, timelines,
discovering threads of documents and the uses of
hierarchies in generating summaries.
6.1 Traditional MDS
Traditionally, MDS systems have focused on three
to six sentence summaries covering 10-15 docu-
ments. Most extractive summarization research
aims to maximize coverage while reducing redun-
dancy (e.g. (Carbonell and Goldstein, 1998; Sag-
gion and Gaizauskas, 2004; Radev et al, 2004)).
Lin and Bilmes (2011) proposed a state-of-the-art
system that uses submodularity in sentence selec-
tion to accomplish these goals. Christensen et al
(2013) presented an algorithm for coherent MDS,
but it does not scale to larger output.
Structured Summaries: Some research has ex-
plored generating structured summaries. These
approaches attempt to identify major aspects of
a topic, but do not compile content to describe
those aspects. Rather, they rely on pre-existing, la-
beled paragraphs (for example, a paragraph titled,
?Symptoms of Meningitis?). Aspects are identi-
fied either by a training corpus of articles in the
909
same domain (Sauper and Barzilay, 2009), by an
entity-aspect LDA model (Li et al, 2010), or by
Wikipedia templates of related topics (Yao et al,
2011). These methods assume a common struc-
ture for all topics in a category, and do not allow
for more than two levels in the structure.
Timeline Generation: Recent papers in timeline
generation have emphasized the relationship with
summarization. Yan et al (2011b) balanced co-
herence and diversity to create timelines, Yan et
al. (2011a) used inter-date and intra-date sentence
dependencies, and Chieu and Lee (2004) used sen-
tence similarity. Others have emphasized identify-
ing important dates, primarily by bursts of news
(Swan and Allen, 2000; Akcora et al, 2010; Hu
et al, 2011; Kessler et al, 2012). While time-
lines can be useful for understanding events, they
do not generalize to other domains. Additionally,
long timelines can be overwhelming, short time-
lines have low information content, and there is
no method for personalized exploration.
Document Threads: A related track of research
investigates discovering threads of documents.
While we aim to summarize collections of infor-
mation, this track seeks to identify relationships
between documents. This research operates on the
document level, while ours operates on the sen-
tence level. Shahaf and Guestrin (2010) formal-
ized the characteristics of a good chain of articles
and proposed an algorithm to connect two speci-
fied articles. Gillenwater et al (2012) proposed
a probabilistic technique for extracting a diverse
set of threads from a given collection. Shahaf et
al. (2012) extended work on coherent threads to
finding coherent maps of documents, where a map
is set of intersecting threads representing how the
threads interact and relate.
Summarization and Hierarchies: A few papers
have examined the relationship between summa-
rization and hierarchies. Some focused on cre-
ating a hierarchical summary of a single docu-
ment (Buyukkokten et al, 2001; Otterbacher et
al., 2006), relying on the structure inherent in sin-
gle documents. Others investigated creating hier-
archies of words or phrases to organize documents
(Lawrie et al, 2001; Lawrie, 2003; Takahashi et
al., 2007; Haghighi and Vanderwende, 2009).
Other research identifies the hierarchical struc-
ture of the documents and generates a summary
that prioritizes more general information accord-
ing to the structure (Ouyang et al, 2009; Celikyil-
maz and Hakkani-Tur, 2010), or gains coverage by
drawing sentences from different parts of the hier-
archy (Yang and Wang, 2003; Wang et al, 2006).
7 Conclusions
We have introduced a new paradigm for large-
scale summarization called hierarchical summa-
rization, which allows a user to navigate a hier-
archy of relatively short summaries. We present
SUMMA, an implemented hierarchical news sum-
marization system,
7
and demonstrate its effective-
ness in a user study that compares SUMMA with
a timeline system and a flat MDS system. When
compared to timelines, users learned more with
SUMMA in twice as many cases, and SUMMA was
preferred more than three times as often. When
compared to flat summaries, users overwhelming
preferred SUMMA and learned just as much.
This first implementation performs temporal
clustering ? in future work, we will investigate dy-
namically selecting an organizing principle that is
best suited to the data at each level of the hierar-
chy: by entity, by location, by event, or by date.
We also intend to scale the system to even larger
document collections, and explore joint clustering
and summarization. Lastly, we plan to research
hierarchical summarization in other domains.
Acknowledgments
We thank Amitabha Bagchi, Niranjan Balasubra-
manian, Danish Contractor, Oren Etzioni, Tony
Fader, Carlos Guestrin, Prachi Jain, Lucy Van-
derwende, Luke Zettlemoyer, and the anonymous
reviewers for their helpful suggestions and feed-
back. We thank Hui Lin and Jeff Bilmes for
providing us with their code. This research was
supported in part by ARO contract W911NF-
13-1-0246, DARPA Air Force Research Labora-
tory (AFRL) contract FA8750-13-2-0019, UW-
IITD subcontract RP02815, and the Yahoo! Fac-
ulty Research and Engagement Award. This pa-
per is also supported in part by the Intelligence
Advanced Research Projects Activity (IARPA)
via AFRL contract number FA8650-10-C-7058.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, AFRL, or the U.S. Government.
7
http://knowitall.cs.washington.edu/summa/
910
References
C. G. Akcora, M. A. Bayir, M. Demirbas, and H. Fer-
hatosmanoglu. 2010. Identifying breakpoints in
public opinion. In 1st KDD Workshop on Social Me-
dia Analytics.
Berkhin Berkhin. 2006. A survey of clustering data
mining techniques. Grouping Multidimensional
Data, pages 25?71.
Orkut Buyukkokten, Hector Garcia-Molina, and An-
dreas Paepcke. 2001. Seeing the whole in parts:
Text summarization for web browsing on handheld
devices. In Proceedings of WWW 2001, pages 652?
662.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?
824.
Angel Chang and Christopher Manning. 2012. SU-
Time: A library for recognizing and normalizing
time expressions. In Proceedings of LREC 2012.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of SIGIR 2004, pages 425?432.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of
NAACL 2013.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.
2012. Discovering diverse and salient threads in
document collections. In Proceedings of EMNLP-
CoNLL 2012, pages 710?720.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K.
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic
retrospection. In Proceedings of ICDM 2011.
Remy Kessler, Xavier Tannier, Caroline Hag`ege,
V?eronique Moriceau, and Andr?e Bittar. 2012. Find-
ing salient dates for building thematic timelines. In
Proceedings of ACL 2012, pages 730?739.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423?430.
Jon Kleinberg. 2002. Bursty and hierarchical struc-
ture in streams. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?02, pages 91?
101.
Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.
2001. Finding topic words for hierarchical summa-
rization. In Proceedings of SIGIR ?01, pages 349?
357.
Dawn J. Lawrie. 2003. Language models for hierar-
chical summarization. Ph.D. thesis, University of
Massachusetts Amherst.
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
ACL 2010, pages 640?649.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74?81.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of EMNLP 2012, pages 523?534.
Jahna Otterbacher, Dragomir Radev, and Omer Ka-
reem. 2006. News to go: Hierarchical text sum-
marization for mobile devices. In Proceedings of
SIGIR 2006, pages 589?596.
You Ouyang, Wenji Li, and Qin Lu. 2009. An
integrated multi-document summarization approach
based on word hierarchical representation. In Pro-
ceedings of the ACLShort 2009, pages 113?116.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
and Management, 40(6):919?938.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile rele-
vance and redundancy removal. In Proceedings of
DUC 2004.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings of ACL 2009, pages
208?216.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting
the dots between news articles. In Proceedings of
KDD 2010, pages 623?632.
Dafna Shahaf, Carlos Guestrin, and Eric Horvitz.
2012. Trains of thought: Generating information
maps. In Proceedings of WWW 2012.
911
Russell Swan and James Allen. 2000. Automatic gen-
eration of overview timelines. In Proceedings of SI-
GIR 2000, pages 49?56.
Kou Takahashi, Takao Miura, and Isamu Shioya. 2007.
Hierarchical summarizing and evaluating for web
pages. In Proceedings of the 1st workshop on
emerging research opportunities for Web Data Man-
agement (EROW 2007).
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in
a dataset via the gap statistic. Journal of the Royal
Statistical Society, Series B, 32(2):411?423.
Fu Lee Wang, Christopher C. Yang, and Xiaodong Shi.
2006. Multi-document summarization for terrorism
information extraction. In Proceedings of ISI?06.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of EMNLP 2011, pages
433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceeding
of SIGIR 2011, pages 745?754.
Christopher C. Yang and Fu Lee Wang. 2003. Fractal
summarization: summarization based on fractal the-
ory. In Proceedings of SIGIR 2003, pages 391?392.
Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng
Zhou, and Hongyan Liu. 2011. Autopedia: Auto-
matic domain-independent wikipedia article genera-
tion. In Proceedings of WWW 2011, pages 161?162.
912
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 52?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Semantic Role Labeling for Open Information Extraction
Janara Christensen, Mausam, Stephen Soderland and Oren Etzioni
University of Washington, Seattle
Abstract
Open Information Extraction is a recent
paradigm for machine reading from arbitrary
text. In contrast to existing techniques, which
have used only shallow syntactic features, we
investigate the use of semantic features (se-
mantic roles) for the task of Open IE. We com-
pare TEXTRUNNER (Banko et al, 2007), a
state of the art open extractor, with our novel
extractor SRL-IE, which is based on UIUC?s
SRL system (Punyakanok et al, 2008). We
find that SRL-IE is robust to noisy heteroge-
neous Web data and outperforms TEXTRUN-
NER on extraction quality. On the other
hand, TEXTRUNNER performs over 2 orders
of magnitude faster and achieves good pre-
cision in high locality and high redundancy
extractions. These observations enable the
construction of hybrid extractors that output
higher quality results than TEXTRUNNER and
similar quality as SRL-IE in much less time.
1 Introduction
The grand challenge of Machine Reading (Etzioni
et al, 2006) requires, as a key step, a scalable
system for extracting information from large, het-
erogeneous, unstructured text. The traditional ap-
proaches to information extraction (e.g., (Soderland,
1999; Agichtein and Gravano, 2000)) do not oper-
ate at these scales, since they focus attention on a
well-defined small set of relations and require large
amounts of training data for each relation. The re-
cent Open Information Extraction paradigm (Banko
et al, 2007) attempts to overcome the knowledge
acquisition bottleneck with its relation-independent
nature and no manually annotated training data.
We are interested in the best possible technique
for Open IE. The TEXTRUNNER Open IE system
(Banko and Etzioni, 2008) employs only shallow
syntactic features in the extraction process. Avoid-
ing the expensive processing of deep syntactic anal-
ysis allowed TEXTRUNNER to process at Web scale.
In this paper, we explore the benefits of semantic
features and in particular, evaluate the application of
semantic role labeling (SRL) to Open IE.
SRL is a popular NLP task that has seen sig-
nificant progress over the last few years. The ad-
vent of hand-constructed semantic resources such as
Propbank and Framenet (Martha and Palmer, 2002;
Baker et al, 1998) have resulted in semantic role la-
belers achieving high in-domain precisions.
Our first observation is that semantically labeled
arguments in a sentence almost always correspond
to the arguments in Open IE extractions. Similarly,
the verbs often match up with Open IE relations.
These observations lead us to construct a new Open
IE extractor based on SRL. We use UIUC?s publicly
available SRL system (Punyakanok et al, 2008) that
is known to be competitive with the state of the art
and construct a novel Open IE extractor based on it
called SRL-IE.
We first need to evaluate SRL-IE?s effectiveness
in the context of large scale and heterogeneous input
data as found on the Web: because SRL uses deeper
analysis we expect SRL-IE to be much slower. Sec-
ond, SRL is trained on news corpora using a re-
source like Propbank, and so may face recall loss
due to out of vocabulary verbs and precision loss due
to different writing styles found on the Web.
In this paper we address several empirical ques-
52
tions. Can SRL-IE, our SRL based extractor,
achieve adequate precision/recall on the heteroge-
neous Web text? What factors influence the relative
performance of SRL-IE vs. that of TEXTRUNNER
(e.g., n-ary vs. binary extractions, redundancy, local-
ity, sentence length, out of vocabulary verbs, etc.)?
In terms of performance, what are the relative trade-
offs between the two? Finally, is it possible to design
a hybrid between the two systems to get the best of
both the worlds? Our results show that:
1. SRL-IE is surprisingly robust to noisy hetero-
geneous data and achieves high precision and
recall on the Open IE task on Web text.
2. SRL-IE outperforms TEXTRUNNER along di-
mensions such as recall and precision on com-
plex extractions (e.g., n-ary relations).
3. TEXTRUNNER is over 2 orders of magnitude
faster, and achieves good precision for extrac-
tions with high system confidence or high lo-
cality or when the same fact is extracted from
multiple sentences.
4. Hybrid extractors that use a combination of
SRL-IE and TEXTRUNNER get the best of
both worlds. Our hybrid extractors make effec-
tive use of available time and achieve a supe-
rior balance of precision-recall, better precision
compared to TEXTRUNNER, and better recall
compared to both TEXTRUNNER and SRL-IE.
2 Background
Open Information Extraction: The recently pop-
ular Open IE (Banko et al, 2007) is an extraction
paradigm where the system makes a single data-
driven pass over its corpus and extracts a large
set of relational tuples without requiring any hu-
man input. These tuples attempt to capture the
salient relationships expressed in each sentence. For
instance, for the sentence, ?McCain fought hard
against Obama, but finally lost the election? an
Open IE system would extract two tuples <McCain,
fought (hard) against, Obama>, and <McCain, lost,
the election>. These tuples can be binary or n-ary,
where the relationship is expressed between more
than 2 entities such as <Gates Foundation, invested
(arg) in, 1 billion dollars, high schools>.
TEXTRUNNER is a state-of-the-art Open IE sys-
tem that performs extraction in three key steps. (1)
A self-supervised learner that outputs a CRF based
classifier (that uses unlexicalized features) for ex-
tracting relationships. The self-supervised nature al-
leviates the need for hand-labeled training data and
unlexicalized features help scale to the multitudes of
relations found on the Web. (2) A single pass extrac-
tor that uses shallow syntactic techniques like part of
speech tagging, noun phrase chunking and then ap-
plies the CRF extractor to extract relationships ex-
pressed in natural language sentences. The use of
shallow features makes TEXTRUNNER highly effi-
cient. (3) A redundancy based assessor that re-ranks
these extractions based on a probabilistic model of
redundancy in text (Downey et al, 2005). This ex-
ploits the redundancy of information in Web text and
assigns higher confidence to extractions occurring
multiple times. All these components enable TEX-
TRUNNER to be a high performance, general, and
high quality extractor for heterogeneous Web text.
Semantic Role Labeling: SRL is a common NLP
task that consists of detecting semantic arguments
associated with a verb in a sentence and their classi-
fication into different roles (such as Agent, Patient,
Instrument, etc.). Given the sentence ?The pearls
I left to my son are fake? an SRL system would
conclude that for the verb ?leave?, ?I? is the agent,
?pearls? is the patient and ?son? is the benefactor.
Because not all roles feature in each verb the roles
are commonly divided into meta-roles (A0-A7) and
additional common classes such as location, time,
etc. Each Ai can represent a different role based
on the verb, though A0 and A1 most often refer to
agents and patients respectively. Availability of lexi-
cal resources such as Propbank (Martha and Palmer,
2002), which annotates text with meta-roles for each
argument, has enabled significant progress in SRL
systems over the last few years.
Recently, there have been many advances in SRL
(Toutanova et al, 2008; Johansson and Nugues,
2008; Coppola et al, 2009; Moschitti et al, 2008).
We use UIUC-SRL as our base SRL system (Pun-
yakanok et al, 2008). Our choice of the system is
guided by the fact that its code is freely available and
it is competitive with state of the art (it achieved the
highest F1 score on the CoNLL-2005 shared task).
UIUC-SRL operates in four key steps: pruning,
argument identification, argument classification and
53
inference. Pruning involves using a full parse tree
and heuristic rules to eliminate constituents that are
unlikely to be arguments. Argument identification
uses a classifier to identify constituents that are po-
tential arguments. In argument classification, an-
other classifier is used, this time to assign role labels
to the candidates identified in the previous stage. Ar-
gument information is not incorporated across argu-
ments until the inference stage, which uses an inte-
ger linear program to make global role predictions.
3 SRL-IE
Our key insight is that semantically labeled argu-
ments in a sentence almost always correspond to the
arguments in Open IE extractions. Thus, we can
convert the output of UIUC-SRL into an Open IE
extraction. We illustrate this conversion process via
an example.
Given the sentence, ?Eli Whitney created the cot-
ton gin in 1793,? TEXTRUNNER extracts two tuples,
one binary and one n-ary, as follows:
binary tuple:
arg0 Eli Whitney
rel created
arg1 the cotton gin
n-ary tuple:
arg0 Eli Whitney
rel created (arg) in
arg1 the cotton gin
arg2 1793
UIUC-SRL labels constituents of a sentence with
the role they play in regards to the verb in the sen-
tence. UIUC-SRL will extract:
A0 Eli Whitney
verb created
A1 the cotton gin
temporal in 1793
To convert UIUC-SRL output to Open IE format,
SRL-IE treats the verb (along with its modifiers and
negation, if present) as the relation. Moreover, it
assumes SRL?s role-labeled arguments as the Open
IE arguments related to the relation. The arguments
here consist of all entities labeled Ai, as well as any
entities that are marked Direction, Location, or Tem-
poral. We order the arguments in the same order as
they are in the sentence and with regard to the re-
lation (except for direction, location and temporal,
which cannot be arg0 of an Open IE extraction and
are placed at the end of argument list). As we are
interested in relations, we consider only extractions
that have at least two arguments.
In doing this conversion, we naturally ignore part
of the semantic information (such as distinctions be-
tween various Ai?s) that UIUC-SRL provides. In
this conversion process an SRL extraction that was
correct in the original format will never be changed
to an incorrect Open IE extraction. However, an in-
correctly labeled SRL extraction could still convert
to a correct Open IE extraction, if the arguments
were correctly identified but incorrectly labeled.
Because of the methodology that TEXTRUNNER
uses to extract relations, for n-ary extractions of the
form <arg0, rel, arg1, ..., argN>, TEXTRUNNER
often extracts sub-parts <arg0, rel, arg1>, <arg0,
rel, arg1, arg2>, ..., <arg0, rel, arg1, ..., argN-1>.
UIUC-SRL, however, extracts at most only one re-
lation for each verb in the sentence. For a fair com-
parison, we create additional subpart extractions for
each UIUC-SRL extraction using a similar policy.
4 Qualitative Comparison of Extractors
In order to understand SRL-IE better, we first com-
pare with TEXTRUNNER in a variety of scenarios,
such as sentences with lists, complex sentences, sen-
tences with out of vocabulary verbs, etc.
Argument boundaries: SRL-IE is lenient in de-
ciding what constitutes an argument and tends to
err on the side of including too much rather than
too little; TEXTRUNNER is much more conservative,
sometimes to the extent of omitting crucial informa-
tion, particularly post-modifying clauses and PPs.
For example, TEXTRUNNER extracts <Bunsen, in-
vented, a device> from the sentence ?Bunsen in-
vented a device called the Spectroscope?. SRL-IE
includes the entire phrase ?a device called the Spec-
troscope? as the second argument. Generally, the
longer arguments in SRL-IE are more informative
than TEXTRUNNER?s succinct ones. On the other
hand, TEXTRUNNER?s arguments normalize better
leading to an effective use of redundancy in ranking.
Lists: In sentences with a comma-separated lists of
nouns, SRL-IE creates one extraction and treats the
entire list as the argument, whereas TEXTRUNNER
separates them into several relations, one for each
item in the list.
Out of vocabulary verbs: While we expected
54
TEXTRUNNER to handle unknown verbs with lit-
tle difficulty due to its unlexicalized nature, SRL-
IE could have had severe trouble leading to a lim-
ited applicability in the context of Web text. How-
ever, contrary to our expectations, UIUC-SRL has
a graceful policy to handle new verbs by attempt-
ing to identify A0 (the agent) and A1 (the patient)
and leaving out the higher numbered ones. In prac-
tice, this is very effective ? SRL-IE recognizes the
verb and its two arguments correctly in ?Larry Page
googled his name and launched a new revolution.?
Part-of-speech ambiguity: Both SRL-IE and
TEXTRUNNER have difficulty when noun phrases
have an identical spelling with a verb. For example,
the word ?write? when used as a noun causes trouble
for both systems. In the sentence, ?Be sure the file
has write permission.? SRL-IE and TEXTRUNNER
both extract <the file, write, permission>.
Complex sentences: Because TEXTRUNNER only
uses shallow syntactic features it has a harder time
on sentences with complex structure. SRL-IE,
because of its deeper processing, can better handle
complex syntax and long-range dependencies, al-
though occasionally complex sentences will create
parsing errors causing difficulties for SRL-IE.
N-ary relations: Both extractors suffer significant
quality loss in n-ary extractions compared to binary.
A key problem is prepositional phrase attachment,
deciding whether the phrase associates with arg1 or
with the verb.
5 Experimental Results
In our quantitative evaluation we attempt to answer
two key questions: (1) what is the relative difference
in performance of SRL-IE and TEXTRUNNER on
precision, recall and computation time? And, (2)
what factors influence the relative performance of
the two systems? We explore the first question in
Section 5.2 and the second in Section 5.3.
5.1 Dataset
Our goal is to explore the behavior of TEXTRUN-
NER and SRL-IE on a large scale dataset containing
redundant information, since redundancy has been
shown to immensely benefit Web-based Open IE ex-
tractors. At the same time, the test set must be a
manageable size, due to SRL-IE?s relatively slow
processing time. We constructed a test set that ap-
proximates Web-scale distribution of extractions for
five target relations ? invent, graduate, study, write,
and develop.
We created our test set as follows. We queried a
corpus of 500M Web documents for a sample of sen-
tences with these verbs (or their inflected forms, e.g.,
invents, invented, etc.). We then ran TEXTRUNNER
and SRL-IE on those sentences to find 200 distinct
values of arg0 for each target relation, 100 from each
system. We searched for at most 100 sentences that
contain both the verb-form and arg0. This resulted
in a test set of an average of 6,000 sentences per re-
lation, for a total of 29,842 sentences. We use this
test set for all experiments in this paper.
In order to compute precision and recall on this
dataset, we tagged extractions by TEXTRUNNER
and by SRL-IE as correct or errors. A tuple is cor-
rect if the arguments have correct boundaries and
the relation accurately expresses the relationship be-
tween all of the arguments. Our definition of cor-
rect boundaries does not favor either system over
the other. For instance, while TEXTRUNNER ex-
tracts <Bunsen, invented, a device> from the sen-
tence ?Bunsen invented a device called the Spectro-
scope?, and SRL-IE includes the entire phrase ?a
device called the Spectroscope? as the second argu-
ment, both extractions would be marked as correct.
Determining the absolute recall in these experi-
ments is precluded by the amount of hand labeling
necessary and the ambiguity of such a task. Instead,
we compute pseudo-recall by taking the union of
correct tuples from both methods as denominator.1
5.2 Relative Performance
Table 1 shows the performance of TEXTRUNNER
and SRL-IE on this dataset. Since TEXTRUNNER
can output different points on the precision-recall
curve based on the confidence of the CRF we choose
the point that maximizes F1.
SRL-IE achieved much higher recall at substan-
tially higher precision. This was, however, at the
cost of a much larger processing time. For our
dataset, TEXTRUNNER took 6.3 minutes and SRL-
1Tuples from the two systems are considered equivalent if
for the relation and each argument, the extracted phrases are
equal or if one phrase is contained within the phrase extracted
by the other system.
55
TEXTRUNNER SRL-IE
P R F1 P R F1
Binary 51.9 27.2 35.7 64.4 85.9 73.7
N-ary 39.3 28.2 32.9 54.4 62.7 58.3
All 47.9 27.5 34.9 62.1 79.9 69.9
Time 6.3 minutes 52.1 hours
Table 1: SRL-IE outperforms TEXTRUNNER in both re-
call and precision, but has over 2.5 orders of magnitude
longer run time.
IE took 52.1 hours ? roughly 2.5 orders of magni-
tude longer. We ran our experiments on quad-core
2.8GHz processors with 4GB of memory.
It is important to note that our results for TEX-
TRUNNER are different from prior results (Banko,
2009). This is primarily due to a few operational
criteria (such as focusing on proper nouns, filtering
relatively infrequent extractions) identified in prior
work that resulted in much higher precision, proba-
bly at significant cost of recall.
5.3 Comparison under Different Conditions
Although SRL-IE has higher overall precision,
there are some conditions under which TEXTRUN-
NER has superior precision. We analyze the perfor-
mance of these two systems along three key dimen-
sions: system confidence, redundancy, and locality.
System Confidence: TEXTRUNNER?s CRF-based
extractor outputs a confidence score which can be
varied to explore different points in the precision-
recall space. Figure 1(a) and Figure 2(a) report the
results from ranking extractions by this confidence
value. For both binary and n-ary extractions the con-
fidence value improves TEXTRUNNER?s precision
and for binary the high precision end has approxi-
mately the same precision as SRL-IE. Because of
its use of an integer linear program, SRL-IE does
not associate confidence values with extractions and
is shown as a point in these figures.
Redundancy: In this experiment we use the re-
dundancy of extractions as a measure of confidence.
Here redundancy is the number of times a relation
has been extracted from unique sentences. We com-
pute redundancy over normalized extractions, ignor-
ing noun modifiers, adverbs, and verb inflection.
Figure 1(b) and Figure 2(b) display the results for
binary and n-ary extractions, ranked by redundancy.
We use a log scale on the x-axis since high redun-
dancy extractions account for less than 1% of the
recall. For binary extractions, redundancy improved
TEXTRUNNER?s precision significantly, but at a dra-
matic loss in recall. TEXTRUNNER achieved 0.8
precision with 0.001 recall at redundancy of 10 and
higher. For highly redundant information (common
concepts, etc.) TEXTRUNNER has higher precision
than SRL-IE and would be the algorithm of choice.
In n-ary relations for TEXTRUNNER and in binary
relations for SRL-IE, redundancy actually hurts
precision. These extractions tend to be so specific
that genuine redundancy is rare, and the highest fre-
quency extractions are often systematic errors. For
example, the most frequent SRL-IE extraction was
<nothing, write, home>.
Locality: Our experiments with TEXTRUNNER led
us to discover a new validation scheme for the ex-
tractions ? locality. We observed that TEXTRUN-
NER?s shallow features can identify relations more
reliably when the arguments are closer to each other
in the sentence. Figure 1(c) and Figure 2(c) report
the results from ranking extractions by the number
of tokens that separate the first and last arguments.
We find a clear correlation between locality and
precision of TEXTRUNNER, with precision 0.77 at
recall 0.18 for TEXTRUNNER where the distance is
4 tokens or less for binary extractions. For n-ary re-
lations, TEXTRUNNER can match SRL-IE?s preci-
sion of 0.54 at recall 0.13. SRL-IE remains largely
unaffected by locality, probably due to the parsing
used in SRL.
6 A TEXTRUNNER SRL-IE Hybrid
We now present two hybrid systems that combine
the strengths of TEXTRUNNER (fast processing time
and high precision on a subset of sentences) with the
strengths of SRL-IE (higher recall and better han-
dling of long-range dependencies). This is set in a
scenario where we have a limited budget on com-
putational time and we need a high performance ex-
tractor that utilizes the available time efficiently.
Our approach is to run TEXTRUNNER on all sen-
tences, and then determine the order in which to pro-
cess sentences with SRL-IE. We can increase preci-
sion by filtering out TEXTRUNNER extractions that
are expected to have low precision.
56
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 1: Ranking mechanisms for binary relations. (a) The confidence specified by the CRF improves TEXTRUN-
NER?s precision. (b) For extractions with highest redundancy, TEXTRUNNER has higher precision than SRL-IE. Note
the log scale for the x-axis. (c) Ranking by the distance between arguments gives a large boost to TEXTRUNNER?s
precision.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 2: Ranking mechanisms for n-ary relations. (a) Ranking by confidence gives a slight boost to TEXTRUNNER?s
precision. (b) Redundancy helps SRL-IE, but not TEXTRUNNER. Note the log scale for the x-axis. (c) Ranking by
distance between arguments raises precision for TEXTRUNNER and SRL-IE.
A naive hybrid will run TEXTRUNNER over all
the sentences and use the remaining time to run
SRL-IE on a random subset of the sentences and
take the union of all extractions. We refer to this
version as RECALLHYBRID, since this does not lose
any extractions, achieving highest possible recall.
A second hybrid, which we call PRECHYBRID,
focuses on increasing the precision and uses the fil-
ter policy and an intelligent order of sentences for
extraction as described below.
Filter Policy for TEXTRUNNER Extractions: The
results from Figure 1 and Figure 2 show that TEX-
TRUNNER?s precision is low when the CRF confi-
dence in the extraction is low, when the redundancy
of the extraction is low, and when the arguments are
far apart. Thus, system confidence, redundancy, and
locality form the key factors for our filter policy: if
the confidence is less than 0.5 and the redundancy
is less than 2 or the distance between the arguments
in the sentence is greater than 5 (if the relation is
binary) or 8 (if the relation is n-ary) discard this tu-
ple. These thresholds were determined by a param-
eter search over a small dataset.
Order of Sentences for Extraction: An optimal
ordering policy would apply SRL-IE first to the sen-
tences where TEXTRUNNER has low precision and
leave the sentences that seem malformed (e.g., in-
complete sentences, two sentences spliced together)
for last. As we have seen, the distance between the
first and last argument is a good indicator for TEX-
TRUNNER precision. Moreover, a confidence value
of 0.0 by TEXTRUNNER?s CRF classifier is good ev-
idence that the sentence may be malformed and is
unlikely to contain a valid relation.
We rank sentences S in the following way, with
SRL-IE processing sentences from highest ranking
to lowest: if CRF.confidence = 0.0 then S.rank = 0,
else S.rank = average distance between pairs of ar-
guments for all tuples extracted by TEXTRUNNER
from S.
While this ranking system orders sentences ac-
cording to which sentence is likely to yield maxi-
mum new information, it misses the cost of compu-
tation. To account for computation time, we also
estimate the amount of time SRL-IE will take to
process each sentence using a linear model trained
on the sentence length. We then choose the sentence
57
that maximizes information gain divided by compu-
tation time.
6.1 Properties of Hybrid Extractors
The choice between the two hybrid systems is a
trade-off between recall and precision: RECALLHY-
BRID guarantees the best recall, since it does not lose
any extractions, while PRECHYBRID is designed to
maximize the early boost in precision. The evalua-
tion in the next section bears out these expectations.
6.2 Evaluation of Hybrid Extractors
Figure 3(a) and Figure 4(a) report the precision of
each system for binary and n-ary extractions mea-
sured against available computation time. PRECHY-
BRID starts at slightly higher precision due to our
filtering of potentially low quality extractions from
TEXTRUNNER. For binary this precision is even
better than SRL-IE?s. It gradually loses precision
until it reaches SRL-IE?s level. RECALLHYBRID
improves on TEXTRUNNER?s precision, albeit at a
much slower rate and remains worse than SRL-IE
and PRECHYBRID throughout.
The recall for binary and n-ary extractions are
shown in Figure 3(b) and Figure 4(b), again mea-
sured against available time. While PRECHYBRID
significantly improves on TEXTRUNNER?s recall, it
does lose recall compared to RECALLHYBRID, es-
pecially for n-ary extractions. PRECHYBRID also
shows a large initial drop in recall due to filtering.
Lastly, the gains in precision from PRECHYBRID
are offset by loss in recall that leaves the F1 mea-
sure essentially identical to that of RECALLHYBRID
(Figures 3(c),4(c)). However, for a fixed time bud-
get both hybrid F-measures are significantly bet-
ter than TEXTRUNNER and SRL-IE F-measures
demonstrating the power of the hybrid extractors.
Both methods reach a much higher F1 than TEX-
TRUNNER: a gain of over 0.15 in half SRL-IE?s
processing time and over 0.3 after the full process-
ing time. Both hybrids perform better than SRL-IE
given equal processing time.
We believe that most often constructing a higher
quality database of facts with a relatively lower
recall is more useful than vice-versa, making
PRECHYBRID to be of wider applicability than RE-
CALLHYBRID. Still the choice of the actual hybrid
extractor could change based on the task.
7 Related Work
Open information extraction is a relatively recent
paradigm and hence, has been studied by only a
small number of researchers. The most salient is
TEXTRUNNER, which also introduced the model
(Banko et al, 2007; Banko and Etzioni, 2008).
A version of KNEXT uses heuristic rules and syn-
tactic parses to convert a sentence into an unscoped
logical form (Van Durme and Schubert, 2008). This
work is more suitable for extracting common sense
knowledge as opposed to factual information.
Another Open IE system, Kylin (Weld et al,
2008), suggests automatically building an extractor
for each relation using self-supervised training, with
training data generated using Wikipedia infoboxes.
This work has the limitation that it can only extract
relations expressed in Wikipedia infoboxes.
A paradigm related to Open IE is Preemptive IE
(Shinyama and Sekine, 2006). While one goal of
Preemptive IE is to avoid relation-specificity, Pre-
emptive IE does not emphasize Web scalability,
which is essential to Open IE.
(Carlson et al, 2009) presents a semi-supervised
approach to information extraction on the Web. It
learns classifiers for different relations and couples
the training of those classifiers with ontology defin-
ing constraints. While we attempt to learn unknown
relations, it learns a pre-defined set of relations.
Another related system is WANDERLUST (Akbik
and Bro?, 2009). The authors of this system anno-
tated 10,000 sentences parsed with LinkGrammar,
resulting in 46 general linkpaths as patterns for rela-
tion extraction. With these patterns WANDERLUST
extracts binary relations from link grammar link-
ages. In contrast to our approaches, this requires a
large set of hand-labeled examples.
USP (Poon and Domingos, 2009) is based on
Markov Logic Networks and attempts to create a
full semantic parse in an unsupervised fashion. They
evaluate their work on biomedical text, so its appli-
cability to general Web text is not yet clear.
8 Discussion and Future Work
The Heavy Tail: It is well accepted that informa-
tion on the Web is distributed according to Zipf?s
58
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 3: (a) Precision for binary extractions for PRECHYBRID starts higher than the precision of SRL-IE. (b) Recall
for binary extractions rises over time for both hybrid systems, with PRECHYBRID starting lower. (c) Hybrid extractors
obtain the best F-measure given a limited budget of computation time.
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 4: (a) PRECHYBRID also gives a strong boost to precision for n-ary extractions. (b) Recall for n-ary extractions
for RECALLHYBRID starts substantially higher than PRECHYBRID and finally reaches much higher recall than SRL-
IE alone. (c) F-measure for n-ary extractions. The hybrid extractors outperform others.
Law (Downey et al, 2005), implying that there is a
heavy tail of facts that are mentioned only once or
twice. The prior work on Open IE ascribes prime
importance to redundancy based validation, which,
as our results show (Figures 1(b), 2(b)), captures a
very tiny fraction of the available information. We
believe that deeper processing of text is essential to
gather information from this heavy tail. Our SRL-
IE extractor is a viable algorithm for this task.
Understanding SRL Components: UIUC-SRL
as well as other SRL algorithms have different sub-
components ? parsing, argument classification, joint
inference, etc. We plan to study the effective con-
tribution of each of these components. Our hope is
to identify the most important subset, which yields
a similar quality at a much reduced computational
cost. Another alternative is to add the best perform-
ing component within TEXTRUNNER.
9 Conclusions
This paper investigates the use of semantic features,
in particular, semantic role labeling for the task of
open information extraction. We describe SRL-IE,
the first SRL based Open IE system. We empirically
compare the performance of SRL-IE with TEX-
TRUNNER, a state-of-the-art Open IE system and
find that on average SRL-IE has much higher re-
call and precision, however, TEXTRUNNER outper-
forms in precision for the case of highly redundant
or high locality extractions. Moreover, TEXTRUN-
NER is over 2 orders of magnitude faster.
These complimentary strengths help us design hy-
brid extractors that achieve better performance than
either system given a limited budget of computation
time. Overall, we provide evidence that, contrary to
belief in the Open IE literature (Banko and Etzioni,
2008), semantic approaches have a lot to offer for
the task of Open IE and the vision of machine read-
ing.
10 Acknowledgements
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750-09-C-0179, and carried
out at the University of Washington?s Turing Cen-
ter.
59
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Alan Akbik and Ju?gen Bro?. 2009. Wanderlust: Extract-
ing semantic relations from natural language text us-
ing dependency grammar patterns. In Proceedings of
the Workshop on Semantic Search (SemSearch 2009)
at the 18th International World Wide Web Conference
(WWW 2009).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07: Pro-
ceedings of the 20th international joint conference on
Artifical intelligence, pages 2670?2676.
Michele Banko. 2009. Open Information Extraction for
the Web. Ph.D. thesis, University of Washington.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka
Jr., and Tom M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workskop on
Semi-supervised Learning for Natural Language Pro-
cessing.
Bonaventura Coppola, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Shallow semantic parsing
for spoken language understanding. In NAACL ?09:
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 85?88.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI ?05: Proceedings of the
20th international joint conference on Artifical intelli-
gence, pages 1034?1041.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In AAAI?06: proceedings of
the 21st national conference on Artificial intelligence,
pages 1517?1519.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP ?09: Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?10.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1-3):233?272.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
knowledge extraction through compositional language
processing. In STEP ?08: Proceedings of the 2008
Conference on Semantics in Text Processing, pages
239?254.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008.
Using wikipedia to bootstrap open information extrac-
tion. SIGMOD Rec., 37(4):62?68.
60
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	