167
168
169
170
Comparing Automatic and Human Evaluation of NLG Systems
Anja Belz
Natural Language Technology Group
CMIS, University of Brighton
UK
A.S.Belz@brighton.ac.uk
Ehud Reiter
Dept of Computing Science
University of Aberdeen
UK
ereiter@csd.abdn.ac.uk
Abstract
We consider the evaluation problem in
Natural Language Generation (NLG) and
present results for evaluating several NLG
systems with similar functionality, includ-
ing a knowledge-based generator and sev-
eral statistical systems. We compare eval-
uation results for these systems by human
domain experts, human non-experts, and
several automatic evaluation metrics, in-
cluding NIST, BLEU, and ROUGE. We
find that NIST scores correlate best (>
0.8) with human judgments, but that all
automatic metrics we examined are bi-
ased in favour of generators that select on
the basis of frequency alone. We con-
clude that automatic evaluation of NLG
systems has considerable potential, in par-
ticular where high-quality reference texts
and only a small number of human evalua-
tors are available. However, in general it is
probably best for automatic evaluations to
be supported by human-based evaluations,
or at least by studies that demonstrate that
a particular metric correlates well with hu-
man judgments in a given domain.
1 Introduction
Evaluation is becoming an increasingly important
topic in Natural Language Generation (NLG), as
in other fields of computational linguistics. Some
NLG researchers are impressed by the success of
the BLEU evaluation metric (Papineni et al, 2002)
in Machine Translation (MT), which has trans-
formed the MT field by allowing researchers to
quickly and cheaply evaluate the impact of new
ideas, algorithms, and data sets. BLEU and re-
lated metrics work by comparing the output of an
MT system to a set of reference (?gold standard?)
translations, and in principle this kind of evalua-
tion could be done with NLG systems as well. In-
deed NLG researchers are already starting to use
BLEU (Habash, 2004; Belz, 2005) in their evalua-
tions, as this is much cheaper and easier to organ-
ise than the human evaluations that have tradition-
ally been used to evaluate NLG systems.
However, the use of such corpus-based evalua-
tion metrics is only sensible if they are known to
be correlated with the results of human-based eval-
uations. While studies have shown that ratings of
MT systems by BLEU and similar metrics corre-
late well with human judgments (Papineni et al,
2002; Doddington, 2002), we are not aware of any
studies that have shown that corpus-based evalu-
ation metrics of NLG systems are correlated with
human judgments; correlation studies have been
made of individual components (Bangalore et al,
2000), but not of systems.
In this paper we present an empirical study
of how well various corpus-based metrics agree
with human judgments, when evaluating several
NLG systems that generate sentences which de-
scribe changes in the wind (for weather forecasts).
These systems do not perform content determina-
tion (they are limited to microplanning and realisa-
tion), so our study does not address corpus-based
evaluation of content determination.
2 Background
2.1 Evaluation of NLG systems
NLG systems have traditionally been evaluated
using human subjects (Mellish and Dale, 1998).
NLG evaluations have tended to be of the intrinsic
type (Sparck Jones and Galliers, 1996), involving
subjects reading and rating texts; usually subjects
313
are shown both NLG and human-written texts, and
the NLG system is evaluated by comparing the rat-
ings of its texts and human texts. In some cases,
subjects are shown texts generated by several NLG
systems, including a baseline system which serves
as another point of comparison. This methodology
was first used in NLG in the mid-1990s by Coch
(1996) and Lester and Porter (1997), and contin-
ues to be popular today.
Other, extrinsic, types of human evaluations
of NLG systems include measuring the impact
of different generated texts on task performance
(Young, 1999), measuring how much experts post-
edit generated texts (Sripada et al, 2005), and
measuring how quickly people read generated
texts (Williams and Reiter, 2005).
In recent years there has been growing interest
in evaluating NLG texts by comparing them to a
corpus of human-written texts. As in other ar-
eas of NLP, the advantages of automatic corpus-
based evaluation are that it is potentially much
cheaper and quicker than human-based evaluation,
and also that it is repeatable. Corpus-based evalu-
ation was first used in NLG by Langkilde (1998),
who parsed texts from a corpus, fed the output of
her parser to her NLG system, and then compared
the generated texts to the original corpus texts.
Similar evaluations have been used e.g. by Banga-
lore et al (2000) andMarciniak and Strube (2004).
Such corpus-based evaluations have sometimes
been criticised in the NLG community, for example
by Reiter and Sripada (2002). Grounds for crit-
icism include the fact that regenerating a parsed
text is not a realistic NLG task; that texts can be
very different from a corpus text but still effec-
tively meet the system?s communicative goal; and
that corpus texts are often not of high enough qual-
ity to form a realistic test.
2.2 Automatic evaluation of generated texts
in MT and Summarisation
The MT and document summarisation communi-
ties have developed evaluation metrics based on
comparing output texts to a corpus of human texts,
and have shown that some of these metrics are
highly correlated with human judgments.
The BLEU metric (Papineni et al, 2002) in MT
has been particularly successful; for example MT-
05, the 2005 NIST MT evaluation exercise, used
BLEU-4 as the only method of evaluation. BLEU
is a precision metric that assesses the quality of a
translation in terms of the proportion of its word n-
grams (n = 4 has become standard) that it shares
with one or more high-quality reference transla-
tions. BLEU scores range from 0 to 1, 1 being the
highest which can only be achieved by a transla-
tion if all its substrings can be found in one of the
reference texts (hence a reference text will always
score 1). BLEU should be calculated on a large
test set with several reference translations (four ap-
pears to be standard in MT). Properly calculated
BLEU scores have been shown to correlate reliably
with human judgments (Papineni et al, 2002).
The NIST MT evaluation metric (Doddington,
2002) is an adaptation of BLEU, but where BLEU
gives equal weight to all n-grams, NIST gives more
importance to less frequent (hence more infor-
mative) n-grams. BLEU?s ability to detect subtle
but important differences in translation quality has
been questioned, some research showing NIST to
be more sensitive (Doddington, 2002; Riezler and
Maxwell III, 2005).
The ROUGE metric (Lin and Hovy, 2003) was
conceived as document summarisation?s answer to
BLEU, but it does not appear to have met with the
same degree of enthusiasm. There are several dif-
ferent ROUGE metrics. The simplest is ROUGE-N,
which computes the highest proportion in any ref-
erence summary of n-grams that are matched by
the system-generated summary. A procedure is
applied that averages the score across leave-one-
out subsets of the set of reference texts. ROUGE-
N is an almost straightforward n-gram recall met-
ric between two texts, and has several counter-
intuitive properties, including that even a text com-
posed entirely of sentences from reference texts
cannot score 1 (unless there is only one refer-
ence text). There are several other variants of the
ROUGE metric, and ROUGE-2, along with ROUGE-
SU (based on skip bigrams and unigrams), were
among the official scores for the DUC 2005 sum-
marisation task.
2.3 SUMTIME
The SUMTIME project (Reiter et al, 2005) de-
veloped an NLG system which generated textual
weather forecasts from numerical forecast data.
The SUMTIME system generates specialist fore-
casts for offshore oil rigs. It has two modules:
a content-determination module that determines
the content of the weather forecast by analysing
the numerical data using linear segmentation and
314
other data analysis techniques; and a microplan-
ning and realisation module which generates texts
based on this content by choosing appropriate
words, deciding on aggregation, enforcing the
sublanguage grammar, and so forth. SUMTIME
generates very high-quality texts, in some cases
forecast users believe SUMTIME texts are better
than human-written texts (Reiter et al, 2005).
SUMTIME is a knowledge-based NLG system.
While its design was informed by corpus analysis
(Reiter et al, 2003), the system is based on manu-
ally authored rules and code.
As part of the project, the SUMTIME team cre-
ated a corpus of 1045 forecasts from the commer-
cial output of five different forecasters and the in-
put data (numerical predictions of wind, tempera-
ture, etc) that the forecasters examined when they
wrote the forecasts (Sripada et al, 2003). In other
words, the SUMTIME corpus contains both the in-
puts (numerical weather predictions) and the out-
puts (forecast texts) of the forecast-generation pro-
cess. The SUMTIME team also derived a con-
tent representation (called ?tuples?) from the cor-
pus texts similar to that produced by SUMTIME?s
content-determination module. The SUMTIME
microplanner/realiser can be driven by these tu-
ples; this mode (combining human content deter-
mination with SUMTIME microplanning and real-
isation) is called SUMTIME-Hybrid. Table 1 in-
cludes an example of the tuples extracted from the
corpus text (row 1), and a SUMTIME-Hybrid text
produced from the tuples (row 5).
2.4 pCRU language generation
Statistical NLG has focused on generate-and-select
models: a set of alternatives is generated and one
is selected with a language model. This technique
is computationally very expensive. Moreover, the
only type of language model used in NLG are n-
gram models which have the additional disadvan-
tage of a general preference for shorter realisa-
tions, which can be harmful in NLG (Belz, 2005).
pCRU1 language generation (Belz, 2006) is a
language generation framework that was designed
to facilitate statistical generation techniques that
are more efficient and less biased. In pCRU gen-
eration, a base generator is encoded as a set of
generation rules made up of relations with zero
or more atomic arguments. The base generator
1Probabilistic Context-free Representational Underspeci-
fication.
is then trained on raw text corpora to provide a
probability distribution over generation rules. The
resulting PCRU generator can be run in several
modes, including the following:
Random: ignoring pCRU probabilities, randomly
select generation rules.
N-gram: ignoring pCRU probabilities, generate
set of alternatives and select the most likely ac-
cording to a given n-gram language model.
Greedy: select the most likely among each set of
candidate generation rules.
Greedy roulette: select rules with likelihood pro-
portional to their pCRU probability.
The greedy modes are deterministic and there-
fore considerably cheaper in computational terms
than the equivalent n-gram method (Belz, 2005).
3 Experimental Procedure
The main goal of our experiments was to deter-
mine how well a variety of automatic evaluation
metrics correlated with human judgments of text
quality in NLG. A secondary goal was to deter-
mine if there were types of NLG systems for which
the correlation of automatic and human evaluation
was particularly good or bad.
Data: We extracted from each forecast in the
SUMTIME corpus the first description of wind (at
10m height) from every morning forecast (the text
shown in Table 1 is a typical example), which re-
sulted in a set of about 500 wind forecasts. We
excluded several forecasts for which we had no in-
put data (numerical weather predictions) or an in-
complete set of system outputs; this left 465 texts,
which we used in our evaluation.
The inputs to the generators were tuples com-
posed of an index, timestamp, wind direction,
wind speed range, and gust speed range (see ex-
amples at top of Table 1).
We randomly selected a subset of 21 forecast
dates for use in human evaluations. For these 21
forecast dates, we also asked two meteorologists
who had not contributed to the original SUMTIME
corpus to write new forecasts texts; we used these
as reference texts for the automatic metrics. The
forecasters created these texts by rewriting the cor-
pus texts, as this was a more natural task for them
than writing texts based on tuples.
500 wind descriptions may seem like a small
corpus, but in fact provides very good coverage as
315
Input [[0,0600,SSW,16,20,-,-],[1,NOTIME,SSE,-,-,-,-],[2,0000,VAR,04,08,-,-]]
Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING
Human1 SSW?LY 16-20 GRADUALLY BACKING SSE?LY THEN DECREASING VARIABLE 4-8 BY LATE EVENING
Human2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENING
SumTime SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT
pCRU
-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING
-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8
-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATER
-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8
Table 1: Input tuples with corresponding forecasts in corpus, written by two experts and generated by all
systems (for 5 Oct 2000).
the domain language is extremely simple, involv-
ing only about 90 word forms (not counting num-
bers and wind directions) and a small handful of
different syntactic structures.
Systems and texts evaluated: We evaluated
four pCRU generators and the SUMTIME system,
operating in Hybrid mode (Section 2.3) for better
comparability because the pCRU generators do not
perform content determination.
A base pCRU generator was created semi-
automatically by running a chunker over the cor-
pus, extracting generation rules and adding some
higher-level rules taking care of aggregation, eli-
sion etc. This base generator was then trained on
9/10 of the corpus (the training data). 5 different
random divisions of the corpus into training and
testing data were used (i.e. all results were val-
idated by 5-fold hold-out cross-validation). Ad-
ditionally, a back-off 2-gram model with Good-
Turing discounting and no lexical classes was built
from the same training data, using the SRILM
toolkit (Stolcke, 2002). Forecasts were then gen-
erated for all corpus inputs, in all four generation
modes (Section 2.4).
Table 1 shows an example of an input to the sys-
tems, along with the three human texts (Corpus,
Human1, Human2) and the texts produced by all
five NLG systems from this data.
Automatic evaluations: We used NIST2,
BLEU3, and ROUGE4 to automatically evaluate the
above systems and texts. We computed BLEU-N
for N = 1..4 (using BLEU-4 as our main BLEU
score). We also computed NIST-5 and ROUGE-4.
As a baseline we used string-edit (SE) distance
2
http://cio.nist.gov/esd/emaildir/lists/mt list/bin00000.bin
3
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
4
http://www.isi.edu/?cyl/ROUGE/latest.html
with substitution at cost 2, and deletion and
insertion at cost 1, and normalised to range 0 to
1 (perfect match). When multiple reference texts
are used, the SE score for a generator forecast
is the average of its scores against the reference
texts; the SE score for a set of generator forecasts
is the average of scores for individual forecasts.
Human evaluations: We recruited 9 experts
(people with experience reading forecasts for off-
shore oil rigs) and 21 non-experts (people with no
such experience). Subjects did not have a back-
ground in NLP, and were native speakers of En-
glish. They were shown forecast texts from all the
generators and from the corpus, and asked to score
them on a scale of 0 to 5, for readability, clarity
and general appropriateness. Experts were addi-
tionally shown the numerical weather data that the
forecast text was based on. At the start, subjects
were shown two practice examples. The exper-
iments were carried out over the web. Subjects
completed the experiment unsupervised, at a time
and place of their choosing.
Expert subjects were shown a randomly se-
lected forecast for 18 of the dates. The non-experts
were shown 21 forecast texts, in a repeated Latin
squares (non-repeating column and row entries)
experimental design where each combination of
date and system is assigned one evaluation.
4 Results
Table 2 shows evaluation scores for the five NLG
systems and the corpus texts as assessed by ex-
perts, non-experts, NIST-5, BLEU-4, ROUGE-4 and
SE. Scores are averaged over the 18 forecasts that
were used in the expert experiments (for which we
had scores by all metrics and humans) in order
to make results as directly comparable as possi-
316
System Experts Non-experts NIST-5 BLEU-4 ROUGE-4 SE
SUMTIME-Hybrid 0.762 (1) 0.77 (1) 5.985 (2) 0.552 (2) 0.192 (3) 0.582 (3)
pCRU-greedy 0.716 (2) 0.68 (3) 6.549 (1) 0.613 (1) 0.315 (1) 0.673 (1)
SUMTIME-Corpus 0.644 (-) 0.736 (-) 8.262 (-) 0.877 (-) 0.569 (-) 0.835 (-)
pCRU-roulette 0.622 (3) 0.714 (2) 5.833 (3) 0.478 (4) 0.156 (4) 0.571 (4)
pCRU-2gram 0.536 (4) 0.65 (4) 5.592 (4) 0.519 (3) 0.223 (2) 0.626 (2)
pCRU-random 0.484 (5) 0.496 (5) 4.287 (5) 0.296 (5) 0.075 (5) 0.464 (5)
Table 2: Evaluation scores against 2 reference texts, for set of 18 forecasts used in expert evaluation.
Experts Non-experts NIST-5 BLEU-4 ROUGE-4 SE
Experts 1 (0.799) 0.845 (0.510) 0.825 0.791 0.606 0.576
Non-experts 0.845 (0.496) 1 (0.609) 0.836 0.812 0.534 0.627
NIST-5 0.825 (0.822) 0.836 (0.83) 1 (0.991) 0.973 0.884 0.911
BLEU-4 0.791 (0.790) 0.812 (0.808) 0.973 1 (0.995) 0.925 0.949
ROUGE-4 0.606 (0.604) 0.534 (0.534) 0.884 0.925 1 (0.995) 0.974
SE 0.576 (0.568) 0.627 (0.614) 0.911 0.949 0.974 1 (0.984)
Table 3: Pearson correlation coefficients between all scores for systems in Table 2.
ble. Human scores are normalised to range 0 to 1.
Systems are ranked in order of the scores given to
them by experts. All ranks are shown in brackets
behind the absolute scores.
Both experts and non-experts score SUMTIME-
Hybrid the highest, and pCRU-2gram and pCRU-
random the lowest. The experts have pCRU-
greedy in second place, where the non-experts
have pCRU-roulette. The experts rank the corpus
forecasts fourth, the non-experts second.
We used approximate randomisation (AR) as
our significance test, as recommended by Riezler
and Maxwell III (2005). Pair-wise tests between
results in Table 2 showed all but three differences
to be significant with the likelihood of incorrectly
rejecting the null hypothesis p < 0.05 (the stan-
dard threshold in NLP). The exceptions were the
differences in NIST and SE scores for SUMTIME-
Hybrid/pCRU-roulette, and the difference in BLEU
scores for SUMTIME-Hybrid/pCRU-2gram.
Table 3 shows Pearson correlation coefficients
(PCC) for the metrics and humans in Table 2.
The strongest correlation with experts and non-
experts is achieved by NIST-5 (0.82 and 0.83),
with ROUGE-4 and SE showing especially poor
correlation. BLEU-4 correlates fairly well with the
non-experts but less with the experts.
We computed another correlation statistic
(shown in brackets in Table 3) which measures
how well scores by an arbitrary single human or
run of a metric correlate with the average scores by
a set of humans or runs of a metric. This is com-
puted as the average PCC between the scores as-
signed by individual humans/runs of a metric (in-
dexing the rows in Table 3) and the average scores
assigned by a set of humans/runs of a metric (in-
dexing the columns in Table 3). For example, the
PCC for non-experts and experts is 0.845, but the
average PCC between individual non-experts and
average expert judgment is only 0.496, implying
that an arbitrary non-expert is not very likely to
correlate well with average expert judgments. Ex-
perts are better predictors for each other?s judg-
ments (0.799) than non-experts (0.609). Interest-
ingly, it turns out that an arbitrary NIST-5 run is a
better predictor (0.822) of average expert opinion
than an arbitrary single expert (0.799).
The number of forecasts we were able to use
in our human experiments was small, and to back
up the results presented in Table 2 we report
NIST-5, BLEU-4, ROUGE-4 and SE scores aver-
aged across the five test sets from the pCRU val-
idation runs, in Table 4. The picture is similar
to results for the smaller data set: the rankings
assigned by all metrics are the same, except that
NIST-5 and SE have swapped the ranks of SUM-
TIME-Hybrid and pCRU-roulette. Pair-wise AR
tests showed all differences to be significant with
p < 0.05, except for the differences in BLEU, NIST
and ROUGE scores for SUMTIME-Hybrid/pCRU-
roulette, and the difference in BLEU scores for
SUMTIME-Hybrid/pCRU-2gram.
In both Tables 2 and 4, there are two major
differences between the rankings assigned by hu-
317
System Experts NIST-5 BLEU-4 ROUGE-4 SE
SUMTIME-Hybrid 1 6.076 (3) 0.527 (2) 0.278 (3) 0.607 (4)
pCRU-greedy 2 6.925 (1) 0.641 (1) 0.425 (1) 0.758 (1)
SUMTIME-Corpus - 9.317 (-) 1 (-) 1 (-) 1 (-)
pCRU-roulette 3 6.175 (2) 0.497 (4) 0.242 (4) 0.679 (3)
pCRU-2gram 4 5.685 (4) 0.519 (3) 0.315 (2) 0.712 (2)
pCRU-random 5 4.515 (5) 0.313 (5) 0.098 (5) 0.551 (5)
Table 4: Evaluation scores against the SUMTIME corpus, on 5 test sets from pCRU validation.
man and automatic evaluation: (i) Human evalua-
tors prefer SUMTIME-Hybrid over pCRU-greedy,
whereas all the automatic metrics have it the
other way around; and (ii) human evaluators score
pCRU-roulette highly (second and third respec-
tively), whereas the automatic metrics score it very
low, second worst to random generation (except
for NIST which puts it second).
There are two clear tendencies in scores going
from left (humans) to right (SE) across Tables 2
and 4: SUMTIME-Hybrid goes down in rank, and
pCRU-2gram comes up.
In addition to the BLEU-4 scores shown in the
tables, we also calculated BLEU-1, BLEU-2, BLEU-
3 scores. These give similar results, except that
BLEU-1 and BLEU-2 rank pCRU-roulette as highly
as the human judges.
It is striking how low the experts rank the cor-
pus texts, and to what extent they disagree on their
quality. This appears to indicate that corpus qual-
ity is not ideal. If an imperfect corpus is used
as the gold standard for the automatic metrics,
then high correlation with human judgments is less
likely, and this may explain the difference in hu-
man and automatic scores for SUMTIME-Hybrid.
5 Discussion
If we assume that the human evaluation scores are
the most valid, then the automatic metrics do not
do a good job of comparing the knowledge-based
SUMTIME system to the statistical systems.
One reason for this could be that there are cases
where SUMTIME deliberately does not choose the
most common option in the corpus, because its
developers believed that it was not the best for
readers. For example, in Table 1, the human
forecasters and pCRU-greedy use the phrase by
late evening to refer to 0000, pCRU-2gram uses
the phrase later, while SUMTIME-Hybrid uses the
phrase by midnight. The pCRU choices reflect fre-
quency in the SUMTIME corpus: later (837 in-
stances) and by late evening (327 instances) are
more common than by midnight (184 instances).
However, forecast readers dislike this use of later
(because later is used to mean something else in
a different type of forecast), and also dislike vari-
ants of by evening, because they are unsure how
to interpret them (Reiter et al, 2005); this is why
SUMTIME uses by midnight.
The SUMTIME system builders believe deviat-
ing from corpus frequency in such cases makes
SUMTIME texts better from the reader?s perspec-
tive, and it does appear to increase human ratings
of the system; but deviating from the corpus in
such a way decreases the system?s score under
corpus-similarity metrics. In other words, judg-
ing the output of an NLG system by comparing it
to corpus texts by a method that rewards corpus
similarity will penalise systems which do not base
choice on highest frequency of occurrence in the
corpus, even if this is motivated by careful studies
of what is best for text readers.
The MT community recognises that BLEU is not
effective at evaluating texts which are as good as
(or better than) the reference texts. This is not
a problem for MT, because the output of current
(wide-coverage) MT systems is generally worse
than human translations. But it is an issue for NLG,
where systems are domain-specific and can gen-
erate texts that are judged better by humans than
human-written texts (as seen in Tables 4 and 2).
Although the automatic evaluation metrics gen-
erally replicated human judgments fairly well
when comparing different statistical NLG systems,
there was a discrepancy in the ranking of pCRU-
roulette (ranked high by humans, low by several of
the automatic metrics). pCRU-roulette differs from
the other statistical generators because it does not
always try to make the most common choice (max-
imise the likelihood of the corpus), instead it tries
to vary choices. In particular, if there are several
competing words and phrases with similar prob-
318
abilities, pCRU-roulette will tend to use different
words and phrases in different texts, whereas the
other statistical generators will stick to those with
the highest frequency. This behaviour is penalised
by the automatic evaluation metrics, but the hu-
man evaluators do not seem to mind it.
One of the classic rules of writing is to vary lex-
ical and syntactic choices, in order to keep text in-
teresting. However, this behaviour (variation for
variation?s sake) will always reduce a system?s
score under corpus-similarity metrics, even if it
enhances text quality from the perspective of read-
ers. Foster and Oberlander (2006), in their study of
facial gestures, have also noted that humans do not
mind and indeed in some cases prefer variation,
whereas corpus-based evaluations give higher rat-
ings to systems which follow corpus frequency.
Using more reference texts does counteract this
tendency, but only up to a point: no matter how
many reference texts are used, there will still be
one, or a small number of, most frequent variants,
and using anything else will still worsen corpus-
similarity scores.
Canvassing expert opinion of text quality and
averaging the results is also in a sense frequency-
based, as results reflect what the majority of ex-
perts consider good variants. Expert opinions can
vary considerably, as shown by the low correla-
tion among experts in our study (and as seen in
corpus studies, e.g. Reiter et al, 2005), and eval-
uations by a small number of experts may also be
problematic, unless we have good reason to be-
lieve that expert opinions are highly correlated in
the domain (which was certainly not the case in
our weather forecast domain). Ultimately, such
disagreement between experts suggests that (in-
trinsic) judgments of the text quality ? whether
by human or metric ? really should be be backed
up by (extrinsic) judgments of the effectiveness of
a text in helping real users perform tasks or other-
wise achieving its communicative goal.
6 Future Work
We plan to further investigate the performance of
automatic evaluation measures in NLG in the fu-
ture: (i) performing similar experiments to the
one described here in other domains, and with
more subjects and larger test sets; (ii) investigating
whether automatic corpus-based techniques can
evaluate content determination; (iii) investigating
how well both human ratings and corpus-based
measures correlate with extrinsic evaluations of
the effectiveness of generated texts. Ultimately,
we would like to move beyond critiques of exist-
ing corpus-based metrics to proposing (and vali-
dating) new metrics which work well for NLG.
7 Conclusions
Corpus quality plays a significant role in auto-
matic evaluation of NLG texts. Automatic metrics
can be expected to correlate very highly with hu-
man judgments only if the reference texts used are
of high quality, or rather, can be expected to be
judged high quality by the human evaluators. This
is especially important when the generated texts
are of similar quality to human-written texts.
In MT, high-quality texts vary less than gener-
ally in NLG, so BLEU scores against 4 reference
translations from reputable sources (as in MT ?05)
are a feasible evaluation regime. It seems likely
that for automatic evaluation in NLG, a larger num-
ber of reference texts than four are needed.
In our experiments, we have found NIST a more
reliable evaluation metric than BLEU and in par-
ticular ROUGE which did not seem to offer any ad-
vantage over simple string-edit distance. We also
found individual experts? judgments are not likely
to correlate highly with average expert opinion, in
fact less likely than NIST scores. This seems to
imply that if expert evaluation can only be done
with one or two experts, but a high-quality refer-
ence corpus is available, then a NIST-based eval-
uation may produce more accurate results than an
expert-based evaluation.
It seems clear that for automatic corpus-based
evaluation to work well, we need high-quality
reference texts written by many different authors
and large enough to give reasonable coverage of
phenomena such as variation for variation?s sake.
Metrics that do not exclusively reward similarity
with reference texts (such as NIST) are more likely
to correlate well with human judges, but all of the
existing metrics that we looked at still penalised
generators that do not always choose the most fre-
quent variant.
The results we have reported here are for a
relatively simple sublanguage and domain, and
more empirical research needs to be done on how
well different evaluation metrics and methodolo-
gies (including different types of human evalua-
tions) correlate with each other. In order to es-
tablish reliable and trusted automatic cross-system
319
evaluation methodologies, it seems likely that the
NLG community will need to establish how to col-
lect large amounts of high-quality reference texts
and develop new evaluation metrics specifically
for NLG that correlate more reliably with human
judgments of text quality and appropriateness. Ul-
timately, research should also look at developing
new evaluation techniques that correlate reliably
with the real world usefulness of generated texts.
In the shorter term, we recommend that automatic
evaluations of NLG systems be supported by con-
ventional large-scale human-based evaluations.
Acknowledgments
Anja Belz?s part of the research reported in this
paper was supported under UK EPSRC Grant
GR/S24480/01. Many thanks to John Carroll,
Roger Evans and the anonymous reviewers for
very helpful comments.
References
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proc. 1st In-
ternational Conference on Natural Language Gen-
eration, pages 1?8.
A. Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proc. 10th Euro-
pean Workshop on Natural Language Generation
(ENLG?05), pages 15?23.
A. Belz. 2006. pCRU: Probabilistic generation using
representational underspecification. Technical Re-
port ITRI-06-01, ITRI, University of Brighton.
J. Coch. 1996. Evaluating and comparing three
text production techniques. In Proc. 16th Inter-
national Conference on Computational Linguistics
(COLING-1996).
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proc. ARPA Workshop on
Human Language Technology.
M. E. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. In Proceedings
of EACL-2006.
N. Habash. 2004. The use of a structural n-gram lan-
guage model in generation-heavy hybrid machine
translation. In Proc. 3rd International Conference
on Natural Language Generation (INLG ?04), vol-
ume 3123 of LNAI, pages 61?69. Springer.
I. Langkilde. 1998. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ?02).
J. Lester and B. Porter. 1997. Developing and empir-
ically evaluating robust explanation generators: The
KNIGHT experiments. Computational Linguistics,
23(1):65?101.
C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL 2003, pages 71?78.
T. Marciniak and M. Strube. 2004. Classification-
based generation using TAG. In Natural Language
Generation: Proceedings of INLG-2994, pages 100?
109. Springer.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12:349?373.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proc. ACL-2002, pages 311?318.
E. Reiter and S. Sripada. 2002. Should corpora texts be
gold standards for NLG? In Proc. 2nd International
Conference on Natural Language Generation, pages
97?104.
E. Reiter, S. Sripada, and R. Robertson. 2003. Ac-
quiring correct knowledge for natural language gen-
eration. Journal of Artificial Intelligence Research,
18:491?516.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005.
Choosing words in computer-generated weather
forecasts. Artificial Intelligence, 167:137?169.
S. Riezler and J. T. Maxwell III. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proc. ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Sum-
marization, pages 57?64.
K. Sparck Jones and J. R. Galliers. 1996. Evaluating
Natural Language Processing Systems: An Analysis
and Review. Springer Verlag.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2003. Ex-
ploiting a parallel TEXT-DATA corpus. In Proc.
Corpus Linguistics 2003, pages 734?743.
S. Sripada, E. Reiter, and L. Hawizy. 2005. Evalua-
tion of an NLG system used post-edit data: Lessons
learned. In Proc. ENLG-2005, pages 133?139.
A. Stolcke. 2002. SRILM: An extensible language
modeling toolkit. In Proc. 7th International Confer-
ence on Spoken Language Processing (ICSLP ?02),
pages 901?904,.
S. Williams and E. Reiter. 2005. Generating read-
able texts for readers with low basic skills. In Proc.
ENLG-2005, pages 140?147.
M. Young. 1999. Using Grice?s maxim of quantity
to select the content of plan descriptions. Artificial
Intelligence, 115:215?256.
320
Generating Spatio-Temporal Descriptions in Pollen Forecasts
Ross Turner, Somayajulu Sripada and Ehud Reiter
Dept of Computing Science,
University of Aberdeen, UK
{rturner,ssripada,ereiter}@csd.abdn.ac.uk
Ian P Davy
Aerospace and Marine International,
Banchory, Aberdeenshire, UK
idavy@weather3000.com
Abstract
We describe our initial investigations into
generating textual summaries of spatio-
temporal data with the help of a prototype
Natural Language Generation (NLG) system
that produces pollen forecasts for Scotland.
1 Introduction
New monitoring devices such as remote sensing sys-
tems are generating vast amounts of spatio-temporal
data. These devices, coupled with the wider accessi-
bility of the data, have spurred large amounts of re-
search into how it can best be analysed. There has been
less research however, into how the results of the data
analysis can be effectively communicated. As part of
a wider research project aiming to produce textual re-
ports of complex spatio-temporal data, we have devel-
oped a prototype NLG system which produces textual
pollen forecasts for the general public.
Pollen forecast texts describe predicted pollen con-
centration values for different regions of a country.
Their production involves two subtasks; predicting
pollen concentration values for different regions of a
country, and describing these numerical values textu-
ally.In our work, we focus on the later subtask, tex-
tual description of spatio-temporally distributed pollen
concentration values. The subtask of predicting pollen
concentrations is carried out by our industrial collab-
orator, Aerospace and Marine International (UK) Ltd
(AMI).
A fairly substantial amount of work already exists
on weather forecast generation. A number of systems
have been developed and are currently in commercial
use with two of the most notable being FOG (Goldberg
et al, 1994) and MultiMeteo (Coch, 1998).
2 Knowledge Acquisition
Our knowledge acquisition activities consisted of cor-
pus studies and discussions with experts. We have
collected a parallel corpus (69 data-text pairs) of
pollen concentration data and their corresponding hu-
man written pollen reports which our industrial collab-
orator has provided for a local commercial television
station. The forecasts were written by two expert mete-
orologists, one of whom provided insight into how the
forecasts were written. An example of a pollen fore-
cast text is shown in Figure 1, its corresponding data is
shown in table 1. A pollen forecast in the map form is
shown in Figure 2.
?Monday looks set to bring another day of
relatively high pollen counts, with values up
to a very high eight in the Central Belt. Fur-
ther North, levels will be a little better at a
moderate to high five to six. However, even
at these lower levels it will probably be un-
comfortable for Hay fever sufferers.?
Figure 1: Human written pollen forecast text for the
pollen data shown in table 1
Figure 2: Pollen forecast map for the pollen data shown
in table 1
Analysis of a parallel corpus (texts and their under-
lying data) can be performed in two stages:
? In the first stage, traditional corpus analysis pro-
cedure outlined in (Reiter and Dale, 2000) and
(Geldof, 2003) can be used to analyse the pollen
forecast texts (the textual component of the paral-
lel corpus). This stage will identify the different
message types and uncover the sub language of
the pollen forecasts.
? In the second stage the more recent analysis meth-
ods developed in the SumTime project (Reiter et
163
ValidDate AreaID Value
27/06/2005 1 (North) 6
27/06/2005 2 (North West) 5
27/06/2005 3 (Central) 5
27/06/2005 4 (North East) 6
27/06/2005 5 (South West) 8
27/06/2005 6 (South East) 8
Table 1: Pollen Concentration Data for Scotland - Input
data for Figures 1 and 2
al., 2003) which exploit the availability of the un-
derlying pollen data corresponding to the forecast
texts can be used to map messages to input data
and also map parts of the sub language such as
words to the input data. Due to the fact that we
are modeling the task of automatically producing
pollen forecast texts from predicted pollen con-
centration values, knowledge of how to map in-
put data to messages and words/phrases is abso-
lutely necessary. Studies connecting language to
data are useful for understanding the semantics of
language in a more novel way than the traditional
logic-based formalisms (Roy and Reiter, 2005).
We have performed the first stage of the corpus anal-
ysis and part of the second stage so far. In the first
stage, we abstracted out the different message types
from the forecast texts (Reiter and Dale, 2000). These
are shown in Table 2. The main two message types
are forecast messages and trend messages. The for-
mer communicate the actual pollen forecast data (the
communicative goal) and the latter describe patterns in
pollen levels over time as shown in Figure 3
?Grass pollen counts continue to ease from
the recent high levels?
Figure 3: A trend message describing a fall in pollen
levels
Table 2 also shows three other identified message
types. We have ignored both the forecast explanation
and general message types in our system development
because they cannot be generated from pollen data
alone. For example, the explanation type messages ex-
plain the weather conditions responsible for the pollen
predictions. Hayfever messages in our system are rep-
resented as canned text. Examples of a forecast ex-
planation message and hayfever message are shown in
Figure 4 and Figure 5 respectively.
From our corpus analysis we have also been able to
learn the text structure for pollen forecasts. The fore-
casts normally start with a trend message and then in-
clude a number of forecast messages. Where hayfever
messages are present, they normally occur at the end of
the forecast.
Due to the fact that the input to our pollen text gen-
?Windier and wetter weather over last 24
hours has dampened down the grass pollen
count?
Figure 4: An example forecast explanation message
?Even though values are mostly low, those
sensitive to pollen may still be affected?
Figure 5: An example hayfever message
erator is the pollen data in numerical form, as part of
the second stage of the corpus analysis we need to map
the input data to the messages. In earlier ?numbers
to text? NLG systems such as SumTime (Sripada et
al., 2003) and TREND (Boyd, 1998), well known data
analysis techniques such as segmentation and wavelet
analysis were employed for this task. Since pollen data
is spatio-temporal we need to employ spatio-temporal
data analysis techniques to achieve this mapping. We
describe our method in the next section.
Our corpus analysis revealed that forecast texts con-
tain a rich variety of spatial descriptions for a location.
For example, the same region could be referred to by
it?s proper name e.g. ?Suthlerland and Caithness? or
by its? relation to a well known geographical landmark
e.g. ?North of the Great Glen? or simply by its? geo-
graphical location on the map e.g. ?the far North and
Northwest?. In the context of pollen forecasts which
describe spatio-temporal data, studying the semantics
of phrases or words used for describing locations or re-
gions is a challenge. We are currently analysing the
forecast texts along with the underlying data to under-
stand how spatial descriptions map to the underlying
data using the methods applied in the SumTime project
(Sripada et al, 2003).
As part of this analysis, in a seperate study, we asked
twenty four further education students in the Glasgow
area of Scotland a Geography question. The question
asked how many out of four major place names in Scot-
land did they consider to be in the south west of the
country. The answers we got back were very mixed
with a sizeable number of respondents deciding that
the only place we considered definitely not to be in the
south west of Scotland was in fact there.
3 Spatio-temporal Data Analysis
We have followed the pipeline architecture for text gen-
eration outlined in (Reiter and Dale, 2000). The mi-
croplanning and surface realisation modules from the
Sumtime project (Sripada et al, 2003) have largely
been reused. We have developed new data analysis
and document planning modules for the system and de-
scribe the data analysis module in the rest of this sec-
tion. The data analysis module performs segmentation
and trend detection on the data before providing the re-
sults as input to the Natural Language Generation Sys-
164
Message Type Data Dependency Corpus Coverage
Forecast Pollen data for day of forecast 100%
Trend Past/Future pollen forecasts 54%
Forecast Explanation Weather forecast for day of forecast 35%
Hayfever Pollen levels affect hay fever 23%
General General Domain Knowledge 17%
Table 2: Message Categorisation of the Pollen Corpus
tem. An example of the input data to our system is
shown in Table 1. Our data analysis is based on three
steps:-
1. segmentation of the geographic regions by their
non-spatial attributes (pollen values)
2. further segmentation of the segmented geographic
regions by their spatial attributes (geographic
proximity)
3. detection of trends in the generalised pollen level
for the whole region over time
3.1 Segmentation
The task of segmentation consists of two major sub-
tasks, clustering and classification (Miller and Han,
2001). Spatial clustering involves grouping objects into
similar subclasses, whereas spatial classification in-
volves finding a description for those subclasses which
differentiates the clustered objects from each other (Es-
ter et al, 1998).
Pollen values are measured on a scale of 1 to 10(low
to very high). We defined 4 initial categories for seg-
mentation, these are:-
1. VeryHigh - {8,9,10}
2. High - {6,7}
3. Moderate - {4,5}
4. Low - {1,2,3}
These categories proved rather rigid for our pur-
poses. This was due to the fact that human forecasters
take a flexible approach to classifying pollen values.
For example, in the corpus the pollen value of 4 could
be referred to as both a moderate level of pollen and a
low-to-moderate level of pollen. This lead us to define
3 further categories which are derived from our 4 initial
categories:-
5. LowModerate - {3,4}
6. ModerateHigh - {5,6}
7. HighVeryhigh - {7,8}
Thus, the initial segmentation of data carried out by
our system is a two stage process. Firstly regions are
clustered into the initial four categories by pollen value.
The second stage involves merging adjacent categories
that only contain regions with adjacent values. For ex-
ample if we take the input data from Table 1, after the
first stage we have the sets:-
? {{AreaID=2,Value=5},{AreaID=3,Value=5}}
? {{AreaID=1,Value=6},{AreaID=4,Value=6}}
? {{AreaID=5,Value=8},{AreaID=6,Value=8}}
In stage two we create the union of the moderate and
high sets to give:-
? {{AreaID=1,Value=6},{AreaID=2,Value=5},
{AreaID=3,Value=5},{AreaID=4,Value=6}}
? {{AreaID=5,Value=8},{AreaID=6,Value=8}}
Although this initial segmentation could be accom-
plished all in one step, completing it in two steps pro-
vided a more simple software engineering solution.
We can now carry out further segmentation of these
sets according to their spatial attributes. In our set of
regions with ModerateHigh pollen levels we can see
that AreaIDs 1,2,3,4 are in fact all spatial neighbours.
The north, north east and north west regions can be
described spatially as the northern part of the country.
Therefore we can now say that ?Pollen levels are at a
moderate to high 5 or 6 in the northern and central
parts of the country? . Similarly, as the two members of
our set containing regions with VeryHigh pollen levels
are also spatial neighbours we can also say that ?Pollen
levels are at a very high level 8 in the south of the coun-
try?. This process now yields the following two sets:-
? {{AreaID=1234,Value=[5,6]}}
? {{AreaID=56,Value=[8]}}
Our two sets we have now created can now be passed
to the Document Planner were they will be encapsu-
lated as individual Forecast messages.
3.2 Trend Detection
Trend detection in our system works by generalising
over all sets created by segmentation. From our two
sets we can say that generally pollen levels are high
over the whole of Scotland. Looking at the previous
days forecast we can detect a trend by comparing the
two generalisations. If the previous days forecast was
also high we can say ?pollen levels remain at the high
165
levels of yesterday?. By looking further back, and if
those previous days were also high, we can say ?pollen
levels remain at the high levels of recent days?. If the
previous days forecast was low, we can say ?pollen lev-
els have increased from yesterdays low levels?. Our
data analysis module then conveys the information that
there is a relation between the general pollen level
of today and the general pollen level of some recent
timescale to the Document Planner, which then encap-
sulates the information as a Trend message.
After the results of data analysis have been input into
the NLG pipeline the output in Figure 6 is produced.
?Grass pollen levels for Monday remain at
the moderate to high levels of recent days
with values of around 5 to 6 across most parts
of the country. However, in southern areas,
pollen levels will be very high with values of
8.?
Figure 6: The output text from our system for the input
data in Table 1
4 Evaluation
A demo of the pollen forecasting system can be found
on the internet at 1. The evaluation of the system is be-
ing carried out in two stages. The first stage has used
this demo to obtain feedback from expert meteorolo-
gists at AMI. We found the feedback on the system to
be very positive and hope to deploy the system for the
next pollen season. Two main areas identified for im-
provement of the generated texts:-
? Use of a more varied amount of referring expres-
sions for geographic locations.
? An ability to vary the length of the text dependent
on the context it was being used, i.e in a newspa-
per or being read aloud.
These issues will be dealt with subsequent releases
of the software. The second and more thorough evalu-
ation will be carried out when the system is deployed.
5 Further Research
The current work on pollen forecasts is carried out as
part of RoadSafe2 a collaborative research project be-
tween University of Aberdeen and Aerospace and Ma-
rine International (UK) Ltd. The main objective of
the project is to automatically generate road mainte-
nance instructions to ensure efficient and correct ap-
plication of salt and grit to the roads during the win-
ter. The core requirement of this project is to describe
spatio-temporal data of detailed weather and road sur-
face temperature predictions textually. In a previous
1www.csd.abdn.ac.uk/?rturner/cgi bin/pollen.html
2www.csd.abdn.ac.uk/?rturner/RoadSafe/
research project SumTime (Sripada et al, 2003) we
have developed techniques for producing textual sum-
maries of time series data. In RoadSafe we plan to ex-
tend these techniques to generate textual descriptions
of spatio-temporal data. Because the spatio-temporal
weather prediction data used in road maintenance ap-
plications is normally of the order of a megabyte, we
initially studied pollen forecasts which are based on
smaller spatio-temporal data sets. We will apply the
various techniques we have learnt from the study of
pollen forecasts to the spatio-temporal data from the
road maintenance application.
6 Summary
Automatically generating spatio-temporal descriptions
involves two main subtasks. The first subtask focuses
on the spatio-temporal analysis of the input data to
extract information required by the different message
types identified in the corpus analysis. The second sub-
task is to find appropriate linguistic form for the spatial
location or region information.
References
S. Boyd. 1998. Trend: a system for generating in-
telligent descriptions of time-series data. In IEEE
International Conference on Intelligent Processing
Systems (ICIPS1998).
J. Coch. 1998. Multimeteo: multilingual production
of weather forecasts. ELRA Newsletter, 3(2).
M. Ester, A. Frommelt, H. Kriegel, and J. Sander.
1998. Algorithms for characterization and trend de-
tection in spatial databases. In KDD, pages 44?50.
S. Geldof. 2003. Corpus analysis for nlg. cite-
seer.ist.psu.edu/583403.html.
E. Goldberg, N. Driedger, and R. Kittredge. 1994. Us-
ing natural-language processing to produce weather
forecasts. IEEE Expert, 9(2):45?53.
H. J. Miller and J. Han. 2001. Geographic Data Min-
ing and Knowledge Discovery. Taylor and Francis.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
E. Reiter, S. Sripada, and R. Robertson. 2003. Ac-
quiring correct knowledge for natural language gen-
eration. Journal of Artificial Intelligence Research,
18:491?516.
D. Roy and E. Reiter. 2005. Connecting language to
the world. Artificial Intelligence, 167:1?12.
S. Sripada, E. Reiter, and I. Davy. 2003. Sumtime-
mousam: Configurable marine weather forecast gen-
erator. Expert Update, 6:4?10.
166
Squibs and Discussions 
Pipelines and Size Constraints 
Ehud Reiter* 
University of Aberdeen 
Some types of documents need to meet size constraints, uch as fitting into a limited number of 
pages. This can be a difficult constraint to enforce in a pipelined natural anguage generation 
(NLG) system, because size is mostly determined by content decisions, which usually are made 
at the beginning of the pipeline, but size cannot be accurately measured until the document has 
been completely processed by the NLG system. I present experimental data on the performance 
of single-solution pipeline, multiple-solution pipeline, and revision-based variants of the STOP 
system (which produces personalized smoking-cessation leaflets) in meeting a size constraint. 
This shows that a multiple-solution pipeline does much better than a single-solution pipeline, 
and that a revision-based system does best of all. 
1. Introduction 
Some types of documents need to fit on a limited number of pages. For example, this 
article, because it is a squib, must fit on eight pages in the style (font, layout, etc.) 
specified by Computational Linguistics. However, in certain cases it is useful to include 
as much information as possible given the size limit; for example, I want to convey as 
much information as possible about my research in the allowed eight pages. 
Maximizing the amount of content subject o a size limit is also a problem for some 
natural language generation (NLG) systems. For example, the STOP system (Reiter, 
Robertson, and Osman 1999) produces personalized smoking-cessation leaflets that 
must fit on four A5 pages, in a certain style; but it is useful if the leaflets can convey 
as much information as possible given this size constraint. 
One problem with performing this optimization in an NLG system is that the 
size of a document is primarily determined by how much content it contains, that 
is by decisions made during the content determination process. However, an NLG 
system cannot accurately determine the size of a document until the document has 
been completely processed by the NLG system and (in some cases) by an external 
document presentation system, such as LaTeX or Microsoft Word. This is because 
the size of the document is highly dependent on its exact surface form. This is a 
phenomenon that may be familiar to readers who have tried to revise a paper to fit a 
page-limit constraint by making small changes to wording or even orthography. 
In consequence, it may be difficult to satisfy the size constraint while "filling up" 
the allowed pages in a pipelined NLG system that performs content determination i
an early pipeline module, before the surface form of the document is known. This is 
especially true if each pipeline module is restricted to sending a single solution to the 
next pipeline module, instead of multiple possible solutions. 
In this paper I give a brief summary of the pipeline debate and of STOP, present 
my experimental results, and then discuss the implications of this work. 
* Department of Computing Science, Aberdeen AB24 3UE, UK. E-maih ereiter@csd.abdn.ac.uk 
(~) 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 2 
2. Pipelines in NLG 
For the past 20 years, the NLG community has generally agreed that modularizing 
NLG systems is sensible. This has become ven more true in recent years, because of 
a growing trend to incorporate xisting modules (especially realization systems uch 
as FUF/SURGE \[Elhadad and Robin 1997\]) into new systems. While different systems 
use different numbers of modules, all recent systems that I am aware of are divided 
into modules. 
This leads to the question of how modules should interact. In particular, is it 
acceptable to arrange modules in a simple pipeline, where a later module cannot 
affect an earlier module? Or is it necessary to allow revision or feedback, where a later 
module can request hat an earlier module modify its results? If a pipeline is used, 
should modules pass a single solution down the line, or should they pass multiple 
solutions and let subsequent modules choose between these? 
Many authors have argued that pipelines cannot optimally handle certain lin- 
guistic phenomena. For example, Danlos and Namer (1988) point out that in French, 
whether a pronoun unambiguously refers to an entity depends on word ordering. 
This is because the pronouns le or la (which convey gender information) are abbre- 
viated to 1' (which does not contain gender information) when the word following 
the pronoun starts with a vowel. But in a pipelined NLG system, pronominalization 
decisions are typically made earlier than word-ordering decisions; for example in the 
three-stage pipelined architecture presented by Reiter and Dale (2000), pronominal- 
ization decisions are made in the second stage (microplanning), but word ordering 
is chosen during the third stage (realization). This means that the microplanner will 
not be able to make optimal pronominalization decisions in cases where le or la are 
unambiguous, but I' is not, since it does not know word order and hence whether the 
pronoun will be abbreviated. 
Many other such cases are described in Danlos's book (Danlos 1987). The com- 
mon theme behind many of these examples i  that pipelines have difficulties atisfying 
linguistic constraints (such as unambiguous reference) or performing linguistic opti- 
mizations (such as using pronouns instead of longer referring expressions whenever 
possible) in cases where the constraints or optimizations depend on decisions made 
in multiple modules. This is largely due to the fact that pipelined systems cannot per- 
form general search over a decision space that includes decisions made in more than 
one module. 
Despite these arguments, most applied NLG systems use a pipelined architecture; 
indeed, a pipeline was used in every one of the systems urveyed by Reiter (1994) and 
Paiva (1998). This may be because pipelines have many engineering advantages, and 
in practice the sort of problems pointed out by Danlos and other pipeline critics do 
not seem to be a major problem in current applied NLG systems (Mittal et al 1998). 
3. STOP 
The STOP system (Reiter, Robertson, and Osman 1999) generates personalized smoking- 
cessation leaflets, based on responses to a questionnaire about smoking likes and dis- 
likes, previous attempts to quit, and so forth. The output of the system is a four-page 
leaflet; each page is size A5. An example of the two "inside" pages of a leaflet pro- 
duced by STOP is shown in Figure 1. A STOP leaflet alo contains a front page that 
is only partially generated (the rest is logos and fixed text) and a back page that is 
selected from a collection of 16 possible back pages, but is not otherwise personalized; 
these are not shown here due to space restrictions. 
252 
Reiter P ipel ines and Size Constraints 
iiii ill ~i~ii;!;?! 
iii !ii iiii!!ii~ 
~Tx  
:" ir 
:N,. 
N 
N 
W 
r s i l l '  
7 ~ 
i 
i 
imsm' 
iiii!i!iiiiiiii~;i!~ 
~iiiiiiiiiiiiiiiiiiiiiiii!i 
XD 
g 
o.% 
E d~m 
0 0 
E >, 
00.  
~a o 
C~. c o 
o,O o . 
? -ao= ~ 0 0 ~ ~1 
> > O. 0 
>->- 
6_ ~ ~ .~ 
0 
o~- ,-~o o~o 
m ? .9o 0 -~ ~.~m o-  
o_  ~ ~ o~ 
=~ : .'=- _~ oo  O-  - "0 ~0 ~ 0 ~ ~- ~ 0 o o ~ =9,~-~ ~:=- . - -  
c~xE o ' ~ U ~  (~ 
~=~ o ~ ~,~, -~o~-~== ?~_.~_ 
~ m E ~  =o. .~o_~ (0 >, ~oo . -E (~ 
m~,~ .~ ? o o . _  ~:._= ~: -  
~c ~. ~o= ~: ~0>~=o~ ? ~ ~ :o~ ~,=o.~ 
,0~ ,... o~o : :  o~'o  E 
0 0o  ~ o o a  ~ 
_~_  i "~ = "~ ._~ -~ 
~:~ 0 ,', E ~ (~ '~ '~ 
>, 
O o-  
"O C 
~o~ 
O 
J~  
0 0 0 r~ ~er -  
"~o~ 
C ~ 
?d  < 'o  
~ ~ C 0 ? - -~-~ ~ 
)oo_~ 
~gE~ 
:I~ C tO 
? ~ E m.=_ ~ ~0 >,~ 
r~ c n 
N~eg 
6_ 
O 
O 
r ,  
< 
? 6 
.=_ o o ,,, .~= ~ ~ o= = ~ 
~o~ 7 oo  . ..c:: = 
o -  = - ,~-~ - "~ E.~_ -o o o 
~ ~: ~ ~ ~ "-- m'O >-I~ m ~ m ~  '- m-  >" 
~ 0 ? 
li o .C~ ~ x~ m ? c Z. . ,a  o m ~. .~ ~"= ~ 
: o f .~  . . . . . . . . . .  - ~ ,  ~ "~o 
._: ~- o~ ~_~ 
o_. ?- ~ 
~ 0 0 "-- 
~.~o 
C O.~ 
mm =4 
"~ C 0 0 tU .~-- ~ c ~ oo .~o.~ >~ o?~>~-.=-~ -
0 ~ - ~ m ~  
X: _~ ~ ~ oC~?'o 
0 C 
g 
o 
O 
? ? 
"O  O 
$= E 
a a .  =o? o N '~ 
Q 
0. .~ ~ 0 .0 >~ O~ >0 E ~E ~ o~ m 
0-~ ~ CO 0 l_l "~0--. ~ o  ! .~o  
o .~ .~ ~0= .-~ ~0 >" '~  o . 
0 C 0 0 .~ 
=~-~ ~ o~.~- -~  
Q; 
0~ 
0~ 
? 
Q; 
C 
~D 
r~ 
~a ~a 
253 
Computational Linguistics Volume 26, Number 2 
A STOP leaflet must fit on four A5 pages; this is a hard constraint. Furthermore, 
it is important to communicate as much information as possible subject o the size 
constraint; his is a characteristic that the system tries to optimize. However, it is even 
more important that leaflets be easy to read, and size optimization should not be at 
the expense of readability. For example, replacing an itemized list (such as the one at 
the top of the second page in Figure 1) by a complex multiclause sentence can reduce 
size but often makes leaflets harder to read, especially for poor readers; hence we do 
not do this. 
The original version of STOP used a three-stage pipelined architecture (with each 
pipeline module producing only one solution) similar to the one presented by Reiter 
and Dale (2000). An initial document-planning stage produced a document plan data 
structure, which specified the content of the document in terms of messages. In STOP, 
messages were represented as strings (or lists of strings) that specified word forms 
and word order, but not punctuation, capitalization, and intertoken white space. The 
document plan also specified how messages were grouped into higher-level structures 
(such as paragraphs); discourse relations between messages orgroups of messages; and 
the importance of each message and message group. 
Once an initial document plan had been produced, the document trimmer compo- 
nent of the document planner attempted toensure that the document produced by the 
document plan did not exceed four A5 pages. It did this using a heuristic function that 
estimated the size of the final document from the document plan. If the heuristic size 
estimator indicated that the document was too large, the trimmer identified the least 
important message in the document plan, deleted this message, and recalculated the 
document's estimated size. 1 This process continued until the document fitted on four 
A5 pages according to the size estimator. At this point the document plan was passed 
on to the other stages of the system, microplanning and realization. These performed 
tasks such as deciding when discourse relations hould be expressed via cue phrases, 
and adding appropriate punctuation, capitalization, and white space to the text (both 
of which tasks, incidentally, are affected by trimming and hence must take place after 
it). The realizer produced an RTF file, which was printed using Microsoft Word; in a 
sense Word could be considered to be a fourth pipeline stage. 
The main difficulty in this approach was estimating the size of the final document. 
Since messages were represented as strings, we initially thought it would be easy to 
build an accurate size estimator. But in fact this proved to be a difficult ask, because 
the size of a document is highly dependent on its exact surface form, including cue 
phrases, punctuation and capitalisation, and even typographic features uch as bold 
face. 
For example, consider the leaflet extract shown in Figure 1. This fits on two A5 
pages, as desired. However, if "bad for your health" in the paragraph just below the 
graphic were changed from italic face to bold face, then this paragraph would require 
four lines instead of three lines. Our layout style does not allow a section to start on a 
page unless both the section header and two lines of section text can fit on the page. 
Therefore, increasing the size of this paragraph to four lines causes Word to start the 
section headed "You could do it . . ." on the next page; this makes the leaflet overflow 
onto an additional page, and thus violate the overall size constraint. 
Thus, a very small change in a document (such as changing a few words from 
italics to bold) can cause significant changes in a document's size. The fact that a 
1 This is a simplification, as the trimmer also considers dependencies between messages and the 
importance of message groups. Trimming is in essence a type of bin-packing, and no doubt there is 
scope for improving the trimmer by incorporating into it sophisticated bin-packing algorithms. 
254 
Reiter Pipelines and Size Constraints 
document's size is so sensitive to its exact surface form is what makes ize estimation 
difficult. 
As a result of such problems, although the size estimator soon grew in complexity 
considerably beyond what we had originally intended, it still made mistakes. In most 
cases it was fairly accurate, but it was not 100% accurate on 100% of the documents. 
As the estimator grew in complexity, another problem appeared, which was the 
difficulty of keeping it up-to-date. A clinical trial of the STOP system started in Octo- 
ber 1998, and in the months immediately preceding the trial, numerous bug fixes and 
improvements were made to STOP by the development team. Some of these changes 
impacted the size estimator, but developers did not always update the size estimator 
accordingly. In part this was because updating the size estimator in some cases re- 
quired considerably more work than making the actual bug fix or improvement, and 
the developers had many urgent changes that needed to be made to the core software 
in this period. 
In other words, another difficulty with building an estimator that predicted the 
behavior of the microplanner, realizer, and Word was that it was difficult and time- 
consuming to maintain the accuracy of the estimator as changes were made to the 
microplanner and realizer, and also to the exact RTF structures produced by our system 
for Word to process. 
4. Experimental Results 
STOP is currently being tested in a clinical trial, in order to determine its effectiveness 
in helping people stop smoking. The version of STOP used in the clinical trial had a 
single-solution pipeline architecture as described above. Its trimmer used a size esti- 
mator that was tuned to be conservative (and hence often produced leaflets that were 
smaller than they could have been), but still in a few cases underestimated true length 
and hence resulted in leaflets that were five A5 pages instead of four. Such leaflets 
were manually fixed by the researchers unning the trial, usually by adjusting the for- 
matting of the leaflet (for example, margins or interparagraph separation). We felt this 
was not acceptable for a production version of STOP, however; such a system should 
guarantee conformance tothe length constraint without needing manual intervention. 
Also, conformance should be achieved by adjusting content, not formatting. The for- 
matting of STOP leaflets was designed by an expert graphic designer with the goal of 
enhancing readability, and we believed it should be treated as fixed, not variable. 2 
In order to explore what should be done in a production version of STOP, we 
conducted some experiments (after the STOP clinical trial had started) on the impact 
of different architectures on satisfying the size constraint while utilizing as much as 
possible of the available space. For these experiments, we took the version of the 
system used in the clinical trial (including accumulated bug fixes and enhancements), 
and retuned the size estimator to take into account these accumulated changes. After 
retuning, STOP produced leaflets that fit the size constraint for all members of a 
"tuning set" of 150 questionnaires. Then we made the following changes: 
A delta parameter was added to the size estimator; essentially, a delta of 
N makes the estimator think that a page can hold N more lines of text 
than it can in reality contain. 
2 Similarly, I believe the editors of Computational Linguistics would not be pleased if I submitted a squib 
that conformed tothe eight-page size limit by using nonstandard margins or line spacing. 
255 
Computational Linguistics Volume 26, Number 2 
A multiple-solution mode was added to the system. In this mode, the 
trimmer is run several times, at different delta values. The resultant 
document plans are processed by the rest of the system and by Word, 
and a choice module picks the resulting document that has the highest 
word count while still satisfying the size constraint. 
A revision mode was added to the system. In this mode, the system 
generates an initial document using a fixed delta. Then, a revision 
module obtains the actual size of the document from Word, and either 
deletes an additional message (if the document is too large) or restores 
the last deleted message (if the document meets the size constraint). This 
process continues until the system finds the largest document that meets 
the size constraint. 3 
The modified system was run on a set of 1,000 questionnaires from the clinical 
trial, in the original single-solution pipeline mode, in the multiple-solution pipeline 
mode, and in revision mode. For the pipeline modes, the system was run with the 
deltas -2, -1, 0, 1, 2, 3, 4, 5, and 6. Measurements were made of: 
? The percentage of leaflets that exceeded the size constraint. 
? For leaflets atisfying the size constraint, he average number of words in 
the two inside pages, both as an absolute number and as a percentage of
the number of words in the inside pages when processed under revision 
mode. 
? The average processing time (total elapsed time, not just computation 
time) required per document, on a Pentium 266MHz with 128MB of 
memory. 4 
These results are shown in Tables 1 and 2. For multiple-solution pipelines, we tried all 
pairs, triples, and quadruples of deltas between -2 and 6, and in Table 2 only show 
the results for the pair, triple, and quadruple that led to the highest average word 
count while always satisfying the size constraint. We also ran STOP on the full set 
of 2,582 clinical-trial questionnaires in single-delta mode with deltas of -1, 0, and 1, 
in order to get a more accurate stimate of the number of constraint violations under 
these deltas. 
5. D iscuss ion  of Results 
As expected, the single-delta figures how that as the delta increases, both the average 
word count and the number of leaflets that exceed the size constraint also increase. 
Note that although none of the leaflets produced from the 150-questionnaire "tuning 
3 Our revision module  did not give any guidance as to where messages should be added. This 
sometimes led to wasted space in situations where a message could be added to one part of the leaflet 
but  not others (for example, to the first inside page but  not the second), if the next message in the 
undelete list was in a portion of the leaflet that had no unused space. 
4 This measurement  was made on a subset of 100 documents,  because this is the size of collection that 
STOP was designed to be able to process in one run. While the core NLG system could process any 
number  of documents,  the support  code (user-interface, logging, file management)  worked poorly 
when processing more than 100-200 documents  in one run. For word count and constraint violation 
data, we s imply restarted the system if it hung  when processing 1,000 questionnaires; but  this seemed 
less appropriate for execution time data. 
256 
Reiter Pipelines and Size Constraints 
Table 1 
Results of single-solution pipeline mode 
delta -2  - 1 0 1 2 3 4 5 6 
size constraint violations (%) 0 0 0.04 0.97 7.3 16 25 35 42 
Average word count (legal eaflets) 303 320 336 350 359 364 373 375 378 
Word count as % of revision mode 79 84 88 92 93 96 98 98 99 
Table 2 
Performance of different modes when meeting the size constraint in 100% of cases 
Average size Average 
architecture Average word count (% of revision) processing time 
1 solution (delta = -1) 320 84 2.2s 
2 solutions (deltas = -1, 4) 369 96 5.2s 
3 solutions (deltas = -1, 2, 6) 378 98 5.9s 
4 solutions (deltas = -1, 2, 4, 6) 380 99 6.2s 
revision 385 100 9.8s 
set" violated the size constraint with a delta of 0, one leaflet produced from the full 
2,582-questionnaire data set did break the size constraint at this delta. This is perhaps 
not surprising, it merely shows that as the size of the document set increases, so does 
the worst-case performance of the heuristic size estimator. It is possible that in a very 
large data set (hundreds of thousands of questionnaires), some leaflets might break 
the size constraint even at a delta of -1.  
Shifting to a multiple-solution pipeline dramatically improves performance. Av- 
erage leaflet size while guaranteeing conformance to the size constraint jumps from 
320 words in single-delta mode to 369 with two solutions; an increase of 15% in the 
number of words in the leaflet. We get still better results with three and four solu- 
tions, although the increase is not as dramatic. The best results of all are in revision 
mode, although the increase in size over a four-solution pipeline (385 words versus 
380 words) is small. However, revision mode also is robust in the face of increased ata 
set size (we can be confident hat the size constraint will be satisfied even on a set of 
a million questionnaires) and "last-minute" changes to the code. If developers tweak 
the main STOP code and forget to update the size estimator, revision mode will still 
always produce documents that conform to the size constraint; it just may take longer 
to do the revision. In contrast, changes to the code may result in the multiple-solution 
pipeline producing documents that do not conform to the size constraint. 
As expected, processing time is lowest for the single-solution pipeline and highest 
for revision mode. However, in the context of STOP, even the 9.8 seconds required 
in revision mode is acceptable; under this mode a batch of 100 leaflets can still be 
generated in under 20 minutes. 
6. Implications 
In STOP, the single-solution pipeline does a poor job at meeting the size constraint 
while utilizing as much of the available space as possible. No doubt the performance 
of the single-solution pipeline could be enhanced by adding more complexity to the 
257 
Computational Linguistics Volume 26, Number 2 
size estimator; but such a system still would not give 100% accurate stimates on 
100% of the generated ocuments. Furthermore additional complexity would make 
the estimator harder to maintain as changes were made to the code being estimated. 
Both the multiple-solution pipeline and revision mode do a much better job of 
utilizing the available space while observing the size constraint. Revision mode does 
better than the multiple-solution pipeline, but only slightly. However, revision mode 
is robust in the face of increased ata set size and changes to the code. 
The effectiveness of multiple-solution pipelines hould perhaps not be surprising, 
given the popularity of such pipelines in other areas of speech and language pro- 
cessing. For example, in a speech system a word-level analysis component may pass 
several word hypotheses to a language model; and in a natural language analysis 
system, a morphology system may pass several possible analyses of a surface form 
word to a parser. However, multiple-solution pipelines have not received a great deal 
of attention in the NLG community. I am not aware of any previous NLG papers that 
presented experimental data comparing single-solution to multiple-solution pipelines, 
and many NLG pipeline critics (including Danlos) assume that pipeline modules only 
produce one solution. 
Do these results generalize to other constraints and optimizations? In principle, it 
seems that similar findings should apply to other constraints and optimizations that 
depend on decisions or measurements made in more than one module. However, abig 
caveat is that many of the constraints and optimizations important o NLG systems 
are difficult to measure, which may lessen the benefits of complex architectures. For 
example, an important constraint in STOP is that texts should be easy to read for poor 
readers. However, the only computational mechanism we are aware of for measur- 
ing reading difficulty is reading-level formulas (such as Flesch Reading Ease), whose 
accuracy is doubtful (Kintsch and Vipond 1979). Without reliable global measures of 
readability, perhaps the best we can do (and the approach adopted in STOP) is to 
design messages that readability experts think are appropriate for poor readers; this 
is something that can be done in a single-solution pipeline architecture. 
In other words, if we cannot properly measure the thing we are trying to optimize 
or satisfy (which may be the case with the majority of constraints and optimizations 
that today's NLG systems builders are concerned with), then there may be little value 
in shifting to a complex architecture that supports more sophisticated search (which 
is perhaps the main benefit of revision and multiple-solution pipelines). This may 
explain the continuing popularity of single-solution pipeline architectures in applied 
NLG systems. 
Acknowledgments 
Many thanks to the STOP team and 
especially Roma Robertson, who kept on 
producing examples of STOP leaflets which 
the size estimator had difficulties with. My 
thanks also to Michael Elhadad, Chris 
Mellish, Vibhu Mittal, Daniel Paiva, and the 
anonymous reviewers for their very helpful 
comments; and a special thanks to Stephan 
Busemann for suggesting we investigate 
multiple-solution pipelines. This research 
was supported by the Scottish Office 
Department ofHealth under grant 
K/OPR/2/2/D318, and the Engineering 
and Physical Sciences Research Council 
under grant GR/L48812. 
References 
Danlos, Laurence. 1987. The Linguistic Basis 
of Text Generation. Cambridge University 
Press, Cambridge, UK. 
Danlos, Laurence and Fiammetta Namer. 
1988. Morphology and cross 
dependencies in the synthesis of personal 
pronouns in Romance languages. In 
Proceedings ofthe 12th International 
Conference on Computational Linguistics 
(COLING-88), volume 1, pages 139-141. 
Elhadad, Michael and Jacques Robin. 1997. 
SURGE: A comprehensive plug-in 
syntactic realisation component for text 
generation. Technical Report, Computer 
Science Dept, Ben-Gurion University, Beer 
258 
Reiter Pipelines and Size Constraints 
Sheva, Israel. 
Kintsch, Walter and Douglas Vipond. 1979. 
Reading comprehension a d readability 
in educational practice and psychological 
theory. In Lars-GOran Nilsson, editor, 
Perspectives on Memory Research. Lawrence 
Erlbaum, pages 329-365. 
Mittal, Vibhu, Johanna Moore, Guiseppe 
Carenini, and Steven Roth. 1998. 
Describing complex charts in natural 
language: A caption generation system. 
Computational Linguistics, 24:431-467. 
Paiva, Daniel. 1998. A survey of applied 
natural anguage generation systems. 
Technical Report ITRI-98-03, Information 
Technology Research Institute, University 
of Brighton, UK. 
Reiter, Ehud. 1994. Has a consensus NL 
generation architecture appeared, and is it 
psycholinguistically plausible? In 
Proceedings ofthe Seventh International 
Workshop on Natural Language Generation 
(INLGW-1994), pages 163-170. 
Reiter, Ehud and Robert Dale. 2000. Building 
Natural Language Generation Systems. 
Cambridge University Press. In press. 
Reiter, Ehud, Roma Robertson, and Liesl 
Osman. 1999. Types of knowledge 
required to personalise smoking cessation 
letters. In Werner Horn et al, editors, 
Artificial Intelligence and Medicine: 
Proceedings ofAIMDM-1999, 
pages 389-399. Springer-Verlag. 
259 

c? 2002 Association for Computational Linguistics
Squibs and Discussions
Human Variation and Lexical Choice
Ehud Reiter? Somayajulu Sripada?
University of Aberdeen University of Aberdeen
Much natural language processing research implicitly assumes that word meanings are fixed in
a language community, but in fact there is good evidence that different people probably associate
slightly different meanings with words. We summarize some evidence for this claim from the
literature and from an ongoing research project, and discuss its implications for natural language
generation, especially for lexical choice, that is, choosing appropriate words for a generated text.
1. Introduction
A major task in natural language generation (NLG) is lexical choice, that is, choosing
lexemes (words) to communicate to the reader the information selected by the system?s
content determination module. From a semantic perspective lexical choice algorithms
are based on models of word meanings, which state when a word can and cannot be
used; of course, lexical choice algorithms may also consider syntactic constraints and
pragmatic features when choosing words.
Such models assume that it is possible to specify what a particular word means to
a particular user. However, both the cognitive science literature and recent experiments
carried out in the SUMTIME project at the University of Aberdeen, of which the current
authors are a part, suggest that this may be difficult to do because of variations among
people, that is, because the same word may mean different things to different people.
More precisely, although people may agree at a rough level about what a word means,
they may disagree about its precise definition, and in particular, to what objects or
events a word can be applied. This means that it may be impossible even in principle
to specify precise word meanings for texts with multiple readers, and indeed for texts
with a single reader, unless the system has access to an extremely detailed user model.
A corpus study in our project also showed that there were differences in which words
individuals used (in the sense that some words were used only by a subset of the
authors) and also in how words were orthographically realized (spelled).
This suggests that it may be risky for NLG systems (and indeed human authors) to
depend for communicative success on the human reader?s interpreting words exactly
as the system intends. This in turn suggests that perhaps NLG systems should be
cautious in using very detailed lexical models and also that it may be useful to add
some redundancy to texts in case the reader does not interpret a word as expected.
This is especially true in applications in which each user reads only one generated text;
if users read many generated texts, then perhaps over time they will learn about and
adapt to the NLG system?s lexical usage. Human variability also needs to be taken
into account by natural language processing (NLP) researchers performing corpora
analyses; such analyses should not assume that everyone uses identical rules when
making linguistic decisions.
? Department of Computing Science, University of Aberdeen, Aberdeen AB24 3UE, UK. E-mail:
ereiter@csd.abdn.ac.uk
? Department of Computing Science, University of Aberdeen, Aberdeen AB24 3UE, UK. E-mail:
ssripada@csd.abdn.ac.uk
546
Computational Linguistics Volume 28, Number 4
2. Evidence for Human Lexical Variation
2.1 Previous Research
Linguists have acknowledged that people may associate different meanings with the
same word. Nunberg (1978, page 81), for example, writes:
There is considerable variation among speakers in beliefs about what
does and does not constitute a member of the category. . . . Take jazz. I
may believe that the category includes ragtime, but not blues; you may
believe the exact opposite. After all, we will have been exposed to a
very different set of exemplars. And absent a commonly accepted au-
thority, we must construct our own theories of categories, most prob-
ably in the light of varying degrees of musical sophistication.
Many modern theories of mental categorization (Rosch 1978; Smith and Medin 1981)
assume that mental categories are represented by prototypes or exemplars. Therefore,
if different people are exposed to different category prototypes and exemplars, they
are likely to have different rules for evaluating category membership.
Parikh (1994) made a similar point and backed it up with some simple experimen-
tation. For example, he showed squares from the Munsell chart to participants and
asked them to characterize the squares as red or blue; different individuals character-
ized the squares in different ways. In another experiment he showed that differences
remained even if participants were allowed to associate fuzzy-logic-type truth values
with statements.
In the psychological community, Malt et al (1999) investigated what names par-
ticipants gave to real-world objects. For example, these researchers wished to know
whether participants would describe a pump-top hand lotion dispenser as a bottle or a
container. They were primarily interested in variations across linguistic communities,
but they also discovered that even within a linguistic community there were differ-
ences in how participants named objects. They state (page 242) that only 2 of the 60
objects in their study were given the same name by all of their 76 native-English-
speaker participants.
In the lexicographic community, field-workers for the Dictionary of American Re-
gional English (DARE) (Cassidy and Hall 1996) asked a representative set of Americans
to respond to fill-in-the-blank questionnaires. The responses they received revealed
substantial differences among participants. For example, there were 228 different re-
sponses to question B12, When the wind begins to increase, you say it?s , the most
common of which were getting windy and blowing up; and 201 different responses to
question B13, When the wind begins to decrease, you say it?s , the most common of
which were calming down and dying down.
2.2 SUMTIME Project
The SUMTIME project at the University of Aberdeen is researching techniques for gen-
erating summaries of time-series data.1 Much of the project focuses on content deter-
mination (see, for example, Sripada et al [2001]), but it is also examining lexical choice
algorithms for time-series summaries, which is where the work described in this ar-
ticle originated. To date, SUMTIME has primarily focused on two domains, weather
forecasts and summaries of gas turbine sensors, although we have recently started
1 See ?http://www.csd.abdn.ac.uk/research/sumtime? for general information about SUMTIME.
547
Reiter and Sripada Human Variation and Lexical Choice
work in a third domain as well, summaries of sensor readings in neonatal intensive
care units.
2.2.1 Gas Turbine Domain. In order to develop a lexicon for describing patterns in
gas turbine sensor data, we asked two experts to write short descriptions of 38 signal
fragments. (A signal fragment is an interval of a single time-series data channel.) The
descriptors were small, with an average size of 8.3 words. In no case did the experts
produce exactly the same descriptor for a fragment. Many of the differences simply
reflected usage of different words to express the same underlying concept; for example,
one expert typically used rise to describe what the other expert called increase. In other
cases the differences reflected different levels of detail. For example, one fragment was
described by expert A as Generally steady with a slow rising trend. Lots of noise and a few
small noise steps, whereas expert B used the shorter phrase Rising trend, with noise. Both
experts also had personal vocabulary; for example, the terms bathtub and dome were
used only by expert A, whereas the terms square wave and transient were used only by
expert B.
Most importantly from the perspective of this article, there were cases in which the
differences between the experts reflected a difference in the meanings associated with
words. For example, both experts used the word oscillation. Six signals were described
by both experts as oscillations, but two signals, including the one shown in Figure 1,
were described as oscillations only by expert B. We do not have enough examples
to solidly support hypotheses about why the experts agreed on application of the
term oscillation to some signals and disagreed on its application to others, but one
explanation that seems to fit the available data is that the experts agreed on applying
the term to signals that were very similar to a sine wave (which presumably is the
prototype [Rosch 1978] of an oscillation), but sometimes disagreed on its application
to signals that were less similar to a sine wave, such as Figure 1.
2.2.2 Meteorology Domain. In the meteorology domain we accumulated and ana-
lyzed a corpus of 1,099 human-written weather forecasts for offshore oil rigs, together
with the data files (produced by a numerical weather simulation) that the forecasters
examined when writing the forecasts. The forecasts were written by five different fore-
casters. A short extract from a typical forecast is shown in Figure 2; this text describes
changes in wind speed and direction predicted to occur two days after the forecast
was issued. An extract from the corresponding data file is shown in Table 1; it de-
scribes the predicted wind speed and direction from the numerical weather simulation
at three-hourly intervals.
As in the gas turbine domain, our corpus analysis showed that individual fore-
casters had idiosyncratic vocabulary that only they used. For example, one forecaster
used the verb freshening to indicate a moderate increase in wind speed from a low
or moderate initial value, but no other forecaster used this verb. There were also
Figure 1
Signal fragment (gas turbine exhaust temperature): Is this an oscillation?
548
Computational Linguistics Volume 28, Number 4
FORECAST 00-24 GMT, WEDNESDAY, 04-Oct 2000
WIND(10M): WSW 20-24 BACKING SSW 10-14 BY MIDDAY THEN
VEERING SW 24-28 BY EVENING
Figure 2
Wind (at 10m) extract from five-day weather forecast issued on October 2, 2000.
differences in orthography. For example, some forecasters lexicalized the four basic
directions as N, E, S, and W, whereas others used the lexicalizations N?LY, E?LY, S?LY,
and W?LY.
We performed a number of semantic analyses to determine when different fore-
casters used different words; these invariably showed differences between authors.
For example, we attempted to infer the meaning of time phrases such as by evening by
searching for the first data file record that matched the corresponding wind descriptor.
The forecast in Figure 2, for example, says that the wind will change to SSW 10?14
at the time suggested by BY MIDDAY. In the corresponding data shown in Table 1,
the first entry with a direction of SSW and a speed in the 10?14 range is 1200; hence
in this example the time phrase by midday is associated with the time 1200. A similar
analysis suggests that in this example the time phrase by evening is associated with the
time 0000 (on 5-10-00).
We repeated this procedure for every forecast in our corpus and statistically an-
alyzed the results to determine how individual forecasters used time phrases. More
details about the analysis procedure are given by Reiter and Sripada (2002). As re-
ported in that paper, the forecasters seemed to agree on the meaning of some time
phrases; for example, all forecasters predominantly used by midday to mean 1200.
They disagreed, however, on the use of other terms, including by evening. The use of
by evening is shown in Table 2; in particular, whereas forecaster F3 (the author of the
text in Figure 2) most often used this phrase to mean 0000, forecasters F1 and F4 most
often used this phrase to mean 1800. The differences between forecasters in their usage
of by evening are significant at p < .001 under both a chi-squared test (which treats
time as a categorical variable) and a one-way analysis of variance (which compares
the mean time for each forecaster; for this test we recoded the hour 0 as 24).
2.2.3 Knows Java Experiment. Some colleagues pointed out to us that meteorology
in particular was a domain with an established sublanguage and usage conventions,
whose words might correspond to technical terms, and wondered what would hap-
Table 1
Wind (at 10m) extract from October 2, 2000, data file (output of numerical weather model).
Day Hour Wind Direction Wind Speed
4-10-00 0 WSW 22
4-10-00 3 WSW 20
4-10-00 6 SW 16
4-10-00 9 SW 14
4-10-00 12 SSW 12
4-10-00 15 SSW 18
4-10-00 18 SSW 22
4-10-00 21 SSW 24
5-10-00 0 SW 26
549
Reiter and Sripada Human Variation and Lexical Choice
Table 2
How often by evening was used to refer to each time, for each forecaster (mode in boldface
font).
Hour F1 F2 F3 F4 F5 Total
0 5 35 1 3 44
3 1 1
6 1 1
9 0
12 1 1
15 5 2 3 10
18 19 3 1 22 4 49
21 7 5 22 3 6 43
Total 31 14 61 29 14 149
pen in a domain in which there was no established sublanguage and technical termi-
nology. We therefore performed a small experiment at the University of Aberdeen
in which we asked 21 postgraduate students and academic staff members to fill
out a questionnaire asking which of the following individuals they would regard as
knowing Java:2
? A cannot program in Java, but knows that Java is a popular
programming language.
? B cannot write a Java program from scratch, but can make very simple
changes to an existing Java program (such as changing a string constant
that specifies a URL).
? C can use a tool such as JBuilder to write a very simple Java program,
but cannot use control flow constructs such as while loops.
? D can write Java programs that use while loops, arrays, and the Java
class libraries, but only within one class; she cannot write a program that
consists of several classes.
? E can create complex Java programs and classes, but needs to
occasionally refer to documentation for details of the Java language and
class libraries.
Respondents could tick Yes, Unsure, or No. All 21 respondents ticked No for A and Yes
for E. They disagreed about whether B, C, and D could be considered to know Java; 3
ticked Yes for B, 5 ticked Yes for C, and 13 ticked Yes for D. In other words, even among
this relatively homogeneous group, there was considerable disagreement over what
the phrase knows Java meant in terms of actual knowledge of the Java programming
language.
2 The questionnaire in fact contained a sixth item: ?F can create complex Java libraries and almost never
needs to refer to documentation because she has memorised most of it.? However, a few participants
were unsure whether create complex Java libraries meant programming or meant assembling compiled
object files into a single archive file using a tool such as tar or jartool, so we dropped F from our study.
550
Computational Linguistics Volume 28, Number 4
3. Implications for Natural Language Generation
3.1 Lexical Choice
The previous section has argued that people in many cases do associate different
meanings with lexemes and phrases such as oscillation, by evening, and knows; and that
some words, such as bathtub and freshening, are used only by a subset of authors in a
particular domain. What impact does this have on lexical choice?
In applications in which users read only one generated text, it may be necessary
to restrict lexeme definitions to those that we expect all users to share. Indeed, essen-
tially this advice was given to us by a domain expert in an earlier project on generating
personalized smoking-cessation letters (Reiter, Robertson, and Osman 2000). In appli-
cations in which users read many generated texts over a period of time, however, an
argument could be made for using a richer vocabulary and set of lexeme definitions
and expecting users to adapt to and learn the system?s vocabulary and usage over the
course of time; it may be appropriate to add ?redundant? information to texts (sec-
tion 3.2) while the user is still learning the system?s lexical usage. This strategy has
some risks, but if successful, can lead to short and less awkward texts. Consistency is
essential if this strategy is followed; the system should not, for example, sometimes
use by evening to mean 1800 and sometimes use by evening to mean 0000.
We are not aware of previous research in lexical choice that focuses on dealing
with differences in the meanings that different human readers associate with words.
Perhaps the closest research strand is that which investigates tailoring word choice and
phrasing according to the expertise of the user (Bateman and Paris 1989; Reiter 1991;
McKeown, Robin, and Tanenblatt 1993). For example, the Coordinated Multimedia
Explanation Testbed (COMET) (McKeown, Robin, and Tanenblatt 1993) could generate
Check the polarity for skilled users and Make sure the plus on the battery lines up with the
plus on the battery compartment for unskilled users. General reviews of previous lexical
choice research in NLG are given by Stede (1995) and Wanner (1996); Zukerman and
Litman (2001) review research in user modeling and NLP.
In the linguistic community, Parikh (1994) has suggested that utility theory be
applied to word choice. In other words, if we know (1) the probability of a word?s
being correctly interpreted or misinterpreted and (2) the benefit to the user of correct
interpretation and the cost of misinterpretation, then we can compute an overall utility
to the user of using the word. This seems like an interesting theoretical model, but in
practice (at least in the applications we have looked at), whereas it may be just about
possible to get data on the likelihood of correct interpretation of a word, it is probably
impossible to calculate the cost of misinterpretation, because we do not have accurate
task models that specify exactly how the user will use the generated texts (and our
domain experts have told us that it is probably impossible to construct such models).
3.1.1 Near Synonyms. A related problem is choosing between near synonyms, that
is, words with similar meanings: for example, choosing between easing and decreasing
when describing changes in wind speed, or saw-tooth transient and shark-tooth transient
when describing gas turbine signals. The most in-depth examination of choosing be-
tween near synonyms was undertaken by Edmonds (1999), who essentially suggested
using rules based on lexicographic work such as Webster?s New Dictionary of Synonyms
(Gove 1984).
Edmonds was working on machine translation, not generating texts from nonlin-
guistic data, and hence was looking at larger differences than the ones with which
we are concerned. Indeed, dictionaries do not in general give definitions at the level
of detail required by SUMTIME; for example, we are not aware of any dictionary that
551
Reiter and Sripada Human Variation and Lexical Choice
defines oscillation in enough detail to specify whether it is appropriate to describe the
signal in Figure 1 as an oscillation. We also have some doubts, however, as to whether
the definitions given in synonym dictionaries such as Gove (1984) do indeed accu-
rately represent how all members of a language community use near synonyms. For
example, when describing synonyms and near synonyms of error, Gove (page 298)
states that faux pas is ?most frequently applied to a mistake in etiquette.? This seems
to be a fair match to DARE?s fieldwork question (see section 2.1) JJ41, An embarrassing
mistake: Last night she made an awful , and indeed DARE (volume 2, page 372)
states that faux pas was a frequent response to this question. DARE adds, however,
that faux pas was less often used in the South Midland region of the United States (the
states of Kentucky and Tennessee and some adjacent regions of neighboring states)
and also was less often used by people who lacked a college education. So although
faux pas might be a good lexicalization of a ?mistake in etiquette? for most Americans,
it might not be appropriate for all Americans; and, for example, if an NLG system
knew that its user was a non-college-educated man from Kentucky, then perhaps it
should consider using another word for this concept.
3.2 Redundancy
Another implication of human variation in word usage is that if there is a chance that
people may not interpret words as expected, it may be useful for an NLG system to
include extra information in the texts it generates, beyond what is needed if users could
be expected to interpret words exactly as the system intended. Indeed, unexpected
word interpretations could perhaps be considered to be a type of ?semantic? noise;
and as with all noise, redundancy in the signal (text) can help the reader recover the
intended meaning.
For example, the referring-expression generation model of Dale and Reiter (1995)
selects attributes to identify referents based on the assumption that the hearer will
interpret the attributes as the system expects. Assume, for instance, that there are
two books in focus, B1 and B2, and the system?s knowledge base records that B1 has
color red and B2 has color blue. Then, according to Dale and Reiter, the red book is a
distinguishing description that uniquely identifies B1.
Parikh, however, has shown that people can in fact disagree about which objects
are red and which are blue; if this is the case, then it is possible that the hearer will not
in fact be able to identify B1 as the referent after hearing the red book. To guard against
this eventuality, it might be useful to add additional information about a different
attribute to the referring expression; for example, if B1 is 100 pages and B2 is 1,000
pages, the system could generate the thin red book. This is longer than the red book and
thus perhaps may take longer to utter and comprehend, but its redundancy provides
protection against unexpected lexical interpretations.
3.3 Corpus Analysis
A final point is that differences between individuals should perhaps be considered
in general by people performing corpus analyses to derive rules for NLG systems
(Reiter and Sripada 2002). To take one randomly chosen example, Hardt and Rambow
(2001) suggest a set of rules for deciding on verb phrase (VP) ellipsis that are based
on machine learning techniques applied to the Penn Treebank corpus. These achieve
a 35% reduction in error rate against a baseline rule. They do not consider variation
in author, and we wonder if a considerable amount of the remaining error is due to
individual variation in deciding when to ellide a VP. It would be interesting to perform
a similar analysis with author specified as one of the features given to the machine
learning algorithm and see whether this improved performance.
552
Computational Linguistics Volume 28, Number 4
4. Natural Language Generation versus Human-Written Texts
To finish on a more positive note, human variation may potentially be an opportunity
for NLG systems, because they can guarantee consistency. In the weather-forecasting
domain we examined, for example, users receive texts written by all five forecasters,
which means that they may have problems reliably interpreting phrases such as by
evening; an NLG system, in contrast, could be programmed always to use this phrase
consistently. An NLG system could also be programmed to avoid idiosyncratic terms
with which users might not be familiar (bathtub, for example) and not to use terms in
cases in which people disagree about their applicability (e.g., oscillation for Figure 1).
Our corpus analyses and discussions with domain experts suggest that it is not always
easy for human writers to follow such consistency rules, especially if they have limited
amounts of time.
Psychologists believe that interaction is a key aspect of the process of humans
agreeing on word usage (Garrod and Anderson 1987). Perhaps a small group of people
who constantly communicate with each other over a long time period (presumably the
circumstances under which language evolved) will agree on word meanings. But in the
modern world it is common for human writers to write documents for people whom
they have never met or with whom they have never otherwise interacted, which may
reduce the effectiveness of the natural interaction mechanism for agreeing on word
meanings.
In summary, dealing with lexical variation among human readers is a challenge
for NLG systems and will undoubtably require a considerable amount of thought,
research, and data collection. But if NLG systems can do a good job of this, they
might end up producing superior texts to many human writers, which would greatly
enhance the appeal of NLG technology.
Acknowledgments
Our thanks to the many individuals who
have discussed this work with us (not all of
whom agree with our analysis!), including
Regina Barzilay, Ann Copestake, Robert
Dale, Phil Edmonds, Jim Hunter, Adam
Kilgarriff, Owen Rambow, Graeme Ritchie,
Rosemary Stevenson, Sandra Williams, and
Jin Yu. We are also grateful to the
anonymous reviewers for their helpful
comments. Special thanks to DARE editor
Joan Hall for providing us with the DARE
fieldwork data. Last but certainly not least,
this work would not have been possible
without the help of our industrial
collaborators at Intelligent Applications and
WNI/Oceanroutes. This work was
supported by the UK Engineering and
Physical Sciences Research Council
(EPSRC), under grant GR/M76681.
References
Bateman, John and Cecile Paris. 1989.
Phrasing a text in terms the user can
understand. In Proceedings of the 11th
International Joint Conference on Artificial
Intelligence (IJCAI-89), volume 2,
pages 1511?1517.
Cassidy, Frederick and Joan Hall, editors.
1996. Dictionary of American Regional
English. Belknap.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
19:233?263.
Edmonds, Philip. 1999. Semantic
Representations of Near-Synonyms for
Automatic Lexical Choice. Ph.D. thesis,
Computer Science Department,
University of Toronto, Toronto.
Garrod, Simon and Anthony Anderson.
1987. Saying what you mean in dialogue:
A study in conceptual and semantic
co-ordination. Cognition, 27:181?218.
Gove, Philip, editor. 1984. Webster?s New
Dictionary of Synonyms. Merriam-Webster.
Hardt, Daniel and Owen Rambow. 2001.
Generation of VP-ellipsis: A corpus-based
approach. In Proceedings of the 39th Meeting
of the Association for Computation Linguistics
(ACL-01), pages 282?289.
Malt, Barbara, Steven Sloman, Silvia
Gennari, Meiyi Shi, and Yuan Wang. 1999.
Knowing versus naming: Similarity and
553
Reiter and Sripada Human Variation and Lexical Choice
the linguistic categorization of artifacts.
Journal of Memory and Language, 40:230?262.
McKeown, Kathleen, Jacques Robin, and
Michael Tanenblatt. 1993. Tailoring lexical
choice to the user?s vocabulary in
multimedia explanation generation. In
Proceedings of 31st Annual Meeting of the
Association for Computational Linguistics
(ACL93), pages 226?234.
Nunberg, Geoffrey. 1978. The Pragmatics of
Reference. University of Indiana
Linguistics Club, Bloomington.
Parikh, Rohit. 1994. Vagueness and utility:
The semantics of common nouns.
Linguistics and Philosophy, 17:521?535.
Reiter, Ehud. 1991. A new model of lexical
choice for nouns. Computational
Intelligence, 7(4):240?251.
Reiter, Ehud, Roma Robertson, and Liesl
Osman. 2000. Knowledge acquisition for
natural language generation. In
Proceedings of the First International
Conference on Natural Language Generation,
pages 217?215.
Reiter, Ehud and Somayajulu Sripada. 2002.
Should corpora texts be gold standards
for NLG? In Proceedings of the Second
International Conference on Natural Language
Generation, pages 97?104.
Rosch, Eleanor. 1978. Principles of
categorization. In E. Rosch and B. Lloyd,
editors, Cognition and Categorization.
Lawrence Erlbaum, Hillsdale, NJ,
pages 27?48.
Smith, Edward and Douglas Medin. 1981.
Categories and Concepts. Harvard
University Press, Cambridge.
Sripada, Somayajulu, Ehud Reiter, Jim
Hunter, and Jin Yu. 2001. A two-stage
model for content determination. In
Proceedings of ENLGW-2001, pages 3?10.
Stede, Manfred. 1995. Lexicalization in
natural language generation: A survey.
Artificial Intelligence Review, 8:309?336.
Wanner, Leo. 1996. Lexical choice in text
generation and machine translation.
Machine Translation, 11:3?35.
Zukerman, Ingrid and Diane Litman. 2001.
Natural language processing and user
modeling: Synergies and limitations. User
Modeling and User-Adapted Interaction,
11:129?158.
Using a Randomised Controlled Clinical Trial to Evaluate an NLG
System
Ehud Reiter
 
Roma Robertson  A Scott Lennox  Liesl Osman 
Departments of Computing Science
 
, General Practice  , and Medicine and Therapeutics 
University of Aberdeen, Aberdeen, Scotland, UK

e.reiter, roma.robertson, s.lennox, l.osman  @abdn.ac.uk
Abstract
The STOP system, which generates
personalised smoking-cessation letters,
was evaluated by a randomised con-
trolled clinical trial. We believe this is
the largest and perhaps most rigorous
task effectiveness evaluation ever per-
formed on an NLG system. The de-
tailed results of the clinical trial have
been presented elsewhere, in the med-
ical literature. In this paper we discuss
the clinical trial itself: its structure and
cost, what we did and did not learn from
it (especially considering that the trial
showed that STOP was not effective),
and how it compares to other NLG eval-
uation techniques.
1 Introduction
There is increasing interest in techniques for eval-
uating Natural Language Generation (NLG) sys-
tems. However, we are not aware of any previ-
ously reported evaluations of NLG systems which
have rigorously compared the task effectiveness
of an NLG system to a non-NLG alternative. In
this paper we discuss such an evaluation, a large
scale (2553 subjects) randomised controlled clin-
ical trial which evaluated the effectiveness of per-
sonalised smoking-cessation letters generated by
the STOP system (Reiter et al, 1999). We be-
lieve that this is the largest, most expensive, and
perhaps most rigorous evaluation ever done of an
NLG system; it was also a disappointing evalua-
tion, as it showed that STOP letters in general were
no more effective than control letters.
The detailed results of the STOP evaluation
have been presented elsewhere, in the medical lit-
erature (Lennox et al, 2001). The purpose of this
paper is to discuss the clinical trial from an NLG
evaluation perspective, in order to help future re-
searchers decide when a clinical trial (or similar
large-scale task effectiveness evaluation) would
be an appropriate way to evaluate their systems.
2 Evaluation of NLG Systems
Evaluation is becoming increasingly important in
NLG, as in other areas of NLP; see Mellish and
Dale (1998) for a summary of NLG evaluation.
As Mellish and Dale point out, we can evalu-
ate the effectiveness of underlying theories, gen-
eral properties of NLG systems and texts (such as
computational speed, or text understandability),
or the effectiveness of the generated texts in an
actual task or application context. Theory eval-
uations are typically done by comparing predic-
tions of a theory to what is observed in a human-
authored corpus (for example, (Yeh and Mellish,
1997)). Evaluations of text properties are typi-
cally done by asking human judges to rate the
quality of generated texts (for example, (Lester
and Porter, 1997)); sometimes human-authored
texts are included in the rated set (without judges
knowing which texts are human-authored) to pro-
vide a baseline. Task evaluations (for example,
(Young, 1999)) are typically done by showing hu-
man subjects different texts, and measuring dif-
ferences in an outcome variable, such as success
in performing a task.
However, despite the above work, we are not
aware of any previous evaluation which has com-
pared the effectiveness of NLG texts at meeting
a communicative goal against the effectiveness
of non-NLG control texts. Young?s task eval-
uation, which may be the most rigorous previ-
ous task evaluation of an NLG system, compared
the effectiveness of texts generated by different
NLG algorithms, while the IDAS task evaluation
(Levine and Mellish, 1995) did not include a con-
trol text of any kind. Coch (1996) and Lester and
Porter (1997) have compared NLG texts to human-
written and (in Coch?s case) mail-merge texts, but
the comparisons were judgements by human do-
main experts, they did not measure the actual im-
pact of the texts on users. Carenini and Moore
(2000) probably came closest to a controlled eval-
uation of NLG vs non-NLG alternatives, because
they compared the impact of NLG argumentative
texts to a no-text control (where users had access
to the underlying data but were not given any texts
arguing for a particular choice).
Task evaluations that compare the effectiveness
of texts from NLG systems to the effectiveness of
non-NLG alternatives (mail-merge texts, human-
written texts, or fixed texts) are expensive and
difficult to organise, but we believe they are es-
sential to the progress of NLG, both scientifically
and technologically. In this paper we describe
such an evaluation which we performed on the
STOP system. The evaluation was indeed expen-
sive and time-consuming, and ultimately was dis-
appointing in that it suggested STOP texts were no
more effective than control texts, but we believe
that this kind of evaluation was essential to the
project. We hope that our description of the STOP
clinical trial and what we learned from it will en-
courage other researchers to consider performing
effectiveness evaluations of NLG systems against
non-NLG alternatives.
3 STOP and its Clinical Trial
The STOP system has been described elsewhere
(Reiter et al, 1999). Very briefly, the system took
as input a 4-page questionnaire about smoking
history, habits, intentions, and so forth, and from
this produced a small (4 pages of A5) person-
alised smoking cessation letter. All interactions
with the smoker were paper-based; he or she filled
out a paper questionnaire which was scanned into
the computer system, and the resultant letter was
printed out and posted back to the smoker. The
first page of a typical questionnaire is shown in
Figure 1, and part of the letter produced from this
questionnaire is shown in Figure 2.1 We wish to
emphasise that producing personalised health in-
formation letters is not a new idea, many previous
researchers have worked in this area; see Lennox
et al(2001) for a comparison of STOP to previous
work in this area.
The STOP clinical trial, which is the focus of
this paper, was organised as follows. We con-
tacted 7427 smokers, and asked them to partici-
pate in the trial. 2553 smokers agreed to partic-
ipate, and filled out our smoking questionnaire.
These smokers were randomly split among three
groups:
  Tailored. These smokers received the letter
generated by STOP from their questionnaire.
  Non-tailored. These smokers received a
fixed (non-tailored) letter. The non-tailored
letter was essentially the letter produced by
STOP from a blank questionnaire, with some
manual post-editing and tidying up. In other
words, during the course of developing STOP
we created a set of default rules for han-
dling incomplete or inconsistent question-
naires; the non-tailored letter was produced
by activating these default rules without any
smoker data. Part of the non-tailored letter is
shown in Figure 3.
  No-letter. These smokers just received a let-
ter thanking them for participating in our
study.
After six months we sent a followup question-
naire asking participants if they had quit, and also
other questions (for example, if they were intend-
ing to try to quit even if they had not actually done
so yet). Smokers could also make free-text com-
ments about the letter they received. 2045 smok-
ers responded to the followup questionnaire, of
which 154 claimed to have quit. Because people
do not always tell the truth about their smoking
habits, we asked these 154 people to give saliva
samples, which were tested in a lab for nicotine
residues. 99 smokers agreed to give such samples,
and 89 of these were confirmed as non-smokers.
1To protect patient confidentiality, we have changed the
name of the smoker and her medical practice, and typed her
handwritten responses.
1SM
O
KI
NG
 Q
UE
ST
IO
NN
AI
RE
Pl
ea
se
 a
n
sw
er
 b
y 
m
ar
ki
n
g 
th
e 
m
o
st
 a
pp
ro
pr
ia
te
 b
ox
 fo
r 
ea
ch
 q
ue
st
io
n
 li
ke
 th
is:
  
_
Q1
 H
a
v
e 
yo
u
 s
m
o
ke
d 
a
 c
ig
ar
et
te
 in
 th
e 
la
st
 w
ee
k,
 e
v
en
 a
 p
uf
f?
Y
ES
_
N
O
?
Pl
ea
se
 c
o
m
pl
et
e 
th
e 
fo
llo
w
in
g 
qu
es
tio
n
s
Pl
ea
se
 r
et
u
rn
 th
e 
qu
es
tio
n
n
ai
re
 u
n
an
sw
er
ed
 in
 th
e
en
v
el
o
pe
 p
ro
v
id
ed
. T
ha
n
k 
yo
u
.
Pl
ea
se
 r
ea
d 
th
e 
qu
es
tio
n
s 
ca
re
fu
lly
.   
 
 
 
If 
yo
u
 a
re
 n
o
t s
u
re
 h
ow
 to
 a
n
sw
er
,
 
jus
t g
iv
e 
th
e 
be
st
 a
n
sw
er
 y
o
u
 c
an
.
Q2
H
o
m
e 
sit
u
a
tio
n
:
Li
v
e
al
o
n
e
_
Li
v
e 
w
ith
hu
sb
an
d/
w
ife
/p
ar
tn
er
?
Li
v
e 
w
ith
o
th
er
 a
du
lts
?
Li
v
e 
w
ith
ch
ild
re
n
?
Q3
   
 N
u
m
be
r 
o
f c
hi
ld
re
n
 
u
n
de
r 
16
 li
v
in
g 
at
 h
om
e 
  
  
  
 ?
?
?
0?
?
?
 
bo
ys
  
  
?
?
?
0?
?
.
 
gi
rls
Q4
D
o
es
 a
n
yo
n
e 
el
se
 in
 y
ou
r 
ho
u
se
ho
ld
 sm
o
ke
? 
 
 
 
(If
 so
, 
pl
ea
se
 m
a
rk
 a
ll 
bo
xe
s 
w
hi
ch
 a
pp
ly
)
hu
sb
an
d/
w
ife
/p
ar
tn
er
?
o
th
er
 fa
m
ily
 m
em
be
r
?
o
th
er
s
?
Q5
 
 
 
 
H
o
w
 lo
n
g 
ha
v
e 
yo
u
 s
m
o
ke
d 
fo
r?
 
 
 
?
20
?
 
ye
ar
s
 
 
 
 
 
 
Ti
ck
 h
er
e 
if 
yo
u
 h
av
e 
sm
o
ke
d 
fo
r 
le
ss
 th
an
 a
 y
ea
r 
  
  
  
?
Q6
   
 H
o
w
 m
a
n
y 
ci
ga
re
tt
es
 d
o
 y
ou
 s
m
o
ke
 in
 a
 d
a
y?
  (P
le
a
se
 m
a
rk
 th
e 
a
m
o
u
n
t b
el
o
w
)
Le
ss
 th
an
 5
 ?
5 
? 
10
  ?
11
 ?
 1
5 
 _
16
 ?
 2
0 
 ?
21
 - 
30
  ?
31
 o
r 
m
o
re

?
Q7
 
 
 
 
 
H
o
w
 s
o
o
n
 a
fte
r 
yo
u
 w
a
ke
 u
p 
do
 y
ou
 s
m
o
ke
 y
ou
r 
fir
st
 c
ig
ar
et
te
? 
(P
le
a
se
 m
a
rk
 th
e 
tim
e 
be
lo
w
)
W
ith
in
 5
 m
in
u
te
s 
 ?
6 
-
 
30
 m
in
u
te
s 
 _
31
 - 
60
 m
in
u
te
s 
 ?
A
fte
r 
60
 m
in
u
te
s 
 ?
Q8
   
 D
o
 y
ou
 fi
n
d 
it 
di
ffi
cu
lt 
n
o
t t
o
 s
m
o
ke
 in
 p
la
ce
s 
w
he
re
 it
 is
fo
rb
id
de
n
  
 e
g 
in
 c
hu
rc
h,
 a
t t
he
 li
br
a
ry
, i
n
 th
e 
ci
n
em
a
?
Y
ES
 
_


N
O
?

Q9
   
 W
hi
ch
 c
ig
ar
et
te
 w
o
u
ld
 y
ou
 h
a
te
 m
o
st
 to
 g
iv
e 
u
p?
Th
e 
fir
st
 o
n
e 
in
 th
e 
m
o
rn
in
g 
_
A
n
y 
o
f t
he
 o
th
er
s 
  
 ?
Q1
0 
   
D
o
 y
ou
 s
m
o
ke
 m
o
re
 fr
eq
ue
n
tly
 d
ur
in
g 
th
e 
fir
st
 h
o
u
rs
 a
fte
r
w
a
ki
n
g 
th
a
n
 d
ur
in
g 
th
e 
re
st
 o
f t
he
 d
a
y?
Y
ES
 
?


N
O
_

Q1
1 
  D
o
 y
ou
 s
m
o
ke
 if
 y
ou
 a
re
 s
o
 il
l t
ha
t y
ou
 a
re
 in
 b
ed
 m
o
st
 o
f t
he
da
y?
Y
ES
 
?


N
O
_

Y
ES
?
Q1
3  
 
If
 y
es
,
 
a
re
 y
ou
 in
te
n
di
n
g 
to
 s
to
p 
sm
o
ki
n
g
w
ith
in
 th
e 
n
ex
t m
o
n
th
?
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Y
ES
 
?


N
O
?

Q1
2
A
re
 y
ou
 in
te
n
di
n
g 
to
 s
to
p
sm
o
ki
n
g 
in
 th
e 
n
ex
t 6
m
o
n
th
s?
N
O
   
  _

Q1
4  
 
If
 n
o
,
 
w
o
u
ld
 y
ou
 li
ke
 to
 s
to
p 
sm
o
ki
n
g 
if 
it 
w
a
s
ea
sy
?
Y
ES
 
?



N
o
t S
u
re
  
 _



N
O
?

Figure 1: First page of a STOP questionnaire
3.1 Practical Aspects of the Clinical Trial
The STOP clinical trial took 20 months to run
(of which the first 4 months overlapped soft-
ware development), and cost about UK?75,000
(US$110,000). We believe the STOP clinical trial
was the longest and costliest evaluation ever done
of an NLG system. The length and cost of the clin-
ical trial were primarily due to the large numbers
of subjects. Whereas Levine and Mellish (1995),
Young (1999), and Carenini and Moore (2000) in-
cluded 10, 26, and 30 subjects (respectively) in
their task effectiveness evaluations, we had 2553
subjects in our clinical trial. The cost of the trial
was partially stationary and postage (we sent out
over 10000 mailings to smokers, each of which
included a reply-paid envelope), but mostly staff
costs to set up the trial, perform the mailings, pro-
cess and analyse the returns from smokers, and
handle various glitches in the trial.
Another way of looking at the trial was that we
spent about UK?30 (US$45) per subject (includ-
ing staff time as well as materials). Perhaps the
trial could have been done a bit more cheaply, but
any experiment involving 2553 subjects is bound
to be expensive and time-consuming.
The reason the trial needed to be so large was
that we were measuring a binary outcome vari-
able (laboratory-verified smoking cessation) with
a very low positive rate (since smoking is a very
difficult habit to quit). Young, in contrast, mea-
sured numerical variables (such as the number of
mistakes made by a user when following textual
instructions) with substantial standard deviations.
Another complication was that we wanted to
use a representative sample of smokers in our
trial, which meant that we could not (as Young
and Levine and Mellish did) just recruit students
and acquaintances. Instead, we contacted a repre-
sentative set of GPs in our area, and asked them
for a list of smokers from their patient record sys-
tems. This was the source of the 7427 initial
smokers mentioned above.
4 Results of the Clinical Trial
Detailed results of the STOP clinical trial, includ-
ing statistical tables, have been published in the
medical literature (Lennox et al, 2001). Here we
just summarise the key findings which are of NLG
Smoking Information for Heather Stewart
You have good reasons to stop...
People stop smoking when they really want to stop.  It is encouraging that
you have many good reasons for stopping.  The scales show the good
and bad things about smoking for you.  They are tipped in your favour.
You could do it...
Most people who really want to stop eventually succeed.  In fact, 10
million people in Britain have stopped smoking - and stayed stopped - in
the last 15 years.  Many of them found it much easier than they expected.
Although you don't feel confident that you would be able to stop if you
were to try, you have several things in your favour.
? You have stopped before for more than a month.
? You have good reasons for stopping smoking.
? You expect support from your family, your friends, and your
workmates.
We know that all of these make it more likely that you will be able to stop.
Most people who stop smoking for good have more than one attempt.
Overcoming your barriers to stopping...
You said in your questionnaire that you might find it difficult to stop
because smoking helps you cope with stress.  Many people think that
cigarettes help them cope with stress.  However, taking a cigarette only
makes you feel better for a short while.  Most ex-smokers feel calmer and
more in control than they did when they were smoking.  There are some
ideas about coping with stress on the back page of this leaflet.
You also said that you might find it difficult to stop because you would put
on weight.  A few people do put on some weight.  If you did stop smoking,
your appetite would improve and you would taste your food much better.
Because of this it would be wise to plan in advance so that you're not
reaching for the biscuit tin all the time.  Remember that putting on weight
is an overeating problem, not a no-smoking one.  You can tackle it later
with diet and exercise.
And finally...
We hope this letter will help you feel more confident about giving up
cigarettes.  If you have a go, you have a real chance of succeeding.
With best wishes,
The Health Centre.
THINGS YOU LIKE
it's relaxing
it stops stress
you enjoy it
it relieves boredom
it stops weight gain
it stops you craving
THINGS YOU DISLIKE
it makes you less fit
it's a bad example for kids
you're addicted
it's unpleasant for others
other people disapprove
it's a smelly habit
it's bad for you
it's expensive
it's bad for others' health
Figure 2: Inside pages of the STOP letter generated from the Figure 1 questionnaire
Information for Stopping Smoking
Do you want to stop smoking?
Everyone has things they like and dislike about their smoking.  The
decision to stop smoking depends on the things you don't like being more
important than the things you do like.  It can be useful to think of it as a
balance.  Have a look on the scales.  What are the good and bad things
for you?
Add any more that you can think of.  Are you ready to stop smoking?  If
yes, maybe it's the right time to have a go.  If no, think about the good and
bad things about smoking.  This might swing the balance for you.
You can do it.....
People who want to stop smoking usually succeed.  10 million people in
Britain have stopped smoking - and stayed stopped - in the last 15 years.
Many of them found it much easier than they expected!
Try it out.....
If you don't feel ready for an all-out attempt to stop smoking, there are
some useful ways to prepare yourself.  You could try some of the
following ideas now.  This will help you when you try to stop smoking.
? Delay your first cigarette of the day by half an hour.
? Stop smoking for 24 hours.
? Cut down the number you smoke by 5 cigarettes per day.
Planning will help.....
When you stop, it helps to plan ahead.  Here are some things that have
worked for others:
? Pick a day to stop, and let your family and friends know.
? Think of situations where you might feel tempted to smoke, and plan
how you could avoid or deal with them.
? Get rid of all cigarettes and ashtrays the day before.
? When you do stop, take one day at a time; don't look too far ahead.
If it gets tough.....
Many people do hit rough patches; there are ways to deal with these.  On
the back page are some suggestions that other people have found useful.
If you do have a cigarette after a few days just put it behind you and keep
on trying.  Prepare yourself for another attempt, many people have more
than one go before they stop for good!
With best wishes.
The Health Centre.
GOOD THINGS
you enjoy it
it's relaxing
it stops stress
it breaks up the day
it relieves boredom
it's sociable
it stops weight gain
it stops you craving
BAD THINGS
it's bad for you
it makes you less fit
it's expensive
it's a bad example for kids
it's bad for others? health
you're addicted
it's unpleasant for others
other people disapprove
it's a smelly habit
Figure 3: Inside pages of the non-tailored letter
(as well as medical) interest.
Of the 2553 smokers in the trial, 89 were val-
idated as having stopped smoking. These broke
down by group as follows:
  3.5% (30 out of 857) of the tailored group
stopped smoking
  4.4% (37 out of 846) of the non-tailored
group stopped smoking
  2.6% (22 out of 850) of the no-letter group
stopped smoking
The non-tailored group had the lowest number of
heavy (more than 20 cigarettes per day) smok-
ers, who are less likely to stop smoking (because
they are probably addicted to nicotine) than light
smokers; the tailored group had the highest num-
ber of heavy smokers. After adjusting for this
fact, cessation rates were still higher in the non-
tailored group than in the tailored group, but this
difference was not statistically significant. We
can see this if we look just at cessation rates in
light smokers (few heavy smokers from any cate-
gory managed to stop smoking):
  4.3% (25 out of 563) of the light smokers in
the tailored group stopped smoking
  4.9% (31 out of 597) of the light smokers in
the non-tailored group stopped smoking
  2.7% (16 out of 582) of the light smokers in
the no-letter group stopped smoking
The overall conclusion is therefore that recipi-
ents of the non-tailored letters were more likely to
stop than people who got no letter2 (p=.047 over-
all unadjusted; p=.069 overall after adjusting for
differences between groups, such as heavy/light
smoker split; p=.049 for light smokers). How-
ever, there was no evidence that the tailored let-
ters were any better than the non-tailored ones in
terms of increasing cessation rates.
2Note that while a 1% or 2% increase in cessation rates
is small, it is medically useful if it can be achieved cheaply.
See Law and Tang (1995) for a discussion of success rates
and cost-effectiveness of various smoking-cessation tech-
niques, and Lennox et al(2001) for an analysis that shows
that sending letters is very cost-effective compared to most
other smoking-cessation techniques.
There is some very weak evidence that the tai-
lored letter may have been better than the non-
tailored letter among smokers for whom quitting
was especially difficult. For example, among dis-
couraged smokers (people who wanted to quit
but were not intending to quit, usually because
they didn?t think they could quit), cessation rates
were 60% higher among recipients of tailored let-
ters than recipients of non-tailored letters, but the
numbers were too small to reach statistical signif-
icance, since (as with heavy smokers) very few
such people managed to stop smoking. Further-
more, among heavy smokers, recipients of the tai-
lored letter were 50% more likely than recipients
of the non-tailored letters to show increased inten-
tion to quit (for example, say in their initial ques-
tionnaire that they did not intend to quit, but say
in the followup questionnaire that they did intend
to quit) (p=.059). It would be nice to test the hy-
pothesis that tailored letters were effective among
discouraged smokers or heavy smokers by run-
ning another clinical trial, but such a trial would
need to be even bigger and more expensive than
the STOP trial, in order to have enough validated
quitters from these categories to make it possible
to draw statistically significant conclusions.
Recipients of the tailored letters were more
likely than recipients of non-tailored letters to re-
member receiving the letter (67% vs 44%, signif-
icant at p   .01), to have kept the letter (30% vs
19%, significant at p   .01), and to make a free-
text comment about the letter (20% vs 12%, sig-
nificant at p   .01). However, there was no statis-
tically significant difference in perceptions of the
usefulness and relevance of the tailored and non-
tailored letters.
Free-text comments on the tailored letters were
varied, ranging from I carried mine with me all
the time and looked at it whenever I felt like
giving in to I found it patronising . . . Smoking
obviously impairs my physical health ? not
my intelligence! The most common complaint
about content was that not enough information
was given about practical ?how-to-stop-smoking?
techniques. STOP?s tailoring rules only included
such information in about one third of the letters;
this was in accordance with the well-established
Stages of Change model of smoking cessation
(Prochaska and diClemente, 1992). Note that all
recipients of the non-tailored letter received such
information. If practical advice was useful to
more than one third of smokers, then the Stages-
of-Change based tailoring rules which decided
when to include such information may have de-
creased rather than increased letter effectiveness.
5 What Can be Learned from a Negative
Result
One of the remarkable things about the NLG,
NLP, and indeed AI literatures is that little men-
tion is made of experiments with negative results.
In more established fields such as medicine and
physics, papers which report negative experimen-
tal findings are common and are valued; but in
NLP they are rare. It seems unlikely that NLP ex-
periments always produce positive results (unless
the experiments are badly designed and biased to-
wards demonstrating the experimenter?s desired
outcome); what is probably happening is that peo-
ple are choosing not to report negative results.
One reason for this may be that it can be diffi-
cult to draw clear lessons from a negative result.
In the case of STOP, for example, the clinical trial
did not tell us why STOP failed. There are many
possible reasons for the negative result, including:
1. Tailoring cannot have much effect. That is, if
a smoker receives a letter from his/her doctor
about smoking, then the content of the let-
ter is only of secondary importance, the im-
portant thing is the fact of having received a
communication from his/her doctor encour-
aging smoking cessation.
2. Tailoring could have an impact, but only if it
was based on much more knowledge about
the smoker?s circumstances than is available
via a 4-page multiple choice questionnaire.
3. Tailoring based on a multiple-choice ques-
tionnaire can work, we just didn?t do it right
in STOP, perhaps in part because we based
our system on inappropriate theoretical mod-
els of smoking cessation.
4. The STOP letters did in fact have an effect
on some groups (such as heavy or discour-
aged smokers), but the clinical trial was too
small to provide statistically significant evi-
dence of this.
In other words, did we fail because (1) what we
were attempting could not work; (2) what we
were attempting could only work if we had a lot
more knowledge available to us; or (3) we built
a poor system? Or (4) did the system actually
work to some degree, but the evaluation didn?t
show this because it was too small? This is a key
question for NLG researchers and developers (as
opposed to doctors and health administrators who
just want to know if they should use STOP as a
black-box system), but the clinical trial does not
distinguish between these possibilities.
Arguments can be made for all three of the
above possibilities. For example, we could argue
for (1) on the basis that brief discussions about
smoking with a doctor have about a 2% success
rate (Law and Tang, 1995), and this may be an up-
per limit for the effectiveness of a brief letter from
a doctor. If so, then letters cannot do much better
that the 1.8% increase in cessation rates produced
by the STOP non-tailored letter. Or we could ar-
gue for (2) by noting that when we asked smok-
ers to comment on STOP letters in a small pilot
study, many of their comments were very specific
to their particular circumstances For example, a
single mother mentioned that a previous attempt
to stop failed because of stress caused by dealing
with a child?s tantrum, and an older woman dis-
cussed the various stop-smoking techniques she
had tried in the past and how they failed. Per-
haps tailoring according to such specific circum-
stances would add value to letters; but such tai-
loring would require much more information than
can be obtained from a 4-page multiple-choice
questionnaire. We could also argue for (3) be-
cause there clearly are many ways in which the
tailored letters could have been improved (such
as having practical ?how-to-stop? tips in more let-
ters, as mentioned at the end of Section 4); and
for (4) on the basis of the weak evidence for this
mentioned in Section 4.
We do not know which of the above reason(s)
were responsible for STOP?s failure, so we can-
not give clear lessons for future researchers or de-
velopers. This is perhaps true of many negative
experimental results, and may be a reason why
people do not publish them in the NLP commu-
nity. Again there is perhaps a different attitude
in the medical community, where papers describ-
ing experiments are taken as ?data points? and
more theoretically minded researchers may look
at a number of experimental papers and see what
patterns and insights emerge from the collection
as a whole. Under this perspective it is less im-
portant to state what lessons or insights can be
drawn from a particular negative result, what mat-
ters is the overall pattern of positive and negative
results in a group of related experiments. And
like most such procedures, the process of infer-
ring general rules from a collection of specific ex-
perimental results will work much better if it has
access to both positive and negative examples; in
other words, if researchers publish their failures
as well as their successes.
We believe that negative results are also impor-
tant in NLG, NLP, and AI, even if it is not possible
to draw straightforward lessons from them; and
we hope that more such results are reported in the
future.
6 Other Evaluation Techniques in STOP
The clinical trial was by far the biggest evaluation
exercise in STOP, but we also performed some
smaller evaluations in order to test our algorithms
and knowledge acquisition methodology (Reiter,
2000; Reiter et al, 2000). These included:
1. Asking smokers or domain experts to read
two letters, and state which one they thought
was superior;
2. Statistical analyses of characteristics of
smokers; and
3. Comparing the effectiveness of different al-
gorithms at filling up but not exceeding 4 A5
pages.
These evaluations were much smaller, simpler,
and cheaper than the clinical trial, and often
gave easier to interpret results. For example,
the letter-comparison experiments suggested (al-
though they did not prove) that older people pre-
ferred a more formal writing style than younger
people; the statistical analysis suggested (al-
though again did not prove) that the tailoring rules
should have been more influenced by level of ad-
diction; and the algorithmic analysis showed that
a revision architecture outperformed a conven-
tional pipeline architecture.
So, these experiments produced clearer results
at a fraction of the cost of the clinical trial. But
the cheapness of (1) and (2) were partially due to
the fact that they were too small to produce sta-
tistically solid findings, and the cheapness of (2)
and (3) were partially due to the fact that they ex-
ploited data sets and resources that were built as
part of the clinical trial. Overall, we believe that
these small-scale experiments were worth doing,
but as a supplement to, not a replacement for, the
clinical trial.
7 When is a Clinical Trial Appropriate?
When is it appropriate to evaluate an NLG system
with a large-scale task or effectiveness evaluation
which compares the NLG system to a non-NLG al-
ternative? Certainly this should be done when a
customer is seriously considering using the sys-
tem, indeed customers may refuse to use a system
without such testing.
Controlled task/effectiveness evaluations are
also scientifically important, because they provide
a technique for testing applied hypotheses (such
as ?STOP produces effective smoking-cessation
letters?). As such, they should be considered
whenever a researcher is interested in testing such
hypotheses. Of course, much research in NLG
is primarily theoretical, and thus perhaps best
tested by corpus studies or psycholinguistic ex-
periments; and much work in applied NLG is con-
cerned with pilot studies and other hypothesis for-
mation exercises. But at the end of the day, re-
searchers interested in applied NLG need to test as
well as formulate hypotheses. While many speech
recognition and natural-language understanding
applications can be tested by comparing their out-
put to a human-produced ?gold standard? (for ex-
ample, speech recogniser output can be compared
to a human transcription of a speech signal), this
to date has been harder to do in NLG, especially in
applications such as STOP where there are no hu-
man experts (Reiter et al, 2000) (there are many
experts on personalised oral communication with
smokers, but none on personalised written com-
munication, because no one currently writes per-
sonalised letters to smokers). In such applica-
tions, the only way to test hypotheses about the
effects of systems on human users may be to run
a controlled task/effectiveness evaluation.
In other words, there?s probably no point in
conducting a large-scale task/effectiveness evalu-
ation of an NLG system if you?re interested in for-
mulating hypotheses instead of testing them, or if
you?re interested in theoretical instead of applied
hypotheses. But if you want to test an applied hy-
pothesis about the effect of an NLG system on hu-
man users, the most rigorous way of doing this is
to conduct an experiment where you show some
users your NLG texts and other users control texts,
and measure the degree to which the desired ef-
fect is achieved in both groups.
Large-scale evaluation exercises also have the
benefit of forcing researchers and developers to
make systems robust, and to face up to the messi-
ness of real data, such as awkward boundary cases
and noisy data. Indeed we suspect that STOP is
one of the most robust non-commercial NLG sys-
tems ever built, because the clinical trial forced us
to think about issues such as what we should do
with inconsistent or improperly scanned question-
naires, or what we should say to unusual smokers.
In conclusion, large-scale task/effectiveness
evaluations are expensive, time-consuming, and a
considerable hassle. But they are also an essential
part of the scientific and technological process,
especially in testing applied hypotheses about the
effectiveness of systems on real users. We hope
that more such evaluations are performed in the
future, and that their results are reported whether
they are positive or negative.
Acknowledgements
Many thanks to the rest of the STOP team, and
especially to Ian McCann and Annette Hermse
for their work in the clinical trial. Thanks also
to Yaji Sripada, Sandra Williams, and the anony-
mous reviewers for their comments on drafts
of this paper. This research was supported by
the Scottish Office Department of Health under
grant K/OPR/2/2/D318, and the Engineering and
Physical Sciences Research Council under grant
GR/L48812.
References
Guiseppe Carenini and Johanna Moore. 2000. An em-
pirical study of the influence of argument concise-
ness on argument effectiveness. In Proceedings of
ACL-2000.
Jose? Coch. 1996. Evaluating and comparing three text
production techniques. In Proceedings of the Six-
teenth International Conference on Computational
Linguistics (COLING-1996).
Malcolm Law and Jin Tang. 1995. An analysis of the
effectiveness of interventions intended to help peo-
ple stop smoking. Archives of Internal Medicine,
155:1933?1941.
A Scott Lennox, Liesl Osman, Ehud Reiter, Roma
Robertson, James Friend, Ian McCann, Diane
Skatun, and Peter Donnan. 2001. The cost-
effectiveness of computer-tailored and non-tailored
smoking cessation letters in general practice: A ran-
domised controlled study. British Medical Journal.
In press.
James Lester and Bruce Porter. 1997. Developing and
empirically evaluating robust explanation genera-
tors: The KNIGHT experiments. Computational
Linguistics, 23(1):65?101.
John Levine and Chris Mellish. 1995. The IDAS user
trials: Quantitative evaluation of an applied natu-
ral language generation system. In Proceedings of
the Fifth European Workshop on Natural Language
Generation, pages 75?93, Leiden, The Netherlands.
Chris Mellish and Robert Dale. 1998. Evaluation in
the context of natural language generation. Com-
puter Speech and Language, 12:349?373.
James Prochaska and Carlo diClemente. 1992. Stages
of Change in the Modification of Problem Behav-
iors. Sage.
Ehud Reiter. 2000. Pipelines and size constraints.
Computational Linguistics, 26(2):251?259.
Ehud Reiter, Roma Robertson, and Liesl Osman.
1999. Types of knowledge required to person-
alise smoking cessation letters. In Werner Horn
et al, editors, Artificial Intelligence and Medicine:
Proceedings of AIMDM-1999, pages 389?399.
Springer-Verlag.
Ehud Reiter, Roma Robertson, and Liesl Osman.
2000. Knowledge acquisition for natural language
generation. In Proceedings of the First Interna-
tional Conference on Natural Language Genera-
tion, pages 217?215.
Ching-Long Yeh and Chris Mellish. 1997. An empir-
ical study on the generation of anaphora in chinese.
Computational Linguistics, 23(1):169?190.
Michael Young. 1999. Using Grice?s maxim of quan-
tity to select the content of plan descriptions. Arti-
ficial Intelligence, 115:215?256.
Knowledge Acquisition for Natural Language.Generation 
Ehud Re i te r  and Roma Rober tson  
Dept  o f  Comput ing  Science 
Univ of  Aberdeen,  Scot land  
{ere i te r ,  r rober ts}@csd,  abdn.  ac .  uk 
L ies l  Osman 
Dept  of  Medic ine and Therapeut ics  
Univ of Aberdeen,  Scot land  
I. osman@abdn, ac .  uk 
Abst ract  
We describe the knowledge acquisition (KA) tech- 
niques used to build the STOP system, especially 
sorting and think-aloud protocols. That is, we de- 
scribe the ways in which we interacted with domain 
experts to determine appropriate user categories, 
schemas, detailed content rules, and so forth for 
STOP. Informal evaluations of these techniques sug- 
gest that they had some benefit, but perhaps were 
most successful as a source of insight and hypothe- 
ses, and should ideally have been supplemented by 
other techniques when deciding on the specific rules 
and knowledge incorporated into STOP. 
1 In t roduct ion  
An important aspect of building natural-language 
generat ion  (NLG) systems is knowledge acquisition. 
This is the process of acquiring the specific knowl- 
edge needed in a particular application about the 
domain, the language used in the domain genre, 
the readers of the texts, and so forth. Such knowl- 
edge influences, for example, the system's content 
selection rules (whether represented asschemas, pro- 
duction rules, or plan operators); the system's mi- 
croplanning choice rules (lexicalisation, referring ex- 
pression generation, aggregation); and perhaps even 
the system's grammar (if a genre grammar is needed, 
as is tile case, for example, with weather eports). 
To date, knowledge acquisition for NLG systems 
has largely been based on corpus analysis, infor- 
mal interactions with experts, and informal feed- 
back from users (Reiter et al, 1997; Reiter and Dale, 
2000). For example, the PlanDoc developers (McK- 
eown et al, 1994) interviewed users to get a gen- 
eral understanding of the domain and user require- 
ments; asked a single expert to write some example 
output texts; and then analysed this corpus in vari- 
ous ways. Other KA techniques used in the past for 
building NLG systems include letting domain experts 
specify content rules in pseudo-code (Goldberg et al, 
1994) and ethnographic techniques such as observing 
doctors and patients in real consultations (Forsythe, 
1995). 
As part of the .?,ToP project (Reiter et al, 1999) to 
generate personalised smoking-cessation leaflets, we 
investigated using some of the structured KA tech- 
niques developed by the expert-system community 
(see, for example, (Scott et al, 1991)) for acquiring 
the knowledge needed by an NLG system. In this 
paper we summarise our experiences. Very briefly, 
our overall conclusion is that in STOP, structured KA 
was probably useful for getting insight into and for- 
mulating hypotheses about the knowledge needed by 
an NaG system. However, formulating detailed rules 
purely on the basis of such KA was not ideal, and it 
would have been preferable to use other information 
as well during this process, such as statistics about 
smokers and feedback from smoker evaluations of 
draft STOP leaflets. 
2 Background:  The  STOP System 
The STOP system generates personalised smoking- 
cessation leaflets, based on the recipient's responses 
to a questionnaire about smoking beliefs, concerns, 
and experiences. STOP leaflets consist of four A5 
pages, of which only the two inside pages are fully 
generated; an example of the inside pages of a STOP 
leaflet are shown in Figure 2. Internally, STOP is 
a fairly conventional shallow NLG system, with its 
main innovation being the processing used to control 
the length of leaflets (Reiter, 2000). STOP has been 
evaluated in a clinical trial, which compared cessa- 
tion rates among smokers who received STOP leaflets; 
smokers who received anon-personalised leaflet with 
similar structure and appearance to a STOP leaflet: 
and smokers who did not receive any leaflet (but did 
fill out a questionnaire). Unfortunately, we cannot 
discuss the results of the clinical trial in this paper I . 
One of the research goals of the STOP project was 
to explore the use of expert-system knowledge acqtfi- 
sition techniques in buitding anNLO system. These 
knowledge acquisition sessions were primarily car- 
ried out with the following experts: 
1Our medical colleagues intend to publish a paper about 
the clinical trial in a medical journal,  and have requested 
that we not publish anything about the results of the trial in 
a computing journal or conference until they have published 
in a medical journal. 
217 
o three doctors (two general practitioners, one 
consultant in Thoracic Medicine) 
o one psychologist pecialising in health informa- 
tion leaflets 
o one nurse 
None of these experts were paid for their time. We 
also did a small amount of KA with a (paid) graphic 
designer on layout and typography issues. 
2.1 Unusual  Aspects  o f  STOP f rom a KA 
"Perspective 
KA research in the expert-system community has 
largely focused on applications uch as medical di- 
agnosis, where (1) there is a single correct solution, 
and (2) the task being automated is one currently 
done by a human expert. STOP is a different ype 
of application in that (1) there are many possible 
leaflets which can be generated (and the system can- 
not tell which is best), and (2) no human currently 
writes personalised smoking-cessation leaflets (be- 
cause manually writing such leaflets is too expen- 
sive). Point (2) in particular was repeatedly em- 
phasised by the experts we worked with. The doc- 
tors and the nurse were experts on oral consultations 
with smokers, and the health psychologist was an ex- 
pert on writing non-personalised health information 
leaflets, but none of them had experience writing 
personalised smoking-cessation leaflets. 
Many NLG systems have similar characteristics. 
The flexibility of language means that there are al- 
most always many ways of communicating informa- 
tion and fulfilling communicative goals in a gen- 
erated text; in other words, there are many pos- 
sible texts that can be generated. Furthermore, 
while some synthesis tasks, such as configuration and 
scheduling, can be formalised as finding an optimal 
solution under a well-defined numerical evaluation 
function, this is difficult in NLG because of our poor 
understanding of how to computationally evaluate 
texts for effectiveness. 
With regard to human expertise, some NLG sys- 
tems do indeed generate documents, uch as weather 
reports and customer-service letters, which are cur- 
rently written by humans. But many systems are 
similar to STOP in that they generate texts - -  such as 
descriptions of software models (Lavoie et al, 1997) 
and customised escriptions of museum items (0ber- 
lander et al, 1998) - -  which are useful in principle 
but are not currently writ l}en by humans, perhaps 
because of cost or response-time issues. 
3 KA  Techn iques  Used  in STOP 
3.1 Sort ing 
Sorting is a standard KA technique for building up 
taxonomies. Experts in a sorting exercise are given 
a set of entities, and asked to divide the set into 
subsets, and 'think aloud' into a tape recorder as 
they do so. 
In STOP, we used sorting to build a classification of 
smokers. We started off with an initial classification 
which was motivated by the Stages of Change psy- 
chological theory (Prochaska nd diClemente, 1992): 
this divided smokers into the three categories of Pre- 
contemplators (not intending to quit anytime soon), 
Contemplators ( eriously considering quitting), and 
Preparers (definitely decided to quit, and soon). We 
wished to refine these categories, especially Precon- 
templator (which includes 67% of smokers in the Ab- 
erdeen area), and used sorting to do so. The basic 
exercise consisted of giving a doctor three sets of 
questionnaires (a set from Precontemplators; a set 
from Contemplators; and a set from Preparers), and 
asking him or her to subdivide each set into sub- 
sets. We repeated this exercise with three different 
doctors. 
The results of this exercise were complex, and the 
doctors were not in full agreement. After some anal- 
ysis, we proposed to them that we subdivide all three 
categories on the basis of desire to quit. Precon- 
templators in particular would be divided up into 
people who neither want nor intend to quit (Com- 
mitted Smokers); people who have mixed feelings 
about smoking but don't yet intend to quit (Clas- 
sic Precontemplators); and people who would like to 
quit but aren't intending to quit, typically because 
they don't think they'll succeed (Lacks Confidence). 
The three doctors agreed that this was a reasonable 
subcategorisation, and we proceeded on this basis. 
In particular, we operationalised this categorisation 
as follows: 
o We added the question Would you like to stop 
if it was easy to the questionnaire. People 
who answered No were put into the 'Commit- 
ted Smoker' category. For people who answered 
Not Sure or Yes, we looked at their decisional 
balance, that is the number of likes and dislikes 
they had about smoking, and placed them into 
Lacks Confidence if their dislikes clearly out- 
numbered their likes, and Classic Precontem- 
plator otherwise. 
We defined different high-level schemas for 
each of these categories; these schemas essen- 
tially specified which sections and (in some 
cases) paragraphs hould be included in the 
leaflet, but not tile detailed content of individ- 
ual paragraphs: Under these schemas; Commit- 
ted Smokers got short non-argumentative let- 
ters which gently reminded smokers of some 
of the drawbacks of smoking, and suggested 
some sources of information if the smoker ever 
changed his/her mind; Classic Precontempla- 
tors got letters which focused on the draw- 
backs of smoking; and Lacks Confidence smok- 
218 
ers got letters which focused on confidence- 
building and:deali.ng.~.with.'barriers to :quitting 
(such as addiction or fear of weight gain). The 
example leaflet shown in Figure 2, incidentally, 
is for a Lacks Confidence smoker. 
3.1.1 Eva luat ion  
After the clinical trial was underway, we attempted 
to partially evaluate the sorting-derived categories 
by doing a statisticalanalysis of the differences be- 
tween smokers in the groups. In other words, we 
hypothesised that if our categories were correct in 
distinguishing different types of smokers, then we 
should observe differences in characteristics such as 
addiction and confidence between the groups. Of 
course, this is not an ideal evaluation because it does 
not test the hypothesis that the different classes of 
smokers we proposed should receive different types 
of leaflets; but this is a difficult hypothesis to test 
directly. 
In any case, our analysis uggested that the smok- 
ers in each group did indeed have different character- 
istics. However, it also suggested that we might have 
done as well (in terms of creating subgroups with dif- 
ferent characteristics) by subcategorising purely on 
the Would you like to stop if it was easy question, 
and ignoring likes and dislikes about smoking. The 
analysis also suggested that it might have been use- 
ful to subcategorise on the basis of addiction, which 
we did not do. In fact during the sorting exercises 
the doctors did mention dividing into groups par- 
tially on the basis of the difficulty that individuals 
would have in quitting, but we did not implement 
this. 
The statistical analysis also suggested some ways 
of possibly improving the content schemas. For 
example, the analysis showed that the Committed 
Smoker category included many light smokers who 
probably smoked for social reasons; it might have 
been useful to specifically address this in the STOP 
leaflets ('quit now, before you become addicted'). 
In retrospect, then, the sorting exercise was use- 
ful in proposing ideas about how to divide Stages 
of Change categories, and in new questions to ask 
smokers. However, the process of .defining detailed 
category classification rules and content schemas 
would have benefited greatly from statistical data 
about smokers in our target region. In STOP we 
did not have such data until after the clinical trial 
had started (and smokers had returned their ques- 
tionnaires), by which time the system could not be 
changed. So it would have been difficult to base 
smoker classification on statistical smoker data in 
STOP; but certainly we would recommend such an 
approach in projects where good data is available at 
the outset. 
3.2 Th ink -a loud  Protocols 
- .The-detaited.coatent.-and" phrasing of :STOi :i letters 
was largely based on think-aloud example sessions 
with experts. In these sessions, health professionals 
would be given a questionnaire and asked to write 
a letter or leaflet for this person. They were also 
asked to 'think aloud' into a tape recorder while they 
did this, explaining their reasoning. Again this is a 
standard expert-system technique for KA. 
3.2.1 Example  
...... -k simpte-exainpte=~:the think-aloud process is as fol- 
lows. One of the doctors wrote a letter for a smoker 
who had tried to quit before, and managed to stop 
for several weeks before starting again. The doc- 
tor made the following comments in the think-aloud 
transcript: 
Has he tried to stop smoking before? Yes, 
and the longest he has managed to stop - -  
he has ticked the one week right up to three 
months and that's encouraging in that he 
has managed to stop at least once before, 
because it is always said that the people 
who have had one or two goes are more 
likely to succeed in the future. 
He also included the following paragraph in the 
letter that he wrote for this smoker: 
I see that you managed to stop smoking on 
one or two occasions before but have gone 
back to smoking, but you will be glad to 
know that this is very common and most 
people who finally stop smoking have had 
one or two attempts in the past before 
they finally succeed. What it does show is 
that you are capable of stopping even for a 
short period, and that means you are much 
more likely to be able to stop permanently 
than somebody who has never ever stopped 
smoking at all. 
After analysing this session, we proposed two 
rules: 
* IF (previous attempt o quit) THEN (message: 
more likely to succeed) 
e IF (previous attempt o quit) THEN (message: 
most people who quit have a few unsuccessful 
attempts first) 
The final system incorporated a imle (based Off . . . .  
several KA sessions, not just the above one) that 
stated that if the smoker had tried to quit before, 
then the confidence-building section of the leaflet 
(which is only included for some smoker categories, 
see Section 3A) should include a short message 
about previous attempts to quit. This message 
should mention length of previous cessation if this 
219 
was greater than one week; otherwise, it should men- 
tion recency of previous,,attempt if .this was within..:-- 
the past 6 months. The actual text generated from 
this rule in the example leaflet of Figure 2 is 
Although you don't feel confident hat you 
would be able to stop if you were to try, 
you have several things in your favour. 
? You have stopped before for more than 
a month. 
Note that the message (text)-produced by-the ac- 
tual STOP code is considerably simpler than the text 
originally written by the expert. This is fairly com- 
mon, as is simplifications in the logic used to decide 
whether to include a message in a leaflet or not. In 
some cases this is due to the expert having much 
more knowledge and expertise than the computer 
system (Reiter and Dale, 2000, pp 30-36). Con- 
sider, for example, the following extract from the 
same think-aloud session 
The other thing I notice is that he lives in 
\[Address\] which I would suspect is quite a 
few floors up and that he is probably get- 
ting quite puffy on the stairs . . .  and if he 
gets more breathless he'll end up being a 
prisoner in his own house because he'll be 
able to get down, but he won't be able to 
get up again 
This type of reasoning perhaps requires too much 
general 'world knowledge' about addresses, stairs, 
and breathlessness to be implementable in a com- 
puter system. 
3.2.2 Eva luat ion  
Afterwards, we attempted to partially evaluate the 
rules derived from think-aloud sessions by showing 
STOP leaflets to smokers and other smoking profes- 
sionals, and asking for comments. The results were 
mixed. In terms of content, some smokers found 
the content of the leaflets to be useful and appropri- 
ate for them, but others said they would have liked 
to see different types of information. For example, 
STOP leaflets did not go into the medical details of 
smoking (as none of the think-aloud expert-written 
letters contained such information), and while this 
seemed like the right choice for many smokers, a few 
smokers did say that they would have liked to see 
more medical information about smoking. Reactions 
to style were also mixed. For example, based on KA 
sessions we adopted a positive tone and did not try 
to scare smokers; and again this seemed right for 
most smokers, but some smokers said that a more 
'brutal' approach would be more effective for them. 
An issue which our experts (and other project 
members) disagreed on was whether leaflets should 
always use stmrt and simple sentences, or whether 
sentence length and complexity should be varied de- 
,pending, on the '  characteristics of'.the smoker. In  
the STOP implementation we decided to always use 
moderately simple sentences, and not vary sentence 
complexity for different users. After the clinical trial 
started, we performed a small experiment to test this 
hypothesis. In this experiment, we took a computer- 
generated leaflet and asked one expert (who be- 
lieved that short sentences with simple words should 
always be used) to revise the computer-generated 
.leaflet o .make it as.~easy to  readas  possible, and 
another expert (who believed that more complex 
sentences were sometimes appropriate, and such 
sentences could in some cases make letters seem 
friendlier and more understanding) to revise the 
computer-generated l aflet to make it friendlier and 
more understanding. The revisions made by the ex- 
perts were primarily microplanning ones (using NLG 
terminology) - -  that is, aggregation, ellipsis, lexical 
choice, and syntactic choice. We then showed the 
two expert-revised leaflets to 20 smokers and asked 
them which they preferred. The smokers essentially 
split 50-50 on this question (8 preferred the easy-to- 
read leaflet, 9 preferred the friendly-understanding 
leaflet, 3 thought both were the same). This sug- 
gests that in principle it indeed may be useful to 
vary microplanning choices for different leaflet re- 
cipients. We hope to further investigate this issue in 
future research. 
Overall, a general finding of the evaluation was 
that there were many kinds of variations (includ- 
ing whether to include detailed medical information, 
whether to adopt a 'positive' or 'brutal '  tone, and 
how complex sentences hould be) which were not 
performed by STOP but might have increased leaflet 
effectiveness if they had been performed. These 
types of variations were either not observed at all 
in the think-aloud sessions, or were observed in ses- 
sions with some experts but not others. 
In terms of KA methodology, perhaps the key les- 
son is similar to the one from the sorting sessions; the 
think-aloud KA sessions were very useful in suggest- 
ing ideas and hypotheses about STOP content and 
phrasing rules, but we should have used other in- 
formation sources, such as smoker evaluations and 
small comparison experiments, to help refine and 
test these rules. 
3.3 Other  techniques  
Some of the other KA techniques we tried are briefly 
described below. These had less influence on the 
system than the sorting and think-aloud exercises 
described above. 
3.3.1 Exper t  Rev is ion  
We gave experts leaflets produced by the STOP 
system and asked them to critique and revise 
them. This was especially useful in suggesting local 
220 
changes, such as what phrases or sentences hould Paragraph from Nov 97 KA exercise: 
be used to communicate .a.particular~message._ For  Finally, .if :yotL.~.do: make: an. ~atter~pt.t0 =stop,  you 
example, an early version of the STOP system used 
the phrase there are lots of good reasons for stop- 
ping. One of the experts commented uring a-re- 
vision session that the phrasing should be changed 
to emphasise that the reasons listed (in this particu- 
lar section of the STOP leaflet) were ones the smoker 
himself had selected in the questionnaire he filled 
out. This eventually led to the revised wording It 
is encouraging that.you have_ many.good~ reasons/or : 
stopping, which is in the first paragraph of the ex- 
ample leaflet in Figure 2. 
Revision was less useful in suggesting larger 
changes to the system, and after the clinical trial 
was underway, one of our experts commented that 
he might have been able to suggest larger changes 
if we had explained the system's reasoning to him, 
instead of just giving him a leaflet o revise. In other 
words, just as we asked experts to 'think-aloud' as 
they wrote leaflets, in order to understand their rea- 
soning, it would be useful if we could give the experts 
something like the computer-system 'thinking aloud' 
as it produced a leaflet, so they could understand its 
reasoning. 
3.3.2 Group act iv i t ies  
Because xperts often disagreed, we tried a variety of 
activities where a group of experts either discussed 
or collaboratively authored a leaflet, in the hopes 
that this would help resolve or at least clarify con- 
flicting opinions. This seemed to work best when we 
asked two experts to collaborate, and was less sat- 
isfactory with larger groups. Several experts com- 
mented that the larger (that is, more than 2-3 peo- 
ple) group sessions would have benefited from more 
structure and perhaps a professional facilitator. 
3.3.3 Smoker  feedback 
As mentioned in Section 3.2, we showed several 
smokers the leaflet STOP produced for them, and 
asked them to comment on the leaflet. In addi- 
tion to its role as an evaluation exercise for other 
KA techniques, we hoped that these sessions would 
in themselves give us ideas for leaflet content and 
phrasing rules. This was again less successful than 
we had hoped. Part of the problem was the smokers 
knew very little about STOP (unlike our expeits, who" 
were all familiar with the project), and-often made 
comments which were not useful for improving the 
system, such as \[ did stop .for t0 -days til-my -daugh- 
ter threw a wobbly and then I wanted a cigarette and 
bought some and after smoking for over 30 years I've 
tried acupuncture and hypnosis all to no avail. 
We were also concerned that most of our com- 
ments came from well-educated and articulate smok- 
ers (for example, university students). It was harder 
to get feedback-from less well-educated smokers (for 
could consider using nicotine patches. For people 
like yourself who smoke 10-20 cigarettes per day, 
patches double your chances of success if you are 
determined to stop. You can get more information 
on patches from your local pharmacist or GP. 
Paragraph from Feb 99 KA exercise: 
You. smoke 1.1=20 .cigare?tes..a d y,.:and, smokeyour -  
first cigarette within 30 minutes of waking. These 
facts suggest you are moderately addicted to nico- 
tine, and so you might get withdrawal symptoms for 
a time on stopping. It would be worth considering 
using nicotine patches when you stop; these double 
the chances of success for moderately heavy smokers 
such as yourself who make a determined attempt o 
stop smoking. Your pharmacist or GP can give you 
more information about this. 
Figure 1: Paragraphs written by the same doctor for 
the same smoker in different KA exercises 
example, single mothers living in public housing es- 
tates). This led to the worry that the feedback we 
were getting was not representative of the popula- 
tion of smokers as a whole. 
3.4 KA  and the Smoker  Quest ionna i re  
KA sessions also effected the smoker questionnaire 
(STOP'S input) as well as the text-generation com- 
ponent of the system. We started with an initial 
questionnaire which was largely based on a liter- 
ature review of previous projects, and then modi- 
fied it based on the information that experts used in 
KA sessions. For example, the original questionnaire 
asked people who had tried to quit before what quit- 
ting techniques they had used in their previous at- 
tempts. However, in KA sessions the experts eemed 
primarily interested in previous experiences with one 
particular technique, nicotine replacement (nicotine 
patches or gum); so we replaced the general question 
about previous quitting techniques with two ques- 
tions whichfocused on experiences with nicotine re- 
placement. 
..... 4 . .S tab i l i ty  o f  Knowledge  
In order to determine how stable the results of NiX 
sessions were, we asked one of our doctors to repeat 
in February 1999 a think-aloud exercise which he 
had originally done in November 1997. This exer- 
cise required examining and writing letters for two 
smokers. The letters and accompanying think-aloud 
from the 1999 exercise were somewhat different from 
221 
the letters and think-aloud from the 1997 exercise; in 
very general terms, the 19991etters_hadsimilar. co e 
content, but expressed the information differently, in 
a perhaps (this is very difficult to objectively mea- 
sure) more 'empathetic' manner. An extract from 
this experiment is shown in Figure 1. 
We asked a group of seven smokers to compare 
one of the 1999 letters with the corresponding letter 
from the 1997 exercise. Five preferred the 1999 let- 
ter; one preferred the 1997 letter; one thought both 
were similar. Written comments from the smokers 
suggested that they found the'1999 letter ~riendlier 
and more understanding than the 1997 letter. 
In summary, it appears that our experts may 
themselves have been learning how to write effec- 
tive smoking-cessation leaflets during the course of 
the STOP project. In retrospect this is perhaps not 
surprising iven that none of them had written such 
leaflets before the project started. 
5 Eva luat ion o f  KA 
An issue that arose several times during the project 
was whether we could formally evaluate the effec- 
tiveness of KA techniques, in an analogous way to 
the manner in which we formally evaluated the ef- 
fectiveness of STOP in a clinical trial which com- 
pared smoking-cessation rates in STOP and non- 
STOP groups. Unfortunately, it was not clear to us 
how to do this; how can one evaluate a develop- 
ment methodology such as KA? In principle, perhaps 
it might be possible to ask two groups to develop 
the same system, one using KA and one not, and 
then compare the effectiveness of the resultant sys- 
tems (perhaps using a STOP-like clinical trial), and 
also engineering issues such as development cost and 
time-to-completion. This would be an expensive n- 
deavour, however, as it would be necessary to pay 
for two development efforts. Also, the size of a clin- 
ical trial depends on the size of the effect it is trying 
to validate, and a clinical trial which compared (for 
example) the effectiveness of two kinds of computer- 
generated smoking-cessation leaflet might need to 
be substantially larger (and hence more expensive) 
than a clinical trial that tested the effectiveness of a 
computer-generated l aflet against a no-leaflet con- 
trol group. 
An even more fundamental problem is that in or- 
der for such an experiment to produce meaningful re- 
sults, it would be necessary to control for differences 
-in skill, expertise, enthusiasm.,-and \]suck between ~the 
development teams. It might be necessary to re- 
peat this exercise several times, perhaps randomly 
choosing which development team will use KA and 
which will not. Of course, repeating the experiment 
N times will increase the total cost by a factor of N. 
As we did not have the resources to do the above, 
we elected instead to focus on the smaller 'informal' 
evaluations described above. We also conducted a 
? small. :experiment where we asked, a"~gr~ottp :of-five 
smoking-cessation counsellors to compare lea/lets 
produced by an early prototype of STOP with leaflets 
produced by the system used in the  clinical trial. 
60% of the counsellors thought he clinical trial sys- 
tem's leaflets were more likely to be effective, with 
the other 40% thinking the two systems produced 
letters of equal effectiveness. This suggests (al- 
though does not prove) that the development ef- 
fort behind the .clinical_ trial system .improved leaflet 
effectiveness. However, we cannot deterrnifie how 
much of the improvement was due to KA and how 
much was due to other development activities. 
6 Cur rent  Work  
We are currently in the process of analysing the re- 
sults of the clinical trial (which we cannot discuss in 
this paper), to see if it sheds any light on the effec- 
tiveness of KA. This is not straightforward because 
the clinical trial was not designed to give feedback 
about KA, but there nevertheless seem to be some 
useful lessons here, which we hope to report in sub- 
sequent publications. 
We also are applying the KA techniques used in 
STOP to a project in a different domain, to see how 
domain-dependent our findings are. A first attempt 
to do this, in a domain which involved giving advice 
to university students, failed because the relevant 
expert, who initially seemed very enthusiastic, did 
not give us enough time for KA. This highlights the 
practical observation that KA requires a substantial 
amount of time from the expert(s), who must either 
be paid or otherwise motivated to participate in the 
sessions. In this case we could not pay the expert, 
but instead tried to motivate him by pointing out 
that a successful system would be useful to him in 
his job; this was not in the end sufficient motivation 
to get the expert to make time for KA in his (busy) 
schedule. 
After the above failure we switched to another do- 
main, giving feedback to adults who are taking basic- 
literacy courses. In this domain, we are working with 
a company, Cambridge Training and Development, 
which is paying experts for their time when appro- 
priate. This work is currently in progress. One inter- 
esting KA idea which has already emerged from this 
work is observing tutors working with students (we 
did not in STOP observe doctors discussing smok- 
ing with-their.patients); this~is.similar to the ethno- 
graphic techniques uggested by Forsythe (1995). 
7 Conc lus ion  
The expert system community believes that it is 
worth interacting with experts using structured KA 
techniques, instead of just informally chatting to 
them or non-interactively studying what they do (as 
222 
happens in a traditional corpus analysis). We be- 
lieve 'structured KA techniques ran also:he, useful in 
developing NLG systems, but they are not a panacea 
and need to be used with some caution. 
In retrospect, KA was probably most effective in 
STOP when used as a source of hypotheses about 
smoker categories, detailed content rules, the phras- 
ing of messages, and so forth. But ideally these hy- 
potheses hould have been tested and refined using 
statistical data about smokers and small-scale val- 
uation exercises . . . . .  :.- : ., 
Of course, a key problem in STOP was that we 
were trying to produce texts (personalised smoking- 
cessation leaflets) which were not currently produced 
by humans; and hence there were perhaps no real hu- 
man experts on producing STOP texts. It would be 
interesting to see if structured KA techniques were 
more effective for developing systems which pro- 
duced texts that humans do currently write, such 
as weather forecasts and instructional texts. 
Acknowledgements 
Many thanks to James Friend, Scott Lennox, Mar- 
tin Pucchi, Margaret Taylor, and all of the other 
experts who worked with us. Thanks also to Yaji 
Sripada and the anonymous reviewers for their very 
helpful comments. This research was supported 
by the Scottish Office Department of Health un- 
der grant K/OPR/2/2/D318,  and the Engineering 
and Physical Sciences Research Council under grant 
GR/L48812. 
References  
Diana Forsythe. 1995. Using ethnography in the 
design of an explanation system. Expert Systems 
with Applications, 8(4):403-417. 
Eli Goldberg, Norbert Driedger, and Richard Kit- 
tredge. 1994. Using natural-language process- 
ing to produce weather forecasts. IEEE Expert, 
9(2):45-53. 
Benoit Lavoie, Owen Rainbow, and Ehud Re- 
iter. 1997. Customizable descriptions of object- 
oriented models. In Proceedings of the Fifth Con- 
ference on Applied Natural-Language Processing 
(ANLP-1997), pages 253-256. 
Kathleen McKeown, Karen Kukich, and James 
Shaw. 1994. Practical issues in automatic doc- 
ument generation. In Proceedings of the Fourth 
, Conference on Applied Natural-Language Process- 
in9 (ANLP-1994), pages '7-14. 
Jon Oberlander, Mick O'Donnell, Alistair Knott, 
and Chris Mellish. 1998. Conversation i the mu- 
seum: experiments in dynamic hypermedia with 
the intelligent labelling explorer. New Review of 
Hypermedia nd Multimedia, 4:11-32. 
James Prochaska and Carlo diClemente. 1992. 
Stages of Change in the Modification of Problem 
? Beh'aviors. Sage. 
Ehud Reiter and Robert Dale. 2000. Building Nat- 
ural Language Generation Systems. Cambridge 
University Press. 
Ehud Reiter, Alison Cawsey, Liesl Osman, and 
Yvonne Roff. 1997. Knowledge acquisition for 
content selection. In Proceedings of the Sixth Eu- 
ropean Workshop on Natural Language Genera- 
tion, pages 117-126, Duisberg, Germany. 
.Ehud,  Reiter~ :.Roma .Robertson,~ and Liesl Os- 
man. 1999. Types of knowledge required to per- 
sonalise smoking cessation letters. In Werner 
Horn et al, editors, Artificial Intelligence and 
Medicine: Proceedings of AIMDM-1999, pages 
389-399. Springer-Verlag. 
Ehud Reiter. 2000. Pipelines and size constraints. 
Computational Linguistics. Forthcoming. 
A. Carlisle Scott, Jan Clayton, and Elizabeth Gib- 
son. 1991. A Practical Guide to Knowledge Ac- 
quisition. Addison-Wesley. 
" 3 z2~ 
m 
*,~, ; ?;:~ 
~ : 
E B  
0 
E 
m m  
0 
E 
.C~.. 
~.~ ,~ 
_~ ~8 8 BE= o=~_- 
?E  8.~o 
~- ,~ ~ = ~ o ~  ~_~ o_m 
"-- . ; : 3 0 3 3  ~ o~~ o~Eo~o 
__ .~ ~ ~,~ >,.~_ 00 
0 
> 
"e) o 
-~o  
= 30. -  -~ t,/~ I--. e .~  oo- , - - -  
? ~o  ~_  . ~  ~ . E ~  o~. . . .=_a .o  ' o~ 
",,,,I2, E ,  . . i : ,E :  . "  ' : ? ">  "(D" "~0'-~ " "~ 'I~311"\[-" "O~ ~> E E '  '~  ~ >,~ .T:: ~ ? "~ ? ~. - . -~m o =o 
e"~ = ? - - ' . o  ._o E~ ~- ,-., 
~- 0 9 O. .C:  ..: ~ e E ? 
.~_ E ~ 0 :~., 0 0 -E  E L.. ~ ~ ~" ~- ~ c: 
~ 0 ~.u ,~.-, ~ -ID 0 -0  '~  (D E ~ ::3 '~ o .~E.c  o 
o E c >,~.  >,~0 E~ o-o  = mE = ~0 =08. ~ ._O~o_O?= 
,~.~ 0 o '~ =~'~ ~ ~' -  o o..~ ~ ~= 
~_~ o_o '~ o 
? ~ ,oE~.o  ~ ~ = - -  . . . . .  0 
. -~ .^~o~ ~o,?~'~ ~ ? ~ = 
o ~o ? o "e~=~.?:~ ~,e ,o -~=~ = ~-  ~ 
T~8 @ @ ~ = E~O O "-- 
..E O O ~ ~ 
=Oo ~,  5~o-  _~ ' -  - _.m E ~ ~  = .~ 
X 0 ~Y ,..~. >., >'~, > 0 
"-- c0 -~  >' .0 -~ 0 0 0  E 0 
d.o .~ c~ ~ x ~  
.~ ~ ~ E~o~=-  ~ ~ 
.~_.~_ >,F= o .~_ F=' F'~" . ~ _
: 0~-  ~ 
? ~ ~ . 
Noo  
~ . ~  .... 
0 L~ 9_~a D ~ E' E.-~ ~ > 0 
0 o ~ _~E '~-  ~ "0 
~ O O CO > oE~ . -~  >.~ 
~>~.~ = , ~  o 
-oo  tO 
3 ~. .E  J3 
0 o ~ -o ~ = ~- O 
0 E >-  0_>.~ >-  
.= ~ 
O O ~ O_ :3 
~ ~ o 
~~ ~ ~ ~-~- 
' ~ "0  0 I:: ..~ 
? -~ ~ 
o &o  _ 8 ~- 
"0---- ~ 0 >,  
- ? ?~o ~ 
~.  .~  
? ~ O. .0  0..  
0 O3 E -~ 0"0  ~ ? 0 0 69  
~ ~..  ~ :::3 >., O ~ ~ ~ 
O > > EL"~ 
0 '.u ,r- .,C ..,~. 
~-  = 
Figure 2: Inside pages of an examplo STOP leaflet 
224 
A Two-stage Model for Content Determination
Somayajulu G.
Sripada
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
ssripada@csd.
abdn.ac.uk
Ehud Reiter
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
ereiter@csd.abdn
.ac.uk
Jim Hunter
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
jhunter@csd.abdn
.ac.uk
Jin Yu
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
jyu@csd.abdn.ac
.uk
Abstract
In this paper we describe a two-stage
model for content determination in
systems that summarise time series
data. The first stage involves building a
qualitative overview of the data set, and
the second involves using this
overview, together with the actual data,
to produce summaries of the time-
series data.  This model is based on our
observations of how human experts
summarise time-series data.
1  Introduction
This paper addresses the problem of content
determination in data summarisation. Content
determination as the name indicates is the
process responsible for determining the content
of the texts generated by an NLG system (Reiter
and Dale 2000).  Although content-
determination is probably the most important
part of an NLG system from the end-user's
perspective, there is little agreement in the NLG
community as to how content-determination
should be done, with different systems adapting
widely varying approaches.  Also, algorithms
and architectures for content-determination
seem to often be based on the intuitions of
system developers, instead of on empirical
observations, although detailed content
determination rules are often based on corpus
analysis and interaction with experts.
In this paper we propose a general architecture
for content determination in data summarisation
systems which assumes that content
determination happens in two stages: first a
qualitative overview of the data is formed, and
second the content of the actual summaries is
decided upon.  This model is based on extensive
knowledge acquisition (KA) activies that we
have carried out in the SUMTIME  project
(Sripada, 2001), and also matches observations
made during KA activities carried out in the
STOP  project (Reiter et al 2000).  We have not
yet implemented this model, and indeed one of
the issues that we need to think about is to what
degree a content-determination strategy used by
human experts is also an appropriate one for a
computer NLG system.
2 Content Determination
Content determination is the task of deciding on
the information content of a generated text.  In
the three-stage pipeline model of Reiter and
Dale (2000), content determination is part of the
first stage, document planning, along with
document structuring (determining the textual
and rhetorical structure of a text).  Content
determination is extremely important to end
users; in most applications users probably prefer
a text which poorly expresses appropriate
content to a text which nicely expresses
inappropriate content.  From a theoretical
perspective content determination should
probably be based on deep reasoning about the
system's communicative goal, the user's
intentions, and the current context (Allen and
Perrault 1980), but this requires an enormous
amount of knowledge and reasoning, and is
difficult to do robustly in real applications.
In recent years many new content determination
strategies have been proposed, ranging from the
use of sophisticated signal-processing
techniques (Boyd 1997) to complex planning
algorithms (Mittal et al 1998) to systems which
exploit cognitive models of the user (Fiedler
1998).  However, most of these strategies have
only been demonstrated in one application.
Furthermore, as far as we can tell these
strategies are usually based on the intuition and
experiences of the developers.  While
realisation, microplanning, and document
structuring techniques are increasingly based on
analyses of how humans perform these tasks
(including corpus analysis, psycholinguistic
studies, and KA activities), most papers on
content determination make little reference to
how human experts determine the content of a
text.  Human experts are often consulted with
regard to the details of content rules, especially
when schemas are used for content
determination (Goldberg et al 1994, McKeown
et al 1994, Reiter et al 2000); but they rarely
seem to be consulted (as far as we can tell) when
deciding on the general algorithm or strategy to
use for content determination.
3  Summarising Time-Series Data
3.1  Text summaries of Time-Series Data
Time-series data is a collection of values of a set
of parameters over time.  Such data is very
common in the modern world, with its
proliferation of databases and sensors, and
humans frequently need to examine and make
inferences from time-series data.
Currently, human examination of time-series
data is generally done either by direct inspection
of the data (for small data sets), by graphical
visualisation, or by statistical analyses.
However, in some cases textual summaries of
time-series data are also useful.  For example,
newspapers regularly publish textual summaries
of weather predictions, the results of polls and
surveys, and stock market activity, instead of
just showing numbers and graphs.  This may be
because graphical depictions of time-series data
require time and skill to interpret, which is not
always available.   A doctor rushing to the side
of a patient who is suffering from a heart attack,
for example, may not have time to examine a set
of graphs of time-series data, and a newspaper
reader may not have the statistical knowledge
necessary to interpret raw poll results.
Perhaps the major problem today with textual
descriptions of time-series data is that they must
be produced manually, which makes them
expensive and also means they can not be
produced instantly.  Graphical depictions of
data, in contrast, can be produced quickly and
cheaply using off-the-shelf computer software;
this may be one reason why they are so popular.
If textual summaries of time-series data could be
automatically produced by software as cheaply
and as quickly as graphical depictions, then they
might be more widely used.
3.2 SUMTIME
The goal of the SUMTIME  project is to develop
better techniques for automatically generating
textual summaries of time-series data, in part by
integrating leading-edge NLG and time-series
analysis technology.  We are currently focusing
on two domains:
Meteorology ? producing weather forecasts
from numerical weather simulations.  This work
is done in collaboration with Weather News Inc
(WNI)/Oceanroutes, a leading meteorological
company.
Gas Turbines  ? summarising sensor readings
from a gas turbine.  This work is done in
collaboration with Intelligent Applications, a
leading developer of monitoring software for
gas turbines.
These domains are quite different in time-series
terms, not least in the size of the data set. A
typical weather forecast is based on tens of
values for tens of parameters, while a summary
of gas-turbine sensor readings may be based on
tens of thousands of values for hundreds of
parameters.  We hope that looking at such
different domains will help ensure that our
results are generalisable and not domain-
specific.  We will start working on a third
domain in 2002; this is likely to be a medical
one, perhaps (although this is not definite)
summarising sensor readings in neonatal
intensive care units.
The first year of SUMTIME  (which started in
April 2000) has mostly been devoted to
knowledge acquisition, that is to trying to
understand how human experts summarise time-
series data.  This was done using various
techniques, including corpus analysis,
observation of experts writing texts, analysis of
content rules suggested by experts, discussion
with experts, and think-aloud sessions, where
experts ?think aloud? while writing texts
(Sripada, 2001).
3.3  Example
The following table shows an example segment
of meteorological time series data, specifically
predicted wind speed and wind direction at an
offshore oil exploration site. The time field is
shown in ?day/hour? format.
Time
(day/hour)
Wind
Direction
Wind
Speed
Knots
05/06 SE 22
05/09 SE 24
05/12 SE 30
05/15 SE 25
05/18 SSE 28
05/21 SSE 22
06/00 SE 16
This data was summarised by WNI's human
forecasters as follows:
FORECAST 06-24 GMT,FRIDAY,05-Jan
2001
WIND(KTS)      CONF   HIGH
  10M: SE 20-25 OCCASIONALLY
       25-30, EASING 15-20 LATER
The above example is just a sample showing the
data and its corresponding forecast text for the
wind subsystem. Real weather forecast reports
are much longer and are produced from data
involving many more weather parameters than
just wind speed and wind direction.
4 Human Summarisation
4.1 Meteorology
In the domain of weather forecasting, we
observed how human experts carry out the task
of summarising weather data by video recording
a meteorologist thinking aloud while writing
weather forecasts. Details of the KA have been
described in Sripada (2001). Our observations
included the following:
1. In the case of weather forecasts, time-series
data represent the values of important
weather parameters (wind speed, direction,
temperature, rainfall), which collectively
describe a single system, the weather. It
seemed as though the expert was
constructing a mental picture of their source
using the significant patterns in time series.
Thus the first activity is that of data
interpretation to obtain a mental model of
weather.
2. The mental model of the weather is mostly
in terms of the elements/objects related to
atmosphere, like cold fronts and warm
fronts; it also seems to be qualitative instead
of numerical. In other words, it qualitatively
describes the meteorological state of the
atmosphere. The expert calls this an
?overview of the weather?.
3. Building the overview involves the task of
interpretation of the time series weather
data. While interpreting this data the expert
used his meteorological knowledge (which
includes his personal experience in
interpreting weather data) to arrive at an
overview of the weather. During this phase,
he appeared to be unconcerned about the
end user of the overview (see 4.1.1 below).
We call this process Domain Problem
Solving (DPS) where information is
processed using exclusively the domain
knowledge.
4. Forecasts are written after the forecaster gets
a clear mental picture (overview) of the
weather. Building the overview from the
data is an objective process which does not
depend on the forecast client (user), whereas
writing the forecast is subjective and varies
with client.
4.1.1  Examples
Two examples of the influence of the overview
on wind texts (Section 3.3) are:
1.  When very cold air flows over a warm sea,
surface winds may be underestimated by the
numerical weather model.  In such cases the
forecaster uses his ?overview of the weather?
to increase wind speeds and also perhaps
add other instability features to the forecast
such as squalls.
2.  If the data contains an outlier, such as a
wind direction which is always N except for
one time period in which it is NE, then the
expert uses the overview to decide if the
outlier is meteorologically plausible and
hence should be reported or if it is likely to
be an artefact of the simulation and hence
should not be reported.
The above examples involve reasoning about the
weather system.  Forecasters also consider user
goals and tasks, but this may be less affected by
the overview.  For example, in one think-aloud
session, the forecaster decided to use the phrase
20-24 to describe wind speed when the data file
predicted a wind speed of 19kt.  He explained to
us that he did this because he knew that oil-rig
staff used different operational procedures (for
example for docking supply boats) when the
wind exceeded 20kt, and he also knew that even
if the average wind speed in the period was
19kt, the actual speed was going to vary minute
by minute and often be above 20kt.  Hence he
decided to send a clear signal to the rig staff that
they should expect to use ?20kt or higher?
procedures, by predicting a wind speed of 20-24.
This reasoning about the user took place after
the overview had been created, and did not seem
to involve the overview.
4.2 Gas Turbine Sensors
Unlike the previous domain, in the domain of
gas turbine (GT), currently there are no textual
summaries of turbine data written by humans.
Thus we have asked the domain experts to
comment orally on the data. However, the
experts have attempted to summarise their
comments at the end of each session if they
found something worth summarising. Our
observations included:
1. The main task is to identify the abnormal
data and summarise it. However, an
abnormal trend in a specific channel might
have been caused due to a change in another
channel (for instance, an increase in the
output voltage can be explained with a
corresponding increase in the fuel input).
Thus individual channel data needs to be
interpreted in the context of the other
channels.
2. The expert agrees during the KA session
that he first analyses the data numerically to
obtain  qualitative trends relating to the GT
before generating comments. Therefore the
state of the GT that produced the data is
constructed through data interpretation and
the knowledge of the state is then used to
check if the turbine is in a healthy state or
not. Since GT is an artefact created by
humans it is possible to have a fairly
accurate model of states of a GT (unlike
weather!).
3. The phrases used by the expert often express
the trends in the data as if they were
physical happenings on the turbine, like
?running down? for a decreasing trend in
shaft speed data. This indicates that the
expert is merely expressing the state of the
GT. This in turn indicates that at the time
the summarisation is done, the mental model
of the state of the GT is available.
5 Evidence from Other Projects
After making the above observations, we
examined think-aloud transcripts from an earlier
project at the University of Aberdeen, STOP
(Reiter et al 2000), which involved building an
NLG system that produced smoking-cessation
letters from smoking questionnaires.   These
transcripts (from think-aloud sessions of doctors
and other health professionals manually writing
smoking-cessation letters) showed that in this
domain as well experts would usually first build
an overview (in this case, of the smoker) before
starting to determine the detailed content of a
letter. Below is an excerpt from one of the
transcripts of a KA session :
?   ?. The first thing I have got to do is to read
through the questionaire just to get some idea of
where he is at with his smoking. ?? ?
We did not investigate overview formation in
any detail in STOP, but the issue did come up
once in a general discussion with a doctor about
the think-aloud process.  This particular doctor
said that he built in his mind a mental image of
the smoker (including a guess at what he or she
looked like), and that he found this image very
useful in deciding how best to communicate
with the smoker.
In another work, RajuGuide, once again there is
evidence of an overview influencing content
determination (Sripada 1997). RajuGuide is a
system that generates route descriptions. At a
higer level of abstraction, RajuGuide has two
parts. The first part is responsible for planning
the route the user wanted. The second module is
responsible for generating the text describing the
route. The route computed by the first part,
which is in the form of a series of coordinates, is
not directly communicated to the user. Instead
the second part attempts to enrich the route
depending upon what the user already knows
and what additional information the knowledge
base knows for that particular route. We believe
that the route computed by the route planner is
the overview in this case and it drives the
content determination process in the second part.
6 Two-stage Model for content
determination
These observations have led us to make the
following hypotheses:
1. Humans form a qualitative overview of the
input data set.
2. Not all the information in the overview is
used in the text.
3. The overview is not dependent on pragmatic
factors such as the user?s taste, these are
considered at a later stage of the content
determination process.
Based on the above hypotheses, we propose a
two-stage model for content determination as
depicted in Figure 1. It is assumed that Domain
Data Source (DDS) is external to the text
generator. It has been assumed that a Domain
Problem Solver or Domain Reasoner (DR) is
available for data processing. This reasoning
module is essentially useful to draw inferences
while interpreting the input data set and
ultimately is responsible for generating the
overview. Communication Goal (CG) is the
input to the data summarisation system in
response to which it accesses DDS to produce
an overview of the data using the DR. In the
context of the overview produced by DR, the
Data Comprehension
Goal (Derived from
Comm. Goal)
Domain
Data Source
(DDS)
Domain
Reasoner (DR)
Data
Overview Communication
Reasoner (CR)
Final
Content
Communication
Goal
Constraints
due to User
Constraints due to other
pragmatic factors
Figure 1. Two stage model for content determination
Communication Reasoner (CR) system
generates the final content specification taking
into account the influence of the User
Constraints (UC) and other pragmatic factors.
This content is then sent to subsequent NLG
modules (not shown), such as microplanning
and surface realisation.
Our model has some similarities to the one
proposed by Barzilay et al (1998), in that the
Domain Reasoner uses general domain
knowledge similar to their RDK, while the
Communication Reasoner uses communication
knowledge similar to their CDK and DCK.
The central feature of the above model is the
idea of data overview and its effect on content
selection. One possible use of overviews is to
trigger context-dependent content rules.  The
time-series analysis part of SUMTIME  is largely
based on Shahar's model (1997), which makes
heavy use of such rules.  In Shahar's model
contexts are inferred by separate mechanisms;
we believe that these should be incorporated into
the overview, but this needs further
investigation.
At the current stage of our project we have only
a gross idea of what makes up the proposed data
overview. Our suspicion is that it is hard to
make a generic definition of the data overview
for all domains. Instead, we would like to
imagine the data overview as the result of
inferences made from the input data so as to
help in triggering the right content determination
rules. For example, in out meteorology domain,
the input time-series data comes from a
numerical weather prediction (NWP) model, but
even the most sophisticated NWP models do not
fully represent the real atmosphere ? all models
work with approximations. Thus the NWP data
displayed to the meteorologist is interpreted by
him to arrive at a conceptual model in his or her
head, which is the overview.
7  Issues with the two stage model
There are a number of issues that need to be
resolved with respect to the two-stage model
described above.
7.1 Is overview creation a human
artefact?
The main basis for including the overview in
two stage model has been the observation made
during the think aloud sessions that experts form
overviews before writing texts. Now it can be
argued that even if humans need an overview,
computer programs may not. Evidently, it is
hard to ever prove the contrary. But what can be
done is to show the advantages gained by a
computer program by using an overview for
content selection.
7.2 Does the overview have any other
utility than just providing context for
content determination rules?
We believe that the overview can play multiple
roles in the overall process of writing textual
forecasts. First, the overview can bring in
additional information into the text that is not
directly present in the underlying raw data. In
Reiter and Dale's (2000) terminology, overviews
are a technique for generating Computable Data
content, that is content which is not directly
present in the input data but can be computed or
inferred from it.  Such content provides much of
the value of summary texts.  Indeed, one could
argue that simple textual descriptions of a set of
data values without extra computed or inferred
content, such as those produced by TREND
(Boyd, 1997), might not be that much more
useful than a graph of the data.
The overview may also help in deciding how
reliable the input data is, which is especially
important in the meteorology domain, since the
data comes from an NWP simulation. This
could, for example, help the generation system
decide whether to use precise temporal terms
such as Midnight  or vague temporal terms such
as tonight. Again one could argue that the ability
to convey such uncertainty and reliability
information to a non-specialist is a key
advantage of textual summaries over graphs.
In general, the overview allows reasoning to be
carried out on the raw data and this will
probably be useful in many ways.
7.3  How is the overview related to the
domain ontology?
The basic concepts present in an overview may
be quite different from the basic concepts
present in a written text.  For example, the
overview built by our expert meteorologist was
based on concepts such as lapse rate (the rate at
which temperature varies with height),
movement of air masses, and atmospheric
stability.   However, the texts he wrote
mentioned none of these, instead it talked about
wind speed, wind direction, and showers.  In the
STOP  domain, overviews created by doctors
seemed to often contain many qualitative
psychological attributes (depression, self-
confidence, motivation to quit, etc) which were
not explicitly mentioned in the actual texts
written by the doctors.
This suggests that the conceptual ontology, that
is the specification of underlying concepts,
underlying the overview may be quite different
from the ontology underlying the actual texts.
The overview ontology includes concepts used
by experts when reasoning about a domain (such
as air masses or motivation), while the text
ontology includes concepts useful for
communicating information to the end user
(such as wind speed, or longer life expectancy).
7.4  What do experts think about the two-
stage model?
When the two stage model was reported back to
a WNI expert who participated in a think-aloud
session, the expert agreed that he does build an
overview (as he did during the KA session)
while writing forecasts, but felt that it?s use may
not be necessary for writing all forecasts. In his
opinion, the interpretation of most data sets
doesn?t require the use of the overview.
However, he was quick to add that the quality of
the forecasts can be improved by using
overviews which faciliate reasoning with the
weather data.
8  Evaluation
We are currently building a testbed system
called SUMTIME-MOUSAM  which will enable us
to test the hypotheses we have presented in this
paper and other hypotheses suggested by our
KA activities. SUMTIME-MOUSAM  is a
framework system that consists of
? "Infrastructure" software for accessing data
files, regression testing of new software
versions, etc.
? An ontology, which defines a conceptual
level of representation of texts.
? A corpus of human-written texts with their
corresponding conceptual representations
defined using the above ontology.
? Scoring software which compares the output
of a module (either at a conceptual or text
level) against the human corpus.
Because we are primarily interested in content
issues, it is important to evaluate our system at a
content level as well as at a text level.  To
support this, we are developing conceptual
representations of the texts we will be
generating, which can also be extracted from
human texts by manual analysis.
SUMTIME-MOUSAM  is currently best developed
in the area of producing wind texts.  In this area,
we have developed a conceptual representation
and manual annotation guide (with good inter-
annotator agreement, generally kappa values of
.9 or higher); built an initial software system to
automatically produce such texts based on a
threshold model without an overview; and
begun the process of analysing differences.  We
are currently working on extending SUMTIME-
MOUSAM  to other parts of weather forecasts,
such as statements describing clouds and
precipitation, and plan in the future to extend it
to the gas-turbine domain.
With regard to testing hypotheses specifically
about two-stage content determination (the
subject of this paper), our plan is as follows
1. Compare the output of the non-overview
based software to human summary texts,
and identify cases where an overview seems
to be used.
2. Ask human experts to build an overview
(using a GUI), modify our software to use
this overview when generating texts, and see
if this results in texts more similar to the
human texts.
3.  Attempt to automatically generate the
overview from the data, and again compare
the resultant texts to human texts.
At some point towards the end of SUMTIME, we
also hope to conduct user task evaluations.  For
example, we may show gas-turbine engineers
our summary texts and see if this helps them
detect problems in the gas turbine.
9  Conclusion
Our experience in three domains shows that
human experts build qualitative overviews when
writing texts, and that these overviews are used
by the experts for inference and to provide a
context for specific content rules.  We believe
that overviews could also be very useful in
computer NLG systems, and are currently
working on testing this hypothesis, as part of the
SUMTIME  project.
Acknowledgements
Many thanks to our collaborators at
WNI/Oceanroutes and Intelligent Applications,
especially Ian Davy, Dave Selway, Rob Milne,
and Jon Aylett; this work would not be possible
without them!  Thanks also to Sandra Williams
and the anonymous reviewers for their
comments on a draft of this paper.  This project
is supported by the UK Engineering and
Physical Sciences Research Council (EPSRC),
under grant GR/M76881.
References
Allen J. and Perrault C. R. (1980).  Analyzing
Intention in Utterances.  Artificial Intelligence,
26:1-33.
Barzilay R, McCullough D, Rambow O,
DeChristofaro J, Korelsky T, and Lavoie B (1998)
A New Approach to Expert System Explanations,
In Proceedings of INLG-1998,  pages 78-87.
Boyd S (1997). Detecting and Describing Patterns in
Time-varying Data Using Wavelets.  In Advances
in Intelligent Data Analysis: Reasoning About
Data, X Lui and P Cohen (Eds.), Lecture Notes in
Computer Science 1280, Springer Verlag.
Fiedler A (1998). Macroplanning with a Cognitive
Architecture for the Adaptive Explanation of
Proofs. In Proceedings of INLG-1998, pp 88-97.
Goldberg E, N Driedger and RL Kittredge (1994),
Using Natural-Language Processing to Produce
Weather Forecasts,  IEEE Expert,  9, 2, pp 45-53.
McKeown K, Kukich K, Shaw J (1994).  Practical
Issues in Automatic Document Generation.  In
Proceedings of ANLP-1994,  pp 7-14.
Mittal V, Moore J, Carenini G, and Roth S (1998).
Describing Complex Charts in Natural Language:
A Caption Generation System. Computational
Linguistics 24: 431-467.
Reiter E. and Dale R. (2000)  Building Natural
Language Generation Systems. Cambridge
University Press.
Reiter E., Robertson R. and Osman L. (2000)
Knowledge Acquisition for Natural Language
Generation.  In Proceedings of the First
International Conference on Natural Language
Generation (INLG-2000), 217-224 pp.
Shahar Y (1997), ?Framework for Knowledge-Based
Temporal Abstraction?, Artificial Intelligence
90:79-133..
Sripada S. G. (1997) Communicating Plans in
Natural Language: Planning and Realisation.
PhD Thesis,  Indian Institute of Technology,
Madras, India.
Sripada S. G. (2001) SUMTIME: Observations from
KA for Weather Domain.  Technical Report,
Computing Science Dept. Univ of Aberdeen,
Aberdeen AB24 3UE, UK. Awaiting approval
from industrial collaborators.
Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data
Corpus
Ehud Reiter
Department of Computing Science
University of Aberdeen
ereiter@csd.abdn.ac.uk
Somayajulu Sripada
Department of Computing Science
University of Aberdeen
ssripada@csd.abdn.ac.uk
Abstract
We present an empirical corpus study of the
meaning and usage of time phrases in weather
forecasts; this is based on a novel corpus anal-
ysis technique where we align phrases from
the forecast text with data extracted from a nu-
merical weather simulation. Previous papers
have summarised this analysis and discussed
the substantial variations we discovered among
individual writers, which was perhaps our most
surprising finding. In this paper we describe
our analysis procedure and results in consid-
erably more detail, and also discuss our cur-
rent work on using parallel text-data corpora to
learn the meanings of other types of words.
1 Introduction
NLP systems that interact with the world often need mod-
els of what words mean in terms of the non-linguistic
world. In this paper, we describe how we have deter-
mined the meaning of time phrases in weather forecasts
by analysing a parallel corpus of (A) manually-written
weather forecast texts and (B) the numerical data (from a
weather simulation) that the human forecasters examined
when writing the textual forecasts. The analysis proce-
dure first aligns (associates) text fragments with data seg-
ments, and then infers the meaning of each time phrase by
statistically analysing the time of data segments that are
aligned to textual phrases that contain this time phrase.
This is broadly similar in concept to the use of parallel
multilingual corpora in machine translation (Brown et al,
1990), except that our parallel corpus consists of texts and
underlying numeric data, not texts and their translations.
In other words, we are trying to learn what words mean
in terms of non-linguistic data, not the best translations
of words in another language.
Probably the biggest surprise in our analysis was the
substantial variation we saw between individuals. For
example, by evening apparently meant 1800 to some peo-
ple, but 0000 to others. Although the possibility of such
variation in individual idiolects has been acknowledged
in the past (for example, (Nunberg, 1978; Parikh, 1994)),
it seems to be ignored by most recent work on lexical se-
mantics.
We have published other papers that have summarised
our key findings, notably variation between individu-
als (Reiter and Sripada, 2002a; Reiter and Sripada,
2002b); and also described the corpus itself (Sripada et
al., 2003b). The purpose of this paper is to describe
our analysis procedure (including alignment) and results
in detail, and to also discuss our current work on using
parallel text-data corpora to learn the meanings of other
types of words.
2 Previous Research
Linguists and lexicographers have used a number of dif-
ferent techniques to determine the meanings of words.
These include asking native-speaker informants to judge
the acceptability and oddness of test sentences (Cruse,
1986); defining word senses via lexicographic analysis
of citations and corpora (Landau, 1984); and asking sub-
jects to respond to ?fill in the blank? questions (Cassidy
and Hall, 1996). These techniques have focused purely
on texts, and have not analysed how texts and words re-
late to non-linguistic representations of the meanings of
a text, which is our focus.
Psychologists interested in categorisation have done
formal experiments to determine which objects human
subjects consider to be in a mental category (Rosch,
1978; Malt et al, 1999). If we assume that the mean-
ing of a word is one or more mental categories, then this
research has shed considerable light on what words mean.
However, like all psychological research, it has examined
language usage in an artificial experimental context, not
naturally occurring language.
In the NLP community, models of word meanings are
typically either entered by a user or developer (for exam-
ple in Microsoft?s English Query natural-language inter-
day hour wind dir wind speed
25-10-00 0 SSW 12
25-10-00 3 SSE 11
25-10-00 6 ESE 18
25-10-00 9 ESE 16
25-10-00 12 E 15
25-10-00 15 ENE 15
25-10-00 18 ENE 18
25-10-00 21 NNE 20
26-10-00 0 NNW 26
Table 1: Wind (at 10m) extract from 24-Oct-00 data file
face) or derived from a hand-built knowledge base (eg,
(Reiter, 1991)). There is growing interest in trying to
learn word meanings from parallel text-data corpora, for
example (Siskind, 2001; Barzilay and Lee, 2002; Roy,
2002). We believe our work is unusual because we are
using naturally occurring texts and data. Siskind (2001),
in contrast, used data which was explicitly created for his
experiments; Barzilay and Lee (2002) used texts which
subjects had written for a previous experiment; and Roy
(2002) used both data and texts that were created for his
experiments.
3 SumTime Project and Corpora
The SUMTIME project is investigating better technology
for building software systems that automatically gener-
ate textual summaries of time-series data. One of the
domains SUMTIME is working in is weather forecasts,
and in this domain we acquired a corpus of 1119 weather
forecasts (for off-shore oil rigs) written by five profes-
sional meteorologists (Sripada et al, 2002; Sripada et al,
2003b). The reports were primarily based on the output
of a numerical weather simulation, and our corpus con-
tains this information as well as the forecast texts. Each
forecast is roughly 400 words long, giving a total corpus
size of about 400,000 words. The forecasts are split into
an initial section which gives an overview of the weather,
and then additional sections which give detailed forecasts
for different periods of time. Figure 1 shows an example
extract from a forecast text; this is the detailed descrip-
tion of predicted weather on 25 Oct 2000, from a forecast
issued at 3AM on 24 Oct 2000.
Much of our analysis has focused on statements de-
scribing predicted wind speed and direction at 10 meters
altitude during the first 72 hours after the forecast was is-
sued. In other words, the WIND(10M) field from the de-
tailed weather descriptions up to 3 days after the forecast
was issued. One reason for focusing on wind statements
is that they are based fairly directly on two fields from
the data files, predicted wind direction and speed; the re-
lationship between some of the other statements (such as
weather) and the data files is more complex. The pre-
dicted wind (at 10m) speed and direction on 25 Oct 2000,
from the 24 Oct 2000 data file, is shown in Table 1. This
is the primary information that the meteorologists looked
at when writing the wind statement in Figure 1, although
they also have access to other information sources, such
as satellite weather photographs.
Each forecast contains 3 such wind statements, with an
average length of approximately 10 words; hence there
are about 30,000 words in our wind-statement subcorpus.
This of course is very small compared to many text-only
corpora such as the British National Corpus (BNC), but
we believe that our weather forecast corpus is one of the
largest parallel text-data corpora in existence.
4 Analysis Procedure for Time Phrases
One of SUMTIME?s research goals is to learn the meaning
of time phrases; in other words, what a forecaster meant
when he used a time phrase such as by evening or af-
ter midnight. We also wished to learn which time phrase
should be included in a computer-generated weather fore-
cast text to indicate a time; for example, which time
phrase should be used to indicate a change in the weather
at 1200. Note that it is rare for weather forecasts to ex-
plicitly mention numerical times such as 1200, and also
that although there are standard terminologies for some
meteorological phenomena such as cloud cover and pre-
cipitation, we are not aware of any standard terminologies
for the use of time phrases in weather forecasts.
We performed this analysis as follows. First we ex-
tracted the wind at 10 meters statements for the next 72
hours from all forecasts in our corpus, and parsed these
texts with a simple parser tuned to the linguistic structure
of these texts. The parser essentially broke sentences up
into individual phrases, and then recorded the speed, di-
rection, and time phrase mentioned in each such phrase,
along with other information (such as verb) which was
not used in the analysis described here. For example the
WIND (10M) statement from Figure 1 was broken up by
the parser into four wind phrases:
1. SSW 12-16
(speed:12-16, direction:SSW, timephrase: none)
2. BACKING ESE 16-20 IN THE MORNING,
(speed:16-20, direction:ESE, timephrase: IN THE
MORNING)
3. BACKING NE EARLY AFTERNOON
(speed:(16-20), direction:NE, timephrase: EARLY
AFTERNOON)
4. THEN NNW 24-28 LATE EVENING
(speed:24-28, direction:NNW, timephrase: LATE
EVENING)
FORECAST 00-24 GMT, WEDNESDAY, 25-Oct 2000
WIND(10M): SSW 12-16 BACKING ESE 16-20 IN THE MORNING, BACKING
NE EARLY AFTERNOON THEN NNW 24-28 LATE EVENING
(50M): SSW 15-20 BACKING ESE 20-25 IN THE MORNING, BACKING
NE EARLY AFTERNOON THEN NNW 30-35 LATE EVENING
SIG WAVE: 2.0-2.5 RISING 3.0-3.5 BY AFTERNOON
MAX WAVE: 3.0-4.0 RISING 5.0-5.5 BY AFTERNOON
WEATHER: RAIN SOON, CLEARING TO SHOWERS IN THE EVENING
VIS: GOOD BECOMING MODERATE IN RAIN
Figure 1: Extract from 5-day forecast issued on 24-Oct-00
If a wind phrase did not specify speed or direction, the
parser assumed that this was unchanged from the pre-
vious wind phrase; such elision is common in weather
forecast texts. Thus, for example, the speed recorded for
BACKING NE EARLY AFTERNOON is 16-20, which
is the speed from the previous phrase (BACKING ESE
16-20 IN THE MORNING). Our parser successfully
parsed 3225 of the 3357 WIND(10M) statements; 132
(4%) of the statements could not be parsed. The parser
produced 8198 wind phrases in total.
From these 8198 wind phrases we selected those
phrases which (a) included a time phrase, (b) did not
use a qualifier such as mainly or occasionally, (c) did not
specify that wind speed or direction was variable, (d) for
which we had the corresponding data files, and (e) for
which we knew the forecast author. There were 3654
such phrases. The majority (4014 phrases) of the elim-
inated phrases did not specify a time phrase, such as the
first phrase (SSW 12-16) in the above example.
We next associated each wind phrase with an entry in
the corresponding data file. In other words, we aligned
the textual wind phrases with the numeric data file en-
tries. As in other uses of parallel corpora, good alignment
is essential in order for the results to be meaningful (Och
and Ney, 2000).
To associate data file entries with wind phrases, we
first searched the data file for entries which matched the
wind phrase. An entry matched if its speed was within the
range defined in the phrase, and if its direction was within
12 degrees of the direction mentioned in the phrase. In
343 cases, no data file entry matched the wind phrase. We
believe that such cases were mostly due to (a) forecasters
not literally reporting the data file, but instead adjusting
what they said based on their meteorological expertise
and on information not available to the numerical weather
simulation (such as satellite weather images); (b) fore-
casters reporting a simultaneous change in wind speed
and direction, when in fact speed and direction changed
at different times (this may be due to forecasters trying to
write texts quickly, so that they can use the most up-to-
date data (Reiter et al, 2003)); and (c) forecaster errors.
For example, the third phrase in our example, BACK-
ING NE EARLY AFTERNOON, does not match any of
the data file entries shown in Table 1. This could be be-
cause the forecaster decided that the numerical forecast
was underestimating the speed at which the wind was
shifting, and hence he believed that the wind would be
NE at 12 or 15, even though the data file predicted E and
ENE for these times. It could also be that the forecaster
made a mistake, and perhaps was intending to write ENE
but wrote NE instead because he was writing under time
pressure. In any case, wind phrases which did not match
any data file entries were dropped from our analysis.
Out of the 3311 matched wind phrases, 1434 (43%)
were unambiguous and only matched one data file en-
try. For example, the fourth wind phrase in our example,
THEN NNW 24-28 LATE EVENING, matches only one
data file entry, the one for 0000 on 26 Oct 2000.
1877 (57%) of the matched wind phrases were ambigu-
ous and matched more than one data file entry. Typically
this happened when the wind was changing slowly and
hence two or more adjacent data file entries matched the
wind phrase. In such cases we checked if one data file
entry had a speed which was was closer than the other
data files entries to the middle of the speed range in the
textual wind phrase. This heuristic produced a preferred
match for 1105 (33%) of the matched wind phrases, and
left 772 (23%) phrases as ambiguous and unmatched.
For example, the second wind phrase in our example,
BACKING ESE 16-20 IN THE MORNING, matches two
data file entries: 0600 (direction ESE, speed 18) and 0900
(direction ESE, speed 16). The midpoint of the 16-20
speed range reported in the forecast is 18, so our speed
heuristic matches this wind phrase to the 0600 data file
entry, since its speed is closer to the speed range midpoint
(indeed, it matches the midpoint).
We evaluated our alignment process by applying it to
the subset of wind phrases which used a time phrase
which we thought had a clear and unambiguous inter-
pretation, namely an absolute time (such as by 0600), by
time F1 F2 F3 F4 F5 total
0000 2 9 80 5 14 110
0300 1 1
0600 1 1
0900 0
1200 1 1
1500 2 1 1 2 6
1800 30 5 2 27 13 77
2100 13 6 8 2 11 40
total 37 22 91 34 42 236
Significance of differences: p   0.001
Table 2: Usage of by evening, by forecaster (mode in
bold)
time F1 F2 F3 F4 F5 total
0000 2 1 3
0300 1 1
0600 1 1
0900 3 1 7 2 13
1200 23 71 86 11 191
1500 7 1 9 5 2 24
1800 2 2 1 5
2100 1 1
total 34 1 85 103 16 239
Significance of differences: p  0.1
Table 3: Usage of by midday, by forecaster (mode in
bold)
midday (which means 1200), by midnight (which means
0000), and by end of period (which means either 0000 or
0600, depending on the forecast section). The matching
process was fairly accurate; in 86% of cases it produced
the expected meaning (such as 0000 for by midnight).
Clearly we would benefit from better matching and
alignment techniques, and we wonder if perhaps some of
the alignment techniques used for parallel multi-lingual
corpora (Och and Ney, 2000) could be adapted to help
align our text-data corpora. This is a topic we plan to
investigate in future research.
This matching/alignment procedure is different in de-
tail from the preliminary analysis procedure reported in
(Reiter and Sripada, 2002b). The procedure used in our
earlier paper aligned fewer phrases (1382 vs. 2539) and
had a higher error rate (22% vs. 14%), so it was inferior.
5 Results
We examined the association between time phrase and
time in the 2539 aligned (wind phrase, data file entry)
pairs. In this analysis, we regarded time phrases as dif-
ferent if they involved different head nouns (for example,
time F1 F2 F3 F4 F5 total
0000 215 9 15 239
0300 1 1
0600 0
0900 0
1200 1 1
1500 0
1800 0
2100 3 3 2 8
total 0 0 219 12 18 249
Significance of differences p   0.001 (ANOVA: p = 0.06)
Table 4: Usage of by late evening, by forecaster (mode in
bold)
by evening and by afternoon), different prepositions (for
example, midday and by midday) and/or different adjec-
tives (for example, by afternoon and by late afternoon).
However, we ignored determiners (for example, by this
evening was regarded as the same phrase as by evening).
Tables 2, 3, and 4 gives details of the usage of the three
most common non-contextual time phrases: by evening,
by midday, and by late evening. This tables also shows
the statistical significance of differences between fore-
casters, calculated with a a chi-square test (which treats
time as a categorical variable). As some colleagues
have expressed an interest in a one-way ANOVA analysis
(which compares mean time), we show this as well where
it gives a substantially different value from the chi-square
analysis. This data suggests that
 by evening means different things to different peo-
ple; for example, F1 and F4 primarily use this phrase
to mean 1800, while F3 primarily uses this phrase to
mean 0000.
 by midday was used in a very similar way by all fore-
casters (ignoring F2, who only used the term once).
 by late evening was used by all forecasters (who
used this term) primarily to mean 0000. However,
the usages of the different forecasters was still sig-
nificantly different. This reflects a difference in the
distribution of usage; in particular, F3 almost always
(98% of cases) used this phrase to mean 0000, while
F4 and F5 used this phrase to mean 0000 in about
80% of cases.
These patterns are replicated across the corpus: some
phrases (such as by midday and by morning) are used in
the same way by all forecasters; some phrases (such as
by evening and by late morning) are used in very differ-
ent ways by the forecasters; and some phrases (such as by
late evening and by midnight) have the same core mean-
ing (eg, 0000) but different distributions around the core.
We have, incidentally, looked for seasonal variations in
meaning (for example, by evening meaning one thing in
the winter and another in the summer), but we have found
no evidence of such variation.
Roy (2002) has also noted variation in the meanings
that individuals assign to words, in his parallel text-data
study of object descriptions. For example, one object
might be described as having the colour pink by one sub-
ject, but other subjects might have problems identifying
the object when it was described as pink, because they did
not consider it to have this colour.
Table 5 presents the most common time-phrase used
by each forecaster for each time, including context-
dependent phrases such as later. This highlights ma-
jor ?stylistic? differences between forecasters in terms of
which time phrases they prefer to use. For example, F1
and F2 make heavy use of contextual time phrases such
as later and soon, while F5 (and to a lesser extent F4)
seem to prefer to avoid such terms. It is also interest-
ing that contextual time phrases are especially commonly
used to refer to the time 0300. We wonder if this could
reflect a ?lexical gap? in English; there are no commonly
used time phrases in English for times around 0300, and
perhaps this encourages the forecasters to use contextual
time phrases to refer to 0300.
Table 6 presents the most common (mode) meaning
of non-contextual time phrases, for each forecaster. Per-
haps not surprisingly, the greatest variability occurs when
a time phrase denoting a time period (morning, afternoon,
or evening) occurs without being modified by an adjec-
tive (early, mid, or late). The data also suggests that the
forecasters may disagree about the meaning of morning,
with F4 in particular considering morning to be the period
0300-0900, while F5 considers morning to be the period
0600-1200.
6 Current and Future Work
6.1 Verb Choice
We would like to use our corpus to learn choice rules
for verbs which are near-synonyms (Edmonds and Hirst,
2002). We are currently attempting to learn rules which
predict which of three possible verbs ? decreasing, eas-
ing, and falling ? are used when the wind speed de-
creases.
We have conducted two experiments. The first was a
semantic analysis, where we attempted to learn a choice
rule based on features extracted from the numerical data.
To do this, we used our aligned corpus to extract seman-
tic features which we thought could be relevant to this
decision (such as the amount by which the wind speed
has decreased), and then analysed this with Ripper (Co-
hen, 1995). This gave the rules shown in Figure 2; these
again show substantial variation between individual fore-
verb F1 F2 F3 F4 F5 total
decreasing 0 0 3 2 0 5
easing 1 19 0 0 0 20
falling 4 0 61 0 0 65
Table 7: Choice of wind decrease verb when subsequent
word is variable, by forecaster (mode in bold)
casters. These rules are mildly effective; 10-fold cross
validation error is 25%, compared to a baseline error rate
of 33% from always choosing the most common verb
(easing). These rules suggest that at least for some fore-
casters, decreasing suggests a larger change in the wind
speed than easing; this is the sort of near-synonym con-
notational difference that we expected to find. More sur-
prisingly (at least to us), the presence of forecast date in
some of the rules suggests that forecasters change how
they write over time. Perhaps in retrospect this should
not have been a surprise, because we have also observed
changes over time in how people write in a previous
project (Reiter et al, 2000).
We also analysed collocation effects, that is whether
we could predict verb choice based purely on the words
immediately preceding and following the verb (and hence
ignoring the numerical prediction data). This was done
on the complete corpus (not just verbs that were part of
successfully aligned phrases). It is difficult to directly
compare the collocation analysis with the semantic one
due to differences in the corpora texts used, but in gen-
eral terms the reduction in baseline error rate seems com-
parable to the semantic analysis. Some of the collocation
effects were both strong and forecaster-dependent. For
example, Table 7 shows the choice of wind decrease verb
when the word following the verb was variable (indicat-
ing wind direction was variable). In this context, fore-
casters F1 and F3 usually used falling; F2 always used
easing; and F4 always used decreasing (F5 never used
variable in his forecasts). Similar individual differences
were observed in other collocations. For example, when
the word preceding the verb was gradually, F3, F4, and
F5 preferred decreasing, but F2 always used easing (F1
never used gradually in his forecasts).
In summary, it seems that the choice between the near
synonyms decreasing, easing, and falling depends on
 semantics: how much the actual wind speed has
changed;

collocation: immediately preceding and following
words in the sentence;

author: which forecaster wrote this particular text;
 date: when the text was written.
time F1 F2 F3 F4 F5 all
0000 later later by late evening by midnight in evening later
0300 later soon soon soon tonight soon
0600 later overnight soon by morning later in period later
0900 soon soon soon by midday in morning soon
1200 by midday soon by midday by midday in morning by midday
by by mid by mid early by mid
1500 afternoon soon afternoon afternoon afternoon afternoon
1800 by evening by evening by late afternoon by evening by evening by evening
in evening/ later/
2100 later later by evening later by evening by evening
bold font means this phrases was at least twice as common as the second-most common term.
X/Y means X and Y were equally common
Table 5: Most common time-phrases for each time, by forecaster
Choose decreasing if
Forecaster rule
F1 never
F2 never
F3 speed change    10 knots AND time interval    15 hours
F4 speed change    9 knots OR forecast date    2-November-2001
F5 forecast date    30 March 2001
Otherwise choose easing
Figure 2: Verb choice rule based on data features
All of these factors are important, and in particular the
kind of semantic differences investigated by (Edmonds
and Hirst, 2002) are only one factor among many, and do
not dominate the choice decision. We plan to continue
working on this and other analyses of near-synonyms,
and obtain a better idea of how these factors interact.
6.2 Other corpora
In addition to the weather corpus, the SUMTIME project
also has access to a parallel text-data corpus of doc-
tors describing signal data from a neonatal intensive care
unit (Alberdi et al, 2001). We would like to analyse
this corpus to determine the meanings of words such as
steady and oscillations. However, a preliminary analy-
sis has suggested to us that we cannot conduct such an
analysis until we remove non-physiological events from
the data (Sripada et al, 2003a). For example, a doctor
may describe a signal as steady even when it contains a
large spike, if the doctor believes the spike is due to a
non-physiological event (such as a sensor falling off the
baby and then being replaced by a nurse). Hence non-
physiological events (known in this domain as ?artifacts?)
must be removed from the data before it is possible to
analyse word meaning. We are currently working on ar-
tifact removal, and once this is complete we will start our
analysis of word meanings.
SUMTIME is also working on generating textual sum-
maries of gas turbine sensor data (Yu et al, 2003). Un-
fortunately in this domain, as in many other NLG appli-
cations (Reiter et al, 2003), there is no existing corpus
of manually written texts describing the input data. We
have explicitly asked two experts to write descriptions
of 38 signal fragments. This very small corpus showed
that once again there were major differences between in-
dividuals (Reiter and Sripada, 2002a), but the corpus is
too small to allow meaningful statistical analysis of word
meanings.
7 Conclusion
To conclude, we believe that parallel text-data corpora
are a valuable resource for investigating lexical seman-
tic and pragmatic issues, and can help shed valuable light
on the fundamental question of how words relate to the
non-linguistic world. We have described in detail how we
have used such a corpus to analyse the meaning and usage
of time phrases in weather forecasts, and also sketched
our current work on other analyses of text-data corpora.
We hope that other researchers interested in semantics
and pragmatics will find our techniques interesting, and
consider whether they might be useful in exploring other
semantic and pragmatic questions about word meaning
usage.
phrase F1 F2 F3 F4 F5 combined
after midnight 0600 0600
afternoon * 1630 1630
around midday * * * 1200
by 0600 0600 0600
by afternoon 1500 1200 1200 1200 1200
by early afternoon * * * 1330 1330
by early evening 1800 * 1800
by early morning 0300 0300
by evening 1800 0000 0000 1800 0000 0000
by late afternoon 1800 * 1800
by late evening 0000 0000 0000 0000
by late morning * 0900 1200 1030
by mid afternoon 1500 1500 * 1500
by mid evening * 2100 * 2100
by mid morning * * * 0900
by midday 1200 * 1200 1200 1200 1200
by midnight 0000 0000 0000 0000
by morning 0600 0600 * 0600
during afternoon * * 1500
during evening 0000 0000 0000 0000
during morning * 1030 * 0900 0900
early afternoon * * * 1500 1500
early evening * 1800 1800
evening 1612 2100 0000 0000
from midday 1200 1200
in afternoon * * 1800 1800
in evening 0000 0000 0000
in morning 1200 1200
late evening * 0000 0000 0000
later in evening 0000 0000
later in night 0600 0600
mid morning * * * 0900
overnight 0600 0600 * 0600
tonight * 0000 0000
* means phrase was used fewer than five times by this forecaster.
Phrases used less than 5 times by all forecasters combined are omitted.
Contextual phrases (such as later) are also omitted.
If 2 or more times are equally common, their average is shown.
Table 6: Most common (mode) usage of time phrases, by forecaster
Acknowledgements
Our thanks to the many individuals who have discussed
this work with us, of which there are too many to list
here. Special thanks to our industrial collaborators at
WNI/Oceanroutes, without whom this work would have
been impossible. This work was supported by the UK En-
gineering and Physical Sciences Research Council (EP-
SRC), under grant GR/M76881.
References
Eugenio Alberdi, Julie-Clare Becher, Ken Gilhooly, Jim
Hunter, Robert Logie, Andy Lyon, Neil McIntosh, and
Jan Reiss. 2001. Expertise and the interpretation of
computerized physiological data: implications for the
design of computerized monitoring in neonatal inten-
sive care. International Journal of Human-Computer
Studies, 55:191?216.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2002).
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Frederick Cassidy and Joan Hall, editors. 1996. Dictio-
nary of American Regional English. Belknap.
William Cohen. 1995. Fast effective rule induction.
In Proc. 12th International Conference on Machine
Learning, pages 115?123. Morgan Kaufmann.
D. Cruse. 1986. Lexical Semantics. Cambridge Univer-
sity Press.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Linguis-
tics, pages 105?144.
Sidney Landau. 1984. Dictionaries: the art and craft of
lexicography. Scribner.
Barbara Malt, Steven Sloman, Silvia Gennari, Meiyi Shi,
and Yuan Wang. 1999. Knowing versus naming:
Similarity and the linguistic categorization of artifacts.
Journal of Memory and Language, 40:230?262.
Geoffrey Nunberg. 1978. The Pragmatics of Reference.
University of Indiana Linguistics Club, Bloomington,
Indiana.
Franz Och and Herman Ney. 2000. A comparison of
alignment models for statistical machine translation.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
1086?1090.
Rohit Parikh. 1994. Vagueness and utility: The seman-
tics of common nouns. Linguistics and Philosophy,
17:521?535.
Ehud Reiter and Somayajulu Sripada. 2002a. Human
variation and lexical choice. Computational Linguis-
tics, 28:545?553.
Ehud Reiter and Somayajulu Sripada. 2002b. Should
corpora texts be gold standards for NLG? In Proceed-
ings of the Second International Conference on Natu-
ral Language Generation, pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2000.
Knowledge acquisition for natural language genera-
tion. In Proceedings of the First International Con-
ference on Natural Language Generation, pages 217?
215.
Ehud Reiter, Somayajulu Sripada, and Roma Robertson.
2003. Acquiring correct knowledge for natural lan-
guage generation. Journal of Artificial Intelligence Re-
search. Forthcoming.
Ehud Reiter. 1991. A new model of lexical choice for
nouns. Computational Intelligence, 7(4):240?251.
Eleanor Rosch. 1978. Principles of categorization. In
E. Rosch and B. Lloyd, editors, Cognition and Catego-
rization, pages 27?48. Lawrence Erlbaum, Hillsdale,
NJ.
Deb Roy. 2002. Learning visually grounded words and
syntax for a scene description task. Computer Speech
and Language, 16:353?385.
Jeffrey Siskind. 2001. Grounding the lexical semantics
of verbs in visual perspection using force dynamics
and event logic. Journal of Artificial Intelligence Re-
search, 15:31?90.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2002. SUMTIME-METEO: Parallel corpus of
naturally occurring forecast texts and weather data.
Technical Report AUCS/TR0201, Computing Science
Dept, Univ of Aberdeen, Aberdeen AB24 3UE, UK.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003a. Exploiting a parallel text-data corpus.
In Proceedings of Corpus Linguistics 2003. UCREL,
Lancaster University, UK.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003b. Summarising neonatal time-series data. In
Proceedings of EACL-2003. Forthcoming.
Jin Yu, Ehud Reiter, Jim Hunter, and Somayajulu Sri-
pada. 2003. Sumtime-turbine: A knowledge-based
system to communicate gas turbine time-series data.
In Proceedings of IEA/AIE-2003.
Abstract 
Post-editing is commonly performed on computer-
generated texts, whether from Machine Translation 
(MT) or NLG systems, to make the texts accept-
able to end users. MT systems are often evaluated 
using post-edit data.  In this paper we describe our 
experience of using post-edit data to evaluate 
SUMTIME-MOUSAM, an NLG system that pro-
duces marine weather forecasts. 
1 Introduction 
Natural Language Generation (NLG) systems must of 
course be evaluated, like all NLP systems. Previous work on 
NLG evaluation has focused on either experiments con-
ducted with users who read the generated texts, or on com-
parisons of generated texts to corpora of human-written 
texts.  In this paper we describe an evaluation technique, 
which looks at how much humans need to post-edit gener-
ated texts before they are released to users.  Post-edit 
evaluations are common in machine translation, but we be-
lieve that ours is the first large-scale post-edit evaluation of 
an NLG system.  
The system being evaluated is SUMTIME-MOUSAM [Sri-
pada et al 2003], an NLG system, which generates marine 
weather forecasts from Numerical Weather Prediction 
(NWP) data. SUMTIME-MOUSAM is operational and is used 
by Weathernews (UK) Ltd to generate 150 draft forecasts 
per day, which are post-edited by Weathernews forecasters 
before being released to clients. 
2 Background 
2.1 Evaluating NLG Systems  
Common evaluation techniques for NLG systems [Mellish 
and Dale, 1998] include:  
? Showing generated texts to users, and measuring how 
effective they are at achieving their goal, compared to 
some control text (for example, [Young, 1999]) 
? Asking experts to rate computer-generated texts in 
various ways, and comparing this to their rating of 
manually authored texts (for example, [Lester and 
Porter, 1997]) 
? Automatically comparing generated texts to a corpus of 
human authored texts (for example, [Bangalore et al 
2000]). 
Each of these techniques is effective under different ap-
plication contexts in which NLG systems operate. For in-
stance, a corpus based technique is effective when a high 
quality corpus is available. The appeal of post-edit evalua-
tion as done with SUMTIME-MOUSAM is that (A) the edits 
should indicate actual mistakes instead of just differences in 
how things can be said and (B) the amount of post-editing 
required is a very important practical measure of how useful 
the system is to real users (forecasters in our case). 
Post-edit evaluations are a standard technique in Machine 
Translation [Hutchins and Somers, 1992]. The only previ-
ous use of post-edit evaluation in NLG that we are aware of 
is Mitkov and An Ha [2003], but their evaluation is rela-
tively small, and they give little information about it. 
2.2 SUMTIME-MOUSAM 
SUMTIME-MOUSAM [Sripada et al 2003] is an NLG system 
that generates textual weather forecasts from numerical 
weather prediction (NWP) data.  The forecasts are marine 
forecasts for offshore oilrigs.  Table 1 shows a small extract 
from the NWP data for 12-06-2002, and Table 2 shows part 
of the textual forecast that SUMTIME-MOUSAM generates 
from the NWP data.  The Wind statements in Table 2 are 
mostly based on the NWP data in Table 1.  
 
Time Wind 
Dir 
Wind Spd 
10m 
Wind Spd 
50m 
Gust 
10m 
Gust 
50m 
06:00 W 10.0 12.0 12.0 16.0 
09:00 W 11.0 14.0 14.0 17.0 
12:00 WSW 10.0 12.0 12.0 16.0 
15:00 SW 7.0 9.0 9.0 11.0 
18:00 SSW 8.0 10.0 10.0 12.0 
21:00 S 9.0 11.0 11.0 14.0 
00:00 S 12.0 15.0 15.0 19.0 
 
Table 1. Weather Data produced by an NWP model for 12-
Jun 2002 
 
Evaluation of an NLG System using Post-Edit Data: Lessons Learnt 
Somayajulu G. Sripada and Ehud Reiter and Lezan Hawizy 
Department of Computing Science 
University of Aberdeen 
Aberdeen, AB24 3UE, UK 
{ssripada,ereiter,lhawizy}@csd.abdn.ac.uk 
 SUMTIME-MOUSAM generates texts in three stages 
[Reiter and Dale, 2000]. 
Document Planning: Text structure is specified by 
Weathernews, via a control file.  The key content-
determination task is selecting ?important? or ?significant? 
data points from the underlying weather data to be included 
in the forecast text. SUMTIME-MOUSAM uses a bottom-up 
segmentation algorithm for this task [Sripada et al 2002].  
Micro-planning: The key decisions here are lexical selec-
tion, aggregation, and ellipsis. SUMTIME-MOUSAM uses 
rules for this that are derived from corpus analysis and other 
knowledge acquisition activities [Reiter et al 2003; Sripada 
et al 2003]. 
Realization: SUMTIME-MOUSAM uses a simple realiser 
that is tuned to the Weathernews weather sublanguage. 
SUMTIME-MOUSAM is partially controlled by a control 
data file that Weathernews can edit.  For example, this file 
specifies error function data that controls the segmentation 
process for content determination. The error function data 
decides the level of abstraction achieved by the segmenta-
tion process ? the larger the error function value the higher 
the level of abstraction achieved by segmentation. 
2.3 SUMTIME-MOUSAM at Weathernews 
Weathernews (UK) Ltd, a private sector weather services 
company, uses SUMTIME-MOUSAM to generate draft fore-
casts.  The process is illustrated in Figure 1.  Forecasters 
load the NWP data for the forecast into Marfors, which is 
Weathernews? forecasting tool. Using Marfors, forecasters 
edit the NWP data, using their meteorological expertise and 
additional information such as satellite weather maps. They 
then invoke SUMTIME-MOUSAM to generate an initial draft 
of the forecast. This initial draft helps the forecaster under-
stand the NWP data, and often suggests further edits to the 
NWP data.  The generate-and-edit-data process may be re-
peated.  When the forecaster is satisfied with the NWP data, 
he invokes SUMTIME-MOUSAM again to generate a final 
draft textual forecast, marked ?Pre-edited Text? in Figure 1. 
The forecaster then uses Marfors to post-edit the textual 
forecast.  When the forecaster is done, Marfors assembles 
the complete forecast from the individual fields, and sends it 
to the customer. 
 
Section 2. FORECAST 6 - 24 GMT, Wed 12-Jun 2002 
Field Text 
WIND(KTS) 10M W 8-13 backing SW by mid after-
noon and S 10-15 by midnight. 
WIND(KTS) 50M W 10-15 backing SW by mid after-
noon and S 13-18 by midnight. 
WAVES(M) 
SIG HT 
0.5-1.0 mainly SW swell. 
WAVES(M) 
MAX HT 
1.0-1.5 mainly SW swell falling 1.0 
or less mainly SSW swell by after-
noon, then rising 1.0-1.5 by mid-
night. 
WAVE PERIOD 
(SEC) 
Wind wave 2-4 mainly 6 second 
SW swell. 
WINDWAVE 
PERIOD (SEC) 
2-4. 
SWELL PERIOD 
(SEC) 
5-7. 
WEATHER Mainly cloudy with light rain 
showers becoming overcast around 
midnight. 
VISIBILITY 
(NM) 
Greater than 10. 
AIR TEMP(C) 8-10 rising 9-11 around midnight. 
CLOUD 
(OKTAS/FT) 
4-6 ST/SC 400-600 lifting 6-8 
ST/SC 700-900 around midnight. 
 
Table 2. Extract from SUMTIME-MOUSAM Forecast Pro-
duced for 12-Jun 2002 (AM). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Schematic Showing SUMTIME-MOUSAM Used at Weathernews 
Post-
edited 
Text  
Marfors 
Data Editor 
Marfors Text Editor 
SUMTIME-
MOUSAM 
Marfors 
Data Editor 
Pre-edited 
Text SUMTIME-
MOUSAM 
Text 1 Data 1 
Edited Data 
NWP Data 
 Note that SUMTIME-MOUSAM is used for two purposes 
by Weathernews; to help forecasters understand and there-
fore edit the NWP data, and to help generate texts for cus-
tomers.  In this paper we focus on evaluating the second 
usage of the system (generating texts for customers). 
When a forecast is complete, Marfors saves the final ed-
ited NWP data, marked ?Edited data? in Figure 1 and the 
final edited forecast marked ?Post-edited Text? into a data-
base.  This data is forwarded to us for 150 sites per day; this 
is the basis of our post-edit evaluation.  Marfors does not 
directly save the SUMTIME-MOUSAM text that forecasters 
edit (?Pre-edited Text? in Figure 1), but we can reconstruct 
this text by running the system on the final edited NWP 
data. 
3 Post-Edit Evaluation 
3.1 Data 
The evaluation was carried out on 2728 forecasts, collected 
during period June to August 2003.  Each forecast was 
roughly of 400 words, so there are about 1 million words in 
all in the corpus. 
For each forecast, we have the following data 
 
? Data: The final edited NWP data 
? Pre-edit text: The final draft forecast produced by 
SUMTIME-MOUSAM, which we reconstruct as de-
scribed in Section 2.3. 
? Post-edit text: The manually post-edited forecast, 
which was sent to the client. 
? Background information: includes date, location, and 
forecaster 
 We do not currently use the NWP data (other than for 
reconstructing SUMTIME-MOUSAM texts), although we 
hope in the future to include it in our analyses, in a manner 
roughly analogous to Reiter and Sripada [2003]. This data 
set continues to grow, we receive approximately 150 new 
forecasts per day. 
3.2 Analysis Procedure 
The following procedure is performed automatically by a 
software tool. First, we perform some data transformation 
and cleaning.  This includes breaking sentences up into 
phrases, where each phrase describes the weather at one 
point in time. 
For example, the pre-edit text in Figure 2 would be bro-
ken up into three phrases: 
 
A1 SW 20-25 
A2 backing SSW 28-33 by midday 
A3 then gradually increasing 34-39 by midnight 
 
 
 
 
 
 
 
 
 
 
Figure 2. Example pre-edit and post-edit texts from the post-
edit corpus 
 
The Figure 2 post-edit text is divided into two phrases: 
 
B1 SW 22-27 
B2 gradually increasing SSW 34-39 
 
The second step is to align phrases from these two tables 
as a preparation for comparison in the next step. Alignment 
is a complex activity and is described in detail next. To start 
with we generate an exhaustive list of all the possible com-
binations of phase alignments. 
For example, consider the texts in Figure 2. Here we gen-
erate the following list of possible alignments: 
 
{(A1, B1), (A1, B2), (A2, B1), (A2, B2), (A3, B1), (A3, 
B2)} 
 
  Next, we compute match scores for each of these possi-
ble alignments and use them for selecting the right align-
ments. For each unedited phrase Ai, the alignment with the 
highest matching score is selected. For the purpose of com-
puting the match scores, phrases are parsed using ?parts of 
speech? designed for weather sublanguage such as direction, 
speed and time. The total match score of a pair of phrases is 
computed as the sum of the match scores for their constitu-
ents. Match score (MS) for a pair of constituents depends 
upon their part of speech and also their degree of match. MS 
is defined as a product of two terms as explained below: 
? Match score due to degree of match: we assign a match 
score of 2 for exact matches, 1 for partial matches and 
0 for mismatches. 
? Weight factor denoting importance of constituents for 
alignment: Constituents belonging to certain parts of 
speech (POS) are more significant for alignment than 
others. For example, times are more significant for 
alignment than verbs. Also weights are varied for the 
same POS based on its context in the phrase. For ex-
ample, direction receives higher weight if it occurs in 
a phrase without a time or speed. This is because in 
such phrases direction is the only means for align-
ment. 
Continuing with our example sentences in Figure 2, we 
show below how we find an alignment for A3. As described 
earlier, A3 can be aligned to either B1 or B2. The MS for 
(A3, B1) is zero as shown in Table 3. 
 
A. Pre-edit Text: SW 20-25 backing SSW 28-33 by 
midday, then gradually increasing 34-39 by midnight. 
 
B. Post-edit Text: SW 22-27 gradually increasing 
SSW 34-39. 
 
POS A3 B1 MS 
conjunction Then <none> 0 
Adverb Gradually <none> 0 
Verb Increasing <none> 0 
Direction <none> SW 0 
Speed range 34-39 22-27 0 
Time By midnight <none> 0 
 
Table 3 Match Score for A3 and B1 
 
The MS for (A3, B2) is 2*(2*w1+w2) where w1 is the 
weight for Adverb/verb and w2 (>w1) for speed as shown in 
Table 4. Based on the match scores computed above A3 is 
aligned with B2. Similarly A1 is aligned with B1. A2 is 
unaligned, and treated as a deleted phrase. 
 
POS A3 B2 MS 
conjunction Then <none> 0 
Adverb Gradually Gradually w1*2 
Verb Increasing Increasing w1*2 
Direction <none> SSW 0 
Speed range 34-39 34-39 w2*2 
Time By midnight <none> 0 
 
Table 4. Match Score for A3 and B2 
 
The third step is to compare aligned phrases, such as A1 
and B1.  One evaluation metric is based on comparing 
aligned phrases as a whole. Here we simply record ?match? 
or ?mismatch?.  For example, both (A1, B1) and (A3, B2) 
are mismatches. We then compare constituents in the 
phrases to determine more details about the mismatches.  
For this detailed comparison we use the domain-specific 
part-of-speech tags described earlier. Each part-of-speech 
should occur at most once in a phrase (in our weather sub-
language), so we simply align on the basis of the tag. After 
constituents are aligned, we label each pre-edit/post-edit 
pair as match, replace, add, or delete. For example, A and B 
are analysed as in Table 5. 
 
POS A B label 
Direction SW SW match 
Speed 20-25 22-27 replace 
    
Conjunction then <none> delete 
Adverb gradually gradually match 
Verb increasing increasing match 
Direction <none> SSW add 
Speed 34-39 34-39 match 
Time by midnight <none> delete 
 
Table 5. Detailed Edit Analysis 
3.3 Analysis of Results 
We processed 2728 forecast pairs (pre-edited and post-
edited). These were divided into 73041 phrases. Out of 
these, the alignment procedure failed to align 7608 (10%) 
phrases. For instance, in the example of Section 3.2, phrase 
A2 was not aligned with any B phrase.  Alignment failure 
generally indicates that the forecaster is unhappy with 
SUMTIME-MOUSAM?s segmentation that is with the sys-
tem?s content determination. We have manually analysed 
some of these cases, and in general it seems the forecasters 
are performing more sophisticated data analysis than 
SUMTIME-MOUSAM, and are also more sensitive to which 
changes are significant enough to be reported to the user.  
We have manually inspected alignment quality of 100 
random phrase pairs to determine cases where our alignment 
procedure erroneously aligned phrases. We found one case 
of improper alignment. The pre-edited phrase ?soon becom-
ing variable? has not been aligned to its corresponding iden-
tical post-edited phrase. Inspection of the rest of the corpus 
showed that this error repeated 54 times in the whole cor-
pus. These cases have been classified as alignment failures 
and therefore do not affect the post-edit analysis. 
 
Time (Hours) Direction Speed 
00 ESE 12 
03 ESE 12 
06 ESE 11 
09 ESE 11 
12 ESE 10 
15 ESE 8 
18 ESE 9 
21 ESE 11 
24 ESE 13 
 
Table 6. Wind 10m data for 14 Jul 2003 
 
For example, consider the Wind 10m data shown in Table 
6. Our content determination algorithm first segments the 
data in table 6 (see Sripada et al[2002] for more details). 
Segmentation is the process of fitting straight lines to a data 
set in such a way that a minimum error is introduced by the 
lines. Since the direction data is constant at ESE, there is 
only one segment for this data. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Segmentation of Wind speed data shown in Ta-
ble 6. 
Wind speed data however is segmented by two lines as 
shown in Figure 3, one line joining the point (0,12) with 
(15,8) and the second joining the point (15,8) with (24,13). 
Our content selection algorithm therefore selects data points 
Segmentation of Wind 10m data
0
2
4
6
8
10
12
14
0 3 6 9 12 15 18 21 24
Time
W
in
d 
Sp
e
e
d
(0,12), (15,8) and (24,13) to be included in the forecast. In 
this case our system produced: 
 
?ESE 10-15 gradually easing 10 or less by mid afternoon 
then increasing 11-16 by midnight? 
However, forecasters view this data as a special case and 
don?t segment it the way we do. Here the wind speed is al-
ways in the range of ?10-15? except at 1500 and 1800 hours. 
Therefore they mention the change as an additional informa-
tion to an otherwise constant wind speed. In this case, the 
forecaster edited text is: 
 
?ESE 10-15 decreasing 10 or less for a time later?. 
Talking about the segmentation differences, one of the 
forecasters at Weathernews told us that another factor af-
fecting segmentation is related to the end user. End users of 
the marine forecasts are oil company staff who schedule 
activities on the oilrigs in the North Sea. Over the years 
forecasters at Weathernews have acquired a good under-
standing of the informational needs of the oil company staff. 
So they use the forecast statements as messages to the end 
users about the weather and know what kind of messages 
will be useful to the end users. In the example texts shown 
in Figure 2 the forecaster could have thought that the impor-
tant message to communicate about wind is that it is in-
creasing monotonically and is likely to be in the range be-
tween 22 (the actual initial wind speed) and 39. Everything 
else distracts this primary message and therefore needs to be 
avoided. Once again there is post segmentation reasoning 
used by the forecasters. We are investigating better pattern 
matching techniques and better user models to improve our 
content selection. 
 
S. No. Mismatch Type Freq. % 
1. Ellipses (word additions 
and deletions) 
35874 65 
2. Data Related Replacements 
(range and direction re-
placements) 
10781 20 
3. Lexical Replacements 8264 15 
 Total 54919  
 
Table 7. Results of the Evaluation showing summary cate-
gories and their frequencies 
Going back to the successfully aligned phrases, 43914 
(60%) are perfect matches, and the remaining 21519 (30%) 
are mismatches.  Table 7 summarises the mismatches.  
Here, each mismatch is classified as 
? Ellipses: additions and deletions.  For example, delet-
ing the time phrase by midnight in the (A3, B2) pair.  
These generally indicate problems with SUMTIME-
MOUSAM?s aggregation and ellipsis. 
? Data replacements: changes (replaces) to constituents 
that directly convey NWP data, such as wind speed 
and direction.  For example, changing 20-25 to 22-27 
in the (A1, B1) pair.  These can indicate content prob-
lems.  They also occur when forecasters believe the 
NWP data is incorrect but decide to just correct the 
forecast text and not the data (eg, skip generate-and-
edit step described in section 2.3). 
? Lexical replacements: All other changes (replaces). For 
example, if the conjunction ?then? was replaced by 
?and?.  This generally indicates a problem in 
SUMTIME-MOUSAM?s lexicalisation strategy. 
For each pair of phrases compared in the evaluation, we 
have counted the number of times each edit operation such 
as add, delete and replace is performed by forecasters. For 
example consider the two phrase pairs shown in Table 5. 
For the first phrase pair of ?SW 20-25? and ?SW 22-27? fore-
casters performed zero add, zero delete and one replace 
operation (?20-25? is replaced by ?22-27?). For the second 
phrase pair of ?then gradually increasing 34-39 by mid-
night? and ?gradually increasing SSW 34-39? forecasters 
performed one add (added ?SSW?), two delete (deleted 
?then? and ?by midnight?) and zero replace operations. We 
hypothesized that forecasters were making significantly 
more add and delete operations than replace operations. For 
verifying this, we have performed a pairwise t-test. Vari-
able1 for the t-test represents the sum of the counts of add 
and delete operations for each pair of phrases. Variable2 
represents the count of replace operations. For example, for 
the two phrase pairs shown in Table 5, variable1 has values 
of zero and three where as variable2 has values of one and 
zero. This test showed (with a p value less than 10-20) that 
forecasters were performing more additions and deletions 
than replacements. In other words, ellipsis is the main prob-
lem in our system. Most (25235 out of 35874, 70%) of these 
errors are deletions, where the forecaster deletes words from 
SUMTIME-MOUSAM?s texts. 
A manual analysis of some ellipsis cases has highlighted 
some general phenomena.  First of all, many ellipsis cases 
are ?downstream? consequences of earlier changes. For ex-
ample, if we look at the (A3, B2) pair above, this contains 
three ellipsis changes: then was deleted, SSW was added, 
and by midnight was deleted.  The first two of these changes 
are a direct consequence of the deletion of phrase A2.  If 
SUMTIME-MOUSAM?s content determination system was 
changed so that it did not generate A2, then the micro plan-
ner would have expressed A3 as gradually increasing SSW 
34-39 by midnight, which is identical to B2 except for by 
midnight. 
The deletion of by midnight is an example of another 
common phenomenon, which is disagreement among indi-
viduals as to how text should be written.  As described in 
[Reiter et al 2003], some forecasters elide the last time 
phrase in simple sentences such as this one, and some do 
not.  An earlier version of SUMTIME-MOUSAM in fact 
would have elided this time phrase, but we changed the be-
havior of the system in this regard after consultation.  Ellip-
sis errors are inevitable in cases where the different fore-
casters disagree about when to elide.  However, since post-
editors can delete words more quickly than they can add 
words, it probably makes sense from a practical perspective 
to be conservative about elision, and only elide in unambi-
guous cases. We will not further discuss data replacement 
errors, since they reflect either content problems or cases 
where NWP data was not corrected at the input time but 
edited directly in the final text. 
We have discussed lexical replacement errors in detail 
elsewhere [Reiter and Sripada, 2002].  In general terms, 
some errors reflect problems with SUMTIME-MOUSAM; for 
example, the system overuses then as a connective, so fore-
casters often replaced then by alternative connectives such 
as and.  However, many lexical replacement errors simply 
reflected the lexical preferences of individual forecasters 
[Reiter and Sripada, 2002].  For example, SUMTIME-
MOUSAM always uses the verb easing to indicate a reduc-
tion in wind speed.  Most forecasters were happy with this, 
but 3 individuals usually changed this to decreasing. 
A general observation is that some forecasters post-edited 
texts much more than others.  For example, while overall 
28% of phrases were edited, edit rates by individual fore-
casters varied from 4% to 93%.  We do not know why edit 
rates vary so much, although it may be significant the indi-
vidual with the highest (93%) edit rate is one of the most 
experienced forecasters, who takes well-justified pride in 
producing well-crafted forecasts. 
Summarizing the results of our evaluation: 
1. SUMTIME-MOUSAM?s content determination can defi-
nitely be improved, by using more sophisticated segmenta-
tion techniques. 
2. SUMTIME-MOUSAM?s micro-planner can certainly be 
improved in places, for example by varying connectives.  
However, many post-edits are due to individual differences, 
which we cannot do anything about. 
We are currently carrying out another evaluation of SUM-
TIME-MOUSAM by the end users, oilrig staff and other ma-
rine staff who regularly read weather forecasts. In this study 
we compare user?s comprehension of weather information 
from human written and computer generated forecast texts. 
We also measure user ratings (preference) of human written 
and computer generated texts. Preliminary results from our 
study indicate that users make fewer mistakes on compre-
hension questions when they are shown texts that use com-
puter generated words with human selected content. Gener-
ally users seem to prefer computer generated texts to human 
written texts given the same underlying weather data. 
4 Lessons from our Post-Edit Evaluation 
As stated in Section 2.1, we were attracted to post-edit 
evaluation because we believed that (A) people would only 
edit things that were clearly wrong; and (B) post-editing was 
an important usefulness metric from the perspective of our 
users (forecasters). 
Looking back, (B) was certainly true.  The amount of 
post-editing that generated texts require is a crucial compo-
nent of the cost of using SUMTIME-MOUSAM, and hence of 
the attractiveness of the system to users (forecasters). Al-
though we have not measured the time required for perform-
ing post-edits, we have used edit-distance measures used in 
MT evaluations as an approximate cost metric. We have 
computed our cost metric by setting different cost (weight) 
values to different edit operations. Cost of add and replace 
operations is set to 5 and cost of delete is set to 1 as used in 
Su et al[1992]. The ratio of the cost of edits and the cost of 
writing the entire forecast manually (adding all the words) is 
computed to be 0.15.  (A) however was perhaps less true 
than we had hoped. Wagner [1998] also described post-
edited texts in MT as at times noisy. Our analysis of manu-
ally written forecasts [Reiter and Sripada, 2002] had high-
lighted a number of ?noise? elements that made it more dif-
ficult to extract information from such corpora. Basically 
there are many ways of communicating information in text, 
and the fact that a generated text doesn?t match a corpus text 
does not mean that the generated text is wrong.  We as-
sumed that people would only post-edit mistakes, where the 
generated text was wrong or sub-optimal, and hence post-
edit data would be better for evaluation purposes than cor-
pus comparisons. 
In fact, however, there were many justifications for post-
edits: 
1. Fixing problems in the generated texts (such as 
overuse of then);  
2. Refining/optimizing the texts (such as using for 
a time);  
3. Individual preferences (such as easing vs de-
creasing); and  
4. Downstream consequences of earlier changes 
(such as introducing SSW in B2, in the example 
of Section 3.2).  
We wanted to use our post-edit data to improve the sys-
tem, not just to quantify its performance, and we discovered 
that we could not do this without attempting to analyze why 
post-edits were made.  Probably the best way of doing this 
was to discuss post-edits with the forecasters.  Alternatively, 
we could have asked forecasters to fill in problem sheets to 
capture their explanation of post-edits. Such feedback from 
the forecasters would have allowed us to reason with post-
edit data to improve our system. In [Reiter et al 2003] we 
explained that we found that analysis of human-written cor-
pora was more useful if it was combined with directly work-
ing with domain experts; and essentially this (perhaps not 
surprisingly) is our conclusion about post-edit data as well.  
One of the lessons we learnt from this exercise has been 
that post-edit evaluations are useful to compute a cost metric 
to quantify the usefulness of a system. For example, as de-
scribed earlier, we have computed a cost metric, 0.15 signi-
fying the post-editing effort. Post-edit evaluations are also 
useful in revealing general problem areas in a system. For 
example, as described in section 3.3, our evaluation showed 
that ellipsis related problems are more serious in our system 
than others. However, post-edit evaluations are not affective 
in discovering specific problems in a system. The main rea-
son for this is that many post-edits, as stated earlier, do not 
actually fix problems in the generated text at all. The real 
post-edits that fixed problems in the generated text were 
buried among the other noisy post-edits. 
This lesson of course is the result of our method of post-
edit evaluation. Post-editing was not supported by 
SUMTIME-MOUSAM and forecasters used Marfors (see sec-
tion 2.3) to perform post-editing. Therefore, we had to ac-
cept the post-edit data with all the noise. In MT, post-editors 
often work under predefined guidelines on post-editing and 
also use post-editing tools.  For example, post-editing tools 
automatically revise texts to fix ?down-stream? conse-
quences of human edits. If post-edit tools are similarly inte-
grated into NLG systems, there is going to be a significant 
reduction in the number of noisy post-edits allowing us to 
focus on real post-edits. 
Because post-editing is subjective varying from individ-
ual to individual, we need to understand the post-editing 
behaviour of individuals to analyze the noisy post-edit data. 
Although we have data on forecaster variations in our post-
edit corpus, these variations have not been observed from 
different forecasters post-editing the same text. This we 
could have achieved by performing a pilot before the actual 
evaluation. For the pilot all the forecasters post-edit the 
same set of forecasts, thus revealing their individual prefer-
ences. Post-edit data from the pilot would have enabled us 
to factor out the effects of forecaster variation from the real 
evaluation data. As described above noise in the post-edit 
data can be reduced by using post-edit tools and by perform-
ing a pilot before the real evaluation. This means that post-
edit evaluations need preparation in the form of developing 
post-edit tools and carrying out pilot studies. This is another 
lesson we learnt from our current evaluation. 
Although analyzing the post-edit data was a major en-
deavour for us, the overall cost of post-edit evaluation was 
not much compared to the effort that would have been re-
quired to conduct end user experiments on 2728 texts.  Of 
course, this was only true because SUMTIME-MOUSAM 
texts were being post-edited in any case by Weathernews.  
The cost-effectiveness of post-edit evaluation is less clear if 
the evaluators must organize and pay for the post-editing, as 
Mitkov and An Ha [2003] did. In this context we should 
speculate that when more and more NLG systems are de-
ployed in the real world, post-editing will be accepted as a 
component in the process of automatic text generation much 
in the same way post-editing is now a part of MT. 
5 Conclusion 
Evaluation is a key aspect of NLG; we need to know how 
well theories and systems work.  We have used analysis of 
post-edits, a popular evaluation technique in machine trans-
lation, to evaluate SUMTIME-MOUSAM, an NLG system 
that generates marine weather forecasts.  We encountered 
some problems, such as the need to identify why post-edits 
were made which make post-edit data hard to discover spe-
cific clues for system improvement. However, post-edit 
evaluation can reveal problem areas in the system and also 
quantify system utility for real users. 
References 
[Bangalore et al, 2000] Srinivas Bangalore, Owen Ram-
bow, and Steve Whittaker. 2000. Evaluation metrics for 
generation. In Proc. of the First International Natural 
Language Generation Conference (INLG2000), Israel. 
[Hutchins and Somers, 1992] John Hutchins and Harold L. 
Somers, 1992. An Introduction to Machine Translation, 
Academic Press. 
[Lester and Porter, 1997] James Lester and Bruce Porter. 
1997. Developing and empirically evaluating robust 
explanation generators: The KNIGHT experiments. 
Computational Linguistics, 23-1:65-103. 
[Mellish and Dale, 1998] Chris Mellish and Robert Dale, 
1998. Evaluation in the context of natural language gen-
eration, Computer Speech and Language 12:349-373. 
[Mitkov and An Ha, 2003] Ruslan Mitkov and Le An Ha, 
2003. Computer-Aided Generation of Multiple-Choice 
Tests, In Proc. of the HLT-NAACL03 Workshop on 
Building Educational Applications Using NLP, Edmon-
ton, Canada, pp. 17-22. 
 [Reiter and Dale, 2000] Ehud Reiter and Robert Dale, 2000. 
Building Natural Language Generation Systems. Cam-
bridge University Press. 
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu G. 
Sripada, 2002. Human Variation and Lexical Choice. 
Computational Linguistics 28:545-553. 
[Reiter et al, 2003] Ehud Reiter, Somayajulu G. Sripada, 
and Roma Robertson, 2003. Acquiring Correct Knowl-
edge for Natural Language Generation. Journal of Artifi-
cial Intelligence Research, 18: 491-516, 2003. 
[Reiter and Sripada, 2003] Ehud Reiter and Somayajulu G. 
Sripada, 2003. Learning the Meaning and Usage of Time 
Phrases from a Parallel Text-Data Corpus. In Proc. of the 
HLT-NAACL03 Workshop on Learning Word Meaning 
from Non-Linguistic Data, pp 78-85. 
[Sripada et al, 2002] Somayajulu, G. Sripada, Ehud Reiter, 
Jim Hunter and Jin Yu. 2002 Segmenting Time Series 
for Weather Forecasting. In: Macintosh, A., Ellis, R. and 
Coenen, F. (ed) Proc. of ES2002, pp. 193-206. 
[Sripada et al, 2003] Somayajulu G. Sripada, Ehud Reiter, 
and Ian Davy, 2003. SUMTIME-MOUSAM: Configur-
able Marine Weather Forecast Generator. Expert Update, 
6(3):4-10. 
[Su et al, 1992] Keh-Yih Su, Ming-Wen Wu and Jing-Shin 
Chang, 1992, A new quantitative quality measure for 
machine translation systems. In Proceedings of 
COLING-92, Nantes, pp 433-439.  
[Wagner, 1998] Simone Wagner, 1998. Small Scale Evalua-
tion Methods In: Rita N?bel; Uta Seewald-Heeg (eds.): 
Evaluation of the Linguistic Performance of Machine 
Translation Systems. Proc. of the Workshop at the 
KONVENS-98. Bonn, pp 93-105. 
[Young, 1999] Michael Young, 1999. Using Grice?s maxim 
of quantity to select the content of plan description, Arti-
ficial Intelligence 115:215-256. 
Abstract 
Most NLG systems generate texts for readers with 
good reading ability, but SkillSum adapts its output 
for readers with poor literacy. Evaluation with low-
skilled readers confirms that SkillSum?s knowl-
edge-based microplanning choices enhance read-
ability. We also discuss future readability im-
provements. 
1 Introduction 
Most existing NLG systems assume that generated texts are 
read by proficient readers with good literacy levels. How-
ever, many people in the UK and elsewhere are not profi-
cient readers; indeed, according to a UK Government survey 
[Moser, 1999], twenty percent of the UK adult population 
have problems with reading (and an even greater number 
have problems with simple maths). Some of these individu-
als have physical or cognitive disabilities (such as dyslexia), 
but many have no such problems; their poor basic skills are 
because of factors such as social deprivation and attending 
low-quality schools. NLG systems that generate personal-
ised health information, for example Cawsey et al [2000] 
and Reiter et al [2003a], would probably be more effective 
if they could generate appropriate texts for poor readers as 
well as good readers. Certainly real world NLG applications 
should at least consider such readers; otherwise there is a 
danger that many readers will not understand the texts we 
generate. 
Generating appropriate texts for poor readers is a multifac-
eted problem. At a content level, texts should be short, ex-
plicit, and clearly useful to the reader  [Sripada et al, 2003], 
so that he or she is willing to make the effort required to 
read it. At a linguistic level, texts should use simple and 
easy-to-understand words and short sentences with simple 
syntactic structures [Harley, 2001]. At a presentation level, 
texts should have an easy-to-understand layout [Bouayad-
Agha et al, 2001] and be communicated in clear fonts par-
ticularly for dyslexic readers (e.g. K-type fonts, www.k-
type.com) and for readers with visual impairment (e.g. 
tiresias font, www.tiresias.org). 
The focus of our research is on the linguistic level, and to 
date we have looked at choices related to the expression of 
discourse structure, such as the order in which phrases re-
lated by a discourse relation are expressed. Our hope was 
that rules for linguistic choices at least would be generic and 
easy to ?plug in? to NLG systems intended for poor readers. 
Future work in the project will look at lexical choice and 
also at improved content selection and personalisation.  
1.1 The SkillSum project 
SkillSum is an on-going collaborative project between 
Cambridge Training and Development Ltd. (CTAD), who 
build educational resources, and NLG researchers at Aber-
deen University. The project is developing a web-based 
application that assesses adult basic skills in literacy (read-
ing and writing skills) or numeracy (maths skills) and gen-
erates feedback reports. Users of SkillSum take a test devel-
oped by CTAD that assesses their literacy or numeracy, and 
then SkillSum generates reports that summarise their 
performance. SkillSum is being developed in a user-centred 
manner involving rapid prototyping and frequent evalua-
tions with users. 
The ultimate goal of the SkillSum project is to build a sys-
tem that allows people who are concerned about their liter-
acy or numeracy to assess their skills with minimal support 
from others, and that encourages people with poor skills to 
take steps to improve them. Currently most people with 
poor skills do not in fact enrol in courses to improve their 
skills, and our hope is that making the assessment process as 
easy (and private) as possible will encourage more people 
who need help to seek it out.  
The SkillSum project originally used detailed diagnostic 
literacy and numeracy assessments developed by CTAD. 
However, in pilots with users, we found that these took too 
long to complete and it seemed unlikely that people would 
be able to use them in an unsupported environment. Our 
current solution uses modified versions of CTAD?s shorter 
literacy and numeracy screeners, i.e. tests that identify in a 
broader sense whether a user has problems with literacy or 
numeracy, but without a detailed analysis. These administer 
twenty-seven questions graded according to the Adult Basic 
Skills Core Curriculum for England and Wales [Steeds, 
2001] and covering a broad range of skills from simpler 
levels to higher levels in this curriculum. The tests adminis-
ter the easiest questions first. Ideally, the difficulty of the 
questions that are administered should change according to 
Generating readable texts for readers with low basic skills 
Sandra Williams and Ehud Reiter 
Department of Computing Science 
University of Aberdeen 
Aberdeen AB24 3UE, U.K. 
{ swilliam,ereiter }@csd.abdn.ac.uk 
a user?s ability to answer correctly, but at present if a user 
has difficulties with the questions, the test simply ends. 
The reports generated by SkillSum are, of course, tailored to 
individuals, but this tailoring is in terms of content, rather 
than language. The focus of research to date has been on 
how to generate appropriate texts for readers with below 
average literacy and numeracy; i.e. language tailoring for 
the group as a whole, not for individuals.  
1.2 Related work 
Using NLG in educational applications is not new, but the 
type of text generated is different from SkillSum?s reports. 
For example, generating turns in intelligent tutoring system 
(ITS) dialogues (e.g. [Di Eugenio et al, 2001; [Moore et al, 
2004]). Although turns can give feedback, the kind of feed-
back differs in that it attempts to teach a student about an 
immediate domain-specific learning problem, rather than to 
summarise his/her overall skills.  
With regard to tailoring texts for different readers, a number 
of previous researchers have looked at tailoring generated 
texts according to whether the reader is a domain expert or a 
novice (for example [Paris, 1988; McKeown et al, 1993; 
Milosavljevic and Oberlander, 1998]). Less work has been 
done on tailoring texts according to the reader?s literacy. 
Perhaps the best-known previous work in this area is PSET 
[Devlin et al, 1999], which focused on syntactic and lexical 
choices in texts intended for aphasic readers. Unfortunately 
most of PSET?s adaptation rules were not experimentally 
validated. Siddharthan [2003] similarly proposed and im-
plemented a system for simplifying texts, but did not evalu-
ate how readable his generated texts were for poor readers. 
Scott and de Souza [1990] suggested some psycholinguisti-
cally-motivated rules for expressing discourse relations, but 
did not evaluate them at all. 
2 Linguistic choices investigated 
Figure 1 ? Extract from typical content plan 
The document (content) planners of our system produce as 
output a tree, where core messages are related by discourse 
relations such as explanation or concession; this basically 
follows the architecture described by Reiter and Dale 
[2000]. Discourse relations are essentially rhetorical struc-
ture theory (RST) relations [Mann and Thompson, 1987], 
and messages are represented using a deep-syntactic repre-
sentation, which is loosely based on RealPro [Lavoie and 
Rambow, 1997]. An example of an extract from a typical 
content plan, with messages shown as text glosses instead of 
deep syntactic structures, is shown in Figure 1.  
Our focus to date has been on how discourse relations such 
as Concession and Condition in Figure 1 are expressed, in 
particular: 
? cue phrases: should a cue phrase (or multiple cue 
phrases) be used to express a discourse relation? If so, 
which one(s)? For example, should we generate: 
? If you practise reading, your skills will improve 
(one cue, If) 
? If you practise reading, then your skills will im-
prove (two cues, If and then) 
? ordering: which order should the constituents related 
by a discourse relation be expressed in? Should the 
nucleus (core) be first or second? For example, should 
we generate: 
? Your skills will improve if you practise reading 
(nucleus first) 
? If you practise reading, your skills will improve 
(nucleus second) 
? punctuation (sentence structure): should constituents 
be expressed in separate paragraphs, separate sen-
tences, in a single sentence with punctuation separat-
ing them, or in a single sentence without punctuation? 
For example, should we generate (just showing two of 
these options):  
? Many people find reading hard, but your skills 
will improve if you practise reading (single sen-
tence, comma separation)  
? Many people find reading hard. But your skills 
will improve if you practise reading (two sen-
tences)  
These choices are inter-dependent. For example, we cannot 
say, ?Then your skills will improve, if you practise reading? 
(both if and then cue phrases, nucleus first). 
This problem is related to the document structuring task of 
Power et al [2002]. Power et al?s approach is essentially 
algorithmic; whereas our approach is centred on the knowl-
edge required to make the choices, and the algorithm used is 
less important. Their task is how to map an input RST tree 
to a set of output trees representing possible alternative 
document structures and then choose the best; whereas 
SkillSum?s microplanning task is to map an input RST tree 
to flat, ordered lists of syntactic structures representing pos-
sible lists of alternative sentences and then pick the best. 
Power et al include document layout in their task, whereas 
SkillSum makes layout decisions later, during the final re-
alisation stage.  
3 Choice rules and the microplanner  
We created a microplanner (developed from the one de-
scribed in [Williams, 2004]) that made the above choices 
based on hard constraints and optimisation rules; the hard 
Concession 
Condition [Many people find 
reading hard] 
[Your skills will improve] [You practise reading] 
constraints forbade illegal combinations, and the optimisa-
tion rules expressed readability preferences. 
3.1 Hard constraints 
The hard constraints were intended to forbid combinations 
of choices that led to ungrammatical texts, such as ?Then 
your skills will improve, if you practise reading?. We cre-
ated these by analyzing the RST Discourse Treebank Cor-
pus (RST-DTC) [Carlson et al 2002]; this is a corpus of 
Wall Street Journal texts that have been annotated with dis-
course relations. For each discourse relation of the type that 
occurs in SkillSum texts, we extracted 200 instances of the 
relation from the RST-DTC (or as many as possible if the 
corpus contained fewer than 200 instances), and analysed 
what combination of the above choices were instantiated in 
each of these instances. We then created hard constraints 
that forbade any pair of choices which was not present in 
any of the RST-DTC instances that we analysed. These con-
straints were specified on pairs of choices (for example, 
ordering and punctuation), not a complete choice set (order-
ing, punctuation, cue phrases), because we did not have 
enough instances to make rules for complete choice sets.  
The RST-DTC corpus was not ideal for this exercise, as it is 
based on texts (Wall Street Journal articles) that are in-
tended for good readers and written in U.S. English. It 
would have been preferable to use a corpus of U.K. English 
texts intended for low-skilled readers. Unfortunately there is 
no such corpus that includes discourse relation annotations. 
3.2 Optimisation rules 
The optimisation rules expressed preferences between legal 
sets of choices. We created two sets of rules: control and 
enhanced-readability (ER). The control rules were based on 
the most common choices observed in the RST-DTC; they 
also penalised cue phrases which were highly ambiguous 
(could be used for many discourse relations). The ER rules 
expressed a set of preferences for the above choices which 
we hypothesized would result in more readable texts for 
low-skilled readers. 
The ER model was based on a literature review of relevant 
psycholinguistic findings (such as [Millis and Just, 1994; 
Degand et al, 1999; Harley, 2001]) and also on a series of 
pilot experiments that we performed with low-skilled read-
ers [Williams et al, 2003]. Essentially, it prefers that 
? each discourse relation should be expressed by a cue 
phrase. Only a single cue phrase should be used (for 
example, not both if and then for condition); 
? lexically common cue phrases are preferred, even if 
they are ambiguous; for example but instead of how-
ever for concession;  
? a cue phrase should be placed between the constituents 
if possible, and the nucleus (core) should come first if 
possible. For example, ?Your skills will improve if you 
practise reading? is preferred over ?If you practise 
reading, your skills will improve?; 
? constituents should preferably be in separate sentences; 
if they are in the same sentence, they should be sepa-
rated by a comma. 
With regard to the choice of cue phrases, obviously cue 
phrases do not have identical meanings; even if in a broad 
sense they express the same discourse relation, they have 
different connotations and applicability constraints [Knott, 
1996]. Hence the choice of cue phrases should be influenced   
 
Figure 2 ? Example text produced by SkillSum and generated with enhanced readability (ER) model. 
 
		

	



	


Proceedings of the Fourth International Natural Language Generation Conference, pages 136?138,
Sydney, July 2006. c?2006 Association for Computational Linguistics
GENEVAL: A Proposal for Shared-task Evaluation in NLG
Ehud Reiter
University of Aberdeen, UK
ereiter@csd.abdn.ac.uk
Anja Belz
University of Brighton, UK
a.s.belz@brighton.ac.uk
Abstract
We propose to organise a series of shared-
task NLG events, where participants are
asked to build systems with similar in-
put/output functionalities, and these sys-
tems are evaluated with a range of differ-
ent evaluation techniques. The main pur-
pose of these events is to allow us to com-
pare different evaluation techniques, by
correlating the results of different evalua-
tions on the systems entered in the events.
1 Background
Evaluation is becoming increasingly important in
Natural Language Generation (NLG), as in most
other areas of Natural Language Processing (NLP).
NLG systems can be evaluated in many differ-
ent ways, with different associated resource re-
quirements. For example, a large-scale task-
effectiveness study with human subjects could last
over a year and cost more than US$100,000 (Re-
iter et al, 2003); on the other hand, a small-scale
comparison of generated texts to human-written
reference texts can be done in a manner of days.
However, while the latter kind of study is very
appealing in terms of cost and time, and cheap
and reliable evaluation techniques would be very
useful for people developing and testing new NLG
techniques, it is only worth doing if we have rea-
son to believe that its results tell us something
about how useful the generated texts are to real
human users. It is not obvious that this is the case
(Reiter and Sripada, 2002).
Perhaps the best way to study the reliability of
different evaluation techniques, and more gener-
ally to develop a better empirical understanding of
the strengths and problems of different evaluation
techniques, is to perform studies where a range of
different evaluation techniques are used to evalu-
ate a set of NLG systems with similar functional-
ities. Correlating the results of the different eval-
uation techniques will give us empirical insight as
to how well these techniques work in practice.
Unfortunately, few such studies have been car-
ried out, perhaps because (to date) few NLG sys-
tems have been built with comparable functional-
ity (our own work in this area is discussed below).
We hope to surmount this problem, by organising
?shared task? events to which NLG researchers can
submit systems based on a supplied data set of in-
puts and (human-written) text outputs. We will
then carry out our evaluation experiments on the
submitted systems. We hope that such shared-task
events will also make it easier for new researchers
to get involved in NLG, by providing data sets and
an evaluation framework.
2 Comparative Evaluations in NLG
There is a long history of shared task initiatives
in NLP, of which the best known is perhaps MUC
(Hirschman, 1998); others include TREC, PARSE-
VAL, SENSEVAL, and the range of shared tasks or-
ganised by CoNLL. Such exercises are now com-
mon in most areas of NLP, and have had a major
impact on many areas, including machine transla-
tion and information extraction (see discussion of
history of shared-task initiatives and their impact
in Belz and Kilgarriff (2006)).
One of the best-known comparative studies
of evaluation techniques was by Papineni et al
(2002) who proposed the BLEU metric for machine
translation and showed that BLEU correlated well
with human judgements when comparing several
machine translation systems. Several other studies
of this type have been carried out in the MT and
Summarisation communities.
The first comparison of NLG evaluation tech-
niques which we are aware of is by Bangalore et al
(2000). The authors manually created several
variants of sentences from the Wall Street Jour-
nal, and evaluated these sentences using both hu-
man judgements and several corpus-based metrics.
They used linear regression to suggest a combina-
tion of the corpus-based metrics which they be-
136
lieve is a better predictor of human judgements
than any of the individual metrics.
In our work (Belz and Reiter, 2006), we used
several different evaluation techniques (human
and corpus-based) to evaluate the output of five
NLG systems which generated wind descriptions
for weather forecasts. We then analysed how well
the corpus-based evaluations correlated with the
human-based evaluations. Amongst other things,
we concluded that BLEU-type metrics work rea-
sonably well when comparing statistical NLG sys-
tems, but less well when comparing statistical NLG
systems to knowledge-based NLG systems.
We worked in this domain because of the avail-
ability of the SumTime corpus (Sripada et al,
2003), which contains both numerical weather
prediction data (i.e., inputs to NLG) and human
written forecast texts (i.e., target outputs from
NLG). We are not aware of any other NLG-related
corpora which contain a large number of texts and
corresponding input data sets, and are freely avail-
able to the research community.
3 Our Proposal
We intend to apply for funding for a three-year
project to create more shared input/output data sets
(we are focusing on data-to-text tasks for the rea-
sons discussed in Belz and Kilgarriff (2006)), or-
ganise shared task workshops, and create and test
a range of methods for evaluating submitted sys-
tems.
3.1 Step 1: Create data sets
We intend to create input/output data sets that con-
tain the following types of representations:
? raw non-linguistic input data;
? structured content representations, roughly
corresponding to document plans (Reiter and
Dale, 2000);
? semantic-level representations, roughly cor-
responding to text specifications (Reiter and
Dale, 2000);
? actual human-authored corpus texts.
The presence of intermediate representations in
our data sets means that researchers who are just
interested in document planning, microplanning,
or surface realisation do not need to build com-
plete NLG systems in order to participate.
We will create the semantic-level representa-
tions by parsing the corpus texts, probably us-
ing a LinGO parser1. We will create the content
representations using application-specific analysis
tools, similar to a tool we have already created for
SumTime wind statements. The actual data sets
we currently intend to create are as follows (see
also summary in Table 1).
SumTime weather statements: These are brief
statements which describe predicted precipitation
and cloud over a forecast period. We will extract
the texts (and the corresponding input data) from
the existing SumTime corpus.
Statistics summaries: We will ask people (prob-
ably students) to write paragraph-length textual
summaries of statistical data. The actual data will
come from opinion polls or national statistics of-
fices. The corpus will also include data about the
authors (e.g., age, sex, domain expertise).
Nurses? reports: As part of a new project at Ab-
erdeen, Babytalk2, we will be acquiring a corpus
of texts written by nurses to summarise the status
of a baby in a neonatal intensive care unit, along
with the raw data this is based on (sensor read-
ings, records of actions taken such as giving med-
ication).
3.2 Step 2: Organise workshops
The second step is to organise workshops. We
intend to use a fairly standard organisation (Belz
and Kilgarriff, 2006). We will release the data
sets (but not the reference texts), give people six
months to develop systems, and invite people who
submit systems to a workshop. Participants can
submit either complete data-to-text NLG systems,
or components which just do document planning,
microplanning, or realisation.
We are planning to increase the number and
complexity of tasks from one round to the next,
as this has been useful in other NLP evaluations
(Belz and Kilgarriff, 2006); for example, we will
add surface realisation as a separate task in round
2 and layout/structuring task in round 3.
We will carry out all evaluation activities (see
below) ourselves, workshop participants will not
be involved in this.
3.3 Step 3: Evaluation
The final step is to evaluate the systems and com-
ponents submitted to the workshop. As the main
1http://lingo.stanford.edu/
2http://www.csd.abdn.ac.uk/research/babytalk/
137
Corpus num texts num ref (*) text size main NLG challenges
Weather statements 3000 300 1-2 sentences content det, lex choice, aggregation
Statistical summaries 1000 100 paragraph above plus surface realisation
Nurses? reports 200 50 several paras above plus text structuring/layout
(*) In addition to the main corpus, we will also gather texts which will be used as reference texts for
corpus-based evaluations; ?num ref? is the number of such texts. These texts will not be released.
Table 1: Planned GENEVAL data sets.
purpose of this whole exercise is to see how well
different evaluation techniques correlate with each
other, we plan to carry out a range of different
evaluations, including the following.
Corpus-based evaluations: We will develop
new, linguistically grounded evaluation metrics,
and compare these to existing metrics including
BLEU, NIST, and string-edit distance. We will also
investigate how sensitive different metrics are to
size and make-up of the reference corpus.
Human-based preference judgements: We will
investigate different experimental designs and
methods for overcoming respondent bias (e.g.
what is known as ?central tendency bias?, where
some respondents avoid judgements at either end
of a scale). As we showed previously (Belz and
Reiter, 2006) that there are significant inter-subject
differences in ratings, one thing we want to deter-
mine is how many subjects are needed to get reli-
able and reproducible results.
Task performance. This depends on the do-
main, but e.g. in the nurse-report domain we
could use the methodology of (Law et al, 2005),
who showed medical professionals the texts, asked
them to make a treatment decision, and then rated
the correctness of the suggested treatments.
As well as recommendations about the appro-
priateness of existing evaluation techniques, we
hope the above experiments will allow us to sug-
gest new evaluation techniques for NLG.
4 Next Steps
At this point, we encourage NLG researchers to
give us their views regarding our plans for the or-
ganisation of GENEVAL, the data and evaluation
methods we are planning to use, to suggest addi-
tional data sets or evaluation techniques, and espe-
cially to let us know whether they would be inter-
ested in participating.
If our proposal is successful, we hope that the
project will start in summer 2007, with the first
data set released in late 2007 and the first work-
shop in summer 2008. ELRA/ELDA have also al-
ready agreed to help us with this work, contribut-
ing human and data resources.
References
Srinavas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation. In
Proceedings of INLG-2000, pages 1?8.
Anja Belz and Adam Kilgarriff. 2006. Shared-task
evaluations in HLT: Lessons for NLG. In Proceed-
ings of INLG-2006.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL-2006, pages 313?320.
Lynette Hirschman. 1998. The evolution of evaluation:
Lessons from the Message Understanding Confer-
ences. Computer Speech and Language, 12:283?
285.
Anna Law, Yvonne Freer, Jim Hunter, Robert Logie,
Neil McIntosh, and John Quinn. 2005. Generat-
ing textual summaries of graphical time series data
to support medical decision making in the neonatal
intensive care unit. Journal of Clinical Monitoring
and Computing, 19:183?194.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-2002, pages 311?318.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter and Somayajulu Sripada. 2002. Should
corpora texts be gold standards for NLG? In Pro-
ceedings of INLG-2002, pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41?58.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Exploiting a parallel text-data corpus. In
Proceedings of Corpus Linguistics 2003, pages 734?
743.
138
Proceedings of the 12th European Workshop on Natural Language Generation, pages 1?8,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Using NLG to Help Language-Impaired Users Tell Stories and  
Participate in Social Dialogues 
 
 
Ehud Reiter, Ross Turner 
University of Aberdeen 
Aberdeen, UK 
e.reiter@abdn.ac.uk 
csc272@abdn.ac.uk 
Norman Alm, Rolf Black, 
Martin Dempster, Annalu Waller 
University of Dundee 
Dundee, UK 
{nalm,rolfblack,martindempster, 
awaller}@computing.dundee.ac.uk 
 
 
Abstract 
Augmentative and Alternative Communication 
(AAC) systems are communication aids for 
people who cannot speak because of motor or 
cognitive impairments.  We are developing 
AAC systems where users select information 
they wish to communicate, and this is ex-
pressed using an NLG system.  We believe 
this model will work well in contexts where 
AAC users wish to go beyond simply making 
requests or answering questions, and have 
more complex communicative goals such as 
story-telling and social interaction. 
1 Introduction 
Many people have difficulty in communicating 
linguistically because of cognitive or motor im-
pairments.  Such people typically use communi-
cation aids to help them interact with other peo-
ple.  Such communication aids range from sim-
ple tools that do not involve computers, such as 
picture cards, to complex software systems that 
attempt to ?speak? for the impaired user. 
From a technological perspective, even the 
most complex communication aids have typi-
cally been based on fixed (canned) texts or sim-
ple fill-in-the-blank templates; essentially the 
user selects a text or template from a set of pos-
sible utterances, and the system utters it.  We 
believe that while this may be adequate if the 
user is simply making a request (e.g., please give 
me a drink) or answering a question (e.g., I live 
at home), it is not adequate if the user has a more 
complex communicative goal, such as engaging 
in social interaction, or telling a story. 
We are exploring the idea of supporting such 
interactions by building a system which uses ex-
ternal data and/or knowledge sources, plus do-
main and conversational models, to dynamically 
suggest possible messages (event, facts, or opin-
ions, represented as ontology instances) which 
are appropriate to the conversation. The user se-
lects the specific message which he wishes the 
system to speak, and possibly adds simple anno-
tations (e.g., I like this) or otherwise edits the 
message.  The system then creates an appropriate 
linguistic utterance from the selected message, 
taking into consideration contextual factors. 
In this paper we describe two projects on 
which we are working within this framework.  
The goal of the first project is to help non-
speaking children tell stories about their day at 
school to their parents; the goal of the second 
project is to help non-speaking adults engage in 
social conversation. 
2 Background 
2.1 Augmentative and alternative commu-
nication 
Augmentative and alternative communication 
(AAC) is a term that describes a variety of meth-
ods of communication for non-speaking people 
which can supplement or replace speech.  The 
term covers techniques which require no equip-
ment, such as sign language and cards with im-
ages; and also more technologically complex 
systems which use speech synthesis and a variety 
of strategies to create utterances.  
The most flexible AAC systems allow users to 
specify arbitrary words, but communication rates 
are extremely low, averaging 2-10 words per 
minute. This is because many AAC users interact 
slowly with computers because of their impair-
ments.  For example, some of the children we 
work with cannot use their hands, so they use 
scanning interfaces with head switches.  In other 
words, the computer displays a number of op-
1
tions to them, and then scans through these, 
briefly highlighting each option.  When the de-
sired option is highlighted, the child selects it by 
pressing a switch with her head.   This is ade-
quate for communicating basic needs (such as 
hunger or thirst); the computer can display a 
menu of possible needs, and the child can select 
one of the items.  But creating arbitrary messages 
with such an interface is extremely slow, even if 
word prediction is used; and in general such in-
terfaces do not well support complex social in-
teractions such as story telling (Waller, 2006).  
A number of research projects in AAC have 
developed prototype systems which attempt to 
facilitate this type of human-human interaction.  
At their most basic, these systems provide users 
with a library of fixed ?conversational moves? 
which can be selected and uttered.  These moves 
are based on models of the usual shape and con-
tent of conversational encounters (Todman & 
Alm, 2003), and for example include standard 
conversational openings and closings, such as 
Hello and How are you. They also include back-
channel communication such as Uh-huh, Great!, 
and Sorry, can you repeat that. 
It would be very useful to go beyond standard 
openings, closings, and backchannel messages, 
and allow the user to select utterances which 
were relevant to the particular communicative 
context and goals.  Dye et al(1998) developed a 
system based on scripts of common interactions 
(Schank & Abelson, 1977).  For example, a user 
could activate the MakeAnAppointment script, 
and then could select utterances relevant to this 
script, such as I would like to make an appoint-
ment to see the doctor.  As the interaction pro-
gressed, the system would update the selections 
offered to the user based on the current stage of 
the script; for example during time negotiation a 
possible utterance would be I would like to see 
him next week. This system proved effective in 
trials, but needed a large number of scripts to be 
generally effective.  Users could author their own 
texts, which were added to the scripts, but this 
was time-consuming and had to be done in ad-
vance of the conversation. 
Another goal of AAC is to help users narrate 
stories. Narrative and storytelling play a very 
important part in the communicative repertoire of 
all speakers (Schank, 1990). In particular, the 
ability to draw on episodes from one?s life his-
tory in current conversation is vital to maintain-
ing a full impression of one?s personality in deal-
ing with others (Polkinghorne, 1991). Story tell-
ing tools for AAC users have been developed, 
which include ways to introduce a story, tell it at 
the pace required (with diversions) and give 
feedback to comments from listeners (Waller, 
2006); but again these tools are based on a li-
brary of fixed texts and templates. 
2.2 NLG and AAC 
Natural language generation (NLG) systems 
generate texts in English and other human lan-
guages from non-linguistic input (Reiter and 
Dale, 2000).  In their review of NLP and AAC, 
Newell, Langer, and Hickey (1998) suggest that 
NLG could be used to generate complete utter-
ances from the limited input that AAC users are 
able to provide.  For example, the Compansion 
project (McCoy, Pennington, Badman 1998) 
used NLP and NLG techniques to expand tele-
graphic user input, such as Mary go store?, into 
complete utterances, such as Did Mary go to the 
store?  Netzer and Elhadad (2006) allowed users 
to author utterances in the symbolic language 
BLISS, and used NLG to translate this to English 
and Hebrew texts. 
In recent years there has been growing interest 
in data-to-text NLG systems (Reiter, 2007); 
these systems generate texts based on sensor and 
other numerical data, supplemented with ontolo-
gies that specify domain knowledge.  In princi-
ple, it seems that data-to-text techniques should 
allow NLG systems to provide more assistance 
than the syntactic help provided by Compansion.  
For example, if the user wanted to talk about a 
recent football (soccer) match, a data-to-text sys-
tem could get actual data about the match from 
the web, and generate potential utterances from 
this data, such as Arsenal beat Chelsea 2-1 and 
Van Persie scored two goals; the user could then 
select one of these to utter. 
In addition to helping users interact with other 
people, NLG techniques can also be used to edu-
cate and encourage children with disabilities.  
The STANDUP system (Manurung, Ritchie et 
al., 2008), for example, used NLG and computa-
tional humour techniques to allow children who 
use AAC devices to generate novel punning 
jokes.  This provided the children with successful 
experiences of controlling language, gave them 
an opportunity to play with language and explore 
new vocabulary (Waller et al, in press). In a 
small study with nine children with cerebral 
palsy, the children used their regular AAC tools 
more and also performed better on a test measur-
ing linguistic abilities after they used STANDUP 
for ten weeks. 
2
3 Our Architecture 
Our goal is help AAC users engage in com-
plex social interaction by using NLG and data-
to-text technology to create potential utterances 
and conversational contributions for the users. 
The general architecture is shown in Figure 1, 
and Sections 4 and 5 describe two systems based 
on this architecture. 
 
 
The system has the following components: 
Data analysis: read in data, from sensors, 
web information sources, databases, and so forth.  
This module analyses this data and identifies 
messages (in the sense of Reiter and Dale 
(2000)) that the user is likely to want to commu-
nicate; this analysis is partially based on domain, 
conversation, and user models, which may be 
represented as ontologies. 
Editing: allow the user to edit the messages.  
Editing ranges from adding simple annotations to 
specify opinions (e.g., add BAD to Arsenal beat 
Chelsea 2-1 if the user is a Chelsea fan), to using 
an on-screen keyboard to type free-text com-
ments.  Users can also delete messages, specify 
which messages they are most likely to want to 
utter, and create new messages.  Editing is done 
before the actual conversation, so the user does 
not have to do this under time pressure.  The 
amount of editing which can be done partially 
depends on the extent of the user?s disabilities. 
Narration: allows the user to select mes-
sages, and perhaps conversational moves (e.g., 
Hello), in an actual conversational context.  Edit-
ing is possible, but is limited by the need to keep 
the conversation flowing. 
NLG and Speech Synthesis: Generates actual 
utterances from the selected messages, taking 
into account linguistic context, especially a dia-
logue model. 
4 Narrative for Children: How was 
School Today 
The goal of the How was School Today project is 
to enable non-speaking children with major mo-
tor disabilities but reasonable cognitive skills to 
tell a story about what they did at school during 
the day.  The particular children we are working 
with have cerebral palsy, and use wheelchairs.  A 
few of them can use touch screens, but most of 
them use a head switch and scanning interface, 
as described above.  By ?story?, we mean some-
thing similar to Labov?s (1972) conversational 
narrative, i.e., a series of linked real-world events 
which are unusual or otherwise interesting, pos-
sibly annotated with information about the 
child?s feelings, which can be narrated orally. 
We are not expecting stories in the literary sense, 
with character development and complex plots. 
The motivation of the project is to provide the 
children with successful narrative experience. 
Typically developing children develop narrative 
skills from an early age with adults scaffolding 
conversations to elicit narrative, e.g. ?What did 
you do at school today?? (Bruner, 1975). As the 
child?s vocabulary and language competence 
develops, scaffolding is reduced. This progres-
sion is seldom seen in children with complex 
communication needs ? they respond to closed 
questions but seldom take control of conversa-
Sensor 
data 
Web info 
sources 
Other 
external data 
Data analysis: 
select possible 
messages to 
communicate 
Conversation 
model 
Domain model 
User model 
Editing: User adds 
annotations 
User 
 
NLG: 
Generate 
utterance 
Dialogue 
model 
Speech 
synthesis 
Conversation 
partner 
 
Narration: User 
selects what to say 
Prepare content 
Narrate content 
Figure 1:  General architecture 
3
tion (von Tetzchner and Grove, 2003).  Many 
children who use AAC have very limited narra-
tive skills (Soto et al 2006). Research has shown 
that providing children who use AAC with suc-
cessful narrative experiences by providing full 
narrative text can help the development of writ-
ten and spoken narrative skills  (Waller, 2008).  
The system follows the architecture described 
above.  Input data comes from RFID sensors that 
track where the child went during the day; an 
RFID reader is mounted on the child?s wheel-
chair, and RFID tags are placed around the 
school, especially in doorways so we can moni-
tor children entering and leaving rooms.  Teach-
ers have also been given RFID swipe cards 
which they can swipe against a reader, to record 
that they are interacting with the child; this is 
more robust than attempting to infer interaction 
automatically by tracking teachers? position. 
Teachers can also record interactions with ob-
jects (toys, musical instruments, etc), by using 
special swipe cards associated with these objects. 
Last but not least, teachers can record spoken 
messages about what happened during the day. 
An example of how the child?s wheelchair is set 
up is shown in Figure 2. 
   
 
 
Figure 2: System configuration 
 
The data analysis module combines sensor-
derived location and interaction data with a time-
table which records what the child was expected 
to do during the day, and a domain knowledge 
base which includes information about typical 
activities (e.g., if the child?s location is Swim-
mingPool, the child?s activity is probably 
Swimming).  From this it creates a series of 
events (each of which contain a number of mes-
sages) which describe the child?s lessons and 
activities, including divergences from what is 
expected in the timetable.  Several messages may 
be associated with an event.  The data analysis 
module also infers which events and messages it 
believes are most interesting to the child; this is 
partially based on heuristics about what children 
are interested in (e.g., swimming is more inter-
esting than lunch), and partially based on the 
general principle that unexpected things (diver-
gences from the timetable) are more interesting 
than expected things.  No more than five events 
are flagged as interesting, and only these events 
are shown in the editing interface. 
The editing interface allows children to re-
move events they do not want to talk about (per-
haps for privacy reasons) from the list of interest-
ing events.  It also allows children to add mes-
sages that express simple opinions about events; 
i.e., I liked it or I didn?t like it.  The interface is 
designed to be used with a scanning interface, 
and is based on symbols that represent events, 
annotations, etc. 
The narration interface, shown in Figure 3, is 
similar to the editing interface. It allows children 
to choose a specific event to communicate, 
which must be one of the ones they selected dur-
ing the editing phase.  Children are encouraged 
to tell events in temporal order (this is one of the 
narration skills we are trying to teach), but this is 
not mandated, and they can deviate from tempo-
ral order if they wish.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Figure 3: Narration Interface 
 
The NLG system generates actual texts from 
the events selected by the children.  Most of this 
Tablet PC with NLG system and 
swipe-card RFID sensor  
long range 
RFID  
sensor for 
location 
tracking 
Events 
Opinion Annotations 
Messages  
for event 
4
is fairly simple, since the system deliberately 
uses simple ?child-like? language (Section 6).  
However, the system does need to make some 
decisions based on discourse context, including 
choosing appropriate referring expressions (es-
pecially pronouns), and temporal expressions 
(especially when children deviate from pure 
temporal order). 
4.1 Example 
For example, assume that the timetable speci-
fies the following information 
 
 
Assume that the sensors then recorded the fol-
lowing information 
 
Event 1 
      Location: CL_SEC2 
      Time: 13:23:00.0 - 14:07:00.0 
      Interactions: Mrs. Smith, Rolf, Ross 
 
Event 2 
      Location: HALL 
      Time: 14:10:00.0 ? 14:39:00.0 
      Interactions: none 
 
The data analysis module associates Event 1 with 
the Arts and Crafts timetable entry, since the lo-
cation is right, the timetabled teacher is present, 
and the times approximately match.  From this 
two messages are produced: one corresponding 
to I had Arts and Crafts this afternoon with Mrs. 
Smith (the core activity description), and the oth-
er corresponding to Rolf and Ross were there 
(additional information about people not time-
tabled to be there).  The child can add opinions 
using the editing interface; for example, if he 
added a positive annotation to the event, this 
would become an additional message corre-
sponding to It was great. 
For Event 2, the data analysis module notes 
that it does not match a timetabled event. The 
timetable indicates the child should be at Physio-
therapy after Art and Crafts; however, the sensor 
information indicates they were in the hall. The 
system generates a single message corresponding 
to Then I went to the Hall instead of Physiother-
apy to describe this event.  If the child added a 
negative annotation to this message, this would 
become an additional message expressed as I 
didn?t like it. 
4.2 Evaluation 
We conducted an initial evaluation of the How 
was School Today system in January, 2009.  
Two children used the system for four days: Ju-
lie, age 11, who had good cognitive skills but 
was non-verbal because of severe motor impair-
ments; and Jessica, age 13, who had less severe 
motor impairments but who had some cognitive 
and memory impairments (these are not the chil-
drens? real names).  Julie used the system as a 
communication and interaction aid, as described 
above; Jessica used the system partially as a 
memory aid.  The evaluation was primarily 
qualitative: we observed how Julie and Jessica 
used the system, and interviewed their teachers, 
speech therapists, care assistants, and Julie?s 
mother (Jessica?s parents were not available). 
The system worked very well for Julie; she 
learned it quickly, and was able to use it to have 
real conversations about her day with adults, al-
most for the first time in her life.  This validated 
our vision that our technology could help AAC 
users engage in real interaction, and go beyond 
simple question answering and communication 
of basic needs.  The system also worked rea-
sonably well as a memory aid for Jessica, but she 
had a harder time using it, perhaps because of her 
cognitive impairments. 
Staff and Julie?s mother were very supportive 
and pleased with the system.  They had sugges-
tions for improving the system, including a wider 
range of annotations; more phrases about the 
conversation itself, such as Guess what happened 
at school today; and allowing children to request 
teenager language (e.g., really cool). 
From a technical perspective, the system 
worked well overall.   School staff were happy to 
use the swipe cards, which worked well.  There 
were some problems with the location sensors, 
we need better techniques for distinguishing real 
readings from noise.  A surprising amount of 
effort was needed to enter up-to-date knowledge 
(e.g., daily lunch menus), this would need to be 
addressed if the system was used for a period of 
months as opposed to days. 
5 Social Conversation for Adults 
In our second project, we want to build a tool to 
help adults with cerebral palsy engage in social 
conversation about a football match, movie, 
weather, and so forth.  Many people with severe 
disabilities have great difficulty developing new 
interpersonal relationships, and indeed report that 
forming new relationships and taking part in new 
Time Activity Location Teacher 
?? ?? ?? ?? 
13.20 -14 Arts and 
Crafts 
CL_SEC2 Mrs Smith 
14 -14.40 Physiotherapy PHYSIO1 Mrs Jones 
?? ?? ?? ?? 
5
activities are major priorities in their lives (Datil-
lo et al, 2007).  Supporting these goals through 
the development of appropriate technologies is 
important as it could lead to improved social out-
comes. 
This project builds on the TALK system 
(Todman and Alm, 2003), which helped AAC 
users engage in active social conversation. 
TALK partially overcame the problem of low 
communication rate by requiring users to pre-
author their conversational material ahead of 
time, so that when it was needed it could simply 
be selected and output. TALK also used insights 
from Conversation Analysis (Sacks, 1995) to 
provide appropriate functionality in the system 
for social conversation. For example, it sup-
ported opening and closing statements, stepwise 
topic change, and the use of quick-fire utterances 
to provide fast, idiomatic responses to commonly 
encountered situations. This approach led to 
more dynamic AAC-facilitated interactions with 
higher communication rates, and had a positive 
impact on the perceived communicative compe-
tence of the user (Todman, Alm et al, 2007).   
TALK requires the user to spend a substantial 
amount of time pre-authoring material; this is 
perhaps its greatest weakness.  Our idea is to re-
duce the amount of pre-authoring needed, by us-
ing the architecture shown in Fig 1, where much 
of the material is automatically created from data 
sources, ontologies, etc, and the user?s role is 
largely to edit and annotate this material, not to 
create it from scratch. 
We developed an initial prototype system to 
demonstrate this concept in the domain of foot-
ball results (Dempster, 2008).  We are now 
working on another prototype, whose goal is to 
support social conversations about movies, mu-
sic, television shows, etc (which is a much 
broader domain than football).  We have created 
an ontology which can describe events such as 
watching a film, listening to a music track, or 
reading a book.  Each ?event? has both temporal 
and spatial properties which allow descriptions to 
be produced about where and when an event took 
place, and other particulars relating to that par-
ticular class of event.  For example, if the user 
listened to a radio show, we record the name of 
the show, the presenter and the station it was 
broadcast on.  Ultimately we plan to obtain in-
formation about movies, music tracks, etc from 
web-based databases such as IMDB (movies) 
and last.fm (music). 
Of course, databases such as IMDB do not 
contain information such as what the user 
thought of the movie, or who he saw it with.  
Hence we will allow users to add annotations 
with such information.  Some of these annota-
tions will be entered via a structured tool, such as 
a calendar interface that allows users to specify 
when they watched or listened to something. We 
would like to use NaturalOWL (Galanis and An-
droutsopoulos, 2007) as the NLG component of 
the system; it is well suited to describing objects, 
and is intended to be integrated with an ontology.  
As with the How Was School Today project, 
some of the main low-level NLG challenges are 
choosing appropriate referring expressions and 
temporal references, based on the current dis-
course context.  Speech output is done using Ce-
reproc (Aylett and Pidcock, 2007). 
An example of our current narration interface 
is shown in Figure 4.  In the editing interface, the 
user has specified that he went to a concert at 
8pm on Thursday, and that he rated it 8 out of 
10.  The narration interface gives the user a 
choice of a number of messages based on this 
information, together with some standard mes-
sages such as Thanks and Agree. 
 
 
 
Note that unlike the How Was School Today 
project, in this project we do not attempt to infer 
event information from sensors, but we allow 
(and expect) the user to enter much more infor-
mation at the editing stage.  We could in princi-
ple use sensors to pick up some information, 
such as the fact that the user was in the cinema 
from 12 to 2PM on Tuesday, but this is not the 
research focus of this project. 
We plan to evaluate the system using groups 
of both disabled and non-disabled users.  This 
has been shown in the past to be an effective ap-
proach for the evaluation of prototype AAC sys-
tems (Higginbotham, 1995). Initially pairs of 
non-disabled participants will be asked to pro-
duce short conversations with one person using 
the prototype and the other conversing normally.   
Quantitative measures of the communication rate 
6
will be taken as well as more qualitative observa-
tions relating to the usability of the system.  Af-
ter this evaluation we will improve the system 
based on our findings, and then conduct a final 
evaluation with a small group of AAC users. 
6 Discussion: Challenges for NLG 
From an NLG perspective, generating AAC texts 
of the sort we describe here presents different 
challenges from many other NLG applications. 
First of all, realization and even microplanning 
are probably not difficult, because in this context 
the AAC system should generate short simple 
sentences if possible.  This is because the system 
is speaking ?for? someone with limited or devel-
oping linguistic abilities, and it should try to pro-
duce something similar to what the user would 
say himself if he or she had the time to explicitly 
write a text using an on-screen keyboard. 
To take a concrete example, we had originally 
considered using past-perfect tense (a fairly 
complex linguistic construct) in the How was 
School project, when the narrative jumped to an 
earlier point in time.  For example I ate lunch at 
12.  I had gone swimming at 11.  But it was clear 
from corpora of child-written texts that these 
children never used perfect tenses, so instead we 
opted for I ate lunch at 12.  I went swimming at 
11.  This is less linguistically polished, but much 
more in line with what the children might actu-
ally produce. 
Given this desire for linguistic simplicity, re-
alisation is very simple, as is lexical choice (use 
simple words) and aggregation (keep sentences 
short).  The main microplanning challenges re-
late to discourse coherence, in particular refer-
ring expressions and temporal descriptions.   
On the other hand, there are major challenges 
in document planning.  In particular, in the How 
Was School project, we want the output to be a 
proper narrative, in the sense of Labov (1972).  
That is, not just a list of facts and events, but a 
structure with a beginning and end, and with ex-
planatory and other links between components 
(e.g., I had math in the afternoon because we 
went swimming in the morning, if the child nor-
mally has math in the morning).  We also wanted 
the narrative to be interesting and hold the inter-
est of the person the child is communicating 
with.  As pointed out by Reiter et al(2008), cur-
rent NLG systems do not do a good job of gener-
ating narratives.  
Similarly, in the Social Conversations project 
we want the system to generate a social dialogue, 
not just a list of facts about movies and songs.  
Little previous research has been done on gener-
ating social (as opposed to task-oriented) dia-
logues.  One exception is the NECA Socialite 
system (van Deemter et al 2008), but this fo-
cused on techniques for expressing affect, not on 
high-level conversational structure. 
For both stories and social conversations, it 
would be extremely useful to be able to monitor 
what the conversational partner is saying.  This is 
something we hope to investigate in the future.  
As most AAC users interact with a small number 
of conversational partners, it may be feasible to 
use a speech dictation system to detect at least 
some of what the conversational partner says. 
Last but not least, a major challenge implicit 
in our systems and indeed in the general architec-
ture is letting users control the NLG system.   
Our systems are intended to be speaking aids, 
ideally they should produce the same utterances 
as the user would if he was able to talk.  This 
means that users must be able to control the sys-
tems, so that it does what they want it to do, in 
terms of both content and expression.  To the 
best of our knowledge, little is known about how 
users can best control an NLG system. 
7 Conclusion 
Many people are in the unfortunate position of 
not being able to speak or type, due to cognitive 
and/or motor impairments.  Current AAC tools 
allow such people to engage in simple needs-
based communication, but they do not provide 
good support for richer use of language, such as 
story-telling and social conversation.  We are 
trying to develop more sophisticated AAC tools 
which support such interactions, by using exter-
nal data and knowledge sources to produce can-
didate messages, which can be expressed using 
NLG and speech synthesis technology.  Our 
work is still at an early stage, but we believe that 
it has the potential to help AAC users engage in 
richer interactions with other people.  
Acknowledgements 
We are very grateful to Julie, Jessica, and their 
teachers, therapists, carers, and parents for their 
help in building and evaluating the system de-
scribed in Section 4.  Many thanks to the anony-
mous referees and our colleagues at Aberdeen 
and Dundee for their very helpful comments.  
This research is supported by EPSRC grants 
EP/F067151/1 and EP/F066880/1, and by a 
Northern Research Partnership studentship. 
7
References 
Aylett, M. and C. Pidcock (2007). The CereVoice 
Characterful Speech Synthesiser SDK. Proceed-
ings of Proceedings of the 7th International Con-
ference on Intelligent Virtual Agents, pages 413-
414. 
Bruner, J. (1975). From communication to language: 
A psychological perspective. Cognition 3: 255-
289. 
Datillo, J., G. Estrella, L. Estrella, J. Light, D. 
McNaughton and M. Seabury (2007). "I have cho-
sen to live life abundantly": Perceptions of leisure 
by adults who use Augmentative and Alternative 
Communication. Augmentative & Alternative 
Communication 24(1): 16-28. 
van Deemter, K., B Krenn, P Piwek, M Klesen, M 
Schr?der and S Baumann. Fully generated scripted 
dialogue for embodied agents. Artificial Intelli-
gence 172: 1219?1244. 
Dempster, M. (2008). Using natural language genera-
tion to encourage effective communication in non-
speaking people. Proceedings of Young Research-
ers Consortium, ICCHP'08. 
Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morri-
son (1998). A script-based AAC system for trans-
actional interaction.  Natural Language Engineer-
ing, 4(1), 57-71. 
Galanis, D. and I. Androutsopoulos (2007). Generat-
ing Multilingual Descriptions from Linguistically 
Annotated OWL Ontologies: the NaturalOWL Sys-
tem. Proceedings of ENLG 2007. 
Higginbotham, D. J. (1995). Use of nondisabled sub-
jects in AAC Research : Confessions of a research 
infidel. Augmentative and Alternative Communica-
tion 11(1): 2-5. 
Labov, W (1972).  Language in the Inner City. Uni-
versity of Pennsylvania Press. 
Manurung, R., G. Ritchie, H. Pain, A. Waller, D. 
O'Mara and R. Black (2008). The Construction of a 
Pun Generator for Language Skills Development. 
Applied Artificial Intelligence 22(9): 841 ? 869. 
McCoy, K., C. Pennington and A. Badman (1998). 
Compansion: From research prototype to practical 
integration. Natural Language Engineering 4:73-
95. 
Netzer, Y and Elhadad, M (2006). Using Semantic 
Authoring for Blissymbols Communication 
Boards. In Proc of HLT-2006. 
Newell, A., S. Langer and M. Hickey (1998). The role 
of natural language processing in alternative and 
augmentative communication. Natural Language 
Engineering 4:1-16. 
Polkinghorne, D. (1991). Narrative and self-concept. 
Journal of Narrative and Life History, 1(2/3), 135-
153 
Reiter, E (2007). An Architecture for Data-to-Text 
Systems. In Proceedings of ENLG-2007, pages 
147-155. 
Reiter, E. and R. Dale (2000).  Building Natural Lan-
guage Generation Systems.  Cambridge University 
Press. 
Reiter, E,  A. Gatt, F Portet, and M van der Meulen 
(2008). The Importance of Narrative and Other 
Lessons from an Evaluation of an NLG System 
that Summarises Clinical Data (2007). In Proceed-
ings of INLG-2008, pages 97-104. 
Sacks, H. (1995). Lectures on Conversation. G. Jef-
ferson. Cambridge, MA, Blackwell. 
Schank, R. C. (1990). Tell me a story: A new look at 
real and artificial intelligence. New York, Macmil-
lan Publishing Co. 
Schank, R., and R. Abelson (1977).  Scripts, plans, 
goals, and understanding. New Jersey: Lawrence 
Erlbaum. 
Soto, G., E. Hartmann, and D. Wilkins (2006). Ex-
ploring the Elements of Narrative that Emerge in 
the Interactions between an 8-Year-Old Child who 
uses an AAC Device and her Teacher. Augmenta-
tive and Alternative Communication 4:231 ? 241. 
Todman, J. and N. A. Alm (2003). Modelling conver-
sational pragmatics in communication aids. Jour-
nal of Pragmatics 35: 523-538. 
Todman, J., N. A. Alm, D. J. Higginbotham and P. 
File (2007). Whole Utterance Approaches in AAC. 
Augmentative and Alternative Communication 
24(3): 235-254. 
von Tetzchner, S. and N. Grove (2003). The devel-
opment of alternative language forms. In S. von 
Tetzchner and N. Grove (eds), Augmentative and 
Alternative Communication: Developmental Issues, 
pages 1-27. Wiley. 
Waller, A. (2006). Communication Access to Conver-
sational Narrative. Topics in Language Disorders 
26(3): 221-239. 
Waller, A. (2008). Narrative-based Augmentative and 
Alternative Communication: From transactional to 
interactional conversation. Proceedings of ISAAC 
2008, pages 149-160.  
Waller, A., R. Black, D. A. O'Mara, H. Pain, G. Rit-
chie and R. Manurung (In Press). Evaluating the 
STANDUP Pun Generating Software with Chil-
dren with Cerebral Palsy. ACM Transactions on 
Accessible Computing. 
8
Proceedings of the 12th European Workshop on Natural Language Generation, pages 42?49,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Generating Approximate Geographic Descriptions
Ross Turner, Yaji Sripada and Ehud Reiter
Dept of Computing Science,
University of Aberdeen, UK
{r.turner,yaji.sripada,e.reiter}@abdn.ac.uk
Abstract
Georeferenced data sets are often large and
complex. Natural Language Generation
(NLG) systems are beginning to emerge that
generate texts from such data. One of the
challenges these systems face is the gener-
ation of geographic descriptions referring to
the location of events or patterns in the data.
Based on our studies in the domain of me-
teorology we present a two staged approach
to generating geographic descriptions. The
first stage involves using domain knowledge
based on the task context to select a frame
of reference, and the second involves using
constraints imposed by the end user to select
values within a frame of reference. Because
geographic concepts are inherently vague our
approach does not guarantee a distinguish-
ing description. Our evaluation studies show
that NLG systems, because they can analyse
input data exhaustively, can produce more
fine-grained geographic descriptions that are
more useful to end users than those generated
by human experts.
1 Introduction
Disciplines such as environmental studies, geography,
geology, planning and business marketing make exten-
sive use of Geographical Information Systems (GIS);
however, despite an explosion of available mapping
software, GIS remains a specialist tool with special-
ist skills required to analyse and understand the infor-
mation presented using map displays. Complement-
ing such displays with textual summaries therefore pro-
vides an immediate niche for NLG systems.
Recently, research into NLG systems that gener-
ate text from georeferenced data has begun to emerge
(Dale et al, 2005; Turner et al, 2006; Turner et al,
2008b; Thomas and Sripada, 2008). These systems are
required to textually describe the geographic distribu-
tion of domain variables such as road surface temper-
ature and unemployment rates. For example, descrip-
tions such as ?road surface temperatures will fall below
zero in some places in the southwest? and ?unemploy-
ment is highest in the rural areas? need to be generated
by these systems. One of the main challenges such sys-
tems face is the generation of geographic descriptions
such as ?in some places in the southwest? and ?in the
rural areas?. Such a task is challenging for a number of
reasons:
? many geographic concepts are inherently vague
(see for example (Varzi, 2001) for a discussion on
this topic);
? often the underlying data sets contain little explicit
geographic information for a generation system to
make use of (Turner et al, 2008b);
? as input to a generation system, georeferenced
data is often complex, constraints imposed on the
output text (such as length) may make the tradi-
tional approach to the Referring Expression Gen-
eration (REG) problem in NLG of finding a dis-
tinguishing description implausible (Turner et al,
2008b).
This paper looks at the problem in the context of
work the authors have carried out on summarising geo-
referenced data sets in the meteorology domain. The
main feature of our approach is that geographic de-
scriptions perform the dual function of referring to
a specific geographic locations unambiguously (tradi-
tional function of REG) and also communicate the re-
lationship between the domain information and the ge-
ography of the region (novel function of geographic de-
scriptions).
We present a two staged approach to generating ge-
ographic descriptions that involve regions. The first
stage involves using domain knowledge (meteorolog-
ical knowledge in our case) to select a frame of ref-
erence and the second involves using constraints im-
posed by the end user to select values within a frame
of reference. While generating geographic descriptions
it is not always possible to produce a distinguishing
description because of the inherent vagueness in ge-
ographic concepts. Therefore, in our case we aim to
produce a distinguishing description wherever possi-
ble, but more often allow non-distinguishing descrip-
tions in the output text, which approximate the location
of the event being described as accurately as possible.
After a short overview of the background in ?2,
some empirical observations on geographic descrip-
42
tions from knowledge acquisition (KA) studies we have
carried out are discussed in ?3. Taking these observa-
tions into account, in ?4 we describe how this problem
is approached using examples from RoadSafe (Turner
et al, 2008b), which generates spatial references to
events in georeferenced data in terms of regions that
approximate their location. It pays particular attention
to the use of different perspectives to describe the same
situation and how factors that affect what makes a good
reference in this domain are taken into account by the
system. In ?5 we present a qualitative discussion of as-
pects of geographic description from the evaluations of
RoadSafe that were carried out, and how this relates to
future possible work on this topic.
2 Background
Much work on generation of spatial descriptions has
concentrated on smaller scale spaces that are imme-
diately perceivable. For example, spatial descriptions
have been studied from the perspective of robot com-
munication (Kelleher and Kruijff, 2006), 3D anima-
tion (Towns et al, 1998) and basic visual scenes (Vi-
ethen and Dale, 2008; Ebert et al, 1996). In a more
geographical context route description generation sys-
tems such as (Dale et al, 2005) and (Moulin and Ket-
tani, 1999) have had wide appeal to NLG researchers.
(Varges, 2005) also generate landmark based spatial de-
scriptions using maps from the map task dialogue cor-
pus.
RoadSafe is an NLG system that has been opera-
tionally deployed at Aerospace and Marine Interna-
tional (AMI) to produce weather forecast texts for win-
ter road maintenance. It generates forecast texts de-
scribing various weather conditions on a road network
as shown in Figure 1.
The input to the system is a data set consisting of
numerical weather predictions (NWP) calculated over
a large set of point locations across a road network. An
example static snapshot of the input to RoadSafe for
one parameter is shown in Figure 2. The complete in-
put is a series of such snapshots for a number of param-
eters (see (Turner et al, 2008b) for details).
In applications such as RoadSafe, the same geo-
graphical situation can be expressed in a variety of dif-
ferent ways dependent upon the perspective employed,
henceforth termed as a frame of reference. Space (ge-
ographic or otherwise) is inherently tied to a frame of
reference that provides a framework for assigning dif-
ferent values to different locations in space. For ex-
ample, locations on Earth?s surface can be specified by
latitude and longitude which provide an absolute frame
of reference for geographic space. Cardinal directions
such as {North, East, West and South} provide an alter-
native frame of reference for geographic space. As was
noted in (Turner et al, 2008b), characterising the data
in terms of frames of reference is important because
often the only geographic information input data con-
tains are coordinates (latitude and longitude), while the
Overview: Road surface temperatures will fall
below zero on all routes during the late evening until
around midnight.
Wind (mph): NE 15-25 gusts 50-55 this afternoon
in most places, backing NNW and easing 10-20
tomorrow morning, gusts 30-35 during this evening
until tomorrow morning in areas above 200M.
Weather: Snow will affect all routes at first,
clearing at times then turning moderate during
tonight and the early morning in all areas, and
persisting until end of period. Ice will affect
all routes from the late evening until early morn-
ing. Hoar frost will affect some southwestern
and central routes by early morning. Road surface
temperatures will fall slowly during the evening
and tonight, reaching zero in some far southern
and southwestern places by 21:00. Fog will af-
fect some northeastern and southwestern routes dur-
ing tonight and the early morning, turning freezing
in some places above 400M.
Figure 1: RoadSafe forecast text showing geographic
descriptions underlined
output texts are required to employ a wider choice of
frames of reference such as altitude, direction, coastal
proximity and population. In RoadSafe the frames of
reference employed are always absolute according to
Levinson?s terminology (Levinson, 2003).
Because the geographic descriptions in RoadSafe do
not fit the traditional formulation of the REG problem
as finding the most distinguishing description, the most
pressing question to address is what makes an adequate
reference strategy in this case? This is of course a dif-
ficult question and is reliant to a large extent on the
communication goal of the system. This paper looks
into this problem in the context of the RoadSafe appli-
cation, that uses a simple spatial sublanguage to gener-
ate the types of descriptions required in this application
domain.
3 Observations on geographic
descriptions from the weather domain
In this section we summarise some empirical observa-
tions on how meteorologists use geographic descrip-
tions in weather forecasts. It describes work carried
out over the course of the RoadSafe project involving
knowledge acquisition (KA) studies with experts on
summarising georeferenced weather data, observations
from data-text corpora (one aimed at the general pub-
lic and one aimed at experts) and a small study with
people from the general public. During RoadSafe we
built two prototype georeferenced data-to-text systems
that summarised georeferenced weather data: one that
produces pollen forecasts based on very simple data
(Turner et al, 2006), and the RoadSafe system, which
43
Figure 2: Input data for ?reaching zero in some far southern and southwestern places? in Figure 1
generates road ice forecasts based on complex data.
Small corpora consisting of forecast texts and their un-
derlying NWP data were collected in both application
domains. Using techniques described in (Reiter et al,
2005) these corpora have been analysed to understand
the experts? strategies to describe georeferenced data.
The major finding from our studies is the fact that
experts tailor their geographic descriptions to the task
context. Not only does the geographic knowledge of
the end user have to be taken into account in their de-
scriptions, but also how the geography of the region
causes events and patterns in the data. The latter con-
sideration has a large affect on the frame of reference
experts employ to describe particular geographic situ-
ations. ?3.1 looks at these observations from the point
of view of end users of weather forecasts, while ?3.2
looks at the descriptive strategies of experts.
3.1 End users? geographic knowledge
It is a well known and accepted fact that geographic
knowledge varies greatly between individuals. To il-
lustrate this point 24 students of a further education
college in Scotland were asked a geography question,
without reference to a map. Which of four major place
names in Scotland (Ayr, Glasgow, Isle of Arran and
Stirling) did they consider to be in the south west of
the country? The responses showed a great variation
in the subjects? geographic knowledge. Half of all sub-
jects considered Glasgow and Ayr to be in the south
west, one third considered Stirling to be in the south
west and most surprisingly only four considered this to
be true of the Isle of Arran. The results of this study
are surprising because Stirling is the least south west-
erly place in the list while Isle of Arran is the most
south westerly. This study actually agrees well with
the studies in psychology on variation in individuals?
mental representation of their geographic environment
(Tversky, 1993).
Contrast this with the detailed knowledge of a road
engineer who the RoadSafe texts are intended for. Road
engineers rely upon a large amount of local geographic
knowledge and experience when treating roads. In-
deed, their spatial mental models are specified at a
much finer detail. For example, they get to know
where frost hollows tend to form and also come to learn
of particular unexpected black spots, such as where
garages allow hose water to cover part of a road during
winter. This is an important point to be taken into ac-
count when communicating georeferenced data as geo-
graphic descriptions should be sensitive to that knowl-
edge because it dictates how accurately they will be in-
terpreted by the end user.
Both task context and structural features of data (e.g.
number of observations, granularity of measurement),
as well as functional features of data (how the entities
being described function in space) influence how it is
44
described geographically. Analysis of a small pollen
forecast corpus (Turner et al, 2006) revealed that fore-
cast texts, contain a rich variety of spatial descrip-
tions for a location despite the data containing only six
data points for the whole of Scotland. In general, the
same region could be referred to by its proper name
e.g. Sutherland and Caithness, by its relation to a well
known geographical landmark e.g. North of the Great
Glen, or simply by its geographical location on the map
e.g. the far North and Northwest. In other words, ex-
perts characterise the limited geographic information
contained within the data according to the task context.
As the consumers of such forecasts are the general pub-
lic, there is a greater onus on the expert to make the
texts more interesting, unlike more restricted domains
such as marine (see (Reiter et al, 2005)) or road ice
forecasts that require consistent terminology.
3.2 Experts? descriptive strategy
Work in psychology has suggested that meteorologists
use a dynamic mental model to arrive at an inference to
predict and explain weather conditions (Trafton, 2007).
Vital to this process is also their ability to take into
account how the geography of a region influences the
general weather conditions. Understanding the weath-
ers interaction with the terrain enables them to make
reliable meteorological inferences particularly when a
certain pattern in the data may appear random. It is
often unfeasible for a human forecaster to spend large
amounts of time inspecting every data point in a de-
tailed visual display. Using experience and expertise a
forecaster can use her mental model to ?play out dif-
ferent hypothetical situations? (Trafton, 2007, p.2) and
thus arrive at a plausible explanation for an apparently
random weather pattern. Consider the following exam-
ple description of a weather event by an expert taken
from our road ice corpus:
? ?exposed locations may have gales at times.?
This is a good example of a forecaster using her me-
teorological expertise to make an inference about a ran-
dom weather pattern. Clearly there is no way from
inspection of a map one can ascertain with certainty
where the exposed locations are in a region. How-
ever, an expert?s knowledge of how the referent entity
(the wind parameter) is affected by geographical fea-
tures allow her to make such an inference. These prag-
matic factors play a large part in determining an experts
descriptive strategy, where certain frames of reference
may be considered more appropriate to describe certain
weather events (Turner et al, 2008a). This comes from
weather forecasters? explicit knowledge of spatial de-
pendence (the fact that observations points in georefer-
enced data at nearby locations are related, and the val-
ues of their non-spatial attributes will be influenced by
certain geographical features). This is one of the most
important and widely understood fact about spatial data
from an analysis point of view, and one of the main rea-
sons that it requires special treatment in comparison to
other types of non-spatial data. This fact is most clearly
outlined by an observation made in (Tobler, 1970, p.3)
that ?everything is related to everything else, but near
things are more related than distant things?. This is
commonly known as the first law of geography and still
resonates strongly today amongst geographers (Miller,
2004). The implication of Tobler?s first law (TFL) is
that samples in spatial data are not independent, and
observations located at nearby locations are more likely
to be similar. Recasting this into meteorological terms,
exposed locations are more likely to be windier and el-
evated areas colder for example.
In fact, an analogy can be drawn between how me-
teorologists consider perspectives in their descriptive
strategy and the preferred attribute list in the semi-
nal work on REG by (Dale and Reiter, 1995). In
their specification of an algorithm for generating refer-
ring expressions content selection is performed through
the iteration over a pre-determined and task specific
list of attributes. In our context, preferred attributes
are replaced by preferred frames of reference. This
means describing georeferenced data requires situa-
tional knowledge of when to apply a particular frame
of reference given a particular geographic distribution
to describe.
The most striking observation about the expert strat-
egy is that the geographic descriptions in the corpora
are approximations of the input (Turner et al, 2008a).
The input is highly overspecified with 1000s of points
for a small forecast region, sampled at sub hourly inter-
vals during a forecast period. Meteorologists use vague
descriptions in the texts to refer to weather events such
as:
? ?in some places in the south, temperatures will
drop to around zero or just above zero.?
There are a number of reasons they use this descrip-
tive strategy: the forecasts are highly compressed sum-
maries, as a few sentences describes megabytes of data;
very specific descriptions are avoided unless the pat-
tern in the data is very clear cut; experts try to avoid
misinterpretation, road engineers often have detailed
local geographic knowledge and experts may not be
aware the more provincial terminology they use to refer
to specific areas. The following section demonstrates
how the problem of generating such descriptions is ad-
dressed in RoadSafe.
4 Generating Approximate Geographic
Descriptions
In its current form, where summaries are meant to give
a brief synopsis of conditions to the user, RoadSafe
follows the approach taken by forecasters as discussed
previously. This is unconventional in comparison to
traditional REG approaches that aim to rule out all dis-
tractors in the domain (properties that are not true of
the referent). In a description such as ?reaching zero
45
in some places above 100M by 16:00? above, distrac-
tors can be defined as the set of points above 100M that
do not satisfy the premise that temperatures will drop
below zero. More succinctly, these can be defined as
false positives. In fact, the problem can be formulated
as a trade off between false positives and false nega-
tives, where false negatives constitute points that are
wrongly omitted from the description. For road grit-
ting purposes, costs can be assigned to each type of
error: road accidents in the case of false negatives and
wasted salt in the case of false positives. As the task
dictates, with the higher associated cost it is impera-
tive that a referring expression eliminates all false neg-
atives. Ideally a truly optimal description should then
seek to minimise false positives as far as possible, thus
reducing the overall cost for the reader. While reduc-
ing errors descriptions should also be meteorologically
correct, as discussed in the previous section. Using cer-
tain frames of reference in certain contexts may result
in a poor inference about a particular weather situation
(Turner et al, 2008b).
Given this domain knowledge, we can formulate
constraints for what makes a good approximate geo-
graphic description in this task context:
1. Meteorological correctness (inferencing about
causal relationships).
2. Minimise false positives.
3. Complete coverage of the event being described
(no false negatives).
These constraints have been realized in a two staged
approach to generating geographic descriptions. The
first stage involves using domain knowledge (meteo-
rological knowledge in our case) to select a frame of
reference, while the second accounts for end-user con-
straints to select values within that frame of reference.
Before we describe the individual stages, two necessary
pre-processing stages for generation are described.
4.1 Geographic characterisation
As noted in ?2, observations in georeferenced data of-
ten contain little explicit geographic information apart
from their coordinates. Geographic characterisation is
responsible for assigning a set of qualitative descrip-
tors to each observation based upon a set of reference
frames, such that observations can be collectively dis-
tinguished from each other. This provides both a cri-
terion for partitioning the data, and a set of properties
to generate geographic descriptions. A frame of ref-
erence in this context consists of a set of descriptions
based upon a common theme such as coastal proximity
e.g. {inland,coastal} or population e.g. {urban,rural}.
In RoadSafe four frames of reference have been imple-
mented: altitude, coastal proximity, population and di-
rection. Those that make use of human (population)
and physical geographical features (altitude, coastal
Proximity) can be represented by existing GIS data
sets; therefore, in these cases geographic characterisa-
tion is simply responsible for mapping observation co-
ordinates to areas of these data sets. In contrast, direc-
tions are abstract and require definition. In RoadSafe,
geographic characterisation maps each observation to a
set of directional areas with crisp boundaries, described
in the following section.
4.2 Pattern formation
To generate descriptions, the geographic distribution
of the event to be communicated has to be approxi-
mated using data analysis techniques such as cluster-
ing. While not new to data-to-text systems, the novel
aspect here is that the data is partitioned based upon
the frames of reference that make up the spatial sublan-
guage of the system. This process summarises the lo-
cation of the event by measuring its density within each
frame of reference?s set of descriptions. An example of
such a distribution is shown in Figure 3.
Reference Frame Description Proportion
Altitude
100M 0.033
200M: 0.017
300M 0.095
400M 0.042
Direction
SSE 0.037
SSW 0.014
WSW: 0.048
TSE 0.489
TSW 0.444
Population
Rural: 0.039
Figure 3: Density of zero temperatures in Figure 2
While the descriptions within each frame of refer-
ence with human and geographical features are dictated
by the granularity of available GIS data sets (altitude
resolution for example), the boundaries of directional
areas require definition. In RoadSafe, because some
flexibility in the generated geographic descriptions is
desirable, the system uses a four by four grid to split
the domain into sixteen equally sized directional areas
defined by their their latitude longitude extents. This
configuration is shown below where T stands for true
and C for central in this case:
TNW NNW NNE TNE
WNW CNW CNE ENE
WSW CSW CSE ESE
TSW SSW SSE TSE
Using a simple set of adjacency matrices based on
this grid, RoadSafe represents a set of descriptions de-
picting the traditional eight main points of the compass
plus a further five that we term gradable (central, far
south, far north, far east and far west). Alternative con-
46
figurations using a greater number of gradable descrip-
tions are possible. These matrices are used by the mi-
croplanner to choose attributes to refer to events using
the direction frame of reference. One example matrix
for each category of directional description are listed
below. In each matrix a value of 1 indicates that the
event has a non-zero density in that area.
Gradable
? Far South:
{TSW,SSW,SSE, TSE} =
?
??
0 0 0 0
0 0 0 0
0 0 0 0
1 1 1 1
?
??
Intercardinal
? South West:
{TSW,WSW,SSW,CSW} =
?
??
0 0 0 0
0 0 0 0
1 1 0 0
1 1 0 0
?
??
Cardinal
? South:
SouthEast ? SouthWest =
?
??
0 0 0 0
0 0 0 0
1 1 1 1
1 1 1 1
?
??
In what follows we describe how our two stage strat-
egy is implemented in our system.
4.3 Frame of reference selection
The main content selection decision made by the doc-
ument planner is the choice of which frame of refer-
ence to describe a specific weather event such as wind
gusts increasing or road surface temperature falling be-
low zero. This decision is based upon both the location
of the event as discussed previously, and situational
knowledge stored in the knowledge base of the system.
Frames of reference where all descriptions have non-
zero densities are not considered. Situational knowl-
edge consists of the probability of using each frame of
reference given the context (the weather parameter to
describe), and is based on corpus frequencies. Rather
than simply choosing the frame of reference with the
highest density, weighting each frame of reference in
this way ensures meteorological correctness as far as
possible.
4.4 Attribute selection
Once a frame of reference has been selected the mi-
croplanner maps the descriptions to abstract syntax
templates. As this is fairly trivial for most frames of
reference in RoadSafe, because they contain a limited
number of descriptions, we will provide an example
how this is accomplished for directional descriptions.
The input to the microplanner is a structure comprised
of the density of the event within the containing area
plus its associated adjacency matrix as shown in Figure
4.
Location {Pointratio : 0.21
Relation : in
Container :
?
???
0 0 0 0
0 0 0 0
1 0 0 0
1 1 1 1
?
???
}
Figure 4: REG input to describe Figure 2
The attribute selection algorithm is based upon four
constraints incorporating the first two principles of the
descriptive strategy outlined at the beginning of this
section. They are:
1. Minimise false positives - The description de-
scribing the distribution should introduce the least
number of distractors. For the above example distri-
bution the set {South} ensures coverage but introduces
three distractors: CSW, CSE and ESE. While the set
of directions {Far South, South West} only introduces
one: CSW. In general, a measure of how distinguishing
a description x is of a distribution y is given by:
distinguishing(x, y) = |x ? y||x|
Thus, for a distribution z and descriptions x and y,
x is a more distinguishing description of z than y iff
distinguishing(x,z) > distinguishing(y,z).
2. Coverage (no false negatives) - The descrip-
tion should completely describe the distribution. The
set of directions {Far South,South West} completely
describes the above example distribution while {Far
South} does not. For the set of directions x and dis-
tribution y, the predicate covers(x, y) is true iff
|x ? y|
|y| = 1
3. Brevity - The set of directions should yield the
shortest description of the distribution. For the above
example distribution there is only one set of direc-
tions that ensures complete coverage. But when faced
with a choice for example {South} and {South West,
South East} brevity constraint favours {South}. In gen-
eral,the set x should be chosen over y because it is a
shorter description. For the distribution z and sets of
directions x, y with equal coverage of z, x is a shorter
description of z than y iff |x| < |y|.
4. Ordering: If two descriptions have equal cov-
erage, cardinality and are equally distinguishing for a
47
given distribution, a description is chosen based upon
a predefined preference ordering. Each type of prop-
erty is assigned a score: Cardinal = 3, Intercardinal =
2 and Gradeable = 1. Therefore, the set of directions
{Far South, South West} would be assigned a value of
3.
In classification terms, the first constraint can be con-
sidered as precision and the second as recall. The algo-
rithm firstly ranks each individual description in the set
described in ?4.2 according to the constraints outlined
above. If a single directional term cannot be used to de-
scribe the distribution it then incrementally tries to find
the highest ranking combination of directions that sat-
isfy the coverage constraint and do not cover the whole
region; otherwise, the algorithm terminates by return-
ing the empty set. So, for the example input provided
at the beginning of this section it would return the ab-
stract syntax template shown in Figure 4. Quantifiers
are selected by applying a simple threshold to the point
ratio (which is recalculated should distractors be intro-
duced): some = > 0, many = > 0.5, most = > 0.7.
This would be realised as ?in some far southern and
southwestern places?.
?
??????????????????????
Type: LocationSyntax
Head: | in |
Object:
?
?????????????????
Head: | place |
Features:
[
definite:false
plural:true
]
Quantifier: | some |
Modifier:
?
??????
Head: | and |
Coord1
[
Head: | southern |
Modifier: | far |
]
Coord2
[
Head | southwestern |
]
?
??????
?
?????????????????
?
??????????????????????
Figure 5: Phrase syntax for input in Figure 4
5 Evaluation and Discussion
RoadSafe has been evaluated in post-edit evaluations
with meteorologists at AMI and by asking potential
users to compare the quality of the summaries to corpus
texts based on the same data. While evaluations have
been intended to test the overall quality of the texts
we have received much feedback on the geographic de-
scriptions the system generates. We have also carried
out some comparison of the direction descriptions to
those in the corpus, by annotating the corpus descrip-
tions with our adjacency matrices and running them
through the system. Descriptions were compared by
calculating the Jaccard coefficient between the two ma-
trices. Overall the mean score was 0.53, with a fairly
low perfect recall percentage of 30%. The low pre-
cision score is perhaps not surprising as the descrip-
tions generated by RoadSafe are crisp and the corpus
descriptions are not solely based on the input data we
have available. However, the majority (67%) of par-
tial alignments were the result of RoadSafe producing
a subset of the human desciprition, e.g. northwest ver-
sus north, which indicates the system descriptions are
more fine grained. In terms of the human descriptions,
what was most apparent from this evaluation is the fact
that they almost exclusively used the eight major points
of the compass.
In terms of feedback experts have commented that
generally the location descriptions generated by the
system are accurate but should be more general. Of
97 post edited texts generated by the system 20% of
the geographic descriptions were edited.
Most notable was feedback from twenty one road
maintenance personnel, who participated in an exper-
iment asking them to compare expert written texts to
RoadSafe generated texts based on the same five data
sets. The details of this experiment are to be published
elsewhere; however, one of the main reasons they gave
for liking the style of the generated texts was because
they contained more geographic descriptions than the
corresponding human ones. The fact that a data-to-text
system can analyse every data point is an advantage. In
contrast experts have a huge amount of knowledge and
experience to draw upon and this reflects in their more
general and conservative approach in their geographic
descriptions. Perhaps one of their biggest criticisms
of the system as a whole is that it doesn?t do a good
job of generating geographic descriptions that involve
motion, such as ?a band of rain works east across the
area?. Indeed, this was the most edited type of gener-
ated phrase during the post-edit evaluation. There has
been little work to our knowledge on describing motion
in the NLG literature.
There are many aspects of the generation of geo-
graphic that haven?t been addressed in this paper and
warrant further exploration. Particularly at the con-
tent level, there is a need to consider how to account
for semantic composition effects caused by overlaying
frames of reference. Another question that arises is
when is it best to use an intensional rather than exten-
sional description. There is also the question of when
to use descriptions that involve relations or gradable
properties. These are all choices that a data-to-text sys-
tem can make that will affect how the summary is in-
terpreted.
6 Conclusions
This paper has described an approach for generating
approximate geographic descriptions involving regions
in the RoadSafe system, which is based on empirical
work carried out in the weather domain. Our strat-
egy takes into account constraints on what constitutes a
good reference in the application domain described, by
taking into account pragmatic factors imposed by both
the task context and the end user. What is most appar-
ent from our empirical studies is that geographic de-
scriptions describing georeferenced data are influenced
48
by not only by location but also task context. An im-
portant observation based on our evaluation studies is
that NLG systems by virtue of their ability to analyse
input data exhaustively can generate descriptions that
are more useful to end users than those generated by
human experts.
References
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19:233?263.
R Dale, S Geldof, and J-P Prost. 2005. Using natu-
ral language generation in automatic route descrip-
tion. Journal of Research and Practice in Informa-
tion Technology, 37(1):89?105.
C. Ebert, D. Glatz, M. Jansche, R. Meyer-Klabunde,
and R. Porzel. 1996. From conceptualization to
formulation in generating spatial descriptions. In
U. Schmid, J. Krems, and F. Wysotzki, editors, Pro-
ceedings of the First European Workshop on Cogni-
tive Modeling, pages 235?241.
John D. Kelleher and Geert-Jan M. Kruijff. 2006. In-
cremental generation of spatial referring expressions
in situated dialog. In Proceedings of ACL06, pages
1041?1048.
S. Levinson. 2003. Spatial language. In Nadel L.,
editor, Encyclopedia of Cognitive Science, volume 4,
pages 131?137. Nature Publishing Group.
Harvey J. Miller. 2004. Tobler?s first law and spatial
analysis. Annals of the Association of American Ge-
ographers, 93(3),:574?594.
B. Moulin and D. Kettani. 1999. Route generation
and description using the notions of objects influence
area and spatial conceptual map. Spatial Cognition
and Computation, 1:227?259.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. In Artificial Intelligence, vol-
ume 67, pages 137?169.
Kavita E Thomas and Somayajulu Sripada. 2008.
What?s in a message? interpreting geo-referenced
data for the visually-impaired. In Proceedings of
INLG08.
Waldo Tobler. 1970. A computer movie simulating
urban growth in the detroit region. Economic Geog-
raphy, 46(2):234?240.
Stuart Towns, Charles Callaway, and James Lester.
1998. Generating coordinated natural language and
3D animations for complex spatial explanations. In
Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pages 112?119, Madison, WI.
J. Gregory Trafton. 2007. Dynamic mental models in
weather forecasting. In Proceedings of the Human
Factors and Ergonomics Society 51st Annual Meet-
ing, pages 311?314.
R. Turner, S. Sripada, E. Reiter, and I. Davy. 2006.
Generating spatio-temporal descriptions in pollen
forecasts. EACL06 Companion Volume, pages 163?
166.
R. Turner, S. Sripada, E. Reiter, and I. Davy. 2008a.
Building a parallel spatio-temporal data-text cor-
pus for summary generation. In Proceedings of
the LREC2008 Workshop on Methodologies and
Resources for Processing Spatial Language, Mar-
rakech, Morocco.
R. Turner, S. Sripada, E. Reiter, and I Davy. 2008b.
Using spatial reference frames to generate grounded
textual summaries of georeferenced data. In Pro-
ceedings of INLG08.
B. Tversky. 1993. Cognitive maps, cognitive col-
lages, and spatial mental models. In A.U. Frank
and I. Campari, editors, Spatial Information Theory,
pages 14?24. Springer-Verlag, Berlin.
Sebastian Varges. 2005. Spatial descriptions as refer-
ring expressions in the maptask domain. In ENLG-
05, Aberdeen, UK.
Achille C. Varzi. 2001. Vagueness in geography. Phi-
losophy & Geography, 4:1:4965.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expressions. In Proceedings of
INLG08, Salt Fork, Ohio, USA.
49
Proceedings of the 12th European Workshop on Natural Language Generation, pages 90?93,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
SimpleNLG: A realisation engine for practical applications
Albert Gatt and Ehud Reiter
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
{a.gatt,e.reiter}@abdn.ac.uk
Abstract
This paper describes SimpleNLG, a re-
alisation engine for English which aims
to provide simple and robust interfaces to
generate syntactic structures and linearise
them. The library is also flexible in al-
lowing the use of mixed (canned and non-
canned) representations.
1 Introduction
Over the past several years, a significant consensus
has emerged over the definition of the realisation
task, through the development of realisers such as
REALPRO (Lavoie and Rambow, 1997), ALETH-
GEN (Coch, 1996), KPML (Bateman, 1997),
FUF/SURGE (Elhadad and Robin, 1996), HALO-
GEN (Langkilde, 2000), YAG (McRoy et al, 2000),
and OPENCCG (White, 2006).
Realisation involves two logically distinguish-
able tasks. Tactical generation involves making
appropriate linguistic choices given the semantic
input. However, once tactical decisions have been
taken, building a syntactic representation, apply-
ing the right morphological operations, and lin-
earising the sentence as a string are comparatively
mechanical tasks. With the possible exception
of template-based realisers, such as YAG, exist-
ing wide-coverage realisers usually carry out both
tasks. By contrast, a realisation engine focuses on
the second of the two tasks, making no commit-
ments as to how semantic inputs are mapped to
syntactic outputs. This leaves the (tactical) prob-
lem of defining mappings from semantic inputs
to morphosyntactic structures entirely up to the
developer, something which may be attractive in
those applications where full control of the out-
put of generation is required. Such control is not
always easily available in wide-coverage tactical
generators, for a number of reasons:
1. Many such realisers define an input formal-
ism, which effectively circumscribes the (se-
mantic) space of possibilities that the realiser
handles. The developer needs to ensure that
the input to realisation is mapped to the req-
uisite formalism.
2. Since the tactical problem involves search
through a space of linguistic choices, the
broader the coverage, the more efficiency
may be compromised. Where real-time de-
ployment is a goal, this may be an obstacle.
3. Many application domains have sub-
language requirements. For example, the
language used in summaries of weather data
(Reiter et al, 2005) or patient information
(Portet et al, to appear) differs from standard
usage, and does not always allow variation
to the same extent. Since realisers don?t
typically address such requirements, their
use in a particular application may require
the alteration of the realiser?s rule-base or,
in the case of statistical realisers, re-training
on large volumes of appropruately annotated
data.
This paper describes SimpleNLG, a realisa-
tion engine which grew out of recent experiences
in building large-scale data-to-text NLG systems,
whose goal is to summarise large volumes of nu-
meric and symbolic data (Reiter, 2007). Sub-
language requirements and efficiency are impor-
tant considerations in such systems. Although
meeting these requirements was the initial motiva-
tion behind SimpleNLG, it has since been devel-
oped into an engine with significant coverage of
English syntax and morphology, while at the same
time providing a simple API that offers users di-
rect programmatic control over the realisation pro-
cess.
90
Feature Values Applicable classes
lexical ADJPOSITION Attrib1/2/3, PostNominal, Predicative ADJ
ADVPOSITION Sentential, PostVerbal, Verbal ADV
AGRTYPE Count, Mass, Group, Inv-Pl, Inv-Sg N
COMPLTYPE AdjP, AdvP, B-Inf, WhFin, WhInf, . . . V
VTYPE Aux, Main, Modal V
phrasal FUNCTION Subject, Obj, I-Obj, Prep-Obj, Modifier all
SFORM B-Inf, Gerund, Imper, Inf, Subj S
INTERROGTYPE Yes/No, How, What, . . . S
NUMBERAGR Plural, Singular NP
TENSE Pres, Past, Fut VP
TAXIS (boolean) true (=perfective), false VP
POSSESSIVE (boolean) true (=possessive), false NP
PASSIVE (boolean) true, false VP
Table 1: Features and values available in SimpleNLG
2 Overview of SimpleNLG
SimpleNLG is a Java library that provides inter-
faces offering direct control over the realisation
process, that is, over the way phrases are built and
combined, inflectional morphological operations,
and linearisation. It defines a set of lexical and
phrasal types, corresponding to the major gram-
matical categories, as well as ways of combining
these and setting various feature values. In con-
structing a syntactic structure and linearising it as
text with SimpleNLG, the following steps are un-
dertaken:
1. Initialisation of the basic constituents re-
quired, with the appropriate lexical items;
2. Using the operations provided in the API to
set features of the constituents, such as those
in bottom panel of Table 1;
3. Combining constituents into larger struc-
tures, again using the operations provided in
the API which apply to the constituents in
question;
4. Passing the resulting structure to the lin-
eariser, which traverses the constituent struc-
ture, applying the correct inflections and lin-
ear ordering depending on the features, be-
fore returning the realised string.
Constituents in SimpleNLG can be a mixture
of canned and non-canned representations. This
is useful in applications where certain inputs can
be mapped to an output string in a deterministic
fashion, while others require a more flexible map-
ping to outputs depending, for example, on seman-
tic features and context. SimpleNLG tries to meet
these needs by providing significant syntactic cov-
erage with the added option of combining canned
and non-canned strings.
Another aim of the engine is robustness: struc-
tures which are incomplete or not well-formed will
not result in a crash, but typically will yield infe-
licitous, though comprehensible, output. This is a
feature that SimpleNLG shares with YAG (McRoy
et al, 2000). A third design criterion was to
achieve a clear separation between morphological
and syntactic operations. The lexical component
of the library, which includes a wide-coverage
morphological generator, is distinct from the syn-
tactic component. This makes it useful for applica-
tions which do not require complex syntactic op-
erations, but which need output strings to be cor-
rectly inflected.
2.1 Lexical operations
The lexical component provides interfaces that de-
fine a Lexicon, a MorphologicalRule, and
a LexicalItem, with subtypes for different lex-
ical classes (Noun, Preposition etc). Mor-
phological rules, a re-implementation of those in
MORPHG (Minnen et al, 2001), cover the full
range of English inflection, including regular and
irregular forms1. In addition to the range of mor-
phological operations that apply to them, various
features can be specified for lexical items. For ex-
ample, as shown in the top panel of Table 1, ad-
jectives and adverbs can be specified for their typ-
ical syntactic positions. Thus, an adjective such
as red would have the values Attrib2, indicating
that it usually occurs in attribute position 2 (fol-
lowing Attrib1 adjectives such as large), and Pred-
icative. Similarly, nouns are classified to indicate
1Thanks are due to John Carroll at the University of Sus-
sex for permission to re-use these rules.
91
their agreement features (count, mass, etc), while
verbs can be specified for the range of syntactic
complement types they allow (e.g. bare infinitives
and WH-complements).
A typical development scenario involves the
creation of a Lexicon, the repository of the rel-
evant items and their properties. Though this
can be done programmatically, the current distri-
bution of SimpleNLG provides an interface to a
database constructed from the NIH Specialist Lexi-
con2, a large (> 300,000 entries) repository of lex-
ical items in the medical and general English do-
mains, which incorporates information about lexi-
cal features such as those in Table 1.
2.2 Syntactic operations
The syntactic component of SimpleNLG de-
fines interfaces for HeadedPhrase and
CoordinatePhrase. Apart from various
phrasal subtypes (referred to as PhraseSpecs)
following the usage in Reiter and Dale (2000)),
several grammatical features are defined, includ-
ing Tense, Number, Person and Mood (see
Table 1). In addition, a StringPhraseSpec
represents a piece of canned text of arbitrary
length.
A complete syntactic structure is achieved by
initialising constituents with the relevant fea-
tures, and combining them using the operations
specified by the interface. Any syntactic struc-
ture can consist of a mixture of Phrase or
CoordinatePhrase types and canned strings.
The input lexical items to phrase constructors can
themselves be either strings or lexical items as de-
fined in the lexical component. Once syntactic
structures have been constructed, they are passed
to a lineariser, which also handles basic punctua-
tion and other orthographic conventions (such as
capitalisation).
The syntactic component covers the full range
of English verbal forms, including participals,
compound tenses, and progressive aspect. Sub-
types of CoordinatePhrase allow for fully
recursive coordination. As shown in the bottom
panel of Figure 1, subjunctive forms and different
kinds of interrogatives are also handled using the
same basic feature-setting mechanism.
The example below illustrates one way of con-
structing the phrase the boys left the house, ini-
2http://lexsrv3.nlm.nih.gov/
SPECIALIST/index.html
tialising a sentence with the main verb leave
and setting a Tense feature. Note that the
SPhraseSpec interface allows the setting of the
main verb, although this is internally represented
as the head of a VPPhraseSpec dominated by
the clause. An alternative would be to construct
the verb phrase directly, and set it as a constituent
of the sentence. Similarly, the direct object, which
is specified directly as a constituent of the sen-
tence, is internally represented as the object of the
verb phrase. In this example, the direct object
is an NPPhraseSpec consisting of two words,
passed as arguments and internally rendered as
lexical items of type Determiner and Noun re-
spectively. By contrast, the subject is defined as a
canned string.
(1) Phrase s1 =
new SPhraseSpec(?leave?);
s1.setTense(PAST);
s1.setObject(
new NPPhraseSpec(?the?, ?house?));
Phrase s2 =
new StringPhraseSpec(?the boys?);
s1.setSubject(s2);
Setting the INTERROGATIVETYPE feature of
sentence (1) turns it into a question. Two exam-
ples, are shown below. While (2) exemplifies a
simple yes/no question, in (3), a WH-constituent
is specified as establishing a dependency with the
direct object (the house).
(2) s1.setInterrogative(YES NO);
(Did the boys leave home?)
(3) s1.setInterrogative(WHERE, OBJECT);
(Where did the boys leave?)
In summary, building syntactic structures in
SimpleNLG is largely a question of feature setting,
with no restrictions on whether representations are
partially or exclusively made up of canned strings.
2.2.1 Interaction of lexicon and syntax
The phrasal features in the bottom panel of Table 1
determine the form of the output, since they are
automatically interpreted by the realiser as instruc-
tions to call the correct morphological operations
on lexical items. Hence, the syntactic and morpho-
logical components are closely integrated (though
distinct). Currently, however, lexical features such
as ADJPOSITION are not fully integrated with the
syntactic component. For example, although ad-
jectives in the lexicon are specified for their po-
sition relative to other modifiers, and nouns are
92
specified for whether they take singular or plural
agreement, this informaiton is not currently used
automatically by the realiser. Full integration of
lexical features and syntactic realisation is cur-
rently the focus of ongoing development.
2.3 Efficiency
As an indication of efficiency, we measured the
time taken to realise 26 summaries with an aver-
age text length of 160.8 tokens (14.4 sentences),
and sentences ranging in complexity from simple
declaratives to complex embedded clauses3. The
estimates, shown below, average over 100 itera-
tions per text (i.e. a total of 2600 runs of the re-
aliser) on a Dell Optiplex GX620 machine running
Windows XP with a 3.16 GHz Pentium proces-
sor. Separate times are given for the initialisation
of constituents based on semantic representations,
along the lines shown in (1), (SYN), and linearisa-
tion (LIN). These figures suggest that a medium-
length, multiparagraph text can be rendered in un-
der a second in most cases.
MEAN (ms) SD MIN MAX
SYN 280.7 229.7 13.8 788.34
LIN 749.38 712.6 23.26 2700.38
3 Conclusions and future work
This paper has described SimpleNLG, a realisa-
tion engine which differs from most tactical gen-
erators in that it provides a transparent API to carry
out low-level tasks such as inflection and syntac-
tic combination, while making no commitments
about input specifications or input-output map-
pings.
The simplicity of use of SimpleNLG is reflected
in its community of users. The currently avail-
able public distribution4, has been used by several
groups for three main purposes: (a) as a front-end
to NLG systems in projects where realisation is not
the primary research focus; (b) as a simple natu-
ral language component in user interfaces for other
kinds of systems, by researchers who do not work
in NLG proper; (c) as a teaching tool in advanced
undergraduate and postgraduate courses on Natu-
ral Language Processing.
SimpleNLG remains under continuous develop-
ment. Current work is focusing on the inclusion of
output formatting and punctuation modules, which
3The system that generates these summaries is fully de-
scribed by Portet et al (to appear).
4SimpleNLG is available, with exhaus-
tive documentation, at the following URL:
http://www.csd.abdn.ac.uk/?ereiter/simplenlg/.
are currently handled using simple defaults. More-
over, an enhanced interface to the lexicon is being
developed to handle derivational morphology and
a fuller integration of complementation frames of
lexical items with the syntactic component.
References
J. A. Bateman. 1997. Enabling technology for multi-
lingual natural language generation: the KPML de-
velopment environment. Natural Language Engi-
neering, 3(1):15?55.
J. Coch. 1996. Overview of AlethGen. In Proceedings
of the 8th International Natural Language Genera-
tion Workshop.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic realiza-
tion component. In Proceedings of the 8th Interna-
tional Natural Language Generation Workshop.
I. Langkilde. 2000. Forest-based statistical language
generation. In Proceedings of the 1st Meeting of
the North American Chapter of the Association for
Computational Linguistics.
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on Applied Natural Language
Processing.
S.W. McRoy, S. Channarukul, and S. Ali. 2000. YAG:
A template-based generator for real-time systems.
In Proceedings of the 1st International Conference
on Natural Language Generation.
G. Minnen, J. J. Carroll, and D. Pearce. 2001. Ap-
plied morphological processing of English. Natural
Language Engineering, 7(3):207?223.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. to appear. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge, UK.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
E. Reiter. 2007. An architecture for Data-to-Text sys-
tems. In Proceedings of the 11th European Work-
shop on Natural Language Generation.
M. White. 2006. Chart realization from disjunctive
inputs. In Proceedings of the 4th International Con-
ference on Natural Language Generation.
93
An Investigation into the Validity of Some
Metrics for Automatically Evaluating Natural
Language Generation Systems
Ehud Reiter?
University of Aberdeen
Anja Belz??
University of Brighton
There is growing interest in using automatically computed corpus-based evaluation metrics to
evaluate Natural Language Generation (NLG) systems, because these are often considerably
cheaper than the human-based evaluations which have traditionally been used in NLG. We
review previous work on NLG evaluation and on validation of automatic metrics in NLP, and
then present the results of two studies of how well some metrics which are popular in other
areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of
computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics
may provide a useful measure of language quality, although the evidence for this is not as strong
as we would ideally like to see; however, they do not provide a useful measure of content quality.
We also discuss a number of caveats which must be kept in mind when interpreting this and
other validation studies.
1. Introduction
Evaluation is becoming an increasingly important topic in Natural Language Genera-
tion (NLG), as in other fields of computational linguistics. Many NLG researchers are
impressed by the BLEU evaluation metric (Papineni et al 2002) in Machine Translation
(MT), which has allowed MT researchers to quickly and cheaply evaluate the impact
of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing
the output of an MT system to a set of reference translations (human translations of the
source text), and in principle this kind of evaluation could be done with NLG systems as
well. As in other areas of NLP, the advantages of automatic corpus-based evaluation are
that it is potentially much cheaper and quicker than human-based evaluation, and that
it is repeatable. Indeed, NLG researchers have used BLEU in their evaluations for some
time (Langkilde 2002; Habash 2004).
The use of such automatic evaluation metrics is, however, only sensible if they are
known to be correlated with the results of reliable human-based evaluations. Although
a number of previous studies have analyzed correlations between human judgments
? Department of Computing Science, University of Aberdeen, UK. E-mail: e.reiter@abdn.ac.uk.
?? Natural Language Technology Group, University of Brighton, UK. E-mail: A.S.Belz@brighton.ac.uk.
Submission received: 23 March 2007; revised submission received: 6 October 2008; accepted for publication:
29 December 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
and automatic evaluation metrics in machine translation and document summarization
(Doddington 2002; Papineni et al 2002; Lin and Hovy 2003), much less is known about
how well automatic metrics correlate with human judgments in NLG. In this article we
present two empirical studies of how well BLEU and various other corpus-based metrics
agree with human judgments, when evaluating the outputs of several NLG systems
that generate texts which describe changes in the wind (for weather forecasts). We also
discuss several caveats that need to be kept in mind when interpreting our study and
perhaps other validation studies of automatic metrics as well.
2. Background: Evaluation in NLG and Related Fields
As Hirschman (1998), Mellish and Dale (1998), and others have pointed out, evaluations
can be used for many purposes, and different evaluations are often needed for different
stakeholders. For example, the BabyTalk project at Aberdeen (Portet et al 2009), which
is attempting to create a set of NLG systems which can generate textual summaries of
clinical data about babies in a neonatal intensive care unit (NICU), is a collaboration
between medical researchers, psychologists, computer scientists, and a commercial
software house. Each of these groups has its own evaluation agenda:
 The medical researchers want to know if BabyTalk is medically effective. To
evaluate this, they ideally would like to do a study similar to Cunningham
et al?s (1998) evaluation of the effectiveness of a visualization system in an
intensive care unit; that is, deploy BabyTalk in a hospital, use it for half of
the children in a ward, and determine if there is any difference in outcome
(e.g., mortality) between the children in the BabyTalk group and the
control group.
 The psychologists want to understand the effectiveness of textual
presentation of information for decision support. To evaluate this, they
would like to do a study similar to Law et al (2005); that is, show medical
subjects textual summaries (as well as standard graphical visualizations as
a control) in a controlled ?off-ward? context, ask them to make a treatment
decision, and compare this decision against a gold standard.
 The computer scientists want to know if BabyTalk is effective (under either
of these measures). They also would like to conduct evaluations
throughout the project, so as to assess whether their development efforts
are making the system better or worse; the stakeholders, in contrast,
would be satisfied with a single evaluation at the end of the project.
 The software house would like to know if BabyTalk would be commercially
profitable. This partially depends on medical effectiveness (see previous
point), which determines the demand for the system. But it also depends
on how expensive it is to develop and support BabyTalk; from this
perspective the company is especially interested in evaluations of the cost
of adapting/porting BabyTalk to different hospitals in the NICU domain
in the short term, and to different medical domains in the longer term. See
Harris (2008) for a commercial perspective on medical NLG systems.
All of these stakeholders are interested in evaluations which assess the quality and effec-
tiveness of generated texts; such evaluations are the focus of our article. The software
530
Reiter and Belz Validity of Some Metrics for NLG Evaluation
house and the computer scientists are also interested in engineering-cost evaluations;
although this is a very important topic, we will not discuss it here: a separate article
would be needed to do justice to this topic.
2.1 Evaluation in NLG
The quality of texts generated by NLG systems has been evaluated in many different
ways in the past, most of which can be classified as evaluations based on task perfor-
mance, human judgments and ratings, or comparison to corpus texts using automatic
metrics.
2.1.1 Task-Based Evaluation. Task-based evaluations involve directly measuring the im-
pact of generated texts on end users; these are extrinsic evaluations (Spa?rck Jones and
Galliers 1995), and typically involve techniques from psychology or from an application
domain such as medicine. One of the first task-based evaluations of an NLG system was
done by Young (1999), who generated instructional texts using four different algorithms,
asked subjects to carry out the instructions, and then measured how many mistakes
they made. Although task performance is the most common measure used in task-based
evaluations in NLG, other measures can also be used. For example, Carenini and Moore
(2006) evaluated the impact of persuasive texts (in a house-selling context) by seeing
how users ranked houses in a hot list; and Di Eugenio, Glass, and Trolio (2002) evaluated
the impact of adding an NLG component to an intelligent tutoring system by measuring
learning gain.
We have been involved in a number of task-based evaluations of NLG systems and
components. STOP (Reiter, Robertson, and Osman 2003), which generates personalized
smoking-cessation letters, was evaluated on the basis of medical effectiveness; we sent
a group of 2,000 smokers either STOP-generated letters or one of two kinds of control
letters, and measured how many smokers in each group managed to quit smoking.
BT45 (Portet et al 2009) (which is one of the BabyTalk systems) was evaluated for its
decision-support effectiveness, using the ?psychologist? methodology described earlier
(van der Meulen et al 2009). SKILLSUM (Williams and Reiter 2008), which generates
feedback reports from literacy assessments, was evaluated on the basis of educational
effectiveness; we gave 200 assessment takers either SKILLSUM texts or control texts, and
measured whether they increased the accuracy of self-assessments of their literacy skills.
We also evaluated several referring-expression generation algorithms by conducting
experiments in which participants were presented with generated referring expressions
and asked to identify the target referent (Belz and Gatt 2007; Gatt, Belz, and Kow
2008, 2009); these were carried out in conjunction with shared-task events organized
under the Generation Challenges initiative (Generation Challenges is further discussed
in Section 2.1.4).
Task-based evaluations have traditionally been regarded as the most meaningful
kind of evaluation in NLG, especially in contexts where the evaluation needs to convince
people in other communities (such as psychologists and doctors). However, they can
be expensive and time-consuming. The STOP evaluation cost UK?75,000, and required
20 months to design, carry out, and analyze; the SKILLSUM and BT45 evaluations
(which are perhaps more typical) cost about UK?20,000 over six months. The referring-
expression identification experiments were cheaper (less than UK?1,000 each, not count-
ing data and system creation), because they involved smaller numbers of subjects, and
531
Computational Linguistics Volume 35, Number 4
evaluated system components in laboratory-based settings, rather than by means of
systems deployed in the real world.
In addition to monetary and time costs, all of these evaluations also depended on
goodwill from participants, in most cases busy domain experts who used their own
standing in their community to arrange access to subjects and otherwise facilitate the
evaluation. Such goodwill in itself is a scarce resource which must be used with care.
2.1.2 Evaluations Based on Human Ratings and Judgments. Another way of evaluating an
NLG system is to ask human subjects to rate generated texts on an n-point rating scale;
this is an intrinsic form of evaluation (Spa?rck Jones and Galliers 1995). This methodol-
ogy was first used in NLG by Lester and Porter (1997), who asked eight domain experts
to each rate 15 texts on a number of different dimensions: overall quality and coherence,
content, organization, writing style, and correctness. Some of the texts were human-
written and some were computer-generated, but the judges did not know the origin of
specific texts they read. Many more such evaluations have been performed since, often
with fewer dimensions. For example, Binsted, Pain, and Ritchie (1997) evaluated a joke-
generation system by asking children to rate the funniness of texts on a 5-point scale;
and Walker, Rambow, and Rogati (2002) evaluated the SPOT sentence-planning system
by asking human subjects to rate the overall quality of generated texts on a 5-point
scale. A variation of this technique is to show subjects different versions of a text, and
ask them which one they prefer. For example, the SUMTIME weather-forecast generator
was evaluated by showing subjects both human corpus texts and computer-generated
texts, and asking which they preferred (Reiter et al 2005).
Evaluations based on human ratings and judgments are currently probably the most
popular way of evaluating NLG systems, perhaps in part because such evaluations tend
to be significantly quicker and cheaper to carry out than task-based evaluations, and
do not require as much support from domain experts. For example, the previously
mentioned evaluation of SUMTIME was carried out in two months without any external
research grant funding.
In addition to resource issues, another reason why some researchers prefer evalua-
tions based on human ratings over task-based evaluations is that task-based evaluations
need to focus on a very specific task, and performance on this task may not correlate
with performance on other tasks. For example, as mentioned previously, the medical
researchers in BabyTalk would like to conduct a medical effectiveness evaluation, which
involves operationally deploying systems in a real ward and measuring impact on
patient outcome. However, for ethical reasons such an experiment cannot be carried out
until we have good evidence that the BabyTalk systems are effective, which is not yet the
case. We carried out an off-ward task-based evaluation of BT45 using the ?psychologist?
methodology (van der Meulen et al 2009), and we would like to think that the results
of this evaluation would correlate with the results of a medical effectiveness evaluation.
However, we do not have any empirical evidence that this is the case, and certainly
there are major differences between the off-ward and on-ward contexts (for example,
doctors in the off-ward experiment could not visually observe the babies, which is a
very important information source when doctors are actually caring for a baby in a
hospital ward). From this perspective, an argument can be made that asking doctors
to explicitly rate the medical usefulness of the texts might tell us as much about their
genuine medical effectiveness as our off-ward task-based evaluation.
Last but not least, it is not always possible to conduct meaningful task-based evalua-
tions of some NLG systems. For example, it is unclear how to evaluate the overall quality
532
Reiter and Belz Validity of Some Metrics for NLG Evaluation
of jokes produced by a humor generation system other than by asking for human ratings
(Binsted, Pain, and Ritchie 1997), although one can perform a task-based evaluation
of the educational impact of humor generation software (Black et al 2007), or (more
speculatively) perhaps evaluate the psychological impact of a joke by monitoring facial
expressions and laughter (which is a non-task-based extrinsic evaluation).
Little is known about how well human ratings of texts produced by NLG systems
correlate with task-effectiveness measures. Law et al (2005), who worked in the same
domain (NICU) as BabyTalk, conducted an off-ward decision-support evaluation which
compared human-written text summaries and graphical visualizations of clinical data.
They found that subjects preferred the visualizations, but were more likely to make
correct decisions from the text summaries. It is unclear whether this is because subjects
had inappropriate preferences, or because there was a big difference between genuine
medical effectiveness and off-ward decision-support effectiveness (as mentioned ear-
lier). The only studies we are aware of which examined how well human judgments
predict task-effectiveness of computer-generated texts occurred in the recent Generation
Challenges evaluations of referring expression generation, which measured the corre-
lations between human assessments of language quality and adequacy of content with
task-performance measures (referent identification time and accuracy) (Gatt, Belz, and
Kow 2009). The results revealed a strong and highly significant correlation between
human judgments of content adequacy and identification accuracy; there was also
a significant inverse correlation between human judgments of language quality and
identification speed (i.e., those systems that tended to be judged more fluent by the
human assessors also tended to have shorter identification times).
2.1.3 Evaluations Based on Automatic Metrics which Compare Computer-Generated Texts
to Human-Authored Corpus Texts. In recent years there has been growing interest in
evaluating NLG texts by comparing them to a corpus of human-written reference texts,
using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineni
et al 2002); this is another type of intrinsic evaluation. Such evaluations have been
used by Bangalore, Rambow, and Whittaker (2000) and Marciniak and Strube (2004),
for example. Langkilde (2002) evaluated an NLG system by parsing texts from a corpus,
feeding the parser output to her NLG system, and then comparing the generated
texts to the original corpus texts. Similar ?corpus regeneration? evaluations have
since been used by a number of other researchers (Callaway 2003; Zhong and Stent
2005; Cahill and van Genabith 2006). Corpus-based evaluation has been especially
popular in the evaluation of surface realizers. This may be because the most important
attribute of many realizers is grammatical coverage and robust handling of special and
unusual cases, and corpus-based techniques are well suited to evaluating this. Also,
the range of acceptable outputs can be smaller in realizer evaluations because content,
microplanning, and (in some cases) lexical choices do not vary; this means there is less
concern about reference texts not adequately covering the solution space.
Automatic corpus-based evaluations are appealing in NLG, as in other areas of NLP,
because they are relatively cheap and quick to do if a corpus is available, do not require
support from domain experts, and are repeatable. However, their use in NLG is contro-
versial, at least when evaluating systems as a whole instead of just surface realizers,
because many people are concerned that the results of such evaluations may not be
meaningful. For example Reiter and Sripada (2002) point out that corpus texts are often
not of high enough quality to form good reference texts; and Scott and Moore (2007)
express concern that metrics will not be able to evaluate many important linguistic
properties such as information structure.
533
Computational Linguistics Volume 35, Number 4
A more general concern is that automatic metrics based on comparison to reference
texts measure how well a text matches what writers do, whereas most human evalua-
tions (task or judgment-based) measure the impact of a text on readers. Because writers
do not always produce optimal texts from a reader?s perspective (Oberlander 1998;
Reiter et al 2005), a metric which is a good evaluator of how likely it is that a text has
been written by a human writer is not necessarily a good predictor of how effective and
useful the text is from the perspective of a human reader. Of course automatic metrics
do not need to be writer-based. Indeed, some reader-based automatic metrics, such as
the Flesch score (Flesch 1949) (based on average sentence and word length), are widely
used as practical tools to help writers, but such metrics have not been widely used to
evaluate NLP systems.
An important practical consideration is that corpus-based evaluations require a
corpus of human-written reference texts; BLEU-like metrics in fact work best when the
reference text corpus contains several reference texts for the same input, written by
different authors. If reference texts have to be created specifically for an evaluation,
this can be an expensive endeavor. In the BabyTalk domain, for example, it can take
an experienced clinician several hours to write a corpus text from the raw data; hence
creating a corpus of 100 reference texts in this domain could require 2?3 months effort
by a clinician (as they do not create such reports in the course of their normal work).
Getting this much time from an expert doctor or nurse would be difficult unless a very
strong case could be made for the utility of the evaluation.
2.1.4 Other Validation Studies. In recent years some validation studies which examine cor-
relations between automatic metrics and human evaluations in NLG have been carried
out. The first such study we are aware of is Bangalore, Rambow, and Whittaker (2000),
who looked at string-edit and tree-edit metrics (this work predates BLEU and ROUGE)
using a small number of manually simulated system ?outputs.? Probably the most
similar study to our work is that by Stent, Marge, and Singhai (2005), who examined the
correlation between human judgments and several automatic metrics when evaluating
computer-generated paraphrases; this is further discussed in Section 3.3.3.
Very recently some validation studies have been done in the context of the Gen-
eration Challenges initiative for shared tasks in NLG, by evaluating systems entered
in the shared task using automatic metrics, human ratings, and task-based evaluation,
and analyzing correlations between these. For example Belz and Gatt (2008) analyzed
correlations between several automatic evaluation metrics and task performance in a
referring-expression generation task; they found that there was no significant correla-
tion between any of the automatic metrics they looked at (which included specialized
metrics for the reference task as well as BLEU and ROUGE) and their task-based measures
of effectiveness, such as how long it took human subjects to identify objects from a
referring expression, and how many mistakes the subjects made. However the different
automatic metrics they examined did tend to correlate with each other, as did the differ-
ent measures of task performance. In general shared tasks offer a promising context for
validation studies, and we hope that future Generation Challenges events will continue
to provide data on how well automatic metrics correlate with human-based evaluations.
2.2 Insights from Evaluations in Other Areas of NLP
Of course, evaluation and experimentation are crucial to all fields of NLP; here we look
at insights from two other NLP subfields which need to evaluate the quality of texts:
machine translation and document summarization.
534
Reiter and Belz Validity of Some Metrics for NLG Evaluation
2.2.1 Evaluation in Machine Translation. There is a rich literature in MT evaluation, includ-
ing a number of specialist workshops on this topic; as in NLG, there is also considerable
interest in using shared-task events to provide data about how well different evaluation
techniques correlate with each other (Callison-Burch et al 2008). From an NLG perspec-
tive, the most surprising aspect of current MT evaluation is the dominance of BLEU and
other automatic corpus-based metrics (Callison-Burch, Osborne, and Koehn 2006). BLEU
was first proposed as a supplement (the U in BLEU stands for ?understudy?) for human
evaluation (Papineni et al 2002), but it is now routinely used as the main technique for
evaluating research contributions. It is accepted and indeed the norm for an article on
MT in Computational Linguistics to report evaluations that are solely based on automatic
corpus-based metrics; this is not the case in NLG, where human evaluations are expected
at least in high-prestige venues.
We are not aware of any studies in MT that have tried to correlate BLEU-like met-
rics with the results of task-effectiveness studies. Although a number of studies have
analyzed the correlation between BLEU-type metrics and human judgments, most of
these have used human judgments from NIST MT evaluations. Human judgments in
most of these evaluations were solicited from monolingual subjects who were asked to
compare the output of MT systems to a single reference translation, without any context;
also in many of these studies the subjects were asked to assess individual sentences or
even phrases, not complete texts (Doddington 2002). As Coughlin (2003) and others
have pointed out, it is not clear that human judgments solicited in this way would
match the judgments of bilingual subjects who were shown complete source and MT
texts, and asked to evaluate the quality of the translation in a specific real-world context.
Papineni et al (2002) in fact found that BLEU scores were more highly correlated with
human judgments from monolingual subjects than human judgments from bilingual
subjects.
In any case, regardless of the effectiveness of BLEU as an MT evaluation metric,
another issue is whether an MT evaluation technique can in general be expected to
work as an NLG evaluation technique. There are some obvious differences between MT
systems and NLG systems; for example:
 Content determination: NLG systems need to decide on what information
should be communicated in a text, as well as how this information is
linguistically expressed; MT systems generally do not have to perform
content determination.
 Linguistic variety: Many NLG systems produce text that is fairly simple
from a linguistic perspective (partially because many NLG users prefer
such texts); MT systems, in contrast, usually need to produce linguistically
complex texts.
 Genre/domain: Most applied NLG systems (with some exceptions) try
to generate high-quality texts in a limited domain and genre such as
marine weather forecasts; MT systems, in contrast, typically generate
lower-quality texts in a broad text category such as newspaper articles.
These differences presumably need to be considered when deciding whether it makes
sense to use an MT evaluation technique in NLG. For example, there is no reason to
expect MT evaluation techniques to be useful for evaluating NLG content determination,
since MT systems do not perform this task. Also, MT evaluation techniques which
535
Computational Linguistics Volume 35, Number 4
work well when evaluating less-than-human-quality texts from an MT system may
not necessarily work well when evaluating human-quality texts produced by an NLG
system.
2.2.2 Evaluation in Document Summarization. Another branch of NLP which requires the
evaluation of textual documents is document summarization. From an evaluation per-
spective, an important difference between MT and summarization is that summarization
evaluations have placed much more emphasis on content determination. Perhaps in
part because of this, the summarization community places more emphasis on human
evaluations. Although there are automatic corpus-based metrics for summarization
such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization
evaluation in the same way that BLEU-type metrics dominate MT evaluation.
The main summarization evaluation technique in the NIST TAC 2008 summa-
rization track is the pyramid technique (Nenkova and Passonneau 2004), which is a
structured human-based evaluation, based on asking human judges to identify ?sum-
marization content units? (SCU) in model and system-generated summaries, and mea-
suring how many SCUs from the model summaries occur in the system summary. This
is an interesting technique for evaluating content, and might be worth investigating for
evaluating content determination in NLG systems.
In terms of validation, a number of studies have claimed that ROUGE correlates with
human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al (2005)
checked if ROUGE scores correlated with task effectiveness; they did not find a strong
correlation.
2.3 Summary
In summary, evaluation of NLG texts in the past has primarily been done using human
subjects, either by measuring the impact of texts on task performance, or by asking
subjects to rate texts. However, a growing number of NLG researchers are using auto-
matic metrics to evaluate their systems, perhaps inspired by the popularity of automatic
metrics in other areas of NLP which involve evaluating output texts, most notably ma-
chine translation and document summarization. This use of metrics assumes that they
correlate with human-based evaluations. A number of studies in machine translation
and document summarization have shown that some automatic metrics correlate with
human ratings; however we are not aware of any studies in these areas which have
shown any metric to strongly correlate with task performance. Fewer validation studies
have been carried out in NLG, although this is beginning to change as researchers place
more importance on such studies.
3. Our Experiments
Given the growing interest in using automatic evaluation metrics such as BLEU in
NLG, we decided to carry out some experiments to determine how well such metrics
predicted the results of human judgments. As in other such studies, we did this by eval-
uating a number of systems with the same input/output functionality, using different
evaluation techniques, and then analyzing the correlation between the techniques.
One potential weakness of our experiments was that we did not look at correlations
with task-effectiveness evaluations. This was because we did not have the resources
(money and domain-expert goodwill) to conduct a task-based evaluation. This issue is
further discussed in Section 4.2.
536
Reiter and Belz Validity of Some Metrics for NLG Evaluation
3.1 Domain and Systems
Our work was done in the domain of computer-generated weather forecasts. This is
one of the most popular applications of NLG (Goldberg, Driedger, and Kittredge 1994;
Coch 1998; Reiter et al 2005), and several NLG weather-forecast systems have been
fielded and used. Weather forecast generation is probably the closest that NLG comes to
a ?standard? application domain, and hence seems a good choice for validation studies
from this perspective.
On the other hand, though, one could also argue that weather-forecast generators
are atypical in that the language they generate tends to be very simple, even by the
standards of NLG systems: very limited syntax (which differs from conventional Eng-
lish), very small vocabulary, no real text structure above the sentence level, and so on.
Hence it is not clear to what degree results obtained in this domain will generalize to
other domains with less simple language and content (such as BabyTalk); this is further
discussed in Section 4.1.
From a practical perspective, the great advantage of the weather forecast domain
was that we had access to a number of systems, built using different NLG technologies,
with the same input/output functionality; at the time (2006) there was no other domain
where this was the case. This situation is changing, because of the emergence of shared-
task events in NLG such as Generation Challenges (see Section 2.1.4).
3.1.1 SUMTIME Systems and SUMTIME-METEO Corpus. In particular, we based our experi-
ments on the SUMTIME system (Sripada et al 2004; Reiter et al 2005) and its associated
SUMTIME-METEO corpus (Sripada et al 2003), which were developed at Aberdeen.
SUMTIME generates textual weather forecasts from numerical forecast data for off-
shore oil rigs. It has two modules: a content-determination module that determines
the content of the weather forecast by analyzing the numerical data using linear seg-
mentation and other data analysis techniques; and a microplanning and realization
module which generates texts based on this content by choosing appropriate words,
deciding on aggregation, enforcing the sublanguage grammar, and so forth. SUMTIME
generates very high-quality texts; in some cases forecast users believe SUMTIME texts
are better than human-written texts (Reiter et al 2005; see also Table 4 of this paper).
The SUMTIME system has been used operationally to produce draft weather forecasts;
these are post-edited by meteorologists before they are released to end users (Sripada
et al 2004).
SUMTIME is a knowledge-based NLG system. Although its design was informed
by corpus analysis (Reiter, Sripada, and Robertson 2003), the system is composed of
manually authored rules and code.
The SUMTIME project also created a corpus and data set, called SUMTIME-METEO
(Sripada et al 2003). This consists of a corpus of 1,045 weather forecasts written by
professional forecasters, and the numerical predictions of wind, temperature, and so
forth, that forecasters examined when they wrote the forecasts. For wind descriptions
only, the corpus also contains simple content representations containing information
about wind speed and direction, time of day, and position in forecast (we call these
?content tuples?). The content tuples were created by parsing the corpus texts and
extracting the relevant information (Reiter and Sripada 2003), and are similar to the
representations produced by the SUMTIME content-determination system. Figures 1, 2,
and 3 show an extract from a numerical data file, an extract from the corresponding
human-written forecast, and the content tuples derived from the human text.
537
Computational Linguistics Volume 35, Number 4
wind avg wind max (gust)
day/hour direction speed wind speed
05/06 SSW 18 22
05/09 S 16 20
05/12 S 14 17
05/15 S 14 17
05/18 SSE 12 15
05/21 SSE 10 12
06/00 VAR 6 7
Figure 1
Extract from meteorological data file for 05-10-2000 (morning forecast).
FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000
WIND(KTS) CONFIDENCE: HIGH
10M: SSW 16-20 GRADUALLY BACKING SSE THEN FALLING
VARIABLE 04-08 BY LATE EVENING
...
Figure 2
Extract from corpus (human) forecast for 05-10-2000 (morning forecast).
index wind min wind max wind time
direction speed speed
1 SSW 16 20 0600
2 SSE - - -
3 VAR 04 08 2400
Figure 3
Content tuples extracted from the forecast in Figure 2.
In addition to the main SUMTIME system, two other generation methods were
developed in the SUMTIME project and used in the experiments described here.
SUMTIME-Hybrid uses the SUMTIME microplanner/realizer to generate text from the
corpus-derived content tuples (Figure 3). In other words, it combines human content-
determination with SUMTIME microplanning and realization. The other method is an
algorithm which is based on a spreadsheet and flowchart which one of the forecast-
ers gave to the SUMTIME team at the beginning of the project (Reiter, Sripada, and
Robertson 2003, page 499); a simplified version of this algorithm is presented in Figure 4.
We did not implement this algorithm as software, but we manually executed it for a
number of forecasts. We refer to it below as the Template system since the linguistic
part of the flowchart was based on template-filling.
3.1.2 pCRU Generators for the SUMTIME-METEO Domain. Independently of the SUMTIME
Project, we created a range of statistical generators for the SUMTIME-METEO domain
using pCRU generation (probabilistic context-free representational underspecification)
(Belz 2008). These took content tuples as input (as in Figure 3), not meteorological data
files (as in Figure 1); in other words, they did not perform content determination.
pCRU is a probabilistic language generation framework that was developed with the
aim of providing the formal underpinnings for creating narrow-domain, applied NLG
systems that are driven by comprehensive probabilistic models of the entire generation
538
Reiter and Belz Validity of Some Metrics for NLG Evaluation
Start text with direction, speed (5 kt range around actual value) from the first entry
in the data file
For each subsequent data file entry
If direction has changed 45 degrees or more since last mentioned direction, or
direction has changed at all, and speed is greater than 15 kts, then
add the following phrase to the text
? veering if direction change is clockwise, backing otherwise
? new direction
? new speed (5 kt range around value) if speed has changed by 5 kts or more
? time phrase (from fixed table which maps numeric time to a phrase)
Else if speed has changed by 5 kts or more since last mentioned speed, then
add the following phrase to the text
? becoming
? new speed (5 kt range around actual value)
? time phrase (from fixed table which maps numeric time to a phrase)
end if
end for
Figure 4
Template algorithm (simplified by removing special cases).
space. NLG systems are modeled as sets of generation rules that apply transformations
to representations. The basic idea in pCRU is that as long as the generation rules are all of
the form relation(arg1, ...argn) ? relation1(arg1, ...argp) ... relationm(arg1, ...argq), m ? 1,n, p, q ? 0,
then the set of all generation rules can be seen as defining a context-free language and
a single probabilistic model can be estimated from raw or annotated text to guide the
generation processes.
pCRU uses straightforward context-free technology in combination with underspec-
ification techniques, to encode a base generator as a set of expansion rules G. The
pCRU decision-maker is created by estimating a probability distribution over the base
generator, as follows:
1. Convert corpus into multi-treebank: Determine for each sentence all
(left-most) derivation trees licensed by the base generator?s rules, using
maximal partial derivations if there is no complete derivation tree;
annotate the (sub)strings in the sentence with the derivation trees,
resulting in a set of generation trees for the sentence.
2. Train base generator: Obtain frequency counts for each individual
generation rule from the multi-treebank, adding 1/n to the count for every
rule, where n is the number of alternative derivation trees; convert counts
into probability distributions over alternative rules, using add-1
smoothing and standard maximum likelihood estimation.
The resulting probability distribution is used in one of the following three ways to
control generation.
1. Viterbi generation: Do a Viterbi search of the generation forest for a given
input, which maximizes the joint likelihood of all decisions taken in the
generation process.
539
Computational Linguistics Volume 35, Number 4
2. Greedy generation: Make the single most likely decision at each choice point
(rule expansion) in a generation process.
3. Greedy roulette-wheel generation: Base decisions on a non-uniform random
distribution proportional to the likelihoods of alternatives.
We also implemented two baseline pCRU systems, both of which ignore pCRU probabili-
ties: the randommode, which randomly selects generation rules; and the n-grammode,
which generates the set of alternatives and selects the most likely one according to an
n-gram language model (Langkilde and Knight 1998).
Combining the pCRU, SUMTIME, and Template systems gave us a set of systems
which had the same target functionality, but attempted to achieve it using quite different
NLG techniques and technologies. Tables 1 and 2 show examples of texts produced by
humans and our systems. The human texts include reference texts for automatic metrics
(see Section 3.3) as well as the corpus texts.
Note that we could not use other marine weather-forecast generators in our exper-
iments, such as FOG (Goldberg, Driedger, and Kittredge 1994), because they use differ-
ent inputs (that is, different numerical weather prediction models), and they produce
outputs targeted at different audiences (e.g., FOG forecasts are intended for mariners in
general, whereas SUMTIME forecasts are targeted at the offshore oil industry).
3.2 Human Evaluations
We conducted two experiments where we asked human subjects to rate texts produced
by our different marine weather-forecast generators. The main difference was that the
first experiment focused on evaluating linguistic quality, and only looked at texts with
the same information content. The second experiment also evaluated content quality,
and used texts that varied in content as well as in linguistic expression. We also changed
the experimental design in the second experiment, based on our experiences in the first
experiment.
3.2.1 First Human Evaluation. In Experiment 1 (the main results of which we reported
previously in Belz and Reiter [2006]), we focused on the content-to-realization mapping,
so we restricted ourselves to systems which generated texts from content tuples (Fig-
ure 3) (SUMTIME-Hybrid and the pCRU systems). We also included the corresponding
texts from the SUMTIME-METEO corpus.
We used a randomly selected subset of 21 forecast dates from the SUMTIME-METEO
corpus. We restricted ourselves to morning forecasts (half the corpus), as these are
based on a single data file (evening forecasts are based on two data files), and to the
first wind description in a forecast, as subsequent wind descriptions have the added
constraint of being consistent in form and content with earlier wind descriptions. For
each of these dates, we obtained seven texts: the corpus text, the texts produced by the
previously mentioned systems, and one of the reference texts used by the automatic
metrics (Section 3.3.1).1 This gave us a total of 147 texts.
1 We wanted to obtain ratings for reference texts to check that these were regarded as reasonable by the
human subjects. This was indeed the case, but we do not report the ratings of the reference texts here,
because we do not have permission to do so. Also we cannot use human ratings of reference texts in the
correlation studies reported in Section 3.3, because automatic metrics cannot be used to evaluate their
own reference texts.
540
Reiter and Belz Validity of Some Metrics for NLG Evaluation
Table 1
Texts produced for 5 Oct 2000, from content tuples in Figure 3.
Human texts:
Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY
LATE EVENING
Reference 1 SSW?LY 16-20 GRADUALLY BACKING SSE?LY THEN DECREASING VARIABLE
4-8 BY LATE EVENING
Reference 2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE
4-8 BY LATE EVENING
Reference 3 SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 04-08
BY LATE EVENING
System-generated texts:
ST-Hybrid SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR
LESS BY MIDNIGHT
pCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY
LATE EVENING
pCRU-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8
pCRU-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATER
pCRU-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON
THEN VARIABLE 4-8
Table 2
Texts produced for 6 Oct 2000, directly from numerical wind data.
Human texts:
Corpus N-NW 12-18 BACKING / EASING SSW LESS THAN 08 BY END OF
PERIOD
Reference 1 N-NW 12-17 DECREASING 10 OR LESS BY LATE AFTERNOON AND BACKING SW
LATER
Reference 2 NW-NNW 12-15 KNOTS DECREASING AND BACKING TO BE WSW 05-08 BY
EVENING AND VARIABLE LESS THAN 05 KNOTS BY MIDNIGHT
Reference 3 W-NNW 12-16 BACKING AND EASING THROUGH LATE AFTERNOON / EARLY
EVENING SW-SSW LESS THAN 10
System-generated texts:
SUMTIME NNW 12-17 EASING 10 OR LESS BY MID AFTERNOON THEN BACKING WSW BY
LATE AFTERNOON AND SSW BY MIDNIGHT
Template NNW 13-18 BECOMING 08-13 IN THE MID-AFTERNOON BACKING WSW IN THE
EARLY EVENING BECOMING 03-08 DURING THE NIGHT BACKING SSW 00-05
AROUND MIDNIGHT
For our human evaluators, we recruited nine people with experience reading fore-
casts for offshore oil rigs (?experts?). Note that these were experienced forecast readers,
not forecast writers. We also recruited 21 people with no experience in reading forecasts
for offshore oil rigs (?non-experts?). The reason for including non-experts was that we
wanted to see if ratings by non-experts were similar to ratings by experts (as non-experts
are often much easier to recruit for experiments). None of the subjects had a background
in NLP, and all were native speakers of English.
Subjects were shown forecast texts from all the content-to-text generators, and from
the corpus, and asked to give each text a single score on a scale of 0 to 5, which was
explained as reflecting readability, clarity, and general appropriateness. Experts (only) were
also shown the numerical weather data that the forecast text was based on. Subjects
were not shown reference texts, as is often done in MT evaluations (Section 2.2.1). The
541
Computational Linguistics Volume 35, Number 4
experiment was done over the World Wide Web, at a time and place convenient to the
subjects.
All subjects were shown two practice examples at the beginning of the test which
were not included in the analysis. Expert subjects were then shown one randomly
selected text for 18 of the dates. The non-experts were shown 21 forecast texts, in a
Repeated Latin Squares experimental design, where each of the 147 texts was rated by
three subjects.
The average scores assigned by experts and non-experts are shown in Table 3. There
was good correlation between experts and non-experts: Pearson?s r = 0.874 (p = 0.011,
one-tailed). Experts and non-experts also agreed about relative rankings, except that
experts rank pCRU-greedy second and the corpus texts third, whereas the non-experts
have these the other way around.
To determine if any of the differences were statistically significant, we used SPSS?s
General Linear Model (GLM), with rating as the dependent variable, and generator,
subject, and forecast date as independent variables; we used a post hoc Tukey HSD test to
identify significant differences between individual systems. This analysis is essentially
equivalent to normalizing (adjusting) the ratings to remove differences due to subjects
(some people give lower ratings than others) and forecast dates (some meteorological
data sets are harder to describe than others), and then performing a one-way ANOVA on
the normalized scores using generator as the independent variable.
The GLM analysis showed a very significant effect of generator on ratings, p <
0.001 (two-tailed). Table 3 shows the homogeneous subsets identified by the Tukey
HSD post-hoc test. These correspond to the following pairwise results. For the experts,
SUMTIME-Hybrid was significantly better than pCRU-random and pCRU-2gram, and
pCRU-greedy was better than pCRU-random. For the non-experts, all systems were
better than pCRU-random, and SUMTIME-Hybrid was also better than pCRU-2gram.
The SPSS GLM analysis also showed that scores were significantly affected by sub-
ject, for both experts and non-experts (p < 0.001); in other words, different individuals
rated texts differently. Non-expert scores were also significantly influenced by forecast
Table 3
Experiment 1: Mean human ratings (single criterion of output quality), and homogeneous
subsets from Tukey HSD analysis.
Experts Non-Experts
Mean Subsets Mean Subsets
SUMTIME-Hybrid 3.82 A SUMTIME-Hybrid 3.90 A
pCRU-greedy 3.59 A B SUMTIME-Corpus 3.62 A B
SUMTIME-Corpus 3.22 A B C pCRU-greedy 3.51 A B
pCRU-roulette 3.11 A B C pCRU-roulette 3.49 A B
pCRU-2gram 2.68 B C pCRU-2gram 3.29 B
pCRU-random 2.43 C pCRU-random 2.51 C
The subsets show which differences are statistically significant. More specifically, systems which
are in the same subset do not have statistically significant differences in their mean ratings; systems
which are not in the same subset do have statistically significant differences in their mean ratings.
For example, for both experts and non-experts, pCRU-greedy is not significantly different from
SUMTIME-Hybrid (since both are in subset A), and pCRU-greedy is not significantly different from
pCRU-2gram (since both are in subset B). However, SUMTIME-Hybrid is significantly different
from pCRU-2gram, since no subset contains both of these.
542
Reiter and Belz Validity of Some Metrics for NLG Evaluation
date (p < 0.001); in other words, some forecast data sets were harder to describe than
others.
The fact that subject and forecast date influence human ratings suggests that a Latin
Square experimental design should be used. In a Latin Square design, every subject
rates the same number of texts from each generator, and every generator is rated an
equal number of times on each forecast date; this reduces the impact of idiosyncratic
differences between individual subjects (and forecast dates). For example, in Exper-
iment 1, expert subject TR gave higher ratings than average (mean of 4.17, against
an overall mean for expert subjects of 3.06), as did non-expert subject MP (mean of
4.57, again an overall mean for non-experts of 3.34). In the non-Latin-Square design
used for expert subjects, TR rated 4 texts from pCRU-greedy, but only one text from
SUMTIME-Hybrid; this may have inflated pCRU-greedy?s mean score (Table 3) relative
to SUMTIME-Hybrid?s. In the Latin Square design used for non-experts, in contrast, all
the subjects, including MP, rated the same number of texts (three) from each generator;
hence MP?s generosity presumably benefited all generators equally, and did not inflate
the score of any one generator compared to the other generators.
3.2.2 Second Human Evaluation. Our first experiment focused on linguistic expression,
but of course content determination is very important in NLG, so we decided to run
another experiment which also included texts generated from meteorological data, by
systems which performed content determination (that is, SUMTIME and Template). In
this experiment we showed subjects the raw forecast data and asked them for separate
ratings on ?clarity and readability? (which was intended to elicit an assessment of
linguistic quality) and ?accuracy and appropriateness? (which was intended to elicit
an assessment of content quality). For brevity, we refer to these scores as Clarity and
Accuracy, respectively, herein.
We used 14 new randomly selected forecast dates, and 14 new expert subjects (we
did not ask non-experts to rate these texts, because we were not confident that they
could assess the accuracy and appropriateness of texts). The subjects were asked to rate
seven types of texts: corpus texts, SUMTIME texts, Template texts, and texts produced
by the Experiment 1 systems (except that we dropped pCRU-2gram); we did not in this
experiment ask subjects to rate reference texts. We would have liked to recruit more
than 14 subjects, but this proved difficult (see also Section 4.2); however, 14 subjects is
an improvement over the 9 expert subjects used in Experiment 1 from the perspective
of limiting the impact of individual differences between subjects.
We also made a number of changes to our experimental design, based on issues
identified in the first experiment with expert subjects. The most important ones were
that we used a Latin Square design (with two subjects rating each system/date combi-
nation), we asked for ratings on a seven-point scale instead of a six-point one (so the
scale had a middle position which subjects could select), we explicitly gave instructions
as to what the ratings meant (to reduce variation due to differing interpretations of the
scale), and we carried out a non-parametric as well as parametric statistical analysis. A
screenshot from the experiment is shown in Figure 5.
There was a significant (p < 0.001) correlation between the accuracy and clarity
scores that subjects gave to texts (Pearson r = 0.58), when computed on the 196 indi-
vidual ratings made by subjects (when correlation is computed on the mean values, sig-
nificance cannot be shown, because there are far fewer data points: Pearson?s r = 0.572,
p = 0.09). It is not clear whether this is because subjects did not properly distinguish
accuracy from clarity, or because generators that generated high-accuracy texts (such as
SUMTIME) also generated high-clarity texts.
543
Computational Linguistics Volume 35, Number 4
Figure 5
Screenshot from Experiment 2.
The averaged results of the human evaluations in Experiment 2 are shown in Table 4.
Because the Experiment 1 texts were communicating the same content, and only dif-
fered in linguistic expression, it seems likely that Experiment 2?s clarity scores should
correlate with Experiment 1?s scores. This is indeed the case: The correlation between
average scores for the five systems that were included in both experiments is high with
Pearson?s r = 0.9 (p < 0.05).
Table 4
Experiment 2: Clarity and Accuracy average scores (expert subjects); homogeneous subsets from
Tukey HSD analysis.
Clarity Accuracy
Mean Subsets Mean Subsets
SUMTIME 5.04 A SUMTIME 5.07 A
pCRU-greedy 4.79 A B Template 4.46 A B
SUMTIME-Hybrid 4.61 A B pCRU-greedy 4.32 A B
pCRU-roulette 4.54 A B SUMTIME-Corpus 4.11 B
SUMTIME-Corpus 4.50 A B SUMTIME-Hybrid 3.96 B
Template 4.04 B C pCRU-roulette 3.89 B
pCRU-random 3.64 C pCRU-random 3.79 B
544
Reiter and Belz Validity of Some Metrics for NLG Evaluation
The SPSS GLM found a very significant effect of generator on both accuracy and
clarity scores (p < 0.001). Table 4 shows the homogeneous subsets identified by the
Tukey HSD post hoc test. The corresponding pairwise results are as follows. For the
clarity scores, SUMTIME is significantly better than Template and pCRU-random; and
all systems except Template are better than pCRU-random. For the accuracy scores,
SUMTIME is significantly better than all systems except pCRU-greedy and Template. The
GLM analysis also showed that subject and forecast date (as well as generator) had a
significant impact on accuracy and clarity ratings (p < 0.002).
This statistical analysis assumes that it is appropriate to use ANOVA-like tests to
analyze quality ratings. Although this is common practice in many NLP papers, in-
cluding most previous validation studies of automatic metrics which we are aware
of, a good argument can be made that quality ratings should be analyzed using non-
parametric tests. This is because ratings are ordinal, so it is not clear that it makes sense
to compute their mean, which is what the ANOVA and GLM tests do. Hence we also
carried out a non-parametric analysis to identify significant differences in the human
ratings. More specifically, we used the Wilcoxon Signed-Rank test, with a Bonferroni
multiple-hypothesis correction, to identify pairs of systems that had significantly differ-
ent ratings. When comparing two systems, the Wilcoxon test requires each rating of the
first system to be paired with a related rating of the second system. Because we had two
ratings from every subject for each system, we paired the lowest rating that a subject
gave to a text produced by the first system with the lowest rating that that subject gave
to a text produced by the second system; we similarly paired the highest ratings given
by each subject to the two systems.
For example, subject AG evaluated two SUMTIME texts and gave them clarity
ratings of 5 and 6; he also evaluated two Template texts, and gave them clarity ratings
of 4 and 6. In our non-parametric analysis, we paired the lowest rating given by AG
to a SUMTIME text (that is, 5) with the lowest rating given by AG to a Template text
(that is, 4); we also paired the highest rating given by AG to a SUMTIME text (that is,
6) with the highest rating given by AG to a Template text (that is, 6). This pairing was
possible because we used a Latin Square design in this experiment, which meant that
every subject rated the same number of texts (two) from each generator.
This procedure identified the same four significant differences in Accuracy as in
Table 4?namely, SUMTIME is significantly better than all systems except pCRU-greedy
and Template. However, it identified only three significant differences in Clarity?
namely, SUMTIME is significantly better than Template and pCRU-random, and pCRU-
greedy is better than pCRU-random (this is a subset of the significant differences in
Clarity shown in Table 4).
3.3 Correlation between Automatic Metrics and Human Judgments
In line with standard practice in validating metrics in MT, our main tool in analyzing the
ability of automatically calculated metrics to predict human judgments is calculating
Pearson correlation coefficients between sets of scores produced by metrics and the
human ratings from Section 3.2. We computed correlations between metric scores for
the different systems and the mean human ratings for these systems; for example, we
computed the correlation between the BLEU score for SUMTIME and the average rating
given by the human subjects to SUMTIME texts. We did not compute correlations on
individual texts; for example, we did not try to correlate the BLEU score for the specific
SUMTIME text shown in Table 2 against the human ratings of this specific text. This
545
Computational Linguistics Volume 35, Number 4
is because metrics such as BLEU and ROUGE are not intended to be meaningful for
individual sentences.
Note that because correlations are being computed on a small set of numbers (seven
at most), fairly high correlation coefficients are needed to achieve significance. In part
because of this, we were less conservative in our statistical significance calculations
than in the experiments reported in Section 3.2. In particular, we computed statistical
significance of correlations using one-tailed (instead of two-tailed) tests, and we did not
apply a Bonferroni multiple-hypothesis correction. None of the correlations presented
subsequently would be significant if Bonferroni-adjusted two-tailed p-values were
used; indeed we could only realistically expect to get significant correlations under this
measure if we looked at more systems and/or fewer metrics (this is further discussed
in Section 4.3).
3.3.1 Metrics and Reference Texts Used. We tested five automatic corpus-based metrics:
two variants of the BLEU metric used in machine translation (Papineni et al 2002); two
variants of the ROUGE metric used in document summarization (Lin and Hovy 2003);
and a simple sting-edit distance metric (as a baseline).
BLEU is a precision metric that assesses the quality of a generated text in terms of
the proportion of its word n-grams that it shares with reference texts. BLEU scores range
from 0 to 1, where 1 is the highest which can only be achieved by a generated text if all its
substrings can be found in one of the reference texts (hence a reference text will always
score 1). BLEU should be calculated on a large test set with multiple reference texts. We
used BLEU-42 (that is, BLEU calculated using n-grams of size up to n = 4) because this
version of BLEU is the main metric used in recent NIST Machine Translation evaluations
(and indeed seems to have become a standard in the MT community). We also used the
NIST
3
MT evaluation score (Doddington 2002); this is an adaptation of BLEU which gives
more weight to less frequent n-grams which are assumed to be more informative.
There are several different ROUGE metrics. The simplest is ROUGE-N, which com-
putes the highest proportion in any reference text of n-grams of length N that are
matched by the generated text. A procedure is applied that averages the score across
leave-one-out subsets of the set of reference texts. ROUGE-N is an almost straightforward
n-gram recall metric between two texts, and has several counter-intuitive properties,
including that even a text composed entirely of sentences from reference texts cannot
score 1 (unless there is only one reference text). ROUGE-SUN looks at ?skip bigrams?
that occur in the generated text and reference texts; a skip bigram is two words which
are not necessarily adjacent, but may be separated by up to N intermediate words. We
used ROUGE-2 and ROUGE-SU44 because these are the main automatic metrics used in
recent NIST Document Understanding Conferences (DUC).
We also included string-edit distance as a very simple automatic metric, which can
be considered a sort of baseline. String-edit distance (SE) was computed with substitu-
tion at cost 2, and deletion and insertion at cost 1, and normalized to range 0 to 1 (perfect
match). When multiple reference texts are used, the SE score for a generated text is the
average of its scores against the reference texts; the SE score for a set of generated texts
is the average of scores for the individual texts.
2 Calculated by ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl.
3 Calculated by http://cio.nist.gov/esd/emaildir/lists/mt list/bin00000.bin.
4 ROUGE code obtained via http://www.isi.edu/~cyl/rouge/latest.html.
546
Reiter and Belz Validity of Some Metrics for NLG Evaluation
Table 5
Experiment 1: Metric scores against three reference texts (produced by rewriting corpus texts),
for the set of 18 forecasts used in expert evaluation.
System Exp Non NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SE
ST-Hybrid 3.82 3.90 (1) 6.382 (3) 0.584 (4) 0.558 (4) 0.528 (4) 0.705 (5)
pCRU-greedy 3.59 3.51 (3) 6.871 (2) 0.694 (2) 0.656 (2) 0.634 (2) 0.800 (2)
ST-Corpus 3.22 3.62 (2) 8.705 (1) 0.951 (1) 0.839 (1) 0.815 (1) 0.917 (1)
pCRU-roulette 3.11 3.49 (4) 6.206 (4) 0.563 (5) 0.554 (5) 0.51 (5) 0.735 (4)
pCRU-2gram 2.68 3.29 (5) 5.925 (5) 0.598 (3) 0.586 (3) 0.556 (3) 0.783 (3)
pCRU-random 2.43 2.51 (6) 4.608 (6) 0.355 (6) 0.462 (6) 0.419 (6) 0.649 (6)
For convenience, expert (Exp) and non-expert (Non) scores are also shown.
All of these automatic metrics require reference texts. For Experiment 1, which
focused on content-to-text, we asked three meteorologists5 (who had not contributed
to the original SUMTIME-METEO corpus) to rewrite the corpus texts for the 21 dates
used in Experiment 1 (each meteorologist rewrote all 21 corpus texts), correcting and
improving them as they saw fit; examples are shown in Table 1. In principle, it would
have been preferable to ask the forecasters to write texts based on content tuples (the
actual input to the systems), but this is not a natural task for forecasters (they write texts
from data, not from content tuples). However, asking them to rewrite corpus texts meant
they were unable to change the content, and focused on lexical choice and syntactic
structure, as intended.
For Experiment 2, which also looked at systems which performed content determi-
nation, we asked the same three meteorologists to write reference texts based on the raw
numerical input data for the 14 dates used in Experiment 2 (each meteorologist wrote a
text for all 14 dates); examples are shown in Table 2. They were not shown the corpus
forecasts corresponding to the data.
3.3.2 Correlation with Results from Experiment 1. Table 5 shows the average scores each
metric assigned to each system when calculated on the texts used in the Experiment 1
expert evaluations.6 The metrics all rank the corpus texts highest, the pCRU-greedy texts
second, and the pCRU-random texts lowest. Their strong preference for the corpus texts
is probably an artefact of the way the reference texts were produced. The forecasters
were asked to rewrite the corpus texts, which resulted in considerable similarity be-
tween the reference texts and the corpus texts. In calculating correlation figures (shown
in Table 6), we therefore produced two sets of figures, one for the NLG systems and
the corpus texts (I in the table) and one for just the NLG systems (II); set II should be
regarded as a post hoc analysis. For set I, none of the metrics significantly correlate
5 When we first reported results for Experiment 1 in Belz and Reiter (2006), we only had reference texts
from two meteorologists, but we have since obtained reference texts from a third meteorologist. This is
why the numbers in Tables 5 and 6 differ from the numbers given in Belz and Reiter (2006).
6 The important information in Table 5 is the differences in the scores assigned by the same metric to
different systems. Differences in the scores assigned by different metrics to the same system are not
meaningful; they are just mathematical artefacts of the formulas used to calculate the metrics. For
example, the fact that BLEU-4 gives SUMTIME-Hybrid a higher score than pCRU-random is important: this
shows that BLEU-4 (correctly) predicts that human subjects prefer SUMTIME-Hybrid texts to
pCRU-random texts. The fact that String-Edit (SE) gives a higher rating than BLEU-4 to SUMTIME-Hybrid
is not important, as it does not tell us anything about how well SE or BLEU-4 can predict differences in
human ratings of texts.
547
Computational Linguistics Volume 35, Number 4
Table 6
Experiment 1: Correlation (Pearson?s r) between human scores and automatic metrics.
Experts Non-exp. NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SE
I. Pearson?s r, all NLG systems and corpus texts
Experts 1 0.874 0.534 0.461 0.362 0.379 0.260
Non-exp. 0.874 1 0.685 0.639 0.518 0.527 0.483
II. Pearson?s r, just NLG systems (not corpus texts) (post hoc)
Experts 1 0.884 0.836 0.700 0.611 0.616 0.339
Non-exp. 0.884 1 0.886 0.797 0.654 0.647 0.505
Significant correlations (1-tailed, no Bonferroni correction) are shown in bold.
with human ratings. For set II, NIST-5 has a significant correlation with both expert and
non-expert scores.
3.3.3 Correlation with Results from Experiment 2. Table 7 shows the average scores each
metric assigned to each system for the texts used in Experiment 2: The rankings assigned
by NIST-5 and BLEU-4 are identical, and SE largely agrees, with small differences in the
top rankings (where differences between scores are very small), whereas the ROUGE
metrics disagree with the other metrics to some degree. Table 8 shows the corresponding
correlation figures for all NLG systems and the corpus texts (I), all NLG systems (II),
and texts that communicate the same content (III). Set III consists of corpus texts and
texts produced by systems whose input is corpus-derived content tuples (pCRU and
SUMTIME-Hybrid). Note that the figures in sets I and II are similar; because reference
texts for Experiment 2 were produced from raw data, the automatic metrics are not
biased towards the corpus texts as they were in Experiment 1.
The most striking result from the correlation figures is that not a single metric corre-
lates significantly with human judgments of accuracy. Clarity is significantly correlated
with NIST-5 in sets I and III, and BLEU-4 and SE in set III.
This analysis assumes that it is sensible to use mean values of the human ratings;
however, as mentioned at the end of Section 3.2.2, a good argument can be made that
ordinal ratings should not be averaged. An alternative way of assessing the predictive
accuracy of the metrics is to count how many of the significant differences identified
by the non-parametric analysis in Section 3.2.2 were predicted by differences in
metric scores. For example, the non-parametric analysis showed that human subjects
Table 7
Experiment 2: Metric scores against three reference texts (written from raw data), for the set of
14 forecasts.
System Cla. Acc. NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SE
SUMTIME 5.04 5.07 (1) 4.668 (5) 0.187 (5) 0.241 (5) 0.155 (6) 0.392 (5)
pCRU-greedy 4.79 4.32 (3) 5.118 (2) 0.244 (2) 0.258 (3) 0.180 (3) 0.42 (2)
ST-Hybrid 4.61 3.96 (5) 5.223 (1) 0.281 (1) 0.281 (1) 0.227 (1) 0.415 (3)
pCRU-roulette 4.54 3.89 (6) 4.798 (4) 0.221 (4) 0.244 (4) 0.169 (4) 0.421 (1)
ST-Corpus 4.50 4.11 (4) 4.969 (3) 0.227 (3) 0.281 (1) 0.21 (2) 0.415 (3)
Template 4.04 4.46 (2) 3.299 (7) 0.106 (7) 0.168 (7) 0.089 (7) 0.333 (7)
pCRU-random 3.64 3.79 (7) 4.011 (6) 0.162 (6) 0.238 (6) 0.156 (5) 0.379 (6)
For convenience, human ratings also shown.
548
Reiter and Belz Validity of Some Metrics for NLG Evaluation
Table 8
Experiment 2: Correlation (Pearson?s r) between human score and metrics.
Cla. Acc. NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SE
I. Pearson?s r, all NLG systems and corpus texts
Clarity 1 0.572 0.701 0.577 0.431 0.401 0.571
Accuracy 0.572 1 ?0.118 ?0.281 ?0.308 ?0.375 ?0.286
II. Pearson?s r, just NLG systems (not corpus texts)
Clarity 1 0.583 0.711 0.578 0.455 0.417 0.578
Accuracy 0.583 1 ?0.092 ?0.265 ?0.285 ?0.358 ?0.266
III. Pearson?s r, content-to-text NLG systems and corpus texts (same content)
Clarity 1 0.741 0.959 0.858 0.550 0.549 0.969
Accuracy 0.741 1 0.682 0.502 0.432 0.294 0.607
Significant (1-tailed, no Bonferroni correction) correlations are shown in bold.
considered SUMTIME to be significantly more accurate than pCRU-random. If we look
at the metric scores shown in Table 7, we see that ROUGE-2 rated pCRU-random higher
than SUMTIME, and all other metrics rated SUMTIME higher than pCRU-random.
Hence ROUGE-2 did not predict this significant difference in human Accuracy ratings
(SUMTIME better than pCRU-random), but the other metrics did.
Table 9 shows the predictive accuracy of the metrics (in this sense). Overall this
agrees with the main finding of the parametric analysis (see Table 3), namely that
existing metrics are better at predicting Clarity than Accuracy.
It is interesting to compare our findings with Stent, Marge, and Singhai (2005), who
examined the correlation between human judgments and several automatic metrics
(including BLEU, NIST, and ROUGE) when evaluating computer-generated paraphrases.
They in fact found more or less the opposite result, namely, that the metrics correlated
with adequacy (similar to our Accuracy) but not fluency (similar to our Clarity). This
may partially be due to the fact that Stent, Marge, and Singhai used a single reference
text, which was the original input text to the paraphraser. With such a reference text, we
wonder if the metrics largely measured the amount of paraphrasing done; that is, texts
with less paraphrasing (whose surface form was thus closer to the original texts) were
rated higher by the metrics. If so, it would not be surprising if the metrics correlated with
accuracy but not with fluency, because it is possible that subjects regarded sentences
whose surface form was close to the original text as more accurate but not necessarily
more fluent. In their discussion section, Stent, Marge, and Singhai acknowledged that
using a single reference text is problematic, and recommended that multiple reference
Table 9
Number of significant non-parametric differences predicted by each metric.
Metric Clarity Accuracy
NIST-5 3 1
BLEU-4 3 1
ROUGE-SU4 3 1
ROUGE-2 2 0
SE 3 1
actual number of
significant diff 3 4
549
Computational Linguistics Volume 35, Number 4
texts should be used if possible; however they also pointed out that this is not a panacea,
since even 3?4 reference texts are unlikely to capture all of the acceptable variations in
a text.
3.3.4 Summary of Results. In Experiment 1 all systems communicated the same content
(they take the same content tuples as inputs), subjects were asked to give a single overall
rating for texts, and reference texts were created by rewriting corpus texts. Under these
conditions, NIST-5 scores are significantly correlated with expert scores (Table 6).
In Experiment 2, systems communicated different contents, subjects were asked
to give separate clarity and accuracy ratings for a system, and reference texts were
created by writing new texts from numerical data. Under these conditions, NIST-5 is
significantly correlated with human clarity judgments. If only texts which communicate
the same content at the content tuple level are included in the analysis, then NIST-5 and
SE are strongly correlated with clarity judgments (r > 0.95), and BLEU-4 also correlates
significantly (Table 8). However, no metric correlates significantly with human accuracy
judgments under any analysis.
4. Discussion: What Can We Conclude from Our Results
The most obvious interpretation of our results is that it is acceptable to use BLEU-like
metrics (with caution) to estimate the linguistic quality of generated texts, especially
when comparing texts which are communicating the same content; but current auto-
matic metrics should not be used to evaluate the quality of the content of generated
texts. However, there are a number of caveats which must be considered which we
discuss subsequently. These caveats may have relevance to other validation studies of
automatic metrics in NLP.
Determining the validity of ?cheap? evaluation techniques which are intended to
approximate the genuine outcome measure is of course a problem that occurs in many
areas of science, and we believe it is useful to look at what other fields do in this regard.
Hence we relate our discussion to validation requirements in clinical medicine for
?surrogate measures? (Greenhalgh 2006) (for example, using blood tests that measure
HIV viral load to evaluate the effectiveness of AIDS treatments, instead of measuring
actual mortality); and criterion validation requirements in psychology for psychometric
and other tests (Kaplan and Saccuzzo 2001).
One general lesson from psychology is that there can be a strong temptation to
use evaluation techniques which are quick, cheap, and appear to be impartial, even if
they are known to have very limited validity. For example, psychometric tests which
claim to predict academic success, such as the American SAT test, are very heavily used
by American universities when they make admissions decisions, despite the fact that
numerous validation studies have shown that these tests are poor predictors of how
well a student does at university over the four-year span of a typical degree (although
they do have a limited correlation with academic performance in a student?s first year
at university).
4.1 Generality across Domains, Genres, and Systems
Our experiments have been carried out in the specific domain and genre of marine
weather forecasts for offshore oil rigs, and are based on a set of seven specific NLG
systems. Will they apply to other domains, and indeed even to marine-weather-forecast
generators built with different NLG technologies? Of course similar concerns have been
550
Reiter and Belz Validity of Some Metrics for NLG Evaluation
raised about automatic metrics in other areas of NLP. For example, most validation
studies of automatic metrics in machine translation and document summarization have
been done with newswire texts; it is not clear, however, that results obtained from
translated newswire texts also apply to translated scientific papers, for example.
A similar point is made strongly in the medical and psychological literature: Vali-
dation studies are performed in a particular context, and it is very risky to generalize
them to other contexts, without additional evidence that they are effective in these new
contexts. For example, Kaplan and Saccuzzo (2001, chapters 11 and 19) discuss the
WAIS intelligence test, the original version of which was developed solely using data
from subjects of European descent. Later research suggested it was not valid for other
subjects; for example a variant called WISC, which was used in some school systems to
decide which children should go to special education classes, was shown in the 1970s to
correlate with teacher assessments of children of European descent, but not with teacher
assessments of children from other ethnic groups. The test was subsequently revised to
enhance its validity for children from diverse backgrounds.
We do not know how generalizable our findings are to other NLG contexts. We
would have a better idea of generalizability if we performed similar experiments in
other domains and genres, using systems built with a wide range of NLG technologies;
but of course we cannot realistically expect to conduct enough experiments to examine
all domains, genres, and technologies of interest.
Ultimately, the key to generalizing experimental results is a good theoretical model
which is scientifically plausible and fits the experimental data. Perhaps for this reason,
surrogate measures used in medicine are expected to be biologically plausible predictors
of the actual outcome measures as well as empirically correlated with them (Greenhalgh
2006, page 95). The theoretical basis behind most current metrics used in NLG seems to
be an intuition that similarity in surface forms should correlate with similarity in task
effectiveness. It may be worth investigating whether psycholinguistic models of lan-
guage comprehension (for example, Kintsch 1998) could provide a stronger theoretical
basis for metric plausibility.
For what it is worth, our intuition is that our findings will apply to other application
domains which involve generating texts which are short, linguistically simple, and not
very varied. We would be extremely cautious about attempting to generalize our results
to application domains which require texts which are longer, more complex, and more
varied, such as BabyTalk.
4.2 Does Correlation with Human Judgments Mean Correlation
with Task-Effectiveness?
As mentioned in Section 2.1.1, task-effectiveness evaluations are the most highly re-
garded evaluations in NLG; ultimately what we usually want to know is how effective
NLG texts are in achieving their communicative goal, not whether readers like them
or not. From this perspective, a major weakness in our study is that we correlated
automatic ratings with human ratings, not task-effectiveness evaluations.
Attempts to correlate automatic metrics with task-based evaluations have been
quite rare. The only ones we are aware of in NLG took place in the Generation Chal-
lenges events mentioned earlier; none of the automatic metrics used in these events had
a significant correlation with task performance. In the summarization community, Dorr
et al (2005) found very weak correlation between an automatic metric (ROUGE) and
task performance. We are not aware of any studies in machine translation which have
analyzed correlation between automatic metrics and task-performance.
551
Computational Linguistics Volume 35, Number 4
This is a major concern (as noted by Belz 2009) because we also do not know how
well human ratings predict task-effectiveness; in other words, the fact that NIST scores
predict human clarity ratings of NLG texts does not guarantee that NIST scores will
predict task effectiveness, because we do not know that human clarity ratings correlate
with task effectiveness. Looking again at medicine and psychology, validation studies
in these fields need to show correlation with the actual outcome variable or at least a
previously validated measure; in the words of Kaplan and Saccuzzo (2001, page 141),
?a meaningless [test] which is well correlated with another meaningless [test] remains
meaningless.?
A major reason why so few correlation studies have been done between automatic
metrics and task effectiveness is the significant amount of resources needed for such
studies (and this is why we did not look at task-effectiveness in this study). The problem
is not just time and money, it is also that task-based evaluations require support from
domain experts (as mentioned in Section 2.1.1), and such support can be difficult to
get for validation studies. To take a concrete example, a senior consultant at a hospital
might be willing to encourage his medical colleagues to participate in the evaluation
of a high-quality NLG system, for the purpose of determining whether this system was
a useful medical decision-support aid; but such a consultant might be less willing to
encourage his colleagues to participate in the evaluation of several NLG systems of
mixed quality, for the purpose of determining whether human ratings correlated with
automatic metrics.
Even obtaining subjects for ratings-based correlation studies can be difficult. For
example, when we ran a human judgment-based study to test the effectiveness of
SUMTIME texts (Reiter et al 2005), we managed to recruit 72 subjects in a few weeks; in
contrast it took us several months to recruit the 23 expert subjects who participated in
the studies reported in this article. Both experiments required similar time commitments
from similar subjects. However, subjects (and the domain experts who facilitated subject
recruitment) were much more enthusiastic about testing the effectiveness of a system
which they might themselves use; they were less enthused about testing hypotheses
about correlations between NLG evaluation metrics.
A related issue is how well human judgments of the clarity of texts correlate with
human judgments of the overall quality of texts; this is important because our results
suggest that current metrics are much better at predicting human clarity judgments
than human accuracy judgments. Intuitively, it seems likely that readers place more
importance on content than on linguistic expression. In the SUMTIME domain, this
intuition is supported by the SUMTIME evaluation (Reiter et al 2005), in which forecast
readers were asked to compare two forecasts, and say which was easier to read, which
was more accurate, and which was overall more appropriate. In cases where subjects
rated one forecast as easier to read and another as more accurate, they said the ?more
accurate? forecast was overall more appropriate in 55% of cases, and the ?easier to
read? forecast was overall more appropriate in only 18% of cases (in 27% of the cases
they said neither of the forecasts was overall more appropriate than the other); this is
significant at p < 0.001. Given this, it is a pity that the metrics we examined were so
much better at predicting clarity than they were at predicting accuracy.
4.3 Statistical Issues
Like other scientific experiments, NLP evaluations are regarded as producing a signifi-
cant result if they have a p-value (likelihood of incorrectly rejecting the null hypothesis)
of 0.05 or less. Of course, statistical significance can be calculated in many ways; for
552
Reiter and Belz Validity of Some Metrics for NLG Evaluation
example parametric or non-parametric tests can be applied, multiple-hypothesis (e.g.,
Bonferroni) corrections may or may not be applied, one or two-tailed p-values can be
used, post hoc findings may or not be presented, and so on.
In medicine, recent work by Ioannidis (2005a, 2005b) and others (partially based on
analyses of whether experimental results are replicated in follow-up studies) has sug-
gested that a very conservative statistical analysis should be used in medical research.
Ionnadis concludes that a very high quality medical experiment with very conservative
statistical analysis has about an 85% chance of being replicated successfully; and that
this chance quickly declines to noise levels once the design, execution, and/or statistical
analysis of the experiment becomes less than ideal.
One could argue that computational linguistics should insist on similarly strict
statistical analyses; in particular always use two-tailed p-values, always apply multiple
hypothesis corrections, always discard post hoc findings (unless they are from tests
specifically designed for post hoc analysis, such as Tukey HSD), and always use non-
parametric tests if there is any doubt about the appropriateness of parametric tests.
As we mentioned in Section 3.3, none of the correlations we observed between auto-
matic metrics and human judgments would be considered significant under such a
conservative statistical analysis. Indeed, in order to have a reasonable chance of seeing
a statistically significant correlation under a conservative statistical analysis (to have
sufficient power in a statistical sense), we would need to either look at more systems
(since the value of Pearson?s r needed to achieve statistically significant correlations
decreases as the number of points in the correlation increase), and/or look at fewer
metrics (since the impact of multiple hypothesis corrections decreases when fewer
hypotheses are being tested).
The last point is particularly worth bearing in mind, because there is a strong
temptation in validation studies to include as many metrics as possible. After all, once
the human evaluations have been collected and the reference corpora have been created,
we can compute correlations with other metrics (additional metrics such as METEOR
(Banerjee and Lavie 2005), and variations of metrics we are already examining, such as
BLEU-2 and BLEU-3 as well as BLEU-4) at the touch of a button. But if we are applying
multiple hypothesis corrections, then there is a major drawback to including a large
number of metrics in the study, which is that this will make it more difficult to find
statistically significant correlations.
However, perhaps it is wrong to use such strict statistics in computational lin-
guistics, and indeed we are aware of many reports in computational linguistics which
present one-tailed p-values, do not apply multiple hypothesis corrections, present
post hoc analyses as significant, and/or use parametric tests to analyse data which does
not have the characteristics assumed by the parametric test. In this respect the practice in
computational linguistics is perhaps closer to psychology, where (for example) multiple
hypothesis corrections are less common than they are in medicine; indeed a textbook on
statistics for psychologists used at the University of Aberdeen does not even mention
the topic.
So should our results be considered statistically insignificant (using a very strict
statistical analysis) or statistically significant (using a less strict statistical analysis)? Our
personal opinion is that the correlations we have observed are real, but it would be
extremely useful to verify this by running larger experiments which showed significant
results under a stricter statistical analysis. However, other readers may have different
opinions, and in this article we have tried to follow the advice of Greenhalgh (2006) by
giving enough information about our statistical analyses to enable readers to make their
own informed judgments as to how to interpret them.
553
Computational Linguistics Volume 35, Number 4
5. Discussion: When Should Automatic Metrics be Used in Evaluating NLG?
Our goal in this experiment was to shed light on when automatic metrics should be
used in NLG. Given all the previously mentioned caveats, we cannot of course draw
firm conclusions about this topic. But we can make some suggestions.
First of all, the automatic metrics we examined should not be used to predict
human judgments of content quality; none of them had a significant correlation with
human accuracy judgments, even when statistical significance is calculated in a less-
than-conservative fashion.
Second of all, even when evaluating linguistic quality, current automatic metrics
should be used with caution, as a supplement rather than a replacement for human
evaluation; similar comments have been made about the use of automatic metrics in
MT (Papineni et al 2002; Callison-Burch, Osborne, and Koehn 2006). Particular caution
should be used when evaluating NLG systems which generate significantly longer and
more complex texts than the marine weather forecasts we examined here. We are not
aware of any validation studies on such texts, and there are important aspects of the
linguistic quality of longer and more complex texts (such as discourse coherence) which
are not measured by current metrics.
Thirdly, automatic metrics are most appealing in contexts where a suitable corpus
of reference texts already exists. Creating good-quality reference texts is an expensive
endeavor, especially in domains (such as summaries of clinical data) where texts must
be written by skilled domain experts. Therefore we suspect that it may be difficult in
many cases to justify creating large corpora of reference texts solely for the purposes of
automatic evaluation of NLG systems.
5.1 Should Automatic Metrics be Used in Shared-Task Evaluations?
In the wider NLP community, automatic metrics are especially popular in shared-task
evaluations. This is partially because such metrics have a very low marginal cost com-
pared to human evaluations. Automatic metrics need reference texts, and obtaining
good reference texts can be costly; but once a collection of reference texts has been
created, it can be used to evaluate any number of systems. Also automatic metrics are
very easy to use once the software and reference corpus has been created; developers do
not need to be trained in using BLEU and ROUGE. In contrast human-based evaluations
generally need a certain number of subjects per system, so their cost goes up with the
number of systems evaluated. Also, expertise and/or training is needed to conduct
experiments with people, which not all NLP researchers possess. Finally, evaluation with
metrics is entirely reproducible.
However, despite the cost-effectiveness and other appealing aspects of automatic
metrics in shared tasks, we do not believe that shared tasks in NLG should use auto-
matic metrics as the sole evaluation criterion. Until there is better evidence that au-
tomatic metrics correlate with human evaluations, shared tasks in NLG should also
include human evaluations, preferably task-effectiveness ones. This strategy is being
followed in the Generation Challenges shared-task NLG events (Section 2.1.4).
5.2 Should Automatic Metrics be Used in Diagnostic Evaluation?
We have focused in this article on evaluations that measure the quality of generated
texts, but many NLG developers are also interested in diagnostic evaluations whose
purpose is to identify problems in a system and suggest improvements. From this
554
Reiter and Belz Validity of Some Metrics for NLG Evaluation
perspective, an advantage of human evaluations is that human subjects can be asked
to make free-text comments on the texts that they see, and these comments are often
extremely useful from a diagnostic perspective. On the other hand, an advantage of
automatic metrics is that they allow developers to rapidly evaluate changes to systems
and algorithms; indeed, some machine translation researchers use automatic metrics to
automatically tune parameters without human intervention (Och 2003). However, as
Och points out, this is only sensible if automatic metrics are known to be very accurate
predictors of text quality. Because our results suggest that current automatic metrics
are not highly accurate predictors of the quality of texts produced by NLG systems, we
recommend developers be cautious in using metrics for diagnostic evaluation, and do
not use metrics for automatic parameter tuning.
On the other hand, automatic metrics do have a potential advantage in small diag-
nostic evaluations, which is that they are not influenced by the individual preferences of
a small number of human subjects. There are large differences in how different human
subjects rate texts, as we pointed out at the end of Section 3.2.1. Such differences are
not unusual: we have seen them in most human evaluations of NLG systems which we
have carried out. These differences can be controlled for in a large experiment which
uses many subjects. But if a diagnostic evaluation is conducted with a small number of
subjects, who are chosen partially on the basis of being easy to recruit, there is a risk that
the preferences expressed by these subjects will not be representative of users in general,
and hence may mislead the developer as to how the system should be changed.
6. Conclusions
Automatic evaluation metrics have many desirable properties, such as being fast,
cheap, and repeatable, and they have had a significant impact in many areas of
NLP. We have compared the scores produced by several popular metrics, including
BLEU and ROUGE, to human evaluations of NLG systems, in the domain of weather
forecasts. Our results suggest that it may be appropriate to use existing automatic
metrics (with caution) to evaluate the linguistic quality of generated texts, especially
if metric evaluations supplement (rather than replace) human evaluations; for example
metric-based evaluations could be used to provide diagnostic feedback to developers
in the period before a large human evaluation. NIST-5 is perhaps the best metric to use
for this purpose (out of the ones we investigated). However, existing metrics should
not be used to evaluate the content of texts. Also it would be premature to use metrics
to test hypotheses about the effectiveness of NLG systems; we need more experimental
validation data (including validation against task effectiveness measures) and ideally a
good theoretical model as well.
Acknowledgments
Anja Belz?s work was in part supported
under UK EPSRC grant GR/S24480/01.
Many thanks to Yaji Sripada for his help
obtaining SUMTIME texts, and to Roger
Evans, Chris Mellish, Kees van Deemter,
and the anonymous reviewers for their
very helpful comments. Last but not
least, we would like to thank the
3 meteorologists, 23 expert forecast readers
(master mariners for the most part), and 21
non-experts (family, friends, and colleagues)
for their generous help in the evaluation
experiments.
References
Banerjee, Satanjeev and Alon Lavie. 2005.
Meteor: An automatic metric for MT
evaluation with improved correlation with
human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization,
pages 65?72, Ann Arbor, MI.
555
Computational Linguistics Volume 35, Number 4
Bangalore, Srinivas, Owen Rambow, and
Steve Whittaker. 2000. Evaluation metrics
for generation. In Proceedings of the 1st
International Conference on Natural Language
Generation, pages 1?8, Mitzpe Ramon.
Belz, Anja. 2008. Automatic generation
of weather forecast texts using
comprehensive probabilistic
generation-space models. Natural
Language Engineering, 14:431?455.
Belz, Anja. 2009. That?s nice ... what do you
do with it? Computational Linguistics,
35:111?118.
Belz, Anja and Albert Gatt. 2007. The
attribute selection for GRE challenge:
Overview and evaluation results. In
Proceedings of the 2nd UCNLG Workshop:
Language Generation and Machine
Translation (UCNLG+MT), pages 75?83,
Copenhagen.
Belz, Anja and Albert Gatt. 2008. Intrinsic vs.
extrinsic evaluation measures for referring
expression generation. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics (ACL?08),
pages 197?200, Columbus, OH.
Belz, Anja and Ehud Reiter. 2006. Comparing
automatic and human evaluation of NLG
systems. In Proceedings of the EACL?06,
pages 313?320, Trento.
Binsted, Kim, Helen Pain, and Graeme
Ritchie. 1997. Children?s evaluation of
computer-generated punning riddles.
Pragmatics and Cognition, 5:309?358.
Black, Rolf, Annalu Waller, Graeme Ritchie,
Helen Pain, and Ruli Manurung. 2007.
Evaluation of joke-creation software with
children with complex communication
needs. Communication Matters, 21:23?28.
Cahill, Aoife and Josef van Genabith.
2006. Robust PCFG-based generation
using automatically acquired LFG
approximations. In Proceedings of ACL?06,
pages 1033?1044, Sydney.
Callaway, Charles. 2003. Evaluating coverage
for large symbolic NLG grammars. In
Proceedings of the 18th International Joint
Conference on Artificial Intelligence (IJCAI
2003), pages 811?817, Acapulco.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and
Josh Schroeder. 2008. Further
meta-evaluation of machine translation.
In Proceedings of the Third Workshop on
Statistical Machine Translation,
pages 70?106, Columbus, OH.
Callison-Burch, Chris, Miles Osborne, and
Philipp Koehn. 2006. Re-evaluating the
role of BLEU in machine translation
research. In Proceedings of EACL-2006,
pages 249?256, Trento.
Carenini, Giuseppe and Johanna D. Moore.
2006. Generating and evaluating
evaluative arguments. Artificial
Intelligence, 170:925?952.
Coch, Jose?. 1998. Interactive generation and
knowledge administration in MultiMeteo.
In Proceedings of the Ninth International
Workshop on Natural-Language Generation
(INLG-1996), pages 300?303, Sussex.
Coughlin, Deborah. 2003. Correlating
automated and human assessments of
machine translation quality. In Proceedings
of MT Summit IX, pages 63?70, New
Orleans, LA.
Cunningham, Steven, Sarah Deere, Andrew
Symon, Robert Elton, and Neil McIntosh.
1998. A randomized, controlled trial of
computerized physiologic trend
monitoring in an intensive care unit.
Critical Care Medicine, 26:2053?2060.
Dang, Hoa. 2006. Duc 2005: Evaluation of
question-focused summarization systems.
In Proceedings of the ACL-COLING 2006
Workshop on Task-Focused Summarization
and Question Answering, pages 48?55,
Sydney.
Di Eugenio, Barbara, Michael Glass,
and Michael Trolio. 2002. The DIAG
experiments: Natural language generation
for tutoring systems. In Proceedings of the
Second International Conference on Natural
Language Generation (INLG-2002),
pages 120?127, Harriman, NY.
Doddington, George. 2002. Automatic
evaluation of machine translation quality
using n-gram co-occurrence statistics.
In Proceedings of the Second International
Conference on Human Language Technology
Research, pages 138?145, San Diego, CA.
Dorr, Bonnie, Christof Monz, Stacy President,
Richard Schwartz, and David Zajic. 2005.
Methodology for extrinsic evaluation of
text summarization: Does ROUGE
correlate? Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or
Summarization, pages 1?8, Ann Arbor, MI.
Flesch, Rudolf. 1949. The Art of Readable
Writing. Harper Brothers, New York.
Gatt, Albert, Anja Belz, and Eric Kow. 2008.
The TUNA Challenge 2008: Overview and
evaluation results. In Proceedings of the 5th
International Natural Language Generation
Conference (INLG?08), pages 198?206, Salt
Fork, OH.
Gatt, Albert, Anja Belz, and Eric Kow.
2009. The TUNA-REG Challenge
556
Reiter and Belz Validity of Some Metrics for NLG Evaluation
2009: Overview and evaluation results. In
Proceedings of the 12th European Workshop on
Natural Language Generation (ENLG?09),
pages 174?182, Athens.
Goldberg, Eli, Norbert Driedger, and Richard
Kittredge. 1994. Using natural-language
processing to produce weather forecasts.
IEEE Expert, 9(2):45?53.
Greenhalgh, Trisha. 2006. How to Read a
Paper: The Basics of Evidence Based Medicine.
BMJ Books, Oxford, third edition.
Habash, Nizar. 2004. The use of a
structural n-gram language model in
generation-heavy hybrid machine
translation. In Proceedings of the 3rd
International Conference on Natural Language
Generation (INLG ?04), volume 3123 of
LNAI, pages 61?69, Brockenhurst.
Harris, Mary Dee. 2008. Building a
large-scale commercial NLG system
for am EMR. In Proceedings of INLG-2008,
pages 157?160, Salt Fork, OH.
Hirschman, Lynette. 1998. The evolution of
evaluation: Lessons from the message
understanding conferences. Computer
Speech and Language, 12:283?285.
Ioannidis, John. 2005a. Contradicted and
initially stronger effects in highly cited
clinical research. Journal of American
Medical Association, 294:218?228.
Ioannidis, John. 2005b. Why most published
research findings are false. PLoS Medicine,
2, doi:10.1371/journal.pmed.0020124.
Kaplan, Robert and Dennis Saccuzzo.
2001. Psychological Testing: Principles,
Applications, and Issues (Fifth Edition).
Wadsworth, London.
Kintsch, Walter. 1998. Comprehension.
Cambridge University Press.
Langkilde, Irene. 2002. An empirical
verification of coverage and correctness for
a general-purpose sentence generator. In
Proceedings of the 2nd International Natural
Language Generation Conference (INLG ?02),
pages 17?24, Harriman, NY.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings of
COLING-ACL 1998, pages 704?710,
Montreal.
Law, Anna, Yvonne Freer, Jim Hunter, Robert
Logie, Neil McIntosh, and John Quinn.
2005. Generating textual summaries of
graphical time series data to support
medical decision making in the neonatal
intensive care unit. Journal of Clinical
Monitoring and Computing, 19:183?194.
Lester, James and Bruce Porter. 1997.
Developing and empirically evaluating
robust explanation generators: The
KNIGHT experiments. Computational
Linguistics, 23(1):65?101.
Lin, Chin-Ye and Eduard Hovy. 2003.
Automatic evaluation of summaries
using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL 2003,
pages 71?78, Edmonton.
Marciniak, Tomasz and Michael Strube.
2004. Classification-based generation
using TAG. In Natural Language
Generation: Proceedings of INLG-2994.
Springer, pages 100?109, Brockenhurst.
Mellish, Chris and Robert Dale. 1998.
Evaluation in the context of natural
language generation. Computer Speech
and Language, 12:349?373.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method.
In Proceedings of NAACL-2004,
pages 145?152, Boston, MA.
Oberlander, Jon. 1998. Do the right thing . . .
but expect the unexpected. Computational
Linguistics, 24:501?507.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL-2003, pages 160?167,
Sapporo.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of
ACL-2002, pages 311?318, Philadelphia, PA.
Portet, Franc?ois, Ehud Reiter, Albert Gatt,
Jim Hunter, Somayajulu Sripada, Yvonne
Freer, and Cindy Sykes. 2009. Automatic
generation of textual summaries from
neonatal intensive care data. Artificial
Intelligence, 173:789?816.
Reiter, Ehud, Roma Robertson, and Liesl
Osman. 2003. Lessons from a failure:
Generating tailored smoking cessation
letters. Artificial Intelligence, 144:41?58.
Reiter, Ehud and Somayajulu Sripada.
2002. Should corpora texts be gold
standards for NLG? In Proceedings of the
2nd International Conference on Natural
Language Generation, pages 97?104,
Harriman, NY.
Reiter, Ehud and Somayajulu Sripada. 2003.
Learning the meaning and usage of time
phrases from a parallel text-data corpus.
In Proceedings of the HLT-NAACL 2003
Workshop on Learning Word Meaning from
Non-Linguistic Data, pages 78?85,
Edmonton.
Reiter, Ehud, Somayajulu Sripada, Jim
Hunter, and Jin Yu. 2005. Choosing words
557
Computational Linguistics Volume 35, Number 4
in computer-generated weather forecasts.
Artificial Intelligence, 167:137?169.
Reiter, Ehud, Somayajulu Sripada, and Roma
Robertson. 2003. Acquiring correct knowledge
for natural language generation. Journal of
Artificial Intelligence Research, 18:491?516.
Scott, Donia and Johanna Moore. 2007. An
NLG evaluation competition? Eight
reasons to be cautious. In Proceedings of the
Workshop on Shared Tasks and Comparative
Evaluation in Natural Language Generation,
pages 22?23, Arlington, VA.
Spa?rck Jones, K. and J. R. Galliers. 1995.
Evaluating Natural Language Processing
Systems: An Analysis and Review. Springer
Verlag, Berlin.
Sripada, Somayajulu, Ehud Reiter, Ian Davy,
and Kristian Nilssen. 2004. Lessons from
deploying NLG technology for marine
weather forecast text generation. In
Proceedings of PAIS-2004, pages 760?764,
Valencia.
Sripada, Somayajulu, Ehud Reiter, Jim
Hunter, and Jin Yu. 2003. Exploiting a
parallel TEXT-DATA corpus. In Proceedings
of Corpus Linguistics 2003, pages 734?743,
Lancaster.
Stent, Amanda, Matthew Marge, and Mohit
Singhai. 2005. Evaluating evaluation
methods for generation in the presence of
variation. In Proceedings of CICLing 2005,
pages 341?351, Mexico City.
van der Meulen, Marian, Robert Logie,
Yvonne Freer, Cindy Sykes, Neil McIntosh,
and Jim Hunter. 2009. When a graph is
poorer than 100 words: A comparison
of computerised natural language
generation, human generated descriptions
and graphical displays in neonatal
intensive care. Applied Cognitive
Psychology, doi:10.1002/acp.1545.
Walker, Marilyn, Owen Rambow, and
Monica Rogati. 2002. Training a sentence
planner for spoken dialogue using
boosting. Computer Speech and Language,
16:409?433.
Williams, Sandra and Ehud Reiter. 2008.
Generating basic skills reports for
low-skilled readers. Natural Language
Engineering, 14:495?535.
Young, Michael. 1999. Using Grice?s maxim
of quantity to select the content of plan
descriptions. Artificial Intelligence,
115:215?256.
Zhong, Huayan and Amanda Stent. 2005.
Building surface realizers automatically
from corpora. In Proceedings of UCNLG?05,
pages 49?54, Birmingham.
558
Proceedings of NAACL-HLT 2013, pages 1174?1184,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Generating Expressions that Refer to Visible Objects
Margaret Mitchell
Johns Hopkins HLTCOE
m.mitchell@jhu.edu
Kees van Deemter
University of Aberdeen
k.vdeemter@abdn.ac.uk
Ehud Reiter
University of Aberdeen
e.reiter@abdn.ac.uk
Abstract
We introduce a novel algorithm for generat-
ing referring expressions, informed by human
and computer vision and designed to refer to
visible objects. Our method separates abso-
lute properties like color from relative proper-
ties like size to stochastically generate a di-
verse set of outputs. Expressions generated
using this method are often overspecified and
may be underspecified, akin to expressions
produced by people. We call such expressions
identifying descriptions. The algorithm out-
performs the well-known Incremental Algo-
rithm (Dale and Reiter, 1995) and the Graph-
Based Algorithm (Krahmer et al, 2003; Vi-
ethen et al, 2008) across a variety of images
in two domains. We additionally motivate
an evaluation method for referring expression
generation that takes the proposed algorithm?s
non-determinism into account.
1 Introduction
Referring expression generation (REG) is the task
of generating an expression that can identify a ref-
erent to a listener. These expressions generally take
the form of a definite noun phrase such as ?the large
orange plate? or ?the furry running dog?. Research
in REG primarily focuses on the subtask of select-
ing a set of properties that may be used to construct
the final surface expression, e.g., ?color:orange,
size:large, type:plate?. This property selection task
is optimized to meet different goals: for example,
to be identical to those a person would generate in
the same situation, or to be unique to the intended
referent and no other item in the discourse.
We focus on the task of generating referring ex-
pressions for visible objects, specifically with the
goal of generating descriptive, human-like referring
expressions. We are motivated by the desire to con-
nect this algorithm to input from a computer vision
system, and discuss how this may work through-
out the paper. Computer vision (CV) does not yet
reliably provide features for some of the most fre-
quent properties that people use in visual descrip-
tion (in particular, size-based features), and so we
use a gold-standard visual input, evaluating purely
on REG. The proposed algorithm, which we call
the Visible Objects Algorithm, is designed to ap-
proximate human variation identifying an object in
a group of visible, real world objects.
Our primary contributions are the following.
Background for each issue is provided in Section 2:
1. An approach accounting for overspecification,
underspecification, and some of the known ef-
fects of vision on reference.
2. A function to approximate the stochastic nature
of reference. This reflects that people will pro-
duce different references to the same object.
3. A separation between absolute properties like
color, which may be detected directly by CV,
from relative properties like size and loca-
tion, which require reasoning over visual fea-
tures to determine an appropriate form (e.g.,
height/width and distance features between
pixels are available from a visual input; saying
an object is ?tall? requires further reasoning).
4. An evaluation method for non-deterministic
REG that aligns generated and observed data
and calculates accuracy over alignments.
1174
2 Motivation & Overview
Most implemented algorithms for referring expres-
sion generation focus on unique identification of a
referent, determining the set of properties that dis-
tinguish a particular target object from the other ob-
jects in the scene (the contrast set) (Dale, 1989; Re-
iter and Dale, 1992; Dale and Reiter, 1995; Krahmer
et al, 2003; Areces et al, 2008). This view of refer-
ence was first outlined by Olson (1970), ?the spec-
ification of an intended referent relative to a set of
alternatives?. A substantial body of evidence now
shows that contrastive value relative to alternatives
is not the only factor motivating speakers? property
choices, specifically in visual domains. The phe-
nomena of overspecification and redundancy, where
speakers select properties that have little or no con-
trastive value, was observed in early developmen-
tal studies in visual domains (Ford and Olson, 1975;
Whitehurst, 1976; Sonnenschein, 1985) as well as
later studies on adult speakers in visual domains
(Pechmann, 1989; Engelhardt et al, 2006; Koolen et
al., 2011). The related phenomenon of underspecifi-
cation, where speakers select a set of properties that
do not linguistically specify the referent, has also re-
ceived some attention, particularly in visual domains
(Clark et al, 1983; Kelleher et al, 2005).
These findings make sense in light of visual ev-
idence that some properties ?pop out? in the scene
(Treisman and Gelade, 1980), and speakers may be-
gin referring before scanning the full set of scene ob-
jects (Pechmann, 1989), selecting those properties
that are salient for them (Horton and Keysar, 1996;
Bard et al, 2009) without spending a great amount
of cognitive effort considering the perception of a
hearer (Keysar and Henly, 2002).
We take this evidence to suggest an approach for
a visual reference algorithm that generates natural,
human-like reference by generating visual proper-
ties that are salient for a speaker.1 We can under-
stand what is salient visually (what does the visual
system first respond to, what guides attention?), lin-
guistically (what do people tend to mention in visual
scenes?), and cognitively, which we will not have
room to discuss in this paper (what is atypical for
1We can also add functionality to ensure that a referent is
uniquely identified against the contrast set (whether or not that
reflects what a person would do), which we will describe.
Figure 1: Relative properties, like size and location, are
difficult to obtain from a two-dimensional image. We find
it easy to perceive the background object as larger than
the one in the front; but they are technically the same size
in the image (from Murray et al (2006)).
this object?); as well as in terms of broader notions
of salience, e.g., discourse salience (Krahmer and
Theune, 2002).
This suggests a paradigm shift in the generation
task when referring to visible objects, if the goal is
to produce human-like reference. In particular, this
suggests moving from selecting properties that rule
out other scene objects to selecting properties that
are salient for the speaker (visually, conversation-
ally, based on previous experiences, etc.). This mir-
rors related research on the tradeoff between audi-
ence design and egocentrism in language production
(Clark and Murphy, 1982; Horton and Keysar, 1996;
Bard et al, 2009; Gann and Barr, 2013). Under-
and overspecification naturally fall out from such an
approach, with no need to specifically model either
phenomenon.
Perhaps unsurprisingly, the set of properties that
are visually salient and the set of properties that are
linguistically salient largely overlap. Color is the
first property our visual system processes, followed
soon after by size (Murray et al, 2006; Fang et al,
2008; Schwarzkopf et al, 2010); and people tend
to use color (Pechmann, 1989; Viethen et al, 2012)
and size when identifying objects, with size com-
mon when there is another object of the same type
in the scene (Brown-Schmidt and Tanenhaus, 2006).
Following this, our algorithm gives a privileged
position to these properties, processing them first.
Using computer vision techniques to determine an
object?s color works reasonably well (Berg et al,
2011), and the relevant visual features for this task
may be useful in future work to return several pos-
sible color labels that capture differences in lexical
choice (cf. Reiter and Sripada (2002)).
Detecting size does not work well (Forsyth,
1175
2011); and when it does, it will likely not take the
form supposed in recent generation work. Most
REG algorithms use a predefined single-featured
value, such as ?big?; however, given an image-based
input, obtaining such a value requires (1) determin-
ing how the object is situated in a three-dimensional
space, difficult to obtain from a two-dimensional im-
age (see Figure 1); and (2) determining what the
value should be: object detectors currently can pro-
vide the height and width of the location where an
object is likely to exist (its bounding box), as well as
the x- and y-axis locations of the pixels within the
object detection; but a value from these features like
?big?, ?tall?, or ?long? requires further reasoning.
As such, we incorporate the top-performing size al-
gorithm introduced in Mitchell et al (2011), which
takes as input the height and widths of objects in the
image and outputs a size value or NONE, indicating
that size should not be used to describe the object.
In addition to color and size, location and orien-
tation begin to be processed early on in the visual
system (Treisman, 1985; Itti and Koch, 2001), with
our first perception of location corresponding to ba-
sic cues of where an object is relative to our focus
of attention. For an input image, this simple type of
location corresponds to surface forms such as, e.g.,
?on the right of the image? or ?at the top of the im-
age?. Along with size, location and orientation make
up the three primary relative properties that we aim
to generate language for.
After the simple forms for color, size, location,
and orientation properties are processed, our visual
system feeds forward to two parallel pathways, the
so-called ?what? and ?where? pathways (Ungerlei-
der and Mishkin, 1982), which process properties
with growing complexity. The ?what? pathway in-
cludes absolute properties like shape and material,
which computer vision has had some success de-
tecting (Ferrari and Zisserman, 2007; Farhadi et al,
2009) while the ?where? pathway corresponds to
more complex spatial orientation and location infor-
mation, such as where objects are relative to one an-
other and which way they are facing.
To begin connecting this process to the genera-
tion of human-like descriptions of visible objects,
we start with the following simplification: Color and
size have a privileged status, the first properties pro-
cessed. These are followed by the relative properties
Figure 2: Initial model for generating visual reference.
of location and orientation, which may feed forward
to more complex location and orientation properties
in one pathway; and absolute properties following
color, like material and shape, which may be pro-
cessed in another pathway.
This gives us the basic model for generating ref-
erence to visible objects shown in Figure 2. To gen-
erate reference in this model, nodes correspond to
general visual attributes and may generate forms for
visual properties (attribute:value pairs). That is, a
property such as color:red is generated from the at-
tribute node color and a property such as size:tall is
generated from the attribute node size. We are lim-
ited by existing REG corpora in which properties we
can evaluate; in this paper, we examine the effect of
the independent selection of color and size, followed
by location and orientation.2
Generating human-like expressions in this setting
begins to be possible by adopting recent propos-
als that REG handle speaker variation (Viethen and
Dale, 2010) and the non-deterministic nature of ref-
erence (van Gompel et al, 2012; van Deemter et
al., 2012b). We can capture such variation simply
by estimating ?att, the likelihood that an attribute
att generates a corresponding visual property. Dur-
ing generation, the algorithm passes through each at-
tribute node, and uses this estimate to stochastically
add each property to the output property set.
Such a non-deterministic process means that the
algorithm will not return the same output every time,
which offers new challenges for evaluation. If we
run the algorithm 1,000 times, we have a distribu-
tion over several possible output property sets. From
this we can obtain the majority set and check if it
matches the majority observed set. Similarly, we can
2We have also built an algorithm and corpus with more com-
plex properties in order to tease out further details of visual ref-
erence, but must leave these details for follow up work; for now,
we focus on the properties common to REG corpora.
1176
run the algorithm for as many instances as we have
in our test data, and see how well the property sets
it produces aligns to the observed property sets. We
discuss evaluation using both methods in Section 6.
3 The State of the Art in REG
3.1 Algorithms
In order to understand how this approach compares
to the state of the art in REG, we evaluate against
two of the most well-known algorithms, the Incre-
mental Algorithm (Dale and Reiter, 1995) and the
Graph-Based Algorithm (Krahmer et. al, 2003, as
implemented in Viethen et al, 2008). Details on
these algorithms are available in their corresponding
papers. As a brief summary, both algorithms formal-
ize the objects in the discourse as a set of properties
(attribute:value pairs). For example, one object may
be represented as ?type:box, color:red, size:large?.
The task is to find the set of properties that uniquely
specify the referent. This is known as a content se-
lection problem, and the set of properties chosen by
the algorithm is called a distinguishing description.
The Incremental Algorithm (IA) proceeds by it-
erating through attributes in a predefined order (a
preference order), and for each attribute, it checks
whether specifying a value would rule out at least
one item in the contrast set that has not already been
ruled out. If it will, the attribute:value is added to
the distinguishing description. This process contin-
ues until all contrast items (distractors) are ruled out
or all available properties have been checked. We
use the implementation of the IA available from the
NLTK (Bird et al, 2009).3
In the Graph-Based Algorithm (GB), the objects
in the discourse are represented within a labeled di-
rected graph, and content selection is a subgraph
construction problem. Each object is represented as
a vertex, with properties for an object represented as
self-edges on the object vertex, and spatial relations
between objects represented as edges between ver-
tices. The algorithm seeks to find the cheapest sub-
graph, calculated from the edge costs. We use the
implementation available from Viethen et al (2008),
which adds a preference order to decide between
edges with the same cost during search. This has
3https://github.com/nltk/nltk contrib/blob/master/
nltk contrib/referring.py retrieved 1.Aug.2012.
been one of the best-performing systems in recent
generation challenges (Gatt and Belz, 2008; Gatt et
al., 2009).
An important commonality between these algo-
rithms, and much of the work on REG that they
have influenced, is the focus on unique identifica-
tion and operating deterministically. Both produce
one property set (and only one), and stop once a tar-
get item has been uniquely identified (or else fail).
Their driving goal is to rule out distractor objects.
In the approach introduced here, the algorithm
produces a distribution over several possible out-
puts, and the initial driving mechanism is based on
likelihood estimates for each attribute independent
of the other objects in the scene, rather than ruling
out all distractors. This offers a way to capture some
aspects of human-like reference, including under-
and overspecification and speaker variation. Due to
the fundamentally different objective of this algo-
rithm, we will call the kind of expression it generates
an identifying description, following Searle (1969).
This is a description that the system finds (1) useful
to describe the referent and (2) true of the referent.
4 The Algorithm
The Visible Objects Algorithm iterates through lists
of visible attributes, stochastically adding properties
to the property set it will generate. After this initial
search, the algorithm then scans through the objects
in the scene, roughly corresponding to how people
scan a scene when referring (Pechmann, 1989). The
target referent type, corresponding to the head noun
in the final generated description, is added to the
property set at the end of the algorithm.
We represent the basic components of the algo-
rithm graphically in Figure 3. Full code is available
online.4 After START, the algorithm proceeds in par-
allel through a list of absolute attributes and a list
of relative attributes. The likelihood of generating a
property is a function of the prior likelihood ?att and
?, a penalty on the length of the constructed prop-
erty set up to that point. This ensures that only a few
properties are generated for a referent, and the ex-
pression will not be too complex. This is also in line
with recent research suggesting that there are rarely
more than three adjectives in a visual noun phrase
4https://github.com/itallow/VisibleObjectsAlgorithm.
1177
(Berg et al, 2011). Once the algorithm hits END,
it scans through the objects in the scene. If it finds
an object that is the same type as the referent object,
the algorithm checks through the attributes again in
a preference order akin to the IA, comparing the ob-
ject?s properties against the referent?s and generating
properties as a function of the length penalty alone.
If the algorithm does not find an object that it is the
same type, no further properties are added.
4.1 Requirements
The algorithm requires the following:
1. Prior likelihood estimates on the inclusion of
different attributes. Represented as ?att.
2. Ordered list of absolute attributes beyond color.
Represented as AP.
3. Ordered list of relative attributes beyond size.
Represented as RP.
4. Ordered list of all attributes. Represented as P.
5. Ordered list specifying the order in which to
scan through other scene objects. The current
implementation uses the order in which the ob-
jects are listed in the corpora it is run on.
(1) is similar to the cost functions for GB, but
attributes are selected non-deterministically using
prior likelihoods. (2), (3), and (4) are similar to
the IA?s and GB?s preference order. For our eval-
uation corpora, AP is empty and RP contains loca-
tion and orientation. (5) is novel to this algorithm,
defining an order in which to compare the target ob-
ject against other objects in the scene. This is in-
spired by the process of incremental speech produc-
tion (Pechmann, 1989), where speakers scan objects
during naming, incrementally producing properties.
4.2 The Stochastic Process
Generally speaking, we want to penalize longer de-
scriptions and encourage the attributes that we know
people are likely to use. We can encourage a likely
attribute by using its prior likelihood as an estimate
of whether to include it. We can penalize longer de-
scriptions with a penalty proportional to the length
of the property set under construction. In other
words, given a prior likelihood estimate for includ-
ing an attribute att, ?att, and the property set con-
structed so farA, we compute whether to add a prop-
a. b.
TUNA corpus GRE3D3 corpus
Figure 4: Example scenes from corpora.
erty for att toA as a function of ?att and the length-
based penalty ?:
f(A ? {x}) = ??att
where
? =
{
1
?|A| if |A| > 0
1 otherwise
and ? is an empirically determined weight. The
algorithm then chooses a random number n, 0 ?
n ? 1. If n < f(A ? {x}), it adds the property.
4.3 Scanning Through Objects
After the initial pass through the properties, the al-
gorithm compares each object in the scene that is
the same type as the target. If the values for an
attribute are different, then the corresponding prop-
erty is added to the property set based on the length
penalty alone; when the goal is unique identification,
the algorithm can use no penalty. In development,
we found that incrementally scanning through ob-
jects after initially adding properties resulted in bet-
ter performance than an algorithm that did not con-
tain this step.
4.4 Worked Example
Suppose the input in Figure 6 (visualized in Figure
4a), with the goal of referring to obj1 by producing
a property set A. First, the algorithm scans through
color and size in parallel. For color, it finds the cor-
responding value grey; with a computer vision in-
put, this would be possible using the object pixels
as features. There is no length penalty at this point
(|A|=0), so it adds the property color:grey to A with
likelihood ?color. For our evaluation domains, ?color
is around .90 across folds, and so a color property is
usually added.
For size, the algorithm finds an appropriate value
using the Size Algorithm from Mitchell et al (2011).
The Size Algorithm uses the average height and
1178
Figure 3: Basic model for generating visual reference.
width of all objects that are the same type as the ref-
erent object; in this case, obj2, obj3, obj4. This re-
turns a size value large, and so the property size:large
is added toAwith likelihood ?size (around .40 to .70
across folds, depending on the domain).
The most likely property set at this point is sim-
ply ?color:grey?. The next most likely is ?color:grey,
size:large?, then ?size:large?. There are no fur-
ther absolute properties in this example, but there
are values for the relative attributes loc (location)
and ori (orientation). Assuming RP=?location,
orientation?, the algorithm first analyzes location,
then orientation. A location property is added to A
with likelihood ?loc multiplied by the length penalty
?= 1(??1) if A=?color:grey?; ?=
1
(??2) if A=?color:grey,
size:large?, etc.; and an orientation property is added
to A with likelihood ?ori multiplied by the length
penalty ?= 1(??1) if the property set is ?color:grey?,
etc. At this point, the likelihood of adding further
properties quickly diminishes.
Once all properties have been analyzed, the algo-
rithm scans through the objects in the scene. For
each object obj2. . . objn, if the object is the same
type as the target object obj1, then any different
property of the target referent is added to A with
a likelihood based on the length penalty alone ?.
?type:desk? is added at the end.
For this example scene, the algorithm will gen-
erate the property sets ?color:grey, type:desk?,
?color:grey, size:large, type:desk?, ?size:large,
type:desk?, ?color:grey, ori:front, type:desk?,
?color:grey, loc:(3, 1), type:desk?, etc., with dif-
ferent frequencies. Due to the length penalty,
generated property sets will almost never have more
than 3 properties.
tg color:yellow size:(63,63) type:ball loc:right-hand
lm color:red size:(345,345) type:cube loc:right-hand
obj3 color:yellow size:(70,70) type:cube loc:left-hand
Figure 5: Example input scene: GRE3D3 corpus. For IA
And GB, gold-standard size values are provided rather
than measurements (small, large).
obj1 colour:grey size:(454,454) type:desk loc:(3,1) ori:front
obj2 colour:blue size:(454,454) type:desk loc:(2,1) ori:front
obj3 colour:red size:(454,454) type:desk loc:(3,2) ori:back
obj4 colour:green size:(254,254) type:desk loc:(4,1) ori:left
obj5 colour:blue size:(454,454) type:fan loc:(1,1) ori:front
obj6 colour:red size:(454,454) type:fan loc:(5,1) ori:back
obj7 colour:green size:(254,254) type:fan loc:(2,2) ori:left
Figure 6: Example input scene: TUNA corpus. For IA
And GB, gold-standard size values are provided rather
than measurements (small, large).
As such, although ?color:grey, type:desk? would
sufficiently distinguish the intended referent, we
instead produce a variety of sets, overspecify-
ing in some instances (e.g., ?color:grey, ori:front,
type:desk?), and with a small chance of underspec-
ifying in others (e.g., ?size:large, type:desk?).
5 Evaluation Algorithms & Corpora
5.1 Corpora
We evaluate on two well-known REG corpora, the
GRE3D3 corpus (Viethen and Dale, 2008) and the
singular furniture section of the TUNA corpus (van
Deemter et al, 2006). Both corpora contain expres-
sions elicited to computer-generated objects, and so
provide a reasonable starting point for evaluating
reference to visible objects. For all algorithms, we
evaluate on the selection of referent attributes. Lex-
ical choice and word order are not taken into ac-
count. Example images from GRE3D3 and TUNA
are shown in Figure 4, and example algorithm input
1179
from these corpora are shown in Figures 5 and 6.
In GRE3D3, we evaluate on the selection of type,
color, size, and location, but leave aside proper-
ties of relatum objects, which are not currently ad-
dressed by this algorithm or the IA. In TUNA, we
evaluate on the selection of type, color, size and
orientation.5
5.2 Algorithms
5.2.1 The Incremental Algorithm
The Incremental Algorithm requires a preference
order list (PO) specifying the order to iterate through
scene attributes. We determine the preference or-
der from corpus frequencies using cross-validation
to hold out a test scene and list attributes from the
training scenes in descending order. We find that
color precedes size in the preference orders, in line
with recent research showing that this allows the al-
gorithm to perform optimally on the TUNA corpus
(van Deemter et al, 2012a). In development, we find
that IA performs best with type as the last attribute
in the PO, and report on numbers with this approach.
5.2.2 The Graph-Based Algorithm
The version of the Graph-Based Algorithm that
we use is available from Viethen et al (2008). This
algorithm requires (1) a set of cost functions for each
edge, and (2) a PO for deciding between properties
in the case of a tie. For (1), we use the method from
Theune et al (2011) to assign two costs (0, 1) to
the edges. We first determine the relative frequency
with which each property is mentioned for a target
object, and then create costs for each property using
k-means clustering (k=2) in the Weka toolkit (Hall
et al, 2009). We refer interested readers to the The-
une et al paper for further details. For (2), we follow
the same method as for the Incremental Algorithm.
5.2.3 The Visual Objects Algorithm
The proposed algorithm requires ?att, which we
estimate as the relative frequency of each attribute
att in the training data. The ordered attribute lists for
the algorithm (AP, RP and P) are built in the same
way as the preference order list for the IA and GB,
listing attributes from the training data in order of
5We remove location from evaluation in this corpus. Lo-
cation is not annotated directly, but split such that only x-
dimension or y-dimension may be marked for a reference.
descending frequency. For these corpora, there are
not absolute properties beyond color, so AP is empty.
6 Evaluation
Previous evaluation of REG algorithms have used
measurements such as Uniqueness, Minimality,
Dice (Belz and Gatt, 2008), and Accuracy (Gatt et
al., 2009; Reiter and Belz, 2009). Uniqueness is
the proportion of outputs that identify the referent
uniquely, and Minimality is the proportion of out-
puts that are both minimal and unique. As our goal
is to mimic human reference, these metrics are not
as useful for the evaluations as the others.
The Dice metric provides a value for the similar-
ity between a generated description and a human-
produced description, and therefore serves as a rea-
sonable objective measure for how human-like the
produced sets are. Given the generated property set
(DS) and the human-produced property set (DH ),
Dice is calculated as:
2? |DS ?DH |
|DS |+ |DH |
For each input domain, we evaluate over boolean
values (included or excluded) for the attributes D
(see Table 1). Note that this means the specific val-
ues for the attributes are not compared. In this for-
mulation based on boolean values, |DS |=|DH |=|D|
and Dice reduces to:
|DS ?DH |
|D|
Calculating Dice over the same number of at-
tributes for both the observed and generated data
has the nice mathematical property of making Dice
equal to other common metrics for evaluating a
model, including Accuracy, Precision, and Recall.6
Since the proposed algorithm is stochastic, this in-
troduces a problem in using a metric that compares
single expressions. We therefore seek to find the
best alignment between the set of expressions pro-
duced by the algorithm and the set of expressions
produced by people. We formulate this alignment as
an assignment problem weighted by Dice. For the
corpus of observed property sets H and the corpus
of generated property sets S, we find the best align-
6A false positive is a false negative, and there are no true
negatives, so all four metrics are equivalent.
1180
Example Corresponding Evaluated
Expression Property Set Property Set
the red ball ?color:red, type:ball? type:1 color:1
size:0 loc:0
Table 1: Example human expression and corresponding
boolean-valued property set for evaluation in GRE3D3,
with D={type, color, size, and location}.
ment x out of all possible alignmentsX between the
corpora:
argmaxx?X
?
(S,H)?x
Dice(DS , DH )
This may be solved in polynomial time using the
Hungarian method (Kuhn, 1955; Munkres, 1957).
Note that because IA and GB are deterministic, find-
ing an optimal alignment is trivial. We call this
method ALIGNED DICE.
It is an open question whether an alignment-based
evaluation is fair: the proposed algorithm has more
than one chance to match the human descriptions.
In the second evaluation method (MAJORITY) we
address this issue, comparing how often the most
frequent generated set compares with the most fre-
quent observed set. We run the proposed algorithm
1,000 times, and the generated property sets are or-
dered by frequency. The most frequent generated
set is compared against the most frequent human-
produced set. The majority score is the percentage
of folds where these two sets match. For IA and FB,
the most frequent generated set is the only gener-
ated set. This is a simple way to fairly compare the
output of deterministic and non-deterministic algo-
rithms. There are no ties in the generated sets, but
in the case of a tie in the observed data, we count a
match if any match the most frequent generated set.
6.1 GRE3D3
We randomly select two scenes (7, 9) from Set 1
and their mirrored counterparts in Set 2 (17, 19) for
development. We empirically determine ?=5 for the
length-based penalty ? in the proposed algorithm.
We use the eight remaining scenes in each Set
for eight-fold cross-validation, estimating parame-
ters for the algorithms on the seven training scenes
in each fold, as discussed in Section 5.2.
For ALIGNED DICE, we run the proposed algo-
rithm five times in each fold and report the average
Algorithm
ALIGNED DICE MAJORITY
Set 1 Set 2 Set 1 Set 2
Proposed Alg. 88.23 90.06 62.50 50.00
IA 87.71 85.13 62.50 25.00
GB 87.71 88.73 62.50 50.00
Table 2: GRE3D3: Results (in %).
Algorithm
ALIGNED DICE MAJORITY
+LOC -LOC +LOC -LOC
Proposed Alg. 88.75 86.07 40.00 40.00
IA 81.79 81.55 0.00 100.00
GB 75.36 66.04 20.00 20.00
Table 3: TUNA: Results (in %).
score. Results are shown in Table 2.7
The proposed Visible Objects Algorithm achieves
higher accuracy than either version of the Incremen-
tal Algorithm or the Graph-Based Algorithm using
ALIGNED DICE. In MAJORITY, the Graph-Based
and the Visible Objects Algorithm both predict the
majority property set in this evaluation at least 50%
of the time. The algorithm is competitive with the
state of the art on this corpus.
6.2 TUNA
TUNA is split into two conditions: subjects discour-
aged to use location (-LOC) or not (+LOC). We ran-
domly hold out two scenes from both conditions (1
and 2), and find a value of ?=5 again works well on
the development data.
As in the GRE3D3 corpus, we use the TUNA
scenes in five-fold cross-validation, estimating pa-
rameters on the four training scenes in each fold. For
ALIGNED DICE, we average over five runs of the al-
gorithm, and for MAJORITY, we run the proposed
algorithm 1,000 times for each test scene.
Results are shown in Table 3. Again we see that
the proposed Visible Objects Algorithm is compet-
itive with the IA and GB for both ALIGNED DICE
and MAJORITY. GB performs poorly here, and this
may be due to the data sparsity issue that arises when
requiring the algorithm to train on properties.8 In
7We do not report statistical significance; the proposed algo-
rithm produces several possible outputs for one input, while the
IA and GB produce only one.
8The original property-based weighting approach (Theune
et al, 2011; Koolen et al, 2012, see Section 5.2) trained on ob-
ject collections that were identical to their test data in all proper-
ties except x- and y-dimension, and so this was less of an issue.
We hope to explore whether basing weights on attributes alone
1181
MAJORITY, the Visible Objects Algorithm is rela-
tively stable across conditions, generating the ma-
jority property set in 40% of the test scenes. It does
not outperform the IA in the -LOC condition, but the
IA has a large range across the two conditions (0%
and 100%).
7 Conclusions and Future Work
We have introduced a new algorithm for generating
referring expressions, inspired by human and com-
puter vision and aiming to refer in a human-like way
to visible objects. The algorithm successfully gener-
ates the most common attributes that people choose
for different objects, and offers a varied output to
capture speaker variation. In contrast to most algo-
rithms for the generation of referring expressions,
which have aimed to produce distinguishing descrip-
tions when these exist (Krahmer and van Deemter,
2012), the core idea behind this algorithm is to gen-
erate what is likely for a speaker in a visual domain.
Since the driving mechanism behind the algorithm
is not to uniquely identify the object, but rather to
pipeline the analysis of properties in a way similar
to human visual processing, the generated expres-
sion may be overspecified or underspecified.
We are limited by available REG corpora to re-
liably assess methods for generating more com-
plex absolute properties like shape and material, but
adding such properties would help advance the gen-
eration of human-like reference in visual scenes and
offers further points of connection between the gen-
eration process and computer vision property detec-
tion. Models for generating more complex spatial
relations are currently available, and are a natural
extension to this framework (e.g., those of Kelleher
and Costello (2009)) as object detection becomes
more robust.
We may also be able to build more sophisticated
graphical models as larger corpora become avail-
able. For example, modeling the conditional proba-
bility of generating reference for a property vn given
the previously generated context p(vn|v1 . . . vn?1)
may bring us closer to human-like output.
There are several additional issues that do not
arise in this evaluation, but we expect must be ac-
counted for when referring to naturalistic objects in
improves performance.
visual domains. These include:
? The interconnected nature of properties, where
some properties entail others; for example, a
wooden object is likely to be called wooden, re-
ferring to its material, rather than tan or brown.
? The role of typicality, where properties are se-
lected because they are atypical for the object.
? Referring to more complex properties, e.g., ma-
terial, texture, etc., and object parts.
? Better methods for determining the length
penalty and attribute likelihoods.
We hope to discuss extensions to this algorithm
covering these aspects of reference in future work.
Acknowledgments
Funding for this research has been provided by
SICSA and ORSAS. We thank the anonymous re-
viewers for useful comments on this paper.
References
Carlos Areces, Alexander Koller, and Kristina Striegnitz.
2008. Referring expressions as formulas of descrip-
tion logic. Proceedings of the 5th International Nat-
ural Language Generation Conference (INLG 2008),
pages 42?29.
Ellen Gurman Bard, Robin Hill, Manabu Arai, and
Mary Ellen Foster. 2009. Accessibility and attention
in situated dialogue: Roles and regulations. Proceed-
ings of the Workshop on the Production of Referring
Expressions (PRE-CogSci 2009).
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrin-
sic evaluation measures for referring expression gen-
eration. Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL
2008), pages 197?200.
Alexander C. Berg, Tamara L. Berg, Hal Daume? III,
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, and Kota Yam-
aguchi. 2011. An exploration of how to learn from
visually descriptive text. JHU-CLSP Summer Work-
shop Whitepaper.
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia Inc., Sebastopol, CA.
Sarah Brown-Schmidt and Michael K. Tanenhaus. 2006.
Watching the eyes when talking about size: An investi-
gation of message formulation and utterance planning.
Journal of Memory and Language, 54:592?609.
1182
Herbert H. Clark and Gregory L. Murphy. 1982. Audi-
ence Design in Meaning and Reference. In J. F. LeNy
and W. Kintsch, editors, Language and Comprehen-
sion, volume 9 of Advances in Psychology, pages 287?
299. North-Holland, Amsterdam.
Herbert H. Clark, Robert Schreuder, and Samuel But-
trick. 1983. Common ground and the understanding
of demonstrative reference. Journal of Verbal Learn-
ing and Verbal Behavior, 22:245?258.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the gricean maxims in the generation
of referring expressions. Cognitive Science, 19:233?
263.
Robert Dale. 1989. Cooking up referring expressions.
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 1989).
P. E. Engelhardt, K. Bailey, and F. Ferreira. 2006. Do
speakers and listeners observe the gricean maxim of
quantity? Journal of Memory and Language, 54:554?
573.
Fang Fang, Huseyin Boyaci, Daniel Kersten, and Scott O.
Murray. 2008. Attention-dependent representation
of a size illusion in human V1. Current biology,
18(21):1707?1712.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR 2009).
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. Advances in Neural Information Processing
Systems (NIPS 2007).
William Ford and David Olson. 1975. The elabora-
tion of the noun phrase in children?s description of ob-
jects. The Journal of Experimental Child Psychology,
19:371?382.
David A. Forsyth. 2011. Personal communica-
tion. Video clip of communication available from:
http://vimeo.com/40553150. At 1:06:46.
T. M. Gann and D. J. Barr. 2013. Speaking from expe-
rience: Audience design as expert performance. Lan-
guage and Cognitive Processes. In press.
Albert Gatt and Anja Belz. 2008. Attribute selec-
tion for referring expression generation: New algo-
rithms and evaluation methods. Proceedings of 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 50?58.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA
REG challenge 2009: Overview and evaluation results.
Proceedings of the 12th European Workshop on Natu-
ral Language Generation (ENLG 2009), pages 174?
182.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
William S. Horton and Boaz Keysar. 1996. When do
speakers take into account common ground? Cogni-
tion, 59(1):91?117.
Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature Reviews Neuro-
science, 2:194?203.
John Kelleher and Fintan Costello. 2009. Applying
computational models of spatial prepositions to vi-
sually situated dialog. Computational Linguistics,
35(2):271?306.
John Kelleher, Fintan Costello, and Josef van Genabith.
2005. Dynamically structuring, updating and interre-
lating representations of visual and linguistic discourse
context. Artificial Intelligence, 167:62?102.
Boaz Keysar and Anne S. Henly. 2002. Speakers? over-
estimation of their effectiveness. Psychological Sci-
ence, 13(3):207?212.
Ruud Koolen, Martijn Goudbeek, and Emiel Krahmer.
2011. Effects of scene variation on referential over-
specification. Proceedings of the 33rd Annual Meeting
of the Cognitive Science Society (CogSci 2011).
Ruud Koolen, Emiel Krahmer, and Marie?t Theune. 2012.
Learning preferences for referring expression genera-
tion: Effects of domain, language and algorithm. Pro-
ceedings of the 7th International Workshop on Natural
Language Generation (INLG 2012).
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expressions.
Information Sharing: Reference and Presupposition
in Language Generation and Interpretation, 143:223?
263.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38:173?218.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly,
2:83?97.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
Proceedings of the 13th European Workshop on Natu-
ral Language Generation (ENLG 2011).
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of Industrial and
Applied Mathematics, 5(1):32?38.
Scott O. Murray, Huseyin Boyaci, and Daniel Kersten.
2006. The representation of perceived angular size in
human primary visual cortex. Nature Neuroscience,
9(3):429?434.
1183
David R. Olson. 1970. Language and thought: Aspects
of a cognitive theory of semantics. Psychological Re-
view, 77:257?273.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:89?110.
Ehud Reiter and Anja Belz. 2009. An investigation into
the validity of some metrics for automatically evalu-
ating natural language generation systems. Computa-
tional Linguistics, 35(4):529?558.
Ehud Reiter and Robert Dale. 1992. A fast algorithm
for the generation of referring expressions. Proceed-
ings of the 14th International Conference on Compu-
tational Linguistics (COLING 1992), 1:232?238.
Ehud Reiter and Somayajulu Sripada. 2002. Human
variation and lexical choice. Computational Linguis-
tics, 28:545?553.
D. Samuel Schwarzkopf, Chen Song, and Geraint Rees.
2010. The surface area of human V1 predicts the
subjective experience of object size. Nature Neuro-
science, 14(1):28?30.
J. R. Searle. 1969. Speech Acts: An Essay in the Philos-
ophy of Language. Cambridge University Press, Cam-
bridge.
Susan Sonnenschein. 1985. The development of referen-
tial communication skills: Some situations in which
speakers give redundant messages. Journal of Psy-
cholinguistic Research, 14:489?508.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter ? how much
data is required to train a REG algorithm? Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL 2011).
Anne M. Treisman and Garry Gelade. 1980. A feature
integration theory of attention. Cognitive Psychology,
12:97?13.
Anne Treisman. 1985. Preattentive processing in vi-
sion. Computer Vision, Graphics, and Image Process-
ing, 31:156177.
L. G. Ungerleider and M. Mishkin. 1982. Two Corti-
cal Visual Systems. In D. J. Ingle, M. Goodale, and
R. J. W. Mansfield, editors, Analysis of Visual Be-
haviour, chapter 18, pages 549?586. The MIT Press.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. Proceedings
of the 4th International Conference on Natural Lan-
guage Generation (INLG 2006).
Kees van Deemter, Albert Gatt, Ielka van der Sluis, and
Richard Power. 2012a. Generation of referring ex-
pressions: Assessing the incremental algorithm. Cog-
nitive Science, 36(5):799?836.
Kees van Deemter, Emiel Krahmer, Roger van Gompel,
and Albert Gatt. 2012b. Towards a computational psy-
cholinguistics of reference production. TopiCS: Pro-
duction of Referring Expressions - Bridging the Gap
between Computational and Empirical Approaches to
Reference.
Roger P. G. van Gompel, Albert Gatt, Emiel Krahmer,
and Kees van Deemter. 2012. PRO: A computational
model of referential overspecification. Architectures
and Mechanisms for Language Processing (AMLaP
2012).
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 59?67.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. Proceedings of the 8th Australasian Lan-
guage Technology Workshop (ALTW 2010), pages 81?
89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t The-
une, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008).
Jette Viethen, Martijn Goudbeek, and Emiel Krahmer.
2012. The impact of colour difference and coloure
codability on reference production. Proceedings of the
34th Annual Meeting of the Cognitive Science Society
(CogSci 2012).
G. J. Whitehurst. 1976. The development of communi-
cation: Changes with age and modeling. Child Devel-
opment, 47:473?482.
1184
Using Spatial Reference Frames to Generate Grounded Textual
Summaries of Georeferenced Data
Ross Turner, Somayajulu Sripada and Ehud Reiter
Dept of Computing Science,
University of Aberdeen, UK
{rturner,ssripada,ereiter}@csd.abdn.ac.uk
Ian P Davy
Aerospace and Marine Intl.,
Banchory, Aberdeenshire, UK
idavy@weather3000.com
Abstract
Summarising georeferenced (can be iden-
tified according to it?s location) data in
natural language is challenging because it
requires linking events describing its non-
geographic attributes to their underlying
geography. This mapping is not straightfor-
ward as often the only explicit geographic
information such data contains is latitude
and longitude. In this paper we present an
approach to generating textual summaries
of georeferenced data based on spatial ref-
erence frames. This approach has been im-
plemented in a data-to-text system we have
deployed in the weather forecasting domain.
1 Introduction
Data-to-text systems are NLG systems that gener-
ate texts from raw input data. Many examples of
such systems have been reported in the literature,
which have been applied in a number of domains and
to different types of input. For example, BabyTalk
(Portet et al, 2007) generates medical reports from
sensors monitoring a baby in a Neonatal Intensive
Care Unit, while (Hallett and Scott, 2005) describe a
system for generating reports from events in medical
records. SumTime (Reiter et al, 2005), (Coch, 1998)
and Fog (Goldberg et al, 1994) generate weather
forecasts from the output of weather computer sim-
ulation models, while (Iordanskaja et al, 1992) and
(Ro?sner, 1987) both generate summaries from em-
ployment statistics.
As the above examples show most work in data-to-
text up to now has concentrated almost exclusively
on time series data. Work on generating text from
spatial data has been reported in Coral (Dale et al,
2005), which generates route descriptions of a path
constructed from Geographical Information Systems
(GIS) datasets. Unlike the input to Coral however,
most georeferenced data contains only limited spatial
information(in many cases, only latitude and longi-
tude).
As (Roy and Reiter, 2005) point out, connecting
language to the non-linguistic world is an important
issue in Cognitive Science and Aritificial Intelligence;
moreover, geographic data is becoming increasingly
ubiquitous as the availability of low cost locational
devices such as GPS increases, and GIS become more
user friendly. Therefore, we believe exploring the
issue of generating textual reports grounded in real
world geographical data is an important challenge.
On a more practical level, it is also a natural next
step in the application of data-to-text technology to
apply it to geographically referenced data.
In the RoadSafe project described in the following
section, we have been investigating this issue in a
data-to-text system that generates road ice weather
forecasts. The subsequent focus of this paper is the
adaption of NLG techniques to the task of summaris-
ing georeferenced data. In particular, the incorpora-
tion of spatial reference frames to generate grounded
(from external GIS data sources) spatial references.
2 Background
Weather forecasting has been one of the most suc-
cessful and widely researched application domains for
NLG systems. The main novel aspect that sets Road-
Safe apart from other weather forecast generators
and indeed, other data-to-text systems, is it?s appli-
cation to spatio-temporal data. The input to Road-
Safe is generated by a road ice simulation model,
which outputs a large (in order of Megabytes) mul-
tivariate data set, shown in Figure 1.
The output of the model contains predicted mea-
surements of 9 meteorological parameters for 1000?s
of points across a road network, each measured at
20 minute intervals during a 24 hour forecast pe-
riod. A map of such a network, belonging to a lo-
cal council in the UK, is shown in Figure 2. This
model forms the basis of a road ice forecasting ser-
16
Figure 1: Part of a RoadSafe input data set show-
ing corresponding spatial and non-spatial attribute ta-
bles; T=Air Temperature (Deg C), W=Dew Point (Deg
C), R=Road Surface Temperature (Deg C), C=Weather
Code, D=Wind direction (Degrees), V=Mean wind
Speed (knots), G=Wind Gust (knots),S=Sky Cover (%),
P=Precipitation Water Equivalent (mm).
vice provided by Aerospace and Marine International
(AMI), which is delivered to local councils via an
online Road Weather Information System (RWIS).
This service provides road engineers with up to the
minute weather information using graphs, graphics
and textual reports that allows them to base their
road maintenance operations on during the winter
months. In RoadSafe we have been working on gen-
erating the textual reports, such as the one shown in
Figure 3, automatically from the model data.
The communicative goal of the textual reports is
to complement detailed tabular and graphical pre-
sentations of the model data with a more general
overview of the weather conditions. In the context
of our work this presents a number of challenges:
1. The input data has to be analysed, this is non-
Figure 2: Road Ice Model Data Points Map
trivial due to the complexity and size of the in-
put data.
2. Our system is required to achieve a huge
data/text compression ratio (Human authored
texts are short and concise summaries). There-
fore, content selection is a serious issue for our
system.
3. Describing the effect of the underlying geogra-
phy on weather conditions, such as ?possible gale
force gusts on higher ground?, is an integral part
of the communicative goal of the text. Infor-
mation containing such relationships is not ex-
plicit in the input data and therefore must be
grounded.
?Another night with all routes quickly falling be-
low zero this evening. Only isolated urban spots in
the south will only drop to around zero. Freezing
fog patches will become more widespread during the
night but thin a little tomorrow morning especially
in the south.?
Figure 3: Example Human Authored Corpus Text
3 Architecture
As noted in the previous section, the input data to
our system contains only limited spatial information:
a point identifier that ties the measurement site to
a particular route and a latitude longitude coordi-
nate. Therefore it is necessary for our system to per-
form additional spatial reasoning to characterise the
input in terms of its underlying geography. The ar-
chitecture of our system shown in Figure 4, extends
17
Figure 4: RoadSafe System Architecture
the architecture for data-to-text systems proposed
in (Reiter, 2007) to include this additional process-
ing. In Section 3.1 we explain some of the rationale
behind these design decisions based on observations
from our knowledge acquisition(KA) Studies. In Sec-
tions 3.2 and 3.3 we explain the additional modules
we have introduced in more detail.
3.1 Observations from Knowledge
Acquisition Studies
We have been working closely with experts at AMI
for a number of winters now in the development of
RoadSafe. During this time we have found that two
interrelated aspects in particular have influenced the
architecture of our system, which we describe next.
Spatial Reference Frames Frames of reference
in this context are a particular perspective in which
the domain can be observed. More precisely, they are
sets of related geographical features (such as elevated
areas) which partition the domain into meaningful
sub areas for descriptive purposes. In Levinson?s ter-
minology (Levinson, 2003), they are absolute refer-
ence systems as they employ fixed bearings. In the
RoadSafe domain we have identified 4 main spatial
frames of reference used by experts in our corpus de-
scribed in (Turner et al, 2008):
1. Altitude e.g. ?rain turning to snow on higher
ground?.
2. Absolute Direction e.g. ?some heavier bursts in
the north?.
3. Coastal Proximity e.g. ?strong winds along the
coast?.
4. Population e.g. e.g. ?Roads through the Urban
areas holding just above freezing?.
Communicative Purpose of Spatial Descrip-
tions From our studies we have found that experts
generally follow 4 steps when writing road ice fore-
casts:
1. Build frames of reference to geographical fea-
tures that may affect general weather condi-
tions.
2. Build an overview of the general weather pat-
tern.
3. Select important features to communicate from
the pattern.
4. Communicate the summary.
Building frames of reference to geographical fea-
tures is important for a human forecaster to be able
to take into account how the geography of the region
influences the general weather conditions. Under-
standing the weathers interaction with the terrain
enables them to make reliable meteorological infer-
ences. For example a spatial description such as ?rain
turning to snow in rural areas? may be geographically
accurate, but does not make sense meteorologically
as it is purely by chance that this event is occurring
at that location.
18
From a NLG system perspective it is important to
take into account the communicative purpose of spa-
tial descriptions in this context, which are express-
ing causality (the effect of geographical features on
weather conditions) rather than being purely loca-
tive. For example, changes in precipitation type are
more commonly seen in higher elevation areas where
the air temperature is generally lower, so a spatial de-
scription describing such an event should make use of
a reference frame that reflects this interaction. Simi-
larly, road surface temperatures are generally higher
in urban areas where there is a general population
effect. For a referring expression generation (REG)
strategy this means that this requires not only ade-
quate spatial representation and reasoning capabili-
ties about an objects location, but also additional in-
formation about an objects function in space. This is
a problem which has been acknowledged in the psy-
cholinguistic literature e.g. (Coventry and Garrod,
2004).
3.2 Geographic Characterisation
Geographic Characterisation is responsible for
grounding the location of the data by making the
relationship between it?s underlying geography ex-
plicit. As the first stage of data analysis it assigns
additional spatial properties to each data point by in-
tersecting the point with external GIS data sources
representing the frames of reference we have iden-
tified. For example after characterisation, the first
point in the spatial attribute table shown in Figure
1 is assigned values [0m,SSW,Urban,Coastal] to rep-
resent elevation, absolute compass direction, popula-
tion density of its immediate area and its proximity
to the coast respectively. This process is more com-
monly known as a form of data enrichment in the
Spatial Data Mining community (Miller and Han,
2001). In the scope of our work it is important for
two reasons: most importantly, it provides a set of
properties that are used by the REG module to gen-
erate spatial descriptions; secondly, these properties
can be taken into account by our analysis method
during the initial segmentation of the data.
3.3 Spatial Reasoner and Spatial Database
The spatial database provides a repository of geo-
graphic information. Frames of reference are stored
as thematic layers from various GIS data sources con-
sisting of sets of boundary objects. For example, al-
titude is represented as sets of polygons representing
altitude contours at a given resolution and popula-
tion is a set of town boundary polygons. The spatial
reasoning module provides a high level interface be-
tween the spatial database and the rest of the system.
It is responsible for performing geographic character-
isation and providing spatial query functionality to
the rest of the system.
4 Text Generation
In Section 2 we outlined 3 main challenges that our
system must address. Our approach to the first,
analysis of the input data, is described in (Turner
et al, 2007). In the following Sections 4.1 and 4.2,
we describe the approach taken by our text generator
to the former two: content selection and generating
spatial references.
4.1 Content Selection
The input to the document planning module of our
system is a series of meteorological events (such as
rises in temperature) describing each parameter over
specific periods of time and locations. The basic
events are generated by data analysis which are then
abstracted into higher level concepts by data inter-
pretation. As it is impossible to include all these
events in such a short summary our system also gen-
erates a table as well as text shown in Figure 5.
In our KA studies we have found experts use
a qualitative overview of weather conditions when
writing forecasts to perform this task, confirming
similar observations reported in (Sripada et al,
2001). We take the same approach as experts in
our system by including the internal information
of the table (generated by the data analysis mod-
ule) as input to document planning. This serves as
the overview for content selection and allows con-
struction of an initial document plan consisting of
overview event leaf nodes. An example of this struc-
ture for the system output shown in Figure 5 is given
in Figure 6. Each overview event corresponds to a
column (or columns in the case of snow and rain) in
the table if the column indicates a significant thresh-
old for the parameter it describes (i.e. yes for ice).
Figure 6: Overview event tree for the text output in Fig-
ure 5
19
Figure 5: Example system output with text and partial table
The next stage is to construct messages from the
leaf nodes of the document plan. This is done in a
top down fashion by further annotating the tree with
events from the input list. Additional events are se-
lected by using the information from the overview
events to retrieve them from the list. This has the
benefit of keeping the content of both text and ta-
ble consistent. The final tree comprises the input
to the microplanner where messages are realised as
sentences in the final text and typically contain two
events per message (as observed in our corpus). For
example the overview event describing Precip in Fig-
ure 6 is realised as two sentences in Figure 5: Win-
try precipitation will affect most routes throughout
the forecast period at first [overview event], falling
as snow flurries in some places above 300M at first
[event]. Snow spreading throughout the forecast pe-
riod to all areas [event] and persisting in some places
above 300M until end of period [event].
4.2 Generating Spatial References to
Geographic Areas
Approaches to REG to date have concentrated
on distinguishing descriptions (e.g. (Gatt and
van Deemter, 2007),(van Deemter, 2006),(Horacek,
2006),(Krahmer et al, 2003),(Dale and Reiter,
1995); more specifically that is given a domain, they
look to generate a description of a target object that
uniquely distinguishes it from all other objects within
that domain. In a large geographic environment
such as a road network consisting of 1000?s of points,
where the task is to refer to an event occurring at a
small subset of those points, it is impractical (gen-
erated descriptions may be long and complex) and
prohibitively expensive (large numbers of spatial re-
lations between objects may have to be computed) to
take this approach. A more practical approach is to
generate spatial descriptions in terms of regions that
are not strictly distinguishing (i.e. urban areas, high
ground) rather than in terms of the points contained
within that region. Indeed, this is the strategy em-
ployed by human authors in our corpus. Therefore,
in a description such as ?road surface temperatures
will fall below zero in some places in the south west?,
distractors can be defined as the set of points within
the south western boundary that do not satisfy this
premise.
The relaxation of the requirement to generate a
distinguishing description simplifies the REG task in
this context as a single referring expression may be
deemed acceptable to refer to a wide range of situa-
tions. For example, ?in some places in the south west?
could be used to refer to a large number of possible
subsets of points that fall within the south western
boundary of the network. A simple REG strategy
is to find the set of properties to use in a descrip-
tion that introduce the least number of distractors.
However, as mentioned previously in Section 3.1, an
added constraint is that a spatial description should
use an appropriate frame of reference in the context
of the event it is describing. For example, describing
a change in precipitation type using population as
20
a frame of reference (i.e ?rain turning snow in some
rural places?) is not a sound meteorological inference
because population density does not affect precipi-
tation. This could cause a reader to infer false im-
plicatures (Grice, 1975), and consequently lead to
unnecessary treatment of part of the road network
so should be avoided. To account for this, following
(Dale and Reiter, 1995) we include a preference set
of reference frames for each type of event that must
be described. Absence from the set signifies that the
specified frame of reference should not be used in
that context.
Recall from Section 3.2 that properties in this case
relate directly to sets of boundary objects within a
frame of reference. Our content selection module
takes as input a series of individual proportions de-
scribing the spatial distribution of each parameter
within each frame of reference at a particular time
point. A score is calculated for each set of properties
by averaging over the sum of proportions for each
frame of reference. An appropriate frame of refer-
ence is then selected by choosing the one with the
highest score from the preference set for the given
event. An example1 of the input for the generated
description ?falling as snow flurries in some places
above 300M at first? in Figure 5 is shown in Figure
7.
5 Evaluation
The system presented in this paper is in its first
incarnation, RoadSafe is still actively under devel-
opment in preparation for a full scale user evalua-
tion. We have been evaluating the quality of the
output of the current system using post edit tech-
niques and feedback from expert meteorologists at
AMI. Our prototype has been installed at AMI since
the start of the year and is being used to generate
draft road ice forecasts for one of their local council
clients. One forecast is generated per day which is
then post-edited by an on duty forecaster before it is
sent to the client. While common in Machine Trans-
lation post-edit evaluations are still relatively rare in
NLG. The only large scale post-edit evaluation of an
NLG system to our knowledge has been reported in
(Sripada et al, 2005).
Our current evaluation is small in comparison to
that evaluation; SumTime-Mousam, the system be-
ing evaluated in that work was generating 150 draft
forecasts per day. However, it does try to address
some of the problems the authors encountered during
that evaluation. The main issue outlined by (Sripada
1N.B. this example is taken from route network that is
land locked and therefore coastal proximity is not taken into
account in this case.
Parameter: Snow
Class: Flurries
Time point: 12:00 {
Reference Frame Boundary Proportion
Altitude
0m: 0.0
100m: 0.0
200m: 0.0
300m: 0.07
400m: 1.0
500m: 1.0
Direction
CentralNE: 0.0
CentralNW: 0.0
CentralSE: 0.0
CentralSW: 0.0
EastNorthEast: 0.0
EastSouthEast: 0.0
SouthSouthEast: 0.0
SouthSouthWest: 0.18
TrueNorthEast: 0.0
TrueSouthEast: 0.0
TrueSouthWest: 0.56
WestSouthWest: 0.23
Population
Rural: 0.02
Urban: 0.0
}
Figure 7: Example input to content selection for REG.
Proportions are number of points affected by snow within
given boundary at the specified time point. Scores by
Reference Frame: Altitude = 0.35, Direction = 0.07, Pop-
ulation = 0.01
et al, 2005) was that their analysis was post-hoc and
therefore not supported by authors or by an editing
tool, which made it difficult to analyse why post-edits
were made. We have accounted for this by including
post-editing as part of our development process and
making use of a simple online interface that allows
the editor to select check boxes as they edit and in-
sert any general comments they may have. Check
boxes record edit reasons at a high level, for exam-
ple content, sentence order, spatial description used
etc. This is because it is not reasonable to expect a
time-constrained forecaster to spend time recording
every edit he makes.
Another important lesson pointed out by (Sripada
et al, 2005) is the need for a pilot study to analyse
the post-edit behaviour of individual authors to ac-
count for noisy data. This is certainly worthwhile,
but is difficult to carry out in our domain where fore-
casters work in variable shift patterns and on vari-
able forecasting tasks at different times. Instead, we
21
have used feedback forms as a way to gain qualitative
data on both the general quality of the texts and the
post-editing process. We present our results in Sec-
tion 5.1. In Section 5.2 we provide some discussion
of the results and describe future work.
5.1 Results
Our post-edit corpus currently consists of 112 texts,
2 texts(1 generated,1 edited) for 56 forecast days.
Of the 56 generated texts 54 have been edited before
being released to the user. As a general evaluation
criterion, our generated texts are generally too long
with a mean word length of 72 (standard deviation
of 21) compared to a mean word length of 53 (stan-
dard deviation of 17). The mean word count differ-
ence per forecast is 21 (standard deviation of 15). In
general analysis of the corpus is difficult, as in some
cases (18) texts have been basically rewritten. This
is not reflecting the quality of the text as such, but
the fact that the author has access to other informa-
tion sources such as satellite maps, which can lead
him to draw different inferences to those in the raw
model data available to the system. Furthermore,
(Hopwood, 2004) acknowledge as ice prediction mod-
els have become increasingly advanced, the primary
added value provided by weather forecasters is to
function as quality control and error mitigation for
the model, using their insight and experience to make
amendments particularly on marginal nights (where
the road surface temperature may or may not fall be-
low zero). Such cases can only be considered as noise
for analysis purposes, and the fact that our system
cannot account for this without the additional infor-
mation has been acknowledged by all forecasters in
their editing comments and feedback forms.
Focusing on 74 real post-edits (not attributed to
model data) recorded in our corpus, they can be clas-
sified into the following broad error categories: con-
tent edits - 65% and microplanning edits 35%. One
major problem we have identified with the current
generated text is the way in which overview events
described in 4.1 are realised. Deletions of whole sen-
tences describing overview events such as the one
highlighted in bold in Figure 8 constitute over half
(52%) of content edits, which may help to explain
the large descrepency in word counts. Essentially
forecasters believe they can often communicate sim-
ilar information as subsequent statements about the
same parameter making the texts repetitive at times.
Therefore they suggest they should either be omit-
ted or be realised as more interpretative statements,
such as ?A marginal night for most routes? for the
omitted statement in Figure 8. Forecasters also of-
ten delete subsequent statements following overview
Generated Text:
?Road surface temperatures will reach
near critical levels on some routes from the
late evening until tomorrow morning. Rain
will affect all routes during the afternoon and
evening. Road surface temperatures will fall slowly
during the mid afternoon and evening, reaching
near critical levels in areas above 500M by 21:00.?
Post-edited Text:
?Rain will affect all routes during the after-
noon and evening. Road surface temperatures will
fall slowly during the mid afternoon and evening,
reaching near critical levels in areas above 500M by
21:00.?
Figure 8: Content selection post-edit example (road
surface temperature overview information removed)
Generated Text:
?Road surface temperatures will reach near
critical levels on some routes after midnight until
tomorrow morning. Rain will affect all routes
throughout the forecast period, falling as snow
in some places above 500M by 08:00. Snow
clearing by 08:00. Road surface temperatures
will fall slowly during the late evening and tonight,
reaching near critical levels in areas above 500M by
03:00.?
Post-edited Text:
?Road surface temperatures will reach near
critical levels on some routes after midnight until
tomorrow morning. Rain will affect all routes
during the forecast period, this may fall as sleet
later on highest ground before dying out.
Road surface temperatures will fall slowly during
the late evening and tonight, reaching near critical
levels in areas above 500M by 03:00.?
Figure 9: Microplanning post-edit example (lexicalisa-
tion and aggregation)
sentences when they describe an event (such as rain
turning heavy) occuring only at a small number of
locations. So the spatial extent of an event and not
only its meteorological importance should be con-
sidered during content selection. RoadSafe does not
currently include much domain reasoning at the doc-
ument planning level to be able to do this.
22
Microplanning edits, as highlighted in bold in Fig-
ure 9, are due to individual lexical choice or aggrega-
tion issues. In all questionnaires experts have com-
mented that the generated texts are grammatically
sound but could flow better. Aggregation is done
in a fairly basic fashion in our system at present as
is lexicalisation. There have been no edits to the
frame of reference used in the generated spatial de-
scriptions, which we have taken as indication that
our REG strategy works well.
5.2 Discussion
The general feedback to our system has been encour-
aging. In terms of the exploitability of the system in
its current form it has received mixed reviews from
4 forecasters: 1 forecaster rated the system as good
for content and very poor on fluency; 1 rated it as
ok for both; 1 forecaster rated it as poor for content
and ok for fluency; 1 forecaster rated it as poor for
both. Generally all forecasters believe the generated
texts should tell a more fluent story about weather
conditions with more causal linking between events.
In terms of the techniques and approach outlined in
this paper they have worked well, although as ac-
knowledged in the previous section more sophisti-
cated domain reasoning and aggregation techniques
are required if the text is to function as a concise
summary, and indeed reach the standard of human
authored texts.
Making the required improvements highlighted in
the previous section is the focus of current work. Af-
ter these improvements have been made we plan to
carry out an evaluation with users of the forecasts.
We hope to also extend the functionality of the sys-
tem by generating individual route forecasts, which
can be accessed interactively through the table.
6 Conclusions
We have presented an approach to generating ge-
ographically grounded summaries of georeferenced
data using spatial reference frames. This approach
has been implemented in a data-to-text system for
generating road ice forecasts. An important task in
summarising georeferenced data is to describe the
data in terms of its underlying geography it refer-
ences. This presents an interesting challenge for con-
ventional REG approaches as finding a distinguish-
ing description for large numbers of objects in geo-
graphic space is not practical. We have found char-
acterising the geography in terms of spatial reference
frames provides a good solution as it provides a flex-
ible representation to describe set of objects in terms
of geographic areas.
We have also implemented a simple top down
content selection approach based on the idea of
overview, taken from how we have observed ex-
perts commonly performing the summarisation task.
While this approach works well for content selection,
a post-edit evaluation with experts has highlighted
that realising the overview in the text can make texts
verbose and have the effect of making subsequent
statements describing related events in the discourse
sound repetitive. This is important as experts re-
quire a short concise summary of weather conditions.
Acknowledgments
Many thanks to our collaborators at Aerospace and
Marine International UK, especially Keith Thom-
son and the other Meteorologists, for their helpful
feedback and comments. The RoadSafe project is
supported jointly by Aerospace and Marine Inter-
national UK, and the UK Engineering and Physical
Sciences Research Council (EPSRC), under a CASE
PhD studentship.
References
J. Coch. 1998. Multimeteo: multilingual production of
weather forecasts. ELRA Newsletter, 3(2).
K. R. Coventry and S. C. Garrod. 2004. Saying, Seeing
and Acting: The Psychological Semantics of Spatial
Prepositions. Psychology Press.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19:233?263.
R Dale, S Geldof, and J-P Prost. 2005. Using natu-
ral language generation in automatic route description.
Journal of Research and Practice in Information Tech-
nology, 37(1):89?105.
A. Gatt and K. van Deemter. 2007. Lexical choice and
conceptual perspective in the generation of plural re-
ferring expressions. Journal of Logic, Language and
Information, 16:423?443.
E. Goldberg, N. Driedger, and R. Kittredge. 1994. Using
natural-language processing to produce weather fore-
casts. IEEE Expert, 9(2):45?53.
H. Grice. 1975. Logic and conversation. In P. Cole and
J. Morgan, editors, Syntax and Semantics, volume 3,
Speech Acts, pages 43?58. Academic Press: New York.
C. Hallett and D. Scott. 2005. Structural variation in
generated health reports. In Proceedings of the 3rd
International Workshop on Paraphrasing (IWP2005),
pages 33?40, Jeju Island, Republic of Korea.
Philip Hopwood. 2004. Improvements in road forecasting
techniques & their applications. In 12th International
Road Weather Conference, Bingen, Germany.
Helmut Horacek. 2006. Generating references to parts
of recursively structured objects. In Proceedings of
the 4th International Conference on Natural Language
Generation, pages 47?54.
23
Lidija Iordanskaja, Richard Kittredge, Benoit Lavoie,
and Alain Polgue`re. 1992. Generation of extended
bilingual statistical reports. COLING-92, pages 1019?
1023.
Emiel Krahmer, Sebastiaan van Erk, and Andr Verleg.
2003. Graph-based generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Stephen C. Levinson. 2003. Space in language and cog-
nition: explorations in cognitive diversity. Cambridge
University Press, Cambridge.
Harvey J. Miller and Jiawei Han. 2001. Geographic data
mining and knowledge discovery: An overview. In Ge-
ographic Data Mining and Knowledge Discovery, chap-
ter 1, pages 1?32. Taylor & Francis.
F. Portet, E. Reiter, J. Hunter, and S. Sripada. 2007.
Automatic generation of textual summaries from
neonatal intensive care data. In 11th Conference on
Artificial Intelligence in Medicine (AIME 07), pages
227?236.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated weather
forecasts. In Artificial Intelligence, volume 67, pages
137?169.
E. Reiter. 2007. An architecture for data-to-text sys-
tems. In ENLG07, pages 97?104.
D. Ro?sner. 1987. The automated news agency: Sem-
tex: A text generator for german. In Natural Lan-
guage Generation: New Results in Artificial Intelli-
gence, Psychology, and Linguistics. Nijhoff.
D. Roy and E. Reiter. 2005. Connecting language to the
world. Artificial Intelligence, 167:1?12.
S. Sripada, E. Reiter, J. Hunter, and Jin Yu. 2001.
A two-stage model for content determination. In
ENLG2001, pages 3?10.
S Sripada, E Reiter, and L Hawizy. 2005. Evaluation of
an nlg system using post-edit data: Lessons learnt. In
10th European Workshop on Natural Language Gener-
ation.
R. Turner, S. Sripada, E. Reiter, and I. Davy. 2007. Se-
lecting the content of textual descriptions of geograph-
ically located events in spatio-temporal weather data.
In Applications and Innovations in Intelligent Systems
XV, pages 75?88.
R. Turner, S. Sripada, E. Reiter, and I. Davy. 2008.
Building a parallel spatio-temporal data-text cor-
pus for summary generation. In Proceedings of the
LREC2008 Workshop on Methodologies and Resources
for Processing Spatial Language, Marrakech, Morocco.
K van Deemter. 2006. Generating referring expressions
that involve gradable properties. Computational Lin-
guistics, 32:195?222.
24
The Importance of Narrative and Other Lessons from an Evaluation of an
NLG System that Summarises Clinical Data
Ehud Reiter, Albert Gatt, Franc?ois Portet
Dept of Computing Science
University of Aberdeen, UK
{e.reiter,a.gatt,fportet}@
abdn.ac.uk
Marian van der Meulen?
Dept of Psychology
University of Edinburgh, UK
m.a.van-der-meulen@
sms.ed.ac.uk
Abstract
The BABYTALK BT-45 system generates tex-
tual summaries of clinical data about babies in
a neonatal intensive care unit. A recent task-
based evaluation of the system suggested that
these summaries are useful, but not as effec-
tive as they could be. In this paper we present
a qualitative analysis of problems that the
evaluation highlighted in BT-45 texts. Many
of these problems are due to the fact that BT-
45 does not generate good narrative texts; this
is a topic which has not previously received
much attention from the NLG research com-
munity, but seems to be quite important for
creating good data-to-text systems.
1 Introduction
Data-to-text NLG systems produce textual output
based on the analysis and interpretation of non-
linguistic data (Reiter, 2007). Systems which pro-
duce short summaries of small amounts of data, such
as weather-forecast generators (Reiter et al, 2005),
have been one of the most successful applications of
NLG, and there is growing interest in creating sys-
tems which produce longer summaries of larger data
sets.
We have recently carried out an evaluation of one
such system, BT-45 (Portet et al, 2007), which gen-
erates multi-paragraph summaries of clinical data
from a Neonatal Intensive Care Unit (NICU). The
summaries cover a period of roughly 45 minutes,
and describe both sensor data (heart rate, blood oxy-
gen saturation, etc, sampled at 1 sec intervals) as
well as discrete events such as drug administration;
?Now at the Department of Clinical Neurosciences, Univer-
sity Hospital, Geneva, Switzerland
they are intended to help medical staff make treat-
ment decisions. This evaluation showed that from a
decision-support perspective, the BT-45 texts were
as effective as visualisations of the data, but less ef-
fective than human-written textual summaries.
In addition to quantitative performance data,
which is presented elsewhere (van der Meulen et
al., submitted), the evaluation also gave us valuable
clues about what aspects of data-to-text technology
need to be improved in order to make texts gener-
ated by such systems more effective as decision sup-
port aids; this is the subject of this paper. Somewhat
to our surprise, many of the problems identified in
the evaluation relate to the fact that BT-45 could not
produce a good narrative describing the data. Gen-
eration of non-fictional narratives is not something
which has been the focus of much NLG research in
the past, but our results suggest it is important, at
least in the context of producing texts which are ef-
fective decision-support aids.
1.1 Background: Data-to-Text
Data-to-text systems are motivated by the belief that
(brief) linguistic summaries of datasets may in some
cases be more effective than more traditional pre-
sentations of numeric data, such as tables, statistical
analyses, and graphical visualisations (even simple
visual/graphical displays require relatively complex
cognitive processing (Carpenter and Shah, 1998)).
Also linguistic summaries can be delivered in some
contexts where visualisations are not possible, such
as text messages on a mobile phone, or when the
user is visually impaired (Ferres et al, 2006). In the
NICU domain, Law et al (2005) conducted an ex-
periment which showed that medical professionals
were more likely to make the correct treatment deci-
147
sion when shown a human-written textual summary
of the data than when they were shown a graphical
visualisation of the data.
A number of data-to-text systems have been de-
veloped and indeed fielded, especially in the domain
of weather forecasts (Goldberg et al, 1994; Reiter
et al, 2005). Most of these systems have gener-
ated short (paragraph-length or smaller) summaries
of relatively small data sets (less than 1KB). Some
research has been on systems that summarise larger
data sets (Yu et al, 2007; Turner et al, 2008), but
these systems have also generated paragraph-length
summaries; we are not aware of any previous re-
search on generating multi-paragraph summaries in
a data-to-text system.
Data-to-texts systems have been evaluated in a
number of ways, including human ratings (the most
common technique) (Reiter et al, 2005), BLEU-like
scores against human texts (Belz and Reiter, 2006),
post-edit analyses (Sripada et al, 2005), and per-
suasive effectiveness (Carenini and Moore, 2006).
However, again to the best of our knowledge no pre-
vious data-to-text system has been evaluated by ask-
ing users to make decisions based on the generated
texts, and measuring the quality of these decisions.
2 BabyTalk and BT-45
Law et al (2005) showed that human-written textual
summaries were effective decision-support aids in
NICU, but of course it is not practical to expect medi-
cal professionals to routinely write such summaries,
especially considering that the summaries used by
Law et al in some cases took several hours to write.
The goal of the BABYTALK research project is to
use NLG and data-to-text technology to automati-
cally generate textual summaries of NICU data, for a
variety of audiences and purposes. The first system
developed in BABYTALK, and the subject of this pa-
per, is BT-45 (Portet et al, 2007), which generates
summaries of 30-60 minute chunks of clinical data,
for the purpose of helping nurses and doctors make
appropriate treatment decisions.
An example of BABYTALK input data is shown in
Figures 1 (sensor data) and 2 (selected event data).
Figure 3 shows the human-written corpus text for
this scenario, and Figure 4 shows the BT-45 text gen-
erated for this scenario. Note that for the purposes of
Figure 1: Example Babytalk Input Data: Sensors
HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 =
blood CO2 level; SaO2 = oxygen saturation; T1 = chest
temperature; T2 = toe temperature; mean BP = blood
pressure. The bars and triangles at the bottom show the
time of discrete events (Figure 2).
event time
Blood transfusion 13.35
Intermittent crying 13.38
FiO2 (oxygen level) changed to 50% 13.51
Incubator temperature changed to 36.4 13.52
Attempt to insert line 13.53
Line removed 13.57
Attempt to insert line 13.58
Line removed 14.00
FiO2 (oxygen level) changed to 45% 14.03
Attempt to insert line 14.08
Line removed 14.09
Figure 2: Example Babytalk Input Data: Selected Dis-
crete Events
148
You saw the baby between 13:30 and 14:10.
Over the course of the monitoring period the HR in-
creases steadily from 140 to 165; the BP varies between
41 and 49.
At the start of the period T1 is 37 and T2 is 35.8C.
During the first 15 minutes the pO2 is 7.8-9.2 and the
pCO2 is 5.9-7.3.
At 13:35 a blood transfusion is commenced.
At 13:38 the baby is crying and there are a few up-
ward spikes in the pO2 trace corresponding to downward
spikes in the pCO2 trace. At 13.45 the humidity on the
incubator walls is wiped away and T1 and T2 fall to 36.3
and 35.4 respectively
At 13:50 the baby is examined. There is a desaturation to
72% and the FiO2 is changed to 50%. Between now and
14.10 there are several attempts to site a peripheral can-
nula. The pO2 and pCO2 both have spikes in the traces,
pO2 reaching 19.2 and pCO2 reaching 0.4. There are sev-
eral episodes of artefact in the oxygen saturation trace.
T1 and T2 fall to 36.2 and 35.7 and the oxygen saturation
falls to 65%. The FiO2 is adjusted to 50%. Also at this
time the incubator temperature is adjusted to 36.4C.
At 14:03 with the pO2 at 13.9 and oxygen saturation at
100%, the FiO2 is reduced to 45
At 14:04 T1 rises sharply to 40, then drops fairly steeply
to 28.5C. Between 14.06 and 14.10 there are several
spikes in the pO2 and pCO2 traces but by 14.10 the pO2
is 8, pCO2 is 3.7, the oxygen saturation is 71%, the HR
is 163, the BP 39, T1 29C and T2 35.4C.
Figure 3: Corpus Text for Fig 1, 2 data
this paper, we have deliberately selected a relatively
poor quality BT-45 output text.
The processing performed by BT-45 is described
by Portet et al (2007). Very briefly, BT-45 generates
texts in four stages:
? signal analysis: for example, detecting spikes
in the sensor channels; this is done using adap-
tations of standard pattern detection and recog-
nition algorithms.
? data abstraction: for example, identifying
three line-insertion-attempt and line-removal
events within a short span of time and
grouping these into the higher-level con-
cept LINE-INSERTION-PROCESS-FAILED (BT-
You saw the baby between 13:30 and 14:10. Heart Rate
(HR) = 149. Core Temperature (T1) = 37.0. Periph-
eral Temperature (T2) = 35.8. Transcutaneous Oxygen
(TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7.
Mean Blood Pressure (mean BP) = 41. Oxygen Satura-
tion (SaO2) = 94.
Over the next 39 minutes SaO2 decreased to 81, T2 de-
creased to 34.5, HR stayed at around 151, T1 decreased
to 36.2 and mean BP stayed at around 40.
A blood transfusion was given to the baby at 13:35.
At 13:50 there was a desaturation down to 65. As a result,
Fraction of Inspired Oxygen (FIO2) was set to 50
There were 3 failed attempts to insert a peripheral venous
line at 13:53. TcPO2 suddenly decreased to 8.1. SaO2
suddenly increased to 92. TcPO2 suddenly decreased to
9.3. There was a spike in TcPO2 up to 14.8. There had
been another spike in T1 up to 40.5. FIO2 had been low-
ered to 45%. Previously the baby had been examined.
Figure 4: BT-45 Text for Fig 1, 2 data
45 includes a domain ontology of such con-
cepts); this is done using knowledge-based
techniques.
? document planning: for example, deciding not
to mention most of the spikes in O2 and CO2;
this is primarily done in a bottom-up fashion,
using information (computed by the data ab-
straction module) on the medical importance of
events, and also on causal and other relation-
ships between events.
? microplanning and realisation: producing the
actual text shown in Figure 4; this is mostly
done using relatively standard NLG techniques,
although we have developed new techniques
for communicating temporal information and
relationships.
3 Evaluation of BT-45
BT-45 was evaluated by asking medical staff to
make decisions about what actions they should take
with regard to a baby, after viewing either a BT-45
text, a human-written textual summary, or a visual-
isation of the baby?s data; this was similar in gen-
eral terms to the experiment described by Law et al
(2005). van der Meulen et al (submitted) gives de-
149
tails about the evaluation design and quantitative re-
sults of the evaluation; in this paper we just briefly
summarise these aspects of the evaluation
Material: Our medical collaborators selected 24
scenarios (data sets), and defined 18 types of ac-
tions. For each of the data sets, they specified which
of the 18 actions were appropriate, inappropriate, or
neutral (neither appropriate nor inappropriate); one
appropriate action was identified as the main target
action. For the data set shown in Figures 1 and 2, for
example:
? Main target action: Adjust monitoring equip-
ment
? Other appropriate actions: calm/comfort baby,
manage temperature, analyse blood sample
? Neutral actions: adjust CPAP (ventilation) set-
tings, baby care (e.g., nappy change)
? Inappropriate actions: all other actions (e.g.
blood transfusion, order X-Ray) (12 in all)
For each scenario, we created three presentations: a
visualisation (similar to Figure 1), a human-written
text summary written by our collaborators, and the
summary produced by BT-45. Our collaborators
were asked not to include any explicit medical in-
terpretation of the data in their human-written sum-
maries. For each scenario, our collaborators also
prepared a text which gave background information
about the baby.
When developing BT-45, we had access to the
data collection that the scenario data sets were taken
from (which includes several months of data), but
did not know ahead of time which specific scenarios
would be used in the experiment.
Subjects: 35 medical professionals, including ju-
nior nurses, senior nurses, junior doctors, and senior
doctors.
Procedure: Each subject was shown 8 scenarios
in each condition (visualisation, human text, BT-45
text) in a Latin Square design; all subjects were also
shown the background texts. Subjects were asked to
specify which actions should be taken for this baby,
selecting actions from a fixed set of check-boxes;
they were given three minutes to make this deci-
sion. Subjects were not explicitly asked for free-text
comments, but any comments spontaneously made
by subjects were recorded. Subject responses were
scored by computing the percentage of appropriate
actions they selected, and subtracting from this the
percentage of inappropriate actions.
Results: The highest score was achieved by the
human texts (mean score of 0.39); there was no sig-
nificant difference between the BT-45 texts (mean
score of 0.34) and the visualisations (mean score of
0.33). The largest differences occurred in the ju-
nior nurses group. van der Meulen et al (submitted)
present a detailed statistical analysis of the results.
Discussion: This shows that BT-45 texts were as
effective as visualisation, but less effective than the
human texts. This suggests that data-to-text technol-
ogy as it stands could be useful as a supplement to
visualisations (since some individuals do better with
texts and some with graphics; also some data sets are
visualised effectively and some are not), and in con-
texts where visualisation is not possible. But it also
suggests that if we can improve the technology so
that computer-generated texts are as effective as hu-
man texts, we should have a very effective decision-
support technology.
4 Quantitative Comparison of BT-45 and
Corpus Texts
In addition to the task-based evaluation described
above, we also quantitatively compared the BT-45
and human texts, and qualitatively analysed prob-
lems in the BT-45 texts. Quantitative comparison
was done by annotating the BT-45 and human texts
to identify which events they mentioned. For each
scenario, we computed the MASI coefficient (Pas-
sonneau, 2006) between the set of events mentioned
in the BT-45 and human texts. The average MASI
score was 0.21 (SD = 0.13), which is low; this
suggests that BT-45 and the human writers choose
different content. We also checked whether similar
human and BT-45 texts (as judged by MASI score)
obtained similar evaluation scores; in fact there was
no significant correlation between MASI similarity
of human and BT-45 texts and the difference be-
tween their evaluation scores.
We performed a second analysis based on com-
paring the structure (e.g., number and size of para-
graphs) of the BT-45 and human texts, using a
tree-edit-distance metric to compare text structures.
150
Again this showed that there were large differences
between the structure of the BT-45 and human texts,
and that these differences did correlate with differ-
ences in evaluation scores.
In other words, simple metrics of content and
structural differences do not seem to be good pre-
dictors of text effectiveness; this is perhaps not sur-
prising given the complexity of the texts and the in-
formation they are communicating.
5 Qualitative Analysis of Problems in
BT-45 texts
The final step in our evaluation was to qualitatively
analyse the BT-45 texts and the results of the task-
based evaluation, in order to highlight problems in
the BT-45 texts. Of course we were aware of numer-
ous ways in which the software could be improved,
but the evaluation gave us information about which
of these mattered most in terms of overall effective-
ness. We report this analysis below, including issues
identified from subjects? comments, issues identi-
fied from scenarios where BT-45 texts did poorly,
and problems identified via manual inspection of the
texts. We do not distinguish between ?linguistic? and
?reasoning? problems, in part because it is usually
difficult (and indeed somewhat artificial) to separate
these aspects of BT-45.
5.1 Problems Identified by Subjects
Subjects made a number of comments during the ex-
periment. Two aspects of BT-45 were repeatedly
criticised in these comments.
5.1.1 Layout and bullet lists
Subjects wanted better layout and formatting, in
the human texts as well as the BT-45 texts (BT-45
texts do not currently include any visual formatting).
In particular, they wanted bullet lists to be used, es-
pecially for lab results. Such issues have been exten-
sively discussed by other researchers (e.g., (Power et
al., 2003)), we will not further discuss them here.
5.1.2 Continuity
BT-45 sometimes described changes in signals (or
other events) which didn?t make sense because they
omitted intermediate events. For example, consider
the last paragraph in the BT-45 text shown in Fig-
ure 4 (with italics added):
There were 3 failed attempts to insert a pe-
ripheral venous line at 13:53. TcPO2 sud-
denly decreased to 8.1. SaO2 suddenly in-
creased to 92. TcPO2 suddenly decreased
to 9.3. There was a spike in TcPO2 up
to 14.8. There had been another spike in
T1 up to 40.5. FIO2 had been lowered to
45%. Previously the baby had been exam-
ined.
Subjects complained that it made no sense for
TcPO2 to decrease to 9.3 when the last value men-
tioned for this parameter was 8.1
In this case (and in many others like it), BT-45
had identified the decrease events as being medically
important, but had not assigned as much importance,
and hence not mentioned, the increase event (TcPO2
went up to 19) between these decrease events. This
is partially because BT-45 believed that a TcPO2 of
19 is a sensor artefact (not a real reading of blood
oxygen), since 19kPa is a very high value for this
channel. In fact this is a correct inference on BT-
45?s part, but the text is still confusing for readers.
We call this problem continuity, making an anal-
ogy to the problems that film-makers have in ensur-
ing that scenes in a film (which maybe shot in very
different times and locations) fit together in the eyes
of the viewer. It is interesting to note that some of
the human texts also seem to have continuity prob-
lems (for example, the text in Figure 3 says T2 falls
to 35.4, and then says T2 falls to 35.7), but none of
the subjects complained about continuity problems
in the human texts. So some kinds of continuity vio-
lations seem more problematical to readers than oth-
ers. Perhaps this depends on the proximity of the
events both in the document structure and in time;
we hope to empirically explore this hypothesis.
Continuity is just one aspect of the broader prob-
lem of deciding which events need to be explicitly
mentioned in the text, and which can be omitted.
Making such decisions is perhaps one of the hard-
est aspects of data-to-text.
5.2 Scenarios Where BT-45 did Badly
When analysing the results of the experiment, we
noticed that BT-45 texts did as well as the human
texts for scenarios based on five of the eight target
actions; however they did significantly worse than
151
main target action human BT-45 diff
Adjust CPAP 0.37 0.37 0
Adjust monitoring equip 0.59 0.22 0.37
Adjust ventilation 0.22 0.23 -0.01
Extubate 0.14 0.12 0.02
Manage temperature 0.55 0.33 0.22
No action 0.61 0.43 0.18
Suction 0.34 0.42 -0.08
Support blood pressure 0.45 0.55 -0.10
Table 1: Average evaluation score by main target action
the human texts on the scenarios based on the other
three actions (Adjust Monitoring Equipment, Man-
age Temperature, and No Action). Details are shown
in Table 1; an ANOVA confirms that there is a sig-
nificant effect of main target action on scores (p <
.001). We have identified a number of reasons why
we believe this is the case, which we discuss below.
5.2.1 Too much focus on medically important
events
Content-selection in BT-45 is largely driven by
rules that assess the medical importance of events
and patterns. In particular, BT-45 tends to give low
importance to events which it believes are due to
sensor artefacts. While this strategy makes sense
in many cases, it leads to poor performance in sce-
narios where the target action is Adjust Monitor-
ing Equipment, when sensor problems need to be
pointed out to the reader.
This can be seen in the example scenario used in
this paper. The TcPO2 and TcPCO2 traces shown in
Figure 1 are full of sensor artefacts (such as the im-
plausibly high values of TcPO2 mentioned above).
The human text shown in Figure 3 explicitly men-
tions these, for example (italics added)
At 13:50 the baby is examined. There
is a desaturation to 72% and the FiO2 is
changed to 50%. Between now and 14.10
there are several attempts to site a periph-
eral cannula. The pO2 and pCO2 both
have spikes in the traces, pO2 reaching
19.2 and pCO2 reaching 0.4. There are
several episodes of artefact in the oxygen
saturation trace.
The BT-45 text shown in Figure 4, in contrast, only
mentions one spike in TcPO2, and does not mention
any artefacts.
This is a difficult problem to solve, because in
a context where medical intervention was needed,
BT-45 would be correct to ignore the sensor prob-
lems. One solution would be for BT-45 to perform
a top-level diagnosis itself, and adjust its texts based
on whether it believed staff should focus on medi-
cal intervention or adjusting sensors. Whether this
is desirable or even feasible is unclear; it relates to
the more general issue of how a data-summarisation
system such as BT-45 should be integrated with
the kind of diagnosis systems developed by the
AI/Medicine community.
5.2.2 Poor description of related channels
BT-45 essentially describes each channel inde-
pendently. For temperature, however, it is often bet-
ter to describe the two temperature channels together
and even contrast them, which is what the human
texts do; this contributes to BT-45?s poor perfor-
mance in Manage Temperature scenarios.
For example, in one of the Manage Temperature
scenarios, the BT-45 text says
Core Temperature (T1) = 36.4. Peripheral
Temperature (T2) = 34.0. . . .
(new paragraph)Over the next 44 minutes
T2 decreased to 33.4.
The human text says
He is warm centrally but T2 drifts down
over the 45 minutes from 34 to 33.3C.
The information content of the two texts is quite
similar, but the human text describes temperature
in an integrated fashion. Similar problems occur in
other scenarios. In fact, over the 24 scenarios as
a whole, the human texts include only three para-
graphs which mention just one of the temperatures
(T1 or T2, but not both), while the BT-45 texts in-
clude 18 such paragraphs.
BT-45?s document planner is mostly driven by
medical importance and causal relationships; al-
though it does try to group together information
about related channels, this is done as a secondary
optimisation, not as a primary organising principle.
The human texts place a much higher priority on
152
grouping ?physiological systems? (to use NICU ter-
minology) of related channels and events together,
including the respiratory and cardiac systems as well
as the temperature system. We suspect that BT-45
should place more emphasis on systems in its docu-
ment planning.
5.2.3 Poor long-term overview
BT-45 does not do a good job of summarising a
channel?s behaviour over the entire scenario. This
isn?t a problem in eventful scenarios, where the key
is to describe the events; but it does reduce the effec-
tiveness of texts in uneventful scenarios where the
main target action is No Action (i.e., do nothing).
This problem can be seen in the text extracts
shown in the previous section. Even at the level of
individual channels, He is warm centrally is a better
overview than Core Temperature (T1) = 36.4; and
T2 drifts down over the 45 minutes from 34 to 33.3C
is better than Peripheral Temperature (T2) = 34.0.
. . . Over the next 44 minutes T2 decreased to 33.4.
At a signal analysis level, BT-45 also does not
do a good job of detecting patterns (such as spikes)
with a duration of minutes instead of seconds. This
contributes to the system?s poor performance in
Manage Temperature scenarios, because tempera-
ture changes relatively slowly.
We believe these problems can be solved, by
putting more emphasis on analysis and reporting of
long time-scale events in the BT-45 modules.
5.3 Other Problems
We manually examined the texts, looking for cases
where the BT-45 texts did not seem clear. This high-
lighted a number of additional issues.
5.3.1 Describing events at different temporal
time-scales
BT-45 does not always do a good job of correctly
identifying long-term trends in a context where there
are also short-term patterns such as spikes. In fact
accurately detecting simultaneous events at different
time-scales is one of the major signal analysis chal-
lenges in BT-45. There are linguistic issues as well
as signal analysis ones; for example, should long-
duration and short-duration events be described in
separate paragraphs?
5.3.2 Poor communication of time
BT-45 texts often did not communicate time well.
This is for a number of reasons, of which the
most fundamental is problems describing the time
of long-duration events. For instance, in our ex-
ample scenario, the sequence of insert/remove line
events in Figure 2 is analysed by the data-abstraction
module as the abstract event LINE-INSERTION-
PROCESS-FAILED, with a start time of 13.53 (first
insertion event) and an end time of 14.09 (last re-
moval event). BT-45 expresses this as There were 3
failed attempts to insert a peripheral venous line at
13:53; the time given is the time the abstract event
started, which is reasonable in this case. Now, if the
final insertion attempt at 14.08 had been successful,
the BT-45 data abstraction module would have in-
stead produced the abstract event LINE-INSERTION-
PROCESS-SUCCEEDED, with similar times, and BT-
45 would have produced the text
After three attempts, at 13.53 a peripheral
venous line was inserted successfully.
In other words, the time given would still be the time
that the abstract event started; but this is mislead-
ing, because readers of the above text expect that
the stated time is the time of the successful inser-
tion (14.08), not the time at which the sequence of
insert/remove events started.
We need a much better model of how to communi-
cate time, and how this communication depends on
the semantics and linguistic expression of the events
being described. An obvious first step, which we are
currently working on, is to include a linguistically-
motivated temporal ontology (Moens and Steedman,
1988), which will be separate from the existing do-
main ontology. We also need better techniques for
communicating the temporal relationships between
events in cases where they are not listed in chrono-
logical order (Oberlander and Lascarides, 1992).
6 Discussion
Two discourse analysts from Edinburgh University,
Dr. Andy McKinlay and Dr Chris McVittie, kindly
examined and compared some of the human and BT-
45 texts. Their top-level comment was that the hu-
man texts had much better narrative structures than
the BT-45 texts. They use the term ?narrative? in
153
the sense of Labov (1972, Chapter 9); that is story-
like structures which describe real experiences, and
which go beyond just describing the events and in-
clude information that helps listeners make sense of
what happened, such as abstracts, evaluatives, cor-
relatives, and explicatives.
Dr McKinlay and Dr. McVittie pointed out many
of the problems mentioned above, but they also
pointed out a number of other narrative deficiencies
in the BT-45 texts. The most fundamental was that
the human texts did a much better job of linking re-
lated events into a coherent whole. Other deficien-
cies include the lack of any kind of conclusion in the
BT-45 texts.
We agree with this analysis; it is striking that
many of the specific problems identified are related
to the problem of generating narratives. Continu-
ity, description of related channels, overview of be-
haviour over time, and communication of time are
all aspects of narrative in the broad sense; they are
things we need to get right in order to turn a text
into a story. This point is especially significant in
light of the fact that many of our medical collabora-
tors at Edinburgh have informally told us that they
believe stories are valuable when presenting infor-
mation about the babies, and indeed that a major
problem with data visualisation systems compared
to written notes (which they used many years ago) is
that the visualisation systems do not tell stories.
Unfortunately, we are not aware of any previ-
ous research in the NLG community about these is-
sues. Researchers in the creativity community have
looked at issues such as plot and character develop-
ment in systems that generate fictional stories (Perez
y Perez and Sharples, 2004); but this is not relevant
to our problem, which is presenting non-fictional
events as a narrative. Callaway and Lester (2002)
looked at microplanning issues in narrative genera-
tion, including reference, lexical variation, and ag-
gregation; but none of these were identified in our
evaluation as major problems in text quality.
7 Future Work
The BABYTALK project continues until August
2010, and during this period we hope to investigate
most of the issues identified above, especially the
ones related to narrative. We are currently conduct-
ing experiments to improve the way we communi-
cate time, and we have started redoing the docu-
ment planner to do a better job of describing sys-
tems of related channels in a unified manner. We
are also investigating top-down data abstraction and
document planning approaches which we hope will
address continuity problems, and which may assist
in better overviews and narrative structures. We are
also working on many issues not directly related to
narrative, such as reasoning about and communicat-
ing uncertainty, use of vague language, generation
of texts for non-specialists (e.g., parents), and HCI
issues.
We would welcome interest by other researchers
in these topics (there is more that needs investigat-
ing than we can do on our own!), and we would be
happy to assist such people, for example by sharing
some of our code and data resources.
8 Conclusion
We believe that there is enormous potential in sys-
tems such as BABYTALK which generate textual
summaries of data; the world desperately needs bet-
ter techniques to help people understand data sets,
and our experiments suggest that good textual sum-
maries really can help communicate data sets, at
least in some contexts. However, building good
data summarisation systems requires the NLG re-
search community to address a number of problems
which it has not traditionally focused on, many of
which have to do with generating good narratives.
We intend to focus much of our energy on these
issues, and would welcome research contributions
from other members of the community.
Acknowledgements
Many thanks to our colleagues in the BabyTalk
project, and to the doctors and nurses who partic-
ipated in the evaluation; this work would not have
been possible without them. Special thanks to Dr
McKinlay and Dr McVittie for agreeing to anal-
yse the texts for us. We are also grateful to our
colleagues in the Aberdeen NLG group, and to the
anonymous reviewers, for their helpful comments.
This research was funded by the UK Engineer-
ing and Physical Sciences Research Council, under
grant EP/D049520/1.
154
References
A Belz and E Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of
EACL-2006, pages 313?320.
C Callaway and J Lester. 2002. Narrative prose genera-
tion. Artificial Intelligence, 139:213?252.
G Carenini and J Moore. 2006. Generating and eval-
uating evaluative arguments. Artificial Intelligence,
170:925?952.
P Carpenter and P Shah. 1998. A model of the percep-
tual and conceptual processes in graph comprehension.
Journal of Experimental Psychology: Applied, 4:74?
100.
L Ferres, A Parush, S Roberts, and G Lindgaard. 2006.
Helping people with visual impairments gain access to
graphical information through natural language: The
iGraph system. In Proceedings of ICCHP-2008.
E Goldberg, N Driedger, and R Kittredge. 1994. Using
natural-language processing to produce weather fore-
casts. IEEE Expert, 9(2):45?53.
W Labov. 1972. Language in the Inner City. University
of Pennsylvania Press.
A Law, Y Freer, J Hunter, R Logie, N McIntosh, and
J Quinn. 2005. A comparison of graphical and textual
presentations of time series data to support medical de-
cision making in the neonatal intensive care unit. Jour-
nal of Clinical Monitoring and Computing, 19:183?
194.
M Moens and M Steedman. 1988. Temporal ontology
and temporal reference. Computational Linguistics,
14(2):15?28.
J Oberlander and A Lascarides. 1992. Preventing false
temporal implicatures: Interactive defaults for text
generation. In Proceedings of COLING-1992, pages
721?727.
R Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. In Proceedings of LREC-2006.
R Perez y Perez and M Sharples. 2004. Three computer-
based models of storytelling: Brutus, Minstrel, and
Mexica. Knowledge-Based Systems, 17:15?29.
F Portet, E Reiter, J Hunter, and S Sripada. 2007. Auto-
matic generation of textual summaries from neonatal
intensive care data. In Proceedings of AIME 2007.
R Power, D Scott, and N Bouayad-Agha. 2003. Doc-
ument structure. Computational Linguistics, 29:211?
260.
E Reiter, S Sripada, J Hunter, J Yu, and I Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137?169.
E Reiter. 2007. An architecture for Data-to-Text sys-
tems. In Proceedings of ENLG-07, pages 97?104.
S Sripada, E Reiter, and L Hawizy. 2005. Evaluation of
an NLG system using post-edit data: Lessons learned.
In Proceedings of ENLG-2005, pages 133?139.
R Turner, S Sripada, E Reiter, and I Davy. 2008. Using
spatial reference frames to generate grounded textual
summaries of georeferenced data. In Proceedings of
INLG-2008.
M van der Meulen, R Logie, Y Freer, C Sykes, N McIn-
tosh, and J Hunter. submitted. When a graph is poorer
than 100 words: A comparison of computerised natu-
ral language generation, human generated descriptions
and graphical displays in neonatal intensive care.
J Yu, E Reiter, J Hunter, and C Mellish. 2007. Choosing
the content of textual summaries of large time-series
data sets. Natural Language Engineering, 13:25?49.
155
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using NLG and Sensors to Support Personal Narrative for 
Children with Complex Communication Needs 
 
 
Rolf Black Joe Reddington, Ehud Reiter, Nava Tintarev Annalu Waller 
School of Computing Department of Computing Science School of Computing 
University of Dundee            University of Aberdeen University of Dundee            
rolfblack@ 
computing.dundee.ac.uk 
{j.reddington, e.reiter  n.tintarev}@abdn.ac.uk awaller@ 
computing.dundee.ac.uk 
 
 
Abstract 
We are building a tool that helps children with 
Complex Communication Needs1 (CCN) to 
create stories about their day at school. The 
tool uses Natural Language Generation (NLG) 
technology to create a draft story based on 
sensor data of the child?s activities, which the 
child can edit. This work is still in its early 
stages, but we believe it has great potential to 
support interactive personal narrative which is 
not well supported by current Augmentative 
and Alternative Communication (AAC) tools. 
1 Introduction 
Many tools have been developed to help children and 
adults who cannot speak (or who have limited speech) 
communicate better.  However, most of these tools have 
focused on supporting communication for practical 
goals, such as ?I am thirsty.? But human communica-
tion is also used for social goals; we develop friendships 
and other inter-personal relationships via social interac-
tion and communication. The bulk of conversation is 
characterized by free narrative (Cheepen 1988). One of 
the most important types of conversational narrative is 
personal narrative: someone telling a story about what 
happened to him or her. 
 People with limited or no functional speech do tell 
stories, but these tend to be in monologue form, or in a 
sequence of pre-stored utterances on voice output com-
munication aids (Waller 2006). Individuals who use 
                                                          
1
 The term Complex Communication Needs (CCN) describes 
individuals who, due to motor, language, cognitive, and/or 
sensory perceptual impairments (e.g., as a result of cerebral 
palsy), do not develop speech and language skills as expected. 
This heterogeneous group typically experiences restricted 
access to the environment, limited interactions with their 
communication partners, and few opportunities for communi-
cation (Light and Drager 2007). 
Augmentative and Alternative Communication (AAC) 
tools tend to be passive, responding to questions with 
single words or short sentences (e.g. Soto, Hartmann et 
al. 2006) and if able to initiate and maintain extended 
conversations tend to relate experience word for word 
each time they tell a story, even though much of conver-
sation is reused (Clarke and Clarke 1977). This is time 
consuming and physically exhausting ? typical rates 
range from 8 to 10 words per minute up to 12 to 15 per 
minute when techniques such as word prediction are 
used (Higginbotham, Shane et al 2007), with the result 
that people seldom engage in storytelling. Despite the 
importance of narrative, little work has been done on 
specific tools to help language-impaired individuals 
engage in personal storytelling. In this paper, we de-
scribe our work in progress on building a tool that uses 
Natural Language Generation (NLG) technology to help 
children tell stories about their day at school, describing 
both the work we have done to date, and the challenges 
that we face in further developing this concept. 
2 Background 
2.1 AAC 
Technology underpins much of Augmentative and Al-
ternative Communication (AAC), a field that attempts to 
augment natural speech and provides alternative ways to 
communicate for people with limited or no speech. At 
the simplest level, people with Complex Communica-
tion Needs (CCN) can cause a pre-stored message to be 
spoken by activating a single switch. At the most so-
phisticated level, literate users can generate novel text 
using input methods ranging from a single switch to a 
full keyboard.  
Despite advances in AAC, there are still many indi-
viduals for whom communication remains problematic. 
Although some individuals with CCN become effective 
communicators, most do not ? they tend to be passive 
communicators, responding mainly to questions or 
prompts at a one or two word level. Conversational 
1
skills such as initiation, elaboration and storytelling are 
seldom observed (Waller 2006). 
One reason for the reduced levels of communicative 
ability is the cognitive demands of AAC interfaces. Cur-
rent AAC technology provides the user with a purely 
physical link to speech output. The user is required to 
have sufficient cognitive abilities and physical stamina 
to translate what they want to say into the sequence of 
operations needed to produce the desired output. Mne-
monic codes and dynamic displays (Beukelman and 
Mirenda 2005) provide some help in the retrieval 
process, but users still have to master complex retrieval 
and production strategies.  
A second reason for the impoverished quality of 
conversation is the focus of AAC devices on transac-
tional communication; conversation which expresses 
needs wants and information transfer, for example, ?I 
am thirsty?, ?I use a straw for drinking?. Instead, inter-
active conversation is characterized by free narrative 
and phatic conversation, for example, ?Guess what 
happened this morning??, ?Hello?, and ?How are 
you?? Without easy access to extended interactive 
communication, it is difficult to develop the skills 
needed to initiate new topics and engage in storytelling.  
2.2 Importance of Narrative 
 Conversational narratives (oral stories told during 
interactive conversations) are crucial to social engage-
ment. Narratives provide a means for people to relate 
and share experiences, develop organizational skills, 
work through problems, develop self image, express 
personality, give form and meaning to life, and allow 
people to be interesting entertainers (Waller 2006).  
 Narrative skills develop experientially with children 
being able to engage in storytelling even before they are 
verbal (Bruner 1975). Early personal experience stories 
consist of a high point, for example, ?Mummy fall!? 
with adults scaffolding the full story, eliciting the ?who?, 
?what?, ?when? and ?where?. However, not all expe-
riences make good stories. An experience becomes a 
story if the storyteller has an emotional connection to 
the event (Labov, 1972), or if the event is unusual (Qua-
sthoff & Nikolaus, 1982). 
 Parents of typically developing children encourage 
development of narrative skills by eliciting stories from 
their children (Peterson and McCabe 1983), but the de-
velopment of narrative skills is problematic for people 
with CCN. We recall a study where disabled children 
were told different stories more often than typically 
developing peers who were read the same story night 
after night (Light, Binger et al 1994). In doing so, the 
disabled children did not have the chance to learn the 
sequence of stories, or the structure commonly used in 
narrative such as beginning, middle and end. As such, 
initially children should use the same story template 
consistently until they are ready to progress to another 
one. 
 It is difficult to provide access to event information 
which may become a story, and few AAC systems pro-
vide support for interactive story narration. However 
NLG gives us a possibility to change the underlying 
paradigm of AAC. Instead of placing the entire cogni-
tive load on the user, AAC devices can be designed to 
support the retrieval of story events and the scaffolding 
of story narration for individuals with CCN. 
2.3 NLG, Data-to-text 
NLG systems generate texts in English (or other human 
languages) from non-linguistic data (Reiter and Dale 
2000).  Our vision is to use an NLG system to generate 
a draft story, which the child can edit.  The non-
linguistic input to our story-generator is sensor data 
about the child?s activities, including location data 
(where the child was) and interaction data (what people 
and objects the child interacted with).  We also want to 
allow teachers and school staff to enter information 
about the child?s activities (such as voice messages). 
 A number of data-to-text systems (Reiter 2007) have 
been developed in recent years, which generate English 
summaries of sensor and other numerical data.  The 
most popular application area has been weather fore-
casting (generating textual weather forecasts from the 
results of a numerical atmosphere simulation model), 
and indeed several weather forecast generators have 
been fielded and used operationally (Goldberg, Driedger 
et al 1994; Reiter, Sripada et al 2005). A number of 
data-to-text systems have also been developed in the 
medical community, such as BabyTalk (Gatt, Portet et 
al. 2009), which generates summaries of clinical data 
from a neonatal intensive care unit, and the commercial 
Narrative Engine (Harris 2008) which summarizes data 
acquired during a doctor/patient encounter. 
 Most previous research in data-to-text has focused 
on summarizing technical data for expert users, with the 
goal of effectively communicating key information.  In 
our work, in contrast, the focus is on summarizing data 
about everyday events, with the goal of having some-
thing interesting to talk about.  There has been consider-
able work in the computational creativity community on 
generating interesting stories (P?r?z and Sharples 2004), 
but it has focused on fictional written stories, where the 
computer system can say whatever it wishes, without 
the constraint of describing real events. 
 Most previous work in NLG has focused on com-
puter systems which generate texts without human in-
put.  However, in our case we want children to be able 
to annotate (evaluate) and edit stories, as far as their 
abilities permit.  There has been some research on hu-
man post-editing of NLG texts (Sripada, Reiter et al 
2005), but this has focused on editing at the text level.  
2
Since editing at the text level is very laborious for AAC 
users, we need a higher-level interface that lets children 
edit content and structure without needing to type 
words.  We also want children to be able to control how 
a story is narrated, perhaps in response to a listener?s 
questions or body language.  For example children may 
wish to add comments such as ?It was awesome!?, or 
tell events out of sequence. 
    In short, we need to develop interfaces and interac-
tion techniques that allow our users to control the NLG 
system.  Unfortunately there has been very little pre-
vious work on this topic, indeed almost nothing is 
known about Human-Computer Interaction aspects of 
NLG systems.  Developing a better understanding of 
these aspects is one of the main research challenges we 
face from an NLG perspective. 
3 Current and ongoing work 
3.1 ?How was School today??? 
We developed an initial version of ?How was School 
today??? in 2009; see Reiter et al(2009) for more de-
tails about this system. 
 
 
 
Fig. 1: Participating pupil with support worker:  
The prototype system is mounted on the wheelchair, and 
the pupil has access to the system via head switch con-
trolled row/column scanning. 
 
 This system used Radio Frequency Identification 
(RFID), an emerging application in AAC to identify or 
give access to relevant vocabulary (Bart, Riny et al 
2008; DeRuyter and Fried-Oken 2010). Sensors were 
used to track both location (by putting tags on doors, 
which were automatically sensed by a long-range RFID 
reader) and interaction (by asking staff to manually 
swipe RFID cards in a short-range reader when the child 
interacted with a person or object). Staff could also 
record spoken messages about interesting events during 
the day (see Fig. 1). 
The software analyzed this data to remove sensor noise, 
and then compared it to a timetable which specified 
where children were supposed to be, what they were 
supposed to be doing and which teacher was supposed 
to be taking the class throughout the day.  This allowed 
the software to both fill in missing information, and to 
identify divergences from the schedule. The result of 
this process was a series of events (which corresponded 
to classes, for example, maths class), each of which had 
a set of associated messages (interactions during the 
event, divergence from schedule, etc.). 
 After the data analysis was completed, an NLG sys-
tem identified the events most interesting (to the child), 
using a heuristic that took into consideration both how 
inherent interesting an event was (for example, lunch 
was regarded as an inherently interesting event that 
children were likely to want to talk about) and also 
whether an event was unusual or not.  The latter is based 
on the observation that most personal narratives focus 
on unexpected or unusual events.  Unusual events were 
identified by the presence of recorded voice messages 
and by divergence from the timetable, e.g. a different 
teacher present or a different location. The system se-
lected the five most ?interesting? events and displayed 
them to the child in a simple visual editing interface.  In 
this interface the child could delete events he/she did 
not wish to talk about, and also annotate events with 
simple opinions (evaluations), such as I liked it, using 
the evaluation buttons on the interface, generating ap-
propriate utterances according to the last narrated event 
or message.(see Fig. 2).  
     When editing was finished, the NLG system generat-
ed texts describing the events and messages, which the 
child communicated using a simple narration interface 
(which was similar to the editing interface). Emphasis 
was placed on providing quick access to messages to 
minimize the length of pauses between utterances due to 
the physical accessing difficulties of the users. The 
narrative model is based on the Labov social narrative 
model (Labov 1972) which emphasizes the highpoint 
and evaluation. The dialogue model from beginning 
through to highpoint to the end with the user being able 
to add evaluations at any point of the narration. Stories 
are initially chronological order but interactively under 
the control of the user. This control of narration differs 
significantly from current AAC interventions where 
narrative tends to be output in a monologue format.  
 From an NLG perspective, the system was fairly 
straightforward. The most challenging microplanning 
tasks were choosing connectives, time phrases, lexical 
variety in embellishments, and pronouns based on dis-
course context. Connectives and time phrases were ne-
cessary since children could narrate events in different 
orders (for example, ?I went to maths.  Then I went to 
lunch? versus ?I went to lunch.  Before lunch, I went to 
maths?).  Document structuring  was simple because we 
Short range RFID reader 
and microphone for voice 
message recording 
Visual interface 
Access switch in head 
rest 
Long range RFID reader 
3
assumed that the children would choose their own order 
in which to narrate events. In fact some children are not 
able to do this; such children would need to be sup-
ported by a more sophisticated document planner that 
had a model of appropriate text structures in this do-
main. 
 We asked two children to use the system for one 
week for a qualitative formative evaluation.  Research-
ers supplied ongoing support during this, primarily trial 
observing how the children used the system, and dis-
cussing it with teachers, therapists and parents.  Gener-
ally it worked well for one child, Julie2, who had severe 
motor impairment (no independent means of mobility 
and interacted with a computer using a head switch with 
row/column scanning, see Fig. 1). Her expressive abili-
ties were limited but her comprehension skills were 
comparable to her non-disabled peers with some deve-
lopmental delay. The other child, Jessica, had more 
cognitive impairment, and found the interface too diffi-
cult. 
 
 
 
 
Fig. 2: Example screenshot from interface 
1: Navigation: Day and date of story, maximum of five 
story events, exit; 2: Event messages, numbers vary for 
each event. Here: 2 computer-generated messages, 3 
recorded messages, 1 user added evaluation; 
3: Sequential message navigation: previous, repeat, 
next; 4: Evaluation: delete event, negative evaluation, 
positive evaluation; 
 
   
 In a second evaluation, a third child, Eric, joined and 
all three children used the system over two weeks each. 
In this evaluation, we asked teachers and other staff to 
use the system without on-site support from the re-
searchers. This highlighted many practical usability 
issues, such as delays caused by starting the system in 
the morning, and problems caused by limited battery 
life. We eliminated the long-range RFID sensor because 
of its difficult setup; instead we asked staff to swipe 
                                                          
2
 The names of the children mentioned in this paper are 
changed to ensure anonymity. 
door cards when children entered rooms.  However, this 
strategy was not successful, as it was difficult for staff 
to remember to swipe both interactions and location 
changes. 
 The participants took the system home for use with 
their parents who gave positive feedback but also re-
ported issues with system usability (e.g. lack of access 
to stories from previous days) or suitability (too compli-
cated interface for Jessica).   
 Eric?s timetable was different from Julie and Jessi-
ca?s, because he visited college one morning a day, and 
we could not collect data during this period. Since some 
of the most exciting events in a school day happen out-
side the school building (sports and school trips as well 
as college), in the long term we do need to see if we can 
collect data outside as well as inside the school.  
3.2 HWST example 
     Julie used the system on her DynaVox? Vmax? 
Voice Output Communication Aid (VOCA) via head 
switch using row/column scanning. The above transcript 
shows an extract of a conversation Julie had with her 
Speech and Language Therapist (SLT) on day three 
about her experiences during day two. The researcher 
(RA) had been present all day for technical support. The 
conversation extract starts with Julie reporting about her 
morning break.  
     In this example Julie is able to quickly reply to con-
text related questions from her communication partner 
using the evaluations (?So what happened?? ? ?It was 
fun!?). Compared to conversations usually observed 
between aided and unaided partners Julie is able to con-
trol the conversation when starting a new topic after 
talking about the morning break, inviting her communi-
cation partner to prompt for more detailed information. 
Julie provides this with her next generated phrase. 
When she is asked about the event she replies with an 
evaluation the system has generated in relation to its 
previously generated message ?A visitor was there.?. 
We note that the system is able to refer to the correct 
gender of the visitor. 
 
1 Julie {next} [I had break.] 
2 Julie {next} [Lesley was there.] 
3 SLT Lesley was there?  
4 Julie ((Opens mouth in agreement, then turns back 
to screen)) 
5 SLT Ok mhmh. So what happened? 
6 Julie {positive evaluation} [It was fun.] 
7 SLT Oh good! ((laughs)) I?m glad to hear it! 
8 RA We like Lesley. 
9 SLT ((nods in direction of RA)) 
10 Julie ((smiles)) 
11 Julie:  {next} [Then I went to Junior Primary in-
stead of Reading Class.] 
1 
2 
3 
4 
4
12 SLT: Right, you went to Junior Primary? I wonder 
why that was?  
13 Julie: {next} [A visitor was there.] 
14 SLT:  Oh, a visitor, right. Wonder what the visitor 
was doing?  
15 Julie: {next} [?The dental hygienist came to give a 
talk.?] 
16 SLT: Oh, dental hygienist. 
17 Julie:  {previous} [A visitor was there.] 
18 SLT: That was the visitor, okay. That?s why you 
went to junior primary, uhm, what did you 
think of the talk? 
19 Julie:  {positive evaluation} [She was nice.] 
20 SLT: She was nice, that was good! ((laughs)) 
 
Notation: 
- Switch selected button by Julie: {curly brackets} 
- Natural speech: standard text.  
- Computer generated language accessed using one 
button: [standard text in square brackets].  
- Recorded messages accessed using one button: 
[?quoted standard text in square brackets?]. 
- Paralinguistic behaviors:  
((standard text in double brackets)).  
 
3.3  ?How was School Today? ? in the Wild 
We have now started a new project to further develop 
our work, called ?How was School today??? ? in the 
Wild (?in the wild? indicates that the focus is on how the 
technology works in a real school environment). The 
basic goal is to improve the system sufficiently so that it 
can be tried out over a period of several months, with 
children with varying levels and types of impairments; 
we will also work with several schools in the initial 
phase, although for practical reasons the evaluations 
may be at just one school. 
     During this project we will do some work on the 
issues described in Section 4; in particular we will try to 
make the system usable by children with different im-
pairments and ability levels (Section 4.1). This means 
having a very simple interface for children with consi-
derable cognitive impairments (such as Jessica); but 
also giving children with more cognitive abilities the 
opportunity to exert more control over the story (during 
both editing and narration), for example by supporting a 
richer range of annotations, and by making it easier to 
describe events and messages in any order. 
     Another intermediate goal is to improve the integra-
tion of voice messages entered by staff with the com-
puter-generated messages. This could be done by some 
combination of training staff to enter messages in a spe-
cific way (referring to the child in the first person); ask-
ing staff to annotate the messages so the computer 
knows something about their content; and/or using 
speech recognition to analyze the voice messages. In 
general there is a lot of interesting information that can 
only come from staff, and we need to think about the 
best way to help staff enter information in a way that is 
easy for them and useful for our system. 
 Now that a complete system is built, we are also able 
to thoroughly and formally evaluate the system. Mul-
tiple baseline single case study methodology will be 
used (Schlosser 2003) to evaluate the use and impact of 
our system. We intend to have up to four children (with 
varying ability levels) use the system for a period of 3 
months. This will give us a chance to observe the im-
pact of the system on the users and their environment 
such as the children?s interaction with the system and 
how staff at the school envisage using this new tool. 
The observations will be supported by semi-structured 
interviews with the children, their classroom teacher, 
their speech and language therapist and a parent. 
 We will look at the children?s conversations (with 
and without using our system) about interesting, staged 
events with different partners, analyzing them conversa-
tional characteristics such as narrative initiation, struc-
ture, length and evaluation. Analysis methods will 
include the Revised Edinburgh Functional Communica-
tion Profile (REFCP) (Wirz, Skinner et al 1990). 
     However, much of our focus will be on addressing 
the practical issues that make it difficult to use our cur-
rent research prototype over a period of months.  We 
have identified many such issues, both from our pre-
vious evaluations (Section 3.1) and also from a ques-
tionnaire that was distributed to school staff during an 
in-service day. 
      Location tracking ? There are problems with both 
of the techniques we have tried to date (automatically 
reading RFID tags on doors, and asking staff to swipe 
location information).  In this project we intend to try 
tracking the location of a child using Wi-Fi location 
tracking, which seems to be rapidly gaining popularity 
in the commercial world (Liu, Sen et al 2008). 
 Data entry, 2D bar codes ? We need to allow staff 
to easily enter and update information about the children 
(for example, their timetables) and sensor tags (e.g., if a 
new tag is given to a visitor).  For the latter, we want to 
investigate 2D bar codes, which could allow encoding 
of alphanumeric input data without reference to a cen-
tral database. 
 Portability, battery life ? The current system runs on 
a tablet PC (8?-12? touch screen, generic or VOCA 
hardware). During the evaluation, late powering up, run-
down batteries or simply forgetting a component caused 
significant data loss and usability issues. A future proto-
type should favor an ?always-on? system, such as a mo-
bile phone, allowing for easy portability and extended 
battery life. 
 Story generation ? The prototype system was only 
able to create a story towards the end of a day and gave 
5
only access to stories generated on that day. However, 
often the user desired to tell stories that had occurred on 
previous days, or to, say, tell a story at lunch that oc-
curred in the morning.  When data was insufficient for 
the system to create a story, the only output was an error 
message ?Can?t generate story right now.? This fru-
strated users, so future systems should be able to deliver 
a story with incomplete data.  
 Voice messages ? as mentioned above, we want to 
handle these in a more sophisticated way.  From a more 
practical perspective, we also want to make it easier for 
staff to listen to and change previously recorded mes-
sages.  We also want to allow parents to record messag-
es about events at home. 
4 Long-term vision and issues 
4.1 Supporting children with different levels 
and types of impairment 
A key issue in AAC is of course the diversity of AAC 
users. Children with CCN differ enormously in terms of 
cognitive ability, motor ability, and social ability. This 
was clear even in our initial evaluation where we 
worked just with two children, and discovered that our 
interface worked well for Julie but not Jessica. 
 Julie has little functional speech and severe physical 
impairments, and accesses her VOCA using a head 
switch through the slow process of scanning the inter-
face. Her VOCA interface consists of a grid of 15 to 30 
buttons per page, with more than 20 pages of vocabu-
lary. However, her cognitive skills were sufficient for 
her to master the interface on the second day. She used 
the system quite successfully, as shown in the example 
in Section 3.2.  
 Jessica also has severe physical impairments but 
does not use technology to support her communication 
(she has functional speech).  She has cognitive impair-
ments, which (amongst other things) affect her ability to 
remember and place events correctly in time. She had 
more difficulty mastering the interface than Julie. We 
simplified the interface for her (no editing, minimal 
control of narration), and then she displayed pragmatics 
known from typical language development in children, 
by telling her story with no room for interaction of her 
communication partner.  
 We also need to keep in mind that abilities are not 
static, but are likely to progress with age (see also Sec-
tion 4.2) and (hopefully) with the assistance of commu-
nication aids. For example, the WriteTalk project 
showed how pupils were both able to initiate and con-
trol communication more effectively with Talk:About? 
and how their formal writing skills improved over time 
(Waller et al, 1999). 
 In summary, some children may need a very simple 
interface because of cognitive impairments, but this 
should grow with them.  For example, the best narration 
tool for Jessica at her current stage of development is 
probably a single button that advances sequentially 
through the computer-generated story. The challenge is 
to provide an interface that Jessica can initially use via 
repeatedly pressing an ?Advance? button, but which 
gives her the possibility of exerting more control as her 
skills and abilities develop. 
 Other children (such as Julie) may have motor diffi-
culties that restrict the way in which they can interact 
with computer systems, and thus may require simple 
controls although they have reasonable cognitive skills. 
Restricted motor skills make certain tasks, such as en-
tering an arbitrary word, quite difficult and time con-
suming; hence the interface must avoid such tasks, and 
instead endeavor to give the child as much control as 
possible with a minimum amount of data entry.  Once 
these users master a basic story telling structure, it may 
help them develop their conversation skills if they use a 
wide variety of conversation patterns.  For this purpose, 
it may be worthwhile for the system to randomly vary 
the structure and language used in the narratives. 
 Still other children, for example on the autistic spec-
trum, may have good cognitive and motor abilities, but 
not have the experience of expressive communication 
necessary to develop interactive skills. These children 
are more likely to benefit from a system that supports 
the pragmatics of language in general and personal narr-
ative in particular.  For example, children on the high 
functioning end of autism may be comfortable with ra-
ther advanced software, which can help them adapt their 
storytelling according to the intended listener.  Indeed, 
giving these children more complex controls, if done 
correctly, can make the software fun and challenging in 
a positive way. 
     In the long term, as we broaden the range of children 
we work with, there may be overlaps between our work 
and research on tools to help typically developing child-
ren create stories, such as Robertson and Good (2005), 
and also between our work and research on tools to help 
adults with CCN tell personal narratives, such as Demp-
ster (2008).  Ideally it would be very nice to combine 
these efforts and create a story telling tool that could be 
used across the age and impairment spectrum. 
4.2 Narrative across the lifespan 
We would like our tool to be able to support children 
over time, as their abilities grow and as their expe-
riences accumulate. From the perspective of changing 
abilities, the challenge is to offer children an interface 
which is not only appropriate for their current stage of 
development (Sect 4.1), but also allows and indeed en-
6
courages them to exert more control over story content, 
language, and narration as their abilities grow. 
 We would also like our tool to become a repository 
of a child?s personal stories. The ability to relate rele-
vant stories can influence the quality of life, as well as 
social development and successful transitions. The life 
stories of people who use AAC are often held by parents 
and siblings (e.g., stories relating to health care 
(Hemsley, Balandin et al 2007)), and there is the inevit-
able concern that these stories and others are lost as 
parents age and siblings move away.  
 Technology has the potential both to support the 
acquisition of conversational skills for people who use 
AAC and to provide a repository for life stories. In the 
context of our work, it is essential that we provide ways 
of enabling children to develop their narrative skills so 
that they are more able to manage their own story repo-
sitory. In terms of development, young children will 
narrate recent stories regardless of conversational con-
text. By enabling the child to develop story structure by 
scaffolding interaction and enabling children to easily 
annotate stories, the child will begin to anticipate and 
control conversation. 
 Conversational narratives have traditionally not been 
supported by AAC tools partly due to the fact that they 
are so nebulous; they emerge during interactive conver-
sation (to date, events have to be manually input into a 
system and it is difficult to predict what events will be-
come a story); ?new? stories are repeated often (to date it 
is difficult to save conversation online); as stories age 
they are repeated in context (retrieval is often contextual 
e.g. topic based) and they grow longer having more em-
bellishments added to them. The technology we are de-
veloping provides an opportunity for children to access 
information about personal events over time, which they 
can communicate and narrate during a conversation. 
They can also evaluate (annotate) their stories, thereby 
embellishing and lengthening the stories.  However this 
will only be possible if the children can easily access 
previously experienced, generated and saved stories.  
 We can provide fast access to recent stories while 
anticipating the use of older stories such as for example 
those which closely match the current conversation top-
ic. In a research prototype called PROSE (Waller and 
Newell 1997), stories had to be physically tagged; there 
is now the potential to automate topic matching by re-
cognizing topic words spoken by a listener and parsing 
stored information for appropriate stories. Over a life-
time, some stories may fall into disuse, while others will 
be weighted more strongly depending on frequency of 
use and relevance. 
4.3 True dialogue in narration 
The ultimate goal of our research is to enable children to 
tell stories in the context of a social dialogue; for exam-
ple, we want children to be able to chat to their parents 
and other interested parties about what they did during 
the day.  
 Our current system incorporates a simple model of a 
conversation, where children are restricted at any point 
to choosing from a small number of options. The child 
chooses an event to talk about, and then goes through 
the sequence of messages associated with that event.  
The child has the freedom to switch to a different event, 
hence controlling the conversation, and to add annota-
tions/evaluations (for example ?it was fun!?). 
     This is adequate in many cases, but in the long term 
we would like to support more complex conversations; 
for example interrupting a discussion about today?s 
events to talk about what happened yesterday, or to dis-
cuss a particular teacher instead of an event.  We would 
also like children to easily be able to add conversational 
phrases, such as ?Guess what happened today at 
school?. 
 Because our children have motor and cognitive im-
pairments, we cannot present them with a large number 
of options for conversational moves. Ideally, the system 
would detect what the conversational partner wishes to 
talk about, and from this present the child with a small 
number of appropriate choices. For example, if the con-
versational partner asks the child what happened over 
the past week, our system would detect this and then 
give the child the option of talking about any individual 
weekday or the week in general. 
 One way of detecting what the conversational part-
ner intends is to use speech recognition and Natural 
Language Processing (NLP) technology to analyze what 
he or she says. Speech and NLP technology tend to 
work best when it is possible to train the system to the 
user?s voice, and also (in essence) train the user to un-
derstand what the speech/NLP system can and cannot 
do. This should be possible in our context, at least for 
people (such as parents) with whom the child regularly 
interacts. 
 Another possibility is to create a graphical user in-
terface for the conversational partner, perhaps on the 
same device that the child uses, which the partner could 
use to indicate what he/she wants to talk about.  This is 
probably technically easier, but does move away from 
the goal of having as natural a dialogue as possible. 
4.4 Pragmatics of interacting with others 
Currently, ?How was School today??? supports story-
telling between language-impaired children and adults 
who are the children?s parents, carers, teachers, and 
therapists.  But of course for normally developing child-
ren, many of their most important social interactions are 
with other children. 
 An interesting example here is the STANDUP sys-
tem, which was developed to help children who use 
7
AAC create and tell novel punning riddles. The study 
results suggested that children saved the jokes so that 
they could retell them to friends and family (Waller, 
Black et al 2009). Whilst the evidence is anecdotal, 
there did also appear to be a marked increase in joke 
telling by participants, both amongst their peers and 
with adults in the home environment. Hence STANDUP 
succeeded in supporting interaction with other children 
as well as with adults. 
 One of the key challenges in interacting with other 
children, and indeed with adults who are not formally 
involved in the care or teaching of the child, is to adapt 
the story to the interests of the recipients. In other 
words, a child?s parents and teachers will not insist on 
stories that are interesting to them, but other conversa-
tional partners will.  These conversational partners may 
also need additional information.  For example ?Jane 
came to take me to the OT room? makes more sense if 
the recipient knows that Jane is the occupational therap-
ist; parents and teachers already know this, but other 
people may need to be told this. Also if the conversa-
tional partner was present at an event, this should be 
acknowledged and indeed used in the story. For exam-
ple, "Did you really enjoy maths? I thought it was bor-
ing!? 
 In short, telling stories to peers and adults who do 
not know the child well requires adapting the story to 
the interests, knowledge, and involvement of the part-
ner; this is part of learning pragmatics. This is not some-
thing we are looking at currently, but it is something 
that we hope to look at in the future. 
4.5 Security and privacy issues 
We need to ensure that data about the children is private 
and secure.  Taken to its logical conclusion, our project 
would result in an intimate record of the child's life at 
school, home and beyond.  It is important that both the 
raw data and the generated content are under the control 
of the child and his/her guardians, with the child exer-
cising as much control as possible. This is especially 
important since children with learning difficulties are 
very vulnerable; there is potential for great harm if data 
about a child?s activities got into the hands of a mali-
cious outsider. 
 In a study on the software tool TalksBac, which 
supports personal narrative (Waller, Dennis et al 1997), 
privacy issues were coded along with stories. This al-
lowed the NLG process to decide the appropriateness of 
telling a story to a specific communication partner. 
Children in general do not care who they tell their sto-
ries to. Only when older children learn to distinguish 
which story is appropriate for a conversation partner. 
This process could be embedded into the prediction 
algorithm that presents stories for narration. Currently 
prediction on AAC devices only support character, word 
or phrase selection.   
 Another concern is information that is embarrassing 
or otherwise puts the child in a negative light; for exam-
ple, imagine a staff member entered the voice message 
"I refused to eat my lunch today".  We believe that the 
child should be free to delete such messages; she should 
never be forced to include material in a story that she 
does not want to include. 
 A related issue is whether we should allow stories 
generated for one child to use information acquired 
about another child.  In principle this is very valuable, 
for example it allows messages such as ?Jane didn?t eat 
her lunch today?. But is this acceptable from the pers-
pective of ensuring the privacy of data about Jane?s ac-
tivities? On the other hand, this is exactly the sort of 
thing that a normally developed child would say about a 
classmate. 
5 Conclusion 
In addition to having difficulty in communicating de-
sires and needs, language-impaired children also find it 
hard to participate in social linguistic interaction that 
would help create and build up friendships and other 
interpersonal relationships.  We believe that we can help 
these children participate in such interactions by giving 
them a tool that helps them tell a story about their day at 
school, by using an NLG system that has access to sen-
sor and other data about the child?s activities. We are 
still at an early stage in this work, but our initial proto-
type system has shown great potential to improve the 
quality of life of children with limited speech. Our cur-
rent work plans to explore this potential further while 
evaluating the efficacy of the system for four children 
with varying ability levels.  
Acknowledgements 
We would like to express our thanks to the children, 
their parents and staff and the special school where this 
project had its base. Without their valuable contribu-
tions and feedback this research would not have been 
possible. We would also like to thank DynaVox Sys-
tems Ltd for supplying the communication devices to 
run our system on. 
 This research was supported by the UK Engineering 
and Physical Sciences Research Council under grants 
EP/F067151/1, EP/F066880/1, EP/E011764/1, 
EP/H022376/1, and EP/H022570/1. 
8
References 
Agrawal, R. and Ramakrishnan, S. (2000) Privacy-
preserving data mining. ACM International 
Conference on Management of Data, pp. 439--450, 
Bart, H., V. Riny, et al (2008). LinguaBytes. 
Proceedings of the 7th international conference on 
Interaction design and children. Chicago, Illinois, 
ACM: 17-20. 
Beukelman, D. R. and P. Mirenda (2005). Augmentative 
and Alternative Communication: Management of 
Severe Communication Disorders in Children and 
Adults. Baltimore, Paul H. Brookes Publishing Co. 
Bruner, J. (1975). "From communication to language: A 
psychological perspective." Cognition 3: 255-289. 
Cheepen, C. (1988). The predictability of  informal 
conversation. Oxford, Printer Publishers Ltd. 
Clarke, H. H. and E. V. Clarke (1977). Psychology and 
Language. New York, Harcourt Brace Jovanovich. 
Dempster, M. (2008). Using natural language 
generation to encourage effective communication in 
nonspeaking people. Proceedings of Young 
Researchers Consortium, ICCHP'08. 
DeRuyter and Fried-Oken. (2010). "Context-sensitive 
messaging with RFID technology."   Retrieved 2010, 
April 10, from http://aac-rerc.psu.edu/index.php/projecttypes/list 
Gatt, A., F. Portet, et al (2009). "From Data to Text in 
the Neonatal Intensive Care Unit: Using NLG 
Technology for Decision Support and Information 
Management." AI Communications 22: 153-186. 
Goldberg, E., N. Driedger, et al (1994). "Using natural-
language processing to produce weather forecasts." 
IEEE Expert 9(2): 45-53. 
Harris, M. (2008). Building a Large-Scale Commer-cial 
NLG System for an EMR. Proc of INLG-2008. 
Hemsley, B., S. Balandin, et al (2007). "Family 
caregivers discuss roles and  needs in supporting 
adults with cerebral palsy and complex 
communication needs in the hospital setting." 
Journal of Developmental and Physical Disabilities 
19(2): 115-124. 
Higginbotham, D. J., H. Shane, et al (2007). "Access to 
AAC: Present, past, and future." Augmentative & 
Alternative Communication 23(3): 243-257. 
Labov, W. (1972). Language in the inner city: Studies in 
the Black English Vernacular. Philadelphia, 
University of Pennsylvania Press. 
Light, J., C. Binger, et al (1994). "Story Reading 
interactions between preschoolers who use AAC and 
their mothers." Augmentative and Alternative 
Communication 10: 255-268. 
Light, J. and K. Drager (2007). "AAC Technologies for 
Young Children with Complex Communication 
Needs: State of the Science and Future Research 
Directions." Augmentative and Alternative 
Communication 23(3): pp. 204 ? 216. 
Liu, X., A. Sen, et al (2008). A Software Client for Wi-
Fi Based Real-Time Location Tracking of Patients. 
Medical Imaging and Informatics. 
Berlin/Heidelberg, Springer. 4987/2008: 141-150. 
P?r?z, R. P. y. and M. Sharples (2004). "Three 
Computer-Based Models of StoryTelling: BRUTUS, 
MINSTREL, and MEXICA." Knowledge-Based 
Systems 17: 15-29. 
Peterson, C. and A. McCabe (1983). Developmental 
psycholinguistics: Three ways of looking at a child?s 
narrative. New York, Plenum. 
Reiter, E. (2007). An Architecture for Data-to-Text 
Systems. ENLG-2007. 
Reiter, E. and R. Dale (2000). Building Natural-
Language Generation Systems., Cambridge 
University Press. 
Reiter, E., S. Sripada, et al (2005). "Choosing Words in 
Computer-Generated Weather Forecasts." Artificial 
Intelligence 167: 137-169. 
Reiter, E., R. Turner, et al (2009). Using NLG to Help 
Language-Impaired Users Tell Stories and 
Participate in Social Dialogues. ENLG2009. Athens, 
Greece, Association for Computational Linguistics. 
Robertson, J. and J. Good (2005). "Story creation in 
virtual game worlds." Communications of the ACM 
48: 61-65. 
Schlosser, R. W. (2003). The Efficacy of Augmentative 
and Alternative Communication. San Diego, 
Elsevier Science. 
Soto, G., E. Hartmann, et al (2006). "Exploring the 
Elements of Narrative that Emerge in the 
Interactions between an 8-Year-Old Child who uses 
an AAC Device and her Teacher." Augmentative 
and Alternative Communication 22(4): pp. 231 - 
241. 
Sripada, S., E. Reiter, et al (2005). Evaluating an NLG 
System using Post-Edit Data: Lessons Learned. 
Proceedings of ENLG-2005, 10th European 
Workshop on Natural Language Generation, 
Aberdeen, Scotland. 
Waller, A. (2006). "Communication Access to 
Conversational Narrative." Topics in Language 
Disorders 26(3): 221-239. 
Waller, A., R. Black, et al (2009). "Evaluating the 
STANDUP Pun Generating Software with Children 
with Cerebral Palsy." ACM Trans. Access. Comput. 
1(3): 27. 
Waller, A., F. Dennis, et al (1997). "Evaluating the use 
of TalksBac, a predictive communication device for 
non-fluent aphasic adults." International Journal of 
Language and Communication Disorders 33: 45-70. 
Waller, A. and A. F. Newell (1997). "Towards a 
narrative based communication system." European 
Journal of Disorders of Communication 32: 289-
306. 
 
9
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic generation of conversational utterances and narrative for 
Augmentative and Alternative Communication: a prototype system 
Martin Dempster & Norman Alm Ehud Reiter 
School of Computing Computer Science Department 
University of Dundee University of Aberdeen 
Dundee, Scotland, DD1 4HN, UK Aberdeen, Scotland, AB24 3UE, UK 
m.k.dempster@dundee.ac.uk 
nalm@computing.dundee.ac.uk 
e.reiter@abdn.ac.uk 
 
 
 
Abstract 
We detail the design, development and evalua-
tion of Augmentative and Alternative Com-
munication (AAC) software which encourages 
rapid conversational interaction. The system 
uses Natural Language Generation (NLG) 
technology to automatically generate conver-
sational utterances from a domain knowledge 
base modelled from content suggested by a 
small AAC user group. Findings from this 
work are presented along with a discussion 
about how NLG might be successfully applied 
to conversational AAC systems in the future. 
1 Introduction 
Augmentative and Alternative Communication 
(AAC) systems assist non-speaking people communi-
cate. Reasons for lack of speech are varied and can be 
complex, but they are typically related to some pro-
found cognitive and/or motor impairment.  
Most AAC systems are computer based, utilize syn-
thesized speech output and employ a phrase-
construction approach to input. This approach requires 
the user to construct the majority of their utterances live 
during conversation. Undoubtedly this facilitated com-
munication is hugely important to those without natural 
speech. However, this process is often unacceptably 
slow and can lead to problematic and stilted interac-
tions, mostly due to the rapid nature of unimpeded face-
to-face communication. 
Previous work has shown that it is possible to hold 
mutually rewarding conversations using wholly pre-
stored material, known as the phrase-storage approach.  
Utterances are authored ahead of time and can be se-
lected and output immediately leading to quicker com-
munication rates. However, this approach suffers from 
several drawbacks which may have affected its more 
general adoption. 
Furthermore, Natural Language Processing (NLP) 
technology has proven to be a fruitful line of inquiry 
within the field. It has offered a powerful means to im-
prove system productivity and usability. We are current-
ly investigating how Natural Language Generation 
(NLG) might be applied in a useful way within an AAC 
device geared towards fast-paced and rewarding social 
interactions. It is hoped that the linguistic control and 
automaticity offered by NLG may go some way towards 
addressing the previous criticisms of pre-stored material 
regarding its inflexibility and cost in effort. 
2 Background 
2.1 Limitations of current AAC 
 High-tech AAC systems typically augment commu-
nication for non-speaking people by allowing live mes-
sage construction through some orthographic means. 
Completed messages are generally sent to a speech syn-
thesis engine for output during communication with 
others. Many people who require AAC have associated 
physical disabilities which reduce the speed achievable 
using input methods such as keyboards, pointing devic-
es or touch-screens. The rate achievable using most 
commercial AAC systems is highly dependent on the 
nature of the user?s disabilities but a generally accepted 
figure is in the region of 2-15 words per minute (Hig-
ginbotham, Shane et al 2007), at least an order of mag-
nitude slower than most natural speakers. 
This relatively slow rate of input is a crucial factor in 
some of the issues that arise in AAC-facilitated commu-
nication. Because of the effort and time required to 
create utterances, the user may not be able to construct 
messages quickly enough to take active roles in fast 
10
paced conversations. As a result users may become pas-
sive while also typically using a smaller communicative 
repertoire than natural speakers (Light 1988).  
Narrative, an important type of interpersonal com-
munication, is not well handled in most communication 
aids (Waller 1992). Delayed response and slow rate of 
aided-communication are correlated with higher inci-
dence of breakdown in communication and lesser per-
ceptions of the AAC user (Todman and Rzepecka 2003; 
McCarthy and Light 2005). This is primarily due to 
conflict between the relatively long time necessary to 
formulate an utterance and the fast paced nature of con-
versation.  
These problems are particularly critical in social con-
texts. AAC users typically have small social circles and 
are dependent on contact with families and carers. They 
often lack self-esteem and have negative self-image.  As 
a result, developing new relationships and experiencing 
new things can be difficult, despite being a major priori-
ty in their lives (Datillo, Estrella et al 2007). 
Some work has suggested that the use of pre-stored 
conversational material based on conversation models 
could help increase communication rate and conversa-
tion quality. Alm (1988) showed that it is possible to 
successfully model short ?chat? conversations involving 
greetings, personal enquiries and small-talk.  Further-
more, the TALK system allowed a user to pre-store a 
large volume of material on specific topics so that 
whole utterances could be selected and output. The sys-
tem also made heavy use of quick-fire phrases, classes 
of regularly used utterance which could be accessed 
quickly, and showed that communication using solely 
pre-stored material was viable (Todman and Alm 2003). 
Despite encouraging results and the development of a 
commercial product, the phrase-storage approach to 
social communication has not gained wide popularity. 
The reasons for this are complex, but include: the rela-
tive inflexibility of pre-stored material; the costs asso-
ciated with authoring the material and keeping the 
material up-to-date; and the vastly different nature of 
the approach and different training requirements neces-
sary to achieve success. 
2.2 The role of NLP in AAC 
NLP technology has provided many benefits to AAC 
system designers. Possibly the first technology to be 
included in many commercial systems to date was word 
prediction and completion. There have also been many 
research prototypes exploring the applicability of more 
emerging technologies such as named entity recognition 
from synthesized speech (Wisenburn and Higginbotham 
2008), the generation of well-formed utterances from 
telegraphic input (McCoy, Pennington et al 1998) and 
the automatic identification of contextual vocabulary 
from the web (Higginbotham, Bisantz et al 2008). 
Netzer and Elhadad (2006) used NLG to allow the se-
mantic authoring of utterances. 
However, NLG, in the sense of data-to-text (Reiter 
and Dale 2000), has had limited application within AAC 
thus far, although Reiter et al (2009) showed it is possi-
ble to generate stories from sensor data which allow a 
child using AAC to tell others about their day at school. 
2.3 System Rationale 
This project is exploring the use of NLG to produce 
conversational utterances in AAC systems designed for 
social interaction.  At the outset it was hoped that using 
NLG might address some of the difficulties observed in 
pre-storage systems. For instance, the generation com-
ponent could theoretically produce a range of utterances 
and speech act types automatically from the same un-
derlying data and adapt these somewhat to the interac-
tional context.  Using NLG would also have the benefit 
of offering control over the well-formedness of the out-
put, an important consideration given the difficulty 
some AAC users have in achieving literacy (Sandberg 
and Hjelmquist 1997). The fact that the system has an 
inherent awareness of the semantic content of the lin-
guistic output, rather than simply being stored as canned 
text, is also a potential benefit. In other words, NLG 
might offer a level of automaticity and flexibility that 
traditional pre-storage systems cannot offer, as well as 
potentially reducing the level of pre-authoring required 
from the user.  
3 System Development 
3.1 User-centered methodology 
To try to assess how useful NLG could be in this 
context we adopted a user-centered approach to the de-
sign of the system. A group of 3 AAC users has been 
recruited, all of whom currently use some form of high-
tech AAC. Literacy amongst the group is varied. Two of 
the individuals use the alphabetic keyboard-based 
Lightwriter communication device currently, and have 
normal cognitive and visual-perceptive skills.  All of the 
users have cerebral palsy and dysarthria, and have been 
involved in previous software evaluations.  
Weekly or twice weekly sessions were held with 
each user for several months while the software was 
being produced. Sessions consisted of various activities: 
11
discussion about the user?s ideas for the software and 
technology; the identification of topics and collation of 
input data to the system; demonstrations of the new fea-
tures or changes since the last session; system training; 
and dry-run test conversations between the investigators 
and the users. 
3.2 System Architecture 
A growing line of inquiry in the NLG community is 
the generation of language from ontologies (Mellish and 
Sun 2005).  An ontology is a logical and hierarchical 
model of the different concepts and the nature of rela-
tionships between concepts in a particular domain. 
These concepts and relationships can be mapped onto 
linguistic constructs to allow for the production of natu-
ral language descriptions (Karakatsiotis, Galanis et al 
2008) of parts of the ontology. 
In the case of our system, we are trying to model 
conversational topics that would be of interest in social 
conversation between users of the system and their co-
conversationalists. The current categories of topic we 
are experimenting with include travelling, listening to 
music, watching films and attending concerts.  Many 
categories are based on a simple event model which 
defines the basic characteristics common to all events, 
such as a time of occurrence (see Fig.1). We have also 
included concepts such as Person and Place which are 
associated with events to form a logical model of a par-
ticular event type.  
A separate file is created unique to each user which is 
linked to the original model. This is filled with individ-
uals consisting of data from the user. In other words, 
rather than defining the concept of an event as we did 
with the original ontology, here we are creating a de-
scription of an actual event and any other details, such 
as people or places, associated with it. We have defined 
our ontology in OWL, a standard language for the defi-
nition of ontologies, and each piece of knowledge is 
effectively stored as a RDF Triple consisting of a sub-
ject, predicate and object.   
 
 
 
 
 
 
 
 
 
 
Figure 1: The abstract event model 
The user?s knowledge base is turned into useful con-
versational utterances through a template-driven utter-
ance generation system (e.g. Van Deemter, Krahmer et 
al. 2005).  A large set of templates has been authored, 
using the SimpleNLG programming interface, which 
turn data from the onotlogy into natural language utter-
ances. The templates are created as concrete syntax 
trees containing unspecified ?slots? and parameters (See 
Fig.2).  These syntax trees map out the syntactic struc-
ture of the template), and are linked to a particular class 
in the ontology so that only appropriate templates are 
applied to each individual.   Slots are used to add con-
textually relevant clauses to our utterances. For exam-
ple, a template might contain a ?time?  slot, the contents 
of which are derived from the time of the event in ques-
tion.  For instance, the slot might be filled with ?next 
Tuesday evening?, ?a month ago? or ?this morning? 
depending on the context. Example parameters include 
the tense with which the utterance should be generated, 
and whether a pronoun or full noun phrase should be 
used to refer to the subject of the utterance. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: An example syntax tree with empty slots 
In addition to the language produced from the model 
and knowledge base we have included the ability to add 
canned text phrases to each individual.  
This is necessary because there may be things that 
you wish to be able say about a topic which it is not 
feasible to model.  Because we have a fairly diverse set 
of topics it is simply not possible to model all aspects of 
these topics in a reasonable time.  There is effectively a 
trade-off between complexity of the model and how 
maintainable and representative it is. A more complex 
model will lead to more expressive generated language, 
but will cost a great deal more to design and maintain. 
In the case of our system, a ?lowest common denomina-
tor? domain model combined with additional canned 
text has proven to be a relatively straightforward and 
inexpensive design. 
12
The system has also been designed to learn over time 
the sequences of utterances a user selects and suggest 
next moves based on past behavior.  The system does 
this by maintaining a directional weighted graph which 
records sequences of utterances as they are used.  The 
graph works by recording each individual utterance as a 
node in the graph and creating relationships between 
these nodes as they occur. The more often two utter-
ances appear in sequence the higher the value given to 
the edge between the two corresponding nodes. 
3.3 Conversation model and interface issues 
Perhaps the most challenging aspect of taking the 
system from initial concept to working prototype has 
been finding the most effective way of interfacing the 
technology. We have found that due to the complexity 
of the underlying technology, reaching the stage where 
generated utterances are both useful and accessible to 
the user during conversation has required careful con-
sideration and the trialing of several approaches with the 
user group. 
It was envisaged that the generative power of NLG 
would be its most powerful benefit. The system could 
realize the same piece of data as numerous speech act 
types and, within a speech type, in several different 
phrasings. This offered the ability to counteract the in-
flexibility and uniformity of pre-stored utterances 
somewhat.  However, we have had mixed success in 
achieving this goal as it has proven difficult to find an 
effective way to interface this enhanced choice and va-
riety to the user.  If there is a large volume of generated 
utterances available to choose from we must provide an 
efficient means by which the material is presented or 
organized so that the desired utterance can be located 
quickly. If a large choice results in a delayed selection 
and thus conversational turn, we may then lose any rate 
and speed of response benefits which would negate the 
need to use pre-stored and generated material at all. 
To address this problem, we attempted to design a 
conversational model which controlled the generation of 
utterances so that only the utterances deemed most like-
ly were presented to the user, thus reducing the cogni-
tive load required to search through a large set. This was 
done using a basic system where the templates were 
tagged according to where it might be most likely to be 
used in a conversation on a topic. For instance, a tem-
plate might produce a pre-sequence, an introduction, 
elaboration or concluding remark, or it may produce a 
interrogative. With the addition of historical sequential 
moves from our directional graph we could begin to 
present subsets of utterances to the user according to 
where they were in topic development. 
Another approach trialed was inspired by the Gricean 
maxim of quantity. Each template contains meta-data 
about the information it expresses. For each generated 
utterance selected, we can ?rule out? further generation 
of the same information. This is based on the assump-
tion that speakers will generally avoid repetition. We 
have found that this technique provides a useful way of 
supporting discourse coherence within conversations. 
Finally, using the logical model of topics we have 
created, it is possible to support and model stepwise 
topic progression. We can suggest, based on the model 
and the user knowledge base, other topics linked to the 
one currently selected. For example, if we were talking 
about an upcoming holiday to London with a friend 
called Bob we may want to the change topical perspec-
tive to related aspects of the trip. We might want to talk 
about London as a place, Bob as a friend, and other trips 
we have taken with Bob or to London. Because these 
concepts are all distinct within the model, they each 
have their own set of associated templates and result in 
sets of candidate utterances with differing perspectives. 
Navigating to related topics in this manner should be 
quicker since related topics do not have to be located 
manually. Although the users are still being trained in 
this approach to topic change, early evaluations are 
promising. It enables a ?one-click? transition to related 
topics, allowing the user to elaborate on certain aspects 
of a previous topic and respond quickly to questions 
from their conversational partners. 
Building on the last two mechanisms, we can also 
generate bridging phrases which allow for more cohe-
sive changes in topic. This allows for a more eloquent 
transition to a new topic and also aids the discourse co-
herence.  
All of these approaches in fact belie, to some degree, 
the complexity of conversation. By its very nature, con-
versation is unpredictable, and the purpose and meaning 
of sequential moves are highly dependent on their con-
text (Clark 1996). However, any form of context identi-
fication, such as speech recognition (Wisenburn and 
Higginbotham 2008), is likely to present a major tech-
nical challenge in any production AAC system at the 
current time. The above are simply at attempt to model, 
using the NLP/AI techniques available, aspects of 
communication process, to show the potential benefits 
when using NLG-produced utterances rather than sim-
ple canned text utterances. 
Application of some of the above techniques resulted 
in a highly fluid interface in which the utterances dis-
played changed rapidly according to the conversation 
model. This presented a major challenge to users learn-
ing the system, with all displaying a strong preference 
for a static interface where the same utterances could be 
found in the same location each time they were desired.  
13
Table 1: An example conversation produced 
using the system. Speaker A is the user and speaker 
B is an unaided speaker. The right-hand column 
shows the interface selections necessary prior to 
selecting the utterance from a set of possibilities. 
The marker G represents a generated utterance, C 
represents canned text. The remainder are quick-
fire utterances. 
We believe this does not suggest that use of such 
conversational models and semantic processing is not 
feasible, but simply that in the scope of the current 
work it has not been possible to fully evaluate their 
potential. Thus we have chosen to generate candidate 
phrases in a static manner without the predictive as-
pects described above. These changes have allowed for 
quicker achievement of proficiency and have lowered 
the cognitive effort required to navigate the interface.    
In the latest version of the software, we have de-
fined a set of templates for each topic which when 
realized in series produce a coherent narrative. They 
can still be selected individually by the user for output, 
so they retain ultimate control of what is said, but the 
utterances are presented in a natural order. This means 
that the user can easily make use of the utterances as a 
narrative or can choose according to the particular sit-
uation and context.  Any interrogative templates are 
displayed in a different part of the interface. We have 
set up a two column display so that interrogatives and 
other statements are clearly delineated.  
This approach has had very promising results as we 
have found that users no longer have to search through 
a list of suggestions which changes after each conver-
sational turn. They can also use the structured nature 
of the generated utterances to confidently introduce the 
different topics in conversation.  We are finding some 
evidence of increased self-selection at the end of their 
current turn as the user is easily able to continue their 
narrative automatically without having to worry about 
the location of their next turn in the interface. There is 
some other evidence of this structured application of 
NLG to narrative as being a promising area (Reiter, 
Turner et al 2009). 
We also believe that the passivity and lack of initia-
tion observed in AAC users could be positively ad-
dressed if AAC systems can better support a more 
varied communicative repertoire and suitable training 
is administered to show users how to confidently use 
these different constructs (e.g. Todman 2000). Early 
training sessions with our user group have again 
proved positive with increased use of the trained fea-
tures and interaction styles.  
 UTTERANCE USER 
SELECTION 
A: Hi Robert [GREET] 
B:      Oh, Hi. Nice to see you.  
A: And you. [GREET] 
A: How?s it going? [INTRO] 
B: Fine. And you?  
A: Not too bad. [INTRO] 
B: So you been keeping busy?  
A: Yeah [YES] 
A: I certainly have! [YES] 
 
A: 
 
I was out at a concert on Thursday 
night. (G) 
[GIGS] 
[Select ?Mar-
tin Taylor?] 
B: Great. Who did you go to see?  
 
A: 
 
Have you heard of Martin Taylor? 
(G) 
[ARTIST] 
[Select ?Mar-
tin Taylor?] 
B: No.....I don?t think so.  
A: He is a Jazz guitarist. (G) [Select ?Mar-
tin Taylor?] 
B: Oh, great. I like jazz music.  
A: Me too. [AGREE] 
B: So how was the concert?  
 
A: 
 
It was really good. (G) 
[GIGS] 
[Select ?Mar-
tin Taylor?] 
A: John and David came with me. (G)  
A: We all enjoyed it. (C)  
A: We had a bit of an interesting jour-
ney home because it was snowing 
heavily, but we made it back safe. 
(C) 
 
B:     Well that?s good news. Where was 
the concert? 
 
 
A: 
 
 
It was at the Tron Theatre in Glas-
gow. (G) 
I?ve been to Glasgow a few times 
lately. [G] 
[GIGS] 
[Select ?Mar-
tin Taylor?] 
[Select ?Glas-
gow?] 
A: Anyway, I best be getting on. [WRAP UP] 
A: It was great talking to you. [WRAP UP] 
B:      Yes, likewise.  
B: See you soon.  
A: OK. Cheerio. [FINISH] 
B: Bye  
14
  
 
 
 
 
 
 
 
 
 
 
3.4 Authoring user content 
Currently we have not managed to produce a tool that 
the user can use to update their knowledge base them-
selves. The ontology editing tool used in the program, 
Prot?g?, is a free academic software package designed 
for knowledge engineers and thus has a high degree of 
internal complexity and takes time to learn. It is also not 
a particularly accessible piece of software. 
We have worked with the users to build up their 
knowledge bases over a series of meetings by allowing 
them to suggest individuals to add while entering the 
details for them into the system. The process of defining 
new individual is very quick, usually requiring the input 
of just a few words and selection of the associated indi-
viduals. However, one of the main criticisms on the part 
of the users is that for the system to be useful in the long 
term, it must be kept up to date, as old material will 
quickly become less relevant and useful in less frequent 
situations.  For this reason it is critical to the success of 
any NLG-driven communication system that the data 
input is as simple and seamless as possible. 
We have shown in our system that it is possible to get 
some limited data automatically from online sources, 
rather than having to input it manually. Many web ser-
vices are being made available which enable program-
mers to access data from online services in their 
applications. For instance, both Amazon and YouTube 
have their own APIs which allow 3rd party applications 
to request content information from these services.  
The notion of the semantic web is also related to this. 
There is a large effort underway to define how we might 
structure and link information on the web in such a way 
that more of it can be processed automatically by com-
puters and made available in interchangeable formats. 
Shared data and semantic web technologies such as 
these operate on the same premise as our proposed 
communication system in that they map out the basic 
vocabulary required to describe a domain, and allow 
people describe aspects of the domain in these terms.  
We have used an API provided by social music web-
site Last.fm to show that it is possible to create relevant 
conversational utterances without any authoring re-
quirement whatsoever. By supplying the users Last.fm 
username, we can use a web service supplied by the site 
to query the user?s recent activity, for instance the songs 
they have listened to, songs rated highly or events which 
they have signed up to attend. Because the output from 
these services comes as structured XML document we 
can simply map it?s schema onto our own vocabulary 
and feed the appropriate data to our templates to pro-
duce utterances. 
If web services are to be used we must have an 
equivalent local vocabulary to which we can map the 
data returned from any queries we send the service. 
However, in the case of semantic web sources, for ex-
ample the FOAF (friend-of-a-friend) vocabulary (Brick-
ley) describing online social networks, the process is 
simpler as we can simply use the pre-existing vocabu-
lary standard ourselves rather than having to develop 
our own. Despite the semantic web being in its infancy, 
the notion of shared data is growing in popularity and 
many popular websites and organizations are providing 
access to their information in a structured way. 
One problem with using these types of data acquisi-
tion methods for our purposes is that the data is largely 
generic and any personal opinion or evaluative informa-
tion personal to the user is limited. In some cases we 
may be able to query the data source for a rating 
awarded to a particular piece of content, for instance the 
star rating system on YouTube, but it is not clear how 
expressive the produced language will be since the 
process is likely to be a simple mapping from the rating 
to a suitable adjective. As in our system, the potential 
usefulness of the generated language is likely to be in-
creased if it is possible for the user to annotate the top-
ics with their own canned text expressions and 
evaluations. This will enable the system to express more 
of the individual?s personality and opinions. 
We believe this is an area of great interest for AAC. 
There is growing evidence of the importance of the in-
ternet in the lives of disabled people, particularly its role 
as a communication medium for people with communi-
cation impairments (Cohen 1999). By harnessing the 
large volumes of data created when using modern hard-
ware and software systems and transforming it into use-
ful utterances, we can begin to address one of the main 
criticisms of whole utterance approaches to AAC since 
there would be no authoring requirement on the part of 
the users.   This is certainly by no means a simple 
process and this approach will require further investiga-
tion, but as semantic web technologies reach maturity 
Figure 3 - System interface 
15
and gain wider adoption it should be clearer what the 
potential of the technology is. 
4 Formal Evaluation Methodology 
In our evaluations so far, we have concentrated on 
training the users in its operation, updating conversa-
tional material and implementing changes based on the 
user feedback. We have recently begun testing the sys-
tem in real conversational encounters and the results 
have been promising. We have found it is possible to 
hold pleasing conversations lasting up to 20 minutes 
with unfamiliar partners, with the aided communicator 
achieving a rate of upwards of 40 wpm. 
There also seems to be higher incidences of initia-
tions on the part of the user, with them making good use 
of both the scripted NLG material, the quick fire phrases 
and their own pre-stored material. The topic progression 
feature is currently being underused but subjects are 
responding well to training sessions on how to incorpo-
rate this to reduce their response time and expand on 
topics to extend the amount they are able to say. 
Formal evaluations are now being undertaken. An 
AB multiple-baseline study design is being conducted in 
which each aided communicator has a series of conver-
sations with 12 unknown and unaided conversation 
partners.  In the A condition, the aided participants use 
their existing AAC system, while in the B condition 
they use our prototype system. Each conversation will 
be limited to approximately 10 minutes, and the ses-
sions will be split across a three non-consecutive days to 
avoid user fatigue.  
There will be at least 3 conversations in both the A 
and B conditions, and the intervention point will be ran-
domized across the remaining 6 conversations to allow 
for valid inferences to be made despite the small n value 
(Todman and Dugard 2001). This also reduces the bias 
introduced by any training effects and avoids the need 
to use a response-guided intervention after baseline per-
formance has been established. The difficulty and ex-
pense of recruiting large numbers of subjects in AAC 
studies is a known problem (Higginbotham 1995) and 
therefore any findings from quantitative analysis per-
formed cannot be generalized across the AAC popula-
tion. However, we expect to be able to achieve a p value 
using the randomization design of <0.05 so the results 
should at least be internally robust and give a good indi-
cation of whether further investigation is warranted.   
The conversations will be audio-recorded and ana-
lyzed for a number of metrics. Primarily we are interest-
ed in measuring the rate at which people are able to 
communicate using the new system as this seems to be 
one of the clearest indicators of success when evaluating 
a new AAC intervention. We expect to the effect size 
observed across the conditions to be large.  
We are also particularly interested whether it is poss-
ible to use automatically generated material while main-
taining or enhancing the enjoyment and quality of the 
encounter for all participants.  It is still unclear how 
acceptable generated material will be to the user so we 
will measure the relative frequency of generated and 
canned-text utterances.  
In previous studies it has also been shown that the 
use of a whole-utterance approach can change the dy-
namics of communication, such as relative speech act 
distribution and number and type of initiation, so we are 
interested to see how the availability generated material 
might impact this and what role it might play. A coding 
schema based on Wang (2007) will be used to categor-
ize the utterances used. 
We are also asking the aided and unaided conversa-
tion partners to complete questionnaires regarding vari-
ous subjective ratings of the interactions and, in the case 
of the unaided speakers, impressions of the aided com-
municator. The questions will be based on a re-
evaluation of those suggested by Todman (2000) and 
answers will be requested on a 7-point rating scale. Pre-
vious work has shown that quicker, flowing interactions 
with less breakdowns or delays can lead to more re-
warding interactions for both participants. We expect to 
observe these effects in our system but it?s as yet un-
clear what impact the automatically generated phrases 
will have, if any, on perceptions of the user.  
Although the relatively small number of participants 
means it is unlikely that we will be able to make robust 
inferences from this data, we hope that results will be 
indicative of the naturalness and acceptability of auto-
matically generated utterances. 
5 Discussion & Future Plans 
One of the primary reasons that AAC systems featur-
ing NLP technology prove useful is that they go some 
way to leveling the playing field for many users. They 
have the potential to support the user in ways which 
reduce the effort required to communicate yet may im-
prove the quality of the communication. There are many 
NLP technologies, such as NLG, that deserve further 
attention within the field of AAC to determine what 
they can offer.  
Although our system has shown some encouraging 
preliminary results there are still many unanswered 
questions with regards to the role NLG can play.  For 
example, it is not clear how appealing NLG utterances 
16
are to use. Given that the user has not authored the form 
of the utterances themselves there is an argument that 
using them may feel unnatural. After the formal evalua-
tions we should be able provide analysis indicating 
whether NLG phrases are being used and in which sit-
uations they are proving most useful.  
One of the most challenging aspects of designing the 
system was the HCI challenge of incorporating some of 
these technologies. While it is obvious to the user that 
phrases are being generated automatically, and that 
these phrases are generated when a topic is selected, it is 
still important to note that the technologies have been 
intentionally kept largely transparent to the user. When 
using a communication system, the most important 
thing is the ability to say what you want to say, but is 
not yet clear whether the technical nature of the soft-
ware  may be an alienating factor since the user current-
ly has no access to the template construction or domain 
modeling aspects of the system. 
At the current time, the domain modeling and tem-
plate construction processes are quite complex and ex-
pensive. Tools are becoming available, from the NLG 
community, which go some way to addressing the diffi-
culty of interfacing these types of technology to non-
experts (Bilidas, Theologou et al 2007; Power, Stevens 
et al 2009) but these are largely unsolved problems. 
Domain modeling itself is problematic in that one 
persons notion of what defines a particular concept is 
often different to someone else?s. For instance, one per-
son?s idea of sport might encompass the sporting activi-
ties they take part in, while another person?s idea of 
sport is that which they follow or watch on the televi-
sion. This has clear implications for the general usabili-
ty of the system.  Using semantic web vocabularies may 
address this somewhat since they are likely to be more 
specific to a particular purpose and be more mature and 
interoperable than the ?home-brew? domain models we 
have used for the prototype. 
Using whole-utterance approaches to communication 
clearly requires the adoption of a different mindset. Ra-
ther than being able to construct a novel message the 
user has to ?make do? with whatever is available in the 
system. Despite the advantages observed while using 
such systems, they have still not become generally pop-
ular. It is likely that any NLG whole utterance system 
would similarly not gain immediate acceptance because 
it is vastly different to other systems and approaches to 
communication available. To some degree we are ask-
ing the user of our NLG system to think in an object 
orientated manner since they must understand the un-
derlying model and the way the concepts are structured 
to make the most of the system. Again it is not yet clear 
how natural this process is and how much training is 
required to become an expert user of such a system. 
However perhaps the major strength of these types of 
system is the way in which they help scaffold interac-
tion so the AAC user can be much more active in con-
versation and use an increased repertoire. The design of 
the software is such that it encourages the use of types 
of phrases often underused by AAC users, for example, 
initiations, elaborating moves, questions and the differ-
ent classes of quick-fire remarks. One interesting ques-
tion is whether the use of NLG might make it easier to 
encourage the user to use new types of conversational 
move. Since no full text-authoring is required the user 
does not even have feel confident authoring the utter-
ance, it is simply provided and can be used or experi-
mented with.  Scaffolding interactions in this way may 
be one of the most interesting avenues for NLP and AI 
technologies with AAC in the future. 
 The architecture of the prototype, although effective, 
lacks efficiency and may be difficult to reuse. A great 
deal of work is being done by NLG researchers investi-
gating how NLG architectures might be made more 
modular and reusable. This is an ongoing problem but it 
seems sensible to consider how a pipeline architecture 
(Reiter and Dale 2000) might work in practice for this 
type of system. 
At the moment, the system requires a reasonable lev-
el of literacy because the interface is mainly text based. 
However, semiotic systems are preferred because of the 
literacy problems observed in many AAC users. It is not 
clear how NLG may impact on semiotic message con-
struction but systems such as Compansion (McCoy, 
Pennington et al 1998) show there may useful applica-
tions in this area too. 
6 Conclusion 
Despite having only been able to perform informal 
evaluations so far, we believe we have seen some en-
couraging signals that NLG may have potential as an 
augmentative communication technology to assist in 
generating conversational utterances.  We believe that 
the rapid access to well-formed, contextually generated 
material offered in our system could lead to significant 
benefits for the AAC user and their interlocutors. 
There are further exciting possibilities with regards to 
the technology, particularly the ability to harvest per-
sonal data from the internet and other computer usage 
so that it can be transformed into useful phrases for in-
clusion in communication aids. We hope to have a rich-
er set of data and results in the coming months after the 
system training and formal evaluations have been com-
pleted.  
17
7 References 
Alm, N. A. (1988). Towards a Conversation Aid for 
Severely Phisically Disabled Non-Speaking People. 
Applied Computing Department. Dundee, University 
Of Dundee. Doctor Philosophy: 197. 
Bilidas, D., M. Theologou, et al (2007). Enriching 
OWL Ontologies with Linguistic and User-related 
Annotations: the ELEON system. 19th IEEE Interna-
tional Conference on Tools with Artificial Intelli-
gence, IEEE. 
Brickley, D. (25.2.10). "FOAF Vocabulary Specifica-
tion ", from http://xmlns.com/foaf/0.1/  
Clark, H. H. (1996). Using Language, Cambridge Uni-
versity Press. 
Cohen, K. J. (1999). Using the Internet to Empower 
Augmented Communicators. CSUN'99. 
Datillo, J., G. Estrella, et al (2007). ""I have chosen to 
live life abundantly": Perceptions of leisure by adults 
who use Augmentative and Alternative Communica-
tion." Augmentative & Alternative Communication 
24(1): 16-28. 
Higginbotham, D. J. (1995). "Use of nondisabled sub-
jects in AAC Research : Confessions of a research in-
fidel." Augmentative and Alternative Communication 
11(1): 2-5. 
Higginbotham, D. J., A. M. Bisantz, et al (2008). "The 
Effect of Context Priming and Task Type on Augmen-
tative Communication Performance." Augmentative & 
Alternative Communication. 
Higginbotham, D. J., H. Shane, et al (2007). "Access to 
AAC: Present, past, and future." Augmentative & Al-
ternative Communication 23(3): 243-257. 
Karakatsiotis, G., D. Galanis, et al (2008). Natura-
lOWL: Generating Texts from OWL Ontologies in 
Protege and in Second Life. 18th European Confe-
rence on Artificial Intelligence. 
Light, J. (1988). "Interaction Involving Individuals us-
ing Augmentative and Alternative Communication 
Systems: State of the Art and Future Directions." 
Augmentative and Alternative Communication 4(2): 
66-82. 
McCarthy, J. and J. Light (2005). "Attitudes towards 
individuals who use Augmentative and Alternative 
Communication: Research Review." Augmentative 
and Alternative Communication 21(1): 41-55. 
McCoy, K. F., C. A. Pennington, et al (1998). "Com-
pansion: From research prototype to practical integra-
tion." Natural Language Engineering 4(1): 73-95. 
Mellish, C. and X. Sun (2005). The Semantic Web as a 
Linguistic Resource: Opportunities for Natural Lan-
guage Generation. International Conference on theory, 
practical and application of Artificial Intelligence. M. 
Bramer, F. Coenen and T. Allen. Cambridge, UK, 
Springer: 77. 
Netzer, Y. and M. Elhadad (2006). Using Semantic Au-
thoring for Blissymbols Communication Boards. 
HLT-2006. 
Power, R., R. Stevens, et al (2009). Editing OWL 
through generated CNL. Workshop on Controlled 
Natural Language (CNL'09). Marettimo Island, Italy. 
Reiter, E. and R. Dale (2000). Building Natural Lan-
guage Generation Systems. Cambridge Cambridge 
University Press. 
Reiter, E., R. Turner, et al (2009). Using NLG to help 
language-impaired users tell stories and participate in 
social dialogues. Proceedings of the 12th European 
Workshop on Natural Language Generation. Athens, 
Greece, ACL. 
Sandberg, A. D. and E. Hjelmquist (1997). " Language 
and literacy in nonvocal children with cerebral palsy." 
Reading and Writing 9(2): 107-133. 
Todman, J. (2000). "Rate and quality of conversations 
using a text-storage AAC system: Single-case training 
study." Augmentative and Alternative Communication 
16: 164-179. 
Todman, J. and N. A. Alm (2003). "Modelling conver-
sational pragmatics in communication aids." Journal 
of Pragmatics(35): 523-538. 
Todman, J. and P. Dugard (2001). Single-case and 
small-n experimental designs: A practical guide to 
randomisation tests. Mahwah, NJ, Lawrence Erlbaum 
Associates. 
Todman, J. and H. Rzepecka (2003). "Effect of pre-
utterance pause length on perceptions of communica-
tive competence in AAC-aided social conversations." 
Augmentative and Alternative Communication 19(4): 
222-234. 
Van Deemter, K., E. Krahmer, et al (2005). "Plan-based 
vs. Template-based NLG: A false opposition?" Com-
putational Linguistics 31(1). 
Waller, A. (1992). Providing Narratives in an Augmen-
tative Communication System. Applied Computing. 
Dundee, University Of Dundee. Doctor of Philoso-
phy: 163. 
Wang, Y. (2007). A model of conversational structure 
for augmentative and alternative communication 
(AAC) systems. University of Dundee, Unpublished 
PhD Thesis. PhD. 
Wisenburn, B. and D. J. Higginbotham (2008). "An 
AAC Application Using Speaking Partner Speech 
Recognition to Automatically Produce Contextually 
Relevant Utterances: Objective Results." Augmenta-
tive and Alternative Communication 24(2): 100-109. 
 
 
 
18
Natural Reference to Objects in a Visual Domain
Margaret Mitchell
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
Kees van Deemter
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
{m.mitchell, k.vdeemter, e.reiter}@abdn.ac.uk
Ehud Reiter
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
Abstract
This paper discusses the basic structures
necessary for the generation of reference
to objects in a visual scene. We construct
a study designed to elicit naturalistic re-
ferring expressions to relatively complex
objects, and find aspects of reference that
have not been accounted for in work on
Referring Expression Generation (REG).
This includes reference to object parts,
size comparisons without crisp measure-
ments, and the use of analogies. By draw-
ing on research in cognitive science, neu-
rophysiology, and psycholinguistics, we
begin developing the input structure and
background knowledge necessary for an
algorithm capable of generating the kinds
of reference we observe.
1 Introduction
One of the dominating tasks in Natural Language
Generation (NLG) is the generation of expressions
to pick out a referent. In recent years there has
been increased interest in generating referential
expressions that are natural, e.g., like those pro-
duced by people. Although research on the gener-
ation of referring expressions has examined differ-
ent aspects of how people generate reference, there
has been surprisingly little research on how people
refer to objects in a real-world setting. This paper
addresses this issue, and we begin formulating the
requirements for an REG algorithm that refers to
visible three-dimensional objects in the real world.
Reference to objects in a visual domain pro-
vides a straightforward extension of the sorts of
reference REG research already tends to consider.
Toy examples outline reference to objects, peo-
ple, and animals that are perceptually available
before the speaker begins generating an utterance
(Dale and Reiter, 1995; Krahmer et al, 2003; van
Deemter et al, 2006; Areces et al, 2008). Exam-
ple referents may be referred to by their color, size,
type (?dog? or ?cup?), whether or not they have a
beard, etc.
Typically, the reference process proceeds by
comparing the properties of the referent with the
properties of all the other items in the set. The
final expression roughly conforms to the Gricean
maxims (Grice, 1975).
However, when the goal is to generate natural
reference, this framework is too simple. The form
reference takes is profoundly affected by modality,
task, and audience (Chapanis et al, 1977; Cohen,
1984; Clark and Wilkes-Gibbs, 1986), and even
when these aspects are controlled, different people
will refer differently to the same object (Mitchell,
2008). In light of this, we isolate one kind of nat-
ural reference and begin building the algorithmic
framework necessary to generate the observed lan-
guage.
Psycholinguistic research has examined refer-
ence in a variety of settings, which may inform
research on natural REG, but it is not always clear
how to extend this work to a computational model.
This is true in part because these studies favor an
analysis of reference in the context of collabora-
tion; reference is embedded within language, and
language is often a joint activity. However, most
research on referring expression generation sup-
poses a solitary generating agent.1 This tacitly
assumes that reference will be taking place in a
monologue setting, rather than a dialogue or group
setting. Indeed, the goal of most REG algorithms
is to produce uniquely distinguishing, one-shot re-
ferring expressions.
Studies on natural reference usually use a
two person (speaker-listener) communication task
(e.g., Flavell et al, 1968; Krauss and Glucksberg,
1969; Ford and Olson, 1975). This research has
1A notable exception is Heeman and Hirst (1995).
shown that reference is more accurate and efficient
when it incorporates things like gesture and gaze
(Clark and Krych, 2004). There is a trade-off in
effort between initiating a noun phrase and refash-
ioning it so that both speakers understand the ref-
erent (Clark and Wilkes-Gibbs, 1986), and speak-
ers communicate to form lexical pacts on how
to refer to an object (Sacks and Schegloff, 1979;
Brennan and Clark, 1996). Mutual understanding
of referents is achieved in part by referring within
a subset of potential referents (Clark et al, 1983;
Beun and Cremers, 1998). A few studies have
compared monologue to dialogue reference, and
have shown that monologue references tend to be
harder for a later listener to disambiguate (Clark
and Krych, 2004) and that subsequent references
tend to be longer than those in dialogues (Krauss
and Weinheimer, 1967).
Aiming to generate natural reference in a mono-
logue setting raises questions about what an algo-
rithm should use to produce utterances like those
produced by people. In a monologue setting, the
speaker (or algorithm) gets no feedback from the
listener; the speaker?s reference is not tied to in-
teractions with other participants. The speaker
is therefore in a difficult position, attempting to
clearly convey a referent without being able to
check if the reference is understood along the way.
Recent studies that have focused on monologue
reference do so rather explicitly, which may af-
fect participant responses. These studies utilize
2D graphical depictions of simple 3D objects (van
Deemter et al, 2006; Viethen and Dale, 2008),
where a small set of properties can be used to dis-
tinguish one item from another. The expressions
are elicited in isolation, typed and then submitted,
which may hide some of the underlying referen-
tial processes. None of these studies utilize actual
objects. It is therefore difficult to use these data
to draw conclusions about how reference works in
naturalistic settings. It is unclear if these experi-
mental settings are natural enough, i.e., if they get
at reference as it may occur every day.
The study in this paper attempts to bring out in-
formation about reference in a number of ways.
First, we conduct the study in-person, using real-
world objects. This design invites referential phe-
nomena that may not have been previously ob-
served in simpler domains. Second, the refer-
ring expressions are produced orally. This allows
us access to reference as it is generated, without
the participants revising and so potentially obscur-
ing information about their reference. Third, we
use a relatively complicated task, where partici-
pants must explain how to use pieces to put to-
gether a picture of a face. The fact that we are
looking at reference is not made explicit, which
lessens any experimental effects caused by sub-
jects guessing the purpose of the study. This ap-
proach also situates reference within a larger task,
which may draw out aspects of reference not usu-
ally seen in experiments that elicit reference in iso-
lation. Fourth, the objects used display a variety
of different features: texture, material, color, size
along several dimensions, etc. This brings the data
set closer to objects that people interact with every
day. A monologue setting offers a picture of the
phenomena at play during a single individual?s re-
ferring expression generation.
The referring expressions gathered in this study
exhibit several aspects of reference that have not
yet been addressed in REG. This includes (1) part-
whole modularity; (2) size comparisons across
three dimensions; and (3) analogies. Work in cog-
nitive sciences suggests that these phenomena are
interrelated, and may be possible to represent in a
computational framework. This research also of-
fers connections to further aspects of natural refer-
ence that were not directly observed in the study,
but will need to be accounted for in future work on
naturalistic referring expression generation. Us-
ing these ideas, we begin formulating the struc-
tures that an REG algorithm would need in order
to produce reference to real-world objects in a vi-
sual setting.
Approaching REG in this way allows us to tie
research in the generation of referring expressions
to computational models of visual perception and
cognitively-motivated computer vision. Moving in
this direction offers the prospect of eventually de-
veloping an application for the generation of nat-
ural reference to objects automatically recognized
by a computer vision system.
In the next section, we describe our study. In
Section 3, we analyze the results and discuss what
they tell us about natural reference. In Section 4,
we draw on our results and cognitive models of ob-
ject recognition to begin building the framework
for a referring expression algorithm that generates
naturalistic reference to objects in a visual scene.
In Section 5, we offer concluding remarks and out-
line areas for further study.
Figure 1: Object Board.
2 Method
2.1 Subjects
The subjects were 20 residents of Aberdeen, Scot-
land, and included undergraduates, graduates, and
professionals. All were native speakers of English,
had normal or corrected vision, and had no other
known visual issues (such as color-blindness).
Subjects were paid for their participation. Two
recordings were left out of the analysis: one par-
ticipant?s session was not fully recorded due to a
software error, and one participant did not pick out
many objects in each face and so was not included.
The final set of participants included 18 people, 10
female and 8 male.
2.2 Materials
A board was prepared with 51 craft objects. The
objects were chosen from various craft sets, and
included pom-poms, pipe-cleaners, beads, and
feathers (see Table 1). The motley group of objects
had different colors, textures, shapes, patterns, and
were made of different materials. Similar objects
were grouped together on the board, with a label
placed underneath. This was done to control the
head noun used in each reference. The objects
were used to make up 5 different craft ?face? pic-
tures. Subjects sat at a desk facing the board and
the stack of pictures. A picture of the board is
shown in Figure 1.
Subjects were recorded on a head-mounted mi-
crophone, which fed directly into a laptop placed
on the left of the desk. The open-source audio-
recording program Audacity (Mazzoni, 2010) was
used to record the audio signal and export it to
wave format.
2.3 Procedure
Subjects were told to give instructions on how to
construct each face using the craft supplies on the
board. They were instructed to be clear enough for
a listener to be able to reconstruct each face with-
out the pictures, with only the board items in front
of them. A pilot study revealed that such open-
ended instructions left some subjects spending an
inordinate amount of time on the exact placement
of each piece, and so in the current study sub-
jects were told that each face should take ?a cou-
ple? minutes, and that the instructions should be
as clear as possible for a listener to use the same
objects in reconstructing the pictures without be-
ing ?overly concerned? with the details of exactly
how each piece is angled in relation to the other.
Subjects were first given a practice face to de-
scribe. This face was the same face for all subjects.
They were then allowed to voice any concerns or
ask questions, but the experimenter only repeated
portions of the original instructions; no new infor-
mation was given. The subject could then proceed
to the next four faces, which were in a random or-
der for each subject. A transcript of a single face
from a session is provided in Figure 2.
2.4 Analysis
The recordings of each monologue were tran-
scribed, including disfluencies, and each face sec-
tion (?eyes?, ?chin?, etc.) was marked. First refer-
ence to items on the board were annotated with
their corresponding item numbers, yielding 722
references.2 Initial references to single objects
were extracted, creating a final data set with 505
references to single objects.
3 Results
Each reference was annotated in terms of the prop-
erties used to pick out the referent. For exam-
ple, ?the red feather? was annotated as contain-
ing the <ATTRIBUTE:value> pairs <COLOR:red,
TYPE:feather>. Discerning properties from the
modifiers used in reference is generally straight-
forward, and all of the references produced may
be partially deconstructed using such properties.
2This corpus is available at
http://www.csd.abdn.ac.uk/?mitchema/craft corpus.
14 foam shapes 2 large red hearts 2 small red hearts 2 small neon green hearts
2 small blue hearts 1 small green heart 1 green triangle 1 red circle
1 red square 1 red rectangle 1 white rectangle
11 beads 4 large round wooden beads 2 small white plastic beads 2 brown patterned beads
1 gold patterned bead 1 shiny gold patterned heart 1 red patterned heart
9 pom poms 2 big green pom-poms 2 small neon green pom-poms 2 small silver pom-poms
1 small metallic green pom-pom 1 large white pom-pom 1 medium white pom-pom
8 pipe cleaners 1 gold pipe-cleaner 1 gold pipe-cleaner in half 1 silver pipe-cleaner
1 circular neon yellow soft pipe-cleaner 1 neon orange puffy pipe-cleaner 1 grey puffy pipe-cleaner
1 purple/yellow striped pipe-cleaner 1 brown/grey striped pipe-cleaner
5 feathers 2 purple feathers 2 red feathers 1 yellow feather
3 ribbons 1 gold sequined wavy ribbon 1 silver wavy ribbon 1 small silver wavy ribbon
1 star 1 gold star
Table 1: Board items.
<CHIN> Okay so this face again um this face has um uh
for the chin, it uses (10 a gold pipe-cleaner in a V shape)
where the bottom of the V is the chin. </CHIN>
<MOUTH> The mouth is made up of (9 a purple feather).
And the mouth is slightly squint, um as if the the person
is smiling or even smirking. So this this smile is almost
off to one side. </MOUTH>
<NOSE> The nose is uh (5 a wooden bead, a medium-
sized wooden bead with a hole in the center). </NOSE>
<EYES> And the eyes are made of (2,3 white pom-poms),
em just uh em evenly spaced in the center of the face.
</EYES>
<FOREHEAD> Em it?s see the person?s em top of the per-
son?s head is made out of (1 another, thicker pipe-cleaner
that?s uh a grey color, it?s kind of uh a knotted blue-type
pipe-cleaner). So that that acts as the top of the person?s
head. </FOREHEAD>
<HAIR> And down the side of the person?s face, there are
(7,8 two ribbons) on each side. (7,8 And those are silver
ribbons). Um and they just hang down the side of the face
and they join up the the grey pipe-cleaner and the top um
of the person?s head to the to the chin and then hang down
either side of the chin. </HAIR>
<EARS> And the person?s ears are made up of (4,6 two
beads, which are um love-heart-shaped beads), where the
points of the love-hearts are facing outwards. And those
are just placed um around same em same em horizontal
line as the nose of the person?s face is. </EARS>
Figure 2: Excerpt Transcript.
Using sets of properties to distinguish referents
is nothing new in REG. Algorithms for the genera-
tion of referring expressions commonly use this as
a starting point, proposing that properties are orga-
nized in some linear order (Dale and Reiter, 1995)
or weighted order (Krahmer et al, 2003) as input.
However, we find evidence that more is at play. A
breakdown of our findings is listed in Table 2.
3.1 Spatial Reference
In addition to properties that pick out referents,
throughout the data we see reference to objects
as they exist in space. Size is compared across
different dimensions of different objects, and ref-
erence is made to different parts of the objects,
picking out pieces within the whole. These two
phenomena ? relative size comparisons and part-
whole modularity ? point to an underlying spatial
object representation that may be utilized during
reference.
3.1.1 Relative Size Comparisons
A total of 122 (24.2%) references mention size
with a vague modifier (e.g., ?big?, ?wide?). This
includes comparative (e.g, ?larger?) and superla-
tive (e.g., ?largest?) size modifiers, which occur 40
(7.9%) times in the data set. Examples are given
below.
(1) ?the bigger pom-pom?
(2) ?the green largest pom-pom?
(3) ?the smallest long ribbon?
(4) ?the large orange pipe-cleaner?
Of the references that mention size, 35 (6.9%)
use a vague modifier that applies to one or two di-
mensions. This includes modifiers for height (?the
short silver ribbon?), width (?quite a fat rectan-
gle?), and depth (?the thick grey pipe-cleaner?).
87 (17.2%) use a modifier that applies to the over-
all size of the object (e.g., ?big? or ?small?). Table
3 lists these values. Crisp measurements (such as
?1 centimeter?) occur only twice (0.4%), with both
produced by the same participant.
Comparative/Superlative: 40 (7.9%)
Base: 82 (16.2%)
Height/Width/Depth: 35 (6.9%)
Overall size: 87 (17.2%)
Table 3: Size Modifier Breakdown.
Part-whole modularity Relative size Analogies
?a green pom-pom. . . ?a red foam-piece. . . ?a natural-looking piece
with the tinsel on the outside? which is more square of pipe-cleaner, it looks
?your gold twisty ribbon. . . in shape rather than a bit like a rope?
with sequins on it? the longer rectangle? ?a pipe-cleaner that
?a wooden bead. . . ?the grey pipe-cleaner. . . looks a bit like. . .
with a hole in the center? which is the thicker one. . . a fluffy caterpillar?
?one of the green pom-poms. . . ?the slightly larger one? ?the silver ribbon
with the sort of strands ?the smaller silver ribbon? that?s almost like
coming out from it.? ?the short silver ribbon? a big S shape.?
?the silver ribbon. . . with the chainmail ?quite a fat rectangle? ?a. . . pipe-cleaner
detail down through the middle of it.? ?thick grey pipe-cleaner? that looks like tinsel.?
11 References 122 References 16 References
Table 2: Examples of Observed Reference.
Participants produce such modifiers without
sizes or measurements explicitly given; with an
input of a visual object presentation, the output
includes size modifiers. Such data suggests that
natural reference in a visual domain utilizes pro-
cesses comparing the length, width, and height of
a target object with other objects in the set. Indeed,
5 references (1.0%) in our data set include explicit
comparison with the size of other objects.
(5) ?a red foam-piece. . . which is more square in
shape rather than the longer rectangle?
(6) ?the grey pipe-cleaner. . . which is the thicker
one. . . of the selection?
(7) ?the shorter of the two silver ribbons?
(8) ?the longer one of the ribbons?
(9) ?the longer of the two silver ribbons?
In Example (5), height and width across two
different objects are compared, distinguishing a
square from a rectangle. In (6) ?thicker? marks
the referent as having a larger circumference than
other items of the same type. (7) (8) and (9) com-
pare the height of the target referent to the height
of similar items.
The use of size modifiers in a domain without
specified measurements suggests that when peo-
ple refer to an object in a visual domain, they
are sensitive to its size and structure within a di-
mensional, real-world space. Without access to
crisp measurements, people compare relative size
across different objects, and this is reflected in the
expressions they generate. These comparisons are
not only limited to overall size, but include size
in each dimension. This suggests that objects?
structures within a real-world space are relevant
to REG in a visual domain.
3.1.2 Part-Whole Modularity
The role that a spatial object understanding has
within reference is further detailed by utterances
that pick out the target object by mentioning an ob-
ject part. 11 utterances (2.2%) in our data include
mention of an object part within reference to the
whole object. This is spread across participants,
such that half of the participants make reference
to an object part at least once.
(10) ?a green pom-pom, which is with the tinsel
on the outside?
(11) ?your gold twisty ribbon...with sequins on
it?
(12) ?a wooden bead...with a hole in the center?
In (10), pieces of tinsel are isolated from the
whole object and specified as being on the outside.
In (11), smaller pieces that lay on top of the ribbon
are picked out. And in (12), a hole within the bead
is isolated.
The use of part-whole modularity suggests an
understanding that parts of the object take up their
own space within the object. An object is not only
viewed as a whole during reference, but parts in,
on, and around it may be considered as well. For
an REG algorithm to generate these kinds of ref-
erences, it must be provided with a representation
that details the structure of each object.
3.2 ANALOGIES
The data from this study also provide information
on what can be expected from a knowledge base
in an algorithm that aims to generate naturalistic
reference. Reference is made 16 times (3.2%) to
objects not on the board, where the intended refer-
ent is compared against something it is like. Some
examples are given below.
(13) ?a gold. . . pipe-cleaner. . . completely
straight, like a ruler?
(14) ?a natural-looking piece of pipe-cleaner, it
looks a bit like a rope?
(15) ?a pipe-cleaner that looks a bit like. . . a
fluffy caterpillar. . . ?
In (13), a participant makes reference to a
SHAPE property of an object not on the board. In
(14) and (15), participants refer to objects that may
share a variety of properties with the referent, but
are also not on the board.
Reference to these other items do not pick out
single objects, but types of objects (e.g., an object
type, not token). They correspond to some pro-
totypical idea of an object with properties similar
to those of the referent. Work by Rosch (1975)
has examined this tendency, introducing the idea
of prototype theory, which proposes that there may
be some central, ?prototypical? notions of items. A
knowledge base with stored prototypes could be
utilized by an REG algorithm to compare the tar-
get referent to item prototypes. Such representa-
tions would help guide the generation of reference
to items not in the scene, but similar to the target
referent.
4 Discussion
We have discussed several different aspects of ref-
erence in a study where referring expressions are
elicited for objects in a spatial, visual scene. Ref-
erence in this domain draws on object forms as
they exist in a three-dimensional space and uti-
lizes background knowledge to describe referents
by analogy to items outside of the scene. This
is undoubtedly not an exhaustive account of the
phenomena at play in such a domain, but offers
some initial conclusions that may be drawn from
exploratory work of this kind.
Before continuing with the discussion, it is
worthwhile to consider whether some of our data
might be seen as going beyond reference. Perhaps
the participants are doing something else, which
could be called describing. How to draw the line
between a distinguishing reference and a descrip-
tion, and whether such a line can be drawn at all, is
an interesting question. If the two are clearly dis-
tinct, then both are interesting to NLG research.
If the two are one in the same, then this sheds
some light on how REG algorithms should treat
reference. We leave a more detailed discussion of
this for future work, but note recent psycholinguis-
tic work suggesting that referring establishes (1)
an individual as the referent; (2) a conceptualiza-
tion or perspective on that individual (Clark and
Bangerter, 2004). Schematically, referring = indi-
cating + describing.
We now turn to a discussion of how the ob-
served phenomena may be best represented in an
REG algorithm. We propose that an algorithm ca-
pable of generating natural reference to objects in
a visual scene should utilize (1) a spatial object
representation; (2) a non-spatial feature-based rep-
resentation; and (3) a knowledge base of object
prototypes.
4.1 Spatial and Visual Properties
It is perhaps unsurprising to find reference that ex-
hibits spatial knowledge in a study where objects
are presented in three-dimensional space. Hu-
man behavior is anchored in space, and spatial in-
formation is essential for our ability to navigate
the world we live in. However, referring expres-
sion generation algorithms geared towards spa-
tial representations have oversimplified this ten-
dency, keeping objects within the realm of two-
dimensions and only looking at the spatial rela-
tions between objects.
For example, Funakoshi et al (2004) and Gatt
(2006) focus on how objects should be clustered
together to form groups. This utilizes some of
the spatial information between objects, but does
not address the spatial, three-dimensional nature
of objects themselves. Rather, objects exist as en-
tities that may be grouped with other entities in a
set or singled out as individual objects; they do not
have their own spatial characteristics. Similarly,
one of the strengths of the Graph-Based Algorithm
(Krahmer et al, 2003) is its ability to generate ex-
pressions that involve relations between objects,
and these include spatial ones (?next to?, ?on top
of?, etc.). In all these approaches, however, ob-
jects are essentially one-dimensional, represented
as individual nodes.
Work that does look at the spatial information
of different objects is provided by Kelleher et al
(2005). In this approach, the overall volume of
each object is calculated to assign salience rank-
ings, which then allow the Incremental Algorithm
(Dale and Reiter, 1995) to produce otherwise ?un-
derspecified? reference. The spatial properties of
the objects are kept relatively simple. They are
not used in constructing the referring expression,
but one aspect of the object?s three-dimensional
shape (volume) affects the referring expression?s
final form. To the authors? knowledge, the cur-
rent work is the first to suggest that objects them-
selves should have their spatial properties repre-
sented during reference.
Research in cognitive modelling supports the
idea that we attend to the spatial properties of ob-
jects when we view them (Blaser et al, 2000), and
that we have purely spatial attentional mechanisms
operating alongside non-spatial, feature-based at-
tentional mechanisms (Treue and Trujillo, 1999).
These feature-based attentional mechanisms pick
out properties commonly utilized in REG, such as
texture, orientation, and color. They also pick out
edges and corners, contrast, and brightness. Spa-
tial attentional mechanisms provide information
about where the non-spatial features are located in
relation to one another, size, and the spatial inter-
relations between component parts.
Applying these findings to our study, an REG
algorithm that generates natural reference should
utilize a visual, feature-based representation of ob-
jects alongside a structural, spatial representation
of objects. A feature-based representation is al-
ready common to REG, and could be represented
as a series of <ATTRIBUTE:value> pairs. A spa-
tial representation is necessary to define how the
object is situated within a dimensional space, pro-
viding information about the relative distances be-
tween object components, edges, and corners.
With such information provided by a spatial
representation, the generation of part-whole ex-
pressions, such as ?the pom-pom with the tinsel on
the outside?, is possible. This also allows for the
generation of size modifiers (?big?, ?small?) with-
out the need for crisp measurements, for example,
by comparing the difference in overall height of
the target object with other objects in the scene, or
against a stored prototype (discussed below). Rel-
ative size comparisons across different dimensions
would also be possible, used to generate size mod-
ifiers such as ?wide? and ?thick? that refer to one
dimensional axis.
4.2 Analogies
A feature-based and a spatial representation may
also play a role in analogies. When we use analo-
gies, as in ?the pipe-cleaner that looks like a cater-
pillar?, we use world knowledge about items that
are not themselves visible. Such an expression
draws on similarity that does not link the referent
with a particular object, but with a general type of
object: the pipe-cleaner is caterpillar-like.
To generate these kinds of expressions, an REG
algorithm would first need a knowledge base with
prototypes listing prototypical values of attributes.
For example, a banana prototype might have a pro-
totypical COLOR of yellow. With prototypes in the
knowledge base, the REG algorithm would need
to calculate similarity of a target referent to other
known items. This would allow a piece of yellow
cloth, for example, to be described as being the
color of a banana.
Implementing such similarity measures in an
REG algorithm will be challenging. One difficulty
is that prototype values may be different depend-
ing on what is known about an item; a prototypical
unripe banana may be green, or a prototypical rot-
ten banana brown. Another difficulty will be in
determining when a referent is similar enough to
a prototype to warrant an analogy. Additional re-
search is needed to explore how these properties
can be reasoned about.
4.3 Further Implications
A knowledge base containing prototypes opens up
the possibility of generating many other kinds of
natural references. In particular, such knowledge
would allow the algorithm to compute which prop-
erties a given kind of referent may be expected
to have, and which properties may be unexpected.
Unexpected properties may therefore stand out as
particularly salient.
For example, a dog missing a leg may be de-
scribed as a ?three-legged dog? because the pro-
totypical dog has four legs. We believe that this
perspective, which hinges on the unexpectedness
of a property, suggests a new approach to at-
tribute selection. Unlike the Incremental Algo-
rithm, the Preference Order that determines the or-
der in which attributes are examined would not be
fixed, but would depend on the nature of the refer-
ent and what is known about it.
Approaching REG in this way follows work in
cognitive science and neurophysiology that sug-
gests that expectations about objects? visual and
spatial characteristics are derived from stored rep-
resentations of object ?prototypes? in the infe-
rior temporal lobe of the brain (Logothetis and
- A spatial representation (depicting size, inter-
relations between component parts)
- A non-spatial, propositional representation
(describing color, texture, orientation, etc.)
- A knowledge base with stored prototypical ob-
ject propositional and spatial representations
Table 4: Requirements for an REG algorithm that
generates natural reference to visual objects.
Sheinberg, 1996; Riesenhuber and Poggio, 2000;
Palmeri and Gauthier, 2004). Most formal theo-
ries of object perception posit some sort of cate-
gory activation system (Kosslyn, 1994), a system
that matches input properties of objects to those
of stored prototypes, which then helps guide ex-
pectations about objects in a top-down fashion.3
This appears to be a neurological correlate of the
knowledge base we propose to underlie analogies.
Such a system contains information about pro-
totypical objects? component parts and where they
are placed relative to one another, as well as rele-
vant values for material, color, etc. This suggests
that the spatial and non-spatial feature-based rep-
resentations proposed for visible objects could be
used to represent prototype objects as well. In-
deed, how we view and refer to objects appears to
be influenced by the interaction of these structures:
Expectations about an object?s spatial properties
guide our attention towards expected object parts
and non-spatial, feature-based properties through-
out the scene (Kosslyn, 1994; Itti and Koch, 2001).
This affects the kinds of things we are most likely
to generate language about (Itti and Arbib, 2005).
We can now outline some general requirements
for an algorithm capable of generating naturalis-
tic reference to objects in a visual scene: Input to
such an algorithm should include a feature-based
representation, which we will call a propositional
representation, with values for color, texture, etc.,
and a spatial representation, with symbolic infor-
mation about objects? size and the spatial relation-
ships between components. A system that gener-
ates naturalistic reference must also use a knowl-
edge base storing information about object proto-
types, which may be represented in terms of their
own propositional/spatial representations.
3Note that this is not the only proposed matching structure
in the brain ? an exemplar activation system matches input to
stored exemplars.
5 Conclusions and Future Work
We have explored the interaction between view-
ing objects in a three-dimensional, spatial domain
and referring expression generation. This has led
us to propose structures that may be used to con-
nect vision in a spatial modality to naturalistic ref-
erence. The proposed structures include a spatial
representation, a propositional representation, and
a knowledge base with representations for object
prototypes. Using structures that define the propo-
sitional and spatial content of objects fits well with
work in psycholinguistics, cognitive science and
neurophysiology, and may provide the basis to
generate a variety of natural-sounding references
from a system that recognizes objects.
It is important to note that any naturalistic ex-
perimental design limits the kinds of conclusions
that can be drawn about reference. A study that
elicits reference to objects in a visual scene pro-
vides insight into reference to objects in a visual
scene; these conclusions cannot easily be extended
to reference to other kinds of phenomena, such as
reference to people in a novel. We therefore make
no claims about reference as a whole in this paper;
generalizations from this research can provide hy-
potheses for further testing in different modalities
and with different sorts of referents.
Our data leave open many areas for further
study, and we hope to address these in future work.
Experiments designed specifically to elicit relative
size modifiers, reference to object components,
and reference to objects that are like other things
would help further detail the form our proposed
structures take.
What is clear from our data is that both a spa-
tial understanding and a non-spatial feature-based
understanding appear to play a role in reference
to objects in a visual scene, and further, refer-
ence in such a setting is bolstered by a knowl-
edge base with stored prototypical object repre-
sentations. Utilizing structures representative of
these phenomena, we may be able to extend ob-
ject recognition research into object reference re-
search, generating natural-sounding reference in
everyday settings.
Acknowledgements
Thanks to Advaith Siddarthan for thought-
provoking discussions and to the anonymous re-
viewers for useful suggestions.
References
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 42?29.
Robbert-Jan Beun and Anita H. M. Cremers. 1998.
Object reference in a shared domain of conversation.
Pragmatics and Cognition, 6:121?52.
Erik Blaser, Zenon W. Pylyshyn, and Alex O. Hol-
combe. 2000. Tracking an object through feature
space. Nature, 408:196?199.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22:1482?93.
Alphonse Chapanis, Robert N. Parrish, Robert B.
Ochsman, and Gerald D. Weeks. 1977. Studies
in interactive communication: II. the effects of four
communication modes on the linguistic performance
of teams during cooperative problem solving. Hu-
man Factors, 19:101?125.
Herbert H. Clark and Adrian Bangerter. 2004. Chang-
ing ideas about reference. In Ira A. Noveck and Dan
Sperber, editors, Experimental pragmatics, pages
25?49. Palgrave Macmillan, Basingstoke, England.
Herbert H. Clark and Meredyth A. Krych. 2004.
Speaking while monitoring addressees for under-
standing. Journal of Memory and Language, 50:62?
81.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?
39.
Herbert H. Clark, Robert Schreuder, and Samuel But-
trick. 1983. Common ground and the understand-
ing of demonstrative reference. Journal of Verbal
Learning and Verbal Behavior, 22:1?39.
Philip R. Cohen. 1984. The pragmatics of referring
and the modality of communication. Computational
Linguistics, 10(2):97?146.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
J. H. Flavell, P. T. Botkin, D. L. Fry Jr., J. W. Wright,
and P. E. Jarvice. 1968. The Development of Role-
Taking and Communication Skills in Children. John
Wiley, New York.
William Ford and David Olson. 1975. The elaboration
of the noun phrase in children?s description of ob-
jects. The Journal of Experimental Child Psychol-
ogy, 19:371?382.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generating refer-
ring expressions using perceptual groups. In Pro-
ceedings of the 3rd International Conference on Nat-
ural Language Generation, pages 51?60.
Albert Gatt. 2006. Structuring knowledge for refer-
ence generation: A clustering algorithm. Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-06), pages 321?328.
Paul H. Grice. 1975. Logic and conversation. Syntax
and Semantics, 3:41?58.
Peter A. Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21.
Laurent Itti and Michael A. Arbib. 2005. Attention and
the minimal subscene. In Michael A. Arbib, editor,
Action to Language via the Mirror Neuron System.
Cambridge University Press.
Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature Reviews Neu-
roscience.
J. Kelleher, F. Costello, and J. van Genabith. 2005.
Dynamically structuring, updating and interrelating
representations of visual and linguistic discourse
context. Artificial Intelligence, 167:62?102.
Stephen M. Kosslyn. 1994. Image and Brain: The
Resolution of the Imagery Debate. MIT Press, Cam-
bridge, MA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Robert M. Krauss and Sam Glucksberg. 1969. The
development of communication: Competence as a
function of age. Child Development, 40:255?266.
Robert M. Krauss and Sidney Weinheimer. 1967. Ef-
fect of referent similarity and communication mode
on verbal encoding. Journal of Verbal Learning and
Verbal Behavior, 6:359?363.
Nikos K. Logothetis and David L. Sheinberg. 1996.
Visual object recognition. Annual Review Neuro-
science, 19:577?621.
Dominic Mazzoni. 2010. Audacity.
Margaret Mitchell. 2008. Towards the generation
of natural reference. Master?s thesis, University of
Washington.
Thomas J. Palmeri and Isabel Gauthier. 2004. Vi-
sual object understanding. Nature Reviews Neuro-
science, 5:291?303.
Maximilian Riesenhuber and Tomaso Poggio. 2000.
Models of object recognition. Nature Neuroscience
Supplement, 3:1199?1204.
Eleanor Rosch. 1975. Cognitive representation of
semantic categories. Journal of Experimental Psy-
chology, 104:192?233.
Harvey Sacks and Emanuel A. Schegloff. 1979. Two
preferences in the organization of reference to per-
sons in conversation and their interaction. In George
Psathas, editor, Everyday Language: Studies in Eth-
nomethodology, pages 15?21. Irvington Publishers,
New York.
Stegan Treue and Julio C. Martinez Trujillo. 1999.
Feature-based attention influences motion process-
ing gain in macaque visual cortex. Nature, 399:575?
579.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Pro-
ceedings of the 4th International Conference on Nat-
ural Language Generation, Sydney, Australia. ACL.
Jette Viethen and Robert Dale. 2008. The use of spatial
descriptions in referring expressions. In Proceed-
ings of the 5th International Conference on Natural
Language Generation, INLG-08, Salt Fork, Ohio.
ACL.
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 28?32,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Task-Based Evaluation of NLG Systems: Control vs Real-World Context
Ehud Reiter
Dept of Computing Science
University of Aberdeen
e.reiter@abdn.ac.uk
Abstract
Currently there is little agreement about, or
even discussion of, methodologies for task-
based evaluation of NLG systems. I discuss
one specific issue in this area, namely the im-
portance of control vs the importance of eco-
logical validity (real-world context), and sug-
gest that perhaps we need to put more empha-
sis on ecological validity in NLG evaluations.
1 Introduction
Task-based extrinsic evaluation of a Natural Lan-
guage Generation (NLG) system involves measuring
the impact of an NLG system on how well subjects
perform a task. It is usually regarded as the ?gold
standard? for NLG evaluation, and it is the only type
of evaluation which will be seriously considered by
many external user communities.
Despite the importance of task-based evaluations,
however, there is surprisingly little discussion (or
agreement) in the NLG community about how these
should be carried out. In recent years there has been
a fair amount of discussion about the appropriate
use of corpus-based metrics, and there seems (de
facto) to be some level of agreement about evalua-
tions based on opinions of human subjects. But there
is little discussion and much diversity in task-based
evaluation methodology.
In this paper I focus on one one specific method-
ological issue, which is the relative importance of
control and ecological validity (real-world context).
An ideal task-based evaluation would be controlled,
that is the impact of NLG texts would be compared
against the impact of controlled or baseline texts in
a manner which minimises confounding factors. It
would also be ecologically valid, that is the eval-
uation would be carried out by representative real-
world users in a real-world context while performing
real-world tasks. Unfortunately, because of prag-
matic constraints including time, money, and ethical
approval, it is not always possible to achieve both of
these goals. So which is more important?
The methodologies currently used for task-based
evaluation in NLG largely derive from the Human-
Computer Interaction community, which in turn are
largely based on methodologies for experiments in
cognitive psychology. Now, psychologists place
much more emphasis on control than on ecologi-
cal validity; they regard control as absolutely es-
sential, but (with some exceptions) they see little
wrong with conducting experiments on unrepresen-
tative subjects (undergraduates) in artificial contexts
(psychology labs). Indeed many psychologists are
now embracing web-based experiments, where they
do not even know who the subjects are and what con-
texts they are working in. For the research goals of
psychologists, this probably makes sense. But the
research goals of the NLG community are different
from the research goals of the psychological com-
munity; should we place more emphasis on ecologi-
cal validity than they do, and less on control?
My own opinions on this matter are changing.
Five years ago, I would have echoed the feeling that
control is all-important. Now, though, I am begin-
ning to think that in order to achieve both NLG?s
scientific goals (understanding language and com-
putation) and NLG?s technological goals (developing
28
useful real-world technology), we need to put more
emphasis on ecological validity in our evaluations.
2 Evaluation which is both controlled and
in real-world context: STOP and DIAG
An ideal evaluation is one which is both controlled
and done in a real-world context. An example is the
evaluation of the STOP system. which generated tai-
lored smoking-cessation advice based on the user?s
response to a questionnaire (Lennox et al, 2001; Re-
iter et al, 2003). The STOP project was a collabora-
tion with medical colleagues, and the STOP evalua-
tion (which was designed by the medics) was car-
ried out as a randomised controlled clinical trial. We
recruited 2500 smokers, and sent one-third of them
STOP letters, one-third a non-tailored (canned) let-
ter, and one-third a letter which just thanked them
for being in out study. After 6 months we asked
participants if they had stopped smoking; we tested
saliva samples from people who said they had quit
in order to verify their smoking status. The result
of this evaluation was that the STOP tailored letters
were no more effective than the control non-tailored
letter. The STOP evaluation cost about UK?75,000,
and took about 20 months to design, organise, and
carry out.
The STOP evaluation was carried out in a real-
world context; the letters were sent to actual smok-
ers, and we measured whether they quit smoking. It
was also controlled, since the impact of STOP letters
was compared to the impact of non-tailored letters.
However there was a lot of ?noise? (in the statistical
sense) in the STOP evaluation, because different peo-
ple (with different personalities, attitudes towards
smoking, personal circumstances, etc) received the
tailored and non-tailored letters, and this impacted
smoking-cessation rates in the the three groups.
Another evaluation which was controlled and was
done at least partially in a real-world context was the
evaluation of the DIAG-NLP intelligent tutoring sys-
tem (di Eugenio et al, 2005). In this experiment, 75
students (the appropriate subject group for this tu-
toring system) were divided into three groups: two
groups interacted with two versions of the DIAG-
NLP system, and a third interacted with a control
version of DIAG which did not include any NLG. Ef-
fectiveness was measured by learning gain (change
in knowledge, measured by differences in scores in
a pre-test and post-test), which is standard in the tu-
toring system domain. The evaluation showed that
students learned more from the second (more ad-
vanced) version of the DIAG-NLP system than from
the non-NLG version of DIAG.
The DIAG-NLP evaluation was controlled, and it
was real-world in the sense that it used represen-
tative subjects and measured real-world outcome.
However, it appears (the paper is not completely ex-
plicit about this) that the evaluation assessed learn-
ing about a topic (fixing a home heating system)
which was not part of the student?s normal curricu-
lum; if this is the case, then the evaluation was not
100% in a real-world context.
3 Evaluation which is controlled but not
real-world: BT-45 and Young (1999)
The Babytalk project (Gatt et al, 2009) developed
several NLG systems which summarised clinical data
from babies in neonatal intensive care (NICU), for
different audiences and purposes; one of these sys-
tems, BT45 (Portet et al, 2009), summarised 45
minutes of data for doctors and nurses, to support
immediate decision-making. Babytalk was a collab-
orative project with clinical staff and psychologists,
and the psychologists designed the BT45 evaluation
(van der Meulen et al, 2010).
We picked 24 data sets (scenarios) based on his-
torical data from babies who had been in NICU
5 years previously, and for each data set cre-
ated three presentations: visualisation, computer-
generated text, and human-written text. For each
data set, we also asked expert consultants what ac-
tions should be taken by medical staff. We then
asked 35 medical staff (doctors and nurses of var-
ied expertise levels) to look at the scenarios using a
mix of presentations, in a Latin Square design; eg,
1/3 of the subjects saw the visualisation of scenario
1 data, 1/3 saw the computer-generated summary
of scenario 1 data, and 1/3 saw the human-written
summary of this data. Also each subject saw the
same number of scenarios in each condition, this re-
duced the impact of individual differences between
subjects. Subjects were asked to make decisions
about appropriate medical actions (or say no action
should be taken), and responses were compared to
29
the ?gold standard? recommendations from the con-
sultants. The result was that decision performance
was best with the human-written summaries; there
was no significant difference between overall deci-
sion performance with the computer-generated sum-
maries and the visualisation (although at the level
of individual scenarios, computer texts were more
effective in some scenarios, and visualisations was
more effective in other scenarios). The BT45 evalua-
tion cost about UK?20,000, and took about 6 months
to design, organise, and carry out.
The BT45 evaluation was carefully controlled
However, it was not done in a real-world context.
Doctors and nurses sat in an experiment room (not
in the ward) and looked at data from babies they
did not remember (as opposed to babies whom they
knew well because they has been looking after them
for the past few weeks); they also did not visually
observe the babies, which is a very important infor-
mation source for NICU staff.
Many other task-based evaluations of NLG sys-
tems have been controlled but not done in a real-
world context, including the very first task-based
NLG evaluation I am aware of, by Young (1999).
Young developed four algorithms for generating in-
structional texts, and tested these by asking 26 stu-
dents to follow the instructions generated by the var-
ious algorithms on several scenarios, and measured
error rates in carrying out the instructions. The in-
structions involved carrying out actions on campus
(going to labs, playing in soccer matches, etc). The
students did not actually carry out these actions, in-
stead they interacted with a ?text-based virtual real-
ity system?. Hence the evaluation was controlled but
not carried out in real-world context.
4 Evaluation which is real-world but not
controlled: BT-Nurse
The next Babytalk system (after BT45) was BT-
NURSE; it generated summaries of 12-hours of clin-
ical data, to support nursing shift handover (Hunter
et al, 2011). We initially expected to evaluate BT-
NURSE using a similar methodology to the BT45
evaluation. However the medical people involved
in BabyTalk complained that it was unrealistic to
evaluate the system in an artificial controlled con-
text, where clinical staff were looking at data out of
context. So instead we evaluated BT-NURSE by in-
stalling the system in the NICU, so that nurses used
it to get information about babies they were actu-
ally caring for. The primary outcome measure was
subjective ratings by nurses as to the helpfulness of
BT-NURSE texts; and indeed most nurses thought the
texts were helpful.
The BT-NURSE evaluation was significantly more
expensive than the BT45 evaluation, because we
hired a full-time software engineer for a year to
ensure that the software was sufficiently well en-
gineered so that it could be deployed and used in
the hospital; we were also required by the medical
ethics committee to have a research nurse on-site
who checked texts for errors before they were shown
to the duty nurses, and removed them from the ex-
periment if they were factually incorrect and could
damage patient care (in fact this never happened, the
research nurse did not regard any of the BT-NURSE
texts as potentially harmful from this perspective).
All in all cost was probably about UK?50,000, and
the entire process (including the software engineer-
ing) took about 18 months.
The BT-NURSE evaluation was not controlled; we
did not compare the computer generated texts to
anything else, and indeed did not directly measure
any task outcome variable, instead we solicited opin-
ions as to utility. It was however ecologically valid,
since it was carried out by asking nurses (real-world
users) to use BT-NURSE for care planning (real-
world task) in a real-world context (on-ward, involv-
ing babies the nurses were familiar with and could
visually observe).
5 Discussion
Ideally a task-based evaluation should be both con-
trolled and ecologically valid (done in a real-world
context). But if it is not possible to achieve both
of these objectives, which is most important? Obvi-
ously in many cases the desires of collaborators need
to be considered; for example psychologists gener-
ally place much more emphasis on control than on
ecological validity, whereas many commercial or-
ganisation take the opposite perspective. But which
is more important from an NLG perspective?
From a pragmatic perspective, two important ar-
guments for focusing on control are cost and publi-
30
cations. The figures given above suggest that doing
an evaluation in a real-world context makes it sub-
stantially more expensive. Of course this is based
on very limited data, but I believe this is correct,
deploying a system in a real-world context requires
addressing engineering and ethical issues which are
expensive and time-consuming to resolve. From a
publications perspective; most NLG reviewers are
much more concerned about control than about eco-
logical validity. Especially in high-prestige venues,
reviewers are likely to complain about uncontrolled
evaluations, while making little (if any) mention of
concerns about lack of ecological validity.
For what its worth, my own view on this issue has
changed. If asked five years ago, I would have said
that control was more important, but now I am veer-
ing more towards ecological validity. The techno-
logical goal of NLG is to develop technology which
is used in real-world applications, and from this per-
spective if we do not evaluate in real-world contexts,
we risk being side-tracked into technology which
looks good in a controlled environment but is useless
in the real world. Similarly, if our goal is to develop
a better scientific understanding of computation and
language, I think we have to look at how language
is used in real-world contexts, which (at least in my
mind) is quite different from how language is used
in artificial contexts.
Plaisant (2004) made some related points in her
discussion of evaluation of information visualisa-
tion. She pointed out that controlled evaluations
of visualisation systems in artificial contexts might
be less informative than uncontrolled evaluations
in real-world contexts. She also pointed out that
controlled evaluations could not evaluate some of
the most important benefits of visualisation systems.
For example, sometimes the primary objective of vi-
sualisation systems is to support scientific discov-
ery, that is to make it easier for scientists who are
analysing data to come up with new insights and
hypotheses. However, testing effectiveness at sup-
porting scientific discovery in a controlled fashion
is almost impossible. Perhaps in theory one could
compare the ?productivity? of two groups of scien-
tists, one with and one without visualisation tools,
but the comparison would have to involve a large
number of scientists over a period of months or even
years, with scientists in one group not allowed to
communicate with scientists in the other group. It
is difficult to imagine that such an experiment could
in fact be carried out (or that it would be approved
by a research ethics committee). Plaisant argues that
focusing on controlled experiments means focusing
on things that are easily measurable in such experi-
ments, which may lead researchers to ignore the out-
comes that we really care about.
Another important point is that the goal of evalua-
tion is not just to assess if something works, but also
to come up with insights as to how to improve an
algorithm, module, or system. In NLG evaluations
such insights are often based on free-text comments
made by subjects, and in my experience better and
more insightful comments are obtained from evalu-
ations in real-world contexts.
An important potential caveat is that all of the ex-
amples cited above were system evaluations, which
attempted to assess how useful a system was from an
applied perspective. If the goal of an evaluation is to
test a scientific theory or model, should we always
(as psychologists do) favour control over ecological
validity? My own belief is that the psychologists are
missing important insights and findings by ignoring
ecological validity, and the most effective way for
the NLG community to ?add value? to the enterprise
of understanding language is not to imitate the psy-
chologists, but rather to use a different experimental
paradigm, which focuses much more on ecological
validity. But others will no doubt disagree.
6 Conclusion
It is difficult to choose between control and ecolog-
ical validity, because clearly both greatly contribute
to the usefulness of an evaluation. But this trade-
off must be made in many cases, and it would be
preferable for it to be explicitly discussed. And of
course there are many other desirable factors which
may need to be involved in a tradeoff; for example,
how important is it that subjects be representative of
the user community, instead of whoever is easiest
to recruit (eg, undergraduates). My hope is that the
NLG community can explicitly discuss such issues,
and come up with recommended evaluation method-
ologies for task-based studies, which are based the
scientific and technological objectives of our com-
munity.
31
References
Barbara di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Aggregation im-
proves learning: experiments in natural language gen-
eration for intelligent tutoring systems. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 50?57. Association
for Computational Linguistics.
Albert Gatt, Francois Portet, Ehud Reiter, Jum Hunter,
Saad Mahamood, Wendy Moncur, and Somayajulu
Sripada. 2009. From data to text in the neonatal in-
tensive care unit: Using NLG technology for decision
support and information management. AI Communi-
cations, 22(3):153?186.
James Hunter, Yvonne Freer, Albert Gatt, Ehud Reiter,
Somayajulu Sripada, Cindy Sykes, and Dave Westwa-
ter. 2011. BT-Nurse: Computer generation of natu-
ral language shift summaries from complex heteroge-
neous medical data. Journal of the Americal Medical
Informatics Association. In press.
Scott Lennox, Liesl Osman, Ehud Reiter, Roma Robert-
son, James Friend, Ian McCann, Diane Skatun, and
Peter Donnan. 2001. The cost-effectiveness of
computer-tailored and non-tailored smoking cessation
letters in general practice: A randomised controlled
study. British Medical Journal, 322:1396?1400.
Catherine Plaisant. 2004. The challenge of information
visualization evaluation. In Proceedings of Advanced
Visual Interfaces (AVI) 2004.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy Sykes.
2009. Automatic generation of textual summaries
from neonatal intensive care data. Artificial Intelli-
gence, 173(7-8):789?816.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41?58.
Marianne van der Meulen, Robert Logie, Yvonne Freer,
Cindy Sykes, Neil McIntosh, and Jim Hunter. 2010.
When a graph is poorer than 100 words: A compari-
son of computerised natural language generation, hu-
man generated descriptions and graphical displays in
neonatal intensive care. Applied Cognitive Psychol-
ogy, 24(1):77?89.
Michael Young. 1999. Using Grice?s maxim of quan-
tity to select the content of plan descriptions. Artificial
Intelligence, 115:215?256.
32
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 100?104,
Utica, May 2012. c?2012 Association for Computational Linguistics
Working with Clinicians to Improve a Patient-Information NLG System
Saad Mahamood and Ehud Reiter
Department of Computing Science
University of Aberdeen
Aberdeen, Scotland, United Kingdom
{s.mahamood, e.reiter}@abdn.ac.uk
Abstract
NLG developers must work closely with do-
main experts in order to build good NLG sys-
tems, but relatively little has been published
about this process. In this paper, we describe
how NLG developers worked with clinicians
(nurses) to improve an NLG system which
generates information for parents of babies in
a neonatal intensive care unit, using a struc-
tured revision process. We believe that such a
process can significantly enhance the quality
of many NLG systems, in medicine and else-
where.
1 Introduction
Like other artificial intelligence (AI) systems, most
Natural Language Generation (NLG) systems incor-
porate domain knowledge (and domain communica-
tion knowledge (Kittredge et al, 1991)), either im-
plicitly or explicitly. Developers must work with do-
main experts to acquire such knowledge. Also like
software systems in general, applied NLG systems
must meet domain and application specific require-
ments in order to be useful; these again must come
from domain experts.
Since very few domain experts are familiar with
NLG, it is usually extremely difficult to acquire a
complete set of requirements, domain knowledge,
and domain communication knowledge at the be-
ginning of an NLG project. Especially, if no pre-
existing ?golden standard? corpus of domain texts
exists. Indeed, in many cases domain experts may
find it difficult to give detailed requirements and
knowledge until they can see a version of the NLG
system working on concrete examples. This sug-
gests that an iterative software development method-
ology should be used, where domain experts re-
peatedly try out an NLG system, revise underly-
ing domain (communication) knowledge and re-
quest changes to the system?s functionality, and wait
for developers to implement these changes before re-
peating the process.
We describe how we carried out this process on
BabyTalk-Family (Mahamood and Reiter, 2011), an
NLG system which generates summaries of clini-
cal data about a baby in a neonatal intensive care
unit (NICU), for the babys parents. Over a 6 month
period, this process enabled us to improve an ini-
tial version of the system (essentially the result of
a PhD) to the point where the system was good
enough to be deployable live in a hospital context.
We also describe how the feedback from the clini-
cians changed over the course of this period.
2 Previous Research
Reiter et al (2003) describe a knowledge acquisi-
tion strategy for building NLG systems which in-
cludes 4 stages: directly asking domain experts for
knowledge, structured knowledge acquisition activ-
ities with experts, corpus analysis, and revision with
experts. In this paper we focus on the last of these
phases, revision with experts. Reiter et al describe
this process in high-level qualitative terms; in this
paper our goal is to give a more detailed description
of the methodology, and also concrete data about
the comments received, and how they changed over
time.
The most similar previous work which we are
100
aware of is Williams and Reiter (2005), who de-
scribe a methodology for acquiring content selection
rules from domain experts, which is also based on
an iterative refinement process with domain experts.
Their process is broadly similar to what we describe
in this paper, but they focus just on content selection,
and do not give quantitative data about the revision
process.
In the wider software engineering community,
there has been a move to iterative development
methodologies, instead of the classic ?waterfall?
pipeline. In particular, agile methodologies (Mar-
tin, 2002) are based on rapid iterations and frequent
feedback from users; we are in a sense trying to ap-
ply some ideas from agile software engineering to
the task of building NLG systems. Our methodology
also can be considered to be a type of user-centred
design (Norman and Draper, 1986).
3 BabyTalk-Family
BabyTalk-Family (Mahamood and Reiter, 2011)
generates summaries of clinical data about babies in
a neonatal intensive care unit (NICU) for parents.
For more details about BabyTalk-Family, including
example outputs, please see Mahamood and Reiter.
BabyTalk-Family (BT-Family) was initially de-
veloped as part of a PhD project (Mahamood, 2010).
As such it was evaluated by showing output texts
(based on real NICU data) to people who had previ-
ously had a baby in NICU; the texts did not describe
the subject?s own baby (i.e., the subjects read texts
which summarised other people?s babies; they had
no previous knowledge of these babies). BT-Family
was also not rigorously tested from a software qual-
ity assurance perspective. The work presented here
arose from a followup project whose goal was to de-
ploy BT-Family live in a NICU, where parents who
currently had babies in NICU could read summaries
of their baby?s clinical data. Such a deployment re-
quired generated texts to be of much higher quality
(in terms of both content and language); we achieved
this quality using the revision process described in
this paper.
BT-Family is part of the BabyTalk family of sys-
tems (Gatt et al, 2009). All BabyTalk systems use
the same input data (NICU patient record), but they
produce different texts from this data; in particular
BT45 (Portet et al, 2009) produces texts which sum-
marise short periods to help real-time decision mak-
ing by clinicians, and BT-Nurse (Hunter et al, 2011)
produces summaries of 12 hours of data for nurses,
to support shift handover. BT-Nurse was also de-
ployed in the ward, to facilitate evaluation by nurses
who read reports about babies they were currently
looking after. To support this deployment, the BT-
Nurse developers spent about one month carrying
out a revision process with clinicians, in a somewhat
unstructured fashion. One outcome of the BT-Nurse
evaluation was that the system suffered because the
revision process was neither sufficiently well struc-
tured nor long enough; this was one of the motiva-
tions for the work presented here.
4 Revision Methodology
The revision process was carried out at the Neona-
tal Intensive Care Unit in conjunction with the hos-
pital Principal Investigator (PI) of our project and
two research nurses. We started with an initial fa-
miliarisation period for the nurses (the hospital PI
was already familiar with BT-Family), where we ex-
plained the goals of the project and asked the nurses
to examine some example BT-Family texts, which
we then discussed.
After the nurses were familiar with the project, we
conducted a number of revision cycles. Each cycle
followed the following procedure:
1. The clinicians (either the hospital PI or the research
nurses) choose between 3 and 11 scenarios (one
day?s worth of data from one baby). These scenar-
ios were chosen to test the system against a diverse
range of babies in different clinical conditions; sce-
narios were also chosen to check whether issues
identified in previous cycles had been addressed.
2. The nurses examined the texts generated by BT-
Family for the chosen scenarios. They both directly
commented on the texts (by writing notes on hard-
copy), and also (in some cases) edited the texts to
show what they would have liked to see.
3. The NLG developers analysed the comments and
revised texts; distilled from these a list of specific
change requests; prioritised the change requests on
the basis of importance and difficulty; and imple-
mented as many change requests as possible given
the time constraints of the cycle.
101
Figure 1: Example of marked up text annotated by a research nurse. The baby?s forename has been blacked out.
4. The scenarios were rerun through the updated sys-
tem, and the NLG developers checked that the is-
sues had been addressed. Clinicians did not usually
look at the revised texts, instead they would check
that the issues had been resolved in new scenarios
in the next cycle.
The above process was carried out 14 times over
a 6 month period with each cycle taking on average
11.28 days. A research fellow (Saad Mahamood)
was assigned to implement these changes working
full-time over this 6 month period. The length be-
tween each revision cycle was variable due to the
availability of the domain experts and the variable
level of complexity to implement identified changes
to the BT-Family system.
Figure 1 shows a extract from an early BT-Family
text generated in July 2011 that needed a lot of re-
vision. In this example, the nurse has identified the
following issues:
? Incorrect pronoun: He instead of His.
? Unnecessary phrase: Because XXXX was born ear-
lier than expected.
? Change in tense: is being instead of has been.
? Change in wording of time phrase: In the last 24
hours instead of Since yesterday.
? Incorrect content: incubator oxygen has increased,
it is not stable.
? Grammar mistake: were instead of was.
? Change in content: some (frequency) instead of
moderate (severity).
? Change in wording: self-resolving instead of self-
resolved.
5 Analysis of Feedback over Time
We extracted hand-written comments on BT-Family
texts (of the type shown in Figure 1) and annotated
the comments using a scheme similar to that used
by Hunter et al(2011) for analysing comments on
BT-Nurse texts. Two annotators were used with the
first annotating the entire set of 75 reports using a
pre-agreed classification scheme. The classification
scheme that was used consisted of three types of
categories: Content Errors, Language Errors, and
Comments with each containing specific categori-
sation labels as shown in Table 1. Content Errors
labels were used to annotate comments when there
were content based mistakes. Language error labels
were used to categorise the different types of lan-
guage based mistakes. Finally, comment labels were
used to classify different types of comments made
by the nurses. The second annotator annotated a
random partial subset of the reports independently
to check for the level of agreement between the first
and second annotators. By using Cohen?s kappa co-
efficient we found the level of inter-annotator agree-
ment was k=0.702.
Content errors were the most predominate type of
annotation (50.54%), followed by Language errors
(25.18%), and comments (24.27%). Positive com-
ments were unusual (only 5 in total), because the
clinicians were explicitly asked to focus on prob-
102
Content Errors Language Errors Comment
unnecessary (44.20%) spelling mistake (8.14%) positive (3.75%)
missing (28.26%) grammar mistake (22.22%) negative (0.75%)
wrong (22.82%) incorrect tense/aspect (18.51%) no agreement (1.50%)
should-be-elsewhere (4.71%) different word(s) required (35.55%) reformulation (12.78%)
unnecessary words (3.70%) observation (66.16%)
precision/vagueness (11.85%) question (15.03%)
Table 1: List of annotation categories and the labels within each category that was used. The frequency for each label
in it?s category is given in brackets.
Month Number of Avg. scenarios Avg. number of Avg. number of Avg. number of
revision cycles per cycle content errors language errors comments
June 1 5 1.8 4.2 1.2
July 2 8 4.93 5.5 1.87
August 2 5 4.8 4 5.8
September 2 4 6.37 8.5 4
October 3 7 2.95 1.57 6.42
November 3 5 1.6 1.6 3.6
December 1 5 0.8 0 0.4
Overall 14 5.7 6.92 3.62 3.32
Table 2: Summary table showing the average number of content errors, language errors, and comments per scenario.
lems. Table 2 shows statistics for the revision pro-
cess per month; the process started in the second half
of June, and ended in the first half of December.
From a qualitative perspective, the data suggests
that there were two phases to the revision process.
In the first phase (June to September), the number
of content and language errors in fact went up. We
believe this is because during this phase we were
adding around 16 new types of content to the re-
ports (based on requests from the clinicians) as well
as fixing problems with existing content (of the sort
shown in Figure 1); this additional content itself of-
ten needed to be revised in subsequent revision cy-
cles, which increased the error count for these cy-
cles. These additional errors from the addition of
new content may of arisen due to the complexity
and variation of clinical data. Additionally, our 3-
year old anonymised test set of clinical data may
not of been as representative as the live data due
to changes/additions in patient data. In the sec-
ond phase (October to December), requests for new
content diminished (around 4 requests) and we fo-
cused on fixing problems with existing content; in
this phase, the number of content and language er-
rors steadily decreased (that is, the system improved
from the clinician?s perspective), until we reached
the point in mid December when the clinicians were
satisfied that the quality of BT-Family texts was con-
sistently good from their perspective.
When the revision process ended, we started eval-
uating BT-Family texts directly with parents, by
showing parents texts about their babies. This work
is ongoing, but initial pilot results to date indicate
that parents are very happy with the texts, and do
not see major problems with either the language or
the content of the texts.
6 Discussion
The revision process had a major impact on the qual-
ity of BT-Family texts, as perceived by the clini-
cians. At the start of the process (June 2011), the
texts had so many mistakes that they were unusable;
the clinicians would not allow us to show parents
BT-Family texts about their babies, even in the con-
text of a pilot study. After 14 revision rounds over a
6 month period, text quality had improved dramati-
cally, to the point where clinicians allowed us to start
working directly with parents to get their feedback
and comments on BT-Family texts.
The fact that a new set of scenarios was used in
every iteration of the revision process was essen-
103
tial to giving clinicians confidence that text quality
would be acceptable in new cases; they would not
have had such confidence if we had focused on im-
proving the same set of texts.
The revision process took 6 months, which is a
considerable amount of time. This process would
have been shorter if BT-Family had undergone a
more rigorous testing and quality assurance (QA)
process ahead of time, which would for example
have addressed grammar mistakes, and (more im-
portantly) tested the system?s handling of boundary
and unusual cases. The process probably could also
have been further shortened in other ways, for ex-
ample by performing 3 revision cycles per month
instead of 2.
However, one reason the process took so long was
that the functionality of the system changed; as the
clinicians got a better idea of what BT-Family could
do and how it could help parents, they requested
new features, which we tried to add to the system
whenever possible. We also had to accommodate
changes in the input data (patient record), which
reflected changes in NICU procedures due to new
drugs, equipment, procedures, etc. So we were not
just tweaking the system to make it work better, we
were also enhancing its functionality and adapting it
to changing input data, which is a time consuming
process.
7 Conclusion
We have presented a methodology for improving
the quality and appropriateness of texts produced by
applied NLG systems, by repeatedly revising texts
based on feedback from domain experts. As we have
show in the results, the process is a time consuming
one, but appears to be quite effective in bringing an
NLG system to the required level of quality in a clin-
ical domain.
Acknowledgements
This work is funded by the UK Engineering and Physical Sciences
Council (EPSRC) and Digital Economy grant EP/H042938/1. Many
thanks to Dr. Yvonne Freer, Alison Young, and Joanne McCormick of
the Neonatal Intensive Care Unit at Simpson Centre for Reproductive
Health, Royal Infirmary of Edinburgh Hospital, for their help.
References
Albert Gatt, Francois Portet, Ehud Reiter, Jum Hunter,
Saad Mahamood, Wendy Moncur, and Somayajulu
Sripada. 2009. From data to text in the neonatal in-
tensive care unit: Using NLG technology for decision
support and information management. AI Communi-
cations, 22(3):153?186.
James Hunter, Yvonne Freer, Albert Gatt, Ehud Reiter,
Somayajulu Sripada, Cindy Sykes, and Dave Westwa-
ter. 2011. BT-Nurse: Computer generation of natu-
ral language shift summaries from complex heteroge-
neous medical data. Journal of the Americal Medical
Informatics Association, 18(5):621?624.
Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991. On the need for domain communication lan-
guage. Computational Intelligence, 7(4):305?314.
Saad Mahamood and Ehud Reiter. 2011. Generating
affective natural language for parents of neonatal in-
fants. In Proceedings of the 13th European Work-
shop on Natural Language Generation, pages 12?21,
Nancy, France, September. Association for Computa-
tional Linguistics.
Saad Mahamood. 2010. Generating Affective Natural
Language for Parents of Neonatal Infants. Ph.D. the-
sis, University of Aberdeen, Department of Comput-
ing Science.
Richard Martin. 2002. Agile Software Development,
Principles, Patterns, and Practices.
Donald A. Norman and Stephen W. Draper. 1986.
User Centered System Design; New Perspectives on
Human-Computer Interaction. L. Erlbaum Associates
Inc., Hillsdale, NJ, USA.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy Sykes.
2009. Automatic generation of textual summaries
from neonatal intensive care data. Artificial Intelli-
gence, 173(7-8):789?816.
Ehud Reiter, Somayajulu Sripada, and Roma Robertson.
2003. Acquiring correct knowledge for natural lan-
guage generation. Journal of Artificial Intelligence
Research, 18:491?516.
Sandra Williams and Ehud Reiter. 2005. Deriving con-
tent selection rules from a corpus of non-naturally oc-
curring documents for a novel NLG application. In
Proceedings of Corpus Linguistics workshop on using
Corpora for NLG.
104
Proceedings of the 14th European Workshop on Natural Language Generation, pages 152?156,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
MIME - NLG in Pre-Hospital Care
Anne H. Schneider Alasdair Mort Chris Mellish Ehud Reiter Phil Wilson
University of Aberdeen
{a.schneider, a.mort, c.mellish, e.reiter, p.wilson}@abdn.ac.uk
Pierre-Luc Vaudry
Universite? de Montre?al
vaudrypl@iro.umontreal.ca
Abstract
The cross-disciplinary MIME project aims
to develop a mobile medical monitoring
system that improves handover transac-
tions in rural pre-hospital scenarios be-
tween the first person on scene and am-
bulance clinicians. NLG is used to pro-
duce a textual handover report at any time,
summarising data from novel medical sen-
sors, as well as observations and actions
recorded by the carer. We describe the
MIME project with a focus on the NLG
algorithm and an initial evaluation of the
generated reports.
1 Introduction
Applications of Natural Language Generation
(NLG) in the medical domain have been manifold.
A new area where NLG could contribute to the im-
provement of services and to patient safety is pre-
hospital care: care delivered to a patient before ar-
rival at hospital. There are many challenges in de-
livering pre-hospital care, making it different from
care taking place in the controlled circumstances
of emergency departments or hospital wards.
Some Ambulance Services have developed in-
novative models to care for patients whilst an am-
bulance is en-route. Community First Responder
(CFR) schemes recruit volunteers from local com-
munities and give them the necessary training and
equipment to deal with a limited range of medical
emergencies. The premise is that even those with
basic first-aid skills can save a life. It is their task
to attend the casualty while waiting for the am-
bulance and to record their observations and ac-
tions on a paper patient report form (PRF). They
may also assess the patient?s physiological mea-
surements (e.g. heart rate). In practice, due to
time constraints, a verbal handover is performed
and the PRF is filled in later. Physiological mea-
surements may be written in ink on the back of a
protective glove, and are rarely passed on in any
systematic way.
The MIME (Managing Information in Medical
Emergencies)1 project is developing technology to
support CFRs in the UK when they respond to pa-
tients. The project aims to enable CFRs to capture
a greater volume of physiological patient data, giv-
ing them a better awareness of a patient?s medical
status so they can deliver more effective care.
There are two parts to our work: the use of novel
lightweight wireless medical sensors that are sim-
ple and quick to apply, and the use of novel soft-
ware that takes these inherently complex sensor
data, along with some other information inputted
by the user (e.g. patient demographics or actions
performed) on a tablet computer, and present it
very simply. We are working with two sensors that
provide measurements of the patient?s respiratory
rate, heart rate and blood oxygen saturation. Our
software can use NLG to produce a textual han-
dover report at any time. This can be passed to an
arriving paramedic to give a quick summary of the
situation and can accompany the patient to inform
later stages of care. We anticipate that our sys-
tem will also provide some basic decision support
based upon the patients clinical condition.
2 Related Work
Many situations arise in the medical domain where
vast amounts of data are produced and their correct
interpretation is crucial to the lives of patients. In-
terpreting these data is usually a demanding and
complex task. Medical data are therefore often
presented graphically or preferably in textual sum-
maries (Law et al, 2005) making NLG important
for various applications in the medical domain.
A number of systems address the problem of
presenting medical information to patients in a
form that they will understand. Examples are
1www.dotrural.ac.uk/mime
152
STOP (Reiter et al, 2003), PILLS (Bouayad-Agha
et al, 2002), MIGRANE (Buchanan et al, 1992),
and Healthdoc (Hirst et al, 1997). Other systems,
such as TOPAZ (Kahn et al, 1991) and Suregen
(Hu?ske-Kraus, 2003), aim to summarise informa-
tion in order to support medical decision-making.
In the case of MIME, the challenge is to sum-
marise large amounts of sensor data, in the context
of carer observations and actions, in a coherent
way that supports quick decision making by the
reader. The problem of describing the data relates
to previous work on summarising time series data
(e.g. (Yu et al, 2007)). In many ways, though, our
problem is most similar to that of Babytalk BT-
Nurse system (Hunter et al, 2012), which gener-
ates shift handover reports for nurses in a neona-
tal intensive care unit. The nature of the recipi-
ent is, however, different. Whereas BabyTalk ad-
dresses clinical staff in a controlled environment,
MIME is aimed at people with little training who
may have to deal with emergency situations very
quickly. Further, while BT-Nurse works with an
existing clinical record system, which does not al-
ways record all actions and observations which
ideally would be included in a report, in MIME
users enter exactly the information which MIME
needs. This simplifies the NLG task, at the cost of
adding a new task (interface construction).
3 The MIME project
In the first stage of MIME, we have developed
a desktop application to prototype the generation
of handover reports. We used simulated scenar-
ios, where a panel of medical experts determined
the sequence of events and predicted the stream of
data from the simulated sensors.
The generated reports must provide a quick
overview of the situation but at the same time be
sufficiently comprehensive, while the format must
enhance the readability. A general structure for
the handover reports was determined in a user-
centred development process together with ambu-
lance clinicians. After the demographic descrip-
tion of the casualty and incident details (entered by
the responder whenever they have an opportunity),
two sections of generated text follow: the initial
assessment section and the treatments and findings
section. The initial assessment contains informa-
tion on the patient gathered by the CFRs just after
the sensors are applied and also any observations
made during the first minute after the application
of the sensors. The treatment and findings section
is a report on the observations and actions of the
CFRs while they waited for the ambulance to ar-
rive. This includes a paragraph that sums up the
condition of the patient at the time of handover.
Using sensors to capture physiological data
continuously introduces the problem that irrele-
vant information needs to be suppressed in order
not to overload the ambulance clinicians and hin-
der interpretation. The NLG algorithm that gen-
erates short as well as comprehensive handover re-
ports accomplishes text planning in the two stages
of document planning and micro-planning (Re-
iter and Dale, 2000). Document planning is re-
sponsible for the selection of the information that
will be mentioned in the generated report. Events
that will be mentioned in the text are selected
and structured into a list of trees (similar to trees
in Rhetorical Structure Theory (Scott and Siecke-
nius de Souza, 1990)). In the micro-planning step
the structure of the document plan is linearised and
sentences are compiled using coordination and ag-
gregation.
Whereas some parts of the handover document
(e.g. patient demographics) are relatively stylised,
the main technically demanding part of the NLG
involves the description of the ?treatment and find-
ings?, which describes the events that happen
whilst the patient is being cared for and relevant
parts of the sensor data (see Figure 1). For this
section of the report, the document planning al-
gorithm is based on that of (Portet et al, 2007),
which identifies a number of key events and cre-
ates a paragraph for each key event. Events that
are explicitly linked to the key event or events that
happen at the same time are added to the relevant
paragraph. This is based on the earlier work of
(Hallett et al, 2006).
4 Evaluation
In an initial evaluation we sought to assess how
our reports would be received in comparison with
the current situation ? either short verbal reports
or paper report forms (PRFs)? and also in com-
parison with what might be regarded as a ?gold
standard? report produced by an expert.
Materials: Two videos were produced indepen-
dently of the NLG team, based on two scenarios
of medical incidents typical of a CFRs caseload.
These scenarios, a farm injury and chest pain, in-
cluded a short description of the incident, similar
153
At 02:12, after RR remained fairly
constant around 30 bpm for 4 minutes,
high flow oxygen was applied, she took
her inhaler and RR decreased to 27
bpm. However, subsequently RR once
more remained fairly constant around
30 bpm for 8 minutes.
At 02:15 she was feeling faint.
At 02:15 the casualty was moved.
At 02:17 the casualty was once more
moved.
Figure 1: Part of the ?Treatment and Findings? for an
asthma scenario.
to the initial information a CFR would receive, a
time line of events that happened before the ambu-
lance arrived as well as simulated sensor data from
the patient. The videos showed an actor in the
role of CFR and another as patient, with the sce-
nario time displayed in one corner. When the CFR
performed readings of the physiological measures
they were shown as subtitles.
The videos were presented to two CFRs and a
paramedic, who were asked to imagine themselves
in the situation of the CFR in the video, and to
produce a handover report. Each video was only
played once in order to produce more realistic re-
sults. We asked one CFR to construct a written
?verbal? handover for the first scenario and to fill
out a PRF for the other scenario, and the other
CFR to do the ?verbal? handover for the second
scenario and to fill out the PRF for the first. To
anonymise the PRF it was transcribed into a digi-
tal version. The paramedic received a blank sheet
of paper and was requested to produce a handover
report that he would like to receive from a CFR
when arriving at the scene. Based on the scenarios
we also generated two reports with the MIME sys-
tem. This process resulted in four reports for each
of the two scenarios, one transcribed verbal han-
dover and a PRF from a CFR, a written handover
report from a paramedic and the generated report.
Hypotheses: Our hypothesis was that the gen-
erated reports would improve on the current prac-
tice of verbal handovers and PRFs, and that
paramedics would perceive them to be more suit-
able, hence rank them higher than the CFRs? ver-
bal or PRF reports. The paramedic handover re-
port might be regarded as a gold standard pro-
duced by an expert and we were interested in how
the generated reports fared in comparison. Fur-
ther, we hoped to gain information on how to im-
prove our generated reports.
Participants: We approached paramedics in
the Scottish Ambulance Service to participate in
our study. Nine paramedics responded (eight male
and one female; age range 32?56 years with 10?24
years? service).
Procedure: Participants received an invitation
email with a link to a brief online survey and the
eight reports as attachments. After an introduction
and consent form they were forwarded to one of
the two scenario descriptions and asked to rank the
respective four reports. After that the participant
was asked to rate the accuracy, understandability
and usefulness of the generated report for this sce-
nario on a 5-point Likert scale ranging from very
good to very bad and to indicate what they liked
or disliked about it in a free text box. This process
was repeated for the second scenario.
4.1 Results
Ranking: An overview of the rankings can be
found in Table 1. Apart from the rankings of par-
ticipant 7 and 8, no large differences in how the
reports were ranked could be observed between
the two scenarios. We performed a Friedman
test (Friedman, 1937) (farm injury scenario: chi-
squared=4.3, df=3, p=0.23; chest pain scenario:
chi-squared=12.44, df=3, p=0.006): some reports
were ranked consistently higher or lower than oth-
ers. The verbal CFR report was ranked worst in all
but five cases. There is a high disparity in the rank-
ings for the PRF, which was ranked first on eight
occasions and in the other ten instances in third
or fourth place. The generated report was ranked
in first place only once, but eleven times in sec-
ond place and in third place the other six times. In
general the paramedic report, which was regarded
as the ?gold standard?, was ranked better than the
generated report, but in five cases the generated
report was ranked better.
Rating: An overview of the ratings for the gen-
erated reports can be found in Table 2. The rat-
ings for both scenarios were good on average, with
a majority of ratings lying between very good to
moderate. Only one rating (the accuracy of the
generated report for the farm injury scenario) was
bad; none was very bad. The ratings for the gen-
erated report of the chest pain scenario were on
average better than those for the farm injury sce-
nario. Accuracy had better ratings than usefulness
and understandability in both scenarios.
154
Participant: 1 2 3 4 5 6 7 8 9 med min max
farm injury scenario
Paramedic 2 2 3 1 1 3 3 2 1 2 1 3
Generated 3 3 2 2 2 2 2 3 2 2 2 3
CFR PRF 1 1 1 3 4 1 4 4 3 3 1 4
CFR verbal 4 4 4 4 3 4 1 1 4 4 1 4
chest pain scenario
Paramedic 2 2 3 1 1 2 2 1 1 2 1 3
Generated 3 3 2 2 2 3 1 2 2 2 1 3
CFR PRF 1 1 1 3 4 1 4 3 3 3 1 4
CFR verbal 4 4 4 4 3 4 3 4 4 4 3 4
Table 1: Overview of the ranking results (most preferred
(1) to least preferred (4)), median (med), maximum (max)
and minimum (min) values for the patient report form (CFR
PRF), paramedic report (Paramedic), generated report (gen-
erated) and verbal report (verbal CFR).
Participant: 1 2 3 4 5 6 7 8 9 med min max
farm injury scenario
accuracy 1 2 1 4 2 2 1 1 1 1 1 4
useful. 3 3 2 2 1 2 2 1 1 2 1 3
unders. 2 3 2 2 1 3 3 1 1 2 1 3
chest pain scenario
accuracy 2 2 1 1 1 3 1 2 1 1 1 3
useful. 2 3 2 1 1 2 1 1 1 1 2 3
unders. 2 3 2 1 1 3 2 1 1 2 1 3
Table 2: Overview of the rating results, median (med), max-
imum (max) and minimum (min) values for accuracy, useful-
ness (useful.) and understandability (unders.) of the gener-
ated reports, on a Likert scale (very good (1) to very bad (5)).
4.2 Discussion
We hypothesised that the generated reports would
fare better than the verbal handovers and the PRFs.
Results confirm a preference for the generated re-
ports over the verbal handover. The paramedic
reports, which were regarded as our ?gold stan-
dard? were ranked higher than the generated re-
ports. Interestingly, in almost half the cases there
was a clear preference for the PRF and in the other
cases the PRF ranked badly. This may have been
affected by the familiarity of this medium and per-
haps by the background assumption that this is
how handover reports ?should? be presented.
We regard this as a tentative confirmation that
the generated texts compete favourably with the
status quo. In a real world scenario the paramedics
often get a verbal handover instead of the PRF and
it should be noted that the PRF was printed and not
handwritten. Furthermore, although the CFRs and
paramedics only saw the scenario video once they
were under no time pressure to submit the reports.
Hence the quality of all the human reports in our
experiment is likely to be better than normal.
Although each individual generally provided
consistent responses across the two scenarios,
there were variations between individuals. These
different preferences may be merely stylistic
choices or they may reflect in task performance.
Preferences are not necessarily an indication of
usefulness for a task (cf. (Law et al, 2005)).
In general the accuracy, understandability and
usefulness of the generated reports received good
ratings. Although participation was low, the qual-
itative data we gathered were valuable, every par-
ticipant offered comments in the free text box on
what they liked or disliked about the generated re-
port. In general there seemed to be an impres-
sion that some sections were longer than neces-
sary. One participant observed that reporting on
observations a long time later is only useful if
things have changed significantly. The structure
and organisation of the report received some posi-
tive comments. For example one participant stated
that he liked ?the separate sections for informa-
tion? and another commented that the report was
?logically laid out?, that it was ?easy to obtain
information? from the report and that it ?clearly
states intervention and outcome of intervention?.
5 Conclusion and Future Work
Despite the fact that the experiment reported here
involved a small number of participants, which
implies that its results need to be interpreted with
some caution, the generated reports produced by
the MIME system appear to improve on the cur-
rent practice of verbal handover. We aim to col-
lect more responses and repeat the evaluation that
has been presented. Our next step in evaluating the
report generator will be to carry out a task based
evaluation to see whether the preference ratings
we have gathered can be reflected in performance
measures.
We are now moving into the second stage of
MIME and have started developing a new proto-
type, a mobile device that gets signals from two
lightweight sensors. Here we will collect data
from real emergency ambulance callouts by hav-
ing a researcher join ambulance crews for their
normal activity, which will be used to modify the
NLG system (e.g. in order to allow for more reli-
able handling of noise).
6 Acknowledgments
This work is supported by the RCUK dot.rural
Digital Economy Research Hub, University of Ab-
erdeen (Grant reference: EP/G066051/1)
155
References
N. Bouayad-Agha, R. Power, D. Scott, and A. Belz.
2002. PILLS: Multilingual generation of medical
information documents with overlapping content. In
Proceedings of LREC 2002, pages 2111?2114.
B. Buchanan, J. Moore, D. Forsythe, G. Banks, and
S. Ohlsson. 1992. Involving patients in health care:
explanation in the clinical setting. In Proceedings of
the Annual Symposium on Computer Application in
Medical Care, pages 510?514, January.
M. Friedman. 1937. The Use of Ranks to Avoid the
Assumption of Normality Implicit in the Analysis
of Variance. Journal of the American Statistical As-
sociation, 32(200):675?701.
C. Hallett, R. Power, and D. Scott. 2006. Summari-
sation and visualisation of e-Health data repositories
Conference Item Repositories. In UK E-Science All-
Hands Meeting, pages 18?21.
G. Hirst, C. DiMarco, E. Hovy, and K. Parsons. 1997.
Authoring and Generating Health-Education Docu-
ments That Are Tailored to the Needs of the Individ-
ual Patient. In Anthony Jameson, Ce?cile Paris, and
Carlo Tasso, editors, User Modeling: Proceedings
of the Sixth International Conference, UM97, pages
107?118. Springer Wien New York.
J. Hunter, Y. Freer, A. Gatt, E. Reiter, S. Sripada, and
C Sykes. 2012. Automatic generation of natural
language nursing shift summaries in neonatal in-
tensive care: BT-Nurse. Artificial Intelligence in
Medicine, 56:157?172.
D. Hu?ske-Kraus. 2003. Suregen-2: A Shell System for
the Generation of Clinical Documents. In Proceed-
ings of the 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2003), pages 215?218.
M. Kahn, L. Fagan, and L. Sheiner. 1991. Combining
physiologic models and symbolic methods to inter-
pret time-varying patient data. Methods of informa-
tion in medicine, 30(3):167?78, August.
A. Law, Y. Freer, J. Hunter, R. Logie, N. McIntosh,
and J. Quinn. 2005. A comparison of graphical and
textual presentations of time series data to support
medical decision making in the neonatal intensive
care unit. Journal of clinical monitoring and com-
puting, 19(3):183?94, June.
F. Portet, E. Reiter, J. Hunter, and S. Sripada. 2007.
Automatic generation of textual summaries from
neonatal intensive care data. In In Proccedings
of the 11th Conference on Artificial Intelligence in
Medicine (AIME 07). LNCS, pages 227?236.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Studies in Natural Lan-
guage Processing. Cambridge University Press.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessa-
tion letters. Artificial Intelligence, 144(1-2):41?58,
March.
D. Scott and C. Sieckenius de Souza. 1990. Get-
ting the message across in rst-based text genera-
tion. In R. Dale, C. Mellish, and M. Zock, editors,
Current Research in Natural Language Generation.
Academic Press.
J. Yu, E. Reiter, J. Hunter, and C. Mellish. 2007.
Choosing the content of textual summaries of large
time-series data sets. Natural Language Engineer-
ing, 13(1):25?49.
156
Proceedings of the 14th European Workshop on Natural Language Generation, pages 198?199,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
MIME- NLG Support for Complex and Unstable Pre-hospital
Emergencies
Anne H. Schneider Alasdair Mort Chris Mellish Ehud Reiter Phil Wilson
University of Aberdeen
{a.schneider, a.mort, c.mellish, e.reiter, p.wilson}@abdn.ac.uk
Pierre-Luc Vaudry
Universite? de Montre?al
vaudrypl@iro.umontreal.ca
Abstract
We present the first prototype of a han-
dover report generator developed for the
MIME (Managing Information in Medi-
cal Emergencies) project. NLG applica-
tions in the medical domain have been var-
ied but most are deployed in clinical situa-
tions. We develop a mobile device for pre-
hospital care which receives streamed sen-
sor data and user input, and converts these
into a handover report for paramedics.
1 Introduction
Natural Language Generation underlies many ap-
plications in the medical domain but most are em-
ployed under relatively predictable clinical situa-
tions. The MIME project employs a mobile de-
vice with novel lightweight sensors to improve
pre-hospital care service delivery. The term pre-
hospital care denotes the treatment delivered to
a patient before they arrive at hospital. Usu-
ally this entails paramedics and ambulance teams,
but it can also include a wide range of volun-
tary and professional care groups. Care for ru-
ral pre-hospital patients can sometimes be car-
ried out by volunteers from local communities:
Community First Responders (CFR). Their task is
to assess patients, perform potentially life-saving
first aid procedures and record medical observa-
tions whilst the ambulance clinicians are en-route.
These data are then handed over to the receiv-
ing ambulance team upon arrival. Because of
their time-critical nature, handover reports are of-
ten verbal and hence maybe incomplete or misun-
derstood.
MIME was inspired by the Babytalk BT-Nurse
system (Hunter et al, 2012), which generates shift
handover reports for nurses in a neonatal intensive
care unit. While BT-Nurse works with an exist-
ing clinical record system, which does not always
At 02:12, after RR remained fairly
constant around 30 bpm for 4 minutes,
high flow oxygen was applied, she took
her inhaler and RR decreased to 27
bpm. However, subsequently RR once
more remained fairly constant around
30 bpm for 8 minutes.
At 02:15 she was feeling faint.
At 02:15 the casualty was moved.
At 02:17 the casualty was once more
moved.
Figure 1: Part of the ?Treatment and Findings? for
an asthma scenario.
record all actions and observations which ideally
would be included in a report, in MIME the elec-
tronic record and user interface for acquiring ex-
actly the desired information are effectively de-
signed. This simplifies the NLG task, at the cost
of adding a new task (interface construction).
2 The MIME project
Pre-hospital care is especially challenging because
the environment in which it is delivered is inher-
ently unpredictable. The clinical condition of a
patient may have improved or deteriorated since
the original call for help. The unpredictability of
the environment at the scene of the call and the
minimal level of clinical training of the CFRs con-
tributes to the challenges presented to developers
of a mobile device for this situation. In particular,
the continuous capture of physiological data intro-
duces the problem that irrelevant material needs
to be suppressed in order not to overload the am-
bulance clinicians and hinder interpretation. The
generated reports must provide a quick overview
of the situation but at the same time be compre-
hensive. It is also vital that the format must en-
hance the readability, and the user-interface be
simple and intuitive in order to avoid what has
198
Figure 2: First hardware prototype of the MIME
project (GETAC Z710 tablet and Pulse Oxymeter
sensor).
been termed ?creeping featurism? (His and Potts,
2000), whereby option saturation hinders task per-
formance.
In a user centred development process we estab-
lished a structure for the handover reports. After
the demographic description of the casualty (i.e.
age and gender) and incident details that were re-
layed to the CFR by the ambulance control centre
two elements of generated text follow, the initial
assessment section and the treatment and findings
section. The initial assessment contains informa-
tion on the casualty that is gathered by the CFRs
before the sensors are applied including baseline
observation during the first minute after the ap-
plication of the sensors. The treatment and find-
ings section (Figure 1) is a report of the observa-
tions and actions of the CFRs while they attended
the casualty and waited for the ambulance to ar-
rive. This includes a paragraph that sums up the
condition of the patient at the time of handover.
There are three types of events included in the re-
port: discrete events (action and observation) and
continuous events (trends in sensor readings). Ac-
tions (e.g. applying oxygen) and observations (e.g.
the patient feels faint) have to be entered by the
CFR through an interface. Continuous events are
derived from the medical sensors: currently res-
piratory rate, blood oxygen saturation, and heart
rate are recorded. Since some events, especially
those that deviate from the norm are more impor-
tant than others (Hallett et al, 2006), in the docu-
ment planning stage we employ an algorithm that
decides which events are mentioned in the report
and in which order. This process is loosely based
on similar decision processes reported in (Hallett
et al, 2006) and (Portet et al, 2007).
3 Summary and Conclusion
We have developed a first prototype of the system
which uses simulated data to produce handover re-
ports. This runs on standard desktop PCs. For our
second prototype, which is currently being devel-
oped, we port the NLG algorithm onto a GETAC
Z710 tablet1 which has been chosen for it?s robust-
ness, capacitative touch screen, and long battery
life (Figure 2). Our research also includes the es-
tablishment of a connection between the tablet and
sensors, the recording of the incoming data stream
and the development of an interface for the tablet,
which can be used by the CFR to enter observa-
tions and actions taken or any other useful infor-
mation.
At the ENLG workshop we will present our first
hardware prototype alongside the desktop com-
puter version, highlighting the challenges that the
project faces in developing a handover report gen-
erator for pre-hospital care.
4 Acknowledgments
This work is supported by the RCUK dot.rural
Digital Economy Research Hub, University of Ab-
erdeen (Grant reference: EP/G066051/1)
References
C. Hallett, R. Power, and D. Scott. 2006. Summarisa-
tion and visualisation of e-Health data repositories.
In UK E-Science All-Hands Meeting, pages 18?21,
Nottingham, UK.
I. His and C. Potts. 2000. Studying the Evolution and
Enhancement of Software Features. In Proceedings
of the International Conference on Software Mainte-
nance, ICSM ?00, pages 143?151, Washington, DC,
USA.
J. Hunter, Y. Freer, A. Gatt, E. Reiter, S. Sripada, and
C Sykes. 2012. Automatic generation of natural
language nursing shift summaries in neonatal in-
tensive care: BT-Nurse. Artificial Intelligence in
Medicine, 56:157?172.
F. Portet, E. Reiter, J. Hunter, and S. Sripada. 2007.
Automatic generation of textual summaries from
neonatal intensive care data. In Proccedings of
the 11th Conference on Artificial Intelligence in
Medicine (AIME 07). LNCS, pages 227?236.
1http://en.getac.com/products/Z710/
Z710_overview.html
199
Proceedings of the 8th International Natural Language Generation Conference, pages 123?127,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Generating Annotated Graphs using the NLG Pipeline Architecture
Saad Mahamood, William Bradshaw and Ehud Reiter
Arria NLG plc
Aberdeen, Scotland, United Kingdom
{saad.mahamood, william.bradshaw, ehud.reiter}@arria.com
Abstract
The Arria NLG Engine has been extended
to generate annotated graphs: data graphs
that contain computer-generated textual
annotations to explain phenomena in those
graphs. These graphs are generated along-
side text-only data summaries.
1 Introduction
Arria NLG1 develops NLG solutions, primarily in
the data-to-text area. These solutions are NLG
systems, which generate textual summaries of
large numeric data sets. Arria?s core product is
the Arria NLG Engine,2 which is configured and
customised for the needs of different clients.
Recently Arria has extended this core engine
so that it can automatically produce annotated
graphs, that is, data graphs that have textual an-
notations explaining phenomena in those graphs
(see example in Figure 1). This was developed af-
ter listening to one of our customers, whose staff
manually created annotated graphs and found this
process to be very time-consuming. The anno-
tated graph generation process is integrated into
the NLG pipeline, and is carried out in conjunc-
tion with the generation of a textual summary of a
data set.
In this short paper we summarise the relevant
research background, and briefly describe what we
have achieved in this area.
2 Background: Multimodality and NLG
Rich media such as web pages and electronic doc-
uments typically include several modalities in a
given document. A web page, for example, can
contain images, graphs, and interactive elements.
Because of this there has been an interest within
1Arria NLG plc (https://www.arria.com)
2For more information see: https://www.arria.
com/technology-A300.php
the NLG community in generating multimodal
documents. However, basic questions remain as
how best to combine and integrate multiple modal-
ities within a given NLG application.
2.1 Annotated Graphics
Sripada and Gao (2007) conducted a small study
where they showed scuba divers different possi-
ble outputs from their ScubaText system, including
text-only, graph-only and annotated graphs. They
found that divers preferred the annotated graph
presentation. The ScubaText software could not in
practice produce annotated graphs for arbitrary in-
put data sets and automatically set the scale based
on detected events, so this was primarily a study
of user preferences.
McCoy and colleagues have been developing
techniques to automatically generate textual sum-
maries of data graphics for visually impaired users
(Demier et al., 2008). This differs from our work
because their goal is to replace the graph, whereas
our goal is to generate an integrated text/graphics
presentation.
There were several early systems in the 1990s
(Wahlster et al., 1993; Feiner and McKeown,
1990, for example), which generated integrated
presentations of figures and texts, but these sys-
tems focused on annotating static pictures and dia-
grams, not data graphics. The WIP system, which
combined static diagram images and text, used a
deep planning approach to produce tightly inte-
grated multimodal documents; it is not clear how
robustly this approach handled new data sets and
contexts.
2.2 Embodied Conversational Agents
In recent years the challenge of combining mul-
tiple modalities such as text, speech, and/or ani-
mation has been addressed in the context of Em-
bodied Conversational Agents or ECAs. One ex-
ample is the NECA system (Krenn et al., 2002).
123
It allowed two embodied agents to converse with
each other via spoken dialogue while being able
to make gestures as well. From an architectural
perspective, NECA used a pipeline architecture in
some ways similar to the standard NLG data-to-
text pipeline (Reiter and Dale, 2000). Document
Planning is handled by the Scene Generator, which
selects the dialogue content. The ?Multi-modal
NLG? (M-NLG) component handles Microplan-
ning and Surface Realisation, and also deals with
specifying gestures, mood, and information struc-
ture. Thus the textual output generated by the sur-
face realiser in the NECA M-NLG component is
annotated with metadata for other modalities. In
particular, information on gestures, emotions, in-
formation structure, syntactic structure and dia-
logue structure (Krenn et al., 2002) are also in-
cluded to help inform the speech synthesis and
gesture assignment modules.
2.3 Background: Psychology
The question of whether information is best pre-
sented in text or graphics is in principle largely one
for cognitive psychology. Which type of presenta-
tion is most effective, and in which context? The
answer of course depends on the communicative
goal, the type of data being presented, the type of
user, the communication medium and other con-
textual factors.
In particular, a number of researchers (Petre,
1995, for example) have pointed out that graphical
presentations require expertise to interpret them
and hence may be more appropriate for experi-
enced users than for novices. Tufte (1983) points
out that statistical graphs can be very misleading
for people who are not used to interpreting them.
Alberdi et al (2001) report on a number of psy-
chological studies on effectiveness of data visual-
isations which were performed with clinicians in
a Neonatal Intensive Care Unit (NICU). At a high
level, these studies found that visualisations were
less effective and less used than had been hoped.
Detailed findings include the following:
? Although consultants, junior doctors, and
nurses all claimed in interviews to make
heavy use of the computer system which
displayed visualisations, when observed on-
ward only senior consultants actually did so;
junior doctors and nurses rarely looked at the
computer screen.
? Senior consultants were much better than ju-
nior staff at distinguishing real events from
noise (sensor artefacts).
? Even senior consultants could only identify
70% of key events purely from the visualisa-
tions.
Law et al (2005) followed up the above work by
explicitly comparing the effectiveness of visuali-
sations and textual summaries. The textual sum-
maries in the experiment were manually written,
but did not contain any diagnoses and instead fo-
cused on describing the data. Law et al found that
clinicians at all levels made better decisions when
showed the textual summaries; however at all lev-
els they preferred the visualisations.
A similar study with computer generated sum-
mary texts produced by the Babytalk BT45 sys-
tem (Portet et al., 2009), conducted by van der
Meulen et al (2008), found that decision quality
was best when clinicians were shown manually
written summaries; computer generated texts were
of similar effectiveness to visualisations. An er-
ror analysis of this study (Reiter et al., 2008) con-
cluded that computer generated texts were much
more effective in some contexts than in others.
An implication of the above studies is that in
many cases the ideal strategy is to produce both
text and graphics. This increases decision effec-
tiveness (since the modalities work best in differ-
ent situations), and also increases user satisfaction,
since users see the modality they like as well as the
one which is most effective for decision support.
2.4 Annotated Graphs in NLG Engine
We have extended our NLG Engine to generate an-
notated graphs as well as texts; an example output,
generated by a demonstration system, is shown in
figure 1. This example shows a very simple textual
output; examples of more complex textual output
are on the Arria website3.
This example output shows a comparison of
performance between the FTSE 100 and a given
stock portfolio. The value of the FTSE 100 is used
as a performance benchmark to see if a given stock
portfolio is performing better or worse than com-
pared to the stock market in general.
As can be seen in the graph in figure 1, the anno-
tations are small text fragments, which are placed
3A more detailed example is given here: https://
www.arria.com/case-study-oilgas-A231.php
124
Figure 1: Combined text and annotated graph detailing the stock portfolio performance
Figure 2: Graph illustrating stacking capabilities
when two annotations intersect each other
on top of the graph, and are linked to the relevant
events in that occur in the graph. Annotations can
also be placed at the bottom of graphs and at the
sides and can rearrange themselves depending on
the space available. In figure 1 the range annota-
tion used indicates the reason for the increase in
the value of a given stock portfolio over a particu-
lar time period. If one or more annotations collide
or intersect a stacking algorithm is used prior to
presentation to rearrange the placement of collid-
ing annotations as shown in figure 2.
Figure 3 illustrates the architecture that is used
by our NLG engine. The data analysis and data
interpretation modules analyse the input data and
produce a set of messages which can be communi-
cated to the user in the generated report. The doc-
ument planner decides on which messages should
be communicated overall, and where messages
should appear (for example, situational analysis
text, diagnosis text, impact text, graph annotation,
or a combination of these). The document planner
also decides on the type of graph used, and which
data channels it displays; these data channels must
include any channels which are annotated, but in
some cases other channels are displayed as well.
Once document planning is complete, the vi-
sualisation planning module generates the graph
design, including X and Y scale and the position
of the annotations on the graph. The time range
shown in the graph is largely determined by the
annotation messages. In other words, the decision
about what data to show on the graph is partially
driven by the annotations.
The annotation microplanner and realiser gener-
ate the actual annotation texts, using special rules
which are optimised for annotations (which need
125
Figure 3: Pipeline architecture of the Arria NLG
Engine
to be short and have different referring expres-
sions). After this has been completed, a renderer
produces the actual annotated graph. The final
task lies with the presenter module, which recom-
bines the separately generated summary text (gen-
erated by the NLG Microplanning and Realisation
modules) with the annotated graphs.
3 Conclusion
Annotated graphs are a very appealing mechanism
for combining text and data graphics into a sin-
gle multimodal information presentation; this is
shown both by the findings of Sripada and Gao
(2007) and by the experiences of our customers.
Amongst other benefits, we believe that annotated
graphs will address some of the deficiencies in
data graphics which were pointed out by Alberdi
et al (2001), by helping users (especially inexpe-
rienced ones) to more easily identify key events
in a graph and also to distinguish real events from
sensor artefacts and other noise.
We have developed software to create annotated
graphs, by modifying the standard NLG data-to-
text pipeline as described above. Our clients have
reacted very positively so far, and we are now ex-
ploring extensions, for example by making anno-
tated graphs interactive.
References
E. Alberdi, J. C. Becher, K. J. Gilhooly, J. R.W. Hunter,
R. H. Logie, A. Lyon, N. McIntosh, and J. Reiss.
2001. Expertise and the interpretation of comput-
erised physiological data: Implications for the de-
sign of computerised physiological monitoring in
neonatal intensive care. International Journal of
Human Computer Studies, 55(3):191?216.
S. Demier, S. Carberry, and K. F. McCoy. 2008. Gen-
erating textual summaries of bar charts. In Fifth
International Natural Language Generation Con-
ference (INLG 2008), pages 7?15. Association for
Computational Linguistics.
S. Feiner and K. R. McKeown. 1990. Generating Co-
ordinated Multimedia Explanations. In Sixth Con-
ference on Artificial Intelligence Applications, vol-
ume 290-296.
B. Krenn, H. Pirker, M. Grice, S. Baumann, P. Pi-
wek, K. van Deemter, M. Schroeder, M. Klesen, and
E. Gstrein. 2002. Generation of multi-modal di-
alogue for a net environment. In Proceedings of
KONVENS-02, Saarbruecken, Germany.
A. S. Law, Y. Freer, J. Hunter, R. H. Logie, N. McIn-
tosh, and J. Quinn. 2005. A comparison of graph-
ical and textual presentations of time series data to
support medical decision making in the neonatal in-
tensive care unit. Journal of Clinical Monitoring
and Computing, 19(3):183?194.
M. Petre. 1995. Why Looking isn?t always See-
ing: Readership Skills and Graphical Programming.
Communications of the ACM, 38:33?44.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173(7-8):789?816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
E. Reiter, A. Gatt, F. Portet, and M. van der Meulen.
2008. The Importance of Narrative and Other
Lessons from an Evaluation of an NLG System that
Summarises Clinical Data. Fifth International Natu-
ral Language Generation Conference (INLG 2008),
pages 147?155.
S. G. Sripada and F. Gao. 2007. Summarizing dive
computer data: A case study in integrating textual
and graphical presentations of numerical data. In
MOG 2007 Workshop on Multimodal Output Gen-
eration, pages 149?157.
126
E. Tufte. 1983. The Visual Display of Quantitative
Information. Graphics Press.
M. van der Meulen, R. Logie, Y. Freer, C. Sykes,
N. McIntosh, and J. Hunter. 2008. When a graph
is poorer than 100 words: A comparison of com-
puterised natural language generation, human gen-
erated descriptions and graphical displays in neona-
tal intensive care. Applied Cognitive Psychology,
24:77?89.
W. Wahlster, E. Andre?, W. Finkle, HJ. Profitlich, and
T. Rist. 1993. Plan-based integration of natural
language and graphics generation. Artificial Intel-
ligence, 63:387?427.
127
