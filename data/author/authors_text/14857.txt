From a Surface Analysis to a Dependency Structure
Lu??sa Coheur
L2F INESC-ID / GRIL
Lisboa, Portugal
Luisa.Coheur@l2f.inesc-id.pt
Nuno Mamede
L2F INESC-ID / IST
Lisboa, Portugal
Nuno.Mamede@inesc-id.pt
Gabriel G. Be`s
GRIL / Univ. Blaise-Pascal
Clermont-Ferrand, France
BesGabriel@yahoo.fr
Abstract
This paper describes how we use the arrows
properties from the 5P Paradigm to generate
a dependency structure from a surface analy-
sis. Besides the arrows properties, two mod-
ules, Algas and Ogre, are presented. Moreover,
we show how we express linguistic descriptions
away from parsing decisions.
1 Introduction
Following the 5P Paradigm (Be`s, 1999; Hage`ge,
2000; Be`s and Hage`ge, 2001) we build a
syntactic-semantic interface which obtains a
graph from the analysis of input text. The
graph express a dependency structure, which is
the domain of a function that will obtain as out-
put a logic semantic interpretation.
The whole syntactic-semantic interface is in-
tegrated by four modules: Susana in charge
of surface analysis, Algas and Ogre, defining
the graph, and ASdeCopas, that obtains the
logic semantic representation. In this paper
we present the first three modules, focussing
mainly on Algas and Ogre.
5P argues for a carefully separation between
linguistic descriptions and algorithms. The first
ones are expressed by Properties and the last
ones by Processes. Futhermore, linguistic mod-
elised and formalised descriptions (i.e. Prop-
erties, P2 of 5P) are not designed to be the
declarative source of algorithms, but rather as
a repository of information (Hage`ge and Be`s,
2002) that one should be able to re-use (to-
tally or partially) in each task. Following and
completing this, we assume that the parsing is-
sue can be viewed from at least three different
points of view: (i) modelised and formalised
linguistic observation; (ii) computational effec-
tive procedures; (iii) useful computational con-
straints. These three aspects of the same issue
are distinctly tackled in the proposed syntactic-
semantic interface, but they converge in the ob-
tention of results.
There are three different kinds of Properties
(P2) in 5P: existence, linearity and arrow prop-
erties. The first two underly the Susana module
(3.1). They express which are the possible mor-
phological categories of some expression and the
possible order between them. The third ones
arrow properties specify arrow pairs, which for-
mally are directed arcs of a graph. Arrow prop-
erties underly the Algas (3.2) and Ogre (3.3)
modules. At the level of Projections (i.e. P3
of 5P) the balanced parentheses structure un-
derlying sentences is exploited (2). Computa-
tional useful constraints improve Algas perfor-
mance (5).
2 Arrow properties
The motivation behind an arrow property is to
connect two elements, because the established
relation is needed to reach the desired semantic
representation (Be`s, 1999). Notice that this for-
malism can be applied to establish dependencies
either between words, chunks or phrases. Nev-
ertheless, arrows can be seen as dependencies
but, contrary to the main dependency theories,
an arrow is not labeled and go from dependents
to the head (Hage`ge, 2000).
Let C be the set of category labels available,
M the set of chunk labels, P a set of phrase
labels and I a set of indexes.
Arrow Property: An arrow property is a
tuple (X, n, Z, Y, m, R+, R?) noted by:
Xn ?Z Ym,
+R+
-R?
where:
? X, Y ? M ? C (X is said to be the source
and Y the target of the arrow);
? Z ? M ? P (the segment labeled Z contains
X and Y);
? R+, R? are sets of constraints over the ar-
rows (respectively, the set of constraints
that Z must verify, either positive ones
(R+) on symbols which must be attested or
negative ones (R?) on symbols which must
not occur);
? n, m ? I.
Both R+, R? impose simple constraints over
the arrows, such as symbols that should or
should not occur within Z or linear order re-
lations that should be satisfied between its con-
stituents. As an example, the following ar-
row property says that within an interroga-
tive phrase (Pint), an interrogative chunk (IntC)
with an interrogative pronoun inside (pint) ar-
rows a nominal chunk (NC) on its right (i ?
k), as long as there is no other nominal chunk
between them (i ? j ? k).
IntCi({pint}/) ?Pint NCk
-{NCj}
A more complex type of constraint is the
?stack? constraint (Coheur, 2004). This con-
straint is based on the linguistically motivated
work over balanced parentheses of (Be`s and
Dahl, 2003; Be`s et al, 2003). Briefly, the
idea behind that work is the following: given
a sentence, if we introduce a left parentheses
everytime we find a word such as que(that),
se(if ), ...) ? the introducers ? and a right
parentheses everytime we find an inflected
verbal form1, at the end of the sentence, the
number of left parentheses is equal to the
number of right ones, and at any point of it,
the number of left ones is equal or greater
that the number of right ones (Be`s and Dahl,
2003). In (Be`s and Dahl, 2003), they use this
natural language evidence in order to identify
the main phrase, relatives, coordinations, etc.
Within our work, we use it to precise arrowing
relations. For example, consider the sentence
Quais os hote?is que te?m piscina? (Which are
the hotels that have a swimming pool? ). The
surface analysis of this statement results in the
following (where VC stands for verbal chunk):
(Quais)IntC (os hote?is)NC (que)RelC
(te?m)V C (piscina)NC
Typically the NC os hote?is arrows the main
VC, but in this situation, as there is no main VC
we want it to arrow itself. Nevertheless, there is
an arrow property saying that an NC can arrow
a VC, which applied to this particular situation
1See (Be`s and Dahl, 2003) for details about how to
deal with coordination.
would establish a wrong dependency (Figure 1).
Figure 1: Wrong dependency
Roughly, we use the stack constraint that says
that an NC arrows a VC if the stack of introduc-
ers and flexioned verbs is empty between them2:
NCi ?S VCk
+{stackj = [ ]}
As a result, if we consider again the example
Quais os hote?is que te?m piscina, the NC hote?is
will not arrow the VC te?m, because the stack
constraint is not verified between them (there
is only the introducer que).
3 Reaching the dependency
structure
3.1 Surface analysis
From existence and linearity properties (P2
of 5P) specifiyng chunks, it can be deduced
what categories can or must start a chunk,
and which ones can or must be the last one.
Drawing on this linguistic information, chunks
are detected in a surface analysis made by
Susana (Batista and Mamede, 2002). As an
example, consider the question Qual a maior
praia do Algarve? (Which is the biggest beach
in Algarve? ). Susana outputs the following
surface analysis (where PC stands for preposi-
tional chunk):
(Qual)IntC (a maior praia)NC (do Al-
garve)PC (? )Ponct
3.2 Algas
Algas is the C++ program responsible for con-
necting chunks and the elements inside them,
taking as input a structure that contains infor-
mation from arrow properties and also informa-
tion that can limit the search space (see section
4 from details about this). Additionally, as in-
side the majority of the chunks all the elements
arrow the last element (the head), the user can
declare which are the chunks that verify this
property. As a result, no calculus need to be
made in order to compute dependencies inside
these chunks: all its elements arrow the last one.
This possibility is computational very usefull.
2In fact, this restriction is a little more complicated
than this.
Continuing with our example, after Algas ex-
ecution, we have the output from Figure 2.
Both the IntC and the PC chunks arrow the
NC and inside them, all the elements arrow the
head.
Figure 2: Algas?s output.
Algas is able to skip unalyzable parts of a
sentence, but (for the moment) some constraints
are made to its output:
(1) There is at most an element arrowing itself,
inside each chunk;
(2) Cycles are not allowed;
(3) Arrow crossing is not allowed (projectiv-
ity);
(4) An element cannot be the target of an ar-
row if it is not the source of any arrow.
Notice that these constraints are made inside
the program. Notice that, in particular the pro-
jectivity requirement is not imposed by 5P. We
impose it, due to the fact that ? for the moment
? we are only dealing with written Portuguese,
that typically respects this property.
3.3 Ogre
After Algas, the text is processed by Ogre, a
pipeline of Perl and XSLT scripts, that gener-
ates a graph from the arrowed structures pro-
duced by Algas3. This process is based on the
following: if a chunk arrows another chunk, the
head of the first chunk will arrow the head of
the second chunk, and the chunk label can be
omitted.
Continuing with our example, after Ogre we
have the graph of Figure 3 (a dependency struc-
ture). Basically, IntC and PC head ? respec-
tively qual and Algarve ? arrow now the NC
head.
Figure 3: Ogre?s output.
3Arrowed structures produced by Algas can also be
seen as a graph, having nodes containing graphs.
It might seem that we are keeping away infor-
mation in this step, but the new arrowing rela-
tion between chunk heads keeps the lost struc-
tures. Beside, as information about the direc-
tion of the arrows is kept, and the position of
each word is also kept in the graph, we are
still able to distinguish behaviours dependent
on word order for the following semantic task.
That is, both semantic relations and word order
are kept within our graph.
Ogre?s motivation is to converge different
structures into the same graph. For example,
after Ogre?s execution O Ritz e? onde?, E? onde
o Ritz? and Onde e? o Ritz?, they all share the
same graph (appart from positions).
4 From descriptions to the
algorithm input structures
In order to keep descriptions apart from pro-
cessing, arrow properties and Algas input struc-
tures are developed in parallel. Then, arrow
properties are formally mapped into Algas in-
put structures (see (Coheur, 2004) for details).
This decision allowed us to add computational
constraints to Algas input structures, leaving
descriptions untouchable.
In fact, in order to reduce the search space,
Algas has the option of letting the user control
the distance between the source and the target
of an arrow. This is particularly very usefull
to control PP attachments (in this case PC
attachments). Thus, if we want a PC to arrow
an NC that is at most n positions away, we
simply say:
PC ?S NC [{NC <n PC}/]
Notice that we could make an extension over
the arrow properties formalism in order to al-
low this kind of information. Nevertheless, it
is well know that in natural language there is
no fix distance between two elements. Adding a
distance constraint over arrow properties would
add procedural information to a repository re-
sulting from natural language observations.
5 Applications
Both Algas and Ogre are part of a syntactic-
semantic interface, where the module responsi-
ble for the generation of logical forms is called
AsdeCopas (Coheur et al, 2003). This interface
has been applied in a semantic disambiguation
task of a set of quantifiers and also in question
interpretation.
Notice that, although arrows are not labeled,
the fact that we know its source, target and
direction, give us enough information to find
(or at least guess) a label for it. In fact, we
could add a label to the majority of the ar-
rows. For example, using the link-types from
the Link Grammar (Sleator and Temperley,
1993; Sleator, 1998), if an adverb connects an
adjective, this connection would be labeled EA,
if an adverb connects another adverb, the la-
bel would be EE. AsdeCopas can be used to
add this information to the graph. Neverthe-
less, the fact that we are using an unlabelled
connection serves languages as Portuguese par-
ticularly well. In Portuguese, it is not 100% sure
that we are able to identify the subject. For
example, we can say ?O Toma?s come a sopa.?,
?Come a sopa o Toma?s.?, or even ?A sopa come
o Toma?s.? having all the same (most probable)
interpretation: Thomas eats the soup. That is,
there is no misleading interpretation due to our
knowledge of the world: a man can eat a soup,
but a soup cannot eat a man. As so, arrow prop-
erties simply establish relations, and we leave to
semantic analysis the task of deciding what is
the nature of these relations.
6 Conclusions
We presented two modules ? Algas and Ogre
? that build a dependency graph from a sur-
face analysis. Algas uses information from a
formalism called arrows properties. Neverthe-
less this formalism is independent from Algas
input structures, that can be enriched with in-
formation that limits the relations to establish.
In the future we want the user to be able to
control the constraints over Algas output. That
is, the user will have the option to chose if out-
put may contain arrows crossing or not.
For the moment the Susana-Algas-Ogre mod-
ules of the syntactic-semantic interface behave
without problems in the domain of question
interpretation. They apply successfully to an
elicited corpus of questions produced by N por-
tuguese speakers which were asked to produce
them simulating effective and natural questions.
Our next step is to try to use them incremen-
tally (A??t-Mokhtar et al, 2002).
Also, another improvement will be over arrow
properties, as we want to organise them in a
hierarchy.
7 Acknowledgements
This paper was supported by FCT (Fundac?a?o
para a Cie?ncia e Tecnologia) and by Project
POSI/PLP/41319/2001 (FEDER).
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and
Claude Roux. 2002. Robustness beyound
shallowness: incremental deep parsing. Nat-
ural Language Engineering, pages 121?144.
Fernando Batista and Nuno Mamede. 2002.
SuSAna: Mo?dulo multifuncional da ana?lise
sinta?ctica de superf??cie. In Julio Gonzalo,
Anselmo Pen?as, and Antonio Ferra?ndez, ed-
itors, Proc. Multilingual Information Access
and Natural Language Processing Workshop
(IBERAMIA 2002), pages 29?37, Sevilla,
Spain, November.
Gabriel G. Be`s and Veronica Dahl. 2003. Bal-
anced parentheses in nl texts: a useful cue
in the syntax/semantics interface. In Nacy
Workshop on Prospects and Advances in the
Syntax/Semantics Interface.
Gabriel G. Be`s and Caroline Hage`ge. 2001.
Properties in 5P. Technical report, GRIL,
Universite? Blaise-Pascal, Clermont-Ferrand,
France, November.
Gabriel G. Be`s, Veronica Dahl, Daniel Guil-
lot, Lionel Lamadon, Ioana Milutinovici, and
Joana Paulo. 2003. A parsing system for bal-
anced parentheses in nl texts. In CLIN?2003.
Gabriel G. Be`s. 1999. La phrase verbal noyau
en franc?ais. In in Recherches sur le franc?ais
parle?, volume 15, pages 273?358. Universite?
de Provence, France.
Lu??sa Coheur, Nuno Mamede, and Gabriel G.
Be?s. 2003. ASdeCopas: a syntactic-semantic
interface. In Epia, Beja, Portugal, Dezembro.
Springer-Verlag.
Lu??sa Coheur. 2004. A interface entre a sin-
taxe e a sema?ntica no quadro das l??nguas
naturais. Ph.D. thesis, Instituto Superior
Te?cnico, Universidade Te?cnica de Lisboa,
Portugal, Universite? Blaise-Pascal, France.
work in progress.
Caroline Hage`ge and Gabriel G. Be`s. 2002. En-
coding and reusing linguistic information ex-
pressed by linguistic properties. In Proceed-
ings of COLING?2002, Taipei.
Caroline Hage`ge. 2000. Analyse Syntatic
Automatique du Portugais. Ph.D. thesis,
Universite? Blaise Pascal, Clermont-Ferrand,
France.
Daniel Sleator and Davy Temperley. 1993.
Parsing english with a link grammar. In Pro-
ceedings of the Third International Workshop
on Parsing Technologies.
Daniel Sleator. 1998. Summary of link types.
A step towards incremental generation of logical forms
Lu??sa Coheur
L2F INESC-ID / GRIL
Lisboa, Portugal
Luisa.Coheur@l2f.inesc-id.pt
Nuno Mamede
L2F INESC-ID / IST
Lisboa, Portugal
Nuno.Mamede@inesc-id.pt
Gabriel G. Be`s
GRIL / Univ. Blaise-Pascal
Clermont-Ferrand, France
Gabriel.Bes@univ-bpclermont.fr
Abstract
This paper presents AsdeCopas, a module de-
signed to interface syntax and semantics. Asde-
Copas is based on hierarchically organised se-
mantic rules, that output formulas in a flat lan-
guage. In this paper, we show how this system
can be used in the following applications: a) se-
mantic disambiguation; b) logical formulas con-
struction (in Minimal Recursion Semantics); c)
question interpretation.
1 Introduction
We present AsdeCopas, a syntax-semantic in-
terface based on hierarchically organised rules.
AsdeCopas is integrated in a system where
the input text is first transformed into a graph
and then passed to AsdeCopas. AsdeCopas can
be used in several ways.
It can be used to enrich the graph (Figure 1),
for example, by labeling its arrows.
          	 

 


 
    
  
 
Figure 1: Enriching the graph
It can be used in a desambiguation process
and to generate logical formulas. In this paper
we show how AsdeCopas can be used to choose
between several semantic values of some quan-
tifiers and also how it can generate underspec-
ified formulas in Minimal Recursion Semantics
(MRS) (Copestake et al, 2001). Additionally,
it can be used to add constraints to these under-
specified formulas. As AsdeCopas makes a con-
troled generation of variables, these new formu-
las can be simply added to the previous under-
specified MRS formulas and the rules respon-
sible for generating MRS underspecified struc-
tures remain unchangeable.
       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450?454,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Modeling using Weighted Alignment Matrices
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur and Isabel Trancoso
L2F Spoken Systems Lab
INESC-ID Lisboa
{wang.ling,tiago.luis,joao.graca}@inesc-id.pt
{luisa.coheur,isabel.trancoso}@inesc-id.pt
Abstract
In most statistical machine translation sys-
tems, the phrase/rule extraction algorithm uses
alignments in the 1-best form, which might
contain spurious alignment points. The usage
of weighted alignment matrices that encode all
possible alignments has been shown to gener-
ate better phrase tables for phrase-based sys-
tems. We propose two algorithms to generate
the well known MSD reordering model using
weighted alignment matrices. Experiments on
the IWSLT 2010 evaluation datasets for two
language pairs with different alignment algo-
rithms show that our methods produce more
accurate reordering models, as can be shown
by an increase over the regular MSD models
of 0.4 BLEU points in the BTEC French to
English test set, and of 1.5 BLEU points in the
DIALOG Chinese to English test set.
1 Introduction
The translation quality of statistical phrase-based
systems (Koehn et al, 2003) is heavily dependent
on the quality of the translation and reordering mod-
els generated during the phrase extraction algo-
rithm (Ling et al, 2010). The basic phrase extrac-
tion algorithm uses word alignment information to
constraint the possible phrases that can be extracted.
It has been shown that better alignment quality gen-
erally leads to better results (Ganchev et al, 2008).
However the relationship between the word align-
ment quality and the results is not straightforward,
and it was shown in (Vilar et al, 2006) that better
alignments in terms of F-measure do not always lead
to better translation quality.
The fact that spurious word alignments might oc-
cur leads to the use of alternative representations for
word alignments that allow multiple alignment hy-
potheses, rather than the 1-best alignment (Venu-
gopal et al, 2009; Mi et al, 2008; Christopher
Dyer et al, 2008). While using n-best alignments
yields improvements over using the 1-best align-
ment, these methods are computationally expen-
sive. More recently, the method described in (Liu
et al, 2009) produces improvements over the meth-
ods above, while reducing the computational cost
by using weighted alignment matrices to represent
the alignment distribution over each parallel sen-
tence. However, their results were limited by the
fact that they had no method for extracting a reorder-
ing model from these matrices, and used a simple
distance-based model.
In this paper, we propose two methods for gener-
ating the MSD (Mono Swap Discontinuous) reorder-
ing model from the weighted alignment matrices.
First, we test a simple approach by using the 1-best
alignment to generate the reordering model, while
using the alignment matrix to produce the translation
model. This reordering model is a simple adaptation
of the MSD model to read from alignment matrices.
Secondly, we develop two algorithms to infer the re-
ordering model from the weighted alignment matrix
probabilities. The first one uses the alignment infor-
mation within phrase pairs, while the second uses
contextual information of the phrase pairs.
This paper is organized as follows: Section 2 de-
scribes the MSD model; Section 3 presents our two
algorithms; in Section 4 we report the results from
the experiments conducted using these algorithms,
450
and comment on the results; we conclude in Sec-
tion 5.
2 MSD models
Moses (Koehn et al, 2007) allows many config-
urations for the reordering model to be used. In
this work, we will only refer to the default config-
uration (msd-bidirectional-fe), which uses the MSD
model, and calculates the reordering orientation for
the previous and the next word, for each phrase pair.
Other possible configurations are simpler than the
default one. For instance, the monotonicity model
only considers monotone and non-monotone orien-
tation types, whereas the MSD model also considers
the monotone orientation type, but distinguishes the
non-monotone orientation type between swap and
discontinuous. The approach presented in this work
can be adapted to the other configurations.
In the MSD model, during the phrase extraction,
given a source sentence S and a target sentence T ,
the alignment set A, where aji is an alignment from i
to j, the phrase pair with words in positions between
i and j in S, Sji , and n and m in T , T
m
n , can be
classified with one of three orientations with respect
to the previous word:
? The orientation is monotonous if only the pre-
vious word in the source is aligned with the pre-
vious word in the target, or, more formally, if
an?1i?1 ? A ? a
n?1
j+1 /? A.
? The orientation is swap, if only the next word
in the source is aligned with the previous word
in the target, or more formally, if an?1j+1 ? A ?
an?1i?1 /? A.
? The orientation is discontinuous if neither of
the above are true, which means, (an?1i?1 ?
A ? an?1j+1 ? A) ? (a
n?1
i?1 /? A ? a
n?1
j+1 /? A).
The orientations with respect to the next word are
given analogously. The reordering model is gener-
ated by grouping the phrase pairs that are equal, and
calculating the probabilities of the grouped phrase
pair being associated each orientation type and di-
rection, based on the orientations for each direction
that are extracted. Formally, the probability of the
phrase pair p having a monotonous orientation is
prev 
word(s)
source phrase
target phrase
prev 
word(t)
next 
word(s)
source phrase
target phrase
prev 
word(t)
a) b)
c)
source phrase
target phrase
prev 
word(t)
d)
next 
word(s)
source phrase
target phrase
prev 
word(t)
prev 
word(s)
Figure 1: Enumeration of possible reordering cases with
respect to the previous word. Case a) is classified as
monotonous, case b) is classified as swap and cases c)
and d) are classified as discontinuous.
given by:
P (p,mono) = C(mono)C(mono)+C(swap)+C(disc) (1)
Where C(o) is the number of times a phrase is ex-
tracted with the orientation o in that group of phrase
pairs. Moses also provides many options for this
stage, such as types of smoothing. We use the de-
fault smoothing configuration which adds the fixed
value of 0.5 to all C(o).
3 Weighted MSD Model
When using a weighted alignment matrix, rather
than working with alignments points, we use the
probability of each word in the source aligning with
each word in the target. Thus, the regular MSD
model cannot be directly applied here.
One obvious solution to solve this problem is to
produce a 1-best alignment set alng with the align-
ment matrix, and use the 1-best alignment to gen-
erate the reordering model, while using the align-
ment matrix to produce the translation model. How-
ever, this method would not be taking advantage of
the weighted alignment matrix. The following sub-
sections describe two algorithms that are proposed
to make use of the alignment probabilities.
3.1 Score-based
Each phrase pair that is extracted using the algorithm
described in (Liu et al, 2009) is given a score based
on its alignments. This score is higher if the align-
ment points in the phrase pair have high probabili-
ties, and if the alignment is consistent. Thus, if an
451
extracted phrase pair has better quality, its orienta-
tion should have more weight than phrase pairs with
worse quality. We implement this by changing the
C(o) function in equation 1 from being the number
of the phrase pairs with the orientation o, to the sum
of the scores of those phrases. We also need to nor-
malize the scores for each group, due to the fixed
smoothing that is applied, since if the sum of the
scores is much lower (e.g. 0.1) than the smoothing
factor (0.5), the latter will overshadow the weight
of the phrase pairs. The normalization is done by
setting the phrase pair with the highest value of the
sum of all MSD probabilities to 1, and readjusting
other phrase pairs accordingly. Thus, a group of 3
phrase pairs that have the MSD probability sums of
0.1, 0.05 and 0.1, are all set to 1, 0.5 and 1.
3.2 Context-based
We propose an alternative algorithm to calculate
the reordering orientations for each phrase pair.
Rather than classifying each phrase pair with either
monotonous (M ), swap (S) or discontinuous (D),
we calculate the probability for each orientation, and
use these as weighted counts when creating the re-
ordering model. Thus, for the previous word, given
a weighted alignment matrix W , the phrase pair be-
tween the indexes i and j in S, Sji , and n and m in
T , Tmn , the probability values for each orientation
are given by:
? Pc(M) = W
n?1
i?1 ? (1?W
n?1
j+1 )
? Pc(S) = W
n?1
j+1 ? (1?W
n?1
i?1 )
? Pc(D) = W
n?1
i?1 ?W
n?1
j+1
+ (1?Wn?1i?1 )? (1?W
n?1
j+1 )
These formulas derive from the adaptation of con-
ditions of each orientation presented in 2. In the
regular MSD model, the previous orientation for a
phrase pair is monotonous if the previous word in
the source phrase is aligned with the previous word
in the target phrase and not aligned with the next
word. Thus, the probability of a phrase pair to have a
monotonous orientation Pc(M) is given by the prob-
ability of the previous word in the source phrase
being aligned with the previous word in the target
phrase Wn?1i?1 , and the probability of the previous
word in the source to not be aligned with the next
word in the target (1 ? Wn?1j+1 ). Also, the sum of
the probabilities of all orientations (Pc(M), Pc(S),
Pc(D)) for a given phrase pair can be trivially shown
to be 1. The probabilities for the next word are
given analogously. Following equation 1, the func-
tion C(o) is changed to be the sum of all Pc(o), from
the grouped phrase pairs.
4 Experiments
4.1 Corpus
Our experiments were performed over two datasets,
the BTEC and the DIALOG parallel corpora from
the latest IWSLT evaluation 2010 (Paul et al, 2010).
BTEC is a multilingual speech corpus that contains
sentences related to tourism, such as the ones found
in phrasebooks. DIALOG is a collection of human-
mediated cross-lingual dialogs in travel situations.
The experiments performed with the BTEC cor-
pus used only the French-English subset, while the
ones perfomed with the DIALOG corpus used the
Chinese-English subset. The training corpora con-
tains about 19K sentences and 30K sentences, re-
spectively. The development corpus for the BTEC
task was the CSTAR03 test set composed by 506
sentences, and the test set was the IWSLT04 test set
composed by 500 sentences and 16 references. As
for the DIALOG task, the development set was the
IWSLT09 devset composed by 200 sentences, and
the test set was the CSTAR03 test set with 506 sen-
tences and 16 references.
4.2 Setup
We use weighted alignment matrices based on Hid-
den Markov Models (HMMs), which are produced
by the the PostCAT toolkit1, based on the poste-
rior regularization framework (V. Grac?a et al, 2010).
The extraction algorithm using weighted alignment
matrices employs the same method described in (Liu
et al, 2009), and the phrase pruning threshold was
set to 0.1. For the reordering model, we use the
distance-based reordering, and compare the results
with the MSD model using the 1-best alignment.
Then, we apply our two methods based on align-
ment matrices. Finally, we combine our two meth-
ods above by adapting the function C(o), to be the
1http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html
452
sum of all Pc(o), weighted by the scores of the re-
spective phrase pairs. The optimization of the trans-
lation model weights was done using MERT, and
each experiment was run 5 times, and the final score
is calculated as the average of the 5 runs, in order to
stabilize the results. Finally, the results were eval-
uated using BLEU-4, METEOR, TER and TERp.
The BLEU-4 and METEOR scores were computed
using 16 references. The TER and TERp were com-
puted using a single reference.
4.3 Reordering model comparison
Tables 1 and 2 show the scores using the differ-
ent reordering models. Consistent improvements in
the BLEU scores may be observed when changing
from the MSD model to the models generated us-
ing alignment matrices. The results were consis-
tently better using our models in the DIALOG task,
since the English-Chinese language pair is more de-
pendent on the reordering model. This is evident
if we look at the difference in the scores between
the distance-based and the MSD models. Further-
more, in this task, we observe an improvement on all
scores from the MSD model to our weighted MSD
models, which suggests that the usage of alignment
matrices helps predict the reordering probabilities
more accurately.
We can also see that the context based reordering
model performs better than the score based model
in the BTEC task, which does not perform sig-
nificantly better than the regular MSD model in
this task. Furthermore, combining the score based
method with the context based method does not lead
to any improvements. We believe this is because the
alignment probabilities are much more accurate in
the English-French language pair, and phrase pair
scores remain consistent throughout the extraction,
making the score based approach and the regular
MSD model behave similarly. On the other hand,
in the DIALOG task, score based model has bet-
ter performance than the regular MSD model, and
the combination of both methods yields a significant
improvement over each method alone.
Table 3 shows a case where the context based
model is more accurate than the regular MSD model.
The alignment is obviously faulty, since the word
?two? is aligned with both ?deux?, although it
should only be aligned with the first occurrence.
BTEC BLEU METEOR TERp TER
Distance-based 61.84 65.38 27.60 22.40
MSD 62.02 65.93 27.40 22.80
score MSD 62.15 66.18 27.30 22.20
context MSD 62.42 66.29 27.00 22.00
combined MSD 62.42 66.14 27.10 22.20
Table 1: Results for the BTEC task.
DIALOG BLEU METEOR TERp TER
Distance-based 36.29 45.15 49.00 41.20
MSD 39.56 46.85 47.20 39.60
score MSD 40.2 47.16 46.52 38.80
context MSD 40.14 47.14 45.88 39.00
combined MSD 41.03 47.69 46.20 38.20
Table 2: Results for the DIALOG task.
Furthermore, the word ?twin? should be aligned
with ?a` deux lit?, but it is aligned with ?cham-
bres?. If we use the 1-best alignment to compute
the reordering type of the sentence pair ?Je voudrais
re?server deux? / ?I?d like to reserve two?, the re-
ordering type for the following orientation would
be monotonous, since the next word ?chambres?
is falsely aligned with ?twin?. However, it should
clearly be discontinuous, since the right alignment
for ?twin? is ?a` deux lit?. This problem is less seri-
ous when we use the weighted MSD model, since
the orientation probability mass would be divided
between monotonous and discontinuous since the
probability weighted matrix for the wrong alignment
is 0.5. On the BTEC task, some of the other scores
are lower than the MSD model, and we suspect that
this stems from the fact that our tuning process only
attempts to maximize the BLEU score.
5 Conclusions
In this paper we addressed the limitations of the
MSD reordering models extracted from the 1-best
alignments, and presented two algorithms to ex-
tract these models from weighted alignment matri-
ces. Experiments show that our models perform bet-
ter than the distance-based model and the regular
MSD model. The method based on scores showed a
good performance for the Chinese-English language
pair, but the performance for the English-French pair
was similar to the MSD model. On the other hand,
the method based on context improves the results on
453
Alignment Je vo
ud
ra
is
re?
se
rv
er
de
ux
ch
am
br
es
a` de
ux
lit
s
.
I 1
?d 0.7
like 0.7
to
reserve 1
two 1 0.5
twin 0.5 0.5
rooms 1
. 1
Table 3: Weighted alignment matrix for a training sen-
tence pair from BTEC, with spurious alignment proba-
bilities. Alignment points with 0 probabilities are left
empty.
both pairs. Finally, on the Chinese-English test, by
combining both methods we can achieve a BLEU
improvement of approximately 1.5%. The code used
in this work is currently integrated with the Geppetto
toolkit2 , and it will be made available in the next
version for public use.
6 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Tiago Lu??s is supported by
FCT grant SFRH/BD/62151/2009. The PhD the-
sis of Wang Ling is supported by FCT grant
SFRH/BD/51157/2010. The authors also wish to
thank the anonymous reviewers for many helpful
comments.
References
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. Tech-
nical Report LAMP-TR-149, University of Maryland,
College Park, February.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of ACL-08: HLT, pages 986?993, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
2http://code.google.com/p/geppetto/
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Wang Ling, Tiago Lu??s, Joao Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2 - Volume 2, EMNLP ?09, pages 1017?1026,
Morristown, NJ, USA. Association for Computational
Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June. Association
for Computational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Wider pipelines: N-best
alignments and parses in MT training.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
Aer: Do we need to ?improve? our alignments? In
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 205?212.
454
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 61?66,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Meet EDGAR, a tutoring agent at MONSERRATE
Pedro Fialho, Lu??sa Coheur, Se?rgio Curto, Pedro Cla?udio
A?ngela Costa, Alberto Abad, Hugo Meinedo and Isabel Trancoso
Spoken Language Systems Lab (L2F), INESC-ID
Rua Alves Redol 9
1000-029 Lisbon, Portugal
name.surname@l2f.inesc-id.pt
Abstract
In this paper we describe a platform for
embodied conversational agents with tu-
toring goals, which takes as input written
and spoken questions and outputs answers
in both forms. The platform is devel-
oped within a game environment, and cur-
rently allows speech recognition and syn-
thesis in Portuguese, English and Spanish.
In this paper we focus on its understand-
ing component that supports in-domain in-
teractions, and also small talk. Most in-
domain interactions are answered using
different similarity metrics, which com-
pare the perceived utterances with ques-
tions/sentences in the agent?s knowledge
base; small-talk capabilities are mainly
due to AIML, a language largely used by
the chatbots? community. In this paper
we also introduce EDGAR, the butler of
MONSERRATE, which was developed in
the aforementioned platform, and that an-
swers tourists? questions about MONSER-
RATE.
1 Introduction
Several initiatives have been taking place in the
last years, targeting the concept of Edutainment,
that is, education through entertainment. Fol-
lowing this strategy, virtual characters have ani-
mated several museums all over the world: the
3D animated Hans Christian Andersen is ca-
pable of establishing multimodal conversations
about the writer?s life and tales (Bernsen and
Dybkjr, 2005), Max is a virtual character em-
ployed as guide in the Heinz Nixdorf Museums
Forum (Pfeiffer et al, 2011), and Sergeant Black-
well, installed in the Cooper-Hewitt National De-
sign Museum in New York, is used by the U.S.
Army Recruiting Command as a hi-tech attrac-
tion and information source (Robinson et al,
Figure 1: EDGAR at MONSERRATE.
2008). DuARTE Digital (Mendes et al, 2009)
and EDGAR are also examples of virtual charac-
ters for the Portuguese language with the same
edutainment goal: DuARTE Digital answers ques-
tions about Custo?dia de Bele?m, a famous work of
the Portuguese jewelry; EDGAR is a virtual butler
that answers questions about MONSERRATE (Fig-
ure 1).
Considering the previous mentioned agents,
they all cover a specific domain of knowledge (al-
though a general Question/Answering system was
integrated in Max (Waltinger et al, 2011)). How-
ever, as expected, people tend also to make small
talk when interacting with these agents. There-
fore, it is important that these systems properly
deal with it. Several strategies are envisaged to
this end and EDGAR is of no exception. In this
paper, we describe the platform behind EDGAR,
which we developed aiming at the fast insertion of
in-domain knowledge, and to deal with small talk.
This platform is currently in the process of being
industrially applied by a company known for its
expertise in building and deploying kiosks. We
will provide the hardware and software required
to demonstrate EDGAR, both on a computer and
on a tablet.
This paper is organized as follows: in Sec-
tion 2 we present EDGAR?s development platform
61
Figure 2: EDGAR architecture
and describe typical interactions, in Section 3 we
show how we move from in-domain interactions
to small talk, and in Section 4 we present an anal-
ysis on collected logs and their initial evaluation
results. Finally, in Section 5 we present some con-
clusions and point to future work.
2 The Embodied Conversational Agent
platform
2.1 Architecture overview
The architecture of the platform, generally de-
signed for the development of Embodied Con-
versational Agents (ECAs) (such as EDGAR), is
shown in Figure 2. In this platform, several mod-
ules intercommunicate by means of well defined
protocols, thus leveraging the capabilities of inde-
pendent modules focused on specific tasks, such
as speech recognition or 3D rendering/animation.
This independence allows us to use subsets of this
platform modules in scenarios with different re-
quirements (for instance, we can record characters
uttering a text).
Design and deployment of the front end of
EDGAR is performed in a game engine, which has
enabled the use of computer graphics technologies
and high quality assets, as seen in the video game
industry.
2.2 Multimodal components
The game environment, where all the interac-
tion with EDGAR takes place, is developed in the
Unity1 platform, being composed of one highly
1http://unity3d.com/
detailed character, made and animated by Rocket-
box studios2, a virtual keyboard and a push-while-
talking button.
In this platform, Automatic Speech Recogni-
tion (ASR) is performed by AUDIMUS (Meinedo
et al, 2003) for all languages, using generic acous-
tic and language models, recently compiled from
broadcast news data (Meinedo et al, 2010). Lan-
guage models were interpolated with all the do-
main questions defined in the Natural Language
Understanding (NLU) framework (see below),
while ASR includes features such as speech/non-
speech (SNS) detection and automatic gain control
(AGC). Speech captured in a public space raises
several ASR robustness issues, such as loudness
variability of spoken utterances, which is partic-
ularly bound to happen in a museological envi-
ronment (such as MONSERRATE) where silence is
usually incited. Thus, we have added a bounded
amplication to the captured signal, despite the
AGC mechanism, ensuring that too silent sounds
are not discarded by the SNS mechanism.
Upon a spoken input, AUDIMUS translates it
into a sentence, with a confidence value. An
empty recognition result, or one with low con-
fidence, triggers a control tag (? REPEAT ?) to
the NLU module, which results in a request for
the user to repeat what was said. The answer re-
turned by the NLU module is synthesized in a lan-
guage dependent Text To Speech (TTS) system,
with DIXI (Paulo et al, 2008) being used for Por-
tuguese, while a recent version of FESTIVAL (Zen
et al, 2009) covers both English and Spanish. The
2http://www.rocketbox-libraries.com/
62
synthesized audio is played while the correspond-
ing phonemes are mapped into visemes, repre-
sented as skeletal animations, being synchronized
according to phoneme durations, available in all
the employed TTS engines.
Emotions are declared in the knowledge sources
of the agent. As shown in Figure 3, they are coor-
dinated with viseme animations.
Figure 3: The EDGAR character in a joyful state.
2.3 Interacting with EDGAR
In a typical interaction, the user enters a ques-
tion with a virtual keyboard or says it to the mi-
crophone while pressing a button (Figure 4), in
the language chosen in the interface (as previously
said, Portuguese, English or Spanish).
Figure 4: A question written in the EDGAR inter-
face.
Then, the ASR will transcribe it and the NLU
module will process it. Afterwards, the answer,
chosen by the NLU module, is heard through
the speakers, due to the TTS, and sequentially
written in a talk bubble, according to the pro-
duced speech. The answer is accompanied with
visemes, represented by movements of the char-
acter?s mouth/lips, and by facial emotions as
marked in the answers of the NLU knowledge
base. A demo of EDGAR, only for English interac-
tions, can be tested in https://edgar.l2f.
inesc-id.pt/m3/edgar.php.
3 The natural language understanding
component
3.1 In-domain knowledge sources
The in-domain knowledge sources of the agent
are XML files, hand-crafted by domain experts.
This XML files have multilingual pairs consti-
tuted by different paraphrases of the same ques-
tion and possible answers. The main reason to
follow this approach (and contrary to other works
where grammars are used), is to ease the process
of creating/enriching the knowledge sources of the
agent being developed, which is typically done
by non experts in linguistics or computer science.
Thus, we opted for following a similar approach
of the work described, for instance, in (Leuski et
al., 2006), where the agents knowledge sources are
easy to create and maintain. An example of a ques-
tions/answers pair is:
<questions>
<q en="How is everything?"
es="Todo bien?">
Tudo bem?</q>
</questions>
<answers>
<a en="I am ok, thank you."
es="Estoy bien, gracias."
emotion="smile_02">
Estou bem, obrigado.</a>
</answers>
As it can been see from this example, emotions
are defined in these files, associated to each ques-
tion/answer pair (emotion=?smile? in the exam-
ple, one of the possible smile emotions).
These knowledge sources can be (automati-
cally) extended with ?synonyms?. We call them
?synonyms?, because they do not necessarily fit
in the usual definition of synonyms. Here we fol-
low a broader approach to this concept and if two
words, within the context of a sentence from the
knowledge source, will lead to the same answer,
then we consider them to be ?synonyms?. For
instance ?palace? or ?castle? are not synonyms.
However, people tend to refer to MONSERRATE in
both forms. Thus, we consider them to be ?syn-
onyms? and if one of these is used in the orig-
inal knowledge sources, the other is used to ex-
pand them. It should be clear that we will gener-
ate many incorrect questions with this procedure,
but empirical tests (out of the scope of this paper)
show that these questions do not hurt the system
performance. Moreover, they are useful for ASR
language model interpolation, which is based on
N-grams.
63
3.2 Out-of-domain knowledge sources
The same format of the previously described
knowledge sources can be used to represent out-
of-domain knowledge. Here, we extensively used
the ?synonyms? approach. For instance, words
wife and girlfriend are considered to be ?syn-
onyms? as all the personal questions with these
words should be answered with the same sentence:
I do not want to talk about my private life.
Nevertheless, and taking into consideration the
work around small talk developed by the chat-
bots community (Klwer, 2011), we decided to
use the most popular language to build chat-
bots: the ?Artificial Intelligence Markup Lan-
guage?, widely known as AIML, a derivative of
XML. With AIML, knowledge is coded as a set
of rules that will match the user input, associ-
ated with templates, the generators of the out-
put. A detailed description of AIML syntax can
be found in http://www.alicebot.org/
aiml.html. In what respects AIML inter-
preters, we opted to use Program D (java), which
we integrated in our platform. Currently, we use
AIML to deal with slang and to answer questions
that have to do with cinema and compliments.
As a curiosity, we should explain that we deal
with slang when input came from the keyboard,
and not when it is speech, as the language models
are not trained with this specific lexicon. The rea-
son we do that is because if the language models
were trained with slang, it would be possible to er-
roneously detect it in utterances and then answer
them accordingly, which could be extremely un-
pleasant. Therefore, EDGAR only deals with slang
when the input is the keyboard.
The current knowledge sources have 152 ques-
tion/answer pairs, corresponding to 763 questions
and 206 answers. For Portuguese, English and
Spanish the use of 226, 219 and 53 synonym re-
lations, led to the generation of 22 194, 16 378
and 1 716 new questions, respectively.
3.3 Finding the appropriate answer
The NLU module is responsible for the answer se-
lection process. It has three main components.
The first one, STRATEGIES, is responsible to
choose an appropriate answer to the received inter-
action. Several strategies are implemented, includ-
ing the ones based on string matching, string dis-
tances (as for instance, Levenshtein, Jaccard and
Dice), N-gram Overlap and support vector ma-
chines (seeing the answer selection as a classifica-
tion problem). Currently, best results are attained
using a combination of Jaccard and bigram Over-
lap measures and word weight through the use of
tf-idf statistic. In this case, Jaccard takes into ac-
count how many words are shared between the
user?s interaction and the knowledge source en-
try, bigram Overlap gives preference to the shared
sequences of words and tf-idf contributes to the
results attained by previous measures, by given
weight to unfrequent words, which should have
more weight on the decision process (for example,
the word MONSERRATE occurs in the majority of
the questions in the corpus, so it is not very infor-
mative and should not have the same weight as, for
instance, the word architect or owner).
The second component, PLUGINS, deals with
two different situations. First, it accesses Pro-
gram D when interactions are not answered by the
STRATEGIES component. That is, when the tech-
nique used by STRATEGIES returns a value that
is lower than a threshold (dependent of the used
technique), the PLUGIN component runs Program
D in order to try to find an answer to the posed
question. Secondly, when the ASR has no confi-
dence of the attained transcription (and returns the
? REPEAT ? tag) or Program D is not able to find
an answer, the PLUGINS component does the fol-
lowing (with the goal of taking the user again to
the agent topic of expertise):
? In the first time that this occurs, a sentence
such as Sorry, I did not understand you. is
chosen as the answer to be returned.
? The second time this occurs, EDGAR asks the
user I did not understand you again. Why
don?t you ask me X?, being X generated in
run time and being a question from a subset
of the questions from the knowledge sources.
Obviously, only in-domain (not expanded)
questions are considered for replacing X.
? The third time there is a misunderstanding,
EDGAR says We are not understanding each
other, let me talk about MONSERRATE. And
it randomly choses some answer to present to
the user.
The third component is the HISTORY-
TRACKER, which handles the agent knowledge
about previous interactions (kept until a default
time without interactions is reached).
64
4 Preliminary evaluation
Edgar is more a domain-specific Question An-
swering (QA) than a task-oriented dialogue sys-
tem. Therefore, we evaluated it with the metrics
typically used in QA. The mapping of the dif-
ferent situations in true/false positives/negatives is
explained in the following.
We have manually transcribed 1086 spoken ut-
terances (in Portuguese), which were then labeled
with the following tags, some depending on the
answer given by EDGAR:
? 0: in-domain question incorrectly answered,
although there was information in the knowl-
edge sources (excluding Program D) to an-
swer it;
? 1: out-of-domain question, incorrectly an-
swered;
? 2: question correctly answered by Program
D;
? 3: question correctly answered by using
knowledge sources (excluding Program D);
? 4: in-domain question, incorrectly answered.
There is no information in the knowledge
source to answer it, but it should be;
? 5: multiple questions, partially answered;
? 6: multiple questions, unanswered;
? 7: question with implicit information (there,
him, etc.), unanswered;
? 8: question which is not ?ipsis verbis? in the
knowledge source, but has a paraphrase there
and was not correctly answered;
? 9: question with a single word (garden,
palace), unanswered;
? 10: question that we do not want the system
to answer (some were answered, some were
not).
The previous tags were mapped into:
? true positives: questions marked with 2, 3
and 5;
? true negatives: questions marked with 0 and
10 (the ones that were not answered by the
system);
? false positives: questions marked with 0 and
10 (the ones that were answered by the sys-
tem);
? false negatives: questions marked with 4, 6,
7, 8 and 9.
Then, two experiments were conducted: in the
first, the NLU module was applied to the manual
transcriptions; in the second, directly to the output
of the ASR. Table 1 shows the results.
NLU input = manual transcriptions
Precision Recall F-measure
0.92 0.60 0.72
acNLU input = ASR
Precision Recall F-measure
0.71 0.32 0.45
Table 1: NLU results
The ASR Word Error Rate (WER) is of 70%.
However, we detect some problems in the way we
were collecting the audio, and in more recent eval-
uations (by using 363 recent logs where previous
problems were corrected), that error decreased to a
WER of 52%, including speech from 111 children,
21 non native Portuguese speakers (thus, with a
different pronunciation), 23 individuals not talking
in Portuguese and 27 interactions where multiple
speakers overlap. Here, we should refer the work
presented in (Traum et al, 2012), where an eval-
uation of two virtual guides in a museum is pre-
sented. They also had to deal with speakers from
different ages and with question off-topic, and re-
port a ASR with 57% WER (however they major-
ity of their user are children: 76%).
We are currently preparing a new corpus for
evaluating the NLU module, however, the follow-
ing results remain: in the best scenario, if tran-
scription is perfect, the NLU module behaves as
indicated in Table 1 (manual transcriptions).
5 Conclusions and Future Work
We have described a platform for developing
ECAs with tutoring goals, that takes both speech
and text as input and output, and introduced
EDGAR, the butler of MONSERRATE, which was
developed in that platform. Special attention was
given to EDGAR?s NLU module, which couples
techniques that try to find distances between the
user input and sentences in the existing knowledge
65
sources, with a framework imported from the chat-
bots community (AIML plus Program D). EDGAR
has been tested with real users for the last year and
we are currently performing a detailed evaluation
of it. There is much work to be done, including to
be able to deal with language varieties, which is an
important source of recognition errors. Moreover,
the capacity of dealing with out-of-domain ques-
tions is still a hot research topic and one of our
priorities in the near future. We have testified that
people are delighted when EDGAR answers out-
of-domain questions (Do you like soccer?/I rather
have a tea and read a good criminal book) and we
cannot forget that entertainment is also one of this
Embodied Conversational Agent (ECA)?s goal.
Acknowledgments
This work was supported by national
funds through FCT ? Fundac?a?o para a
Cie?ncia e a Tecnologia, under project PEst-
OE/EEI/LA0021/2013. Pedro Fialho, Se?rgio
Curto and Pedro Cla?udio scholarships were sup-
ported under project FALACOMIGO (ProjectoVII
em co-promoc?a?o, QREN n 13449).
References
N. O. Bernsen and L. Dybkjr. 2005. Meet hans chris-
tian andersen. In In Proceedings of Sixth SIGdial
Workshop on Discourse and Dialogue, pages 237?
241.
Tina Klwer. 2011. ?i like your shirt? ? dialogue acts
for enabling social talk in conversational agents. In
Proceedings of the 11th International Conference on
Intelligent Virtual Agents. International Conference
on Intelligent Virtual Agents (IVA), 11th, September
17-19, Reykjavik, Iceland. Springer.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In 7th SIGdial Workshop
on Discourse and Dialogue, Sydney, Australia.
Hugo Meinedo, Diamantino Caseiro, Joa?o Neto, and
Isabel Trancoso. 2003. Audimus.media: a broad-
cast news speech recognition system for the euro-
pean portuguese language. In Proceedings of the 6th
international conference on Computational process-
ing of the Portuguese language, PROPOR?03, pages
9?17, Berlin, Heidelberg. Springer-Verlag.
H. Meinedo, A. Abad, T. Pellegrini, I. Trancoso, and
J. P. Neto. 2010. The l2f broadcast news speech
recognition system. In Proceedings of Fala2010,
Vigo, Spain.
Ana Cristina Mendes, Rui Prada, and Lu??sa Coheur.
2009. Adapting a virtual agent to users? vocabu-
lary and needs. In Proceedings of the 9th Interna-
tional Conference on Intelligent Virtual Agents, IVA
?09, pages 529?530, Berlin, Heidelberg. Springer-
Verlag.
Se?rgio Paulo, Lu??s C. Oliveira, Carlos Mendes, Lu??s
Figueira, Renato Cassaca, Ce?u Viana, and Helena
Moniz. 2008. Dixi ? a generic text-to-speech sys-
tem for european portuguese. In Proceedings of the
8th international conference on Computational Pro-
cessing of the Portuguese Language, PROPOR ?08,
pages 91?100, Berlin, Heidelberg. Springer-Verlag.
Thies Pfeiffer, Christian Liguda, Ipke Wachsmuth, and
Stefan Stein. 2011. Living with a virtual agent:
Seven years with an embodied conversational agent
at the heinz nixdorf museumsforum. In Proceedings
of the International Conference Re-Thinking Tech-
nology in Museums 2011 - Emerging Experiences,
pages 121 ? 131. thinkk creative & the University of
Limerick.
Susan Robinson, David Traum, Midhun Ittycheriah,
and Joe Henderer. 2008. What would you ask a
conversational agent? observations of human-agent
dialogues in a museum setting. In International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
Ulli Waltinger, Alexa Breuing, and Ipke Wachsmuth.
2011. Interfacing virtual agents with collaborative
knowledge: Open domain question answering us-
ing wikipedia-based topic models. In IJCAI, pages
1896?1902.
Heiga Zen, Keiichiro Oura, Takashi Nose, Junichi Ya-
magishi, Shinji Sako, Tomoki Toda, Takashi Ma-
suko, Alan W. Black, and Keiichi Tokuda. 2009.
Recent development of the HMM-based speech syn-
thesis system (HTS). In Proc. 2009 Asia-Pacific
Signal and Information Processing Association (AP-
SIPA), Sapporo, Japan, October.
66
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 33?38,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Exploring linguistically-rich patterns for question generation
Se?rgio Curto
L2F/INESC-ID Lisbon
sslc@l2f.inesc-id.pt
Ana Cristina Mendes
L2F/INESC-ID Lisbon
IST, Tech. Univ. Lisbon
acbm@l2f.inesc-id.pt
Lu??sa Coheur
L2F/INESC-ID Lisbon
IST, Tech. Univ. Lisbon
lcoheur@inesc-id.pt
Abstract
Linguistic patterns reflect the regularities of
Natural Language and their applicability is
acknowledged in several Natural Language
Processing tasks. Particularly, in the task of
Question Generation, many systems depend
on patterns to generate questions from text.
The approach we follow relies on patterns
that convey lexical, syntactic and semantic in-
formation, automatically learned from large-
scale corpora.
In this paper we discuss the impact of varying
several parameters during pattern learning and
matching in the Question Generation task. In
particular, we introduce semantics (by means
of named entities) in our lexico-syntactic pat-
terns. We evaluate and compare the number
and quality of the learned patterns and the
matched text segments. Also, we detail the
influence of the patterns in the generation of
natural language questions.
1 Introduction
Natural Language (NL) is known for its variability
and expressiveness. There are hundreds of ways to
express an idea, to describe a fact. But language also
comprises several regularities, or patterns, that de-
note the presence of certain information. For exam-
ple, Paris is located in France is a common way to
say that Paris is in France, indicated by the words
located in.
The use of patterns is a widely accepted as an ef-
fective approach in the field of Natural Language
Processing (NLP), in tasks like Question-Answering
(QA) (Soubbotin, 2001; Ravichandran and Hovy,
2002) or Question Generation (QG) (Wyse and Pi-
wek, 2009; Mendes et al, 2011).
Particularly, QG aims at generating questions
from text and has became a vibrant line of re-
search. Generating questions (and answers), on one
hand, allows QA or Dialogue Systems to be easily
ported to different domains, by quickly providing
new questions to train the systems. On the other
hand, it is useful for knowledge assessment-related
tasks, by reducing the amount of time allocated for
the creation of tests by teachers (a time consuming
and tedious task if done manually), or by allowing
the self evaluation of knowledge acquired by learn-
ers.
Most systems dedicated to QG are based on hand-
crafted rules and rely on pattern matching to gener-
ate questions. For example, in (Chen et al, 2009),
after the identification of key points, a situation
model is built and question templates are used to
generate questions. The Ceist system (Wyse and Pi-
wek, 2009) uses syntactic patterns and the Tregex
tool (Levy and Andrew, 2006) that receives a set
of hand-crafted rules and matches the rules against
parsed text, generating, in this way, questions (and
answers). Kalady et al(2010) bases the QG task
in Up-keys (significant phrases in documents), parse
tree manipulation and named entity recognition.
Our approach to QG also relies on linguistic pat-
terns, defined as a sequence of symbols that convey
lexical, syntactic and semantic information, reflect-
ing and expressing a regularity of the language. The
patterns associate a question to its answer and are
automatically learned from a set of seeds, based on
large-scale information corpora, shallow parsing and
named entities recognition. The generation of ques-
tions uses the learned patterns, as questions are cre-
ated from text segments found in free text after being
matched against the patterns.
33
This paper studies the impact on QG of vary-
ing linguistic parameters during pattern learning and
matching. It is organized as follows: in Sec. 2
we introduce our pattern-based approach to QG; in
Sec. 3 we show the experiments and discuss results;
in Sec. 4 we conclude and point to future work.
2 Linguistically-Rich Patterns for
Question Generation
The generation of questions involves two phases: a
first offline phase ? pattern learning ? where pat-
terns are learned from a set of seeds; and a sec-
ond online phase ? pattern matching and question
generation ? where the learned patterns are matched
against a target document and the questions are gen-
erated. Next we describe these phases.
Pattern Learning Our approach to pattern learn-
ing is inspired by the work of Ravichandran and
Hovy (2002), who propose a method to learn pat-
terns based on a two-step technique: the first ac-
quires patterns from the Web given a set of seeds and
the second validates the patterns. Despite the sim-
ilarities, ours and Ravichandran and Hovy?s work
have some differences: our patterns also contain
syntactic and semantic information and are not vali-
dated. Moreover, our seeds are well formulated NL
questions and their respective correct answers (in-
stead of two entities), which allows to directly take
advantage of the test sets already built and made
available in evaluation campaigns for QA systems
(like Text REtrieval Conference (TREC) or Cross
Language Evaluation Forum (CLEF)).
We use a set of seeds, each composed by a NL
question and its correct answer. We start by classi-
fying each seed question into a semantic category,
in order to discover the type of information these
are seeking after: for example, the question ?Who
painted the Birth of Venus ?? asks for a person?s
name. Afterwards, we extract the phrase nodes of
each seed question (excluding the Wh-phrase), en-
close each in double quotes and submit them as a
query to a search engine. For instance, given the
seed ?Who painted the Birth of Venus ??/Botticelli
and the syntactic structure of its question [WHNP
Who] [VBD painted] [NP the Birth of Venus]
1, we
1The Penn Treebank II Tags (Bies et al, 1995) are used.
build the query: "painted" "the Birth of
Venus" "Botticelli".
We build patterns that associate the entities
in the question to the answer from the top re-
trieved documents. From the sentence The Birth
of Venus was painted around 1486 by Botti-
celli, retrieved as result to the above query, we
learn the pattern ?NP VBD[was] VBN PP[around
1486]:[Date] IN:[by] NP{ANSWER}?2. The
syntactic labels without lexical information are re-
lated with the constituents of the question, while
those with ?{ANSWER}? mark the answer.
By creating queries with the inflected forms of the
main verb of the question, we learn patterns where
the surface form of the verb is different to that of
the verb in the seed question (e.g., ?NP{ANSWER}
VBD[began] VBG NP? is learned from the sen-
tence Botticelli began painting the Birth of Venus).
The patterns generated by verb inflection are IN-
FLECTED; the others are STRONG patterns.
Our patterns convey linguistic information ex-
tracted from the sentences in the documents where
all the constituents of the query exist. The pat-
tern is built with the words, their syntactic and se-
mantic classes, that constitute the segments where
those constituents are found. For that, we per-
form syntactic analysis and named entity recog-
nition in each sentence. In this paper, we ad-
dress the impact of adding semantic information
to the patterns, that is, the difference in hav-
ing a pattern ?NP VBD[was] VBN PP[around
1486]:[Date] IN:[by] NP{ANSWER}? with or
without the named entity of type DATE, for instance.
Pattern Matching and Question Generation
The match of the patterns against a given free text
is done at the lexical, syntactic and semantic lev-
els. We have implemented a (recursive) algorithm
that explores the parsed tree of the text sentences in
a top-down, left-to-right, depth-first search, unifying
the text with the linguistic information in the pattern.
Also, we discard all matched segments in which
the answer does not agree with the semantic cate-
gory expected by the question.
The generation of questions from the matched text
2The patterns are more complex than the ones presented:
they are linked to the seed question by indexes, mapping the po-
sition of each of its components into the question constituents.
34
segments is straightforward, since we keep track of
the syntactic structure of the questions and the sen-
tences on the origin of the patterns. There is a di-
rect unification of all components of the text seg-
ment with the constituents of the pattern. In the
INFLECTED patterns, the verb is inflected with the
tense and person of the seed question and the auxil-
iary verb is also used.
3 Experiments
3.1 Experimental Setup
We used the 6 seeds shown in Table 1, chosen be-
cause the questions contain regular verbs and they
focus on known entities ? being so, it is probable
that there will be several texts in the Web referring to
them. However, understanding the characteristics of
a pair that makes it a good seed is an important and
pertinent question and a direction for future work.
GId: 1
Syntactic Structure: WHNP VBD NP
Semantic Category: HUMAN:INDIVIDUAL
?Who wrote Hamlet??/Shakespeare
?Who painted Guernica??/Picasso
?Who painted The Starry Night??/Van Gogh
GId: 2
Syntactic Structure: WHADVP VBD NP VBN
Semantic Category: NUMERIC:DATE
?When was Hamlet written??/1601
?When was Guernica painted??/1937
?When was The Starry Night painted??/1889
Table 1: Seeds used in the experiments.
The syntactic analysis of the questions was done
by the Berkeley Parser (Petrov and Klein, 2007)
trained on the QuestionBank (Judge et al, 2006).
For question classification, we used Li and Roth
(2002) taxonomy and a machine learning-based
classifier fed with features derived from a rule-based
classifier (Silva et al, 2011).
For the learning of patterns we used the top
64 documents retrieved by Google and to recog-
nize the named entities in the pattern we apply
several strategies, namely: 1) the Stanford?s Con-
ditional Random-Field-based named entity recog-
nizer (Finkel et al, 2005) to detect entities of type
HUMAN; 2) regular expressions to detect NUMERIC
and DATE type entities; 3) gazetteers to detect enti-
ties of type LOCATION.
For the generation of questions we used the top 16
documents retrieved by the Google for 9 personali-
ties from several domains, like literature (e.g., Jane
Austen) and politics (e.g., Adolf Hitler). We do not
have influence on the content of the retrieved doc-
uments, nor perform any pre-processing (like text
simplification or anaphora resolution). The Berkeley
Parser (Petrov and Klein, 2007) was used to parse
the sentences, trained with the Wall Street Journal.
3.2 Pattern Learning Results
A total of 272 patterns was learned, from which 212
are INFLECTED and the remaining are STRONG. On
average, each seed led to 46 patterns.
Table 2 shows the number of learned patterns of
types INFLECTED and STRONG according to each
group of seed questions. It indicates the number of
patterns in which at least one named entity was rec-
ognized (W) and the number of patterns which do not
contain any named entity (WO). Three main results
of the pattern learning phase are shown: 1) the num-
ber of learned INFLECTED patterns is much higher
than the number of learned STRONG patterns: nearly
80% of the patterns are INFLECTED; 2) most of the
patterns do not have named entities; and 3) the num-
ber of patterns learned from the questions of group
1 are nearly 70% of the total number of patterns.
INFLECTED STRONG
GId WO W WO W TOTAL
1 127 19 36 8
146 44 190
2 40 26 10 6
66 16 82
All 167 45 46 14
212 60 272
Table 2: Number of learned patterns.
The following are examples of patterns and the
actual sentences from where they were learned:
? ?NP{ANSWER} VBZ NP?: an INFLECTED pattern
learned from group 1, from the sentence 1601
William Shakespeare writes Hamlet in London.,
without named entities;
? ?NP VBD VBN IN[in] NP{ANSWER}?: a
35
STRONG pattern learned from group 2, from the
sentence (Guernica was painted in 1937.), without
named entities;
? ?NNP VBZ[is] NP[a tragedy] ,[,]
VBN[believed] VBN IN[between]
NP[1599]:[NUMERIC COUNT,NUMERIC DATE]
CC[and] NP{ANSWER}?: an INFLECTED pattern
learned from group 2, from the sentence William
Shakespeare?s Hamlet is a tragedy , believed written
between 1599 and 1601, with 1599 being recog-
nized as named entity of type NUMERIC COUNT
and NUMERIC DATE.
3.3 Pattern Matching and Question
Generation Results
Regarding the number of text segments matched in
the texts retrieved for the 9 personalities, Table 3
shows that, from the 272 learned patterns, only 30
(11%) were in fact effective (an effective pattern
matches at least one text segment). The most effec-
tive patterns were those from group 2, as 12 from 82
(14.6%) matched at least one instance in the text.
GId INFLECTED STRONG TOTAL
1 13 5 18
2 9 3 (2 W) 12
All 22 8 30
Table 3: Matched patterns.
Regarding the patterns with named entities, only
those from group 2 matched instances in the texts.
The pattern that matched the most instances was
?NP{ANSWER} VBD NP?, learned from group 1.
In the evaluation of the questions, we use the
guidelines of Chen et al (2009), who classify ques-
tions as plausible ? if they are grammatically correct
and if they make sense regarding the text from where
they were extracted ? and implausible (otherwise).
However, we split plausible questions in three cat-
egories: 1) Pa for plausible, anaphoric questions,
e.g., When was she awarded the Nobel Peace Prize?;
2) Pc for plausible questions that need a context to
be answered, e.g., When was the manuscript pub-
lished?; and 3) Pp, a plausible perfect question. If
a question can be marked both as PLa and PLc, we
mark it as PLa. Also, we split implausible questions
in: 1) IPi: for implausible questions due to incom-
pleteness, e.g., When was Bob Marley invited?; and
2) IP: for questions that make no sense, e.g., When
was December 1926 Agatha identified?.
A total of 447 questions was generated: 31 by
STRONG patterns, 269 by INFLECTED patterns and
147 by both STRONG and INFLECTED patterns. We
manually evaluated 100 questions, randomly se-
lected. Results are in Table 4, shown according
to the type (INFLECTED/STRONG) and presence of
named entities (W/WO) in the pattern that generated
them.
Pa Pc Pp IPi IP Total
INFLECTED 57
WO 2 0 27 23 5
STRONG 13
W 1 0 1 0 1
WO 1 2 3 3 1
INFL/STR 30
WO 0 0 9 18 3
All 4 2 40 44 10 100
Table 4: Evaluation of the generated questions.
46 of the evaluated questions were considered
plausible and, from these, 40 can be used without
modifications. From the 54 implausible questions,
44 were due to lack of information in the question.
69% (9 in 13) of the questions originated in STRONG
patterns were plausible. This value is smaller for
questions generated by INFLECTED patterns: 50.8%
(29 in 57). Questions that had in their origin both a
STRONG and a INFLECTED pattern were mostly im-
plausible, only 9 in 30 were plausible (30%). The
presence of named entities led to an increase of
questions of only 3 (2 plausible and 1 implausible).
3.4 Discussion
The results concerning the transition from lexico-
syntactic to lexico-syntactic-semantic patterns were
not conclusive. There were 59 patterns with named
entities, but only 2 matched new text segments.
Only 3 questions were generated from patterns
with semantics. We think that this happened due to
two reasons: 1) not all of the named entities in the
patterns were detected; and 2) the patterns contained
lexical information that did not allow a match with
the text (e.g., ?NP{ANSWER} VBD[responded]
36
PP[in 1937]:textit[Date] WHADVP[when]
NP[he] VBD NP? requires the words responded,
when and he.)
From a small set of seeds, our approach learned
patterns that were later used to generate 447 ques-
tions from previously unseen text. In a sample of
100 questions, 46% were judged as plausible. Two
plausible questions are: ?Who had no real interest
in the former German African colonies??, ?When
was The Road to Resurgence published?? and ?Who
launched a massive naval and land campaign de-
signed to seize New York??.
The presence of syntactic information (a differ-
ence between ours and Ravichandran and Hovy?s
work) allows to relax the patterns and to gener-
ate questions of various topics: e.g., the questions
?Who invented the telegraph?? and ?Who di-
rected the Titanic?? can be generated from match-
ing the pattern ?NP VBD[was] VBN IN:[by]
NP{ANSWER}? with the sentences The telegraph was
invented by Samuel Morse and The Titanic was di-
rected by James Cameron, respectively.
4 Conclusions and Future Work
We presented an approach to generating questions
based on linguistic patterns, automatically learned
from the Web from a set of seeds. We addressed the
impact of adding semantics to patterns in matching
text segments and generating new NL questions.
We did not detect any improvement when adding
semantics to the patterns, mostly because the pat-
terns with named entities did not match too many
text segments. Nevertheless, from a small set of 6
seeds, we generated 447 NL questions. From these,
we evaluated 100 and 46% were considered correct
at the lexical, syntactic and semantic levels.
In the future, we intend to pre-process the texts
against which the patterns are matched and from
which the questions are generated. Also, we are
experimenting this approach in another language.
We aim at using more complex questions as seeds,
studying its influence on the generation of questions.
Acknowledgements
This work was supported by FCT (INESC-ID mul-
tiannual funding) through the PIDDAC Program
funds, and also through the project FALACOMIGO
(ProjectoVII em co-promoc?a?o, QREN n 13449).
Ana Cristina Mendes is supported by a PhD fel-
lowship from Fundac?a?o para a Cie?ncia e a Tecnolo-
gia (SFRH/BD/43487/2008).
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
intyre. 1995. Bracketing Guidelines for Treebank II
Style Penn Treebank Project.
Wei Chen, Gregory Aist, , and Jack Mostow. 2009. Gen-
erating questions automatically from informational
text. In The 2nd Workshop on Question Generation.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proc. 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 363?
370. ACL.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In ACL-44: Proc. 21st Int. Conf. on Com-
putational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, pages
497?504. ACL.
Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.
2010. Natural language question generation using
syntax and keywords. In The 3rd Workshop on Ques-
tion Generation.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC 2006.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proc. 19th Int. Conf. on Computational Lin-
guistics, pages 1?7. ACL.
Ana Cristina Mendes, Se?rgio Curto, and Lu??sa Coheur.
2011. Bootstrapping multiple-choice tests with the-
mentor. In CICLing, 12th International Conference
on Intelligent Text Processing and Computational Lin-
guistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proc. Main Conference, pages 404?411. ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering sys-
tem. In ACL ?02: Proc. 40th Annual Meeting on As-
sociation for Computational Linguistics, pages 41?47.
ACL.
Joa?o Silva, Lu??sa Coheur, Ana Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
37
formation in question classification. Artificial Intelli-
gence Review, 35:137?154.
M. M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In Proc. 10th
Text REtrieval Conference (TREC), pages 293?302.
Brendan Wyse and Paul Piwek. 2009. Generating ques-
tions from openlearn study units. In The 2nd Workshop
on Question Generation.
38
