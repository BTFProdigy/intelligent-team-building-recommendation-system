Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 21?22,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Detection of Tags for Political Blogs 
 
 
Khairun-nisa Hassanali Vasileios Hatzivassiloglou 
Human Language Technology Institute Human Language Technology Institute 
The University of Texas at Dallas The University of Texas at Dallas 
Richardson, TX 75080, USA Richardson, TX 75080, USA 
nisa@hlt.utdallas.edu vh@hlt.utdallas.edu 
 
  
 
Abstract 
This paper describes a technique for automati-
cally tagging political blog posts using SVM?s 
and named entity recognition. We compare 
the quality of the tags detected by this ap-
proach to earlier approaches in other domains, 
observing effects from the political domain 
and benefits from NLP techniques comple-
mentary to the core SVM method. 
1 Introduction 
Political blogs are a particular type of communica-
tion platform that combines analyses provided by 
the blog owner or a team of regular contributors 
with shorter, but far more numerous, entries by 
visitors. Given the enthusiasm that activities for or 
against a particular politician or party can generate, 
political blogs are a vibrant part of the blogos-
phere: more than 38,500 blogs specifically dedi-
cated to politics exist in the US alone according to 
Technorati, and some of the more active ones at-
tract more than 30 million unique visitors each 
month (double that number just before major elec-
tions). 
Political blogs provide a wealth of factual in-
formation about political events and activities, but 
also by their nature are colored by strong opinions. 
They are therefore a particularly attractive target 
for semantic analysis methods using natural lan-
guage processing technology. In fact, the past two 
years have brought an increased number of colla-
borations between NLP researchers and political 
scientists using data from political sources, includ-
ing two special issues of leading political science 
journals on such topics (see (Cardie and Wilker-
son, 2008) for an overview). Our motivation for 
working with this kind of data is the construction 
of a system that collates information across blog 
posts, combines evidence to numerically rate atti-
tudes of blogs on different topics, and traces the 
evolution of these attitudes across time and in re-
sponse to events. To enable these tasks, we first 
identify the major topics that each blog post cov-
ers. In the present paper, we describe our recogniz-
er of blog post topics. We show that, perhaps 
because of the richness of political blogs in named 
entities, an SVM-based keyword learning approach 
can be complemented with named entity recogni-
tion and co-reference detection to achieve preci-
sion and recall scores higher than those reported by 
earlier topic recognition systems in other domains. 
2 Related Work  
In our approach, as in earlier published work, we 
take tags assigned by many blogs to individual 
blog posts as a reference list of the topics covered 
by that post. Tags are single words or short phras-
es, most often noun phrases, and are usually cho-
sen by each post?s authors without a controlled 
vocabulary; examples include ?Michigan?, 
?George Bush?, ?democracy?, and ?health care?. 
Earlier work in predicting tags includes (Mishne, 
2006), who adopts a collaborative filtering ap-
proach; in contrast, we rely on training classifiers 
from earlier posts in each blog. Our approach is 
more similar to (Sood et al, 2007) and (Wang and 
Davison, 2008) who use different machine learning 
techniques applied to a training set. We differ from 
the last two approaches in our addition of proper 
noun and named entity recognition methods to our 
core SVM classifiers, in our exploration of specifi-
cally political data, and in our subsequent use of 
21
the predicted tags (for semantic analysis rather 
than tag set compression or query expansion). 
3 Data 
We collected data from two major political blogs, 
Daily Kos (www.dailykos.com) and Red State 
(www.redstate.com). Red State is a conserva-
tive political blog whereas Daily Kos is a liberal 
political blog. Both these blogs are widely read and 
tag each of their blog entries. We collected data 
from both these blogs over a period of two years 
(January 2008 ? February 2010). We collected a 
total of 100,000 blog posts from Daily Kos and 
70,000 blog posts from Red State and a total of 
787,780 tags across both blogs (an average of 4.63 
tags per post). 
4 Methods 
We used SVM Light (Joachims, 2002) to predict 
the tags for a given blog post. We constructed one 
classifier for each of the tags present in the training 
set. The features used were counts of each word 
encountered in the title or the body of a post (two 
counts per word), further subdivided by whether 
the word appears in any tags in the training data or 
not, and whether it is a synonym of known tag 
words. We extract the top five proposed tags for 
each post, corresponding to the five highest scoring 
SVM classifiers. 
We also attempt to detect the main entities being 
talked about. We perform shallow parsing and ex-
tract noun phrases and then proper nouns. The 
most frequent proper NPs are probable tags. We 
also added named entity recognition and co-
reference resolution using the OpenNLP toolkit 
(maxent.sourceforge.net). We found that 
named entity recognition proposes additional use-
ful tags while the effect of co-reference resolution 
is marginal, mostly because of limited success in 
actually matching co-referent entities. 
5 Results and Evaluation 
For evaluating our methods, we used 2,681 posts 
from Daily Kos and 571 posts from Red State. We 
compared the tags assigned by our tagger to the 
original tags of the blog post, using an automated 
method (Figure 1). A tag was considered a match if 
it exactly matched the original tag or was a word 
super set ? for example ?health care system? is 
considered a match to ?health care?. We also ma-
nually evaluated the relevance of the proposed tags 
on a small portion of our test set (100 posts). 
 
Method Precision Recall F-Score 
Single word SVM 27.3% 60.3% 37.6% 
+ Stemming 26.1% 59.5% 36.3% 
   + Proper Nouns 36.5% 56.8% 44.4% 
Named Entities 48.4% 49.1% 48.7% 
All Combined 21.1% 65.0% 31.9% 
Manual Scoring 67.0% 75.0% 70.8% 
    Single word SVM 19.0% 30.0% 23.3% 
+ Stemming 22.0% 30.2% 25.5% 
   + Proper Nouns 46.3% 54.0% 49.9% 
Named Entities 60.1% 41.5% 49.1% 
All Combined 20.3% 65.7% 31.0% 
Manual Scoring 47.0% 62.0% 53.5% 
Figure 1: Results on Daily Kos (top) and Red State 
(bottom) data. Best scores in bold. 
6 Conclusion  
We described and evaluated a tool for automatical-
ly tagging political blog posts. Political blogs differ 
from other blogs as they often involve named enti-
ties (politicians, organizations, and places). There-
fore, tagging of political blog posts benefits from 
using basic name entity recognition to improve the 
tagging. The recall in particular exceeds the score 
obtained by earlier techniques applied to other do-
mains (Sood et al (2007) report precision of 13% 
and recall of 23%; Wang and Davison (2008) re-
port precision of 45% and recall of 23%).  
 
References  
Claire Cardie and John Wilkerson (editors). ?Special 
Volume: Text Annotation for Political Science Re-
search?. Journal of Information Technology and 
Politics, 5(1):1-6, 2008. 
Thorsten Joachims. SVM-Light. 2002. http://www. 
svmlight.joachims.org. 
Gilad Mishne. ?AutoTag: A Collaborative Approach to 
Automated Tag Assignment for Weblog Posts?. In 
Proceedings of WWW, 2006. 
Sanjay C. Sood, Sara H. Owsley, Kristian J. Hammond, 
and Larry Birnbaum. ?TagAssist: Automatic Tag 
Suggestion for Blog Posts?. In Proceedings of 
ICWSM, 2007. 
Jian Wang and Brian D. Davison. ?Explorations in Tag 
Suggestion and Query Expansion?. In Proceedings of 
SSM ?08, 2008. 
22
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 87?95,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Measuring Language Development in Early Childhood Education: A Case
Study of Grammar Checking in Child Language Transcripts
Khairun-nisa Hassanali
Computer Science Department
The University of Texas at Dallas
Richardson, TX, USA
nisa@hlt.utdallas.edu
Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX, USA
yangl@hlt.utdallas.edu
Abstract
Language sample analysis is an important
technique used in measuring language devel-
opment. At present, measures of grammati-
cal complexity such as the Index of Productive
Syntax (Scarborough, 1990) are used to mea-
sure language development in early childhood.
Although these measures depict the overall
competence in the usage of language, they do
not provide for an analysis of the grammati-
cal mistakes made by the child. In this paper,
we explore the use of existing Natural Lan-
guage Processing (NLP) techniques to provide
an insight into the processing of child lan-
guage transcripts and challenges in automatic
grammar checking. We explore the automatic
detection of 6 types of verb related grammat-
ical errors. We compare rule based systems
to statistical systems and investigate the use
of different features. We found the statistical
systems performed better than the rule based
systems for most of the error categories.
1 Introduction
Automatic grammar checking and correction has
been used extensively in several applications. One
such application is in word processors where the
user is notified of a potential ungrammatical sen-
tence. This feature makes it easier for the users to
detect and correct ungrammatical sentences. Au-
tomatic grammar checking can also be beneficial
in language learning where students are given sug-
gestions on potential grammatical errors (Lee and
Seneff, 2006). Another application of grammar
checking is in improving a parser?s performance for
ungrammatical sentences. Since most parsers are
trained on written data consisting mostly of gram-
matical sentences, the parsers face issues when pars-
ing ungrammatical sentences. Automatic detection
and correction of these ungrammatical sentences
would improve the parser?s performance by detect-
ing the ungrammatical sentences and performing
a second parse on the corrected sentences (Caines
and Buttery, 2010). From an education perspective,
measuring language skills has been extensively ex-
plored. There are systems in place that automatically
detect and correct errors for second language learn-
ers (Eeg-Olofsson and Knuttson, 2003; Leacock et
al., 2010).
One method used in measuring language devel-
opment is the analysis of transcripts of child lan-
guage speech. Child language transcripts are sam-
ples of a child?s utterances during a specified pe-
riod of time. Educators and speech language pathol-
ogists use these samples to measure language de-
velopment. In particular, speech language pathol-
ogists score these transcripts for grammatical mea-
sures of complexity amidst other measures. Since
manual analysis of transcripts is time consuming,
many of these grammatical complexity measures re-
quire the speech language pathologists to look for
just a few examples. The Index of Productive Syn-
tax (IPSyn) (Scarborough, 1990) is one such mea-
sure of morphological and syntactic structure devel-
oped for measuring language samples of preschool
children. The advantage of measures such as IPSyn
is that they give a single score that can be used to
holistically measure language development. How-
ever, they focus on grammatical constructs that the
87
child uses correctly and do not take into account
the number and type of grammatical errors that are
made by the child.
Educators wishing to measure language develop-
ment and competence in a child will benefit from
having access to the grammatical errors made by a
child. Analysis of these grammatical errors will en-
able educators and speech language pathologists to
identify shortcomings in the child?s language and
recommend intervention techniques customized to
the child. Since manual identification of grammat-
ical errors is both cumbersome and time consum-
ing, a tool that automatically does grammar check-
ing would be of great use to clinicians. Addition-
ally, we see several uses of automatic grammar de-
tection. For example, we can use the statistics of
grammatical errors as features in building classifiers
that predict language impairment. Furthermore, we
could also use the statistics of these grammatical er-
rors to come up with a measure of language develop-
ment that takes into account both grammatical com-
petence and grammatical deficiencies.
In this paper, we use existing NLP techniques to
automatically detect grammatical errors from child
language transcripts. Since children with Language
Impairment (LI) have a greater problem with correct
usage of verbs compared to Typically Developing
(TD) children (Rice et al, 1995), we focus mainly
on verb related errors. We compare rule based sys-
tems to statistical systems and investigate the use
of different features. We found the statistical sys-
tems performed better than the rule based systems
for most error categories.
2 Related Work
While there has been considerable work (Sagae et
al., 2007) done on annotating child language tran-
scripts for grammatical relations, as far as we know,
there has been no work done on automatic gram-
mar checking of child language transcripts. Most
of the existing work in automatic grammar check-
ing has been done on written text. Spoken language
on the other hand, presents challenges such as dis-
fluencies and false restarts which are not present in
written text. We believe that the specific research
challenges that are encountered in detecting and cor-
recting child language transcripts warrant a more de-
tailed examination.
Caines and Buttery (2010) focused on identify-
ing sentences with the missing auxiliary verb in the
progressive aspect constructions. They used logistic
regression to predict the presence of zero auxiliary
occurrence in the spoken British National Corpus
(BNC). An example of a zero auxiliary construction
is ?You talking to me??. They first identified con-
structions with the progressive aspect and annotated
the constructions for the following features: sub-
ject person, subject case, perfect aspect, presence of
negation and use of pronouns. Their model identi-
fied zero auxiliary constructions with 96.9% accu-
racy. They also demonstrated how their model can
be integrated into existing parsing tools, thereby in-
creasing the number of successful parses for zero
auxiliary constructions by 30%.
Lee and Seneff (2008) described a system for verb
error correction using template matching on parse
trees in two ways. Their work focused on correct-
ing the error types related to subject-verb agreement,
auxiliary agreement and complementation. They
considered the irregularities in parse trees caused
by verb error forms and used n-gram counts to fil-
ter proposed corrections. They used the AQUAINT
Corpus of English News Text to detect the irregular-
ities in the parse trees caused by verb error forms.
They reported an accuracy of 98.93% for verb er-
rors related to subject-verb agreement, and 98.94%
for verb errors related to auxiliary agreement and
complementation. Bowden and Fox (2002) devel-
oped a system to detect and explain errors made by
non-native English speakers. They used classifica-
tion and pattern matching rules instead of thorough
parsing. Their system searched for the verb-related
errors and noun-related errors one by one in one sen-
tence by narrowing down the classification of errors.
Lee and Seneff (2006) developed a system to auto-
matically correct grammatical errors related to arti-
cles, verbs, prepositions and nouns.
Leacock et al (2010) discuss automated gram-
matical error detection for English language learn-
ers. They focus on errors that language learners find
most difficult - constructions that contain preposi-
tions, articles, and collocations. They discuss the
existing systems in place for automated grammati-
cal error detection and correction for these and other
classes of errors in a number of languages. Addi-
88
Label Meaning Example
0 No error I like it.
1 Missing auxiliary verb You talking to me?
2 Missing copulae She lovely.
3 Subject-auxiliary verb agreement You is talking to me.
4 Incorrect auxiliary verb used e.g. using does instead of is She does dead girl.
5 Missing verb She her a book.
6 Wrong verb usage including subject-verb disagreement He love dogs.
7 Missing preposition The book is the table.
8 Missing article She ate apple.
9 Missing subject before verb I know loves me.
10 Missing infinitive marker ?to? I give it her.
11 Other errors not covered in 1-10 The put.
Table 1: Different types of errors considered in this study
tionally, they touch on error annotations and system
evaluation for grammatical error detection.
3 Data
For the purpose of our experiments, we used the Par-
adise dataset (Paradise et al, 2005). This dataset
contains 677 transcripts corresponding to 677 chil-
dren aged six that were collected in the course of
a study of the relationship of otitis media and child
development. The only household language spoken
by these children was English. The transcripts in
the Paradise set consist of conversations between a
child and his/her caretaker. We retained only the
child?s utterances and removed all other utterances.
The Paradise dataset (considering only the child?s
utterances) contains a total of 108,711 utterances,
394,290 words, and an average Mean Length of Ut-
terance of 3.64. Gabani (2009) used scores on the
Peabody Picture Vocabulary Test (Dunn, 1965), total
percentage phonemes repeated correctly on a non-
word repetition task and mean length of utterance
in morphemes to label these transcripts for language
impairment. A transcript was labeled as having been
produced by a child with LI if the child scored 1.5
or more standard deviations below the mean of the
entire sample on at least two of the three tests. Of
the 677 transcripts, 623 were labeled as TD and 54
as LI.
We manually annotated each utterance in the tran-
scripts for 10 different types of errors. Table 1 gives
the different types of errors we considered along
with examples. We focused on these 10 different
types of errors since children with LI have problems
with the usage of verbs in particular. The list of er-
rors we arrived at was based on the errors we ob-
served in the transcripts. Since an utterance could
have more than one error, we annotated each ut-
terance in the transcript for all the errors present
in the utterance. While annotating the utterances,
we observed that there were utterances that could
correspond to multiple types of error. For exam-
ple, consider the following sentence: ?She go to
school?. The error in this sentence could be an er-
ror of a missing auxiliary and a wrong verb form
in which case the correct sentence would be ?She is
going to school?; or it could be a missing modal, in
which case the correct form would be ?She will go to
school?; or it could just be a subject-verb disagree-
ment in which case ?She goes to school? would be
the correct form. Therefore, although we know that
the utterance definitely has an error, it is not always
possible to assign a single error. We also observed
several utterances had both a missing subject and a
missing auxiliary verb error. For example, instead of
saying ?I am going to play?, some children say ?Go-
ing to play?, which misses both the subject and aux-
iliary verb. In this case, the utterance was annotated
as having two errors: missing subject and missing
auxiliary. Finally, single word utterances were la-
beled as being correct.
Table 2 gives the distribution of the errors in the
corpus and percentage of TD and LI population that
89
No Error Type Percentage
(Count)
% of LI children
making error
% of TD children
making error
1 Missing auxiliary 8.43% (641) 7% 5%
2 Missing copulae 36.67% (2788) 77.78% 45%
3 Subject-auxiliary agreement 6.31% (480) 40.74% 35%
4 Incorrect auxiliary verb used 0.71%(54) 11.47% 3%
5 Missing verb 5% (380) 29.63% 10%
6 Wrong verb usage 14.59% (1109) 68.5% 50%
7 Missing preposition 5% (380) 7.4% 5%
8 Missing article 3.97% (302) 29.63% 35%
9 Missing subject 7.69% (585) 3.7% 5%
10 Missing infinitive marker ?To? 1.58% (120) 7.5% 11.67%
11 Other errors 10.05% (764) 56.7% 23.2%
Table 2: Statistics of Errors
made the error at least once in the entire transcript.
As we can see from Table 2, 36.67% of the errors in
the corpus are due to missing copulae. Wrong verb
usage was the next most common error contributing
to 14.59% of the errors in the corpus. We observed
that there was a higher percentage of children with
LI that made errors on all error categories except for
errors related to missing article and missing subject.
We observed that on average, the transcripts belong-
ing to children with LI had fewer utterances as com-
pared to transcripts belonging to TD children. Ad-
ditionally, children with LI used many single word
and two word utterances.
One annotator labeled the entire corpus for gram-
matical errors. To calculate inter-annotator agree-
ment, we randomly selected 386 utterances anno-
tated by the first annotator with different error types.
The second annotator was provided these utterances
along with the labels given by the first annotator1.
In case of a disagreement, the second annotator pro-
vided a different label/labels. The annotator agree-
ment using the average Cohen?s Kappa coeffiecient
was 77.7%. Out of the 386 utterances, there were
43 disagreements between the annotators. We found
that for some error categories such as the missing
auxiliary, there was high inter-annotator agreement
of 95.32%, whereas for other categories such as
wrong verb usage and missing articles, there was
1We will perform independent annotation of the errors and
calculate inter-annotator agreement based on these independent
annotations
less agreement (64.2% and 65.3% respectively). In
particular, we found low inter-annotator agreement
on utterances that have errors that could be assigned
to multiple categories.
4 Experiments
The transcripts were parsed using the Charniak
parser (Charniak, 2000). Since the Paradise dataset
consists of children?s utterances, and many of them
have not mastered the language, we observed that
processing these transcripts is challenging. As is
prevalent in spoken language corpora, these tran-
scripts had disfluencies, false restarts and incom-
plete utterances, which sometimes pose problems to
the parser.
We conducted experiments in detecting errors re-
lated to the usage of the -ing participle, subject-
auxiliary agreement, missing copulae, missing
verb, subject-verb agreement and missing infinitive
marker ?to?. For each of these categories, we con-
structed one rule based classifier using regular ex-
pressions based on the parse tree structure, an alter-
nating decision tree classifier that used rules as fea-
tures and a naive Bayes multinomial classifier that
used a variety of features. For every category, we
performed 10 fold cross validation using all the ut-
terances. We used the naive Bayes multinomial clas-
sifier and the alternating decision tree classifier from
the WEKA toolkit (Hall et al, 2009). Table 3 gives
the results using the three classifiers for the different
categories of errors, where (P/R) F1 stands for (Pre-
90
Error Rule Based System
(P/R)F1
Decision Tree Clas-
sifier using Rules as
features (P/R)F1
Naive Bayes Classifier
using a variety of fea-
tures (P/R)F1
Usage of -ing participle (0.984/0.978) 0.981 (0.986/1) 0.993 (0.736/0.929) 0.821
Missing copulae (0.885/0.9) 0.892 (0.912/0.94) 0.926 (0.82/0.86) 0.84
Missing verb (0.875/0.932) 0.903 (0.92/0.89) 0.905 (0.87/0.91) 0.9
Subject-auxiliary agree-
ment
(0.855/0.932) 0.888 (0.95/0.84) 0.892 (0.89/0.934) 0.912
Subject-verb agreement (0.883/0.945) 0.892 (0.92/0.877) 0.898 (0.91/0.914) 0.912
Missing infinitive marker
?To?
(0.97/0.954) 0.962 (0.94/0.84) 0.887 (0.95/0.88) 0.914
Overall (0.935/0.923) 0.929 (0.945/0.965) 0.955 (0.956/0.978) 0.967
Table 3: Detection of errors using rule based system, alternating decision tree classifier and naive Bayes classifier
No Feature Type
1 Verb Adjective Bigram
2 Auxiliary Noun Bigram
3 Auxiliary Progressive-verb Bigram
4 Pronoun Auxiliary Bigram
5 Wh-Pronoun Progressive verb Bigram
6 Progressive-verb Wh-adverb Bigram
7 Adverb Auxiliary Skip-1
8 Pronoun Auxiliary Skip-1
9 Wh-adverb Progressive-verb Skip-1
10 Auxiliary Preposition Skip-2
Table 4: Top most bigram features useful for detecting
misuse of -ing participle
cision/Recall) F1-measure. Below we describe the
different experiments we conducted.
4.1 Misuse of the -ing Participle
The -ing participle can be used as a progressive as-
pect, a verb complementation, or a prepositional
complementation. In the progressive aspect, it is
necessary that the progressive verb be preceded by
an auxiliary verb. When used as a verb comple-
mentation, the -ing participle should be preceded by
a verb and similiarly when used as a prepositional
complement, the -ing participle should be preceded
by a preposition.
Rule based system
The -ing participle is denoted by the VBG tag in the
Penn tree bank notation. VP and PP correspond to
the verb phrase and prepositional phrase structures
respectively. The rules that we formed were as fol-
lows:
1. Check that the utterance has a VBG tag (if it
does not have a VBG tag, it does not contain an
-ing participle).
2. If none of the following conditions are met,
there is an error in the usage of -ing participle:
(a) The root of the subtree that contains the
-ing participle should be a VP with the
head being a verb if used as a verb com-
plementation
(b) The root of the subtree that contains the
-ing participle should be a PP if used as a
prepositional complement
(c) The root of the subtree that contains the
-ing participle should be a VP with the
head being an auxiliary verb if used as a
progressive aspect
Predictive model
The features that we considered were:
1. Bigrams from POS tags
2. Skip bigrams from POS tags
We used the skip bigrams to account for the
fact that there could be other POS tags between
an auxiliary verb and the progressive aspect of
the verb such as adverbs. A skip-n bigram is
a sequence of 2 POS tags with a distance of n
between them. We used skip-1 and skip-2 bi-
grams in this study.
91
Analysis
As we can see from Table 3, the alternating decision
tree classifier with rules as features gave the best re-
sults with an F1-measure of 0.993. Table 4 gives
the topmost 10 features extracted using feature se-
lection. We got the best results when we used the
reduced set of features as opposed to using all bi-
grams and skip-1 and skip-2 bigrams. We also used
the results reported by (Caines and Buttery, 2010)
to see if their method was successful in identifying
zero auxiliary constructs on our corpus. When we
used logistic regression with the coefficients and fea-
tures used by (Caines and Buttery, 2010), we got a
recall of 0%. When we trained the logistic regres-
sion model on our data with their features, we got a
precision of 1.09%, recall of 53.6% and F1-measure
of 2.14%. This leads us to conclude that the features
that were used by them are not suitable for child lan-
guage transcripts. Additionally, we also observed
that based on the features they used, in some cases
it is difficult to distinguish zero auxiliary constructs
from those with auxiliary constructs. For example,
?You talking to me?? and ?Are you talking to me??
would have the same values for their features, al-
though the former is a zero auxiliary construct and
the latter is not.
4.2 Identifying Missing Copulae
A copular verb is a verb that links a subject to its
complement. In English, the most common copular
verb is ?be?. Examples of sentences that contain a
copular verb is ?She is lovely? and ?The child who
fell sick was healthy earlier?. An example of a sen-
tence that misses a copular verb is ?She lovely?.
Rule based system
The rule that we used was as follows:
If an Adjective Phrase follows a noun phrase, or
a Noun phrase follows a noun phrase, the likelihood
that the utterance is missing a copular verb is quite
high. However, there are exceptions to such rules,
for example, ?Apple Pie?. We formed additional
rules to identify such utterances and examined their
parse trees to determine the function of the two noun
phrases.
Predictive model
The features we used were as follows:
1. Does the utterance contain a noun phrase fol-
lowed by a noun phrase?
2. Does the utterance contain a noun phrase fol-
lowed by an adjective phrase?
3. Is the parent a verb phrase?
4. Is the parent a prepositional phrase?
5. Is the parent the root of the parse tree?
6. Is there an auxiliary verb or a verb between the
noun phrase and/or adjective phrase?
Analysis
As we can see from Table 3, the alternating deci-
sion tree classifier performed the best with an F1-
measure of 0.926. Our rules capture simple con-
structs that are used by young children. The majority
of the utterances that missed a copulae consisted of
noun phrase and an adjective phrase or a noun phrase
and a noun phrase. Hence, the rules based system
performed the best. Some of the false positives were
due to utterances like ?She an apple? where it is un-
likely that the missing verb is a copular verb.
4.3 Identifying Missing Verbs
Errors of this type occur when a sentence is miss-
ing the verb. For example, the sentence ?You can
an apple? lacks the main verb after the modal verb
?can?. Similarly, ?I did not it? lacks a main verb af-
ter ?did not?. For the purpose of this experiment, we
consider only utterances that contain a modal or an
auxiliary verb but do not have a main verb. We also
consider utterances that use the verb ?do? and detect
the main missing verb in such cases.
Rule based system
The rule we used was to check if the utterance con-
tains an auxiliary verb or a modal verb but not a main
verb. In this case, the utterance is definitely missing
a main verb. In order to identify utterances where the
words ?did?, ?do? and ?does? are auxiliary verbs, we
use the following procedure: If the negation ?not?
is present after did/do/does, then did/do/does is an
auxiliary verb and needs to be followed by a main
verb. In the case of the utterance being a question,
the presence of did/do/does at the beginning of the
utterances indicates the use as an auxiliary verb. In
92
such a case, we need to check for the presence of a
main verb. The same holds for the other auxiliary
verbs.
Predictive model
We used the following as features:
1. Is an auxiliary verb present?
2. Is a modal verb present?
3. Is a main verb present after the auxiliary verb?
4. Is a main verb present after the modal verb?
5. Type of utterance - interrogative, declarative
6. Is a negation (not) present?
Analysis
As we can see from Table 3, the alternating decision
tree classifier using rules as features gave the best
result with an F1-measure of 0.905. At present, we
handle only a subset of missing verbs and specif-
ically those verbs that contain an auxiliary verb.
Since most of the utterances are simple constructs,
the alternating decision tree classifier performs well.
4.4 Identifying Subject-auxiliary Agreement
In the case of the subject-auxiliary agreement and
subject-verb agreement, the first verb in the verb
phrase has to agree with the subject unless the first
verb is a modal verb. In the sentence ?The girls has
bought a nice car?, since the subject ?The girls? is
a plural noun phrase, the auxiliary verb should be in
the plural form. While considering the number and
person of the subject, we take into account whether
the subject is an indefinite pronoun or contains a
conjunction since special rules apply to these cases.
Indefinite pronouns are words which replace nouns
without specifying the nouns they replace. Some in-
definite pronouns such as all, any and more take both
singular and plural forms. On the other hand, indefi-
nite pronouns like somebody and anyone always take
the singular form.
Rule based system
The rule we used to identify subject-auxiliary agree-
ment was as follows:
1. Extract the number (singular, plural) of the sub-
ject and the auxiliary verb in the verb phrase.
2. If the number of the subject and auxiliary verb
do not match, there is a subject-auxiliary agree-
ment error.
Predictive model
The features were as follows:
1. Number of subject - singular or plural
2. Type of noun phrase - pronoun or other noun
phrase
3. Person of noun phrase - first, second, third
4. Presence of a main verb in the utterance (we are
looking at the agreement only for the auxiliary
verb)
Analysis
As we can see from Table 3, the naive Bayes multi-
nomial classifier performed the best with an F1-
measure of 0.912. We found that our system did
not detect the subject-auxiliary agreement correctly
if there was an error in the subject such as number
agreement.
4.5 Identifying Subject-verb Agreement
In order to achieve subject-verb agreement, the num-
ber and person of the subject and verb must agree.
The subject-verb agreement applies to the first verb
in the verb phrase. We consider cases wherein the
first verb is a main verb or contains a modal verb.
An example of a sentence that has subject-verb dis-
agreement is ?The boy have an ice cream?. The
number and person of the subject ?The boy? and the
verb ?have? do not match.
Rule based system
The rule we used to identify subject-verb agreement
was as follows:
1. Extract the number (singular, plural) and per-
son (first, second, third) of the subject and the
first verb in the verb phrase.
2. If the verb is not a modal verb and the num-
ber and person of the subject and verb do not
match, there is a subject-verb agreement error.
Predictive model
We used the following features to be used in a statis-
tical setup:
93
1. Type of sentence - interrogative or declarative
2. Number of subject - singular or plural
3. Person of subject if pronoun - first, second or
third
4. Number of verb - singular or plural
5. Person of verb - first, second or third
6. Type of verb - modal, main
Analysis
We found that our system did not detect errors in
cases where there was a number disagreement. For
example, in the sentence ?The two dog is playing?,
our system based on the POS tag would assume
that the subject is singular and therefore there is no
subject-verb error. One way to improve this would
be to detect number disagreement in the subject and
correct it before detecting the subject-verb agree-
ment.
4.6 Identifying Missing Infinitive Marker ?To?
Errors of this type occur when the sentence lacks the
infinitive marker ?to?. An example of such a sen-
tence would be ?She loves sleep?. In this case, ?She
loves to sleep? would be the correct form. On the
other hand, this statement is ambiguous since sleep
could be used as a noun sense or a verb sense. We
concentrated on identifying utterances that have the
progressive verb followed by the verb in the infini-
tive form. Examples of such sentences are: ?She is
going cry?. In this case, we can see that the sentence
is missing the ?to?.
Rule based system
If the utterance contains a progressive verb followed
by a verb in its infinitive form, it is missing the in-
finitive marker ?to?.
Predictive model
The features we used are:
1. Presence of a progressive verb followed by the
infinitive
2. Presence of infinitive marker ?to? before the in-
finitive
Analysis
The naive Bayes multinomial classifier performed
the best with an F1-measure of 0.967. We encoun-
tered exceptions with words like ?saying?. An ex-
ample of such a sentence would be ?He was saying
play?. Most of our false positives were due to sen-
tences such as this. We considered a subset of utter-
ances in which the infinitive was used along with the
progressive verb. The missing infinitive marker ?to?
is also found in other utterances such as ?I would
love to swim? in which case we have two verbs that
are in the base form - ?love? and ?swim?.
4.7 Combining the Classifiers
Finally, we perform sentence level binary classifica-
tion - does the sentence have a grammatical error?
Since an utterance can contain more than one error,
we serially apply the binary classifiers that we de-
scribed above for each error category. If any one of
the classifiers reports an error in the utterance, we
flag the utterance as having a grammatical error. For
evaluation, as long as the utterance had any gram-
matical error, we considered the decision to be cor-
rect. As we can see from Table 3, the best result
for detecting the overall errors was obtained by se-
rially applying the classifiers that used the features
that were not rule based.
5 Conclusions and Future Work
In this paper, we described a study of grammati-
cal errors in child language transcripts. Our study
showed that a higher percentage of children with
LI made at least one mistake than TD children on
most error categories. We created different systems
including rule based systems that used parse tree
template matching and classifiers to detect errors re-
lated to missing verbs, subject-auxiliary agreement,
subject-verb agreement, missing infinitive marker
?to?, missing copulae and wrong usage of -ing par-
ticiple. In all cases, we had a recall higher than 84%.
When combining the classifiers to detect sentences
with grammatical errors, the classifiers that used fea-
tures other than rules performed the best with an F1-
measure of 0.967.
The error categories that we detect at present are
restricted in their scope to specific kind of errors.
In future, we plan to enhance our systems to de-
94
tect other grammatical errors such as missing arti-
cles, missing prepositions and missing main verbs
in utterances that do not have an auxiliary verb. Fur-
thermore, we will investigate methods to address is-
sues in child language transcripts due to incomplete
utterances and disfluencies.
At present, we treat sentences that conform to
formal English language as correct. We could en-
hance our systems to look at dialect specific con-
structs and grammatical errors made across differ-
ent demographics. For example, African American
children have a different dialect and do not always
follow the formal English language while speaking.
Therefore, in the context of detecting language im-
pairment, it would be interesting to see whether both
TD children and LI children make the same errors
that are otherwise considered the norm in the dialect
they speak.
Acknowledgments
The authors thank Chris Dollaghan for sharing the
Paradise data, and Thamar Solorio for discussions.
This research is partly supported by an NSF award
IIS-1017190.
References
Mari I. Bowden and Richard K. Fox. 2002. A Diagnostic
Approach to the Detection of Syntactic Errors in En-
glish for Non-Native Speakers. Technical report, The
University of Texas-Pan American.
Andrew Caines and Paula Buttery. 2010. You talking to
me?: A predictive model for zero auxiliary construc-
tions. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
43?51.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139.
Lloyd M. Dunn. 1965. Peabody picture vocabulary test.
American Guidance Service Circle Pines, MN.
Jens Eeg-Olofsson and Ola Knuttson. 2003. Automatic
grammar checking for second language learners-the
use of prepositions. In Proceedings of NoDaLiDa.
Keyur Gabani. 2009. Automatic identification of lan-
guage impairment in monolingual English-speaking
children. Master?s thesis, The University Of Texas At
Dallas.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?18.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical Er-
ror Detection for Language Learners. Synthesis Lec-
tures on Human Language Technologies, 3(1):1?134.
John Lee and Stephanie Seneff. 2006. Automatic gram-
mar correction for second-language learners. In Pro-
ceedings of INTERSPEECH-2006, pages 1978?1981.
John Lee and Stephanie Seneff. 2008. Correcting misuse
of verb forms. In Proceedings of ACL-08:HLT, pages
174?182.
Jack L. Paradise, Thomas F. Campbell, Christine A.
Dollaghan, Heidi M. Feldman, Bernard S. Bernard,
D. Kathleen Colborn, Howard E. Rockette, Janine E.
Janosky, Dayna L. Pitcairn, Marcia Kurs-Lasky, et al
2005. Developmental outcomes after early or delayed
insertion of tympanostomy tubes. New England Jour-
nal of Medicine, 353(6):576?586.
Mabel L. Rice, Kenneth Wexler, and Patricia L. Cleave.
1995. Specific language impairment as a period of
extended optional infinitive. Journal of Speech and
Hearing Research, 38(4):850.
Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-
ney, and Shuly Wintner. 2007. High-accuracy annota-
tion and parsing of CHILDES transcripts. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 25?32.
Hollis S. Scarborough. 1990. Index of productive syntax.
Applied Psycholinguistics, 11(01):1?22.
95
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 111?115,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Latent Dirichlet Allocation for Child Narrative Analysis
Khairun-nisa Hassanali and Yang Liu
The University of Texas at Dallas
Richardson, TX, USA
nisa,yangl@hlt.utdallas.edu
Thamar Solorio
University of Alabama at Birmingham
Birmingham, AL, USA
solorio@uab.edu
Abstract
Child language narratives are used for lan-
guage analysis, measurement of language
development, and the detection of lan-
guage impairment. In this paper, we ex-
plore the use of Latent Dirichlet Alloca-
tion (LDA) for detecting topics from nar-
ratives, and use the topics derived from
LDA in two classification tasks: automatic
prediction of coherence and language im-
pairment. Our experiments show LDA is
useful for detecting the topics that corre-
spond to the narrative structure. We also
observed improved performance for the
automatic prediction of coherence and lan-
guage impairment when we use features
derived from the topic words provided by
LDA.
1 Introduction
Language sample analysis is a common technique
used by speech language researchers to measure
various aspects of language development. These
include speech fluency, syntax, semantics, and co-
herence. For such analysis, spontaneous narratives
have been widely used. Narrating a story or a per-
sonal experience requires the narrator to build a
mental model of the story and use the knowledge
of semantics and syntax to produce a coherent nar-
rative. Children learn from a very early age to nar-
rate stories. The different processes involved in
generating a narrative have been shown to provide
insights into the language status of children.
There has been some prior work on child lan-
guage sample analysis using NLP techniques. Sa-
hakian and Snyder (2012) used a set of linguistic
features computed on child speech samples to cre-
ate language metrics that included age prediction.
Gabani et al (2011) combined commonly used
measurements in communication disorders with
several NLP based features for the prediction of
Language Impairment (LI) vs. Typically Develop-
ing (TD) children. The features they used included
measures of language productivity, morphosyntac-
tic skills, vocabulary knowledge, sentence com-
plexity, probabilities from language models, stan-
dard scores, and error patterns. In their work, they
explored the use of language models and machine
learning methods for the prediction of LI on two
types of child language data: spontaneous and nar-
rative data.
Hassanali et al (2012a) analyzed the use of
coherence in child language and performed auto-
matic detection of coherence from child language
transcripts using features derived from narrative
structure such as the presence of critical narrative
components and the use of narrative elements such
as cognitive inferences and social engagement de-
vices. In another study, Hassanali et al (2012b)
used several coherence related features to auto-
matically detect language impairment.
LDA has been used in the field of narrative anal-
ysis. Wallace et al (2012) adapted LDA to the task
of multiple narrative disentanglement, in which
the aim was to tease apart narratives by assigning
passages from a text to the subnarratives that they
belong to. They achieved strong empirical results.
In this paper, we explore the use of LDA for
child narrative analysis. We aim to answer two
questions: Can we apply LDA to children nar-
ratives to identify meaningful topics? Can we
represent these topics automatically and use them
for other tasks, such as coherence detection and
language impairment prediction? Our results are
promising. We found that using LDA topic model-
ing can infer useful topics, and incorporating fea-
tures derived from such automatic topics improves
the performance of coherence classification and
language impairment detection over the previously
reported results.
111
Coherence Scale TD LI Total
Coherent 81 6 87
Incoherent 18 13 31
Total 99 19 118
Table 1: Number of TD and LI children on a 2-
scale coherence level
2 Data
For the purpose of the experiments, we used the
Conti-Ramsden dataset (Wetherell et al, 2007a;
Wetherell et al, 2007b) from the CHILDES
database (MacWhinney, 2000). This dataset con-
sists of transcripts belonging to 118 adolescents
aged 14 years. The adolescents were given the
wordless picture story book ?Frog, where are
you?? and asked to narrate the story based on the
pictures. The storybook is about the adventures of
a boy who goes searching for his missing pet frog.
Even though our goal is to perform child narrative
analysis, we used this dataset from adoloscents
since it was publicly available, and was annotated
for language impairment and coherence. Of the
118 adolescents, 99 adolescents belonged to the
TD group and 19 adolescents belonged to the lan-
guage impaired group. Hassanali et al (2012a)
annotated this dataset for coherence. A transcript
was annotated as coherent, as long as there was no
difficulty in understanding the narrative, and in-
coherent otherwise. Table 1 gives the TD and LI
distribution on a 2-scale coherence level. Figure
1 shows an example of a transcript produced by a
TD child.
Figure 1: Sample transcript from a TD child
3 Narrative Topic Analysis Using LDA
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) has been used in NLP to model topics within
a collection of documents. In this study, we use
LDA to detect topics in narratives. Upon exam-
ining the transcripts, we observed that each topic
was described in about 3 to 4 utterances. We there-
fore segmented the narratives into chunks of 3 ut-
terances, with the assumption that each segment
corresponds roughly to one topic.
We used the software by Blei et al1 to perform
LDA. Prior to performing LDA, we removed the
stop words from the transcripts. We chose ? to
be 0.8 and K to be 20, where ? is the parameter
of the Dirichlet prior on the per-document topic
distributions and K denotes the number of topics
considered in the model.
We chose to use the transcripts of TD children
for generating the topics, because the transcripts of
TD children have fewer disfluencies, incomplete
utterances, and false starts. As we can observe
from Table 1, a higher percentage of TD children
produced coherent narratives when compared to
children with LI.
Table 2 gives the topic words for the top 10
topics extracted using LDA. The topics in Table
2 were manually labeled after examination of the
topic words extracted using LDA. We found that
some of the topics extracted by LDA corresponded
to subtopics. For example, searching for the frog
in the house has subtopics of the boy searching
for the frog in room and the dog falling out of the
window, which were part of the topics covered by
LDA. The subtopics are marked in italics in Table
2.
The following narrative components were iden-
tified as important features for the automatic pre-
diction of coherence by Hassanali et al (2012a).
1. Instantiation: introduce the main characters
of the story: the boy, the frog, and the dog,
and the frog goes missing
2. 1st episode: search for the frog in the house
3. 2nd episode: search for the frog in the tree
4. 3rd episode: search for the frog in the hole in
the ground
5. 4th episode: search for the frog near the rock
6. 5th episode: search for the frog behind the
log
7. Resolution: boy finds the frog in the river and
takes a frog home
Upon examining the topics extracted by LDA, we
observed that all the components mentioned above
1http://www.cs.princeton.edu/ blei/lda-c/index.html
112
Topic
No
Topic Words Used by TD Population Topic Described
1 went,frog,sleep,glass,put,caught,jar,yesterday,out,house Introduction
2 frog,up,woke,morning,called,gone,escaped,next,kept,realized Frog goes missing
3 window,out,fell,dog,falls,broke,quickly,opened,told,breaking Dog falls out of window
4 tree,bees,knocked,running,popped,chase,dog,inside,now,flying Dog chases the bees
5 deer,rock,top,onto,sort,big,up,behind,rocks,picked Deer behind the rock
6 searched,boots,room,bedroom,under,billy,even, floor,tilly,tried Search for frog in room
7 dog,chased,owl,tree,bees,boy,came,hole,up,more Boy is chased by owl from a
tree with beehives
8 jar,gone,woke,escaped,night,sleep,asleep,dressed,morning,frog Frog goes missing
9 deer,top,onto,running,ways,up,rocks,popped,suddenly,know Boy runs into the deer
10 looking,still,dog,quite,cross,obviously,smashes,have,annoyed Displeasure of boy with dog
Table 2: Top 10 topic words extracted by LDA on the story telling task. Subtopics are shown in italics.
were present in these topics. Many of the LDA
topics corresponded to a picture or two in the sto-
rybook.
4 Using LDA Topics for Coherence and
Language Impairment Classification
We extended the use of LDA for two tasks,
namely: the automatic evaluation of coherence
and the automatic evaluation of language impair-
ment. For the experiments below, we used the
WEKA toolkit (Hall et al, 2009) and built sev-
eral models using the naive Bayes, Bayesian net
classifier, Logistic Regression, and Support Vec-
tor Machine (SVM) classifier. Of all these classi-
fiers, the naive Bayes classifier performed the best,
and we report the results using the naive Bayes
classifier in Tables 3 and 4. We performed all the
experiments using leave-one-out cross-validation,
wherein we excluded the test transcript that be-
longed to a TD child from the training set when
generating topics using LDA.
4.1 Automatic Evaluation of Coherence
We treat the automatic evaluation of coherence
as a classification task. A transcript could either
be classified as coherent or incoherent. We use
the results of Hassanali et al (2012a) as a base-
line. They used the presence of narrative episodes,
and the counts of narrative quality elements such
as cognitive inferences and social engagement de-
vices as features in the automatic prediction of co-
herence. We add the features that we automati-
cally extracted using LDA.
We checked for the presence of at least six of
the ten topic words or their synonyms per topic in
a window of 3 utterances. If the topic words were
present, we took this as a presence of a topic; oth-
erwise we denoted it as an absence of a topic. In
total, there were 20 topics that we extracted using
LDA, which is higher compared to the 8 narrative
structure topics that were annotated for by Has-
sanali et al (2012a).
Table 3 gives the results for the automatic clas-
sification of coherence. As we observe in Table
3, there is an improvement in performance over
the baseline. We attribute this to the inclusion of
subtopics that were extracted using LDA.
4.2 Automatic Evaluation of Language
Impairment
We extended the use of LDA to create a summary
of the narratives. For the purpose of generating the
summary, we considered only the narratives gen-
erated by TD children in the training set. We gen-
erated a summary, by choosing 5 utterances cor-
responding to each topic that was generated using
LDA, thereby yielding a summary that consisted
of 100 utterances.
We observed that different words were used to
represent the same concept. For example, ?look?
and ?search? were used to represent the concept
of searching for the frog. Since the narration was
based on a picture storybook, many of the children
used different terms to refer to the same animal.
For example, ?the deer? in the story has been inter-
preted to be ?deer?, ?reindeer?, ?moose?, ?stag?,
?antelope? by different children. We created an
extended topic vocabulary using Wordnet to in-
clude words that were semantically similiar to the
topic keywords. In addition, for an utterance to be
113
Feature Set
Coherent Incoherent Accuracy
(%)Precision Recall F-1 Precision Recall F-1
Narrative (Hassanali et al,
2012a) (baseline)
0.869 0.839 0.854 0.588 0.645 0.615 78.814
Narrative + automatic topic
features
0.895 0.885 0.89 0.688 0.71 0.699 83.898
Table 3: Automatic classification of coherence on a 2-scale coherence level
in the summary, we put in the additional constraint
that neighbouring utterances within a window of
3 utterances also talk about the same topic. We
used this summary for constructing unigram and
bigram word features for the automatic prediction
of LI.
The features we constructed for the prediction
of LI were as follows:
1. Bigrams of the words in the summary
2. Presence or absence of the words in the sum-
mary regardless of the position
3. Presence or absence of the topics detected by
LDA in the narratives
4. Presence or absence of the topic words that
were detected using LDA
We used both the topics detected and the pres-
ence/absence of topic words as features since the
same topic word could be used across several top-
ics. For example, the words ?frog?, ?dog?, ?boy?,
and ?search? are common across several topics.
We refer to the above features as ?new features?.
Table 4 gives the results for the automatic pre-
diction of LI using different features. As we can
observe, the performance improves to 0.872 when
we add the new features to Gabani?s and the nar-
rative structure features. When we use the new
features by themselves to predict language impair-
ment, the performance is the worst. We attribute
this to the fact that other feature sets are richer
since these features take into account aspects such
as syntax and narrative structure.
We performed feature analysis on the new fea-
tures to see what features contributed the most.
The top scoring features were the presence or ab-
sence of the topics detected by LDA that corre-
sponded to the introduction of the narrative, the
resolution of the narrative, the search for the frog
in the room, and the search for the frog behind
the log. The following bigram features generated
from the summary contributed the most: ?deer
Feature P R F-1
Gabani?s (Gabani et
al., 2011)
0.824 0.737 0.778
Narrative (Hassanali et
al., 2012a)
0.385 0.263 0.313
New features 0.308 0.211 0.25
Narrative + Gabani?s 0.889 0.842 0.865
Narrative + Gabani?s +
new features
0.85 0.895 0.872
Table 4: Automatic classification of language im-
pairment
rock?, ?lost frog?, and ?boy hole?. Using a subset
of these best features did not improve the perfor-
mance when we added them to the narrative fea-
tures and Gabani?s features.
5 Conclusions
In this paper, we explored the use of LDA in the
context of child language analysis. We used LDA
to extract topics from child language narratives
and used these topic keywords to create a sum-
mary of the narrative and an extended vocabu-
lary. The topics extracted using LDA not only
covered the main components of the narrative but
also covered subtopics too. We then used the LDA
topic words and the summary to create features
for the automatic prediction of coherence and lan-
guage impairment. Due to higher coverage of the
LDA topics as compared to manual annotation, we
found an increase in performance of both auto-
matic prediction of coherence and language im-
pairment with the addition of the new features. We
conclude that the use of LDA to model topics and
extract summaries is promising for child language
analysis.
Acknowledgements
This research is supported by NSF awards IIS-
1017190 and 1018124.
114
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Keyur Gabani, Thamar Solorio, Yang Liu, Khairun-
nisa Hassanali, and Christine A. Dollaghan. 2011.
Exploring a corpus-based approach for detect-
ing language impairment in monolingual English-
speaking children. Artificial Intelligence in
Medicine, 53(3):161?170.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012a. Coherence in child language nar-
ratives: A case study of annotation and automatic
prediction of coherence. In Proceedings of WOCCI
2012 - 3rd Workshop on Child, Computer and Inter-
action.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012b. Evaluating NLP features for au-
tomatic prediction of language impairment using
child speech transcripts. In Proceedings of INTER-
SPEECH.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, Volume I: Transcription for-
mat and programs. Lawrence Erlbaum Associates.
Sam Sahakian and Benjamin Snyder. 2012. Automat-
ically learning measures of child language develop-
ment. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 95?99. Association
for Computational Linguistics.
Bryon C. Wallace. 2012. Multiple narrative disentan-
glement: Unraveling infinite jest. In Proceeding of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1?10.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007a. Narrative in adolescent specific
language impairment (SLI): a comparison with peers
across two different narrative genres. International
Journal of Language & Communication Disorders,
42(5):583?605.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007b. Narrative skills in adolescents
with a history of SLI in relation to non-verbal IQ
scores. Child Language Teaching and Therapy,
23(1):95.
115
