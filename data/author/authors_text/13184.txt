53
54
55
56
The Szeged Corpus: a POS tagged and syntactically annotated Hungarian 
natural language corpus 
D?ra CSENDES 
University of Szeged Department of 
Informatics 
?rp?d t?r 2. 
Szeged, Hungary, H-6720 
dcsendes@inf.u-szeged.hu 
J?nos CSIRIK 
University of Szeged Department of 
Informatics 
?rp?d t?r 2. 
Szeged, Hungary, H-6720 
csirik@inf.u-szeged.hu 
Tibor GYIM?THY 
University of Szeged Department of Informatics 
?rp?d t?r 2. 
Szeged, Hungary, H-6720 
gyimothy@inf.u-szeged.hu 
 
Abstract 
The Szeged Corpus is a manually annotated 
natural language corpus currently comprising 1.2 
million word entries, 145 thousand different word 
forms, and an additional 225 thousand punctuation 
marks. With this, it is the largest manually 
processed Hungarian textual database that serves 
as a reference material for research in natural 
language processing as well as a learning database 
for machine learning algorithms and other software 
applications. Language processing of the corpus 
texts so far included morpho-syntactic analysis, 
POS tagging and shallow syntactic parsing. 
Semantic information was also added to a pre-
selected section of the corpus to support automated 
information extraction. 
The present state of the Szeged Corpus (Alexin 
et al, 2003) is the result of three national projects 
and the cooperation of the University of Szeged, 
Department of Informatics, MorphoLogic Ltd. 
Budapest, and the Research Institute for 
Linguistics at the Hungarian Academy of Sciences. 
Corpus texts have gone through different phases of 
natural language processing (NLP) and analysis. 
Extensive and accurate manual annotation of the 
texts, incorporating over 124 person-months of 
manual work, is a great value of the corpus.  
1 Texts of the Szeged Corpus 
When selecting texts for the Szeged Corpus, the 
main criteria was that they should be thematically 
representative of different text types. The first 
version of the corpus, therefore, contains texts 
from five genres, roughly 200 thousand words 
each. Due to its relative variability, it serves as a 
good reference material for natural language 
research applications, and proves to be large 
enough to guarantee the robustness of machine 
learning methods. Genres of Szeged Corpus 1.0 
include: 
? fiction (two Hungarian novels and the 
Hungarian translation of Orwell's 1984) 
? compositions of 14-16-year-old students 
? newspaper articles (excerpts from three 
daily and one weekly paper) 
? computer-related texts (excerpts from a 
Windows 20001 manual book and some 
issues of the ComputerWorld, 
Sz?m?t?stechnika magazine) 
? law (excerpts from legal texts on economic 
enterprises and authors' rights). 
 
Size 
 
 
Text type Number  
of words 
Number  
of punct. 
marks 
Number 
of ambig. 
words 
Ratio of 
ambig. 
words 
Fiction 187191 47968 100968 53.94% 
Composi-
tions 
223058 47208 127821 57.30% 
Newspaper 187334 32948 91375 48.78% 
Computer 181980 31906 82698 45.44% 
Law 222186 38715 106581 47.97% 
Short 
business 
news 
188345 25817 82813 43,7% 
Total 1190094 224562 592256 49.52% 
Table 1. Data about Szeged Corpus 2.0 
During further developments, the first version of 
the corpus was extended with a 200 thousand-
word-long sample of short business news2. The 
                                                     
1
 Kis, Bal?zs: Windows 2000, Szak kiad?, 2000 
2
 Short business news originate from the archive of 
the Hungarian News Agency (http://www.mti.hu/). 
newly added section served as an experimental 
database for learning semantic frame mapping to 
be later integrated in an IE technology. Table 1. 
shows data referring to Szeged Corpus 2.0. 
2 Annotation of the Szeged Corpus 
Morpho-syntactic analysis and POS tagging of 
the corpus texts included two steps. Initially, words 
were morpho-syntactically analysed with the help 
of the Humor3 automatic pre-processor. The 
program determined the possible morpho-syntactic 
labels of the lexicon entries, thereby creating the 
ambiguous version of the corpus. After the pre-
processing, the entire corpus was manually 
disambiguated (POS tagged) by linguists. For the 
tagging of the Szeged Corpus, the Hungarian 
version of the internationally acknowledged MSD 
(Morpho-Syntactic Description) scheme (Erjavec, 
Monachini, 1997) was selected. Due to the fact that 
the MSD encoding scheme is extremely detailed 
and refined (one label can store information on up 
to 17 positions), there is a large number of 
ambiguous cases, i.e. one word is likely to have 
more than one possible labels. Experiences show 
that by applying the MSD encoding scheme, 
roughly every second word of the corpus is 
ambiguous. Disambiguation, therefore, required 
accurate and detailed work cumulating up to 64 
person-months of manual annotation. Currently all 
possible labels as well as the selected ones are 
stored in the corpus. 
A unique feature of the corpus is that parallel to 
POS tagging, users? rules have been defined for 
each ambiguous word in a pre-selected (202 600-
word-long) section of the corpus. The aim of 
applying users? rules was to mark the relevant 
context (relevant set of words) that determines the 
selection of a certain POS tag. Users? rules apply 
before1, before2, ... after1, after2, ... 
predicates for marking the relevant context of a 
word4. The manually defined rules can then be 
generalised to regular disambiguation rules 
applicable to unknown texts as well. Out of the 
selected 202 600 words 114 951 were ambiguous. 
Annotators defined users? rules for these cases 
among which 26 912 different ones were found. 
The major advantage of the defined rules lies in 
their accuracy and specificity, wherefore they are 
an interesting and valuable source of additional 
linguistic information that can e.g. support the 
more precise training of machine learning 
algorithms. 
                                                     
3
 The Humor morpho-syntactic analyser is a product 
of the MorphoLogic Ltd. Budapest. 
4
 The predicate after2 e.g., denotes the second 
word to the right of the focus word. 
After the completion of POS tagging, a project5 
was initiated to encompass shallow syntactic 
parsing of the Szeged Corpus. The linguistic 
information identified by shallow syntactic parsing 
proves to be rich enough to support a number of 
large-scale NLP applications including information 
extraction (IE), text summarisation, machine 
translation, phrase identification in information 
retrieval, named entity identification, and a variety 
of text-mining operations. In order to achieve their 
goal, researchers of the University of Szeged, 
Department of Informatics, the MorphoLogic Ltd. 
Budapest, and the Research Institute for 
Linguistics at the Hungarian Academy of Sciences 
had to conduct some research concerning the 
syntax of Hungarian sentences, NP annotation 
schemes, and rules covering the recognition of 
phrases. Results showed that in Hungarian, 
nominal structures typically bear the most 
significant meaning (semantic content) within a 
sentence, therefore NP annotation seemed to be the 
most reasonable step forward.  
Shallow parsing was carried out on the entire 
Szeged Corpus 2.0 (1.2 million words). Automated 
pre-parsing was completed with the help of the 
CLaRK6 program, in which regular syntactic rules 
have been defined by linguistic experts for the 
recognition of NPs. Due to the fact that the CLaRK 
parser did not fully cover the occurring NP 
structures (its coverage was around 70%), manual 
validation and correction could not be avoided. In 
total, 250 thousand highest level NPs were found, 
and the deepest NP structure contained 9 NPs 
imbedded into each other. The majority of the 
hierarchic NP structures were between 1 to 3 NPs 
deep. Manual validation and correction lasted 60 
person-months.  
As a continuation of shallow parsing, the clause 
structure (CPs) of the corpus sentences was also 
marked. Labelling clauses followed the same 
approach as earlier phases of NLP: it comprised an 
automatic pre-annotation followed by manual 
correction and supplementation. 
3 Use of the Szeged Corpus for training and 
testing machine learning algorithms 
Due to the accurate and exhaustive manual 
annotation, the resulting corpus (both first and 
second versions) could serve as an adequate 
                                                     
5
 National Research and Development Programmes 
(NKFP) 2/017/2001 project funded by the Hungarian 
Ministry of Education, titled Information Extraction 
from Short Business News. 
6
 The CLaRK system was developed by Kiril Simov 
at the Bulgarian Academy of Sciences in the framework 
of the BulTreeBank project. 
database for the training and testing of machine 
learning algorithms. The applicability of these 
algorithms in Hungarian NLP was extensively 
studied in the past couple of years (Horv?th et al, 
1999), (H?cza et al, 2003). Researchers of the 
University of Szeged experimented with different 
kind of POS tagging methods and compared their 
results based on accuracy. Brill?s transformation-
based learning method (Brill, 1995) worked with 
96.52% per word accuracy when trained and tested 
on the corpus. The HMM-based TnT tagger 
(Brants, 2000) performed 96.18%, while the 
RGLearn rule-based tagger (H?cza et al, 2003) 
produced 94.54% accuracy. Researchers also 
experimented with the combination of the different 
learning methods in order to increase accuracy. 
The best accuracy result, delivered by combining 
the above three methods, was 96.95%. Overall 
results showed that despite the agglutinating nature 
of Hungarian language and the structural 
differences between Hungarian and other Indo-
European languages, all of the mentioned methods 
can be used effectively for learning POS tagging. 
The applicability of machine learning methods 
for learning NP recognition rules was also 
investigated. The C 4.5 (Quinlan, 1993) and the 
RGLearn rule-based algorithms were selected for 
the learning process. NP recognition rules have 
been retrieved from the annotated corpus and were 
combined with manually defined expert rules. The 
main task of the NP recognition parser is to 
provide the best possible coverage of NP structures 
 
Categories of 
recognition 
Precision Recall Accuracy 
Complete NPs 81.28% 87.43% 84.32% 
Boundaries (first and 
last element) of NPs 88.31% 92.08% 90.54% 
NPs (depth<=2) 86.02% 89.72% 88.37% 
NPs (depth>2) 74.71% 78.19% 76.61% 
Average 82.58% 86.85% 84.96% 
Table 3. Test results of the RGLearn parser 
The mentioned algorithms ? although still under 
development ? already perform between 80-90% 
accuracy (see Table 3.). Their performance 
strongly depends on the type of the processed text: 
phrase structures are recognised with better 
accuracy in news or technical type of texts than in 
student?s compositions (where sentences are often 
grammatically inaccurate) or legal texts (where 
sentences are typically extremely long, and 
fragmented). 
As a continuation of the work, an automated 
method was developed to perform IE from short 
business news. The 200 thousand word long, short 
business news section of the corpus was used as 
the training database for the IE tool. In the 
preparatory phase, the selected section of the 
corpus was enriched with semantic information. 
Possible semantic roles, such as SELLER, 
BUYER, PRODUCT, PRICE, DATE etc., were 
associated with each word, and were stored in a 
semantic dictionary. The most typical events of 
business life were represented by so-called 
semantic frames describing the relations of the 
different semantic roles. Possible frames were 
defined manually by linguists and allowed 
mapping between the lexical representation and the 
semantic role of a word. Semantic mapping rules 
were acquired by machine learning algorithms that 
used the manually annotated semantic roles as their 
learning source. The recognition of semantic 
frames was also supported by the series of NLP 
methods described earlier (i.e. POS tagging and 
shallow parsing).  
During the developed information extraction 
process, the trained mapping tool takes a morpho-
syntactically and syntactically annotated piece of 
text and performs two operations. First, it 
processes the morpho-syntactically disambiguated 
and shallow parsed text and assigns semantic roles 
to the words. The second operation determines 
relationships between the roles, i.e. maps semantic 
frames onto the existing structures. Semantic 
mapping is realised by simple pattern-matching 
methods using the frames previously defined by 
experts. Based on the results of the described 
operations, the mapping tool builds a semantic 
representation of the input text, already containing 
the required information. Results produced by this 
method were tested against the manually annotated 
corpus and showed that it identifies semantic roles 
with 94-99% accuracy and maps frames with up to 
80% accuracy. 
4 Current and future works 
Current works aim at a more detailed syntactic 
analysis of the Szeged Corpus. With this, 
developers intend to lay the foundation of a 
Hungarian treebank, which is planned to be 
enriched with detailed semantic information as 
well in the future. The development of a suitable 
technique for the recognition and annotation of 
named entities (e.g., multi-word proper nouns) and 
special tokens (e.g., time expressions, dates, 
measures, bank account numbers, web- and e-mail 
addresses, etc.) is also planned in the near future. 
Further works aim at building firstly domain 
specific, later general ontologies and at developing 
automated methods that allow for extensive 
semantic analysis and processing of Hungarian 
sentences. 
5 Related work 
Corpus-based methods play an important role in 
empirical linguistics as well as in the application of 
machine learning algorithms. Annotated reference 
corpora, such as the Brown Corpus (Kucera, 
Francis, 1967), the Penn Treebank (Marcus et al, 
1993), and the BNC (Leech et al, 2001.), have 
helped both the development of English 
computational linguistics tools and English corpus 
linguistics. Manual POS tagging and syntactic 
annotation are costly but allow one to build and 
improve sizable linguistic resources and also to 
train and evaluate automated analysers.  
. The NEGRA (Skut at al., 1997) POS tagged 
and syntactically annotated corpus of 355 thousand 
tokens was the first initiative in corpus linguistics 
for German. The more recent TIGER Treebank 
project (Brants et al, 2002) aims at building the 
largest and most extensively annotated treebank for 
German. Currently, it comprises 700 thousand 
tokens of newspaper text that were automatically 
analysed and manually checked. Considerable 
results were achieved for Czech in the framework 
of the Prague Dependency Treebank project 
(Hajic, 1998), and for Bulgarian in the 
BulTreeBank project (Simov et al, 2003) as well. 
The Szeged Corpus project is comparable both 
in size and in depth of analysis to the corpus and 
treebank initiatives mentioned above7. As the first 
such like initiative for Hungarian language, it is a 
valuable source for linguistic research and a 
suitable training and testing basis for machine 
applications and automated induction of linguistic 
knowledge. 
References  
Alexin Z., Csirik J., Gyim?thy T., Bibok K., 
Hatvani Cs., Pr?sz?ky G., Tihanyi L.: Manually 
Annotated Hungarian Corpus in Proc. of the 
Research Note Sessions of the 10th Conference 
of the European Chapter of the Association for 
Computational Linguistics (EACL?03), pp. 53-
56, Budapest, Hungary (2003) 
Brants, T.: TnT - A Statistical Part-of-Speech 
Tagger, in Proc. of the Sixth Conference on 
Applied Natural Language Processing (ANLP), 
Seattle, WA (2000) 
Brants, S., Dipper, S., Hansen, S., Lezius, W. and 
Smith, G.: The TIGER Treebank in Proc. of the 
Workshop on Treebanks and Linguistic 
Theories, Sozopol, Bulgaria (2002) 
Brill, E.: Transformation-based error-driven 
learning and natural language processing: A 
                                                     
7
 The different versions of the Szeged Corpus are 
available at http://www.inf.u-szeged.hu/hlt. 
case study in part-of-speech tagging, 
Computational Linguistics, vol. 21 (4), pp. 543-
565, (1995) 
Erjavec, T., Monachini, M.: Specification and 
Notation for Lexicon Encoding, Copernicus 
Project 106 ?MULTEX-EAST?, Work Package 
1 ? Task 1.1, Deliverable D1.1F (1997) 
Hajic, J.: Building a Syntactically Annotated 
Corpus: The Prague Dependency Treebank in 
Issues of Valency and Meaning, pp. 106-132, 
Charles University Press, Prague (1998) 
Horv?th T., Alexin Z., Gyim?thy T., Wrobel S.: 
Application of Different Learning Methods to 
Hungarian Part-of-Speech Tagging in Proc. of 
the 9th International Workshop on Inductive 
Logic Programming (ILP99), pp. 128-139, Bled, 
Slovenia and in the LNAI series vol 1634, 
Springer Verlag (1999) 
H?cza A., Alexin Z., Csendes D., Csirik J., 
Gyim?thy T.: Application of ILP methods in 
different natural language processing phases for 
information extraction from Hungarian texts in 
Proc. of the Kalm?r Workshop on Logic and 
Computer Science, pp. 107-116, Szeged, 
Hungary (2003) 
Kucera H., and Francis, W. N.: Brown Corpus 
Manual Providence, Rhode Island, Brown 
University Press (1979) 
Leech, G., P. Rayson and A. Wilson: Word 
Frequencies in Written and Spoken English: 
based on the British National Corpus, Longman, 
London (2001) 
Marcus, M., Santorini, B., Marcinkiewicz, M.: 
Building a large annotated corpus of English: 
the Penn Treebank in Computational 
Linguistics, vol. 19 (1993) 
Quinlan, J. R.: C 4.5: Programs for Machine 
Learning, Morgan Kaufmann Publisher (1993) 
Simov, K., Simov, A., Kouylekov, M., Ivanova, 
K., Grigorov, I., Ganev, H.: Development of 
Corpora within the CLaRK System: The 
BulTreeBank Project Experience in Proc. of the 
Demo Sessions of the 10th Conference of the 
European Chapter of the Association for 
Computational Linguistics (EACL'03), pp. 243-
246, Budapest, Hungary (2003) 
Skut, W., Brants, T., Krenn, B., Uszkoreit, H.: A 
linguistically interpreted corpus of German 
newspaper text in Proc. of the Conference on 
Language Resources and Evaluation (LREC-98), 
pp. 705-711, Granade, Spain (1997) 
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 38?45,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The BioScope corpus: annotation for negation, uncertainty and their 
scope in biomedical texts 
 
 
Gy?rgy Szarvas1, Veronika Vincze1, Rich?rd Farkas2 and J?nos Csirik2 
1Department of Informatics 2Research Group on Artificial Intelligence 
University of Szeged Hungarian Academy of Science 
H-6720, Szeged, ?rp?d t?r 2. H-6720, Szeged, Aradi v?rtan?k tere 1. 
{szarvas, vinczev, rfarkas, csirik}@inf.u-szeged.hu 
 
Abstract 
This article reports on a corpus annotation 
project that has produced a freely available re-
source for research on handling negation and 
uncertainty in biomedical texts (we call this 
corpus the BioScope corpus). The corpus con-
sists of three parts, namely medical free texts, 
biological full papers and biological scientific 
abstracts. The dataset contains annotations at 
the token level for negative and speculative 
keywords and at the sentence level for their 
linguistic scope. The annotation process was 
carried out by two independent linguist anno-
tators and a chief annotator ? also responsible 
for setting up the annotation guidelines ? who 
resolved cases where the annotators disagreed. 
We will report our statistics on corpus size, 
ambiguity levels and the consistency of anno-
tations. 
1 Introduction 
Detecting uncertain and negative assertions is es-
sential in most Text Mining tasks where in general, 
the aim is to derive factual knowledge from textual 
data. This is especially so for many tasks in the 
biomedical (medical and biological) domain, 
where these language forms are used extensively in 
textual documents and are intended to express im-
pressions, hypothesised explanations of experi-
mental results or negative findings. Take, for 
example, the clinical coding of medical reports, 
where the coding of a negative or uncertain disease 
diagnosis may result in an over-coding financial 
penalty. Another example from the biological do-
main is interaction extraction, where the aim is to 
mine text evidence for biological entities with cer-
tain relations between them. Here, while an uncer-
tain relation or the non-existence of a relation 
might be of some interest for an end-user as well, 
such information must not be confused with real 
textual evidence (reliable information). A general 
conclusion is that for text mining, extracted infor-
mation that is within the scope of some negative / 
speculative (hedge or soft negation) keyword 
should either be discarded or presented separately 
from factual information.  
Even though many successful text processing 
systems (Friedman et al, 1994, Chapman et al 
2001, Elkin et al 2005) handle the above-
mentioned phenomena, most of them exploit hand-
crafted rule-based negation/uncertainty detection 
modules. To the best of our knowledge, there are 
no publicly available standard corpora of reason-
able size that are usable for evaluating the auto-
matic detection and scope resolution of these 
language phenomena. The availability of such a 
resource would undoubtedly facilitate the devel-
opment of corpus-based statistical systems for ne-
gation/hedge detection and resolution.  
Our study seeks to fill this gap by presenting the 
BioScope corpus, which consists of medical and 
biological texts annotated for negation, speculation 
and their linguistic scope. This was done to permit 
a comparison between and to facilitate the devel-
opment of systems for negation/hedge detection 
and scope resolution. The corpus described in this 
paper has been made publicly available for re-
search purposes and it is freely downloadable1. 
                                                          
1 www.inf.u-szeged.hu/rgai/bioscope  
38
1.1 Related work 
Chapman et al (2001) created a simple regular 
expression algorithm called NegEx that can detect 
phrases indicating negation and identify medical 
terms falling within the negative scope. With this 
process, a large part of negatives can be identified 
in discharge summaries. 
Mutalik et al (2001) earlier developed 
Negfinder in order to recognise negated patterns in 
medical texts. Their lexer uses regular expressions 
to identify words indicating negation and then it 
passes them as special tokens to the parser, which 
makes use of the single-token look-ahead strategy. 
Thus, without appealing to the syntactic structure 
of the sentence, Negfinder can reliably identify 
negated concepts in medical narrative when they 
are located near the negation markers. 
Huang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified on 
the basis of syntactic categories and they are 
located in parse trees. Their hybrid approach is 
able to identify negated concepts in radiology 
reports even when they are located at some 
distance from the negative term. 
The Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor in order to encode 
clinical documents in a structured form (Friedman 
et al, 1994). Negated concepts and certainty 
modifiers are also encoded within the system, thus 
it enables them to make a distinction between 
negated/uncertain concepts and factual information 
which is crucial in information retrieval. 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words in order 
to identify negated statements and their scope. 
Although a fair amount of literature on 
uncertainty (or hedging) in scientific texts has been 
produced since the 1990s (e.g. Hyland, 1994), 
speculative language from a Natural Language 
Processing perspective has only been studied in the 
past few years. Previous studies (Light et al, 2004) 
showed that the detection of hedging can be solved 
effectively by looking for specific keywords which 
imply speculative content. 
Another possibility is to treat the problem as a 
classification task and train a statistical  model to 
discriminate speculative and non-speculative 
assertions. This approach requires the availability 
of labeled instances to train the models on. 
Medlock and Briscoe (2007) proposed a weakly 
supervised setting for hedge classification in 
scientific texts where the aim is to minimise human 
supervision needed to obtain an adequate amount 
of training data. Their system focuses on locating 
hedge cues in text and thus they do not determine 
the scopes (in other words in a text they define the 
scope to be a whole sentence). 
1.2 Related resources 
Even though the problems of negation (mainly in 
the medical domain) and hedging (mainly in the 
scientific domain) have received much interest in 
the past few years, open access annotated resources 
for training, testing and comparison are rare and 
relatively small in size. Our corpus is the first one 
with an annotation of negative/speculative 
keywords and their scope. The authors are only 
aware of the following related corpora: 
 
? The Hedge classification corpus (Medlock 
and Briscoe, 2007), which has been 
annotated for hedge cues (at the sentence 
level) and consists of five full biological 
research papers (1537 sentences). No scope 
annotation is given in the original corpus. 
We included this publicly available corpus 
in ours, enriching the data with annotation 
for negation cues and linguistic scope for 
both hedging and negation. 
? The Genia Event corpus (Kim et al, 2008), 
which annotates biological events with 
negation and three levels of uncertainty 
(1000 abstracts). 
? The BioInfer corpus (Pyysalo et al, 2007), 
where biological relations are annotated for 
negation (1100 sentences in size).  
In the two latter corpora biological terms 
(relations and events) have been annotated for both 
negation and hedging, but linguistic cues (i.e. 
which keyword modifies the semantics of the 
statement) have not been annotated. We annotated 
keywords and their linguistic scope, which is very 
useful for machine learning or rule-based negation 
and hedge detection systems. 
39
2 Annotation guidelines 
This section describes the basic principles on the 
annotation of speculative and negative scopes in 
biomedical texts. Some basic definitions and tech-
nical details are given in Section 2.1, then the gen-
eral guidelines are discussed in Section 2.2 and the 
most typical keywords and their scopes are illus-
trated with examples in Section 2.3. Some special 
cases and exceptions are listed in Section 2.4, then 
the annotation process of the corpus is described 
and discussed in Section 2.5. The complete annota-
tion guidelines document is available from the cor-
pus homepage. 
2.1 Basic issues 
In a text, just sentences with some instance of 
speculative or negative language are considered for 
annotation. The annotation is based on linguistic 
principles, i.e. parts of sentences which do not con-
tain any biomedical term are also annotated if they 
assert the non-existence/uncertainty of something.  
As for speculative annotation, if a sentence is a 
statement, that is, it does not include any specula-
tive element that suggests uncertainty, it is disre-
garded. Questions inherently suggest uncertainty ? 
which is why they are asked ?, but they will be 
neglected and not annotated unless they contain 
speculative language. 
Sentences containing any kind of negation are 
examined for negative annotation. Negation is un-
derstood as the implication of the non-existence of 
something. However, the presence of a word with 
negative content does not imply that the sentence 
should be annotated as negative, since there are 
sentences that include grammatically negative 
words but have a speculative meaning or are actu-
ally regular assertions (see the examples below). 
In the corpus, instances of speculative and nega-
tive language ? that is, keywords and their scope ? 
are annotated. Speculative elements are marked by 
angled brackets: <or>, <suggests> etc., while 
negative keywords are marked by square brackets: 
[no], [without] etc. The scope of both negative and 
speculative keywords is denoted by parentheses. 
Also, the speculative or negative cue is always in-
cluded within its scope: 
This result (<suggests> that the valency of Bi in 
the material is smaller than + 3). 
Stable appearance the right kidney ([without] hy-
dronephrosis). 
In the following, the general guidelines for specu-
lative and negative annotation are presented. 
2.2 General guidelines 
During the annotation process, we followed a min-
max strategy for the marking of keywords and their 
scope. When marking the keywords, a minimalist 
strategy was followed: the minimal unit that ex-
pressed hedging or negation was marked as a key-
word. However, there are some cases when hedge 
or negation can be expressed via a phrase rather 
than a single word. Complex keywords are phrases 
that express uncertainty or negation together, but 
they cannot do this on their own (the meaning or 
the semantics of its subcomponents are signifi-
cantly different from the semantics of the whole 
phrase). An instance of a complex keyword can be 
seen in the following sentence: 
Mild bladder wall thickening (<raises the question 
of> cystitis). 
On the other hand, a sequence of words cannot be 
marked as a complex keyword if it is only one of 
those words that express speculative or negative 
content (even without the other word). Thus prepo-
sitions, determiners, adverbs and so on are not an-
notated as parts of the complex keyword if the 
keyword can have a speculative or negative con-
tent on its own: 
The picture most (<likely> reflects airways dis-
ease). 
Complex keywords are not to be confused with the 
sequence of two or more keywords because they 
can express hedge or negation on their own, that is, 
without the other keyword as well. In this case, 
each keyword is annotated separately, as is shown 
in the following example: 
Slightly increased perihilar lung markings (<may> 
(<indicate> early reactive airways disease)). 
2.3 Scope marking 
When marking the scopes of negative and specula-
tive keywords, we extended the scope to the big-
gest syntactic unit possible (in contrast to other 
corpora like the one described in (Mutalik et al, 
2001)). Thus, annotated scopes always have the 
40
maximal length ? as opposed to the strategy for 
annotating keywords, where we marked the mini-
mal unit possible. Our decision was supported by 
two facts. First, since scopes must contain their 
keywords, it seemed better to include every ele-
ment in between the keyword and the target word 
in order to avoid ?empty? scopes, that is, scopes 
without a keyword. In the next example, however 
is not affected by the hedge cue but it should be 
included within the scope, otherwise the keyword 
and its target phrase would be separated: 
(Atelectasis in the right mid zone is, however, 
<possible>). 
Second, the status of modifiers is occasionally 
vague: it is sometimes not clear whether the modi-
fier of the target word belongs to its scope as well. 
The following sentence can describe two different 
situations: 
There is [no] primary impairment of glucocorti-
coid metabolism in the asthmatics. 
First, the glucocorticoid metabolism is impaired in 
the asthmatics but not primarily, that is, the scope 
of no extends to primary. Second, the scope of no 
extends to impairment (and its modifiers and com-
plements as well), thus there is no impairment of 
the glucocorticoid metabolism at all. Another ex-
ample is shown here: 
Mild viral <or> reactive airways disease is de-
tected. 
The syntactic structure of the above sentence is 
ambiguous. First, the airways disease is surely 
mild, but it is not known whether it is viral or reac-
tive; or second, the airways disease is either mild 
and viral or reactive and not mild. Most of the sen-
tences with similar problems cannot be disambigu-
ated on the basis of contextual information, hence 
the proper treatment of such sentences remains 
problematic. However, we chose to mark the wid-
est scope available: in other words, we preferred to 
include every possible element within the scope 
rather than exclude elements that should probably 
be included. 
 The scope of a keyword can be determined on 
the basis of syntax. The scope of verbs, auxiliaries, 
adjectives and adverbs usually extends to the right 
of the keyword. In the case of verbal elements, i.e. 
verbs and auxiliaries, it ends at the end of the 
clause (if the verbal element is within a relative 
clause or a coordinated clause) or the sentence, 
hence all complements and adjuncts are included, 
in accordance with the principle of maximal scope 
size. Take the following examples: 
The presence of urothelial thickening and mild 
dilatation of the left ureter (<suggest> that the 
patient may have continued vesicoureteral reflux). 
These findings that (<may> be from an acute 
pneumonia) include minimal bronchiectasis as 
well. 
These findings (<might> be chronic) and (<may> 
represent reactive airways disease). 
The scope of attributive adjectives generally ex-
tends to the following noun phrase, whereas the 
scope of predicative adjectives includes the whole 
sentence. For example, in the following two state-
ments: 
This is a 3 month old patient who had (<possible> 
pyelonephritis) with elevated fever. 
(The demonstration of hormone receptor proteins 
in cells from malignant effusions is <possible>). 
Sentential adverbs have a scope over the entire 
sentence, while the scope of other adverbs usually 
ends at the end of the clause or sentence. For in-
stance, 
(The chimaeric oncoprotein <probably> affects 
cell survival rather than cell growth). 
Right upper lobe volume loss and (<probably> 
pneumonia). 
The scope of conjunctions extends to all members 
of the coordination. That is, it usually extends to 
the both left and right: 
Symptoms may include (fever, cough <or> itches). 
Complex keywords such as either ? or have one 
scope: 
Mild perihilar bronchial wall thickening may rep-
resent (<either> viral infection <or> reactive 
airways disease). 
Prepositions have a scope over the following 
(noun) phrase: 
Mildly hyperinflated lungs ([without] focal opac-
ity). 
41
When the subject of the sentence contains the 
negative determiners no or neither, its scope ex-
tends to the entire sentence: 
Surprisingly, however, ([neither] of these proteins 
bound in vitro to EBS1 or EBS2). 
The main exception that changes the original scope 
of the keyword is the passive voice. The subject of 
the passive sentence was originally the object of 
the verb, that is, it should be within its scope. This 
is why the subject must also be marked within the 
scope of the verb or auxiliary. For instance, 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Another example of scope change is the case of 
raising verbs (seem, appear, be expected, be likely 
etc.). These can have two different syntactic pat-
terns, as the following examples suggest:  
It seems that the treatment is successful. 
The treatment seems to be successful. 
In the first case, the scope of seems starts right 
with the verb. If this was the case in the second 
pattern, the treatment would not be included in the 
scope, but it should be like that shown in the first 
pattern. Hence in the second sentence, the scope 
must be extended to the subject as well: 
It (<seems> that the treatment is successful). 
(The treatment <seems> to be successful). 
Sometimes a negative keyword is present in the 
text apparently without a scope: negative obviously 
expresses negation, but the negated fact ? what 
medical problem the radiograph is negative for ? is 
not part of the sentence. In such cases, the keyword 
is marked and the scope contains just the keyword: 
([Negative]) chest radiograph. 
In the case of elliptic sentences, the same strategy 
is followed: the keyword is marked and its scope 
includes only the keyword since the verbal phrase, 
that is, the scope of not, is not repeated in the sen-
tence. 
This decrease was seen in patients who responded 
to the therapy as well as in those who did ([not]). 
Generally, punctuation marks or conjunctions 
function as scope boundary markers in the corpus, 
in contrast to the corpus described in (Mutalik et 
al., 2001) where certain lexical items are treated as 
negation-termination tokens. Since in our corpus 
the scope of negation or speculation is mostly ex-
tended to the entire clause in the case of verbal 
elements, it is clear that markers of a sentence or 
clause boundary determine the end of their scope. 
2.4 Special cases 
It seems unequivocal that whenever there is a 
speculative or negative cue in the sentence, the 
sentence expresses hedge or negation. However, 
we have come across several cases where the pres-
ence of a speculative/negative keyword does not 
imply a hedge/negation. That is, some of the cues 
do not denote speculation or negation in all their 
occurrences, in other words, they are ambiguous. 
For instance, the following sentence is a state-
ment and it is the degree of probability that is pre-
cisely determined, but it is not an instance of 
hedging although it contains the cue probable: 
The planar amide groups in which is still digging 
nylon splay around 30 less probable event. 
As for negative cues, sentences including a nega-
tive keyword are not necessarily to be annotated 
for negation. They can, however, have a specula-
tive content as well. The following sentence con-
tains cannot, which is a negative keyword on its 
own, but not in this case: 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Some other sentences containing a negative key-
word are not to be annotated either for speculation 
or for negation. In the following example, the 
negative keyword is accompanied by an adverb 
and their meaning is neither speculative nor nega-
tive. The sequence of the negative keyword and the 
adverb can be easily substituted by another adverb 
or adjective having the same (or a similar) mean-
ing, which is by no means negative ? as shown in 
the example. In this way, the sentence below can 
be viewed as a positive assertion (not a statement 
of the non-existence of something). 
Thus, signaling in NK3.3 cells is not always 
(=sometimes) identical with that in primary NK 
cells. 
As can be seen from the above examples, hedging 
or negation is determined not just by the presence 
42
of an apparent cue: it is rather an issue of the key-
word, the context and the syntactic structure of the 
sentence taken together. 
2.5 Annotation process 
Our BioScope corpus was annotated by two inde-
pendent linguists following the guidelines written 
by our linguist expert before the annotation of the 
corpus was initiated. These guidelines were devel-
oped throughout the annotation process as annota-
tors were often confronted with problematic issues. 
The annotators were not allowed to communicate 
with each other as far as the annotation process 
was concerned, but they could turn to the expert 
when needed and regular meetings were also held 
between the annotators and the linguist expert in 
order to discuss recurring and/or frequent problem-
atic issues. When the two annotations for one sub-
corpus were finalised, differences between the two 
were resolved by the linguist expert, yielding the 
gold standard labeling of the subcorpus. 
3 Corpus details 
In this section we will discuss in detail the overall 
characteristics of the corpus we developed, includ-
ing a brief description of the texts that constitute 
the BioScope corpus and some general statistics 
concerning the size of each part, distribution of 
negation/hedge cues, ambiguity levels and finally 
we will present statistics on the final results of the 
annotation work. 
3.1 Corpus texts 
The corpus consists of texts taken from 4 different 
sources and 3 different types in order to ensure that 
it captures the heterogenity of language use in the 
biomedical domain. We decided to add clinical 
free-texts (radiology reports), biological full papers 
and biological paper abstracts (texts from Genia). 
Table 1 summarises the chief characteristics of 
the three subcorpora. The 3rd and 5th rows of the 
table show the ratio of sentences which contain 
negated or uncertain statements. The 4rd and 6th 
rows show the number of negation and hedge cue 
occurrences in the given corpus.  
A major part of the corpus consists of clinical 
free-texts. We chose to add medical texts to the 
corpus in order to facilitate research on nega-
tion/hedge detection in the clinical domain. The 
radiology report corpus that was used for the clini-
cal coding challenge (Pestian et al, 2007) organ-
ised by the Computational Medicine Center in 
Cincinatti, Ohio in 2007 was annotated for nega-
tions and uncertainty along with the scopes of each 
phenomenon. This part contains 1954 documents, 
each having a clinical history and an impression 
part, the latter being denser in negated and specula-
tive parts. 
Another part of the corpus consists of full sci-
entific articles. 5 articles from FlyBase (the same 
data were used by Medlock and Briscoe (2007) for 
evaluating sentence-level hedge classifiers) and 4 
articles from the open access BMC Bioinformatics 
website were downloaded and annotated for nega-
tions, uncertainty and their scopes. Full papers are 
particularly useful for evaluating negation/hedge 
classifiers as different parts of an article display 
different properties in the use of speculative or ne-
gated phrases. Take, for instance, the Conclusions 
section of scientific papers that tends to contain 
significantly more uncertain or negative findings 
than the description of Experimental settings and 
methods. 
Scientific abstracts are the main targets for 
various Text Mining applications like protein-
protein interaction mining due to their public ac-
cessibility (e.g. through PubMed). We therefore 
decided to include quite a lot of texts from the ab-
stracts of scientific papers. This is why we in-
cluded the abstracts of the Genia corpus (Collier et 
al., 1999). This decision was straightforward for 
two reasons. First, the Genia corpus contains syn-
tax tree annotation, which allows a comparison 
between scope annotation and syntactic structure. 
Being syntactic in nature, scopes should align with 
the bracket structure of syntax trees, while scope 
resolution algorithms that exploit treebank data can 
be used as a theoretical upper bound for the 
evaluation of parsers for resolving negative/hedge 
scopes. The other reason was that scope annotation 
can mutually benefit from the rich annotations of 
the Genia corpus, such as term annotation (evalua-
tion) and event annotation (comparison with the 
biologist uncertainty labeling of events). 
The corpus consists of more than 20.000 anno-
tated sentences altogether. We consider this size to 
be sufficiently large to serve as a standard evalua-
tion corpus for negation/hedge detection in the 
biomedical domain. 
 
43
 Clinical Full Paper Abstract 
#Documents 1954 9 1273 
#Sentences 6383 2624 11872 
Negation  
sentences 6.6% 13.76% 13.45% 
#Negation cues 871 404 1757 
Hedge sentences 13.4% 22.29% 17.69% 
#Hedge cues 1137 783 2691 
Table 1: Statistics of the three subcorpora. 
3.2 Agreement analysis 
We measured the consistency level of the annota-
tion using inter-annotator agreement analysis. The 
inter-annotator agreement rate is defined as the 
F?=1 measure of one annotation, treating the second 
one as the gold standard. We calculated agreement 
rates for all three subcorpora between the two in-
dependent annotators and between the two annota-
tors and the gold standard labeling. The gold 
standard labeling was prepared by the creator of 
the annotation guide, who resolved all cases where 
the two annotators disagreed on a keyword or its 
scope annotation. 
We measured the agreement rate of annotating 
negative and hedge keywords, and the agreement 
rate of annotating the linguistic scope for each 
phenomenon. We distinguished left-scope, right-
scope and full scope agreement that required both 
left and right scope boundaries to match exactly to 
be considered as coinciding annotations. A detailed 
analysis of the consistency levels for the three sub-
corpora and the ambiguity levels for each negative 
and hedge keyword (that is, the ratio of a keyword 
being annotated as a negative/speculative cue and 
the number of all the occurrences of the same 
keyword in the corpus) can be found at the corpus 
homepage. 
 
3.3 BioScope corpus availability 
The corpus is available free of charge for research 
purposes and can be obtained for a modest price 
for business use. For more details, see the Bio-
Scope homepage: 
www.inf.u-szeged.hu/rgai/bioscope. 
4 Conclusions 
In this paper we reported on the construction of a 
corpus annotated for negations, speculations and 
their linguistic scopes. The corpus is accessible for 
academic purposes and is free of charge. Apart 
from the intended goal of serving as a common 
resource for the training, testing and comparison of 
biomedical Natural Language Processing systems, 
the corpus is also a good resource for the linguistic 
analysis of scientific and clinical texts. 
The most obvious conclusions here are that the 
usual language of clinical documents makes it 
much easier to detect negation and uncertainty 
cues than in scientific texts because of the very 
high ratio of the actual cue words (i.e. low ambigu-
ity level), which explains the high accuracy scores 
reported in the literature. In scientific texts ? which 
are nowadays becoming a popular target for Text 
Mining (for literature-based knowledge discovery) 
? the detection and scope resolution of negation 
and uncertainty is, on the other hand, a problem of 
great complexity, with the percentage of non-
hedge occurrences being as high as 90% for some 
hedge cue candidates in biological paper abstracts. 
Take for example the keyword or which is labeled 
as a speculative keyword in only 11.32% of the 
cases in scientific abstracts, while it was labeled as 
speculative in 97.86% of the cases in clinical texts. 
Identifying the scope is also more difficult in sci-
entific texts where the average sentence length is 
much longer than in clinical data, and the style of 
the texts is also more literary in the former case. 
In our study we found that hedge detection is a 
more difficult problem than identifying negations 
because the number of possible cue words is higher 
and the ratio of real cues is significantly lower in 
the case of speculation (higher keyword/non-
keyword ambiguity). The annotator-agreement ta-
ble also confirms this opinion: the detection of 
hedging is more complicated than negation even 
for humans. 
Our corpus statistics also prove the importance 
of negation and hedge detection. The ratio of ne-
gated and hedge sentences in the corpus varies in 
the subcorpora, but we can say that over 20% of 
the sentences contains a modifier that radically 
influences the semantic content of the sentence. 
One of the chief construction principles of the 
BioScope corpus was to facilitate the train-
ing/development of automatic negation and hedge 
detection systems. Such systems have to solve two 
sub-problems: they have to identify real cue words 
(note that the probability of any word being a key-
word can be different for various domains) and 
44
then they have to determine the linguistic scope of 
actual keywords. 
These automatic hedge and negation detection 
methods can be utilised in a variety of ways in a 
(biomedical) Text Mining system. They can be 
used as a preprocessing tool, i.e. each word in a 
detected scope can be removed from the docu-
ments if we seek to extract true assertions. This can 
significantly reduce the level of noise for process-
ing in such cases where only a document-level la-
beling is provided (like that for the ICD-9 coding 
dataset) and just clear textual evidence for certain 
things should be extracted. On the other hand, 
similar systems can classify previously extracted 
statements according to their certainty or uncer-
tainty, which is generally an important issue in the 
automatic processing of scientific texts. 
Acknowledgments 
This work was supported in part by the NKTH 
grant of the Jedlik ?nyos R&D Programme 2007 
(project codename TUDORKA7) of the Hungarian 
government. The authors wish to thank the anony-
mous reviewers for their useful suggestions and 
comments. The authors also wish to thank the crea-
tors of the ICD-9 coding dataset and the Genia 
corpus for making the texts that were used here 
publicly available. The authors thank Jin-Dong 
Kim as well for the useful comments and sugges-
tions on the annotation guide and Orsolya Vincze 
and Mih?ly Mink? (the two annotators) for their 
work. 
References  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34(5):301?310. 
N. Collier, H. S. Park, N. Ogata, Y. Tateishi, C. Nobata, 
T. Ohta, T. Sekimizu, H. Imai, K. Ibushi, and J. Tsu-
jii. 1999. The GENIA project: corpus-based knowl-
edge acquisition and information extraction from 
genome research papers. Proceedings of EACL-99. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom and 
Dietlind L. Wahner-Roedler. 2005. A controlled trial 
of automated classification of negation from clinical 
notes. BMC Medical Informatics and Decision Mak-
ing 5:13 doi:10.1186/1472-6947-5-13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural-language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161?174. 
Yang Huang and Henry J. Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clini-
cal Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304?311. 
Ken Hyland. 1994. Hedging in academic writing and 
EAP textbooks. English for Specific Purposes, 
13(3):239?256. 
Jin-Dong Kim, Tomoko Ohta, and Jun'ichi Tsujii. 2008. 
Corpus annotation for mining biomedical events 
from literature. BMC Bioinformatics 2008, 9:10. 
Marc Light, Xin Ting Qui, and Padmini Srinivasan. 
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of 
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users. 
Boston, Massachusetts, Association for Computa-
tional Linguistics, 17?24. 
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific 
literature. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics. Pra-
gue, Association for Computational Linguistics, 992?
999. 
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of General-purpose 
Negation Detection to Augment Concept Indexing of 
Medical Documents: A Quantitative Study Using the 
UMLS. Journal of the American Medical Informatics 
Association, 8(6):598?609. 
John P. Pestian, Chris Brew, Pawel Matykiewicz, DJ 
Hovermale, Neil Johnson, K. Bretonnel Cohen, and 
Wlodzislaw Duch. 2007. A shared task involving 
multi-label classification of clinical free text. In Bio-
logical, translational, and clinical language process-
ing. Prague, Association for Computational 
Linguistics, 97?104. 
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen, and Tapio 
Salakoski. 2007. BioInfer: a corpus for information 
extraction in the biomedical domain. BMC Bioinfor-
matics 2007, 8:50 doi:10.1186/1471-2105-8-50. 
45
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1110?1118,
Beijing, August 2010
Hungarian Corpus of Light Verb Constructions
Veronika Vincze
University of Szeged
Department of Informatics
vinczev@inf.u-szeged.hu
Ja?nos Csirik
Hungarian Academy of Sciences
Research Group on Artificial Intelligence
csirik@inf.u-szeged.hu
Abstract
The precise identification of light verb
constructions is crucial for the successful
functioning of several NLP applications.
In order to facilitate the development of
an algorithm that is capable of recogniz-
ing them, a manually annotated corpus of
light verb constructions has been built for
Hungarian. Basic annotation guidelines
and statistical data on the corpus are also
presented in the paper. It is also shown
how applications in the fields of machine
translation and information extraction can
make use of such a corpus and an algo-
rithm.
1 Introduction
In this paper, we report a corpus containing light
verb constructions in Hungarian. These expres-
sions are neither productive nor idiomatic and
their meaning is not totally compositional (the
noun is usually taken in one of its literal senses but
the verb usually loses its original sense to some
extent), as it can be seen in the examples from dif-
ferent languages shown below. Since their mean-
ing is the same, only literal translations are pro-
vided:
? English: to give a lecture, to come into
bloom, the problem lies (in)
? German: halten eine Vorlesung to hold a pre-
sentation, in Blu?te stehen in bloom to stand,
das Problem liegt (in) the problem lies (in)
? French: faire une pre?sentation to make a pre-
sentation, e?tre en fleur to be in bloom, le
proble`me re?side (dans) the problem resides
(in)
? Hungarian: elo?ada?st tart presentation-
ACC holds, vira?gba borul bloom-ILL falls,
proble?ma rejlik (vmiben) problem hides (in
sg)
Several terms like complex verb structures, sup-
port verb constructions or light verb constructions
have been used1 for these constructions in the lit-
erature (Langer, 2004). In this paper, the term
light verb constructions will be employed.
The structure of the paper is as follows. First,
the importance of the special NLP treatment of
light verb constructions is emphasized in section
2. The precise identification of such constructions
is crucial for the successful functioning of NLP
applications, thus, it is argued that an algorithm
is needed to automatically recognize them (sec-
tion 4). In order to facilitate the development of
such an algorithm, a corpus of light verb construc-
tions has been built for Hungarian, which is pre-
sented together with statistical data in section 5.
Finally, it is shown how NLP applications in the
fields of machine translation and information ex-
traction can profit from the implementation of an
algorithm capable of identifying light verb con-
structions (section 6).
2 Light verb constructions in NLP
In natural language processing, one of the most
challenging tasks is the proper treatment of col-
1There might be slight theoretical differences in the usage
of these terms ? e.g. semantically empty support verbs are
called light verbs in e.g. Meyers et al (2004a), that is, the
term support verb is a hypernym of light verb. However,
these differences are not analyzed in detail in this paper.
1110
locations, which term comprises light verb con-
structions as well. Every multiword expression
is considered to be a collocation if its members
often co-occur and its form is fixed to some ex-
tent (Siepmann, 2005; Siepmann, 2006; Sag et al,
2001; Oravecz et al, 2004; Va?radi, 2006). Col-
locations are frequent in language use and they
usually exhibit unique behaviour, thus, they often
pose a problem to NLP systems.
Light verb constructions deserve special atten-
tion in NLP applications for several reasons. First,
their meaning is not totally compositional, that is,
it cannot be computed on the basis of the mean-
ings of the parts of the collocation and the way
they are related to each other. Thus, the result of
translating the parts of the collocation can hardly
be considered as the proper translation of the orig-
inal expression. Second, light verb constructions
(e.g. make a mistake) often share their syntac-
tic pattern with other constructions such as lit-
eral verb + noun combinations (e.g. make a cake)
or idioms (e.g. make a meal), thus, their identi-
fication cannot be based on solely syntactic pat-
terns. Third, since the syntactic and the seman-
tic head of the construction are not the same ?
the syntactic head being the verb and the seman-
tic head being the noun ?, they require special
treatment when parsing. It can be argued that
they form a complex verb similarly to phrasal or
prepositional verbs (as reflected in the term com-
plex verb structures). Thus, it is advisable to indi-
cate their special syntacto-semantic relationship:
in dependency grammars, the new role QUASI-
ARGUMENT might be proposed for this purpose.
3 Related work
Light verb constructions ? as a subtype of multi-
word expressions ? have been paid special atten-
tion in NLP literature. Sag et al (2001) classify
them as a subtype of lexicalized phrases and flex-
ible expressions. They are usually distinguished
from productive or literal verb + noun construc-
tions on the one hand and idiomatic verb + noun
expressions on the other hand: e.g. Fazly and
Stevenson (2007) use statistical measures in order
to classify subtypes of verb + noun combinations
and Diab and Bhutada (2009) developed a chunk-
ing method for classifying multiword expressions.
Identifying multiword expressions in general
and light verb constructions in particular is not
unequivocal since constructions with similar syn-
tactic structure (e.g. verb + noun combinations)
can belong to different subclasses on the produc-
tivity scale (i.e. productive combinations, light
verb constructions and idioms). That is why well-
designed and tagged corpora of multiword ex-
pressions are invaluable resources for training and
testing algorithms that are able to identify multi-
word expressions. For instance, Gre?goire (2007)
describes the design and implementation of a lexi-
con of Dutch multiword expressions. Focusing on
multiword verbs, Kaalep and Muischnek (2006;
2008) present an Estonian database and a corpus
and Krenn (2008) describes a database of German
PP-verb combinations. The Prague Dependency
Treebank also contains annotation for light verb
constructions (Cinkova? and Kola?r?ova?, 2005) and
NomBank (Meyers et al, 2004b) provides the ar-
gument structure of common nouns, paying atten-
tion to those occurring in support verb construc-
tions as well. On the other hand, Zarrie? and Kuhn
(2009) make use of translational correspondences
when identifying multiword expressions (among
them, light verb constructions). A further exam-
ple of corpus-based identification of light verb
constructions in English is described in Tan et al
(2006).
Light verb constructions are considered to be
semi-productive, that is, certain verbs tend to co-
occur with nouns belonging to a given semantic
class. A statistical method is applied to measure
the acceptability of possible light verb construc-
tions in Stevenson et al (2004), which correlates
reasonably well with human judgments.
4 Identifying light verb constructions
A database of light verb constructions and an an-
notated corpus might be of great help in the au-
tomatic recognition of light verb constructions.
They can serve as a training database when imple-
menting an algorithm for identifying those con-
structions.
The recognition of light verb constructions can-
not be solely based on syntactic patterns for other
(productive or idiomatic) combinations may ex-
hibit the same verb + noun scheme (see section
1111
2). However, in agglutinative languages such as
Hungarian, nouns can have several grammatical
cases, some of which typically occur in a light
verb construction when paired with a certain verb.
For instance, the verb hoz ?bring? is a transitive
verb, that is, it usually occurs with a noun in the
accusative case. On the other hand, when it is pre-
ceded or followed by a noun in the sublative or
illative case (the typical position of the noun in
Hungarian light verb constructions being right be-
fore or after the verb2), it is most likely a light verb
construction. To illustrate this, we offer some ex-
amples:
vizet hoz
water-ACC bring
?to bring some water?
zavarba hoz
trouble-ILL bring
?to embarrass?
The first one is a productive combination (with
the noun being in the accusative form) while the
second one is a light verb construction. Note that
the light verb construction also has got an argu-
ment in the accusative case (syntactically speak-
ing, a direct object complement) as in:
Ez a megjegyze?s mindenkit zavarba ho-
zott.
this the remark everyone-ACC trouble-
ILL bring-PAST-3SG
?This remark embarrassed everybody.?
Thus, the presence of an argument in the ac-
cusative does not imply that the noun + verb com-
bination is a light verb construction. On the other
hand, the presence of a noun in the illative or
sublative case immediately preceding or follow-
ing the verb strongly suggests that a light verb in-
stance of hoz is under investigation.
Most light verb constructions have a verbal
counterpart derived from the same stem as the
noun, which entails that it is mostly deverbal
2In a neutral sentence, the noun is right before the verb,
in a sentence containing focus, it is right after the verb.
nouns that occur in light verb constructions (as
in make/take a decision compared to decide or
do?nte?st hoz vs. do?nt in Hungarian). The identifi-
cation of such nouns is possible with the help of a
morphosyntactic parser that is able to treat deriva-
tion as well (e.g. hunmorph for Hungarian (Tro?n
et al, 2005)), and the combination of a possible
light verb and a deverbal noun typically results in
a light verb construction.
Thus, an algorithm that makes use of mor-
phosyntactic and derivational information and
previously given lists can be constructed to iden-
tify light verb constructions in texts. It is impor-
tant that the identification of light verb construc-
tions precedes syntactic parsing, for the noun and
the verb in the construction form one complex
predicate, which has its effects on parsing: other
arguments belong not solely to the verb but to the
complex predicate.
To the best of our knowledge, there are no cor-
pora of light verb constructions available for Hun-
garian. That is why we decided to build such a
corpus. The corpus is described in detail in sec-
tion 5. On the basis of the corpus developed, we
plan to design an algorithm to automatically iden-
tify light verb constructions in Hungarian.
5 The corpus
In order to facilitate the extraction and the NLP
treatment of Hungarian light verb constructions,
we decided to build a corpus in which light verb
constructions are annotated. The Szeged Tree-
bank (Csendes et al, 2005) ? a database in which
words are morphosyntactically tagged and sen-
tences are syntactically parsed ? constitutes the
basis for the annotation. We first selected the
subcorpora containing business news, newspaper
texts and legal texts for annotation since light verb
constructions are considered to frequently occur
in these domains (see B. Kova?cs (1999)). How-
ever, we plan to extend the annotation to other
subcorpora as well (e.g. literary texts) in a later
phase. Statistical data on the annotated subcor-
pora can be seen in Table 1.
5.1 Types of light verb constructions
As Hungarian is an agglutinative language, light
verb constructions may occur in various forms.
1112
sentences words
business news 9574 186030
newspapers 10210 182172
legal texts 9278 220069
total 29062 582871
Table 1: Number of sentences and words in the
annotated subcorpora
For instance, the verbal component may be in-
flected for tense, mood, person, number, etc.
However, these inflectional differences can be eas-
ily resolved by a lemmatizer. On the other hand,
besides the prototypical noun + verb combination,
light verb constructions may be present in differ-
ent syntactic structures, that is, in participles and
infinitives and they can also undergo nominaliza-
tion. These types are all annotated in the corpus
texts since they also occur relatively frequently
(see statistical data in 5.3). All annotated types
are illustrated below.
? Noun + verb combination <verb>
bejelente?st tesz
announcement-ACC makes
?to make an announcement?
? Participles <part>
? Present participle
e?letbe le?po? (inte?zkede?s)
life-ILL stepping (instruction)
?(an instruction) taking effect?
? Past participle
cso?dbe ment (ce?g)
bankrupt-ILL gone (firm)
?(a firm) that went bankrupt?
? Future participle
fontolo?ra veendo? (aja?nlat)
consideration-SUB to be taken (offer)
?(an offer) that is to be taken into con-
sideration?
? Infinitive
forgalomba hozni
circulation-ILL bring-INF
?to put into circulation?
? Nominalization <nom>
be?rbe ve?tel
rent-ILL taking
?hiring?
Split light verb constructions, where the noun
and the verb are not adjacent, are also annotated
and tagged. In this way, their identification be-
comes possible and the database can be used for
training an algorithm that automatically recog-
nizes (split) light verb constructions.
5.2 Annotation principles
Corpus texts contain single annotation, i.e. one
annotator worked on each text. Light verb con-
structions can be found in between XML tags
<FX></FX>. In order to decide whether a noun
+ verb combination is a light verb construction or
not, annotators were suggested to make use of a
test battery developed for identifying Hungarian
light verb constructions (Vincze, 2008).
The annotation process was carried out manu-
ally on the syntactically annotated version of the
Szeged Treebank, thus, phrase boundaries were
also taken into consideration when marking light
verb constructions. Since the outmost boundary
of the nominal component was considered to be
part of the light verb construction, in several cases
adjectives and other modifiers of the nominal head
are also included in the construction, e.g.:
<FX>nyilva?nos aja?nlatot tesz</FX>
public offer-ACC make
?to make a public offer?
In the case of participles, NP arguments may
be also included (although in English, the same
argument is expressed by a PP):
<FX>Ny??regyha?za?n tartott
u?le?se?n</FX>
Ny??regyha?za-SUP hold-PPT session-
3SGPOSS-SUP
?at its session held in Ny??regyha?za?
Constructions with a nominal component in the
accusative case can be nominalized in two ways
in Hungarian, as in:
1113
szerzo?de?st ko?t
contract-ACC bind
?to make a contract?
<FX>szerzo?de?sko?te?s</FX>
contract+bind-GERUND
?making a contract?
<FX>ada?sve?teli szerzo?de?sek
megko?te?se</FX>
sale contract-PL PREVERB-bind-
GERUND-3SGPOSS
?making of sales contracts?
Both types are annotated in the corpus.
Besides the prototypical occurrences of light
verb constructions (i.e. a bare common noun +
verb3), other instances were also annotated in the
corpus. For instance, the noun might be accompa-
nied by an article or a modifier (recall that phrase
boundaries were considered during annotation) or
? for word order requirements ? the noun follows
the verb as in:
O? hozta a jo? do?nte?st.
he bring-PAST-3SG-OBJ the good
decision-ACC
?It was him who made the good deci-
sion.?
For the above reasons, a single light verb con-
struction manifests in several different forms in
the corpus. However, each occurrence is manu-
ally paired with its prototypical (i.e. bare noun +
verb) form in a separate list, which is available at
the corpus website.
5.3 Statistics on corpus data
The database contains 3826 occurrences of 658
light verb constructions altogether in 29062 sen-
tences. Thus, a specific light verb construction
3As opposed to other languages where prototypical light
verb constructions consist of a verb + a noun in accusative or
a verb + a prepositional phrase (see e.g. Krenn (2008)), in
Hungarian, postpositional phrases rarely occur within a light
verb construction. However, annotators were told to annotate
such cases as well.
occurs 5.8 times in the corpus on average. How-
ever, the participle form ira?nyado? occurs in 607
instances (e.g. in ira?nyado? kamat ?prime rate?)
due to the topic of the business news subcorpus,
which may distort the percentage rates. For this
reason, statistical data in Table 2 are shown the
occurrences of ira?nyado? excluded.
verb part nom split total
business 565 270 90 40 965
news 58.6% 28% 9.3% 4.1% 25.2%
news- 458 192 55 67 772
papers 59.3% 24.9% 7.1% 8.7% 20.2%
legal 640 504 709 236 2089
texts 30.7% 24.1% 33.9% 11.3% 54.6%
total 1663 966 854 236 3826
43.5% 25.2% 22.3% 9% 100%
Table 2: Subtypes of light verb constructions in
the corpus
It is revealed that although it is verbal occur-
rences that are most frequent, the percentage rate
of participles is also relatively high. The number
of nominalized or split constructions is consider-
ably lower (except for the law subcorpus, where
their number is quite high), however, those to-
gether with participles are responsible for about
55% of the data, which indicates the importance
of their being annotated as well.
As for the general frequency of light verb con-
structions in texts, we compared the number of
verb + argument relations found in the Szeged De-
pendency Treebank (Vincze et al, 2010) where
the argument was a common noun to that of light
verb constructions. It has turned out that about
13% of verb + argument relations consist of light
verb constructions. This again emphasizes that
they should be paid attention to, especially in the
legal domain (where this rate is as high as 36.8%).
Statistical data are shown in Table 3.
V + argument LVC
business news 9524 624 (6.6%)
newspapers 3637 539 (14.8%)
legal texts 2143 889 (36.8%)
total 15574 2052 (13.2%)
Table 3: Verb + argument relations and light verb
constructions
The corpus is publicly available for re-
1114
search and/or educational purposes at
www.inf.u-szeged.hu/rgai/nlp.
6 The usability of the corpus
As emphasized earlier, the proper treatment of
light verb constructions is of primary importance
in NLP applications. In order to achieve this,
their identification is essential. The corpus cre-
ated can function as the training database for the
implementation of an algorithm capable of recog-
nizing light verb constructions, which we plan to
develop in the near future. In the following, the
ways machine translation and information extrac-
tion can profit from such a corpus and algorithm
are shortly presented.
6.1 Light verb constructions and machine
translation
When translating collocations, translation pro-
grams face two main problems. On the one hand,
parts of the collocation do not always occur next
to each other in the sentence (split collocations).
In this case, the computer must first recognize that
the parts of the collocation form one unit (Oravecz
et al, 2004), for which the multiword context of
the given word must be considered. On the other
hand, the lack (or lower degree) of compositional-
ity blocks the possibility of word-by-word trans-
lation (Siepmann, 2005; Siepmann, 2006). How-
ever, a (more or less) compositional account of
light verb constructions is required for successful
translation (Dura and Gawron?ska, 2005).
To overcome these problems, a reliable method
is needed to assure that the nominal and verbal
parts of the construction be matched. This re-
quires an algorithm that can identify light verb
constructions. In our corpus, split light verb con-
structions are also annotated, thus, it is possible to
train the algorithm to recognize them as well: the
problem of split collocations can be eliminated in
this way.
A comprehensive list of light verb construc-
tions can enhance the quality of machine transla-
tion ? if such lists are available for both the source
and the target language. Annotated corpora (es-
pecially and most desirably, parallel corpora) and
explanatory-combinatorial dictionaries4 are possi-
4Explanatory combinatorial dictionaries are essential for
ble sources of such lists. Since in foreign language
equivalents of light verb constructions, the nomi-
nal components are usually literal translations of
each other (Vincze, 2009), by collating the cor-
responding noun entries in these lists the foreign
language variant of the given light verb construc-
tion can easily be found. On the other hand, in or-
der to improve the building of such lists, we plan
to annotate light verb constructions in a subcorpus
of SzegedParalell, a Hungarian-English manually
aligned parallel corpus (To?th et al, 2008).
6.2 Light verb constructions and
information extraction
Information extraction (IE) seeks to process large
amounts of unstructured text, in other words, to
collect relevant items of information and to clas-
sify them. Even though humans usually overper-
form computers in complex information process-
ing tasks, computers also have some obvious ad-
vantages due to their capacity of processing and
their precision in performing well-defined tasks.
For several IE applications (e.g. relationship
extraction) it is essential to identify phrases in
a clause and to determine their grammatical role
(subject, object, verb) as well. This can be carried
out by a syntactic parser and is a relatively sim-
ple task. However, the identification of the syn-
tactic status of the nominal component is more
complex in the case of light verb constructions
for it is a quasi-argument of the verb not to be
confused with other arguments (Alonso Ramos,
1998). Thus, the parser should recognize the spe-
cial status of the quasi-argument and treat it in a
specific way as in the following sentences, one of
which contains a light verb construction while the
other one a verbal counterpart of the construction:
Pete made a decision on his future.
Pete decided on his future.
relation descriptions (up to the present, only fractions of the
dictionary have been completed for Russian (Mel?c?uk and
Z?olkovskij, 1984) and for French (see Mel?c?uk et al (1984
1999)), besides, trial entries have been written in Polish, En-
glish and German that contain the relations of a certain lexi-
cal unit to other lexemes given by means of lexical functions
(see e.g. Mel?c?uk et al (1995)). These dictionaries indicate
light verb constructions within the entry of the nominal com-
ponent.
1115
In the sentence with the verbal counterpart, the
event of deciding involves two arguments: he and
his future. In the sentence with the light verb con-
struction, the same arguments can be found, how-
ever, it is unresolved whether they are the argu-
ments of the verb (made) or the nominal compo-
nent (decision). If a precise syntactic analysis is
needed, it is crucial to know which argument be-
longs to which governor. Nevertheless, it is still
debated if syntactic arguments should be divided
between the nominal component and the verb (see
Meyers et al (2004a) on argument sharing) and if
yes, how (Alonso Ramos, 2007).
For the purpose of information extraction, such
a detailed analysis is unnecessary and in general
terms, the nominal component can be seen as part
of the verb, that is, they form a complex verb sim-
ilarly to phrasal or prepositional verbs and this
complex verb is considered to be the governor
of arguments. Thus, the following data can be
yielded by the IE algorithm: there is an event
of decision-making, Pete is its subject and it is
about his future (and not an event of making
with the arguments decision, Pete and his fu-
ture). Again, the precise identification of light
verb constructions can highly improve the perfor-
mance of parsers in recognizing relations between
the complex verb and its arguments.
7 Conclusion
In this paper, we have presented the development
of a corpus of Hungarian light verb constructions.
Basic annotation guidelines and statistical data
have also been included. The annotated corpus
can serve as a training database for implementing
an algorithm that aims at identifying light verb
constructions. Several NLP applications in the
fields of e.g. machine translation and information
extraction may profit from the successful integra-
tion of such an algorithm into the system, which
we plan to develop in the near future.
Acknowledgements
This work was supported in part by the National
Office for Research and Technology of the Hun-
garian government within the framework of the
project MASZEKER.
The authors wish to thank Gyo?rgy Szarvas for
his help in developing the annotation tool and
Richa?rd Farkas for his valuable comments on an
earlier draft of this paper.
References
Alonso Ramos, Margarita. 1998. Etude se?mantico-
syntaxique des constructions a` verbe support. Ph.D.
thesis, Universite? de Montre?al, Montreal, Canada.
Alonso Ramos, Margarita. 2007. Towards the Syn-
thesis of Support Verb Constructions. In Wanner,
Leo, editor, Selected Lexical and Grammatical Is-
sues in the Meaning-Text Theory. In Honour of Igor
Mel?c?uk, pages 97?138, Amsterdam / Philadelphia.
Benjamins.
B. Kova?cs, Ma?ria. 1999. A funkcio?ige?s szerkezetek
a jogi szaknyelvben [Light verb constructions in the
legal terminology]. Magyar Nyelvo?r, 123(4):388?
394.
Cinkova?, Silvie and Veronika Kola?r?ova?. 2005. Nouns
as Components of Support Verb Constructions in the
Prague Dependency Treebank. In S?imkova?, Ma?ria,
editor, Insight into Slovak and Czech Corpus Lin-
guistics, pages 113?139. Veda Bratislava, Slovakia.
Csendes, Do?ra, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged TreeBank.
In Matousek, Va?clav, Pavel Mautner, and Toma?s
Pavelka, editors, Proceedings of the 8th Interna-
tional Conference on Text, Speech and Dialogue,
TSD 2005, Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg, September.
Springer.
Diab, Mona and Pravin Bhutada. 2009. Verb Noun
Construction MWE Token Classification. In Pro-
ceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambigua-
tion and Applications, pages 17?22, Singapore, Au-
gust. Association for Computational Linguistics.
Dura, Elz?bieta and Barbara Gawron?ska. 2005. To-
wards Automatic Translation of Support Verbs Con-
structions: the Case of Polish robic/zrobic and
Swedish go?ra. In Proceedings of the 2nd Language
& Technology Conference, pages 450?454, Poznan?,
Poland, April. Wydawnictwo Poznan?skie Sp. z o.o.
Fazly, Afsaneh and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
1116
Gre?goire, Nicole. 2007. Design and Implemen-
tation of a Lexicon of Dutch Multiword Expres-
sions. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 17?
24, Prague, Czech Republic, June. Association for
Computational Linguistics.
Kaalep, Heiki-Jaan and Kadri Muischnek. 2006.
Multi-Word Verbs in a Flective Language: The Case
of Estonian. In Proceedings of the EACL Workshop
on Multi-Word Expressions in a Multilingual Con-
texts, pages 57?64, Trento, Italy, April. Association
for Computational Linguistics.
Kaalep, Heiki-Jaan and Kadri Muischnek. 2008.
Multi-Word Verbs of Estonian: a Database and a
Corpus. In Proceedings of the LREC Workshop
Towards a Shared Task for Multiword Expressions
(MWE 2008), pages 23?26, Marrakech, Morocco,
June.
Krenn, Brigitte. 2008. Description of Evaluation Re-
source ? German PP-verb data. In Proceedings
of the LREC Workshop Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 7?10,
Marrakech, Morocco, June.
Langer, Stefan. 2004. A Linguistic Test Battery for
Support Verb Constructions. Lingvisticae Investi-
gationes, 27(2):171?184.
Mel?c?uk, Igor and Aleksander Z?olkovskij. 1984.
Explanatory Combinatorial Dictionary of Modern
Russian. Wiener Slawistischer Almanach, Vienna,
Austria.
Mel?c?uk, Igor, Andre? Clas, and Alain Polgue`re. 1995.
Introduction a` lexicologie explicative et combina-
toire. Duculot, Louvain-la-Neuve, France.
Mel?c?uk, Igor, et al 1984?1999. Dictionnaire ex-
plicatif et combinatoire du franc?ais contemporain:
Recherches lexico-se?mantiques I?IV. Presses de
l?Universite? de Montre?al, Montreal, Canada.
Meyers, Adam, Ruth Reeves, and Catherine Macleod.
2004a. NP-External Arguments: A Study of Argu-
ment Sharing in English. In Tanaka, Takaaki, Aline
Villavicencio, Francis Bond, and Anna Korhonen,
editors, Second ACL Workshop on Multiword Ex-
pressions: Integrating Processing, pages 96?103,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004b. The NomBank
Project: An Interim Report. In Meyers, Adam,
editor, HLT-NAACL 2004 Workshop: Frontiers in
Corpus Annotation, pages 24?31, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Oravecz, Csaba, Ka?roly Varasdi, and Viktor Nagy.
2004. To?bbszavas kifejeze?sek sza?m??to?ge?pes
kezele?se [The treatment of multiword expressions
in computational linguistics]. In Alexin, Zolta?n
and Do?ra Csendes, editors, MSzNy 2004 ? II. Ma-
gyar Sza?m??to?ge?pes Nyelve?szeti Konferencia, pages
141?154, Szeged, Hungary, December. University
of Szeged.
Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002, pages 1?15, Mexico City,
Mexico.
Siepmann, Dirk. 2005. Collocation, colligation and
encoding dictionaries. Part I: Lexicological Aspects.
International Journal of Lexicography, 18(4):409?
444.
Siepmann, Dirk. 2006. Collocation, colligation
and encoding dictionaries. Part II: Lexicographical
Aspects. International Journal of Lexicography,
19(1):1?39.
Stevenson, Suzanne, Afsaneh Fazly, and Ryan North.
2004. Statistical Measures of the Semi-Productivity
of Light Verb Constructions. In Tanaka, Takaaki,
Aline Villavicencio, Francis Bond, and Anna Ko-
rhonen, editors, Second ACL Workshop on Multi-
word Expressions: Integrating Processing, pages 1?
8, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Tan, Yee Fan, Min-Yen Kan, and Hang Cui. 2006.
Extending corpus-based identification of light verb
constructions using a supervised learning frame-
work. In Proceedings of the EACL Workshop on
Multi-Word Expressions in a Multilingual Contexts,
pages 49?56, Trento, Italy, April. Association for
Computational Linguistics.
To?th, Krisztina, Richa?rd Farkas, and Andra?s Kocsor.
2008. Hybrid algorithm for sentence alignment of
Hungarian-English parallel corpora. Acta Cyber-
netica, 18(3):463?478.
Tro?n, Viktor, Gyo?rgy Gyepesi, Pe?ter Hala?csy, Andra?s
Kornai, La?szlo? Ne?meth, and Da?niel Varga. 2005.
hunmorph: Open Source Word Analysis. In Pro-
ceedings of the ACL Workshop on Software, pages
77?85, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Va?radi, Tama?s. 2006. Multiword Units in an MT
Lexicon. In Proceedings of the EACL Workshop on
Multi-Word Expressions in a Multilingual Contexts,
pages 73?78, Trento, Italy, April. Association for
Computational Linguistics.
1117
Vincze, Veronika, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010.
Hungarian Dependency Treebank. In Calzolari,
Nicoletta, Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odjik, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
May. European Language Resources Association
(ELRA).
Vincze, Veronika. 2008. A puszta ko?zne?v + ige kom-
plexumok sta?tusa?ro?l [On the status of bare common
noun + verb constructions]. In Sinkovics, Bala?zs,
editor, LingDok 7. Nyelve?sz-doktoranduszok dolgo-
zatai, pages 265?283, Szeged, Hungary. University
of Szeged.
Vincze, Veronika. 2009. Fo?ne?v + ige szerkezetek a
szo?ta?rban [Noun + verb constructions in the dictio-
nary]. In Va?radi, Tama?s, editor, III. Alkalmazott
Nyelve?szeti Doktorandusz Konferencia, pages 180?
188, Budapest. MTA Nyelvtudoma?nyi Inte?zet.
Zarrie?, Sina and Jonas Kuhn. 2009. Exploit-
ing Translational Correspondences for Pattern-
Independent MWE Identification. In Proceedings
of the Workshop on Multiword Expressions: Identi-
fication, Interpretation, Disambiguation and Appli-
cations, pages 23?30, Singapore, August. Associa-
tion for Computational Linguistics.
1118
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text
Richa?rd Farkas1,2, Veronika Vincze1, Gyo?rgy Mo?ra1, Ja?nos Csirik1,2, Gyo?rgy Szarvas3
1 University of Szeged, Department of Informatics
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
3 Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing Lab
{rfarkas,vinczev,gymora,csirik}@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de
Abstract
The CoNLL-2010 Shared Task was dedi-
cated to the detection of uncertainty cues
and their linguistic scope in natural lan-
guage texts. The motivation behind this
task was that distinguishing factual and
uncertain information in texts is of essen-
tial importance in information extraction.
This paper provides a general overview
of the shared task, including the annota-
tion protocols of the training and evalua-
tion datasets, the exact task definitions, the
evaluation metrics employed and the over-
all results. The paper concludes with an
analysis of the prominent approaches and
an overview of the systems submitted to
the shared task.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
provides a competitive shared task for the Com-
putational Linguistics community. After a five-
year period of multi-language semantic role label-
ing and syntactic dependency parsing tasks, a new
task was introduced in 2010, namely the detection
of uncertainty and its linguistic scope in natural
language sentences.
In natural language processing (NLP) ? and in
particular, in information extraction (IE) ? many
applications seek to extract factual information
from text. In order to distinguish facts from unre-
liable or uncertain information, linguistic devices
such as hedges (indicating that authors do not
or cannot back up their opinions/statements with
facts) have to be identified. Applications should
handle detected speculative parts in a different
manner. A typical example is protein-protein in-
teraction extraction from biological texts, where
the aim is to mine text evidence for biological enti-
ties that are in a particular relation with each other.
Here, while an uncertain relation might be of some
interest for an end-user as well, such information
must not be confused with factual textual evidence
(reliable information).
Uncertainty detection has two levels. Auto-
matic hedge detectors might attempt to identify
sentences which contain uncertain information
and handle whole sentences in a different man-
ner or they might attempt to recognize in-sentence
spans which are speculative. In-sentence uncer-
tainty detection is a more complicated task com-
pared to the sentence-level one, but it has bene-
fits for NLP applications as there may be spans
containing useful factual information in a sentence
that otherwise contains uncertain parts. For ex-
ample, in the following sentence the subordinated
clause starting with although contains factual in-
formation while uncertain information is included
in the main clause and the embedded question.
Although IL-1 has been reported to con-
tribute to Th17 differentiation in mouse
and man, it remains to be determined
{whether therapeutic targeting of IL-1
will substantially affect IL-17 in RA}.
Both tasks were addressed in the CoNLL-2010
Shared Task, in order to provide uniform manu-
ally annotated benchmark datasets for both and to
compare their difficulties and state-of-the-art so-
lutions for them. The uncertainty detection prob-
lem consists of two stages. First, keywords/cues
indicating uncertainty should be recognized then
either a sentence-level decision is made or the lin-
guistic scope of the cue words has to be identified.
The latter task falls within the scope of semantic
analysis of sentences exploiting syntactic patterns,
as hedge spans can usually be determined on the
basis of syntactic patterns dependent on the key-
word.
1
2 Related Work
The term hedging was originally introduced by
Lakoff (1972). However, hedge detection has re-
ceived considerable interest just recently in the
NLP community. Light et al (2004) used a hand-
crafted list of hedge cues to identify specula-
tive sentences in MEDLINE abstracts and several
biomedical NLP applications incorporate rules for
identifying the certainty of extracted information
(Friedman et al, 1994; Chapman et al, 2007; Ara-
maki et al, 2009; Conway et al, 2009).
The most recent approaches to uncertainty de-
tection exploit machine learning models that uti-
lize manually labeled corpora. Medlock and
Briscoe (2007) used single words as input features
in order to classify sentences from biological ar-
ticles (FlyBase) as speculative or non-speculative
based on semi-automatically collected training ex-
amples. Szarvas (2008) extended the methodology
of Medlock and Briscoe (2007) to use n-gram fea-
tures and a semi-supervised selection of the key-
word features. Kilicoglu and Bergler (2008) pro-
posed a linguistically motivated approach based
on syntactic information to semi-automatically re-
fine a list of hedge cues. Ganter and Strube (2009)
proposed an approach for the automatic detec-
tion of sentences containing uncertainty based on
Wikipedia weasel tags and syntactic patterns.
The BioScope corpus (Vincze et al, 2008) is
manually annotated with negation and specula-
tion cues and their linguistic scope. It consists
of clinical free-texts, biological texts from full pa-
pers and scientific abstracts. Using BioScope for
training and evaluation, Morante and Daelemans
(2009) developed a scope detector following a su-
pervised sequence labeling approach while O?zgu?r
and Radev (2009) developed a rule-based system
that exploits syntactic patterns.
Several related works have also been published
within the framework of The BioNLP?09 Shared
Task on Event Extraction (Kim et al, 2009), where
a separate subtask was dedicated to predicting
whether the recognized biological events are un-
der negation or speculation, based on the GENIA
event corpus annotations (Kilicoglu and Bergler,
2009; Van Landeghem et al, 2009).
3 Uncertainty Annotation Guidelines
The shared task addressed the detection of uncer-
tainty in two domains. As uncertainty detection
is extremely important for biomedical information
extraction and most existing approaches have tar-
geted such applications, participants were asked
to develop systems for hedge detection in bio-
logical scientific articles. Uncertainty detection
is also important, e.g. in encyclopedias, where
the goal is to collect reliable world knowledge
about real-world concepts and topics. For exam-
ple, Wikipedia explicitly declares that statements
reflecting author opinions or those not backed up
by facts (e.g. references) should be avoided (see
3.2 for details). Thus, the community-edited en-
cyclopedia, Wikipedia became one of the subjects
of the shared task as well.
3.1 Hedges in Biological Scientific Articles
In the biomedical domain, sentences were manu-
ally annotated for both hedge cues and their lin-
guistic scope. Hedging is typically expressed by
using specific linguistic devices (which we refer to
as cues in this article) that modify the meaning or
reflect the author?s attitude towards the content of
the text. Typical hedge cues fall into the following
categories:
? auxiliaries: may, might, can, would, should,
could, etc.
? verbs of hedging or verbs with speculative
content: suggest, question, presume, suspect,
indicate, suppose, seem, appear, favor, etc.
? adjectives or adverbs: probable, likely, possi-
ble, unsure, etc.
? conjunctions: or, and/or, either . . . or, etc.
However, there are some cases where a hedge is
expressed via a phrase rather than a single word.
Complex keywords are phrases that express un-
certainty together, but not on their own (either the
semantic interpretation or the hedging strength of
its subcomponents are significantly different from
those of the whole phrase). An instance of a com-
plex keyword can be seen in the following sen-
tence:
Mild bladder wall thickening {raises
the question of cystitis}.
The expression raises the question of may be sub-
stituted by suggests and neither the verb raises nor
the noun question convey speculative meaning on
their own. However, the whole phrase is specula-
tive therefore it is marked as a hedge cue.
2
During the annotation process, a min-max strat-
egy for the marking of keywords (min) and their
scope (max) was followed. On the one hand, when
marking the keywords, the minimal unit that ex-
presses hedging and determines the actual strength
of hedging was marked as a keyword. On the other
hand, when marking the scopes of speculative key-
words, the scope was extended to the largest syn-
tactic unit possible. That is, all constituents that
fell within the uncertain interpretation were in-
cluded in the scope. Our motivation here was that
in this way, if we simply disregard the marked text
span, the rest of the sentence can usually be used
for extracting factual information (if there is any).
For instance, in the example above, we can be sure
that the symptom mild bladder wall thickening is
exhibited by the patient but a diagnosis of cystitis
would be questionable.
The scope of a speculative element can be de-
termined on the basis of syntax. The scopes of
the BioScope corpus are regarded as consecutive
text spans and their annotation was based on con-
stituency grammar. The scope of verbs, auxil-
iaries, adjectives and adverbs usually starts right
with the keyword. In the case of verbal elements,
i.e. verbs and auxiliaries, it ends at the end of the
clause or sentence, thus all complements and ad-
juncts are included. The scope of attributive ad-
jectives generally extends to the following noun
phrase, whereas the scope of predicative adjec-
tives includes the whole sentence. Sentential ad-
verbs have a scope over the entire sentence, while
the scope of other adverbs usually ends at the end
of the clause or sentence. Conjunctions generally
have a scope over the syntactic unit whose mem-
bers they coordinate. Some linguistic phenomena
(e.g. passive voice or raising) can change scope
boundaries in the sentence, thus they were given
special attention during the annotation phase.
3.2 Wikipedia Weasels
The chief editors of Wikipedia have drawn the at-
tention of the public to uncertainty issues they call
weasel1. A word is considered to be a weasel
word if it creates an impression that something im-
portant has been said, but what is really commu-
nicated is vague, misleading, evasive or ambigu-
ous. Weasel words do not give a neutral account
of facts, rather, they offer an opinion without any
1http://en.wikipedia.org/wiki/Weasel_
word
backup or source. The following sentence does
not specify the source of information, it is just the
vague term some people that refers to the holder of
this opinion:
Some people claim that this results in a
better taste than that of other diet colas
(most of which are sweetened with as-
partame alone).
Statements with weasel words usually evoke ques-
tions such as Who says that?, Whose opinion is
this? and How many people think so?.
Typical instances of weasels can be grouped in
the following way (we offer some examples as
well):
? Adjectives and adverbs
? elements referring to uncertainty: prob-
able, likely, possible, unsure, often, pos-
sibly, allegedly, apparently, perhaps,
etc.
? elements denoting generalization:
widely, traditionally, generally, broadly-
accepted, widespread, etc.
? qualifiers and superlatives: global, su-
perior, excellent, immensely, legendary,
best, (one of the) largest, most promi-
nent, etc.
? elements expressing obviousness:
clearly, obviously, arguably, etc.
? Auxiliaries
? may, might, would, should, etc.
? Verbs
? verbs with speculative content and their
passive forms: suggest, question, pre-
sume, suspect, indicate, suppose, seem,
appear, favor, etc.
? passive forms with dummy subjects: It
is claimed that . . . It has been men-
tioned . . . It is known . . .
? there is / there are constructions: There
is evidence/concern/indication that. . .
? Numerically vague expressions / quantifiers
? certain, numerous, many, most, some,
much, everyone, few, various, one group
of, etc. Experts say . . . Some people
think . . .More than 60% percent . . .
3
? Nouns
? speculation, proposal, consideration,
etc. Rumour has it that . . . Common
sense insists that . . .
However, the use of the above words or grammat-
ical devices does not necessarily entail their being
a weasel cue since their use may be justifiable in
their contexts.
As the main application goal of weasel detec-
tion is to highlight articles which should be im-
proved (by reformulating or adding factual is-
sues), we decided to annotate only weasel cues
in Wikipedia articles, but we did not mark their
scopes.
During the manual annotation process, the fol-
lowing cue marking principles were employed.
Complex verb phrases were annotated as weasel
cues since in some cases, both the passive con-
struction and the verb itself are responsible for the
weasel. In passive forms with dummy subjects and
there is / there are constructions, the weasel cue
included the grammatical subject (i.e. it and there)
as well. As for numerically vague expressions, the
noun phrase containing a quantifier was marked
as a weasel cue. If there was no quantifier (in the
case of a bare plural), the noun was annotated as
a weasel cue. Comparatives and superlatives were
annotated together with their article. Anaphoric
pronouns referring to a weasel word were also an-
notated as weasel cues.
4 Task Definitions
Two uncertainty detection tasks (sentence clas-
sification and in-sentence hedge scope detec-
tion) in two domains (biological publications and
Wikipedia articles) with three types of submis-
sions (closed, cross and open) were given to the
participants of the CoNLL-2010 Shared Task.
4.1 Detection of Uncertain Sentences
The aim of Task1 was to develop automatic proce-
dures for identifying sentences in texts which con-
tain unreliable or uncertain information. In par-
ticular, this task is a binary classification problem,
i.e. factual and uncertain sentences have to be dis-
tinguished.
As training and evaluation data
? Task1B: biological abstracts and full articles
(evaluation data contained only full articles)
from the BioScope corpus and
? Task1W: paragraphs from Wikipedia possi-
bly containing weasel information
were provided. The annotation of weasel/hedge
cues was carried out on the phrase level, and sen-
tences containing at least one cue were considered
as uncertain, while sentences with no cues were
considered as factual. The participating systems
had to submit a binary classification (certain vs.
uncertain) of the test sentences while marking cues
in the submissions was voluntary (but participants
were encouraged to do this).
4.2 In-sentence Hedge Scope Resolution
For Task2, in-sentence scope resolvers had to be
developed. The training and evaluation data con-
sisted of biological scientific texts, in which in-
stances of speculative spans ? that is, keywords
and their linguistic scope ? were annotated manu-
ally. Submissions to Task2 were expected to auto-
matically annotate the cue phrases and the left and
right boundaries of their scopes (exactly one scope
must be assigned to a cue phrase).
4.3 Evaluation Metrics
The evaluation for Task1 was carried out at the
sentence level, i.e. the cue annotations in the sen-
tence were not taken into account. The F?=1 mea-
sure (the harmonic mean of precision and recall)
of the uncertain class was employed as the chief
evaluation metric.
The Task2 systems were expected to mark cue-
and corresponding scope begin/end tags linked to-
gether by using some unique IDs. A scope-level
F?=1 measure was used as the chief evaluation
metric where true positives were scopes which ex-
actly matched the gold standard cue phrases and
gold standard scope boundaries assigned to the cue
word. That is, correct scope boundaries with in-
correct cue annotation and correct cue words with
bad scope boundaries were both treated as errors.
This scope-level metric is very strict. For in-
stance, the requirement of the precise match of the
cue phrase is questionable as ? from an application
point of view ? the goal is to find uncertain text
spans and the evidence for this is not so impor-
tant. However, the annotation of cues in datasets
is essential for training scope detectors since lo-
cating the cues usually precedes the identification
of their scope. Hence we decided to incorporate
cue matches into the evaluation metric.
4
Another questionable issue is the strict bound-
ary matching requirement. For example, includ-
ing or excluding punctuations, citations or some
bracketed expressions, like (see Figure 1) from
a scope is not crucial for an otherwise accurate
scope detector. On the other hand, the list of
such ignorable phenomena is arguable, especially
across domains. Thus, we considered the strict
boundary matching to be a straightforward and un-
ambiguous evaluation criterion. Minor issues like
those mentioned above could be handled by sim-
ple post-processing rules. In conclusion we think
that the uncertainty detection community may find
more flexible evaluation criteria in the future but
the strict scope-level metric is definitely a good
starting point for evaluation.
4.4 Closed and Open Challenges
Participants were invited to submit results in dif-
ferent configurations, where systems were allowed
to exploit different kinds of annotated resources.
The three possible submission categories were:
? Closed, where only the labeled and unla-
beled data provided for the shared task were
allowed, separately for each domain (i.e.
biomedical train data for biomedical test set
and Wikipedia train data for Wikipedia test
set). No further manually crafted resources
of uncertainty information (i.e. lists, anno-
tated data, etc.) could be used in any domain.
On the other hand, tools exploiting the man-
ual annotation of linguistic phenomena not
related to uncertainty (such as POS taggers
and parsers trained on labeled corpora) were
allowed.
? Cross-domain was the same as the closed one
but all data provided for the shared task were
allowed for both domains (i.e. Wikipedia
train data for the biomedical test set, the
biomedical train data for Wikipedia test set
or a union of Wikipedia and biomedical train
data for both test sets).
? Open, where any data and/or any additional
manually created information and resource
(which may be related to uncertainty) were
allowed for both domains.
The motivation behind the cross-domain and the
open challenges was that in this way, we could
assess whether adding extra (i.e. not domain-
specific) information to the systems can contribute
to the overall performance.
5 Datasets
Training and evaluation corpora were annotated
manually for hedge/weasel cues and their scope
by two independent linguist annotators. Any dif-
ferences between the two annotations were later
resolved by the chief annotator, who was also re-
sponsible for creating the annotation guidelines
and training the two annotators. The datasets
are freely available2 for further benchmark experi-
ments at http://www.inf.u-szeged.hu/
rgai/conll2010st.
Since uncertainty cues play an important role
in detecting sentences containing uncertainty, they
are tagged in the Task1 datasets as well to enhance
training and evaluation of systems.
5.1 Biological Publications
The biological training dataset consisted of the bi-
ological part of the BioScope corpus (Vincze et al,
2008), hence it included abstracts from the GE-
NIA corpus, 5 full articles from the functional ge-
nomics literature (related to the fruit fly) and 4 ar-
ticles from the open access BMC Bioinformatics
website. The automatic segmentation of the doc-
uments was corrected manually and the sentences
(14541 in number) were annotated manually for
hedge cues and their scopes.
The evaluation dataset was based on 15 biomed-
ical articles downloaded from the publicly avail-
able PubMedCentral database, including 5 ran-
dom articles taken from the BMC Bioinformat-
ics journal in October 2009, 5 random articles to
which the drosophila MeSH term was assigned
and 5 random articles having the MeSH terms
human, blood cells and transcription factor (the
same terms which were used to create the Genia
corpus). These latter ten articles were also pub-
lished in 2009. The aim of this article selection
procedure was to have a theme that was close to
the training corpus. The evaluation set contained
5003 sentences, out of which 790 were uncertain.
These texts were manually annotated for hedge
cues and their scope. To annotate the training and
the evaluation datasets, the same annotation prin-
ciples were applied.
2under the Creative Commons Attribute Share Alike li-
cense
5
For both Task1 and Task2, the same dataset was
provided, the difference being that for Task1, only
hedge cues and sentence-level uncertainty were
given, however, for Task2, hedge cues and their
scope were marked in the text.
5.2 Wikipedia Datasets
2186 paragraphs collected from Wikipedia
archives were also offered as Task1 training
data (11111 sentences containing 2484 uncertain
ones). The evaluation dataset contained 2346
Wikipedia paragraphs with 9634 sentences, out of
which 2234 were uncertain.
For the selection of the Wikipedia paragraphs
used to construct the training and evaluation
datasets, we exploited the weasel tags added by
the editors of the encyclopedia (marking unsup-
ported opinions or expressions of a non-neutral
point of view). Each paragraph containing weasel
tags (5874 different ones) was extracted from the
history dump of EnglishWikipedia. First, 438 ran-
domly selected paragraphs were manually anno-
tated from this pool then the most frequent cue
phrases were collected. Later on, two other sets
of Wikipedia paragraphs were gathered on the ba-
sis of whether they contained such cue phrases or
not. The aim of this sampling procedure was to
provide large enough training and evaluation sam-
ples containing weasel words and also occurrences
of typical weasel words in non-weasel contexts.
Each sentence was annotated manually for
weasel cues. Sentences were treated as uncer-
tain if they contained at least one weasel cue, i.e.
the scope of weasel words was the entire sentence
(which is supposed to be rewritten by Wikipedia
editors).
5.3 Unlabeled Data
Unannotated but pre-processed full biological arti-
cles (150 articles from the publicly available Pub-
MedCentral database) and 1 million paragraphs
from Wikipedia were offered to the participants as
well. These datasets did not contain any manual
annotation for uncertainty, but their usage permit-
ted data sampling from a large pool of in-domain
texts without time-wasting pre-processing tasks
(cleaning and sentence splitting).
5.4 Data Format
Both training and evaluation data were released
in a custom XML format. For each task, a sep-
arate XML file was made available containing the
whole document set for the given task. Evaluation
datasets were available in the same format as train-
ing data without any sentence-level certainty, cue
or scope annotations.
The XML format enabled us to provide more
detailed information about the documents such as
segment boundaries and types (e.g. section titles,
figure captions) and it is the straightforward for-
mat to represent nested scopes. Nested scopes
have overlapping text spans which may contain
cues for multiple scopes (there were 1058 occur-
rences in the training and evaluation datasets to-
gether). The XML format utilizes id-references
to determine the scope of a given cue. Nested
constructions are rather complicated to represent
in the standard IOB format, moreover, we did not
want to enforce a uniform tokenization.
To support the processing of the data files,
reader and writer software modules were devel-
oped and offered to the participants for the uCom-
pare (Kano et al, 2009) framework. uCompare
provides a universal interface (UIMA) and several
text mining and natural language processing tools
(tokenizers, POS taggers, syntactic parsers, etc.)
for general and biological domains. In this way
participants could configure and execute a flexible
chain of analyzing tools even with a graphical UI.
6 Submissions and Results
Participants uploaded their results through the
shared task website, and the official evaluation was
performed centrally. After the evaluation period,
the results were published for the participants on
the Web. A total of 23 teams participated in the
shared task. 22, 16 and 13 teams submitted output
for Task1B, Task1W and Task2, respectively.
6.1 Results
Tables 1, 2 and 3 contain the results of the submit-
ted systems for Task1 and Task2. The last name
of the first author of the system description pa-
per (published in these proceedings) is used here
as a system name3. The last column contains the
type of submission. The system of Kilicoglu and
Bergler (2010) is the only open submission. They
adapted their system introduced in Kilicoglu and
Bergler (2008) to the datasets of the shared task.
Regarding cross submissions, Zhao et al (2010)
and Ji et al (2010) managed to achieve a no-
ticeable improvement by exploiting cross-domain
3O?zgu?r did not publish a description of her system.
6
Name P / R / F type
Georgescul 72.0 / 51.7 / 60.2 C
Ji 62.7 / 55.3 / 58.7 X
Chen 68.0 / 49.7 / 57.4 C
Morante 80.6 / 44.5 / 57.3 C
Zhang 76.6 / 44.4 / 56.2 C
Zheng 76.3 / 43.6 / 55.5 C
Ta?ckstro?m 78.3 / 42.8 / 55.4 C
Mamani Sa?nchez 68.3 / 46.2 / 55.1 C
Tang 82.3 / 41.4 / 55.0 C
Kilicoglu 67.9 / 46.0 / 54.9 O
Tjong Kim Sang 74.0 / 43.0 / 54.4 C
Clausen 75.1 / 42.0 / 53.9 C
O?zgu?r 59.4 / 47.9 / 53.1 C
Zhou 85.3 / 36.5 / 51.1 C
Li 88.4 / 31.9 / 46.9 C
Prabhakaran 88.0 / 28.4 / 43.0 C
Ji 94.2 / 6.6 / 12.3 C
Table 1: Task1 Wikipedia results (type ?
{Closed(C), Cross(X), Open(O)}).
data. Zhao et al (2010) extended the biological
cue word dictionary of their system ? using it as
a feature for classification ? by the frequent cues
of the Wikipedia dataset, while Ji et al (2010)
used the union of the two datasets for training
(they have reported an improvement from 47.0 to
58.7 on the Wikipedia evaluation set after a post-
challenge bugfix).
Name P / R / F type
Morante 59.6 / 55.2 / 57.3 C
Rei 56.7 / 54.6 / 55.6 C
Velldal 56.7 / 54.0 / 55.3 C
Kilicoglu 62.5 / 49.5 / 55.2 O
Li 57.4 / 47.9 / 52.2 C
Zhou 45.6 / 43.9 / 44.7 O
Zhou 45.3 / 43.6 / 44.4 C
Zhang 46.0 / 42.9 / 44.4 C
Fernandes 46.0 / 38.0 / 41.6 C
Vlachos 41.2 / 35.9 / 38.4 C
Zhao 34.8 / 41.0 / 37.7 C
Tang 34.5 / 31.8 / 33.1 C
Ji 21.9 / 17.2 / 19.3 C
Ta?ckstro?m 2.3 / 2.0 / 2.1 C
Table 2: Task2 results (type ? {Closed(C),
Open(O)}).
Each Task2 and Task1W system achieved a
Name P / R / F type
Tang 85.0 / 87.7 / 86.4 C
Zhou 86.5 / 85.1 / 85.8 C
Li 90.4 / 81.0 / 85.4 C
Velldal 85.5 / 84.9 / 85.2 C
Vlachos 85.5 / 84.9 / 85.2 C
Ta?ckstro?m 87.1 / 83.4 / 85.2 C
Shimizu 88.1 / 82.3 / 85.1 C
Zhao 83.4 / 84.8 / 84.1 X
O?zgu?r 77.8 / 91.3 / 84.0 C
Rei 83.8 / 84.2 / 84.0 C
Zhang 82.6 / 84.7 / 83.6 C
Kilicoglu 92.1 / 74.9 / 82.6 O
Morante 80.5 / 83.3 / 81.9 X
Morante 81.1 / 82.3 / 81.7 C
Zheng 73.3 / 90.8 / 81.1 C
Tjong Kim Sang 74.3 / 87.1 / 80.2 C
Clausen 79.3 / 80.6 / 80.0 C
Szidarovszky 70.3 / 91.0 / 79.3 C
Georgescul 69.1 / 91.0 / 78.5 C
Zhao 71.0 / 86.6 / 78.0 C
Ji 79.4 / 76.3 / 77.9 C
Chen 74.9 / 79.1 / 76.9 C
Fernandes 70.1 / 71.1 / 70.6 C
Prabhakaran 67.5 / 19.5 / 30.3 X
Table 3: Task1 biological results (type ?
{Closed(C), Cross(X), Open(O)}).
higher precision than recall. There may be two
reasons for this. The systems may have applied
only reliable patterns, or patterns occurring in the
evaluation set may be imperfectly covered by the
training datasets. The most intense participation
was on Task1B. Here, participants applied vari-
ous precision/recall trade-off strategies. For in-
stance, Tang et al (2010) achieved a balanced pre-
cision/recall configuration, while Li et al (2010)
achieved third place thanks to their superior preci-
sion.
Tables 4 and 5 show the cue-level performances,
i.e. the F-measure of cue phrase matching where
true positives were strict matches. Note that it was
optional to submit cue annotations for Task1 (if
participants submitted systems for both Task2 and
Task1B with cue tagging, only the better score of
the two was considered).
It is interesting to see that Morante et al (2010)
who obtained the best results on Task2 achieved
a medium-ranked F-measure on the cue-level (e.g.
their result on the cue-level is lower by 4% com-
7
pared to Zhou et al (2010), while on the scope-
level the difference is 13% in the reverse direc-
tion), which indicates that the real strength of the
system of Morante et al (2010) is the accurate de-
tection of scope boundaries.
Name P / R / F
Tang 63.0 / 25.7 / 36.5
Li 76.1 / 21.6 / 33.7
O?zgu?r 28.9 / 14.7 / 19.5
Morante 24.6 / 7.3 / 11.3
Table 4: Wikipedia cue-level results.
Name P / R / F type
Tang 81.7 / 81.0 / 81.3 C
Zhou 83.1 / 78.8 / 80.9 C
Li 87.4 / 73.4 / 79.8 C
Rei 81.4 / 77.4 / 79.3 C
Velldal 81.2 / 76.3 / 78.7 C
Zhang 82.1 / 75.3 / 78.5 C
Ji 78.7 / 76.2 / 77.4 C
Morante 78.8 / 74.7 / 76.7 C
Kilicoglu 86.5 / 67.7 / 76.0 O
Vlachos 82.0 / 70.6 / 75.9 C
Zhao 76.7 / 73.9 / 75.3 X
Fernandes 79.2 / 64.7 / 71.2 C
Zhao 63.7 / 74.1 / 68.5 C
Ta?ckstro?m 66.9 / 58.6 / 62.5 C
O?zgu?r 49.1 / 57.8 / 53.1 C
Table 5: Biological cue-level results (type ?
{Closed(C), Cross(X), Open(O)}).
6.2 Approaches
The approaches to Task1 fall into two major cat-
egories. There were six systems which handled
the task as a classical sentence classification prob-
lem and employed essentially a bag-of-words fea-
ture representation (they are marked as BoW in
Table 6). The remaining teams focused on the
cue phrases and sought to classify every token if
it was a part of a cue phrase, then a sentence was
predicted as uncertain if it contained at least one
recognized cue phrase. Five systems followed a
pure token classification approach (TC) for cue de-
tection while others used sequential labeling tech-
niques (usually Conditional Random Fields) to
identify cue phrases in sentences (SL).
The feature set employed in Task1 systems typ-
ically consisted of the wordform, its lemma or
stem, POS and chunk codes and about the half of
the participants constructed features from the de-
pendency and/or constituent parse tree of the sen-
tences as well (see Table 6 for details).
It is interesting to see that the top ranked sys-
tems of Task1B followed a sequence labeling ap-
proach, while the best systems on Task1W applied
a bag-of-words sentence classification. This may
be due to the fact that biological sentences have
relatively simple patterns. Thus the context of the
cue words (token classification-based approaches
used features derived from a window of the token
in question, thus, they exploited the relationship
among the tokens and their contexts) can be uti-
lized while Wikipedia weasels have a diverse na-
ture. Another observation is that the top systems
in both Task1B and Task1W are the ones which
did not derive features from syntactic parsing.
Each Task2 system was built upon a Task1 sys-
tem, i.e. they attempted to recognize the scopes
for the predicted cue phrases (however, Zhang et
al. (2010) have argued that the objective functions
of Task1 and Task2 cue detection problems are
different because of sentences containing multiple
hedge spans).
Most systems regarded multiple cues in a sen-
tence to be independent from each other and
formed different classification instances from
them. There were three systems which incor-
porated information about other hedge cues (e.g.
their distance) of the sentence into the feature
space and Zhang et al (2010) constructed a cas-
cade system which utilized directly the predicted
scopes (it processes cue phrases from left to right)
during predicting other scopes in the same sen-
tence.
The identification of the scope for a certain cue
was typically carried out by classifying each to-
ken in the sentence. Task2 systems differ in the
number of class labels used as target and in the
machine learning approaches applied. Most sys-
tems ? following Morante and Daelemans (2009)
? used three class labels (F)IRST, (L)AST and
NONE. Two participants used four classes by
adding (I)NSIDE, while three systems followed
a binary classification approach (SCOPE versus
NONSCOPE). The systems typically included a
post-processing procedure to force scopes to be
continuous and to include the cue phrase in ques-
tion. The machine learning methods applied can
be again categorized into sequence labeling (SL)
8
NA
ME
ap
pro
ach
ma
ch
ine
fea
tur
e
fea
tur
es
em
plo
ye
d
lea
rne
r
sel
ect
ion
dic
t
ort
ho
lem
ma
/st
em
PO
S
ch
un
k
de
p
do
cp
art
oth
er
Cl
au
sen
Bo
W
Ma
xE
nt
+
+
he
dg
ec
ue
dis
tan
ce
Ch
en
Bo
W
Ma
xE
nt
sta
tis
tic
al
+
+
+
+
sen
ten
cel
en
gth
Fe
rna
nd
es
SL
ET
L
+
+
+
Ge
org
esc
ul
Bo
W
SV
M+
pa
ram
tun
ing
+
Ji
TC
Mo
dA
vg
Pe
rce
ptr
on
+
Ki
lic
og
lu
TC
ma
nu
al
+
+
+
ex
ter
na
ld
ict
Li
SL
CR
F+
po
stp
roc
gre
ed
yf
wd
+
+
+
Ma
ma
ni
Sa?
nc
he
z
Bo
W
SV
MT
ree
Ke
rne
l
+
+
+
+
+
Mo
ran
te
(w
iki
)
TC
SV
M+
po
stp
roc
sta
tis
tic
al
+
+
+
+
+
Mo
ran
te
(bi
o)
SL
KN
N
sta
tis
tic
al
+
+
+
+
+
+
Pra
bh
ak
ara
n
SL
CR
F
gre
ed
yf
wd
+
+
+
+
Le
vin
Cl
ass
Re
i
SL
CR
F
+
+
+
+
Sh
im
izu
SL
Ba
ye
sP
oin
tM
ach
ine
s
GA
+
+
+
+
NE
s,u
nla
be
led
da
ta
Sz
ida
rov
szk
y
SL
CR
F
ex
ha
ust
ive
+
+
+
Ta?
ck
str
o?m
Bo
W
SV
M
gre
ed
yf
wd
+
+
+
+
+
sen
ten
cel
en
gth
Ta
ng
SL
CR
F,S
VM
HM
M
sta
tis
tic
al
+
+
+
+
+
Tjo
ng
Ki
m
Sa
ng
TC
Na
ive
Ba
ye
s
Ve
lld
al
TC
Ma
xE
nt
ma
nu
al
+
+
+
+
Vl
ach
os
TC
Ba
ye
sia
nL
og
Re
g
ma
nu
al
+
+
+
+
Zh
an
g
SL
CR
F+
fea
tur
ec
om
bin
ati
on
gre
ed
yf
wd
+
+
+
+
+
NE
s
Zh
ao
SL
CR
F
sta
tis
tic
al
+
+
+
+
Zh
en
g
SL
CR
F,M
ax
En
t
ma
nu
al
+
+
+
+
Co
nst
itu
en
tP
ars
ing
Zh
ou
SL
CR
F
sta
tis
tic
al
+
+
+
+
Wo
rdN
et
Ta
ble
6:
Sy
ste
m
arc
hit
ect
ure
so
ve
rvi
ew
for
Ta
sk1
.A
pp
roa
ch
es:
seq
ue
nc
el
ab
eli
ng
(SL
),t
ok
en
cla
ssi
fic
ati
on
(T
C)
,b
ag
-of
-w
ord
sm
od
el
(B
oW
);M
ach
ine
lea
rne
rs:
En
tro
py
Gu
ide
dT
ran
sfo
rm
ati
on
Le
arn
ing
(E
TL
),A
ve
rag
ed
Pe
rce
ptr
on
(A
P),
k-n
ear
est
ne
igh
bo
ur
(K
NN
);F
eat
ure
sel
ect
ion
:g
ath
eri
ng
ph
ras
es
fro
m
the
tra
ini
ng
co
rpu
su
sin
gs
tat
ist
ica
lth
res
ho
lds
(st
ati
sti
cal
);F
eat
ure
s:
ort
ho
gra
ph
ica
lin
for
ma
tio
na
bo
ut
the
tok
en
(or
tho
),l
em
ma
or
ste
m
of
the
tok
en
(st
em
),P
art
-of
-Sp
eec
h
co
de
s(
PO
S),
syn
tac
tic
ch
un
ki
nfo
rm
ati
on
(ch
un
k),
de
pe
nd
en
cy
pa
rsi
ng
(de
p),
po
sit
ion
ins
ide
the
do
cu
me
nt
or
sec
tio
ni
nfo
rm
ati
on
(do
cp
os)
an
dt
ok
en
cla
ssi
fic
ati
on
(T
C)
ap
pro
ach
es
(se
eT
ab
le
7).
Th
ef
eat
ure
set
su
sed
he
re
are
the
sam
ea
sf
or
Ta
sk1
,e
xte
nd
ed
by
sev
era
lf
eat
ure
sd
esc
rib
ing
the
rel
ati
on
shi
pb
etw
een
the
cu
ep
hra
se
an
dt
he
tok
en
in
qu
est
ion
mo
stl
yb
yd
esc
rib
ing
the
de
pe
nd
en
cy
pa
th
be
tw
een
the
m.
9
NAME approach scope ML postproc tree dep multihedge
Fernandes TC FL ETL
Ji TC I AP +
Kilicoglu HC manual + + +
Li SL FL CRF, SVMHMM + + +
Morante TC FL KNN + +
Rei SL FIL manual+CRF + +
Ta?ckstro?m TC FI SVM +
Tang SL FL CRF + + +
Velldal HC manual +
Vlachos TC I Bayesian MaxEnt + +
Zhang SL FIL CRF + +
Zhao SL FL CRF +
Zhou SL FL CRF + +
Table 7: System architectures overview for Task2. Approaches: sequence labeling (SL), token clas-
sification (TC), hand-crafted rules (HC); Machine learners: Entropy Guided Transformation Learning
(ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); The way of identifying scopes: predict-
ing first/last tokens (FL), first/inside/last tokens (FIL), just inside tokens (I); Multiple Hedges: the system
applied a mechanism for handling multiple hedges inside a sentence
and token classification (TC) approaches (see Ta-
ble 7). The feature sets used here are the same
as for Task1, extended by several features describ-
ing the relationship between the cue phrase and the
token in question mostly by describing the depen-
dency path between them.
7 Conclusions
The CoNLL-2010 Shared Task introduced the
novel task of uncertainty detection. The challenge
consisted of a sentence identification task on un-
certainty (Task1) and an in-sentence hedge scope
detection task (Task2). In the latter task the goal
of automatic systems was to recognize speculative
text spans inside sentences.
The relatively high number of participants in-
dicates that the problem is rather interesting for
the Natural Language Processing community. We
think that this is due to the practical importance
of the task for (principally biomedical) applica-
tions and because it addresses several open re-
search questions. Although several approaches
were introduced by the participants of the shared
task and we believe that the ideas described in
this proceedings can serve as an excellent starting
point for the development of an uncertainty de-
tector, there is a lot of room for improving such
systems. The manually annotated datasets and
software tools developed for the shared task may
act as benchmarks for these future experiments
(they are freely available at http://www.inf.
u-szeged.hu/rgai/conll2010st).
Acknowledgements
The authors would like to thank Joakim Nivre
and Llu??s Ma?rquez for their useful suggestions,
comments and help during the organisation of the
shared task.
This work was supported in part by the
National Office for Research and Technol-
ogy (NKTH, http://www.nkth.gov.hu/)
of the Hungarian government within the frame-
work of the projects TEXTREND, BELAMI and
MASZEKER.
References
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recogni-
tion and Modality Identification. In Proceedings of
the BioNLP 2009 Workshop, pages 185?192, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Wendy W. Chapman, David Chu, and John N. Dowl-
ing. 2007. ConText: An Algorithm for Identifying
Contextual Features from Clinical Text. In Proceed-
ings of the ACL Workshop on BioNLP 2007, pages
81?88.
Mike Conway, Son Doan, and Nigel Collier. 2009. Us-
ing Hedges to Enhance a Disease Outbreak Report
10
Text Mining System. In Proceedings of the BioNLP
2009 Workshop, pages 142?143, Boulder, Colorado,
June. Association for Computational Linguistics.
Carol Friedman, Philip O. Alderson, John H. M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A General Natural-language Text Processor
for Clinical Radiology. Journal of the American
Medical Informatics Association, 1(2):161?174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Feng Ji, Xipeng Qiu, and Xuanjing Huang. 2010. De-
tecting Hedge Cues and their Scopes with Average
Perceptron. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 139?146,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Yoshinobu Kano, William A. Baumgartner, Luke
McCrohon, Sophia Ananiadou, Kevin B. Cohen,
Lawrence Hunter, and Jun?ichi Tsujii. 2009. U-
Compare: Share and Compare Text Mining Tools
with UIMA. Bioinformatics, 25(15):1997?1998,
August.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing Speculative Language in Biomedical Research
Articles: A Linguistically Motivated Perspective.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
46?53, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syn-
tactic Dependency Based Heuristics for Biological
Event Extraction. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 119?127, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and Their
Scopes. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 103?110, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
George Lakoff. 1972. Linguistics and natural logic.
In The Semantics of Natural Language, pages 545?
665, Dordrecht. Reidel.
Xinxin Li, Jianping Shen, Xiang Gao, and Xuan
Wang. 2010. Exploiting Rich Features for Detect-
ing Hedges and Their Scope. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning (CoNLL-2010): Shared Task,
pages 36?41, Uppsala, Sweden, July. Association
for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Proceed-
ings of the HLT-NAACL 2004 Workshop: Biolink
2004, Linking Biological Literature, Ontologies and
Databases, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992?
999, Prague, Czech Republic, June.
Roser Morante andWalter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based Resolution of In-
sentence Scopes of Hedge Cues. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 48?55, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. De-
tecting Speculations and their Scopes in Scientific
Text. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1398?1407, Singapore, August. Associ-
ation for Computational Linguistics.
Gyo?rgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A Cascade Method for De-
tecting Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 25?29, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2009. Analyzing Text in
Search of Bio-molecular Events: A High-precision
Machine Learning Framework. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
11
Shared Task, pages 128?136, Boulder, Colorado,
June. Association for Computational Linguistics.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
Shaodian Zhang, Hai Zhao, Guodong Zhou, and Bao-
liang Lu. 2010. Hedge Detection and Scope Find-
ing by Sequence Labeling with Procedural Feature
Selection. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 70?77, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong
Cheng. 2010. Learning to Detect Hedges and their
Scope Using CRF. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 64?
69, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Huiwei Zhou, Xiaoyan Li, Degen Huang, Zezhong Li,
and Yuansheng Yang. 2010. Exploiting Multi-
Features to Detect Hedges and Their Scope in
Biomedical Texts. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 56?
63, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
12
