Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 512?520,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Streaming for large scale NLP: Language Modeling
Amit Goyal, Hal Daume? III, and Suresh Venkatasubramanian
University of Utah, School of Computing
{amitg,hal,suresh}@cs.utah.edu
Abstract
In this paper, we explore a streaming al-
gorithm paradigm to handle large amounts
of data for NLP problems. We present an
efficient low-memory method for construct-
ing high-order approximate n-gram frequency
counts. The method is based on a determinis-
tic streaming algorithm which efficiently com-
putes approximate frequency counts over a
stream of data while employing a small mem-
ory footprint. We show that this method eas-
ily scales to billion-word monolingual corpora
using a conventional (8 GB RAM) desktop
machine. Statistical machine translation ex-
perimental results corroborate that the result-
ing high-n approximate small language model
is as effective as models obtained from other
count pruning methods.
1 Introduction
In many NLP problems, we are faced with the chal-
lenge of dealing with large amounts of data. Many
problems boil down to computing relative frequen-
cies of certain items on this data. Items can be
words, patterns, associations, n-grams, and others.
Language modeling (Chen and Goodman, 1996),
noun-clustering (Ravichandran et al, 2005), con-
structing syntactic rules for SMT (Galley et al,
2004), and finding analogies (Turney, 2008) are
examples of some of the problems where we need
to compute relative frequencies. We use language
modeling as a canonical example of a large-scale
task that requires relative frequency estimation.
Computing relative frequencies seems like an
easy problem. However, as corpus sizes grow,
it becomes a highly computational expensive task.
Cutoff Size BLEU NIST MET
Exact 367.6m 28.73 7.691 56.32
2 229.8m 28.23 7.613 56.03
3 143.6m 28.17 7.571 56.53
5 59.4m 28.33 7.636 56.03
10 18.3m 27.91 7.546 55.64
100 1.1m 28.03 7.607 55.91
200 0.5m 27.62 7.550 55.67
Table 1: Effect of count-based pruning on SMT per-
formance using EAN corpus. Results are according to
BLEU, NIST and METEOR (MET) metrics. Bold #s are
not statistically significant worse than exact model.
Brants et al (2007) used 1500 machines for a
day to compute the relative frequencies of n-grams
(summed over all orders from 1 to 5) from 1.8TB
of web data. Their resulting model contained 300
million unique n-grams.
It is not realistic using conventional computing re-
sources to use all the 300 million n-grams for ap-
plications like speech recognition, spelling correc-
tion, information extraction, and statistical machine
translation (SMT). Hence, one of the easiest way to
reduce the size of this model is to use count-based
pruning which discards all n-grams whose count is
less than a pre-defined threshold. Although count-
based pruning is quite simple, yet it is effective for
machine translation. As we do not have a copy of
the web, we will use a portion of gigaword i.e. EAN
(see Section 4.1) to show the effect of count-based
pruning on performance of SMT (see Section 5.1).
Table 1 shows that using a cutoff of 100 produces a
model of size 1.1 million n-grams with a Bleu score
of 28.03. If we compare this with an exact model
of size 367.6 million n-grams, we see an increase of
0.8 points in Bleu (95% statistical significance level
512
? Size BLEU NIST MET
Exact 367.6m 28.73 7.691 56.32
1e-10 218.4m 28.64 7.669 56.33
5e-10 171.0m 28.48 7.666 56.38
1e-9 148.0m 28.56 7.646 56.51
5e-9 91.9m 28.27 7.623 56.16
1e-8 69.4m 28.15 7.609 56.19
5e-7 28.5m 28.08 7.595 55.91
Table 2: Effect of entropy-based pruning on SMT perfor-
mance using EAN corpus. Results are as in Table 1
is ? 0.53 Bleu). However, we need 300 times big-
ger model to get such an increase. Unfortunately, it
is not possible to integrate such a big model inside a
decoder using normal computation resources.
A better way of reducing the size of n-grams is to
use entropy pruning (Stolcke, 1998). Table 2 shows
the results with entropy pruning with different set-
tings of ?. We see that for three settings of ? equal to
1e-10, 5e-10 and 1e-9, we get Bleu scores compara-
ble to the exact model. However, the size of all these
models is not at all small. The size of smallest model
is 25% of the exact model. Even with this size it is
still not feasible to integrate such a big model inside
a decoder. If we take a model of size comparable to
count cutoff of 100, i.e., with ? = 5e-7, we see both
count-based pruning as well as entropy pruning per-
forms the same.
There also have been prior work on maintain-
ing approximate counts for higher-order language
models (LMs) ((Talbot and Osborne, 2007a; Tal-
bot and Osborne, 2007b; Talbot and Brants, 2008))
operates under the model that the goal is to store a
compressed representation of a disk-resident table of
counts and use this compressed representation to an-
swer count queries approximately.
There are two difficulties with scaling all the
above approaches as the order of the LM increases.
Firstly, the computation time to build the database of
counts increases rapidly. Secondly, the initial disk
storage required to maintain these counts, prior to
building the compressed representation is enormous.
The method we propose solves both of these prob-
lems. We do this by making use of the streaming al-
gorithm paradigm (Muthukrishnan, 2005). Working
under the assumption that multiple-GB models are
infeasible, our goal is to instead of estimating a large
model and then compressing it, we directly estimate
a small model. We use a deterministic streaming al-
gorithm (Manku and Motwani, 2002) that computes
approximate frequency counts of frequently occur-
ring n-grams. This scheme is considerably more ac-
curate in getting the actual counts as compared to
other schemes (Demaine et al, 2002; Karp et al,
2003) that find the set of frequent items without car-
ing about the accuracy of counts.
We use these counts directly as features in an
SMT system, and propose a direct way to integrate
these features into an SMT decoder. Experiments
show that directly storing approximate counts of fre-
quent 5-grams compared to using count or entropy-
based pruning counts gives equivalent SMT perfor-
mance, while dramatically reducing the memory us-
age and getting rid of pre-computing a large model.
2 Background
2.1 n-gram Language Models
Language modeling is based on assigning probabil-
ities to sentences. It can either compute the proba-
bility of an entire sentence or predict the probability
of the next word in a sequence. Let wm1 denote a se-
quence of words (w1, . . . , wm). The probability of
estimating word wm depends on previous n-1 words
where n denotes the size of n-gram. This assump-
tion that probability of predicting a current word de-
pends on the previous words is called a Markov as-
sumption, typically estimated by relative frequency:
P (wm | wm?1m?n+1) =
C(wm?1m?n+1wm)
C(wm?1m?n+1)
(1)
Eq 1 estimates the n-gram probability by taking the
ratio of observed frequency of a particular sequence
and the observed frequency of the prefix. This is
precisely the relative frequency estimate we seek.
2.2 Large-scale Language modeling
Using higher order LMs to improve the accuracy
of SMT is not new. (Brants et al, 2007; Emami
et al, 2007) built 5-gram LMs over web using dis-
tributed cluster of machines and queried them via
network requests. Since the use of cluster of ma-
chines is not always practical, (Talbot and Osborne,
2007b; Talbot and Osborne, 2007a) showed a ran-
domized data structure called Bloom filter, that can
be used to construct space efficient language models
513
for SMT. (Talbot and Brants, 2008) presented ran-
domized language model based on perfect hashing
combined with entropy pruning to achieve further
memory reductions. A problem mentioned in (Tal-
bot and Brants, 2008) is that the algorithm that com-
putes the compressed representation might need to
retain the entire database in memory; in their paper,
they design strategies to work around this problem.
(Federico and Bertoldi, 2006) also used single ma-
chine and fewer bits to store the LM probability by
using efficient prefix trees.
(Uszkoreit and Brants, 2008) used partially class-
based LMs together with word-based LMs to im-
prove SMT performance despite the large size of
the word-based models used. (Schwenk and Koehn,
2008; Zhang et al, 2006) used higher language mod-
els at time of re-ranking rather than integrating di-
rectly into the decoder to avoid the overhead of
keeping LMs in the main memory since disk lookups
are simply too slow. Now using higher order LMs at
time of re-ranking looks like a good option. How-
ever, the target n-best hypothesis list is not diverse
enough. Hence if possible it is always better to inte-
grate LMs directly into the decoder.
2.3 Streaming
Consider an algorithm that reads the input from a
read-only stream from left to right, with no ability
to go back to the input that it has already processed.
This algorithm has working storage that it can use to
store parts of the input or other intermediate compu-
tations. However, (and this is a critical constraint),
this working storage space is significantly smaller
than the input stream length. For typical algorithms,
the storage size is of the order of logk N , where N
is the input size and k is some constant.
Stream algorithms were first developed in the
early 80s, but gained in popularity in the late 90s
as researchers first realized the challenges of dealing
with massive data sets. A good survey of the model
and core challenges can be found in (Muthukrish-
nan, 2005). There has been considerable work on the
problem of identifying high-frequency items (items
with frequency above a threshold), and a detailed re-
view of these methods is beyond the scope of this ar-
ticle. A new survey by (Cormode and Hadjielefthe-
riou, 2008) comprehensively reviews the literature.
3 Space-Efficient Approximate Frequency
Estimation
Prior work on approximate frequency estimation for
language models provide a ?no-false-negative? guar-
antee, ensuring that counts for n-grams in the model
are returned exactly, while working to make sure the
false-positive rate remains small (Talbot and Os-
borne, 2007a). The notion of approximation we use
is different: in our approach, it is the actual count
values that will be approximated. We also exploit
the fact that low-frequency n-grams, while consti-
tuting the vast majority of the set of unique n-grams,
are usually smoothed away and are less likely to in-
fluence the language model significantly. Discard-
ing low-frequency n-grams is particularly important
in a stream setting, because it can be shown in gen-
eral that any algorithm that generates approximate
frequency counts for all n-grams requires space lin-
ear in the input stream (Alon et al, 1999).
We employ an algorithm for approximate fre-
quency counting proposed by (Manku and Motwani,
2002) in the context of database management. Fix
parameters s ? (0, 1), and ? ? (0, 1), ? ? s. Our
goal is to approximately find all n-grams with fre-
quency at least sN . For an input stream of n-grams
of length N , the algorithm outputs a set of items
(and frequencies) and guarantees the following:
? All items with frequencies exceeding sN are
output (no false negatives).
? No item with frequency less than (s ? ?)N is
output (few false positives).
? All reported frequencies are less than the true
frequencies by at most ?N (close-to-exact fre-
quencies).
? The space used by the algorithm is
O(1? log ?N).
A simple example illustrates these properties. Let
us fix s = 0.01, ? = 0.001. Then the algorithm guar-
antees that all n-grams with frequency at least 1%
will be returned, no element with frequency less than
0.9% will be returned, and all frequencies will be no
more than 0.1% away from the true frequencies. The
space used by the algorithm is O(logN), which can
be compared to the much larger (close to N ) space
514
needed to store the initial frequency counts. In addi-
tion, the algorithm runs in linear time by definition,
requiring only one pass over the input. Note that
there might be 1? elements with frequency at least
?N , and so the algorithm uses optimal space (up to
a logarithmic factor).
3.1 The Algorithm
We present a high-level overview of the algorithm;
for more details, the reader is referred to (Manku
and Motwani, 2002). The algorithm proceeds by
conceptually dividing the stream into epochs, each
containing 1/? elements. Note that there are ?N
epochs. Each such epoch has an ID, starting from
1. The algorithm maintains a list of tuples1 of the
form (e, f,?), where e is an n-gram, f is its re-
ported frequency, and ? is the maximum error in the
frequency estimation. While the algorithm reads n-
grams associated with the current epoch, it does one
of two things: if the new element e is contained in
the list of tuples, it merely increments the frequency
count f . If not, it creates a new tuple of the form
(e, 1, T ?1), where T is the ID of the current epoch.
After each epoch, the algorithm ?cleans house? by
eliminating tuples whose maximum true frequency
is small. Formally, if the epoch that just ended
has ID T , then the algorithm deletes all tuples sat-
isfying condition f + ? ? T . Since T ? ?N ,
this ensures that no low-frequency tuples are re-
tained. When all elements in the stream have been
processed, the algorithm returns all tuples (e, f,?)
where f ? (s??)N . In practice, however we do not
care about s and return all tuples. At a high level,
the reason the algorithm works is that if an element
has high frequency, it shows up more than once each
epoch, and so its frequency gets updated enough to
stave off elimination.
4 Intrinsic Evaluation
We conduct a set of experiments with approxi-
mate n-gram counts (stream counts) produced by
the stream algorithm. We define various metrics on
which we evaluate the quality of stream counts com-
pared with exact n-gram counts (true counts). To
1We use hash tables to store tuples; however smarter data
structures like suffix trees could also be used.
Corpus Gzip-MB M-wrds Perplexity
EP 63 38 1122.69
afe 417 171 1829.57
apw 1213 540 1872.96
nyt 2104 914 1785.84
xie 320 132 1885.33
Table 3: Corpus Statistics and perplexity of LMs made
with each of these corpuses on development set
evaluate the quality of stream counts on these met-
rics, we carry out three experiments.
4.1 Experimental Setup
The freely available English side of Europarl (EP)
and Gigaword corpus (Graff, 2003) is used for
computing n-gram counts. We only use EP along
with two sections of the Gigaword corpus: Agence
France Press English Service(afe) and The New
York Times Newswire Service (nyt). The unigram
language models built using these corpuses yield
better perplexity scores on the development set (see
Section 5.1) compared to The Xinhua News Agency
English Service (xie) and Associated Press World-
stream English Service (apw) as shown in Table 3.
The LMs are build using the SRILM language mod-
elling toolkit (Stolcke, 2002) with modified Kneser-
Ney discounting and interpolation. The evaluation
of stream counts is done on EP+afe+nyt (EAN) cor-
pus, consisting of 1.1 billion words.
4.2 Description of the metrics
To evaluate the quality of counts produced by our
stream algorithm four different metrics are used.
The accuracy metric measures the quality of top N
stream counts by taking the fraction of top N stream
counts that are contained in the top N true counts.
Accuracy = Stream Counts ? True Counts
True Counts
Spearman?s rank correlation coefficient or Spear-
man?s rho(?) computes the difference between the
ranks of each observation (i.e. n-gram) on two vari-
ables (that are top N stream and true counts). This
measure captures how different the stream count or-
dering is from the true count ordering.
? = 1? 6
? d2i
N(N2 ? 1)
515
di is the difference between the ranks of correspond-
ing elements Xi and Yi; N is the number of elements
found in both sets; Xi and Yi in our case denote the
stream and true counts.
Mean square error (MSE) quantifies the amount
by which a predicted value differs from the true
value. In our case, it estimates how different the
stream counts are from the true counts.
MSE = 1N
N?
i=1
(truei ? predictedi)2
true and predicted denotes values of true and stream
counts; N denotes the number of stream counts con-
tained in true counts.
4.3 Varying ? experiments
In our first experiment, we use accuracy, ? and MSE
metrics for evaluation. Here, we compute 5-gram
stream counts with different settings of ? on the EAN
corpus. ? controls the number of stream counts pro-
duced by the algorithm. The results in Table 4 sup-
port the theory that decreasing the value of ? im-
proves the quality of stream counts. Also, as ex-
pected, the algorithm produces more stream counts
with smaller values of ?. The evaluation of stream
counts obtained with ? = 50e-8 and 20e-8 reveal that
the stream counts learned with this large value are
more susceptible to errors.
If we look closely at the counts for ? = 50e-8, we
see that we get at least 30% of the stream counts
from 245k true counts. This number is not signifi-
cantly worse than the 36% of stream counts obtained
from 4, 018k true counts for the smallest value of
? = 5e-8. However, if we look at the other two met-
rics, the ranking correlation ? of stream counts com-
pared with true counts on ? = 50e-8 and 20e-8 is low
compared to other ? values. For the MSE, the error
with stream counts on these ?m values is again high
compared to other values. As we decrease the value
of ? we continually get better results: decreasing ?
pushes the stream counts towards the true counts.
However, using a smaller ? increases the memory
usage. Looking at the evaluation, it is therefore ad-
visable to use 5-gram stream counts produced with
at most ? ? 10e-7 for the EAN corpus.
Since it is not possible to compute true 7-grams
counts on EAN with available computing resources,
? 5-gram Acc ? MSEproduced
50e-8 245k 0.294 -3.6097 0.4954
20e-8 726k 0.326 -2.6517 0.1155
10e-8 1655k 0.352 -1.9960 0.0368
5e-8 4018k 0.359 -1.7835 0.0114
Table 4: Evaluating quality of 5-gram stream counts for
different settings of ? on EAN corpus
? 7-gram Acc ? MSEproduced
50e-8 44k 0.509 0.3230 0.0341
20e-8 128k 0.596 0.5459 0.0063
10e-8 246k 0.689 0.7413 0.0018
5e-8 567k 0.810 0.8599 0.0004
Table 5: Evaluating quality of 7-gram stream counts for
different settings of ? on EP corpus
we carry out a similar experiment for 7-grams on EP
to verify the results for higher order n-grams 2. The
results in Table 5 tell a story similar to our results for
7-grams. The size of EP corpus is much smaller than
EAN and so we see even better results on each of the
metrics with decreasing the value of ?. The overall
trend remains the same; here too, setting ? ? 10e-
8 is the most effective strategy. The fact that these
results are consistent across two datasets of different
sizes and different n-gram sizes suggests that they
will carry over to other tasks.
4.4 Varying top K experiments
In the second experiment, we evaluate the quality
of the top K (sorted by frequency) 5-gram stream
counts. Here again, we use accuracy, ? and MSE for
evaluation. We fix the value of ? to 5e-8 and com-
pute 5-gram stream counts on the EAN corpus. We
vary the value of K between 100k and 4, 018k (i.e
all the n-gram counts produced by the stream algo-
rithm). The experimental results in Table 6 support
the theory that stream count algorithm computes the
exact count of most of the high frequency n-grams.
Looking closer, we see that if we evaluate the algo-
rithm on just the top 100k 5-grams (roughly 5% of
all 5-grams produced), we see almost perfect results.
Further, if we take the top 1, 000k 5-grams (approx-
imately 25% of all 5-grams) we again see excellent
2Similar evaluation scores are observed for 9-gram stream
counts with different values of ? on EP corpus.
516
Top K Accuracy ? MSE
100k 0.994 0.9994 0.01266
500k 0.934 0.9795 0.0105
1000k 0.723 0.8847 0.0143
2000k 0.504 0.2868 0.0137
4018k 0.359 -1.7835 0.0114
Table 6: Evaluating top K sorted 5-gram stream counts
for ?=5e-8 on EAN corpus
performance on all metrics. The accuracy of the re-
sults decrease slightly, but the ? and MSE metrics
are not decreased that much in comparison. Perfor-
mance starts to degrade as we get to 2, 000k (over
50% of all 5-grams), a result that is not too surpris-
ing. However, even here we note that the MSE is
low, suggesting that the frequencies of stream counts
(found in top K true counts) are very close to the
true counts. Thus, we conclude that the quality of
the 5-gram stream counts produced for this value of
? is quite high (in relation to the true counts).
As before, we corroborate our results with higher
order n-grams. We evaluate the quality of top K 7-
gram stream counts on EP.3 Since EP is a smaller
corpus, we evaluate the stream counts produced by
setting ? to 10e-8. Here we vary the value of K be-
tween 10k and 246k (the total number produced by
the stream algorithm). Results are shown in Table
7. As we saw earlier with 5-grams, the top 10k (i.e.
approximately 5% of all 7-grams) are of very high
quality. Results, and this remains true even when
we increase K to 100k. There is a drop in the accu-
racy and a slight drop in ?, while the MSE remains
the same. Taking all counts again shows a signifi-
cant decrease in both accuracy and ? scores, but this
does not affect MSE scores significantly. Hence, the
7-gram stream counts i.e. 246k counts produced by
? = 10e-8 are quite accurate when compared to the
top 246k true counts.
4.5 Analysis of tradeoff between coverage and
space
In our third experiment, we investigate whether a
large LM can help MT performance. We evaluate
the coverage of stream counts built on the EAN cor-
pus on the test data for SMT experiments (see Sec-
3Similar evaluation scores are observed for different top K
sorted 9-gram stream counts with ?=10e-8 on EP corpus.
Top K Accuracy ? MSE
10k 0.996 0.9997 0.0015
20k 0.989 0.9986 0.0016
50k 0.950 0.9876 0.0016
100k 0.876 0.9493 0.0017
246k 0.689 0.7413 0.0018
Table 7: Evaluating top K sorted 7-gram stream counts
for ?=10e-8 on EP corpus
tion 5.1) with different values of ?m. We compute
the recall of each model against 3071 sentences of
test data where recall is the fraction of number of
n-grams of a dataset found in stream counts.
Recall = Number of n-grams found in stream counts
Number of n-grams in dataset
We build unigram, bigram, trigram, 5-gram and
7-gram with four different values of ?. Table 8 con-
tains the gzip size of the count file and the recall
of various different stream count n-grams. As ex-
pected, the recall with respect to true counts is max-
imum for unigrams, bigrams, trigrams and 5-grams.
However the amount of space required to store all
true counts in comparison to stream counts is ex-
tremely high: we need 4.8GB of compressed space
to store all the true counts for 5-grams.
For unigram models, we see that the recall scores
are good for all values of ?. If we compare the
approximate stream counts produced by largest ?
(which is worst) to all true counts, we see that the
stream counts compressed size is 50 times smaller
than the true counts size, and is only three points
worse in recall. Similar trends hold for bigrams,
although the loss in recall is higher. As with uni-
grams, the loss in recall is more than made up for by
the memory savings (a factor of nearly 150). For
trigrams, we see a 14 point loss in recall for the
smallest ?, but a memory savings of 400 times. For
5-grams, the best recall value is .020 (1.2k out of
60k 5-gram stream counts are found in the test set).
However, compared with the true counts we only
loss a recall of 0.05 (4.3k out of 60k) points but
memory savings of 150 times. In extrinsic evalua-
tions, we will show that integrating 5-gram stream
counts with an SMT system performs slightly worse
than the true counts, while dramatically reducing the
memory usage.
517
N -gram unigram bigram trigram 5-gram 7-gram
? Gzip Recall Gzip Recall Gzip Recall Gzip Recall Gzip RecallMB MB MB MB MB
50e-8 .352 .785 2.3 .459 3.3 .167 1.9 .006 .864 5.6e-5
20e-8 .568 .788 4.5 .494 7.6 .207 5.3 .011 2.7 1.3e-4
10e-8 .824 .791 7.6 .518 15 .237 13 .015 9.7 4.1e-4
5e-8 1.3 .794 13 .536 30 .267 31 .020 43 5.9e-4
all 17 .816 228 .596 1200 .406 4800 .072 NA
Table 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different ?m
For 7-gram we can not compute the true n-gram
counts due to limitations of available computational
resources. The memory requirements with smallest
value of ? are similar to those of 5-gram, but the re-
call values are quite small. For 7-grams, the best re-
call value is 5.9e-4 which means that stream counts
contains only 32 out of 54k 7-grams contained in
test set. The small recall value for 7-grams suggests
that these counts may not be that useful in SMT.
We further substantiate our findings in our extrinsic
evaluations. There we show that integrating 7-gram
stream counts with an SMT system does not affect
its overall performance significantly.
5 Extrinsic Evaluation
5.1 Experimental Setup
All the experiments conducted here make use of
publicly available resources. Europarl (EP) corpus
French-English section is used as parallel data. The
publicly available Moses4 decoder is used for train-
ing and decoding (Koehn and Hoang, 2007). The
news corpus released for ACL SMT workshop in
2007 consisting of 1057 sentences5 is used as the de-
velopment set. Minimum error rate training (MERT)
is used on this set to obtain feature weights to opti-
mize translation quality. The final SMT system per-
formance is evaluated on a uncased test set of 3071
sentences using the BLEU (Papineni et al, 2002),
NIST (Doddington, 2002) and METEOR (Banerjee
and Lavie, 2005) scores. The test set is the union of
the 2007 news devtest and 2007 news test data from
ACL SMT workshop 2007.6
4http://www.statmt.org/moses/
5http://www.statmt.org/wmt07/
6We found that testing on Parliamentary test data was com-
pletely insensitive to large n-gram LMs, even when these LMs
are exact. This suggests that for SMT performance, more data
5.2 Integrating stream counts feature into
decoder
Our method only computes high-frequency n-gram
counts; it does not estimate conditional probabili-
ties. We can either turn these counts into conditional
probabilities (by using SRILM) or use the counts di-
rectly. We observed no significant difference in per-
formance between these two approaches. However,
using the counts directly consumes significantly less
memory at run-time and is therefore preferable. Due
to space constraints, SRILM results are omitted.
The only remaining open question is: how should
we turn the counts into a feature that can be used in
an SMT system? We considered several alternatives;
the most successful was a simple weighted count
of n-gram matches of varying size, appropriately
backed-off. Specifically, consider an n-gram model.
For every sequence of words wi, . . . , wi+N?1, we
obtain a feature score computed recursively accord-
ing to Eq (2).
f(wi) = log
?
C(wi)
Z
?
(2)
f(wi, . . . , wi+k) = log
?
C(wi, . . . , wi+k)
Z
?
+ 12f(wi+1, . . . , wi+k)
Here, 12 is the backoff factor and Z is the largest
count in the count set (the presence of Z is simply to
ensure that these values remain manageable). In or-
der to efficiently compute these features, we store
the counts in a suffix-tree. The computation pro-
ceeds by first considering wi+N?1 alone and then
?expanding? to consider the bigram, then trigram
and so on. The advantage to this order of computa-
tion is that the recursive calls can cease whenever a
is better only if it comes from the right domain.
518
n-gram(?) BLEU NIST MET MemGB
3 EP(exact) 25.57 7.300 54.48 2.7
5 EP(exact) 25.79 7.286 54.44 2.9
3 EAN(exact) 27.04 7.428 55.07 4.6
5 EAN(exact) 28.73 7.691 56.32 20.5
4(10e-8) 27.36 7.506 56.19 2.7
4(5e-8) 27.40 7.507 55.90 2.8
5(10e-8) 27.97 7.605 55.52 2.8
5(5e-8) 27.98 7.611 56.07 2.8
7(10e-8) 27.97 7.590 55.88 2.9
7(5e-8) 27.88 7.577 56.01 2.9
9(10e-8) 28.18 7.611 55.95 2.9
9(5e-8) 27.98 7.608 56.08 2.9
Table 9: Evaluating SMT with different LMs on EAN.
Results are according to BLEU, NIST and MET metrics.
Bold #s are not statistically significant worse than exact.
zero count is reached. (Extending Moses to include
this required only about 100 lines of code.)
5.3 Results
Table 9 summarizes SMT results. We have 4 base-
line LMs that are conventional LMs smoothed using
modified Kneser-Ney smoothing. The first two tri-
gram and 5-gram LMs are built on EP corpus and
the other two are built on EAN corpus. Table 9
show that there is not much significant difference
in SMT results of 5-gram and trigram LM on EP.
As expected, the trigram built on the large corpus
EAN gets an improvement of 1.5 Bleu Score. How-
ever, unlike the EP corpus, building a 5-gram LM
on EAN (huge corpus) gets an improvement of 3.2
Bleu Score. (The 95% statistical significance bound-
ary is about ? 0.53 Bleu on the test data, 0.077 Nist
and 0.16 Meteor according to bootstrap resampling)
We see similar gains in Nist and Meteor metrics as
shown in Table 9.
We use stream counts computed with two values
of ?, 5e-8 and 10e-8 on EAN corpus. We use all
the stream counts produced by the algorithm. 4, 5, 7
and 9 order n-gram stream counts are computed with
these settings of ?. These counts are used along with
a trigram LM built on EP to improve SMT perfor-
mance. The memory usage (Mem) shown in Table
9 is the full memory size required to run on the test
data (including phrase tables).
Adding 4-gram and 5-gram stream counts as fea-
ture helps the most. The performance gain by using
5-gram stream counts is slightly worse than com-
pared to true 5-gram LM on EAN. However, using
5-gram stream counts directly is more memory ef-
ficient. Also, the gains for stream counts are ex-
actly the same as we saw for same sized count-
based and entropy-based pruning counts in Table 1
and 2 respectively. Moreover, unlike the pruning
methods, our algorithm directly computes a small
model, as opposed to compressing a pre-computed
large model.
Adding 7-gram and 9-gram does not help signifi-
cantly, a fact anticipated by the low recall of 7-gram-
based counts that we saw in Section 4.5. The results
with two different settings of ? are largely the same.
This validates our intrinsic evaluation results in Sec-
tion 4.3 that stream counts learned using ? ? 10e-8
are of good quality, and that the quality of the stream
counts is high.
6 Conclusion
We have proposed an efficient, low-memory method
to construct high-order approximate n-gram LMs.
Our method easily scales to billion-word monolin-
gual corpora on conventional (8GB) desktop ma-
chines. We have demonstrated that approximate n-
gram features could be used as a direct replacement
for conventional higher order LMs in SMT with
significant reductions in memory usage. In future,
we will be looking into building streaming skip n-
grams, and other variants (like cluster n-grams).
In NLP community, it has been shown that having
more data results in better performance (Ravichan-
dran et al, 2005; Brants et al, 2007; Turney, 2008).
At web scale, we have terabytes of data and that can
capture broader knowledge. Streaming algorithm
paradigm provides a memory and space-efficient
platform to deal with terabytes of data. We hope
that other NLP applications (where we need to com-
pute relative frequencies) like noun-clustering, con-
structing syntactic rules for SMT, finding analogies,
and others can also benefit from streaming methods.
We also believe that stream counts can be applied to
other problems involving higher order LMs such as
speech recognition, information extraction, spelling
correction and text generation.
519
References
Noga Alon, Yossi Matias, and Mario Szegedy. 1999. The
space complexity of approximating the frequency mo-
ments. J. Comput. Syst. Sci., 58(1).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
S. Chen and J. Goodman. 1996. An Empirical Study
of Smoothing Techniques for Language Modeling. In
Proceedings of 34th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 310?318,
Santa Cruz, CA, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
E.D. Demaine, A. Lopez-Ortiz, and J.I. Munro. 2002.
Frequency estimation of internet packet streams with
limited space.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of the 2007 IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), volume 4, pages 37?40.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings on the Workshop on
Statistical Machine Translation at ACL06.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Richard M. Karp, Christos H. Papadimitriou, and Scott
Shenker. 2003. A simple algorithm for finding fre-
quent elements in streams and bags.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In Proceedings of
the 28th International Conference on Very Large Data
Bases.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics.
Holger Schwenk and Philipp Koehn. 2008. Large and
diverse language models for statistical machine trans-
lation. In Proceedings of The Third International Joint
Conference on Natural Language Processing (IJCNP).
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broadcast
News Transcription and Understanding Workshop.
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO, September.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
David Talbot and Miles Osborne. 2007b. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning (EM
NLP-CoNLL).
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-08: HLT.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing.
520
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 27?32,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Domain Adaptation meets Active Learning
Piyush Rai, Avishek Saha, Hal Daume? III, and Suresh Venkatasubramanian
School of Computing, University of Utah
Salt Lake City, UT 84112
{piyush,avishek,hal,suresh}@cs.utah.edu
Abstract
In this work, we show how active learning
in some (target) domain can leverage infor-
mation from a different but related (source)
domain. We present an algorithm that har-
nesses the source domain data to learn the best
possible initializer hypothesis for doing active
learning in the target domain, resulting in im-
proved label complexity. We also present a
variant of this algorithm which additionally
uses the domain divergence information to se-
lectively query the most informative points in
the target domain, leading to further reduc-
tions in label complexity. Experimental re-
sults on a variety of datasets establish the effi-
cacy of the proposed methods.
1 Introduction
Acquiring labeled data to train supervised learning
models can be difficult or expensive in many prob-
lem domains. Active Learning tries to circumvent
this difficultly by only querying the labels of the
most informative examples and, in several cases, has
been shown to achieve exponentially lower label-
complexity (number of queried labels) than super-
vised learning (Cohn et al, 1994). Domain Adap-
tation (Daume? & Marcu, 2006), although motivated
somewhat differently, attempts to address a seem-
ingly similar problem: lack of labeled data in some
target domain. Domain Adaptation deals with this
problem using labeled data from a different (but re-
lated) source domain.
In this paper, we consider the supervised domain
adaptation setting (Finkel & Manning, 2009; Daume?
III, 2007) having a large amount of labeled data from
a source domain, a large amount of unlabeled data
from a target domain, and additionally a small bud-
get for acquiring labels in the target domain. We
show how, apart from leveraging information in the
usual domain adaptation sense, the information from
the source domain can be leveraged to intelligently
query labels in the target domain.We achieve this
by first training the best possible classifier without
using target domain labeled data 1 and then using
the learned classifier to leverage the inter-domain in-
formation when we are additionally provided some
fixed budget for acquiring extra labeled target data
(i.e., the active learning setting (Settles, 2009)).
There are several ways in which our ?best clas-
sifier? can be utilized. Our first approach uses this
classifier as the initializer while doing (online) ac-
tive learning in the target domain (Section 3). Then
we present a variant augmenting the first approach
using a domain-separator hypothesis which leads to
additionally ruling out querying the labels of those
target examples that appear ?similar? to the source
domain (Section 4).
Figure 1 shows our basic setup which uses a
source (or unsupervised domain-adapted source)
classifier v0 as an initializer for doing active learn-
ing in the target domain having some small, fixed
budget for querying labels. Our framework consists
of 2 phases: 1) Learning the best possible classi-
1For instance, either by simply training a supervised classi-
fier on the labeled source data, or by using unsupervised domain
adaptation techniques (Blitzer et al, 2006; Sugiyama et al,
2007) that use labeled data from the source domain, and ad-
ditionally unlabeled data from the source and target domains.
27
        
        
        
        
        
        
        
        
        









LS
UT
US
DOMAIN
ADAPTATION
v0
LT
ACTIVE
LEARNING
ORACLE
wT
Figure 1: Block diagram of our basic approach. Stage-1 can
use any black-box unsupervised domain adaptation approach
(e.g., (Blitzer et al, 2006; Sugiyama et al, 2007))
fier v0 using source labeled (LS) and unlabeled data
(US), and target unlabeled (UT ) data, and 2) Query-
ing labels for target domain examples by leveraging
information from the classifier learned in phase-1.
2 Online Active Learning
The active learning phase of our algorithm is based
on (Cesa-Bianchi et al, 2006), henceforth referred
to as CBGZ. In this section, we briefly describe this
approach for the sake of completeness.
Their algorithm (Algorithm 1) starts with a zero
initialized weight vector w0T and proceeds in rounds
by querying the label of an example xi with proba-
bility bb+|ri| , where |ri| is the confidence (in terms of
margin) of the current weight vector on xi. b is a pa-
rameter specifying how aggressively the labels are
queried. A large value of b implies that a large num-
ber of labels will be queried (conservative sampling)
whereas a small value would lead to a small number
of examples being queried. For each label queried,
the algorithm updates the current weight vector if the
label was predicted incorrectly. It is easy to see that
the total number of labels queried by this algorithm
is
?T
i=1 E[ bb+|ri| ].
3 Active Online Domain Adaptation
In our supervised domain adaptation setting, we are
given a small budget for acquiring labels in a tar-
get domain, which makes it imperative to use active
learning in the target domain. However, our goal
is to additionally also leverage inter-domain related-
ness by exploiting whatever information we might
already have from the source domain. To accom-
plish this, we take the online active learning ap-
Algorithm 1 CBGZ
Input: b > 0; T : number of rounds
Initialization: w0T = 0; k = 1;
for i = 1 to T do
x?i = xi/||xi||, set ri = wi?1T x?i;
predict y?i = SIGN(ri);
sample Zi ? Bernoulli( bb+|ri|);
if Zi = 1 then
query label yi ? {+1,?1}
if y?i 6= yi then
update: wkT = wk?1T + yix?i; k ? k + 1;
end if
end if
end for
proach of (Cesa-Bianchi et al, 2006) described in
Section 2 and adapt it such that the algorithm uses
the best possible classifier learned (without target la-
beled data; see Figure 1) as the initializer hypothesis
in the target domain, and thereafter updates this hy-
pothesis in an online fashion using actively acquired
labels as is done in (Cesa-Bianchi et al, 2006). This
amounts to using w0T = v0 in Algorithm 1. We refer
to this algorithm as Active Online Domain Adapta-
tion (AODA). It can be shown that the modified al-
gorithm (AODA) yields smaller mistake bound and
smaller label complexity than the CBGZ algorithm.
We skip the proofs here and reserve the presentation
for a longer version. It is however possible to pro-
vide an intuitive argument for the smaller label com-
plexity: Since AODA is initialized with a non-zero
(but not randomly chosen) hypothesis v0 learned us-
ing data from a related source domain, the sequence
of hypotheses AODA produces are expected to have
higher confidences margins |r?i| as compared that of
CBGZ which is based on a zero initialized hypothe-
sis. Therefore, at each round, the sampling proba-
bility of AODA given by bb+|r?i| will also be smaller,
leading to a smaller number of queried labels since
it is nothing but
?T
i=1 E[ bb+|r?i| ].
4 Using Domain Separator Hypothesis
The relatedness of source and target domains can be
additionally leveraged to further improve the algo-
rithm described in Section 3. Since the source and
target domains are assumed to be related, one can
28
use this fact to upfront rule out acquiring the labels
of some target domain examples that ?appear? to be
similar to the source domain examples. As an il-
lustration, Fig. 2 shows a typical distribution sepa-
rator hypothesis (Blitzer et al, 2007a) which sepa-
rates the source and target examples. If the source
and target domains are reasonably different, then the
separator hypothesis can perfectly distinguish be-
tween the examples drawn from these two domains.
On the other hand, if the domains are similar, one
would expected that there will be some overlap and
therefore some of the target domain examples will
lie on the source side (cf., Fig. 2). Acquiring la-
bels for such examples is not really needed since
the initializing hypothesis v0 (cf., Fig 1) of AODA
would already have taken into account such exam-
ples. Therefore, such target examples can be out-
rightly ignored from being queried for labels. Our
second algorithm (Algorithm 2) is similar to Algo-
rithm 1, but also makes use of the distribution sepa-
rator hypothesis (which can be learned using source
and target unlabeled examples) as a preprocessing
step before doing active learning on each incom-
ing target example. We denote this algorithm by
DS-AODA (for Domain-Separator based AODA).
Since some of the target examples are upfront ruled
out from being queried, this approach resulted even
smaller number of queried labels (Section 5.4).
wds
w
DS
DT
Figure 2: An illustrative diagram showing distribution
separator hypothesis wds separating source data from tar-
get data. w is the actual target hypothesis
5 Experiments
In this section, we demonstrate the empirical perfor-
mance of our algorithms and compare them with a
Algorithm 2 DS-AODA
Input: b > 0; wds: distribution separator hypoth-
esis; v0 : initializing hypothesis ; T : number of
rounds
Initialization: w0T = v0; k = 1;
for i = 1 to T do
x?i = xi/||xi||,
if x?i does not lie on the source side of w?ds then
set ri = wi?1T x?i;
predict y?i = SIGN(ri);
sample Zi ? Bernoulli( bb+|ri|);
if Zi = 1 then
query label yi ? {+1,?1}
if y?i 6= yi then
update: wkT = wk?1T +yix?i; k ? k+1;
end if
end if
end if
end for
number of baselines. Table 1 summarizes the meth-
ods used with a brief description of each. Among the
first three (ID, SDA, FEDA), FEDA (Daume? III,
2007) is a state-of-the-art supervised domain adap-
tation method but assumes passively acquired la-
bels. The last four, RIAL, ZIAL, SIAL and AODA
methods in Table 1 acquire labels in an active fash-
ion. As the description denotes, RIAL and ZIAL
start active learning in target with a randomly ini-
tialized and zero initialized base hypothesis, respec-
tively. It is also important to distinguish between
SIAL and AODA here: SIAL uses an unmodi-
fied classifier learned only from source labeled data
as the initializer, whereas AODA uses an unsuper-
vised domain-adaptation technique (i.e., without us-
ing labeled target data) to learn the initializer. In our
experiments, we use the instance reweighting ap-
proach (Sugiyama et al, 2007) to perform the unsu-
pervised domain adaptation step. However, we note
that this step can also be performed using any other
unsupervised domain adaptation technique such as
Structural Correspondence Learning (SCL) (Blitzer
et al, 2006). We compare all the approaches based
on classification accuracies achieved for a given
budget of labeled target examples (Section-5.2), and
number of labels requested for a fixed pool of unla-
beled target examples and corresponding accuracies
29
Method Summary Active ?
ID In-domain (DT ) data No
SDA UDA followed by passively chosen labeled target data No
FEDA Frustratingly Easy Domain Adaptation Daume? III (2007) No
ZIAL Zero initialized active learning Cesa-Bianchi et al (2006) Yes
RIAL Randomly initialized active learning with fixed label budget Yes
SIAL Source hypothesis initialized active learning Yes
AODA UDA based source hypothesis initialized active learning Yes
Table 1: Description of the methods compared
(Section-5.3). We use the vanilla Perceptron as the
base classifier of each of the algorithms and each
experiment has been averaged over 20 runs corre-
sponding to random data order permutations.
5.1 Datasets
We report our empirical results for the task of senti-
ment classification using data provided by (Blitzer
et al, 2007b) which consists of user reviews of
eight product types (apparel, books, DVD, electron-
ics, kitchen, music, video, and other) from Ama-
zon.com. We also apply PCA to reduce the data-
dimensionality to 50. The sentiment classification
task for this dataset is binary classification which
corresponds to classifying a review as positive or
negative. The sentiment dataset consists of several
domain pairs with varying A-distance (which mea-
sures the domain separation), akin to the sense de-
scribed in (Ben-David et al, 2006). Table 2 presents
the domain pairs used in our experiments and their
corresponding domain divergences in terms of the
A-distance (Ben-David et al, 2006).
To compute theA-distance from finite samples of
source and target domain, we use a surrogate to the
true A-distance (the proxy A-distance) in a manner
similar to (Ben-David et al, 2006): First, we train a
linear classifier to separate the source domain from
the target domain using only unlabeled examples
from both. The average per-instance hinge-loss of
this classifier subtracted from 1 serves as our esti-
mate of the proxy A-distance. A score of 1 means
perfectly separable distributions whereas a score of
0 means that the two distributions are essentially the
same. As a general rule, a high score means that the
two domains are reasonably far apart.
Source Target A-distance
Dvd (D) Book (B) 0.7616
Dvd (D) Music (M) 0.7314
Books (B) Apparel (A) 0.5970
Dvd (D) Apparel (A) 0.5778
Electronics (E) Apparel (A) 0.1717
Kitchen (K) Apparel (A) 0.0459
Table 2: ProxyA-distances between some domain pairs
5.2 Classification Accuracies
In our first experiment, we compare our first ap-
proach of Section 3 (AODA, and also SIAL
which na??vely uses the unadapted source hypoth-
esis) against other baselines on two domain pairs
from the sentiments dataset: DVD?BOOKS (large
A distance) and KITCHEN?APPAREL (small A
distance) with varying target budget (1000 to 5000).
The results are shown in Table 3 and Table 4. As
the results indicate, on both datasets, our approaches
(SIAL, AODA) perform consistently better than the
baseline approaches (Table 1) which also include
one of the state-of-the-art supervised domain adap-
tation algorithms (Daume? III, 2007). On the other
hand, we observe that the zero-initialized and ran-
domly initialized approaches do not perform as well.
In particular, the latter case suggests that it?s impor-
tant to have a sensible initialization.
5.3 Label Complexity Results
Next, we compare the various algorithms on the
basis of the number of labels acquired (and corre-
sponding accuracies) when given the complete pool
of unlabeled examples from the target domain. Ta-
ble 5 shows that our approaches result in much
smaller label complexities as compared to other ac-
30
Met- Target Budget
hod 1000 2000 3000 4000 5000
Acc (Std) Acc (Std) Acc (Std) Acc (Std) Acc (Std)
ID 65.94 (?3.40) 66.66 (?3.01) 67.00 (?2.40) 65.72 (?3.98) 66.25 (?3.18)
SDA 66.17 (?2.57) 66.45 (?2.88) 65.31 (?3.13) 66.33 (?3.51) 66.22 (?3.05)
RIAL 51.79 (?4.36) 53.12 (?4.65) 55.01 (?4.20) 57.56 (?4.18) 58.57 (?2.44)
ZIAL 66.24 (?3.16) 66.72 (?3.30) 63.97 (?4.82) 66.28 (?3.61) 66.36 (?2.82)
SIAL 68.22 (?2.17) 69.65 (?1.20) 69.95 (?1.55) 70.54 (?1.42) 70.97 (?0.97)
AODA 67.64 (?2.35) 68.89 (?1.37) 69.49 (?1.63) 70.55 (1.15) 70.65 (?0.94)
FEDA 67.31 (?3.36) 68.47 (?3.15) 68.37 (?2.72) 66.95 (3.11) 67.13 (?3.16)
Acc: Accuracy | Std: Standard Deviation
Table 3: Classification accuracies for DVD?BOOKS, for fixed target budget.
Met- Target Budget
hod 1000 2000 3000 4000 5000
Acc (Std) Acc (Std) Acc (Std) Acc (Std) Acc (Std)
ID 69.64 (?3.14) 69.61 (?3.17) 69.36 (?3.14) 69.77 (?3.58) 70.77 (?3.05)
SDA 69.70 (?2.57) 70.48 (?3.42) 70.29 (?2.56) 70.86 (?3.16) 70.71 (?3.65)
RIAL 52.13 (?5.44) 56.83 (?5.36) 58.09 (?4.09) 59.82 (?4.16) 62.03 (?2.52)
ZIAL 70.09 (?3.74) 69.96 (?3.27) 68.6 (?3.94) 70.06 (?2.84) 69.75 (?3.26)
SIAL 73.82 (?1.47) 74.45 (?1.27) 75.11 (?0.98) 75.35 (?1.30) 75.58 (?0.85)
AODA 73.93 (?1.84) 74.18 (?1.85) 75.13 (?1.18) 75.88 (?1.32) 76.02 (?0.97)
FEDA 70.05 (?2.47) 69.34 (?3.50) 71.22 (?3.00) 71.67 (?2.59) 70.80 (?3.89)
Acc: Accuracy | Std: Standard Deviation
Table 4: Classification accuracies for KITCHEN?APPAREL, for fixed target budget.
tive learning based baselines and still gives better
classification accuracies. We also note that although
RIAL initializes with a non-zero hypothesis and
queries almost similar number of labels as our algo-
rithms, it actually performs worse than even ZIAL
in terms of classification accuracies, which implies
the significant of a sensible initializing hypothesis.
5.4 DS-AODA Results
Finally, we evaluate our distribution separator hy-
pothesis based approach (DS-AODA) discussed in
Section 4. As our experimental results (on four do-
main pairs, Fig. 3) indicate, this approach leads to
considerably smaller number of labels acquired than
our first approach AODA which does not use the
information about domain separation, without any
perceptible loss in classification accuracies. Simi-
lar improvements in label complexity (although not
reported here) were observed when we grafted the
distribution separator hypothesis around SIAL (the
unaltered source initialized hypothesis).
Met- DVD?BOOK KITCHEN?APPAREL
hod Acc (Std) Labels Acc (Std) Labels
RIAL 62.74 (?3.00) 7618 62.15 (?4.51) 4871
ZIAL 65.65 (?2.82) 10459 70.19 (?2.64) 6968
SIAL 72.11 (?1.20) 7517 75.62 (?1.14) 4709
AODA 72.00 (?1.31) 7452 75.62 (?0.82) 4752
Acc: Accuracy | Std: Standard Deviation
Table 5: Accuracy and label complexity of DVD?BOOKS
and KITCHEN?APPAREL with full target training data
treated as the unlabeled pool.
50
60
70
80
D->B D->M E->A K->A
ac
cu
ra
cy
domains
AODA
DS-AODA
2K
4K
6K
8K
10K
12K
D->B D->M E->A K->A
#la
bel
s q
uer
ied
domains
AODA
DS-AODA
Figure 3: Test accuracy and label complexity of D?B, D?M,
E?A and K?A.
31
5.5 SIAL vs AODA
Some of the results might indicate from na??vely ini-
tializing using even the unadapted source trained
classifier (SIAL) tends to be as good as initializing
with a classifer trained using unsupervised domain
adaptation (AODA). However, it is mainly due to
the particular unsupervised domain adaptation tech-
nique (na??ve instance weighting) we have used here
for the first stage. In some cases, the weights es-
timated using instance weighting may not be accu-
rate and the bias in importance weight estimation is
potentially the reason behind AODA not doing bet-
ter than SIAL in such cases. As mentioned earlier,
however, any other unsupervised domain adaptation
technique can be used here and, in general, AODA
is expected to perform better than SIAL.
6 Related Work
Active learning in a domain adaptation setting has
received little attention so far. One interesting set-
ting was proposed in (Chan & Ng, 2007) where they
apply active learning for word sense disambiguation
in a domain adaptation setting. Their active learn-
ing setting is pool-based whereas ours is a stream-
ing (online) setting. Furthermore, our second algo-
rithm also uses the domain separator hypothesis to
rule out querying the labels of target examples simi-
lar to the source. A combination of transfer learning
with active learning has been presented in (Shi et al,
2008). One drawback of their approach is the re-
quirement of an initial pool of labeled target domain
data used to train an in-domain classifier. Without
this in-domain classifier, no transfer learning is pos-
sible in their setting.
7 Discussion
There are several interesting variants of our ap-
proach that can worth investigating. For instance,
one can use a hybrid oracle setting where the source
classifier v0 could be used as an oracle that provides
labels for free, whenever it is reasonably highly con-
fident about its prediction (maybe in terms of its rel-
ative confidence as compared to the actual classifier
being learned; it would also be interesting to set,
and possibly adapt, this confidence measure as the
active learning progresses). Besides, in the distri-
bution separator hypothesis based approach of Sec-
tion 4, we empirically observed significant reduc-
tions in label-complexity, and it is supported by in-
tuitive arguments. However, it would be interesting
to be able to precisely quantify the amount by which
the label-complexity is expected to reduce.
References
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.
Analysis of Representations for Domain Adaptation.
In NIPS, 2006.
Blitzer, J., Mcdonald, R., and Pereira, F. Domain Adap-
tation with Structural Correspondence Learning. In
EMNLP, 2006.
Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and
Wortman, J. Learning Bounds for Domain Adaptation.
In NIPS, 2007a.
Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bol-
lywood, Boom-boxes and Blenders: Domain Adapta-
tion for Sentiment Classification. In ACL, 2007b.
Cesa-Bianchi, Nicolo`, Gentile, Claudio, and Zaniboni,
Luca. Worst-Case Analysis of Selective Sampling for
Linear Classification. JMLR, 7, 2006.
Chan, Y. S. and Ng, H. T. Domain adaptation with active
learning for word sense disambiguation. In ACL, 2007.
Cohn, David, Atlas, Les, and Ladner, Richard. Improving
Generalization with Active Learning. Machine Learn-
ing, 15(2), 1994.
Daume?, III, Hal and Marcu, Daniel. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26(1), 2006.
Daume? III, H. Frustratingly Easy Domain Adaptation. In
ACL, 2007.
Finkel, Jenny Rose and Manning, Christopher D. Hier-
archical Bayesian domain adaptation. In NAACL, pp.
602?610, Morristown, NJ, USA, 2009.
Settles, B. Active Learning Literature Survey. In Com-
puter Sciences Technical Report 1648, University of
Wisconsin-Madison, 2009.
Shi, Xiaoxiao, Fan, Wei, and Ren, Jiangtao. Actively
Transfer Domain Knowledge. In ECML/PKDD (2),
2008.
Sugiyama, M., Nakajima, S., Kashima, H., von Bu?nau,
P., and Kawanabe, M. Direct Importance Estimation
with Model Selection and Its Application to Covariate
Shift Adaptation. In NIPS, 2007.
32
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 17?25,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sketching Techniques for Large Scale NLP
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and Suresh Venkatasubramanian
University of Utah, School of Computing
{amitg,jags,hal,suresh}@cs.utah.edu
Abstract
In this paper, we address the challenges
posed by large amounts of text data by
exploiting the power of hashing in the
context of streaming data. We explore
sketch techniques, especially the Count-
Min Sketch, which approximates the fre-
quency of a word pair in the corpus with-
out explicitly storing the word pairs them-
selves. We use the idea of a conservative
update with the Count-Min Sketch to re-
duce the average relative error of its ap-
proximate counts by a factor of two. We
show that it is possible to store all words
and word pairs counts computed from 37
GB of web data in just 2 billion counters
(8 GB RAM). The number of these coun-
ters is up to 30 times less than the stream
size which is a big memory and space gain.
In Semantic Orientation experiments, the
PMI scores computed from 2 billion coun-
ters are as effective as exact PMI scores.
1 Introduction
Approaches to solve NLP problems (Brants et al,
2007; Turney, 2008; Ravichandran et al, 2005) al-
ways benefited from having large amounts of data.
In some cases (Turney and Littman, 2002; Pat-
wardhan and Riloff, 2006), researchers attempted
to use the evidence gathered from web via search
engines to solve the problems. But the commer-
cial search engines limit the number of automatic
requests on a daily basis for various reasons such
as to avoid fraud and computational overhead.
Though we can crawl the data and save it on disk,
most of the current approaches employ data struc-
tures that reside in main memory and thus do not
scale well to huge corpora.
Fig. 1 helps us understand the seriousness of
the situation. It plots the number of unique word-
s/word pairs versus the total number of words in
5 10 15 20 25
5
10
15
20
25
Log2 of # of words
Lo
g 2 
of 
# o
f u
niq
ue
 Ite
ms
 
 
Items=word?pairs
Items=words
Figure 1: Token Type Curve
a corpus of size 577 MB. Note that the plot is in
log-log scale. This 78 million word corpus gen-
erates 63 thousand unique words and 118 million
unique word pairs. As expected, the rapid increase
in number of unique word pairs is much larger
than the increase in number of words. Hence, it
shows that it is computationally infeasible to com-
pute counts of all word pairs with a giant corpora
using conventional main memory of 8 GB.
Storing only the 118 million unique word pairs
in this corpus require 1.9 GB of disk space. This
space can be saved by avoiding storing the word
pair itself. As a trade-off we are willing to tolerate
a small amount of error in the frequency of each
word pair. In this paper, we explore sketch tech-
niques, especially the Count-Min Sketch, which
approximates the frequency of a word pair in the
corpus without explicitly storing the word pairs
themselves. It turns out that, in this technique,
both updating (adding a new word pair or increas-
ing the frequency of existing word pair) and query-
ing (finding the frequency of a given word pair) are
very efficient and can be done in constant time1.
Counts stored in the CM Sketch can be used to
compute various word-association measures like
1depend only on one of the user chosen parameters
17
Pointwise Mutual Information (PMI), and Log-
Likelihood ratio. These association scores are use-
ful for other NLP applications like word sense
disambiguation, speech and character recognition,
and computing semantic orientation of a word. In
our work, we use computing semantic orientation
of a word using PMI as a canonical task to show
the effectiveness of CM Sketch for computing as-
sociation scores.
In our attempt to advocate the Count-Min
sketch to store the frequency of keys (words or
word pairs) for NLP applications, we perform both
intrinsic and extrinsic evaluations. In our intrinsic
evaluation, first we show that low-frequent items
are more prone to errors. Second, we show that
computing approximate PMI scores from these
counts can give the same ranking as Exact PMI.
However, we need counters linear in size of stream
to achieve that. We use these approximate PMI
scores in our extrinsic evaluation of computing se-
mantic orientation. Here, we show that we do not
need counters linear in size of stream to perform
as good as Exact PMI. In our experiments, by us-
ing only 2 billion counters (8GB RAM) we get the
same accuracy as for exact PMI scores. The num-
ber of these counters is up to 30 times less than the
stream size which is a big memory and space gain
without any loss of accuracy.
2 Background
2.1 Large Scale NLP problems
Use of large data in the NLP community is not
new. A corpus of roughly 1.6 Terawords was used
by Agirre et al (2009) to compute pairwise sim-
ilarities of the words in the test sets using the
MapReduce infrastructure on 2, 000 cores. Pan-
tel et al (2009) computed similarity between 500
million terms in the MapReduce framework over a
200 billion words in 50 hours using 200 quad-core
nodes. The inaccessibility of clusters for every one
has attracted the NLP community to use stream-
ing, randomized, approximate and sampling algo-
rithms to handle large amounts of data.
A randomized data structure called Bloom fil-
ter was used to construct space efficient language
models (Talbot and Osborne, 2007) for Statis-
tical Machine Translation (SMT). Recently, the
streaming algorithm paradigm has been used to
provide memory and space-efficient platform to
deal with terabytes of data. For example, We
(Goyal et al, 2009) pose language modeling as
a problem of finding frequent items in a stream
of data and show its effectiveness in SMT. Subse-
quently, (Levenberg and Osborne, 2009) proposed
a randomized language model to efficiently deal
with unbounded text streams. In (Van Durme and
Lall, 2009b), authors extend Talbot Osborne Mor-
ris Bloom (TOMB) (Van Durme and Lall, 2009a)
Counter to find the highly ranked k PMI response
words given a cue word. The idea of TOMB is
similar to CM Sketch. TOMB can also be used to
store word pairs and further compute PMI scores.
However, we advocate CM Sketch as it is a very
simple algorithm with strong guarantees and good
properties (see Section 3).
2.2 Sketch Techniques
A sketch is a summary data structure that is used
to store streaming data in a memory efficient man-
ner. These techniques generally work on an input
stream, i.e. they process the input in one direc-
tion, say from left to right, without going back-
wards. The main advantage of these techniques
is that they require storage which is significantly
smaller than the input stream length. For typical
algorithms, the working storage is sublinear in N ,
i.e. of the order of logk N , where N is the input
size and k is some constant which is not explicitly
chosen by the algorithm but it is an artifact of it..
Sketch based methods use hashing to map items in
the streaming data onto a small-space sketch vec-
tor that can be easily updated and queried. It turns
out that both updating and querying on this sketch
vector requires only a constant time per operation.
Streaming algorithms were first developed in
the early 80s, but gained in popularity in the late
90s as researchers first realized the challenges of
dealing with massive data sets. A good survey
of the model and core challenges can be found in
(Muthukrishnan, 2005). There has been consid-
erable work on coming up with different sketch
techniques (Charikar et al, 2002; Cormode and
Muthukrishnan, 2004; Li and Church, 2007). A
survey by (Rusu and Dobra, 2007; Cormode and
Hadjieleftheriou, 2008) comprehensively reviews
the literature.
3 Count-Min Sketch
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data structure
used to store the frequencies of all items in the in-
put stream. The sketch allows fundamental queries
18
on the data stream such as point, range and in-
ner product queries to be approximately answered
very quickly. It can also be applied to solve the
finding frequent items problem (Manku and Mot-
wani, 2002) in a data stream. In this paper, we are
only interested in point queries. The aim of a point
query is to estimate the count of an item in the in-
put stream. For other details, the reader is referred
to (Cormode and Muthukrishnan, 2004).
Given an input stream of word pairs of length N
and user chosen parameters ? and ?, the algorithm
stores the frequencies of all the word pairs with the
following guarantees:
? All reported frequencies are within the true
frequencies by at most ?N with a probability
of at least ?.
? The space used by the algorithm is
O(1? log 1? ).
? Constant time of O(log(1? )) per each update
and query operation.
3.1 CM Data Structure
A Count-Min Sketch with parameters (?,?) is rep-
resented by a two-dimensional array with width w
and depth d :
?
?
?
sketch[1,1] ? ? ? sketch[1,w]
.
.
.
.
.
.
.
.
.
sketch[d,1] ? ? ? sketch[d,w]
?
?
?
Among the user chosen parameters, ? controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within the accepted error. These val-
ues of ? and ? determine the width and depth of the
two-dimensional array respectively. To achieve
the guarantees mentioned in the previous section,
we set w=2? and d=log(1? ). The depth d denotes
the number of pairwise-independent hash func-
tions employed by the algorithm and there exists
an one-to-one correspondence between the rows
and the set of hash functions. Each of these hash
functions hk:{1 . . . N} ? {1 . . . w} (1 ? k ? d)
takes an item from the input stream and maps it
into a counter indexed by the corresponding hash
function. For example, h2(w) = 10 indicates that
the word pair w is mapped to the 10th position in
the second row of the sketch array. These d hash
functions are chosen uniformly at random from a
pairwise-independent family.
Figure 2: Update Procedure for CM sketch and conserva-
tive update (CU)
Initially the entire sketch array is initialized
with zeros.
Update Procedure: When a new item (w,c) ar-
rives, where w is a word pair and c is its count2,
one counter in each row, as decided by its corre-
sponding hash function, is updated by c. Formally,
?1 ? k ? d
sketch[k,hk(w)]? sketch[k,hk(w)] + c
This process is illustrated in Fig. 2 CM. The item
(w,2) arrives and gets mapped to three positions,
corresponding to the three hash functions. Their
counts before update were (4,2,1) and after update
they become (6,4,3). Note that, since we are using
a hash to map a word into an index, a collision can
occur and multiple word pairs may get mapped to
the same counter in any given row. Because of
this, the values stored by the d counters for a given
word pair tend to differ.
Query Procedure: The querying involves find-
ing the frequency of a given item in the input
stream. Since multiple word pairs can get mapped
into same counter and the observation that the
counts of items are positive, the frequency stored
by each counter is an overestimate of the true
count. So in answering the point query, we con-
sider all the positions indexed by the hash func-
tions for the given word pair and return the mini-
mum of all these values. The answer to Query(w)
is:
c? = mink sketch[k,hk(w)]
Note that, instead of positive counts if we had neg-
ative counts as well then the algorithm returns the
median of all the counts and the bounds we dis-
cussed in Sec. 3 vary. In Fig. 2 CM, for the word
pair w it takes the minimum over (6,4,3) and re-
turns 3 as the count of word pair w.
2In our setting, c is always 1. However, in other NLP
problem, word pairs can be weighted according to recency.
19
Both update and query procedures involve eval-
uating d hash functions and a linear scan of all the
values in those indices and hence both these pro-
cedures are linear in the number of hash functions.
Hence both these steps require O(log(1? )) time. In
our experiments (see Section 4.2), we found that a
small number of hash functions are sufficient and
we use d=3. Hence, the update and query oper-
ations take only a constant time. The space used
by the algorithm is the size of the array i.e. wd
counters, where w is the width of each row.
3.2 Properties
Apart from the advantages of being space efficient,
and having constant update and constant querying
time, the Count-Min sketch has also other advan-
tages that makes it an attractive choice for NLP
applications.
? Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of
the combined data stream can be easily ob-
tained by adding the individual sketches in
O(1? log 1? ) time which is independent of the
stream size.
? The linearity is especially attractive because,
it allows the individual sketches to be com-
puted independent of each other. Which
means that it is easy to implement it in dis-
tributed setting, where each machine com-
putes the sketch over a sub set of corpus.
? This technique also extends to allow the dele-
tion of items. In this case, to answer a point
query, we should return the median of all the
values instead of the minimum value.
3.3 Conservative Update
Estan and Varghese introduce the idea of conser-
vative update (Estan and Varghese, 2002) in the
context of networking. This can easily be used
with CM Sketch to further improve the estimate
of a point query. To update an item, word pair, w
with frequency c, we first compute the frequency
c? of this item from the existing data structure and
the counts are updated according to: ?1 ? k ? d
sketch[k,hk(w)]? max{sketch[k,hk(w)], c? + c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update
a counter only if it is necessary as indicated by
the above equation. Though this is a heuristic, it
avoids the unnecessary updates of counter values
and thus reduces the error.
The process is also illustrated in Fig. 2CU.
When an item ?w? with a frequency of 2 arrives
in the stream, it gets mapped into three positions
in the sketch data structure. Their counts before
update were (4,2,1) and the frequency of the item
is 1 (the minimum of all the three values). In this
particular case, the update rule says that increase
the counter value only if its updated value is less
than c? + 2 = 3. As a result, the values in these
counters after the update become (4,3,3).
However, if the value in any of the counters
is already greater than 3 e.g. 4, we cannot at-
tempt to correct it by decreasing, as it could con-
tain the count for other items hashed at that posi-
tion. Therefore, in this case, for the first counter
we leave the value 4 unchanged. The query pro-
cedure remains the same as in the previous case.
In our experiments, we found that employing the
conservative update reduces the Average Relative
Error (ARE) of these counts approximately by a
factor of 2. (see Section 4.2). But unfortunately,
this update prevents deletions and items with neg-
ative updates cannot be processed3.
4 Intrinsic Evaluations
To show the effectiveness of the Count-Min sketch
in the context of NLP, we perform intrinsic evalu-
ations. The intrinsic evaluations are designed to
measure the error in the approximate counts re-
turned by CMS compared to their true counts. By
keeping the total size of the data structure fixed,
we study the error by varying the width and the
depth of the data structure to find the best setting
of the parameters for textual data sets. We show
that using conservative update (CU) further im-
proves the quality of counts over CM sketch.
4.1 Corpus Statistics
Gigaword corpus (Graff, 2003) and a copy of web
crawled by (Ravichandran et al, 2005) are used
to compute counts of words and word pairs. For
both the corpora, we split the text into sentences,
tokenize and convert into lower-case. We generate
words and word pairs (items) over a sliding win-
dow of size 14. Unlike previous work (Van Durme
3Here, we are only interested in the insertion case.
20
Corpus Sub Giga 50% 100%
set word Web Web
Size
.15 6.2 15 31GB
# of sentences 2.03 60.30 342.68 686.63(Million)
# of words 19.25 858.92 2122.47 4325.03(Million)
Stream Size 0.25 19.25 18.63 39.0510 (Billion)
Stream Size 0.23 25.94 18.79 40.0014 (Billion)
Table 1: Corpus Description
and Lall, 2009b) which assumes exact frequen-
cies for words, we store frequencies of both the
words and word pairs in the CM sketch4. Hence,
the stream size in our case is the total number of
words and word pairs in a corpus. Table 1 gives
the characteristics of the corpora.
Since, it is not possible to compute exact fre-
quencies of all word pairs using conventional main
memory of 8 GB from a large corpus, we use a
subset of 2 million sentences (Subset) from Giga-
word corpus for our intrinsic evaluation. We store
the counts of all words and word pairs (occurring
in a sliding window of length 14) from Subset us-
ing the sketch and also the exact counts.
4.2 Comparing CM and CU counts and
tradeoff between width and depth
To evaluate the amount of over-estimation in CM
and CU counts compared to the true counts, we
first group all items (words and word pairs) with
same true frequency into a single bucket. We then
compute the average relative error in each of these
buckets. Since low-frequent items are more prone
to errors, making this distinction based on fre-
quency lets us understand the regions in which the
algorithm is over-estimating. Average Relative er-
ror (ARE) is defined as the average of absolute dif-
ference between the predicted and the exact value
divided by the exact value over all the items in
each bucket.
ARE = 1N
N
?
i=1
|Exacti ? Predictedi|
Exacti
Where Exact and Predicted denotes values of exact
and CM/CU counts respectively; N denotes the
number of items with same counts in a bucket.
In Fig. 3(a), we fixed the number of counters
to 50 million with four bytes of memory per each
4Though a minor point, it allows to process more text.
counter (thus it only requires 200 MB of main
memory). Keeping the total number of counters
fixed, we try different values of depth (2, 3, 5 and
7) of the sketch array and in each case the width
is set to 50Md . The ARE curves in each case are
shown in Fig. 3(a). There are three main observa-
tions: First it shows that most of the errors occur
on low frequency items. For frequent items, in al-
most all the different runs the ARE is close to zero.
Secondly, it shows that ARE is significantly lower
(by a factor of two) for the runs which use conser-
vative update (CUx run) compared to the runs that
use direct CM sketch (CMx run). The encouraging
observation is that, this holds true for almost all
different (width,depth) settings. Thirdly, in our ex-
periments, it shows that using depth of 3 gets com-
paratively less ARE compared to other settings.
To be more certain about this behavior with re-
spect to different settings of width and depth, we
tried another setting by increasing the number of
counters to 100 million. The curves in 3(b) follow
a pattern which is similar to the previous setting.
Low frequency items are more prone to error com-
pared to the frequent ones and employing conser-
vative update reduces the ARE by a factor of two.
In this setting, depth 3 and 5 do almost the same
and get lowest ARE. In both the experiments, set-
ting the depth to three did well and thus in the rest
of the paper we fix this parameter to three.
Fig. 4 studies the effect of the number of coun-
ters in the sketch (the size of the two-dimensional
sketch array) on the ARE. Using more number of
counters decreases the ARE in the counts. This is
intuitive because, as the length of each row in the
sketch increases, the probability of collision de-
creases and hence the array is more likely to con-
tain true counts. By using 200 million counters,
which is comparable to the length of the stream
230 million (Table. 1), we are able to achieve al-
most zero ARE over all the counts including the
rare ones5. Note that the actual space required
to represent the exact counts is almost two times
more than the memory that we use here because
there are 230 million word pairs and on an aver-
age each word is eight characters long and requires
eight bytes (double the size of an integer). The
summary of this Figure is that, if we want to pre-
serve the counts of low-frequent items accurately,
then we need counters linear in size of stream.
5Even with other datasets we found that using counters
linear in the size of the stream leads to ARE close to zero ?
counts.
21
0 2 4 6 8 10 12
0
0.5
1
1.5
2
2.5
3
3.5
4
Log2 of true frequency counts of words/word?pairs
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM7
CM5
CM3
CM2
CU7
CU5
CU3
CU2
(a) 50M counters
0 2 4 6 8 10 12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Log2 of true frequency counts of words/word?pairs
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM7
CM5
CM3
CM2
CU7
CU5
CU3
CU2
(b) 100M counters
Figure 3: Comparing 50 and 100 million counter models with different (width,depth) settings. The notation CMx represents
the Count-Min Sketch with a depth of ?x? and CUx represents the CM sketch along with conservative update and depth ?x?.
0 2 4 6 8 10 12
0
1
2
3
4
5
6
Log2 of true frequency counts of words/word?pairs
Av
er
ag
e 
Re
lat
ive
 E
rro
r
 
 
20M
50M
100M
200M
Figure 4: Comparing different size models with depth 3
4.3 Evaluating the CU PMI ranking
In this experiment, we compare the word pairs as-
sociation rankings obtained using PMI with CU
and exact counts. We use two kinds of measures,
namely accuracy and Spearman?s correlation, to
measure the overlap in the rankings obtained by
both these approaches.
4.3.1 PointWise Mutual Information
The Pointwise Mutual Information (PMI) (Church
and Hanks, 1989) between two words w1 and w2
is defined as:
PMI(w1, w2) = log2
P (w1, w2)
P (w1)P (w2)
Here, P (w1, w2) is the likelihood that w1 and w2
occur together, and P (w1) and P (w2) are their in-
dependent likelihoods respectively. The ratio be-
tween these probabilities measures the degree of
statistical dependence between w1 and w2.
4.3.2 Description of the metrics
Accuracy is defined as fraction of word pairs that
are found in both rankings to the size of top ranked
word pairs.
Accuracy = |CP-WPs ? EP-WPs||EP-WPs|
Where CP-WPs represent the set of top ranked K
word pairs under the counts stored using the CU
sketch and EP-WPs represent the set of top ranked
word pairs with the exact counts.
Spearman?s rank correlation coefficient (?)
computes the correlation between the ranks of
each observation (i.e. word pairs) on two variables
(that are top N CU-PMI and exact-PMI values).
This measure captures how different the CU-PMI
ranking is from the Exact-PMI ranking.
? = 1? 6
? d2i
F (F 2 ? 1)
Where di is the difference between the ranks of
a word pair in both rankings and F is the number
of items found in both sets.
Intuitively, accuracy captures the number of
word pairs that are found in both the sets and then
Spearman?s correlation captures if the relative or-
der of these common items is preserved in both the
rankings. In our experimental setup, both these
measures are complimentary to each other and
measure different aspects. If the rankings match
exactly, then we get an accuracy of 100% and a
correlation of 1.
4.3.3 Comparing CU PMI ranking
The results with respect to different sized counter
(50, 100 and 200 million) models are shown in Ta-
ble 2. Table 2 shows that having counters linear
22
Counters 50M 100M 200M
Top K Acc ? Acc ? Acc ?
50 .20 -0.13 .68 .95 .92 1.00
100 .18 .31 .77 .80 .96 .95
200 .21 .68 .73 .86 .97 .99
500 .24 .31 .71 .97 .95 .99
1000 .33 .17 .74 .87 .95 .98
5000 .49 .38 .82 .82 .96 .97
Table 2: Evaluating the PMI rankings obtained using CM
Sketch with conservative update (CU) and Exact counts
in size of stream (230M ) results in better rank-
ing (i.e. close to the exact ranking). For example,
with 200M counters, among the top 50 word pairs
produced using the CU counts, we found 46 pairs
in the set returned by using exact counts. The ?
score on those word pairs is 1 means that the rank-
ing of these 46 items is exactly the same on both
CU and exact counts. We see the same phenom-
ena for 200M counters with other Top K values.
While both accuracy and the ranking are decent
with 100M counters, if we reduce the number of
counters to say 50M , the performance degrades.
Since, we are not throwing away any infrequent
items, PMI will rank pairs with low frequency
counts higher (Church and Hanks, 1989). Hence,
we are evaluating the PMI values for rare word
pairs and we need counters linear in size of stream
to get alost perfect ranking. Also, using coun-
ters equal to half the length of the stream is decent.
However, in some NLP problems, we are not inter-
ested in low-frequency items. In such cases, even
using space less than linear in number of coun-
ters would suffice. In our extrinsic evaluations, we
show that using space less than the length of the
stream does not degrades the performance.
5 Extrinsic Evaluations
5.1 Experimental Setup
To evaluate the effectiveness of CU-PMI word
association scores, we infer semantic orientation
(S0) of a word from CU-PMI and Exact-PMI
scores. Given a word, the task of finding the SO
(Turney and Littman, 2002) of the word is to iden-
tify if the word is more likely to be used in positive
or negative sense. We use a similar framework as
used by the authors6 to infer the SO. We take the
seven positive words (good, nice, excellent, posi-
tive, fortunate, correct, and superior) and the nega-
tive words (bad, nasty, poor, negative, unfortunate,
6We compute this score slightly differently. However, our
main focus is to show that CU-PMI scores are useful.
wrong, and inferior) used in (Turney and Littman,
2002) work. The SO of a given word is calculated
based on the strength of its association with the
seven positive words, and the strength of its asso-
ciation with the seven negative words. We com-
pute the SO of a word ?w? as follows:
SO-PMI(W) = PMI(+, w)? PMI(?, w)
PMI(+,W) =
?
p?Pwords
log hits(p, w)hits(p) ? hits(w)
PMI(-,W) =
?
n?Nwords
log hits(n,w)hits(n) ? hits(w)
Where, Pwords and Nwords denote the seven pos-
itive and negative prototype words respectively.
We compute SO score from different sized cor-
pora (Section 4.1). We use the General Inquirer
lexicon7 (Stone et al, 1966) as a benchmark to
evaluate the semantic orientation scores similar to
(Turney and Littman, 2002) work. Words with
multiple senses have multiple entries in the lexi-
con, we merge these entries for our experiment.
Our test set consists of 1619 positive and 1989
negative words. Accuracy is used as an evaluation
metric and is defined as the fraction of number of
correctly identified SO words.
Accuracy = Correctly Identified SO Words ? 100Total SO words
5.2 Results
We evaluate SO of words on three different sized
corpora: Gigaword (GW) 6.2GB, GigaWord +
50% of web data (GW+WB1) 21.2GB and Gi-
gaWord + 100% of web data (GW+WB2) 31GB.
Note that computing the exact counts of all word
pairs on these corpora is not possible using main
memory, so we consider only those pairs in which
one word appears in the prototype list and the
other word appears in the test set.
We compute the exact PMI (denoted using Ex-
act) scores for pairs of test-set words w1 and proto-
type words w2 using the above data-sets. To com-
pute PMI, we count the number of hits of individ-
ual words w1 and w2 and the pair (w1,w2) within a
sliding window of sizes 10 and 14 over these data-
sets. After computing the PMI scores, we compute
SO score for a word using SO-PMI equation from
Section 5.1. If this score is positive, we predict
the word as positive. Otherwise, we predict it as
7The General Inquirer lexicon is freely available at
http://www.wjh.harvard.edu/ inquirer/
23
Model Accuracy window 10 Accuracy window 14
#of counters Mem. Usage GW GW+WB1 GW+WB2 GW GW+WB1 GW+WB2
Exact n/a 64.77 75.67 77.11 64.86 74.25 75.30
500M 2GB 62.98 71.09 72.31 63.21 69.21 70.35
1B 4GB 62.95 73.93 75.03 63.95 72.42 72.73
2B 8GB 64.69 75.86 76.96 65.28 73.94 74.96
Table 3: Evaluating Semantic Orientation of words with different # of counters of CU sketch with increasing amount of data
on window size of 10 and 14. Scores are evaluated using Accuracy metric.
negative. The results on inferring correct SO for
a word w with exact PMI (Exact) are summarized
in Table 3. It (the second row) shows that increas-
ing the amount of data improves the accuracy of
identifying the SO of a word with both the win-
dow sizes. The gain is more prominent when we
add 50% of web data in addition to Gigaword as
we get an increase of more than 10% in accuracy.
However, when we add the remaining 50% of web
data, we only see an slight increase of 1% in accu-
racy8. Using words within a window of 10 gives
better accuracy than window of 14.
Now, we use our CU Sketches of 500 million
(500M ), 1 billion (1B) and 2 billion (2B) coun-
ters to compute CU-PMI. These sketches contain
the number of hits of all words/word pairs (not just
the pairs of test-set and prototype words) within a
window size of 10 and 14 over the whole data-
set. The results in Table 3 show that even with
CU-PMI scores, the accuracy improves by adding
more data. Again we see a significant increase in
accuracy by adding 50% of web data to Gigaword
over both window sizes. The increase in accuracy
by adding the rest of the web data is only 1%.
By using 500M counters, accuracy with CU-
PMI are around 4% worse than the Exact. How-
ever, increasing the size to 1B results in only 2
% worse accuracy compared to the Exact. Go-
ing to 2B counters (8 GB of RAM), results in ac-
curacy almost identical to the Exact. These re-
sults hold almost the same for all the data-sets
and for both the window sizes. The increase in
accuracy comes at expense of more memory Us-
age. However, 8GB main memory is not large as
most of the conventional desktop machines have
this much RAM. The number of 2B counters is
less than the length of stream for all the data-sets.
For GW, GW+WB1 and GW+WB2, 2B counters
are 10, 20 and 30 times smaller than the stream
size. This shows that using counters less than the
stream length does not degrade the performance.
8These results are similar to the results reported in (Tur-
ney and Littman, 2002) work.
The advantage of using Sketch is that it con-
tains counts for all words and word pairs. Suppose
we are given a new word to label it as positive or
negative. We can find its exact PMI in two ways:
First, we can go over the whole corpus and com-
pute counts of this word with positive and nega-
tive prototype words. This procedure will return
PMI in time needed to traverse the whole corpus.
If the corpus is huge, this could be too slow. Sec-
ond option is to consider storing counts of all word
pairs but this is not feasible as their number in-
creases rapidly with increase in data (see Fig. 1).
Therefore, using a CM sketch is a very good al-
ternative which returns the PMI in constant time
by using only 8GB of memory. Additionally, this
Sketch can easily be used for other NLP applica-
tions where we need word-association scores.
6 Conclusion
We have explored the idea of the CM Sketch,
which approximates the frequency of a word pair
in the corpus without explicitly storing the word
pairs themselves. We used the idea of a conserva-
tive update with the CM Sketch to reduce the av-
erage relative error of its approximate counts by
a factor of 2. It is an efficient, small-footprint
method that scales to at least 37 GB of web data
in just 2 billion counters (8 GB main memory). In
our extrinsic evaluations, we found that CU Sketch
is as effective as exact PMI scores.
Word-association scores from CU Sketch can be
used for other NLP tasks like word sense disam-
biguation, speech and character recognition. The
counts stored in CU Sketch can be used to con-
struct small-space randomized language models.
In general, this sketch can be used for any applica-
tion where we want to query a count of an item.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
24
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
?09: Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Moses Charikar, Kevin Chen, and Martin Farach-
colton. 2002. Finding frequent items in data
streams.
K. Church and P. Hanks. 1989. Word Association
Norms, Mutual Information and Lexicography. In
Proceedings of the 27th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 76?83,
Vancouver, Canada, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
756?764, Singapore, August. Association for Com-
putational Linguistics.
Ping Li and Kenneth W. Church. 2007. A sketch algo-
rithm for estimating two-way and multi-way associ-
ations. Comput. Linguist., 33(3).
G. S. Manku and R. Motwani. 2002. Approximate
frequency counts over data streams. In Proceedings
of the 28th International Conference on Very Large
Data Bases.
S. Muthukrishnan. 2005. Data streams: Algorithms
and applications. Foundations and Trends in Theo-
retical Computer Science, 1(2).
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-
scale distributional similarity and entity set expan-
sion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 938?947, Singapore, August. Association
for Computational Linguistics.
S. Patwardhan and E. Riloff. 2006. Learning Domain-
Specific Information Extraction Patterns from the
Web. In Proceedings of the ACL 2006 Workshop on
Information Extraction Beyond the Document.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ?07. ACM.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EM NLP-CoNLL).
Peter D. Turney and Michael L. Littman. 2002.
Unsupervised learning of semantic orientation
from a hundred-billion-word corpus. CoRR,
cs.LG/0212012.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1574?
1579.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
25
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 51?56,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Sketch Techniques for Scaling Distributional Similarity to the Web
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and Suresh Venkatasubramanian
School of Computing
University of Utah
Salt Lake City, UT 84112
{amitg,jags,hal,suresh}@cs.utah.edu
Abstract
In this paper, we propose a memory, space,
and time efficient framework to scale dis-
tributional similarity to the web. We
exploit sketch techniques, especially the
Count-Min sketch, which approximates
the frequency of an item in the corpus
without explicitly storing the item itself.
These methods use hashing to deal with
massive amounts of the streaming text. We
store all item counts computed from 90
GB of web data in just 2 billion coun-
ters (8 GB main memory) of CM sketch.
Our method returns semantic similarity
between word pairs in O(K) time and
can compute similarity between any word
pairs that are stored in the sketch. In our
experiments, we show that our framework
is as effective as using the exact counts.
1 Introduction
In many NLP problems, researchers (Brants et al,
2007; Turney, 2008) have shown that having large
amounts of data is beneficial. It has also been
shown that (Agirre et al, 2009; Pantel et al, 2009;
Ravichandran et al, 2005) having large amounts
of data helps capturing the semantic similarity be-
tween pairs of words. However, computing distri-
butional similarity (Sec. 2.1) between word pairs
from large text collections is a computationally ex-
pensive task. In this work, we consider scaling dis-
tributional similarity methods for computing se-
mantic similarity between words to Web-scale.
The major difficulty in computing pairwise sim-
ilarities stems from the rapid increase in the num-
ber of unique word-context pairs with the size of
text corpus (number of tokens). Fig. 1 shows that
5 10 15 20 25
5
10
15
20
Log2 of # of words
Lo
g 2
 
o
f #
 o
f u
ni
qu
e 
Ite
m
s
 
 
word?context pairs
words
Figure 1: Token Type Curve
the number of unique word-context pairs increase
rapidly compared to the number words when plot-
ted against the number of tokens1. For example,
a 57 million word corpus2 generates 224 thousand
unique words and 15 million unique word-context
pairs. As a result, it is computationally hard to
compute counts of all word-context pairs with a gi-
ant corpora using conventional machines (say with
main memory of 8 GB). To overcome this, Agirre
et al (2009) used MapReduce infrastructure (with
2, 000 cores) to compute pairwise similarities of
words on a corpus of roughly 1.6 Terawords.
In a different direction, our earlier work (Goyal
et al, 2010) developed techniques to make the
computations feasible on a conventional machines
by willing to accept some error in the counts. Sim-
ilar to that work, this work exploits the idea of
Count-Min (CM) sketch (Cormode and Muthukr-
ishnan, 2004) to approximate the frequency of
word pairs in the corpus without explicitly stor-
ing the word pairs themselves. In their, we stored
1Note that the plot is in log-log scale.
2
?Subset? column of Table 1 in Section 5.1
51
counts of all words/word pairs in fixed amount of
main memory. We used conservative update with
CM sketch (referred as CU sketch) and showed
that it reduces the average relative error of its ap-
proximate counts by a factor of two. The approx-
imate counts returned by CU Sketch were used
to compute approximate PMI between word pairs.
We found their that the approximate PMI values
are as useful as the exact PMI values for com-
puting semantic orientation (Turney and Littman,
2002) of words. In addition, our intrinsic evalua-
tions in their showed that the quality of approxi-
mate counts and approximate PMI is good.
In this work, we use CU-sketch to store counts
of items (words, contexts, and word-context pairs)
using fixed amount of memory of 8 GB by using
only 2B counters. These approximate counts re-
turned by CU Sketch are converted into approx-
imate PMI between word-context pairs. The top
K contexts (based on PMI score) for each word
are used to construct distributional profile (DP) for
each word. The similarity between a pair of words
is computed based on the cosine similarity of their
respective DPs.
The above framework of using CU sketch to
compute semantic similarity between words has
five good properties. First, this framework can re-
turn semantic similarity between any word pairs
that are stored in the CU sketch. Second, it can
return the similarity between word pairs in time
O(K). Third, because we do not store items ex-
plicitly, the overall space required is significantly
smaller. Fourth, the additive property of CU
sketch (Sec. 3.2) enables us to parallelize most
of the steps in the algorithm. Thus it can be easily
extended to very large amounts of text data. Fifth,
this easily generalizes to any kind of association
measure and semantic similarity measure.
2 Background
2.1 Distributional Similarity
Distributional Similarity is based on the distribu-
tional hypothesis (Firth, 1968; Harris, 1985) that
words occur in similar contexts tend to be sim-
ilar. The context of a word is represented by
the distributional profile (DP), which contains the
strength of association between the word and each
of the lexical, syntactic, semantic, and/or depen-
dency units that co-occur with it3. The association
3In this work, we only consider lexical units as context.
is commonly measured using conditional proba-
bility, pointwise mutual information (PMI) or log
likelihood ratios. Then the semantic similarity be-
tween two words, given their DPs, is calculated
using similarity measures such as Cosine, ?-skew
divergence, and Jensen-Shannon divergence. In
our work, we use PMI as association measure and
cosine similarity to compute pairwise similarities.
2.2 Large Scale NLP problems
Pantel et al (2009) computed similarity between
500 million word pairs using the MapReduce
framework from a 200 billion word corpus using
200 quad-core nodes. The inaccessibility of clus-
ters for every one has attracted NLP community to
use streaming, and randomized algorithms to han-
dle large amounts of data.
Ravichandran et al (2005) used locality sensi-
tive hash functions for computing word-pair simi-
larities from large text collections. Their approach
stores a enormous matrix of all unique words and
their contexts in main memory which makes it
hard for larger data sets. In our work, we store
all unique word-context pairs in CU sketch with a
pre-defined size4.
Recently, the streaming algorithm paradigm has
been used to provide memory and time-efficient
platform to deal with terabytes of data. For
example, we (Goyal et al, 2009); Levenberg
and Osborne (2009) build approximate language
models and show their effectiveness in SMT. In
(Van Durme and Lall, 2009b), a TOMB Counter
(Van Durme and Lall, 2009a) was used to find the
top-K verbs ?y? with the highest PMI for a given
verb ?x?. The idea of TOMB is similar to CU
Sketch. However, we use CU Sketch because of
its simplicity and attractive properties (see Sec. 3).
In this work, we go one step further, and compute
semantic similarity between word-pairs using ap-
proximate PMI scores from CU sketch.
2.3 Sketch Techniques
Sketch techniques use a sketch vector as a data
structure to store the streaming data compactly in
a small-memory footprint. These techniques use
hashing to map items in the streaming data onto a
small sketch vector that can be easily updated and
queried. These techniques generally process the
input stream in one direction, say from left to right,
4We use only 2 billion counters which takes up to 8 GB
of main memory.
52
without re-processing previous input. The main
advantage of using these techniques is that they
require a storage which is significantly smaller
than the input stream length. A survey by (Rusu
and Dobra, 2007; Cormode and Hadjieleftheriou,
2008) comprehensively reviews the literature.
3 Count-Min Sketch
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data struc-
ture used to store the frequencies of all items in
the input stream.
Given an input stream of items of length N
and user chosen parameters ? and ?, the algorithm
stores the frequencies of all the items with the fol-
lowing guarantees:
? All reported frequencies are within ?N of
true frequencies with probability of atleast ?.
? Space used by the algorithm is O(1? log 1? ).
? Constant time of O(log(1? )) per each update
and query operation.
3.1 CM Data Structure
A Count-Min Sketch with parameters (?,?) is rep-
resented by a two-dimensional array with width w
and depth d :
?
?
?
sketch[1, 1] ? ? ? sketch[1, w]
.
.
.
.
.
.
.
.
.
sketch[d, 1] ? ? ? sketch[d,w]
?
?
?
Among the user chosen parameters, ? controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within this acceptable error. These
values of ? and ? determine the width and depth
of the two-dimensional array respectively. To
achieve the guarantees mentioned in the previous
section, we set w=2? and d=log(1? ). The depth d
denotes the number of pairwise-independent hash
functions employed by the algorithm and there
exists an one-to-one correspondence between the
rows and the set of hash functions. Each of these
hash functions hk:{x1 . . . xN} ? {1 . . . w}, 1 ?
k ? d takes an item from the input stream and
maps it into a counter indexed by the correspond-
ing hash function. For example, h2(x) = 10 indi-
cates that the item ?x? is mapped to the 10th posi-
tion in the second row of the sketch array. These
d hash functions are chosen uniformly at random
from a pairwise-independent family.
Initialize the entire sketch array with zeros.
Update Procedure: When a new item ?x? with
count c arrives5, one counter in each row, as de-
cided by its corresponding hash function, is up-
dated by c. Formally, ?1 ? k ? d
sketch[k,hk(x)]? sketch[k,hk(x)] + c
Query Procedure: Since multiple items can be
hashed to the same counter, the frequency stored
by each counter is an overestimate of the true
count. Thus, to answer the point query, we con-
sider all the positions indexed by the hash func-
tions for the given item and return the minimum
of all these values. The answer to Query(x) is:
c? = mink sketch[k, hk(x)].
Both update and query procedures involve eval-
uating d hash functions. Hence, both these proce-
dures are linear in the number of hash functions. In
our experiments (see Section5), we use d=3 simi-
lar to our earlier work (Goyal et al, 2010). Hence,
the update and query operations take only constant
time.
3.2 Properties
Apart from the advantages of being space efficient
and having constant update and querying time, the
CM sketch has other advantages that makes it at-
tractive for scaling distributional similarity to the
web:
1. Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches.
2. The linearity allows the individual sketches
to be computed independent of each other.
This means that it is easy to implement it in
distributed setting, where each machine com-
putes the sketch over a subset of the corpus.
3.3 Conservative Update
Estan and Varghese introduce the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of networking. This can easily be used with
CM Sketch (CU Sketch) to further improve the es-
timate of a point query. To update an item, w with
frequency c, we first compute the frequency c? of
5In our setting, c is always 1.
53
this item from the existing data structure and the
counts are updated according to: ?1 ? k ? d
sketch[k,hk(x)]? max{sketch[k,hk(x)], c? + c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. This heuristic avoids the unneces-
sary updating of counter values and thus reduces
the error.
4 Efficient Distributional Similarity
To compute distributional similarity efficiently, we
store counts in CU sketch. Our algorithm has three
main steps:
1. Store approximate counts of all words, con-
texts, and word-context pairs in CU-sketch
using fixed amount of counters.
2. Convert these counts into approximate PMI
scores between word-context pairs. Use these
PMI scores to store top K contexts for a word
on the disk. Store these top K context vectors
for every word stored in the sketch.
3. Use cosine similarity to compute the similar-
ity between word pairs using these approxi-
mate top K context vectors constructed using
CU sketch.
5 Word pair Ranking Evaluations
As discussed earlier, the DPs of words are used to
compute similarity between a pair of words. We
used the following four test sets and their corre-
sponding human judgements to evaluate the word
pair rankings.
1. WS-353: WordSimilarity-3536 (Finkelstein
et al, 2002) is a set of 353 word pairs.
2. WS-203: A subset of WS-353 containing 203
word pairs marked according to similarity7
(Agirre et al, 2009).
3. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
4. MC-30: A smaller subset of the RG-65
dataset containing 30 word pairs (Miller and
Charles, 1991).
6http://www.cs.technion.ac.il/ gabr/resources/data/word-
sim353/wordsim353.html
7http://alfonseca.org/pubs/ws353simrel.tar.gz
Each of these data sets come with human ranking
of the word pairs. We rank the word pairs based
on the similarity computed using DPs and evalu-
ate this ranking against the human ranking. We
report the spearman?s rank correlation coefficient
(?) between these two rankings.
5.1 Corpus Statistics
The Gigaword corpus (Graff, 2003) and a copy of
the web crawled by (Ravichandran et al, 2005)
are used to compute counts of all items (Table. 1).
For both the corpora, we split the text into sen-
tences, tokenize, convert into lower-case, remove
punctuations, and collapse each digit to a sym-
bol ?0? (e.g. ?1996? gets collapsed to ?0000?).
We store the counts of all words (excluding num-
bers, and stop words), their contexts, and counts
of word-context pairs in the CU sketch. We de-
fine the context for a given word ?x? as the sur-
rounding words appearing in a window of 2 words
to the left and 2 words to the right. The context
words are concatenated along with their positions
-2, -1, +1, and +2. We evaluate ranking of word
pairs on three different sized corpora: Gigaword
(GW), GigaWord + 50% of web data (GW-WB1),
and GigaWord + 100% of web data (GW-WB2).
Corpus Sub GW GW- GW-
set WB1 WB2
Size
.32 9.8 49 90(GB)
# of sentences 2.00 56.78 462.60 866.02(Million)
Stream Size
.25 7.65 37.93 69.41(Billion)
Table 1: Corpus Description
5.2 Results
We compare our system with two baselines: Ex-
act and Exact1000 which use exact counts. Since
computing the exact counts of all word-context
pairs on these corpora is not possible using main
memory of only 8 GB , we generate context vec-
tors for only those words which appear in the test
set. The former baseline uses all possible contexts
which appear with a test word, while the latter
baseline uses only the top 1000 contexts (based on
PMI value) for each word. In each case, we use
a cutoff (of 10, 60 and 120) on the frequency of
word-context pairs. These cut-offs were selected
based on the intuition that, with more data, you
get more noise, and not considering word-context
pairs with frequency less than 120 might be a bet-
54
Data GW GW-WB1 GW-WB2
Model Frequency cutoff Frequency cutoff Frequency cutoff10 60 120 10 60 120 10 60 120
? ? ?
WS-353
Exact .25 .25 .22 .29 .28 .28 .30 .28 .28
Exact1000 .36 .28 .22 .46 .43 .37 .47 .44 .41
Our Model .39 .28 .22 -0.09 .48 .40 -0.03 .04 .47
WS-203
Exact .35 .36 .33 .38 .38 .37 .40 .38 .38
Exact1000 .49 .40 .35 .57 .55 .47 .56 .56 .52
Our Model .49 .39 .35 -0.08 .58 .47 -0.06 .03 .55
RG-65
Exact .21 .12 .08 .42 .28 .22 .39 .31 .23
Exact1000 .14 .09 .08 .45 .16 .13 .47 .26 .12
Our Model .13 .10 .09 -0.06 .32 .18 -0.05 .08 .31
MC-30
Exact .26 .23 .21 .45 .33 .31 .46 .39 .29
Exact1000 .27 .18 .21 .63 .42 .32 .59 .47 .36
Our Model .36 .20 .21 -0.08 .52 .39 -0.27 -0.29 .52
Table 2: Evaluating word pairs ranking with Exact and CU counts. Scores are evaluated using ? metric.
ter choice than a cutoff of 10. The results are
shown in Table 2
From the above baseline results, first we learn
that using more data helps in better capturing
the semantic similarity between words. Second,
it shows that using top (K) 1000 contexts for
each target word captures better semantic similar-
ity than using all possible contexts for that word.
Third, using a cutoff of 10 is optimal for all differ-
ent sized corpora on all test-sets.
We use approximate counts from CU sketch
with depth=3 and 2 billion (2B) counters (?Our
Model?)8. Based on previous observation, we re-
strict the number of contexts for a target word to
1000. Table 2 shows that using CU counts makes
the algorithm sensitive to frequency cutoff. How-
ever, with appropriate frequency cutoff for each
corpus, approximate counts are nearly as effective
as exact counts. For GW, GW-WB1, and GW-
WB2, the frequency cutoffs of 10, 60, and 120 re-
spectively performed the best. The reason for de-
pendence on frequency cutoffs is due to the over-
estimation of low-frequent items. This is more
pronounced with bigger corpus (GW-WB2) as the
size of CU sketch is fixed to 2B counters and
stream size is much bigger (69.41 billion) com-
pared to GW where the stream size is 7.65 billion.
The advantages of using our model is that the
sketch contains counts for all words, contexts, and
word-context pairs stored in fixed memory of 8
GB by using only 2B counters. Note that it is not
8Our goal is not to build the best distributional similarity
method. It is to show that our framework scales easily to large
corpus and it is as effective as exact method.
feasible to keep track of exact counts of all word-
context pairs since their number increases rapidly
with increase in data (see Fig. 1). We can use our
model to create context vectors of size K for all
possible words stored in the Sketch and computes
semantic similarity between two words in O(K)
time. In addition, the linearity of sketch allows
us to include new incoming data into the sketch
without building the sketch from scratch. Also,
it allows for parallelization using the MapReduce
framework. We can generalize our framework to
any kind of association and similarity measure.
6 Conclusion
We proposed a framework which uses CU Sketch
to scale distributional similarity to the web. It can
compute similarity between any word pairs that
are stored in the sketch and returns similarity be-
tween them in O(K) time. In our experiments, we
show that our framework is as effective as using
the exact counts, however it is sensitive to the fre-
quency cutoffs. In future, we will explore ways to
make this framework robust to the frequency cut-
offs. In addition, we are interested in exploring
this framework for entity set expansion problem.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
55
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
?09: Proceedings of HLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In
ACM Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III,
and Suresh Venkatasubramanian. 2010. Sketching
techniques for Large Scale NLP. In 6th Web as Cor-
pus Workshop in conjunction with NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press, New York.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings of EMNLP, August.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Computational Lin-
guistics, 8:627?633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ?07. ACM.
P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
56
