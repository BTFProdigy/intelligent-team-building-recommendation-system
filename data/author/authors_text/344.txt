Extending MT evaluation tools with translation complexity metrics 
Bogdan BABYCH 
Centre for Translation 
Studies, University of Leeds  
Leeds, UK, LS2 9JT 
bogdan@comp.leeds.ac.uk 
Debbie ELLIOTT 
School of Computing 
University of Leeds  
Leeds, UK, LS2 9JT 
debe@comp.leeds.ac.uk 
Anthony HARTLEY 
Centre for Translation 
Studies, University of Leeds 
Leeds, UK, LS2 9JT 
a.hartley@leeds.ac.uk 
 
Abstract 
In this paper we report on the results of an 
experiment in designing resource-light metrics that 
predict the potential translation complexity of a 
text or a corpus of homogenous texts for state-of-
the-art MT systems. We show that the best 
prediction of translation complexity is given by the 
average number of syllables per word (ASW). The 
translation complexity metrics based on this 
parameter are used to normalise automated MT 
evaluation scores such as BLEU, which otherwise 
are variable across texts of different types. The 
suggested approach makes a fairer comparison 
between the MT systems evaluated on different 
corpora. The translation complexity metric was 
integrated into two automated MT evaluation 
packages ? BLEU and the Weighted N-gram 
model. The extended MT evaluation tools are 
available from the first author?s web site: 
http://www.comp.leeds.ac.uk/bogdan/evalMT.html 
1 Introduction 
Automated evaluation tools for MT systems aim 
at producing scores that are consistent with the 
results of human assessment of translation quality 
parameters, such as adequacy and fluency. 
Automated metrics such as BLEU (Papineni et al, 
2002), RED (Akiba et al 2001), Weighted N-gram 
model (WNM) (Babych, 2004), syntactic relation / 
semantic vector model (Rajman and Hartley, 2001) 
have been shown to correlate closely with scoring 
or ranking by different human evaluation 
parameters. Automated evaluation is much quicker 
and cheaper than human evaluation. 
Another advantage of the scores produced by 
automated MT evaluation tools is that intuitive 
human scores depend on the exact formulation of 
an evaluation task, on the granularity of the 
measuring scale and on the relative quality of the 
presented translation variants: human judges may 
adjust their evaluation scale in order to 
discriminate between slightly better and slightly 
worse variants ? but only those variants which are 
present in the evaluation set. For example, absolute 
figures for a human evaluation of a set which 
includes MT output only are not directly 
comparable with the figures for another evaluation 
which might include MT plus a non-native human 
translation, or several human translations of 
different quality. Because of the instability of this 
intuitive scale, human evaluation figures should be 
treated as relative rather than absolute. They 
capture only a local picture within an evaluated set, 
but not the quality of the presented texts in a larger 
context. Although automated evaluation scores are 
always calibrated with respect to human evaluation 
results, only the relative performance of MT 
systems within one particular evaluation exercise 
provide meaningful information for such 
calibration. 
In this respect, automated MT evaluation scores 
have some added value: they rely on objective 
parameters in the evaluated texts, so their results 
are comparable across different evaluations. 
Furthermore, they are also comparable for 
different types of texts translated by the same MT 
system, which is not the case for human scores. 
For example, automated scores are capable of 
distinguishing improved MT performance on 
easier texts or degraded performance on harder 
texts, so the automated scores also give 
information on whether one collection of texts is 
easier or harder than the other for an MT system: 
the complexity of the evaluation task is directly 
reflected in the evaluation scores. 
However, there may be a need to avoid such 
sensitivity. MT developers and users are often 
more interested in scores that would be stable 
across different types of texts for the same MT 
system, i.e., would reliably characterise a system?s 
performance irrespective of the material used for 
evaluation. Such characterisation is especially 
important for state-of-the-art commercial MT 
systems, which typically target a wide range of 
general-purpose text types and are not specifically 
tuned to any particular genre, like weather reports 
or aircraft maintenance manuals. 
The typical problem of having ?task-dependent? 
evaluation scores (which change according to the 
complexity of the evaluated texts) is that the 
reported scores for different MT systems are not 
directly comparable. Since there is no standard 
collection of texts used for benchmarking all MT 
systems, it is not clear how a system that achieves, 
e.g., BLEUr4n4 1  score 0.556 tested on ?490 
utterances selected from the WSJ? (Cmejrek et al 
2003:89) may be compared to another system 
which achieves, e.g., the BLEUr1n4 score 0.240 
tested on 10,150 sentences from the ?Basic Travel 
Expression Corpus? (Imamura et al, 2003:161). 
Moreover, even if there is no comparison 
involved, there is a great degree of uncertainty in 
how to interpret the reported automated scores. For 
example, BLEUr2n4 0.3668 is the highest score 
for a top MT system if MT performance is 
measured on news reports, but it is a relatively 
poor score for a corpus of e-mails, and a score that 
is still beyond the state-of-the-art for a corpus of 
legal documents. These levels of perfection have to 
be established experimentally for each type of text, 
and there is no way of knowing whether some 
reported automated score is better or worse if a 
new type of text is involved in the evaluation. 
The need to use stable evaluation scores, 
normalised by the complexity of the evaluated 
task, has been recognised in other NLP areas, such 
as anaphora resolution, where the results may be 
relative with regard to a specific evaluation set. So 
?more absolute? figures are obtained if we use 
some measure which quantifies the complexity of 
anaphors to be resolved (Mitkov, 2002). 
MT evaluation is harder than evaluation of other 
NLP tasks, which makes it partially dependent on 
intuitive human judgements about text quality. 
However, automated tools are capable of capturing 
and representing the ?absolute? level of 
performance for MT systems, and this level could 
then be projected into task-dependent figures for 
harder or easier texts. In this respect, there is 
another ?added value? in using automated scores 
for MT evaluation. 
Stable evaluation scores could be achieved if a 
formal measure of a text?s complexity for 
translation could be cheaply computed for a source 
text. Firstly, the score for translation complexity 
allows the user to predict ?absolute? performance 
figures of an MT system on harder or easier texts, 
by computing the ?absolute? evaluation figures and 
the complexity scores for just one type of text. 
Secondly, it lets the user compute ?standardised? 
performance figures for an MT system that do not 
depend on the complexity of a text (they are 
reliably within some relatively small range for any 
type of evaluated texts). 
Designing such standardised evaluation scores 
requires choosing a point of reference for the 
complexity measure: e.g., one may choose an 
                                                   
1
 BLEUrXnY means the BLEU score with produced 
with X reference translations and the maximum size of 
compared N-grams = Y. 
average complexity of texts usually translated by 
MT as the reference point. Then the absolute 
scores for harder or easier texts will be corrected to 
fit the region of absolute scores for texts of average 
complexity. 
In this paper we report on the results of an 
experiment in measuring the complexity of 
translation tasks using resource-light parameters 
such as the average number of syllables per word 
(ASW), which is also used for computing the 
readability of a text. On the basis of these 
parameters we compute normalised BLEU and 
WNM scores which are relatively stable across 
translations produced by the same general-purpose 
MT systems for texts of varying difficulty. We 
suggest that further testing and fine-tuning of the 
proposed approach on larger corpora of different 
text types and using additional source text 
parameters and normalisation techniques can give 
better prediction of translation complexity and 
increase the stability of the normalised MT 
evaluation scores. 
2 Set-up of the experiment 
We compared the results of the human and 
automated evaluation of translations from French 
into English of three different types of texts which 
vary in size and style: an EU whitepaper on child 
and youth policy (120 sentences), a collection of 
36 business and private e-mails and 100 news texts 
from the DARPA 94 MT evaluation corpus (White 
et al, 1994). The translations were produced by 
two leading commercial MT systems. Human 
evaluation results are available for all of the texts, 
with the exception of the news reports translated 
by System-2, which was not part of the DARPA 94 
evaluation. However, the human evaluation scores 
were collected at different times under different 
experimental conditions using different 
formulations of the evaluation tasks, which leads to 
substantial differences between human scores 
across different evaluations, even if the evaluations 
were done at the same time.  
Further, we produced two sets of automated 
scores: BLEUr1n4, which have a high correlation 
with human scores for fluency, and WNM Recall, 
which strongly correlate with human scores for 
adequacy. These scores were produced under the 
same experimental conditions, but they uniformly 
differ for both evaluated systems: BLEU and 
WNM scores were relatively higher for e-mails 
and relatively low for the whitepaper, with the 
news texts coming in between. We interpreted 
these differences as reflecting the relative 
complexity of texts for translation. 
For the French originals of all three sets of texts 
we computed resource-light parameters used in 
standard readability measures (Flesch Reading 
Ease score or Flesch-Kincaid Grade Level score), 
i.e. average sentence length (ASL ? the number of 
words divided by the number of sentences) and 
average number of syllables per word (ASW ? the 
number of syllables divided by the number of 
words). 
We computed Pearson?s correlation coefficient r 
between the automated MT evaluation scores and 
each of the two readability parameters. Differences 
in the ASL parameter were not strongly linked to 
the differences in automated scores, but for the 
ASW parameter a strong negative correlation was 
found. 
Finally, we computed normalised (?absolute?) 
BLEU and WNM scores using the automated 
evaluation results for the DARPA news texts (the 
medium complexity texts) as a reference point. We 
compared the stability of these scores with the 
stability of the standard automated scores by 
computing standard deviations for the different 
types of text. The absolute automated scores can be 
computed on any type of text and they will indicate 
what score is achievable if the same MT system 
runs on DARPA news reports. The normalised 
scores allow the user to make comparisons 
between different MT systems evaluated on 
different texts at different times. In most cases the 
accuracy of the comparison is currently limited to 
the first rounded decimal point of the automated 
score. 
3 Results of human evaluations  
The human evaluation results were produced 
under different experimental conditions. The 
output of the compared systems was evaluated 
each time within a different evaluation set, in some 
cases together with different MT systems, or native 
or non-native human translations. As a result 
human evaluation scores are not comparable across 
different evaluations. 
Human scores available from the DARPA 94 
MT corpus of news reports were the result of a 
comparison of five MT systems (one of which was 
a statistical MT system) and a professional 
(?expert?) human translation. For our experiment 
we used DARPA scores for adequacy and fluency 
for one of the participating systems. 
We obtained human scores for translations of the 
whitepaper and the e-mails from one of our MT 
evaluation projects at the University of Leeds. This 
had involved the evaluation of French-to-English 
versions of two leading commercial MT systems ? 
System 1 and System 2 ? in order to assess the 
quality of their output and to determine whether 
updating the system dictionaries brought about an 
improvement in performance. (An earlier version 
of System 1 also participated in the DARPA 
evaluation.) Although the human evaluations of 
both texts were carried out at the same time, the 
experimental set-up was different in each case. 
The evaluation of the whitepaper for adequacy 
was performed by 20 postgraduate students who 
knew very little or no French. A professional 
human translation of each segment was available 
to the judges as a gold standard reference. Using a 
five-point scale in each case, judgments were 
solicited on adequacy by means of the following 
question: 
?For each segment, read carefully the reference 
text on the left. Then judge how much of the 
same content you can find in the candidate text.? 
Five independent judgments were collected for 
each segment. 
The whitepaper fluency evaluation was 
performed by 8 postgraduate students and 16 
business users under similar experimental 
conditions with the exception that the gold 
standard reference text was not available to the 
judges. The following question was asked: 
?Look carefully at each segment of text and give 
each one a score according to how much you 
think the text reads like fluent English written by 
a native speaker.? 
For e-mails a different quality evaluation 
parameter was used: 26 human judges (business 
users) evaluated the usability (or utility) of the 
translations. We also included translations 
produced by a non-professional, French-speaking 
translator in the evaluation set for e-mails. (This 
was intended to simulate a situation where, in the 
absence of MT, the author of the e-mail would 
have to write in a foreign language (here English); 
we anticipated that the quality would be judged 
lower than the professional, native speaker 
translations.) The non-native translations were 
dispersed anonymously in the data set and so were 
also judged. The following question was asked: 
?Using each reference e-mail on the left, rate the 
three alternative versions on the right according 
to how usable you consider them to be for 
getting business done.? 
Figure 1 and Table 1 summarise the human 
evaluation scores for the two compared MT 
systems. The judges had scored versions of the e-
mails (?em?) and whitepaper (?wp?) produced both 
before and after dictionary update (?DA?), 
although no judge saw the before and after variants 
of the same text. (The scores for the DARPA news 
texts are converted from [0, 1] to [0, 5] scale). 
00.5
1
1.5
2
2.5
3
3.5
4
4.5
5
em-USL wp-FLU wp-ADE news-FLU news-ADE
System-1 Before DA
System-1 After DA
System-2 Before DA
System-2 After DA
Non-native transl.
 
Figure 1. Human evaluation results 
 
 S1 S1da S2 S2da NN 
em [usl] 2.511 3.139 2.35 2.733 4.314 
wp [flu] 3.15 3.47 2.838 3.157  
wp [ade] 3.94 4.077 3.858 3.977  
news [flu] 2.54     
news [ade] 3.945     
Table 1. Human evaluation scores 
It can be inferred from the data that human 
evaluation scores do not allow us to make any 
meaningful comparison of the scores outside a 
particular evaluation experiment, which 
necessarily must be interpreted as relative rather 
than absolute. 
We can see that dictionary update consistently 
improves the performance of both systems, that 
System 1 is slightly better than System 2 in all 
cases, although after dictionary update System 2 is 
capable of reaching the baseline quality of System 
1. However, the usability scores for supposedly 
easier texts (e-mails) are considerably lower than 
the adequacy scores for harder texts (the 
whitepaper), although the experimental set-up for 
adequacy and usability is very similar: both used a 
gold-standard human reference translation. We 
suggest that the presence of a higher quality 
translation done by a human non-native speaker of 
the target language ?over-shadowed? lower quality 
MT output, which dragged down evaluation scores 
for e-mail usability. No such higher quality 
translation was present in the evaluation set for the 
whitepaper adequacy, so the scores went up. 
Therefore, no meaning can be given to any 
absolute value of the evaluation scores across 
different experiments involving intuitive human 
judgements. Only a relative comparison of these 
evaluation scores produced within the same 
experiment is possible. 
4 Results of automated evaluations 
Automated evaluation scores use objective 
parameters, such the number of N-gram matches in 
the evaluated text and in a gold standard reference 
translation. Therefore, these scores are more 
consistent and comparable across different 
evaluation experiments. The comparison of the 
scores indicates the relative complexity of the texts 
for translation. For the output of both MT systems 
under consideration we generated two sets of 
automated evaluation scores: BLEUr1n4 and 
WNM Recall. 
BLEU computes the modified precision of N-
gram matches between the evaluated text and a 
professional human reference translation. It was 
found to produce automated scores, which strongly 
correlate with human judgements about translation 
fluency (Papineni et al, 2002). 
WNM is an extension of BLEU with weights of 
a term?s salience within a given text. As compared 
to BLEU, the WNM recall-based evaluation score 
was found to produce a higher correlation with 
human judgements about adequacy (Babych, 
2004). The salience weights are similar to standard 
tf.idf scores and are computed as follows: ( )
)(
)()(),( /)(log),(
icorp
iidoccorpjidoc
P
NdfNPPjiS ???= ? ,  
where: 
? Pdoc(i,j) is the relative frequency of the word wi in 
the text j; (?Relative frequency? is the number 
of tokens of this word-type divided by the total 
number of tokens). 
? Pcorp-doc(i) is the relative frequency of the same 
word wi in the rest of the corpus, without this 
text; 
? dfi is the number of documents in the corpus 
where the word wi occurs; 
? N is the total number of documents in the corpus. 
? Pcorp(i) is the relative frequency of the word wi in 
the whole corpus, including this particular 
text.  
Figures 2 and 3 and Table 2 summarise the 
automated evaluation scores for the two MT 
systems. 
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
bleu-wp bleu-news bleu-em
S1
S1da
S2
S2da
 
Figure 2. Automated BLEUr1n4 scores 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
wnmR-wp wnmR-news wnmR-em
S1
S1da
S2
S2da
 
Figure 3. Automated WMN Recall scores 
scores S1 S1da S2 S2da 
bleu-wp 0.1874 0.2351 0.1315 0.1701 
bleu-news 0.2831  0.1896  
bleu-em 0.3257 0.3573 0.2006 0.326 
wnmR-wp 0.3247 0.3851 0.2758 0.3172 
wnmR-news 0.3644  0.3439  
wnmR-em 0.3915 0.4256 0.3792 0.4129 
r correlation [flu] [ade/usl]   
bleu-wp 0.9827 0.9453   
bleu-em  0.7872   
wnmR-wp 0.9896 0.9705   
wnmR-em  0.9673   
Table 2. Automated evaluation scores 
It can be seen from the charts that automated 
scores consistently change according to the type of 
the evaluated text: for both evaluated systems 
BLEU and WNM are the lowest for the whitepaper 
texts, which emerge as most complex to translate, 
the news reports are in the middle and the highest 
scores are given to the e-mails, which appear to be 
relatively easy. A similar tendency also holds for 
the system after dictionary update. However, 
technically speaking the compared systems are no 
longer the same, because the dictionary update was 
done individually for each system, so the quality of 
the update is an additional factor in the system?s 
performance ? in addition to the complexity of the 
translated texts. 
The complexity of the translation task is 
integrated into the automated MT evaluation 
scores, but for the same type of texts the scores are 
perfectly comparable. For example, for the 
DARPA news texts, newly generated BLEU and 
WNM scores confirm the observation made, on the 
basis of comparison of the whitepaper and the e-
mail texts, that S1 produces higher translation 
quality than S2, although there is no human 
evaluation experiment where such translations are 
directly compared. 
Thus the automated MT evaluation scores derive 
from both the ?absolute? output quality of an 
evaluated general-purpose MT system and the 
complexity of the translated text. 
5 Readability parameters 
In order to isolate the ?absolute? MT quality and 
to filter out the contribution of the complexity of 
the evaluated text from automated scores, we need 
to find a formal parameter of translation 
complexity which should preferably be resource-
light, so as to be easily computed for any source 
text in any language submitted to an MT system. 
Since automated scores already integrate the 
translation complexity of the evaluated text, we 
can validate such a parameter by its correlation 
with automated MT evaluation scores computed on 
the same set that includes different text types. 
In our experiment, we examined the following 
resource-light parameters for their correlation with 
both automated scores: 
? Flesch Reading Ease score, which rates text on 
a 100-point scale according to how easy it is to 
understand; the score is computed as follows: 
FR = 206.835 ? (1.015 * ASL) ? (84.6 * 
ASW), where: 
ASL is the average sentence length (the 
number of words divided by the number of 
sentences); 
ASW is the average number of syllables per 
word (the number of syllables divided by the 
number of words) 
? Flesch-Kincaid Grade Level score which rates 
texts on US grade-school level and is 
computed as: 
FKGL = (0.39 * ASL) + (11.8 * ASW) ? 
15.59 
? each of the ASL and ASW parameters 
individually. 
Table 3 presents the averaged readability 
parameters for all French original texts used in our 
evaluation experiment and the r correlation 
between these parameters and the corresponding 
automated MT evaluation scores. 
 FR FKGL ASL ASW 
wp 17.3 15.7 19.65 2 
news 27.8 14.7 21.4 1.86 
em 61.44   6.98   9.22 1.608 
r/bleu-S1 0.872 -0.804 -0.641 -0.928 
r/bleu-S2 0.785 -0.701 -0.513 -0.859 
r/wnm-S1 0.92 -0.864 -0.721 -0.963 
r/wnm-S2 0.889 -0.825 -0.669 -0.941 
r Average 0.866 -0.799 -0.636 -0.923 
Table 3. Readability of French originals 
Table 3 shows that the strongest negative 
correlation exists between ASW (average number 
of syllables per word) and the automated 
evaluation scores. Therefore the ASW parameter 
can be used to normalise MT evaluation scores. 
Therefore translation complexity is highly 
dependent on the complexity of the lexicon, which 
is approximated by the ASW parameter. 
The other parameter used to compute readability 
? ASL (average sentence length in words) ? has a 
much weaker influence on the quality of MT, 
which may be due to the fact that local context is 
in many cases sufficient to produce accurate 
translation and the use of the global sentence 
structure in MT analysis is limited. 
6 Normalised evaluation scores 
We used the ASW parameter to normalise the 
automated evaluation scores in order to obtain 
absolute figures for MT performance, where the 
influence of translation complexity is neutralised. 
Normalisation requires choosing some reference 
point ? some average level of translation 
complexity ? to which all other scores for the same 
MT system will be scaled. We suggest using the 
difficulty of the news texts in the DARPA 94 MT 
evaluation corpus as one such ?absolute? reference 
point. Normalised figures obtained on other types 
of texts will mean that if the same general-purpose 
MT system is run on the DARPA news texts, it 
will produce raw BLEU or WNM scores 
approximately equal to the normalised scores. This 
allows users to make a fairer comparison between 
MT systems evaluated on different types of texts. 
We found that for the WNM scores the best 
normalisation can be achieved by multiplying the 
score by the complexity normalisation coefficient 
C, which is the ratio: 
C = ASWevalText/ ASWDARPAnews. 
For BLEU the best normalisation is achieved by 
multiplying the score by C2 (the squared value of 
ASWevalText/ ASWDARPAnews). 
Normalisation makes the evaluation relatively 
stable ? in general, the scores for the same system 
are the same up to the first rounded decimal point. 
Table 4 summarises the normalised automated 
scores for the evaluated systems. 
 
 
C S1 S1da S2 S2da 
bleu-wp 1.156 0.217 0.272 0.152 0.197 
bleu-news 1 0.283  0.19  
bleu-em 0.747 0.243 0.267 0.15 0.244 
wnmR-wp 1.075 0.349 0.414 0.297 0.341 
wnmR-news 1 0.364  0.344  
wnmR-em 0.865 0.338 0.368 0.328 0.357 
Table 4. Normalised BLEU and WNM scores 
The accuracy of the normalisation can be 
measured by standard deviations of the normalised 
scores across texts of different types. We also 
measured the improvement in stability of the 
normalised scores as compared to the stability of 
the raw scores generated on different text types. 
Standard deviation was computed using the 
formula: 
)1(
)( 22
?
?
=
   
nn
xxn
STDEV  
Table 5 summarises standard deviations of the 
raw and normalised automated scores for the e-
mails, whitepaper and news texts. 
 
 
S1 S1da S2 S2da Ave-
rage 
bleu-stdev 0.071 0.086 0.037 0.11 0.076 
N-bleu-stdev 0.033 0.003 0.022 0.033 0.023 
improved *X     3.299 
wnm-stdev 0.034 0.029 0.053 0.068 0.046 
N-wnm-stdev 0.013 0.033 0.024 0.011 0.02 
improved *X     2.253 
Table 5. Standard deviation of BLEU and WNM 
It can be seen from the table that the standard 
deviation of the normalised BLEU scores across 
different text types is 3.3 times smaller; and the 
deviation of the normalised WNM scores is 2.25 
times smaller than for the corresponding raw 
scores. So the normalised scores are much more 
stable than the raw scores across different 
evaluated text types. 
7 Conclusion and future work 
In this paper, we presented empirical evidence 
for the observation that the complexity of an MT 
task influences automated evaluation scores. We 
proposed a method for normalising the automated 
scores by using a resource-light parameter of the 
average number of syllables per word (ASW), 
which relatively accurately approximates the 
complexity of the particular text type for 
translation. 
The fact that the potential complexity of a 
particular text type for translation can be 
accurately approximated by the ASW parameter 
can have an interesting linguistic interpretation. 
The relation between the length of the word and 
the number of its meanings in a dictionary is 
governed by the Menzerath?s law (Koehler, 1993: 
49), which in its most general formulation states 
that there is a negative correlation between the 
length of a language construct and the size of its 
?components? (Menzerath, 1954; Hubey, 1999: 
239). In this particular case the size of a word?s 
components can be interpreted as the number of its 
possible word senses. We suggest that the link 
between ASW and translation difficulty can be 
explained by the fact that the presence of longer 
words with a smaller number of senses requires a 
more precise word sense disambiguation for 
shorter polysemantic words, so the task of word 
sense disambiguation becomes more demanding: 
the choice of very specific senses and the use of 
more precise (often terminological translation 
equivalents) is required. 
Future work will involve empirical testing of this 
suggestion as well as further experiments on 
improving the stability of the normalised scores by 
developing better normalisation methods. We will 
evaluate the proposed approach on larger corpora 
containing different genres, and will investigate 
other possible resource-light parameters, such as 
type/token ratio of the source text or unigram 
entropy, which can predict the complexity of the 
translated text more accurately. Another direction 
of future research is comparison of stability of 
evaluation scores on subsets of the evaluated data 
within one particular text type and across different 
text types. 
Acknowledgments 
We are very grateful for the insightful comments 
of the three anonymous reviewers. 
References  
Y. Akiba, K. Imamura and E. Sumita. 2001. Using 
multiple edit distances to automatically rank 
machine translation output. In "Proc. MT 
Summit VIII". pages 15?20. 
B. Babych. 2004. Weighted N-gram model for 
evaluating Machine Translation output. In 
"Proceedings of the 7th Annual Colloquium for 
the UK Special Interest Group for Computational 
Linguistics". M. Lee, ed., University of 
Birmingham, 6-7 January, 2004. pages 15-22. 
M. Cmejrek, J. Curin and J. Havelka. 2003. Czech-
English Dependency-based Machine 
Translation. In ?Proceedings of the 10th 
Conference of the European Chapter of 
Association for Computational Linguistics 
(EACL 2003)?. April 12th-17th 2003, Budapest, 
Hungary. 
K. Imamura, E. Sumita and Y. Matsumoto. 2003. 
Automatic Construction of Machine Translation 
Knowledge Using Translation Literalness. In 
?Proceedings of the 10th Conference of the 
European Chapter of Association for 
Computational Linguistics (EACL 2003)?. April 
12th-17th 2003, Budapest, Hungary. 
M. Hubey. 1999. Mathematical Foundations of 
Linguistics. Lincom Europa, Muenchen. 
R. Koehler. 1993. Synergetic Linguistics. In 
"Contributions to Quantitative Linguistics", 
R. Koehler and B.B. Rieger (eds.), pages 41-51. 
P. Menzerath. 1954. Die Architektonik des 
deutchen Wortschatzes. Dummler, Bonn. 
R. Mitkov. 2002. Anaphora Resolution. Longman, 
Harlow, UK. 
K. Papineni, S. Roukos, T. Ward, W-J Zhu. 2002 
BLEU: a method for automatic evaluation of ma-
chine translation. In "Proceedings of the 40th 
Annual Meeting of the Association for the 
Computational Linguistics (ACL)", Philadelphia, 
July 2002, pages 311-318. 
M. Rajman and T. Hartley. 2001. Automatically 
predicting MT systems ranking compatible with 
Fluency, Adequacy and Informativeness scores. 
In "Proceedings of the 4th ISLE Workshop on 
MT Evaluation, MT Summit VIII". Santiago de 
Compostela, September 2001. pages. 29-34. 
J. White, T. O?Connell and F. O?Mara. 1994. The 
ARPA MT evaluation methodologies: evolution, 
lessons and future approaches. Procs. 1st 
Conference of the Association for Machine 
Translation in the Americas. Columbia, MD, 
October 1994. 193-205. 
 
ASSIST: Automated semantic assistance for translators
Serge Sharoff, Bogdan Babych
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych}@leeds.ac.uk
Paul Rayson, Olga Mudraya, Scott Piao
UCREL, Computing Department
Lancaster University, LA1 4WA, UK
{p.rayson,o.moudraia,s.piao}@lancs.ac.uk
Abstract
The problem we address in this paper is
that of providing contextual examples of
translation equivalents for words from the
general lexicon using comparable corpora
and semantic annotation that is uniform
for the source and target languages. For
a sentence, phrase or a query expression in
the source language the tool detects the se-
mantic type of the situation in question and
gives examples of similar contexts from
the target language corpus.
1 Introduction
It is widely acknowledged that human transla-
tors can benefit from a wide range of applications
in computational linguistics, including Machine
Translation (Carl and Way, 2003), Translation
Memory (Planas and Furuse, 2000), etc. There
have been recent research on tools detecting trans-
lation equivalents for technical vocabulary in a re-
stricted domain, e.g. (Dagan and Church, 1997;
Bennison and Bowker, 2000). The methodology
in this case is based on extraction of terminology
(both single and multiword units) and alignment
of extracted terms using linguistic and/or statisti-
cal techniques (D?jean et al, 2002).
In this project we concentrate on words from the
general lexicon instead of terminology. The ratio-
nale for this focus is related to the fact that trans-
lation of terms is (should be) stable, while gen-
eral words can vary significantly in their transla-
tion. It is important to populate the terminologi-
cal database with terms that are missed in dictio-
naries or specific to a problem domain. However,
once the translation of a term in a domain has been
identified, stored in a dictionary and learned by
the translator, the process of translation can go on
without consulting a dictionary or a corpus.
In contrast, words from the general lexicon ex-
hibit polysemy, which is reflected differently in
the target language, thus causing the dependency
of their translation on corresponding context. It
also happens quite frequently that such variation
is not captured by dictionaries. Novice translators
tend to rely on dictionaries and use direct trans-
lation equivalents whenever they are available. In
the end they produce translations that look awk-
ward and do not deliver the meaning intended by
the original text.
Parallel corpora consisting of original texts
aligned with their translations offer the possibility
to search for examples of translations in their con-
text. In this respect they provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative: millions of pages of orig-
inal texts are produced daily by native speakers
in major languages, such as English, while trans-
lations are produced by a small community of
trained translators from a small subset of source
texts. The imbalance between original texts and
translations is also reflected in the size of parallel
corpora, which are simply too small for variations
in translation of moderately frequent words. For
instance, frustrate occurs 631 times in 100 million
words of the BNC, i.e. this gives in average about
6 uses in a typical parallel corpus of one million
words.
2 System design
2.1 The research hypothesis
Our research hypothesis is that translators can be
assisted by software which suggests contextual ex-
139
amples in the target language that are semantically
and syntactically related to a selected example in
the source language. To enable greater coverage
we will exploit comparable rather than parallel
corpora.
Our research hypothesis leads us to a number of
research questions:
? Which semantic and syntactic contextual fea-
tures of the selected example in the source
language are important?
? How do we find similar contextual examples
in the target language?
? How do we sort the suggested target lan-
guage contextual examples in order to max-
imise their usefulness?
In order to restrict the research to what is
achievable within the scope of this project, we are
focussing on translation from English to Russian
using a comparable corpus of British and Rus-
sian newspaper texts. Newspapers cover a large
set of clearly identifiable topics that are compara-
ble across languages and cultures. In this project,
we have collected a 200-million-word corpus of
four major British newspapers and a 70-million-
word corpus of three major Russian newspapers
for roughly the same time span (2003-2004).1
In our proposed method, contexts of uses of En-
glish expressions defined by keywords are com-
pared to similar Russian expressions, using se-
mantic classes such as persons, places and insti-
tutions. For instance, the word agreement in the
example the parties were frustratingly close to
an agreement = ??????? ???? ?? ???????? ??????
? ?????????? ?????????? belongs to a seman-
tic class that also includes arrangement, contract,
deal, treaty. In the result, the search for collo-
cates of ??????? (close) in the context of agree-
ment words in Russian gives a short list of mod-
ifiers, which also includes the target: ?? ????????
??????.
2.2 Semantic taggers
In this project, we are porting the Lancaster En-
glish Semantic Tagger (EST) to the Russian lan-
guage. We have reused the existing semantic field
taxonomy of the Lancaster UCREL semantic anal-
ysis system (USAS), and applied it to Russian. We
1Russian newspapers are significantly shorter than their
British counterparts.
have also reused the existing software framework
developed during the construction of a Finnish Se-
mantic Tagger (L?fberg et al, 2005); the main ad-
justments and modifications required for Finnish
were to cope with the Unicode character set (UTF-
8) and word compounding.
USAS-EST is a software system for automatic
semantic analysis of text that was designed at
Lancaster University (Rayson et al, 2004). The
semantic tagset used by USAS was originally
loosely based on Tom McArthur?s Longman Lexi-
con of Contemporary English (McArthur, 1981).
It has a multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories.2
In the ASSIST project, we have been working on
both improving the existing EST and developing a
parallel tool for Russian - Russian Semantic Tag-
ger (RST). We have found that the USAS semantic
categories were compatible with the semantic cat-
egorizations of objects and phenomena in Russian,
as in the following example:3
poor JJ I1.1- A5.1- N5- E4.1- X9.1-
?????? A I1.1- A6.3- N5- O4.2- E4.1-
However, we needed a tool for analysing the
complex morpho-syntactic structure of Russian
words. Unlike English, Russian is a highly in-
flected language: generally, what is expressed in
English through phrases or syntactic structures
is expressed in Russian via morphological in-
flections, especially case endings and affixation.
For this purpose, we adopted a Russian morpho-
syntactic analyser Mystem that identifies word
forms, lemmas and morphological characteristics
for each word. Mystem is used as the equivalent
of the CLAWS part-of-speech (POS) tagger in the
USAS framework. Furthermore, we adopted the
Unicode UTF-8 encoding scheme to cope with the
Cyrillic alphabet. Despite these modifications, the
architecture of the RST software mirrors that of
the EST components in general.
The main lexical resources of the RST include
a single-word lexicon and a lexicon of multi-word
expressions (MWEs). We are building the Russian
lexical resources by exploiting both dictionaries
and corpora. We use readily available resources,
e.g. lists of proper names, which are then se-
2For the full tagset, see http://www.comp.lancs.
ac.uk/ucrel/usas/
3I1.1- = Money: lack; A5.1- = Evaluation: bad; N5- =
Quantities: little; E4.1- = Unhappy; X9.1- = Ability, intel-
ligence: poor; A6.3- = Comparing: little variety; O4.2- =
Judgement of appearance: bad
140
mantically classified. To bootstrap the system, we
have hand-tagged the 3,000 most frequent Russian
words based on a large newspaper corpus. Subse-
quently, the lexicons will be further expanded by
feeding texts from various sources into the RST
and classifying words that remain unmatched. In
addition, we will experiment with semi-automatic
lexicon construction using an existing machine-
readable English-Russian bilingual dictionary to
populate the Russian lexicon by mapping words
from each of the semantic fields in the English lex-
icon in turn. We aim at coverage of around 30,000
single lexical items and up to 9,000 MWEs, com-
pared to the EST which currently contains 54,727
single lexical items and 18,814 MWEs.
2.3 The user interface
The interface is powered by IMS Corpus Work-
bench (Christ, 1994) and is designed to be used in
the day-to-day workflow of novice and practising
translators, so the syntax of the CWB query lan-
guage has been simplified to adapt it to the needs
of the target user community.
The interface implements a search model for
finding translation equivalents in monolingual
comparable corpora, which integrates a number of
statistical and rule-based techniques for extending
search space, translating words and multiword ex-
pressions into the target language and restricting
the number of returned candidates in order to max-
imise precision and recall of relevant translation
equivalents. In the proposed search model queries
can be expanded by generating lists of collocations
for a given word or phrase, by generating sim-
ilarity classes4 or by manual selection of words
in concordances. Transfer between the source
language and target language is done via lookup
in a bilingual dictionary or via UCREL seman-
tic codes, which are common for concepts in both
languages. The search space is further restricted
by applying knowledge-based and statistical fil-
ters (such as part-of-speech and semantic class fil-
ters, IDF filter, etc), by testing the co-occurrence
of members of different similarity classes or by
manually selecting the presented variants. These
procedures are elementary building blocks that are
used in designing different search strategies effi-
cient for different types of translation equivalents
4Simclasses consist of words sharing collocates and are
computed using Singular Value Decomposition, as used by
(Rapp, 2004), e.g. Paris and Strasbourg are produced for
Brussels, or bus, tram and driver for passenger.
and contexts.
The core functionality of the system is intended
to be self-explanatory and to have a shallow learn-
ing curve: in many cases default search parame-
ters work well, so it is sufficient to input a word
or an expression in the source language in or-
der to get back a useful list of translation equiv-
alents, which can be manually checked by a trans-
lator to identify the most suitable solution for a
given context. For example, the word combina-
tion frustrated passenger is not found in the ma-
jor English-Russian dictionaries, while none of the
candidate translations of frustrated are suitable in
this context. The default search strategy for this
phrase is to generate the similarity class for En-
glish words frustrate, passenger, produce all pos-
sible translations using a dictionary and to test co-
occurrence of the resulting Russian words in target
language corpora. This returns a list of 32 Rus-
sian phrases, which follow the pattern of ?annoyed
/ impatient / unhappy + commuter / passenger /
driver?. Among other examples the list includes
an appropriate translation ??????????? ????????
(?unsatisfied passenger?).
The following example demonstrates the sys-
tem?s ability to find equivalents when there is
a reliable context to identify terms in the two
languages. Recent political developments in
Russia produced a new expression ?????????????
?????????? (?representative of president?), which
is as yet too novel to be listed in dictionaries.
However, the system can help to identify the peo-
ple that perform this duty, translate their names
to English and extract the set of collocates that
frequently appear around their names in British
newspapers, including Putin?s personal envoy and
Putin?s regional representative, even if no specific
term has been established for this purpose in the
British media.
As words cannot be translated in isolation and
their potential translation equivalents also often
consist of several words, the system detects not
only single-word collocates, but also multiword
expressions. For instance, the set of Russian
collocates of ?????????? (bureaucracy) includes
???????? (Brussels), which offers a straightfor-
ward translation into English and has such mul-
tiword collocates as red tape, which is a suitable
contextual translation for ??????????.
More experienced users can modify default pa-
rameters and try alternative strategies, construct
141
their own search paths from available basic build-
ing blocks and store them for future use. Stored
strategies comprise several elementary stages but
are executed in one go, although intermediate re-
sults can also be accessed via the ?history? frame.
Several search paths can be tried in parallel and
displayed together, so an optimal strategy for a
given class of phrases can be more easily identi-
fied.
Unlike Machine Translation, the system does
not translate texts. The main thrust of the sys-
tem lies in its ability to find several target language
examples that are relevant to the source language
expression. In some cases this results in sugges-
tions that can be directly used for translating the
source example, while in other cases the system
provides hints for the translator about the range of
target language expressions beyond what is avail-
able in bilingual dictionaries. Even if the preci-
sion of the current version is not satisfactory for an
MT system (2-3 suitable translations out of 30-50
suggested examples), human translators are able
to skim through the suggested set to find what is
relevant for the given translation task.
3 Conclusions
The set of tools is now under further development.
This involves an extension of the English seman-
tic tagger, development of the Russian tagger with
the target lexical coverage of 90% of source texts,
designing the procedure for retrieval of semanti-
cally similar situations and completing the user in-
terface. Identification of semantically similar sit-
uations can be improved by the use of segment-
matching algorithms as employed in Example-
Based MT and translation memories (Planas and
Furuse, 2000; Carl and Way, 2003).
There are two main applications of the pro-
posed methodology. One concerns training trans-
lators and advanced foreign language (FL) learn-
ers to make them aware of the variety of transla-
tion equivalents beyond the set offered by the dic-
tionary. The other application pertains to the de-
velopment of tools for practising translators. Al-
though the Russian language is not typologically
close to English and uses another writing system
which does not allow easy identification of cog-
nates, Russian and English belong to the same
Indo-European family and the contents of Rus-
sian and English newspapers reflect the same set
of topics. Nevertheless, the application of this
research need not be restricted to the English-
Russian pair only. The methodology for multilin-
gual processing of monolingual comparable cor-
pora, first tested in this project, will provide a
blueprint for the development of similar tools for
other language combinations.
Acknowledgments
The project is supported by two EPSRC grants:
EP/C004574 for Lancaster, EP/C005902 for Leeds.
References
Peter Bennison and Lynne Bowker. 2000. Designing a
tool for exploiting bilingual comparable corpora. In
Proceedings of LREC 2000, Athens, Greece.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Oliver Christ. 1994. A modular and flexible archi-
tecture for an integrated corpus query system. In
COMPLEX?94, Budapest.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING 2002.
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka
Juntunen, Asko Nyk?nen, and Krista Varantola.
2005. A semantic tagger for the Finnish language.
In Proceedings of the Corpus Linguistics 2005 con-
ference.
Tom McArthur. 1981. Longman Lexicon of Contem-
porary English. Longman.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of LREC 2004, pages 395?398.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proceedings of the workshop on Be-
yond Named Entity Recognition Semantic labelling
for NLP tasks in association with LREC 2004, pages
7?12.
142
Extending the BLEU MT Evaluation Method with Frequency Weightings  
Bogdan Babych 
Centre for Translation Studies 
University of Leeds 
Leeds, LS2 9JT, UK 
bogdan@comp.leeds.ac.uk 
Anthony Hartley 
Centre for Translation Studies 
University of Leeds 
Leeds, LS2 9JT, UK 
a.hartley@leeds.ac.uk 
 
Abstract 
We present the results of an experiment 
on extending the automatic method of 
Machine Translation evaluation BLUE 
with statistical weights for lexical items, 
such as tf.idf scores. We show that this 
extension gives additional information 
about evaluated texts; in particular it al-
lows us to measure translation Adequacy, 
which, for statistical MT systems, is often 
overestimated by the baseline BLEU 
method. The proposed model uses a sin-
gle human reference translation, which 
increases the usability of the proposed 
method for practical purposes. The model 
suggests a linguistic interpretation which 
relates frequency weights and human in-
tuition about translation Adequacy and 
Fluency. 
1. Introduction 
Automatic methods for evaluating different as-
pects of MT quality ? such as Adequacy, Fluency 
and Informativeness ? provide an alternative to 
an expensive and time-consuming process of 
human MT evaluation. They are intended to yield 
scores that correlate with human judgments of 
translation quality and enable systems (machine 
or human) to be ranked on this basis. Several 
such automatic methods have been proposed in 
recent years. Some of them use human reference 
translations, e.g., the BLEU method (Papineni et 
al., 2002), which is based on comparison of 
N-gram models in MT output and in a set of hu-
man reference translations. 
However, a serious problem for the BLEU 
method is the lack of a model for relative impor-
tance of matched and mismatched items. Words 
in text usually carry an unequal informational 
load, and as a result are of differing importance 
for translation. It is reasonable to expect that the 
choices of right translation equivalents for certain 
key items, such as expressions denoting principal 
events, event participants and relations in a text 
are more important in the eyes of human evalua-
tors then choices of function words and a syntac-
tic perspective for sentences. Accurate rendering 
of these key items by an MT system boosts the 
quality of translation. Therefore, at least for 
evaluation of translation Adequacy (Fidelity), the 
proper choice of translation equivalents for im-
portant pieces of information should count more 
than the choice of words which are used for 
structural purposes and without a clear translation 
equivalent in the source text. (The latter may be 
more important for Fluency evaluation). 
The problem of different significance of N-
gram matches is related to the issue of legitimate 
variation in human translations, when certain 
words are less stable than others across inde-
pendently produced human translations. BLEU 
accounts for legitimate translation variation by 
using a set of several human reference transla-
tions, which are believed to be representative of 
several equally acceptable ways of translating 
any source segment. This is motivated by the 
need not to penalise deviations from the set of N-
grams in a single reference, although the re-
quirement of multiple human references makes 
automatic evaluation more expensive. 
However, the ?significance? problem is not di-
rectly addressed by the BLEU method. On the 
one hand, the matched items that are present in 
several human references receive the same 
weights as items found in just one of the refer-
ences. On the other hand the model of legitimate 
translation variation cannot fully accommodate 
the issue of varying degrees of ?salience? for 
matched lexical items, since alternative syn-
onymic translation equivalents may also be 
highly significant for an adequate translation 
from the human perspective (Babych and Hart-
ley, 2004). Therefore it is reasonable to suggest 
that introduction of a model which approximates 
intuitions about the significance of the matched 
N-grams will improve the correlation between 
automatically computed MT evaluation scores 
and human evaluation scores for translation Ade-
quacy. 
In this paper we present the result of an ex-
periment on augmenting BLEU N-gram compari-
son with statistical weight coefficients which 
capture a word?s salience within a given docu-
ment: the standard tf.idf measure used in the vec-
tor-space model for Information Retrieval (Salton 
and Leck, 1968) and the S-score proposed for 
evaluating MT output corpora for the purposes of 
Information Extraction (Babych et al, 2003). 
Both scores are computed for each term in each 
of the 100 human reference translations from 
French into English available in DARPA-94 MT 
evaluation corpus (White et al, 1994). 
The proposed weighted N-gram model for MT 
evaluation is tested on a set of translations by 
four different MT systems available in the 
DARPA corpus, and is compared with the results 
of the baseline BLEU method with respect to 
their correlation with human evaluation scores.  
The scores produced by the N-gram model 
with tf.idf and S-Score weights are shown to be 
consistent with baseline BLEU evaluation results 
for Fluency and outperform the BLEU scores for 
Adequacy (where the correlation for the S-score 
weighting is higher). We also show that the 
weighted model may still be reliably used if there 
is only one human reference translation for an 
evaluated text. 
Besides saving cost, the ability to dependably 
work with a single human translation has an addi-
tional advantage: it is now possible to create Re-
call-based evaluation measures for MT, which 
has been problematic for evaluation with multiple 
reference translations, since only one of the 
choices from the reference set is used in transla-
tion (Papineni et al 2002:314). Notably, Recall 
of weighted N-grams is found to be a good esti-
mation of human judgements about translation 
Adequacy. Using weighted N-grams is essential 
for predicting Adequacy, since correlation of Re-
call for non-weighted N-grams is much lower. 
It is possible that other automatic methods 
which use human translations as a reference may 
also benefit from an introduction of an explicit 
model for term significance, since so far these 
methods also implicitly assume that all words are 
equally important in human translation, and use 
all of them, e.g., for measuring edit distances 
(Akiba et al 2001; 2003).  
The weighted N-gram model has been imple-
mented as an MT evaluation toolkit (which in-
cludes a Perl script, example files and 
documentation). It computes evaluation scores 
with tf.idf and S-score weights for translation 
Adequacy and Fluency. The toolkit is available at 
http://www.comp.leeds.ac.uk/bogdan/evalMT.html
2. Set-up of the experiment 
The experiment used French?English transla-
tions available in the DARPA-94 MT evaluation 
corpus. The corpus contains 100 French news 
texts (each text is about 350 words long) trans-
lated into English by 5 different MT systems: 
?Systran?, ?Reverso?, ?Globalink?, ?Metal?, 
?Candide? and scored by human evaluators; there 
are no human scores for ?Reverso?, which was 
added to the corpus on a later stage. The corpus 
also contains 2 independent human translations 
of each text. Human evaluation scores are avail-
able for each of the 400 texts translated by the 4 
MT systems for 3 parameters of translation qual-
ity: ?Adequacy?, ?Fluency? and ?Informative-
ness?. The Adequacy (Fidelity) scores are given 
on a 5-point scale by comparing MT with a hu-
man reference translation. The Adequacy pa-
rameter captures how much of the original 
content of a text is conveyed, regardless of how 
grammatically imperfect the output might be. 
The Fluency scores (also given on a 5-point 
scale) determine intelligibility of MT without 
reference to the source text, i.e., how grammati-
cal and stylistically natural the translation ap-
pears to be. The Informativeness scores (which 
we didn?t use for our experiment) determine 
whether there is enough information in MT out-
put to enable evaluators to answer multiple-
choice questions on its content (White, 2003:237) 
In the first stage of the experiment, each of the 
two sets of human translations was used to com-
pute tf.idf and S-scores for each word in each of 
the 100 texts. The tf.idf score was calculated as: 
tf.idf(i,j) = (1 + log (tfi,j)) log (N / dfi), 
if tfi,j ? 1; where:  
? tfi,j is the number of occurrences of the 
word wi in the document dj; 
? dfi is the number of documents in the cor-
pus where the word wi occurs; 
?  N is the total number of documents in the 
corpus. 
The S-score was calculated as: ( )
)(
)()(),( /)(log),(
icorp
iidoccorpjidoc
P
NdfNPP
jiS
???= ?  
where: 
? Pdoc(i,j) is the relative frequency of the 
word in the text; (?Relative frequency? is 
the number of tokens of this word-type 
divided by the total number of tokens). 
? Pcorp-doc(i) is the relative frequency of the 
same word in the rest of the corpus, with-
out this text; 
? (N ? df(i)) / N is the proportion of texts in 
the corpus, where this word does not oc-
cur (number of texts, where it is not 
found,  divided by number of texts in the 
corpus); 
? Pcorp(i) is the relative frequency of the 
word in the whole corpus, including this 
particular text.  
In the second stage we carried out N-gram based 
MT evaluation, measuring Precision and Recall 
of N-grams in MT output using a single human 
reference translation. N-gram counts were ad-
justed with the tf.idf weights and S-scores for 
every matched word. The following procedure 
was used to integrate the S-scores / tf.idf scores 
for a lexical item into N-gram counts. For every 
word in a given text which received an S-score 
and tf.idf score on the basis of the human refer-
ence corpus, all counts for the N-grams contain-
ing this word are increased by the value of the 
respective score (not just by 1, as in the baseline 
BLEU approach). 
The original matches used for BLEU and the 
weighted matches are both calculated. The fol-
lowing changes have been made to the Perl script 
of the BLEU tool: apart from the operator which 
increases counts for every matched N-gram $ngr 
by 1, i.e.: 
$ngr .= $words[$i+$j] . " "; 
$$hashNgr{$ngr}++;  
the following code was introduced: 
[?] 
$WORD = $words[$i+$j]; 
$WEIGHT = 0; 
if(exists 
  $WordWeight{$TxtN}{$WORD}){ 
    $WEIGHT= 
     $WordWeight{$TxtN}{$WORD}; 
} 
 
$ngr .= $words[$i+$j] . " "; 
$$hashNgr{$ngr}++; 
 
$$hashNgrWEIGHTED{$ngr}+= $WEIGHT; 
[?] 
? where the hash data structure:  
   $WordWeight{$TxtN}{$WORD}=$WEIGHT 
represents the table of tf.idf scores or S-scores for 
words in every text in the corpus. 
The weighted N-gram evaluation scores of 
Precision, Recall and F-measure may be pro-
duced for a segment, for a text or for a corpus of 
translations generated by an MT system. 
In the third stage of the experiment the 
weighted Precision and Recall scores were tested 
for correlation with human scores for the same 
texts and compared to the results of similar tests 
for standard BLEU evaluation. 
Finally we addressed the question whether the 
proposed MT evaluation method allows us to use 
a single human reference translation reliably. In 
order to assess the stability of the weighted 
evaluation scores with a single reference, two 
runs of the experiment were carried out. The first 
run used the ?Reference? human translation, 
while the second run used the ?Expert? human 
translation (each time a single reference transla-
tion was used). The scores for both runs were 
compared using a standard deviation measure.   
3. The results of the MT evaluation with 
frequency weights 
With respect to evaluating MT systems, the cor-
relation for the weighted N-gram model was 
found to be stronger, for both Adequacy and Flu-
ency, the improvement being highest for Ade-
quacy. These results are due to the fact that the 
weighted N-gram model gives much more accu-
rate predictions about the statistical MT system 
?Candide?, whereas the standard BLEU approach 
tends to over-estimate its performance for trans-
lation Adequacy. 
Table 1 present the baseline results for non-
weighted Precision, Recall and F-score. It shows 
the following figures: 
? Human evaluation scores for Adequacy and 
Fluency (the mean scores for all texts produced 
by each MT system);  
? BLEU scores produced using 2 human refer-
ence translations and the default script settings 
(N-gram size = 4); 
? Precision, Recall and F-score for the weighted 
N-gram model produced with 1 human refer-
ence translation and N-gram size = 4. 
? Pearson?s correlation coefficient r for Preci-
sion, Recall and F-score correlated with human 
scores for Adequacy and Fluency r(2) (with 2 
degrees of freedom) for the sets which include 
scores for the 4 MT systems. 
The scores at the top of each cell show the results 
for the first run of the experiment, which used the 
?Reference? human translation; the scores at the 
bottom of the cells represent the results for the 
second run with the ?Expert? human translation. 
 
System 
[ade] / [flu] 
BLEU 
[1&2]  
Prec. 
1/2 
Recall 
1/2 
Fscore 
1/2 
CANDIDE 
0.677 / 0.455 
0.3561 0.4068 
0.4012 
0.3806 
0.3790 
0.3933 
0.3898 
GLOBALINK 
0.710 / 0.381 
0.3199 0.3429 
0.3414 
0.3465 
0.3484 
0.3447 
0.3449 
MS 
0.718 / 0.382 
0.3003 0.3289 
0.3286 
0.3650 
0.3682 
0.3460 
0.3473 
REVERSO 
NA / NA 
0.3823 0.3948 
0.3923 
0.4012 
0.4025 
0.3980 
0.3973 
SYSTRAN 
0.789 / 0.508 
0.4002 0.4029 
0.3981 
0.4129 
0.4118 
0.4078 
0.4049 
Corr r(2) with 
[ade] ? MT 
0.5918 
 
0.1809 
0.1871 
0.6691 
0.6988 
0.4063 
0.4270 
Corr r(2) with 
[flu] ? MT 
0.9807 
 
0.9096 
0.9124 
0.9540 
0.9353 
0.9836 
0.9869
Table 1. Baseline non-weighted scores. 
 
Table 2 summarises the evaluation scores for 
BLEU as compared to tf.idf weighted scores, and 
Table 3 summarises the same scores as compared 
to S-score weighed evaluation. 
 
 
System 
[ade] / [flu] 
BLEU 
[1&2]  
Prec. 
(w) 1/2 
Recall 
(w) 1/2 
Fscore 
(w) 1/2 
CANDIDE 
0.677 / 0.455 
0.3561 0.5242 
0.5176 
0.3094 
0.3051 
0.3892 
0.3839 
GLOBALINK 
0.710 / 0.381 
0.3199 0.4905 
0.4890 
0.2919 
0.2911 
0.3660 
0.3650 
MS 
0.718 / 0.382 
0.3003 0.4919 
0.4902 
0.3083 
0.3100 
0.3791 
0.3798 
REVERSO 
NA / NA 
0.3823 0.5336 
0.5342 
0.3400 
0.3413 
0.4154 
0.4165 
SYSTRAN 
0.789 / 0.508 
0.4002 0.5442 
0.5375 
0.3521 
0.3491 
0.4276 
0.4233 
Corr r(2) with 
[ade] ? MT 
0.5918 
 
0.5248 
0.5561 
0.8354 
0.8667 
0.7691 
0.8119 
Corr r(2) with 
[flu] ? MT 
0.9807 
 
0.9987 
0.9998
0.8849 
0.8350 
0.9408 
0.9070 
Table 2. BLEU vs tf.idf weighted scores. 
 
System 
[ade] / [flu] 
BLEU 
[1&2]  
Prec. 
(w) 1/2 
Recall 
(w) 1/2 
Fscore 
(w) 1/2 
CANDIDE 
0.677 / 0.455 
0.3561 0.5034 
0.4982 
0.2553 
0.2554 
0.3388 
0.3377 
GLOBALINK 
0.710 / 0.381 
0.3199 0.4677 
0.4672 
0.2464 
0.2493 
0.3228 
0.3252 
MS 
0.718 / 0.382 
0.3003 0.4766 
0.4793 
0.2635 
0.2679 
0.3394 
0.3437 
REVERSO 
NA / NA 
0.3823 0.5204 
0.5214 
0.2930 
0.2967 
0.3749 
0.3782 
SYSTRAN 
0.789 / 0.508 
0.4002 0.5314 
0.5218 
0.3034 
0.3022 
0.3863 
0.3828 
Corr r(2) with 
[ade] ? MT 
0.5918 
 
0.6055 
0.6137 
0.9069 
0.9215
0.8574 
0.8792 
Corr r(2) with 
[flu] ? MT 
0.9807 
 
0.9912 
0.9769 
0.8022 
0.7499 
0.8715 
0.8247 
Table 3. BLEU vs S-score weights. 
 
It can be seen from the table that there is a 
strong positive correlation between the baseline 
BLEU scores and human scores for Fluency: 
r(2)=0.9807, p <0.05. However, the correlation 
with Adequacy is much weaker and is not statis-
tically significant: r(2)= 0.5918, p >0.05. The 
most serious problem for BLEU is predicting 
scores for the statistical MT system Candide, 
which was judged to produce relatively fluent, 
but largely inadequate translation. For other MT 
systems (developed with the knowledge-based 
MT architecture) the scores for Adequacy and 
Fluency are consistent with each other: more flu-
ent translations are also more adequate. BLEU 
scores go in line with Candide?s Fluency scores, 
but do not account for its Adequacy scores. 
When Candide is excluded from the evaluation 
set, r correlation goes up, but it is still lower than 
the correlation for Fluency and remains statisti-
cally insignificant: r(1)=0.9608, p > 0.05. There-
fore, the baseline BLEU approach fails to 
consistently predict scores for Adequacy. 
Correlation figures between non-weighted N-
gram counts and human scores are similar to the 
results for BLEU: the highest and statistically 
significant correlation is between the F-score and 
Fluency: r(2)=0.9836, p<0.05, r(2)=0.9869, 
p<0.01, and there is somewhat smaller and statis-
tically significant correlation with Precision. This 
confirms the need to use modified Precision in 
the BLEU method that also in certain respect in-
tegrates Recall. 
The proposed weighted N-gram model outper-
forms BLEU and non-weighted N-gram evalua-
tion in its ability to predict Adequacy scores: 
weighted Recall scores have much stronger cor-
relation with Adequacy (which for MT-only 
evaluation is still statistically insignificant at the 
level p<0.05, but come very close to that point: 
t=3.729 and t=4.108; the required value for 
p<0.05 is t=4.303). 
Correlation figures for S-score-based weights 
are higher than for tf.idf weights (S-score: r(2)= 
0.9069, p > 0.05; r(2)= 0.9215, p > 0.05, tf.idf 
score: r(2)= 0.8354, p >0.05; r(2)= 0.8667, p 
>0.05). 
The improvement in the accuracy of evalua-
tion for the weighted N-gram model can be illus-
trated by the following example of translating the 
French sentence: 
ORI-French: Les trente-huit chefs d'entre-
prise mis en examen dans le dossier ont d?j? 
fait l'objet d'auditions, mais trois d'entre eux 
ont ?t? confront?s, mercredi, dans la foul?e de 
la confrontation "politique". 
English translations of this sentence by the 
knowledge-based system Systran and statistical 
MT system Candide have an equal number of 
matched unigrams (highlighted in italic), there-
fore conventional unigram Precision and Recall 
scores are the same for both systems. However, 
for each translation two of the matched unigrams 
are different (underlined) and receive different 
frequency weights (shown in brackets): 
MT ?Systran?:  
The thirty-eight heads (tf.idf=4.605; S=4.614) of 
undertaking put in examination in the file already 
were the subject of hearings, but three of them 
were confronted, Wednesday, in the tread of "po-
litical" confrontation (tf.idf=5.937; S=3.890). 
Human translation ?Expert?:  
The thirty-eight heads of companies ques-
tioned in the case had already been heard, but 
three of them were brought together Wednes-
day following the "political" confrontation. 
MT ?Candide?:  
The thirty-eight counts of company put into con-
sideration in the case (tf.idf=3.719; S=2.199) al-
ready had (tf.idf=0.562; S=0.000) the object of 
hearings, but three of them were checked, 
Wednesday, in the path of confrontal "political." 
(In the human translation the unigrams matched 
by the Systran output sentence are in italic, those 
matched by the Candide sentence are in bold). 
It can be seen from this example that the uni-
grams matched by Systran have higher term fre-
quency weights (both tf.idf and S-scores):  
heads (tf.idf=4.605;S=4.614)  
confrontation (tf.idf=5.937;S=3.890)
The output sentence of Candide instead 
matched less salient unigrams: 
case (tf.idf=3.719;S=2.199)
had (tf.idf=0.562;S=0.000)  
Therefore for the given sentence weighted uni-
gram Recall (i.e., the ability to avoid under-
generation of salient unigrams) is higher for 
Systran than for Candide (Table 4): 
 Systran Candide 
R 0.6538 0.6538 
R * tf.idf 0.5332 0.4211 
R * S-score 0.5517 0.3697 
   
P 0.5484 0.5484 
P * tf.idf 0.7402 0.9277 
P * S-score 0.7166 0.9573 
Table 4. Recall, Precision, and weighted scores  
 
Weighted Recall scores capture the intuition that 
the translation generated by Systran is more ade-
quate than the one generated by Candide, since it 
preserves more important pieces of information. 
On the other hand, weighted Precision scores 
are higher for Candide. This is due to the fact that 
Systran over-generates (doesn?t match in the hu-
man translation) much more ?exotic?, unordinary 
words, which on average have higher cumulative 
salience scores, e.g., undertaking, exami-
nation, confronted, tread ? vs. the 
corresponding words ?over-generated? by Can-
dide: company, consideration, 
checked, path. In some respect higher 
weighted precision can be interpreted as higher 
Fluency of the Candide?s output sentence, which 
intuitively is perceived as sounding more natu-
rally (although not making much sense). 
On the level of corpus statistics the weighted 
Recall scores go in line with Adequacy, and 
weighted Precision scores (as well as the Preci-
sion-based BLEU scores) ? with Fluency, which 
confirms such interpretation of weighted Preci-
sion and Recall scores in the example above. On 
the other hand, Precision-based scores and non-
weighted Recall scores fail to capture Adequacy. 
The improvement in correlation for weighted 
Recall scores with Adequacy is achieved by re-
ducing overestimation for the Candide system, 
moving its scores closer to human judgements 
about its quality in this respect. However, this is 
not completely achieved: although in terms of 
Recall weighted by the S-scores Candide is cor-
rectly ranked below MS (and not ahead of it, as 
with the BLEU scores), it is still slightly ahead of 
Globalink, contrary to human evaluation results. 
For both methods ? BLEU and the Weighted 
N-gram evaluation ? Adequacy is found to be 
harder to predict than Fluency. This is due to the 
fact that there is no good linguistic model of 
translation adequacy which can be easily formal-
ised. The introduction of S-score weights may be 
a useful step towards developing such a model, 
since correlation scores with Adequacy are much 
better for the Weighted N-gram approach than 
for BLEU. 
Also from the linguistic point of view, S-score 
weights and N-grams may only be reasonably 
good approximations of Adequacy, which in-
volves a wide range of factors, like syntactic and 
semantic issues that cannot be captured by N-
gram matches and require a thesaurus and other 
knowledge-based extensions. Accurate formal 
models of translation variation may also be use-
ful for improving automatic evaluation of Ade-
quacy. 
The proposed evaluation method also pre-
serves the ability of BLEU to consistently predict 
scores for Fluency: Precision weighted by tf.idf 
scores has the strongest positive correlation with 
this aspect of MT quality, which is slightly better 
than the values for BLEU; (S-score: r(2)= 
0.9912, p<0.01; r(2)= 0.9769, p<0.05; tf.idf 
score: r(2)= 0.9987, p<0.001; r(2)= 0.9998, 
p<0.001). 
The results suggest that weighted Precision 
gives a good approximation of Fluency. Similar 
results with non-weighted approach are only 
achieved if some aspect of Recall is integrated 
into the evaluation metric (either as modified pre-
cision, as in BLEU, or as an aspect of the F-
score). Weighted Recall (especially with S-
scores) gives a reasonably good approximation of 
Adequacy. 
On the one hand using 1 human reference with 
uniform results is essential for our methodology, 
since it means that there is no more ?trouble with 
Recall? (Papineni et al, 2002:314) ? a system?s 
ability to avoid under-generation of N-grams can 
now be reliably measured. On the other hand, 
using a single human reference translation in-
stead of multiple translations will certainly in-
crease the usability of N-gram based MT 
evaluation tools. 
The fact that non-weighted F-scores also have 
high correlation with Fluency suggests a new 
linguistic interpretation of the nature of these two 
quality criteria: it is intuitively plausible that Flu-
ency subsumes, i.e. presupposes Adequacy (simi-
larly to the way the F-score subsumes Recall, 
which among all other scores gives the best cor-
relation with Adequacy). The non-weighted F-
score correlates more strongly with Fluency than 
either of its components: Precision and Recall; 
similarly Adequacy might make a contribution to 
Fluency together with some other factors. It is 
conceivable that people need adequate transla-
tions (or at least translations that make sense) in 
order to be able to make judgments about natu-
ralness, or Fluency.  
Being able to make some sense out of a text 
could be the major ground for judging Adequacy: 
sensible mistranslations in MT are relatively rare 
events. This may be the consequence of a princi-
ple similar to the ?second law of thermodynam-
ics? applied to text structure, ? in practice it is 
much rarer to some alternative sense to be cre-
ated (even if the number of possible error types 
could be significant), than to destroy the existing 
sense in translation, so the majority of inadequate 
translations are just nonsense. However, in con-
trast to human translation, fluent mistranslations 
in MT are even rarer than disfluent ones, accord-
ing to the same principle. A real difference in 
scores is made by segments which make sense 
and may or may not be fluent, and things which 
do not make any sense and about which it is hard 
to tell whether they are fluent. 
This suggestion may be empirically tested: if 
Adequacy is a necessary precondition for Flu-
ency, there should be a greater inter-annotator 
disagreement in Fluency scores on texts or seg-
ments which have lower Adequacy scores. This 
will be a topic of future research. 
We note that for the DARPA corpus the corre-
lation scores presented are highest if the evalua-
tion unit is an entire corpus of translations 
produced by an MT system, and for text-level 
evaluation, correlation is much lower. A similar 
observation was made in (Papineni et al, 2002: 
313). This may be due to the fact that human 
judges are less consistent, especially for puzzling 
segments that do not fit the scoring guidelines, 
like nonsense segments for which it is hard to 
decide whether they are fluent or even adequate. 
However, this randomness is leveled out if the 
evaluation unit increases in size ? from the text 
level to the corpus level.  
Automatic evaluation methods such as BLEU 
(Papineni et al, 2002), RED (Akiba et al, 2001), 
or the weighted N-gram model proposed here 
may be more consistent in judging quality as 
compared to human evaluators, but human judg-
ments remain the only criteria for meta-
evaluating the automatic methods. 
4. Stability of weighted evaluation scores 
In this section we investigate how reliable is the 
use of a single human reference translation. The 
stability of the scores is central to the issue of 
computing Recall and reducing the cost of auto-
matic evaluation. We also would like to compare 
the stability of our results with the stability of the 
baseline non-weighted N-gram model using a 
single reference. 
In this stage of the experiment we measured 
the changes that occur for the scores of MT sys-
tems if an alternative reference translation is used 
? both for the baseline N-gram counts and for the 
weighted N-gram model. Standard deviation was 
computed for each pair of evaluation scores pro-
duced by the two runs of the system with alterna-
tive human references. An average of these 
standard deviations is the measure of stability for 
a given score. The results of these calculations 
are presented in Table 5. 
 systems StDev-
basln 
StDev-
tf.idf 
StDev-
S-score 
P candide 0.004 0.0047 0.0037 
 globalink 0.0011 0.0011 0.0004 
 ms 0.0002 0.0012 0.0019 
 reverso 0.0018 0.0004 0.0007 
 systran 0.0034 0.0047 0.0068 
 AVE SDEV 0.0021 0.0024 0.0027 
R candide 0.0011 0.003 0.0001 
 globalink 0.0013 0.0006 0.0021 
 ms 0.0023 0.0012 0.0031 
 reverso 0.0009 0.0009 0.0026 
 systran 0.0008 0.0021 0.0008 
 AVE SDEV 0.0013 0.0016 0.0017 
F candide 0.0025 0.0037 0.0008 
 globalink 0.0001 0.0007 0.0017 
 ms 0.0009 0.0005 0.003 
 reverso 0.0005 0.0008 0.0023 
 systran 0.0021 0.003 0.0025 
 AVE SDEV 0.0012 0.0018 0.0021 
Table 5. Stability of scores 
 
Standard deviation for weighted scores is gener-
ally slightly higher, but both the baseline and the 
weighted N-gram approaches give relatively sta-
ble results: the average standard deviation was 
not greater than 0.0027, which means that both 
will produce reliable figures with just a single 
human reference translation (although interpreta-
tion of the score with a single reference should be 
different than with multiple references). 
Somewhat higher standard deviation figures 
for the weighted N-gram model confirm the sug-
gestion that a word?s importance for translation 
cannot be straightforwardly derived from the 
model of the legitimate translation variation im-
plemented in BLEU and needs the salience 
weights, such as tf.idf or S-scores. 
5. Conclusion and future work  
The results for weighted N-gram models have a 
significantly higher correlation with human intui-
tive judgements about translation Adequacy and 
Fluency than the baseline N-gram evaluation 
measures which are used in the BLEU MT 
evaluation toolkit. This shows that they are a 
promising direction of research. Future work will 
apply our approach to evaluating MT into lan-
guages other than English, extending the experi-
ment to a larger number of MT systems built on 
different architectures and to larger corpora. 
However, the results of the experiment may 
also have implications for MT development: sig-
nificance weights may be used to rank the rela-
tive ?importance? of translation equivalents. At 
present all MT architectures (knowledge-based, 
example-based, and statistical) treat all transla-
tion equivalents equally, so MT systems cannot 
dynamically prioritise rule applications, and 
translations of the central concepts in texts are 
often lost among excessively literal translations 
of less important concepts and function words. 
For example, for statistical MT significance 
weights of lexical items may indicate which 
words have to be introduced into the target text 
using the translation model for source and target 
languages, and which need to be brought there by 
the language model for the target corpora. Simi-
lar ideas may be useful for the Example-based 
and Rule-based MT architectures. The general 
idea is that different pieces of information ex-
pressed in the source text are not equally impor-
tant for translation: MT systems that have no 
means for prioritising this information often in-
troduce excessive information noise into the tar-
get text by literally translating structural 
information, etymology of proper names, collo-
cations that are unacceptable in the target lan-
guage, etc. This information noise often obscures 
important translation equivalents and prevents 
the users from focusing on the relevant bits. MT 
quality may benefit from filtering out this exces-
sive information as much as from frequently rec-
ommended extension of knowledge sources for 
MT systems. The significance weights may 
schedule the priority for retrieving translation 
equivalents and motivate application of compen-
sation strategies in translation, e.g., adding or 
deleting implicitly inferable information in the 
target text, using non-literal strategies, such as 
transposition or modulation (Vinay and Darbel-
net, 1995). Such weights may allow MT systems 
to make an approximate distinction between sali-
ent words which require proper translation 
equivalents and structural material both in the 
source and in the target texts. Exploring applica-
bility of this idea to various MT architectures is 
another direction for future research. 
Acknowledgments 
We are very grateful for the insightful comments 
of the three anonymous reviewers. 
References 
Akiba, Y., K. Imamura and E. Sumita. 2001. Using mul-
tiple edit distances to automatically rank machine 
translation output. In Proc. MT Summit VIII. p. 15?
20. 
Akiba, Y., E. Sumita, H. Nakaiwa, S. Yamamoto and 
H.G. Okuno. 2003. Experimental Comparison of MT 
Evaluation Methods: RED vs. BLEU. In Proc. MT 
Summit IX, URL: http://www.amtaweb.org/summit/ 
MTSummit/ FinalPapers/55-Akiba-final.pdf. 
Babych, B., A. Hartley and E. Atwell. 2003. Statistical 
Modelling of MT output corpora for Information Ex-
traction. In: Proceedings of the Corpus Linguistics 
2003 conference, Lancaster University (UK), 28 - 31 
March 2003, pp. 62-70. 
Babych, B. and A. Hartley. 2004. Modelling legitimate 
translation variation for automatic evaluation of MT 
quality. In: Proceedings of LREC 2004 (forthcoming). 
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2002 
BLEU: a method for automatic evaluation of machine 
translation. Proceedings of the 40th Annual Meeting of 
the Association for the Computational Linguistics 
(ACL), Philadelphia, July 2002, pp. 311-318. 
Salton, G. and M.E. Lesk. 1968. Computer evaluation of 
indexing and text processing. Journal of the ACM, 
15(1) , 8-36. 
Vinay, J.P. and J.Darbelnet. 1995. Comparative stylistics 
of French and English : a methodology for translation 
/ translated and edited by Juan C. Sager, M.-J. Hamel. 
J. Benjamins Pub., Amsterdam, Philadelphia. 
White, J., T. O?Connell and F. O?Mara. 1994. The 
ARPA MT evaluation methodologies: evolution, les-
sons and future approaches. Proceedings of the 1st 
Conference of the Association for Machine Transla-
tion in the Americas. Columbia, MD, October 1994. 
pp. 193-205. 
White, J. 2003. How to evaluate machine translation. In: 
H. Somers. (Ed.) Computers and Translation: a trans-
lator?s guide. Ed. J. Benjamins B.V., Amsterdam, 
Philadelphia, pp. 211-244. 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739?746,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using comparable corpora
to solve problems difficult for human translators
Serge Sharoff, Bogdan Babych, Anthony Hartley
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych,a.hartley}@leeds.ac.uk
Abstract
In this paper we present a tool that uses
comparable corpora to find appropriate
translation equivalents for expressions that
are considered by translators as difficult.
For a phrase in the source language the
tool identifies a range of possible expres-
sions used in similar contexts in target lan-
guage corpora and presents them to the
translator as a list of suggestions. In the
paper we discuss the method and present
results of human evaluation of the perfor-
mance of the tool, which highlight its use-
fulness when dictionary solutions are lack-
ing.
1 Introduction
There is no doubt that both professional and
trainee translators need access to authentic data
provided by corpora. With respect to polyse-
mous lexical items, bilingual dictionaries list sev-
eral translation equivalents for a headword, but
words taken in their contexts can be translated
in many more ways than indicated in dictionar-
ies. For instance, the Oxford Russian Dictionary
(ORD) lacks a translation for the Russian expres-
sion ????????????? ????? (?comprehensive an-
swer?), while the Multitran Russian-English dic-
tionary suggests that it can be translated as ir-
refragable answer. Yet this expression is ex-
tremely rare in English; on the Internet it occurs
mostly in pages produced by Russian speakers.
On the other hand, translations for polysemous
words are too numerous to be listed for all pos-
sible contexts. For example, the entry for strong
in ORD already has 57 subentries and yet it fails
to mention many word combinations frequent in
the British National Corpus (BNC), such as strong
{feeling, field, opposition, sense, voice}. Strong
voice is also not listed in the Oxford French, Ger-
man or Spanish Dictionaries.
There has been surprisingly little research on
computational methods for finding translation
equivalents of words from the general lexicon.
Practically all previous studies have concerned
detection of terminological equivalence. For in-
stance, project Termight at AT&T aimed to de-
velop a tool for semi-automatic acquisition of
termbanks in the computer science domain (Da-
gan and Church, 1997). There was also a study
concerning the use of multilingual webpages to
develop bilingual lexicons and termbanks (Grefen-
stette, 2002). However, neither of them concerned
translations of words from the general lexicon. At
the same time, translators often experience more
difficulty in dealing with such general expressions
because of their polysemy, which is reflected dif-
ferently in the target language, thus causing the
dependency of their translation on the correspond-
ing context. Such variation is often not captured
by dictionaries.
Because of their importance, words from the
general lexicon are studied by translation re-
searchers, and comparable corpora are increas-
ingly used in translation practice and training
(Varantola, 2003). However, such studies are
mostly confined to lexicographic exercises, which
compare the contexts and functions of potential
translation equivalents once they are known, for
instance, absolutely vs. assolutamente in Italian
(Partington, 1998). Such studies do not pro-
vide a computational model for finding appropri-
ate translation equivalents for expressions that are
not listed or are inadequate in dictionaries.
Parallel corpora, conisting of original texts and
739
their exact translations, provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative. Many of them are in the
range of a few million words, which is simply too
small to account for variations in translation of
moderately frequent words. Those that are a bit
larger, such as the Europarl corpus, are restricted
in their domain. For instance, all of the 14 in-
stances of strong voice in the English section of
Europarl are used in the sense of ?the opinion of
a political institution?. At the same time the BNC
contains 46 instances of strong voice covering sev-
eral different meanings.
In this paper we propose a computational
method for using comparable corpora to find trans-
lation equivalents for source language expressions
that are considered as difficult by trainee or pro-
fessional translators. The model is based on de-
tecting frequent multi-word expressions (MWEs)
in the source and target languages and finding a
mapping between them in comparable monolin-
gual corpora, which are designed in a similar way
in the two languages.
The described methodology is implemented in
ASSIST, a tool that helps translators to find solu-
tions for difficult translation problems. The tool
presents the results as lists of translation sugges-
tions (usually 50 to 100 items) ordered alphabeti-
cally or by their frequency in target language cor-
pora. Translators can skim through these lists and
identify an example which is most appropriate in
a given context.
In the following sections we outline our ap-
proach, evaluate the output of the prototype of AS-
SIST and discuss future work.
2 Finding translations in comparable
corpora
The proposed model finds potential translation
equivalents in four steps, which include
1. expansion of words in the original expression
using related words;
2. translation of the resultant set using existing
bilingual dictionaries;
3. further expansion of the set using related
words in the target language;
4. filtering of the set according to expressions
frequent in the target language corpus.
In this study we use several comparable cor-
pora for English and Russian, including large ref-
erence corpora (the BNC and the Russian Refer-
ence Corpus) and corpora of major British and
Russian newspapers. All corpora used in the study
are quite large, i.e. the size of each corpus is in
the range of 100-200 million words (MW), so that
they provide enough evidence to detect such col-
locations as strong voice and clear defiance.
Although the current study is restricted to the
English-Russian pair, the methodology does not
rely on any particular language. It can be ex-
tended to other languages for which large com-
parable corpora, POS-tagging and lemmatisation
tools, and bilingual dictionaries are available. For
example, we conducted a small study for transla-
tion between English and German using the Ox-
ford German Dictionary and a 200 MW German
corpus derived from the Internet (Sharoff, 2006).
2.1 Query expansion
The problem with using comparable corpora to
find translation equivalents is that there is no ob-
vious bridge between the two languages. Unlike
aligned parallel corpora, comparable corpora pro-
vide a model for each individual language, while
dictionaries, which can serve as a bridge, are inad-
equate for the task in question, because the prob-
lem we want to address involves precisely transla-
tion equivalents that are not listed there.
Therefore, a specific query needs first to be
generalised in order to then retrieve a suitable
candidate from a set of candidates. One way
to generalise the query is by using similarity
classes, i.e. groups of words with lexically simi-
lar behaviour. In his work on distributional sim-
ilarity (Lin, 1998) designed a parser to identify
grammatical relationships between words. How-
ever, broad-coverage parsers suitable for process-
ing BNC-like corpora are not available for many
languages. Another, resource-light approach treats
the context as a bag of words (BoW) and detects
the similarity of contexts on the basis of colloca-
tions in a window of a certain size, typically 3-4
words, e.g. (Rapp, 2004). Even if using a parser
can increase precision in identification of contexts
in the case of long-distance dependencies (e.g. to
cook Alice a whole meal), we can find a reason-
able set of relevant terms returned using the BoW
approach, cf. the results of human evaluation for
English and German by (Rapp, 2004).
740
For each source word s0 we produce a list of
similar words: ?(s0) = s1, . . . , sN (in our tool
we use N = 20 as the cutoff). Since lists of dis-
tributionally words can contain words irrelevant to
the source word, we filter them to produce a more
reliable similarity class S(s0) using the assump-
tion that the similarity classes of similar words
have common members:
?w ? S(s0), w ? ?(s0)&w ?
?
?(si)
This yields for experience the following similar-
ity class: knowledge, opportunity, life, encounter,
skill, feeling, reality, sensation, dream, vision,
learning, perception, learn.1 Even if there is no
requirement in the BoW approach that words in
the similarity class are of the same part of speech,
it happens quite frequently that most words have
the same part of speech because of the similarity
of contexts.
2.2 Query translation and further expansion
In the next step we produce a translation class by
translating all words from the similarity class into
the target language using a bilingual dictionary
(T (w) for the translation of w). Then for Step 3
we have two options: a full translation class (TF )
and a reduced one (TR).
TF consists of similarity classes produced for
all translations: S(T (S(s0))). However, this
causes a combinatorial explosion. If a similarity
class contains N words (the average figure is 16)
and a dictionary lists on average M equivalents
for a source word (the average figure is 11), this
procedure outputs on average M ? N2 words in
the full translation class. For instance, the com-
plete translation class for experience contains 998
words. What is worse, some words from the full
translation class do not refer to the domain im-
plied in the original expression because of the am-
biguity of the translation operation. For instance,
the word dream belongs to the similarity class of
experience. Since it can be translated into Rus-
sian as ?????? (?fairy-tale?), the latter Russian word
will be expanded in the full translation class with
words referring to legends and stories. In the later
stages of the project, word sense disambiguation
in corpora could improve precision of translation
classes. However at the present stage we attempt
to trade the recall of the tool for greater precision
by translating words in the source similarity class,
1Ordered according to the score produced by the Singular
Value Decomposition method as implemented by Rapp.
and generating the similarity classes of transla-
tions only for the source word:
TR(s0) = S(T (s0)) ? T (S(s0)).
This reduces the class of experience to 128 words.
This step crucially relies on a wide-coverage
machine readable dictionary. The bilingual dictio-
nary resources we use are derived from the source
file for the Oxford Russian Dictionary, provided
by OUP.
2.3 Filtering equivalence classes
In the final step we check all possible combina-
tions of words from the translation classes for their
frequency in target language corpora.
The number of elements in the set of theoreti-
cally possible combinations is usually very large:
?
Ti, where Ti is the number of words in the trans-
lation class of each word of the original MWE.
This number is much larger than the set of word
combinations which is found in the target lan-
guage corpora. For instance, daunting experience
has 202,594 combinations for the full translation
class of daunting experience and 6,144 for the re-
duced one. However, in the target language cor-
pora we can find only 2,256 collocations with fre-
quency > 2 for the full translation class and 92 for
the reduced one.
Each theoretically possible combination is gen-
erated and looked up in a database of MWEs
(which is much faster than querying corpora for
frequencies of potential collocations). The MWE
database was pre-compiled from corpora using a
method of filtering, similar to part-of-speech fil-
tering suggested in (Justeson and Katz, 1995): in
corpora each N-gram of length 2, 3 and 4 tokens
was checked against a set of filters.
However, instead of pre-defined patterns for en-
tire expressions our filtering method uses sets of
negative constraints, which are usually applied to
the edges of expressions. This change boosts re-
call of retrieved MWEs and allows us to use the
same set of patterns for MWEs of different length.
The filter uses constraints for both lexical and
part-of-speech features, which makes configura-
tion specifications more flexible.
The idea of applying a negative feature filter
rather than a set of positive patterns is based on
the observation that it is easier to describe unde-
sirable features than to enumerate complete lists of
patterns. For example, MWEs of any length end-
ing with a preposition are undesirable (particles in
741
British news Russian news
no of words 217,394,039 77,625,002
REs in filter 25 18
2-grams 6,361,596 5,457,848
3-grams 14,306,653 11,092,908
4-grams 19,668,956 11,514,626
Table 1: MWEs in News Corpora
phrasal verbs, which are desirable, are tagged dif-
ferently by the Tree Tagger, so there is no problem
with ambiguity here). Our filter captures this fact
by having a negative condition for the right edge of
the pattern (regular expression /_IN$/), rather than
enumerating all possible configurations which do
not contain a preposition at the end. In this sense
the filter is permissive: everything that is not ex-
plicitly forbidden is allowed, which makes the de-
scription more economical.
The same MWE database is used for check-
ing frequencies of multiword collocates for cor-
pus queries. For this task, candidate N-grams in
the vicinity of searched patterns are filtered us-
ing the same regular expression grammar of MWE
constraints, and then their corpus frequency is
checked in the database. Thus scores for mul-
tiword collocates can be computed from contin-
gency tables similarly to single-word collocates.
In addition, only MWEs with a frequency
higher than 1 are stored in the database. This fil-
ters out most expressions that co-occur by chance.
Table 1 gives an overview of the number of MWEs
from the news corpus which pass the filter. Other
corpora used in ASSIST (BNC and RRC) yield
similar results. MWE frequencies for each corpus
can be checked individually or joined together.
3 Evaluation
There are several attributes of our system which
can be evaluated, and many of them are crucial
for its efficient use in the workflow of professional
translators, including: usability, quality of final so-
lutions, trade-off between adequacy and fluency
across usable examples, precision and recall of po-
tentially relevant suggestions, as well as real-text
evaluation, i.e. ?What is the coverage of difficult
translation problems typically found in a text that
can be successfully tackled??
In this paper we focus on evaluating the quality
of potentially relevant translation solutions, which
is the central point for developing and calibrat-
ing our methodology. The evaluation experiment
discussed below was specifically designed to as-
sess the usefulness of translation suggestions gen-
erated by our tool ? in cases where translators
have doubts about the usefulness of dictionary so-
lutions. In this paper we do not evaluate other
equally important aspects of the system?s func-
tionality, which will be the matter of future re-
search.
3.1 Set-up of the experiment
For each translation direction we collected ten ex-
amples of possibly recalcitrant translation prob-
lems ? words or phrases whose translation is not
straightforward in a given context. Some of these
examples were sent to us by translators in response
to our request for difficult cases. For each exam-
ple, which we included in the evaluation kit, the
word or phrase either does not have a translation in
ORD (which is a kind of a baseline standard ref-
erence for Russian translators), or its translation
has significantly lower frequency in a target lan-
guage corpus in comparison to the frequency of
the source expression. If an MWE is not listed in
available dictionaries, we produced compositional
(word-for-word) translations using ORD. In order
to remove a possible anti-dictionary bias from our
experiment, we also checked translations in Mul-
titran, an on-line translation dictionary, which was
often quoted as one of the best resources for trans-
lation from and into Russian.
For each translation problem five solutions were
presented to translators for evaluation. One or two
of these solutions were taken from a dictionary
(usually from Multitran, and if available and dif-
ferent, from ORD). The other suggestions were
manually selected from lists of possible solutions
returned by ASSIST. Again, the criteria for se-
lection were intuitive: we included those sugges-
tions which made best sense in the given context.
Dictionary suggestions and the output of ASSIST
were indistinguishable in the questionnaires to the
evaluators. The segments were presented in sen-
tence context and translators had an option of pro-
viding their own solutions and comments. Ta-
ble 2 shows one of the questions sent to evalua-
tors. The problem example is ?????? ?????????
(?precise programme?), which is presented in the
context of a Russian sentence with the following
(non-literal) translation This team should be put
together by responsible politicians, who have a
742
Problem example
?????? ?????????, as in
??????? ??? ??????? ?????? ?????????????
????, ??????? ?????? ????????? ?????? ??
???????.
Translation suggestions Score
clear plan
clear policy
clear programme
clear strategy
concrete plan
Your suggestion ? (optional)
Table 2: Example of an entry in questionnaire
clear strategy for resolving the current crisis. The
third translation equivalent (clear programme) in
the table is found in the Multitran dictionary (ORD
offers no translation for ?????? ?????????). The
example was included because clear programme
is much less frequent in English (2 examples in the
BNC) in comparison to ?????? ????????? in Rus-
sian (70). Other translation equivalents in Table 2
are generated by ASSIST.
We then asked professional translators affiliated
to a translator?s association (identity witheld at this
stage) to rate these five potential equivalents using
a five-point scale:
5 = The suggestion is an appropriate translation
as it is.
4 = The suggestion can be used with some minor
amendment (e.g. by turning a verb into a par-
ticiple).
3 = The suggestion is useful as a hint for an-
other, appropriate translation (e.g. suggestion
elated cannot be used, but its close synonym
exhilarated can).
2 = The suggestion is not useful, even though it is
still in the same domain (e.g. fear is proposed
for a problem referring to hatred).
1 = The suggestion is totally irrelevant.
We received responses from eight translators.
Some translators did not score all solutions, but
there were at least four independent judgements
for each of the 100 translation variants. An exam-
ple of the combined answer sheet for all responses
to the question from Table 2 is given in Table 3 (t1,
Translation t1 t2 t3 t4 t5 ?
clear plan 5 5 3 4 4 0.84
clear policy 5 5 3 4 4 0.84
clear programme 5 5 3 4 4 0.84
clear strategy 5 5 5 5 5 0.00
concrete plan 1 5 3 3 5 1.67
Best Dict 5 5 3 4 4 0.84
Best Syst 5 5 5 5 5 0.00
Table 3: Scores to translation equivalents
t2,. . . denote translators; the dictionary translation
is clear programme).
3.2 Interpretation of the results
The results were surprising in so far as for the ma-
jority of problems translators preferred very differ-
ent translation solutions and did not agree in their
scores for the same solutions. For instance, con-
crete plan in Table 3 received the score 1 from
translator t1 and 5 from t2.
In general, the translators very often picked up
on different opportunities presented by the sug-
gestions from the lists, and most suggestions were
equally legitimate ways of conveying the intended
content, cf. the study of legitimate translation vari-
ation with respect to the BLEU score in (Babych
and Hartley, 2004). In this respect it may be unfair
to compute average scores for each potential solu-
tion, since for most interesting cases the scores do
not fit into the normal distribution model. So aver-
aging scores would mask the potential usability of
really inventive solutions.
In this case it is more reasonable to evaluate
two sets of solutions ? the one generated by AS-
SIST and the other found in dictionaries ? but not
each solution individually. In order to do that for
each translation problem the best scores given by
each translator in each of these two sets were se-
lected. This way of generalising data characterises
the general quality of suggestion sets, and exactly
meets the needs of translators, who collectively get
ideas from the presented sets rather than from in-
dividual examples. This also allows us to mea-
sure inter-evaluator agreement on the dictionary
set and the ASSIST set, for instance, via computing
the standard deviation ? of absolute scores across
evaluators (Table 3). This appeared to be a very
informative measure for dictionary solutions.
In particular, standard deviation scores for the
dictionary set (threshold ? = 0.5) clearly split
743
Agreement: ? for dictionary ? 0.5
Example Dict ASSIST
Ave ? Ave ?
political upheaval 4.83 0.41 4.67 0.82
Disagreement: ? for dictionary >0.5
Example Dict ASSIST
Ave ? Ave ?
clear defiance 4.14 0.90 4.60 0.55
Table 4: Examples for the two groups
Agreement: ? for dictionary ? 0.5
Sub-group Dict ASSIST
Ave ? Ave ?
Agreement E?R 4.73 0.46 4.47 0.80
Agreement R?E 4.90 0.23 4.52 0.60
Agreement?All 4.81 0.34 4.49 0.70
Disagreement: ? for dictionary >0.5
Sub-group Dict ASSIST
Ave ? Ave ?
Disagreement E?R 3.63 1.08 3.98 0.85
Disagreement R?E 3.90 1.02 3.96 0.73
Disagreement?All 3.77 1.05 3.97 0.79
Table 5: Averages for the two groups
our 20 problems into two distinct groups: the first
group below the threshold contains 8 examples,
for which translators typically agree on the qual-
ity of dictionary solutions; and the second group
above the threshold contains 12 examples, for
which there is less agreement. Table 4 shows some
examples from both groups and Table 5 presents
average evaluation scores and standard deviation
figures for both groups.
Overall performance on all 20 examples is the
same for the dictionary responses and for the sys-
tem?s responses: average of the mean top scores
is about 4.2 and average standard deviation of the
scores is 0.8 in both cases (for set-best responses).
This shows that ASSIST can reach the level of
performance of a combination of two authoritative
dictionaries for MWEs, while for its own transla-
tion step it uses just a subset of one-word transla-
tion equivalents from ORD. However, there is an-
other side to the evaluation experiment. In fact, we
are less interested in the system?s performance on
all of these examples than on those examples for
which there is greater disagreement among trans-
lators, i.e. where there is some degree of dissatis-
faction with dictionary suggestions.
012345impin
ge
politic
al uph
eaval
contro
versia
l plan
defus
e tens
ions
?????
?????
??? ?
????
?????
?????
?????
 ????
?????
?????
??
?????
?????????
?????
??
?????
?????
Figure 1: Agreement scores: dictionary
Interestingly, dictionary scores for the agree-
ment group are always higher than 4, which means
that whenever translators agreed on the dictionary
scores they were usually satisfied with the dictio-
nary solution. But they never agreed on the inap-
propriateness of the dictionary: inappropriateness
revealed itself in the form of low scores from some
translators.
This agreement/disagreement threshold can be
said to characterise two types of translation prob-
lems: those for which there exist generally ac-
cepted dictionary solutions, and those for which
translators doubt whether the solution is appropri-
ate. Best-set scores for these two groups of dic-
tionary solutions ? the agreement and disagree-
ment group ? are plotted on the radar charts in
Figures 1 and 2 respectively. The identifiers on
the charts are problematic source language expres-
sions as used in the questionnaire (not translation
solutions to these problems, because a problem
may have several solutions preferred by different
judges). Scores for both translation directions are
presented on the same chart, since both follow the
same pattern and receive the same interpretation.
Figure 1 shows that whenever there is little
doubt about the quality of dictionary solutions, the
radar chart approaches a circle shape near the edge
of the chart. In Figure 2 the picture is different:
the circle is disturbed, and some scores frequently
approach the centre. Therefore the disagreement
group contains those translation problems where
dictionaries provide little help.
The central problem in our evaluation experi-
ment is whether ASSIST is helpful for problems
in the second group, where translators doubt the
quality of dictionary solutions.
Firstly, it can be seen from the charts that judge-
744
012345
?????
?????
?????
?????
?????
???
?????
??? ????
?? ??
?????
??
?????
?????
?
?????
?????
????
?????
?????
??? ??
?????
?
due p
roces
s
negot
iated 
settle
ment
clear 
defian
ce
daunt
ing ex
perien
ce
passio
nately
 seekrecrea
tional
 fear
Figure 2: Disagreement scores: dictionary
012345
?????
?????
?????
?????
?????
???
?????
??? ????
?? ??
?????
??
?????
?????
?
?????
?????
????
?????
?????
??? ??
?????
?
due p
roces
s
negot
iated 
settle
ment
clear 
defian
ce
daunt
ing ex
perien
ce
passio
nately
 seekrecrea
tional
 fear
Figure 3: Disagreement scores: ASSIST
ments on the quality of the system output are more
consistent: score lines for system output are closer
to the circle shape in Figure 1 than those for dic-
tionary solutions in Figure 2 (formally: the stan-
dard deviation of evaluation scores, presented in
Table 4, is lower).
Secondly, as shown in Table 4, in this group av-
erage evaluation scores are slightly higher for AS-
SIST output than for dictionary solutions (3.97 vs
3.77) ? in the eyes of human evaluators ASSIST
outperforms good dictionaries. For good dictio-
nary solutions ASSIST performance is slightly
lower: (4.49 vs 4.81), but the standard deviation
is about the same.
Having said this, solutions from our system are
really not in competition with dictionary solutions:
they provide less literal translations, which often
emerge in later stages of the translation task, when
translators correct and improve an initial draft,
where they have usually put more literal equiva-
lents (Shveitser, 1988). It is a known fact in trans-
lation studies that non-literal solutions are harder
to see and translators often find them only upon
longer reflection. Yet another fact is that non-
literal translations often require re-writing other
segments of the sentence, which may not be ob-
vious at first glance.
4 Conclusions and future work
The results of evaluation show that the tool is
successful in finding translation equivalents for a
range of examples. What is more, in cases where
the problem is genuinely difficult, ASSIST consis-
tently provides scores around 4 ? ?minor adapta-
tions needed?. The precision of the tool is low, it
suggests 50-100 examples with only 2-4 useful for
the current context. However, recall of the output
is more relevant than precision, because transla-
tors typically need just one solution for their prob-
lem, and often have to look through reasonably
large lists of dictionary translations and examples
to find something suitable for a problematic ex-
pression. Even if no immediately suitable trans-
lation can be found in the list of suggestions, it
frequently contains a hint for solving the problem
in the absence of adequate dictionary information.
The current implementation of the model is re-
stricted in several respects. First, the majority of
target language constructions mirror the syntactic
structure of the source language example. Even
if the procedure for producing similarity classes
does not impose restrictions on POS properties,
nevertheless words in the similarity class tend to
follow the POS of the original word, because of
the similarity of their contexts of use. Further-
more, dictionaries also tend to translate words
using the same POS. This means that the ex-
isting method finds mostly NPs for NPs, verb-
object pairs for verb-object pairs, etc, even if the
most natural translation uses a different syntactic
structure, e.g. I like doing X instead of I do X
gladly (when translating from German ich mache
X gerne).
Second, suggestions are generated for the query
expression independently from the context it is
used in. For instance, the words judicial, military
and religious are in the similarity class of politi-
cal, just as reform is in the simclass of upheaval.
So the following example
The plan will protect EC-based investors in Russia
from political upheavals damaging their business.
creates a list of ?possible translations? evoking
various reforms and transformations.
745
These issues can be addressed by introduc-
ing a model of the semantic context of situation,
e.g. ?changes in business practice? as in the ex-
ample above, or ?unpleasant situation? as in the
case of daunting experience. This will allow
less restrictive identification of possible transla-
tion equivalents, as well as reduction of sugges-
tions irrelevant for the context of the current ex-
ample.
Currently we are working on an option to iden-
tify semantic contexts by means of ?semantic sig-
natures? obtained from a broad-coverage seman-
tic parser, such as USAS (Rayson et al, 2004).
The semantic tagset used by USAS is a language-
independent multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories
(such as I1.1- = Money: lack; A5.1- = Eval-
uation: bad), which can be used to detect the
semantic context. Identification of semantically
similar situations can be also improved by the
use of segment-matching algorithms as employed
in Example-Based MT (EBMT) and translation
memories (Planas and Furuse, 2000; Carl and
Way, 2003).
The proposed model looks similar to some im-
plementations of statistical machine translation
(SMT), which typically uses a parallel corpus for
its translation model, and then finds the best possi-
ble recombination that fits into the target language
model (Och and Ney, 2003). Just like an MT sys-
tem, our tool can find translation equivalents for
queries which are not explicitly coded as entries
in system dictionaries. However, from the user
perspective it resembles a dynamic dictionary or
thesaurus: it translates difficult words and phrases,
not entire sentences. The main thrust of our sys-
tem is its ability to find translation equivalents for
difficult contexts where dictionary solutions do not
exist, are questionable or inappropriate.
Acknowledgements
This research is supported by EPSRC grant
EP/C005902.
References
Bogdan Babych and Anthony Hartley. 2004. Ex-
tending the BLEU MT evaluation method with fre-
quency weightings. In Proceedings of the 42d An-
nual Meeting of the Association for Computational
Linguistics, Barcelona.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Gregory Grefenstette. 2002. Multilingual corpus-
based extraction and the very large lexicon. In Lars
Borin, editor, Language and Computers, Parallel
corpora, parallel worlds, pages 137?149. Rodopi.
John S. Justeson and Slava M. Katz. 1995. Techninal
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1(1):9?27.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Joint COLING-ACL-98, pages
768?774, Montreal.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Alan Partington. 1998. Patterns and meanings: using
corpora for English language research and teach-
ing. John Benjamins, Amsterdam.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of the Forth Language Resources and Evalua-
tion Conference, LREC 2004, pages 395?398, Lis-
bon.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proc. Beyond Named Entity Recognition
Workshop in association with LREC 2004, pages 7?
12, Lisbon.
Serge Sharoff. 2006. Creating general-purpose
corpora using automated search engine queries.
In Marco Baroni and Silvia Bernardini, editors,
WaCky! Working papers on the Web as Corpus.
Gedit, Bologna.
A.D. Shveitser. 1988. ?????? ????????: ??????, ???-
?????, ???????. Nauka, Moskow. (In Russian:
Theory of Translation: Status, Problems, Aspects).
Krista Varantola. 2003. Translators and disposable
corpora. In Federico Zanettin, Silvia Bernardini,
and Dominic Stewart, editors, Corpora in Transla-
tor Education, pages 55?70. St Jerome, Manchester.
746
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 136?143,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Assisting Translators in Indirect Lexical Transfer 
Bogdan Babych, Anthony Hartley, Serge Sharoff
  Centre for Translation Studies 
  University of Leeds, UK 
{b.babych,a.hartley,s.sharoff}@leeds.ac.uk
Olga Mudraya 
  Department of Linguistics 
  Lancaster University, UK 
 o.mudraya@lancs.ac.uk 
Abstract 
We present the design and evaluation of a 
translator?s amenuensis that uses compa-
rable corpora to propose and rank non-
literal solutions to the translation of expres-
sions from the general lexicon. Using dis-
tributional similarity and bilingual diction-
aries, the method outperforms established 
techniques for extracting translation 
equivalents from parallel corpora. The in-
terface to the system is available at: 
http://corpus.leeds.ac.uk/assist/v05/  
1 Introduction 
This paper describes a system designed to assist 
humans in translating expressions that do not nec-
essarily have a literal or compositional equivalent 
in the target language (TL). In the spirit of (Kay, 
1997), it is intended as a translator's amenuensis 
"under the tight control of a human translator ? to 
help increase his productivity and not to supplant him". 
One area where human translators particularly 
appreciate assistance is in the translation of expres-
sions from the general lexicon. Unlike equivalent 
technical terms, which generally share the same 
part-of-speech (POS) across languages and are in 
the ideal case univocal, the contextually appropri-
ate equivalents of general language expressions are 
often indirect and open to variation. While the 
transfer module in RBMT may acceptably under-
generate through a many-to-one mapping between 
source and target expressions, human translators, 
even in non-literary fields, value legitimate varia-
tion. Thus the French expression il faillit ?chouer 
(lit.: he faltered to fail) may be variously rendered 
as he almost/nearly/all but failed; he was on the 
verge/brink of failing/failure; failure loomed. All 
of these translations are indirect in that they in-
volve lexical shifts or POS transformations. 
Finding such translations is a hard task that can 
benefit from automated assistance. 'Mining' such 
indirect equivalents is difficult, precisely because 
of the structural mismatch, but also because of the 
paucity of suitable aligned corpora. The approach 
adopted here includes the use of comparable cor-
pora in source and target languages, which are 
relatively easy to create. The challenge is to gener-
ate a list of usable solutions and to rank them such 
that the best are at the top. 
Thus the present system is unlike SMT (Och and 
Ney, 2003), where lexical selection is effected by a 
translation model based on aligned, parallel cor-
pora, but the novel techniques it has developed are 
exploitable in the SMT paradigm. It also differs 
from now traditional uses of comparable corpora 
for detecting translation equivalents (Rapp, 1999) 
or extracting terminology (Grefenstette, 2002), 
which allows a one-to-one correspondence irre-
spective of the context. Our system addresses diffi-
culties in expressions in the general lexicon, whose 
translation is context-dependent. 
The structure of the paper is as follows. In Sec-
tion 2 we present the method we use for mining 
translation equivalents. In Section 3 we present the 
results of an objective evaluation of the quality of 
suggestions produced by the system by comparing 
our output against a parallel corpus. Finally, in 
Section 4 we present a subjective evaluation focus-
ing on the integration of the system into the work-
flow of human translators. 
2 Methodology 
The software acts as a decision support system for 
translators. It integrates different technologies for 
136
extracting indirect translation equivalents from 
large comparable corpora. In the following subsec-
tions we give the user perspective on the system 
and describe the methodology underlying each of 
its sub-tasks. 
2.1 User perspective 
Unlike traditional dictionaries, the system is a 
dynamic translation resource in that it can success-
fully find translation equivalents for units which 
have not been stored in advance, even for idiosyn-
cratic multiword expressions which almost cer-
tainly will not figure in a dictionary. While our 
system can rectify gaps and omissions in static 
lexicographical resources, its major advantage is 
that it is able to cope with an open set of transla-
tion problems, searching for translation equivalents 
in comparable corpora in runtime. This makes it 
more than just an extended dictionary. 
Contextual descriptors 
From the user perspective the system extracts indi-
rect translation equivalents as sets of contextual 
descriptors ? content words that are lexically cen-
tral in a given sentence, phrase or construction. 
The choice of these descriptors may determine the 
general syntactic perspective of the sentence and 
the use of supporting lexical items. Many transla-
tion problems arise from the fact that the mapping 
between such descriptors is not straightforward. 
The system is designed to find possible indirect 
mappings between sets of descriptors and to verify 
the acceptability of the mapping into the TL. For 
example, in the following Russian sentence, the 
bolded contextual descriptors require indirect 
translation into English. 
???? ???????? ????? ???????????-
?????? ?????, ? ??????? ????????? 
?????? ???????????? 
(Children attend badly repaired schools, in 
which [it] is missing the most necessary) 
Combining direct translation equivalents of 
these words (e.g., translations found in the Oxford 
Russian Dictionary ? ORD) may produce a non-
natural English sentence, like the literal translation 
given above. In such cases human translators usu-
ally apply structural and lexical transformations, 
for instance changing the descriptors? POS and/or 
replacing them with near-synonyms which fit to-
gether in the context of a TL sentence (Munday, 
2001: 57-58). Thus, a structural transformation of 
????? ????????????????? (badly repaired) may 
give in poor repair while a lexical transformation 
of ????????? ?????? ???????????? ([it] is missing 
the most necessary) gives lacking basic essentials. 
Our system models such transformations of the 
descriptors and checks the consistency of the re-
sulting sets in the TL. 
Using the system 
Human translators submit queries in the form of 
one or more SL descriptors which in their opinion 
may require indirect translation. When the transla-
tors use the system for translating into their native 
language, the returned descriptors are usually suf-
ficient for them to produce a correct TL construc-
tion or phrase around them (even though the de-
scriptors do not always form a naturally sounding 
expression). When the translators work into a non-
native language, they often find it useful to gener-
ate concordances for the returned descriptors to 
verify their usage within TL constructions. 
For example, for the sentence above translators 
may submit two queries: ????? ????????-
????????? (badly repaired) and ????????? 
???????????? (missing necessary). For the first 
query the system returns a list of descriptor pairs 
(with information on their frequency in the English 
corpus) ranked by distributional proximity to the 
original query, which we explain in Section 2.2. At 
the top of the list come: 
bad repair = 30  (11.005) 
bad maintenance = 16  (5.301) 
bad restoration = 2  (5.079) 
poor repair = 60  (5.026)? 
Underlined hyperlinks lead translators to actual 
contexts in the English corpus, e.g., poor repair 
generates a concordance containing a desirable TL 
construction which is a structural transformation of 
the SL query: 
in such a poor state of repair 
bridge in as poor a state of repair as the highways 
building in poor repair. 
dwellings are in poor repair; 
Similarly, the result of the second query may 
give the translators an idea about possible lexical 
transformation: 
missing need = 14  (5.035) 
important missing = 8 (2.930) 
missing vital = 8  (2.322) 
lack necessary = 204  (1.982)? 
essential lack = 86  (0.908)? 
137
The concordance for the last pair of descriptors 
contains the phrase they lack the three essentials, 
which illustrates the transformation. The resulting 
translation may be the following: 
Children attend schools that are in poor re-
pair and lacking basic essentials 
Thus our system supports translators in making 
decisions about indirect translation equivalents in a 
number of ways: it suggests possible structural and 
lexical transformations for contextual descriptors; 
it verifies which translation variants co-occur in 
the TL corpus; and it illustrates the use of the 
transformed TL lexical descriptors in actual con-
texts. 
2.2 Generating translation equivalents 
We have generalised the method used in our previ-
ous study (Sharoff et al, 2006) for extracting 
equivalents for continuous multiword expressions 
(MWEs). Essentially, the method expands the 
search space for each word and its dictionary trans-
lations with entries from automatically computed 
thesauri, and then checks which combinations are 
possible in target corpora. These potential transla-
tion equivalents are then ranked by their similarity 
to the original query and presented to the user. The 
range of retrievable equivalents is now extended 
from a relatively limited range of two-word con-
structions which mirror POS categories in SL and 
TL to a much wider set of co-occurring lexical 
content items, which may appear in a different or-
der, at some distance from each other, and belong 
to different POS categories.  
The method works best for expressions from the 
general lexicon, which do not have established 
equivalents, but not yet for terminology. It relies 
on a high-quality bilingual dictionary (en-ru ~30k, 
ru-en ~50K words, combining ORD and the core 
part of Multitran) and large comparable corpora 
(~200M En, ~70M Ru) of news texts. 
For each of the SL query terms q the system 
generates its dictionary translation Tr(q) and its 
similarity class S(q) ? a set of words with a similar 
distribution in a monolingual corpus. Similarity is 
measured as the cosine between collocation vec-
tors, whose dimensionality is reduced by SVD us-
ing the implementation by Rapp (2004). The de-
scriptor and each word in the similarity class are 
then translated into the TL using ORD or the Mul-
titran dictionary, resulting in {Tr(q)? Tr(S(q))}. 
On the TL side we also generate similarity classes, 
but only for dictionary translations of query terms 
Tr(q) (not for Tr(S(q)), which can make output too 
noisy). We refer to the resulting set of TL words as 
a translation class T.  
T = {Tr(q) ? Tr(S(q)) ? S(Tr(q))} 
Translation classes approximate lexical and 
structural transformations which can potentially be 
applied to each of the query terms. Automatically 
computed similarity classes do not require re-
sources like WordNet, and they are much more 
suitable for modelling translation transformations, 
since they often contain a wider range of words of 
different POS which share the same context, e.g., 
the similarity class of the word lack contains words 
such as absence, insufficient, inadequate, lost, 
shortage, failure, paucity, poor, weakness, inabil-
ity, need. This clearly goes beyond the range of 
traditional thesauri. 
For multiword queries, the system performs a 
consistency check on possible combinations of 
words from different translation classes. In particu-
lar, it computes the Cartesian product for pairs of 
translation classes T1 and T2 to generate the set P 
of word pairs, where each word (w1 and w2) comes 
from a different translation class: 
P = T1 ? T2 = {(w1, w2) | w1 ? T1 and w2 ? T2}  
Then the system checks whether each word pair 
from the set P exists in the database D of discon-
tinuous content word bi-grams which actually co-
occur in the TL corpus: 
P? = P ? D 
The database contains the set of all bi-grams that 
occur in the corpus with a frequency ? 4 within a 
window of 5 words (over 9M bigrams for each 
language). The bi-grams in D and in P are sorted 
alphabetically, so their order in the query is not 
important. 
Larger N-grams (N > 2) in queries are split into 
combinations of bi-grams, which we found to be 
an optimal solution to the problem of the scarcity 
of higher order N-grams in the corpus. Thus, for 
the query gain significant importance the system 
generates P?1(significant importance), P?2(gain impor-
tance), P?3(gain significant) and computes P? as:  
P? = {(w1,w2,w3)| (w1,w2) ? P?1 & (w1, w3) ? P?2 
& (w2,w3) ? P?3 }, 
which allows the system to find an indirect equiva-
lent ???????? ??????? ???????? (lit.: receive 
weighty meaning). 
138
Even though P? on average contains about 2% - 
4% of the theoretically possible number of bi-
grams present in P, the returned number of poten-
tial translation equivalents may still be large and 
contain much noise. Typically there are several 
hundred elements in P?, of which only a few are 
really useful for translation. To make the system 
usable in practice, i.e., to get useful solutions to 
appear close to the top (preferably on the first 
screen of the output), we developed methods of 
ranking and filtering the returned TL contextual 
descriptor pairs, which we present in the following 
sections. 
2.3 Hypothesis ranking 
The system ranks the returned list of contextual 
descriptors by their distributional proximity to the 
original query, i.e. it uses scores cos(vq, vw) gener-
ated for words in similarity classes ? the cosine of 
the angle between the collocation vector for a word 
and the collocation vector for the query or diction-
ary translation of the query. Thus, words whose 
equivalents show similar usage in a comparable 
corpus receive the highest scores. These scores are 
computed for each individual word in the output, 
so there are several ways to combine them to 
weight words in translation classes and word com-
binations in the returned list of descriptors.  
We established experimentally that the best way 
to combine similarity scores is to multiply weights 
W(T) computed for each word within its translation 
class T. The weight W(P?(w1,w2)) for each pair of 
contextual descriptors (w1, w2)?P? is computed as: 
W(P?(w1,w2)) = W(T(w1)) ? W(T(w2)); 
Computing W(T(w)), however, is not straightfor-
ward either, since some words in similarity classes 
of different translation equivalents for the query 
term may be the same, or different words from the 
similarity class of the original query may have the 
same translation. Therefore, a word w within a 
translation class may have come by several routes 
simultaneously, and may have done that several 
times. For each word w in T there is a possibility 
that it arrived in T either because it is in Tr(q) or 
occurs   n times in Tr(S(q)) or k times in S(Tr(q)). 
We found that the number of occurrences n and 
k of each word w in each subset gives valuable in-
formation for ranking translation candidates. In our 
experiments we computed the weight W(T) as the 
sum of similarity scores which w receives in each 
of the subsets. We also discovered that ranking 
improves if for each query term we compute in 
addition a larger (and potentially noisy) space of 
candidates that includes TL similarity classes of 
translations of the SL similarity class S(Tr(S(q))). 
These candidates do not appear in the system out-
put, but they play an important role in ranking the 
displayed candidates. The improvement may be 
due to the fact that this space is much larger, and 
may better support relevant candidates since there 
is a greater chance that appropriate indirect equiva-
lents are found several times within SL and TL 
similarity classes. The best ranking results were 
achieved when the original W(T) scores were mul-
tiplied by 2 and added to the scores for the newly 
introduced similarity space S(Tr(S(q))): 
W(T(w))= 2?(1 if w?Tr(q) )+  
2??( cos(vq, vTr(w)) | {w | w? Tr(S(q)) } ) +  
2??( cos(vTr(q), vw) | {w | w? S(Tr(q)) } ) + 
?(cos(vq, vTr(w))?cos (vTr(q), vw) |  
{w | w? S(Tr(S(q))) } ) 
For example, the system gives the following 
ranking for the indirect translation equivalents of 
the Russian phrase ??????? ???????? (lit.: weighty 
meaning) ? figures in brackets represent W(P?) 
scores for each pair of TL descriptors: 
1. significant importance = 7 (3.610)  
2. significant value = 128    (3.211)  
3. measurable value = 6       (2.657)?  
8. dramatic importance = 2    (2.028)  
9. important significant = 70 (2.014)  
10. convincing importance = 6 (1.843) 
The Russian similarity class for ??????? 
(weighty, ponderous) contains: ???????????? 
(convincing) (0.469), ???????? (significant) 
(0.461), ???????? (notable) (0.452) ?????-
?????? (dramatic) (0.371). The equivalent of 
significant is not at the top of the similarity class of 
the Russian query, but it appears at the top of the 
final ranking of pairs in P?, because this hypothesis 
is supported by elements of the set formed by 
S(Tr(S(q))); it appears in similarity classes for no-
table (0.353) and dramatic (0.315), which contrib-
uted these values to the W(T) score of significant: 
W(T(significant)) = 
    2 ? (Tr(????????)=significant (0.461))  
+ (Tr(????????)=notable (0.452)  
  ? S(notable)=significant (0.353)) 
+ (Tr(???????????)=dramatic (0.371)  
  ? S(dramatic)= significant (0.315)) 
The word dramatic itself is not usable as a 
translation equivalent in this case, but its similarity 
139
class contains the support for relevant candidates, 
so it can be viewed as useful noise. On the other 
hand, the word convincing does not receive such 
support from the hypothesis space, even though its 
Russian equivalent is ranked higher in the SL simi-
larity class. 
2.4 Semantic filtering 
Ranking of translation candidates can be further 
improved when translators use an option to filter 
the returned list by certain lexical criteria, e.g., to 
display only those examples that contain a certain 
lexical item, or to require one of the items to be a 
dictionary translation of the query term. However, 
lexical filtering is often too restrictive: in many 
cases translators need to see a number of related 
words from the same semantic field or subject do-
main, without knowing the lexical items in ad-
vance. In this section we present the semantic fil-
ter, which is based on Russian and English seman-
tic taggers which use the same semantic field tax-
onomy for both languages. 
The semantic filter displays only those items 
which have specified semantic field tags or tag 
combinations; it can be applied to one or both 
words in each translation hypothesis in P?. The 
default setting for the semantic filter is the re-
quirement for both words in the resulting TL can-
didates to contain any of the semantic field tags 
from a SL query term. 
In the next section we present evaluation results 
for this default setting (which is applied when the 
user clicks the Semantic Filter button), but human 
translators have further options ? to filter by tags 
of individual words, to use semantic classes from 
SL or TL terms, etc. 
For example, applying the default semantic filter 
for the output of the query ????? ???????-
?????????? (badly repaired) removes the high-
lighted items from the list: 
 1. bad repair = 30       (11.005)  
[2. good repair = 154     (8.884) ] 
 3. bad rebuild = 6       (5.920)  
[4. bad maintenance = 16  (5.301) ] 
 5. bad restoration = 2   (5.079)  
 6. poor repair = 60      (5.026)  
[7. good rebuild = 38     (4.779) ] 
 8. bad construction = 14 (4.779)  
Items 2 and 7 are generated by the system be-
cause good, well and bad are in the same similar-
ity cluster for many words (they often share the 
same collocations). The semantic filter removes 
examples with good and well on the grounds that 
they do not have any of the tags which come from 
the word ????? (badly): in particular, instead of 
tag A5? (Evaluation: Negative) they have tag A5+ 
(Evaluation: Positive). Item 4 is removed on the 
grounds that the words ????????????????? 
(repaired) and maintenance do not have any tags 
in common ? they appear ontologically too far 
apart from the point of view of the semantic tagger. 
The core of the system?s multilingual semantic 
tagging is a knowledge base in which single words 
and MWEs are mapped to their potential semantic 
field categories. Often a lexical item is mapped to 
multiple semantic categories, reflecting its poten-
tial multiple senses. In such cases, the tags are ar-
ranged by the order of likelihood of meanings, 
with the most prominent first. 
3 Objective evaluation 
In the objective evaluation we tested the perform-
ance of our system on a selection of indirect trans-
lation problems, extracted from a parallel corpus 
consisting mostly of articles from English and 
Russian newspapers (118,497 words in the R-E 
direction, 589,055 words in the E-R direction). It 
has been aligned on the sentence level by JAPA 
(Langlais et al, 1998), and further on the word 
level by GIZA++ (Och and Ney, 2003). 
3.1 Comparative performance 
The intuition behind the objective evaluation 
experiment is that the capacity of our tool to find 
indirect translation equivalents in comparable cor-
pora can be compared with the results of automatic 
alignment of parallel texts used in translation mod-
els in SMT: one of the major advantages of the 
SMT paradigm is its ability to reuse indirect 
equivalents found in parallel corpora (equivalents 
that may never come up in hand-crafted dictionar-
ies). Thus, automatically generated GIZA++ dic-
tionaries with word alignment contain many exam-
ples of indirect translation equivalents. 
We use these dictionaries to simulate the genera-
tor of translation classes T, which we recombine to 
construct their Cartesian product P, similarly to the 
procedure we use to generate the output of our sys-
tem. However, the two approaches generate indi-
rect translation equivalence hypotheses on the ba-
sis of radically different material: the GIZA dic-
tionary uses evidence from parallel corpora of ex-
140
isting human translations, while our system re-
combines translation candidates on the basis of 
their distributional similarity in monolingual com-
parable corpora. Therefore we took GIZA as a 
baseline. 
Translation problems for the objective evalua-
tion experiment were manually extracted from two 
parallel corpora: a section of about 10,000 words 
of a corpus of English and Russian newspapers, 
which we also used to train GIZA, and a section of 
the same length from a corpus of interviews pub-
lished on the Euronews.net website. 
We selected expressions which represented 
cases of lexical transformations (as illustrated in 
Section 0), containing at least two content words 
both in the SL and TL. These expressions were 
converted into pairs of contextual descriptors ? 
e.g., recent success, reflect success ? and submit-
ted to the system and to the GIZA dictionary. We 
compared the ability of our system and of GIZA to 
find indirect translation equivalents which matched 
the equivalents used by human translators. The 
output from both systems was checked to see 
whether it contained the contextual descriptors 
used by human translators. We submitted 388 pairs 
of descriptors extracted from the newspaper trans-
lation corpus and 174 pairs extracted from the Eu-
ronews interview corpus. Half of these pairs were 
Russian, and the other half English. 
We computed recall figures for 2-word combi-
nations of contextual descriptors and single de-
scriptors within those combinations. We also show 
the recall of translation variants provided by the 
ORD on this data set. For example, for the query 
????????? ???????????? ([it] is missing neces-
sary [things]) human translators give the solution 
lacking essentials; the lemmatised descriptors are 
lack and essential. ORD returns direct translation 
equivalents missing and necessary. The GIZA dic-
tionary in addition contains several translation 
equivalents for the second term (with alignment 
probabilities) including: necessary ~0.332, need 
~0.226, essential ~0.023. Our system returns both 
descriptors used in human translation as a pair ? 
lack essential (ranked 41 without filtering and 22 
with the default semantic filter). Thus, for a 2-word 
combination of the descriptors only the output of 
our system matched the human solution, which we 
counted as one hit for the system and no hits for 
ORD or GIZA. For 1-word descriptors we counted 
2 hits for our system (both words in the human 
solution are matched), and 1 hit for GIZA ? it 
matches the word essential ~0.023 (which also il-
lustrates its ability to find indirect translation 
equivalents). 
 2w descriptors 1w descriptors 
 news interv news interv 
ORD 6.7% 4.6% 32.9% 29.3% 
GIZA++ 13.9% 3.4% 35.6% 29.0%
Our system 21.9% 19.5% 55.8% 49.4%
Table 1 Conservative estimate of recall 
It can be seen from Table 1 that for the newspa-
per corpus on which it was trained, GIZA covers a 
wider set of indirect translation variants than ORD. 
But our recall is even better both for 2-word and 1-
word descriptors. 
However, note that GIZA?s ability to retrieve 
from the newspaper corpus certain indirect transla-
tion equivalents may be due to the fact that it has 
previously seen them frequently enough to gener-
ate a correct alignment and the corresponding dic-
tionary entry. 
The Euronews interview corpus was not used for 
training GIZA. It represents spoken language and 
is expected to contain more ?radical? transforma-
tions. The small decline in ORD figures here can 
be attributed to the fact that there is a difference in 
genre between written and spoken texts and conse-
quently between transformation types in them. 
However, the performance of GIZA drops radi-
cally on unseen text and becomes approximately 
the same as the ORD. 
This shows that indirect translation equivalents 
in the parallel corpus used for training GIZA are 
too sparse to be learnt one by one and successfully 
applied to unseen data, since solutions which fit 
one context do not necessarily suit others. 
The performance of our system stays at about 
the same level for this new type of text; the decline 
in its performance is comparable to the decline in 
ORD figures, and can again be explained by the 
differences in genre. 
3.2 Evaluation of hypothesis ranking 
As we mentioned, correct ranking of translation 
candidates improves the usability of the system. 
Again, the objective evaluation experiment gives 
only a conservative estimate of ranking, because 
there may be many more useful indirect solutions 
further up the list in the output of the system which 
are legitimate variants of the solutions found in the 
141
parallel corpus. Therefore, evaluation figures 
should be interpreted in a comparative rather then 
an absolute sense. 
We use ranking by frequency as a baseline for 
comparing the ranking described in Section 2.3 ? 
by distributional similarity between a candidate 
and the original query. 
Table 2 shows the average rank of human solu-
tions found in parallel corpora and the recall of 
these solutions for the top 300 examples. Since 
there are no substantial differences between the 
figures for the newspaper texts and for the inter-
views, we report the results jointly for 556 transla-
tion problems in both selections (lower rank fig-
ures are better). 
 Recall Average rank 
2-word descriptors 
frequency (baseline) 16.7% rank=93.7
distributional similarity 19.5% rank=44.4
sim. + semantic filter 14.4% rank=26.7
1-word descriptors 
frequency (baseline) 48.2% rank=42.7
distributional similarity 52.8% rank=21.6
sim. + semantic filter 44.1% rank=11.3
Table 2 Ranking: frequency, similarity and filter 
It can be seen from the table that ranking by 
similarity yields almost a twofold improvement for 
the average rank figures compared to the baseline. 
There is also a small improvement in recall, since 
there are more relevant examples that appear 
within the top 300 entries. 
The semantic filter once again gives an almost 
twofold improvement in ranking, since it removes 
many noisy items. The average is now within the 
top 30 items, which means that there is a high 
chance that a translation solution will be displayed 
on the first screen. The price for improved ranking 
is decline in recall, since it may remove some rele-
vant lexical transformations if they appear to be 
ontologically too far apart. But the decline is 
smaller: about 26.2% for 2-word descriptors and 
16.5% for 1-word descriptors. The semantic filter 
is an optional tool, which can be used to great ef-
fect on noisy output: its improvement of ranking 
outweighs the decline in recall. 
Note that the distribution of ranks is not normal, 
so in Figure 1 we present frequency polygons for 
rank groups of 30 (which is the number of items 
that fit on a single screen, i.e., the number of items 
in the first group (r030) shows solutions that will 
be displayed on the first screen). The majority of 
solutions ranked by similarity appear high in the 
list (in fact, on the first two or three screens). 
0
10
20
30
40
50
60
70
r0
30
r0
60
r0
90
r1
20
r1
50
r1
80
r2
10
r2
40
r2
70
r3
00
similarity
frequency
 
Figure 1 Frequency polygons for ranks 
4 Subjective evaluation 
The objective evaluation reported above uses a 
single reference translation and is correspondingly 
conservative in estimating the coverage of the sys-
tem. However, many expressions studied have 
more than one fluent translation. For instance, in 
poor repair is not the only equivalent for the Rus-
sian expression ????? ?????????????????. It is 
also possible to translate it as unsatisfactory condi-
tion, bad state of repair, badly in need of repair, 
and so on. The objective evaluation shows that the 
system has been able to find the suggestion used 
by a particular translator for the problem studied. It 
does not tell us whether the system has found some 
other translations suitable for the context. Such 
legitimate translation variation implies that the per-
formance of a system should be studied on the ba-
sis of multiple reference translations, though typi-
cally just two reference translations are used (Pap-
ineni, et al 2001). This might be enough for the 
purposes of a fully automatic MT tool, but in the 
context of a translator's amanuensis which deals 
with expressions difficult for human translators, it 
is reasonable to work with a larger range of ac-
ceptable target expressions. 
With this in mind we evaluated the performance 
of the tool with a panel of 12 professional transla-
tors. Problematic expressions were highlighted and 
the translators were asked to find suitable sugges-
tions produced by the tool for these expressions 
and rank their usability on a scale from 1 to 5 (not 
acceptable to fully idiomatic, so 1 means that no 
usable translation was found at all). 
Sentences themselves were selected from prob-
lems discussed on professional translation forums 
proz.com and forum.lingvo.ru. Given the range of 
corpora used in the system (reference and newspa-
142
per corpora), the examples were filtered to address 
expressions used in newspapers. 
The goal of the subjective evaluation experiment 
was to establish the usefulness of the system for 
translators beyond the conservative estimate given 
by the objective evaluation. The intuition behind 
the experiment is that if there are several admissi-
ble translations for the SL contextual descriptors, 
and system output matches any of these solutions, 
then the system has generated something useful. 
Therefore, we computed recall on sets of human 
solutions rather than on individual solutions. We 
matched 210 different human solutions to 36 trans-
lation problems. To compute more realistic recall 
figures, we counted cases when the system output 
matches any of the human solutions in the set. 
Table 3 compares the conservative estimate of the 
objective evaluation and the more realistic estimate 
on a single data set. 
 2w default 2w with sem filt 
Conservative  32.4%; r=53.68 21.9%; r=34.67 
Realistic 75.0%;   r=7.48 61.1%;   r=3.95 
Table 3 Recall and rank for 2-word descriptors 
Since the data set is different, the figures for the 
conservative estimate are higher than those for the 
objective evaluation data set. However, the table 
shows the there is a gap between the conservative 
estimate and the realistic coverage of the transla-
tion problems by the system, and that real coverage 
of indirect translation equivalents is potentially 
much higher. 
Table 4 shows averages (and standard deviation 
?) of the usability scores divided in four groups: (1) 
solutions that are found both by our system and the 
ORD; (2) solutions found only by our system; (3) 
solutions found only by ORD (4) solutions found 
by neither: 
 system (+) system (?) 
ORD (+) 4.03 (0.42) 3.62 (0.89)
ORD (?) 4.25 (0.79) 3.15 (1.15)
Table 4 Human scores and ? for system output 
It can be seen from the table that human users find 
the system most useful for those problems where 
the solution does not match any of the direct dic-
tionary equivalents, but is generated by the system. 
5 Conclusions 
We have presented a method of finding indirect 
translation equivalents in comparable corpora, and 
integrated it into a system which assists translators 
in indirect lexical transfer. The method outper-
forms established methods of extracting indirect 
translation equivalents from parallel corpora. 
We can interpret these results as an indication 
that our method, rather than learning individual 
indirect transformations, models the entire family 
of transformations entailed by indirect lexical 
transfer. In other words it learns a translation strat-
egy which is based on the distributional similarity 
of words in a monolingual corpus, and applies this 
strategy to novel, previously unseen examples. 
The coverage of the tool and additional filtering 
techniques make it useful for professional transla-
tors in automating the search for non-trivial, indi-
rect translation equivalents, especially equivalents 
for multiword expressions. 
References 
Gregory Grefenstette. 2002. Multilingual corpus-based 
extraction and the very large lexicon. In: Lars Borin, 
editor, Language and Computers, Parallel corpora, 
parallel worlds, pages 137-149. Rodopi. 
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation, 
12(1-2):3-23. 
Philippe Langlais, Michel Simard, and Jean V?ronis. 
1998. Methods and practical issues in evaluating 
alignment techniques. In Proc. Joint COLING-ACL-
98, pages 711-717. 
Jeremy Munday. 2001. Introducing translation studies. 
Theories and Applications. Routledge, New York. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2001). Bleu: a method for automatic evaluation of 
machine translation, RC22176 W0109-022: IBM. 
Reinhard Rapp. 1999. Automatic identification of word 
translations from unrelated English and German cor-
pora. In Procs. the 37th ACL, pages 395-398. 
Reinhard Rapp. 2004. A freely available automatically 
generated thesaurus of related words. In Procs. LREC 
2004, pages 395-398, Lisbon. 
Serge Sharoff, Bogdan Babych and Anthony Hartley 
2006. Using Comparable Corpora to Solve Problems 
Difficult for Human Translators. In: Proceedings of 
the COLING/ACL 2006 Main Conference Poster 
Sessions, pp. 739-746. 
143
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 91?96,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
ACCURAT Toolkit for Multi-Level Alignment and  
Information Extraction from Comparable Corpora 
 
M?rcis Pinnis1, Radu Ion2, Dan ?tef?nescu2, Fangzhong Su3, 
Inguna Skadi?a1, Andrejs Vasi?jevs1, Bogdan Babych3 
 1Tilde, Vien?bas gatve 75a, Riga, Latvia 
{marcis.pinnis,inguna.skadina,andrejs}@tilde.lv 
 
2Research Institute for Artificial Intelligence, Romanian Academy 
{radu,danstef}@racai.ro 
 
3Centre for Translation Studies, University of Leeds 
{f.su,b.babych}@leeds.ac.uk 
 
 
Abstract 
The lack of parallel corpora and linguistic 
resources for many languages and domains is 
one of the major obstacles for the further 
advancement of automated translation. A 
possible solution is to exploit comparable 
corpora (non-parallel bi- or multi-lingual text 
resources) which are much more widely 
available than parallel translation data. Our 
presented toolkit deals with parallel content 
extraction from comparable corpora. It consists 
of tools bundled in two workflows: (1) 
alignment of comparable documents and 
extraction of parallel sentences and (2) 
extraction and bilingual mapping of terms and 
named entities. The toolkit pairs similar 
bilingual comparable documents and extracts 
parallel sentences and bilingual terminological 
and named entity dictionaries from comparable 
corpora. This demonstration focuses on the 
English, Latvian, Lithuanian, and Romanian 
languages. 
Introduction 
In recent decades, data-driven approaches have 
significantly advanced the development of 
machine translation (MT). However, lack of 
sufficient bilingual linguistic resources for many 
languages and domains is still one of the major 
obstacles for further advancement of automated 
translation. At the same time, comparable corpora, 
i.e., non-parallel bi- or multilingual text resources 
such as daily news articles and large knowledge 
bases like Wikipedia, are much more widely 
available than parallel translation data.  
While methods for the use of parallel corpora in 
machine translation are well studied (Koehn, 
2010), similar techniques for comparable corpora 
have not been thoroughly worked out. Only the 
latest research has shown that language pairs and 
domains with little parallel data can benefit from 
the exploitation of comparable corpora (Munteanu 
and Marcu, 2005; Lu et al, 2010; Smith et al, 
2010; Abdul-Rauf and Schwenk, 2009 and 2011). 
In this paper we present the ACCURAT 
toolkit1 - a collection of tools that are capable of  
analysing comparable corpora and extracting 
parallel data which can be used to improve the 
performance of statistical and rule/example-based 
MT systems. 
Although the toolkit may be used for parallel 
data acquisition for open (broad) domain systems, 
it will be most beneficial for under-resourced 
languages or specific domains which are not 
covered by available parallel resources. 
The ACCURAT toolkit produces: 
? comparable document pairs with 
comparability scores, allowing to estimate 
the overall comparability of corpora; 
? parallel sentences which can be used as 
additional parallel data sources for 
statistical translation model learning; 
                                                          
1 http://www.accurat-project.eu/ 
91
? terminology dictionaries ? this type of 
data is expected to improve domain-
dependent translation; 
? named entity dictionaries. 
The demonstration showcases two general use 
case scenarios defined in the toolkit: ?parallel data 
mining from comparable corpora? and ?named 
entity/terminology extraction and mapping from 
comparable corpora?. 
The next section provides a general overview of 
workflows followed by descriptions of methods 
and tools integrated in the workflows. 
1 Overview of the Workflows 
The toolkit?s tools are integrated within two 
workflows (visualised in Figure 1). 
 
  
Figure 1. Workflows of the ACCURAT toolkit. 
 
The workflow for parallel data mining from 
comparable corpora aligns comparable corpora in 
the document level (section 2.1). This step is 
crucial as the further steps are computationally 
intensive. To minimise search space, documents 
are aligned with possible candidates that are likely 
to contain parallel data. Then parallel sentence 
pairs are extracted from the aligned comparable 
corpora (section 2.2). 
The workflow for named entity (NE) and 
terminology extraction and mapping from 
comparable corpora extracts data in a dictionary-
like format. Providing a list of document pairs, the 
workflow tags NEs or terms in all documents using 
language specific taggers (named entity 
recognisers (NER) or term extractors) and 
performs multi-lingual NE (section 2.3) or term 
mapping (section 2.4), thereby producing bilingual 
NE or term dictionaries. The workflow also 
accepts pre-processed documents, thus skipping 
the tagging process. 
Since all tools use command line interfaces, task 
automation and workflow specification can be 
done with simple console/terminal scripts. All 
tools can be run on the Windows operating system 
(some are also platform independent). 
2 Tools and Methods 
This section provides an overview of the main 
tools and methods in the toolkit. A full list of tools 
is described in ACCURAT D2.6. (2011). 
2.1 Comparability Metrics 
We define comparability by how useful a pair of 
documents is for parallel data extraction. The 
higher the comparability score, the more likely two 
documents contain more overlapping parallel data. 
The methods are developed to perform lightweight 
comparability estimation that minimises search 
space of relatively large corpora (e.g., 10,000 
documents in each language). There are two 
comparability metric tools in the toolkit: a 
translation based and a dictionary based metric. 
The Translation based metric (Su and Babych, 
2012a) uses MT APIs for document translation 
into English. Then four independent similarity 
feature functions are applied to a document pair: 
? Lexical feature ? both documents are pre-
processed (tokenised, lemmatised, and 
stop-words are filtered) and then 
vectorised. The lexical overlap score is 
calculated as a cosine similarity function 
over the vectors of two documents. 
? Structural feature ? the difference of 
sentence counts and content word counts 
(equally interpolated). 
? Keyword feature ? the cosine similarity 
of top 20 keywords. 
? NE feature ? the cosine similarity of NEs 
(extracted using Stanford NER). 
These similarity measures are linearly combined in 
a final comparability score. This is implemented by 
a simple weighted average strategy, in which each 
92
type of feature is associated with a weight 
indicating its relative confidence or importance. 
The comparability scores are normalised on a scale 
of 0 to 1, where a higher comparability score 
indicates a higher comparability level. 
The reliability of the proposed metric has been 
evaluated on a gold standard of comparable 
corpora for 11 language pairs (Skadi?a et al, 
2010). The gold standard consists of news articles, 
legal documents, knowledge-base articles, user 
manuals, and medical documents. Document pairs 
in the gold standard were rated by human judges as 
being parallel, strongly comparable, or weakly 
comparable. The evaluation results suggest that the 
comparability scores reliably reflect comparability 
levels. In addition, there is a strong correlation 
between human defined comparability levels and 
the confidence scores derived from the 
comparability metric, as the Pearson R correlation 
scores vary between 0.966 and 0.999, depending 
on the language pair.  
The Dictionary based metric (Su and Babych, 
2012b) is a lightweight approach, which uses 
bilingual dictionaries to lexically map documents 
from one language to another. The dictionaries are 
automatically generated via word alignment using 
GIZA++ (Och and Ney, 2000) on parallel corpora. 
For each word in the source language, the top two 
translation candidates (based on the word 
alignment probability in GIZA++) are retrieved as 
possible translations into the target language. This 
metric provides a much faster lexical translation 
process, although word-for-word lexical mapping 
produces less reliable translations than MT based 
translations. Moreover, the lower quality of text 
translation in the dictionary based metric does not 
necessarily degrade its performance in predicting 
comparability levels of comparable document 
pairs. The evaluation on the gold standard shows a 
strong correlation (between 0.883 and 0.999) 
between human defined comparability levels and 
the confidence scores of the metric. 
2.2 Parallel Sentence Extractor from 
Comparable Corpora 
Phrase-based statistical translation models are 
among the most successful translation models that 
currently exist (Callison-Burch et al, 2010). 
Usually, phrases are extracted from parallel 
corpora by means of symmetrical word alignment 
and/or by phrase generation (Koehn et al, 2003). 
Our toolkit exploits comparable corpora in order to 
find and extract comparable sentences for SMT 
training using a tool named LEXACC (?tef?nescu 
et al, 2012). 
LEXACC requires aligned document pairs (also 
m to n alignments) for sentence extraction. It also 
allows extraction from comparable corpora as a 
whole; however, precision may decrease due to 
larger search space. 
LEXACC scores sentence pairs according to five 
lexical overlap and structural matching feature 
functions. These functions are combined using 
linear interpolation with weights trained for each 
language pair and direction using logistic 
regression. The feature functions are: 
? a lexical (translation) overlap score for 
content words (nouns, verbs, adjectives, 
and adverbs) using GIZA++ (Gao and 
Vogel, 2008) format dictionaries; 
? a lexical (translation) overlap score for 
functional words (all except content 
words) constrained by the content word 
alignment from the previous feature; 
? the alignment obliqueness score, a measure 
that quantifies the degree to which the 
relative positions of source and target 
aligned words differ; 
? a score indicating whether strong content 
word translations are found at the 
beginning and the end of each sentence in 
the given pair; 
? a punctuation score which indicates 
whether the sentences have identical 
sentence ending punctuation. 
For different language pairs, the relevance of 
the individual feature functions differ. For 
instance, the locality feature is more important for 
the English-Romanian pair than for the English-
Greek pair. Therefore, the weights are trained on 
parallel corpora (in our case - 10,000 pairs). 
LEXACC does not score every sentence pair in 
the Cartesian product between source and target 
document sentences. It reduces the search space 
using two filtering steps (?tef?nescu et al, 2012). 
The first step makes use of the Cross-Language 
Information Retrieval framework and uses a search 
engine to find sentences in the target corpus that 
are the most probable translations of a given 
sentence. In the second step (which is optional), 
93
the resulting candidates are further filtered, and 
those that do not meet minimum requirements are 
eliminated.  
To work for a certain language pair, LEXACC 
needs additional resources: (i) a GIZA++-like 
translation dictionary, (ii) lists of stop-words in 
both languages, and (iii) lists of word suffixes in 
both languages (used for stemming). 
The performance of LEXACC, regarding 
precision and recall, can be controlled by a 
threshold applied to the overall interpolated 
parallelism score. The tool has been evaluated on 
news article comparable corpora. Table 1 shows 
results achieved by LEXACC with different 
parallelism thresholds on automatically crawled 
English-Latvian corpora, consisting of 41,914 
unique English sentences and 10,058 unique 
Latvian sentences. 
 
Threshold Aligned pairs Precision 
Useful 
pairs 
0.25 1036 39.19% 406 
0.3 813 48.22% 392 
0.4 553 63.47% 351 
0.5 395 76.96% 304 
0.6 272 84.19% 229 
0.7 151 88.74% 134 
0.8 27 88.89% 24 
0.9 0 - 0 
 
Table 1. English-Latvian parallel sentence extraction 
results on a comparable news corpus. 
 
Threshold Aligned pairs Precision Useful pairs
0.2 2324 10.32% 240 
0.3 1105 28.50% 315 
0.4 722 53.46% 386 
0.5 532 89.28% 475 
0.6 389 100% 389 
0.7 532 100% 532 
0.8 386 100% 386 
0.9 20 100% 20 
 
Table 2. English-Romanian parallel sentence extraction 
results on a comparable news corpus. 
Table 2 shows results for English-Romanian on 
corpora consisting of 310,740 unique English and 
81,433 unique Romanian sentences. 
Useful pairs denote the total number of parallel 
and strongly comparable sentence pairs (at least 
80% of the source sentence is a translation in the 
target sentence). The corpora size is given only as 
an indicative figure, as the amount of extracted 
parallel data greatly depends on the comparability 
of the corpora. 
2.3 Named Entity Extraction and Mapping 
The second workflow of the toolkit allows NE and 
terminology extraction and mapping. Starting with 
named entity recognition, the toolkit features the 
first NER systems for Latvian and Lithuanian 
(Pinnis, 2012). It also contains NER systems for 
English (through an OpenNLP NER2 wrapper) and 
Romanian (NERA). In order to map named entities, 
documents have to be tagged with NER systems 
that support MUC-7 format NE SGML tags.  
The toolkit contains the mapping tool NERA2. 
The mapper requires comparable corpora aligned 
in the document level as input. NERA2 compares 
each NE from the source language to each NE 
from the target language using cognate based 
methods. It also uses a GIZA++ format statistical 
dictionary to map NEs containing common nouns 
that are frequent in location names. This approach 
allows frequent NE mapping if the cognate based 
method fails, therefore, allowing increasing the 
recall of the mapper. Precision and recall can be 
tuned with a confidence score threshold. 
2.4 Terminology Mapping 
During recent years, automatic bilingual term 
mapping in comparable corpora has received 
greater attention in light of the scarcity of parallel 
data for under-resourced languages. Several 
methods have been applied to this task, e.g., 
contextual analysis (Rapp, 1995; Fung and 
McKeown, 1997) and compositional analysis 
(Daille and Morin, 2008). Symbolic, statistical, and 
hybrid techniques have been implemented for 
bilingual lexicon extraction (Morin and 
Prochasson, 2011). 
Our terminology mapper is designed to map 
terms extracted from comparable or parallel 
                                                          
2 Open NLP - http://incubator.apache.org/opennlp/. 
94
documents. The method is language independent 
and can be applied if a translation equivalents table 
exists for a language pair. As input, the application 
requires term-tagged bilingual corpora aligned in 
the document level. 
The toolkit includes term-tagging tools for 
English, Latvian, Lithuanian, and Romanian, but 
can be easily extended for other languages if a 
POS-tagger, a phrase pattern list, a stop-word list, 
and an inverse document frequency list (calculated 
on balanced corpora) are available. 
The aligner maps terms based on two criteria 
(Pinnis et al, 2012; ?tef?nescu, 2012): (i) a 
GIZA++-like translation equivalents table and (ii) 
string similarity in terms of Levenshtein distance 
between term candidates.  For evaluation, Eurovoc 
(Steinberger et al, 2002) was used. Tables 4 and 5 
show the performance figures of the mapper for 
English-Romanian and English-Latvian. 
 
Threshold P R F-measure
0.3 0.562 0.194 0.288 
0.4 0.759 0.295 0.425 
0.5 0.904 0.357 0.511 
0.6 0.964 0.298 0.456 
0.7 0.986 0.216 0.359 
0.8 0.996 0.151 0.263 
0.9 0.995 0.084 0.154 
 
Table 3. Term mapping performance for English-
Romanian. 
 
Threshold P R F-measure 
0.3 0.636 0.210 0.316 
0.4 0.833 0.285 0.425 
0.5 0.947 0.306 0.463 
0.6 0.981 0.235 0.379 
0.7 0.996 0.160 0.275 
0.8 0.996 0.099 0.181 
0.9 0.997 0.057 0.107 
 
Table 4. Term mapping performance for English-
Latvian. 
3 Conclusions and Related Information 
This demonstration paper describes the 
ACCURAT toolkit containing tools for multi-level 
alignment and information extraction from 
comparable corpora. These tools are integrated in 
predefined workflows that are ready for immediate 
use. The workflows provide functionality for the 
extraction of parallel sentences, bilingual NE 
dictionaries, and bilingual term dictionaries from 
comparable corpora. 
The methods, including comparability metrics, 
parallel sentence extraction and named entity/term 
mapping, are language independent. However, they 
may require language dependent resources, for 
instance, POS-taggers, Giza++ translation 
dictionaries, NERs, term taggers, etc.3 
 The ACCURAT toolkit is released under the 
Apache 2.0 licence and is freely available for 
download after completing a registration form4.  
Acknowledgements 
The research within the project ACCURAT 
leading to these results has received funding from 
the European Union Seventh Framework 
Programme (FP7/2007-2013), grant agreement no 
248347. 
References  
Sadaf Abdul-Rauf and Holger Schwenk. On the use of 
comparable corpora to improve SMT performance. 
EACL 2009: Proceedings of the 12th conference of 
the European Chapter of the Association for 
Computational Linguistics, Athens, Greece, 16-23. 
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Parallel 
sentence generation from comparable corpora for 
improved SMT. Machine Translation, 25(4): 341-
375. 
ACCURAT D2.6 2011. Toolkit for multi-level 
alignment and information extraction from 
comparable corpora (http://www.accurat-project.eu). 
Dan Gusfield. 1997. Algorithms on strings, trees and 
sequences. Cambridge University Press. 
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Kay Peterson, Mark Przybocki and Omar Zaidan. 
2010. Findings of the 2010 Joint Workshop on 
Statistical Machine Translation and Metrics for 
Machine Translation. Proceedings of the Joint Fifth 
Workshop on Statistical Machine Translation and 
MetricsMATR, 17-53. 
B?atrice Daille and Emmanuel Morin. 2008. Effective 
compositional model for lexical alignment. 
Proceedings of the 3rd International Joint Conference 
                                                          
3 Full requirements are defined in the documentation of each 
tool (ACCURAT D2.6, 2011). 
4 http://www.accurat-project.eu/index.php?p=toolkit 
95
on Natural Language Processing, Hyderabad, India, 
95-102. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. Proceedings of the 38th 
Annual Meeting of the Association for 
Computational Linguistics, 440-447. 
Pascale Fung and Kathleen Mckeown. 1997. Finding 
terminology translations from non-parallel corpora. 
Proceedings of the 5th Annual Workshop on Very 
Large Corpora, 192-202. 
Qin Gao and Stephan Vogel. 2008. Parallel 
implementations of a word alignment tool. 
Proceedings of ACL-08 HLT: Software Engineering, 
Testing, and Quality Assurance for Natural Language 
Processing, June 20, 2008. The Ohio State 
University, Columbus, Ohio, USA, 49-57. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. 
Proceedings of the Human Language Technology and 
North American Association for Computational 
Linguistics Conference (HLT/NAACL), May 27-
June 1, Edmonton, Canada. 
Philip Koehn. 2010. Statistical machine translation, 
Cambridge University Press. 
Bin Lu, Tao Jiang, Kapo Chow and Benjamin K. Tsou. 
2010. Building a large English-Chinese parallel 
corpus from comparable patents and its experimental 
application to SMT. Proceedings of the 3rd workshop 
on building and using comparable corpora: from 
parallel to non-parallel corpora, Valletta, Malta, 42-
48. 
Drago? ?tefan Munteanu and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
nonparallel corpora. ACL-44: Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Morristown, NJ, USA, 81-88. 
Emmanuel Morin and Emmanuel Prochasson. 2011. 
Bilingual lexicon extraction from comparable 
corpora enhanced with parallel corpora. ACL HLT 
2011, 27-34. 
M?rcis Pinnis. 2012. Latvian and Lithuanian named 
entity recognition with TildeNER. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey. 
M?rcis Pinnis, Nikola Ljube?i?, Dan ?tef?nescu, Inguna 
Skadi?a, Marko Tadi?, Tatiana Gornostay. 2012. 
Term extraction, tagging, and mapping tools for 
under-resourced languages. Proceedings of the 10th 
Conference on Terminology and Knowledge 
Engineering (TKE 2012), June 20-21, Madrid, Spain. 
Reinhard Rapp. 1995. Identifying word translations in 
non-parallel texts. Proceedings of the 33rd annual 
meeting on Association for Computational 
Linguistics, 320-322.  
Jason R. Smith, Chris Quirk, and Kristina Toutanova. 
2010.  Extracting parallel sentences from comparable 
corpora using document level alignment. Proceedings 
of NAACL 2010, Los Angeles, USA. 
Dan ?tef?nescu. 2012. Mining for term translations in 
comparable corpora. Proceedings of the 5th 
Workshop on Building and Using Comparable 
Corpora (BUCC 2012) to be held at the 8th edition of 
Language Resources and Evaluation Conference 
(LREC 2012), Istanbul, Turkey, May 23-25, 2012. 
Ralf Steinberger, Bruno Pouliquen and Johan Hagman. 
2002. Cross-lingual document similarity calculation 
using the multilingual thesaurus Eurovoc. 
Proceedings of the 3rd International Conference on 
Computational Linguistics and Intelligent Text 
Processing (CICLing '02), Springer-Verlag London, 
UK, ISBN:3-540-43219-1. 
Inguna Skadi?a, Ahmet Aker, Voula Giouli, Dan Tufis, 
Rob Gaizauskas, Madara Mieri?a and Nikos 
Mastropavlos. 2010. Collection of comparable 
corpora for under-resourced languages. In 
Proceedings of the Fourth International Conference 
Baltic HLT 2010, IOS Press, Frontiers in Artificial 
Intelligence and Applications, Vol. 219, pp. 161-168. 
Fangzhong Su and Bogdan Babych. 2012a. 
Development and application of a cross-language 
document comparability metric. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey.  
Fangzhong Su and Bogdan Babych.  2012b. Measuring 
comparability of documents in non-parallel corpora 
for efficient extraction of (semi-) parallel translation 
equivalents. Proceedings of  EACL'12 joint 
workshop on Exploiting Synergies between 
Information Retrieval and Machine Translation 
(ESIRMT) and Hybrid Approaches to Machine 
Translation (HyTra), Avignon, France.  
Dan ?tef?nescu, Radu Ion and Sabine Hunsicker. 2012. 
Hybrid parallel sentence mining from comparable 
corpora. Proceedings of the 16th Conference of the 
European Association for Machine Translation 
(EAMT 2012), Trento, Italy.  
96
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?19,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Measuring Comparability of Documents in Non-Parallel Corpora for
Efficient Extraction of (Semi-)Parallel Translation Equivalents
Fangzhong Su
Centre for Translation Studies
University Of Leeds
LS2 9JT, Leeds, UK
smlfs@leeds.ac.uk
Bogdan Babych
Centre for Translation Studies
University Of Leeds
LS2 9JT, Leeds, UK
b.babych@leeds.ac.uk
Abstract
In this paper we present and evaluate three
approaches to measure comparability of
documents in non-parallel corpora. We de-
velop a task-oriented definition of compa-
rability, based on the performance of auto-
matic extraction of translation equivalents
from the documents aligned by the pro-
posed metrics, which formalises intuitive
definitions of comparability for machine
translation research. We demonstrate ap-
plication of our metrics for the task of
automatic extraction of parallel and semi-
parallel translation equivalents and discuss
how these resources can be used in the
frameworks of statistical and rule-based
machine translation.
1 Introduction
Parallel corpora have been extensively exploited
in different ways in machine translation (MT)
? both in Statistical (SMT) and more recently,
in Rule-Based (RBMT) architectures: in SMT
aligned parallel resources are used for building
translation phrase tables and calculating transla-
tion probabilities; and in RBMT, they are used
for automatically building bilingual dictionaries
of translation equivalents and automatically deriv-
ing bilingual mappings for frequent structural pat-
terns. However, large parallel resources are not
always available, especially for under-resourced
languages or narrow domains. Therefore, in re-
cent years, the use of cross-lingual comparable
corpora has attracted considerable attention in
the MT community (Sharoff et al, 2006; Fung
and Cheung, 2004a; Munteanu and Marcu, 2005;
Babych et al, 2008).
Most of the applications of comparable cor-
pora focus on discovering translation equivalents
to support machine translation, such as bilingual
lexicon extraction (Rapp, 1995; Rapp, 1999;
Morin et al, 2007; Yu and Tsujii, 2009; Li and
Gaussier, 2010; Prachasson and Fung, 2011), par-
allel phrase extraction (Munteanu and Marcu,
2006), and parallel sentence extraction (Fung and
Cheung, 2004b; Munteanu and Marcu, 2005;
Munteanu et al, 2004; Smith et al, 2010).
Comparability between documents is often un-
derstood as belonging to the same subject domain,
genre or text type, so this definition relies on these
vague linguistic concepts. The problem with this
definition then is that it cannot be exactly bench-
marked, since it becomes hard to relate automated
measures of comparability to such inexact and un-
measurable linguistic concepts. Research on com-
parable corpora needs not only good measures for
comparability, but also a clearer, technologically-
grounded and quantifiable definition of compara-
bility in the first place.
In this paper we relate comparability to use-
fulness of comparable texts for MT. In particu-
lar, we propose a performance-based definition of
comparability, as the possibility to extract parallel
or quasi-parallel translation equivalents ? words,
phrases and sentences which are translations of
each other. This definition directly relates compa-
rability to texts? potential to improve the quality
of MT by adding extracted phrases to phrase ta-
bles, training corpus or dictionaries. It also can be
quantified as the rate of successful extraction of
translation equivalents by automated tools, such
as proposed in Munteanu and Marcu (2006).
Still, successful detection of translation equiv-
alents from comparable corpora very much de-
10
pends on the quality of these corpora, specifically
on the degree of their textual equivalence and suc-
cessful alignment on various text units. There-
fore, the goal of this work is to provide compa-
rability metrics which can reliably identify cross-
lingual comparable documents from raw corpora
crawled from the Web, and characterize the de-
gree of their similarity, which enriches compara-
ble corpora with the document alignment infor-
mation, filters out documents that are not useful
and eventually leads to extraction of good-quality
translation equivalents from the corpora.
To achieve this goal, we need to define a
scale to assess comparability qualitatively, met-
rics to measure comparability quantitatively, and
the sources to get comparable corpora from. In
this work, we directly characterize comparability
by how useful comparable corpora are for the task
of detecting translation equivalents in them, and
ultimately to machine translation. We focus on
document-level comparability, and use three cat-
egories for qualitative definition of comparability
levels, defined in terms of granularity for possible
alignment:
? Parallel: Traditional parallel texts that are
translations of each other or approximate
translations with minor variations, which can
be aligned on the sentence level.
? Strongly-comparable: Texts that talk about
the same event or subject, but in different
languages. For example, international news
about oil spill in the Gulf of Mexico, or
linked articles in Wikipedia about the same
topic. These documents can be aligned on
the document level on the basis of their ori-
gin.
? Weakly-comparable: Texts in the same sub-
ject domain which describe different events.
For example, customer reviews about hotel
and restaurant in London. These documents
do not have an independent alignment across
languages, but sets of texts can be aligned
on the basis of belonging to the same subject
domain or sub-domain.
In this paper, we present three different ap-
proaches to measure the comparability of cross-
lingual (especially under-resourced languages)
comparable documents: a lexical mapping based
approach, a keyword based approach, and a ma-
chine translation based approach. The experimen-
tal results show that all of them can effectively
predict the comparability levels of the compared
document pairs. We then further investigate the
applicability of the proposed metrics by measur-
ing their impact on the task of parallel phrase ex-
traction from comparable corpora. It turns out
that, higher comparability level predicted by the
metrics consistently lead to more number of paral-
lel phrase extracted from comparable documents.
Thus, the metrics can help select more compara-
ble document pairs to improve the performance of
parallel phrase extraction.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Section
3 introduces our comparability metrics. Section
4 presents the experimental results and evaluation.
Section 5 describes the application of the metrics.
Section 6 discusses the pros and cons of the pro-
posed metrics, followed by conclusions and future
work in Section 7.
2 Related Work
The term ?comparability?, which is the key con-
cept in this work, applies to the level of corpora,
documents and sub-document units. However, so
far there is no widely accepted definition of com-
parability. For example, there is no agreement on
the degree of similarity that documents in com-
parable corpora should have or on the criteria for
measuring comparability. Also, most of the work
that performs translation equivalent extraction in
comparable corpora usually assumes that the cor-
pora they use are reliably comparable and focuses
on the design of efficient extraction algorithms.
Therefore, there has been very little literature dis-
cussing the characteristics of comparable corpora
(Maia, 2003). In this section, we introduce some
representative work which tackles comparability
metrics.
Some studies (Sharoff, 2007; Maia, 2003;
McEnery and Xiao, 2007) analyse comparability
by assessing corpus composition, such as struc-
tural criteria (e.g., format and size), and linguistic
criteria (e.g., topic, domain, and genre). Kilgarriff
and Rose (1998) measure similarity and homo-
geneity between monolingual corpora. They gen-
erate word frequency list from each corpus and
then apply ?2 statistic on the most frequent n (e.g.,
500) words of the compared corpora.
11
The work which deals with comparability
measures in cross-lingual comparable corpora is
closer to our work. Saralegi et al (2008) measure
the degree of comparability of comparable cor-
pora (English and Basque) according to the dis-
tribution of topics and publication dates of docu-
ments. They compute content similarity for all the
document pairs between two corpora. These sim-
ilarity scores are then input as parameters for the
EMD (Earth Mover?s Distance) distance measure,
which is employed to calculate the global com-
patibility of the corpora. Munteanu and Marcu
(2005; 2006) select more comparable document
pairs in a cross-lingual information retrieval based
manner by using a toolkit called Lemur1. The
retrieved document pairs then serve as input for
the tasks of parallel sentence and sub-sentence ex-
traction. Smith et al (2010) treat Wikipedia as
a comparable corpus and use ?interwiki? links to
identify aligned comparable document pairs for
the task of parallel sentence extraction. Li and
Gaussier (2010) propose a comparability met-
ric which can be applied at both document level
and corpus level and use it as a measure to se-
lect more comparable texts from other external
sources into the original corpora for bilingual lex-
icon extraction. The metric measures the propor-
tion of words in the source language corpus trans-
lated in the target language corpus by looking up
a bilingual dictionary. They evaluate the met-
ric on the rich-resourced English-French language
pair, thus good dictionary resources are available.
However, this is not the case for under-resourced
languages in which reliable language resources
such as machine-readable bilingual dictionaries
with broad word coverage or word lemmatizers
might be not publicly available.
3 Comparability Metrics
To measure the comparability degree of document
pairs in different languages, we need to translate
the texts or map lexical items from the source lan-
guage into the target languages so that we can
compare them within the same language. Usually
this can be done by using bilingual dictionaries
(Rapp, 1999; Li and Gaussier, 2010; Prachasson
and Fung, 2011) or existing machine translation
tools. Based on this process, in this section we
present three different approaches to measure the
1Available at http://www.lemurproject.org/
comparability of comparable documents.
3.1 Lexical mapping based metric
It is straightforward that we expect a bilingual dic-
tionary can be used for lexical mapping between a
language pair. However, unlike the language pairs
in which both languages are rich-resourced (e.g.,
English-French, or English-Spanish) and dictio-
nary resources are relatively easy to obtain, it is
likely that bilingual dictionaries with good word
coverage are not publicly available for under-
resourced languages (e.g., English-Slovenian, or
English-Lithuanian). In order to address this
problem, we automatically construct dictionaries
by using word alignment on large-scale parallel
corpora (e.g., Europarl and JRC-Acquis2).
Specifically, GIZA++ toolkit (Och and Ney,
2000) with default setting is used for word align-
ment on the JRC-Acquis parallel corpora (Stein-
berger et al, 2006). The aligned word pairs to-
gether with the alignment probabilities are then
converted into dictionary entries. For example,
in Estonian-English language pair, the alignment
example ?kompanii company 0.625? in the word
alignment table means the Estonian word ?kom-
panii? can be translated as (or aligned with) the
English candidate word ?company? with a prob-
ability of 0.625. In the dictionary, the transla-
tion candidates are ranked by translation proba-
bility in descending order. Note that the dictio-
nary collects inflectional form of words, but not
only base form of words. This is because the dic-
tionary is directly generated from the word align-
ment results and no further word lemmatization is
applied.
Using the resulting dictionary, we then per-
form lexical mapping in a word-for-word map-
ping strategy. We scan each word in the source
language texts to check if it occurs in the dic-
tionary entries. If so, the first translation candi-
date are recorded as the corresponding mapping
word. If there are more than one translation can-
didate, the second candidate will also be kept as
the mapping result if its translation probability is
higher than 0.33. For non-English and English
2The JRC-Acquis covers 22 European languages and
provides large-scale parallel corpora for all the 231 language
pairs.
3From the manual inspection on the word alignment re-
sults, we find that if the alignment probability is higher than
0.3, it is more reliable.
12
language pair, the non-English texts are mapped
into English. If both languages are non-English
(e.g., Greek-Romanian), we use English as a pivot
langauge and map both the source and target
language texts into English4. Due to the lack
of reliable linguistic resources in non-English
languages, mapping texts from non-English lan-
guage into English can avoid language process-
ing in non-English texts and allows us to make
use of the rich resources in English for further
text processing, such as stop-word filtering and
word lemmatization5. Finally, cosine similarity
measure is applied to compute the comparability
strength of the compared document pairs.
3.2 Keyword based metric
The lexical mapping based metric takes all the
words in the text into account for comparability
measure, but if we only retain a small number of
representative words (keywords) and discard all
the other less informative words in each docu-
ment, can we judge the comparability of a doc-
ument pair by comparing these words? Our in-
tuition is that, if two document share more key-
words, they should be more comparable. To
validate this, we then perform keyword extrac-
tion by using a simple TFIDF based approach,
which has been shown effective for keyword or
keyphrase extraction from the texts (Frank et al,
1999; Hulth, 2003; Liu et al, 2009).
More specifically, the keyword based metric
can be described as below. First, similar to the
lexical mapping based metric, bilingual dictionar-
ies are used to map non-English texts into En-
glish. Thus, only the English resources are ap-
plied for stop-word filtering and word lemmatiza-
tion, which are useful text preprocessing steps for
keyword extraction. We then use TFIDF to mea-
sure the weight of words in the document and rank
the words by their TFIDF weights in descending
order. The top n (e.g., 30) words are extracted
as keywords to represent the document. Finally,
the comparability of each document pair is deter-
mined by applying cosine similarity to their key-
4Generally in JRC-Acquis, the size of parallel corpora
for most of non-English langauge pairs is much smaller than
that of language pairs which contain English. Therefore, the
resulting bilingual dictionaries which contain English have
better word coverage as they have many more dictionary en-
tries.
5We use WordNet (Fellbaum, 1998) for word lemmatiza-
tion.
word lists.
3.3 Machine translation based metrics
Bilingual dictionary is used for word-for-word
translation in the lexical mapping based metric
and words which do not occur in the dictionary
will be omitted. Thus, the mapping result is like
a list of isolated words and information such as
word order, syntactic structure and named entities
can not be preserved. Therefore, in order to im-
prove the text translation quality, we turn to the
state-of-the-art SMT systems.
In practice, we use Microsoft translation API6
to translate texts in under-resourced languages
(e.g, Lithuanian and Slovenian) into English and
then explore several features for comparability
metric design, which are listed as below.
? Lexical feature: Lemmatized bag-of-word
representation of each document after stop-
word filtering. Lexical similarity (denoted
by WL) of each document pair is then ob-
tained by applying cosine measure to the lex-
ical feature.
? Structure feature: We approximate it by
the number of content words (adjectives, ad-
verbs, nouns, verbs and proper nouns) and
the number of sentences in each document,
denoted by CD and SD respectively. The in-
tuition is that, if two documents are highly
comparable, their number of content words
and their document length should be similar.
The structure similarity (denoted by WS) of
two documentsD1 andD2 is defined as bel-
low.
WS = 0.5 ? (CD1/CD2)+ 0.5 ? (SD1/SD2)
suppose that CD1<=CD2, and SD1<=SD2.
? Keyword feature: Top-20 words (ranked by
TFIDF weight) of each document. keyword
similarity (denoted by WK) of two docu-
ments is also measured by cosine.
? Named entity feature: Named entities of
each document. If more named entities co-
occur in two documents, they are very likely
to talk about the same event or subject and
6Available at http://code.google.com/p/microsoft-
translator-java-api/
13
thus should be more comparable. We use
Stanford named entity recognizer7 to extract
named entities from the texts (Finkel et al,
2005). Again, cosine is then applied to mea-
sure the similarity of named entities (denoted
by WN ) between a document pair.
We then combine these four different types of
score in an ensemble manner. Specifically, a
weighted average strategy is applied: each indi-
vidual score is associated with a constant weight,
indicating the relative confidence (importance) of
the corresponding type of score. The overall com-
parability score (denoted by SC) of a document
pair is thus computed as below:
SC = ? ?WL + ? ?WS + ? ?WK + ? ?WN
where ?, ?, ?, and ? ? [0, 1], and ?+?+?+? =
1. SC should be a value between 0 and 1, and
larger SC value indicates higher comparability
level.
4 Experiment and Evaluation
4.1 Data source
To investigate the reliability of the proposed
comparability metrics, we perform experiments
for 6 language pairs which contain under-
resoured languages: German-English (DE-EN),
Estonian-English (ET-EN), Lithuanian-English
(LT-EN), Latvian-English (LV-EN), Slovenian-
English (SL-EN) and Greek-Romanian (EL-RO).
A comparable corpus is collected for each lan-
guage pair. Based on the definition of compa-
rability levels (see Section 1), human annota-
tors fluent in both languages then manually anno-
tated the comparability degree (parallel, strongly-
comparable, and weakly-comparable) at the doc-
ument level. Hence, these bilingual comparable
corpora are used as gold standard for experiments.
The data distribution for each language pair, i.e.,
number of document pairs in each comparability
level, is given in Table 1.
4.2 Experimental results
We adopt a simple method for evaluation. For
each language pair, we compute the average
scores for all the document pairs in the same com-
parability level, and compare them to the gold
7Available at http://nlp.stanford.edu/software/CRF-
NER.shtml
Language
pair
#document
pair
parallel strongly-
comparable
weakly-
comparable
DE-EN 1286 531 715 40
ET-EN 1648 182 987 479
LT-EN 1177 347 509 321
LV-EN 1252 184 558 510
SL-EN 1795 532 302 961
EL-RO 485 38 365 82
Table 1: Data distribution of gold standard corpora
standard comparability labels. In addition, in or-
der to better reveal the relation between the scores
obtained from the proposed metrics and compara-
bility levels, we also measure the Pearson correla-
tion between them8. For the keyword based met-
ric, top 30 keywords are extracted from each text
for experiment. For the machine translation based
metric, we empirically set ? = 0.5, ? = ? = 0.2,
and ? = 0.1. This is based on the assumption
that, lexical feature can best characterize the com-
parability given the good translation quality pro-
vided by the powerful MT system, while keyword
and named entity features are also better indica-
tors of comparability than the simple document
length information.
The results for the lexical mapping based met-
ric, the keyword based metric and the machine
translation based metric are listed in Table 2, 3,
and 4, respectively.
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.545 0.476 0.182 0.941
ET-EN 0.553 0.381 0.228 0.999
LT-EN 0.545 0.461 0.225 0.964
LV-EN 0.625 0.494 0.179 0.973
SL-EN 0.535 0.456 0.314 0.987
EL-RO 0.342 0.131 0.090 0.932
Table 2: Average comparability scores for lexical map-
ping based metric
Overall, from the average scores for each
comparability level presented in Table 2, 3,
and 4, we can see that, the scores obtained
from the three comparability metrics can reli-
8For correlation measure, we use numerical calibration
to different comparability degrees: ?Parallel?, ?strongly-
comparable? and ?weakly-comparable? are converted as 3,
2, and 1, respectively. The correlation is then computed
between the numerical comparability levels and the cor-
responding average comparability scores automatically de-
rived from the metrics.
14
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.526 0.486 0.084 0.941
ET-EN 0.502 0.345 0.184 0.990
LT-EN 0.485 0.420 0.202 0.954
LV-EN 0.590 0.448 0.124 0.975
SL-EN 0.551 0.505 0.292 0.937
EL-RO 0.210 0.110 0.031 0.997
Table 3: Average comparability scores for keyword
based metric
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.912 0.622 0.326 0.999
ET-EN 0.765 0.547 0.310 0.999
LT-EN 0.755 0.613 0.308 0.984
LV-EN 0.770 0.627 0.236 0.966
SL-EN 0.779 0.582 0.373 0.988
EL-RO 0.863 0.446 0.214 0.988
Table 4: Average comparability scores for machine
translation based metric
ably reflect the comparability levels across dif-
ferent language pairs, as the average scores
for higher comparable levels are always sig-
nificantly larger than those of lower compara-
ble levels, namely SC(parallel)>SC(strongly-
comparable)>SC(weakly-comparable). In addi-
tion, in all the three metrics, the Pearson correla-
tion scores are very high (over 0.93) across dif-
ferent language pairs, which indicate that there
is strong correlation between the comparability
scores obtained from the metrics and the corre-
sponding comparability level.
Moreover, from the comparison of Table 2, 3,
and 4, we also have several other findings. Firstly,
the performance of keyword based metric (see
Table 3) is comparable to the lexical mapping
based metric (see Table 2) as their comparability
scores for the corresponding comparability levels
are similar. This means it is reasonable to deter-
mine the comparability level by only comparing a
small number of keywords of the texts. Secondly,
the scores obtained from the machine translation
based metric (see Table 4) are significantly higher
than those in both the lexical mapping based met-
ric and the keyword based metric. Clearly, this
is due to the advantages of using the state-of-the-
art MT system. In comparison to the approach
of using dictionary for word-for-word mapping,
it can provide much better text translation which
allows detecting more proportion of lexical over-
lapping and mining more useful features in the
translated texts. Thirdly, in the lexical mapping
based metric and keyword based metric, we can
also see that, although the average scores for EL-
RO (both under-resourced languages) conform to
the comparability levels, they are much lower than
those of the other 5 language pairs. The reason
is that, the size of the parallel corpora in JRC-
Acquis for these 5 language pairs are significantly
larger (over 1 million parallel sentences) than that
of EL-EN, RO-EN9, and EL-RO, thus the result-
ing dictionaries of these 5 language pairs also con-
tain many more dictionary entries.
5 Application
The experiments in Section 4 confirm the reli-
ability of the proposed metrics. The compara-
bility metrics are thus useful for collecting high-
quality comparable corpora, as they can help filter
out weakly comparable or non-comparable doc-
ument pairs from the raw crawled corpora. But
are they also useful for other NLP tasks, such as
translation equivalent detection from comparable
corpora? In this section, we further measure the
impact of the metrics on parallel phrase extraction
(PPE) from comparable corpora. Our intuition is
that, if document pairs are assigned higher com-
parability scores by the metrics, they should be
more comparable and thus more parallel phrases
can be extracted from them.
The algorithm of parallel phrase extraction,
which develops the approached presented in
Munteanu and Marcu (2006), uses lexical over-
lap and structural matching measures (Ion, 2012).
Taking a list of bilingual comparable document
pairs as input, the extraction algorithm involves
the following steps.
1. Split the source and target language docu-
ments into phrases.
2. Compute the degree of parallelism for each
candidate pair of phrases by using the bilin-
gual dictionary generated from GIZA++
(base dictionary), and retain all the phrase
pairs with a score larger than a predefined
parallelism threshold.
9Remember that in our experiment, English is used as the
pivot language for non-English langauge pairs.
15
3. Apply GIZA++ to the retained phrase pairs
to detect new dictionary entries and add them
to the base dictionary.
4. Repeat Step 2 and 3 for several times (empir-
ically set at 5) by using the augmented dic-
tionary, and output the detected phrase pairs.
Phrases which are extracted by this algorithm
are frequently not exact translation equivalents.
Below we give some English-German examples
of extracted equivalents with their corresponding
alignment scores:
1. But a successful mission ? seiner u?beraus
erfolgreichen Mission abgebremst ?
0.815501989333333
2. Former President Jimmy Carter ? Der
ehemalige US-Pra?sident Jimmy Carter ?
0.69708324976825
3. on the Korean Peninsula ? auf der koreanis-
chen Halbinsel ? 0.8677432145
4. across the Muslim world ? mit der muslim-
ischen Welt ermo?glichen ? 0.893330864
5. to join the United Nations ? der Weg
in die Vereinten Nationen offensteht ?
0.397418711927629
Even though some of the extracted phrases are
not exact translation equivalents, they may still
be useful resources both for SMT and RBMT if
these phrases are passed through an extra pre-
processing stage, of if the engines are modified
specifically to work with semi-parallel translation
equivalents extracted from comparable texts. We
address this issue in the discussion section (see
Section 6).
For evaluation, we measure how the metrics af-
fect the performance of parallel phrase extraction
algorithm on 5 language pairs (DE-EN, ET-EN,
LT-EN, LV-EN, and SL-EN). A large raw compa-
rable corpus for each language pair was crawled
from the Web, and the metrics were then applied
to assign comparability scores to all the docu-
ment pairs in each corpus. For each language pair,
we set three different intervals based on the com-
parability score (SC) and randomly select 500
document pairs in each interval for evaluation.
For the MT based metric, the three intervals are
(1) 0.1<=SC<0.3, (2) 0.3<=SC<0.5, and (3)
SC>=0.5. For the lexical mapping based metric
and keyword based metric, since their scores are
lower than those of the MT based metric for each
comparability level, we set three lower intervals at
(1) 0.1<=SC<0.2, (2) 0.2<=SC<0.4, and (3)
SC>=0.4. The experiment focuses on counting
the number of extracted parallel phrases with par-
allelism score>=0.410, and computes the average
number of extracted phrases per 100000 words
(the sum of words in the source and target lan-
guage documents) for each interval. In addition,
the Pearson correlation measure is also applied to
measure the correlation between the interval11 of
comparability scores and the number of extracted
parallel phrases. The results which summarize the
impact of the three metrics to the performance of
parallel phrase extraction are listed in Table 5, 6,
and 7, respectively.
Language
pair
0.1<=
SC<0.2
0.2<=
SC<0.4
SC>=0.4 correlation
DE-EN 728 1434 2510 0.993
ET-EN 313 631 1166 0.989
LT-EN 258 419 894 0.962
LV-EN 470 859 1900 0.967
SL-EN 393 946 2220 0.975
Table 5: Impact of the lexical mapping based metric to
parallel phrase extraction
Language
pair
0.1<=
SC<0.2
0.2<=
SC<0.4
SC>=0.4 correlation
DE-EN 1007 1340 2151 0.972
ET-EN 438 650 1050 0.984
LT-EN 306 442 765 0.973
LV-EN 600 966 1722 0.980
SL-EN 715 1026 1854 0.967
Table 6: Impact of the keyword based metric to parallel
phrase extraction
From Table 5, 6, and 7, we can see that
for all the 5 language pairs, based on the aver-
age number of extracted aligned phrases, clearly
we have interval (3)>(2)>(1). In other words, in
any of the three metrics, a higher comparability
level always leads to significantly more number
10A manual evaluation of a small set of extracted data
shows that parallel phrases with parallelism score>=0.4 are
more reliable.
11For the purpose of correlation measure, the three inter-
vals are numerically calibrated as ?1?, ?2?, and ?3?, respec-
tively.
16
Language
pair
0.1<=
SC<0.3
0.3<=
SC<0.5
SC>=0.5 correlation
DE-EN 861 1547 2552 0.996
ET-EN 448 883 1251 0.999
LT-EN 293 483 1070 0.959
LV-EN 589 1072 2037 0.982
SL-EN 560 1151 2421 0.979
Table 7: Impact of the machine translation based met-
ric to parallel phrase extraction
of aligned phrases extracted from the comparable
documents. Moreover, although the lexical map-
ping based metric and the keyword based metric
produce lower comparability scores than the MT
based metric (see Section 4), they have similar
impact to the task of parallel phrase extraction.
This means, the comparability score itself does
not matter much, as long as the metrics are re-
liable and proper thresholds are set for different
metrics.
In all the three metrics, the Pearson correla-
tion scores are very close to 1 for all the language
pairs, which indicate that the intervals of compa-
rability scores obtained from the metrics are in
line with the performance of equivalent extrac-
tion algorithm. Therefore, in order to extract more
parallel phrases (or other translation equivalents)
from comparable corpora, we can try to improve
the corpus comparability by applying the compa-
rability metrics beforehand to add highly compa-
rable document pairs in the corpora.
6 Discussion
We have presented three different approaches to
measure comparability at the document level. In
this section, we will analyze the advantages and
limitations of the proposed metrics, and the feasi-
bility of using semi-parallel equivalents in MT.
6.1 Pros and cons of the metrics
Using bilingual dictionary for lexical mapping is
simple and fast. However, as it adopts the word-
for-word mapping strategy and out-of-vocabulary
(OOV) words are omitted, the linguistic structure
of the original texts is badly hurt after mapping.
Thus, apart from lexical information, it is diffi-
cult to explore more useful features for the com-
parability metrics. The TFIDF based keyword ex-
traction approach allows us to select more repre-
sentative words and prune a large amount of less
informative words from the texts. The keywords
are usually relevant to subject and domain terms,
which is quite useful in judging the comparabil-
ity of two documents. Both the lexical mapping
based approach and the keyword based approach
use dictionary for lexical translation, thus rely on
the availability and completeness of the dictionary
resources or large scale parallel corpora.
For the machine translation based metric, it
provides much better text translation than the
dictionary-based approach so that the comparabil-
ity of two document can be better revealed from
the richer lexical information and other useful
features, such as named entities. However, the
text translation process is expensive, as it depends
on the availability of the powerful MT systems12
and takes much longer than the simple dictionary
based translation.
In addition, we use a translation strategy of
translating texts from under-resourced (or less-
resourced) languages into rich-resourced lan-
guage. In case that both languages are under-
resourced languages, English is used as the pivot
langauge for translation. This can compensate the
shortage of the linguistic resources in the under-
resourced languages and take advantages of vari-
ous resources in the rich-resourced languages.
6.2 Using semi-parallel equivalents in MT
systems
We note that modern SMT and RBMT sys-
tems take maximal advantage of strictly parallel
phrases, but they still do not use full potential
of the semi-parallel translation equivalents, of the
type that is illustrated in the application section
(see Section 5). Such resources, even though they
are not exact equivalents contain useful informa-
tion which is not used by the systems.
In particular, the modern decoders do not work
with under-specified phrases in phrase tables, and
do not work with factored semantic features. For
example, the phrase:
But a successful mission ? seiner u?beraus er-
folgreichen Mission abgebremst
The English side contains the word but, which
pre-supposes contrast, and on the Greman side
words u?beraus erfolgreichen (?generally success-
ful?) and abgebremst (?slowed down?) ? which
taken together exemplify a contrast, since they
12Alternatively, we can also train MT systems for text
translation by using the available SMT toolkits (e.g., Moses)
on large scale parallel corpora.
17
have different semantic prosodies. In this example
the semantic feature of contrast can be extracted
and reused in other contexts. However, this would
require the development of a new generation of
decoders or rule-based systems which can suc-
cessfully identify and reuse such subtle semantic
features.
7 Conclusion and Future work
The success of extracting good-quality translation
equivalents from comparable corpora to improve
machine translation performance highly depends
on ?how comparable? the used corpora are. In this
paper, we propose three different comparability
measures at the document level. The experiments
show that all the three approaches can effectively
determine the comparability levels of comparable
document pairs. We also further investigate the
impact of the metrics on the task of parallel phrase
extraction from comparable corpora. It turns out
that higher comparability scores always lead to
significantly more parallel phrases extracted from
comparable documents. Since better quality of
comparable corpora should have better applica-
bility, our metrics can be applied to select highly
comparable document pairs for the tasks of trans-
lation equivalent extraction.
In the future work, we will conduct more com-
prehensive evaluation of the metrics by capturing
its impact to the performance of machine transla-
tion systems with extended phrase tables derived
from comparable corpora.
Acknowledgments
We thank Radu Ion at RACAI for providing us
the toolkit of parallel phrase extraction, and the
three anonymous reviewers for valuable com-
ments. This work is supported by the EU funded
ACCURAT project (FP7-ICT-2009-4-248347) at
the Centre for Translation Studies, University of
Leeds.
References
Bogdan Babych, Serge Sharoff and Anthony Hartley.
2008. Generalising Lexical Translation Strategies
for MT Using Comparable Corpora. Proceedings
of LREC 2008, Marrakech, Morocco.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. Proceedings of
COLING 2002, Taipei, Taiwan.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of ACL 2005, University of
Michigan, Ann Arbor, USA.
Eibe Frank, Gordon Paynter and Ian Witten. 1999.
Domain-specific keyphrase extraction. Proceedings
of IJCAI 1999, Stockholm, Sweden.
Pascale Fung and Percy Cheung. 2004a. Mining very
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. Proceedings
of EMNLP 2004, Barcelona, Spain.
Pascale Fung and Percy Cheung. 2004b. Multi-level
bootstrapping for extracting parallel sentences from
a quasicomparable corpus. Proceedings of COL-
ING 2004, Geneva, Switzerland.
Anette Hulth. 2003. Improved Automatic Keyword
Extraction Given More Linguistic Knowledge. Pro-
ceedings of EMNLP 2003, Sapporo, Japan.
Radu Ion. 2012. PEXACC: A Parallel Data Mining
Algorithm from Comparable Corpora. Proceedings
of LREC 2012, Istanbul, Turkey.
Adam Kilgarriff and Tony Rose. 1998. Measures for
corpus similarity and homogeneity. Proceedings of
EMNLP 1998, Granada, Spain.
Bo Li and Eric Gaussier. 2010. Improving cor-
pus comparability for bilingual lexicon extraction
from comparable corpora. Proceedings of COL-
ING 2010, Beijing, China.
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu.
2009. Unsupervised Approaches for Automatic
Keyword Extraction Using Meeting Transcripts.
Proceedings of NAACL 2009, Boulder, Colorado,
USA.
Belinda Maia. 2003. What are comparable corpora?
Proceedings of the Corpus Linguistics workshop on
Multilingual Corpora: Linguistic requirements and
technical perspectives, 2003, Lancaster, U.K.
Anthony McEnery and Zhonghua Xiao. 2007. Par-
allel and comparable corpora? In Incorporating
Corpora: Translation and the Linguist. Translating
Europe. Multilingual Matters, Clevedon, UK.
Emmanuel Morin, Beatrice Daille, Korchi Takeuchi
and Kyo Kageura. 2007. Bilingual terminology
mining ? using brain, not brawn comparable cor-
pora. Proceedings of ACL 2007, Prague, Czech Re-
public.
Dragos Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. Proceedings of ACL 2006, Syn-
dey, Australia.
Dragos Munteanu and Daniel Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31(4): 477-504.
18
Dragos Munteanu, Alexander Fraser and Daniel
Marcu. 2004. Improved machine translation
performance via parallel sentence extraction from
comparable corpora. Proceedings of HLT-NAACL
2004, Boston, USA.
Franz Och and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. Proceedings of ACL 2000,
Hongkong, China.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compa-
rable Documents. Proceedings of ACL-HLT 2011,
Portland, USA.
Reinhard Rapp. 1995. Identifying Word Translation
in Non-Parallel Texts. Proceedings of ACL 1995,
Cambridge, Massachusetts, USA.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. Proceedings of ACL 1999, College
Park, Maryland, USA.
Xabier Saralegi, Inaki Vicente and Antton Gurrutxaga.
2008. Automatic Extraction of Bilingual Terms
from Comparable Corpora in a Popular Science
Domain. Proceedings of the Workshop on Compa-
rable Corpora, LREC 2008, Marrakech, Morocco.
Serge Sharoff. 2007. Classifying Web corpora into
domain and genre using automatic feature identifi-
cation. Proceedings of 3rd Web as Corpus Work-
shop, Louvain-la-Neuve, Belgium.
Serge Sharoff, Bogdan Babych and Anthony Hartley.
2006. Using Comparable Corpora to Solve Prob-
lems Difficult for Human Translators. Proceedings
of ACL 2006, Syndey, Australia.
Jason Smith, Chris Quirk and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compa-
rable Corpora using Document Level Alignment.
Proceedings of NAACL 2010, Los Angeles, USA.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat and Dan Tufis. 2006. The JRC-
Acquis: A multilingual aligned parallel corpus
with 20+ languages. Proceedings of LREC 2006,
Genoa, Italy.
Kun Yu and Junichi Tsujii. 2009. Extracting bilingual
dictionary from comparable corpora with depen-
dency heterogeneity. Proceedings of HLT-NAACL
2009, Boulder, Colorado, USA.
19
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Design of a hybrid high quality machine translation system 
Kurt Eberle 
Johanna Gei? 
Mireia Ginest?-Rosell 
Bogdan Babych  
Anthony Hartley 
Reinhard Rapp  
Lingenio GmbH Serge Sharoff 
Karlsruher Stra?e 10 
69 126 Heidelberg, Germany 
Martin Thomas 
Centre for Translation Studies 
University of Leeds 
 Leeds, LS2 9JT, UK 
[k.eberle,j.geiss,m.ginesti-rosell] 
@lingenio.de 
[B.Babych,A.Hartley,R.Rapp, 
S.Sharoff,M.Thomas]@leeds.ac.uk  
  
 
 
Abstract 
This paper gives an overview of the 
ongoing FP7 project HyghTra (2010 ? 
2014). The HyghTra project is conducted 
in a partnership between academia and 
industry involving the University of Leeds 
and Lingenio GmbH (company). It adopts a 
hybrid and bootstrapping approach to the 
enhancement of MT quality by applying 
rule-based analysis and statistical 
evaluation techniques to both parallel and 
comparable corpora in order to extract 
linguistic information and enrich the lexical 
and syntactic resources of the underlying 
(rule-based) MT system that is used for 
analysing the corpora. The project places 
special emphasis on the extension of 
systems to new language pairs and 
corresponding rapid, automated creation of 
high quality resources. The techniques are 
fielded and evaluated within an existing 
commercial MT environment. 
1 Motivation 
Statistical Machine Translation (SMT) has been 
around for about 20 years, and for roughly half of 
this time SMT and the 'traditional' Rule-based 
Machine Translation (RBMT) have been seen as 
competing paradigms. During the last decade 
however, there is a trend and growing interest in 
combining the two methodologies. In our approach 
these two approaches are viewed as 
complementary. 
Advantages of SMT are low cost and robustness, 
but definite disadvantages of (pure) SMT are that it 
needs huge amounts of data, which for many 
language pairs are not available and are unlikely to 
become available in the future. Also, SMT tends to 
disregard important classificatory knowledge (such 
as morphosyntactic, categorical and lexical class 
features), which can be provided and used 
relatively easily within non-statistical 
representations.  
On the other hand, advantages of RBMT are that 
its (grammar and lexical) rules and information are 
understandable by humans and can be exploited for 
a lot of applications outside of translation 
(dictionaries, text understanding, dialogue systems, 
etc.).  
The slot grammar approach used in Lingenio 
systems (cf.  McCord 1989, Eberle 2001) is a 
prime example of such linguistically rich 
representations that can be used for a number of 
different applications. Fig.1 shows this by a 
visualization of (an excerpt of) the entry for the 
ambiguous German verb einstellen in the database 
that underlies (a)  the Lingenio translation 
products, where it links up with corresponding set 
of the transfer rules, and (b) Lingenio?s dictionary 
product TranslateDict, which is primarily intended 
for human translators.   
 
101
 
 
Fig 1 a) data base entry einstellen 
('translation' represents links between SL and T entries) 
 
 
 
 
Fig 1 b) product entry einstellen 
 
The obvious disadvantages of RBMT are high cost, 
weaknesses in dealing with incorrect input and in 
making correct choices with respect to ambiguous 
words, structures, and transfer equivalents. 
SMT output is often surprisingly good with respect 
to short distance collocations, but often misses 
correct choices are missed in cases where 
selectional restrictions take effect on distant words. 
RBMT output is generally good if the parser 
assigns the correct analysis to a sentence and  if the 
target words can be correctly chosen from the set 
of alternatives. However, in the presence of 
ambiguous words and structures, and where 
linguistic information is lacking, the decisions may 
be wrong. 
Given the complementarity of SMT and RBMT 
and their very different strengths and weaknesses, 
we take a view that an optimized MT architecture 
must comprise elements of both paradigms. The 
key issue therefore lies in the identification of such 
elements and how to connect them to each other. 
We propose a specific type of hybrid translation ? 
hybrid high quality translation (HyghTra), where 
core RBMT systems are created and enhanced by a 
range of reliable statistical techniques. 
 
2 Development Methodology 
Many hybrid systems described in the literature 
have attempted to put some analytical abstraction 
on top of an SMT kernel.1 In our view this is not 
the best option because, according to the 
underlying philosophy, SMT is linguistically 
ignorant at the beginning and learns all linguistic 
rules automatically from corpora. However, the 
extracted information is typically represented in 
huge data sets which are not readable by humans in 
a natural way. This means that this type of 
architecture does not easily provide interfaces for 
incorporating linguistic knowledge in a canonical 
and simple way. 
Thus we approach the problem from the other end, 
, integrating information derived from corpora 
using statistical methods into RBMT systems. 
Provided the underlying RBMT systems are 
linguistically sound and sufficiently modular in 
structure, we believe this to have greater potential 
for generating high quality output. 
We currently use and carry out the following work 
plan: 
 
(I) Creation of MT systems  
(with rule-based core MT information and 
statistical extension and training): 
(a) We start out with declarative analysis and 
generation components of the considered 
languages, and with basic bilingual dictionaries 
connecting to one another the entries of relatively 
small vocabularies comprising the most frequent 
words of each language in a given translation pair 
(cf. Fig 1 a). 
(b) Having completed this phase, we extend the 
dictionaries and train the analysis-, transfer- and 
generation-components of the rule-based core 
systems using monolingual and bilingual corpora.  
 
                                                           
1 A prominent early example is Frederking and 
colleagues (Frederking & Nirenburg, 1994). For an 
overview of  hybrid MT till the late nineties see Streiter 
et al (1999). More recent  approaches include Groves & 
Way (2006a, 2006b). Commercial implementations 
include AppTek (http://www.apptek.com) and Language 
Weaver (http://www.languageweaver.com). An ongoing 
MT important project investigating hybrid methods is 
EuroMatrixPlus (http://www.euromatrixplus.net/) 
102
(II) Error detection and improvement cycle:  
(a) We automatically discover the most frequent 
problematic grammatical constructions and 
multiword expressions for commercial RBMT and 
SMT systems using automatic construction-based 
evaluation as proposed in (Babych and Hartley, 
2009) and develop a framework for fixing 
corresponding grammar rules and extending 
grammatical coverage of the systems in a semi-
automatic way. This shortens development time for 
commercial MT and contributes to yielding 
significantly higher translation quality. 
 
(III) Extension to other languages: 
Structural similarity and translation by pivot 
languages is used to obtain extension to further 
languages: 
High-quality translation between closely related 
languages (e.g., Russian and Ukrainian or 
Portuguese and Spanish) can be achieved with 
relatively simple resources (using linguistic 
similarity, but also homomorphism assumptions 
with respect to parallel text, if available), while 
greater efforts are put into ensuring better-quality 
translation between more distant languages (e.g. 
German and Russian). According to our prior 
research (Babych et al, 2007b) the pipeline 
between languages of different similarity results in 
improved translation quality for a larger number of 
language pairs (e.g., MT from Portuguese or 
Ukrainian into German is easier if there are high-
quality analysis and transfer modules for Spanish 
and Russian into German (respectively). Of course, 
(III) draws heavily on the detailed analysis and MT 
systems that the industrial partner in HyghTra 
provides for a number of languages. 
 
In the following sections we give more details of 
the work currently done with regard to (I) and with 
regard to parts of (II): the creation of a new MT 
system following the strategy sketched. We cannot 
go further into detail with (II) and (III) here, which 
will become a priority for future research. 
3 Creation of a new system 
Early pilot studies covering some aspects of the 
strategy described here (using information from 
pivot languages and similarity) showed promising 
results (Rapp, 1999; Rapp & Mart?n Vide, 2007; 
see also Koehn & Knight, 2002). 
We expect that the proposed semi-automatic 
creation of a new MT system as sketched above 
will work best if one of the two languages involved 
is already 'known' by modules to which the system 
has access. Against the background of the pipeline 
approach mentioned above in (III), this means that 
we assume an analysis and translation system that 
continuously grows by 'learning' new languages 
where 'learning' is facilitated by information about 
the languages already 'known' and by exploiting 
similarity assumptions ? and, of course, by being 
fed with information prepared and provided by the 
human 'companion' of the system. 
From this perspective, we assume the following 
steps of extending the system (with work done by 
the 'companion' and work done by the system) 
 
1. Acquire parallel and comparable corpora. 
2. Define a core of the morphology of the new 
language and compile a basic dictionary for the 
most frequent words and translations. 
Morphological representations and features for 
new languages are derived both manually and 
automatically, as proposed in (Babych et al, 
2012 (in preparation)). 
3. Using established alignment technology (e.g. 
Giza++) and parallel corpora, generate a first 
extension of this dictionary. 
4. Expand the dictionary of step 3 using 
comparable corpora as proposed in a study by 
Rapp (1999). This is applicable mainly to single 
word units. 
5. Expand coverage of multiword-units using 
novel technology. 
6. Cross-validate the new dictionary with respect 
to available ones by transitivity. 
7. Integrate the new dictionary into the new MT 
system as developing from reusing components 
and adding new components as in 8. 
8. Complete morphology and spell out declarative 
analysis and generation grammar for the new 
language. 
9. Automatically evaluate the translations of the 
most frequent grammatical constructions and 
multiword expressions in a machine-translated 
corpus, prioritising support for these 
constructions with a type of risk-assessment 
framework proposed in Babych and Hartley 
(2008). 
10. Extend support for high-priority constructions 
semi-automatically by mining correct 
103
translations from parallel corpora. 
11. Train and evaluate the new grammar and 
transfer of the new MT system using the new 
dictionary on the basis of available parallel 
corpora. 
 
The following sections give an overview of the 
different steps. 
Step 1: Acquire parallel and comparable 
corpora 
As our parallel corpus, we use the Europarl. The 
size of the current version is up to 40 million 
words per language, and several of the languages 
we are currently considering are covered. Also, we 
make use of other parallel corpora such as the 
Canadian Hansards (Proceedings of the Canadian 
Parliament) for the English?French language pair. 
For non-EU Languages (mainly Russian), we 
intend to conduct a pilot study to establish the 
feasibility of retrieving parallel corpora from the 
web, a problem for which various approaches have 
been proposed (Resnik, 1999; Munteanu & Marcu, 
2005; Wu & Fung, 2005).  
In addition to the parallel corpora, we will need 
large monolingual corpora in the future (at least 
200 million words) for each of the six languages. 
Here, we intend to use newspaper corpora 
supplemented with text collections downloadable 
from the web.  
The corpora are stored in a database that allows 
for assigning analyses of different depth and nature 
to the sentences and for alignment between the 
sentences and their analyses. The architecture of 
this database and the corresponding analysis and 
evaluation frontend is described in (Eberle et al
2010, 2012). Section Results contains examples of 
such representations. 
Step 2: Compile a basic dictionary for the most 
frequent words 
A prerequisite of the suggested hybrid approach 
with rule-based kernel is to define morphological 
classifications for the new language(s). This is 
done exploiting similarities to the classifications as 
available for the existing languages. Currently, this 
has been carried out for Dutch (on the basis of 
German) and for Spanish (on the basis of 
French/other Romance languages). The most 
frequent words (the basic vocabulary of a 
language) are typically also the most ambiguous 
ones. Since the Lingenio systems are lexically 
driven transfer systems (cf. Eberle 2001), we 
define (a) structural conditions,  which inform the 
choice of the possible target words (single words 
or multiword expressions) and (b)restructuring 
conditions, as necessary (cf. Fig 1 a:  attributes 
'transfer conditions' and 'structural change'). In 
order to ensure quality this must be done by human 
lexicographers and therefore costly for a large 
dictionary. However, we manually create only very 
small basic dictionaries and extend these (semi-
automatically) step 3 and those which follow. 
Some important morphosyntactic features of the 
language are derived from a monolingual corpus 
annotated with publicly available part-of-speech 
taggers and lemmatisers. However, these tools 
often do not explicitly represent linguistic features 
needed for the generation stage in RBMT. In 
(Babych et al, 2012) we propose a systematic 
approach to recovering such missing generation-
oriented representations from grammar models and 
statistical combinatorial properties of annotated 
features. 
Step 3: Generating dictionary extensions from 
parallel corpora 
Based on parallel corpora, dictionaries can be 
derived using established techniques of automatic 
sentence alignment and word alignment. For 
sentence alignment, the length-based Gale & 
Church aligner (1993) can be used, or ? 
alternatively ? Dan Melamed?s GSA-algorithm 
(Geometric Sentence Alignment; Melamed, 1999).  
For segmentation of text we use corresponding 
Lingenio-tools (unpublished).2 
For word alignment Giza++ (Och & Ney, 2003) is 
the standard tool. Given a word alignment, the 
extraction of a (SMT) dictionary is relatively 
straightforward. With the exception of sentence 
segmentation, these algorithms are largely 
language independent and can be used for all of the 
languages that we consider. We did this for a 
number of language pairs on the basis of the 
                                                           
2  If these cannot be applied because of  lack of 
information about a language, we intend to use the 
algorithm by Kiss & Strunk (2006). An open-source 
implementation of parts of the Kiss & Strunk algorithm 
is available from Patrick Tschorn at 
http://www.denkselbst.de/sentrick/index.html. 
104
Europarl-texts considered (as stored in our 
database). In order to optimize the results we use 
the dictionaries of step 1 as set of cognates (cf. 
Simard at al 1992, Gough & Way 2004), as well as 
other words easily obtainable from the internet that 
can be used for this purpose (like company names 
and other named entities with cross-language 
identity and terminology translations). Using the 
morphology component of the new language and 
the categorial information from the transfer 
relation, we compute the basic forms of the 
inflected words found. Later, we intend to further 
improve the accuracy of word alignment by 
exploiting chunk type syntactic information of the 
narrow context of the words (cf. Eberle & Rapp 
2008). An early stage variant of this is already used 
in Lingenio products. The corresponding function 
AutoLearn<word> extracts new word relations on 
the basis of existing dictionaries and (partial) 
syntactic analyses. (Fig 2 gives an example). 
 
 
 
 
 
 
 
 
 
 
Fig 2 AutoLearn<word>: new entries using 
transfer links and syntactic analysis 
 
Given the relatively small size of the available 
parallel corpora, we expect that the automatically 
generated dictionaries will comprise about 20,000 
entries each (This corresponds to first results on 
the basis of German?English). This is far too 
small for a serious general purpose MT system. 
Note that, in comparison, the English?German 
dictionary used in the current Lingenio MT 
product comprises more than 480,000 keywords 
and phrases. 
Step 4: Expanding dictionaries using 
comparable corpora (word equations) 
In order to expand the dictionaries using a set of 
monolingual comparable corpora, the basic 
approach pioneered by Fung & McKeown (1997) 
and Rapp (1995, 1999) is to be further developed 
and refined in the second phase of the project as to 
obtain a practical tool that can be used in an 
industrial context. 
The basic assumption underlying the approach 
is that across languages there is a correlation 
between the co-occurrences of words that are 
translations of each other. If ? for example ? in a 
text of one language two words A and B co-occur 
more often than expected by chance, then in a text 
of another language those words that are 
translations of A and B should also co-occur more 
frequently than expected. It is further assumed that 
a small dictionary (as generated in step 2) is 
available at the beginning, and that the aim is to 
expand this basic lexicon. Using a corpus of the 
target language, first a co-occurrence matrix is 
computed whose rows are all word types occurring 
in the corpus and whose columns are all target 
words appearing in the basic lexicon. Next a word 
of the source language is considered whose 
translation is to be determined. Using the source-
language corpus, a co-occurrence vector for this 
word is computed. Then all known words in this 
vector are translated to the target language. As the 
basic lexicon is small, only some of the 
translations are known. All unknown words are 
discarded from the vector and the vector positions 
are sorted in order to match the vectors of the 
target-language matrix. Using standard measures 
for vector similarity, the resulting vector is 
compared to all vectors in the co-occurrence 
matrix of the target language. The vector with the 
highest similarity is considered to be the 
translation of our source-language word. 
From a previous pilot study (Rapp, 1999) it can 
be expected that this methodology achieves an 
accuracy in the order of 70%, which means that 
only a relatively modest amount of manual post-
editing is required.  
The automatically generated results are 
improved and the amount of post-editing is 
reduced by exploiting sense (disambiguation) 
information as available from the analysis 
component for the 'known' language of the new 
language pair.. Also we try to exploit categorial 
and underspecified syntactic information of the 
contexts of the words similar to what has been 
suggested for improving word alignment in the 
previous step (see also Fig.2). Also, as the frequent 
words are already covered by the basic lexicon 
(whose production from parallel corpora on the 
basis of a manually compiled kernel does not show 
 
105
an ambiguity problem of similar significance), and 
as experience shows that most low frequency 
words in a full-size lexicon tend to be 
unambiguous, the ambiguity problem is reduced 
further for the words investigated and extracted by 
this comparison method. 
Step 5: Expanding dictionaries using 
comparable corpora (multiword units) 
In order to account for technical terms, idioms, 
collocations, and typical short phrases, an 
important feature of an MT lexicon is a high 
coverage of multiword units. Very recent work 
conducted at the University of Leeds (Sharoff et 
al., 2006) shows that dictionary entries for such 
multiword units can be derived from comparable 
corpora if a dictionary of single words is available. 
It could even be shown that this methodology can 
be superior to deriving multiword-units from 
parallel corpora (Babych et al, 2007). This is a 
major breakthrough as comparable corpora are far 
easier to acquire than parallel corpora. It even 
opens up the possibility of building domain-
specific dictionaries by using texts from different 
domains. 
The outline of the algorithm is as follows: 
? Extract collocations from a corpus of the 
source language (Smadja, 1993) 
? To translate a collocation, look up all its 
words using any dictionary 
? Generate all possible permutations 
(sequences) of the word translations 
? Count the occurrence frequencies of these 
sequences in a corpus of the target 
language and test for significance 
? Consider the most significant sequence to 
be the translation of the source language 
collocation 
Of course, in later steps of the project, we will 
experiment on filtering these sequences by 
exploiting structural knowledge similarly to what 
was described in the two previous steps. This can 
be obtained on the basis of the declarative analysis 
component of the new language which is 
developed in parallel. 
Step 6: Cross-validate dictionaries 
The combination of the corpus-based methods for 
automatic dictionary generation as described in 
steps 3 to 5 will lead to high coverage dictionaries 
as the availability of very large monolingual 
corpora is no major problem for our languages. 
However, as all steps are error prone, it can be 
expected that a considerable number of dictionary 
entries (e.g. 50%) are not correct. To facilitate (but 
not eliminate) the manual verification of the 
dictionary, we will  perform an automatic cross-
check which utilizes the dictionaries? property of 
transitivity. What we mean by this is that if we 
have two dictionaries, one translating from 
language A to language B, the other from language 
B to language C, then we can also translate from 
language A to C by use of the intermediate 
language (or interlingua) B. That is, the property of 
transitivity, although having some limitations due 
to ambiguity problems, can be exploited to 
automatically generate a raw dictionary for A to C. 
Lingenio  has some experience with this method 
having exploited it for extending and improving its 
English ? French dictionaries using French ? 
German and German ? English. 
As the corpus-based approach (steps 3 to 5) 
allows us to also generate this type of dictionary  
via comparable corpora, we have two different 
ways to generate a dictionary for a particular 
language pair. This means that we can validate one 
with the other. Furthermore, with increasing 
number of language pairs created, there are more 
and more languages that can serve as interlingua or 
'pivot': This, step by step, gives an increasing 
potential for mutual cross-validation.  
Specific attention will be paid to automating as 
far as possible the creation of selectional 
restrictions to be assigned to the transfer relations 
of the new dictionaries in all steps of dictionary 
creation (2?6). We will try to do this on the basis 
of the analysis components as available for the 
languages considered: These are: a completely 
worked out analysis component for the 'old' 
language, a declarative (chunk parsing) component 
for the new one (compare the two following steps 
for this).  
Step 7: Integrate dictionaries in existing 
machine translation systems 
Lingenio has a relatively rich infrastructure for 
automatic importation of various kinds of lexical 
information into the database used by the analyses 
and translation systems. If necessary the 
information on hand (for instance from 
conventional dictionaries of publishing houses) is 
106
completed and normalized during or before 
importation. This may be executed completely 
automatically ? by using the existing analyses 
components and resources respectively as 
databases ? or interactively ? by asking the 
lexicographer for additional information, if needed. 
For example, there may be a list of multiword 
expressions to be imported into the database. In 
order to have available correct syntactic and 
semantic information for these expressions, they 
are analysed by the parser of the corresponding 
language. From the analysis found, the information 
necessary to describe the new lemma in the lexicon 
with respect to semantic type and syntactic 
structure is obtained. The same information is used 
to automatically create correct restructuring 
constraints for translation relations which use the 
new lemma as target. If the parser does not find a 
sound syntactic description, for example because 
some basic information or the expression is 
missing in the lexical database, the lexicographer is 
asked for the missing information or is handed 
over the expression to code it manually.  
Using these tools importation of new lexical 
information, as provided in the previous steps, is 
considerably accelerated.  
Step 8: Compile rule bases for new language 
pairs 
Although experience clearly shows that 
construction and maintenance of the dictionaries is 
by far the most expensive task in (rule-based) 
Machine Translation, the grammars (analysis and 
generation) must of course be developed and 
maintained also. Lingenio has longstanding 
experience with the development of grammars, 
dictionaries and all other components of RBMT.  
The used grammar formalism (slot grammar, 
cf. McCord 1991) is unification based and its 
structuring focuses on dependency, where phrases 
are analysed into heads and grammatical roles ? so 
called (complement and adjunct) slots.  
The grammar formalism and basic rule types 
are designed in a very general way in order to 
allow good portability from one language to 
another such that spelling out the declarative part 
of a grammar does not take very much time (2-4 
person months approx. for relatively similar 
languages like Romance languages according to 
our experience). The portation of linguistic rules to 
new languages is also facilitated by the modular 
design with clearly defined interfaces that make it 
relatively straightforward to integrate information 
from corpora. 
Given a parallel corpus as acquired in step 1, 
the following procedure defines grammar develop-
ment:  
 
1. Define a declarative grammar for the new 
language and train this grammar on the parallel 
-corpus according to the following steps: 
2. Use a chunk parser for the grammar on the 
basis of an efficient part-of-speech tagger for 
the new language.  
3. Combine the chunk analyses of the sentence, 
according to suggestions for packed syntactic 
structures (cf. Schiehlen 2001 and others) and 
underspecified representation structures 
respectively (cf. Eberle, 2004, and others), 
such that the result represents a disjunction of 
the possible analyses of the sentence. 
4. Filter the alternatives of the representation by 
using mapping constraints between source and 
target sentence as can be computed from the 
lexical transfer relations and the structural 
analysis of the sentence. For instance, if we 
know, as in the example of the last section, that 
in the source sentence there is a relative clause 
with lexical elements A, B, . . . modifying a 
head H and that there are translations TH, TA, 
TB, . . . of H, A, B,. . . , in the target sentence 
which, among other possibilities, can be 
supposed to stand in a similar structural 
relation there, then we prefer this relation to 
the competing structural possibilities. (Fig. 3 in 
section results shows the corresponding 
selection for a German-Spanish example in the 
project database). 
5. For each of the remaining structural 
possibilities of the thus revised underspecified 
representation, take its lexical material and 
underspecified structuring as a context for its 
successful firing. For instance, if the 
possibility is left that O is the direct object of 
VP, where VP is an underspecified verbal 
phrase and O an underspecified nominal 
phrase (i.e. where details of the substructuring 
are not spelled out), take the sentence as a 
reference for direct object complementation 
and O and VP as contexts which accept this 
complementation. 
107
6. Develop more abstract conditions from the 
conditions learned according to (5) and 
integrate the different cases. 
7. Tune the results using standard methods of 
corpus-based linguistics. Among other things 
this means: Distinguish between training and 
test corpora, adjust weights according to the 
results of test runs, etc. 
 
The basic idea of the proposed learning procedure 
is similar to that used with respect to learning 
lexical transfer relations: Do not define the 
statistical model for the ?ignorant? state, where the 
surface items of the bilingual corpora are 
considered. Instead, define it for appropriate 
maximally abstract analyses of the sentences 
(which, of course, must be available 
automatically), because, then, much smaller sets of 
data will do. Here, the important question is: What 
is the most abstract level of representation that can 
be reached automatically and which shows reliable 
results? We think that it is the level of 
underspecified syntactic description as used in the 
procedure above. 
The result of training the grammar is a set of 
rules which assign weights and contexts to each 
filler rule of the declarative grammar and thus 
allow to estimate how likely it is that a particular 
rule is applied in a particular context in comparison 
with other rules (Fig. 4 and 5 in section results 
give an overview of the relevance of  grammar 
rules and their triggering conditions w.r.t. 
German).  
We mentioned that the task of translating texts 
into each other does not presuppose that each 
ambiguity in a source sentence is resolved. On the 
contrary, translation should be ambiguity 
preserving (cf. Kay, Gawron & Norvig 1994, 
compare the example above). It is obvious that 
underspecified syntactic representations as 
suggested here are also especially suited for 
preserving ambiguities appropriately.  
Step 9: Automatically evaluate translations of 
the most frequent grammatical constructions 
and multiword expressions in a machine-
translated corpus 
In a later work package of the project, we will run 
a large parallel corpus through available 
(competitive) MT engines, which will be enhanced 
by automatic dictionaries developed during the 
previous stages. On the source-language side of the 
corpus we will automatically generate lists of 
frequent multiword expressions (MWEs) and 
grammatical constructions using the methodology 
proposed in (Sharoff et al, 2006). For each of the 
identified MWEs and constructions we will 
generate a parallel concordance using open-source 
CSAR architecture developed by the Leeds team 
(Sharoff, 2006). The concordance will be 
generated by running queries to the sentence-
aligned parallel corpora and will return lists of 
corresponding sentences from gold-standard 
human translations and corresponding sentences 
generated by MT. Each of these concordances will 
be automatically evaluated using standard MT 
evaluation metrics, such as BLEU. Under these 
settings parallel concordances will be used as 
standard MT evaluation corpora in an automated 
MT evaluation scenario. 
Normally BLEU gives reliable results for MT 
corpora over 7000 words. However, in (Babych 
and Hartley, 2009; Babych and Hartley, 2008) we 
demonstrated that if the corpus is constructed in 
this controlled way, where evaluated fragments of 
sentences are selected as local contexts for specific 
multiword expressions or grammatical 
constructions, then BLEU scores have another 
?island of stability? for much smaller corpora, 
which now may consist of only five or more 
aligned concordance lines. This concordance-based 
evaluation scenario gives correct predictions of 
translation quality for the local context of each of 
the evaluated expressions. 
The scores for the evaluated MWEs and 
constructions will be put in a risk-assessment 
framework, where we will balance the frequency 
of constructions and their translation quality. The 
top priority receive the most frequent expressions 
that are the most problematic ones for a particular 
MT engine, i.e., with queries with lowest BLEU 
scores for their concordances. This framework will 
allow MT developers to work down the priority list 
and correct or extend coverage for those 
constructions which will have the biggest impact 
on MT quality. 
Step 10: Extend support for high-priority 
constructions semi-automatically by mining 
correct translations from parallel corpora 
At this stage we will automate the procedure of 
correcting errors and extending coverage for 
108
problematic MWEs and grammatical 
constructions, identified in Step 9. For this we will 
exploit alignment between source-language 
sentences and gold-standard human translations. In 
the target human translations we will identify 
linguistically-motivated multiword expressions, 
e.g., using part-of-speech patterns or tf-idf 
distribution templates (Babych et al, 2007) and 
run standard alignment tools (e.g., GIZA++) for 
finding the most probable candidate MWEs that 
correspond to the problematic source-language 
expressions. Source and target MWEs paired in 
this way will form the basis for automatically-
generated grammar rules. The rules will normally 
generalise several pairs of MWEs, and may be 
underspecified for certain lexical or morphological 
features. Later such rules will be manually checked 
and corrected by language specialists in MT 
development teams that work on specific 
translation directions. 
This procedure will allow to speed up the grammar 
development procedure for large-scale MT projects 
and will focus on grammatical constructions with 
the highest impact on MT quality, establishing 
them as a top priority for MT developers. In 
HyghTra and with respect to the languages 
considered there, this procedure will be integrated 
into the grammar development and optimization of 
step 8, in particular it will be related to step 4 of 
the procedure sketched there. With regard to 
integration, we aim at an interleaved architecture in 
the long run.  
Step 11: Bootstrap the system 
In Step 11, the new grammar and the transfer of 
the new MT system and the new dictionary may be 
mutually trained further using the steps before and 
applying the system to additional corpora. 
 
4 Results 
Declarative slot grammars for Dutch and Spanish 
have been developed using the patterns of German 
and French ? where declarative  means that there 
has been used no relevant semantic or other 
information in order to spell out weighting or 
filters for rule application -- the only constraint 
being morphosyntactic accessibility. The necessary 
morphological information has been adapted 
similarly from the corresponding model languages. 
The basic dictionaries have been compiled 
manually (Dutch) or extracted from a conventional 
electronic dictionary (translateDict Spanish).  
For a subset of the Spanish corpus (reference 
sentences of the grammar, parts of the open source 
Leeds corpus (Sharoff, 2006), and Europarl), 
syntactic analyses have been computed and stored 
in the database. As the number of analyses grows 
extremely with the length of sentences, only 
relatively short sentences (up to 15 words)  have 
been considered. These analyses are currently 
compared to the analyses of the German 
translations of the corresponding sentences (one 
translation per sentence), which are taken as a kind 
of 'gold' standard as the German analysis 
component (as part of the translation products) has 
proven to be sufficiently reliable. On the basis of 
the comparison a preference on the competitive 
analyses of the Spanish sentence is entailed and 
used for defining a statistical evaluation 
component for the Spanish grammar. Fig.3 shows 
the corresponding representations in the database 
for the sentence Aumenta la demana de energ?a 
el?ctrica por la ola de calor3  and its translation die 
Nachfrage nach Strom steigt wegen der 
Hitzewelle/the demand for electricity increases 
because of the heat-wave. 
 
 
 
 
 
 
 
 
 
 
Fig.3 Selection of analyses via correspondences 
(prefer first Spanish analysis because of subj-congruity) 
 
The analyses are associated with the corresponding 
creation protocols, which are structured lists whose 
items describe, via the identifiers, which rule has 
been applied when and to what structures in the 
process of creating the analysis. From the selection 
of a best analysis for a sentence, we can entail the 
circumstances under which the application of 
particular rules are preferred. This has been carried 
                                                           
3 Sentence taken from the online newspaper El D?a de 
Concepci?n del Uruguay 
 
 
109
out - not yet for the 'new' language Spanish, but for 
the 'known' language German, in order to obtain a 
measure about how correctly the existing grammar 
evaluation component can be replaced by the 
results of the corresponding statistical study.  
 
Fig.4  Frequency of applications of rules 
 
 
 cluster 
applications 
similarity feas  mod feas head 
383, 384,.. 0,86 sent, ... emosentaffv,.. 
557,558,566,.. 0,68 denselb,.. gebv, ... 
 
Fig.5  Preliminary constraints related to grammar 
rule clusters 
 
Fig.4 shows the distribution of rule usages within 
the training set of analyses (of approx.30.000 
sentences). 390 different rules were used with a 
total of 133708 rule applications. The subject rule 
(383) and the noun determiner rule (46) the most 
used rules (35% of all applications). Fig 5. 
illustrates the preliminary results of a clustering 
algorithm where different rule applications are 
grouped into clusters and the key features of the 
head and modifier phrases for each cluster are 
extracted. 
Currently, we try to determine further and tare 
the linguistic features and the weighting which 
models best the evaluation for German. (The gold 
standard that is used in this test is the set of 
analyses mentioned above). The investigations are 
not yet completed, but preliminary results on the 
basis of the morphosyntactic and semantic 
properties of the neighboring elements are 
promising. After consolidation, the findings will be 
transferred to Spanish on the basis of the selection 
procedure illustrated in Fig. 3. The next step of 
grammar training in the immediate future will 
consist of  changing the focus to underspecified 
analyses as described in step 8 
5 Conclusions 
The project tries to make state-of-the-art statistical 
methods available for dictionary development and 
grammar development for a rule-based dominated 
industrial setting and to exploit such methods 
there.  
With regard to SMT dictionary creation, it goes 
beyond the current state of the art as it also aims at 
developing and applying algorithms for the semi-
automatic generation of bilingual dictionaries from 
unrelated monolingual (i.e., comparable) corpora 
of the source and the target language, instead of 
using relatively literally translated (i.e., parallel) 
texts only. Comparable corpora are far easier to 
obtain than parallel corpora. Therefore the 
approach offers a solution to the serious data 
acquisition bottleneck in SMT. This approach is 
also more cognitively plausible than previous 
suggestions on this topic, since human bilinguality 
is normally not based on memorizing parallel texts. 
Our suggestion models human capacity to translate 
texts using linguistic knowledge acquired from 
monolingual data, so it also exemplifies many 
more features of a truly self-learning MT system 
(shared also by a human translator).  
In addition, the proposal suggests a new 
method for spelling out grammars and parsers for 
languages by splitting grammars into declarative 
kernels and trainable decision algorithms and by 
exploiting cross-linguistic knowledge for 
optimizing the results of the corresponding parsers.   
For developing different components and 
dictionaries for the system a bootstrapping 
architecture is suggested that uses the acquired 
lexical information for training the grammar of the 
new language, which in turn uses the 
(underspecified) parser results for optimizing the 
lexical information in the corresponding translation 
dictionaries. We expect that the suggested methods 
significantly improve translation quality and 
reduce the costs of creating new language pairs for 
Machine Translation. The preliminary results 
obtained so far in the project appear promising. 
6 Acknowledgments 
This research is supported by a Marie Curie IAPP 
project taking place within the 7th European 
Community Framework Programme (Grant 
agreement no.: 251534) 
110
7 References 
Armstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.; 
Rapp, R.; Thompson, H. (1998). Multilingual Corpora 
for Cooperation. Proceedings of the 1st International 
Conference on Linguistic Resources and Evaluation 
(LREC), Granada, Vol. 2, 975?980. 
Babych, B., Hartley, A., Sharoff S.; Mudraya, O. (2007). 
Assisting Translators in Indirect Lexical Transfer. 
Proceedings of the 45th Annual Meeting of the ACL.  
Babych, B., Anthony Hartley, & Serge Sharoff (2007b) 
Translating from under-resourced languages: 
comparing direct transfer against pivot translation. 
Proceedings of MT Summit XI, 10-14 September 
2007, Copenhagen, Denmark, 29-35 
Babych, B. & Hartley, A. (2008). Automated MT Evaluation 
for Error Analysis: Automatic Discovery of Potential 
Translation Errors for Multiword Expressions. ELRA 
Workshop on Evaluation ?Looking into the Future of 
Evaluation: When automatic metrics meet task-based  
and performance-based approaches?. Marrakech, 
Morocco 27 May 2008. Proceedings of LREC?08. 
Babych, B. and Hartley, A. (2009). Automated error analysis 
for multiword expressions: using BLEU-type scores 
for automatic discovery of potential translation errors. 
Linguistica Antverpiensia, New Series (8/2009): 
Journal of translation and interpreting studies. Special 
Issue on Evaluation of Translation Technology. 
Babych, B., Babych, S. and Eberle, K. (2012). Deriving 
generation-oriented MT resources from corpora: case 
study and evaluation of de/het classification for Dutch 
Noun (in preparation) 
Baroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping 
corpora and terms from the web. Proceedings of 
LREC 2004.  
Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-
evaluating the role of BLEU in machine translation 
research. EACL-2006: 11th Conference of the 
European Chapter of the Association for 
Computational Linguistics, Trento, Italy, April 3-7, 
2006; pp.249-256  
Charniak, E.; Knight, K.; Yamada, K. (2003). Syntax-based 
language models for statistical machine translation". 
Proceedings of MT Summit IX. 
Eberle, Kurt (2001). FUDR-based MT, head switching and the 
lexicon. Proceedings of the the eighth Machine 
Translation Summit, Santiage de Compostela.  
Eberle, Kurt (2004). Flat underspecified representation and its 
meaning for a fragment of German. 
Habilitationsschrift, Universit?t Stuttgart. 
Eberle, K.; Rapp, R. (2008). Rapid Construction of 
Explicative Dictionaries Using Hybrid Machine 
Translation. In: Storrer, A.;  Geyken, A.; Siebert, A.; 
W?rzner, K._M (eds.) Text Resources and Lexical 
Knowledge: Selected Papers from the 9th Conference 
on Natural Language Processing KONVENS 2008. 
Berlin: Mouton de Gruyter..  
Eckart,K., Eberle, K.; Heid, U. (2010) An infrastructure for 
more reliable corpus analysis. Proceedings of the 
Workshop on Web Services and Processing Pipelines 
in HLT of LREC-2010 , Valetta. 
Eberle, K.; Eckart,K., Heid, U.,Haselbach, B. (2012) A 
tool/database interface for multi-level analyses. 
Proceedings of LREC-2012 , Istanbul. 
Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.; 
Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.; 
Attardo, D.; Grannes, D.; Brown, R. (1994). Integrated 
Translation from Multiple Sources within the Pangloss 
MARK II Machine Translation System. Proceedings 
of Machine Translation of the Americas, 73?80. 
Frederking, Robert and Sergei Nirenburg (1994). Three heads 
are better than one. In: Proceedings of ANLP-94, 
Stuttgart, Germany.  
Fung, P.; McKeown, K. (1997). Finding terminology 
translations from non-parallel corpora. Proceedings of 
the 5th Annual Workshop on Very Large Corpora, 
Hong Kong: August 1997, 192-202.  
Gale, W.A.; Church, K.W. (1993). A progrm for aligning 
sentences in bilingual corpora. Computational 
Linguistics, 19(1), 75?102. 
Gonz?lez, J.; Antonio L. Lagarda, Jos? R. Navarro, Laura 
Eliodoro, Adri? Gim?nez, Francisco Casacuberta, Joan 
M. de Val and Ferran Fabregat (2004). SisHiTra: A 
Spanish-to-Catalan hybrid machine translation system. 
Berlin: Springer LNCS. 
Gough, N., Way, A. (2004). Example-Based Controlled 
Translation. Proceedings of the Ninth Workshop of the 
European Association for Machine Translation, 
Valetta, Malta.  
Groves, D. & Way, A. (2006b). Hybridity in MT: Experiments 
on the Europarl Corpus. In Proceedings of the 11th 
Conference of the European Association for Machine 
Translation, Oslo, Norway, 115?124. 
Groves, D.; Way, A. (2006a). Hybrid data-driven models of 
machine translation. Machine Translation, 19(3?4). 
Special Issue on Example-Based Machine Translation. 
301?323. 
Habash, N.; Dorr, B. (2002). Handling translation 
divergences: Combining statistical and symbolic 
techniques in generation-heavy machine translation. 
Proceedings of AMTA-2002, Tiburon, California, 
USA. 
Kiss, T.; Strunk, J. (2006): Unsupervised multilingual 
sentence boundary detection. Computational 
Linguistics 32(4), 485?525. 
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical 
Machine Translation. Proceedings of MT Summit X, 
Phuket, Thailand 
Koehn, P.; Knight, K. (2002). Learning a translation lexicon 
from monolingual corpora. In: Proceedings of ACL-02 
Workshop on Unsupervised Lexical Acquisition, 
Philadelphia PA. 
Language Industry Monitor (1992). Statistical methods 
gaining ground. In: Language Industry Monitor, 
September/October 1992 issue. 
111
McCord, M. (1989). A new version of the machine translation 
system LMT.  Journal of Literary and Linguistic 
Computing, 4, 218?299. 
McCord, M. (1991). The slot grammar system.  In: Wedekind, 
J., Rohrer, C.(eds): Unification in Grammar, MIT-
Press. 
Melamed, I. Dan (1999). Bitext maps and aligment via pattern 
recognition. Computational Linguistics, 25(1), 107?
130. 
Munteanu, D.S.; Marcu, D. (2005). Improving machine 
translation performance by exploiting non-parallel 
corpora. Computational Linguistics, 31(4), 477?504. 
Och, F.J.; Ney, H. (2002). Discriminative trainig and 
maximum entropy models for statistical machine 
translation. Proceedings of the  Annual Meeting of the 
Association for Computational Linguistics, 
Philadelphia, PA, 295?302.  
Och, F.J.; Ney, H. (2003). A systematic comparison of various 
statistical alignment models. Computational 
Linguistics, 29(1), 19?51. 
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002). BLEU: A 
method for automatic evaluation of machine 
translation. In: Proceedings of the 40th Annual 
Meeting of the ACL, Philadelphia, PA, 311?318. 
Rapp, R. (1995). Identifying word translations in non-parallel 
texts. In: Proceedings of the 33rd Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, 1995, 320?322 
Rapp, R. (1999). Automatic identification of word translations 
from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics 1999, College 
Park, Maryland. 519?526. 
Rapp, R. (2004). A freely available automatically generated 
thesaurus of related words. In: Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC), Lisbon, Vol. II, 
395?398. 
Rapp, R.; Martin Vide, C. (2007). Statistical machine 
translation without parallel corpora. In: Georg Rehm, 
Andreas Witt, Lothar Lemnitzer (eds.): Data 
Structures for Linguistic Resources and Applications. 
Proceedings of the Biennial GLDV Conference 2007. 
T?bingen: Gunter Narr. 231?240 
Resnik, R. (1999). Mining the web for bilingual text. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics. 
Sato, S.; Nagao, M. (1990). Toward memory-based 
translation. Proceedings of COLING 1990, 247?252. 
Schiehlen, M. (2001) Syntactic Underspecification. In: Special 
Research Area 340 ? Final report, University of 
Stuttgart.  
Sharoff, S. (2006) Open-source corpora: using the net to fish 
for linguistic data. In International Journal of Corpus 
Linguistics 11(4), 435?462.  
Sharoff, S.; Babych, B.; Hartley, A. (2006). Using comparable 
corpora to solve problems difficult for human 
translators. In: Proceedings of COLING/ACL 2006, 
739?746.  
Sharoff, S. (2006). A uniform interface to large-scale 
linguistic resources. In Proceedings of the Fifth 
Language Resources and Evaluation Conference, 
LREC-2006, Genoa. 
Simard, M., Foster, G., Isabelle, P. (1992). Using Cognates to 
Align Sentences in Bilingual Corpora. Proceeedings of 
the International Conference on Theoretical and 
Methodological Issues, Montr?al. 
Smadja, F. (1993). Retrieving collocations from text: Xtract. 
Computational Linguistics, 19(1), 143?177. 
Streiter, O., Carl, M., Haller, J. (eds)(1999). Hybrid 
Approaches to Machine Translation. IAI working 
papers 36. 
Streiter, O.; Carl, M.; Iomdin, L.L.: 2000, A Virtual 
Translation Machine for Hybrid Machine Translation'. 
In: Proceedings of the Dialogue'2000 International 
Seminar in Computational Linguistics and 
Applications. Tarusa, Russia.  
Streiter, O.; Iomdin, L.L. (2000). Learning Lessons from 
Bilingual Corpora: Benefits for Machine Translation. 
International Journal of Corpus Linguistics, 5(2), 199?
230. 
Thurmair, G. (2005). Hybrid architectures for machine 
translation systems. Language Resources and 
Evaluation, 39 (1), 91?108. 
Thurmair, G. (2006). Using corpus information to improve 
MT quality. Proceedings of the LR4Trans-III 
Workshop, LREC, Genova. 
Thurmair, G. (2007) Automatic evaluation in MT system 
production. MT Summit XI Workshop: Automatic 
procedures in MT evaluation, 11 September 2007, 
Copenhagen, Denmark, 
Veronis, Jean (2006). Technologies du Langue. Actualit?s ? 
Comentaires ? R?flexions. Translation. Systran or 
Reverso? 
http://aixtal.blogspot.com/2006/01/translation-systran-
or-reverso.html  
Wu, D., Fung, P. (2005). Inversion transduction grammar 
constraints for mining parallel sentences from quasi-
comparable corpora. Second International Joint 
Conference on Natural Language Processing 
(IJCNLP-2005). Jeju, Korea. 
 
112
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 1?6,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Workshop on Hybrid Approaches to Translation:
Overview and Developments
Marta R. Costa-jussa`, Rafael E. Banchs
Institute for Infocomm Research1
Patrik Lambert
Barcelona Media3
Kurt Eberle
Lingenio GmbH4
Reinhard Rapp
Aix-Marseille Universite?, LIF2
Bogdan Babych
University of Leeds5
1{vismrc,rembanchs}@i2r.a-star.edu.sg, 2reinhardrapp@gmx.de,
3patrik.lambert@barcelonamedia.org, 4k.eberle@lingenio.de,
5b.babych@leeds.ac.uk
Abstract
A current increasing trend in machine
translation is to combine data-driven and
rule-based techniques. Such combinations
typically involve the hybridization of dif-
ferent paradigms such as, for instance,
the introduction of linguistic knowledge
into statistical paradigms, the incorpora-
tion of data-driven components into rule-
based paradigms, or the pre- and post-
processing of either sort of translation sys-
tem outputs. Aiming at bringing together
researchers and practitioners from the dif-
ferent multidisciplinary areas working in
these directions, as well as at creating a
brainstorming and discussion venue for
Hybrid Translation approaches, the Hy-
Tra initiative was born. This paper gives
an overview of the Second Workshop on
Hybrid Approaches to Translation (HyTra
2013) concerning its motivation, contents
and outcomes.
1 Introduction
Machine translation (MT) has continuously been
evolving from different perspectives. Early sys-
tems were basically dictionary-based. These ap-
proaches were further developed to more complex
systems based on analysis, transfer and genera-
tion. The objective was to climb up (and down)
in the well-known Vauquois pyramid (see Figure
1) to facilitate the transfer phase or to even mini-
mize the transfer by using an interlingua system.
But then, corpus-based approaches irrupted, gen-
erating a turning point in the field by putting aside
the analysis, generation and transfer phases.
Although there had been such a tendency right
from the beginning (Wilks, 1994), in the last
Figure 1: Vauquois pyramid (image from
Wikipedia).
years, the corpus-based approaches have reached
a point where many researchers assume that rely-
ing exclusively on data might have serious limi-
tations. Therefore, research has focused either on
syntactical/hierarchical-based methods or on try-
ing to augment the popular phrase-based systems
by incorporating linguistic knowledge. In addi-
tion, and given the fact that research on rule-based
has never stopped, there have been several propos-
als of hybrid architectures combining both rule-
based and data-driven approaches.
In summary, there is currently a clear trend to-
wards hybridization, with researchers adding mor-
phological, syntactic and semantic knowledge to
statistical systems, as well as combining data-
driven methods with existing rule-based systems.
In this paper we provide a general overview
of current approaches to hybrid MT within the
context of the Second Workshop on Hybrid Ap-
proaches to Translation (HyTra 2013). In our
overview, we classify hybrid MT approaches ac-
cording to the linguistic levels that they address.
We then briefly summarize the contributions pre-
sented and collected in this volume.
1
The paper is organized as follows. First, we mo-
tivate and summarize the main aspects of the Hy-
Tra initiative. Then, we present a general overview
of the accepted papers and discuss them within
the context of other state-of-the-art research in the
area. Finally, we present our conclusions and dis-
cuss our proposed view of future directions for
Hybrid MT research.
2 Overview of the HyTra Initiative
The HyTra initiative started in response to the in-
creasing interest in hybrid approaches to machine
translation, which is reflected on the substantial
amount of work conducted on this topic. An-
other important motivation was the observation
that, up to now, no single paradigm has been able
to successfully solve to a satisfactory extent all of
the many challenges that the problem of machine
translation poses.
The first HyTra workshop took part in conjunc-
tion with the EACL 2012 conference (Costa-jussa`
et al, 2012). The Second HyTra Workshop, which
was co-organized by the authors of this paper, has
been co-located with the ACL 2013 conference
(Costa-jussa` et al, 2013). The workshop has been
supported by an extensive programme committee
comprising members from over 30 organizations
and representing more than 20 countries. As the
outcome of a comprehensive peer reviewing pro-
cess, and based on the recommendations of the
programme committee, 15 papers were finally se-
lected for either oral or poster presentation at the
workshop.
The workshop also had the privilege to be hon-
ored by two exceptional keynote speeches:
? Controlled Ascent: Imbuing Statistical MT
with Linguistic Knowledge by Will Lewis and
Chris Quirk (2013), Microsoft research. The
intersection of rule-based and statistical ap-
proaches in MT is explored, with a particular
focus on past and current work done at Mi-
crosoft Research. One of their motivations
for a hybrid approach is the observation that
the times are over when huge improvements
in translation quality were possible by sim-
ply adding more data to statistical systems.
The reason is that most of the readily avail-
able parallel data has already been found.
? How much hybridity do we have? by Her-
mann Ney, RWTH Aachen. It is pointed
out that after about 25 years the statistical
approach to MT has been widely accepted
as an alternative to the classical approach
with manually designed rules. But in prac-
tice most statistical MT systems make use
of manually designed rules at least for pre-
processing in order to improve MT quality.
This is exemplified by looking at the RWTH
MT systems.
3 Hybrid Approaches Organized by
Linguistic Levels
?Hybridization? of MT can be understood as com-
bination of several MT systems (possibly of very
different architecture) where the single systems
translate in parallel and compete for the best re-
sult (which is chosen by the integrating meta sys-
tem). The workshop and the papers do not fo-
cus on this ?coarse-grained? hybridization (Eisele
et al, 2008), but on a more ?fine grained? one
where the systems mix information from differ-
ent levels of linguistic representations (see Fig-
ure 2). In the past and mostly in the framework
of rule-based machine translation (RBMT) it has
been experimented with information from nearly
every level including phonetics and phonology
for speech recognition and synthesis in speech-
to-speech systems (Wahlster, 2000) and includ-
ing pragmatics for dialog translation (Batliner et
al., 2000a; Batliner et al, 2000b) and text coher-
ence phenomena (Le Nagard and Koehn, 2010).
With respect to work with emphasis on statisti-
cal machine translation (SMT) and derivations of
it mainly those information levels have been used
that address text in the sense of sets of sentences.
As most of the workshop papers relate to this
perspective - i.e. on hybridization which is de-
fined using SMT as backbone, in this introduc-
tion we can do with distinguishing between ap-
proaches focused on morphology, syntax, and se-
mantics. There are of course approaches which
deal with more than one of these levels in an in-
tegrated manner, which are commonly refered to
as multilevel approaches. As the case of treat-
ing syntax and morphology concurrently is espe-
cially common, we also consider morpho-syntax
as a separate multilevel approach.
3.1 Morphological approaches
The main approaches of statistical MT that ex-
ploit morphology can be classified into segmen-
tation, generation, and enriching approaches. The
2
Figure 2: Major linguistic levels (image from
Wikipedia).
first one attempts to minimize the vocabulary of
highly inflected languages in order to symmetrize
the (lexical granularity of the) source and the tar-
get language. The second one assumes that, due
to data sparseness, not all morphological forms
can be learned from parallel corpora and, there-
fore, proposes techniques to learn new morpho-
logical forms. The last one tries to enrich poorly
inflected languages to compensate for their lack of
morphology. In HyTra 2013, approaches treating
morphology were addressed by the following con-
tributions:
? Toral (2013) explores the selection of data to
train domain-specific language models (LM)
from non-domain specific corpora by means
of simplified morphology forms (such as
lemmas). The benefit of this technique is
tested using automatic metrics in the English-
to-Spanish task. Results show an improve-
ment of up to 8.17% of perplexity reduction
over the baseline system.
? Rios Gonzalez and Goehring (2013) propose
machine learning techniques to decide on the
correct form of a verb depending on the con-
text. Basically they use tree-banks to train the
classifiers. Results show that they are able
to disambiguate up to 89% of the Quechua
verbs.
3.2 Syntactic approaches
Syntax had been addressed originally in SMT in
the form of so called phrase-based SMT with-
out any reference to linguistic structures; during
the last decade (or more) the approach evolved
to or, respectively, was complemented by - work
on syntax-based models in the linguistic sense of
the word. Most such approaches can be classi-
fied into three different types of architecture that
are defined by the type of syntactic analysis used
for the source language and the type of generation
aimed at for the target language: tree-to-tree, tree-
to-string and string-to-tree. Additionally, there
are also the so called hierarchical systems, which
combine the phrase-based and syntax-based ap-
proaches by using phrases as translation-units and
automatically generated context free grammars as
rules. Approaches dealing with the syntactic ap-
proach in HyTra 2013 include the following pa-
pers:
? Green and Zabokrtsky? (2013) study three dif-
ferent ways to ensemble parsing techniques
and provide results in MT. They compute cor-
relations between parsing quality and transla-
tion quality, showing that NIST is more cor-
related than BLEU.
? Han et al (2013) provide a framework for
pre-reordering to make Chinese word order
more similar to Japanese. To this purpose,
they use unlabelled dependency structures of
sentences and POS tags to identify verbal
blocks and move them from after-the-object
positions (SVO) to before-the-object posi-
tions (SOV).
? Nath Patel et al (2013) also propose a pre-
reordering technique, which uses a limited
set of rules based on parse-tree modification
rules and manual revision. The set of rules is
specifically listed in detail.
? Saers et al (2013) report an unsupervised
learning model that induces phrasal ITGs by
breaking rules into smaller ones using mini-
mum description length. The resulting trans-
lation model provides a basis for generaliza-
tion to more abstract transduction grammars
with informative non-terminals.
3.3 Morphosyntactical approaches
In linguistic theories, morphology and syntax are
often considered and represented simultaneously
(not only in unification-based approaches) and the
same is true for MT systems.
3
? Laki et al (2013) combine pre-reordering
rules with morphological and factored mod-
els for English-to-Turkish.
? Li et al (2013) propose pre-reordering rules
to be used for alignment-based reordering,
and corresponding POS-based restructuring
of the input. Basically, they focus on tak-
ing advantage of the fact that Korean has
compound words, which - for the purpose of
alignment - are split and reordered similarly
to Chinese.
? Turki Khemakhem et al (2013) present
work about an English-Arabic SMT sys-
tem that uses morphological decomposition
and morpho-syntactic annotation of the target
language and incorporates the correspond-
ing information in a statistical feature model.
Essentially, the statistical feature language
model replaces words by feature arrays.
3.4 Semantic approaches
The introduction of semantics in statistical MT has
been approached to solve word sense disambigua-
tion challenges covering the area of lexical seman-
tics and, more recently, there have been different
techniques using semantic roles covering shallow
semantics, as well as the use of distributional se-
mantics for improving translation unit selection.
Approaches treating the incorporation of seman-
tics into MT in HyTra 2013 include the following
research work:
? Rudnick et al (2013) present a combina-
tion of Maximum Entropy Markov Models
and HMM to perform lexical selection in
the sense of cross-lingual word sense disam-
biguation (i.e. by choice from the set of trans-
lation alternatives). The system is meant to
be integrated into a RBMT system.
? Boujelbane (2013) proposes to build a bilin-
gual lexicon for the Tunisian dialect us-
ing modern standard Arabic (MSA). The
methodology is based on leveraging the large
available annotated MSA resources by ex-
ploiting MSA-dialect similarities and ad-
dressing the known differences. The author
studies morphological, syntactic and lexical
differences by exploiting Penn Arabic Tree-
bank, and uses the differences to develop
rules and to build dialectal concepts.
? Bouillon et al (2013) presents two method-
ologies to correct homophone confusions.
The first one is based on hand-coded rules
and the second one is based on weighted
graphs derived from a pronunciation re-
source.
3.5 Other multilevel approaches
In a number of linguistic theories information
from the morphological, syntactic and semantic
level is considered conjointly and merged in cor-
responding representations (a RBMT example is
LFG (Lexical Functional Grammars) analysis and
the corresponding XLE translation architecture).
In HyTra 2013 there are three approaches dealing
with multilevel information:
? Pal et al (2013) propose a combination of
aligners: GIZA++, Berkeley and rule-based
for English-Bengali.
? Hsieh et al (2013) use comparable corpora
extracted from Wikipedia to extract parallel
fragments for the purpose of extending an
English-Bengali training corpus.
? Tambouratzis et al (2013) describe a hybrid
MT architecture that uses very few bilingual
corpus and a large monolingual one. The
linguistic information is extracted using
pattern recognition techniques.
Table 1 summarizes the papers that have been
presented in the Second HyTra Workshop. The
papers are arranged into the table according to the
linguistic level they address.
4 Conclusions and further work
The success of the Second HyTra Workshop con-
firms that research in hybrid approaches to MT
systems is a very active and promising area. The
MT community seems to agree that pure data-
driven or rule-based paradigms have strong lim-
itations and that hybrid systems are a promising
direction to overcome most of these limitations.
Considerable progress has been made in this area
recently, as demonstrated by consistent improve-
ments for different language pairs and translation
tasks.
The research community is working hard, with
strong collaborations and with more resources at
hand than ever before. However, it is not clear
4
Morphological (Toral, 2013) Hybrid Selection of LM Training Data Using Linguistic Information and Perplexity
(Gonzales and Goehring, 2013) Machine Learning disambiguation of Quechua verb morphology
Syntax (Green and Zabokrtsky?, 2013) Improvements to SBMT using Ensemble Dependency Parser
(Han et al, 2013) Using unlabeled dependency parsing for pre-reordering for Chinese-to-Japanese SMT
(Patel et al, 2013) Reordering rules for English-Hindi SMT
(Saers et al, 2013) Unsupervised transduction grammar induction via MDL
Morpho-syntactic (Laki et al, 2013) English to Hungarian morpheme-based SMT system with reordering rules
(Li et al, 2013) Experiments with POS-based restructuring and alignment based reordering for SMT
(Khemakhem et al, 2013) Integrating morpho-syntactic feature for English Arabic SMT
Semantic (Rudnick and Gasser, 2013) Lexical Selection for Hybrid MT with Sequence Labeling
(Boujelbane et al, 2013) Building bilingual lexicon to create dialect Tunisian corpora and adapt LM
(Bouillon et al, 2013) Two approaches to correcting homophone confusions in a hybrid SMT based system
Multilevels (Pal et al, 2013) A hybrid Word alignment model for PBSMT
(Hsieh et al, 2013) Uses of monolingual in-domain corpora for cross-domain adaptation with hybrid MT approaches
(Tambouratzis et al, 2013) Overview of a language-independent hybrid MT methodology
Table 1: HyTra 2013 paper overview.
whether technological breakthroughs as in the past
are still possible are still possible, or if MT will be
turning into a research field with only incremen-
tal advances. The question is: have we reached
the point at which only refinements to existing ap-
proaches are needed? Or, on the contrary, do we
need a new turning point?
Our guess is that, similar to the inflection point
giving rise to the statistical MT approach during
the last decade of the twentieth century, once again
there might occur a new discovery which will rev-
olutionize further the research on MT. We cannot
know whether hybrid approaches will be involved;
but, in any case, this seems to be a good and smart
direction as it is open to the full spectrum of ideas
and, thus, it should help to push the field forward.
Acknowledgments
This workshop has been supported by the Sev-
enth Framework Program of the European Com-
mission through the Marie Curie actions HyghTra,
IMTraP, AutoWordNet and CrossLingMind and
the Spanish ?Ministerio de Econom??a y Competi-
tividad? and the European Regional Development
Fund through SpeechTech4all. We would like to
thank the funding institution and all people who
contributed towards making the workshop a suc-
cess. For a more comprehensive list of acknowl-
edgments refer to the preface of this volume.
References
Anton Batliner, J. Buckow, Heinrich Niemann, Elmar
No?th, and Volker Warnke, 2000a. The Prosody
Module, pages 106?121. New York, Berlin.
Anton Batliner, Richard Huber, Heinrich Niemann, El-
mar No?th, Jo?rg Spilker, and K. Fischer, 2000b. The
Recognition of Emotion, pages 122?130. New York,
Berlin.
Pierrette Bouillon, Johanna Gerlach, Ulrich Germann,
Barry Haddow, and Manny Rayner. 2013. Two ap-
proaches to correcting homophone confusions in a
hybrid machine translation system. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Rahma Boujelbane, Mariem Ellouze khemekhem, Si-
war BenAyed, and Lamia HadrichBelguith. 2013.
Building bilingual lexicon to create dialect tunisian
corpora and adapt language model. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, and Bogdan Babych, edi-
tors. 2012. Proceedings of the Joint Workshop on
Exploiting Synergies between Information Retrieval
and Machine Translation (ESIRMT) and Hybrid Ap-
proaches to Machine Translation (HyTra). As-
sociation for Computational Linguistics, Avignon,
France, April.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, Bogdan Babych, and Kurl
Eberle, editors. 2013. Proceedings of the Sec-
ond Workshop on Hybrid Approaches to Translation
(HyTra). Association for Computational Linguis-
tics, Sofia, Bulgaria, August.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jelling-
haus, Sabine Hunsicker, Teresa Herrmann, and
Yu Chen. 2008. Hybrid machine translation archi-
tectures within and beyond the euromatrix project.
In John Hutchins and Walther v.Hahn, editors, 12th
annual conference of the European Association for
Machine Translation (EAMT), pages 27?34, Ham-
burg, Germany.
Annette Rios Gonzales and Anne Goehring. 2013.
Machine learning disambiguation of quechua verb
morphology. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Nathan Green and Zdenek Zabokrtsky?. 2013. Im-
provements to syntax-based machine translation us-
ing ensemble dependency parsers. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
5
Dan Han, Pascual Martinez-Gomez, Yusuke Miyao,
Katsuhito Sudoh, and Masaaki NAGATA. 2013.
Using unlabeled dependency parsing for pre-
reordering for chinese-to-japanese statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
An-Chang Hsieh, Hen-Hsen Huang, and Hsin-Hsi
Chen. 2013. Uses of monolingual in-domain cor-
pora for cross-domain adaptation with hybrid mt ap-
proaches. In ACL Workshop on Hybrid Machine Ap-
proaches to Translation, Sofia.
Ines Turki Khemakhem, Salma Jamoussi, and Abdel-
majid Ben Hamadou. 2013. Integrating morpho-
syntactic feature in english-arabic statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
La?szlo? Laki, Attila Novak, and Borba?la Siklo?si. 2013.
English to hungarian morpheme-based statistical
machine translation system with reordering rules. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden, July. Association for
Computational Linguistics.
Will Lewis and Chris Quirk. 2013. Controlled ascent:
Imbuing statistical mt with linguistic knowledge. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Shuo Li, Derek F. Wong, and Lidia S. Chao.
2013. Experiments with pos-based restructuring and
alignment-based reordering for statistical machine
translation. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Santanu Pal, Sudip Naskar, and Sivaji Bandyopadhyay.
2013. A hybrid word alignment model for phrase-
based statistical machine translation. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale, and
Sasikumar M. 2013. Reordering rules for english-
hindi smt. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Alex Rudnick and Michael Gasser. 2013. Lexical se-
lection for hybrid mt with sequence labeling. In ACL
Workshop on Hybrid Machine Approaches to Trans-
lation, Sofia.
Markus Saers, Karteek Addanki, and Dekai Wu. 2013.
Unsupervised transduction grammar induction via
minimum description length. In ACL Workshop on
Hybrid Machine Approaches to Translation, Sofia.
George Tambouratzis, Sokratis Sofianopoulos, and
Marina Vassiliou. 2013. Language-independent hy-
brid mt with presemt. In ACL Workshop on Hybrid
Machine Approaches to Translation, Sofia.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin, Heidelberg, New York.
Yorick Wilks. 1994. Stone soup and the french
room: The empiricist-rationalist debate about ma-
chine translation. Current Issues in Computational
Linguistics: in honor of Don Walker, pages 585?
594. Pisa, Italy: Giardini / Dordrecht, The Nether-
lands: Kluwer Academic.
6
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 75?81,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Deriving de/het gender classification for Dutch nouns for rule-based MT generation tasks 
Bogdan Babych Centre for Translation Studies University of Leeds b.babych@leeds.ac.uk 
Jonathan Geiger Lingenio GmbH  geiger@cl.uni-heidelberg.de 
Mireia Ginest? Rosell Centre for Translation Studies University of Leeds mireia.ginesti@gmail.com 
Kurt Eberle Lingenio GmbH  k.eberle@lingenio.de 
 Abstract Linguistic resources available in the pub-lic domain, such as lemmatisers, part-of-speech taggers and parsers can be used for the development of MT systems: as separate processing modules or as anno-tation tools for the training corpus. For SMT this annotation is used for training factored models, and for the rule-based systems linguistically annotated corpus is the basis for creating analysis, generation and transfer dictionaries from corpora. However, the annotation in many cases is insufficient for rule-based MT, especially for the generation tasks. In this paper we analyze a specific case when the part-of-speech tagger does not provide infor-mation about de/het gender of Dutch nouns that is needed for our rule-based MT systems translating into Dutch. We show that this information can be derived from large annotated monolingual corpo-ra using a set of context-checking rules on the basis of co-occurrence of nouns and determiners in certain morphosyntac-tic configurations. As not all contexts are sufficient for disambiguation, we evalu-ate the coverage and the accuracy of our method for different frequency thresholds                                                 ? 2014 European Association for Computational Linguis-tics.  
in the news corpora. Further we discuss possible generalization of our method, and using it to automatically derive other types of linguistic information needed for rule-based MT: syntactic subcategoriza-tion frames, feature agreement rules and contextually appropriate collocates. 1 Introduction This paper evaluates a methodology for deriving gender classification of nouns based on their con-textual features and light-weight linguistic anno-tation of a corpus. We approach the problem as reconstructing an enriched set of linguistic fea-tures for RBMT generation lexicon from com-bining implicit information available in corpora with a set of general linguistic principles imple-mented as a small set of simple hand-crafted con-textual rules.  These rules are specified as configurations of part-of-speech codes and operate over configura-tions of part-of-speech codes designed to capture certain disambiguating linguistic constructions. Theoretically, the rules can be made highly-accurate if the list of disambiguating construc-tions is exhaustive, but there is a well-known trade-off between Precision, Recall and the de-velopment effort for hand-crafted sets of rules. Additional factors to be taken into account are the quality and size of the annotated corpus. In our experiment we take a practical approach, us-ing a minimal set of contextual rules that cover most typical constructions.  
75
We evaluate Precision and coverage for this set of rules for different frequency thresholds of nouns in the corpus. The results indicate the po-tential of the proposed methodology for a larger set of similar tasks, where we intend to enrich linguistic resources for rule-based MT tasks us-ing implicit linguistic information, which can be discovered in annotated corpora. The paper is organised as follows: Section 2 discusses linguistic aspects of the gender disam-biguation task for Dutch nouns; Section 3 de-scribes the set-up of our experiment on automati-cally deriving the lexicon for Dutch nouns en-riched with gender information; Section 4 pre-sents evaluation results for Precision and cover-age for different frequency thresholds; Section 4 gives interpretation of the results; Section 6 dis-cusses the development context, generalisation of our methodology for rule-based MT and some ideas for future work.  2 Linguistic aspects of gender disam-biguation task for Dutch nouns Predicting gender of Dutch nouns from their context is a simple and clearly defined contextual disambiguation task, and we can evaluate three aspects of the performance of our method: (a) what coverage and accuracy can be achieved on this task compared to the gold standard; (b) how do the coverage and accuracy change in different frequency thresholds; (c) what is the proportion of contexts which can be used for disambiguation in different frequency thresholds (since some contexts will not disambiguate the features of interest). Nouns in Dutch belong to one of the two gen-der classes which determine the choice of the definite articles (used with singular nouns) and other determiners: neuter nouns take determiners het, dat, dit, ons, and nouns with the common gender, which historically is the merged mascu-line and feminine, take de, die, deze, onze. Nouns can only be disambiguated when used as singular and take a definite determiner, so not all contexts in corpus which contain nouns can be useful for disambiguation.  The information about het/de classification for nouns is a non-interpretable (in terms of the gen-erative grammar) system-internal morphological feature: it characterises only combinatorial prop-erties of nouns, but does not directly influence their syntactic functions in a structure of a sen-tence or their semantic interpretation (unlike the 
part-of-speech/Noun category, morphological case and number). Therefore, this feature is much more useful for text generation than for analysis, and belongs to the family of other similar sys-tem-internal features, like inflection classes, sub-categorisation frames, lexical functions (colloca-tional restrictions), etc. Interestingly, this feature normally operates in the local context of several words within a limited number of possible part-of-speech sequences. For machine translation task this information needs to be supplied by the target language gen-eration rules, or by the target language model, since it is normally not present in the source text, and cannot be derived from application of trans-fer rules or the translation model. There are several wide-coverage part-of-speech taggers and lemmatisers for Dutch in the public domain (open source and/or freely availa-ble), such as Dutch parameter files for the Tree-Tagger (Schmid, 1994), TiMBL / Frog tagger / lemmatiser / dependency parser (Van den Bosch et al., 2007), Alpino system (Bouma et al., 2001). Some of them provide only plain high-level an-notation of part-of-speech codes, without gender information for nouns. However, some do gener-ate enriched part-of-speech codes for nouns spec-ifying their gender. Because of this we can benchmark our methodology using this enriched information as gold-standard and calculate Preci-sion in addition to coverage.  3 Set-up of the experiment In our experiment TiMBL / Frog was used to automatically annotate a 60-million-word section of the balanced Dutch SoNaR corpus (Oostdijk et al., 2008). TiMBL/Frog provides gold-standard diction-ary-based information about these classes for identified lemmas. For the prediction task we ignored the gold-standard gender class infor-mation, and used only the generic part-of-speech information and the number category for nouns. In the evaluation stage, we compared these au-tomatically predicted gender classes with the gold-standard classes. Prediction of the de/het classes was performed by a set of regular expressions, which cover most typical contexts, where these determiners are distinguished. If both types of determiners were found in different contexts for the same noun, then the class that has the majority of contexts was assigned. Regular expressions covered sim-ple contexts, e.g.: Det (Adj)? Noun:  
76
Table 1. Evaluation of the task of predicting Dutch determiner classes: Number of tokens and proportions in each frequency threshold   (1) de nieuwe geschiedschrijving the.Gend:COM new history.Gend:COM  -- but not more complex ambiguous contexts, e.g., sequences of nominal compounds:  (2) waar is de apparaat-code van mijn ka-mera? Where is the~Gend:COM device~Gend:NEUT ? code~Gend:COM of my camera?  or cases where het is not a determiner, but is mis-tagged as such: we assumed that such contexts are less frequent and error rate will be limited, so we can save the development effort for our hand-crafted rule set relying on the signal being stronger than noise introduced by such complex cases.  The results reported in this paper were gener-ated using the following two multilevel regular expressions (expressions which operate on the levels of lemmas and parts-of-speech:   de/det__art   /(adj|conjcoord)* (.*)/nounsg  het/det__art   /(adj|conjcoord)* (.*)/nounsg     
                      These regular expressions describe configurations that allow several optional adjectives or coordinative conjunctions between the definite determiner and a singular noun. The noun is captured if the configuration matches the piece of text and classified according to the type of the determiner.  4 Evaluation results The results are presented in Table 1 and Charts 1 and 2, which visualise some of the data from Table 1. Rows in Table 1 represent different frequency cut-off points, e.g: None = no frequency cut-off, Frq>1 = noun types with frequency greater than one, etc. Columns represent:  - Gold standard: the number of noun types identified in the gold-standard above the specified frequency - Predicted: the number of noun types for which prediction of the gender on the ba-sis of the context in the corpus was made (for the rest prediction was not possible since no disambiguating contexts were found for those noun types) - Wrong, %/100: the number and the pro-portion of wrongly predicted noun types (of the total number of Predicted types) 
Gold standard Predicted Wrong %:100 Correct %:100 Missed %:100 Contexts %:100
None 157066 74505 2417 0.032 72088 0.968 84978 0.541 0.752
70006 45710 1604 0.035 44106 0.965 25900 0.37 0.573
48002 35766 1229 0.034 34537 0.966 13465 0.281 0.518
38084 30245 1012 0.033 29233 0.967 8851 0.232 0.491
32051 26515 858 0.032 25657 0.968 6394 0.199 0.475
28025 23818 744 0.031 23074 0.969 4951 0.177 0.465
25026 21735 661 0.03 21074 0.97 3952 0.158 0.456
22789 20053 597 0.03 19456 0.97 3333 0.146 0.450
21002 18701 543 0.029 18158 0.971 2844 0.135 0.444
19546 17553 498 0.028 17055 0.972 2491 0.127 0.440
?
12244 11436 279 0.024 11157 0.976 1087 0.089 0.421
6795 6482 123 0.019 6359 0.981 436 0.064 0.410
4297 4116 69 0.017 4047 0.983 250 0.058 0.401
Frq threshold
Frq>1
Frq>2
Frq>3
Frq>4
Frq>5
Frq>6
Frq>7
Frq>8
Frq>9
Frq>=20
Frq>=50
Frq>=100
77
- Correct, %/100: the number and the pro-portion of correctly predicted noun types (of the total number of Predicted types) - Missed, %/100: the number and the pro-portion of noun types where prediction of gender was not possible (of the total number of nouns in the Gold standard). - Contexts, %/100: the proportion of con-texts for noun tokens, which were useful for disambiguation  For instance, the first row shows the figures when no frequency cut-off is applied, e.g.: there were 157066 types labeled as Nouns in our sec-tion of SoNaR corpus, of which 74505 Nouns were found in a specific context with a definite determiner that allowed to disambiguate gender. Out of these, 2417 types (3.2%) were disambigu-ated wrongly for different reasons, 72088 types (96.8%) were disambiguated correctly. However, there still remain 84978 noun types (or 54.1% of the total number of 157066 in the gold standard), which were not disambiguated. In total, in the corpus 75.2% of contexts were useful for de/het disambiguation (contained a definite determiner in the immediate left context, or in a one-word-apart position, being separated by an adjective). The second row in Table 1 presents the subset of 70006 noun types out of the results presented in the first row for 157066 noun types, i.e., the results only for nouns with frequency more than one; the third row ? for noun types with frequen-cies more than two, etc. The intuition is that pre-diction for more frequent nouns should be more accurate since more token contexts become available for disambiguation of a specific noun type.     
Chart 1. Distribution of correctly predicted, missed and wrongly predicted nouns    
Chart 1 visualizes correct, missing and wrong proportion of noun types in the total count of these types for different frequency cut-off points. On the vertical axis there is a number of noun types, on the horizontal axis ? not greater than frequencies. It can be seen from the chart that the propor-tion of non-disambiguated noun types declines with increasing frequency threshold.    
Chart 2. Proportion of context useful for disam-biguation (evidence), not predicted (missing) and wrongly predicted (wrong) de/het classes for nouns.  Chart 2 examines the relation between frequency cut-off points and Evidence (top yellow/light line) ? the proportions of contexts available for disambiguation; Missing (middle red/medium line) ? the proportion of nouns where de/het dis-ambiguation was not possible and Wrong (bot-tom blue/dark line) ? the error rate. 5 Interpretation of the results The following conclusions can be derived from the evaluation data:  1. The Precision even for simple contextual disambiguation rules is surprisingly high: 96.8% for nouns where the prediction was possible. This indicates that simple disambiguation patterns are sufficiently frequent to outweigh more complex patterns which were not covered by the rule and may have lead to errors. 2. For the whole data set (without frequency cut-off) the Recall is much lower: automatic prediction procedure missed 54% of noun tokens that were found in the corpus and a contained 
0 1 2 3 4 5 6 7 8 9 19 49 99
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
correct
missing
wrong
0 1 2 3 4 5 6 7 8 9 ? ? 19 49 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Wrong %
Missing %
Evidence %
78
gold-standard gender class, since no disambiguation context was found for these nouns in corpus. However, since more frequent nouns have more chances of occurring in a disambiguation context, mostly low frequent nouns are missed: if we exclude nouns which occurred only once, the procedure misses 37% of nouns; in frequency threshold Frq> 2 it misses even less ? 28%, etc. 3. Error rate (proportion of wrongly disambiguated nouns) is relatively stable (3.2% on the whole data set), and does not depend too much on the frequency of nouns: it declines very slowly when the frequency increases (much slower that the coverage of the certain threshold). 4. The proportion of contexts which are useful for disambiguation declines slowly with the increase in frequency threshold, but stabilises around 40% for highly frequent nouns. Interestingly, when the proportion of such contexts goes down, the error rate stays the same. In general, the results indicate that for practical purposes of rule-based MT development ? a suf-ficiently large list of gender-disambiguated Dutch nouns (around 75000) can be successfully collected from a medium-size corpus (60MW) with very high Precision (96.8%). The method will provide gender disambiguation information for around 46% of all nouns found in the corpus; and for higher frequency threshold the percent-age of gender-disambiguated nouns goes up rap-idly, flattening at around 90% for Frq>10. This performance reaches the quality standards for creating wide-coverage generation dictionaries for rule-based MT. 6 Development context and generaliza-tion of the methodology The task of predicting gender classes for nouns gives indication how other types of similar mor-phosyntactic resources and representations can be developed and enhanced. Our methodology is part of a larger develop-ment infrastructure for creating a corpus-based development environment for industry-standard rule-based MT systems enhanced with statistical tools and data. The infrastructure uses large monolingual corpora annotated by openly availa-ble part-of-speech taggers and lemmatisers, and semi-automatically derives a set of morphologi-cal and syntactic patterns for the lexical items 
found there. These patterns represent advanced linguistic features for the lexicon, such as classi-fication by inflectional morphological paradigms, derivational classes (e.g., gender for nouns), lex-ical valencies (subcategorisation and case frames), attachment preferences and lexical col-locates. For individual lexical items these patterns do not need to be fully specified from the training corpus: missing forms are reconstructed on the basis of evidence from other lexemes that fit the same pattern, so the system recognises and gen-erates correct output also for unseen forms.  In the context of our hybrid MT development infrastructure this approach particularly targets creation of linguistically-rich resources that gen-erate correct target language forms and phrases. The generation aspect is usually not covered by the annotation tools available in the public do-main, so parsers, part-of-speech taggers and lemmatisers usually work only in the direction of analysis, and do not deal with generation).  In a more general context the described infra-structure develops lexical and morphosyntactic resources in a systematic way, so they can be used in a wider range of applications and tasks. It also attempts to bridge the gap between rule-based and statistical techniques in MT by creat-ing rich and highly accurate linguistic representa-tions using corpus-based statistical techniques and integrating them within processing models for hybrid MT architecture. The central principle of the proposed infra-structure is that advanced morphosyntactic fea-tures and representations are derived from corpo-ra annotated with light-weight linguistic features.  The interpretation of this principle is that the tools like part-of-speech taggers and lemmatisers implement a unidirectional functional perspec-tive on the morphosyntactic system, which only partially covers the network of linguistic rela-tions involved in the analysis and generation as-pects of the language. Rule-based MT applica-tion instead need to rely on the alternative rela-tional perspective of morphosyntactic representa-tions. Our infrastructure aims at reconstructing this perspective by combining large corpora and unidirectional annotation tools. It derives a range of generation-oriented morphosyntactic features and representations using local context and standard analysis-oriented annotation features in corpora. The main motivation is that from the point of view of rule-based MT there is a certain imbal-ance between resources for analysis and annota-
79
tion of texts on the one hand, and resources for language generation on the other hand. Text an-notation resources, such as part-of-speech tag-gers, lemmatisers, parsers, chunkers ? have a longer history of research and development, e.g., (Greene and Rubin, 1971), have created common standards and are more widely available in the public domain, e.g., (Schmid, 1994; Brants, 2000). In their existing form they can be applied to new languages and are more widely used in practical applications. On the other hand, genera-tion-oriented tools are much less accessible, of-ten propitiatory, and lack common standards and shared frameworks for integration of new lan-guages. The predominant unidirectional text-annotation focus might be explained by a historic reason that text annotation was seen as an inter-esting computational problem with a clearly de-fined evaluation procedure, which was much harder to develop for the generation tasks. The idea behind the infrastructure is that if at least some unidirectional annotation tools are available for a certain language, the relational morphosyntactic resources can be automatically developed from large annotated corpora. This will include automatic acquisition of inflectional paradigms for lexical items, attachment prefer-ence detection, automatic acquisition of lexical functions. Our infrastructure aims at developing standards and building openly available re-sources for a number of languages, including under-resourced languages, such as Portuguese, Russian and Ukrainian, in order to carry out the following morphosyntacitc tasks:  1. word form generation: for a given lem-ma, part-of-speech and inflectional fea-ture values to generate the correct word form, e.g.: drive~V + Person(3rd); Num-ber(singular) ? drives  2. generation of paradigms: for a given lemma and part-of-speech to generate a set of all word forms and their inflec-tional feature values, e.g., drive~V ? drive~VV; drives~VVZ; driving~VVG; drove~VVD; driven~VVN 3. feature agreement generation: for a given sequence of lemmas with their part-of-speech codes to generate a correct se-quence of inflected word forms, where inflectional features, e.g., in a language with adjectives and nouns marked for gender to generate a correct gender agreement between the two: in Spanish, e.g., nuestro~A.Gender(_).Number(_) 
En:'our' + profesora~ N.Gender(fem).Number(plur) En:'professors(female)' ? nuestras profesoras 4. lexical feature generation: to select cor-rect lemmas for lexically underspecified structures, e.g., in a language with the gender feature marked on determiners and nouns to select the correct deter-miner to go with a given noun: Dutch: [Determiner.Def(definite)] + beroep~N.Number(singular) ? het beroep 5. subcategorisation frame generation: to generate the correct prepositional phrase and/or morphological case features for a given verb and a noun (or a noun phrase), e.g.: dispence~V + N ? dis-pense with + N; dispose~V + N ? dis-pose of +N 6. collocate / lexical function generation (in terms of Mel??uk, 1998): to select the correct lemma or ranked set of lemmas for a given word and semantic features of its context, e.g., '[not-real/true] + [Noun]': mock trial; false assumption; counterfeit goods; fake name 7. word order generation: to generate cor-rect linear sequence of words for a given dependency structure, e.g.:   I ? find ? issues ? certain   difficult ??  I find certain issues difficult  The first two functions are performed on internal features of a word, while the other five require contextual input in addition. The described func-tionality has applications for rule-based MT and Natural Language Generation, which could both benefit from shared standards and the infrastruc-ture of relation-oriented linguistic resources.  Acknowledgement The work is supported by the FP7 Marie Curie IAPP project HyghTra: A Hybrid High Quality Translation System, grant agreement no 251534.      
80
References Bouma, G., van Noord, G., and R. Malouf. (2001). Alpino: wide coverage computational analysis of Dutch. In W. Daelemans, K. Sima'an, J. Veenstra, and J. Zavrel, editors, Computational Linguistics in the Netherlands 2000, pages 45--59. Rodolpi, Am-sterdam. Brants, T. (2000), TnT - A Statistical Part-of-Speech Tagger. In Proceedings of the Sixth Applied Natu-ral Language Processing Conference ANLP-2000, Seattle, WA. Greene, B. B. & Rubin, G. M. (1971), Automatic Grammatical Tagging of English. Department of Linguistics, Brown University, Providence, Rhode Island Mel??uk, I. A. (1998). Collocations and Lexical Func-tions. In Anthony P. Cowie (ed.) Phraseology. Theory, analysis, and applications, 23?53. Oxford: Clarendon. Oostdijk, N., M. Reynaert, P. Monachesi, G. van Noord, R. Ordelman, I. Schuurman, V. Vandeghinste. From D-Coi to SoNaR: A reference corpus for Dutch. In: LREC 2008. Schmid, H. (1994), Probabilistic Part-of-Speech Tag-ging Using Decision Trees. Proceedings of Interna-tional Conference on New Methods in Language Processing, Manchester, UK. Van den Bosch, A., Busser, G.J., Daelemans, W., and Canisius, S. (2007). An efficient memory-based morphosyntactic tagger and parser for Dutch, In F. van Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste (Eds.), Selected Papers of the 17th Computational Linguistics in the Netherlands Meeting, Leuven, Belgium, pp. 99-114.  
81
