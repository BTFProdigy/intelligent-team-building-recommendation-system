Construction and Analysis of Japanese-English Broadcast News Corpus
with Named Entity Tags
Tadashi Kumano, Hideki Kashioka and Hideki Tanaka
ATR Spoken Language Translation Research Laboratories
2?2?2, Hikaridai, Keihanna Science City, Kyoto 619?0288, Japan
{tadashi.kumano, hideki.kashioka, hideki.tanaka}@atr.co.jp
Takahiro Fukusima
Otemon Gakuin University
1?15, Nishiai 2-chome, Ibaraki, Osaka 567?8502, Japan
fukusima@res.otemon.ac.jp
Abstract
We are aiming to acquire named entity
(NE) translation knowledge from non-
parallel, content-aligned corpora, by uti-
lizing NE extraction techniques. For this
research, we are constructing a Japanese-
English broadcast news corpus with NE
tags. The tags represent not only NE
class information but also coreference in-
formation within the same monolingual
document and between corresponding
Japanese-English document pairs. Anal-
ysis of about 1,100 annotated article pairs
has shown that if NE occurrence informa-
tion, such as classes, number of occur-
rence and occurrence order, is given for
each language, it may provide a good clue
for corresponding NEs across languages.
1 Introduction
Studies on named entity (NE) extraction are mak-
ing progress for various languages, such as En-
glish and Japanese. A number of evaluation work-
shops have been held, including the Message Under-
standing Conference (MUC)1 for English and other
languages, and the Information Retrieval and Ex-
traction Exercise (IREX)2 for Japanese. Extraction
accuracy for English has reached a nearly practi-
cal level (Marsh and Perzanowski, 1998). As for
Japanese, it is more difficult to find NE bound-
1http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/
2http://nlp.cs.nyu.edu/irex/
aries, however, NE extraction is relatively accurate
(Sekine and Isahara, 2000).
Most of the past research on NE extraction used
monolingual corpora, but the application of NE ex-
traction techniques to bilingual (or multilingual) cor-
pora is expected to obtain NE translation pairs. We
are developing a Japanese-English machine trans-
lation system for documents including many NEs,
such as news articles or documents about current
topics. Translating NE correctly is indispensable for
conveying information correctly. NE translations,
however, are not listed in conventional dictionaries.
It is necessary to retrieve NE translation knowledge
from the latest bilingual documents.
When extracting translation knowledge from
bilingual corpora, using literally translated parallel
corpora, such as official documents written in sev-
eral languages makes it easier to get the desired in-
formation. However, not many of such corpora con-
tain the latest NEs. There are few Japanese-English
corpora which are translated literally. Therefore,
we decided to extract NE translation pairs from
content-aligned corpora, such as multilingual broad-
cast news articles including new NEs daily, which
are not literally translated.
Sentential alignment (Brown et al, 1991; Gale
and Church, 1993; Kay and Ro?scheisen, 1993; Ut-
suro et al, 1994; Haruno and Yamazaki, 1996) is
commonly used as a starting point for finding the
translations of words or expressions from bilingual
corpora. However, it is not always possible to cor-
respond non-parallel corpora in sentences. Past sta-
tistical methods for non-parallel corpora (Fung and
Yee, 1998) are not valid for finding translations of
words or expressions with low frequency. These
methods have a problem in covering NEs because
there are many NEs that appear only once in a cor-
pus. So we need a specialized method for extract-
ing NE translation pairs. Transliteration is used for
finding the translations of NE in the source language
from texts in the target language (Stalls and Knight,
1998; Goto et al, 2001; Al-Onazian and Knight,
2002). Transliteration is useful for the names of per-
sons and places; however, it is not applicable to all
sorts of NEs.
Content-aligned documents, such as a bilingual
news corpus, are made to convey the same top-
ics. Since NEs are the essential element of docu-
ment contents, content-aligned documents are likely
to share NEs pointing to the same objects. Con-
sequently, when extracting all NEs with NE class
information from each of a pair of bilingual docu-
ments separately by applying monolingual NE ex-
traction techniques, the distribution of the NEs in
each document may be similar enough to recognize
correspondences between the NE translation pairs.
A technique for finding bilingual NE correspon-
dences will have a wide range of applications other
than NE translation-pair extraction. For example,
? Bilingual NE correspondences have clues for
identifying corresponding parts in a pair of
noisy bilingual documents.
? The similarity of any two documents in dif-
ferent languages can be estimated by NE
translation-pair correspondence.
For this research, we obtained a Japanese-English
broadcast news corpus (Kumano et al, 2002) by
the Japanese broadcast company NHK3, and we are
manually tagging NEs in the corpus to analyze it
and to conduct NE translation-pair extraction exper-
iments.
The tag specifications are based on the IREX
NE task (Sekine and Isahara, 1999), the evaluation
workshop of Japanese NE extraction. We extended
the specifications to English NEs. In addition, coref-
erence information between NEs, within the same
monolingual document and between the correspond-
ing Japanese-English document pairs (henceforth,
3Nippon Hoso Kyokai (Japan Broadcasting Corporation)
(http://www.nhk.or.jp/englishtop/)
we call these in a language and across languages,
respectively), is added to each of the tagged NEs,
for NE translation-pair extraction studies.
In Section 2, we will introduce the bilingual cor-
pus used in this study and describe its characteris-
tics. Then, we will discuss tag design for NE extrac-
tion studies, and explain the tag specifications and
existing problems. The current status of corpus an-
notation under these specifications will also be in-
troduced. We analyzed an annotated part of the cor-
pus in terms of NE occurrence and translation. This
analysis will be shown in Section 3. In Section 4,
we will mention future plans for the extraction of
NE translation-pairs.
2 Constructing a Japanese-English
broadcast news corpus with NE tags
2.1 Characteristics of the NHK
Japanese-English broadcast news corpus
We are annotating an NHK broadcast news corpus
with NE tags. The corpus is composed of Japanese
news articles for domestic programs and English
news articles translated for international broadcast-
ing4 and domestic bilingual programs5.
Figure 1 shows an example of a Japanese news
article and its translation in English. The original
Japanese article and the translated English article
deal with the same topic, but they differ much in de-
tails. The difference arises from the following rea-
sons (Kumano et al, 2002).
Audience Content might be added or deleted, ac-
cording to the audience, especially for interna-
tional broadcasting.
Broadcasting date The broadcasting of English
news is often delayed compared to the origi-
nal Japanese news. The time expressions might
be changed sometimes or new facts might be
added to the articles.
News styles / languages Comparing news articles
of two languages reveals that they have differ-
ent presentation styles, for example, facts are
sometimes introduced in a different order. The
4NHK WORLD (http://www.nhk.or.jp/
nhkworld/)
5http://www.nhk.or.jp/englishtop/
program_list/
Original article in Japanese (and its literal translation in English by authors):
1: ????????????????????????????????????
????????????
(There was a strong earthquake at 6:42 this morning in Izu Islands, the site of recent
numerous earthquakes. An earthquake of a little less than five in seismic intensity was
observed at Shikine Island.)
2: ????????????????????????????????????
????????????????????
(In addition, an event of seismic intensity four was observed for Niijima and Kozu Is-
land, events seismic intensity three for Toshima Island and Miyake Island, and events of
seismic intensity two and one for various parts of Kanto Area and Shizuoka Prefecture.)
3: ???????????????????
(There is no risk of tsunamis resulting from this earthquake.)
4: ?????????????????????????????????????
????????????????????????????
(According to observations by the Meteorological Agency, the earthquake epicenter
was located in the sea at a depth of ten kilometers near Niijima and Kozu Island. The
magnitude of the earthquakes was estimated to be five point one.)
5: ????????????????????????????????????
????????????????????????????????????
??????????????????
(In Izu Islands, where seismic activity has been observed from the end of June, repeated
cycles of seismic activity and dormancy have been observed. On the 30th of the previ-
ous month, a single strong earthquake having seismic intensity of a little less than six
was observed at Miyake Island, while two earthquakes having seismic intensity of five
were also observed there.)
6: ????????????????????????????????????
????????????????????????????????????
????
(In a series of seismic events, seventeen earthquakes having seismic intensity over five
have been observed up to this point, including strong tremors with a seismic intensity
of a little less than six observed four times at Kozu Island, Niijima, and Miyake Island.)
Translated article in English:
1: A strong earthquake jolted
Shikine Island, one of the Izu
islands south of Tokyo, early
on Thursday morning.
2: The Meteorological Agency
says the quake measured five-
minus on the Japanese scale of
seven.
3: The quake affected other is-
lands nearby.
4: Seismic activity began in the
area in late July, and 17 quakes
of similar or stronger intensity
have occurred.
5: Officials are warning of more
similar or stronger earthquakes
around Niijima and Kozu Is-
lands.
6: Tokyo police say there have
been no reports of damage
from the latest quake.
Figure 1: An article pair in an NHK broadcast news corpus
difference is due to language and socio-cultural
backgrounds.
2.2 NE tag design
We designed NE tags for NE translation-pair extrac-
tion research and working efficiency for manual an-
notation. The specifications are shown below.
? It is desirable that NE recognition guidelines
be consistent with NE tags of existing corpora.
Past guidelines of MUC and IREX should be
respected because they were configured as a re-
sult of many discussions. Consistent guidelines
enable us to utilize existing annotated corpora
and systems designated for the corpora.
? Within each bilingual document pair, corefer-
ence between NEs in a language and across lan-
guages will be specified. When several NEs
exist for the same referent in a document, it
is not always possible to determine the actual
translation for each instance of the NEs from
the counterpart document, because our corpus
is not composed of literal translations. There-
fore, coreference between NEs in a language
should be marked so that the coreference across
languages can be assigned between NE groups
that have the same referent. Coreference be-
tween NE groups is sufficient for our purpose.
? Assignment of coreference in a language is lim-
ited between NEs only. Although NEs may
have the same referent with pronouns or non-
NE expressions, these elements are ignored to
avoid complicating the annotation work.
2.3 Tag specifications
1. The tag specifications conform to IREX NE
tag specifications (IREX Committee, 1999) (an
English description in (Sekine and Isahara,
1999)) as regards the markup form, NE classes,
and NE recognition guidelines.
Japanese:
????????<LOCATION ID=?1? COR=?2?>
(Izu Islands)
????</LOCATION>
?<DATE ID=?2? COR=?4?>
(today)
???</DATE><TIME ID=?3? COR=?5?>
(a.m.)
??
(6:42)
??????</TIME>????????<LOCATION ID=?4? COR=?1?>
(Shikine Island)
???</LOCATION>????????????? ? ? ?
English:
A strong earthquake jolted <LOCATION ID=?1? COR=?4?>
Shikine Island</LOCATION>, one of the <LOCATION ID=?2?
COR=?1?>Izu islands</LOCATION> south of <LOCATION ID=?3?>
Tokyo</LOCATION>, early on <DATE ID=?4? COR=?2?>Thursday
</DATE> <TIME ID=?5? COR=?3?>morning</TIME>. ? ? ?
Figure 2: An annotation example
NE Class Example
Named entities (in the narrow sense):
ORGANIZATION The Diet; IREX Committee
PERSON (Mr.) Obuchi; Wakanohana
LOCATION Japan; Tokyo; Mt. Fuji
ARTIFACT Pentium Processor; Nobel Prize
Temporal expressions:
DATE September 2, 1999; Yesterday
TIME 11 PM; midnight
Number expressions:
MONEY 100 yen; $12,345
PERCENT 10%; a half
Table 1: NE Classes
Eight NE classes were defined at the IREX NE
task ? the same 7 classes as MUC-7 (3 types
of named entities in the narrow sense, 2 types
of temporal expressions, and 2 types of number
expressions), and ARTIFACT (concrete objects
like commercial products and abstract objects
such as laws or intellectual properties). Table 1
shows a list of these.
2. IREX?s NE classes and NE recognition guide-
lines are applied to English for consistency be-
tween Japanese and English NEs. For English-
specific annotation, such as prepositions or de-
terminers in NE, the MUC-7 Named Entity
Task Definition (Chinchor, 1997) is consulted6.
3. The SGML markup form of the IREX tag is
extended by adding the following two tag at-
tributes, which represent coreference informa-
tion in a language, and across languages.
ID=?NE group ID? (mandatory)
Each NE is assigned an attribute ID and
an ID number as its value. All corefer-
ent NEs in each language document are
6The tag specifications of IREX NE and those of MUC-7 do
not differ radically, because IREX NE tags are designed based
on the discussions of MUC.
given the same ID number7. The same
ID number is assigned to NEs that have
different forms, such as the full name and
the first name or the official name and the
abbreviated form, in addition to NEs with
the same form. Basically, NE are assigned
the same ID number when they belong to
an NE class and have the identical surface
form8.
COR=?ID for corresponding NE groups in
the other language? (optional)
When there exists a corresponding NE
(group) belonging to the same NE class
in the other language, an attribute COR
is given to each NE (group) in both lan-
guages, and the ID number for the coun-
terpart is assigned as a value to each other.
Annotations by the specifications are illustrated
in Figure 2.
2.4 Current status of the corpus annotation
Annotators who have experience in translation work
and in the production of linguistic data are engaging
in the tag annotation. Plans call for a total of 2,000
article pairs to be annotated, and about 1,100 pairs
have been finished up to the present.
2.5 Problems
Some problems became obvious in the course of
discussions of tag specifications and tag annotation
work. They confuse annotators and make the result
inaccurate. Typical cases are shown below.
2.5.1 The granularity difference between
Japanese and English
In Japanese, a unit smaller than a morpheme may
be accepted as an NE according to IREX guidelines.
7ID numbers do not maintain uniqueness across the docu-
ments.
8There are some exceptions. See Section 2.5.3.
(last Sunday and this Sunday)
sensyuu-no nichiyou -to konsyuu-no nichiyou
J: ???<DATE ID=?1?>??</DATE>????<DATE ID=?2?>??</DATE>
E: <DATE COR=?1?>last Sunday</DATE> and <DATE COR=?2?>this Sunday</DATE>
Figure 3: Assignment of different group IDs with NEs having the same surface form
On the other hand, English does not accept any unit
smaller than a word by MUC-7 guidelines. Some
Japanese NEs cannot have a counterpart English
NE, even if they have a corresponding English ex-
pression because of the difference in the segmenta-
tion granularity. For example, ????? (amerika;
America)? in the Japanese morpheme ??????
(amerika-jin; America-people)? is treated as an NE,
while no NE can be tagged to ?American?, the En-
glish counterpart of ??????.?
2.5.2 Translation problems
NEs have the same problem that translation in
general has: What is the exact translation word(s)
for an expression?
? Semantically corresponding expressions may
not be assigned corresponding NE relations,
because they belong to different NE classes or
an expression in a language is not recognized
as an NE. For example, a non-NE word ???
(seifu; government)? which means Japanese
government in Japanese articles is often trans-
lated as the English NE: ?Japan.?
? A non-literal translation of an NE may cause
difficulty in recognizing corresponding rela-
tions. Correspondences for some expressions
cannot be decided with the information repre-
sented in documents: Relative temporal expres-
sions in Japanese are often translated as ab-
solute expressions in English and those corre-
spondences cannot be identified without con-
sulting the calendar; Money expressions are
generally converted to dollars and the ex-
change rate at the relative time is needed to
confirm correspondences. For example, we
found a translation pair of money expressions
????? (sanzen-oku-en; three hundred bil-
lion yen)? and ?three billion U-S dollars? in our
corpus, which constitutes a rough conversion
from yen into dollars when the articles were
produced.
2.5.3 Assigning NE group IDs
We defined NEs that have the identical surface
form and the same NE class to be coreferent and
assigned the same NE group ID, in order to make
coreference judgment easier. There are some cases
where we cannot apply this rule, especially to tem-
poral expressions or number expressions.
The example in Figure 3 shows the translation
pair ???????????? (last Sunday and this
Sunday)? and ?last Sunday and this Sunday? anno-
tated with NE tags. Japanese temporal expressions
?????? (last Sunday)? and ?????? (this
Sunday)? are translated into English as ?last Sun-
day? and ?this Sunday? respectively. When anno-
tating NE tags for this translation pair, only ???
(Sunday)? in those temporal expressions in Japanese
is regarded as an NE according to the IREX?s NE
specifications. This causes a problem in which the
two NEs of the same surface form that are assigned
the same NE class have different referents. Each of
them should assign correspondence to different NEs
in the counterpart: the former to ?last Sunday? and
the latter to ?this Sunday.?
Tentatively, we allowed a different NE group ID to
be assigned to an NE with the identical surface form
in an NE class, as shown in Figure 3. It would be
better reexamine the consistency of the NE tag spec-
ification between Japanese and English, and the ne-
cessity of coreference information for temporal ex-
pressions and number expressions.
3 Analysis
We conducted an elementary investigation into
1,096 pairs of annotated Japanese and English ar-
ticles.
3.1 Corpus size
Table 2 shows the content size of our corpus by
the number of sentences and the morphemes/words.
The content decreases significantly when translating
from Japanese to English. This fact points out that
NE class
Japanese English
tokens avr. per types avr. per tokens avr. per types avr. per( art. / sent.) ( art. / sent.) ( art. / sent.) ( art. / sent.)
Total 24,147 (22.03 / 4.13) 12,809 (11.69 / 2.19) 15,844 (14.46 / 2.03) 10,353 ( 9.45 / 1.32)
ORGANIZATION 5,160 ( 4.71 / 0.88) 2,558 ( 2.33 / 0.44) 2,882 ( 2.63 / 0.37) 1,863 ( 1.70 / 0.24)
PERSON 3,525 ( 3.22 / 0.60) 1,628 ( 1.49 / 0.28) 2,800 ( 2.55 / 0.36) 1,410 ( 1.29 / 0.18)
LOCATION 8,737 ( 7.97 / 1.49) 3,752 ( 3.42 / 0.64) 5,792 ( 5.28 / 0.74) 3,302 ( 3.01 / 0.42)
ARTIFACT 455 ( 0.42 / 0.08) 282 ( 0.26 / 0.05) 241 ( 0.22 / 0.03) 193 ( 0.18 / 0.02)
DATE 4,342 ( 3.96 / 0.74) 2,959 ( 2.70 / 0.51) 2,990 ( 2.73 / 0.38) 2,620 ( 2.39 / 0.34)
TIME 854 ( 0.78 / 0.15) 740 ( 0.68 / 0.13) 245 ( 0.22 / 0.03) 232 ( 0.21 / 0.03)
MONEY 577 ( 0.53 / 0.10) 462 ( 0.42 / 0.08) 517 ( 0.47 / 0.07) 375 ( 0.34 / 0.05)
PERCENT 497 ( 0.45 / 0.08) 428 ( 0.39 / 0.07) 377 ( 0.34 / 0.05) 358 ( 0.33 / 0.05)
Table 3: NE frequency
articles sentences morphemes/words(avr. per article) (avr. per sent.)
J 1,096 5,851 (5.34) 321,204 (54.90)E 7,815 (7.13) 181,180 (23.18)
Table 2: Corpus size
the content tends to be lost through the translation
process.
3.2 In-language characteristics of NE
occurrences
3.2.1 Frequency
The number of occurrences for each NE class is
listed in Table 3. The distribution of NE classes is
almost the same as that in the data for MUC-7 or
IREX.
By comparing the decrease in content (cf. Ta-
ble 2), the number of NE tokens also decreases for
translations. However, the degree of the NE de-
crease is less than that of the morphemes/words. It
is also remarkable that the number of NE types is
fairly well preserved. Notice that only a small num-
ber of tokens in the NE class TIME appear in En-
glish. The reason may be that detailed time infor-
mation may become less important for English ar-
ticles, which are intended for audiences outside of
Japan and broadcast later than the original Japanese
articles.
3.2.2 NE characteristics within NE groups
To examine the surface form distribution in the
same NE groups, we counted the number of mem-
bers ( freq) and sorts of surface form (sort) for each
NE group in each article. The probability that a
given member has a unique surface form in a group
NE class
Japanese English
freq sort uniq freq sort uniq
Average 1.89 1.10 0.131 1.53 1.14 0.332
ORG. 2.02 1.12 0.144 1.55 1.16 0.345
PERSON 2.17 1.12 0.121 1.99 1.49 0.655
LOCATION 2.33 1.14 0.114 1.75 1.07 0.105
ARTIFACT 1.61 1.05 0.072 1.25 1.05 0.216
DATE 1.47 1.08 0.175 1.14 1.03 0.200
TIME 1.15 1.02 0.098 1.06 1.01 0.182
MONEY 1.25 1.03 0.109 1.38 1.35 0.936
PERCENT 1.16 1.00 0.008 1.05 1.06 0.278
Table 4: Surface form distribution in the same NE
groups
that has two or more members (uniq) has also been
calculated as follows:
uniq = freq? 2Csort? 2freq? 1Csort? 1 =
sort ? 1
freq ? 1 ( freq ? 2).
Table 4 shows the values averaged for all the NE
groups that appeared in all articles.
In English, a repetition of the same expression is
not conventionally desirable. Therefore, pronouns
or paraphrases are used frequently. On the other
hand, Japanese does not have such a convention.
This difference is considered to be the reason for the
result shown in Table 4: freq in English is smaller
than that in Japanese, and sort in English is larger
than that in Japanese. As a result, uniq in English is
higher than that in Japanese. These tendencies differ
slightly according to the NE classes.
? The sort of English PERSON is notably large. In
English, the name of a person is usually first ex-
pressed in full, and after that, it tends to be ex-
pressed only by the family name. In Japanese,
only the family name is generally used from the
beginning, especially for well-known persons.
NE class
J? E J? E
token type token type
Average 0.742 0.639 0.842 0.786
ORGANIZATION 0.684 0.612 0.877 0.837
PERSON 0.881 0.777 0.938 0.898
LOCATION 0.799 0.673 0.833 0.753
ARTIFACT 0.701 0.628 0.925 0.912
DATE 0.717 0.656 0.761 0.742
TIME 0.207 0.184 0.596 0.591
MONEY 0.593 0.595 0.781 0.733
PERCENT 0.712 0.692 0.830 0.827
Table 5: Cross-language corresponding rate
NE class
Japanese English
freq sort uniq freq sort uniq
Average 2.19 1.14 0.134 1.64 1.17 0.342
ORG. 2.25 1.17 0.164 1.62 1.19 0.364
PERSON 2.45 1.14 0.110 2.07 1.53 0.645
LOCATION 2.77 1.19 0.117 1.94 1.10 0.112
ARTIFACT 1.80 1.06 0.075 1.27 1.05 0.222
DATE 1.60 1.10 0.167 1.17 1.04 0.211
TIME 1.30 1.04 0.106 1.07 1.01 0.250
MONEY 1.24 1.04 0.138 1.47 1.43 0.934
PERCENT 1.20 1.00 0.010 1.06 1.01 0.250
Table 6: Surface form distribution in the same NE
groups (only for those having cross-language corre-
spondences)
? The uniq of English MONEY is quite high. A
money expression in Japanese tends to be trans-
lated into English as both the original currency
(usually yen) and dollars.
? The freq of temporal and number expressions
are smaller than those of named entities in the
narrow sense.
3.3 Cross-language characteristics of NE
occurrences
3.3.1 Correspondence across languages
We calculated the rates for a given NE in a doc-
ument to have a corresponding NE in the counter-
part language. The units of NE correspondences we
used for these calculations are both NE token and
NE group (type). The results, shown in Table 5,
show that an NE that appeared in English will have
a Japanese NE correspondent with a high rate.
We also conducted the same survey as we did in
Table 4 for only NEs having cross-language corefer-
ences, whose results are shown in Table 6. A com-
parison of both results shows that the freq for only
NEs having cross-language coreferences is larger,
NE class
J? E J? E
All Corr. only All Corr. only
All NEs 0.291 0.774 0.483 0.774
Average 0.304 0.790 0.494 0.790
ORG. 0.269 0.808 0.568 0.809
PERSON 0.403 0.877 0.671 0.875
LOCATION 0.318 0.746 0.461 0.745
ARTIFACT 0.410 0.725 0.662 0.710
DATE 0.307 0.805 0.428 0.805
TIME 0.033 0.815 0.227 0.815
MONEY 0.170 0.829 0.407 0.829
PERCENT 0.509 0.903 0.658 0.903
Table 7: Preservation ratio of NE order
especially in Japanese. An NE occurring more times
in an article may have more important information
and is more likely to appear in the translation.
3.3.2 Preservation of NE order
We investigated how well the order of NEs oc-
curring in an article is preserved in the counterpart
language as follows:
1. In every article, we eliminated all NEs except
the first occurrence of every NE group.
2. We calculated the ratio between all of the pos-
sible NE pairs in the source language and those
translated into the target language with the
same order of occurrence.
Table 7 lists the average preservation ratios of the
NE order for all NEs (?All?) and for NEs having
corresponding NEs in the counterpart (?Corr. only?).
The scores labeled ?All NEs? express ratios for the
order of all NEs. The preservation ratio for each
NE class is listed below in the table. The NE or-
ders are preserved so well even for all NEs that they
can be used for determining cross-language corre-
spondences.
4 Conclusion
In this paper, in which we aimed to acquire NE trans-
lation knowledge, we described our construction of
a Japanese-English broadcast news corpus with NE
tags for NE translation-pair extraction. The tags rep-
resent NE characteristics and coreference informa-
tion in a language and across languages. Analysis
of the annotated 1,097 article pairs has shown that
if NE occurrence information, such as classes, num-
ber of occurrences and occurrence order, is given for
each language side, it may provide a good clue for
determining NE correspondence across languages.
Our future plans are listed below.
? The problems in Section 2.5 need to be reex-
amined from the point of view of what infor-
mation bilingual corpora should have for NE
translation-pair extraction research.
? The proposed analysis in Section 3 pointed out
that identifying coreferences in a language is
very important for achieving NE translation-
pair extraction. Richer coreference information
should be annotated in our corpus for coref-
erence identification studies. We are planning
to annotate coreference information for pro-
nouns and some other non-NE expressions, re-
ferring to the MUC-7 coreference task defini-
tion (Hirschman and Chinchor, 1997).
? Corpora with different characteristics, such as a
bilingual newspaper corpus, will be annotated
and analyzed.
Acknowledgments This research was supported
in part by the Telecommunications Advancement
Organization of Japan.
References
Yaser Al-Onazian and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02), pages 400?408.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th Annual Meeting of the Association
for Computational Linguistics (ACL-91), pages 169?
176.
Nancy Chinchor. 1997. MUC-7 named entity task
definition. http://www.itl.nist.gov/iaui/
894.02/related_projects/muc/proceedings/
ne_task.html.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 36th Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
(COLING-ACL ?98), volume I, pages 414?420.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Isao Goto, Noriyoshi Uratani, and Terumasa Ehara.
2001. Cross-language information retrieval of proper
nouns using context information. In Proceedings of
the 6th Natural Language Processing Pacific Rim Sym-
posium (NLPRS 2001), pages 571?578.
Masahiko Haruno and Takefumi Yamazaki. 1996. High-
performance bilingual text alignment using statistical
and dictionary information. In Proceedings of the 34th
International Conference on Computational Linguis-
tics (ACL ?96), pages 131?138.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 coreference task definition. http:
//www.itl.nist.gov/iaui/894.02/related_
projects/muc/proceedings/co_task.html.
IREX Committee. 1999. Named entity extraction task
definition (version 990214). http://nlp.cs.nyu.
edu/irex/NE/df990214.txt. (In Japanese).
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Tadashi Kumano, Isao Goto, Hideki Tanaka, Noriyoshi
Uratani, and Terumasa Ehara. 2002. A translation
aid system by retrieving bilingual news database. Sys-
tems and Computers in Japan, 33(8):19?29. (Original
written in Japanese is in Transactions of the Institute
of Electronics, Information and Communication Engi-
neers, J85-D-II(6):1175?1184. 2001).
Elaine Marsh and Dennis Perzanowski. 1998. MUC-7
evaluation of IE technology: Overview and results.
http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/proceedings/muc_7_
proceedings/marsh_slides.pdf.
Satoshi Sekine and Hitoshi Isahara. 1999. IREX
project overview. http://nlp.cs.nyu.edu/
irex/Paper/irex-e.ps. (Original written in
Japanese is in Proceedings of the IREX Workshop,
pages 1?5).
Satoshi Sekine and Hitoshi Isahara. 2000. IREX: IR
and IE evaluation project in Japanese. In Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation (LREC-2000), pages 1475?
1480.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text. In
Proceedings of the Workshop on Computational Ap-
proaches of the Semitic Languages, pages 34?41.
Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statistics.
In Proceedings of the 32th International Conference
on Computational Linguistics (ACL-94), pages 1076?
1082.
Analysis and modeling of manual summarization of 
 Japanese broadcast news 
 Hideki Tanaka, Tadashi Kumano, Masamichi Nishiwaki and Takayuki Itoh 
Science and Techinical Research Laboratories of NHK 
1-10-11, Kinuta, Setagaya-ku 
Tokyo, 157-8510, Japan 
{tanaka.h-ja,kumano.t-eq,nishiwaki.m-hk,itou.t-gq}@nhk.or.jp 
Abstract 
We describe our analysis and modeling 
of the summarization process of Japa-
nese broadcast news. We have studied 
the entire manual summarization proc-
ess of the Japan Broadcasting Corpora-
tion (NHK). The staff of NHK has been 
making manual summarizations of 
news text on a daily basis since De-
cember 2000. We interviewed these 
professional abstractors and obtained a 
considerable amount of news summa-
ries. We matched the summary with the 
original text, investigated the news text 
structure, and thereby analyzed the 
manual summarization process. We 
then developed a summarization model 
on which we intend to build a summa-
rization system. 
1  Introduction 
Automatic text summarization research has a 
long history that dates back to the late 50?s 
(Mani and Maybury, 1999). It started mainly 
with the purpose of information gathering or 
assimilation, and most of the research has dealt 
with extracting the important parts of the texts. 
The summaries obtained with these techniques, 
so called extracts, have been used for judging 
the importance of the texts. 
We have started research on automatic sum-
marization for the purpose of information 
dissemination, namely summarization of news 
texts for broadcast news. Recently, we have 
studied the entire manual summarization process 
of the Japan Broadcasting Corporation (NHK). 
NHK has been making manual summariza-
tions of news text on a daily basis since Decem-
ber 2000, when it started satellite digital 
broadcasting. The summarized text has been 
used for the data service of the digital broadcast-
ing and on Web pages accessible by mobile 
phones.  
We interviewed NHK?s professional abstrac-
tors and analyzed a considerable amount of 
news summaries. We matched these summaries 
with the original news and studied the summari-
zation process based on the results of our analy-
sis and interviews.  
In this paper, we report on what we found 
during the interviews with the abstractors and 
the results of the automatic text alignment be-
tween summaries and the original news together 
with the word position matching. We also pro-
pose a summarization model for an automatic or 
semi-automatic summarization system. 
2 The manual summarization process 
Most of the radio and TV news services of 
NHK are based on a ?general news manuscript.? 
We call such manuscripts the original news in 
this paper. The original news is manually sum-
marized into summary news that are made avail-
able to the public through Web pages and digital 
broadcasting, as mentioned in section 1.  
We asked professional abstractors about the 
summarization environment and process and in 
so doing discovered the following. 
? Abstractor 
The original news is written by NHK report-
ers, and the text is summarized by different 
writers, i.e., professional abstractors. Most pro-
fessional abstractors are retired reporters who 
have expertise in writing news. 
? Compression rate and time allowance 
The original news is compressed to a maxi-
mum length of 105 Japanese characters. We will 
49
show in section 4 that the average compression 
rate is about 22.5%. The upper bound is decided 
from the display design of the data service of 
digital TV broadcasting. The abstractors must 
work quickly because the summary news must 
be broadcast promptly. 
? Techniques 
The abstractors use only information con-
tained in the original news. They scan the origi-
nal news quickly and repeatedly, not to 
understand the full content, but to select the 
parts to be used in the summary news. The ab-
stractors? special reading tendency has been re-
ported in (Mani, 2001), and we can say the same 
tendency was observed in our Japanese abstrac-
tors. The abstractors focus on the lead (the open-
ing part) of the original news. They sometimes 
use the end part of the original news. 
3    Corpus construction 
We planned the summary news corpus as a 
resource to investigate the manual summariza-
tion process and to look into the possibility of an 
automatic summarization system for broadcast 
news. We obtained 18,777 pieces of summary 
news from NHK. Although each piece is a 
summary of a particular original news text, the 
link between the summary and the original news 
is not available.  
We matched the summary and original news 
and constructed a corpus. There have been sev-
eral attempts to construct <summary text, origi-
nal text> corpora (Marcu, 1999; Jing and 
McKeown, 1999). We decided to use the 
method proposed by Jing and McKeown (1999) 
for the reasons given below. 
As our abstractors mentioned that they used 
only information available in the original news, 
we hypothesize that the summary and the origi-
nal news share many surface words. This indi-
cates that the surface-word-based matching 
methods such as (Marcu, 1999; Jing and McKe-
own, 1999) will be effective. 
In particular, the word position matching re-
alized in (Jing and McKeown, 1999) seems es-
pecially useful. We thought that we might be 
able to observe the summarization process pre-
cisely by tracing the word position links, and we 
employed their work with a little modification.  
As a result, our corpus takes the form of the 
triple: <summary, original, word position corre-
spondence>. 
3.1 Matching algorithm 
Jing and McKeown (1999) treated a word 
matching problem between a summary and its 
text, which they called the summary decomposi-
tion problem. They employed a statistical model 
(briefly described below) and obtained good 
results when they tested their method with the 
Ziff-Davis corpus. In the following explanation, 
we use the notion of summary and text instead 
of summary news and original news for simplic-
ity. 
(1) The word position in a summary is repre-
sented by <I>. 
(2) The word position in the text is repre-
sented by a pair of the sentence position (S) 
and the word position in a sentence (W) as in 
<S, W>. 
(3) Each summary word is checked as to 
whether it appears in the text. If it appears, 
all of the positions in the text are stored in 
the form of <S,W> to form a position trellis. 
(4) Scan the n summary words from left to 
right and find the path on the trellis that 
maximizes the score of formula (1). 
??
=
+ ===
1
1
11221 )),(|),((
n
i
ii WSIWSIPP  (1) 
This formula is the repeated product of the 
probability that the two adjacent words in a 
summary (Ii and Ii+1) appear at positions (S1, W1) 
and (S2, W2) in the text, respectively. This quan-
tity represents the goodness of the summary and 
the text word matching. As a result, the path on 
the trellis with the maximum probability gives 
the overall most likely word position match. 
Jing and McKeown (1999) assigned six-
grade heuristic values to the probability. The 
highest probability of 1.0 was given when two 
adjacent words in a summary appear at adjacent 
positions in the same sentence of the text. The 
lowest probability of 0.5 was given when two 
adjacent words in a summary appear in different 
sentences in the text with a certain distance or 
greater. We fixed the distance at two sentences, 
considering the average sentence count of the 
original news texts. 
50
Original news text 
??????????????????????????????????????????
??????????????????????????????????????????
????????????????????? 
?????????????????????????????????????????
?????????????????????????
?????????????????????????????????????????
?????????????????????????????????????????
??????????????????????? 
?????????????????????????????????????????
?????????????????????????????????????????
?????? 
body
lead
Summary news text
Figure 1. Summary and original news text matching.  
Jing and McKeown?s algorithm (1999) is de-
signed to treat a fixed summary and text pair and 
needs some modification to be applied to our 
two-fold problem of finding the original news of 
a given summary news from a large collection 
of news together with the word position match-
ing. 
Their method has a special treatment for a 
summary word that does not appear in the text. 
It assumes that such a word does not exist in the 
summary and therefore skips the trellis at this 
word with a probability of 1. This unfavorably 
biases news text that contains fewer matching 
words. To alleviate this problem, we experimen-
tally found that the probability score of 0.55 
works well for such a case (This score was the 
second smallest of the original six-grade score). 
We developed a word match browser to pre-
cisely check the words of the summary and 
original news. 
3.2 Summary and original news matching 
We matched 18,777 summary news texts 
from November 2003 to June 2004 against the 
news database, which mostly covers the original 
news of the period. We followed the procedures 
below. 
? Numerical expression normalization 
Numerical expressions in the original news 
are written in Chinese numerals (Kanji) and 
those of the summary news are written in 
Arabic numerals. We normalized the Chinese 
numerals into Arabic numerals. 
? Morphological analysis 
The summary and original news were mor-
phologically analyzed. We used morphemes 
as a matching unit. In this paper, we will use 
morphemes and words interchangeably. 
? Search span 
Each summary news was matched against the 
news written in the three-day period before 
the summary was written. This period was 
chosen experimentally. 
4  Results and observation 
We randomly checked the news matching re-
sults and found more than 90% were correct. 
Some of the summaries were exceptionally long, 
and we consider that such noisy data was the 
main reason for incorrect matching. Figure 1 
shows a matching example. The underlined (line 
and broken line) sentences show the word posi-
tion match.  
The word matching is not easy to evaluate 
because we do not have the correct matching 
answer. Although there are some problems in 
the matching, most of the results seem to be 
good enough for approximate analysis. The fol-
lowing discussion assumes that the word match-
ing is correct. 
4.1 Compression rate 
Table 1 shows the basic statistics of the 
summary and its corresponding original news. 
51
We can see that the average compression rate is 
22.5% in terms of characters. The average sum-
mary news length (109.9 characters per news 
text) was longer than what we were told (105, 
see section 2). 
We then checked the length of the typical 
summary texts. We found that the cumulative 
relative frequency of the summary text with the 
sentence count from 1 to 4 was 0.99 and was 
quite dominant. We checked the average length 
of these summaries and obtained 105.4, which is 
close to what we were told. We guess that noisy 
?long summaries? skewed the figure.  
 
 Original Summary
text counts 18,777 
Ave. sent. count/text 5.13 1.63
Ave. text length (char.) 487.7 109.9
Ave. first line length (char.)  94.9 81.3
0
10
20
30
40
50
60
70
80
%
1 2 3 4 5 6 7 8Sent. No.
Figure 2. Summary word employment
ratio of original news
4 sent. 5 sent 6 sent 7 sent 8 sent
Table 1. Basic statistics of summary and original 
news 
4.2 Word match ratio 
We measured how many of the summary 
words came from original news. As our match-
ing result contains word-to-word correspon-
dence, we calculated the ratio of the matched 
words in a summary text. Table 2 shows a part 
of the result. It shows the relative frequency of 
the summary news in which 100% of the words 
came from the original news reached 0.265 and 
those that had more than 90% reached 0.970.  
 
Word match ratio Rel. summary  freq. 
100? 0.265 
More than 90? 0.970 (cumulative) 
Table 2. Word match ratio 
 
This strongly suggests that most of the sum-
mary news is the ?extract? (Mani, 2001), which 
is written using only vocabulary appearing in the 
original news. This result is in accord with what 
the abstractors told us. 
4.3 Summary word employment in the 
original news sentences 
The previous section indicated that our sum-
mary likely belongs to the extract type. Where in 
the original news do these words come from? 
We next measured the word employment ratio 
of each sentence in the original news and the 
result is presented in Figure 2. 
In this graph, the original news is categorized 
into five cases according to its sentence count 
from 4 to 81 and the average word employment 
ratio is shown for each sentence.  
Of this figure, the following observations can 
be made: 
?  Bias toward the first sentence 
In all five cases, the first sentence recorded 
the highest word employment ratio. The per-
centages of the second and third sentences in-
crease when the news contains many sentences. 
The opening part of the news text is called the 
lead. We will discuss its role in the next section. 
? No clear favorite for the final sentence 
There was no employment ratio rise for the 
closing sentences in any case even though our 
abstractors indicated they often use information 
in the last sentence. This inconsistency may be 
due to the word match error. Final sentences 
actually have an important role in news, as we 
will see in the next section. 
5  Summarization model 
In the previous section, we found a quite 
high word overlap between a summary and the 
opening part of the original news text. We 
checked with our word match browser the simi-
larity of the summary news and lead sentences, 
and found that most of the summary sentences 
                                                          
1 These news texts cover the 88 % of the total news texts.  
52
take exactly the same syntactic pattern of the 
opening sentence. Based on this observation and 
what we found in the interviews, we devised a 
news text summarization model. The model can 
explain our abstractors? behavior, and we are 
planning to develop an automatic or semi-
automatic summarization system with it. We 
will explain the typical news text structure and 
present our model. 
5.1 News text structure 
Most of our news texts are written with a 
three-part structure, i.e., lead, body and supple-
ment. Figure 1 shows the two-fold structure of 
the lead and the body. Each part has the follow-
ing characteristics.   
? Lead 
The most important information is briefly de-
scribed in the opening part of a news text. This 
part is called the lead. Proper nouns are often 
avoided in favor of more abstract expressions 
such as ?a young man? or ?a big insurance com-
pany.? The lead is usually written in one or two 
sentences. 
? Body 
The lead is detailed in the body. The 5W1H 
information is mainly elaborated, and the proper 
names that were vaguely mentioned in the lead 
appear here. The statements of people involved 
in the news sometimes appear here. The repeti-
tive structure of the lead and the body is rooted 
in the nature of radio news; listeners cannot go 
back to the previous part if they missed the in-
formation. 
? Supplement 
Necessary information that has not been cov-
ered in the lead and the body is placed here. 
Take for an example of weather news about a 
typhoon. A caution from the Meteorological 
agency is sometimes added after the typhoon?s 
movement has been described.  
5.2 Model 
We found that most of the summary news is 
written based on the lead sentences. They are 
then shortened or partly modified with the ex-
pressions in the body to make them more infor-
mative and self-contained.  
The essential operation, we consider, lies in 
the editing of the lead sentences under the sum-
mary length constraint. Based on the observation, 
we have proposed a two-step summarization 
model of reading and editing. The summary in 
Figure 1 is constructed with the lead sentence 
with the insertion of a phrase in the body. 
? Reading phase 
(1) Identify the lead, the body and the sup-
plement sentences in the original news. 
(2) Analysis 
Find the correspondences between the parts 
in the lead and those in the body. We can re-
gard this process as a co-reference resolution.  
?  Summary editing phase 
(3) Set the lead sentence as the base sentence 
of the summary. 
(4) Apply the following operations until the 
base sentence length is close enough to the 
predefined length N. 
(4-1) Delete parts in the base sentence. 
(4-2) Substitute parts in the base sentence 
with the corresponding parts in the body with 
the results of (2). 
(4-2?) Add a body part to the base sentence. 
We may view this as a null part substituted 
by a body part. 
(4-3) Add supplement sentences. 
The supplement is often included in a sum-
mary; this part contains different information 
from the other parts.  
5.3 Related works and discussion 
Our two-step model essentially belongs to 
the same category as the works of (Mani et al, 
1999) and (Jing and McKeown, 2000). Mani et 
al. (1999) proposed a summarization system 
based on the ?draft and revision.? Jing and 
McKeown (2000) proposed a system based on 
?extraction and cut-and-paste generation.? Our 
abstractors performed the same cut-and-paste 
operations that Jing and McKeown noted in their 
work, and we think that our two-step model will 
be a reasonable starting point for our subsequent 
research. Below are some of our observations. 
53
The lead sentences play a central role in our 
model since they serve as the base of the final 
summary. Their identification can be achieved 
with the same techniques as used for the impor-
tant sentence extraction. In our case, the sen-
tence position information plays an important 
role as was shown by Kato and Uratani (2000). 
We consider the identification of the body and 
the supplement part together with the lead will 
be beneficial for the co-reference resolution. 
The co-reference resolution problem between 
the lead and the body should be treated in a 
more general way than usual. We found that our 
problem ranges from the word level, the corre-
spondence between named entities and their ab-
stract paraphrases, to the sentence level, an 
entire statement of a person and its short para-
phrase. We are now investigating the types of 
co-reference that we have to cover. 
We found that the deletion of lead parts did 
not occur very often in our summary, unlike the 
case of Jing and McKeown (2000). One reason 
is that most of our leads were short enough2 to 
be included in the summary and therefore the 
substitution operation became conspicuous. This 
usually increased the length of summary but 
contributed to making it more lively and infor-
mative. 
A supplement part was often included in the 
summary. We consider that this feature corre-
sponds to the abstractors? comments on em-
ployment of the final sentence, which was not 
clearly detected in our statistical investigation 
described in section 4.3. We are now investigat-
ing the conditions for including the supplement. 
We have so far listed the basic operations of 
editing through the manual checking of samples, 
and we are currently analyzing the operations 
with more examples. We will then study auto-
matic selection of the optimum operation se-
quence to achieve the most informative and 
natural summary. 
6  Conclusions 
We have described the manual summary 
process of NHK?s broadcast news and experi-
ments on automatic text alignment between 
news summaries and the original news together 
                                                          
2 The present summary length constraint is 105 characters. 
Meanwhile, the average length of the first sentence (typi-
cally the lead) of a  news text is 94.5 as is shown in table 1.  
with the word position matching. Through a sta-
tistical analysis of the results and interviews 
with abstractors, we found that the abstractors 
summarize news by taking advantage of its 
structure. Based on this observation, we pro-
posed a summarization model that consists of a 
reading and editing phase. We are now design-
ing an automatic or semi automatic summariza-
tion system employing the model. 
Acknowledgement  
The authors would like to thank Mr. Isao 
Goto and Dr. Naoto Kato of ATR for valuable 
discussion and Mr. Riuzo Waki of Eugene 
Software Inc. for implementing our ideas. 
References 
Jing, Hongyan and Kathleen R. McKeown. 1999. 
The Decomposition of Human-Written Summary 
Sentences. The 22nd Annual International ACM 
SIGIR Conference, pages 129-136, Berkeley. 
Jing, Hongyan and Kathleen R. McKeown. 2000. Cut 
and Paste Based Text Summarization. The 1st 
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 
178-185, Seattle. 
Kato, Naoto and Noriyoshi Uratani. 2000. Important 
Sentence Selection for Broadcast News (in Japa-
nese), The 6th Annual convention of the Associa-
tion for Natural Language Processing, pages 237-
240, Kanazawa, Japan 
Mani, Inderjeet and Mark T. Maybury. 1999. Ad-
vances in Automatic Summarization, The MIT 
press, Cambridge, Massachusetts 
Mani, Inderjeet, Barbara Gates and Eric Bloedorn. 
1999. Improving Summaries by Revising them, 
The 37th Annual Meeting of the Association for 
Computational Linguisics, pages 558-565, Mary-
land. 
Mani, Inderjeet. 2001. Automatic Summarization. 
John Benjamins, Amsterdam/Philadelphia. 
Marcu, Daniel. 1999. The automatic construction of 
large-scale corpora for summarization research.  
The 22nd Annual International ACM SIGIR Con-
ference, pages 137-144, Berkeley. 
54
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39?47,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Syntax-Driven Sentence Revision for Broadcast News Summarization 
 
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa, 
Tadashi Kumano and Naoto Kato 
NHK Science and Technology Research Labs. 
1-10-11, Kinuta, Setagaya-ku, Tokyo, Japan 
{tanaka.h-ja,kinoshita.a-ek,kobayakawa-t.ko,kumano.t-eq,kato.n-ga}@nhk.or.jp 
 
Abstract 
We propose a method of revising lead sentences in 
a news broadcast. Unlike many other methods pro-
posed so far, this method does not use the corefer-
ence relation of noun phrases (NPs) but rather, 
insertion and substitution of the phrases modifying 
the same head chunk in lead and other sentences. 
The method borrows an idea from the sentence 
fusion methods and is more general than those 
using NP coreferencing as ours includes them. We 
show in experiments the method was able to find 
semantically appropriate revisions thus demon-
strating its basic feasibility. We also show that that 
parsing errors mainly degraded the sentential com-
pleteness such as grammaticality and redundancy.  
1 Introduction 
We address the problem of revising the lead sen-
tence in a broadcast news text to increase the 
amount of background information in the lead. 
This is one of the draft and revision approaches 
to summarization, which has received keen atten-
tion in the research community. Unlike many 
other methods that directly utilize noun phrase 
(NP) coreference (Nenkova 2008; Mani et al 
1999), we propose a method that employs inser-
tion and substitution of phrases that modify the 
same chunk in the lead and other sentences. We 
also show its effectiveness in a revision experi-
ment.  
As is well known, the extractive summary that 
has been extensively studied from the early days 
of summarization history (Luhn, 1958) suffers 
from various drawbacks. These include the prob-
lems of a break in cohesion in the summary text 
such as dangling anaphora and a sudden shift in 
topic.  
To ameliorate these problems, the idea of revis-
ing the extracted sentences was proposed in a 
single document summarization study. Jing and 
McKeown (1999; 2000) found that human sum-
marization can be traced back to six cut-and-
paste operations of a text and proposed a revision 
method consisting of sentence reduction and 
combination modules with a sentence extraction 
part. Mani and colleagues (1999) proposed a 
summarization system based on ?draft and revi-
sion? together with sentence extraction. The re-
vision part is achieved with the sentence aggre-
gation and smoothing modules. 
The cohesion break problem becomes particu-
larly conspicuous in multi-document summariza-
tion. To ameliorate this, revision of the extracted 
sentences is also thought to be effective, and 
many ideas and methods have been proposed so 
far. For example, Otterbacher and colleagues 
(2002) analyzed manually revised extracts and 
factored out cohesion problems. Nenkova (2008) 
proposed a revision idea that utilizes noun 
coreference with linguistic quality improvements 
in mind.  
Other than the break in cohesion, multi-
document summarization faces the problem of 
information overlap particularly when the docu-
ment set consists of similar sentences. Barzilay 
and McKeown (2005) proposed an idea called 
sentence fusion that integrates information in 
overlapping sentences to produce a non-
overlapping summary sentence. Their algorithm 
firstly analyzes the sentences to obtain the de-
pendency trees and sets a basis tree by finding 
the centroid of the dependency trees. It next 
augments the basis tree with the sub-trees in oth-
er sentences and finally prunes the predefined 
constituents. Their algorithm was further modi-
fied and applied to the German biographies by 
Filippova and Strube (2008).  
Like the work of Jing and McKeown (2000) and 
Mani et al (1999), our work was inspired by the 
summarization method used by human abstrac-
tors. Actually, our abstractors first extract impor-
tant sentences, which is called lead identification, 
and then revise them, which is referred to as 
phrase elaboration or specification. In this paper, 
we concentrate on the revision part.   
Our work can be viewed as an application of the 
sentence fusion method to the draft and revision 
39
approach to a single Japanese news document 
summarization. Actually, our dependency struc-
ture alignment is almost the same as that of 
Filippova and Strube (2008), and our lead sen-
tence plays the role of a basis tree in the Barzilay 
and McKeown approach (2005). Though the idea 
of sentence fusion was developed mainly for 
suppressing the overlap in multi-document sum-
marization, we consider this effective in aug-
menting the extracts in a single-document sum-
marization task where we face less overlap 
among sentences. 
Before explaining the method in detail, we will 
briefly introduce the Japanese dependency 1  
structure on which our idea is based. The de-
pendency structure is constructed based on the 
bunsetsu chunk, which we call ?chunk? for sim-
plicity. The chunk usually consists of one con-
tent-bearing word and a series of function words. 
All the chunks in a sentence except for the last 
one modify a chunk in the right direction. We 
call the modifying chunk the modifier and the 
modified chunk the head. We usually span a di-
rected edge from a modifier chunk to the head 
chunk 2 . Our dependency tree has no syntactic 
information such as subject or object. 
                                                
2 Broadcast news summarization 
Tanaka et al (2005) showed that most Japanese 
broadcast news texts are written with a three-part 
structure, i.e., the lead, body, and supplement. 
The most important information is succinctly 
mentioned in the lead, which is the opening sen-
tence(s) of a news story, referred to as an ?arti-
cle? here. Proper names and details are some-
times avoided in favor of more abstract expres-
sions such as ?big insurance company.? The lead 
is then detailed in the body by answering who, 
what, when, where, why, and how, and proper 
names only alluded to in the lead appear here. 
Necessary information that was not covered in 
the lead or the body is placed in the supplement.  
The research also reports that professional news 
abstractors who are hired for digital text services 
summarize articles in a two-step approach. First, 
they identify the lead sentences and set it (them) 
as the starting point of the summary. As the av-
erage lead length is 95 characters and the al-
 
1 This is the kakari-uke (modifier-modifiee) relation of 
Japanese, which differs from the conventional dependency 
relation. We use the term dependency for convenience in 
this paper. 
2 This is the other way around compared to the English de-
pendency such as in Barzilay and McKeown (2005).  
lowed summary length is about 115 characters 
(or 150 characters depending on the screen de-
sign), they revise the lead sentences using ex-
pressions from the remainder of the story.   
We see here that the extraction and revision 
strategy that has been extensively studied by 
many researchers for various reasons was actu-
ally applied by human abstractors, and therefore, 
the strategy can be used as a real summarization 
model. Inspired by this, we decided to study a 
news summarization system based on the above 
approach. To develop a complete summarization 
system, we have to solve three problems: 1) 
identifying the lead, body, and supplement struc-
ture in each article, 2) finding the lead revision 
candidates, and 3) generating a final summary by 
selecting and combining the candidates. 
We have already studied problem 1) and showed 
that automatic recognition of three tags with a 
decision tree algorithm reached a precision over 
92% (Tanaka et al 2007). We then moved to 
problem 2), which we discuss extensively in the 
rest of this paper.  
3 Manual lead revision experiment 
To see how problem 2) in the previous section 
could be solved, we conducted a manual lead-
revision experiment. We asked a native Japanese 
speaker to revise the lead sentences of 15 news 
articles using expressions from the body section 
of each article with cut-and-paste operations (in-
sertion and substitution) of bunsetsu chunk se-
quences. We refer to chunk sequences as phrases. 
We also asked the reviser to find as many revi-
sions as possible.  
In the interview with her, we found that she took 
advantage of the syntactic structure to revise the 
lead sentences. Actually, she first searched for 
the ?same? chunks in the lead and the body and 
checked whether the modifier phrases to these 
chunks could be used for revision. To see what 
makes these chunks the ?same,? we compared 
the syntactic head chunk of the lead and body 
phrases used for substitution and insertion. 
Table 1 summarizes the results of the compari-
son in three categories: perfect match, partial 
match (content word match), and different. 
The table indicates that nearly half of the head 
chunks were exactly the same, and the rest con-
tained some differences. The second row shows 
the number where the syntactic heads had the 
same content words but not the same function 
words. The pair ??? kaidan-shi ?talked? and
?????? kaidan-shi-mashi-ta ?talked? is an 
40
  Ins. Sub. Total 
1) Perfect 9 6 15
2) Partial 6 6 12
3) Different 1 6 7
 Total 16 18 34
Lead
IAEA? 
of the IAEA
???? 
the team
??? 
at Korea 
??????
arrived 
Table 1. Degree of syntactic head agreement 
example. These are the  syntactic and aspectual 
variants of the same verb ???? kaidan-suru 
?talk.?  
The third row represents cases where the syntac-
tic heads had no common surface words. We 
found that even in this case, though, the syntactic 
heads were close in some way. In one example, 
there was accordance in the distant heads, for 
instance, in the pair ?????  mitsuka-tta  
?found? and ??? ichibu-no ?part of.? In this 
case, we can find the chunk ????? mit-
suka-tta ?found? at a short edge distance from ?
?? ichibu-no ?part of.?  Based on the findings, 
we devised a lead sentence revision algorithm. 
4 Revision algorithm 
4.1 Concept 
We explain here the concept of our algorithm 
and show an example in Figure 1. We have a 
lead sentence and a body sentence, both of which 
have the ?same? syntactic head chunk, ????
??, touchaku-shima-shi-ta, ?arrived.?  
The head chunk of the lead has two phrases (un-
derlined with thick lines in Figure 1) that directly 
modify the head. We call such a phrase a maxi-
mum phrase of a head3. Like the lead sentence, 
the body sentence also has two maximum phras-
es. In the following part, we use the term phrase 
to refer to a maximum phrase for simplicity. 
By comparing the phrases in Figure 1, we notice 
that the following operations can add useful in-
formation to the lead sentence; 1) inserting the 
first phrase of the body will supply the fact the 
visit was on the 4th, 2) substituting the first 
phrase of the lead with the second one in the 
body adds the detail of the IAEA team. This re-
vision strategy was employed by the human re-
viser mentioned in section 2, and we consider 
this to be effective because our target document 
has a so-called inverse pyramid structure (Robin 
and McKeown 1996), in which the first sentence 
is elaborated by the following sentences. 
                                                 
3 To be more precise, a maximum phrase is defined as the 
maximum chunk sequence on a dependency path of a head. 
 
 
Figure 1. Concept of revision algorithm 
Further analyzing the above fact, we devised the 
lead sentence revision algorithm below. We pre-
sent the outline here and discuss the details in the 
next section. We suppose an input pair of a lead 
and a body sentence that are syntactically ana-
lyzed. 
1) Trigger search 
We search for the ?same? chunks in the lead 
and body sentences. We call the ?same? 
chunks triggers as they give the starting point 
to the revision. 
2) Phrase alignment 
We identify the maximum phrases of each 
trigger, and these phrases are aligned according 
to a similarity metric. 
3) Substitution 
If a body phrase has a corresponding phrase in 
the lead, and the body phrase is richer in in-
formation, we substitute the body phrase for 
the lead phrase.  
4) Insertion 
If a body phrase has no counterpart in the lead, 
that is, the phrase is floating, we insert it into 
the lead sentence. 
Our method inserts and substitutes any type of 
phrase that modifies the trigger and therefore has 
no limitation in syntactic type. Although NP 
elaboration such as in (Nenkova 2008) is of great 
importance, there are other useful syntactic types 
for revision. An example is the adverbial phrase 
insertion of time and location. The insertion of 
the phrase 4? yokka ?on the 4th? in figure 1 in-
deed adds useful information to the lead sentence.     
4.2 Algorithm 
The overall flow of the revision algorithm is 
shown in Algorithm 1. The inputs are a lead and 
a body sentence that are syntactically parsed, 
which are denoted by L and B respectively. 
The whole algorithm starts with the all-trigger 
search in step 1. Revision candidates are then 
found for each trigger pair in the main loop from 
steps 2 to 6.  The revision for each trigger pair is  
IAEA? 
of the IAEA 
??? 
inspectors 
??????
arrived
5?? 
five 
Body
4?? 
on the 4th
insertion substitution
maximum phrase 
41
Algorithm 14 (Left figures are the step numbers.) 
1: find all trigger pairs between L and B and  
              store them in T.  
 T={(l, b) ; l b, l?L and b?B } ?
2: for all (l, b) ? T do 
  find l?s max phrases and store in Pl. 
  Pl={pl ; pl ? max phrase of l} 
3:  do the same for trigger b 
  Pb={pb ; pb ? max phrase of b} 
4:  align phrases in Pl and Pb and store  
   result in A 
   A={( pl, pb) ; pl  pb ,  ?
     pl ? Pl, pb ? Pb } 
5:  for all (pl, pb) ? A do 
   follow Table 2 
  end for 
6: end for 
 
Body  
pb =?  pb? ?  
pl =?  4: no op. 1: insertion Lead 
pl  ? ? 3: no op. 2: substitution 
Table 2. Operations for step 5 
found based on the idea in the previous section in 
steps 4 and 5. Now we explain the main parts. 
? Step 1: trigger chunk pair search  
We first detect the trigger pairs in step 1 that are 
the base of the revision process. What then can 
be a trigger pair that yields correct revisions? We 
roughly define trigger pairs as the ?coreferential? 
chunk pairs of all parts of speech, i.e., the parts 
of speech that point to the same entity, event, 
action, change, and so on.  
Notice that the term coreferential is used in an 
extended way as it is usually used to describe the 
phenomena in noun group pairs (Mitkov, 2002).  
The chunk ?????? touchaku-shimashita 
?arrived? and IAEA? IAEA-no ?of the IAEA? 
in Figure 1 are examples.  
Identifying our coreferential chunks is even 
harder than the conventional coreference resolu-
tion, and we made a simplifying assumption as in 
Nenkova (2008) with some additional conditions 
that were obtained through our preliminary ex-
periments.  
(1) Assumption: Two chunks having the same 
surface forms are coreferential. 
(2) Conditions for light verb (noun) chunks: 
Agreement of modifying verbal nous is fur-
                                                 ??
4 The sign a b means the chunk ?a? and ?b? are triggers.  
The sign p q means the phrases ?p? and ?q? are aligned. 
ther required for chunks whose content 
words consist only of light verbs such as ?
? aru ?be? and ?? naru ?become?: these 
chunks themselves have little lexical mean-
ing. The agreement is checked with the 
hand-crafted rules. Similar checks are ap-
plied to chunks whose content words consist 
only of light nouns such as ?? koto (?koto? 
makes the previous verb a noun) . 
(3) Conditions for verb inflections: a chunk that 
contains a verb usually ends with a function 
word series that indicates a variety of infor-
mation such as inflection type, dependency 
type, tense, aspect, and modality. Some in-
formation such as tense and aspect is vital to 
decide the coreference relation (exchanging 
the modifier phrases ?arrive? and ?will ar-
rive? will likely bring about inconsistency in 
meaning), although some is not. We are in 
the process of categorizing function words 
that do not affect the coreference relation and 
temporally adopted the empirically obtained 
rule: the difference in verb inflection be-
tween the te-form (predicate modifying 
form) and dictionary form (sentence end 
form) can be ignored.     
? Step 4: phrase alignment 
We used the surface form agreement for similar-
ity evaluation. We applied several metrics and 
explain them one by one. 
1) Chunk similarity t, s 
t, s : x, y? chunk [0, 1]. ?
Function t is the Dice coefficient between the 
set of content words in x and those in y. The 
same coefficient calculated with all words 
(function and content words) is denoted as s. 
2) Phrase absorption ratio 
a : px, py? phrases  [0, 1] ?
This is the function that indicates how many 
chunks in phrase px is represented in py and is 
calculated with t as in, 
?
? ?
=
x
ypx
py
x
yx yxtp
ppa )),((max
1
:),( . 
3) Alignment  quality 
With the above two functions, the alignment qual-
lity is evaluated by the function 
g : px, py ? phrases ?  [0, 1] 
],1,0[
),,()1(),(:),(
?
?+=
?
?? yxsppappg yxyx  
where the shorter phrase is set to px so that 
yx pp < . The variables x and y are the last 
42
chunks in px and py, respectively. Intuitively, 
the function evaluates how many chunks in the 
shorter phrase px are represented in py and how 
similar the last chunks are. The last chunk in a 
phrase, especially the function words in the 
chunk, determines the syntactic character of 
the phrase, and we measured this value with 
the second term of the alignment quality. The 
parameter ? is decided empirically, which was 
set at 0.375 in this paper. 
In alignment, we calculated the score for all 
possible phrase combinations and then greed-
ily selected the pair with the highest score. We 
set the minimum alignment score at 0.185; 
those pairs with scores lower than this value 
were not aligned. 
? Step 5 (Table 2, case 1): insertion 
Step 5 starts either an insertion or substitution 
process, as in Table 2. If pb  (body phrase is 
not null) and pl =  (lead phrase is null) in Table 
2, the insertion process starts.  
? ?
?
In this process, we check the following.  
1) Redundancy check 
Insertion may cause redundancy in informa-
tion. As a matter of fact, redundancy often 
happens when there is an error in syntactic 
analysis. Suppose there are the same lead and 
body phrases that modify the same chunks in 
the lead and body sentences. If the lead phrase 
fails to modify the correct chunk because of an 
error, the body phrase loses the chance to be 
aligned to the lead phrase since they belong to 
different trigger chunks. As a result, the body 
phrase becomes a floating phrase and is in-
serted into the lead chunk, which duplicates 
the same phrase.  
To prevent this, we evaluate the degree of du-
plication with the phrase absorption ratio a 
and allow phrase insertion when the score is 
below a predefined threshold ? : we allow in-
sertion when 
? ),( bb pLpa < ,? phrase, L : lead sentence, 
is satisfied. 
2) Discourse coherence check 
Blind phrase insertion may invite a break in 
cohesion in a lead sentence.  This frequently 
happens when the inserted phrase has words 
that require an antecedent. We then prepared a 
list of words that contain such context-
requiring words and forbid phrase insertions 
that contain words that are on the list.  This list 
contains the pronoun family such as ?? ko-
kono ?this? and special adjectives such as ?? 
chigau ?different.?  
3) Insertion point decision 
The body phrase should be inserted at the 
proper position in the lead sentence to main-
tain the syntactic consistency. Because we 
dealt with single-phrase insertion here, we 
employed a simple heuristics.  
Since the Japanese dependency edge spans 
from left to right as we mentioned in section 1, 
we considered that the right phrase of the in-
serted phrase is important to keep the new de-
pendency from the inserted phrase to the trig-
ger chunk. Because we already know the 
phrase alignment status at this stage, we fol-
low the next steps to determine the insertion 
position in the lead of the insertion phrase. 
A) In the body sentence, find the nearest right 
substitution phrase pr of the insertion 
phrase. 
B) Find the pr?s aligned phrase in the lead prL. 
C) Insert the phrase to the left of the prL. 
D) If there is no pr, insert the phrase to the left 
to the trigger. 
? Step 5 (Table 2, case 2): substitution 
If pb ?  ?  and pl ? ?  in Table 2, the substitu-
tion process starts. This process first checks if 
each aligned phrase pair contains the same chunk 
other than the present trigger. If there is such a 
chunk, the substitution phrase is reduced to the 
subtree from the present trigger to the identical 
chunk. The newly found identical chunks are in 
trigger table T, and the remaining part will be 
evaluated later in the main loop. Owing to the 
phrase partitioning, we can avoid phrase substi-
tutions which are in an inclusive relation.  
The substitution candidate goes through three 
checks: information increase, redundancy, and 
discourse cohesion. As the latter two are almost 
the same as those in the insertion, we explain 
here the information increase. This involves 
checking whether the number of chunks in the 
body phrase is greater than that in the aligned 
lead phrase. This is based on the simple assump-
tion that elaboration requires more words. 
5 Revision experiments 
5.1 Data and evaluation steps 
? Purpose 
We conducted a lead revision experiment with 
three purposes. The first one was to empirically 
evaluate the validity of our simplified assump-
43
tions: trigger identification and concreteness in-
crease evaluation. For trigger identification, we 
basically viewed the identical chunks as triggers 
and added some amendments for light verbs 
(nouns) and verb inflections. For the check of an 
increase in concreteness, we assumed that 
phrases with more chunks were more concrete. 
However, these simplifications should be veri-
fied in experiments. 
The second purpose was to check the validity of 
using the revision phrases only in body sentences 
and not in the supplemental sentences. 
The last one was to determine how ineffective 
the result is if the syntactic parsing fails. With 
these purposes in mind, we designed our experi-
ment as follows.  
? Data  
A total of 257 articles from news programs 
broadcast on 20 Jan., 20 Apr., and 20 July in 
2004 were tagged with lead, body, and supple-
ment tags by a native Japanese evaluator. The 
articles were morphologically analyzed by Me-
cab (Kudo et al, 2003) and syntactically parsed 
by Cabocha (Kudo and Matsumoto, 2002). 
? Evaluator and evaluation detail 
We prepared an evaluation interface that presents 
a lead with one revision point (insertion or sub-
stitution) that was obtained using the body and 
supplemental sentences to an evaluator. 
A Japanese native speaker evaluated the results 
one by one with the above interface. We planned 
a linguistic evaluation like DUC2005 (Hoa Trang, 
2005). Since their five-type evaluation is in-
tended for multi-document summarization, 
whereas our task is single-document summariza-
tion, and we are interested in evaluating our 
questions mentioned above, we carried out the 
evaluation as follows. In future, we plan to in-
crease the number of evaluation items and the 
number of evaluators.  
Concreteness Score 
Decreased 0 
Unchanged 1 
Increased 2 
Table 3. Evaluation of increased concreteness 
Completeness Required operations Score
Poor More than 2 0
Acceptable One 1
Perfect None 2
Table 4. Sentential completeness 
 
E1) The evaluator judged if the revision was ob-
tained from the lead and body sentences with 
or without parsing errors. Here, errors that did 
not affect the revision were not considered.  
E2) Second, she checked whether the revision 
was semantically correct or revised informa-
tion matching the fact described in the lead 
sentence. Here, she did not care about the 
grammaticality or the improvements in con-
creteness of the revision; if the revision was 
problematic but manually correctable, it was 
judged as OK. This step evaluated the correct-
ness of the trigger selection; wrong triggers, 
i.e., those referring to different facts produce 
semantically inconsistent revisions as they mix 
up different facts. 
The following evaluation was done for those 
judged correct in evaluation step E2, as we found 
that revisions that were semantically inconsistent 
with the lead?s facts were often too difficult to 
evaluate further.  
E3) Third, she evaluated the change in concrete-
ness after revision with the revisions that 
passed evaluation E2. She judged whether or 
not the revision increased the concreteness of 
the lead in three categories (Table 3). 
Notice that original lead sentences are sup-
posed to have an average score of 1. 
E4) Last, she checked the sentential complete-
ness of the revision result that passed evalua-
tion E2. They still contained problems such as 
grammatical errors and improper insertion po-
sition. Rather than evaluating these items sepa-
rately, we measured them together for senten-
tial completeness. At this time, we measured in 
terms of the number of operations (insertion, 
deletion, substitution) needed to make the sen-
tence complete5.  
As shown in Table 4, revisions requiring more 
than two operations are categorized as ?poor,? 
those requiring one operation are ?acceptable,? 
and those requiring no operations are ?perfect.? 
We employed this measure because we found 
that grading detailed items such as grammatical-
ity and insertion positions at fine levels was 
rather difficult. We also found that native Japa-
nese speakers can correct errors easily. Notice 
the lead sentences are perfect and are supposed 
                                                 
5 This was not an automatic process and may not be perfect. 
The evaluator simulated the correction in mind and judged 
whether it was done with one action. 
44
to have an average score of 2 in sentential com-
pleteness. Since the revision does not improve 
the completeness further but elicits defects such 
as grammatical errors, it usually produces a score 
below 2. Some examples of the results with their 
scores are shown below. The underlined parts are 
the inserted body chunk phrases, and the paren-
thesized parts are the deleted lead chunks. 
1) Concreteness 2, Completeness 2 
?????????????????
?????????????????
????????????? 
minkan-dantai-no ?private 
organization?, korea-
society-nado-ga ?Korea Soci-
ety and others?, shusai-suru  
?sponsored?, chousen-hantou-
heiwa-forumu-ni  ?Peace Fo-
rum in Korean Peninsula?, 
(moyooshi-ni ?event?), 
shusseki-suru ?attend? 
2) Concreteness 1, Completeness 2 
?????????????????
???? 
buhin-ni ?to the parts? ki-
retsu-ga ?cracks?, haitte-
iru-no-ga ?being there? (), 
mitsuka-tta ?found? 
3) Concreteness 2, Completeness 0 
?????????????????
???????????????? 
Herikoputa-kara ?from a hel-
icopter?, chijou-niju-
metoru-no-takasa-kara ?from 
20 meters high? (), rakka-
shi ?fell and?, shibou-
shima-shita ?killed? 
Example 1 is the perfect substitution and had 
scores of 2 for both concreteness increase and 
completeness. Actually, the originally vaguely 
mentioned term ?event? was replaced by a more 
concrete phrase with proper names, ?Korean Pen-
insula Peace Forum sponsored by Korea Society 
and others.? Notice that this can be achieved by 
NP coreference based methods if they can iden-
tify that these two different phrases are corefer-
ential. Our method does this through the depend-
ency on the same trigger ???? shusseki-suru 
?attend.? 
Example 2 is a perfect sentence, but its concrete-
ness stayed at the same level. As a result, the 
scores were 1 for concreteness increase and 2 for 
completeness. 
 Incorrect Correct Cor. Ratio 
Succ. 70 353 0.83Parse
Fail. 31 149 0.83
Body 50 464 0.90Sent.
Supp. 51 38 0.43
Table 5. Results of semantic correctness 
Score 0 1 2 Ave.
Succ. 0 55 298 1.84Parse
Fail. 1 19 129 1.86
Body 1 61 402 1.86Sent.
Supp. 0 13 25 1.66
Table 6. Results of concreteness increase 
Score 0 1 2 Ave.
Succ. 78 60 215 1.39Parse
Fail. 66 55 28 0.74
Body 120 110 234 1.25Sent.
Supp. 24 5 9 0.61
Table 7. Results of sentential completeness 
Actually, the original sentence that meant ?They 
found a crack in the parts? was revised to ?They 
found there was a crack in the parts,? which did 
not add useful information. Example 3 has a 
grammatical problem although the revision sup-
plied useful information.As a result, it had scores 
of 2 for concreteness increase and 0 for com-
pleteness. The added kara-case phrase (from 
phrase) ????????????? chijou-
niju-metoru-no-takasa-kara ?from 20 meters 
high? is useful, but since the original sentence 
already has the kara-case ???????? 
herikoputa-kara ?from helicopter,? the insertion 
invited a double kara-case, which is forbidden in 
Japanese. To correct the error, we need at least 
two operations, and thus, a completeness score of 
0 was assigned. 
5.2 Results of experiments 
Table 5 presents the results of evaluation E2, the 
semantic correctness with the parsing status of 
evaluation E1 and the source sentence category 
from which the phrases for revision were ob-
tained. Columns 2 and 3 list the number of revi-
sions (insertions and substitutions) that were cor-
rect and incorrect and column 4 shows the cor-
rectness ratio. We obtained a total of 603 revi-
sions and found that 30% (180/603) of them 
were derived with syntactic errors. 
The semantic correctness ratio was unchanged 
regardless of the parsing success. On the contrary, 
it was affected by the source sentence type. The 
correctness ratio with the supplemental sentence 
45
was significantly6 lower than that with the body 
sentence. Table 6 lists the results of the con-
creteness improvements with the parsing status 
and the source sentence type. Columns 2, 3 and 4 
list the number of revisions that fell in the scores 
(0-2) listed in the first row. The average score in 
this table again was not affected by the parsing 
failure but was significantly affected by the 
source sentence category. The result with the 
supplement sentences was significantly worse 
than that with body sentences. 
Table 7 lists the results of the sentential com-
pleteness in the same fashion as Table 6. The 
sentential completeness was significantly wors-
ened by both the parsing failure and source sen-
tence category.  
These results indicate that the answers to the 
questions posed at the beginning of this section 
are as follows. From the semantic correctness 
evaluation, we infer that our trigger selection 
strategy worked well especially when the source 
sentence category was limited to the body.  
From the concreteness-increase evaluation, the 
assumption that we made also worked reasonably 
well when the source sentence category was lim-
ited to the body.  
The effect of parsing was much more limited 
than we had anticipated in that it did not degrade 
either the semantic correctness or the concrete-
ness improvements. Parsing failure, however, 
degraded the sentential completeness of the re-
vised sentences. This seems quite reasonable: 
parsing errors elicit problems such as wrong 
phrase attachment and wrong maximum phrase 
identification. The revisions with these errors 
invite incomplete sentences that need corrections.  
It is worth noting that cases sometimes occurred 
where a parsing error did not cause any problem 
in the revision. We found that the phrases gov-
erned by a trigger pair in many cases were quite 
similar, and therefore, the parser makes the same 
error. In that case, the errors are often offset and 
cause no problems superficially. 
We consider that the sentential completeness 
needs further improvements to make an auto-
matic summarization system, although the se-
mantic correctness and concreteness increase are 
at an almost satisfactory level. Our dependency-
based revision is expected to be potentially use-
ful to develop a summarization system. 
                                                 
6 In this section, the ?significance? was tested with the 
Mann-Whitney U test with Fisher?s exact probability. We 
set the significance level at 5%.  
6 Future work  
Several problems remain to be solved, which will 
be addressed in future work. Obviously, we need 
to improve the parsing accuracy that degraded 
the sentential completeness in our experiments. 
Although we did not quantitatively evaluate the 
errors in phrase insertion position and redun-
dancy, we could see these happening in the re-
vised sentences because of the inaccurate parsing. 
Apart from this, we need to further refine the 
following problems.  
Regarding the trigger selection, one particular 
problem we faced was the mixture of statements 
of different politicians in a news article. The 
statements were often included as direct quota-
tions that end with the chunk ????? nobe-
mashi-ta ?said.? Our system takes the chunk as 
the trigger and does not care whose statements 
they are; thus, it ended up mixing them up. A 
similar problem happened when we had two dif-
ferent female victims of an incident in an article. 
Since our system has no means to distinguish 
them, the modifier phrases about these women 
were mixed up. 
We think that we can improve our method by 
applying more general language generation tech-
niques. An example is the kara-case collision that 
we explained in example 3 in section 5.1. The 
essence of the problem is that the added content 
is useful, but there is a grammatical problem. In 
other words, ?what to say? is ok but ?how to 
say? needs refinement. This particular problem 
can be solved by doing the case-collision check, 
and by synthesizing the colliding phrases into 
one. These can be better treated in the generation 
framework.  
7 Conclusion 
We proposed a lead sentence revision method 
based on the operations of phrases that have the 
same head in the lead and other sentences.  This 
method is a type of sentence fusion and is more 
general than methods that use noun phrase 
coreferencing in that it can add phrases of any 
syntactic type. We described the algorithm and 
the rules extensively, conducted a lead revision 
experiment, and showed that the algorithm was 
able to find semantically appropriate revisions. 
We also showed that parsing errors mainly de-
grade the sentential completeness such as gram-
maticality and repetition. 
 
46
Reference 
Regina Barzilay and Kathleen R. McKeown. 2005. 
Sentence Fusion for Multidocument News Summa-
rization. Computational Linguistics. 31(3): 298-
327. 
Katja Filippova and Michael Strube. 2008. Sentence 
Fusion via Dependency Graph Compression.  proc. 
of the EMNLP 2008: 177-185 
Hongyan Jing and Kathleen R. McKeown. 1999. The 
Decomposition of Human-Written Summary Sen-
tences. proc. of the 22nd International Conference 
on Research and Development in Information Re-
trieval  SIGIR 99: 129-136. 
Hongyan Jing and Kathleen R. McKeown. 2000. Cut 
and Paste Based Text Summarization, proc. of the 
1st meeting of the North American Chapter of the 
Association for Computational Linguistics: 178-
185. 
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. Proc. 
of the 6th Conference on Natural Language Learn-
ing 2002: 63-69. 
Taku Kudo, Kaoru Yamamoto and Yuji Matsumoto. 
2004. Applying Conditional Random Fields to Jap-
anese Morphological Analysis, proc. of the 
EMNLP 2004: 230-237. 
H. P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. Advances in Automatic Text Sum-
marization. The MIT Press: 15-21. 
Inderjeet Mani, Barbara Gates, and Eric Bloedorn.  
1999. Improving Summaries by Revising Them. 
Proc. of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.: 558-565. 
Ruslan Mitkov 2002, Anaphora Resolution, Pearson 
Education. 
Ani Nenkova. 2008. Entity-driven Rewrite for Multi-
document Summarization, proc. of the 3rd Interna-
tional Joint Conference on Natural Language Gen-
eration: 118-125. 
Jahna C. Otterbacher, Dragomir R. Radev, and Airong 
Luo 2002, Revisions that Improve Cohesion in 
Multi-document Summaries: A Preliminary Study. 
Proc. of the ACL-02 Workshop on Automatic 
Summarization: 27-36. 
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision-
based model for summary generation. Artificial In-
telligence.  85: 135-179. 
 
47
