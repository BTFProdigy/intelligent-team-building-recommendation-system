Learning Chinese Bracketing Knowledge Based on  
a Bilingual Language Model 
Yajuan L?, Sheng Li, Tiejun Zhao, Muyun Yang  
School of Computer Science & Engineering, Harbin Institute of Technology 
Harbin, China, 150001 
Email: {lyj,lish,tjzhao,ymy}@mtlab.hit.edu.cn 
 
Abstract  
This paper proposes a new method for 
automatic acquisition of Chinese bracketing 
knowledge from English-Chinese sentence- 
aligned bilingual corpora. Bilingual sentence 
pairs are first aligned in syntactic structure by 
combining English parse trees with a 
statistical bilingual language model. Chinese 
bracketing knowledge is then extracted 
automatically. The preliminary experiments 
show automatically learned knowledge 
accords well with manually annotated 
brackets. The proposed method is 
particularly useful to acquire bracketing 
knowledge for a less studied language that 
lacks tools and resources found in a second 
language more studied. Although this paper 
discusses experiments with Chinese and 
English, the method is also applicable to 
other language pairs. 
Introduction 
The past few years have seen a great success in 
automatic acquisition of monolingual parsing 
knowledge and grammars. The availability of 
large tagged and syntactically bracketed corpora, 
such as Penn Tree bank, makes it possible to 
extract syntactic structure and grammar rules 
automatically (Marcus 1993). Substantial 
improvements have been made to parse western 
language such as English, and many powerful 
models have been proposed (Brill 1993, Collins 
1997). However, very limited progress has been 
achieved in Chinese. 
      Knowledge acquisition is a bottleneck for 
real appication of Chinese parsing. While some 
methods have been proposed to learn syntactic 
knowledge from annotated Chinese corpus, most 
of the methods depended on the annotated or 
partial annotated data(Zhou 1997, Streiter 2000). 
Due to the limited availbility of Chinese 
annotated corpus, tests of these methods are still 
small in scale. Although some institutions and 
universities currently are engaged in building 
Chinese tree bank, no large scale annotated 
corpus has been published until now because the 
complexity in Chinese syntatic sturcture and the 
difficulty in corpus annotation (Chen 1996).  
This paper proposes a novel method to 
facilitate the Chinese tree bank construction. 
Based on English-Chinese bilingual corpora and 
better English parsing, this method obtains 
Chinese bracketing information automatically via 
a bilingual model and word alignment results. 
The main idea of the method is that we may 
acquire knowledge for a language lacking a rich 
collection of resources and tools from a second 
language that is full of them.  
The rest of this paper is organized as 
follows : In the next section, a bilingual language 
model is introduced. Then, a bilingual parsing 
method supervised by English parsing is 
proposed in section 2. Based on the bilingual 
parsing, Chinese bracketing knowlege is 
extracted in section 3. The evaluation and 
discussion are given in section 4. We conclude 
with discussion of future work. 
1 A bilingual language model ? ITG 
Wu (1997) has proposed a bilingual language 
model called Inversion Transduction Grammar 
(ITG), which can be used to parse bilingual 
sentence pairs simultaneously. We will give a 
brief description here. For details please refer to 
(Wu 1995, Wu 1997).  
The Inversion Transduction Grammar is a 
bilingual context-free grammar that generates 
two matched output languages (referred to as L1 
and L2). It also differs from standard context-free 
grammars in that the ITG allows right-hand side 
production in two directions: straight or inverted. 
The following examples are two ITG 
productions: 
C -> [A B] 
C -> <A B> 
Each nonterminal symbol stands for a pair of 
matched strings. For example, the nonterminal A 
stands for the string-pair (A1, A2). A1 is a 
sub-string in L1, and A2 is A1?s corresponding 
translation in L2. Similarly, (B1, B2) denotes the 
string-pair generated by B. The operator [ ] 
performs the usual concatenation, so that C -> [A 
B] yields the string-pair (C1, C2), where C1=A1B1 
and C2=A2B2. On the other hand, the operator <> 
performs the straight concatenation for language 
1 but the reversing concatenation for language 2, 
so that C -> <A B> yields C1=A1B1, but C2=B2A2. 
The inverted concatenation operator permits the 
extra flexibility needed to accommodate many 
kinds of word-order variation between source 
and target languages (Wu 1995). 
There are also lexical productions of the 
following form in ITG: 
A -> x/y 
This means that a symbol x in language L1 is 
translated by the symbol y in language L2.  x or y 
may be a null symbol e, which means there may 
be no counterpart string on other side of the 
bitext.  
ITG based parsing matches constituents for 
an input sentence-pair. For example, Figure 1 
shows an ITG parsing tree for an 
English-Chinese sentence-pair. The inverted 
production is indicated by a horizontal line in the 
parsing tree. The English text is read in the usual 
depth-first left to right order, but for the Chinese 
text, a horizontal line means the right sub-tree is 
traversed before the left. The generated parsing 
results are: 
(1) [[[Mr. Wu]BNP [[plays basketball]VP [on 
Sunday ]PP ]VP ]S . ]S  
(2) [[[ ] [ [ 	]]] 
] 
We can also represent the common structure 
of the two sentences more clearly and compactly 
with the aid of <> notation: 
(3)  [[<Mr./ Wu/>BNP < [plays/ basketball/	]VP 
[on/e Sunday/]PP >VP ]S ./
]S 
where the horizontal line from Figure 1 
corresponds to the <> level of bracketing. 
. 
S 
BNP 
VP 
PP 
VP 
Mr./ Wu/ 
plays/ basketball/

 
on/e Sunday/	 
S 
./ 
Figure 1  Inversion transduction Grammar parsing
      Any ITG can be converted to a normal form, 
where all productions are either lexical 
productions or binary-fanout nonterminal 
productions(Wu 1997). If probability is 
associated with each production, the ITG is 
called the Stochastic Inversion Transduction 
Grammar (SITG). 
2 English parsing supervised bilingual 
bracketing 
Because of the difficulty in finding a suitable 
bilingual syntactic grammar for Chinese and 
English, a practical ITG is the generic Bracketing 
Inversion Transduction Grammar (BTG)(Wu 
1995). BTG is a simplified ITG that has only one 
nonterminal and does not use any syntactic 
grammar. A Statistical BTG (SBTG) grammar is 
as follows: 
j
b
i
b
ji
baa
veAeuA
vuAAAAAAA
ejie
ij
/    ;/ 
   ; /    ;    ];[ 
??????
???><??????
       SBTG employs only one nonterminal 
symbol A that can be used recursively. Here, ?a? 
denotes the probability of syntactic rules. 
However, since those constituent categories are 
not differentiated in BTG, it has no practical 
effect here and can be set to an arbitrary constant. 
The remaining productions are all lexical. bij is 
the translation probability that source word ui 
translates into target word vj. bij can be obtained 
using a statistical word-translation model 
(Melamed 2000) or word alignment(L? 2001a). 
The last two productions denote that the word in 
one language has no counterpart on other side of 
the bitext. A small constant can be chosen for the 
probabilities bie and bej.   
In BTG, no language specific syntactic 
grammar is used. The maximum-likelihood 
parser selects the parse tree that best satisfies the 
combined lexical translation preferences, as 
expressed by the bij probabilities. Because the 
expressiveness characteristics of ITG naturally 
constrain the space of possible matching in a 
highly appropriate fashion, BTG achieves 
encouraging results for bilingual bracketing 
using a word-translation lexicon alone (Wu 
1997). 
Since no syntactic knowledge is used in 
SBTG, output grammaticality can not be 
guaranteed. In particular, if the corresponding 
constituents appear in the same order in both 
languages, both straight and inverted, then lexical 
matching does not provide the discriminative 
leverage needed to identify the sub-constituent 
boundaries. For example, consider an 
English-Chinese sentence pair: 
(4) English: That old teacher is our adviser. 
Chinese: 	
 
Using SBTG, the bilingual bracketing result is : 
(5) [[[[[[The/ old/] teacher/] is/] our/	] 
adviser/
] ./] 
The result is not consistent with the 
expected syntactic structure. In this case, 
grammatical information about one or both of the 
languages can be very helpful. For example, if we 
know the English parsing result shown in (6), 
then the bilingual bracketing can be determined 
easily; the result should be (7).  
(6) [[That old teacher]BNP [is [our adviser]BNP ]VP .]S 
(7) [[That/ old/ teacher/] [is/ [our/	 
adviser/
] ] ./] 
From the example, we can see that if one 
language parser is available, the induced 
bilingual bracketing result would be more 
accurate. English parsing methods have been 
well studied and many powerful models have 
been proposed. It will be helpful to make use of 
English parsing results. In the following, we will 
propose a method of bilingual bracketing 
supervised by English parsing.  
Here, English parsing supervised BTG 
means using an English parser?s bracketing 
information as a boundary restriction in the BTG 
language model. But this does not necessitate 
parsing Chinese completely according to the 
same parsing boundary of English. If the English 
parsing structure is totally fixed, it is possible that 
the structure is not linguistically valid for 
Chinese under the formalism of Inversion 
Transduction Grammar. To illustrate this, see the 
example shown in Figure 2.  
If you want to lose weight, you had better eat less bread . 
    	
 
 

 
  
 
eat 
less bread 
VP 
BNP 

   

     

 
 (a) 
VP 
eat/ less/ 
bread/

 
X 
 (b) 
Figure 2  A example of mismatch subtree 
VP 
eat less bread 
 (c) 
 
 
 
        The sub-tree for blacked underlined part of 
English and corresponding Chinese are shown in 
Figure 2(a). We can see that the Chinese 
constituents do not match the English 
counterparts in the English structure. In this case, 
our solution is that: the whole English constituent 
of ?VP? is aligned with the whole Chinese 
correspondence; i.e., ?eat less bread? is matched 
with ?? shown in Figure 2(b). At the 
same time, we give the inner structure matching 
according to ITG regardless of the English 
parsing constraint. An ?X? tag is introduced to 
indicate that the sub-bilingual-parsing-tree is not 
consistent with the given English sub-tree. Our 
result can also be understood as a flattened 
bilingual parsing tree as shown in Figure 2(c). 
This means that when the bilingual constituents 
couldn?t match in the small syntactic structure, 
we will match them in a larger structure. 
        The main idea is that the given English 
parser is only used as a boundary constraint for 
bilingual parsing. When the constraint is 
incompatible with the bilingual model ITG, we 
use ITG as the default result. This process 
enables parsing to go on regardless of some 
failures in matching. 
We heuristically define a constraint function 
Fe(s, t) to denote the English boundary constraint, 
where s is the beginning position and t is the end. 
There are three cases of structure matching: 
violate match, exact match and inside match. 
Violate match means the bilingual parsing 
conflicts with the given English bracketing 
boundary. For example, given the following 
English bracketing result (8), (1,2), (1,3), (2,3), 
(2,4) etc. are Violate matches. We assign a 
minimum Fe(s, t) (0.0001 at present) to prevent 
the structure match from being chosen when an 
alternative match is available. Exact match 
means the match falls exactly on the English 
parsing boundary, and we assign a high Fe(s, t) 
value (10 at present) to emphasize it. (1,6), (2,5), 
(3,5) are examples. (3,4), (4,5) are examples of 
inside match, and the value 1 is assigned to these 
Fe(s, t) functions. 
(8) [She/1 [is/2 [a/3 lovely/4 girl/5] ] ./6]    
Let the input English and Chinese sentences 
be Tee ,...1  and Vcc ,...1 . As an abbreviation we 
write tse ...  for the sequence of words 
tss eee ..., ,21 ++ , and similarly write vuc ... . The local 
optimization function =),,,( vuts?  
]/[max
.... vuts ceP denotes the maximum probability 
of sub-parsing-tree of node q and that both the 
sub-string tse ...  and vuc ...  derive from node q. 
Thus, the best parser has the 
probability ),0,,0( VT? . ),,,( vuts? is calculated as 
the maximum probability combination of all 
possible sub-tree combinations(Wu 1995). To 
insert English parsing constraints in bilingual 
parsing, we integrate the constraint function Fe(s, 
t) into the local optimization function.   
Computation of the local optimization function is 
then modified as given below:  
.),,,(),,,(),(max),,,(
,),,,(),,,(),(max),,,(
,)],,,(),,,,(max[),,,(
0))(())((
0))(())((
[]
[]
UutSvUSstsFvuts
vUtSUuSstsFvuts
vutsvutsvuts
e
UvuUStsS
vUu
tSs
e
UvuUStsS
vUu
tSs
???
???
???
???+??
??
??
<>
???+??
??
??
<>
=
=
=
 
    Initialization is as follows : 
V1,1),/(
V1,1),/(
V1,1),/(
,1,,
,,,1
,1,,1
????=
????=
????=
?
?
??
vTtceb
vTteeb
vTtceb
vvvtt
tvvtt
vtvvtt
?
?
?
     
where, T ,V is the length of English and Chinese 
sentence respectively. )/( vt ceb is the probability 
of translating English word te  into Chinese word 
vc . A minimal probability can be assigned to 
empty word alignment b( eet / ) and b( vce / ). 
The optimal bilingual parsing tree for a 
given sentence-pair can be computed using  
dynamic programming (DP) algorithm(Wu 1997). 
Using the standard SBTG local optimization 
fuction, the obtained bilingual parsing result for 
the given sentence-pair(4) is shown as example 
(5); when using the above modified local 
optimization function, the parsing result is that 
shown as example (7). Comparing the two results, 
we can see that by intergrating English parsing 
constraints into BTG, the bilingual parsing 
becomes more grammatical. Our experiments 
showed that this English parsing supervised BTG 
would improve the accuracy of bilingual 
bracketing by nearly 20% (L? 2001b). 
The obtained bilingual parsing tree is in the 
normal form of ITG, that is each node in the tree 
is either a lexical node or a binary-fanout 
nonterminal node. We can combine the subtree to 
restore the fanout flexibility using the production 
characters [[AA]A]=[A[AA]]=[AAA] and 
<<AA>A>= <A<AA>>=<AAA>. The combining 
operation could not cross the given English 
parisng boundary.  
3 Chinese bracketing knowledge extraction 
Table 1 shows some bilingual bracketing 
examples obtained using the above method. To 
understand easily, we give the tree form of the 
first example in Figure 3(a). The leaf node is the 
aligned words of the two languages and their 
POS tag categories. These POS tags are 
generated from an English and a Chinese POS 
tagger respectively. The English POS tag and 
phrase tag set are the same as those of the Penn 
Tree Bank (Marcus 1993) and the Chinse POS 
tag set please refer to the web site: 
http://mtlab.hit.edu.cn. The nonterminal node are 
labeled using English sub-tree tags. 
Based on the bilingual parsing result, it is 
easy to extract the Chinese bracketing structure 
according to the Inversion Transduction 
Grammar. For the normal node, the Chinese text 
is traversed in depth-first left to right order, but 
for an inverted node (indicated by a horizontal 
line in the parsing tree or indicated by a <> 
notation in bracketing expression), the right 
sub-tree is traversed before the left. Thus, the 
Chinese parsing tree corresponding to Figure 3(a) 
is shown in Figure 3(b). The nonterminal labels 
are derived from the English sub-tree. The 
extracted Chinese bracketing results from Table1  
Table 1  Bilingual bracketing examples 
1. [<Mr.(NNP)/(nc) Chen(NNP)/(nx) >BNP [is (VBZ) /(vx) < [the(ART)/e representative(NN)/(ng)]BNP 
<of (IN) /(usde) [our (PRP$)/	(r) company(NN)/
(ng)]BNP >PP >NP ]VP .(.)/(wj) ]S 
2. [Spring(NN)/(t) [is(VBZ)/(vx) <[the(ART)/e first(JJ)/(m) e/(q) season(NN)/(ng) ]BNP <in(IN)/
(f) [a(ART)/(m) year(NN)/(q) ]BNP >PP >X ]VP .(.)/(wj) ]S 
3. [[The(ART)/e window(NN)/(ng)]BNP [is/e/VBZ <[e/(d) narrower(JJR)/An Automatic Evaluation Method for Localization Oriented 
Lexicalised EBMT System 
 
Jianmin Yao+, Ming Zhou++, Tiejun Zhao+, Hao Yu+, Sheng Li+ 
  +School of Computer Science and Technology
Harbin Institute of Technology,  
Harbin, China, 150001 
{james, tjzhao, yu, shengli}@mtlab.hit.edu.cn
++Natural Language Computing Group 
Microsoft Research Asia 
Beijing, China, 100080 
Mingzhou@microsoft.com 
 
Abstract  
To help developing a localization oriented 
EBMT system, an automatic machine 
translation evaluation method is 
implemented which adopts edit distance, 
cosine correlation and Dice coefficient as 
criteria. Experiment shows that the 
evaluation method distinguishes well 
between ?good? translations and ?bad? ones. 
To prove that the method is consistent with 
human evaluation, 6 MT systems are scored 
and compared. Theoretical analysis is made 
to validate the experimental results. 
Correlation coefficient and significance tests 
at 0.01 level are made to ensure the 
reliability of the results. Linear regression 
equations are calculated to map the 
automatic scoring results to human scorings. 
Introduction 
Machine translation evaluation has always been 
a key and open problem. Various evaluation 
methods exist to answer either of the two 
questions (Bohan 2000): (1) How can you tell if 
a machine translation system is ?good?? And (2) 
How can you tell which of two machine 
translation systems is ?better?? Since manual 
evaluation is time consuming and inconsistent, 
automatic methods are broadly studied and 
implemented using different heuristics. Jones 
(2000) utilises linguistic information such as 
balance of parse trees, N-grams, semantic 
co-occurrence and so on as indicators of 
translation quality. Brew C (1994) compares 
human rankings and automatic measures to 
decide the translation quality, whose criteria 
involve word frequency, POS tagging 
distribution and other text features. Another type 
of evaluation method involves comparison of the 
translation result with human translations. 
Yokoyama (2001) proposed a two-way MT 
based evaluation method, which compares 
output Japanese sentences with the original 
Japanese sentence for the word identification, 
the correctness of the modification, the syntactic 
dependency and the parataxis. Yasuda (2001) 
evaluates the translation output by measuring the 
similarity between the translation output and 
translation answer candidates from a parallel 
corpus. Akiba (2001) uses multiple edit 
distances to automatically rank machine 
translation output by translation examples. 
Another path of machine translation evaluation 
is based on test suites. Yu (1993) designs a test 
suite consisting of sentences with various test 
points. Guessoum (2001) proposes a 
semi-automatic evaluation method of the 
grammatical coverage machine translation 
systems via a database of unfolded grammatical 
structures. Koh (2001) describes their test suite 
constructed on the basis of fine-grained 
classification of linguistic phenomena. 
There are many other valuable reports on 
automatic evaluation. All the evaluation 
methods show the wisdom of authors in their 
utilisation of available tools and resources for 
automatic evaluation tasks. For our 
localization-oriented lexicalised EBMT system 
an automatic evaluation module is implemented. 
Some string similarity criteria are taken as 
heuristics. Experimental results show that this 
method is useful in quality feedback in 
development of the EBMT system. Six machine 
translation systems are utilised to test the 
consistency between the automatic method and 
human evaluation. To avoid stochastic errors, 
significance test and linear correlation are 
calculated. Compared with previous works, ours 
is special in the following ways: 1) It is 
developed for localisation-oriented EBMT, 
which demands higher translation quality. 2) 
Statistical measures are introduced to verify the 
significance of the experiments. Linear 
regression provides a bridge over human and 
automatic scoring for systems. 
The paper is organised as follows: First the 
localization-oriented lexicalised EBMT system 
is introduced as the background of evaluation 
task. Second the automatic evaluation method is 
further described. Both theoretical and 
implementation of the evaluation method are 
fully discussed. Then six systems are evaluated 
both manually and with our automatic method. 
Consistency between the two methods is 
analysed. At last before the conclusion, linear 
correlation and significance test validate the 
result and exclude the possibility of random 
consistency. 
1 EBMT Evaluation Solution 
1.1 EBMT System Setup 
From Figure 1 you can get a general overview of 
our EBMT system. 
 
Input sentence 
Transfer 
<Phrase Alignment>
Translation result 
Resources 
(Bilingual and 
monolingual) 
Example Base 
(Software 
manual) Match 
Recombine 
Figure 1. Flowchart of the EBMT System 
The EBMT system is developed for 
localization purpose, which demands the 
translation to be restricted in style and 
expression. This makes it rational to take string 
similarity as criterion for translation quality 
evaluation. The solution is useful because in 
localization, an example based machine 
translation system helps only if it outputs the 
very high quality translation results. 
1.2 Evaluation Criteria 
The criteria we utilise for evaluation include edit 
distance, dice coefficient and cosine correlation 
between (the vectors or word bag sets of) the 
machine translation and the gold standard 
translation. Followed is a detailed description of 
the three criteria. 
The edit distance between two strings s1 
and s2, is defined as the minimum number of 
operations to become the same 
(Levenshtein1965). It gives an indication of how 
`close' or ?similar? two strings are. Denote the 
length of a sentence s as |s|. A two-dimensional 
matrix, m[0...|s1|,0...|s2|] is used to hold the edit 
distance values. The algorithm is as follows 
(Wagner 1974): 
Step 1 Initialization: 
For i=0 to |s1| 
m[i, 0] = i//initializing the columns  
For j=1 to |s2| 
m[0, j] = j //initializing the rows 
Step 2 Iteration: 
For i=1 to |s1| 
For j=1 to |s2| 
 if(s1[i] = s2[j]) 
 { 
    d=m[i-1,j-1] 
 }//equality 
 else 
 { 
d=m[i-1,j-1]+1 
 }//substring 
 m[i, j]=min(m[i-1,j]+1,m[i,j-1]+1,d) 
End For 
End For 
Step 3: Result: 
Return m[i,j] 
Figure 2. Algorithm for Edit Distance 
The time complexity of this algorithm is 
O(|s1|*|s2|). If s1 and s2 have a `similar' length, 
about `n' say, this complexity is O(n2). 
Taking into account the lengths of 
translations, the edit distance is normalised as 
21
)2,1(d2
tDistancenormal_edi  
ss
ss
+
?=
     (1) 
Cosine correlation between the vectors of 
two sentences is often used to compute the 
similarity in information retrieval between a 
document and a query (Manning 1999). In our 
task, it is a similarity criterion defined as 
follows: 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
    (2) 
Where 
w1i = weight of ith term in vector of sentence 
s1, 
w2i = weight of ith term in vector for sentence 
s2, 
n = number of words in sum vector of s1 and s2. 
The cosine correlation reaches maximum value 
of 1 when the two strings s1 and s2 are the same, 
while if none of the elements co-occurs in both 
vectors, the cosine value will reach its minimum 
of 0. 
Another criterion we utilised is the Dice 
coefficient of element sets of strings s1 and s2, 
21
21
2)2,1(
ss
ss
ssDice +?=
I
   (3) 
The Dice coefficient demonstrates the 
intuitive that good translation tends to have 
more common words with standard than bad 
ones. This is especially true for example based 
machine translation for localization purpose. 
1.3 Relationship Among Similarity Criteria 
In this section we analyse the relationship 
between the criteria so that we have a better 
understanding of the experiment results. 
If weight of all words are 1, i.e. each word has 
the uniform importance to translation quality, 
the cosine value becomes very similar to the 
Dice coefficient criterion. if we assume 
??
?=
                                                    else        0 
rsboth vectoin  occurs ith word  theiff     1 
bi
 
??
?=
                                                    else        0 
s1 ofin vector  occurs ith word  theiff     1 
1ib
 
??
?=
                                                    else        0 
s2 ofin vector  occurs ith word  theiff     1 
2ib
 
then 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
 
?= ?=
?
?== n
i
n
i
ibib
n
i
bi
1 1
2221
1
21
21
1 1
21
1
ss
ss
n
i
n
i
ibib
n
i
bi
?=?= ?=
?
?== I
  
Similar to (3), this is also a calculation of the 
number of words in common The Dice 
coefficient and cosine function have common 
characteristics. Especially when two strings are 
of the same length, we have 
)2,1(
21
21
2
1
21
11
21
21
21
1 1
21
1)2,1cos(
                 ssDice
ss
ss
s
ss
ss
ss
ss
ss
n
i
n
i
ibib
n
i
bi
ss
=+?==
?
=
?
=
?= ?=
?
?==
II
II
 
The above equation holds if and only if |s1| 
== |s2|. The experimental results will clearly 
demonstrate the correspondence between cosine 
correlation and Dice coefficient. The two values 
become more similar as the lengths of the two 
strings draw nearer. They become the same 
when the two sentences are of the same length. 
The (normalized) edit distance evaluation 
has a somewhat different variance from the other 
two values. Edit distance cares not only how 
many words there are in common, but also takes 
into account the factor of word order adjustment. 
For example, take two strings of s1 and s2 
composed of words, 
s1 = w1 w2 w3 w4 
s2 = w1 w3 w2 w4 
Then, 
1
44
4221
21
2)2,1( =+?=+?= ss
ss
ssDice
I
 
1
44
4
1 1
2221
1)2,1cos( =
?
=
?
=
?
=
?
?
==
n
i
n
i
ibib
n
i
bi
ss
 
5.0
44
22
21
)2,1(d2 tDistancenormal_edi
2s2)ce(s1,editDistan
=+
?=+
?=
=
ss
ss
 
Edit distance and the other two criteria have 
their respective good aspects and shortcomings. 
So they can complement each other in the 
evaluation work.  
In the EBMT development, we sort the 
translations by a combination of the three factors, 
i.e. first by Dice coefficient in descending order, 
then by cosine correlation in descending order, 
last by normalized edit distance in ascending 
order. This method makes a simple combination 
of the three factors, while no more complexity 
arises from this combination. 
2 Experiments and Results 
2.1 Experimental Setup 
Our evaluation method is designed to help in 
developing the EBMT system. It is supposed to 
sort the translations by quality. Experiments 
show that it works well sorting the sentences by 
order of it?s being good or bad translations. In 
order to justify the effectiveness of the 
evaluation method, we also design experiments 
to compare the automatic evaluation with human 
evaluation. The result shows good compatibility 
between the automatic and human evaluation 
results. Followed are details of the experimental 
setup and results. 
In order to evaluate the performance of our 
EBMT system, a sample from a bilingual corpus 
of Microsoft Software Manual is taken as the 
standard test set. Denote the source sentences in 
the test set as set S, and the target T. Sentences 
in S are fed into the EBMT system. We denote 
the output translation set as R. Every sentence ti 
in T is compared with the corresponding 
sentence ri in R. Evaluation results are got via 
the functions cosine(ti, ri), Dice(ti, ri), and 
normalized edit distance normal_editDistance(ti, 
ri). As discussed in the previous section, good 
translations tend to have higher values of cosine 
correlation, Dice coefficient and lower edit 
distance. After sorting the translations by these 
values, we will see clearly which sentences are 
translated with high quality and which are not. 
Knowledge engineers can obtain much help 
finding the weakness of the EBMT system. 
Some sample sentences and evaluation 
results are attached in the Appendix. In our 
experience, with Dice as example, the 
translations scored above 0.7 are fairly good 
translations with only some minor faults; those 
between 0.5 and 0.7 are faulty ones with some 
good points; while those scored under 0.4 are 
usually very bad translations. From these 
examples, we can see that the three criteria 
really help sorting the good translation from 
those bad ones. This greatly aids the developers 
to find out the key faults in sentence types and 
grammar points. 
2.2 Comparison with Human Evaluation 
In the above descriptions, we have presented our 
theoretical analysis and experimental results of 
our string similarity based evaluation method. 
The evaluation has gained the following 
achievements: 1) It helps distinguishing ?good? 
translations from ?bad? ones in developing the 
EBMT system; 2) The scores give us a clear 
view of the quality of the translations in 
localization based EBMT. In this section we will 
make a direct comparison between human 
evaluation and our automatic machine 
evaluation to test the effectiveness of the string 
similarity evaluation method. To tackle this 
problem, we carry out another experiment, in 
which human scoring of systems are compared 
with the machine scoring. 
The human scoring is carried out with a test 
suite of High School English. Six undergraduate 
students are asked to score the translations 
independent from each other. The average of 
their scoring is taken as human scoring result. 
The method is similar to ALPAC scoring system. 
We score the translations with a 6-point scale 
system. The best translations are scored 1. If it?s 
not so perfect, with small errors, the translation 
gets a score of 0.8. If a fatal error occurs in the 
translation but it?s still understandable, a point 
of 0.6 is scored. The worst translation gets 0 
Table 1. Human Evaluation of 6 Machine Translation Systems 
System# #1 #2 #3 #4 #5 #6 
Error5 5 5% 1 1% 2 2% 4 4% 9 9% 7 7% 
Error4 4 4% 6 6% 4 4% 7 7% 18 18% 21 21%
Error3 7 7% 14 14% 21 21% 23 23% 23 23% 26 26%
Error2 14 14% 15 14% 21 21% 19 19% 18 18% 17 17%
Error1 15 14% 17 17% 33 32% 16 16% 15 15% 8 8% 
Perfect 57 56% 49 48% 21 21% 33 32% 19 19% 23 23%
Good% 70% 65% 43% 48% 34% 31% 
Score 81 78 69 68 55 54 
point of score. Table 1 shows the manual 
evaluation results for 6 general-purpose machine 
translation systems available to us. In table 1, 
Error5 means the worst translation. Error4 to 
Error1 are better when the numbering becomes 
smaller. A translation is labelled ?Perfect? when 
it?s a translation without any fault in it. 
?Good%? is the sum of percent of ?Error1? and 
?Perfect?. Because ?Error1? translations refer to 
those have small imperfections. ?Score? is the 
weighted sum of scores of the 6 kinds of 
translations. E.g. for machine translation system 
MTS1, the score is calculated as follows: 
811578.0156.014                         
4.072.0405)1(
=?+?+?
+?+?+?=MTSscore
 
In table 2, the human scorings and automatic 
scorings of the 6 machine translation systems are 
listed. The translations of system #1 are taken as 
standard for automatic evaluations, i.e. all 
scorings are made on the basis of the result of 
system #1. In principle this will introduce some 
errors, but we suppose it not so great as to 
invalidate the automatic evaluation result. This 
is also why the scorings of system #1 are 100. 
The last row labele AutoAver is the average of 
automatic evaluations. 
Table 2. Scoring of 6 MT Systems 
System# #1 #2 #3 #4 #5 #6 
Human 100 78 69 68 55 54 
Dice 100 70 57 65 48 56 
Cosine 100 75 64 72 55 63 
Edistance 100 78 69 75 63 68 
AutoAver 100 74 63 71 55 62 
Figure 3 presents the scorings of Dice 
coefficient, cosine correlation, edit distance and 
the average of the three automatic criterions in a 
chart, we can clearly see the consistency among 
these parameters. 
4 0
5 0
6 0
7 0
8 0
9 0
1 0 0
1 2 3 4 5 6
D i c e C o s i n e
E d i t D A u t o A v e r
 
Figure 3. Automatic Scoring of 6 MT Systems 
In Figure 3, the numbers on X-axis are the 
numbering of machine translation systems, 
while the Y-axis denotes the evaluation scores. 
40
50
60
70
80
90
100
1 2 3 4 5 6
Human Automatic
 
Figure 4. Scoring of 6 MT Systems 
The human and automatic average scoring 
is shown in Figure 4. The Automatic data refers 
to the average of Dice, cosine correlation and 
edit distance scorings. On the whole, human and 
automatic evaluations tend to present similar 
scores for a specific system, e.g. 78/74 for 
system #2, while 69/63 for system #3. 
3 Result Analysis 
The experimental results and the charts have 
shown some intuitionistic relationship among 
the automatic criteria of Dice coefficient, cosine 
value, edit distance and the human evaluation 
result. A more solid analysis is made in this 
section to verify this relationship. Statistical 
analysis is a useful tool to 1) find the 
relationship between data sets and 2) decide 
whether the relationship is significant enough or 
just for random errors.  
The measure of linear correlation is a way 
of assessing the degree to which a linear 
relationship between two variables is implied by 
observed data. The correlation coefficient 
between variable X and Y is defined as 
YXss
YXCOVYXr ),(),( =
   (7) 
where 
COV(X,Y) is the covariance defined by 
? ???= ))((11),( YYXXnYXCOV ii  (8) 
The symbol meanings are as follows: 
sX: sample standard deviation of variable X 
sY: sample standard deviation of variable Y 
n: sample size 
Xi (Yi) : the ith component of variable X (Y) 
X (Y ): the sample mean of variable X (Y) 
From its definition, we know that the correlation 
coefficient is scale-independent and 11 ??? r . 
After we get the correlation coefficient r, a 
significance test at the level 01.0=?  is made 
to verify whether the correlation is real or just 
due to random errors. Linear regression is used 
to construct a model that specifies the linear 
relationship between the variables X and Y. A 
scatter diagram and regression line will be 
presented for an intuitionistic view of the 
relationship. The results are presented in the 
graphs below. In the graphs, the human 
evaluation results are placed on the X axis, while 
the automatic results are on the Y axis. 
Correlation coefficient and the linear regression 
equation are shown below the graphs. Taking 
into the sample size and the correlation 
coefficient, the significance level is also 
calculated for the statistical analysis. 
 
Figure 5. Human (X) and AutoAver (Y) 
Y=8.0+0.89X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 6. Human (X) and Dice (Y) 
Y=6.9+1.03X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 7. Human (X) and Cosine (Y) 
Y=9.3+0.88X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 8. Human (X) and Edistance (Y) 
Y=23.3+0.74X, P < 0.01 
r = 0.95, P < 0.01 
It is a property of r that it has a value 
domain of [-1,+1]. A positive r implies that the 
X and Y tend to increase/decrease together. A 
minus r implies a tendency for Y to decrease as 
X increases and vice versa. When there is no 
particular relation between X and Y, r tends to 
have a value close to zero. From the above 
analysis, we can see that the Dice coefficient, 
cosine, and average of the automatic values are 
highly correlated with the human evaluation 
results with r=0.96. P < 0.01 shows the two 
variables are strongly correlated with a 
significance level beyond the 99%. While P < 
0.01 for the linear regression equation has the 
same meaning. 
Conclusion 
Our evaluation method is designed for the 
localization oriented EBMT system. This is why 
we take string similarity criteria as basis of the 
evaluation. In our approach, we take edit 
distance, dice coefficient and cosine correlation 
between the machine translation results and the 
standard translation as evaluation criteria. A 
theoretical analysis is first made so that we can 
know clearly the goodness and shortcomings of 
the three factors. The evaluation has been used 
in our development to distinguish bad 
translations from good ones. Significance test at 
0.01 level is made to ensure the reliability of the 
results. Linear regression and correlation 
coefficient are calculated to map the automatic 
scoring results to human scorings. 
Acknowledgements 
This work was done while the author visited 
Microsoft Research Asia. Our thanks go to Wei 
Wang, Jinxia Huang, and Professor Changning 
Huang at Microsoft Research Asia and Jing 
Zhang, Wujiu Huang at Harbin Institute of 
Technology. Their help has contributed much to 
this paper. 
References  
A. Guessoum, R. Zantout, Semi-Automatic 
Evaluation of the Grammatical Coverage of 
Machine Translation Systems, MT Summit? 
conference, Santiago de Compostela, 2001 
Brew C, Thompson H.S, Automatic Evaluation of 
Computer Generated Text: A Progress Report on 
the TextEval Project, Proceedings of the Human 
Language Technology Workshop, 108-113, 1994. 
Christopher D. Manning, Hinrich Schutze, 
Foundations of Statistical Natural Language 
Processing, the MIT Press, 1999, 530-572 
Douglas A. Jones, Gregory M. Rusk, 2000, Toward a 
Scoring Function for Quality-Driven Machine 
Translation, Proceedings of COLING-2000. 
Keiji Yasuda, Fumiaki Sugaya, etc, An Automatic 
Evaluation Method of Translation Quality Using 
Translation Answer Candidates Queried from a 
Parallel Corpus, MT Summit? conference, Santiago 
de Compostela, 2001 
Language and Machines. Computers in Translation 
and Linguistics, (ALPAC report, 1966). National 
Academy of Sciences, 1966 
Niamh Bohan, Elisabeth Breidt, Martin Volk, 2000, 
Evaluating Translation Quality as Input to Product 
Development, 2nd International Conference on 
Language Resources and Evaluation, Athens, 2000. 
Shoichi Yokoyama, Hideki Kashioka, etc., An 
Automatic Evaluation Method for Machine 
Translation using Two-way MT, 8th MT Summit 
conference, Santiago de Compostela, 2001 
Sungryong Koh, Jinee Maeng, etc, A Test Suite for 
Evaluation of English-to-Korean Machine 
Translation Systems, MT Summit? conference, 
Santiago de Compostela, 2001 
Shiwen Yu, Automatic Evaluation of Quality for 
Machine Translation Systems, Machine Translation, 
8: 117-126, 1993, Kluwer Academic Publishers, 
printed in the Netherlands. 
Wagner A.R.  and Fischer M., The string-to-stirng 
correction problem, Journal of the ACM, Vol. 21, 
No. 1, 168-173 
V.I. Levenshtein, Binary codes capable of correcting 
deletions, insertions and reversals. Doklady 
Akademii Nauk SSSR 163(4) 845-848, 1965 
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita, 
Using Multiple Edit Distances to Automatically 
Rank Machine Translation Output, MT Summit? 
conference, Santiago de Compostela, 2001 
Appendix: Automatic Evaluation Results 
cosine      Dice  edistance* `  standard translation&EBMT translation 
0.27273     0.27273      44/6=7     ???????MAPI?? 
                   ????extendedmapi? 
0.43301     0.42857     28/6=4     ???????? 
                             ??mail?? 
0.53452     0.53333 30/7=4      ???????? 
                               ??role??? 
0.62994     0.625     32/4=8      ????????? 
                               ??????? 
0.7     0.7      80/16=5      ???????????????????? 
                               ???????????????????? 
0.72058     0.72      50/11=4      ???????????? 
                               ????????????? 
0.78335     0.78261 46/3=15      ???????????? 
                         ??????????? 
0.81786     0.81633 98/20=4      ?????????????????????????? 
                             ??????????????????????? 
0.8528     0.84211 76/12=6      ?????????????????????? 
                               ???????????????? 
0.86772     0.86486 37/2=18      ?????????? 
                               ????????: 
0.875      0.875  32/1=32   ???????? 
                               ???????? 
0.90889     0.90476 42/2=21      ??????????... 
                               ????????... 
 
*Notes: The data presented in ?edistance? is the reciprocal of the normalized edit distance: the numerator is |s1 + s2| in bytes ; the 
denominator is the edit distance in Chinese characters or English words. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1097?1104
Manchester, August 2008
Grammar Comparison Study for Translational Equivalence 
Modeling and Statistical Machine Translation 
Min Zhang1,  Hongfei Jiang2,  Haizhou Li1,  Aiti Aw1  and  Sheng Li2 
1Institute for Infocomm Research, Singapore 
2Harbin Institute of Technology, China 
{mzhang, hli, aaiti}@i2r.a-star.edu.sg 
{hfjiang, lisheng}@mtlab.hit.edu.cn 
 
Abstract 
This paper presents a general platform, 
namely synchronous tree sequence sub-
stitution grammar (STSSG), for the 
grammar comparison study in Transla-
tional Equivalence Modeling (TEM) and 
Statistical Machine Translation (SMT). 
Under the STSSG platform, we compare 
the expressive abilities of various gram-
mars through synchronous parsing and a 
real translation platform on a variety of 
Chinese-English bilingual corpora. Ex-
perimental results show that the STSSG 
is able to better explain the data in paral-
lel corpora than other grammars. Our 
study further finds that the complexity of 
structure divergence is much higher than 
suggested in literature, which imposes a 
big challenge to syntactic transformation-
based SMT. 
1 Introduction 
Translational equivalence is a mathematical rela-
tion that holds between linguistic expressions 
with the same meaning (Wellington et al, 2006).  
The common explicit representations of this rela-
tion are word alignments, phrase alignments and 
structure alignments between bilingual sentences. 
Translational Equivalence Modeling (TEM) is a 
process to describe and build these alignments 
using mathematical models. Thus, the study of 
TEM is highly relevant to Statistical Machine 
Translation (SMT). 
Grammar is the most important infrastructure 
for TEM and SMT since translation models? ex-
pressive and generative abilities are mainly de-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
termined by the grammar. Many grammars, such 
as finite-state grammars (FSG), bracket/inversion 
transduction grammars (BTG/ITG) (Wu, 1997), 
context-free grammar (CFG), tree substitution 
grammar (TSG) (Comon et al, 2007) and their 
synchronous versions, have been explored in 
SMT. Based on these grammars, a great number 
of SMT models have been recently proposed, 
including string-to-string model (Synchronous 
FSG) (Brown et al, 1993; Koehn et al, 2003), 
tree-to-string model (TSG-string) (Huang et al, 
2006; Liu et al, 2006; Liu et al, 2007), string-to-
tree model (string-CFG/TSG) (Yamada and 
Knight, 2001; Galley et al, 2006; Marcu et al, 
2006), tree-to-tree model (Synchronous 
CFG/TSG, Data-Oriented Translation) (Chiang, 
2005; Cowan et al, 2006; Eisner, 2003; Ding and 
Palmer, 2005; Zhang et al, 2007; Bod, 2007; 
Quirk wt al., 2005; Poutsma, 2000; Hearne and 
Way, 2003) and so on. 
Although many achievements have been ob-
tained by these advances, it is still unclear which 
of these important pursuits is able to best explain 
human translation data, as each has its advan-
tages and disadvantages. Therefore, it has great 
meaning in both theory and practice to do com-
parison studies among these grammars and SMT 
models to see which of them are capable of better 
describing parallel translation data. This is a fun-
damental issue worth exploring in multilingual 
information processing. However, little effort in 
previous work has been put in this point. To ad-
dress this issue, in this paper we define a general 
platform, namely synchronous tree sequence 
substitution grammar (STSSG), for the compari-
son studies. The STSSG can be seen as a gener-
alization of Synchronous TSG (STSG) by replac-
ing elementary tree (a single subtree used in 
STSG) with contiguous tree sequence as the ba-
sic translation unit. As a result, most of previous 
grammars used in SMT can be interpreted as the 
reduced versions of the STSSG. Under the 
STSSG platform, we compare the expressive 
1097
abilities of various grammars and translation 
models through linguistically-based synchronous 
parsing and a real translation platform. By syn-
chronous parsing, we aim to study which gram-
mar can well explain translation data (i.e. transla-
tional equivalence alignment) while by the real 
translation platform, we expect to investigate 
which model can achieve better translation per-
formance. In addition, we also measure the im-
pact of various factors in this study, including the 
genera of corpora (newspaper domain via spoken 
domain), the accuracy of word alignments and 
syntax parsing (automatically vs. manually).  
We report our experimental settings, experi-
mental results and our findings in detail in the 
rest of the paper, which is organized as follows: 
Section 2 reviews previous work. Section 3 
elaborates the general framework while Section 4 
reports the experimental results. Finally, we con-
clude our work in Section 5. 
2 Previous Work 
There are only a few of previous work related to 
the study of translation grammar comparison. 
Fox (2002) is the first to look at how well pro-
posed translation models fit actual translation 
data empirically. She examined the issue of 
phrasal cohesion between English and French 
and discovered that while there is less cohesion 
than one might desire, there is still a large 
amount of regularity in the constructions where 
breakdowns occur. This suggests that reordering 
words by phrasal movement is a reasonable strat-
egy (Fox, 2002). She has also examined the dif-
ferences in cohesion between Treebank-style 
parse trees, trees with flattened verb phrases, and 
dependency structures. Their experimental re-
sults indicate that the highest degree of cohesion 
is present in dependency structures. 
Motivated by the same problem raised by Fox 
(2002), Galley et al (2004) study what rule can 
better explain human translation data. They first 
propose a theory that gives formal semantics to 
word-level alignments defined over parallel cor-
pora, and then use the theory to introduce a linear 
algorithm that is used to derive from word-
aligned, parallel corpora the minimal set of syn-
tactically motivated transformation rules to ex-
plain human translation data. Their basic idea is 
to create transformation rules that condition on 
larger fragments of tree structure. Their experi-
mental results suggest that their proposed rules 
provide a good, realistic indicator of the com-
plexities inherent in translation than SCFG. 
Wellington et al (2006) describes their study 
of the patterns of translational equivalence exhib-
ited by a variety of bilingual/monolingual bitexts. 
They empirically measure the lower bounds on 
alignment failure rates with and without gaps 
under the constraints of word alignment alone or 
with one or both side parse trees. Their study 
finds surprisingly many examples of translational 
equivalence that could not be analyzed using bi-
nary-branching structures without discontinuities. 
Thus, they claim that the complexity of these 
patterns in every bitext is higher than suggested 
in the literature. In addition, they suggest that the 
low coverage rates without gaps under the con-
straints of independently generated monolingual 
parse trees might be the main reason why ?syn-
tactic? constraints have not yet increased the ac-
curacy of SMT systems. However, they find that 
simply allowing a single gap in bilingual phrases 
or other types of constituent can improve cover-
age dramatically. 
DeNeefe et al (2007) compares the strengths 
and weaknesses of a syntax-based MT model 
with a phrase-based MT model from the view-
points of translational equivalence extraction 
methods and coverage. They find that there are 
surprising differences in phrasal coverage ? nei-
ther is merely a superset of the other. They also 
investigate the reason why some phrase pairs are 
not learned by the syntax-based model. They fur-
ther propose several solutions and evaluate on 
the syntax-based extraction techniques in light of 
phrase pairs captured and translation accuracy. 
Finally, significant performance improvement is 
reported using their solutions. 
Different from previous work discussed above, 
this paper mainly focuses on the expressive abil-
ity comparison studies among different gram-
mars and models through synchronous parsing 
and a real SMT platform. Fox (2002), Galley et 
al (2004) and Wellington et al (2006) examine 
TEM only. DeNeefe et al (2007) only compares 
the strengths and weaknesses of a syntax-based 
MT model with a phrase-based MT model. 
3 The General Platform: the STSSG 
In this section, we first define the STSSG plat-
form in Subsection 3.1, and then explain why it 
is a general framework that can cover most of 
previous syntax-based translation grammars and 
models in Subsection 3.2. In Subsection 3.3 and 
3.4, we discuss the STSSG-based SMT and syn-
chronous parsing, which are used to compare 
different grammars and translation models. 
1098
1( )
IT e
1( )
JT f
A
 
 
Figure 1.  A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation 
 
 
 
Figure 2. Two examples of translation rules 
3.1 Definition of the STSSG 
The STSSG is an extension of the STSG by us-
ing tree sequences (rather than elementary trees) 
as the basic translation unit. A STSSG is a septet 
, , , , ,,t t ts s sG N N S S P? ?=< > , where: 
z s?  and t?  are source and target terminal 
alphabets (POSs or lexical words), respec-
tively, and 
z sN  and tN are source and target non-
terminal alphabets (linguistic phrase tag, i.e. 
NP/VP?), respectively, and 
z s sS N?  and t tS N?  are the source and tar-
get start symbols (roots of source and target 
parse trees), and 
z P is a production rule set. 
A grammar rule ir  in the STSSG is an aligned 
tree sequence pair, < s? , t? , A  >, where s? and 
t?  are tree sequences of source side and target 
sides, respectively, and A is the alignments be-
tween leaf nodes of two tree sequences. Here, the 
key concept of ?tree sequence? refers to an or-
dered subtree sequence covering a consecutive 
tree fragment in a complete parse tree. The leaf 
nodes of a subtree in a tree sequence can be ei-
ther non-terminal symbols or terminal symbols. 
Fig. 2 shows two STSSG rules extracted from 
the aligned tree pair shown in Fig. 1, where 1r is 
also a STSG rule.  
In the STSSG, a translational equivalence is 
modeled as a tree sequence pair while MT is 
viewed as a tree sequence substitution process. 
From the definition of ?tree sequence?, we can 
see that a subtree in a tree sequence is a so-called 
elementary tree used in TSG. This suggests that 
SCFG and STSG are only a subset of STSSG 
and SCFG is a subset of STSG. The next subsec-
tion discusses how to configure the STSSG to 
implement the other two simplified grammars. 
This is the reason why we call the STSSG a gen-
eral framework for synchronous grammar-based 
translation modeling. 
It is worth noting that, from rule rewriting 
viewpoint, STSSG can be thought of as a re-
stricted version of synchronous multi-component 
TAGs (Schuler et al, 2000) although TAG is 
more powerful than TSG due to the additional 
operation ?adjunctions?. The synchronous multi-
component TAG can also rewrite several non-
terminals in one step of derivation. The differ-
ence between them is that the rewriting sites (i.e. 
the substitution nodes) must be contiguous in 
STSSG. In addition, STSSG is also related to 
tree automata (Comon et al, 2007). However, the 
discussion on the theoretical relation and com-
parison between them is out of the scope of the 
paper. In this paper, we focus on the comparison 
study of SMT grammars using the STSSG plat-
form. 
3.2 Rule Extraction and Grammar Con-
figuration 
All the STSSG mapping rules are extracted from 
bi-parsed trees. Our rule extraction algorithm is 
an extension of that presented at (Chiang, 2005; 
Liu et al, 2006; Zhang et al, 2007). We modify 
their tree-to-tree/string rule extraction algorithms 
to extract tree-sequence-to-tree-sequence rules. 
Our rules2 are extracted in two steps: 
                                                 
2  We classify the rules into two categories: initial 
rules, whose leaf nodes must be terminals, and ab-
1099
1) Extracting initial rules from bi-parsed trees. 
This is rather straightforward. We first generate 
all fully lexicalized source and target tree se-
quences (whose leaf nodes must be lexical words) 
using a DP algorithm and then iterate over all 
generated source and target sequence pairs. If 
their word alignments are all within the scope of 
the current tree sequence pair, then the current 
tree sequence pair is an initial rule. 
2) Extracting abstract rules from the extracted 
initial rules. The idea behind is that we generate 
an abstract rule from a ?big? initial rule by re-
moving one or more ?small? initial rules from 
the ?big? one, where the ?small? ones must be a 
sub-graph of the ?big? one. Please refer to 
(Chiang, 2005; Liu et al, 2006; Zhang et al, 
2007) for the implementation details. 
As indicated before (Chiang, 2005; Zhang et 
al., 2007), the above scheme generates a very 
large number of rules, which not only makes the 
system too complicated but also introduces too 
many undesirable ambiguities. To control the 
overall model complexity, we introduce the fol-
lowing parameters: 
1) The maximal numbers of trees in the source 
and target tree sequences: s? and t? . 
2) The maximal tree heights in the source and 
target tree sequences: s? and t? . 
3) The maximal numbers of non-terminal leaf 
nodes in the source and target tree sequences: 
s? and t? . 
Now let us see how to implement other mod-
els in relation to STSSG based the STSSG 
through configuring the above parameters. 
1) STSG-based tree-to-tree model (Zhang et 
al., 2007; Bod, 2007) when s? = t? =1. 
2) SCFG-based tree-to-tree model when s? = 
t? =1 and s? = t? =2. 
3) Phrase-based translation model only (no re-
ordering model) when s? = t? =0 and s? = t? =1. 
4) TSG-CFG-based tree-to-string model (Liu 
et al, 2006) when s? = t? =1, t? =2 and ignore 
phrase tags in target side.  
5) CFG-TSG-based string-to-tree model (Gal-
ley et al, 2006) when s? = t? =1and s? =2. 
6) TSSG-CFG-based tree-sequence-to-string 
model (Liu et al, 2007) when t? =2 and ignore 
phrase tags in target side. 
                                                                          
stract rule that having at least one non-terminal leaf 
node. 
From the above definitions, we can see that all 
of previous related models/grammars can be can 
be interpreted as the reduced versions of the 
STSSG. This is the reason why we use the 
STSSG as a general platform for our model and 
grammar comparison studies. 
3.3 Model Training and Decoder for SMT 
We use the tree sequence mapping rules to model 
the translation process. Given the source parse 
tree 1( )
JT f , there are multiple derivations3 that 
could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is ob-
tained by summing over the probabilities of all 
derivations. The probability of each derivation?  
is given by the product of the probabilities of all 
the rules ( )ip r  used in the derivation (here we 
assume that a rule is applied independently in a 
derivation). 
1 1 1 1( | ) ( ( ) | ( ))
                  = ( )
i
I J I J
i
r
r rP e f P T e T f
p r
? ??
=
??           (1) 
The model is implemented under log-linear 
framework. We use seven basic features that are 
analogous to the commonly used features in 
phrase-based systems (Koehn, 2004): 1) bidirec-
tional rule mapping probabilities; 2) bidirectional 
lexical translation probabilities; 3) the target lan-
guage model; 4) the number of rules used and 5) 
the number of target words. Besides, we define 
two new features: 1) the number of lexical words 
in a rule to control the model?s preference for 
lexicalized rules over un-lexicalized rules and 2) 
the average tree height in a rule to balance the 
usage of hierarchical rules and more flat rules. 
The overall training process is similar to the 
process in the phrase-based system (koehn et al, 
2007): word alignment, rule extraction, feature 
extraction and probability calculation and feature 
weight tuning. 
Given 1( )
JT f , the decoder is to find the best 
derivation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (2) 
By default, same as other SMT decoder, here 
we use Viterbi derivation in Eq (2) instead of the 
                                                 
3 A derivation is a sequence of tree sequence rules that 
maps a source parse tree to its target one. 
1100
summing probabilities in Eq (3). This is to make 
the decoder speed not too slow. The decoder is a 
standard span-based chart parser together with a 
function for mapping the source derivations to 
the target ones. To speed up the decoder, we util-
ize several thresholds to limit the search beams 
for each span, such as the number of rules used 
and the number of hypotheses generated. 
3.4 Synchronous Parsing   
A synchronous parser is an algorithm that can 
infer the syntactic structure of each component 
text in a multitext and simultaneously infer the 
correspondence relation between these structures. 
When a parser?s input can have fewer dimen-
sions than the parser?s grammar, we call it a 
translator. When a parser?s grammar can have 
fewer dimensions than the parser?s input, we call 
it a synchronizer (Melamed, 2004). Therefore, 
synchronous parsing and MT are closed to each 
other. In this paper, we use synchronous parsing 
to compare the ability of different grammars in 
translational equivalence modeling.  
Given a bilingual sentence pair 1
Jf and 1
Ie , the 
synchronous parser is to find a derivation ?  that 
generates < 1( )
JT f , 1( )
IT e >. Our synchronous 
parser is similar to the synchronous CKY parser 
presented at (Melamed, 2004). The difference is 
that we implement it based on our STSSG de-
coder. Therefore, in nature the parser is a stan-
dard synchronous chart parser but constrained by 
the rules of the STSSG grammar. In our imple-
mentation, we simply use our decoder to simu-
late the bilingual parser: 1) for each sentence pair, 
we extract one model; 2) we use the model and 
the decoder to translate the source sentence of 
the given sentence pair; 3) if the target sentence 
is successfully generated by the decoder, then we 
say the symphonious parsing is successful. 
Please note that the synchronous parsing is con-
sidered as successful once the last words in the 
source and target sentences are covered by the 
decoder even if there is no a complete target 
parse tree generated (it may be a tree sequence). 
This is because our study only concerns whether 
all translational equivalences are linked together 
by the synchronous parser correctly. 
4 Experiments 
4.1 Experimental Settings 
Synchronous parsing settings: Our experiments 
of synchronous parsing are carried on three Chi-
nese-to-English bilingual corpora: the FBIS cor-
pus, the IWSLT 2007 training set and the HIT 
Corpus. The FBIS data is a collection of trans-
lated newswire documents published by major 
news agencies from three representative loca-
tions: Beijing, Taipei and Hongkong. The 
IWSLT data is a multilingual speech corpus on 
travel domain while the HIT corpus consists of 
example sentences of a Chinese-English diction-
ary. The first two corpora are sentence-aligned 
while the HIT corpus is a manually bi-parsed 
corpus with manually annotated word alignments. 
We use the three corpora to study whether the 
models? expressive abilities are domain depend-
ent and how the performance of word alignment 
and parsing affect the ability of translation mod-
els. We selected 2000 sentence pairs from each 
individual corpus for the comparison study of 
translational equivalence modeling. Table 1 
gives descriptive statistics of the tree data set. 
 
 Chinese English 
FBIS 48,331 59,788 
IWSLT  17,667 18,427 
HIT 18,215 20,266 
 
Table 1. # of words of experimental data 
for synchronous parsing (there are 2k sen-
tence pairs in each individual corpus) 
 
In the synchronous parsing experiments, we 
compared three synchronous grammars: SCFG, 
STSG and STSSG using the STSSG platform. 
We use the same settings except the following 
parameters (please refer to Subsection 3.2 for 
their definitions): s? = t? =1, s? = t? =2 for 
SCFG ; s? = t? =1 and s? = t? =6 for STSG; 
s? = t? = 4 and s? = t? =6 for STSSG. We iter-
ate over each sentence pair in the three corpora 
with the following process: 
1) to used Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences separately,  
this means that our study is based on the Penn 
Treebank style grammar.  
2) to extract SCFG, STSG and STSSG rules 
form each sentence pair, respectively; 
3) to do synchronous parsing using the exacted 
rules.  
Finally, we can calculate the successful rate of 
the synchronous parsing on each corpus. 
SMT evaluation settings: For the SMT ex-
periments, we trained the translation model on 
the FBIS corpus (7.2M (Chinese)+9.2M(English) 
words) and trained a 4-gram language model on 
1101
the Xinhua portion of the English Gigaword cor-
pus (181M words) using the SRILM Toolkits 
(Stolcke, 2002) with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). We used 
these sentences with less than 50 characters from 
the NIST MT-2002 test set as our development 
set and the NIST MT-2005 test set as our test set. 
We used the Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences on the training 
set and Chinese sentences on the development 
and test sets. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002). We 
used GIZA++ and the heuristics ?grow-diag-
final? to generate m-to-n word alignments. For 
the MER training, we modified Koehn?s MER 
trainer (Koehn, 2004) for our STSSG-based sys-
tem. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). We com-
pared four SMT systems: Moses (Koehn et al, 
2007), SCFG-based, STSG-based and STSSG-
based tree-to-tree translation models. For Moses, 
we used its default settings. For the others, we 
implemented them on the STSSG platform by 
adopting the same settings as used in the syn-
chronous parsing. We optimized the decoding 
parameters on the development sets empirically. 
4.2 Experimental Results  
 
 SCFG STSG STSSG 
FBIS 7 (0.35%) 143 (7.15%) 388 (19.4%) 
IWSLT 171 (8.6%) 1179 (58.9%) 1708 (85.4%)
HIT 65 (3.23%) 1133 (56.6%) 1532 (76.6%)
 
Table 2. Successful rates (numbers inside 
bracket) of synchronous parsing over 2,000 
sentence pairs, where the integers outside 
bracket are the numbers of successfully-
parsed sentence pairs 
 
Table 2 reports the experimental results of syn-
chronous parsing. It shows that: 
1) As an extension of STSG/SCFG, STSSG 
outperforms STSG and SCFG consistently in the 
three data sets. The significant difference sug-
gests that the STSSG is much more effective in 
modeling translational equivalences and structure 
divergences. The reason is simply because the 
STSSG uses tree sequences as the basic transla-
tion unit so that it can model non-syntactic 
phrase equivalence with structure information 
and handle structure reordering in a large span.  
2) STSG shows much better performance than 
SCFG. It is mainly due to that STSG allow mul-
tiple level tree nodes operation and reordering in 
a larger span than SCFG. It reconfirms that only 
allowing sibling nodes reordering as done in 
SCFG may be inadequate for translational equiva-
lence modeling (Galley et al, 2004)4.  
3) All the three models on the FBIS corpus 
show much lower performance than that on the 
other two corpora. The main reason, as shown in 
Table 1, is that the sentences in the FBIS corpus 
are much longer than that in the other corpus, so 
their syntactic structures are significantly more 
complicated than the other two. In addition, al-
though tree sequences are utilized, STSSG show 
much lower performance in the FBIS corpus. 
This implies that the complexity of structure di-
vergence between two languages is higher than 
suggested in literature (Fox, 2002; Galley et al, 
2004). Therefore, structure divergence is still a 
big challenge to translational equivalence model-
ing when using syntactic structure mapping. 
4) The HIT corpus does not show better per-
formance than the IWSLT corpus although the 
HIT corpus is manually annotated with parse 
trees and word alignments. In order to study 
whether high performance word alignment and 
parsing results can help synchronous parsing, we 
do several cross validations and report the ex-
perimental results in Table 3. 
 
 Gold Word Alignment 
Automatic 
Word Align-
ment 
 Gold Parse 3.2/56.6/76.6 2.9/57.7/80.9
 Automatic  
Parse 3.2/55.6/76.0 2.9/54.2/78.8
 
Table 3. Successful rates (SCFG/STSG/ 
STSSG)(%) with regards to different word 
alignments and parse trees  on the HIT corpus 
 
Table 3 compares the performance of syn-
chronous parsing on the HIT corpus when using 
gold and automatic parser and word alignment. It 
is surprised that gold word alignments and parse 
trees do not help and even decrease the perform-
ance slightly. Our analysis further finds that 
                                                 
4 This claim is mainly hold for linguistically-informed 
SCFG since formal SCFG and BTG already showed 
much better performance in the formally syntax-based 
translation framework (Chiang, 2005). This is because 
the formal syntax is learned from phrase translational 
equivalences directly without relying on any linguistic 
theory (Chiang, 2005). Thus, it may not suffer from 
the issues of non-isomorphic structure alignment and 
non-syntactic phrase usage heavily (Wellington et al, 
2006). 
1102
more than 90% sentence pairs out of all the sen-
tence pairs that can be successfully bi-parsed are 
in common in the four experiments. This sug-
gests that the STSSG/STSG (SCFG achieves too 
much lower performance) and our rule extraction 
algorithm are robust in dealing with the errors 
introduced by the word alignment and parsing 
programs. If a parser, for example, makes a sys-
tematic error, we expect to learn a rule that can 
nevertheless be systematically used to model cor-
rect translational equivalence. Our error analysis 
on the three corpora shows that most of the fail-
ures of synchronous parsing are due to the struc-
ture divergence (i.e. the nature of non-
isomorphic structure mapping) and the long dis-
tance dependence in the syntactic structures.  
 
 SCFG Moses STSG STSSG
BLEU(%) 22.72 23.86 24.71 26.07 
 
     Table 3. Performance comparison of dif-
ferent grammars on FBIS corpus 
 
Table 3 compares different grammars in terms 
of translation performance. It shows that: 
1) The same as synchronous parsing, the 
STSSG-based model statistically significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empiri-
cally verifies the effect of the tree-sequence-based 
grammar for statistical machine translation.  
2) Both STSSG and STSG outperform Moses 
significantly and STSSG clearly outperforms 
STSG, which suggest that: 
z The linguistically motivated structure fea-
tures are still useful for SMT, which can be cap-
tured by the two syntax-based grammars through 
tree node operations. 
z STSSG is much more effective in utiliz-
ing linguistic structures than STSG since it uses 
tree sequence as the basic translation unit. This 
enables STSSG not only to handle structure reor-
derings by tree node operations in a larger span, 
but also to capture non-syntactic phrases with syn-
tactic information, and hence giving the grammar 
more expressive power. 
3) The linguistic-based SCFG shows much 
lower performance. This is largely because SCFG 
only allows sibling nodes reordering and fails to 
utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure trans-
fer.  
The above two experimental results show that 
STSSG achieves significant improvements over 
the other two grammars in terms of synchronous 
parsing?s successful rate and translation Bleu 
score. 
5 Conclusions 
Grammar is the fundamental infrastructure in 
translational equivalence modeling and statistical 
machine translation since grammar formalizes 
what kind of rule to be learned from a parallel 
text. In this paper, we first present a general plat-
form STSSG and demonstrate that a number of 
synchronous grammars and SMT models can be 
easily implemented based on the platform. We 
then compare the expressive abilities of different 
grammars on the platform using synchronous 
parsing and statistical machine translation. Our 
experimental results show that STSSG can better 
explain the data in parallel corpora than the other 
two synchronous grammars. We further finds 
that, although syntactic structure features are 
helpful in modeling translational equivalence, the 
complexity of structure divergence is much 
higher than suggested in literature, which im-
poses a big challenge to syntactic transformation-
based SMT. This may explain why traditional 
syntactic constraints in SMT do not yield much 
performance improvement over robust phrase-
substitution models. 
The fundamental assumption underlying much 
recent work on syntax-based modeling, which is 
considered to be one of next technology break-
throughs in SMT, is that translational equiva-
lence can be well modeled by structural trans-
formation. However, as discussed in prior arts 
(Galley et al, 2004) and this paper, linguisti-
cally-informed SCFG is an inadequate model for 
parallel corpora due to its nature that only allow-
ing child-node reorderings. Although STSG 
shows much better performance than SCFG, its 
two major limitations are that it only allows 
structure distortion operated on a single sub-tree 
and cannot model non-syntactic phrases. STSSG 
extends STSG by using tree sequence as the ba-
sic translation unit. This gives the grammar much 
more expressive power.  
There are many open issues in the syntactic 
transformation-based SMT due to the divergence 
nature between bilingual structure mappings. We 
find that structural divergences are more serious 
than suggested in the literature (Fox, 2002; Gal-
lery et al, 2004) or what we expected when sen-
tences are longer. We will continue to investigate 
1103
whether and how parallel corpora can be well 
modeled by syntactic structure mappings.   
References 
Rens Bod. 2007. Unsupervised Syntax-Based Ma-
chine Translation: The Contribution of Discon-
tinuous Phrases. MT-Summmit-07. 51-56.  
Peter F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311. 
S. F. Chen and J. Goodman. 1998. An empirical study 
of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University 
Center for Research in Computing Technology. 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05. 263-270. 
H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard, 
D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree 
automata techniques and applications. Available at: 
http://tata.gforge.inria.fr/. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241. 
S. DeNeefe, K. Knight, W. Wang and D. Marcu. 2007. 
What Can Syntax-based MT Learn from Phrase-
based MT? EMNLP-CoNLL-07. 755-763 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. ACL-05. 541-548. 
Bonnie J. Dorr (1994). Machine Translation Diver-
gences: A formal description and proposed solu-
tion. Computational Linguistics, 20(4): 597-633 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Heidi J. Fox. 2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP-2002. 304-311  
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. 
DeNeefe, W. Wang and I. Thayer. 2006. Scalable 
Inference and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
M. Galley, M. Hopkins, K. Knight and D. Marcu. 
2004. What?s in a translation rule? HLT-NAACL. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Mary Hearne and Andy Way. 2003. Seeing the wood 
for the trees: data-oriented translation. MT Sum-
mit IX, 165-172. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. AMTA-04, 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
K. Papineni, Salim Roukos, ToddWard and Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
William Schuler, David Chiang and Mark Dras. 2000. 
Multi-Component TAG and Notions of Formal 
Power. ACL-2000. 448-455 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COL-
ING-ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377-403. 
K. Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
M. Zhang, H. Jiang, A. Aw, J. Sun, S. Li and C. Tan. 
2007. A Tree-to-Tree Alignment-based Model for 
SMT. MT-Summit-07. 535-542. 
Y. Zhang, S. Vogel and A. Waibel. 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04.  
1104
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487?495,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Collocation Extraction Using Monolingual Word Alignment Method 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Toshiba (China) Research and Development Center, Beijing, China 
{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn 
lisheng@hit.edu.cn 
 
  
 
Abstract 
Statistical bilingual word alignment has been 
well studied in the context of machine trans-
lation. This paper adapts the bilingual word 
alignment algorithm to monolingual scenario 
to extract collocations from monolingual cor-
pus. The monolingual corpus is first repli-
cated to generate a parallel corpus, where 
each sentence pair consists of two identical 
sentences in the same language. Then the 
monolingual word alignment algorithm is 
employed to align the potentially collocated 
words in the monolingual sentences. Finally 
the aligned word pairs are ranked according 
to refined alignment probabilities and those 
with higher scores are extracted as colloca-
tions. We conducted experiments using Chi-
nese and English corpora individually. Com-
pared with previous approaches, which use 
association measures to extract collocations 
from the co-occurring word pairs within a 
given window, our method achieves higher 
precision and recall. According to human 
evaluation in terms of precision, our method 
achieves absolute improvements of 27.9% on 
the Chinese corpus and 23.6% on the English 
corpus, respectively. Especially, we can ex-
tract collocations with longer spans, achiev-
ing a high precision of 69% on the long-span 
(>6) Chinese collocations. 
1 Introduction 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). In this pa-
per, a collocation is composed of two words oc-
curring as either a consecutive word sequence or 
an interrupted word sequence in sentences, such 
as "by accident" or "take ? advice". The collo-
cations in this paper include phrasal verbs (e.g. 
"put on"), proper nouns (e.g. "New York"), idi-
oms (e.g. "dry run"), compound nouns (e.g. "ice 
cream"), correlative conjunctions (e.g. "either ? 
or"), and the other commonly used combinations 
in following types: verb+noun, adjective+noun, 
adverb+verb, adverb+adjective and adjec-
tive+preposition (e.g. "break rules", "strong tea", 
"softly whisper", "fully aware", and "fond of"). 
Many studies on collocation extraction are 
carried out based on co-occurring frequencies of 
the word pairs in texts (Choueka et al, 1983; 
Church and Hanks, 1990; Smadja, 1993; Dun-
ning, 1993; Pearce, 2002; Evert, 2004). These 
approaches use association measures to discover 
collocations from the word pairs in a given win-
dow. To avoid explosion, these approaches gen-
erally limit the window size to a small number. 
As a result, long-span collocations can not be 
extracted1. In addition, since the word pairs in 
the given window are regarded as potential col-
locations, lots of false collocations exist. Al-
though these approaches used different associa-
tion measures to filter those false collocations, 
the precision of the extracted collocations is not 
high. The above problems could be partially 
solved by introducing more resources into collo-
cation extraction, such as chunker (Wermter and 
Hahn, 2004), parser (Lin, 1998; Seretan and We-
hrli, 2006) and WordNet (Pearce, 2001). 
This paper proposes a novel monolingual 
word alignment (MWA) method to extract collo-
cation of higher quality and with longer spans 
only from monolingual corpus, without using 
any additional resources. The difference between 
MWA and bilingual word alignment (Brown et 
al., 1993) is that the MWA method works on 
monolingual parallel corpus instead of bilingual 
corpus used by bilingual word alignment. The 
                                                 
1  Here, "span of collocation" means the distance of two 
words in a collocation. For example, if the span of the col-
location (w1, w2) is 6, it means there are 5 words interrupt-
ing between w1 and w2 in a sentence. 
487
monolingual corpus is replicated to generate a 
parallel corpus, where each sentence pair con-
sists of two identical sentences in the same lan-
guage, instead of a sentence in one language and 
its translation in another language. We adapt the 
bilingual word alignment algorithm to the mono-
lingual scenario to align the potentially collo-
cated word pairs in the monolingual sentences, 
with the constraint that a word is not allowed to 
be aligned with itself in a sentence. In addition, 
we propose a ranking method to finally extract 
the collocations from the aligned word pairs. 
This method assigns scores to the aligned word 
pairs by using alignment probabilities multiplied 
by a factor derived from the exponential function 
on the frequencies of the aligned word pairs. The 
pairs with higher scores are selected as colloca-
tions. 
The main contribution of this paper is that the 
well studied bilingual statistical word alignment 
method is successfully adapted to monolingual 
scenario for collocation extraction. Compared 
with the previous approaches, which use associa-
tion measures to extract collocations, our method 
achieves much higher precision and slightly 
higher recall. The MWA method has the follow-
ing three advantages. First, it explicitly models 
the co-occurring frequencies and position infor-
mation of word pairs, which are integrated into a 
model to search for the potentially collocated 
word pairs in a sentence. Second, a new feature, 
fertility, is employed to model the number of 
words that a word can collocate with in a sen-
tence. Finally, our method can obtain the long-
span collocations. Human evaluations on the ex-
tracted Chinese collocations show that 69% of 
the long-span (>6) collocations are correct. Al-
though the previous methods could also extract 
long-span collocations by setting the larger win-
dow size, the precision is very low. 
In the remainder of this paper, Section 2 de-
scribes the MWA model for collocation extrac-
tion. Section 3 describes the initial experimental 
results. In Section 4, we propose a method to 
improve the MWA models. Further experiments 
are shown in Sections 5 and 6, followed by a dis-
cussion in Section 7. Finally, the conclusions are 
presented in Section 8. 
2 Collocation Extraction With Mono-
lingual Word Alignment Method 
2.1 Monolingual Word Alignment 
Given a bilingual sentence pair, a source lan-
guage word can be aligned with its correspond- 
 
Figure 1. Bilingual word alignment 
ing target language word. Figure 1 shows an ex-
ample of Chinese-to-English word alignment. 
In Figure 1, a word in one language is aligned 
with its counterpart in the other language. For 
examples, the Chinese word "??/tuan-dui" is 
aligned with its English translation "team", while 
the Chinese word "???/fu-ze-ren" is aligned 
with its English translation "leader". 
In the Chinese sentence in Figure 1, there are 
some Chinese collocations, such as (??/tuan-
dui, ???/fu-ze-ren). There are also some Eng-
lish collocations in the English sentence, such as 
(team, leader). We separately illustrate the collo-
cations in the Chinese sentence and the English 
sentence in Figure 2, where the collocated words 
are aligned with each other. 
 
(a) Collocations in the Chinese sentence 
 
(b) Collocations in the English sentence 
Figure 2. Word alignments of collocations in 
sentence 
Comparing the alignments in Figures 1 and 2, 
we can see that the task of monolingual colloca-
tions construction is similar to that of bilingual 
word alignment. In a bilingual sentence pair, a 
source word is aligned with its corresponding 
target word, while in a monolingual sentence, a 
word is aligned with its collocates. Therefore, it 
is reasonable to regard collocation construction 
as a task of aligning the collocated words in 
monolingual sentences. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
The team leader plays a key role in the project undertaking . 
The team leader plays a key role in the project undertaking.
The team leader plays a key role in the project undertaking. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
??  ??? ? ??  ??  ?  ? ??  ??   ?
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
488
Statistical bilingual word alignment method, 
which has been well studied in the context of 
machine translation, can extract the aligned bi-
lingual word pairs from a bilingual corpus. This 
paper adapts the bilingual word alignment algo-
rithm to monolingual scenario to align the collo-
cated words in a monolingual corpus. 
Given a sentence with l words },...,{ 1 lwwS = , 
the word alignments ]},1[|),{( liaiA i ?=  can be 
obtained by maximizing the word alignment 
probability of the sentence, according to Eq. (1). 
)|(maxarg SApA
A
?=
??
                    (1) 
Where Aai i ?),(  means that the word iw  is 
aligned with the word 
ia
w . 
In a monolingual sentence, a word never col-
locates with itself. Thus the alignment set is de-
noted as }&],1[|),{( ialiaiA ii ??= . 
We adapt the bilingual word alignment model, 
IBM Model 3 (Brown et al, 1993), to monolin-
gual word alignment. The probability of the 
alignment sequence is calculated using Eq. (2). 
???
==
l
j
jaj
l
i
ii lajdwwtwnSAp j
11
),|()|()|()|( ?   (2) 
Where i?  denotes the number of words that are 
aligned with iw . Three kinds of probabilities are 
involved: 
- Word collocation probability )|(
jaj
wwt , 
which describes the possibility of wj collo-
cating with 
ja
w ;  
- Position collocation probability d(j, aj, l), 
which describes the probability of a word 
in position aj collocating with another 
word in position j; 
- Fertility probability )|( ii wn ? , which de-
scribes the probability of the number of 
words that a word wi can collocate with 
(refer to subsection 7.1 for further discus-
sion). 
Figure 3 shows an example of word alignment 
on the English sentence in Figure 2 (b) with the 
MWA method. In the sentence, the 7th word 
"role" collocates with both the 4th word "play" 
and the 6th word "key". Thus, )|( 74 wwt  and 
)|( 76 wwt  describe the probabilities that the 
word "role" collocates with "play" and "key",  
 
Figure 3. Results of MWA method 
respectively. )12,7|4(d  and )12,7|6(d  describe 
the probabilities that the word in position 7 col-
locates with the words in position 4 and 6 in a 
sentence with 12 words. For the word "role", 7?  
is 2, which indicates that the word "role" collo-
cates with two words in the sentence. 
To train the MWA model, we implement a 
MWA tool for collocation extraction, which uses 
similar training methods for bilingual word 
alignment, except that a word can not be aligned 
to itself. 
2.2 Collocation Extraction 
Given a monolingual corpus, we use the trained 
MWA model to align the collocated words in 
each sentence. As a result, we can generate a set 
of aligned word pairs on the corpus. According 
to the alignment results, we calculate the fre-
quency for two words aligned in the corpus, de-
noted as ),( ji wwfreq . In our method, we filtered 
those aligned word pairs whose frequencies are 
lower than 5. Based on the alignment frequency, 
we estimate the alignment probabilities for each 
aligned word pair as shown in Eq. (3) and (4). 
? ?=
?w j
ji
ji wwfreq
wwfreq
wwp
),(
),(
)|(  (3) 
? ?=
?w i
ji
ij wwfreq
wwfreq
wwp
),(
),(
)|(  (4) 
With alignment probabilities, we assign scores 
to the aligned word pairs and those with higher 
scores are selected as collocations, which are 
estimated as shown in Eq. (5). 
2
)|()|(
),( ijjiji
wwpwwp
wwp
+=      (5) 
3 Initial Experiments 
In this experiment, we used the method as de-
scribed in Section 2 for collocation extraction. 
Since our method does not use any linguistic in-
formation, we compared our method with the  
The team leader plays a key role in the project undertaking . 
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)               (12) 
The team leader plays a key role in the project undertaking .
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)              (12) 
489
02
4
6
8
10
12
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Our method (Probability)
Log-likelihood ratio
 
Figure 4. Precision of collocations 
baseline methods without using linguistic knowl-
edge. These baseline methods take all co-
occurring word pairs within a given window as 
collocation candidates, and then use association 
measures to rank the candidates. Those candi-
dates with higher association scores are extracted 
as collocations. In this paper, the window size is 
set to [-6, +6]. 
3.1 Data 
The experiments were carried out on a Chinese 
corpus, which consists of one year (2004) of the 
Xinhua news corpus from LDC 2 , containing 
about 28 millions of Chinese words. Since punc-
tuations are rarely used to construct collocations, 
they were removed from the corpora. To auto-
matically estimate the precision of extracted col-
locations on the Chinese corpus, we built a gold 
set by collecting Chinese collocations from 
handcrafted collocation dictionaries, containing 
56,888 collocations. 
3.2 Results 
The precision is automatically calculated against 
the gold set according to Eq. (6). 
)(#
)(#
Top
goldTop
N
N
C
CC
precision
?
?= I            (6) 
Where CTop-N and Cgold denote the top colloca-
tions in the N-best list and the collocations in the 
gold set, respectively. 
We compared our method with several base-
line methods using different association meas-
ures3: co-occurring frequency, log-likelihood 
                                                 
2 Available at: http://www.ldc.upenn.edu/Catalog/Catalog 
Entry.jsp?catalogId=LDC2007T03 
3 The definitions of these measures can be found in Man-
ning and Sch?tze (1999). 
0
20
40
60
80
100
0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9
log(frequency)
(%
)
Precision
Alignment Probability
 
Figure 5. Frequency vs. precision/alignment 
probability 
ratio, chi-square test, mutual information, and t-
test. Among them, the log-likelihood ratio meas-
ure achieves the best performance. Thus, in this 
paper, we only show the performance of the log-
likelihood ratio measure. 
Figure 4 shows the precisions of the top N col-
locations as N steadily increases with an incre-
ment of 1K, which are extracted by our method 
and the baseline method using log-likelihood 
ratio as the association measure. 
The absolute precision of collocations is not 
high in the figure. For example, among the top 
200K collocations, about 4% of the collocations 
are correct. This is because our gold set contains 
only about 57K collocations. Even if all colloca-
tions in the gold set are included in the 200K-
best list, the precision is only 28%. Thus, it is 
more useful to compare precision curves for col-
locations in the N-best lists extracted by different 
methods. In addition, since this gold set only in-
cludes a small number of collocations, the preci-
sion curves of our method and the baseline 
method are getting closer, as N increases. For 
example, when N is set to 200K, our method and 
the baseline method achieved precisions of 
4.09% and 3.12%, respectively. And when N is 
set to 400K, they achieved 2.78% and 2.26%, 
respectively. For convenience of comparison, we 
set N up to 200K in the experiments. 
From the results, it can also be seen that, 
among the N-best lists with N less than 20K, the 
precision of the collocations extracted by our 
method is lower than that of the collocations ex-
tracted by the baseline, and became higher when 
N is larger than 20K. 
In order to analyze the possible reasons, we 
investigated the relationships among the fre-
quencies of the aligned word pairs, the alignment 
490
xy
b =4
b =2
 
Figure 6.  xbey /?=  
probabilities, and precisions of collocations, 
which are shown in Figure 5. From the figure, 
we can see (1) that the lower the frequencies of 
the aligned word pairs are, the higher the align-
ment probabilities are; and (2) that the precisions 
of the aligned word pairs with lower frequencies 
is lower. According to the above observations, 
we conclude that it is the word pairs with lower 
frequencies but higher probabilities that caused 
the lower precision of the top 20K collocations 
extracted by our method. 
4 Improved MWA Method 
According to the analysis in subsection 3.2, we 
need to penalize the aligned word pairs with 
lower frequencies. In order to achieve the above 
goal, we need to refine the alignment probabili-
ties by using a penalization factor derived from a 
function on the frequencies of the aligned word 
pairs. This function )(xfy =  should satisfy the 
following two conditions, where x  represents 
the log function of frequencies. 
(1) The function is monotonic. When x  is set to 
a smaller number, y  is also small. This re-
sults in the penalization on the aligned word 
pairs with lower frequencies. 
(2) When ??x , y  is set to 1. This means that 
we don?t penalize the aligned word pairs 
with higher frequencies. 
According to the above descriptions, we pro-
pose to use the exponential function in Eq. (7).  
    xbey /?=  (7)
Figure 6 describes this function. The constant 
b in the function is used to adjust the shape of the 
line. The line is sharp with b set to a small num-
ber, while the line is flat with b set to a larger 
number. In our case, if b is set to a larger number,  
0
5
10
15
20
25
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Refined probability
Probability
Baseline (Log-likelihood ratio)
 
Figure 7. Precision of collocations extracted by 
the improved method 
we assign a larger penalization weight to those 
aligned word pairs with lower frequencies. 
According to the above discussion, we can use 
the following measure to assign scores to the 
aligned words pairs generated by the MWA 
method. 
)),(log(
 
2
)|()|(
),(
ji wwfreq
b
ijji
jir
e
wwpwwp
wwp
?
?+=
  (8) 
Where wi and wj are two aligned words. p(wi|wj) 
and p(wj|wi) are alignment probabilities as shown 
in Eq. (3) and (4). )),(log( ji wwfreq  is the log 
function of the frequencies of the aligned word 
pairs (wi, wj). 
5 Evaluation on Chinese corpus 
We used the same Chinese corpus described in 
Section 3 to evaluate the improved method as 
shown in Section 4. In the experiments, b  was 
tuned by using a development set and set to 25. 
5.1 Precision 
In this section, we evaluated the extracted collo-
cations in terms of precision using both auto-
matic evaluation and human evaluation. 
Automatic Evaluation 
Figure 7 shows the precisions of the colloca-
tions in the N-best lists extracted by our method 
and the baseline method against the gold set in 
Section 3. For our methods, we used two differ-
ent measures to rank the aligned word pairs: 
alignment probabilities in Eq. (5) and refined 
491
 Our method Baseline 
True 569 290 
A 25 16 
B 5 4 
C 240 251 
False 
D 161 439 
Table 1. Manual evaluation of the top 1K Chi-
nese collocations. The precisions of our method 
and the baseline method are 56.9% and 29.0%, 
respectively. 
alignment probabilities in Eq. (8). From the re-
sults, it can be seen that with the refined align-
ment probabilities, our method achieved the 
highest precision on the N-best lists, which 
greatly outperforms the best baseline method. 
For example, in the top 1K list, our method 
achieves a precision of 20.6%, which is much 
higher than the precision of the baseline method 
(11.7%). This indicates that the exponential func-
tion used to penalize the alignment probabilities 
plays a key role in demoting most of the aligned 
word pairs with low frequencies. 
Human Evaluation 
In automatic evaluation, the gold set only con-
tains collocations in the existing dictionaries. 
Some collocations related to specific corpora are 
not included in the set. Therefore, we selected 
the top 1K collocations extracted by our im-
proved method to manually estimate the preci-
sion. During human evaluation, the true colloca-
tions are denoted as "True" in our experiments. 
The false collocations were further classified into 
the following classes. 
A: The candidate consists of two words that 
are semantically related, such as (?? doctor,  
?? nurse). 
B: The candidate is a part of the multi-word 
(? 3) collocation. For example, (?? self, ??  
mechanism) is a part of the three-word colloca-
tion (?? self, ?? regulating, ?? mecha-
nism). 
C: The candidates consist of the adjacent 
words that frequently occur together, such as (? 
he, ? say) and (? very, ? good). 
D: Two words in the candidates have no rela-
tionship with each other, but occur together fre-
quently, such as (?? Beijing, ? month) and 
(? and, ? for). 
Table 1 shows the evaluation results. Our 
method extracted 569 true collocations, which  
0
2
4
6
8
10
12
0 1 2 3 4 5 6 7 8 9 10 11 12
Training corpus (Months)
Pr
ec
is
io
n 
(%
)
Our method
Baseline
 
Figure 8. Corpus size vs. precision 
are much more than those extracted by the base-
line method. Further analysis shows that, in addi-
tion to extracting short-span collocations, our 
method extracted collocations with longer spans 
as compared with the baseline method. For ex-
ample, (?? in, ?? state) and (?? because, 
?? so) are two long-span collocations. Among 
the 1K collocations, there are 48 collocation can-
didates whose spans are larger than 6, which are 
not covered by the baseline method since the 
window size is set to 6.  And 33 of them are true 
collocations, with a higher precision of 69%. 
Classes C and D account for the most part of 
the false collocations. Although the words in 
these two classes co-occur frequently, they can 
not be regarded as collocations. And we also 
found out that the errors in class D produced by 
the baseline method are much more than that of 
those produced by our method. This indicates 
that our MWA method can remove much more 
noise from the frequently occurring word pairs. 
In Class A, the two words are semantically re-
lated and occur together in the corpus. These 
kinds of collocations can not be distinguished 
from the true collocations by our method without 
additional resources. 
Since only bigram collocations were extracted 
by our method, the multi-word (? 3) collocations 
were split into bigram collocations, which caused 
the error collocations in Class B4. 
Corpus size vs. precision 
Here, we investigated the effect of the corpus 
size on the precision of the extracted collocations. 
We evaluated the precision against the gold set 
as shown in the automatic evaluation. First, the 
whole corpus (one year of newspaper) was split 
into 12 parts according to the published months. 
Then we calculated the precisions as the training 
                                                 
4 Since only a very small faction of collocations contain 
more than two words, a few error collocations belong to 
Class B. 
492
020
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 9. Recall on the Chinese corpus 
corpus increases part by part. The top 20K collo-
cations were selected for evaluation. 
Figure 8 shows the experimental results. The 
precision of collocations extracted by our method 
is obviously higher than that of collocations ex-
tracted by the baseline method. When the size of 
the training corpus became larger, the difference 
between our method and the baseline method 
also became bigger. When the training corpus 
contains more than 9 months of corpora, the pre-
cision of collocations extracted by the baseline 
method did not increase anymore. However, the 
precision of collocations extracted by our method 
kept on increasing. This indicates the MWA 
method can extract more true collocations of 
higher quality when it is trained with larger size 
of training data. 
5.2 Recall 
Recall was evaluated on a manually labeled sub-
set of the training corpus. The subset contains 
100 sentences that were randomly selected from 
the whole corpus. The sentence average length is 
24. All true collocations (660) were labeled 
manually. The recall was calculated according to 
Eq. (9). 
)(#
)(#
subset
subsetTop
C
CC
recall N
I?=               (9) 
Here, CTop-N denotes the top collocations in the 
N-best list and Csubset denotes the true colloca-
tions in the subset. 
Figure 9 shows the recalls of collocations ex-
tracted by our method and the baseline method 
on the labeled subset. The results show that our 
method can extract more true collocations than 
the baseline method. 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 10. Recall on the English corpus 
 Our method Baseline 
True 591 355 
A 11 4 
B 19 20 
C 200 136 
False
D 179 485 
Table 2. Manual evaluation of the top 1K Eng-
lish collocations. The precisions of our method 
and the baseline method are 59.1% and 35.5%, 
respectively. 
In our experiments, the baseline method ex-
tracts about 20 millions of collocation candidates, 
while our method only extracts about 3 millions 
of collocation candidates5. Although the colloca-
tions of our method are much less than that of the 
baseline, the experiments show that the recall of 
our method is higher. This again proved that our 
method has the stronger ability to distinguish 
true collocations from false collocations. 
6 Evaluation on English corpus 
We also manually evaluated the proposed 
method on an English corpus, which is a subset 
randomly extracted from the British National 
Corpus6. The English corpus contains about 20 
millions of words. 
6.1 Precision 
We estimated the precision of the top 1K collo-
cations. Table 2 shows the results. The classifica-
tion of the false collocations is the same as that 
in Table 1. The results show that our methods 
outperformed the baseline method using log- 
                                                 
5 We set the threshold to 7.88 with a confidence level  of 
005.0=?  (cf. page 174 of Chapter 5 in (McKeown and 
Radev, 2000) for more details). 
6 Available at: http://www.hcu.ox.ac.uk/BNC/ 
493
05
10
15
20
0 20 40 60 80 100 120 140 160 180
Top-N collocation (K)
Pr
ec
is
io
n 
(%
)
 
Figure 11. Fertility vs. precision 
likelihood ratio. And the distribution of the false 
collocations is similar to that on the Chinese cor-
pus. 
6.2 Recall 
We used the method described in subsection 5.2 
to calculate the recall. 100 English sentences 
were labeled manually, obtaining 205 true collo-
cations. Figure 10 shows the recall of the collo-
cations in the N-best lists. From the figure, it can 
be seen that the trend on the English corpus is 
similar to that on the Chinese corpus, which in-
dicates that our method is language-independent. 
7 Discussion 
7.1 The Effect of Fertility 
In the MWA model as described in subsection 
2.1, i?  denotes the number of words that can 
align with iw . Since a word only collocates with 
a few other words in a sentence, we should set a 
maximum number for ? , denote as max? . 
In order to set max? , we examined the true col-
locations in the manually labeled set described in 
subsection 5.2. We found that 78% of words col-
locate with only one word, and 17% of words 
collocate with two words. In sum, 95% of words 
in the corpus can only collocate with at most two 
words. According to the above observation, we 
set max?  to 2. 
In order to further examine the effect of max?  
on collocation extraction, we used several differ-
ent max?  in our experiments. The comparison 
0
1
2
3
4
5
6
7
8
0 20 40 60 80 100
Span of collocation
lo
g(
#(
al
ig
ne
d 
w
or
d 
pa
irs
))
 
Figure 12. Distribution of spans 
results are shown in Figure 11. The highest pre-
cision is achieved when max?  is set to 2. This 
result verifies our observation on the corpus. 
7.2 Span of Collocation 
One of the advantages of our method is that 
long-span collocations can be reliably extracted. 
In this subsection, we investigate the distribution 
of the span of the aligned word pairs. For the 
aligned word pairs occurring more than once, we 
calculated the average span as shown in Eq. (10). 
),(
);,(
),(
ji
corpuss
ji
ji wwfreq
swwSpan
wwAveSpan
?
= ?  (10) 
Where, );,( swwSpan ji  is the span of the words 
wi and wj in the sentence s; ),( ji wwAveSpan  is 
the average span. 
The distribution is shown in Figure 12. It can 
be seen that the number of the aligned word pairs 
decreased exponentially as the average span in-
creased. About 17% of the aligned word pairs 
have spans longer than 6. According to the hu-
man evaluation result for precision in subsection 
5.1, the precision of the long-span collocations is 
even higher than that of the short-span colloca-
tions. This indicates that our method can extract 
reliable collocations with long spans. 
8 Conclusion 
We have presented a monolingual word align-
ment method to extract collocations from mono-
lingual corpus. We first replicated the monolin-
gual corpus to generate a parallel corpus, in 
which each sentence pair consists of the two 
identical sentences in the same language. Then 
we adapted the bilingual word alignment algo-
rithm to the monolingual scenario to align the 
10
3
2
1
max
max
max
max
=
=
=
=
?
?
?
?
494
potentially collocated word pairs in the monolin-
gual sentences. In addition, a ranking method 
was proposed to finally extract the collocations 
from the aligned word pairs. It scores collocation 
candidates by using alignment probabilities mul-
tiplied by a factor derived from the exponential 
function on the frequencies. Those with higher 
scores are selected as collocations. Both Chinese 
and English collocation extraction experiments 
indicate that our method outperforms previous 
approaches in terms of both precision and recall. 
For example, according to the human evaluations 
on the Chinese corpus, our method achieved a 
precision of 56.9%, which is much higher than 
that of the baseline method (29.0%). Moreover, 
we can extract collocations with longer span. 
Human evaluation on the extracted Chinese col-
locations shows that 69% of the long-span (>6) 
collocations are correct. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Yaacov Choueka, S.T. Klein, and E. Neuwitz. 1983. 
Automatic Retrieval of Frequent Idiomatic and 
Collocational Expressions in a Large Corpus. 
Journal for Literary and Linguistic computing, 
4(1):34-38. 
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexi-
cography. Computational Linguistics, 16(1):22-29. 
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational 
Linguistics, 19(1): 61-74. 
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, 
University of Stuttgart. 
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language Proc-
essing, Cambridge, MA; London, U.K.: Bradford 
Book & MIT Press. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Darren Pearce. 2001. Synonymy in Collocation Ex-
traction. In Proceedings of NAACL-2001 Workshop 
on Wordnet and Other Lexical Resources: Applica-
tions, Extensions and Customizations, pp. 41-46. 
Darren Pearce. 2002. A Comparative Evaluation of 
Collocation Extraction Techniques. In Proceedings 
of the 3rd International Conference on Language 
Resources and Evaluation, pp. 651-658. 
Violeta Seretan and Eric Wehrli. 2006. Accurate Col-
location Extraction Using a Multilingual Parser. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual 
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pp. 953-960 
Frank Smadja. 1993. Retrieving Collocations from 
Text: Xtract. Computational Linguistics, 19(1): 
143-177. 
Joachim Wermter and Udo Hahn. 2004. Collocation 
Extraction Based on Modifiability Statistics. In 
Proceedings of the 20th International Conference 
on Computational Linguistics (COLING-2004), pp. 
980-986. 
495
A Hybrid Chinese Language Model based on a Combination of 
Ontology with Statistical Method 
Dequan Zheng, Tiejun Zhao, Sheng Li and Hao Yu 
MOE-MS Key Laboratory of Natural Language Proceessing and Speech 
Harbin Institute of Technology 
Harbin, China, 150001 
{dqzheng, tjzhao, lisheng, yu}@mtlab.hit.edu.cn 
  
Abstract 
In this paper, we present a hybrid Chi-
nese language model based on a com-
bination of ontology with statistical 
method. In this study, we determined 
the structure of such a Chinese lan-
guage model. This structure is firstly 
comprised of an ontology description 
framework for Chinese words and a 
representation of Chinese lingual on-
tology knowledge. Subsequently, a 
Chinese lingual ontology knowledge 
bank is automatically acquired by de-
termining, for each word, its co-
occurrence with semantic, pragmatics, 
and syntactic information from the 
training corpus and the usage of Chi-
nese words will be gotten from lingual 
ontology knowledge bank for a actual 
document. To evaluate the performance 
of this language model, we completed 
two groups of experiments on texts re-
ordering for Chinese information re-
trieval and texts similarity computing. 
Compared with previous works, the 
proposed method improved the preci-
sion of nature language processing. 
1 Introduction 
Language modeling is a description of natural 
language and a good language model can help to 
improve the performance of the natural language 
processing. 
Traditional statistical language model 
(SLM) is fundamental to many natural language 
applications like automatic speech recognitionP[1]P, 
statistical machine translationP[2]P, and information 
retrievalP[3]P. Different statistical models have 
been proposed in the past, but n-gram models (in 
particular, bi-gram and tri-gram models) still 
dominate SLM research. After that, other ap-
proaches were put forward, such as the 
combination of statistical-based approach and 
rule-based approachP[4,5]P, self-adaptive language 
modelsP[6]P, topic-based model P[7]P and cache-based 
model P[8]P. But when the models are applied, the 
crucial disadvantages are that they can?t repre-
sent and process the semantic information of a 
natural language, so they can?t adapt well to the 
environment with changeful topics. 
Ontology was recognized as a conceptual 
modeling tool, which can descript an informa-
tion system in the semantic level and knowledge 
level. After it was first introduced in the field of 
Artificial IntelligenceP[9]P, it was closed combined 
with natural language processing and are widely 
applied in many field such as knowledge engi-
neering, digital library, information retrieval, 
semantic Web, and etc.  
In this paper, combining with the character-
istic of ontology and statistical method, we pre-
sent a hybrid Chinese language model. In this 
study, we determined the structure of Chinese 
language model and evaluate its performance 
with two groups of experiments on texts reorder-
ing for Chinese information retrieval and texts 
similarity computing. 
The rest of this paper is organized as fol-
lows. In section 2, we describe the Chinese lan-
guage model. In section 3, we evaluate the 
language model by several experiments about 
natural language processing. In section 4, we 
present the conclusion and some future work. 
2 The language model description 
Traditional SLM is make use to estimate the 
likelihood (or probability) of a word string, in 
13
this study, we determined the structure of Chi-
nese language model, first, we gave the ontology 
description framework of Chinese word and the 
representation of Chinese lingual ontology 
knowledge, and then, automatically acquired the 
usage of a word with its co-occurrence of con-
text in using semantic, pragmatics, syntactic, etc 
from the corpus to act as Chinese lingual ontol-
ogy knowledge bank. In actual document, the 
usage of lingual knowledge will be gotten from 
lingual ontology knowledge bank. 
2.1   Ontology description framework 
Traditional ontology mainly emphasizes the 
interrelations between essential concept, domain 
ontology is a public concept set of this do-
main P[10]P. We make use of this to present Chinese 
lingual ontology knowledge bank. 
In practical application, ontology can be 
figured in many waysP[11]P, natural languages, 
frameworks, semantic webs, logical languages, 
etc. Presently, popular models, such as Ontolin-
gua, CycL and Loom, are all based on logical 
language. Though logical language has a strong 
expression, its deduction is very difficult to lin-
gual knowledge. Semantic web and natural lan-
guage are non-formal, which have disadvantages 
in grammar and expression. 
For a Chinese word, we provided a frame-
work structure that can be understood by com-
puter combined with WordNet, HowNet and 
Chinese Thesaurus. This framework includes a 
Chinese word in concept, part of speech (POS), 
semantic, synonyms, English translation. Fig-
ure1 shows the ontology description framework 
of a Chinese word. 
 
 
 
 
 
 
 
Fig. 1. Ontology description framework 
2.2   Lingual ontology knowledge representation 
A word is the basic factor that composes the 
natural language, to acquire lingual ontology 
knowledge, we need to know POS, means and 
semantic of a word in a sentence. For example, 
for a Chinese sentence, the POS, means and 
Semantic label of ??? in HowNet are shown in 
table 1. For the Chinese sentence ??????
??????, after words segmented, POS tag-
ging and semantic tagging, we get a characteris-
tic string. They are shown in table 2. 
Table 1. the usage of ??? in Chinese sentence 
Chinese Sentence POS Means Semantic Num 
??? Verb Weave 525(weave|?? ) 
?? ? Verb Buy 348(buy|? ) 
Table 2. Segmentation, POS and Semantic tagging 
Items Results (???? acts as keyword) 
Chinese sentence ?????????? 
Words segmenta-
tion 
??  ??  ?  ??  ??  ? 
POS tagging ?? nd/ ?? Keyword/? vg/ ?? nd/ ??
vg/ ?wj/ 
Semantic label 
tagging 
?? nd/021243 ?? Keyword/070366?  
vg/017545 ?? nd/021243 ?? vg/092317 ?
wj/-1 
Characteristic string nd/021243 ?? Keyword/070366  vg/017545 
nd/021243 vg/092317 
Explanation of 
Semantic label 
021243 represents ????, 070366 represents 
???, 092317 represents ? ?? ?, ?-1? repre-
sents not to be defined or exist this semantic in 
HowNet. 
 
In order to use and express easily, we gave 
a description for ontology knowledge of every 
Chinese word, which learned from corpus, to be 
shown as expression 1. All of them composed 
the Chinese lingual ontology knowledge bank. 
( ) ( ) ( )???????? == UU
n
r
rrr
m
l
lll CLPOSSemCLPOSSemontologyKeyWord
11
,,,,,,,,
 
Where, KeyWord(ontology) is the ontology 
description of a Chinese word, ( )iii CLPOSSem ,,,  is 
the left co-occurrence knowledge of a Chinese 
word got from its context and ( )iii CLPOSSem ,,,  is 
the right co-occurrence knowledge. Symbol 
?? ? represents the aggregate of all the co-
occurrence with the KeyWord. ( )iii CLPOSSem ,,,  denotes the multi-grams 
from context of a Chinese word, which is com-
posed of semantic information SemBi B, part of 
speech POS Bi B, the position L from the word 
KeyWord to its co-occurrence, the average dis-
tance lC  from the word to its left (or right) i-th 
word. 
( )( )LPOSSemKeyword ii ,,,  denotes a seman-
tic relation pair between the keyword and its co-
occurrence in current context. 
The multi-grams of a Chinese word in con-
text, including the co-occurrence and their posi-
tion will act as the composition of lingual 
ontology knowledge too. In figure 2, the charac-
teristic string WB1 B, WB2 B, ?, WBi B represents POS and 
semantic label, Keyword is keyword itself, l or r 
Keywords  <?>
Concept             <?> 
Part of Speech  <?> 
Ontology   Semantic            <?> 
                    Synonym           <?> 
E-translation     <?> 
14
is the position of word that is left or right co-
occurrence with keyword. 
 
Fig. 2. Co-occurrence and the position information 
2.3   Lingual ontology knowledge acquisition 
According to the course that human being ac-
quires and accumulates knowledge, we propose 
a measurable description for Chinese lingual 
ontology knowledge through automatically 
learning typical corpus. In this approach, we will 
acquire the usage of a Chinese word in semantic, 
pragmatic and syntactic in all documents. We 
combine with the multi-grams in context includ-
ing its co-occurrence, POS, semantic, synonym, 
position. In practical application, we will proc-
ess every Chinese keyword that has the same 
grammar expression, semantic representation 
and syntactic structure with Chinese lingual on-
tology knowledge bank. 
2.3.1   Algorithm of automatic acquisition 
Step 1: corpus pre-processing.  
For any Chinese document DBi B in the docu-
ment set {D}, we treat the sentence that includes 
keyword as a processing unit. First, we have a 
Chinese word segmentation, POS tagging, Se-
mantic label tagging based on HowNet, and then, 
confirm a word to act as the keyword for acquir-
ing its co-occurrence knowledge. We wipe off 
the word that can do little contribution to the 
lingual ontology knowledge, such as preposition, 
conjunction, auxiliary word and etc. 
Step 2: Unify the keyword. 
Making use of the ontology description of 
Chinese word, we make the synonym into uni-
form one. 
Step 3: Calculate the co-occurrence distance. 
In our proposal, first, we treat the sentence 
that includes keyword as a processing unit and 
make POS tagging, semantic label tagging, then, 
we get Characteristic string. We take the key-
word as the center, define the left and right dis-
tance factor B Bl B and B Br B to be shown at formula 1. 
ml
B
??
???
??
?
=
2
11
2
11               
nr
B
??
???
??
?
=
2
11
2
11     (1) 
Where, m and n represent the left and right 
number of word that centered with the keyword. 
In this way, we try to get the language intuition, 
in a word, if the co-occurrence is nearer to the 
keyword, we will get more the co-occurrence 
distant. Final, we respectively get the left-side 
and right-side co-occurrence distant from key-
word to its co-occurrence to be shown as for-
mula 2. 
l
i
li BC
1
2
1 ???
???
?=  (i=1,?,m) 
r
j
rj BC
1
2
1 ???
???
?=  (j=1,?,n)       (2) 
Step4: Calculate the average co-occurrence 
distance. 
For a keyword, in the current sentence of 
document DBi,B we regard the keyword and its co-
occurrence (SemBi B, POS Bi B, L) as semantic relation 
pair, and CBjB is their co-occurrence distance. We 
calculate the average of CBjB that appear in corpus 
and act as the average co-occurrence distance 
lC  
between the keyword and its co-occurrence 
(SemBi B, POS Bi B, L). 
When all of documents are learned, all of 
keyword and their co-occurrence information ( )iii CLPOSSem ,,,  compose the Chinese lingual 
ontology knowledge bank. 
Step 5: Rebuild the index. 
In order to improve the processing speed, 
for acquired lingual ontology knowledge bank, 
we first build an index according to Chinese 
word, and then, we respectively make a sorting 
according to the semantic label SemBi B for every 
Chinese word. 
2.3.2 Lingual ontology knowledge application 
In practical application, we will respectively get 
different evaluation of a document from the lin-
gual ontology knowledge bank. For the natural 
language processing, e.g. documents similarity 
computing, text re-ranking for information re-
trieval, information filtering, the general proc-
essing is as follow. 
Step 1: Pre-processing and unify the key-
word. 
The processing is the same as Step 1 and 
Step 2 in section 2.3.1. 
Step 2: Fetch the average co-occurrence 
distance from lingual ontology knowledge bank. 
We regard a sentence including keyword in 
document D as a processing unit. First, we make 
POS tagging, semantic label tagging and get 
Characteristic string, and then, for every key-
word, if it has the same semantic relation pair as 
lingual ontology knowledge bank, i.e. the key-
word and its co-occurrence (SemBi B, POS Bi B, L) in 
practical document is the same one as lingual 
15
ontology knowledge bank, we add up all the 
average co-occurrence distance 
lC  from Chinese 
lingual ontology knowledge bank acquired in 
section 2.3.1. 
Step 3: Get the evaluation value of a docu-
ment. 
Repeat Step 2 until all keywords be proc-
essed and the accumulation of the average co-
occurrence distance 
lC will act as the evaluation 
value of current document. 
3 Evaluation of language model 
We completed two groups of experiments on 
text re-ranking for information retrieval, text 
similarity computing to verify the performance 
of lingual ontology knowledge. 
3.1   Texts reordering 
Information retrieval is used to retrieve relevant 
documents from a large document set for a user 
query, where the user query can be a simple de-
scription by natural. As a general rule, users 
hope more to acquire relevant information from 
the top ranking documents, so they concern 
more on the precision of top ranking documents 
than the recall. 
We use the Chinese document set CIRB011 
(132,173 documents) and CIRB020 (249,508 
documents) from NTCIR3 CLIR dataset and 
select 36 topics from 50 search topics (see 
http://research.nii.ac.jp/ntcir-ws3/work-en.html 
for more information) to evaluate our method. 
We use the same method to retrieve documents 
mentioned by Yang LingpengP[12]P, i.e. we use 
vector space model to retrieve documents, use 
cosine to calculate the similarity between docu-
ment and user query. We respectively use bi-
grams and words as indexing unitsP[13,14]P, the av-
erage precision of top N ranking documents acts 
as the normal results. In this paper, we used a 
Chinese dictionary that contains about 85,000 
items to segment Chinese document and query. 
To measure the effectiveness of informa-
tion retrieval, we use the same two kinds of 
relevant measures: relax-relevant and rigid-
relevantP[14,15]P. A document is rigid-relevant if it?s 
highly relevant or relevant with user query, and 
a document is relax-relevant if it is high relevant 
or relevant or partially relevant with user query. 
We also use PreAt10 and PreAt100 to represent 
the precision of top 10 ranking documents and 
top 100 ranking documents. 
3.1.1   Strategy of texts reordering 
First, we get some keywords to every topic by 
query description. For example, 
Title: ????? (The birth of a cloned 
calf) 
Description: ????????????
?????????????? (Find Arti-
cles relating to the birth of cloned calves using 
the technique called somatic cell nuclear transfer) 
We extract ???, ???, ??, ???
?? as feature word in this topic. 
Second, acquire lingual ontology knowl-
edge every topic by their feature words. In this 
proposal, we arrange 300 Chinese texts of this 
topic as learning corpus to get lingual ontology 
knowledge bank. 
Third, get the evaluation value of every text 
about this topic, i.e. respectively add up all the 
average co-occurrence distance lC  to the same 
semantic relation pairs in every text from lingual 
ontology knowledge bank.  
If a text has several keywords, repeat step3 
to acquire every evaluation value to these key-
words, and then, add up each evaluation value to 
act as the text evaluation value. 
Final, we reorder the initial retrieval texts 
according to the every text evaluation value of 
every topic. 
3.1.2   Experimental results and analysis 
We calculate the evaluation value of every text 
in each topic to reorder the initial relevant 
documents. 
Table 3 lists the normal results and our re-
sults based on bi-gram indexing, our results are 
acquired based on Chinese lingual ontology 
knowledge to enhance the effectiveness. 
PreAt10 is the average precision of 36 topics in 
precision of top 10 ranking documents, while 
PreAt100 is top 100 ranking documents. 
Table 4 lists the normal results and our re-
sults based on word indexing. Ratio displays an 
increase ratio of our result compared with nor-
mal result. 
Table 3. Precision (bi-gram as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3704 0.4389 18.49% 
PreAt100 (Relax) 0.1941 0.2239 15.35% 
PreA10 (Rigid) 0.2625 0.3083 17.45% 
PreAt100 (Rigid) 0.1312 0.1478 12.65% 
16
Table 4. Precision (word as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3829 0.4481 17.03% 
PreAt100 (Relax) 0.2022 0.2306 14.05% 
PreAt10 (Rigid) 0.2745 0.3169 15.45% 
PreAt100 (Rigid) 0.1405 0.1573 11.96% 
In table 3, it is shown that compared with 
bi-grams as indexing units, our method respec-
tively increases 18.49% in relax relevant meas-
ure and 17.45% in rigid in PreAt10. In PreAt100 
level, our method respectively increases 15.35% 
in relax relevant and 12.65% in rigid relevant 
measure. Figure 3 displays the PreAt10 values 
of each topic in relax relevant measure based on 
bi-gram indexing where one denotes the preci-
sion enhanced with our method, another denotes 
the normal precision. It is shown the precision of 
each topic is all improved by using our method. 
 
Fig. 3. PreAt10 of all topics in relax judgment 
In table 4, using words as indexing units, 
our method respectively increases 17.03% in 
relax relevant measure and 15.45% in rigid in 
PreAt10. In PreAt100 level, our method respec-
tively increases 14.05% in relax relevant meas-
ure and 11.96% in rigid. 
In our experiments, compared with the two 
Chinese indexing units: bi-gram and words, our 
method increases the average precision of all 
queries in top 10 and top 100 measure levels for 
about 17.1% and 13.5%. What lies behind our 
method is that for each topic, we manually select 
some Chinese corpus to acquire the lingual on-
tology knowledge, and can help us to focus on 
relevant documents. Our experiment also shows 
improper extract and corpus may decrease the 
precision of top documents. So our method de-
pends on right keywords in texts, queries and the 
corpus. 
3.2   Text similarity computing 
Text similarity is a measure for the matching 
degree between two or more texts, the more high 
the similarity degree is, the more the meaning of 
text expressing is closer, vice versa. Some pro-
posal methods include Vector Space Model P[16]P, 
Ontology-based P[17]P, Distributional Semantics 
model P[18]P. 
3.2.1   Strategy of similarity computation 
First, for two Chinese texts DBiB and DBjB, we re-
spectively extract k same feature words, if the 
same feature words in the two texts is less than k, 
we don?t compare their similarity. 
Second, acquire lingual ontology knowl-
edge every text by their feature words. 
Third, get the evaluation value of every text, 
i.e. respectively add up all the average co-
occurrence distance 
lC  to the same semantic 
relation pairs in two texts. 
Final, compute the similarity ratio of every 
two text DBi B and DBj B. The similarity ratio equals to 
the ratio of the similarity evaluation value of 
text DBi B and DBj B, if the ratio is in the threshold ?, 
then we think that text DBi B is similar to text DBj B. 
3.2.2   Experimental results and analysis 
We download four classes of text for testing 
from Sina, Yahoo, Sohu and Tom, which in-
clude 71 current affairs news, 68 sports news, 69 
IT news, 74 education news. 
For the test of current affairs texts, accord-
ing to the strategy of similarity computation, we 
choose five words as feature word. They are ??
?, ??, ??, ??, ???. In the texts, the 
word ???, ??? are all replaced by word ??
?? and other classes are similar. The testing 
result is shown in table 5.  
Table 5. Testing results for text similarity 
0.95<?<1.05 0.85<?<1.15 Items 
Precision Recall FB1 B-measure Precision Recall FB1 B-measure 
Current affairs news 97.14% 97.14% 97.14% 94.60% 100% 97.23% 
Sports News 88.57% 91.18% 89.86% 84.62% 97.06% 90.41% 
IT news 93.75% 96.77% 95.24% 91.18% 100% 95.39% 
Education news 94.74% 97.30% 96.00% 90.24 100% 94.87% 
General results 93.57% 95.62% 94.58% 90.07% 99.27% 94.42% 
 
17
We analyzed all the experimental results to 
find that the results for current affairs texts are 
the best, while the sports texts are lower than 
others. We think it is mainly because some 
sports terms are unprofessional for the lower 
sports texts recognition, such as ????, ??, 
???. Other feature words are more fixed and 
more concentrated. 
4 Conclusion 
In this paper, we presented a hybrid Chinese 
language model based on a combination of on-
tology with statistical method. We discuss the 
modeling and evaluate its performance. In the 
test about texts reordering, our experiences show 
that our method can increase the performance of 
Chinese information retrieval about 17.1% and 
13.5% at top 10 and top 100 documents measure 
level. In another test about texts similarity com-
puting, F1-measure is above 95%. 
On the other hand, in the current disposal 
of our information processing, we only make 
use of some characteristics ontology and use 
some co-occurrence information, such as seman-
tics, POS, context, position, distance, and etc. 
For the further research and experiment, we will 
be on the following: (1) Research on the charac-
teristics of relations between semantics and 
combine with some mature natural language 
processing techniques. (2) Research traditional 
ontology representation to keep up with interna-
tional stand. (3) Apply our key techniques to 
English information retrieval and cross-lingual 
information retrieval systems and study a 
general approach. 
References 
1. Jelinek, F. 1990. Self-organized language model-
ing for speech recognition. In Readings in Speech 
Recognition,A. Waibel and K. F. Lee, eds. Mor-
gan-Kaufmann, San Mateo, CA,1990, 450-506. 
2.  Brown, P., Pietra, S. D., Pietra, V. D., and Mercer, 
R. 1993. The mathematics of statistical machine-
translation: Parameter estimation. Computational 
Linguistics 19, 2 (1993), 269-311. 
3.  Croft, W. B. and Lafferty, J. (EDS.) 2003. Lan-
guage Modeling for Information Retrieval. Kluwer 
Academic,Amsterdam. 
4. Wang Xiaolong, Wang Kaizhu. 1994. Speech in-
put by sentence, Chinese Journal of Computers, 
17(2): 96-103 
5.  Zhou Ming, Huang Changning, Zhang Min, Bai 
Shuanhu, and Wu Sheng. 1994. A Chinese parsing 
model based on corpus, rules and statistics, Com-
puter research and development, 31(2):40-49 
6. R DeMori, M Federico. 1999. Language model 
adaptation. In: Keith Pointing ed. Computational 
Models of Speech Pattern Processing. NATO ASI 
Series. Berlin: Springer Verlag, 102-111 
7. R Kuhn , R D Mori. 1990. A cache-based natural 
language model for speech reproduction. IEEE 
Trans on Pattern Analysis and Machine Intelli-
gence, PAM2-12(6), 570-583 
8.  Daniel Gildea, Thomas Hofmannl. 1999. Topic-
based language models using EM1. In : Proceed-
ing of the 6th European Conf on Speech Commu-
nication and Technology, Budapest, Hungary: 
ESCA, 2167-2170 
9.  Neches R., Fikes R., Finin T., Gruber T., Patil R., 
Senator T., and Swartout W. R.. 1991. Enabling 
Technology for Knowledge Sharing. AI Magazine, 
12(3) :16~36 
10. Gruber, T. R. 1993. Toward principles for the 
design of ontologies used for knowledge sharing. 
International Workshop on Formal Ontology, Pa-
dova, Italy 
11. Uschold M. 1996. Building Ontologies-Towards 
A Unified Methodology. In expert systems 96 
12. Yang Lingpeng, Ji Donghong, TangLi. 2004. 
Document Re-ranking Based on Automatically 
Acquired Key Terms in Chinese Information Re-
trieval. In Proceedings of the COLING'2004, pp. 
480-486 
13. Kwok, K.L. 1997. Comparing Representation in 
Chinese Information Retrieval. In Proceeding of 
the ACM SIGIR-97, pp. 34-4 
14. Nie, J.Y., Gao, J., Zhang, J., Zhou, M. 2000. On 
the Use of Words and N-grams for Chinese Infor-
mation Retrieval. In Proceedings of the IRAL-
2000, pp. 141-148 
15. Robertson, S.E. and Walker, S. 2001. Microsoft 
Cambridge at TREC-9: Filtering track: In Pro-
ceeding of the TREC 2000, pages 361-369 
16. Salton, G., Buckley, C. Term weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 1988, 24(5), 
pp.513?523 
17. Vladimir Oleshchuk, Asle Pedersen. Ontology 
Based Semantic Similarity Comparison of Docu-
ments, 14th International Workshop on Database 
and Expert Systems Applications, September, 
2003, pp.735-738 
18. Besancon, R., Rajman, M., Chappelier, J. C. Tex-
tual similarities based on a distributional approach, 
Tenth International Workshop on Database and 
Expert Systems Applications, 1-3 Sept. 1999, 
pp.180-184 
 
18
Automated Generalization of Phrasal Paraphrases from the Web*
Weigang Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lee@ir.hit.edu
.cn
Ting Liu 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
tliu@ir.hit.ed
u.cn
Yu Zhang
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
zhangyu@ir.hit
.edu.cn
Sheng Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lis@ir.hit.edu
.cn
Wei He 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
truman@ir.hit.
edu.cn
Abstract
Rather than creating and storing thou-
sands of paraphrase examples, para-
phrase templates have strong 
representation capacity and can be used
to generate many paraphrase examples.
This paper describes a new template
representation and generalization
method. Combing a semantic diction-
ary, it uses multiple semantic codes to
represent a paraphrase template. Using
an existing search engine to extend the
word clusters and generalize the exam-
ples.  We also design three metrics to
measure our generalized templates. The 
experimental results show that the rep-
resentation method is reasonable and 
the generalized templates have a higher 
precision and coverage.
1 Introduction
Paraphrases are alternative ways to convey the 
same information (Barzilay and McKeown,
2001) and they have been applied in many fields
of natural language processing. There are many
previous work on paraphrase examples extrac-
tion or combining them with some applications
such as information retrieval and question an-
swering (Agichtein et al, 2001; Florence et al, 
2003; Rinaldi et al, 2003; Tomuro, 2003; Lin
and Pantel, 2001;), information extraction 
(Shinyama et al, 2002; Shinyama and Sekine, 
2003), machine translation (Hiroshi et al, 2003;
Zhang and Yamamoto, 2003), multi-document
(Barzilay et al, 2003).
There is also some other research about 
paraphrase. (Wu and Zhou, 2003) just extract 
the synonymy collocation, such as <turn on, 
OBJ, light> and <switch on, OBJ, light> using
both monolingual corpora and bilingual corpora 
to get an optimal result, but do not generalize
them. (Glickman and Dagan, 2003) detects verb
paraphrases instances within a single corpus
without relying on any priori structure and in-
formation. Generation of paraphrase examples
was also investigated (Barzilay and Lee, 2003;
Quirk et al, 2004).
Rather than creating and storing thousands of 
paraphrases, paraphrase templates have strong 
representation capacity and can be used to gen-
erate many paraphrase examples. As (Hirst, 
2003) said, for each aspect of paraphrase there 
are two main challenges: representation of 
knowledge and acquisition of knowledge. Cor-
responding to the problem of generalization of 
paraphrase templates, there are also two prob-
lems: the first is the representation of paraphrase
templates and the second is acquisition of para-
phrase templates.
There are several methods about paraphrase
templates representation. The first method is 
using the Part-of-Speech (Barzilay and McKe-
own, 2001; Daum? and Marcu, 2003; Zhang and 
Yamamoto, 2003), the second uses name entity 
as the variable (Shinyama et al, 2002; Shinyama
and Sekine, 2003), the third method is similar to 
the second method which is called the inference 
rules extraction (Lin and Pantel, 2001).
A paraphrases template is a pair of natural
language phrases with variables standing in for
certain grammatical constructs in (Daum? and 
*: Supported by the Key Project of National Natural Sci-
ence Foundation of China under Grant No. 60435020
49
Marcu, 2003). He used Part-of-Speech to repre-
sent templates. But for some cases, the POS will 
be very limited and for some other cases will be 
over generalized. For example:
?????????
(In my view/mind ----I feel)
The above pair of phrases is a paraphrase, it 
can be generalized using POS information: 
? [pronoun]??
(In [pronoun] view/mind)
[pronoun]??
( [pronoun] feel)
But for this template many noun words will
be excluded. From this point of view, the tem-
plate representation capacity is limited. But for 
other examples, the POS information will be 
over generally. For example:
?????????
(What's the price for the apples?)
????????
(How much is the apples per Jin?)
Here, we just generalize one variable ????.
Then, the template becomes:
[noun]???????
(What's the price for the [noun]?)
[noun]??????
(How much is the [noun] per Jin?)
If there is a sentence ??????????
(What's the price for the notebook?)?, its? para-
phrase will be ?????????(How much 
is the notebook per Jin?)? according to this tem-
plate. Obviously, the result is unreasonable.
(Shinyama et al, 2002) tried to find para-
phrases assuming that two sentences sharing
many Named Entities and a similar structure are 
likely to be paraphrases of each other. But just 
name entities are limited, too. And (Lin and 
Pantel, 2001) present an unsupervised algorithm 
for discovering inference rules from text such as 
?X writes Y? and ?X is the author of Y?. This 
generalized method has good ability. But it also
has some limited aspect. For example:
[Jack] writes [his homework].
According to the paraphrase template, the
target sentence will be transformed into ?[Jack]
is the author of [his homework]?. It?s obviously
that the generated sentence is not standard.
So how to represent paraphrase templates
and generalize the paraphrase examples is a very 
interesting task. In this paper, we present a novel
approach to represent paraphrase template with 
semantic code of words and using an existing
search engine to get the paraphrase template.
The remainder of this paper is organized as 
follows. In the next section, we give the over-
view of our method. In section 3, we define the 
representation method in details. Section 4 pre-
sents the generalization method. Some experi-
ments and discussions are shown in Section 5. 
Finally, we draw a conclusion of this method
and give some suggestions about future work. 
2 Overview of Generalization Method
The origin input of our system is a seed phrasal
paraphrase example. And the output is the gen-
eralized paraphrase templates from the given 
examples. The overall architecture of our para-
phrase generalization is represented on figure 1. 
A seed phrasal
paraphrase examples
Getting the slot word
Extend the slot word
using Search Engine
on every example
Mapping two word
sets to their semantic
code sets
Intersection operation
on the two semantic
code sets
Generalizing a
template
Figure 1: Sketch Map of Paraphrase example
Generalization
We also use the example (1) to illustrate the 
representation. Here a semantic dictionary called 
?TongYiCiCiLin? (Extension Version)1 is used. 
The pair of phrases is a phrasal paraphrase. At
first, after preprocessing which includes word
segment, POS tagging and word sense disam-
biguation, we get the slot word in the paraphrase.
In this example, the slot word is ??(I)?. Then
we search the web using the context of the slot 
word. Every phrase in the phrasal pair derives a
set of sentences which include the original 
phrase context. A dependency parser on these 
sentences is used to extract the corresponding
word with the slot word. Two word sets can be 
obtained through the two sentence sets. Then,
we map word sets to their semantic code sets
1 TongYiCiCiLin (Extended Version) can be downloaded
from the website of HIT-IRLab (Http://ir.hit.edu.cn). In the 
past section, we abbreviate the TongYiCiCiLin (Extended
Version) to Cilin (EV) 
50
according to Cilin(EV). Then an intersection 
operation is conducted on the two sets. We use 
the intersection set to replace the slot word and 
generate the final paraphrase template. 
In order to verify the validation of the gener-
alized paraphrase template, we also design an 
automatic algorithm to confirm whether the 
template is reasonable using the existing search 
engine.
3 Representation of Template 
In the section of introduction, some representa-
tion methods of paraphrase template have been 
introduced. And we proposed a new method us-
ing word semantic codes to represent the vari-
able in a template. Before we introduce the 
representation method, Firstly, we give some 
general introduction about the semantic diction-
ary of Cilin(EV). 
3.1 TongYiCiCiLin (Extended Version) 
Cilin (EV) is derived from original TongY-
iCiCilin in which word senses are decomposed 
to 12 large categories, 94 middle categories, 
1,428 small categories. Cilin (EV) removes 
some outdated words and updates many new 
words. More fine-grained categories are added 
on the base of original classification system to 
satisfy the more complex natural language ap-
plications. The encoding criterion is shown in 
the table 1:
Table 1 Encoding table of dictionary
Encoding
bit 1 2 3 4 5 6 7 8
Example D a 1 5 B 0 2 =
Attribute Big Middle Small groups Atom groups
Layer 1 2 3 4 5
The encoding bits are arranged from left to 
right. The first three layers are same with Cilin. 
The fourth layer is represented by capital letters 
and the fifth layer is two-bit decimal digit. The 
last bit is some more detailed information about 
the atom groups. 
3.2 An Example of a Paraphrase Template 
For simplicity, we just select one slot word in 
every paraphrase. And we stipulated that only 
content word can be slot word. We also use the 
above paraphrase example (1). 
?????????
(In my view/mind ----I feel)
Here, we get the slot word ??(I)?. Through 
the Word Sense Disambiguation processing, we 
get its semantic code ?Aa02A01=? according to 
the fifth layer in Cilin(EV). If we just use the 
semantic code of the slot word, we can get a 
simple paraphrase template as follows:  
? [Aa02A01=] ??
(In [Aa02A01=]  view/mind)
[Aa02A01=] ??
([Aa02A01=]  feel)
But it is obviously that the template is very 
limited. Its? representation ability is also limited. 
So how to extend the ability of a paraphrase 
template is a challenging work.  
3.3 Extending the Template Abstract Ability 
According to the feature of Cilin(EV) architec-
ture, we can use the higher layer?s semantic 
code instead of the slot word to generalize the 
paraphrase template naturally. Of course it?s a 
very simple method to extend the template abil-
ity, but it also brings more redundancy of a 
paraphrase template and it will be proven in the 
later section. 
So we use multiple semantic codes of the dif-
ferent layer instead of only one semantic code of 
slot word in Cilin (EV). The later experimental 
results prove this representation has a good per-
formance with a good precision and coverage. 
4 Generalizing to Templates 
As mentioned above, we can use multiple se-
mantic codes to generalize paraphrase examples. 
So the problem of how to generalize paraphrase 
examples is transformed into the problem of 
how to get the multiple semantic codes set. We 
proposed a new method which uses the existing 
search engine to reach the target.  
4.1 Getting the Candidate Sentences 
After we removed the slot word in the para-
phrase examples, two phrasal contexts of the 
original paraphrase phrases were obtained. Each 
phrase without slot word is used as a search 
query for an existing search engine and achiev-
ing many sentences which include the query 
word. For this example, the two queries are ??
??(in?view)? and ???(feel)?. Each query 
gets one sentence set respectively. Part of the 
two result sentence sets are shown in figure 2 
and figure 3: 
51
Figure 2. Sentence Set 1 
Figure 3. Sentence Set 2 
From the above two sentence sets, we can
find that there is some noisy information in the 
sentences. In order to extend the correspondent
words of the slot word, it is not enough that we 
just use the position information or POS tagging
information of the slot word. Even if we extract
these words, many of them can?t be found in the
dictionary because they are not simple words.
Benefiting from the idea of (Lin and Pantel, 
2001), we use a dependency parser to determine
the correspondent extended words. 
4.2 Dependency Parser 
In this paper, we use a dependency parser (Ma et 
al., 2004) to extract the candidate slot word. For 
example, the dependency parsing result of the 
phrase of ?????? is shown in figure 4. 
Figure 4. Dependency parsing result 
The arcs in the figure represent dependency
relationships. The direction of an arc is from the 
head to the modifier in the relationship. Labels 
associated with the arcs represent types of de-
pendency relations. Table 2 lists a subset of the
dependency relations in the HIT-IRLab depend-
ency parser2.
Table 2. A subset of the dependency relations 
Relation Description
ATT ????(attribute)
HED ??(head)
SBJ ??(subject)
ADV ????(adverbial)
VOB ????(verb-object)
???????????????????
??????????????????
??????????????
,7 ????????????"
??????????????????
2 More information about the dependency parser can be got
from http://ir.hit.edu.cn/cuphelp.htm
4.3 Extracting the extended words 
We just use a very simple method to get the ex-
tended words from the parsed sentences. At first, 
we record the relations of the original parsed 
phrasal examples. And then we use these rela-
tions to matched similar part in the candidate 
parsed sentence except slot word. And we omit
these unseen relations and content words which
don?t appear in the original parsed phrasal ex-
amples. Then we can get the extended words. 
????????????
?????????
???????????????
??????????
???????????B720 ????
Figure 5. Dependency parsing result 
Figure 5 shows the dependency parsing result 
of the phrase of ???????????(In for-
eign capital fund manager view). We can easily
find that the extended word of the slot word
?? ?(I) is ??? ?(manager). Two extended
word sets can be extracted from two sentence
sets. Then we map each word to their semantic
code to get two semantic code sets. Intersection
operation is conducted on these two semantic
code sets to obtain their intersection set. Finally, 
we use the semantic code set instead of the slot 
word to generate the paraphrase template.
4.4 Some tricks 
Because the precision of the current dependency
parser on Chinese is not very high, we just ex-
tract a part of the candidate sentences to parse. 
There are three patterns to segment the long 
candidate sentences according to position of slot 
word in paraphrase examples. They are called
FRONT, MIDDLE and BACK. Here we use an
example to illustrate it as shown in table 3: 
Table 3 Examples of sentence segmentation
Pattern Origin Phrase Segment examples
FRONT (SW)?? ????????
?????
MIDDLE ?(SW)?? ????????
????????
????
The bold section in the sentence will be ex-
tracted to parse. Pattern type can be decided by 
52
the position relation between slot word and con-
text words. And these patterns can reduce the 
relative error rate of the dependency parser. That 
is to say, if the original phrase is parsed wrongly, 
the extracted segments may be parsed wrongly 
with the similar error. But according to our 
method, this kind of parser error has little influ-
ence on the final extracting result. 
5 Experiments and Discussions 
5.1 Setting 
We extract about 510 valid paraphrase examples 
from a Chinese paraphrase corpus (Li et al, 
2004). For simplicity, we just select those 
phrasal paraphrase examples which own same 
word. And we stipulate only content word can 
be as slot word. We just use four seed phrasal 
paraphrases as the original paraphrases in this 
paper. And the generalized paraphrase templates 
represented by semantic codes of the fifth layer 
in Cinlin (EV) are also shown in the Table 4: 
Table 4: Examples of the generalized template 
Origin 
Phrases 
Generalized Paraphrase  
templates 
??? [Aa01A01=,Aa01A05=,   
Aa01C03=,Aa02A01=,  ?]??
1 ? ? ?
?
?[Aa01A01=,Aa01A05=, 
  Aa01C03=,Aa02A01=,...  ]??
??? ? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]?2 ??? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]??
? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,? ]??
3 ? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,?  ]??
? ? ?
? ? ?
??
[Aa03A01=,Ac03A01=, 
Ba05A10#,Bb02A01=,?]???
???4
? ? ?
???
[Aa03A01=,Ac03A01=,Ba05A10#,
Bb02A01=,?]?????
5.2 Evaluation on Templates 
The goal of the evaluations is to confirm how 
reasonable this kind of representation method of 
paraphrase templates is and how well the tem-
plate is. We evaluated the generalized para-
phrase template in three ways. They are listed in 
the following three categories: 1) Reasonability; 
2) Precision; 3) Coverage. 
1) Reasonability 
The reasonability of a paraphrase template aims 
to measure the reasonable extent of the presenta-
tion method with multiple semantic codes. For 
example, if we use POS to generalize a para-
phrase template, its reasonability is very lower; 
that is to say, POS is not suitable to represent 
paraphrase template in some extent.  
We use an existing search engine to calcu-
late the reasonability of every paraphrase tem-
plate. Firstly, we instantiate all paraphrase 
examples from a template. Then all these exam-
ples are as the queries of the search engine. If 
two phrases in one paraphrase can be matched 
completely from the search engine, it also means 
that one or more examples are found on the Web 
via search engine, we then consider this para-
phrase is reasonable. Using this method we can 
get the approximate evaluation of all the exam-
ples. We define two metrics: 
Strict_Reasonability = S / N 
Loose_Reasonability = (L + S) / N 
Where N is the total number of the instanti-
ated examples; S is the number of the para-
phrase examples which two phrases in it can be 
matched all; L is the number of paraphrase ex-
amples only one phrase in a paraphrase can be 
matched.
2) Precision 
Every template is correspondent to the examples 
number with the semantic code of different layer 
in Cilin (EV) as shown in table 5.  
Table 5 Templates and their correspond exam-
ples number 
Instantiated examples 
number
Number of 
Paraphrase
templates Cilin3 Cilin4 Cilin5
1 2696 1815 478
2 13032 6354 3011
3 1057 587 177
4 3004 2229 429
From the above table, we can find that every 
template can instantiate many examples. If 
manually judging all of these examples will 
spend plenty of time. So we just sample part of 
all instantiate examples, 200 paraphrase exam-
ples for each template in this paper. For each 
53
phrase in a sample paraphrase example, it is as 
search query to get the first two matched sen-
tences. Evaluators would be asked whether it is
semantically okay to replace the query in the
sentence by the correspondent phrase in a para-
phrase. They were given only two options: Yes
or No. If search query have no matched results, 
we consider that this phrase cannot be replace 
with its correspondent paraphrase. According to 
the above regulations, we know that every para-
phrase examples correspondent to 4 sentences. If 
we sample n examples from a template, the pre-
cision of a paraphrase template can be calculated
by:
Precision = R / (4 * n) 
Where, R is the number of sentences which
is considered to be correct by the evaluator.
3) Coverage 
Evaluating directly the coverage of a paraphrase 
template is difficult because humans can?t enu-
merate all the words to be suitable to the tem-
plate. We use an approximate method to get the
coverage of a template. At first we use another 
search engine to get candidate sentences with 
similar method for generalization of a para-
phrase template. From these retrieved sentences
we can get many different words with the 
known generalized words because more than
85% of search results from different search en-
gine are different. Evaluators extract every sen-
tence which can be replaced with the 
correspondent phrase in a paraphrase and the
new sentences retain the origin meaning. We 
know each sentence is correspondent to a word. 
Then we define two metrics: 
Surface_Coverage = M / NS
Semantic_Coverage =
Map(K) / (Map(NS-M) + Map(K)) 
Where, NS is the number of all manually
tagged right words, M is the number of words 
which can be instantiated from a paraphrase
template, K is the number of all the words that 
generalized the template at the front. Map(X) is 
the total word number of the word clusters 
which derived from X word in the semantic dic-
tionary of Cilin(EV).
5.3 Result 
In order to exhibit the merit of our method, we 
conduct four groups of experiment. They are
POS-Tag, Cilin3, Cilin4 and Ciln5, respectively.
Especially, we just randomly select 400 words 
to satisfy the POS information.
Table 6: Experiment Results 
Reasonability
(%)
Coverage
(%)
St_R Lo_R Su_C Se_C
Preci-
sion
(%)
POS 10.50 17.00 90.00 ---- 11.75
Cilin3 45.57 84.50 27.55 38.71 45.75
Cilin4 46.89 84.54 23.87 44.48 64.13
Cilin5 46.24 83.12 20.39 39.47 69.88
Every value in table 6 is a average value of 
four values correspondent to four templates.
From the table we can find that the reasonability
of the Cilin-based representation template
changes little, and that of POS-based representa-
tion is very lower. We find that the longer origi-
nal phrases are, the lower the coverage of the
generalized template is. Although the average 
coverage of generalized template is relatively
low, we can draw a conclusion that using multi-
ple semantic codes to generalize phrasal para-
phrase examples is reasonable.
The column of the coverage shows that the 
coverage rates of Cilin-based templates are all
not more than 50%. And the POS-based tem-
plate has a very high coverage rate. And we 
know that the extended information is not
enough only depending on one search engine. 
We will combine several different search en-
gines with together to solve this problem in the 
future work. 
1.0 1.5 2.0 2.5 3.0 3.5 4.0
0
10
20
30
40
50
60
70
80
90
100
 strict_Reasonability  loose_Reasonability
 surface_Coverage  semantic_Coverage
 Precision
Va
lu
es
 o
f P
er
ce
nt
Different Template Representation Method
Figure 6. Experimental Results
The numbers from one to four on the X-axis
are correspondent to POS, Cilin3, Cilin4 and 
Cilin5 in figure 6. We can see the features
clearly of different representation methods of 
template from the figure 6. We can find that
54
Cilin5-based template has the highest precision, 
but its coverage is lower. And Cilin3-based 
template has opposite feature. This is because 
that one semantic code of Cilin3 includes more 
words than that of Cilin5. At the same time, 
more words bring more redundant information. 
And Cilin4-based template has a good tradeoff 
between coverage and precision. So we con-
clude that the semantic code of fourth layer in 
Cilin (EV) is more suitable to represent para-
phrase template.  
Some additional information can be extracted 
from the generalized template. Such as, the col-
location information between the slot word and 
the context words can be extract. For example, 
in the fourth template, we can get the informa-
tion about which words can be collocated with 
??(Jin)?.
Although this kind of representation of para-
phrase template has a good performance, it is 
weak for those words or structures that don?t 
exist in dictionary. Also, this method is not suit-
able to the named entities representation. 
6 Conclusion
In this paper, a novel method for automated 
generalization of paraphrase examples is pro-
posed. This method is not dependent on the tra-
ditional limited texts instead it is based on the 
richness of the Web. It uses the multiple seman-
tic codes to generalize a paraphrase example 
combing a semantic dictionary (Cilin (EV)). The 
experimental results proved that this representa-
tion method is reasonable and the generalized 
templates have a good precision and coverage.  
But this is just the beginning of the para-
phrase examples generalization. And we sim-
plify the problem in some aspects, such as we 
limited the number of the slot word in a para-
phrase example, and we stipulate only the same 
word can be slot word. Also, we find that our 
templates are weak for those words or structures 
that don?t exist in dictionary. Some methods in 
information extraction about named entities 
generalization can be used for reference in the 
future. Moreover, how to combine the semantic 
code with other representation forms together is 
also an interesting work. 
References
[1] Chris Quirk, Chris Brockett, and William Dolan. 
Monolingual Machine Translation for Para-
phrase Generation. editors, Dekang Lin and 
Dekai Wu, In Proceedings of EMNLP 2004, 
Barcelona, pages 142-149  
[2] Dekang Lin and Patrick Pantel. 2001. Discovery 
of Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-360 
[3] Dekang Lin and Patrick Pantel. Discovery of 
inference rules for question answering. Natural 
Language Engineering, 1, 2001.  
[4] E. Agichtein, S. Lawrence, and L. Gravano. 
Learning search engine specific query transfor-
mations for question answering. In Proceedings 
of the 10th International World-Wide Web Con-
ference (WWW10), 2001 
[5] Fabio Rinaldi, James Dowdall, Kaarel Kalju-
rand, Michael Hess, Diego Molla. 2003. Ex-
ploiting Paraphrases in a Question Answering 
System. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[6] Florence Duclaye France. Learning paraphrases 
to improve a question-answering system. In 
EACL Natural Language Processing for Ques-
tion Answering, 2003 
[7] Graeme Hirst. Paraphrasing Paraphrased. In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003 
[8] Hal Daum? III and Daniel Marcu. Acquiring 
paraphrase templates from document/abstract 
pairs. In NL Seminar in ISI, 2003 
[9] Hua Wu, Ming Zhou. Optimizing Synonym 
Extraction Using Monolingual and Bilingual 
Resources. In Proceedings of the Second Inter-
national Workshop on Paraphrasing, 2003 
[10] Hua Wu, Ming Zhou. Synonymous Collocation 
Extraction Using Translation Information. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics, 
2003 
[11] Jinshan Ma, Yu Zhang, Ting Liu, and Sheng Li. 
A Statistical Dependency Parser of Chinese un-
der Small Training Data. Workshop: Beyond 
shallow analyses - Formalisms and statistical 
modeling for deep analyses, IJCNLP-04, 4 2004. 
[12] Noriko Tomuro. 2003. Interrogative Reformula-
tion Patterns and Acquisition of Question Para-
phrases. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[13] Oren Glickman and Ido Dagan. Identifying 
lexical paraphrases from a single corpus: A case 
study for verbs. In Proceedings of Recent Ad-
vantages in Natural Language Processing, Sep-
tember 2003 
55
[14] Regina Barzilay and Kathleen McKeown. Ex-
tracting paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL, Toulouse, 2001 
[15] Regina Barzilay and Lillian Lee. Learning to 
Paraphrase: An Unsupervised Approach Using 
Multiple-Sequence Alignment. In Proceedings 
of HLT-NAACL 2003, pages 16-23  
[16] Regina Barzilay, Noemie Elhadad, Kathleen R. 
McKeown. 2003. Inferring Strategies for Sen-
tence Ordering in Multidocument News Sum-
marization. The Second International Workshop 
on Paraphrasing: Paraphrase Acquisition and 
Applications 
[17] Weigang Li, Ting Liu, Sheng Li. Combining 
Sentence Length with Location Information to 
Align Monolingual Parallel Texts. AIRS, 2004, 
pages 71-77 
[18] Yusuke Shinyama and Satoshi Sekine. Para-
phrase acquisition for information extraction. 
editors, Kentaro Inui and Ulf Hermjakob, In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003, pages 65-71 
[19] Yusuke Shinyama, Satoshi Sekine, Kiyoshi 
Sudo, and Ralph Grishman. Automatic para-
phrase acquisition from news articles, In Pro-
ceedings of Human Language Technology 
Conference (HLT2002), San Diego, USA, Mar. 
15, 2002 
[20] Zhang Yujie, Kazuhide Yamamoto. Automatic 
Paraphrasing of Chinese Utterances. Journal of 
Chinese Information Processing. Vol. 117 No. 
16: 31-38(Chinese) 
56
Fast Computing Grammar-driven Convolution Tree Kernel for
Semantic Role Labeling
Wanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li1
1School of Computer Science and Technology
Harbin Institute of Technology, China 150001
{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn
2Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{mzhang,aaiti}@i2r.a-star.edu.sg
3School of Computing
National University of Singapore, Singapore 117543
tancl@comp.nus.edu.sg
Abstract
Grammar-driven convolution tree kernel
(GTK) has shown promising results for se-
mantic role labeling (SRL). However, the
time complexity of computing the GTK is
exponential in theory. In order to speed
up the computing process, we design two
fast grammar-driven convolution tree kernel
(FGTK) algorithms, which can compute the
GTK in polynomial time. Experimental re-
sults on the CoNLL-2005 SRL data show
that our two FGTK algorithms are much
faster than the GTK.
1 Introduction
Given a sentence, the task of semantic role labeling
(SRL) is to analyze the propositions expressed by
some target verbs or nouns and some constituents
of the sentence. In previous work, data-driven tech-
niques, including feature-based and kernel-based
learning methods, have been extensively studied for
SRL (Carreras and Ma`rquez, 2005).
Although feature-based methods are regarded as
the state-of-the-art methods and achieve much suc-
cess in SRL, kernel-based methods are more effec-
tive in capturing structured features than feature-
based methods. In the meanwhile, the syntactic
structure features hidden in a parse tree have been
suggested as an important feature for SRL and need
to be further explored in SRL (Gildea and Palmer,
2002; Punyakanok et al, 2005). Moschitti (2004)
?The work was mainly done when the author was a visiting
student at I2R
and Che et al (2006) are two reported work to use
convolution tree kernel (TK) methods (Collins and
Duffy, 2001) for SRL and has shown promising re-
sults. However, as a general learning algorithm, the
TK only carries out hard matching between two sub-
trees without considering any linguistic knowledge
in kernel design. To solve the above issue, Zhang
et al (2007) proposed a grammar-driven convolu-
tion tree kernel (GTK) for SRL. The GTK can uti-
lize more grammatical structure features via two
grammar-driven approximate matching mechanisms
over substructures and nodes. Experimental results
show that the GTK significantly outperforms the
TK (Zhang et al, 2007). Theoretically, the GTK
method is applicable to any problem that uses syn-
tax structure features and can be solved by the TK
methods, such as parsing, relation extraction, and so
on. In this paper, we use SRL as an application to
test our proposed algorithms.
Although the GTK shows promising results for
SRL, one big issue for the kernel is that it needs ex-
ponential time to compute the kernel function since
it need to explicitly list all the possible variations
of two sub-trees in kernel calculation (Zhang et al,
2007). Therefore, this method only works efficiently
on such kinds of datasets where there are not too
many optional nodes in production rule set. In order
to solve this computation issue, we propose two fast
algorithms to compute the GTK in polynomial time.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the GTK. In Section 3,
we present our two fast algorithms for computing
the GTK. The experimental results are shown in Sec-
tion 4. Finally, we conclude our work in Section 5.
781
2 Grammar-driven Convolution Tree
Kernel
The GTK features with two grammar-driven ap-
proximate matching mechanisms over substructures
and nodes.
2.1 Grammar-driven Approximate Matching
Grammar-driven Approximate Substructure
Matching: the TK requires exact matching between
two phrase structures. For example, the two phrase
structures ?NP?DT JJ NN? (NP?a red car) and
?NP?DT NN? (NP?a car) are not identical, thus
they contribute nothing to the conventional kernel
although they share core syntactic structure property
and therefore should play the same semantic role
given a predicate. Zhang et al (2007) introduces
the concept of optional node to capture this phe-
nomenon. For example, in the production rule
?NP?DT [JJ] NP?, where [JJ] denotes an optional
node. Based on the concept of optional node, the
grammar-driven approximate substructure matching
mechanism is formulated as follows:
M(r1, r2) =
?
i,j
(IT (T ir1 , T jr2)? ?
ai+bj
1 ) (1)
where r1 is a production rule, representing a two-
layer sub-tree, and likewise for r2. T ir1 is the ith vari-
ation of the sub-tree r1 by removing one ore more
optional nodes, and likewise for T jr2 . IT (?, ?) is a bi-
nary function that is 1 iff the two sub-trees are iden-
tical and zero otherwise. ?1 (0 ? ?1 ? 1) is a small
penalty to penalize optional nodes. ai and bj stand
for the numbers of occurrence of removed optional
nodes in subtrees T ir1 and T jr2 , respectively.
M(r1, r2) returns the similarity (i.e., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees.
Grammar-driven Approximate Node Match-
ing: the TK needs an exact matching between two
nodes. But, some similar POSs may represent simi-
lar roles, such as NN (dog) and NNS (dogs). Zhang
et al (2007) define some equivalent nodes that can
match each other with a small penalty ?2 (0 ? ?2 ?
1). This case is called node feature mutation. The
approximate node matching can be formulated as:
M(f1, f2) =
?
i,j
(If (f i1, f j2 )? ?ai+bj2 ) (2)
where f1 is a node feature, f i1 is the ith mutation of
f1 and ai is 0 iff f i1 and f1 are identical and 1 oth-
erwise, and likewise for f2 and bj . If (?, ?) is a func-
tion that is 1 iff the two features are identical and
zero otherwise. Eq. (2) sums over all combinations
of feature mutations as the node feature similarity.
2.2 The GTK
Given these two approximate matching mecha-
nisms, the GTK is defined by beginning with the
feature vector representation of a parse tree T as:
??(T ) = (#subtree1(T ), . . . ,#subtreen(T ))
where #subtreei(T ) is the occurrence number of
the ith sub-tree type (subtreei) in T . Now the GTKis defined as follows:
KG(T1, T2) = ???(T1),??(T2)?
=?i #subtreei(T1) ?#subtreei(T2)=?i((
?
n1?N1 I
?
subtreei(n1))
? (?n2?N2 I
?
subtreei(n2)))
=?n1?N1
?
n2?N2 ?
?(n1, n2)
(3)
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively. I ?subtreei(n) is a function that
is ?a1 ??b2 iff there is a subtreei rooted at node n and
zero otherwise, where a and b are the numbers of
removed optional nodes and mutated node features,
respectively. ??(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
??(n1, n2) =
?
i
I ?subtreei(n1) ? I ?subtreei(n2) (4)
??(n1, n2) can be further computed by the follow-
ing recursive rules:
R-A: if n1 and n2 are pre-terminals, then:
??(n1, n2) = ??M(f1, f2) (5)
where f1 and f2 are features of nodes n1 and n2
respectively, and M(f1, f2) is defined in Eq. (2),
which can be computed in linear time O(n), where
n is the number of feature mutations.
R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-trees
of depth one rooted at n1 and n2 (denoted by Tn1
782
and Tn2 respectively) by removing different optionalnodes, then:
??(n1, n2) = ??
?
i,j IT (T in1 , T jn2)? ?
ai+bj
1
??nc(n1,i)k=1 (1 + ??(ch(n1, i, k), ch(n2, j, k)))
(6)
where T in1 , T jn2 , IT (?, ?), ai and bj have been ex-
plained in Eq. (1). nc(n1, i) returns the number
of children of n1 in its ith subtree variation T in1 .
ch(n1, i, k) is the kth child of node n1 in its ith vari-
ation subtree T in1 , and likewise for ch(n2, j, k). ?
(0 < ? < 1) is the decay factor.
R-C: else ??(n1, n2) = 0
3 Fast Computation of the GTK
Clearly, directly computing Eq. (6) requires expo-
nential time, since it needs to sum up all possible
variations of the sub-trees with and without optional
nodes. For example, supposing n1 = ?A?a [b] c
[d]?, n2 = ?A?a b c?. To compute the Eq. (6), we
have to list all possible variations of n1 and n2?s sub-
trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?a
c?; n2: ?A?a b c?. Unfortunately, Zhang et al
(2007) did not give any theoretical solution for the
issue of exponential computing time. In this paper,
we propose two algorithms to calculate it in polyno-
mial time. Firstly, we recast the issue of computing
Eq. (6) as a problem of finding common sub-trees
with and without optional nodes between two sub-
trees. Following this idea, we rewrite Eq. (6) as:
??(n1, n2) = ?? (1 +
lm?
p=lx
?p(cn1 , cn2)) (7)
where cn1 and cn2 are the child node sequences of
n1 and n2, ?p evaluates the number of common
sub-trees with exactly p children (at least including
all non-optional nodes) rooted at n1 and n2, lx =
max{np(cn1), np(cn2)} and np(?) is the number of
non-optional nodes, lm = min{l(cn1), l(cn2)}and
l(?) returns the number of children.
Now let?s study how to calculate ?p(cn1 , cn2) us-
ing dynamic programming algorithms. Here, we
present two dynamic programming algorithms to
compute it in polynomial time.
3.1 Fast Grammar-driven Convolution Tree
Kernel I (FGTK-I)
Our FGTK-I algorithm is motivated by the string
subsequence kernel (SSK) (Lodhi et al, 2002).
Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:
??0(s, t) = 1, for all s, t,
??p(s, t) = 0, ifmin(|s|, |t|) < p, (8)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)
??p(sx, t) = ????p(sx, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?|t|?j+2)),(10)
p = 1, . . . , n? 1,
?p(sx, t) = ?p(s, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?2)). (11)
where ??p is an auxiliary function since it is only
the interior gaps in the subsequences that are penal-
ized; ? is a decay factor only used in the SSK for
weighting each extra length unit. Lodhi et al (2002)
explained the correctness of the recursion defined
above.
Compared with the SSK kernel, the GTK has
three different features:
f1: In the GTK, only optional nodes can be
skipped while the SSK kernel allows any node skip-
ping;
f2: The GTK penalizes skipped optional nodes
only (including both interior and exterior skipped
nodes) while the SSK kernel weights the length of
subsequences (all interior skipped nodes are counted
in, but exterior nodes are ignored);
f3: The GTK needs to further calculate the num-
ber of common sub-trees rooted at each two match-
ing node pair x and t[j].
To reflect the three considerations, we modify the
SSK kernel as follows to calculate the GTK:
?0(s, t) = opt(s)? opt(t)? ?|s|+|t|1 , for all s, t, (12)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)
?p(sx, t) = ?1 ??p(sx, t)? opt(x)
+
?
j:tj=x
(?p?1(s, t[1 : j ? 1])? ?|t|?j (14)
?opt(t[j + 1 : |t|])???(x, t[j])).
where opt(w) is a binary function, which is 0 if
non-optional nodes are found in the node sequence
w and 1 otherwise (f1); ?1 is the penalty to penalize
skipped optional nodes and the power of ?1 is the
number of skipped optional nodes (f2); ??(x, t[j])
is defined in Eq. (7) (f3). Now let us compare
783
the FGTK-I and SSK kernel algorithms. Based on
Eqs. (8), (9), (10) and (11), we introduce the opt(?)
function and the penalty ?1 into Eqs. (12), (13) and
(14), respectively. opt(?) is to ensure that in the
GTK only optional nodes are allowed to be skipped.
And only those skipped optional nodes are penal-
ized with ?1. Please note that Eqs. (10) and (11)
are merged into Eq. (14) because of the different
meaning of ? and ?1. From Eq. (8), we can see
that the current path in the recursive call will stop
and its value becomes zero once non-optional node
is skipped (when opt(w) = 0).
Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =
?A?a b c? to exemplify how the FGTK-I algorithm
works. In Eq. (14)?s vocabulary, we have s = ?a [b]
c?, t = ?a b c?, x = ?[d]?, opt(x) = opt([d]) = 1,
p = 3. Then according to Eq (14), ?p(cn1 , cn2) can
be calculated recursively as Eq. (15) (Please refer to
the next page).
Finally, we have ?p(cn1 , cn2) = ?1 ???(a, a)?
??(b, b)???(c, c)
By means of the above algorithm, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |2) (Lodhi et
al., 2002). This means that the worst case complex-
ity of the FGTK-I is O(p?3|N1| ? |N2|2), where ? is
the maximum branching factor of the two trees.
3.2 Fast Grammar-driven Convolution Tree
Kernel II (FGTK-II)
Our FGTK-II algorithm is motivated by the partial
trees (PTs) kernel (Moschitti, 2006). The PT kernel
algorithm uses the following recursive formulas to
evaluate ?p(cn1 , cn2):
?p(cn1 , cn2) =
|cn1 |?
i=1
|cn2 |?
j=1
??p(cn1 [1 : i], cn2 [1 : j]) (16)
where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively. Given two child node sequences
s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b are
the last children), the PT kernel computes ??p(?, ?) as
follows:
??p(s1a, s2b) =
{
?2??(a, b)Dp(|s1|, |s2|) if a = b
0 else (17)
where ??(a, b) is defined in Eq. (7) and Dp is recur-
sively defined as follows:
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?Dp(k, l ? 1) + ?Dp(k ? 1, l) (18)
??2Dp(k ? 1, l ? 1)
D1(k, l) = 1, for all k, l (19)
where ? used in Eqs. (17) and (18) is a factor to
penalize the length of the child sequences.
Compared with the PT kernel, the GTK has two
different features which are the same as f1 and f2
when defining the FGTK-I.
To reflect the two considerations, based on the PT
kernel algorithm, we define another fast algorithm
of computing the GTK as follows:
?p(cn1 , cn2 ) =
? |cn1 |
i=1
? |cn2 |
j=1 ??p(cn1 [1 : i], cn2 [1 : j])
?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])
??|cn1 |?i+|cn2 |?j1
(20)
??p(s1a, s2b) =
{ ??(a, b)Dp(|s1|, |s2|) if a = b
0 else (21)
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?1Dp(k, l ? 1)? opt(s2[l]) (22)
+?1Dp(k ? 1, l)? opt(s1[k])
??21Dp(k ? 1, l ? 1)? opt(s1[k])? opt(s2[l])
D1(k, l) = ?k+l1 ? opt(s1[1 : k])? opt(s2[1 : l]), (23)
for all k, l
??p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)
where opt(w) and ?1 are the same as them in the
FGTK-I.
Now let us compare the FGTK-II and the PT al-
gorithms. Based on Eqs. (16), (18) and (19), we in-
troduce the opt(?) function and the penalty ?1 into
Eqs. (20), (22) and (23), respectively. This is to
ensure that in the GTK only optional nodes are al-
lowed to be skipped and only those skipped optional
nodes are penalized. In addition, compared with
Eq. (17), the penalty ?2 is removed in Eq. (21) in
view that our kernel only penalizes skipped nodes.
Moreover, Eq. (24) is only for fast computing. Fi-
nally, the same as the FGTK-I, in the FGTK-II the
current path in a recursive call will stop and its value
becomes zero once non-optional node is skipped
(when opt(w) = 0). Here, we still can use an ex-
ample to derivate the process of the algorithm step
by step as that for FGTK-I algorithm. Due to space
limitation, here, we do not illustrate it in detail.
By means of the above algorithms, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |) (Moschitti,
784
?p(cn1 , cn2 ) = ?p(?a [b] c [d]? , ?a b c?)
= ?1 ??p(?a [b] c?, ?a b c?) + 0 //Since x * t, the second term is 0
= ?1 ? (0 + ?p?1(?a [b]?, ?a b?)? ?3?31 ???(c, c)) //Since opt(?c?) = 0, the first term is 0
= ?1 ???(c, c)? (0 + ?p?2(?a?, ?a b?)? ?2?21 ???(b, b)) //Since p? 1 > |?a?|,?p?2(?a?, ?a b?) = 0
= ?1 ???(c, c)? (0 + ??(a, a)???(b, b)) //?p?2(?a?, ?a?) = ??(a, a)
(15)
2006). This means that the worst complexity of the
FGTK-II is O(p?2|N1| ? |N2|). It is faster than the
FGTK-I?s O(p?3|N1| ? |N2|2) in theory. Please note
that the average ? in natural language parse trees is
very small and the overall complexity of the FGTKs
can be further reduced by avoiding the computation
of node pairs with different labels (Moschitti, 2006).
4 Experiments
4.1 Experimental Setting
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Ma`rquez, 2005) as our experi-
mental corpus.
Classifier: SVM (Vapnik, 1998) is selected as our
classifier. In the FGTKs implementation, we mod-
ified the binary Tree Kernels in SVM-Light Tool
(SVM-Light-TK) (Moschitti, 2006) to a grammar-
driven one that encodes the GTK and the two fast dy-
namic algorithms inside the well-known SVM-Light
tool (Joachims, 2002). The parameters are the same
as Zhang et al (2007).
Kernel Setup: We use Che et al (2006)?s hybrid
convolution tree kernel (the best-reported method
for kernel-based SRL) as our baseline kernel. It is
defined as Khybrid = ?Kpath + (1 ? ?)Kcs (0 ?
? ? 1)1. Here, we use the GTK to compute the
Kpath and the Kcs.
In the training data (WSJ sections 02-21), we get
4,734 production rules which appear at least 5 times.
Finally, we use 1,404 rules with optional nodes for
the approximate structure matching. For the node
approximate matching, we use the same equivalent
node sets as Zhang et al (2007).
4.2 Experimental Results
We use 30,000 instances (a subset of the entire train-
ing set) as our training set to compare the different
kernel computing algorithms 2. All experiments are
1Kpath and Kcs are two TKs to describe predicate-
argument link features and argument syntactic structure fea-
tures, respectively. For details, please refer to (Che et al, 2006).
2There are about 450,000 identification instances are ex-
tracted from training data.
conducted on a PC with CPU 2.8GH and memory
1G. Fig. 1 reports the experimental results, where
training curves (time vs. # of instances) of five
kernels are illustrated, namely the TK, the FGTK-
I, the FGTK-II, the GTK and a polynomial kernel
(only for reference). It clearly demonstrates that our
FGTKs are faster than the GTK algorithm as ex-
pected. However, the improvement seems not so
significant. This is not surprising as there are only
30.4% rules (1,404 out of 4,734)3 that have optional
nodes and most of them have only one optional
node4. Therefore, in this case, it is not time con-
suming to list all the possible sub-tree variations and
sum them up. Let us study this issue from computa-
tional complexity viewpoint. Suppose all rules have
exactly one optional node. This means each rule can
only generate two variations. Therefore computing
Eq. (6) is only 4 times (2*2) slower than the GTK
in this case. In other words, we can say that given
the constraint that there is only one optional node
in one rule, the time complexity of the GTK is also
O(|N1| ? |N2|) 5, where N1 and N2 are the numbers
of tree nodes, the same as the TK.
12000
6000
8000
10000
Train
ing T
ime (
S) GTKFGTK-I
2000
4000Tra
ining
 Time
 (S)
FGTK-IITKPoly
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 1: Training time comparison among different
kernels with rule set having less optional nodes.
Moreover, Fig 1 shows that the FGTK-II is faster
than the FGTK-I. This is reasonable since as dis-
3The percentage is even smaller if we consider all produc-
tion (it becomes 14.4% (1,404 out of 9,700)).
4There are 1.6 optional nodes in each rule averagely.
5Indeed it is O(4 ? |N1| ? |N2|). The parameter 4 is omitted
when discussing time complexity.
785
cussed in Subsection 3.2, the FGTK-I?s time com-
plexity is O(p?3|N1| ? |N2|2) while the FGTK-II?s is
O(p?2|N1| ? |N2|).
40000
45000
20000
25000
30000
35000
Train
ing T
ime (
S) GTKFGTK-I
0
5000
10000
15000Trai
ning 
Time
 (S)
FGTK-IITKPoly
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 2: Training time comparison among different
kernels with rule set having more optional nodes.
To further verify the efficiency of our proposed
algorithm, we conduct another experiment. Here we
use the same setting as that in Fig 1 except that we
randomly add more optional nodes in more produc-
tion rules. Table 1 reports the statistics on the two
rule set. Similar to Fig 1, Fig 2 compares the train-
ing time of different algorithms. We can see that
Fig 2 convincingly justify that our algorithms are
much faster than the GTK when the experimental
data has more optional nodes and rules.
Table 1: The rule set comparison between two ex-
periments.
# rules # rule with at
least optional
nodes
# op-
tional
nodes
# average op-
tional nodes per
rule
Exp1 4,734 1,404 2,242 1.6
Exp2 4,734 4,520 10,451 2.3
5 Conclusion
The GTK is a generalization of the TK, which can
capture more linguistic grammar knowledge into the
later and thereby achieve better performance. How-
ever, a biggest issue for the GTK is its comput-
ing speed, which needs exponential time in the-
ory. Therefore, in this paper we design two fast
grammar-driven convolution tree kennel (FGTK-I
and II) algorithms which can compute the GTK in
polynomial time. The experimental results show that
the FGTKs are much faster than the GTK when data
set has more optional nodes. We conclude that our
fast algorithms enable the GTK kernel to easily scale
to larger dataset. Besides the GTK, the idea of our
fast algorithms can be easily used into other similar
problems.
To further our study, we will use the FGTK algo-
rithms for other natural language processing prob-
lems, such as word sense disambiguation, syntactic
parsing, and so on.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS-
2001.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL-2002, pages 239?246.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL-2004, pages 335?342.
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proceedings of the HHLT-NAACL-2006, June.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In Proceedings of IJCAI-2005.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proceedings of ACL-2007, pages
200?207.
786
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 457?464,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 An Equivalent Pseudoword Solution to Chinese  
Word Sense Disambiguation 
 
Zhimao Lu+    Haifeng Wang++    Jianmin Yao+++    Ting Liu+    Sheng Li+
+ Information Retrieval Laboratory, School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin, 150001, China 
{lzm, tliu, lisheng}@ir-lab.org 
++ Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No. 1, East Chang An Ave., Beijing, 100738, China 
wanghaifeng@rdc.toshiba.com.cn 
+++ School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
jyao@suda.edu.cn 
 
  
 
Abstract 
This paper presents a new approach 
based on Equivalent Pseudowords (EPs) 
to tackle Word Sense Disambiguation 
(WSD) in Chinese language. EPs are par-
ticular artificial ambiguous words, which 
can be used to realize unsupervised WSD. 
A Bayesian classifier is implemented to 
test the efficacy of the EP solution on 
Senseval-3 Chinese test set. The per-
formance is better than state-of-the-art 
results with an average F-measure of 0.80. 
The experiment verifies the value of EP 
for unsupervised WSD. 
1 Introduction 
Word sense disambiguation (WSD) has been a 
hot topic in natural language processing, which is 
to determine the sense of an ambiguous word in 
a specific context. It is an important technique 
for applications such as information retrieval, 
text mining, machine translation, text classifica-
tion, automatic text summarization, and so on. 
Statistical solutions to WSD acquire linguistic 
knowledge from the training corpus using ma-
chine learning technologies, and apply the 
knowledge to disambiguation. The first statistical 
model of WSD was built by Brown et al (1991). 
Since then, most machine learning methods have 
been applied to WSD, including decision tree, 
Bayesian model, neural network, SVM, maxi-
mum entropy, genetic algorithms, and so on. For 
different learning methods, supervised methods 
usually achieve good performance at a cost of 
human tagging of training corpus. The precision 
improves with larger size of training corpus. 
Compared with supervised methods, unsuper-
vised methods do not require tagged corpus, but 
the precision is usually lower than that of the 
supervised methods. Thus, knowledge acquisi-
tion is critical to WSD methods.  
This paper proposes an unsupervised method 
based on equivalent pseudowords, which ac-
quires WSD knowledge from raw corpus. This 
method first determines equivalent pseudowords 
for each ambiguous word, and then uses the 
equivalent pseudowords to replace the ambigu-
ous word in the corpus. The advantage of this 
method is that it does not need parallel corpus or 
seed corpus for training. Thus, it can use a large-
scale monolingual corpus for training to solve 
the data-sparseness problem. Experimental re-
sults show that our unsupervised method per-
forms better than the supervised method. 
The remainder of the paper is organized as fol-
lows. Section 2 summarizes the related work. 
Section 3 describes the conception of Equivalent 
Pseudoword. Section 4 describes EP-based Un-
supervised WSD Method and the evaluation re-
sult. The last section concludes our approach. 
2 Related Work 
For supervised WSD methods,  a knowledge ac-
quisition bottleneck is to prepare the manually 
457
tagged corpus. Unsupervised method is an alter-
native, which often involves automatic genera-
tion of tagged corpus, bilingual corpus alignment, 
etc. The value of unsupervised methods lies in 
the knowledge acquisition solutions they adopt. 
2.1 Automatic Generation of Training Corpus 
Automatic corpus tagging is a solution to WSD, 
which generates large-scale corpus from a small 
seed corpus. This is a weakly supervised learning 
or semi-supervised learning method. This rein-
forcement algorithm dates back to Gale et al 
(1992a). Their investigation was based on a 6-
word test set with 2 senses for each word. 
Yarowsky (1994 and 1995), Mihalcea and 
Moldovan (2000), and Mihalcea (2002) have 
made further research to obtain large corpus of 
higher quality from an initial seed corpus. A 
semi-supervised method proposed by Niu et al 
(2005) clustered untagged instances with tagged 
ones starting from a small seed corpus, which 
assumes that similar instances should have simi-
lar tags. Clustering was used instead of boot-
strapping and was proved more efficient.  
2.2 Method Based on Parallel Corpus 
Parallel corpus is a solution to the bottleneck of 
knowledge acquisition. Ide et al (2001 and 
2002), Ng et al (2003), and Diab (2003, 2004a, 
and 2004b) made research on the use of align-
ment for WSD.  
Diab and Resnik (2002) investigated the feasi-
bility of automatically annotating large amounts 
of data in parallel corpora using an unsupervised 
algorithm, making use of two languages simulta-
neously, only one of which has an available 
sense inventory. The results showed that word-
level translation correspondences are a valuable 
source of information for sense disambiguation. 
The method by Li and Li (2002) does not re-
quire parallel corpus. It avoids the alignment 
work and takes advantage of bilingual corpus. 
In short, technology of automatic corpus tag-
ging is based on the manually labeled corpus. 
That is to say, it still need human intervention 
and is not a completely unsupervised method. 
Large-scale parallel corpus; especially word-
aligned corpus is highly unobtainable, which has 
limited the WSD methods based on parallel cor-
pus.  
3 Equivalent Pseudoword 
This section describes how to obtain equivalent 
pseudowords without a seed corpus. 
Monosemous words are unambiguous priori 
knowledge. According to our statistics, they ac-
count for 86%~89% of the instances in a diction-
ary and 50% of the items in running corpus, they 
are potential knowledge source for WSD.  
A monosemous word is usually synonymous 
to some polysemous words. For example the 
words "?? , ?? , ?? ?? ?? ??, , , , 
??" has similar meaning as one of the senses 
of the ambiguous word "??", while "??, ?
?, ?? ?? ??, , ?? ?? ?? ??, , , , , 
?? ?? ?? ??, , , " are the same for "??". 
This is quite common in Chinese, which can be 
used as a knowledge source for WSD. 
3.1 Definition of Equivalent Pseudoword 
If the ambiguous words in the corpus are re-
placed with its synonymous monosemous word, 
then is it convenient to acquire knowledge from 
raw corpus? For example in table 1, the ambigu-
ous word "??" has three senses, whose syn-
onymous monosemous words are listed on the 
right column. These synonyms contain some in-
formation for disambiguation task. 
An artificial ambiguous word can be coined 
with the monosemous words in table 1. This 
process is similar to the use of general pseu-
dowords (Gale et al, 1992b; Gaustad, 2001; Na-
kov and Hearst, 2003), but has some essential 
differences. This artificial ambiguous word need 
to simulate the function of the real ambiguous 
word, and to acquire semantic knowledge as the 
real ambiguous word does. Thus, we call it an 
equivalent pseudoword (EP) for its equivalence 
with the real ambiguous word. It's apparent that 
the equivalent pseudoword has provided a new 
way to unsupervised WSD. 
S1 ??/??? 
S2 ??/??/??/??/????(ba3 wo4)
S3 ??/??/??/??/??
Table 1. Synonymous Monosemous Words for 
the Ambiguous Word "??" 
The equivalence of the EP with the real am-
biguous word is a kind of semantic synonym or 
similarity, which demands a maximum similarity 
between the two words. An ambiguous word has 
the same number of EPs as of senses. Each EP's 
sense maps to a sense of ambiguous word. 
The semantic equivalence demands further 
equivalence at each sense level. Every corre-
458
sponding sense should have the maximum simi-
larity, which is the strictest limit to the construc-
tion of an EP. 
The starting point of unsupervised WSD based 
on EP is that EP can substitute the original word 
for knowledge acquisition in model training. 
Every instance of each morpheme of the EP can 
be viewed as an instance of the ambiguous word, 
thus the training set can be enlarged easily. EP is 
a solution to data sparseness for lack of human 
tagging in WSD. 
3.2 Basic Assumption for EP-based WSD 
It is based on the following assumptions that EPs 
can substitute the original ambiguous word for 
knowledge acquisition in WSD model training. 
Assumption 1: Words of the same meaning 
play the same role in a language. The sense is an 
important attribute of a word. This plays as the 
basic assumption in this paper. 
Assumption 2: Words of the same meaning 
occur in similar context. This assumption is 
widely used in semantic analysis and plays as a 
basis for much related research. For example, 
some researchers cluster the contexts of ambigu-
ous words for WSD, which shows good perform-
ance (Schutze, 1998). 
Because an EP has a higher similarity with the 
ambiguous word in syntax and semantics, it is a 
useful knowledge source for WSD. 
3.3 Design and Construction of EPs 
Because of the special characteristics of EPs, it's 
more difficult to construct an EP than a general 
pseudo word. To ensure the maximum similarity 
between the EP and the original ambiguous word, 
the following principles should be followed. 
1) Every EP should map to one and only one 
original ambiguous word. 
2) The morphemes of an EP should map one 
by one to those of the original ambiguous word. 
3) The sense of the EP should be the same as 
the corresponding ambiguous word, or has the 
maximum similarity with the word. 
4) The morpheme of a pseudoword stands for 
a sense, while the sense should consist of one or 
more morphemes.  
5) The morpheme should be a monosemous 
word. 
The fourth principle above is the biggest dif-
ference between the EP and a general pseudo 
word. The sense of an EP is composed of one or 
several morphemes. This is a remarkable feature 
of the EP, which originates from its equivalent 
linguistic function with the original word. To 
construct the EP, it must be ensured that the 
sense of the EP maps to that of the original word. 
Usually, a candidate monosemous word for a 
morpheme stands for part of the linguistic func-
tion of the ambiguous word, thus we need to 
choose several morphemes to stand for one sense.  
The relatedness of the senses refers to the 
similarity of the contexts of the original ambigu-
ous word and its EP. The similarity between the 
words means that they serve as synonyms for 
each other. This principle demands that both se-
mantic and pragmatic information should be 
taken into account in choosing a morpheme word. 
3.4 Implementation of the EP-based Solution 
An appropriate machine-readable dictionary is 
needed for construction of the EPs. A Chinese 
thesaurus is adopted and revised to meet this de-
mand. 
Extended Version of TongYiCiCiLin 
To extend the TongYiCiCiLin (Cilin) to hold 
more words, several linguistic resources are 
adopted for manually adding new words. An ex-
tended version of the Cilin is achieved, which 
includes 77,343 items. 
A hierarchy of three levels is organized in the 
extended Cilin for all items. Each node in the 
lowest level, called a minor class, contains sev-
eral words of the same class. The words in one 
minor class are divided into several groups ac-
cording to their sense similarity and relatedness, 
and each group is further divided into several 
lines, which can be viewed as the fifth level of 
the thesaurus. The 5-level hierarchy of the ex-
tended Cilin is shown in figure 1. The lower the 
level is, the more specific the sense is. The fifth 
level often contains a few words or only one 
word, which is called an atom word group, an 
atom class or an atom node. The words in the 
same atom node hold the smallest semantic dis-
tance. 
From the root node to the leaf node, the sense 
is described more and more detailed, and the 
words in the same node are more and more re-
lated. Words in the same fifth level node have 
the same sense and linguistic function, which 
ensures that they can substitute for each other 
without leading to any change in the meaning of 
a sentence. 
 
 
459
 ?  ? 
?
?? ?? 
? ? ? ?
? ? ?
 
? ? ? ?
Level 1 
Level 2 
Level 3 
Level 4 
Level 5 
?  ? 
Figure 1. Organization of Cilin (extended) 
 
The extended version of extended Cilin is 
freely downloadable from the Internet and has 
been used by over 20 organizations in the world1. 
Construction of EPs 
According to the position of the ambiguous word, 
a proper word is selected as the morpheme of the 
EP. Almost every ambiguous word has its corre-
sponding EP constructed in this way. 
The first step is to decide the position of the 
ambiguous word starting from the leaf node of 
the tree structure. Words in the same leaf node 
are identical or similar in the linguistic function 
and word sense. Other words in the leaf node of 
the ambiguous word are called brother words of 
it. If there is a monosemous brother word, it can 
be taken as a candidate morpheme for the EP. If 
there does not exist such a brother word, trace to 
the fourth level. If there is still no monosemous 
brother word in the fourth level, trace to the third 
level. Because every node in the third level con-
tains many words, candidate morpheme for the 
ambiguous can usually be found. 
In most cases, candidate morphemes can be 
found at the fifth level. It is not often necessary 
to search to the fourth level, less to the third. Ac-
cording to our statistics, the extended Cilin con-
tains about monosemous words for 93% of the 
ambiguous words in the fifth level, and 97% in 
the fourth level. There are only 112 ambiguous 
words left, which account for the other 3% and 
mainly are functional words. Some of the 3% 
words are rarely used, which cannot be found in 
even a large corpus. And words that lead to se-
mantic misunderstanding are usually content 
words. In WSD research for English, only nouns, 
verbs, adjectives and adverbs are considered. 
                                                 
1 It is located at http://www.ir-lab.org/. 
From this aspect, the extended version of Cilin 
meets our demand for the construction of EPs. 
If many monosemous brother words are found 
in the fourth or third level, there are many candi-
date morphemes to choose from. A further selec-
tion is made based on calculation of sense simi-
larity. More similar brother words are chosen. 
Computing of EPs 
Generally, several morpheme words are needed 
for better construction of an EP. We assume that 
every morpheme word stands for a specific sense 
and does not influence each other. It is more 
complex to construct an EP than a common 
pseudo word, and the formulation and statistical 
information are also different. 
An EP is described as follows:  
 
iikiiii
k
k
WWWWS
WWWWS
WWWWS
L
MMMMMM
L
L
,,,:
,,,:
,,,:
321
22322212
11312111
2
1
 
WEP?????????? 
Where WEP is the EP word, Si is a sense of the 
ambiguous word, and Wik is a morpheme word of 
the EP. 
The statistical information of the EP is calcu-
lated as follows: 
1? stands for the frequency of the S)( iSC i : 
?=
k
iki WCSC )()(  
2? stands for the co-occurrence fre-
quency of S
),( fi WSC
i and the contextual word Wf : 
?=
k
fikfi WWCWSC ),(),(  
460
 Ambiguous word citation (Qin and Wang, 2005) Ours Ambiguous word 
citation (Qin and 
Wang, 2005) Ours 
??(ba3 wo4) 0.56 0.87 ??(mei2 you3) 0.75 0.68 
?(bao1) 0.59 0.75 ??(qi3 lai2) 0.82 0.54 
??(cai2 liao4) 0.67 0.79 ?(qian2) 0.75 0.62 
??(chong1 ji1) 0.62 0.69 ??(ri4 zi3) 0.75 0.68 
?(chuan1) 0.80 0.57 ?(shao3) 0.69 0.56 
??(di4 fang1) 0.65 0.65 ??(tu1 chu1) 0.82 0.86 
??(fen1 zi3) 0.91 0.81 ??(yan2 jiu1) 0.69 0.63 
??(yun4 dong4) 0.61 0.82 ??(huo2 dong4) 0.79 0.88 
?(lao3) 0.59 0.50 ?(zou3) 0.72 0.60 
?(lu4) 0.74 0.64 ?(zuo4) 0.90 0.73 
Average 0.72 0.69 Note: Average of the 20 words 
Table 2. The F-measure for the Supervised WSD 
 
4 EP-based Unsupervised WSD Method 
EP is a solution to the semantic knowledge ac-
quisition problem, and it does not limit the 
choice of statistical learning methods. All of the 
mathematical modeling methods can be applied 
to EP-based WSD methods. This section focuses 
on the application of the EP concept to WSD, 
and chooses Bayesian method for the classifier 
construction. 
4.1 A Sense Classifier Based on the Bayes-
ian Model 
Because the model acquires knowledge from the 
EPs but not from the original ambiguous word, 
the method introduced here does not need human 
tagging of training corpus. 
In the training stage for WSD, statistics of EPs 
and context words are obtained and stored in a 
database. Senseval-3 data set plus unsupervised 
learning method are adopted to investigate into 
the value of EP in WSD. To ensure the compara-
bility of experiment results, a Bayesian classifier 
is used in the experiments. 
Bayesian Classifier 
Although the Bayesian classifier is simple, it is 
quite efficient, and it shows good performance 
on WSD. 
The Bayesian classifier used in this paper is 
described in (1) 
???
?
???
?
+= ?
? ij
k
cv
kjkSi SvPSPwS )|(log)(logmaxarg)( (1)
Where wi is the ambiguous word,  is the 
occurrence probability of the sense S
)( kSP
k,  
is the conditional probability of the context word 
v
)|( kj SvP
j, and ci is the set of the context words. 
To simplify the experiment process, the Naive 
Bayesian modeling is adopted for the sense clas-
sifier. Feature selection and ensemble classifica-
tion are not applied, which is both to simplify the 
calculation and to prove the effect of EPs in 
WSD. 
Experiment Setup and Results  
The Senseval-3 Chinese ambiguous words are 
taken as the testing set, which includes 20 words, 
each with 2-8 senses. The data for the ambiguous 
words are divided into a training set and a testing 
set by a ratio of 2:1. There are 15-20 training 
instances for each sense of the words, and occurs 
by the same frequency in the training and test set. 
Supervised WSD is first implemented using 
the Bayesian model on the Senseval-3 data set. 
With a context window of (-10, +10), the open 
test results are shown in table 2. 
The F-measure in table 2 is defined in (2). 
RP
RP
F +
??= 2  (2) 
461
Where P and R refer to the precision and recall 
of the sense tagging respectively, which are cal-
culated as shown in (3) and (4) 
)tagged(
)correct(
C
C
P =  (3) 
)all(
)correct(
C
C
R =  (4) 
Where C(tagged) is the number of tagged in-
stances of senses, C(correct) is the number of 
correct tags, and C(all) is the number of tags in 
the gold standard set. Every sense of the am-
biguous word has a P value, a R value and a F 
value. The F value in table 2 is a weighted aver-
age of all the senses. 
In the EP-based unsupervised WSD experi-
ment, a 100M corpus (People's Daily for year 
1998) is used for the EP training instances. The 
Senseval-3 data is used for the test. In our ex-
periments, a context window of (-10, +10) is 
taken. The detailed results are shown in table 3. 
4.2 Experiment Analysis and Discussion 
Experiment Evaluation Method 
Two evaluation criteria are used in the experi-
ments, which are the F-measure and precision. 
Precision is a usual criterion in WSD perform-
ance analysis. Only in recent years, the precision, 
recall, and F-measure are all taken to evaluate 
the WSD performance. 
In this paper, we will only show the f-measure 
score because it is a combined score of precision 
and recall. 
Result Analysis on Bayesian Supervised WSD 
Experiment 
The experiment results in table 2 reveals that the 
results of supervised WSD and those of (Qin and 
Wang, 2005) are different. Although they are all 
based on the Bayesian model, Qin and Wang 
(2005) used an ensemble classifier. However, the 
difference of the average value is not remarkable. 
As introduced above, in the supervised WSD 
experiment, the various senses of the instances 
are evenly distributed. The lower bound as Gale 
et al (1992c) suggested should be very low and 
it is more difficult to disambiguate if there are 
more senses. The experiment verifies this reason-
ing, because the highest F-measure is less than 
90%, and the lowest is less than 60%, averaging 
about 70%. 
With the same number of senses and the same 
scale of training data, there is a big difference 
between the WSD results. This shows that other 
factors exist which influence the performance 
other than the number of senses and training data 
size. For example, the discriminability among the 
senses is an important factor. The WSD task be-
comes more difficult if the senses of the ambigu-
ous word are more similar to each other. 
Experiment Analysis of the EP-based WSD 
The EP-based unsupervised method takes the 
same open test set as the supervised method. The 
unsupervised method shows a better performance, 
with the highest F-measure score at 100%, low-
est at 59% and average at 80%. The results 
shows that EP is useful in unsupervised WSD. 
 
Sequence 
Number Ambiguous word F-measure
Sequence 
Number Ambiguous word 
F-measure 
(%) 
1 ??(ba3 wo4) 0.93 11 ??(mei2 you3) 1.00 
2 ?(bao1) 0.74 12 ??(qi3 lai2) 0.59 
3 ?(cai2 liao4) 0.80 13 ?(qian2) 0.71 
4 ??(chong1 ji1) 0.85 14 ??(ri4 zi3) 0.62 
5 ?(chuan1) 0.79 15 ?(shao3) 0.82 
6 ??(di4 fang1) 0.78 16 ??(tu1 chu1) 0.93 
7 ??(fen1 zi3) 0.94 17 ??(yan2 jiu1) 0.71 
8 ??(yun4 
dong4) 
0.94 18 ??(huo2 dong4) 0.89 
9 ?(lao3) 0.85 19 ?(zou3) 0.68 
10 ?(lu4) 0.81 20 ?(zuo4) 0.67 
Average 0.80 Note: Average of the 20 words 
Table 3. The Results for Unsupervised WSD based on EPs 
462
 
From the results in table 2 and table 3, it can 
be seen that 16 among the 20 ambiguous words 
show better WSD performance in unsupervised 
SWD than in supervised WSD, while only 2 of 
them shows similar results and 2 performs worse . 
The average F-measure of the unsupervised 
method is higher by more than 10%. The reason 
lies in the following aspects: 
1) Because there are several morpheme words 
for every sense of the word in construction of the 
EP, rich semantic information can be acquired in 
the training step and is an advantage for sense 
disambiguation. 
2) Senseval-3 has provided a small-scale train-
ing set, with 15-20 training instances for each 
sense, which is not enough for the WSD model-
ing. The lack of training information leads to a 
low performance of the supervised methods. 
3) With a large-scale training corpus, the un-
supervised WSD method has got plenty of train-
ing instances for a high performance in disam-
biguation. 
4) The discriminability of some ambiguous 
word may be low, but the corresponding EPs 
could be easier to disambiguate. For example, 
the ambiguous word "?" has two senses which 
are difficult to distinguish from each other, but 
its Eps' senses of "??/??/??" and "?/?/
?/?"can be easily disambiguated. It is the same 
for the word "??", whose Eps' senses are "?
?/?? /??" and "??/??". EP-based 
knowledge acquisition of these ambiguous words 
for WSD has helped a lot to achieve high per-
formance. 
5 Conclusion 
As discussed above, the supervised WSD method 
shows a low performance because of its depend-
ency on the size of the training data. This reveals 
its weakness in knowledge acquisition bottleneck. 
EP-based unsupervised method has overcame 
this weakness. It requires no manually tagged 
corpus to achieve a satisfactory performance on 
WSD. Experimental results show that EP-based 
method is a promising solution to the large-scale 
WSD task. In future work, we will examine the 
effectiveness of EP-based method in other WSD 
techniques. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
Sense Disambiguation Using Statistical Methods. 
In Proc. of the 29th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-1991), 
pages 264-270. 
Mona Talat Diab. 2003. Word Sense Disambiguation 
Within a Multilingual Framework. PhD thesis, 
University of Maryland College Park. 
Mona Diab. 2004a. Relieving the Data Acquisition 
Bottleneck in Word Sense Disambiguation. In Proc. 
of the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 303-
310. 
Mona T. Diab. 2004b. An Unsupervised Approach for 
Bootstrapping Arabic Sense Tagging. In Proc. of 
Arabic Script Based Languages Workshop at COL-
ING 2004, pages 43-50. 
Mona Diab and Philip Resnik. 2002. An Unsuper-
vised Method for Word Sense Tagging Using Par-
allel Corpora. In Proc. of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2002), pages 255-262. 
William Gale, Kenneth Church, and David Yarowsky. 
1992a. Using Bilingual Materials to Develop Word 
Sense Disambiguation Methods. In Proc. of the 4th 
International Conference on Theoretical and Meth-
odolgical Issues in Machine Translation(TMI-92), 
pages 101-112. 
William Gale, Kenneth Church, and David Yarowsky. 
1992b. Work on Statistical Methods for Word 
Sense Disambiguation. In Proc. of AAAI Fall Sym-
posium on Probabilistic Approaches to Natural 
Language, pages 54-60. 
William Gale, Kenneth Ward Church, and David 
Yarowsky. 1992c. Estimating Upper and Lower 
Bounds on the Performance of Word Sense Disam-
biguation Programs. In Proc. of the 30th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-1992), pages 249-256. 
Tanja Gaustad. 2001. Statistical Corpus-Based Word 
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words. In Proc. of the 39th ACL/EACL, 
Student Research Workshop, pages 61-66. 
Nancy Ide, Tomaz Erjavec, and Dan Tufi?. 2001. 
Automatic Sense Tagging Using Parallel Corpora. 
In Proc. of the Sixth Natural Language Processing 
Pacific Rim Symposium, pages 83-89. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. 
Sense Discrimination with Parallel Corpora. In 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, pages 54-60. 
Cong Li and Hang Li. 2002. Word Translation Dis-
ambiguation Using Bilingual Bootstrapping. In 
Proc. of the 40th Annual Meeting of the Association 
463
for Computational Linguistics (ACL-2002), pages 
343-351. 
Rada Mihalcea and Dan Moldovan. 2000. An Iterative 
Approach to Word Sense Disambiguation. In Proc. 
of Florida Artificial Intelligence Research Society 
Conference (FLAIRS 2000), pages 219-223. 
Rada F. Mihalcea. 2002. Bootstrapping Large Sense 
Tagged Corpora. In Proc. of the 3rd International 
Conference on Languages Resources and Evalua-
tions (LREC 2002), pages 1407-1411. 
Preslav I. Nakov and Marti A. Hearst. 2003. Cate-
gory-based Pseudowords. In Companion Volume to 
the Proceedings of HLT-NAACL 2003, Short Pa-
pers, pages 67-69. 
Hwee Tou. Ng, Bin Wang, and Yee Seng Chan. 2003. 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In Proc. of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 455-462. 
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 
2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. In 
Proc. of the 43th Annual Meeting of the Association 
for Computational Linguistics (ACL-2005), pages 
395-402. 
Ying Qin and Xiaojie Wang. 2005. A Track-based 
Method on Chinese WSD. In Proc. of Joint Sympo-
sium of Computational Linguistics of China (JSCL-
2005), pages 127-133. 
Hinrich. Schutze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1): 97-
123. 
David Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent Res-
toration in Spanish and French. In Proc. of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics(ACL-1994), pages 88-95. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196. 
 
464
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Convolution Tree Kernel for Semantic Role Labeling
Wanxiang Che
Harbin Inst. of Tech.
Harbin, China, 150001
car@ir.hit.edu.cn
Min Zhang
Inst. for Infocomm Research
Singapore, 119613
mzhang@i2r.a-star.edu.sg
Ting Liu, Sheng Li
Harbin Inst. of Tech.
Harbin, China, 150001
{tliu, ls}@ir.hit.edu.cn
Abstract
A hybrid convolution tree kernel is pro-
posed in this paper to effectively model
syntactic structures for semantic role la-
beling (SRL). The hybrid kernel consists
of two individual convolution kernels: a
Path kernel, which captures predicate-
argument link features, and a Constituent
Structure kernel, which captures the syn-
tactic structure features of arguments.
Evaluation on the datasets of CoNLL-
2005 SRL shared task shows that the
novel hybrid convolution tree kernel out-
performs the previous tree kernels. We
also combine our new hybrid tree ker-
nel based method with the standard rich
flat feature based method. The experi-
mental results show that the combinational
method can get better performance than
each of them individually.
1 Introduction
In the last few years there has been increasing in-
terest in Semantic Role Labeling (SRL). It is cur-
rently a well defined task with a substantial body
of work and comparative evaluation. Given a sen-
tence, the task consists of analyzing the proposi-
tions expressed by some target verbs and some
constituents of the sentence. In particular, for each
target verb (predicate) all the constituents in the
sentence which fill a semantic role (argument) of
the verb have to be recognized.
Figure 1 shows an example of a semantic role
labeling annotation in PropBank (Palmer et al,
2005). The PropBank defines 6 main arguments,
Arg0 is the Agent, Arg1 is Patient, etc. ArgM-
may indicate adjunct arguments, such as Locative,
Temporal.
Many researchers (Gildea and Jurafsky, 2002;
Pradhan et al, 2005a) use feature-based methods

 

 	

 

 



 

 	

Figure 1: Semantic role labeling in a phrase struc-
ture syntactic tree representation
for argument identification and classification in
building SRL systems and participating in eval-
uations, such as Senseval-3 1, CoNLL-2004 and
2005 shared tasks: SRL (Carreras and Ma`rquez,
2004; Carreras and Ma`rquez, 2005), where a
flat feature vector is usually used to represent a
predicate-argument structure. However, it?s hard
for this kind of representation method to explicitly
describe syntactic structure information by a vec-
tor of flat features. As an alternative, convolution
tree kernel methods (Collins and Duffy, 2001)
provide an elegant kernel-based solution to im-
plicitly explore tree structure features by directly
computing the similarity between two trees. In
addition, some machine learning algorithms with
dual form, such as Perceptron and Support Vector
Machines (SVM) (Cristianini and Shawe-Taylor,
2000), which do not need know the exact presen-
tation of objects and only need compute their ker-
nel functions during the process of learning and
prediction. They can be well used as learning al-
gorithms in the kernel-based methods. They are
named kernel machines.
In this paper, we decompose the Moschitti
(2004)?s predicate-argument feature (PAF) kernel
into a Path kernel and a Constituent Structure ker-
1http://www.cs.unt.edu/?rada/senseval/senseval3/
73
nel, and then compose them into a hybrid con-
volution tree kernel. Our hybrid kernel method
using Voted Perceptron kernel machine outper-
forms the PAF kernel in the development sets of
CoNLL-2005 SRL shared task. In addition, the fi-
nal composing kernel between hybrid convolution
tree kernel and standard features? polynomial ker-
nel outperforms each of them individually.
The remainder of the paper is organized as fol-
lows: In Section 2 we review the previous work.
In Section 3 we illustrate the state of the art
feature-based method for SRL. Section 4 discusses
our method. Section 5 shows the experimental re-
sults. We conclude our work in Section 6.
2 Related Work
Automatic semantic role labeling was first intro-
duced by Gildea and Jurafsky (2002). They used
a linear interpolation method and extract features
from a parse tree to identify and classify the con-
stituents in the FrameNet (Baker et al, 1998) with
syntactic parsing results. Here, the basic features
include Phrase Type, Parse Tree Path, Position.
Most of the following works focused on feature
engineering (Xue and Palmer, 2004; Jiang et al,
2005) and machine learning models (Nielsen and
Pradhan, 2004; Pradhan et al, 2005a). Some
other works paid much attention to the robust SRL
(Pradhan et al, 2005b) and post inference (Pun-
yakanok et al, 2004).
These feature-based methods are considered as
the state of the art method for SRL and achieved
much success. However, as we know, the standard
flat features are less effective to model the syntac-
tic structured information. It is sensitive to small
changes of the syntactic structure features. This
can give rise to a data sparseness problem and pre-
vent the learning algorithms from generalizing un-
seen data well.
As an alternative to the standard feature-based
methods, kernel-based methods have been pro-
posed to implicitly explore features in a high-
dimension space by directly calculating the sim-
ilarity between two objects using kernel function.
In particular, the kernel methods could be effective
in reducing the burden of feature engineering for
structured objects in NLP problems. This is be-
cause a kernel can measure the similarity between
two discrete structured objects directly using the
original representation of the objects instead of ex-
plicitly enumerating their features.
Many kernel functions have been proposed in
machine learning community and have been ap-
plied to NLP study. In particular, Haussler (1999)
and Watkins (1999) proposed the best-known con-
volution kernels for a discrete structure. In the
context of convolution kernels, more and more
kernels for restricted syntaxes or specific do-
mains, such as string kernel for text categoriza-
tion (Lodhi et al, 2002), tree kernel for syntactic
parsing (Collins and Duffy, 2001), kernel for re-
lation extraction (Zelenko et al, 2003; Culotta
and Sorensen, 2004) are proposed and explored
in NLP domain. Of special interest here, Mos-
chitti (2004) proposed Predicate Argument Fea-
ture (PAF) kernel under the framework of convo-
lution tree kernel for SRL. In this paper, we fol-
low the same framework and design a novel hybrid
convolution kernel for SRL.
3 Feature-based methods for SRL
Usually feature-based methods refer to the meth-
ods which use the flat features to represent in-
stances. At present, most of the successful SRL
systems use this method. Their features are usu-
ally extended from Gildea and Jurafsky (2002)?s
work, which uses flat information derived from
a parse tree. According to the literature, we
select the Constituent, Predicate, and Predicate-
Constituent related features shown in Table 1.
Feature Description
Constituent related features
Phrase Type syntactic category of the constituent
Head Word head word of the constituent
Last Word last word of the constituent
First Word first word of the constituent
Named Entity named entity type of the constituent?s head word
POS part of speech of the constituent
Previous Word sequence previous word of the constituent
Next Word sequence next word of the constituent
Predicate related features
Predicate predicate lemma
Voice grammatical voice of the predicate, either active or passive
SubCat Sub-category of the predicate?s parent node
Predicate POS part of speech of the predicate
Suffix suffix of the predicate
Predicate-Constituent related features
Path parse tree path from the predicate to the constituent
Position the relative position of the constituent and the predicate, before or after
Path Length the nodes number on the parse tree path
Partial Path some part on the parse tree path
Clause Layers the clause layers from the constituent to the predicate
Table 1: Standard flat features
However, to find relevant features is, as usual,
a complex task. In addition, according to the de-
scription of the standard features, we can see that
the syntactic features, such as Path, Path Length,
bulk large among all features. On the other hand,
the previous researches (Gildea and Palmer, 2002;
Punyakanok et al, 2005) have also recognized the
74
 

 	

 

 



 

 	

Figure 2: Predicate Argument Feature space
necessity of syntactic parsing for semantic role la-
beling. However, the standard flat features cannot
model the syntactic information well. A predicate-
argument pair has two different Path features even
if their paths differ only for a node in the parse
tree. This data sparseness problem prevents the
learning algorithms from generalizing unseen data
well. In order to address this problem, one method
is to list all sub-structures of the parse tree. How-
ever, both space complexity and time complexity
are too high for the algorithm to be realized.
4 Hybrid Convolution Tree Kernels for
SRL
In this section, we introduce the previous ker-
nel method for SRL in Subsection 4.1, discuss
our method in Subsection 4.2 and compare our
method with previous work in Subsection 4.3.
4.1 Convolution Tree Kernels for SRL
Moschitti (2004) proposed to apply convolution
tree kernels (Collins and Duffy, 2001) to SRL.
He selected portions of syntactic parse trees,
which include salient sub-structures of predicate-
arguments, to define convolution kernels for the
task of predicate argument classification. This por-
tions selection method of syntactic parse trees is
named as predicate-arguments feature (PAF) ker-
nel. Figure 2 illustrates the PAF kernel feature
space of the predicate buy and the argument Arg1
in the circled sub-structure.
The kind of convolution tree kernel is similar to
Collins and Duffy (2001)?s tree kernel except the
sub-structure selection strategy. Moschitti (2004)
only selected the relative portion between a predi-
cate and an argument.
Given a tree portion instance defined above, we
design a convolution tree kernel in a way similar
to the parse tree kernel (Collins and Duffy, 2001).
Firstly, a parse tree T can be represented by a vec-
tor of integer counts of each sub-tree type (regard-
less of its ancestors):
?(T ) = (# of sub-trees of type 1, . . . ,
# of sub-trees of type i, . . . ,
# of sub-trees of type n)
This results in a very high dimension since the
number of different subtrees is exponential to the
tree?s size. Thus it is computationally infeasible
to use the feature vector ?(T ) directly. To solve
this problem, we introduce the tree kernel function
which is able to calculate the dot product between
the above high-dimension vectors efficiently. The
kernel function is defined as following:
K(T1, T2) = ??(T1),?(T2)? =
?
i ?i(T1), ?i(T2)=?n1?N1
?
n2?N2
?
i Ii(n1) ? Ii(n2)
where N1 and N2 are the sets of all nodes in
trees T1 and T2, respectively, and Ii(n) is the in-
dicator function whose value is 1 if and only if
there is a sub-tree of type i rooted at node n and
0 otherwise. Collins and Duffy (2001) show that
K(T1, T2) is an instance of convolution kernels
over tree structures, which can be computed in
O(|N1| ? |N2|) by the following recursive defi-
nitions (Let ?(n1, n2) =
?
i Ii(n1) ? Ii(n2)):
(1) if the children of n1 and n2 are different then
?(n1, n2) = 0;
(2) else if their children are the same and they are
leaves, then ?(n1, n2) = ?;
(3) else ?(n1, n2) = ?
?nc(n1)
j=1 (1 +
?(ch(n1, j), ch(n2, j)))
where nc(n1) is the number of the children of
n1, ch(n, j) is the jth child of node n and ?(0 <
? < 1) is the decay factor in order to make the
kernel value less variable with respect to the tree
sizes.
4.2 Hybrid Convolution Tree Kernels
In the PAF kernel, the feature spaces are consid-
ered as an integral portion which includes a pred-
icate and one of its arguments. We note that the
PAF feature consists of two kinds of features: one
is the so-called parse tree Path feature and another
one is the so-called Constituent Structure feature.
These two kinds of feature spaces represent dif-
ferent information. The Path feature describes the
75
 

 	

 

 



 

 	

Figure 3: Path and Constituent Structure feature
spaces
linking information between a predicate and its ar-
guments while the Constituent Structure feature
captures the syntactic structure information of an
argument. We believe that it is more reasonable
to capture the two different kinds of features sepa-
rately since they contribute to SRL in different fea-
ture spaces and it is better to give different weights
to fuse them. Therefore, we propose two convo-
lution kernels to capture the two features, respec-
tively and combine them into one hybrid convolu-
tion kernel for SRL. Figure 3 is an example to il-
lustrate the two feature spaces, where the Path fea-
ture space is circled by solid curves and the Con-
stituent Structure feature spaces is circled by dot-
ted curves. We name them Path kernel and Con-
stituent Structure kernel respectively.
Figure 4 illustrates an example of the distinc-
tion between the PAF kernel and our kernel. In
the PAF kernel, the tree structures are equal when
considering constitutes NP and PRP, as shown in
Figure 4(a). However, the two constituents play
different roles in the sentence and should not be
looked as equal. Figure 4(b) shows the comput-
ing example with our kernel. During computing
the hybrid convolution tree kernel, the NP?PRP
substructure is not computed. Therefore, the two
trees are distinguished correctly.
On the other hand, the constituent structure fea-
ture space reserves the most part in the traditional
PAF feature space usually. Then the Constituent
Structure kernel plays the main role in PAF kernel
computation, as shown in Figure 5. Here, believes
is a predicate and A1 is a long sub-sentence. Ac-
cording to our experimental results in Section 5.2,
we can see that the Constituent Structure kernel
does not perform well. Affected by this, the PAF
kernel cannot perform well, either. However, in
our hybrid method, we can adjust the compromise

 
 	



 
 	


(a) PAF Kernel

 
	


	


 
	



	
 

(b) Hybrid Convolution Tree Kernel
Figure 4: Comparison between PAF and Hybrid
Convolution Tree Kernels
Figure 5: An example of Semantic Role Labeling
of the Path feature and the Constituent Structure
feature by tuning their weights to get an optimal
result.
Having defined two convolution tree kernels,
the Path kernel Kpath and the Constituent Struc-
ture kernel Kcs, we can define a new kernel to
compose and extend the individual kernels. Ac-
cording to Joachims et al (2001), the kernel func-
tion set is closed under linear combination. It
means that the following Khybrid is a valid kernel
if Kpath and Kcs are both valid.
Khybrid = ?Kpath + (1? ?)Kcs (1)
where 0 ? ? ? 1.
According to the definitions of the Path and the
Constituent Structure kernels, each kernel is ex-
plicit. They can be viewed as a matching of fea-
76
tures. Since the features are enumerable on the
given data, the kernels are all valid. Therefore, the
new kernel Khybrid is valid. We name the new ker-
nel hybrid convolution tree kernel, Khybrid.
Since the size of a parse tree is not con-
stant, we normalize K(T1, T2) by dividing it by?K(T1, T1) ?K(T2, T2)
4.3 Comparison with Previous Work
It would be interesting to investigate the differ-
ences between our method and the feature-based
methods. The basic difference between them lies
in the instance representation (parse tree vs. fea-
ture vector) and the similarity calculation mecha-
nism (kernel function vs. dot-product). The main
difference between them is that they belong to dif-
ferent feature spaces. In the kernel methods, we
implicitly represent a parse tree by a vector of in-
teger counts of each sub-tree type. That is to say,
we consider all the sub-tree types and their occur-
ring frequencies. In this way, on the one hand,
the predicate-argument related features, such as
Path, Position, in the flat feature set are embed-
ded in the Path feature space. Additionally, the
Predicate, Predicate POS features are embedded
in the Path feature space, too. The constituent re-
lated features, such as Phrase Type, Head Word,
Last Word, and POS, are embedded in the Con-
stituent Structure feature space. On the other hand,
the other features in the flat feature set, such as
Named Entity, Previous, and Next Word, Voice,
SubCat, Suffix, are not contained in our hybrid
convolution tree kernel. From the syntactic view-
point, the tree representation in our feature space
is more robust than the Parse Tree Path feature in
the flat feature set since the Path feature is sensi-
tive to small changes of the parse trees and it also
does not maintain the hierarchical information of
a parse tree.
It is also worth comparing our method with
the previous kernels. Our method is similar to
the Moschitti (2004)?s predicate-argument feature
(PAF) kernel. However, we differentiate the Path
feature and the Constituent Structure feature in our
hybrid kernel in order to more effectively capture
the syntactic structure information for SRL. In ad-
dition Moschitti (2004) only study the task of ar-
gument classification while in our experiment, we
report the experimental results on both identifica-
tion and classification.
5 Experiments and Discussion
The aim of our experiments is to verify the effec-
tiveness of our hybrid convolution tree kernel and
and its combination with the standard flat features.
5.1 Experimental Setting
5.1.1 Corpus
We use the benchmark corpus provided by
CoNLL-2005 SRL shared task (Carreras and
Ma`rquez, 2005) provided corpus as our training,
development, and test sets. The data consist of
sections of the Wall Street Journal (WSJ) part of
the Penn TreeBank (Marcus et al, 1993), with
information on predicate-argument structures ex-
tracted from the PropBank corpus (Palmer et al,
2005). We followed the standard partition used
in syntactic parsing: sections 02-21 for training,
section 24 for development, and section 23 for
test. In addition, the test set of the shared task
includes three sections of the Brown corpus. Ta-
ble 2 provides counts of sentences, tokens, anno-
tated propositions, and arguments in the four data
sets.
Train Devel tWSJ tBrown
Sentences 39,832 1,346 2,416 426
Tokens 950,028 32,853 56,684 7,159
Propositions 90,750 3,248 5,267 804
Arguments 239,858 8,346 14,077 2,177
Table 2: Counts on the data set
The preprocessing modules used in CONLL-
2005 include an SVM based POS tagger (Gime?nez
and Ma`rquez, 2003), Charniak (2000)?s full syn-
tactic parser, and Chieu and Ng (2003)?s Named
Entity recognizer.
5.1.2 Evaluation
The system is evaluated with respect to
precision, recall, and F?=1 of the predicted ar-
guments. Precision (p) is the proportion of ar-
guments predicted by a system which are cor-
rect. Recall (r) is the proportion of correct ar-
guments which are predicted by a system. F?=1
computes the harmonic mean of precision and
recall, which is the final measure to evaluate the
performances of systems. It is formulated as:
F?=1 = 2pr/(p + r). srl-eval.pl2 is the official
program of the CoNLL-2005 SRL shared task to
evaluate a system performance.
2http://www.lsi.upc.edu/?srlconll/srl-eval.pl
77
5.1.3 SRL Strategies
We use constituents as the labeling units to form
the labeled arguments. In order to speed up the
learning process, we use a four-stage learning ar-
chitecture:
Stage 1: To save time, we use a pruning
stage (Xue and Palmer, 2004) to filter out the
constituents that are clearly not semantic ar-
guments to the predicate.
Stage 2: We then identify the candidates derived
from Stage 1 as either arguments or non-
arguments.
Stage 3: A multi-category classifier is used to
classify the constituents that are labeled as ar-
guments in Stage 2 into one of the argument
classes plus NULL.
Stage 4: A rule-based post-processing stage (Liu
et al, 2005) is used to handle some un-
matched arguments with constituents, such as
AM-MOD, AM-NEG.
5.1.4 Classifier
We use the Voted Perceptron (Freund and
Schapire, 1998) algorithm as the kernel machine.
The performance of the Voted Perceptron is close
to, but not as good as, the performance of SVM on
the same problem, while saving computation time
and programming effort significantly. SVM is too
slow to finish our experiments for tuning parame-
ters.
The Voted Perceptron is a binary classifier. In
order to handle multi-classification problems, we
adopt the one vs. others strategy and select the
one with the largest margin as the final output. The
training parameters are chosen using development
data. After 5 iteration numbers, the best perfor-
mance is achieved. In addition, Moschitti (2004)?s
Tree Kernel Tool is used to compute the tree kernel
function.
5.2 Experimental Results
In order to speed up the training process, in the
following experiments, we ONLY use WSJ sec-
tions 02-05 as training data. The same as Mos-
chitti (2004), we also set the ? = 0.4 in the com-
putation of convolution tree kernels.
In order to study the impact of ? in hybrid con-
volution tree kernel in Eq. 1, we only use the hy-
brid kernel between Kpath and Kcs. The perfor-
mance curve on development set changing with ?
is shown in Figure 6.
Figure 6: The performance curve changing with ?
The performance curve shows that when ? =
0.5, the hybrid convolution tree kernel gets the
best performance. Either the Path kernel (? = 1,
F?=1 = 61.26) or the Constituent Structure kernel
(? = 0, F?=1 = 54.91) cannot perform better than
the hybrid one. It suggests that the two individual
kernels are complementary to each other. In ad-
dition, the Path kernel performs much better than
the Constituent Structure kernel. It indicates that
the predicate-constituent related features are more
effective than the constituent features for SRL.
Table 3 compares the performance comparison
among our Hybrid convolution tree kernel, Mos-
chitti (2004)?s PAF kernel, standard flat features
with Linear kernels, and Poly kernel (d = 2). We
can see that our hybrid convolution tree kernel out-
performs the PAF kernel. It empirically demon-
strates that the weight linear combination in our
hybrid kernel is more effective than PAF kernel for
SRL.
However, our hybrid kernel still performs worse
than the standard feature based system. This is
simple because our kernel only use the syntac-
tic structure information while the feature-based
method use a large number of hand-craft diverse
features, from word, POS, syntax and semantics,
NER, etc. The standard features with polynomial
kernel gets the best performance. The reason is
that the arbitrary binary combination among fea-
tures implicated by the polynomial kernel is useful
to SRL. We believe that combining the two meth-
ods can perform better.
In order to make full use of the syntactic
information and the standard flat features, we
present a composite kernel between hybrid kernel
(Khybrid) and standard features with polynomial
78
Hybrid PAF Linear Poly
Devel 66.01 64.38 68.71 70.25
Table 3: Performance (F?=1) comparison among
various kernels
kernel (Kpoly):
Kcomp = ?Khybrid + (1? ?)Kpoly (2)
where 0 ? ? ? 1.
The performance curve changing with ? in Eq. 2
on development set is shown in Figure 7.
Figure 7: The performance curve changing with ?
We can see that when ? = 0.5, the system
achieves the best performance and F?=1 = 70.78.
It?s statistically significant improvement (?2 test
with p = 0.1) than only using the standard features
with the polynomial kernel (? = 0, F?=1 = 70.25)
and much higher than only using the hybrid con-
volution tree kernel (? = 1, F?=1 = 66.01).
The main reason is that the convolution tree ker-
nel can represent more general syntactic features
than standard flat features, and the standard flat
features include the features that the convolution
tree kernel cannot represent, such as Voice, Sub-
Cat. The two kind features are complementary to
each other.
Finally, we train the composite method using
the above setting (Eq. 2 with when ? = 0.5) on the
entire training set. The final performance is shown
in Table 4.
6 Conclusions and Future Work
In this paper we proposed the hybrid convolu-
tion kernel to model syntactic structure informa-
tion for SRL. Different from the previous convo-
lution tree kernel based methods, our contribution
Precision Recall F?=1
Development 80.71% 68.49% 74.10
Test WSJ 82.46% 70.65% 76.10
Test Brown 73.39% 57.01% 64.17
Test WSJ Precision Recall F?=1
Overall 82.46% 70.65% 76.10
A0 87.97% 82.49% 85.14
A1 80.51% 71.69% 75.84
A2 75.79% 52.16% 61.79
A3 80.85% 43.93% 56.93
A4 83.56% 59.80% 69.71
A5 100.00% 20.00% 33.33
AM-ADV 66.27% 43.87% 52.79
AM-CAU 68.89% 42.47% 52.54
AM-DIR 56.82% 29.41% 38.76
AM-DIS 79.02% 75.31% 77.12
AM-EXT 73.68% 43.75% 54.90
AM-LOC 72.83% 50.96% 59.97
AM-MNR 68.54% 42.44% 52.42
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 49.32% 31.30% 38.30
AM-TMP 82.15% 68.17% 74.51
R-A0 86.28% 87.05% 86.67
R-A1 80.00% 74.36% 77.08
R-A2 100.00% 31.25% 47.62
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 50.00% 100.00% 66.67
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 20.00% 16.67% 18.18
R-AM-TMP 68.75% 63.46% 66.00
V 98.65% 98.65% 98.65
Table 4: Overall results (top) and detailed results
on the WSJ test (bottom).
is that we distinguish between the Path and the
Constituent Structure feature spaces. Evaluation
on the datasets of CoNLL-2005 SRL shared task,
shows that our novel hybrid convolution tree ker-
nel outperforms the PAF kernel method. Although
the hybrid kernel base method is not as good as
the standard rich flat feature based methods, it can
improve the state of the art feature-based methods
by implicating the more generalizing syntactic in-
formation.
Kernel-based methods provide a good frame-
work to use some features which are difficult to
model in the standard flat feature based methods.
For example the semantic similarity of words can
be used in kernels well. We can use general pur-
pose corpus to create clusters of similar words or
use available resources like WordNet. We can also
use the hybrid kernel method into other tasks, such
as relation extraction in the future.
79
Acknowledgements
The authors would like to thank the reviewers for
their helpful comments and Shiqi Zhao, Yanyan
Zhao for their suggestions and useful discussions.
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60435020, 60575042, and 60503072.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the ACL-Coling-1998, pages 86?90.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of CoNLL-2004, pages 89?
97.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, pages
152?164.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-2000.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proceedings of CoNLL-2003, pages 160?163.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proceedings
of NIPS-2001.
Nello Cristianini and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL-2004, pages 423?429.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Computational Learning Theory, pages 209?217.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of ACL-2002, pages 239?246.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and
accurate part-of-speech tagging: The svm approach
revisited. In Proceedings of RANLP-2003.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, July.
Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument
interdependence. In Proceedings of IJCAI-2005.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cat-
egorisation. In Proceedings of ICML-2001, pages
250?257.
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, pages 189?192.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of ACL-2004, pages 335?342.
Rodney D. Nielsen and Sameer Pradhan. 2004. Mix-
ing weak learners in semantic parsing. In Proceed-
ings of EMNLP-2004.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005a. Support vector learning for semantic
argument classification. Machine Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005, pages 581?588.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role
labeling. In Proceedings of IJCAI-2005, pages
1117?1123.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, Jan.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP 2004.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
Proceedings of ACL-08: HLT, pages 559?567,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Tree Sequence Alignment-based Tree-to-Tree Translation Model 
 
 
Min Zhang1  Hongfei Jiang2  Aiti Aw1  Haizhou Li1  Chew Lim Tan3 and Sheng Li2
1Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singapore
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg 
aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn  
hli@i2r.a-star.edu.sg   
 
  
Abstract 
This paper presents a translation model that is 
based on tree sequence alignment, where a tree 
sequence refers to a single sequence of sub-
trees that covers a phrase. The model leverages 
on the strengths of both phrase-based and lin-
guistically syntax-based method. It automati-
cally learns aligned tree sequence pairs with 
mapping probabilities from word-aligned bi-
parsed parallel texts. Compared with previous 
models, it not only captures non-syntactic 
phrases and discontinuous phrases with lin-
guistically structured features, but also sup-
ports multi-level structure reordering of tree 
typology with larger span. This gives our 
model stronger expressive power than other re-
ported models. Experimental results on the 
NIST MT-2005 Chinese-English translation 
task show that our method statistically signifi-
cantly outperforms the baseline systems.  
1 Introduction 
Phrase-based modeling method (Koehn et al, 
2003; Och and Ney, 2004a) is a simple, but power-
ful mechanism to machine translation since it can 
model local reorderings and translations of multi-
word expressions well. However, it cannot handle 
long-distance reorderings properly and does not 
exploit discontinuous phrases and linguistically 
syntactic structure features (Quirk and Menezes, 
2006). Recently, many syntax-based models have 
been proposed to address the above deficiencies 
(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and 
Palmer, 2005; Quirk et al 2005; Cowan et al, 
2006; Zhang et al, 2007; Bod, 2007; Yamada and 
Knight, 2001; Liu et al, 2006; Liu et al, 2007; 
Gildea, 2003; Poutsma, 2000; Hearne and Way, 
2003). Although good progress has been reported, 
the fundamental issues in applying linguistic syn-
tax to SMT, such as non-isomorphic tree align-
ment, structure reordering and non-syntactic phrase 
modeling, are still worth well studying. 
In this paper, we propose a tree-to-tree transla-
tion model that is based on tree sequence align-
ment. It is designed to combine the strengths of 
phrase-based and syntax-based methods. The pro-
posed model adopts tree sequence 1  as the basic 
translation unit and utilizes tree sequence align-
ments to model the translation process. Therefore, 
it not only describes non-syntactic phrases with 
syntactic structure information, but also supports 
multi-level tree structure reordering in larger span. 
These give our model much more expressive 
power and flexibility than those previous models. 
Experiment results on the NIST MT-2005 Chinese-
English translation task show that our method sig-
nificantly outperforms Moses (Koehn et al, 2007), 
a state-of-the-art phrase-based SMT system, and 
other linguistically syntax-based methods, such as 
SCFG-based and STSG-based methods (Zhang et 
al., 2007). In addition, our study further demon-
strates that 1) structure reordering rules in our 
model are very useful for performance improve-
ment while discontinuous phrase rules have less 
contribution and 2) tree sequence rules are able to 
model non-syntactic phrases with syntactic struc-
ture information, and thus contribute much to the 
performance improvement, but those rules consist-
ing of more than three sub-trees have almost no 
contribution.  
The rest of this paper is organized as follows: 
Section 2 reviews previous work. Section 3 elabo-
                                                          
1 A tree sequence refers to an ordered sub-tree sequence that 
covers a phrase or a consecutive tree fragment in a parse tree. 
It is the same as the concept ?forest? used in Liu et al(2007).  
559
rates the modelling process while Sections 4 and 5 
discuss the training and decoding algorithms. The 
experimental results are reported in Section 6. Fi-
nally, we conclude our work in Section 7. 
2 Related Work 
Many techniques on linguistically syntax-based 
SMT have been proposed in literature. Yamada 
and Knight (2001) use noisy-channel model to 
transfer a target parse tree into a source sentence. 
Eisner (2003) studies how to learn non-isomorphic 
tree-to-tree/string mappings using a STSG. Ding 
and Palmer (2005) propose a syntax-based transla-
tion model based on a probabilistic synchronous 
dependency insertion grammar. Quirk et al (2005) 
propose a dependency treelet-based translation 
model. Cowan et al (2006) propose a feature-
based discriminative model for target language 
syntactic structures prediction, given a source 
parse tree. Huang et al (2006) study a TSG-based 
tree-to-string alignment model. Liu et al (2006) 
propose a tree-to-string model. Zhang et al 
(2007b) present a STSG-based tree-to-tree transla-
tion model. Bod (2007) reports that the unsuper-
vised STSG-based translation model performs 
much better than the supervised one. The motiva-
tion behind all these work is to exploit linguistical-
ly syntactic structure features to model the 
translation process. However, most of them fail to 
utilize non-syntactic phrases well that are proven 
useful in the phrase-based methods (Koehn et al, 
2003). 
The formally syntax-based model for SMT was 
first advocated by Wu (1997). Xiong et al (2006) 
propose a MaxEnt-based reordering model for 
BTG (Wu, 1997) while Setiawan et al (2007) pro-
pose a function word-based reordering model for 
BTG. Chiang (2005)?s hierarchal phrase-based 
model achieves significant performance improve-
ment. However, no further significant improve-
ment is achieved when the model is made sensitive 
to syntactic structures by adding a constituent fea-
ture (Chiang, 2005). 
In the last two years, many research efforts were 
devoted to integrating the strengths of phrase-
based and syntax-based methods. In the following, 
we review four representatives of them.   
1) Hassan et al (2007) integrate supertags (a 
kind of lexicalized syntactic description) into the 
target side of translation model and language mod-
el under the phrase-based translation framework, 
resulting in good performance improvement. How-
ever, neither source side syntactic knowledge nor 
reordering model is further explored.  
2) Galley et al (2006) handle non-syntactic 
phrasal translations by traversing the tree upwards 
until a node that subsumes the phrase is reached. 
This solution requires larger applicability contexts 
(Marcu et al, 2006). However, phrases are utilized 
independently in the phrase-based method without 
depending on any contexts.  
3) Addressing the issues in Galley et al (2006), 
Marcu et al (2006) create an xRS rule headed by a 
pseudo, non-syntactic non-terminal symbol that 
subsumes the phrase and its corresponding multi-
headed syntactic structure; and one sibling xRS 
rule that explains how the pseudo symbol can be 
combined with other genuine non-terminals for 
acquiring the genuine parse trees. The name of the 
pseudo non-terminal is designed to reflect the full 
realization of the corresponding rule. The problem 
in this method is that it neglects alignment consis-
tency in creating sibling rules and the naming me-
chanism faces challenges in describing more 
complicated phenomena (Liu et al, 2007).  
4) Liu et al (2006) treat all bilingual phrases as 
lexicalized tree-to-string rules, including those 
non-syntactic phrases in training corpus. Although 
the solution shows effective empirically, it only 
utilizes the source side syntactic phrases of the in-
put parse tree during decoding. Furthermore, the 
translation probabilities of the bilingual phrases 
and other tree-to-string rules are not compatible 
since they are estimated independently, thus hav-
ing different parameter spaces. To address the 
above problems, Liu et al (2007) propose to use 
forest-to-string rules to enhance the expressive 
power of their tree-to-string model. As is inherent 
in a tree-to-string framework, Liu et al?s method 
defines a kind of auxiliary rules to integrate forest-
to-string rules into tree-to-string models. One prob-
lem of this method is that the auxiliary rules are 
not described by probabilities since they are con-
structed during decoding, rather than learned from 
the training corpus. So, to balance the usage of dif-
ferent kinds of rules, they use a very simple feature 
counting the number of auxiliary rules used in a 
derivation for penalizing the use of forest-to-string 
and auxiliary rules. 
In this paper, an alternative solution is presented 
to combine the strengths of phrase-based and syn-
560
1( )
IT e
1( )
JT f
A
 
 
Figure 1: A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation  
 
 
 
Figure 2: Two Examples of tree sequences 
 
 
 
Figure 3: Two examples of translation rules 
tax-based methods. Unlike previous work, our so-
lution neither requires larger applicability contexts 
(Galley et al, 2006), nor depends on pseudo nodes 
(Marcu et al, 2006) or auxiliary rules (Liu et al, 
2007). We go beyond the single sub-tree mapping 
model to propose a tree sequence alignment-based 
translation model. To the best of our knowledge, 
this is the first attempt to empirically explore the 
tree sequence alignment based model in SMT.  
3 Tree Sequence Alignment Model 
3.1 Tree Sequence Translation Rule   
The leaf nodes of a sub-tree in a tree sequence can 
be either non-terminal symbols (grammar tags) or 
terminal symbols (lexical words). Given a pair of 
source and target parse trees 1( )
JT f and 1( )
IT e  in 
Fig. 1, Fig. 2 illustrates two examples of tree se-
quences derived from the two parse trees. A tree 
sequence translation rule r  is a pair of aligned tree 
sequences r =< 2
1
( )jjTS f , 21( )
i
iTS e , A%  >, where: 
z 2
1
( )jjTS f is a source tree sequence, covering 
the span [ 1 2,j j ] in 1( )
JT f , and 
z 2
1
( )iiTS e is a target one, covering the span 
[ 1 2,i i ] in 1( )
IT e , and 
z A% are the alignments between leaf nodes of 
two tree sequences, satisfying the following 
condition: 1 2 1 2( , ) :i j A i i i j j j? ? ? ? ? ? ?% . 
Fig. 3 shows two rules extracted from the tree pair 
shown in Fig. 1, where r1 is a tree-to-tree rule and 
r2 is a tree sequence-to-tree sequence rule. Ob-
viously, tree sequence rules are more powerful 
than phrases or tree rules as they can capture all 
phrases (including both syntactic and non-syntactic 
phrases) with syntactic structure information and 
allow any tree node operations in a longer span. 
We expect that these properties can well address 
the issues of non-isomorphic structure alignments, 
structure reordering, non-syntactic phrases and 
discontinuous phrases translations. 
3.2 Tree Sequence Translation Model 
Given the source and target sentences 1
Jf and 1
Ie  
and their parse trees 1( )
JT f and 1( )
IT e , the tree 
sequence-to-tree sequence translation model is 
formulated as: 
1 1
1 1
1 1 1 1 1 1
( ), ( )
1 1
( ), ( )
1 1 1
1 1 1 1
( | ) ( , ( ), ( ) | )
( ( ( ) | )
( ( ) | ( ), )
( | ( ), ( ), ))
                
                      
                      
J I
J I
I J I I J J
T f T e
J J
T f T e
I J J
I I J J
r r
r
r
r
P e f P e T e T f f
P T f f
P T e T f f
P e T e T f f
=
=
?
?
?
? (1) 
In our implementation, we have: 
561
1) 1 1( ( ) | ) 1
J JrP T f f ? since we only use the best 
source and target parse tree pairs in training. 
2) 1 1 1 1( | ( ), ( ), ) 1
I I J JrP e T e T f f ? since we just 
output the leaf nodes of 1( )
IT e to generate 1
Ie  
regardless of source side information. 
Since 1( )
JT f contains the information of 1
Jf , 
now we have: 
1 1 1 1 1
1 1
( | ) ( ( ) | ( ), )
                 ( ( ) | ( ))
I J I J J
I J
r r
r
P e f P T e T f f
P T e T f
=
=
           (2) 
By Eq. (2), translation becomes a tree structure 
mapping issue. We model it using our tree se-
quence-based translation rules. Given the source 
parse tree 1( )
JT f , there are multiple derivations 
that could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is obtained 
by summing over the probabilities of all deriva-
tions. The probability of each derivation? is given 
as the product of the probabilities of all the rules 
( )ip r  used in the derivation (here we assume that 
a rule is applied independently in a derivation). 
2 2
1 1
1 1 1 1( | ) ( ( ) | ( ))
     = ( : ( ), ( ), )
i
I J I J
i j
i i j
r
r rP e f P T e T f
p r TS e TS f A
? ??
=
< >?? %    (3) 
Eq. (3) formulates the tree sequence alignment-
based translation model. Figs. 1 and 3 show how 
the proposed model works. First, the source sen-
tence is parsed into a source parse tree. Next, the 
source parse tree is detached into two source tree 
sequences (the left hand side of rules in Fig. 3). 
Then the two rules in Fig. 3 are used to map the 
two source tree sequences to two target tree se-
quences, which are then combined to generate a 
target parse tree. Finally, a target translation is 
yielded from the target tree.  
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the commonly 
used features in phrase-based systems (Koehn, 
2004): 1) bidirectional rule mapping probabilities; 
2) bidirectional lexical rule translation probabilities; 
3) the target language model; 4) the number of 
rules used and 5) the number of target words. In 
addition, we define two new features: 1) the num-
ber of lexical words in a rule to control the model?s 
preference for lexicalized rules over un-lexicalized 
rules and 2) the average tree depth in a rule to bal-
ance the usage of hierarchical rules and flat rules. 
Note that we do not distinguish between larger (tal-
ler) and shorter source side tree sequences, i.e. we 
let these rules compete directly with each other. 
4 Rule Extraction 
Rules are extracted from word-aligned, bi-parsed 
sentence pairs 1 1( ), ( ),
J IT f T e A< > , which are 
classified into two categories: 
z initial rule, if all leaf nodes of the rule are 
terminals (i.e. lexical word), and 
z abstract rule, otherwise, i.e. at least one leaf 
node is a non-terminal (POS or phrase tag). 
Given an initial rule 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% , 
its sub initial rule is defined as a triple 
4 4
3 3
?( ), ( ),j ij iTS f TS e A< >  if and only if: 
z 4 4
3 3
?( ), ( ),j ij iTS f TS e A< > is an initial rule. 
z 3 4 3 4( , ) :i j A i i i j j j? ? ? ? ? ? ?% , i.e. 
A? A? %  
z 4
3
( )jjTS f is a sub-graph of 21( )
j
jTS f while  
4
3
( )iiTS e  is a sub-graph of 21( )
i
iTS e . 
Rules are extracted in two steps: 
1) Extracting initial rules first. 
2) Extracting abstract rules from extracted ini-
tial rules with the help of sub initial rules. 
It is straightforward to extract initial rules. We 
first generate all fully lexicalized source and target 
tree sequences using a dynamic programming algo-
rithm and then iterate over all generated source and 
target tree sequence pairs 2 2
1 1
( ), ( )j ij iTS f TS e< > . If 
the condition ? ( , )i j? 1 2 1 2:A i i i j j j? ? ? ? ? ? ? 
is satisfied, the triple 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% is 
an initial rule, where A%  are alignments between 
leaf nodes of 2
1
( )jjTS f  and 21( )
i
iTS e . We then de-
rive abstract rules from initial rules by removing 
one or more of its sub initial rules. The abstract 
rule extraction algorithm presented next is imple-
mented using dynamic programming. Due to space 
limitation, we skip the details here. In order to con-
trol the number of rules, we set three constraints 
for both finally extracted initial and abstract rules:  
1) The depth of a tree in a rule is not greater 
562
than h . 
2) The number of non-terminals as leaf nodes is 
not greater than c . 
3) The tree number in a rule is not greater than d. 
In addition, we limit initial rules to have at most 
seven lexical words as leaf nodes on either side. 
However, in order to extract long-distance reorder-
ing rules, we also generate those initial rules with 
more than seven lexical words for abstract rules 
extraction only (not used in decoding). This makes 
our abstract rules more powerful in handling 
global structure reordering. Moreover, by configur-
ing these parameters we can implement other 
translation models easily: 1) STSG-based model  
when 1d = ; 2) SCFG-based model when 1d =  
and 2h = ; 3) phrase-based translation model only 
(no reordering model) when 0c =  and 1h = . 
 
Algorithm 1: abstract rules extraction 
Input: initial rule set inir  
Output: abstract rule set absr  
1: for each i inir r? , do 
2:    put all sub initial rules of ir  into a set subiniir
3:    for each subset subiniir? ? do 
4:          if there are spans overlapping between 
any two rules in the subset ?  then 
5:                    continue   //go to line 3 
6:           end if  
7:           generate an abstract rule by removing 
the portions covered by ?  from ir  and 
co-indexing the pairs of non-terminals 
that rooting the removed source and 
target parts 
8:           add them into the abstract rule set absr  
9:     end do 
10: end do  
 
5 Decoding 
Given 1( )
JT f , the decoder is to find the best deri-
vation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (4) 
Algorithm 2: Tree Sequence-based Decoder 
 Input: 1( )
JT f   Output: 1( )
IT e  
 Data structures: 
1 2[ , ]h j j    To store translations to a span 1 2[ , ]j j  
1: for s = 0 to J -1 do      // s: span length 
2:     for 1j = 1 to J - s , 2j = 1j + s  do  
3:          for each rule r spanning 1 2[ , ]j j  do  
4:               if r  is an initial rule then 
5:                    insert r into 1 2[ , ]h j j  
6:               else 
7:      generate new translations from 
r by replacing non-terminal leaf 
nodes of r with their correspond-
ing spans? translations that are al-
ready translated in previous steps 
8:      insert them into 1 2[ , ]h j j  
9:  end if 
10: end for 
11: end for 
12: end for 
13: output the hypothesis with the highest score  
in [1, ]h J  as the final best translation 
 
The decoder is a span-based beam search to-
gether with a function for mapping the source deri-
vations to the target ones. Algorithm 2 illustrates 
the decoding algorithm. It translates each span ite-
ratively from small one to large one (lines 1-2).  
This strategy can guarantee that when translating 
the current span, all spans smaller than the current 
one have already been translated before if they are 
translatable (line 7). When translating a span, if the 
usable rule is an initial rule, then the tree sequence 
on the target side of the rule is a candidate transla-
tion (lines 4-5). Otherwise, we replace the non-
terminal leaf nodes of the current abstract rule 
with their corresponding spans? translations that 
are already translated in previous steps (line 7). To 
speed up the decoder, we use several thresholds to 
limit search beams for each span:  
1) ? , the maximal number of rules used 
2) ? , the minimal log probability of rules 
3) ? , the maximal number of translations yield  
It is worth noting that the decoder does not force 
a complete target parse tree to be generated. If no 
rules can be used to generate a complete target 
parse tree, the decoder just outputs whatever have 
563
been translated so far monotonically as one hy-
pothesis. 
6 Experiments 
6.1 Experimental Settings 
We conducted Chinese-to-English translation ex-
periments. We trained the translation model on the 
FBIS corpus (7.2M+9.2M words) and trained a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing. We used sentences with 
less than 50 characters from the NIST MT-2002 
test set as our development set and the NIST MT-
2005 test set as our test set. We used the Stanford 
parser (Klein and Manning, 2003) to parse bilin-
gual sentences on the training set and Chinese sen-
tences on the development and test sets. The 
evaluation metric is case-sensitive BLEU-4 (Papi-
neni et al, 2002). We used GIZA++ (Och and Ney, 
2004) and the heuristics ?grow-diag-final? to gen-
erate m-to-n word alignments. For the MER train-
ing (Och, 2003), we modified Koehn?s MER 
trainer (Koehn, 2004) for our tree sequence-based 
system. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). 
We set three baseline systems: Moses (Koehn et 
al., 2007), and SCFG-based and STSG-based tree-
to-tree translation models (Zhang et al, 2007). For 
Moses, we used its default settings. For the 
SCFG/STSG and our proposed model, we used the 
same settings except for the parameters d and h  
( 1d = and 2h = for the SCFG; 1d = and 6h = for 
the STSG; 4d =  and 6h = for our model). We 
optimized these parameters on the training and de-
velopment sets: c =3, ? =20, ? =-100 and ? =100. 
6.2 Experimental Results   
We carried out a number of experiments to ex-
amine the proposed tree sequence alignment-based 
translation model. In this subsection, we first re-
port the rule distributions and compare our model 
with the three baseline systems. Then we study the 
model?s expressive ability by comparing the con-
tributions made by different kinds of rules, includ-
ing strict tree sequence rules, non-syntactic phrase 
rules, structure reordering rules and discontinuous 
phrase rules2. Finally, we investigate the impact of 
maximal sub-tree number and sub-tree depth in our 
model. All of the following discussions are held on 
the training and test data. 
 
 
Rule 
 Initial Rules  Abstract Rules  
L P U Total 
BP 322,965 0 0  322,965
TR 443,010 144,459 24,871  612,340
TSR 225,570 103,932 714  330,216
 
Table 1: # of rules used in the testing ( 4d = , h =  6) 
(BP: bilingual phrase (used in Moses), TR: tree rule (on-
ly 1 tree), TSR: tree sequence rule (> 1 tree), L: fully 
lexicalized, P: partially lexicalized, U: unlexicalized) 
 
Table 1 reports the statistics of rules used in the 
experiments. It shows that:  
1) We verify that the BPs are fully covered by 
the initial rules (i.e. lexicalized rules), in which the 
lexicalized TSRs model all non-syntactic phrase 
pairs with rich syntactic information. In addition, 
we find that the number of initial rules is greater 
than that of bilingual phrases. This is because one 
bilingual phrase can be covered by more than one 
initial rule which having different sub-tree struc-
tures. 
2) Abstract rules generalize initial rules to un-
seen data and with structure reordering ability. The 
number of the abstract rule is far less than that of 
the initial rules. This is because leaf nodes of an 
abstract rule can be non-terminals that can 
represent any sub-trees using the non-terminals as 
roots.   
Fig. 4 compares the performance of different 
models. It illustrates that: 
1) Our tree sequence-based model significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empirical-
ly verifies the effect of the proposed method. 
2) Both our method and STSG outperform Mos-
es significantly. Our method also clearly outper-
forms STSG. These results suggest that: 
z The linguistically motivated structure features 
are very useful for SMT, which can be cap-
                                                          
2 To be precise, we examine the contributions of strict tree 
sequence rules and single tree rules separately in this section. 
Therefore, unless specified, the term ?tree sequence rules? 
used in this section only refers to the strict tree sequence rules, 
which must contain at least two sub-trees on the source side. 
564
tured by the two syntax-based models through 
tree node operations. 
z Our model is much more effective in utilizing 
linguistic structures than STSG since it uses 
tree sequence as basic translation unit. This 
allows our model not only to handle structure 
reordering by tree node operations in a larger 
span, but also to capture non-syntactic phras-
es, which circumvents previous syntactic 
constraints, thus giving our model more ex-
pressive power. 
3) The linguistically motivated SCFG shows 
much lower performance. This is largely because 
SCFG only allows sibling nodes reordering and fails 
to utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure transfer 
between Chinese and English when using Penn 
Treebank style linguistic grammar and under word-
alignment constraints. However, formal SCFG 
show much better performance in the formally syn-
tax-based translation framework (Chiang, 2005). 
This is because the formal syntax is learned from 
phrases directly without relying on any linguistic 
theory (Chiang, 2005). As a result, it is more ro-
bust to the issue of non-syntactic phrase usage and 
non-isomorphic structure alignment.  
24.71
26.07
23.86
22.72
21.5
22.5
23.5
24.5
25.5
26.5
SCFG Moses STSG Ours
BL
EU
(%
)
 
Figure 4: Performance comparison of different methods 
 
Rule  
Type 
TR 
(STSG) 
TR 
+TSR_L 
TR+TSR_L
+TSR_P 
TR 
+TSR 
BLEU(%) 24.71 25.72 25.93 26.07 
 
Table 2: Contributions of TSRs (see Table 1 for the de-
finitions of the abbreviations used in this table) 
 
Table 2 measures the contributions of different 
kinds of tree sequence rules. It suggests that: 
1) All the three kinds of TSRs contribute to the 
performance improvement and their combination 
further improves the performance. It suggests that 
they are complementary to each other since the 
lexicalized TSRs are used to model non-syntactic 
phrases while the other two kinds of TSRs can ge-
neralize the lexicalized rules to unseen phrases. 
2)  The lexicalized TSRs make the major con-
tribution since they can capture non-syntactic 
phrases with syntactic structure features. 
 
Rule Type BLEU (%) 
TR+TSR 26.07 
(TR+TSR) w/o SRR 24.62 
(TR+TSR) w/o DPR 25.78 
 
Table 3: Effect of Structure Reordering Rules (SRR: 
refers to the structure reordering rules that have at least 
two non-terminal leaf nodes with inverted order in the 
source and target sides, which are usually not captured 
by phrase-based models. Note that the reordering be-
tween lexical words and non-terminal leaf nodes is not 
considered here) and Discontinuous Phrase Rules (DPR: 
refers to these rules having at least one non-terminal 
leaf node between two lexicalized leaf nodes) in our 
tree sequence-based model ( 4d =  and 6h = ) 
 
Rule Type # of rules # of rules overlapped 
(Intersection) 
SRR 68,217 18,379 (26.9%) 
DPR 57,244 18,379 (32.1%) 
 
Table 4: numbers of SRR and DPR rules 
 
Table 3 shows the contributions of SRR and 
DPR. It clearly indicates that SRRs are very effec-
tive in reordering structures, which improve per-
formance by 1.45 (26.07-24.62) BLEU score. 
However, DPRs have less impact on performance 
in our tree sequence-based model. This seems in 
contradiction to the previous observations3 in lite-
rature. However, it is not surprising simply be-
cause we use tree sequences as the basic translation 
units. Thereby, our model can capture all phrases. 
In this sense, our model behaves like a phrase-
based model, less sensitive to discontinuous phras-
                                                          
3 Wellington et al (2006) reports that discontinuities are very 
useful for translational equivalence analysis using binary-
branching structures under word alignment and parse tree 
constraints while they are almost of no use if under word 
alignment constraints only. Bod (2007) finds that discontinues 
phrase rules make significant performance improvement in 
linguistically STSG-based SMT models. 
565
es (Wellington et al, 2006). Our additional expe-
riments also verify that discontinuous phrase rules 
are complementary to syntactic phrase rules (Bod, 
2007) while non-syntactic phrase rules may com-
promise the contribution of discontinuous phrase 
rules. Table 4 reports the numbers of these two 
kinds of rules. It shows that around 30% rules are 
shared by the two kinds of rule sets. These over-
lapped rules contain at least two non-terminal leaf 
nodes plus two terminal leaf nodes, which implies 
that longer rules do not affect performance too 
much. 
 
22.07
25.28
26.1425.94 26.02 26.07
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5 6
BL
EU
(%
)
 
Figure 5: Accuracy changing with different max-
imal tree depths ( h = 1 to 6 when 4d = ) 
 
22.72
24.71
26.0526.03 26.07
25.74
25.2925.2825.2624.78
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5
B
LE
U
(%
)
 
Figure 6: Accuracy changing with the different maximal 
number of trees in a tree sequence ( d =1 to 5), the upper 
line is for 6h =  while the lower line is for 2h = .  
 
Fig. 5 studies the impact when setting different 
maximal tree depth ( h ) in a rule on the perfor-
mance. It demonstrates that:  
1) Significant performance improvement is 
achieved when the value of h  is increased from 1 
to 2. This can be easily explained by the fact that 
when h = 1, only monotonic search is conducted, 
while h = 2 allows non-terminals to be leaf nodes, 
thus introducing preliminary structure features to 
the search and allowing non-monotonic search. 
2) Internal structures and large span (due to h  
increasing) are also useful as attested by the gain 
of 0.86 (26.14-25.28) Blue score when the value of 
h  increases from 2 to 4. 
Fig. 6 studies the impact on performance by set-
ting different maximal tree number (d) in a rule. It 
further indicates that: 
1) Tree sequence rules (d >1) are useful and 
even more helpful if we limit the tree depth to no 
more than two (lower line, h=2). However, tree 
sequence rules consisting of more than three sub-
trees have almost no contribution to the perform-
ance improvement. This is mainly due to data 
sparseness issue when d >3. 
2) Even if only two-layer sub-trees (lower line) 
are allowed, our method still outperforms STSG 
and Moses when d>1. This further validates the 
effectiveness of our design philosophy of using 
multi-sub-trees as basic translation unit in SMT. 
7 Conclusions and Future Work 
In this paper, we present a tree sequence align-
ment-based translation model to combine the 
strengths of phrase-based and syntax-based me-
thods. The experimental results on the NIST MT-
2005 Chinese-English translation task demonstrate 
the effectiveness of the proposed model. Our study 
also finds that in our model the tree sequence rules 
are very useful since they can model non-syntactic 
phrases and reorderings with rich linguistic struc-
ture features while discontinuous phrases and tree 
sequence rules with more than three sub-trees have 
less impact on performance. 
There are many interesting research topics on 
the tree sequence-based translation model worth 
exploring in the future. The current method ex-
tracts large amount of rules. Many of them are re-
dundant, which make decoding very slow. Thus, 
effective rule optimization and pruning algorithms 
are highly desirable. Ideally, a linguistically and 
empirically motivated theory can be worked out, 
suggesting what kinds of rules should be extracted 
given an input phrase pair. For example, most 
function words and headwords can be kept in ab-
stract rules as features. In addition, word align-
ment is a hard constraint in our rule extraction. We 
will study direct structure alignments to reduce the 
impact of word alignment errors. We are also in-
terested in comparing our method with the forest-
to-string model (Liu et al, 2007). Finally, we 
would also like to study unsupervised learning-
based bilingual parsing for SMT.  
566
 References  
Rens Bod. 2007. Unsupervised Syntax-Based Machine 
Translation: The Contribution of Discontinuous 
Phrases. MT-Summmit-07. 51-56. 
David Chiang. 2005. A hierarchical phrase-based mod-
el for SMT. ACL-05. 263-270. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree transla-
tion. EMNLP-06. 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sertion grammars. ACL-05. 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? HLT-
NAACL-04. 
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
Daniel Gildea. 2003. Loosely Tree-Based Alignment for 
Machine Translation. ACL-03. 80-87. 
Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers. HLT-NAACL-2004. 105-112. 
Mary Hearne and Andy Way. 2003. Seeing the wood for 
the trees: data-oriented translation. MT Summit IX, 
165-172. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistic-
al phrase-based translation. HLT-NAACL-03. 127-
133. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. AMTA-04, 115-124 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statistical 
machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and Hermann Ney. 2004a. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk and Arul Menezes. 2006. Do we need 
phrases? Challenging the conventional wisdom in 
SMT. COLING-ACL-06. 9-16. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
Stefan Riezler and John T. Maxwell III. 2006. Gram-
matical Machine Translation. HLT-NAACL-06. 
248-255. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. 
Ordering Phrases with Function Words. ACL-7. 
712-719. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COLING-
ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521? 528. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04. 2051-
2054. 
567
Proceedings of ACL-08: HLT, pages 780?788,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora
Shiqi Zhao1, Haifeng Wang2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Toshiba (China) Research and Development Center, Beijing, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
Paraphrase patterns are useful in paraphrase
recognition and generation. In this paper, we
present a pivot approach for extracting para-
phrase patterns from bilingual parallel cor-
pora, whereby the English paraphrase patterns
are extracted using the sentences in a for-
eign language as pivots. We propose a log-
linear model to compute the paraphrase likeli-
hood of two patterns and exploit feature func-
tions based on maximum likelihood estima-
tion (MLE) and lexical weighting (LW). Us-
ing the presented method, we extract over
1,000,000 pairs of paraphrase patterns from
2M bilingual sentence pairs, the precision
of which exceeds 67%. The evaluation re-
sults show that: (1) The pivot approach is
effective in extracting paraphrase patterns,
which significantly outperforms the conven-
tional method DIRT. Especially, the log-linear
model with the proposed feature functions
achieves high performance. (2) The coverage
of the extracted paraphrase patterns is high,
which is above 84%. (3) The extracted para-
phrase patterns can be classified into 5 types,
which are useful in various applications.
1 Introduction
Paraphrases are different expressions that convey
the same meaning. Paraphrases are important in
plenty of natural language processing (NLP) ap-
plications, such as question answering (QA) (Lin
and Pantel, 2001; Ravichandran and Hovy, 2002),
machine translation (MT) (Kauchak and Barzilay,
2006; Callison-Burch et al, 2006), multi-document
summarization (McKeown et al, 2002), and natural
language generation (Iordanskaja et al, 1991).
Paraphrase patterns are sets of semantically
equivalent patterns, in which a pattern generally
contains two parts, i.e., the pattern words and slots.
For example, in the pattern ?X solves Y?, ?solves? is
the pattern word, while ?X? and ?Y? are slots. One
can generate a text unit (phrase or sentence) by fill-
ing the pattern slots with specific words. Paraphrase
patterns are useful in both paraphrase recognition
and generation. In paraphrase recognition, if two
text units match a pair of paraphrase patterns and the
corresponding slot-fillers are identical, they can be
identified as paraphrases. In paraphrase generation,
a text unit that matches a pattern P can be rewritten
using the paraphrase patterns of P.
A variety of methods have been proposed on para-
phrase patterns extraction (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Ibrahim et al, 2003;
Pang et al, 2003; Szpektor et al, 2004). However,
these methods have some shortcomings. Especially,
the precisions of the paraphrase patterns extracted
with these methods are relatively low.
In this paper, we extract paraphrase patterns from
bilingual parallel corpora based on a pivot approach.
We assume that if two English patterns are aligned
with the same pattern in another language, they are
likely to be paraphrase patterns. This assumption
is an extension of the one presented in (Bannard
and Callison-Burch, 2005), which was used for de-
riving phrasal paraphrases from bilingual corpora.
Our method involves three steps: (1) corpus prepro-
cessing, including English monolingual dependency
780
parsing and English-foreign language word align-
ment, (2) aligned patterns induction, which produces
English patterns along with the aligned pivot pat-
terns in the foreign language, (3) paraphrase pat-
terns extraction, in which paraphrase patterns are ex-
tracted based on a log-linear model.
Our contributions are as follows. Firstly, we are
the first to use a pivot approach to extract paraphrase
patterns from bilingual corpora, though similar
methods have been used for learning phrasal para-
phrases. Our experiments show that the pivot ap-
proach significantly outperforms conventional meth-
ods. Secondly, we propose a log-linear model for
computing the paraphrase likelihood. Besides, we
use feature functions based on maximum likeli-
hood estimation (MLE) and lexical weighting (LW),
which are effective in extracting paraphrase patterns.
Using the proposed approach, we extract over
1,000,000 pairs of paraphrase patterns from 2M
bilingual sentence pairs, the precision of which is
above 67%. Experimental results show that the pivot
approach evidently outperforms DIRT, a well known
method that extracts paraphrase patterns frommono-
lingual corpora (Lin and Pantel, 2001). Besides, the
log-linear model is more effective than the conven-
tional model presented in (Bannard and Callison-
Burch, 2005). In addition, the coverage of the ex-
tracted paraphrase patterns is high, which is above
84%. Further analysis shows that 5 types of para-
phrase patterns can be extracted with our method,
which can by used in multiple NLP applications.
The rest of this paper is structured as follows.
Section 2 reviews related work on paraphrase pat-
terns extraction. Section 3 presents our method in
detail. We evaluate the proposed method in Section
4, and finally conclude this paper in Section 5.
2 Related Work
Paraphrase patterns have been learned and used in
information extraction (IE) and answer extraction of
QA. For example, Lin and Pantel (2001) proposed a
method (DIRT), in which they obtained paraphrase
patterns from a parsed monolingual corpus based on
an extended distributional hypothesis, where if two
paths in dependency trees tend to occur in similar
contexts it is hypothesized that the meanings of the
paths are similar. The examples of obtained para-
(1) X solves Y
Y is solved by X
X finds a solution to Y
......
(2) born in <ANSWER> , <NAME>
<NAME> was born on <ANSWER> ,
<NAME> ( <ANSWER> -
......
(3) ORGANIZATION decides ?
ORGANIZATION confirms ?
......
Table 1: Examples of paraphrase patterns extracted with
the methods of Lin and Pantel (2001), Ravichandran and
Hovy (2002), and Shinyama et al (2002).
phrase patterns are shown in Table 1 (1).
Based on the same hypothesis as above, some
methods extracted paraphrase patterns from the web.
For instance, Ravichandran and Hovy (2002) de-
fined a question taxonomy for their QA system.
They then used hand-crafted examples of each ques-
tion type as queries to retrieve paraphrase patterns
from the web. For instance, for the question type
?BIRTHDAY?, The paraphrase patterns produced by
their method can be seen in Table 1 (2).
Similar methods have also been used by Ibrahim
et al (2003) and Szpektor et al (2004). The main
disadvantage of the above methods is that the pre-
cisions of the learned paraphrase patterns are rela-
tively low. For instance, the precisions of the para-
phrase patterns reported in (Lin and Pantel, 2001),
(Ibrahim et al, 2003), and (Szpektor et al, 2004)
are lower than 50%. Ravichandran and Hovy (2002)
did not directly evaluate the precision of the para-
phrase patterns extracted using their method. How-
ever, the performance of their method is dependent
on the hand-crafted queries for web mining.
Shinyama et al (2002) presented a method that
extracted paraphrase patterns from multiple news ar-
ticles about the same event. Their method was based
on the assumption that NEs are preserved across
paraphrases. Thus the method acquired paraphrase
patterns from sentence pairs that share comparable
NEs. Some examples can be seen in Table 1 (3).
The disadvantage of this method is that it greatly
relies on the number of NEs in sentences. The preci-
781
start Palestinian suicide bomber blew himself up in SLOT1 on SLOT2
killing SLOT3 other people and injuringwounding SLOT4 end
detroit the*e*
a
?s*e* buildingbuilding in detroit
flattened
groundlevelled
to
blastedleveled*e*wasreduced
razedleveled
to downrubble
into ashes*e*
to *e*
(1)
(2)
Figure 1: Examples of paraphrase patterns extracted by
Barzilay and Lee (2003) and Pang et al (2003).
sion of the extracted patterns may sharply decrease
if the sentences do not contain enough NEs.
Barzilay and Lee (2003) applied multi-sequence
alignment (MSA) to parallel news sentences and in-
duced paraphrase patterns for generating new sen-
tences (Figure 1 (1)). Pang et al (2003) built finite
state automata (FSA) from semantically equivalent
translation sets based on syntactic alignment. The
learned FSAs could be used in paraphrase represen-
tation and generation (Figure 1 (2)). Obviously, it
is difficult for a sentence to match such complicated
patterns, especially if the sentence is not from the
same domain in which the patterns are extracted.
Bannard and Callison-Burch (2005) first ex-
ploited bilingual corpora for phrasal paraphrase ex-
traction. They assumed that if two English phrases
e1 and e2 are aligned with the same phrase c in
another language, these two phrases may be para-
phrases. Specifically, they computed the paraphrase
probability in terms of the translation probabilities:
p(e2|e1) =
?
c
pMLE(c|e1)pMLE(e2|c) (1)
In Equation (1), pMLE(c|e1) and pMLE(e2|c) are
the probabilities of translating e1 to c and c to e2,
which are computed based on MLE:
pMLE(c|e1) =
count(c, e1)
?
c? count(c
?, e1)
(2)
where count(c, e1) is the frequency count that
phrases c and e1 are aligned in the corpus.
pMLE(e2|c) is computed in the same way.
This method proved effective in extracting high
quality phrasal paraphrases. As a result, we extend
it to paraphrase pattern extraction in this paper.
S T E (take)
should
We take
market
into
consideration
take
market
into
consideration
take
into
consideration
P S T E (take)
first
T E
demand
demand
Figure 2: Examples of a subtree and a partial subtree.
3 Proposed Method
3.1 Corpus Preprocessing
In this paper, we use English paraphrase patterns ex-
traction as a case study. An English-Chinese (E-
C) bilingual parallel corpus is employed for train-
ing. The Chinese part of the corpus is used as pivots
to extract English paraphrase patterns. We conduct
word alignment with Giza++ (Och and Ney, 2000) in
both directions and then apply the grow-diag heuris-
tic (Koehn et al, 2005) for symmetrization.
Since the paraphrase patterns are extracted from
dependency trees, we parse the English sentences
in the corpus with MaltParser (Nivre et al, 2007).
Let SE be an English sentence, TE the parse tree
of SE , e a word of SE , we define the subtree and
partial subtree following the definitions in (Ouan-
graoua et al, 2007). In detail, a subtree STE(e)
is a particular connected subgraph of the tree TE ,
which is rooted at e and includes all the descendants
of e. A partial subtree PSTE(e) is a connected sub-
graph of the subtree STE(e), which is rooted at e but
does not necessarily include all the descendants of e.
For instance, for the sentence ?We should first take
market demand into consideration?, STE(take) and
PSTE(take) are shown in Figure 21.
3.2 Aligned Patterns Induction
To induce the aligned patterns, we first induce the
English patterns using the subtrees and partial sub-
trees. Then, we extract the pivot Chinese patterns
aligning to the English patterns.
1Note that, a subtree may contain several partial subtrees. In
this paper, all the possible partial subtrees are considered when
extracting paraphrase patterns.
782
Algorithm 1: Inducing an English pattern
1: Input: words in STE(e) : wiwi+1...wj
2: Input: PE(e) = ?
3: For each wk (i ? k ? j)
4: If wk is in PSTE(e)
5: Append wk to the end of PE(e)
6: Else
7: Append POS(wk) to the end of PE(e)
8: End For
Algorithm 2: Inducing an aligned pivot pattern
1: Input: SC = t1t2...tn
2: Input: PC = ?
3: For each tl (1 ? l ? n)
4: If tl is aligned with wk in SE
5: If wk is a word in PE(e)
6: Append tl to the end of PC
7: If POS(wk) is a slot in PE(e)
8: Append POS(wk) to the end of PC
9: End For
Step-1 Inducing English patterns. In this paper, an
English pattern PE(e) is a string comprising words
and part-of-speech (POS) tags. Our intuition for
inducing an English pattern is that a partial sub-
tree PSTE(e) can be viewed as a unit that conveys
a definite meaning, though the words in PSTE(e)
may not be continuous. For example, PSTE(take)
in Figure 2 contains words ?take ... into consid-
eration?. Therefore, we may extract ?take X into
consideration? as a pattern. In addition, the words
that are in STE(e) but not in PSTE(e) (denoted as
STE(e)/PSTE(e)) are also useful for inducing pat-
terns, since they can constrain the pattern slots. In
the example in Figure 2, the word ?demand? indi-
cates that a noun can be filled in the slot X and the
pattern may have the form ?take NN into considera-
tion?. Based on this intuition, we induce an English
pattern PE(e) as in Algorithm 12.
For the example in Figure 2, the generated pat-
tern PE(take) is ?take NN NN into considera-
tion?. Note that the patterns induced in this way
are quite specific, since the POS of each word in
STE(e)/PSTE(e) forms a slot. Such patterns are
difficult to be matched in applications. We there-
2POS(wk) in Algorithm 1 denotes the POS tag of wk.
N N _1 ?? NN _2 NN_1 ?? N N _2
NN_1NN_2 considered byis NN_1 consider NN_2
Figure 3: Aligned patterns with numbered slots.
fore take an additional step to simplify the patterns.
Let ei and ej be two words in STE(e)/PSTE(e),
whose POS posi and posj are slots in PE(e). If ei
is a descendant of ej in the parse tree, we remove
posi from PE(e). For the example above, the POS
of ?market? is removed, since it is the descendant of
?demand?, whose POS also forms a slot. The sim-
plified pattern is ?take NN into consideration?.
Step-2 Extracting pivot patterns. For each En-
glish pattern PE(e), we extract an aligned Chinese
pivot pattern PC . Let a Chinese sentence SC be the
translation of the English sentence SE , PE(e) a pat-
tern induced from SE , we extract the pivot pattern
PC aligning to PE(e) as in Algorithm 2. Note that
the Chinese patterns are not extracted from parse
trees. They are only sequences of Chinese words
and POSes that are aligned with English patterns.
A pattern may contain two or more slots shar-
ing the same POS. To distinguish them, we assign
a number to each slot in the aligned E-C patterns. In
detail, the slots having identical POS in PC are num-
bered incrementally (i.e., 1,2,3...), while each slot in
PE(e) is assigned the same number as its aligned
slot in PC . The examples of the aligned patterns
with numbered slots are illustrated in Figure 3.
3.3 Paraphrase Patterns Extraction
As mentioned above, if patterns e1 and e2 are
aligned with the same pivot pattern c, e1 and e2 may
be paraphrase patterns. The paraphrase likelihood
can be computed using Equation (1). However, we
find that using only the MLE based probabilities can
suffer from data sparseness. In order to exploit more
and richer information to estimate the paraphrase
likelihood, we propose a log-linear model:
score(e2|e1) =
?
c
exp[
N?
i=1
?ihi(e1, e2, c)] (3)
where hi(e1, e2, c) is a feature function and ?i is the
783
weight. In this paper, 4 feature functions are used in
our log-linear model, which include:
h1(e1, e2, c) = scoreMLE(c|e1)
h2(e1, e2, c) = scoreMLE(e2|c)
h3(e1, e2, c) = scoreLW (c|e1)
h4(e1, e2, c) = scoreLW (e2|c)
Feature functions h1(e1, e2, c) and h2(e1, e2, c)
are based on MLE. scoreMLE(c|e) is computed as:
scoreMLE(c|e) = log pMLE(c|e) (4)
scoreMLE(e|c) is computed in the same way.
h3(e1, e2, c) and h4(e1, e2, c) are based on LW.
LW was originally used to validate the quality of a
phrase translation pair in MT (Koehn et al, 2003). It
checks how well the words of the phrases translate
to each other. This paper uses LW to measure the
quality of aligned patterns. We define scoreLW (c|e)
as the logarithm of the lexical weight3:
scoreLW (c|e) =
1
n
n?
i=1
log(
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(ci|ej)) (5)
where a denotes the word alignment between c and
e. n is the number of words in c. ci and ej are words
of c and e. w(ci|ej) is computed as follows:
w(ci|ej) =
count(ci, ej)
?
c?i
count(c?i, ej)
(6)
where count(ci, ej) is the frequency count of
the aligned word pair (ci, ej) in the corpus.
scoreLW (e|c) is computed in the same manner.
In our experiments, we set a threshold T . If the
score between e1 and e2 based on Equation (3) ex-
ceeds T , e2 is extracted as the paraphrase of e1.
3.4 Parameter Estimation
Five parameters need to be estimated, i.e., ?1, ?2,
?3, ?4 in Equation (3), and the threshold T . To
estimate the parameters, we first construct a devel-
opment set. In detail, we randomly sample 7,086
3The logarithm of the lexical weight is divided by n so as
not to penalize long patterns.
groups of aligned E-C patterns that are obtained as
described in Section 3.2. The English patterns in
each group are all aligned with the same Chinese
pivot pattern. We then extract paraphrase patterns
from the aligned patterns as described in Section 3.3.
In this process, we set ?i = 1 (i = 1, ..., 4) and as-
sign T a minimum value, so as to obtain all possible
paraphrase patterns.
A total of 4,162 pairs of paraphrase patterns have
been extracted and manually labeled as ?1? (correct
paraphrase patterns) or ?0? (incorrect). Here, two
patterns are regarded as paraphrase patterns if they
can generate paraphrase fragments by filling the cor-
responding slots with identical words. We use gra-
dient descent algorithm (Press et al, 1992) to esti-
mate the parameters. For each set of parameters, we
compute the precision P , recall R, and f-measure
F as: P = |set1?set2||set1| , R =
|set1?set2|
|set2| , F =
2PR
P+R ,
where set1 denotes the set of paraphrase patterns ex-
tracted under the current parameters. set2 denotes
the set of manually labeled correct paraphrase pat-
terns. We select the parameters that can maximize
the F-measure on the development set4.
4 Experiments
The E-C parallel corpus in our experiments was con-
structed using several LDC bilingual corpora5. After
filtering sentences that are too long (> 40 words) or
too short (< 5 words), 2,048,009 pairs of parallel
sentences were retained.
We used two constraints in the experiments to im-
prove the efficiency of computation. First, only sub-
trees containing no more than 10 words were used to
induce English patterns. Second, although any POS
tag can form a slot in the induced patterns, we only
focused on three kinds of POSes in the experiments,
i.e., nouns (tags include NN, NNS, NNP, NNPS),
verbs (VB, VBD, VBG, VBN, VBP, VBZ), and ad-
jectives (JJ, JJS, JJR). In addition, we constrained
that a pattern must contain at least one content word
4The parameters are: ?1 = 0.0594137, ?2 = 0.995936,
?3 = ?0.0048954, ?4 = 1.47816, T = ?10.002.
5The corpora include LDC2000T46, LDC2000T47,
LDC2002E18, LDC2002T01, LDC2003E07, LDC2003E14,
LDC2003T17, LDC2004E12, LDC2004T07, LDC2004T08,
LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T04,
LDC2007T02, LDC2007T09.
784
Method #PP (pairs) Precision
LL-Model 1,058,624 67.03%
MLE-Model 1,015,533 60.60%
DIRT top-1 1,179 19.67%
DIRT top-5 5,528 18.73%
Table 2: Comparison of paraphrasing methods.
so as to filter patterns like ?the [NN 1]?.
4.1 Evaluation of the Log-linear Model
As previously mentioned, in the log-linear model of
this paper, we use both MLE based and LW based
feature functions. In this section, we evaluate the
log-linear model (LL-Model) and compare it with
the MLE based model (MLE-Model) presented by
Bannard and Callison-Burch (2005)6.
We extracted paraphrase patterns using two mod-
els, respectively. From the results of each model,
we randomly picked 3,000 pairs of paraphrase pat-
terns to evaluate the precision. The 6,000 pairs of
paraphrase patterns were mixed and presented to the
human judges, so that the judges cannot know by
which model each pair was produced. The sampled
patterns were then manually labeled and the preci-
sion was computed as described in Section 3.4.
The number of the extracted paraphrase patterns
(#PP) and the precision are depicted in the first two
lines of Table 2. We can see that the numbers of
paraphrase patterns extracted using the two mod-
els are comparable. However, the precision of LL-
Model is significantly higher than MLE-Model.
Actually, MLE-Model is a special case of LL-
Model and the enhancement of the precision is
mainly due to the use of LW based features.
It is not surprising, since Bannard and Callison-
Burch (2005) have pointed out that word alignment
error is the major factor that influences the perfor-
mance of the methods learning paraphrases from
bilingual corpora. The LW based features validate
the quality of word alignment and assign low scores
to those aligned E-C pattern pairs with incorrect
alignment. Hence the precision can be enhanced.
6In this experiment, we also estimated a threshold T ? for
MLE-Model using the development set (T ? = ?5.1). The pat-
tern pairs whose score based on Equation (1) exceed T ? were
extracted as paraphrase patterns.
4.2 Comparison with DIRT
It is necessary to compare our method with another
paraphrase patterns extraction method. However, it
is difficult to find methods that are suitable for com-
parison. Some methods only extract paraphrase pat-
terns using news articles on certain topics (Shinyama
et al, 2002; Barzilay and Lee, 2003), while some
others need seeds as initial input (Ravichandran and
Hovy, 2002). In this paper, we compare our method
with DIRT (Lin and Pantel, 2001), which does not
need to specify topics or input seeds.
As mentioned in Section 2, DIRT learns para-
phrase patterns from a parsed monolingual corpus
based on an extended distributional hypothesis. In
our experiment, we implemented DIRT and ex-
tracted paraphrase patterns from the English part of
our bilingual parallel corpus. Our corpus is smaller
than that reported in (Lin and Pantel, 2001). To alle-
viate the data sparseness problem, we only kept pat-
terns appearing more than 10 times in the corpus for
extracting paraphrase patterns. Different from our
method, no threshold was set in DIRT. Instead, the
extracted paraphrase patterns were ranked accord-
ing to their scores. In our experiment, we kept top-5
paraphrase patterns for each target pattern.
From the extracted paraphrase patterns, we sam-
pled 600 groups for evaluation. Each group com-
prises a target pattern and its top-5 paraphrase pat-
terns. The sampled data were manually labeled and
the top-n precision was calculated as
PN
i=1 ni
N?n , where
N is the number of groups and ni is the number of
correct paraphrase patterns in the top-n paraphrase
patterns of the i-th group. The top-1 and top-5 re-
sults are shown in the last two lines of Table 2. Al-
though there are more correct patterns in the top-5
results, the precision drops sequentially from top-1
to top-5 since the denominator of top-5 is 4 times
larger than that of top-1.
Obviously, the number of the extracted para-
phrase patterns is much smaller than that extracted
using our method. Besides, the precision is also
much lower. We believe that there are two reasons.
First, the extended distributional hypothesis is not
strict enough. Patterns sharing similar slot-fillers do
not necessarily have the same meaning. They may
even have the opposite meanings. For example, ?X
worsens Y? and ?X solves Y? were extracted as para-
785
Type Count Example
trivial change 79 (e1) all the members of [NNPS 1] (e2) all members of [NNPS 1]
phrase replacement 267 (e1) [JJ 1] economic losses (e2) [JJ 1] financial losses
phrase reordering 56 (e1) [NN 1] definition (e2) the definition of [NN 1]
structural paraphrase 71 (e1) the admission of [NNP 1] to the wto (e2) the [NNP 1] ?s wto accession
information + or - 27 (e1) [NNS 1] are in fact women (e2) [NNS 1] are women
Table 3: The statistics and examples of each type of paraphrase patterns.
phrase patterns by DIRT. The other reason is that
DIRT can only be effective for patterns appearing
plenty of times in the corpus. In other words, it seri-
ously suffers from data sparseness. We believe that
DIRT can perform better on a larger corpus.
4.3 Pivot Pattern Constraints
As described in Section 3.2, we constrain that the
pattern words of an English pattern e must be ex-
tracted from a partial subtree. However, we do not
have such constraint on the Chinese pivot patterns.
Hence, it is interesting to investigate whether the
performance can be improved if we constrain that
the pattern words of a pivot pattern c must also be
extracted from a partial subtree.
To conduct the evaluation, we parsed the Chinese
sentences of the corpus with a Chinese dependency
parser (Liu et al, 2006). We then induced English
patterns and extracted aligned pivot patterns. For the
aligned patterns (e, c), if c?s pattern words were not
extracted from a partial subtree, the pair was filtered.
After that, we extracted paraphrase patterns, from
which we sampled 3,000 pairs for evaluation.
The results show that 736,161 pairs of paraphrase
patterns were extracted and the precision is 65.77%.
Compared with Table 2, the number of the extracted
paraphrase patterns gets smaller and the precision
also gets lower. The results suggest that the perfor-
mance of the method cannot be improved by con-
straining the extraction of pivot patterns.
4.4 Analysis of the Paraphrase Patterns
We sampled 500 pairs of correct paraphrase pat-
terns extracted using our method and analyzed the
types. We found that there are 5 types of para-
phrase patterns, which include: (1) trivial change,
such as changes of prepositions and articles, etc; (2)
phrase replacement; (3) phrase reordering; (4) struc-
tural paraphrase, which contain both phrase replace-
ments and phrase reordering; (5) adding or reducing
information that does not change the meaning. Some
statistics and examples are shown in Table 3.
The paraphrase patterns are useful in NLP appli-
cations. Firstly, over 50% of the paraphrase patterns
are in the type of phrase replacement, which can
be used in IE pattern reformulation and sentence-
level paraphrase generation. Compared with phrasal
paraphrases, the phrase replacements in patterns are
more accurate due to the constraints of the slots.
The paraphrase patterns in the type of phrase re-
ordering can also be used in IE pattern reformula-
tion and sentence paraphrase generation. Especially,
in sentence paraphrase generation, this type of para-
phrase patterns can reorder the phrases in a sentence,
which can hardly be achieved by the conventional
MT-based generation method (Quirk et al, 2004).
The structural paraphrase patterns have the advan-
tages of both phrase replacement and phrase reorder-
ing. More paraphrase sentences can be generated
using these patterns.
The paraphrase patterns in the type of ?informa-
tion + and -? are useful in sentence compression and
expansion. A sentence matching a long pattern can
be compressed by paraphrasing it using shorter pat-
terns. Similarly, a short sentence can be expanded
by paraphrasing it using longer patterns.
For the 3,000 pairs of test paraphrase patterns, we
also investigate the number and type of the pattern
slots. The results are summarized in Table 4 and 5.
From Table 4, we can see that more than 92%
of the paraphrase patterns contain only one slot,
just like the examples shown in Table 3. In addi-
tion, about 7% of the paraphrase patterns contain
two slots, such as ?give [NN 1] [NN 2]? vs. ?give
[NN 2] to [NN 1]?. This result suggests that our
method tends to extract short paraphrase patterns,
786
Slot No. #PP Percentage Precision
1-slot 2,780 92.67% 66.51%
2-slots 218 7.27% 73.85%
?3-slots 2 <1% 50.00%
Table 4: The statistics of the numbers of pattern slots.
Slot Type #PP Percentage Precision
N-slots 2,376 79.20% 66.71%
V-slots 273 9.10% 70.33%
J-slots 438 14.60% 70.32%
Table 5: The statistics of the type of pattern slots.
which is mainly because the data sparseness prob-
lem is more serious when extracting long patterns.
From Table 5, we can find that near 80% of the
paraphrase patterns contain noun slots, while about
9% and 15% contain verb slots and adjective slots7.
This result implies that nouns are the most typical
variables in paraphrase patterns.
4.5 Evaluation within Context Sentences
In Section 4.1, we have evaluated the precision of
the paraphrase patterns without considering context
information. In this section, we evaluate the para-
phrase patterns within specific context sentences.
The open test set includes 119 English sentences.
We parsed the sentences with MaltParser and in-
duced patterns as described in Section 3.2. For each
pattern e in sentence SE , we searched e?s paraphrase
patterns from the database of the extracted para-
phrase patterns. The result shows that 101 of the
119 sentences contain at least one pattern that can
be paraphrased using the extracted paraphrase pat-
terns, the coverage of which is 84.87%.
Furthermore, since a pattern may have several
paraphrase patterns, we exploited a method to au-
tomatically select the best one in the given context
sentence. In detail, a paraphrase pattern e? of e was
reranked based on a language model (LM):
score(e?|e, SE) =
?scoreLL(e
?|e) + (1 ? ?)scoreLM (e
?|SE) (7)
7Notice that, a pattern may contain more than one type of
slots, thus the sum of the percentages is larger than 1.
Here, scoreLL(e?|e) denotes the score based on
Equation (3). scoreLM (e?|SE) is the LM based
score: scoreLM (e?|SE) = 1n logPLM (S
?
E), where
S?E is the sentence generated by replacing e in SE
with e?. The language model in the experiment was
a tri-gram model trained using the English sentences
in the bilingual corpus. We empirically set ? = 0.7.
The selected best paraphrase patterns in context
sentences were manually labeled. The context infor-
mation was also considered by our judges. The re-
sult shows that the precision of the best paraphrase
patterns is 59.39%. To investigate the contribution
of the LM based score, we ran the experiment again
with ? = 1 (ignoring the LM based score) and found
that the precision is 57.09%. It indicates that the LM
based reranking can improve the precision. How-
ever, the improvement is small. Further analysis
shows that about 70% of the correct paraphrase sub-
stitutes are in the type of phrase replacement.
5 Conclusion
This paper proposes a pivot approach for extracting
paraphrase patterns from bilingual corpora. We use
a log-linear model to compute the paraphrase like-
lihood and exploit feature functions based on MLE
and LW. Experimental results show that the pivot ap-
proach is effective, which extracts over 1,000,000
pairs of paraphrase patterns from 2M bilingual sen-
tence pairs. The precision and coverage of the ex-
tracted paraphrase patterns exceed 67% and 84%,
respectively. In addition, the log-linear model with
the proposed feature functions significantly outper-
forms the conventional models. Analysis shows that
5 types of paraphrase patterns are extracted with our
method, which are useful in various applications.
In the future we wish to exploit more feature func-
tions in the log-linear model. In addition, we will try
to make better use of the context information when
replacing paraphrase patterns in context sentences.
Acknowledgments
This research was supported by National Nat-
ural Science Foundation of China (60503072,
60575042). We thank Lin Zhao, Xiaohang Qu, and
Zhenghua Li for their help in the experiments.
787
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolingual
Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in a
Meaning-Text Generation Model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann (Eds.):
Natural Language Generation in Artificial Intelligence
and Computational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li. 2006.
Dependency Parsing Based on Dynamic Local Opti-
mization. In Proceedings of CoNLL-X, pages 211-215.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Natural Language Engineering 13(2): 95-
135.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
A??da Ouangraoua, Pascal Ferraro, Laurent Tichit, and
Serge Dulucq. 2007. Local Similarity between Quo-
tiented Ordered Trees. In Journal of Discrete Algo-
rithms 5(1): 23-35.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP,
pages 41-48.
788
Proceedings of ACL-08: HLT, pages 843?851,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Entity-Mention Model for Coreference Resolution
with Inductive Logic Programming
Xiaofeng Yang1 Jian Su1 Jun Lang2
Chew Lim Tan3 Ting Liu2 Sheng Li2
1Institute for Infocomm Research
{xiaofengy,sujian}@i2r.a-star.edu.sg
2Harbin Institute of Technology
{bill lang,tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
3National University of Singapore,
tancl@comp.nus.edu.sg
Abstract
The traditional mention-pair model for coref-
erence resolution cannot capture information
beyond mention pairs for both learning and
testing. To deal with this problem, we present
an expressive entity-mention model that per-
forms coreference resolution at an entity level.
The model adopts the Inductive Logic Pro-
gramming (ILP) algorithm, which provides a
relational way to organize different knowledge
of entities and mentions. The solution can
explicitly express relations between an entity
and the contained mentions, and automatically
learn first-order rules important for corefer-
ence decision. The evaluation on the ACE data
set shows that the ILP based entity-mention
model is effective for the coreference resolu-
tion task.
1 Introduction
Coreference resolution is the process of linking mul-
tiple mentions that refer to the same entity. Most
of previous work adopts the mention-pair model,
which recasts coreference resolution to a binary
classification problem of determining whether or not
two mentions in a document are co-referring (e.g.
Aone and Bennett (1995); McCarthy and Lehnert
(1995); Soon et al (2001); Ng and Cardie (2002)).
Although having achieved reasonable success, the
mention-pair model has a limitation that informa-
tion beyond mention pairs is ignored for training and
testing. As an individual mention usually lacks ad-
equate descriptive information of the referred entity,
it is often difficult to judge whether or not two men-
tions are talking about the same entity simply from
the pair alone.
An alternative learning model that can overcome
this problem performs coreference resolution based
on entity-mention pairs (Luo et al, 2004; Yang et
al., 2004b). Compared with the traditional mention-
pair counterpart, the entity-mention model aims to
make coreference decision at an entity level. Classi-
fication is done to determine whether a mention is a
referent of a partially found entity. A mention to be
resolved (called active mention henceforth) is linked
to an appropriate entity chain (if any), based on clas-
sification results.
One problem that arises with the entity-mention
model is how to represent the knowledge related to
an entity. In a document, an entity may have more
than one mention. It is impractical to enumerate all
the mentions in an entity and record their informa-
tion in a single feature vector, as it would make the
feature space too large. Even worse, the number of
mentions in an entity is not fixed, which would re-
sult in variant-length feature vectors and make trou-
ble for normal machine learning algorithms. A solu-
tion seen in previous work (Luo et al, 2004; Culotta
et al, 2007) is to design a set of first-order features
summarizing the information of the mentions in an
entity, for example, ?whether the entity has any men-
tion that is a name alias of the active mention?? or
?whether most of the mentions in the entity have the
same head word as the active mention?? These fea-
tures, nevertheless, are designed in an ad-hoc man-
ner and lack the capability of describing each indi-
vidual mention in an entity.
In this paper, we present a more expressive entity-
843
mention model for coreference resolution. The
model employs Inductive Logic Programming (ILP)
to represent the relational knowledge of an active
mention, an entity, and the mentions in the entity. On
top of this, a set of first-order rules is automatically
learned, which can capture the information of each
individual mention in an entity, as well as the global
information of the entity, to make coreference deci-
sion. Hence, our model has a more powerful repre-
sentation capability than the traditional mention-pair
or entity-mention model. And our experimental re-
sults on the ACE data set shows the model is effec-
tive for coreference resolution.
2 Related Work
There are plenty of learning-based coreference reso-
lution systems that employ the mention-pair model.
A typical one of them is presented by Soon et al
(2001). In the system, a training or testing instance
is formed for two mentions in question, with a fea-
ture vector describing their properties and relation-
ships. At a testing time, an active mention is checked
against all its preceding mentions, and is linked with
the closest one that is classified as positive. The
work is further enhanced by Ng and Cardie (2002)
by expanding the feature set and adopting a ?best-
first? linking strategy.
Recent years have seen some work on the entity-
mention model. Luo et al (2004) propose a system
that performs coreference resolution by doing search
in a large space of entities. They train a classifier that
can determine the likelihood that an active mention
should belong to an entity. The entity-level features
are calculated with an ?Any-X? strategy: an entity-
mention pair would be assigned a feature X, if any
mention in the entity has the feature X with the ac-
tive mention.
Culotta et al (2007) present a system which uses
an online learning approach to train a classifier to
judge whether two entities are coreferential or not.
The features describing the relationships between
two entities are obtained based on the information
of every possible pair of mentions from the two en-
tities. Different from (Luo et al, 2004), the entity-
level features are computed using a ?Most-X? strat-
egy, that is, two given entities would have a feature
X, if most of the mention pairs from the two entities
have the feature X.
Yang et al (2004b) suggest an entity-based coref-
erence resolution system. The model adopted in the
system is similar to the mention-pair model, except
that the entity information (e.g., the global num-
ber/gender agreement) is considered as additional
features of a mention in the entity.
McCallum and Wellner (2003) propose several
graphical models for coreference analysis. These
models aim to overcome the limitation that pair-
wise coreference decisions are made independently
of each other. The simplest model conditions coref-
erence on mention pairs, but enforces dependency
by calculating the distance of a node to a partition
(i.e., the probability that an active mention belongs
to an entity) based on the sum of its distances to all
the nodes in the partition (i.e., the sum of the prob-
ability of the active mention co-referring with the
mentions in the entity).
Inductive Logic Programming (ILP) has been ap-
plied to some natural language processing tasks, in-
cluding parsing (Mooney, 1997), POS disambigua-
tion (Cussens, 1996), lexicon construction (Claveau
et al, 2003), WSD (Specia et al, 2007), and so on.
However, to our knowledge, our work is the first ef-
fort to adopt this technique for the coreference reso-
lution task.
3 Modelling Coreference Resolution
Suppose we have a document containing n mentions
{mj : 1 < j < n}, in which mj is the jth mention
occurring in the document. Let ei be the ith entity in
the document. We define
P (L|ei,mj), (1)
the probability that a mention belongs to an entity.
Here the random variable L takes a binary value and
is 1 if mj is a mention of ei.
By assuming that mentions occurring after mj
have no influence on the decision of linking mj to
an entity, we can approximate (1) as:
P (L|ei,mj)
? P (L|{mk ? ei, 1 ? k ? j ? 1},mj) (2)
? max
mk?ei,1?k?j?1
P (L|mk,mj) (3)
(3) further assumes that an entity-mention score
can be computed by using the maximum mention-
844
[ Microsoft Corp. ]11 announced [ [ its ]12 new CEO ]23
[ yesterday ]34. [ The company ]15 said [ he ]26 will . . .
Table 1: A sample text
pair score. Both (2) and (1) can be approximated
with a machine learning method, leading to the tra-
ditional mention-pair model and the entity-mention
model for coreference resolution, respectively.
The two models will be described in the next sub-
sections, with the sample text in Table 1 used for
demonstration. In the table, a mention m is high-
lighted as [ m ]eidmid, where mid and eid are the IDs
for the mention and the entity to which it belongs,
respectively. Three entity chains can be found in the
text, that is,
e1 : Microsoft Corp. - its - The company
e2 : its new CEO - he
e3 : yesterday
3.1 Mention-Pair Model
As a baseline, we first describe a learning framework
with the mention-pair model as adopted in the work
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{mk, mj}, in which mj is
an active mention and mk is a preceding mention.
An instance is associated with a vector of features,
which is used to describe the properties of the two
mentions as well as their relationships. Table 2 sum-
marizes the features used in our study.
For training, given each encountered anaphoric
mention mj in a document, one single positive train-
ing instance is created for mj and its closest an-
tecedent. And a group of negative training in-
stances is created for every intervening mentions
between mj and the antecedent. Consider the ex-
ample text in Table 1, for the pronoun ?he?, three
instances are generated: i(?The company?,?he?),
i(?yesterday?,?he?), and i(?its new CEO?,?he?).
Among them, the first two are labelled as negative
while the last one is labelled as positive.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm. During resolution, an input document is
processed from the first mention to the last. For each
encountered mention mj , a test instance is formed
for each preceding mention, mk. This instance is
presented to the classifier to determine the corefer-
ence relationship. mj is linked with the mention that
is classified as positive (if any) with the highest con-
fidence value.
3.2 Entity-Mention Model
The mention-based solution has a limitation that in-
formation beyond a mention pair cannot be captured.
As an individual mention usually lacks complete de-
scription about the referred entity, the coreference
relationship between two mentions may be not clear,
which would affect classifier learning. Consider
a document with three coreferential mentions ?Mr.
Powell?, ?he?, and ?Powell?, appearing in that or-
der. The positive training instance i(?he?, ?Powell?)
is not informative, as the pronoun ?he? itself dis-
closes nothing but the gender. However, if the whole
entity is considered instead of only one mention, we
can know that ?he? refers to a male person named
?Powell?. And consequently, the coreference rela-
tionships between the mentions would become more
obvious.
The mention-pair model would also cause errors
at a testing time. Suppose we have three mentions
?Mr. Powell?, ?Powell?, and ?she? in a document.
The model tends to link ?she? with ?Powell? be-
cause of their proximity. This error can be avoided,
if we know ?Powell? belongs to the entity starting
with ?Mr. Powell?, and therefore refers to a male
person and cannot co-refer with ?she?.
The entity-mention model based on Eq. (2) per-
forms coreference resolution at an entity-level. For
simplicity, the framework considered for the entity-
mention model adopts similar training and testing
procedures as for the mention-pair model. Specif-
ically, a training or testing instance has the form of
i{ei, mj}, in which mj is an active mention and ei
is a partial entity found before mj . During train-
ing, given each anaphoric mention mj , one single
positive training instance is created for the entity to
which mj belongs. And a group of negative train-
ing instances is created for every partial entity whose
last mention occurs between mj and the closest an-
tecedent of mj .
See the sample in Table 1 again. For the pronoun
?he?, the following three instances are generated for
845
Features describing an active mention, mj
defNP mj 1 if mj is a definite description; else 0
indefNP mj 1 if mj is an indefinite NP; else 0
nameNP mj 1 if mj is a named-entity; else 0
pron mj 1 if mj is a pronoun; else 0
bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0
Features describing a previous mention, mk
defNP mk 1 if mk is a definite description; else 0
indefNP mk 1 if mk is an indefinite NP; else 0
nameNP mk 1 if mk is a named-entity; else 0
pron mk 1 if mk is a pronoun; else 0
bareNP mk 1 if mk is a bare NP; else 0
subject mk 1 if mk is an NP in a subject position; else 0
Features describing the relationships between mk and mj
sentDist sentence distance between two mentions
numAgree 1 if two mentions match in the number agreement; else 0
genderAgree 1 if two mentions match in the gender agreement; else 0
parallelStruct 1 if two mentions have an identical collocation pattern; else 0
semAgree 1 if two mentions have the same semantic category; else 0
nameAlias 1 if two mentions are an alias of the other; else 0
apposition 1 if two mentions are in an appositive structure; else 0
predicative 1 if two mentions are in a predicative structure; else 0
strMatch Head 1 if two mentions have the same head string; else 0
strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0
strMatch Contain 1 if the string of mj is fully contained in that of mk ; else 0
Table 2: Feature set for coreference resolution
entity e1, e3 and e2:
i({?Microsoft Corp.?, ?its?, ?The company?},?he?),
i({?yesterday?},?he?),
i({?its new CEO?},?he?).
Among them, the first two are labelled as negative,
while the last one is positive.
The resolution is done using a greedy clustering
strategy. Given a test document, the mentions are
processed one by one. For each encountered men-
tion mj , a test instance is formed for each partial en-
tity found so far, ei. This instance is presented to the
classifier. mj is appended to the entity that is classi-
fied as positive (if any) with the highest confidence
value. If no positive entity exists, the active mention
is deemed as non-anaphoric and forms a new entity.
The process continues until the last mention of the
document is reached.
One potential problem with the entity-mention
model is how to represent the entity-level knowl-
edge. As an entity may contain more than one candi-
date and the number is not fixed, it is impractical to
enumerate all the mentions in an entity and put their
properties into a single feature vector. As a base-
line, we follow the solution proposed in (Luo et al,
2004) to design a set of first-order features. The fea-
tures are similar to those for the mention-pair model
as shown in Table 2, but their values are calculated
at an entity level. Specifically, the lexical and gram-
matical features are computed by testing any men-
tion1 in the entity against the active mention, for ex-
1Linguistically, pronouns usually have the most direct coref-
ample, the feature nameAlias is assigned value 1 if
at least one mention in the entity is a name alias of
the active mention. The distance feature (i.e., sent-
Dist) is the minimum distance between the mentions
in the entity and the active mention.
The above entity-level features are designed in an
ad-hoc way. They cannot capture the detailed infor-
mation of each individual mention in an entity. In
the next section, we will present a more expressive
entity-mention model by using ILP.
4 Entity-mention Model with ILP
4.1 Motivation
The entity-mention model based on Eq. (2) re-
quires relational knowledge that involves informa-
tion of an active mention (mj), an entity (ei), and
the mentions in the entity ({mk ? ei}). How-
ever, normal machine learning algorithms work on
attribute-value vectors, which only allows the repre-
sentation of atomic proposition. To learn from rela-
tional knowledge, we need an algorithm that can ex-
press first-order logic. This requirement motivates
our use of Inductive Logic Programming (ILP), a
learning algorithm capable of inferring logic pro-
grams. The relational nature of ILP makes it pos-
sible to explicitly represent relations between an en-
tity and its mentions, and thus provides a powerful
expressiveness for the coreference resolution task.
erence relationship with antecedents in a local discourse.
Hence, if an active mention is a pronoun, we only consider the
mentions in its previous two sentences for feature computation.
846
ILP uses logic programming as a uniform repre-
sentation for examples, background knowledge and
hypotheses. Given a set of positive and negative ex-
ample E = E+ ? E?, and a set of background
knowledge K of the domain, ILP tries to induce a
set of hypotheses h that covers most of E+ with no
E?, i.e., K ? h |= E+ and K ? h 6|= E?.
In our study, we choose ALEPH2, an ILP imple-
mentation by Srinivasan (2000) that has been proven
well suited to deal with a large amount of data in
multiple domains. For its routine use, ALEPH fol-
lows a simple procedure to induce rules. It first se-
lects an example and builds the most specific clause
that entertains the example. Next, it tries to search
for a clause more general than the bottom one. The
best clause is added to the current theory and all the
examples made redundant are removed. The proce-
dure repeats until all examples are processed.
4.2 Apply ILP to coreference resolution
Given a document, we encode a mention or a par-
tial entity with a unique constant. Specifically, mj
represents the jth mention (e.g., m6 for the pronoun
?he?). ei j represents the partial entity i before the
jth mention. For example, e1 6 denotes the part of
e1 before m6, i.e., {?Microsoft Corp.?, ?its?, ?the
company?}, while e1 5 denotes the part of e1 be-
fore m5 (?The company?), i.e., {?Microsoft Corp.?,
?its?}.
Training instances are created as described in Sec-
tion 3.2 for the entity-mention model. Each instance
is recorded with a predicate link(ei j , mj), where mj
is an active mention and ei j is a partial entity. For
example, the three training instances formed by the
pronoun ?he? are represented as follows:
link(e1 6,m6).
link(e3 6,m6).
link(e2 6,m6).
The first two predicates are put into E?, while the
last one is put to E+.
The background knowledge for an instance
link(ei j , mj) is also represented with predicates,
which are divided into the following types:
1. Predicates describing the information related to
ei j and mj . The properties of mj are pre-
2http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/aleph toc.html
sented with predicates like f (m, v), where f
corresponds to a feature in the first part of Ta-
ble 2 (removing the suffix mj), and v is its
value. For example, the pronoun ?he? can be
described by the following predicates:
defNP(m6, 0). indefNP(m6, 0).
nameNP(m6, 0). pron(m6, 1).
bareNP(m6, 0).
The predicates for the relationships between
ei j and mj take a form of f (e, m, v). In our
study, we consider the number agreement (ent-
NumAgree) and the gender agreement (entGen-
derAgree) between ei j and mj . v is 1 if all
of the mentions in ei j have consistent num-
ber/gender agreement with mj , e.g,
entNumAgree(e1 6,m6, 1).
2. Predicates describing the belonging relations
between ei j and its mentions. A predicate
has mention(e, m) is used for each mention in
e 3. For example, the partial entity e1 6 has
three mentions, m1, m2 and m5, which can be
described as follows:
has mention(e1 6,m1).
has mention(e1 6,m2).
has mention(e1 6,m5).
3. Predicates describing the information related to
mj and each mention mk in ei j . The predi-
cates for the properties of mk correspond to the
features in the second part of Table 2 (removing
the suffix mk), while the predicates for the re-
lationships between mj and mk correspond to
the features in the third part of Table 2. For ex-
ample, given the two mentions m1 (?Microsoft
Corp.) and m6 (?he), the following predicates
can be applied:
nameNP(m1, 1).
pron(m1, 0).
. . .
nameAlias(m1,m6, 0).
sentDist(m1,m6, 1).
. . .
the last two predicates represent that m1 and
3If an active mention mj is a pronoun, only the previous
mentions in two sentences apart are recorded by has mention,
while the farther ones are ignored as they have less impact on
the resolution of the pronoun.
847
m6 are not name alias, and are one sentence
apart.
By using the three types of predicates, the dif-
ferent knowledge related to entities and mentions
are integrated. The predicate has mention acts as
a bridge connecting the entity-mention knowledge
and the mention-pair knowledge. As a result, when
evaluating the coreference relationship between an
active mention and an entity, we can make use of
the ?global? information about the entity, as well as
the ?local? information of each individual mention
in the entity.
From the training instances and the associated
background knowledge, a set of hypotheses can be
automatically learned by ILP. Each hypothesis is
output as a rule that may look like:
link(A,B):-
predi1, predi2, . . . , has mention(A,C), . . . , prediN.
which corresponds to first-order logic
?A,B(predi1 ? predi2 ? . . .?
?C(has mention(A,C) ? . . . ? prediN)
? link(A,B))
Consider an example rule produced in our system:
link(A,B) :-
has mention(A,C), numAgree(B,C,1),
strMatch Head(B,C,1), bareNP(C,1).
Here, variables A and B stand for an entity and an
active mention in question. The first-order logic is
implemented by using non-instantiated arguments C
in the predicate has mention. This rule states that a
mention B should belong to an entity A, if there ex-
ists a mention C in A such that C is a bare noun
phrase with the same head string as B, and matches
in number with B. In this way, the detailed informa-
tion of each individual mention in an entity can be
captured for resolution.
A rule is applicable to an instance link(e, m), if
the background knowledge for the instance can be
described by the predicates in the body of the rule.
Each rule is associated with a score, which is the
accuracy that the rule can produce for the training
instances.
The learned rules are applied to resolution in a
similar way as described in Section 3.2. Given an
active mention m and a partial entity e, a test in-
stance link(e, m) is formed and tested against every
rule in the rule set. The confidence that m should
Train Test
#entity #mention #entity #mention
NWire 1678 9861 411 2304
NPaper 1528 10277 365 2290
BNews 1695 8986 468 2493
Table 3: statistics of entities (length > 1) and contained
mentions
belong to e is the maximal score of the applicable
rules. An active mention is linked to the entity with
the highest confidence value (above 0.5), if any.
5 Experiments and Results
5.1 Experimental Setup
In our study, we did evaluation on the ACE-2003
corpus, which contains two data sets, training and
devtest, used for training and testing respectively.
Each of these sets is further divided into three do-
mains: newswire (NWire), newspaper (NPaper), and
broadcast news (BNews). The number of entities
with more than one mention, as well as the number
of the contained mentions, is summarized in Table 3.
For both training and resolution, an input raw
document was processed by a pipeline of NLP
modules including Tokenizer, Part-of-Speech tag-
ger, NP Chunker and Named-Entity (NE) Recog-
nizer. Trained and tested on Penn WSJ TreeBank,
the POS tagger could obtain an accuracy of 97% and
the NP chunker could produce an F-measure above
94% (Zhou and Su, 2000). Evaluated for the MUC-
6 and MUC-7 Named-Entity task, the NER mod-
ule (Zhou and Su, 2002) could provide an F-measure
of 96.6% (MUC-6) and 94.1%(MUC-7). For evalu-
ation, Vilain et al (1995)?s scoring algorithm was
adopted to compute recall and precision rates.
By default, the ALEPH algorithm only generates
rules that have 100% accuracy for the training data.
And each rule contains at most three predicates. To
accommodate for coreference resolution, we loos-
ened the restrictions to allow rules that have above
50% accuracy and contain up to ten predicates. De-
fault parameters were applied for all the other set-
tings in ALEPH as well as other learning algorithms
used in the experiments.
5.2 Results and Discussions
Table 4 lists the performance of different corefer-
ence resolution systems. For comparison, we first
848
NWire NPaper BNews
R P F R P F R P F
C4.5
- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9
- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5
- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4
ILP
- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1
- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5
Table 4: Results of different systems for coreference resolution
examined the C4.5 algorithm4 which is widely used
for the coreference resolution task. The first line of
the table shows the baseline system that employs the
traditional mention-pair model (MP) as described in
Section 3.1. From the table, our baseline system
achieves a recall of around 66%-68% and a preci-
sion of around 50%-60%. The overall F-measure
for NWire, NPaper and BNews is 60.4%, 57.9% and
62.9% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure ranging in 50-60% for
the same data set. As our system relies only on sim-
ple and knowledge-poor features, the achieved F-
measure is around 2-4% lower than the state-of-the-
art systems do, like (Ng, 2007) and (Yang and Su,
2007) which utilized sophisticated semantic or real-
world knowledge. Since ILP has a strong capability
in knowledge management, our system could be fur-
ther improved if such helpful knowledge is incorpo-
rated, which will be explored in our future work.
The second line of Table 4 is for the system
that employs the entity-mention model (EM) with
?Any-X? based entity features, as described in Sec-
tion 3.2. We can find that the EM model does not
show superiority over the baseline MP model. It
achieves a higher precision (up to 2.6%), but a lower
recall (2.9%), than MP. As a result, we only see
?0.4% difference between the F-measure. The re-
sults are consistent with the reports by Luo et al
(2004) that the entity-mention model with the ?Any-
X? first-order features performs worse than the nor-
mal mention-pair model. In our study, we also tested
the ?Most-X? strategy for the first-order features as
in (Culotta et al, 2007), but got similar results with-
out much difference (?0.5% F-measure) in perfor-
4http://www.rulequest.com/see5-info.html
mance. Besides, as with our entity-mention predi-
cates described in Section 4.2, we also tried the ?All-
X? strategy for the entity-level agreement features,
that is, whether all mentions in a partial entity agree
in number and gender with an active mention. How-
ever, we found this bring no improvement against
the ?Any-X? strategy.
As described, given an active mention mj , the MP
model only considers the mentions between mj and
its closest antecedent. By contrast, the EM model
considers not only these mentions, but also their an-
tecedents in the same entity link. We were interested
in examining what if the MP model utilizes all the
mentions in an entity as the EM model does. As
shown in the third line of Table 4, such a solution
damages the performance; while the recall is at the
same level, the precision drops significantly (up to
12%) and as a result, the F-measure is even lower
than the original MP model. This should be because
a mention does not necessarily have direct corefer-
ence relationships with all of its antecedents. As the
MP model treats each mention-pair as an indepen-
dent instance, including all the antecedents would
produce many less-confident positive instances, and
thus adversely affect training.
The second block of the table summarizes the per-
formance of the systems with ILP. We were first con-
cerned with how well ILP works for the mention-
pair model, compared with the normally used algo-
rithm C4.5. From the results shown in the fourth
line of Table 4, ILP exhibits the same capability in
the resolution; it tends to produce a slightly higher
precision but a lower recall than C4.5 does. Overall,
it performs better in F-measure (1.8%) for Npaper,
while slightly worse (<1%) for Nwire and BNews.
These results demonstrate that ILP could be used as
849
link(A,B) :-
bareNP(B,0), has mention(A,C), appositive(C,1).
link(A,B) :-
has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).
link(A,B) :-
nameNP(B,0), has mention(A,C), predicative(C,1).
link(A,B) :-
has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).
link(A,B) :-
nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).
link(A,B) :-
pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),
subject(D, 1).
...
Figure 1: Examples of rules produced by ILP (entity-
mention model)
a good classifier learner for the mention-pair model.
The fifth line of Table 4 is for the ILP based entity-
mention model (described in Section 4.2). We can
observe that the model leads to a better performance
than all the other models. Compared with the sys-
tem with the MP model (under ILP), the EM version
is able to achieve a higher precision (up to 4.6% for
BNews). Although the recall drops slightly (up to
1.8% for BNews), the gain in the precision could
compensate it well; it beats the MP model in the
overall F-measure for all three domains (2.3% for
Nwire, 0.4% for Npaper, 1.4% for BNews). Es-
pecially, the improvement in NWire and BNews is
statistically significant under a 2-tailed t test (p <
0.05). Compared with the EM model with the man-
ually designed first-order feature (the second line),
the ILP-based EM solution also yields better perfor-
mance in precision (with a slightly lower recall) as
well as the overall F-measure (1.0% - 1.8%).
The improvement in precision against the
mention-pair model confirms that the global infor-
mation beyond a single mention pair, when being
considered for training, can make coreference rela-
tions clearer and help classifier learning. The bet-
ter performance against the EM model with heuristi-
cally designed features also suggests that ILP is able
to learn effective first-order rules for the coreference
resolution task.
In Figure 1, we illustrate part of the rules pro-
duced by ILP for the entity-mention model (NWire
domain), which shows how the relational knowledge
of entities and mentions is represented for decision
making. An interesting finding, as shown in the last
rule of the table, is that multiple non-instantiated ar-
guments (i.e. C and D) could possibly appear in
the same rule. According to this rule, a pronominal
mention should be linked with a partial entity which
contains a named-entity and contains an indefinite
NP in a subject position. This supports the claims
in (Yang et al, 2004a) that coreferential informa-
tion is an important factor to evaluate a candidate an-
tecedent in pronoun resolution. Such complex logic
makes it possible to capture information of multi-
ple mentions in an entity at the same time, which is
difficult to implemented in the mention-pair model
and the ordinary entity-mention model with heuris-
tic first-order features.
6 Conclusions
This paper presented an expressive entity-mention
model for coreference resolution by using Inductive
Logic Programming. In contrast to the traditional
mention-pair model, our model can capture infor-
mation beyond single mention pairs for both training
and testing. The relational nature of ILP enables our
model to explicitly express the relations between an
entity and its mentions, and to automatically learn
the first-order rules effective for the coreference res-
olution task. The evaluation on ACE data set shows
that the ILP based entity-model performs better than
the mention-pair model (with up to 2.3% increase in
F-measure), and also beats the entity-mention model
with heuristically designed first-order features.
Our current work focuses on the learning model
that calculates the probability of a mention be-
longing to an entity. For simplicity, we just use a
greedy clustering strategy for resolution, that is, a
mention is linked to the current best partial entity.
In our future work, we would like to investigate
more sophisticated clustering methods that would
lead to global optimization, e.g., by keeping a large
search space (Luo et al, 2004) or using integer
programming (Denis and Baldridge, 2007).
Acknowledgements This research is supported
by a Specific Targeted Research Project (STREP)
of the European Union?s 6th Framework Programme
within IST call 4, Bootstrapping Of Ontologies and
Terminologies STrategic REsearch Project (BOOT-
Strep).
850
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 122?129.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
4:493?525.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In Proceedings of the Annual Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL), pages 81?88.
J. Cussens. 1996. Part-of-speech disambiguation using
ilp. Technical report, Oxford University Computing
Laboratory.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North America Chapter of the Association for
Computational Linguistics (NAACL), pages 236?243.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
135?142.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proceedings of IJCAI-
03 Workshop on Information Integration on the Web,
pages 79?86.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of
the 14th International Conference on Artificial Intel-
ligences (IJCAI), pages 1050?1055.
R. Mooney. 1997. Inductive logic programming for nat-
ural language processing. In Proceedings of the sixth
International Inductive Logic Programming Work-
shop, pages 3?24.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 104?111,
Philadelphia.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
157?164.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 536?543.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learn-
ing expressive models for words sense disambiguation.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
41?48.
A. Srinivasan. 2000. The aleph manual. Technical re-
port, Oxford University Computing Laboratory.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann Publishers.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automati-
cally discovered patterns. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 528?535.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improv-
ing pronoun resolution by incorporating coreferential
information of candidates. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 127?134, Barcelona.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An
NP-cluster approach to coreference resolution. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 219?225, Geneva.
G. Zhou and J. Su. 2000. Error-driven HMM-based
chunk tagger with context-dependent lexicon. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 71?79, Hong Kong.
G. Zhou and J. Su. 2002. Named Entity recognition us-
ing a HMM-based chunk tagger. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 473?480, Philadel-
phia.
851
Proceedings of ACL-08: HLT, pages 1021?1029,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining Multiple Resources to Improve SMT-based Paraphrasing Model?
Shiqi Zhao1, Cheng Niu2, Ming Zhou2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Microsoft Research Asia, Beijing, China
{chengniu,mingzhou}@microsoft.com
Abstract
This paper proposes a novel method that ex-
ploits multiple resources to improve statisti-
cal machine translation (SMT) based para-
phrasing. In detail, a phrasal paraphrase ta-
ble and a feature function are derived from
each resource, which are then combined in a
log-linear SMT model for sentence-level para-
phrase generation. Experimental results show
that the SMT-based paraphrasing model can
be enhanced using multiple resources. The
phrase-level and sentence-level precision of
the generated paraphrases are above 60% and
55%, respectively. In addition, the contribu-
tion of each resource is evaluated, which indi-
cates that all the exploited resources are useful
for generating paraphrases of high quality.
1 Introduction
Paraphrases are alternative ways of conveying the
same meaning. Paraphrases are important in many
natural language processing (NLP) applications,
such as machine translation (MT), question an-
swering (QA), information extraction (IE), multi-
document summarization (MDS), and natural lan-
guage generation (NLG).
This paper addresses the problem of sentence-
level paraphrase generation, which aims at generat-
ing paraphrases for input sentences. An example of
sentence-level paraphrases can be seen below:
S1: The table was set up in the carriage shed.
S2: The table was laid under the cart-shed.
?This research was finished while the first author worked as
an intern in Microsoft Research Asia.
Paraphrase generation can be viewed as monolin-
gual machine translation (Quirk et al, 2004), which
typically includes a translation model and a lan-
guage model. The translation model can be trained
using monolingual parallel corpora. However, ac-
quiring such corpora is not easy. Hence, data sparse-
ness is a key problem for the SMT-based paraphras-
ing. On the other hand, various methods have been
presented to extract phrasal paraphrases from dif-
ferent resources, which include thesauri, monolin-
gual corpora, bilingual corpora, and the web. How-
ever, little work has been focused on using the ex-
tracted phrasal paraphrases in sentence-level para-
phrase generation.
In this paper, we exploit multiple resources to
improve the SMT-based paraphrase generation. In
detail, six kinds of resources are utilized, includ-
ing: (1) an automatically constructed thesaurus, (2)
a monolingual parallel corpus from novels, (3) a
monolingual comparable corpus from news articles,
(4) a bilingual phrase table, (5) word definitions
from Encarta dictionary, and (6) a corpus of simi-
lar user queries. Among the resources, (1), (2), (3),
and (4) have been investigated by other researchers,
while (5) and (6) are first used in this paper. From
those resources, six phrasal paraphrase tables are ex-
tracted, which are then used in a log-linear SMT-
based paraphrasing model.
Both phrase-level and sentence-level evaluations
were carried out in the experiments. In the former
one, phrase substitutes occurring in the paraphrase
sentences were evaluated. While in the latter one,
the acceptability of the paraphrase sentences was
evaluated. Experimental results show that: (1) The
1021
SMT-based paraphrasing is enhanced using multiple
resources. The phrase-level and sentence-level pre-
cision of the generated paraphrases exceed 60% and
55%, respectively. (2) Although the contributions of
the resources differ a lot, all the resources are useful.
(3) The performance of the method varies greatly on
different test sets and it performs best on the test set
of news sentences, which are from the same source
as most of the training data.
The rest of the paper is organized as follows: Sec-
tion 2 reviews related work. Section 3 introduces the
log-linear model for paraphrase generation. Section
4 describes the phrasal paraphrase extraction from
different resources. Section 5 presents the parameter
estimation method. Section 6 shows the experiments
and results. Section 7 draws the conclusion.
2 Related Work
Paraphrases have been used in many NLP applica-
tions. In MT, Callison-Burch et al (2006) utilized
paraphrases of unseen source phrases to alleviate
data sparseness. Kauchak and Barzilay (2006) used
paraphrases of the reference translations to improve
automatic MT evaluation. In QA, Lin and Pantel
(2001) and Ravichandran and Hovy (2002) para-
phrased the answer patterns to enhance the recall of
answer extraction. In IE, Shinyama et al (2002)
automatically learned paraphrases of IE patterns to
reduce the cost of creating IE patterns by hand. In
MDS, McKeown et al (2002) identified paraphrase
sentences across documents before generating sum-
marizations. In NLG, Iordanskaja et al (1991) used
paraphrases to generate more varied and fluent texts.
Previous work has examined various resources for
acquiring paraphrases, including thesauri, monolin-
gual corpora, bilingual corpora, and the web. The-
sauri, such as WordNet, have been widely used
for extracting paraphrases. Some researchers ex-
tract synonyms as paraphrases (Kauchak and Barzi-
lay, 2006), while some others use looser defini-
tions, such as hypernyms and holonyms (Barzilay
and Elhadad, 1997). Besides, the automatically
constructed thesauri can also be used. Lin (1998)
constructed a thesaurus by automatically clustering
words based on context similarity.
Barzilay andMcKeown (2001) used monolingual
parallel corpora for identifying paraphrases. They
exploited a corpus of multiple English translations
of the same source text written in a foreign language,
from which phrases in aligned sentences that appear
in similar contexts were extracted as paraphrases. In
addition, Finch et al (2005) applied MT evalua-
tion methods (BLEU, NIST,WER and PER) to build
classifiers for paraphrase identification.
Monolingual parallel corpora are difficult to find,
especially in non-literature domains. Alternatively,
some researchers utilized monolingual compara-
ble corpora for paraphrase extraction. Different
news articles reporting on the same event are com-
monly used as monolingual comparable corpora,
from which both paraphrase patterns and phrasal
paraphrases can be derived (Shinyama et al, 2002;
Barzilay and Lee, 2003; Quirk et al, 2004).
Lin and Pantel (2001) learned paraphrases from
a parsed monolingual corpus based on an extended
distributional hypothesis, where if two paths in de-
pendency trees tend to occur in similar contexts it is
hypothesized that the meanings of the paths are simi-
lar. The monolingual corpus used in their work is not
necessarily parallel or comparable. Thus it is easy
to obtain. However, since this resource is used to
extract paraphrase patterns other than phrasal para-
phrases, we do not use it in this paper.
Bannard and Callison-Burch (2005) learned
phrasal paraphrases using bilingual parallel cor-
pora. The basic idea is that if two phrases are
aligned to the same translation in a foreign language,
they may be paraphrases. This method has been
demonstrated effective in extracting large volume of
phrasal paraphrases. Besides, Wu and Zhou (2003)
exploited bilingual corpora and translation informa-
tion in learning synonymous collocations.
In addition, some researchers extracted para-
phrases from the web. For example, Ravichandran
and Hovy (2002) retrieved paraphrase patterns from
the web using hand-crafted queries. Pasca and Di-
enes (2005) extracted sentence fragments occurring
in identical contexts as paraphrases from one bil-
lion web documents. Since web mining is rather
time consuming, we do not exploit the web to ex-
tract paraphrases in this paper.
So far, two kinds of methods have been pro-
posed for sentence-level paraphrase generation, i.e.,
the pattern-based and SMT-based methods. Auto-
matically learned patterns have been used in para-
1022
phrase generation. For example, Barzilay and Lee
(2003) applied multiple-sequence alignment (MSA)
to parallel news sentences and induced paraphras-
ing patterns for generating new sentences. Pang et
al. (2003) built finite state automata (FSA) from se-
mantically equivalent translation sets based on syn-
tactic alignment and used the FSAs in paraphrase
generation. The pattern-based methods can generate
complex paraphrases that usually involve syntactic
variation. However, the methods were demonstrated
to be of limited generality (Quirk et al, 2004).
Quirk et al (2004) first recast paraphrase gener-
ation as monolingual SMT. They generated para-
phrases using a SMT system trained on parallel sen-
tences extracted from clustered news articles. In
addition, Madnani et al (2007) also generated
sentence-level paraphrases based on a SMT model.
The advantage of the SMT-based method is that
it achieves better coverage than the pattern-based
method. The main difference between their methods
and ours is that they only used bilingual parallel cor-
pora as paraphrase resource, while we exploit and
combine multiple resources.
3 SMT-based Paraphrasing Model
The SMT-based paraphrasing model used by Quirk
et al (2004) was the noisy channel model of Brown
et al (1993), which identified the optimal paraphrase
T ? of a sentence S by finding:
T ? = argmax
T
{P (T |S)}
= argmax
T
{P (S|T )P (T )} (1)
In contrast, we adopt a log-linear model (Och
and Ney, 2002) in this work, since multiple para-
phrase tables can be easily combined in the log-
linear model. Specifically, feature functions are de-
rived from each paraphrase resource and then com-
bined with the language model feature1:
T ? = argmax
T
{
N?
i=1
?TM ihTM i(T, S)+
?LMhLM (T, S)} (2)
where N is the number of paraphrase tables.
hTM i(T, S) is the feature function based on the i-
th paraphrase table PTi. hLM (T, S) is the language
1The reordering model is not considered in our model.
model feature. ?TM i and ?LM are the weights of
the feature functions. hTM i(T, S) is defined as:
hTM i(T, S) = log
Ki?
k=1
Scorei(Tk, Sk) (3)
where Ki is the number of phrase substitutes from
S to T based on PTi. Tk in T and Sk in S are
phrasal paraphrases in PTi. Scorei(Tk, Sk) is the
paraphrase likelihood according to PTi2. A 5-gram
language model is used, therefore:
hLM (T, S) = log
J?
j=1
p(tj |tj?4, ..., tj?1) (4)
where J is the length of T , tj is the j-th word of T .
4 Exploiting Multiple Resources
This section describes the extraction of phrasal
paraphrases using various resources. Similar to
Pharaoh (Koehn, 2004), our decoder3 uses top 20
paraphrase options for each input phrase in the de-
fault setting. Therefore, we keep at most 20 para-
phrases for a phrase when extracting phrasal para-
phrases using each resource.
1 - Thesaurus: The thesaurus4 used in this work
was automatically constructed by Lin (1998). The
similarity of two words e1 and e2 was calculated
through the surrounding context words that have de-
pendency relations with the investigated words:
Sim(e1, e2)
=
P
(r,e)?Tr(e1)?Tr(e2)
(I(e1, r, e) + I(e2, r, e))
P
(r,e)?Tr(e1)
I(e1, r, e) +
P
(r,e)?Tr(e2)
I(e2, r, e)
(5)
where Tr(ei) denotes the set of words that have de-
pendency relation r with word ei. I(ei, r, e) is the
mutual information between ei, r and e.
For each word, we keep 20 most similar words as
paraphrases. In this way, we extract 502,305 pairs of
paraphrases. The paraphrasing score Score1(p1, p2)
used in Equation (3) is defined as the similarity
based on Equation (5).
2If none of the phrase substitutes from S to T is from PTi
(i.e., Ki = 0), we cannot compute hTM i(T, S) as in Equation
(3). In this case, we assign hTM i(T, S) a minimum value.
3The decoder used here is a re-implementation of Pharaoh.
4http://www.cs.ualberta.ca/ lindek/downloads.htm.
1023
2 - Monolingual parallel corpus: Following Barzi-
lay and McKeown (2001), we exploit a corpus
of multiple English translations of foreign nov-
els, which contains 25,804 parallel sentence pairs.
We find that most paraphrases extracted using the
method of Barzilay and McKeown (2001) are quite
short. Thus we employ a new approach for para-
phrase extraction. Specifically, we parse the sen-
tences with CollinsParser5 and extract the chunks
from the parsing results. Let S1 and S2 be a pair
of parallel sentences, p1 and p2 two chunks from S1
and S2, we compute the similarity of p1 and p2 as:
Sim(p1, p2) = ?Simcontent(p1, p2)+
(1 ? ?)Simcontext(p1, p2) (6)
where, Simcontent(p1, p2) is the content similarity,
which is the word overlapping rate of p1 and p2.
Simcontext(p1, p2) is the context similarity, which is
the word overlapping rate of the contexts of p1 and
p26. If the similarity of p1 and p2 exceeds a thresh-
old Th1, they are identified as paraphrases. We ex-
tract 18,698 pairs of phrasal paraphrases from this
resource. The paraphrasing score Score2(p1, p2) is
defined as the similarity in Equation (6). For the
paraphrases occurring more than once, we use their
maximum similarity as the paraphrasing score.
3 - Monolingual comparable corpus: Similar to
the methods in (Shinyama et al, 2002; Barzilay and
Lee, 2003), we construct a corpus of comparable
documents from a large corpus D of news articles.
The corpusD contains 612,549 news articles. Given
articles d1 and d2 from D, if their publication date
interval is less than 2 days and their similarity7 ex-
ceeds a threshold Th2, they are recognized as com-
parable documents. In this way, a corpus containing
5,672,864 pairs of comparable documents is con-
structed. From the comparable corpus, parallel sen-
tences are extracted. Let s1 and s2 be two sentences
from comparable documents d1 and d2, if their sim-
ilarity based on word overlapping rate is above a
threshold Th3, s1 and s2 are identified as parallel
sentences. In this way, 872,330 parallel sentence
pairs are extracted.
5http://people.csail.mit.edu/mcollins/code.html
6The context of a chunk is made up of 6 words around the
chunk, 3 to the left and 3 to the right.
7The similarity of two documents is computed using the vec-
tor space model and the word weights are based on tf?idf.
We run Giza++ (Och and Ney, 2000) on the paral-
lel sentences and then extract aligned phrases as de-
scribed in (Koehn, 2004). The generated paraphrase
table is pruned by keeping the top 20 paraphrases for
each phrase. After pruning, 100,621 pairs of para-
phrases are extracted. Given phrase p1 and its para-
phrase p2, we compute Score3(p1, p2) by relative
frequency (Koehn et al, 2003):
Score3(p1, p2) = p(p2|p1) =
count(p2, p1)
P
p? count(p
?, p1)
(7)
People may wonder why we do not use the same
method on the monolingual parallel and comparable
corpora. This is mainly because the volumes of the
two corpora differ a lot. In detail, the monolingual
parallel corpus is fairly small, thus automatical word
alignment tool like Giza++ may not work well on
it. In contrast, the monolingual comparable corpus
is quite large, hence we cannot conduct the time-
consuming syntactic parsing on it as we do on the
monolingual parallel corpus.
4 - Bilingual phrase table: We first construct
a bilingual phrase table that contains 15,352,469
phrase pairs from an English-Chinese parallel cor-
pus. We extract paraphrases from the bilingual
phrase table and compute the paraphrasing score
of phrases p1 and p2 as in (Bannard and Callison-
Burch, 2005):
Score4(p1, p2) =
?
f
p(f |p1)p(p2|f) (8)
where f denotes a Chinese translation of both p1 and
p2. p(f |p1) and p(p2|f) are the translation probabil-
ities provided by the bilingual phrase table. For each
phrase, the top 20 paraphrases are kept according
to the score in Equation (8). As a result, 3,177,600
pairs of phrasal paraphrases are extracted.
5 - Encarta dictionary definitions: Words and their
definitions can be regarded as paraphrases. Here
are some examples from Encarta dictionary: ?hur-
ricane: severe storm?, ?clever: intelligent?, ?travel:
go on journey?. In this work, we extract words? def-
initions from Encarta dictionary web pages8. If a
word has more than one definition, all of them are
extracted. Note that the words and definitions in the
8http://encarta.msn.com/encnet/features/dictionary/diction-
aryhome.aspx
1024
dictionary are lemmatized, but words in sentences
are usually inflected. Hence, we expand the word
- definition pairs by providing the inflected forms.
Here we use an inflection list and some rules for in-
flection. After expanding, 159,456 pairs of phrasal
paraphrases are extracted. Let < p1, p2 > be a word
- definition pair, the paraphrasing score is defined
according to the rank of p2 in all of p1?s definitions:
Score5(p1, p2) = ?
i?1 (9)
where ? is a constant (we empirically set ? = 0.9)
and i is the rank of p2 in p1?s definitions.
6 - Similar user queries: Clusters of similar user
queries have been used for query expansion and sug-
gestion (Gao et al, 2007). Since most queries are at
the phrase level, we exploit similar user queries as
phrasal paraphrases. In our experiment, we use the
corpus of clustered similar MSN queries constructed
by Gao et al (2007). The similarity of two queries
p1 and p2 is computed as:
Sim(p1, p2) = ?Simcontent(p1, p2)+
(1 ? ?)Simclick?through(p1, p2) (10)
where Simcontent(p1, p2) is the content similarity,
which is computed as the word overlapping rate of
p1 and p2. Simclick?through(p1, p2) is the click
through similarity, which is the overlapping rate of
the user clicked documents for p1 and p2. For each
query q, we keep the top 20 similar queries, whose
similarity with q exceeds a threshold Th4. As a re-
sult, 395,284 pairs of paraphrases are extracted. The
score Score6(p1, p2) is defined as the similarity in
Equation (10).
7 - Self-paraphrase: In addition to the six resources
introduced above, a special paraphrase table is used,
which is made up of pairs of identical words. The
reason why this paraphrase table is necessary is that
a word should be allowed to keep unchanged in para-
phrasing. This is a difference between paraphras-
ing and MT, since all words should be translated in
MT. In our experiments, all the words that occur in
the six paraphrase table extracted above are gath-
ered to form the self-paraphrase table, which con-
tains 110,403 word pairs. The score Score7(p1, p2)
is set 1 for each identical word pair.
5 Parameter Estimation
The weights of the feature functions, namely ?TM i
(i = 1, 2, ..., 7) and ?LM , need estimation9. In MT,
the max-BLEU algorithm is widely used to estimate
parameters. However, it may not work in our case,
since it is more difficult to create a reference set of
paraphrases.
We propose a new technique to estimate parame-
ters in paraphrasing. The assumption is that, since a
SMT-based paraphrase is generated through phrase
substitution, we can measure the quality of a gener-
ated paraphrase by measuring its phrase substitutes.
Generally, the paraphrases containing more correct
phrase substitutes are judged as better paraphrases10.
We therefore present the phrase substitution error
rate (PSER) to score a generated paraphrase T :
PSER(T ) = ?PS0(T )?/?PS(T )? (11)
where PS(T ) is the set of phrase substitutes in T
and PS0(T ) is the set of incorrect substitutes.
In practice, we keep top n paraphrases for each
sentence S. Thus we calculate the PSER for each
source sentence S as:
PSER(S) = ?
n[
i=1
PS0(Ti)?/?
n[
i=1
PS(Ti)? (12)
where Ti is the i-th generated paraphrase of S.
Suppose there are N sentences in the develop-
ment set, the overall PSER is computed as:
PSER =
NX
j=1
PSER(Sj) (13)
where Sj is the j-th sentence in the development set.
Our development set contains 75 sentences (de-
scribed in detail in Section 6). For each sentence,
all possible phrase substitutes are extracted from the
six paraphrase tables above. The extracted phrase
substitutes are then manually labeled as ?correct? or
?incorrect?. A phrase substitute is considered as cor-
rect only if the two phrases have the same meaning
in the given sentence and the sentence generated by
9Note that, we also use some other parameters when extract-
ing phrasal paraphrases from different resources, such as the
thresholds Th1, Th2, Th3, Th4, as well as ? and ? in Equa-
tion (6) and (10). These parameters are estimated using differ-
ent development sets from the investigated resources. We do
not describe the estimation of them due to space limitation.
10Paraphrasing a word to itself (based on the 7-th paraphrase
table above) is not regarded as a substitute.
1025
substituting the source phrase with the target phrase
remains grammatical. In decoding, the phrase sub-
stitutes are printed out and then the PSER is com-
puted based on the labeled data.
Using each set of parameters, we generate para-
phrases for the sentences in the development set
based on Equation (2). PSER is then computed as
in Equation (13). We use the gradient descent algo-
rithm (Press et al, 1992) to minimize PSER on the
development set and get the optimal parameters.
6 Experiments
To evaluate the performance of the method on dif-
ferent types of test data, we used three kinds of sen-
tences for testing, which were randomly extracted
from Google news, free online novels, and forums,
respectively. For each type, 50 sentences were ex-
tracted as test data and another 25 were extracted as
development data. For each test sentence, top 10 of
the generated paraphrases were kept for evaluation.
6.1 Phrase-level Evaluation
The phrase-level evaluation was carried out to in-
vestigate the contributions of the paraphrase tables.
For each test sentence, all possible phrase substitutes
were first extracted from the paraphrase tables and
manually labeled as ?correct? or ?incorrect?. Here,
the criterion for identifying paraphrases is the same
as that described in Section 5. Then, in the stage
of decoding, the phrase substitutes were printed out
and evaluated using the labeled data.
Two metrics were used here. The first is the
number of distinct correct substitutes (#DCS). Ob-
viously, the more distinct correct phrase substitutes
a paraphrase table can provide, the more valuable it
is. The second is the accuracy of the phrase substi-
tutes, which is computed as:
Accuracy =
#correct phrase substitutes
#all phrase substitutes
(14)
To evaluate the PTs learned from different re-
sources, we first used each PT (from 1 to 6) along
with PT-7 in decoding. The results are shown in Ta-
ble 1. It can be seen that PT-4 is the most useful, as
it provides the most correct substitutes and the ac-
curacy is the highest. We believe that it is because
PT-4 is much larger than the other PTs. Compared
with PT-4, the accuracies of the other PTs are fairly
PT combination #DCS Accuracy
1+7 178 14.61%
2+7 94 25.06%
3+7 202 18.35%
4+7 553 56.93%
5+7 231 20.48%
6+7 21 14.42%
Table 1: Contributions of the paraphrase tables.
PT-1: from the thesaurus; PT-2: from the monolingual
parallel corpus; PT-3: from the monolingual comparable
corpus; PT-4: from the bilingual parallel corpus; PT-5:
from the Encarta dictionary definitions; PT-6: from the
similar MSN user queries; PT-7: self-paraphrases.
low. This is because those PTs are smaller, thus they
can provide fewer correct phrase substitutes. As a
result, plenty of incorrect substitutes were included
in the top 10 generated paraphrases.
PT-6 provides the least correct phrase substitutes
and the accuracy is the lowest. There are several
reasons. First, many phrases in PT-6 are not real
phrases but only sets of keywords (e.g., ?lottery re-
sults ny?), which may not appear in sentences. Sec-
ond, many words in this table have spelling mis-
takes (e.g., ?widows vista?). Third, some phrase
pairs in PT-6 are not paraphrases but only ?related
queries? (e.g., ?back tattoo? vs. ?butterfly tattoo?).
Fourth, many phrases of PT-6 contain proper names
or out-of-vocabulary words, which are difficult to be
matched. The accuracy based on PT-1 is also quite
low. We found that it is mainly because the phrase
pairs in PT-1 are automatically clustered, many of
which are merely ?similar? words rather than syn-
onyms (e.g., ?borrow? vs. ?buy?).
Next, we try to find out whether it is necessary to
combine all PTs. Thus we conducted several runs,
each of which added the most useful PT from the
left ones. The results are shown in Table 2. We can
see that all the PTs are useful, as each PT provides
some new correct phrase substitutes and the accu-
racy increases when adding each PT except PT-1.
Since the PTs are extracted from different re-
sources, they have different contributions. Here we
only discuss the contributions of PT-5 and PT-6,
which are first used in paraphrasing in this paper.
PT-5 is useful for paraphrasing uncommon concepts
since it can ?explain? concepts with their definitions.
1026
PT combination #DCS Accuracy
4+7 553 56.93%
4+5+7 581 58.97%
4+5+3+7 638 59.42%
4+5+3+2+7 649 60.15%
4+5+3+2+1+7 699 60.14%
4+5+3+2+1+6+7 711 60.16%
Table 2: Performances of different combinations of para-
phrase tables.
For instance, in the following test sentence S1, the
word ?amnesia? is a relatively uncommon word, es-
pecially for the people using English as the second
language. Based on PT-5, S1 can be paraphrased
into T1, which is much easier to understand.
S1: I was suffering from amnesia.
T1: I was suffering from memory loss.
The disadvantage of PT-5 is that substituting
words with the definitions sometimes leads to gram-
matical errors. For instance, substituting ?heat
shield? in the sentence S2 with ?protective barrier
against heat? keeps the meaning unchanged. How-
ever, the paraphrased sentence T2 is ungrammatical.
S2: The U.S. space agency has been cautious
about heat shield damage.
T2: The U.S. space administration has been
cautious about protective barrier against heat
damage.
As previously mentioned, PT-6 is less effective
compared with the other PTs. However, it is use-
ful for paraphrasing some special phrases, such as
digital products, computer software, etc, since these
phrases often appear in user queries. For example,
S3 below can be paraphrased into T3 using PT-6.
S3: I have a canon powershot S230 that uses
CF memory cards.
T3: I have a canon digital camera S230 that
uses CF memory cards.
The phrase ?canon powershot? can hardly be
paraphrased using the other PTs. It suggests that PT-
6 is useful for paraphrasing new emerging concepts
and expressions.
Test sentences Top-1 Top-5 Top-10
All 150 55.33% 45.20% 39.28%
50 from news 70.00% 62.00% 57.03%
50 from novel 56.00% 46.00% 37.42%
50 from forum 40.00% 27.60% 23.34%
Table 3: Top-n accuracy on different test sentences.
6.2 Sentence-level Evaluation
In this section, we evaluated the sentence-level qual-
ity of the generated paraphrases11. In detail, each
generated paraphrase was manually labeled as ?ac-
ceptable? or ?unacceptable?. Here, the criterion for
counting a sentence T as an acceptable paraphrase of
sentence S is that T is understandable and its mean-
ing is not evidently changed compared with S. For
example, for the sentence S4, T4 is an acceptable
paraphrase generated using our method.
S4: The strain on US forces of fighting in Iraq
and Afghanistan was exposed yesterday when
the Pentagon published a report showing that
the number of suicides among US troops is at
its highest level since the 1991 Gulf war.
T4: The pressure on US troops of fighting in
Iraq and Afghanistan was revealed yesterday
when the Pentagon released a report showing
that the amount of suicides among US forces
is at its top since the 1991 Gulf conflict.
We carried out sentence-level evaluation using the
top-1, top-5, and top-10 results of each test sentence.
The accuracy of the top-n results was computed as:
Accuracytop?n =
?N
i=1 ni
N ? n
(15)
where N is the number of test sentences. ni is the
number of acceptable paraphrases in the top-n para-
phrases of the i-th test sentence.
We computed the accuracy on the whole test set
(150 sentences) as well as on the three subsets, i.e.,
the 50 news sentences, 50 novel sentences, and 50
forum sentences. The results are shown in table 3.
It can be seen that the accuracy varies greatly on
different test sets. The accuracy on the news sen-
tences is the highest, while that on the forum sen-
tences is the lowest. There are several reasons. First,
11The evaluation was based on the paraphrasing results using
the combination of all seven PTs.
1027
the largest PT used in the experiments is extracted
using the bilingual parallel data, which are mostly
from news documents. Thus, the test set of news
sentences is more similar to the training data.
Second, the news sentences are formal while the
novel and forum sentences are less formal. Espe-
cially, some of the forum sentences contain spelling
mistakes and grammar mistakes.
Third, we find in the results that, most phrases
paraphrased in the novel and forum sentences are
commonly used phrases or words, such as ?food?,
?good?, ?find?, etc. These phrases are more dif-
ficult to paraphrase than the less common phrases,
since they usually have much more paraphrases in
the PTs. Therefore, it is more difficult to choose the
right paraphrase from all the candidates when con-
ducting sentence-level paraphrase generation.
Fourth, the forum sentences contain plenty of
words such as ?board (means computer board)?,
?site (means web site)?, ?mouse (means computer
mouse)?, etc. These words are polysemous and have
particular meanings in the domains of computer sci-
ence and internet. Our method performs poor when
paraphrasing these words since the domain of a con-
text sentence is hard to identify.
After observing the results, we find that there are
three types of errors: (1) syntactic errors: the gener-
ated sentences are ungrammatical. About 32% of the
unacceptable results are due to syntactic errors. (2)
semantic errors: the generated sentences are incom-
prehensible. Nearly 60% of the unacceptable para-
phrases have semantic errors. (3) non-paraphrase:
the generated sentences are well formed and com-
prehensible but are not paraphrases of the input sen-
tences. 8% of the unacceptable results are of this
type. We believe that many of the errors above can
be avoided by applying syntactic constraints and by
making better use of context information in decod-
ing, which is left as our future work.
7 Conclusion
This paper proposes a method that improves the
SMT-based sentence-level paraphrase generation
using phrasal paraphrases automatically extracted
from different resources. Our contribution is that
we combine multiple resources in the framework of
SMT for paraphrase generation, in which the dic-
tionary definitions and similar user queries are first
used as phrasal paraphrases. In addition, we analyze
and compare the contributions of different resources.
Experimental results indicate that although the
contributions of the exploited resources differ a lot,
they are all useful to sentence-level paraphrase gen-
eration. Especially, the dictionary definitions and
similar user queries are effective for paraphrasing
some certain types of phrases.
In the future work, we will try to use syntactic
and context constraints in paraphrase generation to
enhance the acceptability of the paraphrases. In ad-
dition, we will extract paraphrase patterns that con-
tain more structural variation and try to combine the
SMT-based and pattern-based systems for sentence-
level paraphrase generation.
Acknowledgments
We would like to thank Mu Li for providing us with
the SMT decoder. We are also grateful to Dongdong
Zhang for his help in the experiments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Michael Elhadad. 1997. Using Lex-
ical Chains for Text Summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10-17.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL, pages 50-57.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. In Computational Linguistics 19(2): 263-311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using Machine Translation Evalua-
tion Techniques to Determine Sentence-level Semantic
Equivalence. In Proceedings of IWP, pages 17-24.
1028
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
Lingual Query Suggestion Using Query Logs of Dif-
ferent Languages. In Proceedings of SIGIR, pages
463-470.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in
a Meaning-Text Generation Model. In Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Version
1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING/ACL,
pages 768-774.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using Paraphrases for Parame-
ter Tuning in Statistical Machine Translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 120-127.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proceedings of ACL,
pages 295-302.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
Marius Pasca and Pe?ter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, pages 119-130.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Hua Wu and Ming Zhou. 2003. Synonymous Collo-
cation Extraction Using Translation Information. In
Proceedings of ACL, pages 120-127.
1029
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834?842,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Application-driven Statistical Paraphrase Generation
Shiqi Zhao, Xiang Lan, Ting Liu, Sheng Li
Information Retrieval Lab, Harbin Institute of Technology
6F Aoxiao Building, No.27 Jiaohua Street, Nangang District
Harbin, 150001, China
{zhaosq,xlan,tliu,lisheng}@ir.hit.edu.cn
Abstract
Paraphrase generation (PG) is important
in plenty of NLP applications. However,
the research of PG is far from enough. In
this paper, we propose a novel method for
statistical paraphrase generation (SPG),
which can (1) achieve various applications
based on a uniform statistical model, and
(2) naturally combine multiple resources
to enhance the PG performance. In our
experiments, we use the proposed method
to generate paraphrases for three differ-
ent applications. The results show that
the method can be easily transformed from
one application to another and generate
valuable and interesting paraphrases.
1 Introduction
Paraphrases are alternative ways that convey the
same meaning. There are two main threads in the
research of paraphrasing, i.e., paraphrase recogni-
tion and paraphrase generation (PG). Paraphrase
generation aims to generate a paraphrase for a
source sentence in a certain application. PG shows
its importance in many areas, such as question
expansion in question answering (QA) (Duboue
and Chu-Carroll, 2006), text polishing in natu-
ral language generation (NLG) (Iordanskaja et al,
1991), text simplification in computer-aided read-
ing (Carroll et al, 1999), and sentence similarity
computation in the automatic evaluation of ma-
chine translation (MT) (Kauchak and Barzilay,
2006) and summarization (Zhou et al, 2006).
This paper presents a method for statistical
paraphrase generation (SPG). As far as we know,
this is the first statistical model specially designed
for paraphrase generation. It?s distinguishing fea-
ture is that it achieves various applications with a
uniform model. In addition, it exploits multiple
resources, including paraphrase phrases, patterns,
and collocations, to resolve the data shortage prob-
lem and generate more varied paraphrases.
We consider three paraphrase applications in
our experiments, including sentence compression,
sentence simplification, and sentence similarity
computation. The proposed method generates
paraphrases for the input sentences in each appli-
cation. The generated paraphrases are then man-
ually scored based on adequacy, fluency, and us-
ability. The results show that the proposed method
is promising, which generates useful paraphrases
for the given applications. In addition, comparison
experiments show that our method outperforms a
conventional SMT-based PG method.
2 Related Work
Conventional methods for paraphrase generation
can be classified as follows:
Rule-based methods: Rule-based PG methods
build on a set of paraphrase rules or patterns,
which are either hand crafted or automatically
collected. In the early rule-based PG research,
the paraphrase rules are generally manually writ-
ten (McKeown, 1979; Zong et al, 2001), which
is expensive and arduous. Some researchers then
tried to automatically extract paraphrase rules (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Zhao
et al, 2008b), which facilitates the rule-based PG
methods. However, it has been shown that the
coverage of the paraphrase patterns is not high
enough, especially when the used paraphrase pat-
terns are long or complicated (Quirk et al, 2004).
Thesaurus-based methods: The thesaurus-based
methods generate a paraphrase t for a source sen-
tence s by substituting some words in s with
their synonyms (Bolshakov and Gelbukh, 2004;
834
Kauchak and Barzilay, 2006). This kind of method
usually involves two phases, i.e., candidate extrac-
tion and paraphrase validation. In the first phase,
it extracts all synonyms from a thesaurus, such as
WordNet, for the words to be substituted. In the
second phase, it selects an optimal substitute for
each given word from the synonyms according to
the context in s. This kind of method is simple,
since the thesaurus synonyms are easy to access.
However, it cannot generate other types of para-
phrases but only synonym substitution.
NLG-based methods: NLG-based methods (Ko-
zlowski et al, 2003; Power and Scott, 2005) gen-
erally involve two stages. In the first one, the
source sentence s is transformed into its semantic
representation r by undertaking a series of NLP
processing, including morphology analyzing, syn-
tactic parsing, semantic role labeling, etc. In the
second stage, a NLG system is employed to gen-
erate a sentence t from r. s and t are paraphrases
as they are both derived from r. The NLG-based
methods simulate human paraphrasing behavior,
i.e., understanding a sentence and presenting the
meaning in another way. However, deep analysis
of sentences is a big challenge. Moreover, devel-
oping a NLG system is also not trivial.
SMT-based methods: SMT-based methods
viewed PG as monolingual MT, i.e., translating s
into t that are in the same language. Researchers
employ the existing SMT models for PG (Quirk
et al, 2004). Similar to typical SMT, a large
parallel corpus is needed as training data in the
SMT-based PG. However, such data are difficult
to acquire compared with the SMT data. There-
fore, data shortage becomes the major limitation
of the method. To address this problem, we have
tried combining multiple resources to improve the
SMT-based PG model (Zhao et al, 2008a).
There have been researchers trying to propose
uniform PG methods for multiple applications.
But they are either rule-based (Murata and Isa-
hara, 2001; Takahashi et al, 2001) or thesaurus-
based (Bolshakov and Gelbukh, 2004), thus they
have some limitations as stated above. Further-
more, few of them conducted formal experiments
to evaluate the proposed methods.
3 Statistical Paraphrase Generation
3.1 Differences between SPG and SMT
Despite the similarity between PG and MT, the
statistical model used in SMT cannot be directly
applied in SPG, since there are some clear differ-
ences between them:
1. SMT has a unique purpose, i.e., producing
high-quality translations for the inputs. On
the contrary, SPG has distinct purposes in
different applications, such as sentence com-
pression, sentence simplification, etc. The
usability of the paraphrases have to be as-
sessed in each application.
2. In SMT, words of an input sentence should
be totally translated, whereas in SPG, not all
words of an input sentence need to be para-
phrased. Therefore, a SPG model should be
able to decide which part of a sentence needs
to be paraphrased.
3. The bilingual parallel data for SMT are easy
to collect. In contrast, the monolingual paral-
lel data for SPG are not so common (Quirk
et al, 2004). Thus the SPG model should
be able to easily combine different resources
and thereby solve the data shortage problem
(Zhao et al, 2008a).
4. Methods have been proposed for automatic
evaluation in MT (e.g., BLEU (Papineni et
al., 2002)). The basic idea is that a translation
should be scored based on their similarity to
the human references. However, they cannot
be adopted in SPG. The main reason is that it
is more difficult to provide human references
in SPG. Lin and Pantel (2001) have demon-
strated that the overlapping between the au-
tomatically acquired paraphrases and hand-
crafted ones is very small. Thus the human
references cannot properly assess the quality
of the generated paraphrases.
3.2 Method Overview
The SPG method proposed in this work contains
three components, i.e., sentence preprocessing,
paraphrase planning, and paraphrase generation
(Figure 1). Sentence preprocessing mainly in-
cludes POS tagging and dependency parsing for
the input sentences, as POS tags and dependency
information are necessary for matching the para-
phrase pattern and collocation resources in the
following stages. Paraphrase planning (Section
3.3) aims to select the units to be paraphrased
(called source units henceforth) in an input sen-
tence and the candidate paraphrases for the source
835
Multiple Paraphrase Tables
PT1 ??
Paraphrase 
Planning
Paraphrase 
Generation t
Sentence 
Preprocessings
A
PT2 PTn
Figure 1: Overview of the proposed SPG method.
units (called target units) from multiple resources
according to the given application A. Paraphrase
generation (Section 3.4) is designed to generate
paraphrases for the input sentences by selecting
the optimal target units with a statistical model.
3.3 Paraphrase Planning
In this work, the multiple paraphrase resources are
stored in paraphrase tables (PTs). A paraphrase ta-
ble is similar to a phrase table in SMT, which con-
tains fine-grained paraphrases, such as paraphrase
phrases, patterns, or collocations. The PTs used in
this work are constructed using different corpora
and different score functions (Section 3.5).
If the applications are not considered, all units
of an input sentence that can be paraphrased us-
ing the PTs will be extracted as source units. Ac-
cordingly, all paraphrases for the source units will
be extracted as target units. However, when a cer-
tain application is given, only the source and target
units that can achieve the application will be kept.
We call this process paraphrase planning, which is
formally defined as in Figure 2.
An example is depicted in Figure 3. The ap-
plication in this example is sentence compression.
All source and target units are listed below the in-
put sentence, in which the first two source units
are phrases, while the third and fourth are a pattern
and a collocation, respectively. As can be seen, the
first and fourth source units are filtered in para-
phrase planning, since none of their paraphrases
achieve the application (i.e., shorter in bytes than
the source). The second and third source units are
kept, but some of their paraphrases are filtered.
3.4 Paraphrase Generation
Our SPG model contains three sub-models: a
paraphrase model, a language model, and a usabil-
ity model, which control the adequacy, fluency,
Input: source sentence s
Input: paraphrase application A
Input: paraphrase tables PTs
Output: set of source units SU
Output: set of target units TU
Extract source units of s from PTs: SU={su1, ?, sun}
For each source unit sui
Extract its target units TUi={tui1, ?, tuim}
For each target unit tuij
If tuij cannot achieve the application A
Delete tuij from TUi
End If
End For
If TUi is empty
Delete sui from SU
End If
End for
Figure 2: The paraphrase planning algorithm.
and usability of the paraphrases, respectively1.
Paraphrase Model: Paraphrase generation is a
decoding process. The input sentence s is first
segmented into a sequence of I units s?I1, which
are then paraphrased to a sequence of units t?I1.
Let (s?i, t?i) be a pair of paraphrase units, their
paraphrase likelihood is computed using a score
function ?pm(s?i, t?i). Thus the paraphrase score
ppm(s?I1, t?I1) between s and t is decomposed into:
ppm(s?I1, t?I1) =
I?
i=1
?pm(s?i, t?i)?pm (1)
where ?pm is the weight of the paraphrase model.
Actually, it is defined similarly to the translation
model in SMT (Koehn et al, 2003).
In practice, the units of a sentence may be para-
phrased using different PTs. Suppose we have K
PTs, (s?ki , t?ki) is a pair of paraphrase units from
the k-th PT with the score function ?k(s?ki , t?ki),
then Equation (1) can be rewritten as:
ppm(s?I1, t?I1) =
K?
k=1
(
?
ki
?k(s?ki , t?ki)?k) (2)
where ?k is the weight for ?k(s?ki , t?ki).
Equation (2) assumes that a pair of paraphrase
units is from only one paraphrase table. However,
1The SPG model applies monotone decoding, which does
not contain a reordering sub-model that is often used in SMT.
Instead, we use the paraphrase patterns to achieve word re-
ordering in paraphrase generation.
836
The US government should take the overall situation into consideration and actively promote bilateral high-tech trades.
The US government
The US administration
The US government on
overall situation 
overall interest
overall picture
overview
situation as a whole
whole situation
take [NN_1] into consideration  
consider [NN_1]
take into account [NN_1]
take account of [NN_1]
take [NN_1] into account
take into consideration [NN_1]
<promote, OBJ, trades>  
<sanction, OBJ, trades>
<stimulate, OBJ, trades>
<strengthen, OBJ, trades>
<support, OBJ, trades>
<sustain, OBJ, trades>
Paraphrase application: sentence compression
Figure 3: An example of paraphrase planning.
we find that about 2% of the paraphrase units ap-
pear in two or more PTs. In this case, we only
count the PT that provides the largest paraphrase
score, i.e., k? = argmaxk{?k(s?i, t?i)?k}.
In addition, note that there may be some units
that cannot be paraphrased or prefer to keep un-
changed during paraphrasing. Therefore, we have
a self-paraphrase table in the K PTs, which para-
phrases any separate word w into itself with a con-
stant score c: ?self (w,w) = c (we set c = e?1).
Language Model: We use a tri-gram language
model in this work. The language model based
score for the paraphrase t is computed as:
plm(t) =
J?
j=1
p(tj |tj?2tj?1)?lm (3)
where J is the length of t, tj is the j-th word of t,
and ?lm is the weight for the language model.
Usability Model: The usability model prefers
paraphrase units that can better achieve the ap-
plication. The usability of t depends on para-
phrase units it contains. Hence the usability model
pum(s?I1, t?I1) is decomposed into:
pum(s?I1, t?I1) =
I?
i=1
pum(s?i, t?i)?um (4)
where ?um is the weight for the usability model
and pum(s?i, t?i) is defined as follows:
pum(s?i, t?i) = e?(s?i,t?i) (5)
We consider three applications, including sentence
compression, simplification, and similarity com-
putation. ?(s?i, t?i) is defined separately for each:
? Sentence compression: Sentence compres-
sion2 is important for summarization, subti-
tle generation, and displaying texts in small
screens such as cell phones. In this appli-
cation, only the target units shorter than the
sources are kept in paraphrase planning. We
define ?(s?i, t?i) = len(s?i) ? len(t?i), where
len(?) denotes the length of a unit in bytes.
? Sentence simplification: Sentence simplifi-
cation requires using common expressions in
sentences so that readers can easily under-
stand the meaning. Therefore, only the target
units more frequent than the sources are kept
in paraphrase planning. Here, the frequency
of a unit is measured using the language
model mentioned above3. Specifically, the
langauge model assigns a score scorelm(?)
for each unit and the unit with larger score
is viewed as more frequent. We define
?(s?i, t?i) = 1 iff scorelm(t?i) > scorelm(s?i).
? Sentence similarity computation: Given a
reference sentence s?, this application aims to
paraphrase s into t, so that t is more similar
(closer in wording) with s? than s. This ap-
plication is important for the automatic eval-
uation of machine translation and summa-
rization, since we can paraphrase the human
translations/summaries to make them more
similar to the system outputs, which can re-
fine the accuracy of the evaluation (Kauchak
and Barzilay, 2006). For this application,
2This work defines compression as the shortening of sen-
tence length in bytes rather than in words.
3To compute the language model based score, the
matched patterns are instantiated and the matched colloca-
tions are connected with words between them.
837
only the target units that can enhance the sim-
ilarity to the reference sentence are kept in
planning. We define ?(s?i, t?i) = sim(t?i, s?)?
sim(s?i, s?), where sim(?, ?) is simply com-
puted as the count of overlapping words.
We combine the three sub-models based on a
log-linear framework and get the SPG model:
p(t|s) =
K?
k=1
(?k
?
ki
log ?k(s?ki , t?ki))
+ ?lm
J?
j=1
log p(tj |tj?2tj?1)
+ ?um
I?
i=1
?(s?i, t?i) (6)
3.5 Paraphrase Resources
We use five PTs in this work (except the self-
paraphrase table), in which each pair of paraphrase
units has a score assigned by the score function of
the corresponding method.
Paraphrase phrases (PT-1 to PT-3): Para-
phrase phrases are extracted from three corpora:
(1) Corp-1: bilingual parallel corpus, (2) Corp-
2: monolingual comparable corpus (comparable
news articles reporting on the same event), and
(3) Corp-3: monolingual parallel corpus (paral-
lel translations of the same foreign novel). The
details of the corpora, methods, and score func-
tions are presented in (Zhao et al, 2008a). In
our experiments, PT-1 is the largest, which con-
tains 3,041,822 pairs of paraphrase phrases. PT-2
and PT-3 contain 92,358, and 17,668 pairs of para-
phrase phrases, respectively.
Paraphrase patterns (PT-4): Paraphrase patterns
are also extracted from Corp-1. We applied the ap-
proach proposed in (Zhao et al, 2008b). Its basic
assumption is that if two English patterns e1 and e2
are aligned with the same foreign pattern f , then
e1 and e2 are possible paraphrases. One can refer
to (Zhao et al, 2008b) for the details. PT-4 con-
tains 1,018,371 pairs of paraphrase patterns.
Paraphrase collocations (PT-5): Collocations4
can cover long distance dependencies in sen-
tences. Thus paraphrase collocations are useful for
SPG. We extract collocations from a monolingual
4A collocation is a lexically restricted word pair with a
certain syntactic relation. This work only considers verb-
object collocations, e.g., <promote, OBJ, trades>.
corpus and use a binary classifier to recognize if
any two collocations are paraphrases. Due to the
space limit, we cannot introduce the detail of the
approach. We assign the score ?1? for any pair
of paraphrase collocations. PT-5 contains 238,882
pairs of paraphrase collocations.
3.6 Parameter Estimation
To estimate parameters ?k(1 ? k ? K), ?lm,
and ?um, we adopt the approach of minimum error
rate training (MERT) that is popular in SMT (Och,
2003). In SMT, however, the optimization objec-
tive function in MERT is the MT evaluation cri-
teria, such as BLEU. As we analyzed above, the
BLEU-style criteria cannot be adapted in SPG. We
therefore introduce a new optimization objective
function in this paper. The basic assumption is that
a paraphrase should contain as many correct unit
replacements as possible. Accordingly, we design
the following criteria:
Replacement precision (rp): rp assesses the pre-
cision of the unit replacements, which is defined
as rp = cdev(+r)/cdev(r), where cdev(r) is the
total number of unit replacements in the generated
paraphrases on the development set. cdev(+r) is
the number of the correct replacements.
Replacement rate (rr): rr measures the para-
phrase degree on the development set, i.e., the per-
centage of words that are paraphrased. We define
rr as: rr = wdev(r)/wdev(s), where wdev(r) is
the total number of words in the replaced units on
the development set, and wdev(s) is the number of
words of all sentences on the development set.
Replacement f-measure (rf): We use rf as the
optimization objective function in MERT, which
is similar to the conventional f-measure and lever-
ages rp and rr: rf = (2? rp? rr)/(rp+ rr).
We estimate parameters for each paraphrase ap-
plication separately. For each application, we first
ask two raters to manually label all possible unit
replacements on the development set as correct or
incorrect, so that rp, rr, and rf can be automati-
cally computed under each set of parameters. The
parameters that result in the highest rf on the de-
velopment set are finally selected.
4 Experimental Setup
Our SPG decoder is developed by remodeling
Moses that is widely used in SMT (Hoang and
Koehn, 2008). The POS tagger and depen-
dency parser for sentence preprocessing are SVM-
838
Tool (Gimenez and Marquez, 2004) and MST-
Parser (McDonald et al, 2006). The language
model is trained using a 9 GB English corpus.
4.1 Experimental Data
Our method is not restricted in domain or sentence
style. Thus any sentence can be used in develop-
ment and test. However, for the sentence similarity
computation purpose in our experiments, we want
to evaluate if the method can enhance the string-
level similarity between two paraphrase sentences.
Therefore, for each input sentence s, we need a
reference sentence s? for similarity computation.
Based on the above consideration, we acquire
experiment data from the human references of
the MT evaluation, which provide several human
translations for each foreign sentence. In detail,
we use the first translation of a foreign sentence
as the source s and the second translation as the
reference s? for similarity computation. In our ex-
periments, the development set contains 200 sen-
tences and the test set contains 500 sentences, both
of which are randomly selected from the human
translations of 2008 NIST Open Machine Transla-
tion Evaluation: Chinese to English Task.
4.2 Evaluation Metrics
The evaluation metrics for SPG are similar to the
human evaluation for MT (Callison-Burch et al,
2007). The generated paraphrases are manually
evaluated based on three criteria, i.e., adequacy,
fluency, and usability, each of which has three
scales from 1 to 3. Here is a brief description of
the different scales for the criteria:
Adequacy 1: The meaning is evidently changed.
2: The meaning is generally preserved.
3: The meaning is completely preserved.
Fluency 1: The paraphrase t is incomprehensible.
2: t is comprehensible.
3: t is a flawless sentence.
Usability 1: t is opposite to the application purpose.
2: t does not achieve the application.
3: t achieves the application.
5 Results and Analysis
We use our method to generate paraphrases for the
three applications. Results show that the percent-
ages of test sentences that can be paraphrased are
97.2%, 95.4%, and 56.8% for the applications of
sentence compression, simplification, and similar-
ity computation, respectively. The reason why the
last percentage is much lower than the first two
is that, for sentence similarity computation, many
sentences cannot find unit replacements from the
PTs that improve the similarity to the reference
sentences. For the other applications, only some
very short sentences cannot be paraphrased.
Further results show that the average number of
unit replacements in each sentence is 5.36, 4.47,
and 1.87 for sentence compression, simplification,
and similarity computation. It also indicates that
sentence similarity computation is more difficult
than the other two applications.
5.1 Evaluation of the Proposed Method
We ask two raters to label the paraphrases based
on the criteria defined in Section 4.2. The labeling
results are shown in the upper part of Table 1. We
can see that for adequacy and fluency, the para-
phrases in sentence similarity computation get the
highest scores. About 70% of the paraphrases are
labeled ?3?. This is because in sentence similar-
ity computation, only the target units appearing
in the reference sentences are kept in paraphrase
planning. This constraint filters most of the noise.
The adequacy and fluency scores of the other two
applications are not high. The percentages of la-
bel ?3? are around 30%. The main reason is that
the average numbers of unit replacements for these
two applications are much larger than sentence
similarity computation. It is thus more likely to
bring in incorrect unit replacements, which influ-
ence the quality of the generated paraphrases.
The usability is needed to be manually labeled
only for sentence simplification, since it can be
automatically labeled in the other two applica-
tions. As shown in Table 1, for sentence simplifi-
cation, most paraphrases are labeled ?2? in usabil-
ity, while merely less than 20% are labeled ?3?.
We conjecture that it is because the raters are not
sensitive to the slight change of the simplification
degree. Thus they labeled ?2? in most cases.
We compute the kappa statistic between the
raters. Kappa is defined as K = P (A)?P (E)1?P (E) (Car-
letta, 1996), where P (A) is the proportion of times
that the labels agree, and P (E) is the proportion
of times that they may agree by chance. We define
P (E) = 13 , as the labeling is based on three point
scales. The results show that the kappa statistics
for adequacy and fluency are 0.6560 and 0.6500,
which indicates a substantial agreement (K: 0.61-
0.8) according to (Landis and Koch, 1977). The
839
Adequacy (%) Fluency (%) Usability (%)
1 2 3 1 2 3 1 2 3
Sentence rater1 32.92 44.44 22.63 21.60 47.53 30.86 0 0 100
compression rater2 40.54 34.98 24.49 25.51 43.83 30.66 0 0 100
Sentence rater1 29.77 44.03 26.21 22.01 42.77 35.22 25.37 61.84 12.79
simplification rater2 33.33 35.43 31.24 24.32 39.83 35.85 30.19 51.99 17.82
Sentence rater1 7.75 24.30 67.96 7.75 22.54 69.72 0 0 100
similarity rater2 7.75 19.01 73.24 6.69 21.48 71.83 0 0 100
Baseline-1 rater1 47.31 30.75 21.94 43.01 33.12 23.87 - - -
rater2 47.10 30.11 22.80 34.41 41.51 24.09 - - -
Baseline-2 rater1 29.45 52.76 17.79 25.15 52.76 22.09 - - -
rater2 33.95 46.01 20.04 27.61 48.06 24.34 - - -
Table 1: The evaluation results of the proposed method and two baseline methods.
kappa statistic for usability is 0.5849, which is
only moderate (K: 0.41-0.6).
Table 2 shows an example of the generated para-
phrases. A source sentence s is paraphrased in
each application and we can see that: (1) for sen-
tence compression, the paraphrase t is 8 bytes
shorter than s; (2) for sentence simplification, the
words wealth and part in t are easier than their
sources asset and proportion, especially for the
non-native speakers; (3) for sentence similarity
computation, the reference sentence s? is listed be-
low t, in which the words appearing in t but not in
s are highlighted in blue.
5.2 Comparison with Baseline Methods
In our experiments, we implement two baseline
methods for comparison:
Baseline-1: Baseline-1 follows the method pro-
posed in (Quirk et al, 2004), which generates
paraphrases using typical SMT tools. Similar to
Quirk et al?s method, we extract a paraphrase ta-
ble for the SMT model from a monolingual com-
parable corpus (PT-2 described above). The SMT
decoder used in Baseline-1 is Moses.
Baseline-2: Baseline-2 extends Baseline-1 by
combining multiple resources. It exploits all PTs
introduced above in the same way as our pro-
posed method. The difference from our method is
that Baseline-2 does not take different applications
into consideration. Thus it contains no paraphrase
planning stage or the usability sub-model.
We tune the parameters for the two baselines
using the development data as described in Sec-
tion 3.6 and evaluate them with the test data. Since
paraphrase applications are not considered by the
baselines, each baseline method outputs a single
best paraphrase for each test sentence. The gener-
ation results show that 93% and 97.8% of the test
sentences can be paraphrased by Baseline-1 and
Baseline-2. The average number of unit replace-
ments per sentence is 4.23 and 5.95, respectively.
This result suggests that Baseline-1 is less capa-
ble than Baseline-2, which is mainly because its
paraphrase resource is limited.
The generated paraphrases are also labeled by
our two raters and the labeling results can be found
in the lower part of Table 1. As can be seen,
Baseline-1 performs poorly compared with our
method and Baseline-2, as the percentage of la-
bel ?1? is the highest for both adequacy and flu-
ency. This result demonstrates that it is necessary
to combine multiple paraphrase resources to im-
prove the paraphrase generation performance.
Table 1 also shows that Baseline-2 performs
comparably with our method except that it does
not consider paraphrase applications. However,
we are interested how many paraphrases gener-
ated by Baseline-2 can achieve the given applica-
tions by chance. After analyzing the results, we
find that 24.95%, 8.79%, and 7.16% of the para-
phrases achieve sentence compression, simplifi-
cation, and similarity computation, respectively,
which are much lower than our method.
5.3 Informal Comparison with Application
Specific Methods
Previous research regarded sentence compression,
simplification, and similarity computation as to-
tally different problems and proposed distinct
method for each one. Therefore, it is interesting
to compare our method to the application-specific
methods. However, it is really difficult for us to
840
Source
sentence
Liu Lefei says that in the long term, in terms of asset alocation, overseas investment should occupy a
certain proportion of an insurance company?s overall allocation.
Sentence
compression
Liu Lefei says that in [the long run]phr , [in area of [asset alocation][NN 1]]pat, overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
simplification
Liu Lefei says that in [the long run]phr , in terms of [wealth]phr [distribution]phr , overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
similarity
Liu Lefei says that in [the long run]phr , in terms [of capital]phr allocation, overseas investment should
occupy [the [certain][JJ 1] ratio of [an insurance company?s overall allocation][NN 1]]pat.
(reference sentence: Liu Lefei said that in terms of capital allocation, outbound investment should make
up a certain ratio of overall allocations for insurance companies in the long run .)
Table 2: The generated paraphrases of a source sentence for different applications. The target units after
replacement are shown in blue and the pattern slot fillers are in cyan. [?]phr denotes that the unit is a
phrase, while [?]pat denotes that the unit is a pattern. There is no collocation replacement in this example.
reimplement the methods purposely designed for
these applications. Thus here we just conduct an
informal comparison with these methods.
Sentence compression: Sentence compression
is widely studied, which is mostly reviewed as a
word deletion task. Different from prior research,
Cohn and Lapata (2008) achieved sentence com-
pression using a combination of several opera-
tions including word deletion, substitution, inser-
tion, and reordering based on a statistical model,
which is similar to our paraphrase generation pro-
cess. Besides, they also used paraphrase patterns
extracted from bilingual parallel corpora (like our
PT-4) as a kind of rewriting resource. However,
as most other sentence compression methods, their
method allows information loss after compression,
which means that the generated sentences are not
necessarily paraphrases of the source sentences.
Sentence Simplification: Carroll et al (1999)
has proposed an automatic text simplification
method for language-impaired readers. Their
method contains two main parts, namely the lex-
ical simplifier and syntactic simplifier. The for-
mer one focuses on replacing words with simpler
synonyms, while the latter is designed to transfer
complex syntactic structures into easy ones (e.g.,
replacing passive sentences with active forms).
Our method is, to some extent, simpler than Car-
roll et al?s, since our method does not contain syn-
tactic simplification strategies. We will try to ad-
dress sentence restructuring in our future work.
Sentence Similarity computation: Kauchak
and Barzilay (2006) have tried paraphrasing-based
sentence similarity computation. They paraphrase
a sentence s by replacing its words with Word-
Net synonyms, so that s can be more similar in
wording to another sentence s?. A similar method
has also been proposed in (Zhou et al, 2006),
which uses paraphrase phrases like our PT-1 in-
stead of WordNet synonyms. These methods can
be roughly viewed as special cases of ours, which
only focus on the sentence similarity computation
application and only use one kind of paraphrase
resource.
6 Conclusions and Future Work
This paper proposes a method for statistical para-
phrase generation. The contributions are as fol-
lows. (1) It is the first statistical model spe-
cially designed for paraphrase generation, which
is based on the analysis of the differences between
paraphrase generation and other researches, espe-
cially machine translation. (2) It generates para-
phrases for different applications with a uniform
model, rather than presenting distinct methods for
each application. (3) It uses multiple resources,
including paraphrase phrases, patterns, and collo-
cations, to relieve data shortage and generate more
varied and interesting paraphrases.
Our future work will be carried out along two
directions. First, we will improve the components
of the method, especially the paraphrase planning
algorithm. The algorithm currently used is sim-
ple but greedy, which may miss some useful para-
phrase units. Second, we will extend the method to
other applications, We hope it can serve as a uni-
versal framework for most if not all applications.
Acknowledgements
The research was supported by NSFC (60803093,
60675034) and 863 Program (2008AA01Z144).
Special thanks to Wanxiang Che, Ruifang He,
Yanyan Zhao, Yuhang Guo and the anonymous re-
viewers for insightful comments and suggestions.
841
References
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation, pages 136-158.
Jean Carletta. 1996. Assessing Agreement on Clas-
sification Tasks: The Kappa Statistic. In Computa-
tional Linguistics, 22(2): 249-254.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, John Tait. 1999. Simpli-
fying Text for Language-Impaired Readers. In Pro-
ceedings of EACL, pages 269-270.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion In Proceed-
ings of COLING, pages 137-144.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had Asked:
The impact of paraphrasing for Question Answer-
ing. In Proceedings of HLT-NAACL, pages 33-36.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, pages
43-46.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL, pages 455-462.
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure us-
ing lexico-grammatical resources. In Proceedings
of IWP, pages 1-8.
J. R. Landis and G. G. Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data.
In Biometrics 33(1): 159-174.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Parsing with a
Two-Stage Discriminative Parser. In Proceedings of
CoNLL.
Kathleen R. McKeown. 1979. Paraphrasing Using
Given and New Information in a Question-Answer
System. In Proceedings of ACL, pages 67-72.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal Model for Paraphrasing - Using Transformation
Based on a Defined Criteria. In Proceedings of NL-
PRS, pages 47-54.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic gen-
eration of large-scale paraphrases. In Proceedings of
IWP, pages 73-79.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Tetsuro Takahashi, Tomoyam Iwakura, Ryu Iida, At-
sushi Fujita, Kentaro Inui. 2001. KURA: A
Transfer-based Lexico-structural Paraphrasing En-
gine. In Proceedings of NLPRS, pages 37-46.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. Combining Multiple Resources
to Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. ParaEval: Using Para-
phrases to Evaluate Summaries Automatically. In
Proceedings of HLT-NAACL, pages 447-454.
Chengqing Zong, Yujie Zhang, Kazuhide Yamamoto,
Masashi Sakamoto, Satoshi Shirai. 2001. Approach
to Spoken Chinese Paraphrasing Based on Feature
Extraction. In Proceedings of NLPRS, pages 551-
556.
842
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125?128,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Statistical Machine Translation Model Based on a Synthetic
Synchronous Grammar
Hongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo Wang
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cn
Abstract
Recently, various synchronous grammars
are proposed for syntax-based machine
translation, e.g. synchronous context-free
grammar and synchronous tree (sequence)
substitution grammar, either purely for-
mal or linguistically motivated. Aim-
ing at combining the strengths of differ-
ent grammars, we describes a synthetic
synchronous grammar (SSG), which ten-
tatively in this paper, integrates a syn-
chronous context-free grammar (SCFG)
and a synchronous tree sequence substitu-
tion grammar (STSSG) for statistical ma-
chine translation. The experimental re-
sults on NIST MT05 Chinese-to-English
test set show that the SSG based transla-
tion system achieves significant improve-
ment over three baseline systems.
1 Introduction
The use of various synchronous grammar based
formalisms has been a trend for statistical ma-
chine translation (SMT) (Wu, 1997; Eisner, 2003;
Galley et al, 2006; Chiang, 2007; Zhang et al,
2008). The grammar formalism determines the in-
trinsic capacities and computational efficiency of
the SMT systems.
To evaluate the capacity of a grammar formal-
ism, two factors, i.e. generative power and expres-
sive power are usually considered (Su and Chang,
1990). The generative power refers to the abil-
ity to generate the strings of the language, and
the expressive power to the ability to describe the
same language with fewer or no extra ambigui-
ties. For the current synchronous grammars based
SMT, to some extent, the generalization ability of
the grammar rules (the usability of the rules for the
new sentences) can be considered as a kind of the
generative power of the grammar and the disam-
biguition ability to the rule candidates can be con-
sidered as an embodiment of expressive power.
However, the generalization ability and the dis-
ambiguition ability often contradict each other in
practice such that various grammar formalisms
in SMT are actually different trade-off be-
tween them. For instance, in our investiga-
tions for SMT (Section 3.1), the Formally SCFG
based hierarchical phrase-based model (here-
inafter FSCFG) (Chiang, 2007) has a better gen-
eralization capability than a Linguistically moti-
vated STSSG based model (hereinafter LSTSSG)
(Zhang et al, 2008), with 5% rules of the former
matched by NIST05 test set while only 3.5% rules
of the latter matched by the same test set. How-
ever, from expressiveness point of view, the for-
mer usually results in more ambiguities than the
latter.
To combine the strengths of different syn-
chronous grammars, this paper proposes a statisti-
cal machine translation model based on a synthetic
synchronous grammar (SSG) which syncretizes
FSCFG and LSTSSG. Moreover, it is noteworthy
that, from the combination point of view, our pro-
posed scheme can be considered as a novel system
combination method which goes beyond the ex-
isting post-decoding style combination of N -best
hypotheses from different systems.
2 The Translation Model Based on the
Synthetic Synchronous Grammar
2.1 The Synthetic Synchronous Grammar
Formally, the proposed Synthetic Synchronous
Grammar (SSG) is a tuple
G = ??
s
,?
t
, N
s
, N
t
, X,P?
where ?
s
(?
t
) is the alphabet set of source (target)
terminals, namely the vocabulary; N
s
(N
t
) is the
alphabet set of source (target) non-terminals, such
125
? ? ???
Figure 1: A syntax tree pair example. Dotted lines
stands for the word alignments.
as the POS tags and the syntax labels; X repre-
sents the special nonterminal label in FSCFG; and
P is the grammar rule set which is the core part of
a grammar. Every rule r in P is as:
r = ??, ?,A
NT
, A
T
, ???
where ? ? [{X}, N
s
,?
s
]
+
is a sequence of one or
more source words in ?
s
and nonterminals sym-
bols in [{X}, N
s
];? ? [{X}, N
t
,?
t
]
+
is a se-
quence of one or more target words in ?
t
and non-
terminals symbols in [{X}, N
t
]; A
T
is a many-to-
many corresponding set which includes the align-
ments between the terminal leaf nodes from source
and target side, and A
NT
is a one-to-one corre-
sponding set which includes the synchronizing re-
lations between the non-terminal leaf nodes from
source and target side; ?? contains feature values
associated with each rule.
Through this formalization, we can see that
FSCFG rules and LSTSSG rules are both in-
cluded. However, we should point out that the
rules with mixture of X non-terminals and syn-
tactic non-terminals are not included in our cur-
rent implementation despite that they are legal
under the proposed formalism. The rule extrac-
tion in current implementation can be considered
as a combination of the ones in (Chiang, 2007)
and (Zhang et al, 2008). Given the sentence pair
in Figure 1, some SSG rules can be extracted as
illustrated in Figure 2.
2.2 The SSG-based Translation Model
The translation in our SSG-based translation
model can be treated as a SSG derivation. A
derivation consists of a sequence of grammar rule
applications. To model the derivations as a latent
variable, we define the conditional probability dis-
tribution over the target translation e and the cor-
Input: A source parse tree T (f
J
1
)
Output: A target translation e?
for u := 0 to J ? 1 do
for v := 1 to J ? u do
foreach rule r = ??, ?,A
NT
, A
T
, ??? spanning
[v, v + u] do
if A
NT
of r is empty then
Add r into H[v, v + u];
end
else
Substitute the non-terminal leaf node pair
(N
src
, N
tgt
) with the hypotheses in the
hypotheses stack corresponding with N
src
?s
span iteratively.
end
end
end
end
Output the 1-best hypothesis in H[1, J] as the final translation.
Figure 3: The pseudocode for the decoding.
responding derivation d of a given source sentence
f as
(1) p
?
(d, e|f) =
exp
?
k
?
k
H
k
(d, e, f)
?
?
(f)
where H
k
is a feature function ,?
k
is the corre-
sponding feature weight and ?
?
(f) is a normal-
ization factor for each derivation of f. The main
challenge of SSG-based model is how to distin-
guish and weight the different kinds of derivations
. For a simple illustration, using the rules listed in
Figure 2, three derivations can be produced for the
sentence pair in Figure 1 by the proposed model:
d
1
= (R
4
, R
1
, R
2
)
d
2
= (R
6
, R
7
, R
8
)
d
3
= (R
4
, R
7
, R
2
)
All of them are SSG derivations while d
1
is also a
FSCFG derivation, d
2
is also a LSTSSG deriva-
tion. Ideally, the model is supposed to be able
to weight them differently and to prefer the better
derivation, which deserves intensive study. Some
sophisticated features can be designed for this is-
sue. For example, some features related with
structure richness and grammar consistency
1
of a
derivation should be designed to distinguish the
derivations involved various heterogeneous rule
applications. For the page limit and the fair com-
parison, we only adopt the conventional features
as in (Zhang et al, 2008) in our current implemen-
tation.
1
This relates with reviewers? questions: ?can a rule ex-
pecting an NN accept an X?? and ?. . . the interaction between
the two typed of rules . . . ?. In our study in progress, we
would design some features to distinguish the derivation steps
which fulfill the expectation or not, to measure how much
heterogeneous rules are applied in a derivation and so on.
126
R6
1?
BA
VV[2]NN[1]
1
VB[2] NP[1]?
PN
to me
TO PRP
PP
1
R7
penthe
DT NN
NP
??
NN
1
R4 Give 1? 1 X[1] X[2]X[2]? X[1] R5 X[1]X[1] ? 2 the pen 1 to 2me1??
R1 penthe 1?? 1 R3 theGive 2 pen 1? 2?? 1R2 to me 1? 1
R8
?
VV
Give
VB
11
Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure
1. R
1
-R
3
are bilingual phrase rules, R
4
-R
5
are FSCFG rules and R
6
-R
8
are LSTSSG rules.
2.3 Decoding
For efficiency, our model approximately search for
the single ?best? derivation using beam search as
(2) (
?
e,
?
d) = argmax
e,d
{
?
k
?
k
h
k
(d, e, f)
}
.
The major challenge for such a SSG-based de-
coder is how to apply the heterogeneous rules in a
derivation. For example, (Chiang, 2007) adopts a
CKY style span-based decoding while (Liu et al,
2006) applies a linguistically syntax node based
bottom-up decoding, which are difficult to inte-
grate. Fortunately, our current SSG syncretizes
FSCFG and LSTSSG. And the conventional de-
codings of both FSCFG and LSTSSG are span-
based expansion. Thus, it would be a natural way
for our SSG-based decoder to conduct a span-
based beam search. The search procedure is given
by the pseudocode in Figure 3. A hypotheses
stack H[i, j] (similar to the ?chart cell? in CKY
parsing) is arranged for each span [i, j] for stor-
ing the translation hypotheses. The hypotheses
stacks are ordered such that every span is trans-
lated after its possible antecedents: smaller spans
before larger spans. For translating each span
[i, j], the decoder traverses each usable rule r =
??, ?,A
NT
, A
T
, ???. If there is no nonterminal
leaf node in r, the target side ? will be added into
H[i, j] as the candidate hypothesis. Otherwise, the
nonterminal leaf nodes in r should be substituted
iteratively by the corresponding hypotheses until
all nonterminal leaf nodes are processed. The key
feature of our decoder is that the derivations are
based on synthetic grammar, so that one derivation
may consist of applications of heterogeneous rules
(Please see d
3
in Section 2.2 as a simple demon-
stration).
3 Experiments and Discussions
Our system, named HITREE, is implemented in
standard C++ and STL. In this section we report
Extracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)
BP 11,137 4,613(41.4%) 323(0.5%)
LSTSSG 45,580 28,497(62.5%) 984(3.5%)
FSCFG 59,339 25,520(43.0%) 1,266(5.0%)
HITREE 93,782 49,404(52.7%) 1,927(3.9%)
Table 1: The statistics of the counts of the rules in
different phases. ?k? means one thousand.
on experiments with Chinese-to-English transla-
tion base on it. We used FBIS Chinese-to-English
parallel corpora (7.2M+9.2M words) as the train-
ing data. We also used SRI Language Model-
ing Toolkit to train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus(181M words). NIST MT2002 test set is used
as the development set. The NIST MT2005 test
set is used as the test set. The evaluation met-
ric is case-sensitive BLEU4. For significant test,
we used Zhang?s implementation (Zhang et al,
2004)(confidence level of 95%). For comparisons,
we used the following three baseline systems:
LSTSSG An in-house implementation of linguis-
tically motivated STSSG based model similar
to (Zhang et al, 2008).
FSCFG An in-house implementation of purely
formally SCFG based model similar to (Chiang,
2007).
MBR We use an in-house combination system
which is an implementation of a classic sentence
level combination method based on the Minimum
Bayes Risk (MBR) decoding (Kumar and Byrne,
2004).
3.1 Statistics of Rule Numbers in Different
Phases
Table 1 summarizes the statistics of the rules for
different models in three phases: after extrac-
tion (Extracted), after scoring(Scored), and af-
ter filtering (Filtered) (filtered by NIST05 test
set just, similar to the filtering step in phrase-
based SMT system). In Extracted phase, FSCFG
127
ID System BLEU4 #of used rules(k)
1 LSTSSG 0.2659?0.0043 984
2 FSCFG 0.2613?0.0045 1,266
3 HITREE 0.2730?0.0045 1,927
4 MBR(1,2) 0.2685?0.0044 ?
Table 2: The Comparison of LSTSSG, FSCFG
,HITREE and the MBR.
has obvious more rules than LSTSSG. However,
in Scored phase, this situation reverses. Inter-
estingly, the situation reverses again in Filtered
phase. The reasons for these phenomenons are
that FSCFG abstract rules involves high-degree
generalization. Each FSCFG abstract rule aver-
agely have several duplicates
2
in the extracted rule
set. Then, the duplicates will be discarded dur-
ing scoring. However, due to the high-degree gen-
eralization , the FSCFG abstract rules are more
likely to be matched by the test sentences. Con-
trastively, LSTSSG rules have more diversified
structures and thus weaker generalization capabil-
ity than FSCFG rules. From the ratios of two tran-
sition states, Table 1 indicates that HITREE can
be considered as compromise of FSCFG between
LSTSSG.
3.2 Overall Performances
The performance comparison results are presented
in Table 2. The experimental results show that
the SSG-based model (HITREE) achieves signifi-
cant improvements over the models based on the
two isolated grammars: FSCFG and LSTSSG
(both p < 0.001). From combination point of
view, the newly proposed model can be consid-
ered as a novel method going beyond the con-
ventional post-decoding style combination meth-
ods. The baseline Minimum Bayes Risk com-
bination of LSTSSG based model and FSCFG
based model (MBR(1, 2)) obtains significant im-
provements over both candidate models (both p <
0.001). Meanwhile, the experimental results show
that the proposed model outperforms MBR(1, 2)
significantly (p < 0.001). These preliminary re-
sults indicate that the proposed SSG-based model
is rather promising and it may serve as an alterna-
tive, if not superior, to current combination meth-
ods.
4 Conclusions
To combine the strengths of different gram-
mars, this paper proposes a statistical machine
2
Rules with identical source side and target side are du-
plicated.
translation model based on a synthetic syn-
chronous grammar (SSG) which syncretizes a
purely formal synchronous context-free gram-
mar (FSCFG) and a linguistically motivated syn-
chronous tree sequence substitution grammar
(LSTSSG). Experimental results show that SSG-
based model achieves significant improvements
over the FSCFG-based model and LSTSSG-based
model.
In the future work, we would like to verify
the effectiveness of the proposed model on vari-
ous datasets and to design more sophisticated fea-
tures. Furthermore, the integrations of more dif-
ferent kinds of synchronous grammars for statisti-
cal machine translation will be investigated.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In computational linguistics, 33(2).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL 2003.
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-
rich syntactic translation models In Proceedings of
ACL-COLING.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
04.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. In Proceedings of ACL-COLING.
Keh-Yin Su and Jing-Shin Chang. 1990. Some key
Issues in Designing Machine Translation Systems.
Machine Translation, 5(4):265-300.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of LREC 2004, pages 2051-2054.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT.
128
Combining Neural Networks and Statistics for 
 Chinese Word sense disambiguation 
Zhimao Lu  Ting Liu  Sheng Li  
Information Retrieval Laboratory of Computer Science & Technology School,  
Harbin Institute of Technology 
Harbin, China, 150001 
{lzm, tliu}@ir.hit.edu.cn 
 
Abstract 
The input of network is the key problem for 
Chinese Word sense disambiguation utilizing 
the Neural Network. This paper presents an 
input model of Neural Network that calculates 
the Mutual Information between contextual 
words and ambiguous word by using statistical 
method and taking the contextual words to 
certain number beside the ambiguous word 
according to (-M, +N). The experiment adopts 
triple-layer BP Neural Network model and 
proves how the size of training set and the 
value of M and N affect the performance of 
Neural Network model. The experimental 
objects are six pseudowords owning three 
word-senses constructed according to certain 
principles. Tested accuracy of our approach on 
a close-corpus reaches 90.31%,, and 89.62% on 
a open-corpus. The experiment proves that the 
Neural Network model has good performance 
on Word sense disambiguation. 
1 Introduction 
It is general that one word with many senses in 
natural language. According statistics, there are 
about 42% ambiguous words in Chinese corpus (Lu, 
2001). Word sense disambiguation (WSD) is a 
method to determine the sense of ambiguous word 
given the context circumstance.  
WSD, a long-standing problem in NLP, has been 
a very active research topic,, which can be well 
applied in many NLP systems, such as Information 
Retrieval, Text Mining, Machine Translation, Text 
Categorization, Text Summarization, Speech 
Recognition, Text to Speech, and so on. 
With rising of Corpus linguistics, the machine 
learning methods based on statistics are booming 
(Yarowsky, 1992). These methods draw the support 
from the high-powered computers, get the statistics of 
large real-world corpus, find and acquire knowledge 
of linguistics automatically. They deal with all change 
by invariability, thus it is easy to trace the evaluation 
and development of natural language. So the statistic 
methods of NLP has attracted the attention of 
professional researchers and become the mainstream 
bit by bit. Corpus-based Statistical approaches are  
Decision Tree (Pedersen, 2001), Decision List, 
Genetic Algorithm, Naive-Bayesian Classifier 
(Escudero, 2000)?Maximum Entropy Model (Adam, 
1996; Li, 1999), and so on.  
Corpus-based statistical approaches can be divided 
into supervised and unsupervised according to whether 
training corpus is sense-labeled text. Supervised 
learning methods have the good learning ability and 
can get better accuracy in WSD experiments (Sch?tze, 
1998). Obviously the data sparseness problem is a 
bottleneck for supervised learning algorithm. If you 
want to get better learning and disambiguating effect, 
you can enlarge the size and smooth the data of 
training corpus. According to practical demand, it 
would spend much more time and manpower to 
enlarge the size of training corpus. Smoothing data is 
merely a subsidiary measure. The sufficient large size 
of training corpus is still the foundation to get a 
satisfied effect in WSD experiment. 
Unsupervised WSD never depend on tagged 
corpus and could realize the training of large real 
corpus coming from all kinds of applying field. So 
researchers begin to pay attention to this kind of 
methods (Lu, 2002). The kind of methods can 
overcome the sparseness problem in a degree. 
It is obvious that the two kinds of methods based 
on statistic have their own advantages and 
disadvantages, and cannot supersede each other.  
This paper researches the Chinese WSD using the 
model of artificial neural network and investigates 
the effect on WSD from input model of neural 
network constructed by the context words and the 
size of training corpus.  
2 BP Neural Network 
At the moment, there are about more than 30 kinds of 
artificial neural network (ANN) in the domain of 
research and application. Especially, BP neural 
network is a most popular model of ANN nowadays.  
2.1 The structure of BP Neural Network 
The BP model provides a simple method to 
calculate the variation of network performance 
cased by variation of single weight. This model 
contains not only input nodes and output nodes, but 
also multi-layer or mono-layer hidden nodes. Fig1.1 
is a construction chart of triple-layer BP neural 
network. As it is including the weights modifying 
process from the output layer to the input layer 
resulting from the total errors, the BP neural 
network is called Error Back Propagation network. 
 
 
 
 
 
 
 
 
 
Fig. 1.1 BP Network 
Fig.1.1 The structure of BP neural network 
Except for the nodes of input layer, all nodes of 
other layers are non-linear input and output. So the 
feature function should be differential on every part 
of function. General speaking, we can choose the 
sigmoid, tangent inspired, or linear function as the 
feature function because they are convenient for 
searching and solving by gradient technique. 
Formula (1) is a sigmoid function.  
                         ?1? 
The output of sigmoid function ranges between 0 
and 1, increasing monotonically with its input. 
Because it maps a very large input domain to a 
small range of outputs, it is often referred to as the 
squashing function of the unit. The output layer and 
hidden layer should adopt the sigmoid inspired 
function under the condition of intervention on the 
output, such as confining the output between 0 and 
1.  
2.2 Back Propagation function of BP 
neural network 
The joint weights should be revised many times 
during the progress of the error propagating back in 
BP networks. The variation of joint weights every 
time is solved by the method of gradient descent. 
Because there is no objective output in hidden layer, 
the variation of joint weight in hidden layer is 
solved under the help of error back propagation in 
output layer. If there are many hidden layers, this 
method can reason out the rest to the first layer by 
analogy. 
1) the variation of joint weights in output layer 
To calculate the variation of joint weights from 
input i?th to output k?th is as following: 
?wik = ??      = ??               (2) 
 = ?(tk - Ok )f2? O?i = ? ?ik O?i 
?ik = (tk ? Ok )f2?                    (3) 
?bki = ??       = ??               (4) 
= ?(tk - Ok )f2? = ? ?ik 
 
2) the variation of joint weights in hidden layer 
To calculate the variation of joint weights from 
input j?th to output i?th is as following: 
?w?ij = -?       = -?                   (5) 
1
1 + e-xf (x)  = ???
?E ????wik
?E ??? ?Ok 
?Ok ??? ? wik 
?E ???? w?ij
n?
k=1 
?E ????bki 
?E ??? ?Ok 
?Ok ??? ?bki 
?? ??
?? 
??
x1  x3 xn x2 
y1  y3 ym y2 
Outputs 
Hidden 
Inputs 
?E ??? ?Ok 
?Ok ????O?i 
?O?i???
? w?ij
n?
k=1 
= ?    (tk - Ok )f2? wik f1?pj  
= ? ?ij pj 
where:  ?ij = ei f1??ei =    ?ik wik          (6) 
     ?b?ki = ? ?ij                          (7) 
3. The construction of WSD model 
Under the consideration of fact that only 
numerical data can be accepted by the input and 
output of neural network, if BP neural network is 
used on WSD, the prerequisite is to vector the part of 
semantic meaning (words or phrases) and sense. 
In the event of training BP model, the input vector 
P and objective vector O of WSD should be 
determined firstly. And then we should choose the 
construction of neural network that needs to be 
designed, say, how many layers is network, how 
many neural nodes are in every layer, and the 
inspired function of hidden layer and output layer. 
The training of model still needs the vector added 
weight, output, and error vector. The training is over 
when the sum of square errors is less than the 
objection of error. Or the errors of output very to 
adjust the joint weight back and repeat the training. 
3.1 To vector the vocabulary 
WSD depends on the context to judge the meaning 
of ambiguous words. So the input of model should 
be the ambiguous words and the contextual words 
round them. In order to vector the words in the 
context, the Mutual Information (MI) of ambiguous 
words and context should be calculated. So MI can 
show the opposite distance of ambiguous words and 
contextual words. MI can replace every contextual 
word. That is suitable to as the input model. The 
function of MI is as follow: 
(8) 
  P(w1) and P(w2) are the probability of word w1 
and w2 to appear in the corpus separately. While 
P(w1, w2) is the probability of word w1 and w2 to 
appear together.  
The experimental corpus in this article stems 
from the People Daily of 1998. The extent is 
123,882 lines (10,000,000 words), including 
121,400 words and phrases.  
3.2 The pretreatment of BP network model 
The supervised WSD need artificial mark of 
meaning. But it is time consuming to mark artificially. 
So it is difficult to get the large scope and high quality 
training linguistic corpus. In order to overcome this 
difficulty and get large enough experimental linguistic 
corpuses, we should turn to seek the new way. 
We use pseudoword in place of the real word. That 
can get the arbitrary large experimental corpus 
according to the real demand.  
3.2.1 The construction of Pseudoword 
Pseudoword is the artificial combination of 
several real words on the basis of experimental 
demand to form an unreal word that possesses 
many features of real words and instead of real 
word as the experimental object in natural language 
research.  
In the real world, one word has many meanings 
derives from the variation and flexible application 
of words. That needs a long-term natural evolution. 
Frankly speaking, that evolution never ceases at all 
times. For example, the word ?? ?(da3) extends 
some new uses in recent years. Actually, in the 
endless history river of human beings, the 
development and variation of words meaning are 
rapid so far as to be more rapid than the 
replacement of dictionaries sometimes. Usually that 
makes an awkward position when you use 
dictionary to define the words meanings. Definitely, 
it is inconvenient for the research of natural 
linguistics based on dictionary.  
But the meaning of pseudoword (Sch?tze, 1992) 
need not defined with the aid of dictionary and 
simulates the real ambiguous word to survey the 
effect of various algorithms of classified meanings.  
To form a pseudoword need the single meaning 
word as a morpheme.  
Set:   Wp =  w1 / w2 / ? / wi 
Wp is a pseudoword formed with wi which 
contains i algorithms and meanings for every 
n ?
k=1 
MI(w1, w2) = 
P(w1, w2) ?????P(w1)P(w2) log 
n ?
k=1 
algorithm of pseudoword is single meaning and 
every living example is about equal to a 
pseudoword marked meaning in corpus. That is 
similar to the effect of artificial marked meaning. 
But the effect is more stable and reliable than 
artificial marked meaning. What?s more, the scope 
of corpus can enlarge endless according to the 
demand to avoid the phenomenon of sparse data. 
To define the number of algorithm, we count the 
average number of meanings according to the 
large-sized Chinese dictionaries (Table 3.1). Table 
3.2 show the overall number of ambiguous word 
and percentage of ambiguous word having 2~4 
meanings in all ambiguous word. These two charts 
indicate that verb is most active in Chinese and its 
average number of meanings is most, about 2.56. 
The percentage of ambiguous word having 2~4 
meanings is most in all ambiguous word.  
part of 
speech 
Average 
sense 
?including 
single-sense 
word? 
Average sense
?only 
ambiguous 
word? 
noun 1.136452 2.361200 
verb 1.220816 2.558158 
adjective 1.144717 2.300774 
adverb 1.059524 2.078431 
Table 3.1 the average number of a Chinese 
word?s sense 
3.2.2 Define the input vector  
It should be based on context to determine the 
sense of ambiguous word. The model?s input should 
be the vector of the ambiguous word and context 
words. It is well-known that the number of context  
ambiguous 
word 7955 / 
Bi-senses 
word 5799 72.80% 
Tri-senses 
word 1154 14.51% 
Four-senses 
word 450 5.66% 
Table 3.2 the distributing of ambiguous word 
words showing on the both sides of ambiguous 
word is not fixed in different sentences. But the 
number of vectors needed by BP network is fixed. 
In other words, the number of neural nodes of input 
model is fixed in the training. If the extracting 
method of feature vector is (-M, +N) in context, in 
other words there are M vectors on the left of 
ambiguous word and N vectors on the right, the 
extraction of feature vectors must span the limit of 
sentences. If the number of feature vectors is not 
enough, the ambiguous words on the left and right 
boundaries of whole corpus do not participate in the 
training.  
According to the extracting method of feature 
vector (-M, +N), the vector of model input is as 
following: 
V ?? = {MI11?MI 12???MI1i?MI 11??MI 12????
MI 1j??MI21?MI 22???MI2i?MI 21??MI 22????
MI2j??MI31?MI 32???MI3i?MI 31??MI 32????
MI 3j?}?1?i?M?1?j?N. 
Where, MI1i , MI1j? are the MI of context and the 
first meaning of ambiguous word?MI2i , MI2j? are 
the MI of context and the second meaning of 
ambiguous word?MI3i ?MI3j? are the MI of context 
and the third meaning of ambiguous word. MI1i, 
MI2i and MI3i are the feature words of ambiguous 
word on the left and MI of ambiguous word. MI1j?, 
MI2j? and MI2j? are the feature words of ambiguous 
word on the right and MI of ambiguous word.  
pseudo-
words
word
ID 
sample 
number 
pseudo-
words 
word 
ID 
sample
number
34466 5550 84323 3773
71345 3715 12751 2284
31796 12098 52915 3900W1 
total 21363
W4 
total 9957
71072 9296 53333 1362
78031 6024 29053 6135
48469 1509 75941 1205W2 
total 16829
W5 
total 8702
7464 25925 39945 2346
77375 2478 71335 1640
23077 4704 51491 1012W3 
total 33107
W6 
total 4998
Table 3.3 the total number of the feature -vector 
sample of ambiguous word 
Training corpus are 105,000 lines, and each line 
is a paragraph, totally about 10,000,000 words. 
Table 3.3 shows the number of collected feature 
vector samples (the frequency of ambiguous word).  
3.3 The definition of output model 
Every ambiguous word has three meanings, 
totally eighteen meanings for six ambiguous words. 
Every ambiguous word trains a model and every 
model has three outputs showed by three-bit integer 
of binary system, such as the three meanings of 
ambiguous word W are showed as followed: 
si1 = 100    si2 = 010   si3 = 001 
3.4 The definition of network structure 
According to statistics, when (-M, +N) are (-8, 
+9) using the method of feature extraction, the 
cover percentage of effective information is more 
than 87% (Lu, 2001). However, if the sentence is 
very short, collecting the contextual feature words 
on the basis of (-8, +9) can include much useless 
information to the input model. Undoubtedly, that 
will increase more noisy effect and deduce the 
meaning-distinguish ability of verve network.  
This article makes an on-the-spot investigation of 
experimental corpus, a fairly integrated meaning 
unit (the marks of border including comma, 
semicolon, ellipsis, period, question mark, 
exclamation mark, and the like), which average 
length is between 9~10 words. So this article 
collects the contextual feature words on the basis of 
(-5, +5) in the experiments, 10 feature words 
available that calculate MI with each meaning of 
ambiguous word separately to get 30 vectors. All 
punctuation marks should be filtered while the 
feature words are collected. The input layer of 
neural network model is regarded as 30 neural 
nodes. The triple-layer neural network adopts the 
inspired S function. From that, the number of neural 
nodes in hidden layer is defined as 12 on the basis 
of experimental contrast, and 3 neural nodes in 
output layer. Hence, the structure of model is 30 ? 
12 ? 3, and the precision of differential training is 
defined as 0.3 based on the experimental contrast. 
3.5 The test and training of model 
The experimental corpus appeared in front are 
123,882 lines. It is divided to three parts according 
to the demand of experiment, C1 (15,000 lines), C2 
(60,000 lines), and C3 (105,000 lines). The open 
test corpus is 18,882 lines. 
Table 3.3 tells us that there is a great disparity 
between the sample numbers of different 
ambiguous words in the experimental corpus of the 
same class. And the distribution of different 
meanings is not even for same ambiguous word. 
For the trained neural network has the good ability 
of differentiation for each word, the number of 
training sample should be about equal to each other 
for each meaning. So this experiment selects the 
least training samples. For example, there are 200 
samples of the first meaning in training corpus, the 
second 400, and the third 500. To balance the input, 
each meaning merely has 200 samples to be elected 
for training. 
Three groups of training corpus can train 3 neural 
networks possessing different vectors for every 
ambiguous word and make the unclose and open 
test for these networks separately.  
4 The result of experiment 
In order to analyze the effect that the extent of 
training corpus influences the meaning distinguish 
ability of neural network, this article trains the 
model of neural network using the experimental 
corpus individually, C1, C2 and C3, and makes the 
close and open test for 6 ambiguities separately.  
The close test means the corpus are same in test 
and training.  
The experiment is divided into two groups 
according to the extracting method of contextual 
feature words.  
4.1 The first experiment one 
Table 4.1 shows the result of the first experiment  
which extracts the contextual feature words using 
the method of ??5?+ 5?. 
In addition, the first experiment investigates that 
the extent of training corpus (the number of training 
samples big or small) influences the ability to 
distinguish the models. The result of test for 6  
close-test open-test pseudo- 
words accuracy Training set accuracy 
Training
set 
W1 0.8800 C2 0.8951 C3 
W2 0.8867 C2 0.8775 C2 
W3 0.8652 C3 0.8574 C3 
W4 0.8532 C3 0.8687 C3 
W5 0.8769 C3 0.8745 C3 
W6 0.8868 C2 0.8951 C3 
Table 4.1 The contrast chat of experimental result 
for six ambiguities 
ambiguities is showed in table 4.2 (close test), table 
4.3 (open test), and table 4.4. Considering the 
length of this article, table 4.2 and table 4.3 shows 
the detailed data, and table 4.4 is brief.  
Training set pseudo- 
words C1 C2 C3 
sense 1 0.9226 0.8169 0.8991
sense 2 0.5513 0.8017 0.6872
sense 3 0.8027 0.9564 0.9510W1 
average 0.7589 0.8800 0.8720
sense 1 0.8121 0.8780 0.9377
sense 2 0.8389 0.8968 0.8804
sense 3 0.7248 0.8856 0.8370
W6 
average 0.7919 0.8868 0.8850
Table 4.2 The result of W1 and W6 in close test  
under the different training corpus 
4.2 The second experiment 
The second experiment investigates emphatically 
the effect that the method to collect the feature 
words influences the ability to distinguish BP 
model. 
Training set pseudo- 
words C1 C2 C3 
sense 1 0.9019 0.7827 0.8942
sense 2 0.4607 0.8097 0.7175
sense 3 0.7792 0.9500 0.9515W1 
average 0.7573 0.8798 0.8951
sense 1 0.8233 0.9093 0.9535
sense 2 0.8799 0.8182 0.8604
sense 3 0.7278 0.8544 0.8038W6 
average 0.8259 0.8683 0.8951
Table 4.3 The result of W1 and W6 in open test 
 under the different training corpus 
There are many methods adopted in this 
experiment, including (?10?+ 10),??3?+ 3?,??3?
+ 7?,??7?+ 3?,??4?+ 6?and??6?+ 4?. Merely 
the ambiguous words W1 and W6 are regarded as the 
Training set pseudo- 
words C1 C2 C3 
W2 0.6628 0.8867 0.8772
W3 0.6695 0.8453 0.8652
W4 0.7414 0.8452 0.8532
W5
close
0.8283 0.8537 0.8769
W2 0.7287 0.8613 0.8700
W3 0.8085 0.8384 0.8574
W4 0.7920 0.8655 0.8687
W5
open
0.8288 0.8775 0.8745
Table 4.4 The contrast chart of experimental 
 result for four ambiguities 
Table 4.5 the experimental result under different 
feature collecting method 
experimental objects in this group experiment. See 
table 4.5 for the correct percentage of WSD.  
5. Analysis and discussion  
See table 5.1 for the number of experimental 
corpus samples in experiment.  
According to the table 3.3 and 5.1, the frequency of 
the each meaning (morpheme) of ambiguous word 
showing in corpus is quite different. That accords 
with the distribution of the every meanings of 
ambiguous word. However, there is one different 
point that the frequency of the each meaning of 
ambiguous word is rather high (that is the outcome 
selected by morpheme.). In other words, there are 
many examples showing for the each meaning of 
ambiguous word in training and test corpus. On the 
contrast, the difference of frequency is quite  
accuracy pseudo-
words
feature 
collecting 
method close-test open-test
Training
set 
??10?+10? 0.8897 0.8685 C1 
??3?+3? 0.7917 0.7176 C2 
??4?+6? 0.8600 0.8888 C3 
??6?+4? 0.8797 0.8938 C2 
??3?+7? 0.8514 0.8827 C3 
W1 
??7?+3? 0.8431 0.8825 C3 
??10?+10? 0.9031 0.8962 C2 
??3?+3? 0.8487 0.8460 C2 
??4?+6? 0.8982 0.8873 C3 
??6?+4? 0.8480 0.8772 C2 
??3?+7? 0.8669 0.8359 C3 
W6 
??7?+3? 0.8982 0.8895 C3 
pseudo- 
words 
Morpheme 
ID 
sample 
number 
pseudo- 
words 
Morpheme
ID 
sample
number
34466 1040 84323 591
71345 662 12751 484W1 
31796 2101 
W4 
52915 829
71072 1296 53333 274
78031 1043 29053 1153W2 
48469 315 
W5 
75941 238
7464 4389 39945 430
77375 469 71335 308W3 
23077 865 
W6 
51491 158
Table 5.1 The number of experimental samples 
obvious for the each meaning of real ambiguous 
word, because some meanings are used in oral 
language. But that never or seldom appears in 
experimental corpus.  
The statistics can uncover this linguistic 
phenomenon. We find that the meaning of the most 
percentage of ambiguous word showing in the 
corpus is 83.54% on the whole percentage of each 
meaning. That illustrates the distribution of each 
meaning has a great disparity in real ambiguous 
word. Seeing that condition, to differentiate the 
meaning of ambiguous word is harder than that of 
real ambiguous word absolutely.  
5.1 The analysis and discussion of the first 
experiment 
Table 4.1 records the results of close and open 
tests in detail and the training materials to get these 
results.  
Seeing from the experimental results, the correct 
percentage reaches 89.51% most (ambiguous word 
W1 and W6) in open test of WSD, and 85.74% the 
least (ambiguous word W3). 
The relationship of correct percentage and the 
extent of training corpus can be deduced from the 
experimental results of table 4.2, 4.3 and 4.4. 
The larger the extent of training corpus (the 
number of training sample?, the larger the result of 
close test. It is obvious to see that from C1 to C2. 
From C2 to C3 one or two experimental results 
fluctuate more or less.  
With the growing of training sample, the 
experimental results of open test increase steadily, 
except ambiguous word W2 (a little bit difference).  
The experimental data prove the growing of 
training samples rise the correct percentage. 
However, when the rising reaches to a certain 
degree, more rising is not good for the improvement 
of model. What?s more, the effect of noise is more 
and more remarkable. That decreases the model?s 
ability of differentiation in a certain degree. On the 
other hand, after the growing of training corpus, the 
linguistic phenomenon around ambiguities is richer 
and richer, more and more complex. That makes it 
harder to determine the meaning.  
5.2 The analysis and discussion of the second 
experiment 
This article emphasizes on the collecting method 
of contextual feature words in experiment two, in 
other words, the effect that the different values of M 
and N influence the model of BP network. The 
experimental results (table 4.1 and 4.5) tell us that 
the context windows influence the correct 
percentage heavily. The correct percentage 
increases almost by leaps and bounds from (?3?+ 3) 
to??5?+5?. The discrepancy is obvious despite 
close test or open test. The correct percentage 
increase again to??10?+ 10?, in which the close test 
of ambiguous word W6 is more than 90% and 
89.62% the close test, with the exception of W1 
which open test is slightly special. That illustrates 
the more widely the context windows open, the 
more the effective information is caught to benefit 
the WSD more. 
Comparing the four feature methods of collection, 
including ??3?+ 7?,??7?+ 3?,??4?+ 6?
and??6?+ 4? with??5?+ 5?, the number of feature 
words besides the ambiguous word is various and 
the experimental results (table 4.1 and 4.5) are not 
same, although the windows are same. Among them, 
the correct percentage of ??5?+ 5?is the highest. 
And that of??4?+ 6?and??6?+ 4?is better than 
that of??3?+ 7?and??7?+ 3?a bit. That shows 
the more balanceable the feature words besides 
ambiguous word, the more advantageous to judge 
meaning, and the better the experimental results.  
In addition, some experimental results of open 
test are better than that of close test. The main 
reason is the experimental corpus of open test is 
smaller than training corpus. So the contextual 
meanings of ambiguous word in experimental 
corpus are rather explicit. Thereby, that explains 
why should be this kind of experimental result.  
5.3 Conclusions 
Considering the analysis of experimental data, 
the conclusions are as following: 
First, the artificial model of neural network 
established in this article has good ability of 
differentiation for Chinese meaning.  
Next, higher correct percentage of WSD stems 
from the large enough corpus. 
At last, the larger the windows of contextual 
feature words, the more the effective information. 
At the same time, the more balanceable the number 
of feature words beside the ambiguous word, the 
more beneficial that for WSD.  
6 Concluding remarks 
Although the BP network is a classified model 
applied extensively, the report of research on WSD 
about it is seldom. Especially the report about the 
Chinese WSD is less, and only one report (Zhang, 
2001) is available in internal reports.  
Zhang (2001) uses 96 semantic classes to instead 
the all words in training corpus according to the 
TongyiciCilin. The input model is the codes of 
semantic class of contextual words and ambiguities. 
The experiment of WSD merely makes for one 
phrase ? ?(cai2liao4) in this document and the 
correct percentage of open test is 80.4%. ? ? 
has 3 meanings and that is similar to the 
ambiguities structured in my article.  
Using BP for Chinese WSD, the key point and 
difficulty are on the determination of input model. 
The performance of input model may influence the 
construction of BP network and the output result 
directly.   
We make the experiment on the input of BP 
network many times and finally find the input 
model introduced as above (table 3.1) which test 
result is satisfied.  
Acknowledgements This work was supported by 
the National Natural Science Foundation of China 
(Grant No. 60203020). 
 
Reference 
Lu Song, Bai Shuo, et al 2001. Supervised word 
sense disambiguation bassed on Vector Space 
Model, Journal of Comouter Research & 
Development, 38(6): 662-667. 
Pedersen. 2001. Lexical semantic ambiguous word 
resolution with bigram-based decision trees, In 
Proceedings of the Second International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 157-168, Mexico City, February. 
Escudero, G., Marquez,L., et al 2000. Naive Bayes 
and examplar based approaches to word sense    
disambiguation revisited. In Proceedings of the 14th 
Europear Conference on Artificial Intelligence, 
ECAI. 
Adam,L.B. 1996. A maximum entropy approach to 
natural language proceeding. Computational 
Linguistics, 22(1):39-71. 
Li, J. Z. 1999. An improved maximum language 
and its application. Journal of software, 3:257-263. 
Yarowsky, D. Word sense disambiguation using 
statistical models of Roget?s categories trained on 
large corpora. In: Zampolli, A., ed. Computation 
Linguistic?92. Nantas: Association for 
Computational Linguistics, 1992. 454~460.  
Hinrich Sch?tze, 1998. Automatic word sense 
discrimination. Computational Linguistics, 24(1): 
97-124. 
Lu Song., Bai Shuo. 2002. An unsupervised 
approach to word sense disambiguation based on 
sense-word in vector space model. Journal of 
Software. 13(06):1082-08 
Hinrich Sch?tze. 1992. Context space. In AAAI 
Fall Symposium on Probabilistic Approaches to 
Natural Language, pages 113?120, Cambridge, 
MA. 
Lu Song Bai Shuo. 2001. Quantitative Analysis of 
Context Field. In Natural Language Processing, 
CHINESEJ.COMPUTERS, 24(7) , 742-747 
Zhang Guoqing, Zhang Yongkui. 2001. A 
Neural-network Based Word Sense Disambiguation 
Method. Computer Engineering, 27(12). 
A New Chinese Natural Language Understanding Architecture
Based on Multilayer Search Mechanism
Wanxiang Che Ting Liu Sheng Li
School of Computer Science and Technology
Harbin Institute of Technology
P.O. Box 321, HIT
Harbin China, 150001
{car, tliu, ls}@ir.hit.edu.cn
Abstract
A classical Chinese Natural Language Under-
standing (NLU) architecture usually includes
several NLU components which are executed
with some mechanism. A new Multilayer Search
Mechanism (MSM) which integrates and quan-
tifies these components into a uniform multi-
layer treelike architecture is presented in this
paper. The mechanism gets the optimal re-
sult with search algorithms. The components
in MSM affect each other. At last, the per-
formance of each component is enhanced. We
built a practical system ? CUP (Chinese Under-
standing Platform) based on MSM with three
layers. By the experiments on Word Segmen-
tation, a better performance was achieved. In
theory the normal cascade and feedback mech-
anism are just some special cases of MSM.
1 Introduction
At present a classical Chinese NLU architec-
ture usually includes several components, such
as Word Segmentation (Word-Seg), POS Tag-
ging, Phrase Analysis, Parsing, Word Sense Dis-
ambiguation (WSD) and so on. These compo-
nents are executed one by one from lower layers
(such as Word-Seg, POS Tagging) to higher lay-
ers (such as Parsing, WSD) to form a kind of
cascade mechanism. But when people build a
NLU system based on these complex language
analysis, it is a very serious problem since the
errors of each layer component are multiplied.
With more and more analysis components, the
final result becomes too bad to be applicable.
Another problem is that the components in
the system affect each other when people build a
practical but toy NLU system. Here the toy sys-
tem means that each component is ideal enough
with perfect input. But in fact, on the one hand
the lower layer components need the informa-
tion of higher layer components; on the other
hand the incorrect analysis of lower layers must
reduce the accuracy of higher layers. In Chinese
Word-Seg component, many segmentation am-
biguities which cannot be solved using only lexi-
cal information. In order to improve the perfor-
mance of Word-Seg, we have to use some syntax
and even semantic information. Without cor-
rect Word-Seg results, however the syntax and
semantic parser cannot obtain a correct analy-
sis. It is a chain debts problem.
People have tried to solve the error-multiplied
problem by integrating multi-layers into a uni-
form model (Gao et al, 2001; Nagata, 1994).
But with the increasing number of integrated
layers, the model becomes too complex to build
or solve.
The feedback mechanism (Wu and Jiang,
1998) helps to use the information of high lay-
ers to control the final result. If the analysis
at feedback point cannot be passed, the whole
analysis will be denied. This mechanism places
too much burden on the function of feedback
point. This leads to the problems that a correct
lower layer result may be rejected or an error
result may be accepted.
We propose a new Multilayer Search Mecha-
nism (MSM) to solve the problems mentioned
above. Based on the mechanism, we build a
practical Chinese NLU platform ? CUP (Chi-
nese Understanding Platform). Section 2 intro-
duces the background and architecture of the
new mechanism and how to build it up. Exper-
imental results with CUP is given in Section 3.
In Section 4, we discuss why the new mechanism
gets better results than the old ones. Conclu-
sions and the some future work follow in Sec-
tion 5.
2 Multilayer Search Mechanism
The novel Multilayer Search Mechanism (MSM)
integrates and quantifies NLU components into
a uniform multilayer treelike platform, such as
Word-Seg, POS Tagging, Parsing and so on.
These components affect each other by comput-
ing the final score and then get better results.
2.1 Background
Considering a Chinese sentence, the sen-
tence analysis task can be formally defined
as finding a set of word segmentation se-
quence (W ), a POS tagging sequence (POS),
a syntax dependency parsing tree (DP ) and
so on which maximize their joint probability
P (W,POS,DP, ? ? ?). In this paper, we assume
that there are only three layers W , POS and
DP in MSM. It is relatively straightforward,
however, to extend the method to the case for
which there are more than three layers. There-
fore, the sentence analysis task can be described
as finding a triple < W,POS,DP > that max-
imize the joint probability P (W,POS,DP ).
< W,POS,DP >= arg max
W,POS,DP
P (W,POS,DP )
The joint probability distribution
P (W,POS,DP ) can be written in the fol-
lowing form using the chain rule of probability:
P (W,POS,DP )
=P (W )P (POS|W )P (DP |W,POS)
Where P (W ) is considered as the probabil-
ity of the word segmentation layer, P (POS|W )
is the conditional probability of POS Tag-
ging with a given word segmentation result,
P (DP |W,POS) is the conditional probability
of a dependency parsing tree with a given word
segmentation and POS Tagging result similarly.
So the form of < W,POS,DP > can be trans-
formed into:
< W,POS,DP >
= arg max
W,POS,DP
P (W,POS,DP )
= arg max
W,POS,DP
P (W )P (POS|W )P (DP |W,POS)
= arg max
W,POS,DP
logP (W ) + logP (POS|W )
+ logP (DP |W,POS)
= arg min
W,POS,DP
? logP (W )? logP (POS|W )
? logP (DP |W,POS)
We consider that each inversion of probability?s
logarithm at the last step of the above equation
is a score given by a component (Such as Word-
Seg, POS Tagging and so on). So at last, we find
an n-tuple < W,POS,DP, ? ? ? > that minimizes
the last score Sn of a sentence analysis result
with n layers. Sn is defined as:
Sn = s1 + s2 + ? ? ?+ sn (1)
si denotes the score of the ith layer compo-
nent.
2.2 The Architecture of Multilayer
Search Mechanism
Because there are lots of analysis results at each
layer, it?s a combinatorial explosion problem to
find the optimal result. Assuming that each
component produces m results for an input on
average and there are n layers in a NLU system,
the final search space is mn. With the increas-
ing of n, it?s impossible for a system to find the
optimal result in the huge search space.
The classical cascade mechanism uses a
greedy algorithm to solve the problem. It only
keeps the optimal result at each layer. But if
it?s a fault analysis result for the optimal result
at a layer, it?s impossible for this mechanism to
find the final correct analysis result.
To overcome the difficulty, we build a new
Multilayer Search Mechanism (MSM). Different
from the cascade mechanism, MSM maintains a
number of results at each component, so that
the correct analysis should be included in these
results with high probability. Then MSM tries
to use the information of all layer components
to find out the correct analysis result. Different
from the feedback mechanism, the acceptance
of an analysis is not based on a higher layer
components alone. The lower layer components
provide some information to help to find the
correct analysis result as well.
According to the above idea, we design the
architecture of MSM with multilayer treelike
structure. The original input is root and the
several analysis results of the input become
branches. Iterating this progress, we get a big-
ger analysis tree. Figure 1 gives an analysis ex-
ample of a Chinese sentence ????????
??? (He likes beautiful flowers). For the in-
put sentence, there are several Word-Seg results
with scores (the lower the better). Then for each
of Word-Seg results, there are several POS Tag-
ging results, too. And for each of POS Tagging
result, the same thing happens. So we get a big
tree structure and the correct analysis result is a
path in the tree from the root to the leaf except
for there is no correct analysis result in some
analysis components.
A search algorithm can be used to find out the
correct analysis result among the lowest score in
the tree. But because each layer cannot give the
exact score in Equation 1 as the standard score
and the ability of analysis are different with
different layers, we should weight every score.
Then the last score is the linear weighted sum
(Equation 2).
Sn = w1s1 + w2s2 + ? ? ?+ wnsn (2)
si denotes the score of the ith layer compo-
nent which we will introduce in Section 3; wi
denotes the weight of the ith layer components
which we will introduce in the next section.
In order to get the optimal result, all kinds
of tree search algorithms can be used. Here
the BEST-FIRST SEARCH Algorithm (Rus-
sell and Norvig, 1995) is used. Figure 2 shows
the main algorithm steps.
POS Tag
Word-Seg
??
?????????
56.2? ?? ?? ? ??? 87.3? ? ?? ? ? ???
108.3? ?? ?? ? ???  r      v        a      u      n
187.4? ?? ?? ? ???  b      v        a      u      n
Figure 1: An Example of Multilayer Search
Mechanism
1. Add the initial node (starting point) to the
queue.
2. Compare the front node to the goal state. If
they match then the solution is found.
3. If they do not match then expand the front
node by adding all the nodes from its links.
4. If all nodes in the queue are expanded then the
goal state is not found (e.g.there is no solution).
Stop.
5. According to Equation 2 evaluate the score of
expanded nodes and reorder the nodes in the
queue.
6. Go to step 2.
Figure 2: BEST-FIRST SEARCH Algorithm
2.3 Layer Weight
We should find out a group of appropriate
w1, w2, ? ? ? , wn in Equation 2 to maximize the
number of the optimal paths in MSM which can
get the correct results. They are expressed by
W ?.
W ? = arg max
W
ObjFun(minSn) (3)
Here W ? is named as Whole Layer Weight.
ObjFun(?) denotes a function to value the re-
sult that a group ofW can get. Here we can con-
sider that the performance of each layer is pro-
portional to the last performance of the whole
system in MSM. So it maybe the F-Score of
Word-Seg, precision of POS Tagging and so on.
minSn returns the optimal analysis results with
the lowest score.
Here, the F-Score of Word-Seg can be defined
as the harmonic mean of recall and precision of
Word-Seg. That is to say:
Seg.F -Score = 2 ? Seg.Pre ? Seg.RecSeg.Pre+ Seg.Rec
Seg.Pre = #words correctly segmented#words segmented
Seg.Rec = #words correctly segmented#words in input texts
Finding out the most suitable group of W
is an optimization problem. Genetic Algo-
rithms (GAs) (Mitchell, 1996) is just an adap-
tive heuristic search algorithm based on the evo-
lutionary ideas of natural selection and genetics
to solve optimization problems. It exploits his-
torical information to direct the search into the
region of better performance within the search
space.
To use GAs to solve optimization prob-
lems (Wall, 1996) the following three questions
should be answered:
1. How to describ genome?
2. What is the objective function?
3. Which controlling parameters to be se-
lected?
A solution to a problem is represented as a
genome. The genetic algorithm then creates a
population of solutions and applies genetic op-
erators such as mutation and crossover to evolve
the solutions in order to find the best one(s) af-
ter several generations. The numbers of popula-
tion and generation are given by controlling pa-
rameters. The objective function decides which
solution is better than others.
In MSM, the genome is just the group of W
which can be denoted by real numbers between
0 and 1. Because the result is a linear weighted
sum, we should normalize the weights to let w1+
w2+ ? ? ?+wn = 1. The objective function is just
ObjFun(?) in Equation 3. Here the F-Score
of Word-Seg is used to describe it. We set the
genetic generations as 10 and the populations in
one generation as 30. The Whole Layer Weight
shows in the row of WLW in Table 4. The F-
Score of Word-Seg shows as Table 3.
We can see that the Word-Seg layer gets an
obviously large weight. So the final result is
inclined to the result of Word-Seg.
2.4 Self Confidence
Our analysis indicates that the method of
weighting a whole layer uniformly cannot re-
flect the individual information of each sen-
tence to some component. So the F-Score of
Word-Seg drops somewhat comparing with us-
ing Only Word-Seg. For example, the most
sentences which have ambiguities in Word-Seg
component are still weighted high with Word-
Seg layer weight. Then the final result may still
be the same as the result of Word-Seg compo-
nent. It is ambiguous, too. So we must use a
parameter to decrease the weight of a compo-
nent with ambiguity. It is used to describe the
analysis ability of a component for an input. We
name it as Self Confident (SC) of a component.
It is described by the difference between the first
and the second score of a component. Then the
bigger SC of a component, the larger weight of
it.
There are lots of methods to value the differ-
ence between two numbers. So there are many
kinds of definitions of SC. We use A and B to
denote the first and the second score of a compo-
nent respectively. Then the SC can be defined
as B?A, BA and so on. We must select the bet-ter one to represent SC. The better means that
a method which gets a lower Error Rate with a
threshold t? which gets the Minimal Error Rate.
t? = arg min
t
ErrRate(t)
ErrRate(t) denotes the Error Rate with the
threshold t. An error has two definitions:
? SC is higher than t but the first result is
fault
? SC is lower than t but the first result is
right
Then the Error Rate is the ratio between the
error number and the total number of sentences.
Table 2 is the comparison list between differ-
ent definitions of SC and their Minimal Error
Rate of Word-Seg. By this table we select B?A
as the last SC because it gets the minimal Min-
imal Error Rate within the different definitions
of SC.
SC is added into Equation 2 to describe the
individual information of each sentence inten-
sively. Equation 4 shows the new score method
of a path.
Sn = w1sc1s1 + w2sc2s2 + ? ? ?+ wnscnsn (4)
sci denotes the SC of a component in the ith
layer.
3 Experimental Results
3.1 Score of Components
We build a practical system CUP (Chinese
Understanding Platform) based on MSM with
three layers ? Word-Seg, POS Tagging and
Parsing. Each component not only provides the
n-best analysis result, but also the score of each
result.
In the Word-Seg component, we use the uni-
gram model (Liu et al, 1998) to value different
results of Word-Seg. So the score of a result is:
ScoreWord?Seg = ? logP (W ) = ?
?
logP (wi)
wi denotes the ith word in the Word-Seg re-
sult of a sentence.
In the POS Tagging component the classical
Markov Model (Manning and Schu?tze, 1999) is
used to select the n-best POS results of each
Word-Seg result. So the score of a result is:
ScorePOS =? logP (POS|W )
=? log P (W |POS)P (POS)P (W )
=?
?
logP (wi|ti)?
?
logP (ti|ti?1)
+ logP (W )
ti denotes the POS of the ith word in a Word-
Seg result of a sentence.
In the Parsing component, we use a Chinese
Dependency Parser System developed by HIT-
IRLab1. The score of a result is:
ScoreParsing =? logP (DP |W,POS)
=? log P (W,POS,DP )P (W,POS)
=?
?
logP (lij)
+ logP (W,POS)
lij denotes a link between the ith and jth
word in a Word-Seg and POS Tagging result
of a sentence.
Table 1 gives the one and five-best results of
each component with a correct input. The test
data comes from Beijing Univ. and Fujitsu Chi-
nese corpus (Huiming et al, 2000). The F-Score
is used to value the performance of the Word-
Seg, Precision to POS Tagging and the correct
rate of links to Parsing.
Table 1: The five-best results of each compo-
nent
1-best 5-best
Word-Seg 87.83% 94.45%
POS Tag 85.34% 93.28%
Parsing 80.25% 82.13%
3.2 Self Confidence Selection
In order to select a better SC, we test all kinds of
definition form to calculate their Minimal Error
Rate. For example B?A, BA and so on. A and Bdenote the first and the second score of a com-
ponent respectively. Table 2 shows the relation-
ship between definition forms of SC and their
Minimal Error Rate. Here, we experimented
with the first and the second Word-Seg results
of more than 7100 Chinese sentences.
3.3 F-Score of Word-Seg
The result of Word-Seg is used to test our
system?s performance, which means that the
ObjFun(?) returns the F-Score of Word-Seg.
There are 1,500 sentences as training data
and 500 sentences as test data. Among these
data about 10% sentences have ambiguities and
the others come from Beijing Univ. and Fujitsu
1The Parser has not been published still.
Chinese corpus (Huiming et al, 2000). In CUP
the five-best results of each component are se-
lected. Table 3 lists the F-Score of Word-Seg.
They use Only Word-Seg (OWS), Whole Layer
Weight (WLW), SC (SC) and FeedBack mecha-
nism (FB) separately. Using the feedback mech-
anism means that the last analysis result of a
sentence is decided by the Parsing. We select
the result which has the lowest score of Pars-
ing. Table 4 shows the weight distributions in
WLW and SC weighting methods.
3.4 The Efficiency of CUP
The efficiency test of CUP was done with 7112
sentences with 20 Chinese characters averagely.
It costs 58.97 seconds on a PC with PIV 2.0
CPU and 512M memory. The average cost of a
sentence is 0.0083 second.
4 Discussions
According to Table 1, we can see that the per-
formance of each component improved with the
increasing of the number of results. But at the
same time, the processing time must increase.
So we should balance the efficiency and effec-
tiveness with an appropriate number of results.
Thus, it?s more possible for CUP to find out the
correct analysis than the original cascade mech-
anism if we can invent an appropriate method.
We define SC as B ? A which gets the mini-
mal Minimal Error Rate with the analysis of the
Table 2: SC and Minimal Error Rate
Definition Form of SC Minimal
Error Rate
1
A ? 1B 23.85%B ?A 21.07%
B
A 23.98%B
A ? AB 23.98%B?A
length of a sentence 24.12%
B?A
length of a sentence+100 23.71%
Table 3: F-Score of Word-Seg
OWS WLW SC FB
F-Score 86.99% 85.80% 88.13% 80.72%
Table 4: Layer Weight
1-layer 2-layer 3-layer
In WLW 0.84 0.12 0.04
In SC 0.44 0.40 0.16
Table 2. Take the case of Word Segmentation:
B ?A =
?
i
logP (wAi )?
?
j
logP (wBj )
It?s just the difference between logarithms of
different word results? probability of the first
and the second result of Word Segmentation.
Table 3 shows that MSM using SC gets a bet-
ter performance than other methods. For a Chi-
nese sentence ???????????. (There
are some drinks under the table). The CUP
gets the correct analysis ? ???/n ?/nd ?/v
?/u ?/m ?/q ?/n ?/w?. But the cascade
and feedback mechanism?s result is ???/n ?
?/v ?/u ?/m ?/q ?/n ?/w?.
The cascade mechanism uses the Only Word-
Seg result. In this method P (??) is more
than P (?) ? P (?). At the same time, the
wrong analysis is a grammatical sentence and
is accepted by Parsing. These create that these
two mechanisms cannot get the correct result.
But the MSM synthesizes all the information of
Word-Seg, POS Tagging and Parsing. Finally
it gets the correct analysis result.
Now, CUP integrates three layers and its effi-
ciency is high enough for practical applications.
5 Conclusions and Future Work
A new Chinese NLU architecture based on Mul-
tilayer Search Mechanism (MSM) integrates al-
most all of NLU components into a uniform
multilayer treelike platform and quantifies these
components to use the search algorithm to find
out the optimal result. Thus any component
can be added into MSM conveniently. They
only need to accept an input and give several
outputs with scores. By experiments we can see
that a practical system ? CUP based on MSM
improves the performance of Word-Seg to a cer-
tain extent. And its efficiency is high enough for
most practical applications.
The cascade and the feedback mechanism are
JUST the special cases of MSM. If greedy algo-
rithm is used at each layer to expand the result
with the lowest score, MSM becomes the cas-
cade mechanism. If the weight of each layer
except the feedback point is set 0, the MSM be-
comes the feedback mechanism.
In the future we are going to add the Phrase
Analysis, WSD (Word Sense Disambiguation)
and Semantic Analysis components into CUP,
because it is impossible to analyze some sen-
tences correctly without semantic understand-
ing and the Phrase Analysis helps to en-
hance the performance of Parsing. At last,
CUP becomes a whole Chinese NLU platform
with Word-Seg, POS Tagging, Phrase Analy-
sis, Parsing, WSD and Semantic Analysis, six
components from lower layers to higher layers.
Under the framework of MSM, it becomes very
easy to add these components.
With the increasing of layers the handle speed
must decrease. So some heuristic search algo-
rithms will be used to improve the speed of
searching while enhancing the speed of each
component. Under the MSM framework, we can
do these easily.
The performance of each component should
be improved in the future. At least, it is impos-
sible for MSM to find out the correct analysis
result if there is a component which cannot give
a correct result within n-best results with a cor-
rect input. In addition, we are going to evalu-
ate the performance of each component not just
Word-Seg only.
6 Acknowledgements
We thank Liqi Gao and Zhuoran Wang provide
the Word Segmentation tool, Wei He provide
the POS Tagging tool and Jinshan Ma provide
the Parser tool for us. We acknowledge Dekang
Lin for his valuable comments on the earlier ver-
sions of this paper. This work was supported by
NSFC 60203020.
References
Shan Gao, Yan Zhang, Bo Xu, ChengQing
Zong, ZhaoBing Han, and RangShen Zhang.
2001. The research on integrated chinese
words segmentation and labeling based on tri-
gram statistic model. In Proceedings of IJCL-
2001, Tai Yuan, Shan Xi, China.
Duan Huiming, Song Jing, Xu Guowei,
Hu Guoxin, and Yu Shiwen. 2000. The de-
velopment of a large-scale tagged chinese cor-
pus and its applications. Applied Linguistics,
(2):72?77.
Ting Liu, Yan Wu, and Kaizhu Wang. 1998.
The problem and algorithm of maximal prob-
ability word segmentation. Journal of Harbin
Institute of Technology, 30(6):37?41.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
Melanie Mitchell. 1996. An Introduction to
Genetic Algorithms. The MIT Press, Cam-
bridge, Massachusetts.
Masaaki Nagata. 1994. A stochastic japanese
morphological analyzer using a forward-dp
backward-A* n-best search algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics, pages
201?207.
Stuart Russell and Peter Norvig. 1995. Artifi-
cial Intelligence: A Modern Approach. Pren-
tice Hall Series in Artificial Intelligence, En-
glewood Cliffs, NJ, USA.
Matthew Wall. 1996. GAlib: A C++ Li-
brary of Genetic Algorithms components.
http://lancet.mit.edu/ga/.
Andi Wu and Zixin Jiang. 1998. Word segmen-
tation in sentence analysis. In Proceedings
of the 1998 International Conference on Chi-
nese Information Processing, pages 169?180,
Beijing, China.
Aligning Bilingual Corpora Using Sentences Location Information* 
Li Weigang   Liu Ting   Wang Zhen   Li Sheng 
Information Retrieval Lab, Computer Science & Technology School, 
Harbin Institute of Technology 321# 
Harbin, China, 150001 
{LEE, tliu, wangzhen, lis}@ir.hit.edu.cn 
 
 
Abstract 
Large amounts of bilingual resource on the Internet 
provide us with the probability of building a large 
scale of bilingual corpus. The irregular characteris-
tics of the real texts, especially without the strictly 
aligned paragraph boundaries, bring a challenge to 
alignment technology. The traditional alignment 
methods have some difficulties in competency for 
doing this. This paper describes a new method for 
aligning real bilingual texts using sentence pair 
location information. The model was motivated by 
the observation that the location of a sentence pair 
with certain length is distributed in the whole text 
similarly. It uses (1:1) sentence beads instead of 
high frequency words as the candidate anchors. 
The method was developed and evaluated through 
many different test data. The results show that it 
can achieve good aligned performance and be ro-
bust and language independent. It can resolve the 
alignment problem on real bilingual text. 
1 Introduction 
There have been a number of papers on aligning 
parallel texts at the sentence level in the last cen-
tury, e.g., (Brown et al 1991; Gale and Church, 
1993; Simard et al 1992; Wu DeKai 1994). On 
clean inputs, such as the Canadian Hansards and 
the Hong Kang Hansards, these methods have been 
very successful.  
(Church, Kenneth W, 1993; Chen, Stanley, 1993) 
proposed some methods to resolve the problem in 
noisy bilingual texts. Cognate information between 
Indo-European languages pairs are used to align n- 
 
 
oisy texts. But these methods are limited when 
aligning the language pairs which are not in the 
same genre or have no cognate information. (Fung, 
1994) proposed a new algorithm to resolve this 
problem to some extent. The algorithm uses fre-
quency, position and recency information as fea-
tures for pattern matching. (W. Bin, 2000) adapted 
the similar idea with (Fung, 1994) to align special 
domain bilingual texts. Their algorithms need 
some high frequency word pairs as anchor points. 
When processing the texts that include less high-
frequency words, these methods will perform 
weakly and with less precision because of the scar-
city of the data problem.  
 (Haruno and Yamazaki, 1996) tried to align 
short texts without enough repeated words in struc-
turally different languages, such as English and 
Japanese. They applied the POS information of 
content words and an online dictionary to find 
matching word pairs. But this is only suitable for 
the short texts. 
The real text always includes some noisy infor-
mation. It has the following characteristics as fol-
lows: 
1) There are no strict aligned paragraph bounda-
ries in real bilingual text;  
2) Some paragraphs may be merged into a larger 
paragraph because of the translator?s individual 
idea;  
3) There are many complex translation patterns 
in real text; 
4)  There exist different styles and themes; 
5) Different genres have different inherent char-
acteristics.  
The tradition approaches to alignment fall into 
two main classes: lexical and length. All these 
methods have limitations when facing the real text 
according to the characteristics mentioned above. 
    * This research was supported by National Natural 
Science Foundation (60203020) and Science Founda-
tion of Harbin Institute of technology (hit.2002.73). 
We proposed a new alignment method based on 
the sentences location information. Its basic idea is 
that the location of a sentence pair with certain 
length is distributed in the whole text similarly. 
The local and global location information of a sen-
tence pair is fully combined together to determine 
the probability with which the sentence pair is a 
sentence bead. 
In the first of the following sections, we describe 
several concepts. The subsequent section reports 
the mathematical model of our alignment approach. 
Section 4 presents the process of anchors selection 
and algorithm implementation is shown in section 
5. The experiment results and discussion are shown 
in section 6. In the final section, we conclude with 
a discussion of future work. 
2 Several conceptions 
1) Alignment anchors: (Brown, 1991) firstly in-
troduced the concept of alignment anchors when 
he aligned Hansard corpus. He considered that the 
whole texts were divided into some small frag-
ments by these alignment anchors. Anchors are 
some aligned sentence pairs. 
2) Sentence bead:  and at the same time, (Brown, 
1991) called each aligned sentence pair a sentence 
bead. Sentence bead has some different styles, 
such as (0:1), (1:0), (1:1), (1:2), (1: more), (2:1), 
(2:2), (2: more), (more: 1), (more: 2), (more: more).  
3) Sentence pair: Any two sentences in the bilin-
gual text can construct a sentence pair. 
4) Candidate anchors: Candidate anchors are 
those that can be possible alignment anchors. In 
this paper, all (1:1) sentence beads are categorized 
as candidate anchors. 
3 Mathematical Model of Alignment 
The alignment process has two steps: the first 
step is to integrate all the origin paragraphs into 
one large paragraph. This can eliminate the prob-
lem induced by the vague paragraph boundaries. 
The second step is the alignment process. After 
alignment, the bilingual text becomes sequences of 
translated fragments. The unit of a fragment can be 
one sentence, two sentences or several sentences. 
The traditional alignment method can be used with 
the fragment with several sentences to improve the 
alignment granularity. In this paper the formal de-
scription of the alignment task was given by ex-
tending the concepts of bipartite graph and 
matching in graph theory. 
3.1 Bipartite graph 
Bipartite graph: Here, we assumed G to be an 
undirected graph, then it could be defined as G=<V, 
E>. The vertex+ set of V has two finite subsets: V1 
and V2, also V1 ? V2?V, V1?V2??. Let E be a 
collection of pairs, when e?E, then e={vi, vj}, 
where vi?V1,vj?V2. The triple G was described 
as, G=<V1, E, V2>, called bipartite graph. In a bi-
partite graph G, if each vertex of V1 is joined with 
each vertex of V2, or vice versa, here an edge 
represents a sentence pair. The collection E is the 
set of all the edges. The triple G=<V1, E, V2> is 
called complete bipartite graph. We considered 
that: |V1|?m, |V2|?n, where the parameters m and 
n are respectively the elements numbers of V1 and 
V2. The complete bipartite graph was usually ab-
breviated as Km, n (as shown in figure 1). 
 
 
 
 
 
 
 
3.2 Matching 
Matching: Assuming G?<V1, E, V2> was a bi-
partite graph. A matching of G was defined as M, a 
subset of E with the property that no two edges of 
M have a common vertex. 
3.3 Best Alignment Matching 
The procedure of alignment using sentence loca-
tion information can be seen as a special matching. 
We defined this problem as ?Best Alignment 
Matching? (BAM). 
BAM: If M=<S, EM, T> is a best alignment 
matching of G=<S, E, T>, then M must meet the 
following conditions:  
1) All the vertexes in the complete bipartite 
graph are ordered; 
2) The weight of any edges in EM d(si, tj) has: 
d(si, tj)< D (where D is alignment threshold); at the 
same time, there are no edges {sk, tr} which made 
k<i and r>j, or k>i and r<j; 
Figure 1 K3,3 complete bipartite graph 
3) If we consider: |S|=m and |T|=n, then the edge 
{sm, tn} belonged to EM; 
Best alignment matching can be attained by 
searching for the smallest weight of edge in collec-
tion E, until the weight of every edge d(si, tj) is 
equal or more than the alignment threshold D. 
Generally, the alignment threshold D is determined 
according to experience because different texts 
have different styles. 
 
 
 
 
 
 
 
 
 
If each sentence in the text S (or T) corre-
sponds with a vertex in V1(or V2), the text S or T 
can be denoted by S(s1, s2, s3,?si, ?sj, ?sm) or 
T(t1, t2, t3?ti, ?tj, ?tn). Considering the form 
merely, each element in S combined with any ele-
ment in T can create a complete bipartite graph. 
Thus the alignment task can be seen as the process 
of searching for the BAM in the complete bipartite 
graph. As shown in figure 2, the edge e = {si, tj} 
belongs to M; this means that the i-th sentence in 
text S and the j-th sentence in text T can make an 
alignment anchor. Each edge is corresponding to 
an alignment value. In order to ensure the bilingual 
texts are divided with the same fragment number, 
we default that the last sentence in the bilingual 
text is aligned. That is to say, {sm, tn} E? M was 
correct, if |S|=m and |T|=n in the BAM mathemati-
cal model.  
We stipulated the smaller the alignment value is, 
the more similar the sentence pair is to a candidate 
anchor. The smallest value of the sentence pair is 
found from the complete bipartite graph. That 
means the selected sentence pair is the most prob-
able aligned (1:1) sentence bead. Alignment proc-
ess is completed until the alignment anchors 
become saturated under alignment threshold value. 
Sentence pairs extracted from all sentence pairs 
are seen as alignment anchors. These anchors di-
vide the whole texts into short aligned fragments. 
The definition of BAM ensures that the selected 
sentence pairs cannot produce cross-alignment er-
rors, and some cases of (1: more) or (more: 1) 
alignment fragments can be attained by the frag-
ments pairs between two selected alignment an-
chors. 
4 Anchors Selection during Alignment 
All (1:1) sentence beads are extracted from dif-
ferent styles of bilingual texts. The distribution 
states that all of them are similar as presented in 
figure 3. The horizontal axis denotes the sentence 
number in Chinese text, and the vertical axis de-
notes the sentence number in English text. 
-20 0 20 40 60 80 100 120 140 160 180
-20
0
20
40
60
80
100
120
140
160
180
200
 
Se
nte
nc
e N
um
be
r in
 E
ng
lis
h T
ex
t
Sentence Number in Chinese Text
Beads
 
 
 
 
Statistical results show that more than 85% sen-
tence beads are (1:1) sentence beads in bilingual 
texts and their distributions obey an obvious law 
well. (DeKai Wu, 1994) offered that (1:1) sentence 
beads occupied 89% in English-Chinese as well. If 
we select these style sentence beads as candidate 
anchors, the alignment method will be general on 
any other language pairs. The main points of our 
alignment method using sentences location infor-
mation are: locating by the whole text, collocating 
by sentence length and checking by a bilingual 
dictionary. Location information of any sentence 
pair is used fully. Three lengths are used: are sen-
tence length, upper context length above the sen-
tence pair and nether context length below the 
sentence. All this information is considered to cal-
culate the alignment weight of each sentence pair. 
Finally, the sentence pair with high weight will be 
checked by a English-Chinese bilingual dictionary. 
In order to study the relationship between every 
sentence pair of {si, tj}, four parameters are defined: 
Whole text length ratio: P0 = Ls / Lt; 
Upper context length ratio: Pu[i, j] = Usi / Utj; 
Nether context length ratio: Pd[i, j] = Dsi / Dtj 
Sentence length ratio: Pl[i, j] = Lsi / Ltj; 
Figure 2 Sketch map of Km, n BAM under 
alignment threshold D 
t1  t2   t3  t4  t5  t6  t7      ti     tj                      tn-2   tn-1  tn  
s1  s2  s3   s4  s5   s6  s7      si   sj                    sm-2  sm-1 sm  
????
????
Figure 3 Distribution of (1:1) sentence beads 
in bilingual texts 
Where  
si    the i-th sentence of S; 
tj    the j-th sentence of T; 
Ls  the length of source language text S; 
Lt   the length of target language text T; 
Lsi  the length of si; 
Ltj  the length of tj; 
Usi  the upper context length above sentence si; 
Utj  the upper context length above sentence tj; 
Dsi  the nether context length below sentence si; 
Dtj  the nether context length below sentence tj; 
Figure 4 illustrates clearly the relationship of all 
variables.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
If si and tj can construct a (1:1) alignment anchor, 
P[i, j] must be less than the alignment threshold, 
where P[i,j] denotes the integrated alignment value 
between si and tj. We assume that the weight coef-
ficient of Pl[i, j] is 1. Only considering the form, 
Pu[i, j] and Pd[i, j] must have the same weight co-
efficient. Here the weight coefficient is set ?. We 
constructed a formal alignment function on every 
sentence pair: 
P[i,j] = 
?(Pu[i, j]-P0)? + (Pl[i, j] -P0)? +?(Pd[i, j] -P0)? 
Where, the parameter ? is the weight coefficient, 
if can adjust the weight of sentence pair length and 
the weight of context lengths well. The longer the 
text is, the more insensitive the effect of the con-
text-length is. So ??s value should increase in order 
to balance the whole proportion. The short text is 
vice versa. In this paper we define: 
?= (Ls/Lsi + Lt/Ltj)/2 
According to the definition of BAM, the smaller 
the alignment function value of P[i, j] is, the more 
the probability of sentence pair {si, tj} being a (1:1) 
sentence bead is. In this paper, we adopt a greedy 
algorithm to select alignment anchors according to 
all the alignment function values of P[i, j] which 
are less than the alignment threshold. This proce-
dure can be implemented with a time complexity 
of O(m*n).  
To obtain further improvement in alignment ac-
curacy requires calculation of the similarity of the 
sentence pairs. An English-Chinese bilingual dic-
tionary is adopted to calculate the semantic simi-
larity between the two sentences in a sentence pair. 
The similarity formula based on a bilingual dic-
tionary is followed: 
 
 
 
Where L| | is the bytes number of all elements, 
Match (T) is (according to English-Chinese dic-
tionary) the English words which have Chinese 
translation in the Chinese sentence, Match (S) is 
the matched Chinese fragments. 
According to the above dictionary check, align-
ment precision is improved greatly. We take a sta-
tistic on all the errors and find that most errors are 
partial alignment errors. Partial alignment means 
that the alignment location is correct, but a half 
pair of the alignment pairs is not integrated. It is 
very difficult to avoid these errors when only tak-
ing into account the sentence location and length 
information. Thus in order to reduce this kind of 
error, we check the semantic similarity of the con-
text-adjacent sentence pairs also. Because these 
pairs could be other alignment patterns, such as 
(1:2) or (2:1), the similarity formulas have some 
difference from the (1:1) sentence pair formula. 
Here, a simple judgement is performed. It is  
shown as: 
If?Lsi-1 * P0 > Ltj-1? 
 
   
 
else 
 
   
 
Here, those alignment anchors whose similari-
ties exceed the similarity threshold based on the 
bilingual dictionary will become the final align-
ment anchors. These final anchors divide the whole 
bilingual texts into aligned fragments. 
Figure 4 Sketch map of variables relationship 
si tjLsi Ltj
Usi Utj
Dsi Dtj
Ls Lt
Chinese Text English Text
s1 t1
tnsm
| ( ) | | ( ) |
| | | |
L Match S L Match TH L S L T
+= +
adjacent
adjacent *)01(
|)(||)(|
LsP
TMatchLSMatchLH +
+=
adjacent
adjacent *)0/11(
|)(||)(|
LtP
TMatchLSMatchLH +
+=
5 Algorithm Implementation 
According to the definition of BAM, the first se-
lected anchor will divide the whole bilingual texts 
into two parts.  We stipulated that the sentences in 
the upper part of source text cannot match any sen-
tence in the nether part of target text. As shown in 
Fig 5, after the first alignment anchors were se-
lected, the second candidate anchors must be se-
lected in the first quadrant or the third quadrant 
and exclusive from the boundary. It is obvious that 
if the candidate anchors exist in the second quad-
rant or fourth quadrant, the cross alignment will 
happen. For example, if the (i, j) is the first se-
lected alignment anchor, and the (i-1, j+1) is the 
second selected alignment anchor, the cross align-
ment appears. We can limit the anchors selection 
field to prevent the cross-alignment errors. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In addition, in order to resolve the problem that 
the first sentence pair is not a (1:1) sentence bead, 
we use a virtual sentence length as the origin 
alignment sentence bead when we initialize the 
alignment process. 
The implementation of alignment algorithm is 
described as followed: 
1) Load the bilingual text and English-Chinese 
dictionary; 
2) Identify the English and Chinese sentences 
boundaries and number each sentence; 
3) Default the last sentence pair to be aligned 
and calculate every sentence pair?s alignment value; 
4) Search for sentence pair that is corresponding 
to the smallest alignment function value; 
5) If the smallest alignment function value is 
less than the alignment threshold and the go to step 
6), and if the smallest value is equal to or more 
than the threshold, then go to step 7); 
6) If the similarity of the sentence pair is more 
than a certain threshold, the sentence pair will be-
come an alignment anchor and divide the bilingual 
text into two parts respectively, then limit the 
search field of the next candidate anchors and go to 
the step 4) 
7) Output the aligned texts, and go to the end. 
6 Results and Discussion 
We use the real bilingual texts of the seven-
teenth chapter in the literary masterpiece ?Wuther-
ing Heights? as our test data. The basic 
information of the data is shown in the table 1. 
 English text size 38.1K 
Chinese text size  25.1K 
English sentence number 273 
Chinese sentence number 277 
Table 1 Basic information of the test data  
In order to verify the validity of our algorithm, 
we implement the classic length-based sentence 
alignment method using dynamic programming. 
The precision is defined:  
Precision = The correct aligned sentence pairs / 
All alignment sentence pairs in bilingual texts 
The comparison results are presented in table 2.  
Method Precision (%) 
Length-based 
alignment method  20.3 
Location-based 
alignment method 87.8 
Table 2 Comparison results between two methods 
Because the origin bilingual texts have no obvi-
ous aligned paragraph boundaries, the error exten-
sion phenomena happen easily in the length-based 
alignment method if the paragraphs are not strictly 
aligned correctly. Its alignment results are so 
weaker that it cannot be used. If we omit all of the 
origin paragraphs information, we merge all the 
paragraphs in the bilingual text into one larger 
paragraph respectively. The length-based align-
ment method rated the precision of 25.4%. This is 
mainly because the English and Chinese languages 
don?t belong to the same genre and have large dif-
ference between the language pairs. But our 
Figure 5 Anchors selection in Bilingual Texts 
j+1
second 
quadrant
first 
quadrant
third 
quadrant
fourth 
quadrant
0 m
n
i
j
i-1
method rated 129 (1:1) sentence pairs as alignment 
anchors which divide the bilingual text into aligned 
fragments. The length-based classic method was 
applied to these aligned fragments and got a high 
precision. Fig 6 shows 129 selected anchors distri-
bution which is in the same trend with all the (1:1) 
sentence beads. Their only difference is the sparse 
extent of the aligned pairs. 
0 50 100 150 200 250 300
0
50
100
150
200
250
300
 
En
gli
sh
 S
en
ten
ce
 N
um
be
r
Chinese Sentence Number
 A
 
Figure 6 Distribution of alignment anchors 
In order to evaluate the adaptability of our 
method, we select texts with different themes and 
styles as the test set. We merge two news bilingual 
texts and two novel texts. The data information is 
show in Table 3. 
Our method is applied on the fixed data and re-
ceives the precision rating of 86.9%. The result 
shows that this alignment method is theme inde-
pendent.  
English text size 63.9K  
Chinese text size  41.5K  
English sentence number 510 
Chinese sentence number 526 
 Table 3 Basic information of the fixed test data 
 (Haruno and Yamazaki, 1996) tried to align 
short texts in structurally different languages, such 
as English and Japanese. In this paper the aligned 
language pairs of English and Chinese belongs to 
structurally different languages as well. Our 
method gets the highest precision in aligning short 
texts. A bilingual news text is selected to be test 
data. The result is shown in table 4. There are two 
aligned sentence error pairs which are induced by 
the lack the corresponding translation.  
English text size 5.6K  
Chinese text size  3.4K  
English sentence number 40 
Chinese sentence number 38 
Precision (%) 94.4 
Table 4 Alignment results of short test data 
It is difficult to attain large test set because do-
ing so need more manual work. We construct the 
test set by merging the aligned sentence pairs in 
the existing sentence aligned bilingual corpus into 
two files. Then the two translated files can be as 
test set. Here we merge 2000 aligned sentence 
pairs. The file information is as follows:  
English text size 200.3K  
Chinese text size  144.2K  
English sentence number 2069 
Chinese sentence number 2033 
Table 5 Basic information of the large test data 
From the table 4, it is evident that there are 
many different styles of sentence beads. The 
method is developed on this large test set and gets 
the precision of 90.5%. The reason of the slight 
precision increase is that the last test set is rela-
tively clean and the sentence length distribution 
relatively average. But overall, our method per-
forms very well to align the real bilingual texts. It 
shows the high robustness and is not related to the 
languages, text themes, text length. This method 
can resolve the alignment problem of the real text. 
7 Conclusion 
This paper proposed a new method for fully 
aligning real bilingual texts using sentence location 
information, described concretely in section 3 and 
4. The model was motivated by the observation 
that the location of a sentence pair with certain 
length is distributed in the whole text similarly. It 
uses the (1:1) sentence beads instead of the high 
frequency words as the candidate anchors. Local 
and global location characteristics of sentence pairs 
are involved to determine the probability which the 
sentence pair is an alignment anchors. 
Every sentence pair corresponds to an alignment 
value which is calculated according to the formal 
alignment function. Then the process of BAM is 
performed to get the alignment anchors. This 
alignment method can restrain the errors extension 
effectively in comparison to the traditional align-
ment method. Furthermore, it has shown strong 
robustness, even if when it meets ill-quality texts 
that include incorrect sentences. To obtain further 
improvement in alignment accuracy sentence simi-
larity based on an English-Chinese dictionary was 
performed. It need not segment the Chinese sen-
tence. The whole procedure requires little cost to 
implement. 
Additionally, we can adjust the alignment and 
similarity thresholds dynamically to get high preci-
sion alignment anchors, for example, applying the 
first test set, even if we get only 105 (1:1) sentence 
beads but the precision is 100%. We found that this 
method can perform the function of paragraph 
alignment very well and ensure simultaneous the 
alignment precision.  
Of these pairs about half of total number of (1:1) 
sentence beads can be even extracted from the bi-
lingual text directly to build a large scale bilingual 
corpus if the original bilingual text is abundant. 
And the rest bilingual text can be used as spare 
resource. Now, we have obtained about 500,000 
English-Chinese aligned sentence pairs with high 
quality. 
In the future, we hope to do further alignment on 
the basis of current work and extend the method to 
align other language pairs. 
References 
Wu, DeKai. 1994. Aligning a parallel English-
Chinese corpus statistically with lexical criteria. 
In Proceedings of the 32nd Annual Conference 
of the Association for Computational Linguistics, 
80--87, Las Cruces, New Mexico 
Simard, M., Foster, G., and Isabelle, P. 1992. Us-
ing Cognates to Align Sentences in Bilingual 
Corpora.Fourth International Conference on 
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-92), Montreal, Canada 
Brown, P., Lai, J. and Mercer, R. 1991. Aligning 
Sentences in Parallel Corpora. ACL-91 
Fung Pascale and Kathleen Mckeown. 1994. Align-
ing noisy parallel corpora across language 
groups: Word pair feature matching by dynamic 
time warping. In AMTA-94, Association for 
Machine Translation in the Americas, 81--88, 
Columbia, Maryland 
Wang Bin, Liu Qin, Zhang Xiang. 2000. Auto-
matic Chinese-English Paragraph Segmentation 
and Alignment. Journal of Software, 
11(11):1547-1553 (Chinese) 
Church, Kenneth W. 1993. Char_align: A Pro-
gram for Aligning Parallel Texts at the Charac-
ter Level. Proceedings of ACL-93, Columbus 
OH  
Chen, Stanley. 1993. Aligning Sentences in Bilin-
gual Corpora Using Lexical Information. In Pro-
ceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics 
(ACL-1993) 
Gale, W.A. Church, K.W. 1993. A Program for 
Aligning Sentences in Bilingual Corpora. Com-
putational Linguistics, 19(2): 75-102 
Haruno, Masahiko & Takefumi Yamazaki (1996), 
High-performance bilingual text alignment using 
statistical and dictionary information, In Pro-
ceedings of ACL '96, Santa Cruz, California, 
USA, pp. 131-138 
M. Kay & M. Roscheisen. 1993. Text-Translation 
Alignment. Computational Linguistics 19:1 
 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 189?192, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Lableing System using Maximum Entropy Classier ?
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu and Huaijun Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology
China, 150001
{tliu, car, ls, yxhu, hjliu}@ir.hit.edu.cn
Abstract
A maximum entropy classifier is used in
our semantic role labeling system, which
takes syntactic constituents as the labeling
units. The maximum entropy classifier is
trained to identify and classify the predi-
cates? semantic arguments together. Only
the constituents with the largest probabil-
ity among embedding ones are kept. Af-
ter predicting all arguments which have
matching constituents in full parsing trees,
a simple rule-based post-processing is ap-
plied to correct the arguments which have
no matching constituents in these trees.
Some useful features and their combina-
tions are evaluated.
1 Introduction
The semantic role labeling (SRL) is to assign syn-
tactic constituents with semantic roles (arguments)
of predicates (most frequently verbs) in sentences.
A semantic role is the relationship that a syntactic
constituent has with a predicate. Typical semantic
arguments include Agent, Patient, Instrument, etc.
and also adjunctive arguments indicating Locative,
Temporal, Manner, Cause, etc. It can be used in
lots of natural language processing application sys-
tems in which some kind of semantic interpretation
is needed, such as question and answering, informa-
tion extraction, machine translation, paraphrasing,
and so on.
?This research was supported by National Natural Science
Foundation of China via grant 60435020
Last year, CoNLL-2004 hold a semantic role la-
beling shared task (Carreras and Ma`rquez, 2004)
to test the participant systems? performance based
on shallow syntactic parser results. In 2005, SRL
shared task is continued (Carreras and Ma`rquez,
2005), because it is a complex task and now it is
far from desired performance.
In our SRL system, we select maximum en-
tropy (Berger et al, 1996) as a classifier to im-
plement the semantic role labeling system. Dif-
ferent from the best classifier reported in litera-
tures (Pradhan et al, 2005) ? support vector ma-
chines (SVMs) (Vapnik, 1995), it is much eas-
ier for maximum entropy classifier to handle the
multi-class classification problem without additional
post-processing steps. The classifier is much faster
than training SVMs classifiers. In addition, max-
imum entropy classifier can be tuned to minimize
over-fitting by adjusting gaussian prior. Xue and
Palmer (2004; 2005) and Kwon et al (2004) have
applied the maximum entropy classifier to semantic
role labeling task successfully.
In the following sections, we will describe our
system and report our results on development and
test sets.
2 System Description
2.1 Constituent-by-Constituent
We use syntactic constituent as the unit of labeling.
However, it is impossible for each argument to find
its matching constituent in all auto parsing trees. Ac-
cording to statistics, about 10% arguments have no
matching constituents in the training set of 245,353
189
constituents. The top five arguments with no match-
ing constituents are shown in Table 1. Here, Char-
niak parser got 10.08% no matching arguments and
Collins parser got 11.89%.
Table 1: The top five arguments with no matching
constituents.
Args Cha parser Col parser Both
AM-MOD 9179 9205 9153
A1 5496 7273 3822
AM-NEG 3200 3217 3185
AM-DIS 1451 1482 1404
A0 1416 2811 925
Therefore, we can see that Charniak parser got a
better result than Collins parser in the task of SRL.
So we use the full analysis results created by Char-
niak parser as our classifier?s inputs. Assume that
we could label all AM-MOD and AM-NEG arguments
correctly with simple post processing rules, the up-
per bound of performance could achieve about 95%
recall.
At the same time, we can see that for some ar-
guments, both parsers got lots of no matchings such
as AM-MOD, AM-NEG, and so on. After analyzing
the training data, we can recognize that the perfor-
mance of these arguments can improve a lot after
using some simple post processing rules only, how-
ever other arguments? no matching are caused pri-
marily by parsing errors. The comparison between
using and not using post processing rules is shown
in Section 3.2.
Because of the high speed and no affection in the
number of classes with efficiency of maximum en-
tropy classifier, we just use one stage to label all ar-
guments of predicates. It means that the ?NULL?
tag of constituents is regarded as a class like ?ArgN?
and ?ArgM?.
2.2 Features
The following features, which we refer to as the
basic features modified lightly from Pradhan et
al. (2005), are provided in the shared task data for
each constituent.
? Predicate lemma
? Path: The syntactic path through the parse tree from the
parse constituent to the predicate.
? Phrase type
? Position: The position of the constituent with respect to
its predicate. It has two values, ?before? and ?after?,
for the predicate. For the situation of ?cover?, we use
a heuristic rule to ignore all of them because there is no
chance for them to become an argument of the predicate.
? Voice: Whether the predicate is realized as an active or
passive construction. We use a simple rule to recognize
passive voiced predicates which are labeled with part of
speech ? VBN and sequences with AUX.
? Head word stem: The stemming result of the con-
stituent?s syntactic head. A rule based stemming algo-
rithm (Porter, 1980) is used. Collins Ph.D thesis (Collins,
1999)[Appendix. A] describs some rules to identify the
head word of a constituent. Especially for prepositional
phrase (PP) constituent, the normal head words are not
very discriminative. So we use the last noun in the PP
replacing the traditional head word.
? Sub-categorization
We also use the following additional features.
? Predicate POS
? Predicate suffix: The suffix of the predicate. Here, we
use the last 3 characters as the feature.
? Named entity: The named entity?s type in the constituent
if it ends with a named entity. There are four types: LOC,
ORG, PER and MISC.
? Path length: The length of the path between a constituent
and its predicate.
? Partial path: The part of the path from the constituent
to the lowest common ancestor of the predicate and the
constituent.
? Clause layer: The number of clauses on the path between
a constituent and its predicate.
? Head word POS
? Last word stem: The stemming result of the last word of
the constituent.
? Last word POS
We also use some combinations of the above fea-
tures to build some combinational features. Lots of
combinational features which were supposed to con-
tribute the SRL task of added one by one. At the
same time, we removed ones which made the per-
formance decrease in practical experiments. At last,
we keep the following combinations:
? Position + Voice
? Path length + Clause layer
? Predicate + Path
? Path + Position + Voice
? Path + Position + Voice + Predicate
? Head word stem + Predicate
? Head word stem + Predicate + Path
? Head word stem + Phrase
? Clause layer + Position + Predicate
All of the features and their combinations are used
without feature filtering strategy.
190
2.3 Classifier
Le Zhang?s Maximum Entropy Modeling Toolkit 1,
and the L-BFGS parameter estimation algorithm
with gaussian prior smoothing (Chen and Rosenfeld,
1999) are used as the maximum entropy classifier.
We set gaussian prior to be 2 and use 1,000 itera-
tions in the toolkit to get an optimal result through
some comparative experiments.
2.4 No Embedding
The system described above might label two con-
stituents even if one embeds in another, which is not
allowed by the SRL rule. So we keep only one ar-
gument when more arguments embedding happens.
Because it is easy for maximum entropy classifier to
output each prediction?s probability, we can label the
constituent which has the largest probability among
the embedding ones.
2.5 Post Processing Stage
After labeling the arguments which are matched
with constituents exactly, we have to handle the ar-
guments, such as AM-MOD, AM-NEG and AM-DIS,
which have few matching with the constituents de-
scribed in Section 2.1. So a post processing is given
by using some simply rules:
? Tag target verb and successive particles as V.
? Tag ?not? and ?n?t? in target verb chunk as AM-NEG.
? Tag modal verbs in target verb chunk, such as words with
POS of ?MD?, ?going to?, and so on, as AM-MOD.
? Tag the words with POS of ?CC? and ?RB? at the start of
a clause which include the target verb as AM-DIS.
3 Experiments
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of
PropBank corpus. It consists of the sections from
the Wall Street Journal part of Penn Treebank. Sec-
tions 02-21 are training sets, and Section 24 is devel-
opment set. The results are evaluated for precision,
recall and F?=1 numbers using the srl-eval.pl script
provided by the shared task organizers.
3.2 Post Processing
After using post processing rules, the final F?=1 is
improved from 71.02% to 75.27%.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
3.3 Performance Curve
Because the training corpus is substantially en-
larged, this allows us to test the scalability of
learning-based SRL systems to large data set and
compute learning curves to see how many data are
necessary to train. We divide the training set, 20
sections Penn Treebank into 5 parts with 4 sections
in each part. There are about 8,000 sentences in each
part. Figure 1 shows the change of performance as
a function of training set size. When all of training
data are used, we get the best system performance as
described in Section 3.4.
Figure 1: Our SRL system performance curve (of
F?=1) effecting of the training set size.
We can see that as the training set becomes larger
and larger, so does the performance of SRL system.
However, the rate of increase slackens. So we can
say that at present state, the larger training data has
favorable effect on the improvement of SRL system
performance.
3.4 Best System Results
In all the experiments, all of the features and their
combinations described above are used in our sys-
tem. Table 2 presents our best system performance
on the development and test sets.
From the test results, we can see that our system
gets much worse performance on Brown corpus than
WSJ corpus. The reason is easy to be understood
for the dropping of automatic syntactic parser per-
formance on new corpus but WSJ corpus.
The training time on PIV 2.4G CPU and 1G Mem
machine is about 20 hours on all 20 sections, 39,832-
191
Precision Recall F?=1
Development 79.65% 71.34% 75.27
Test WSJ 80.48% 72.79% 76.44
Test Brown 71.13% 59.99% 65.09
Test WSJ+Brown 79.30% 71.08% 74.97
Test WSJ Precision Recall F?=1
Overall 80.48% 72.79% 76.44
A0 88.14% 83.61% 85.81
A1 79.62% 72.88% 76.10
A2 73.67% 65.05% 69.09
A3 76.03% 53.18% 62.59
A4 78.02% 69.61% 73.58
A5 100.00% 40.00% 57.14
AM-ADV 59.85% 48.02% 53.29
AM-CAU 68.18% 41.10% 51.28
AM-DIR 56.60% 35.29% 43.48
AM-DIS 76.32% 72.50% 74.36
AM-EXT 83.33% 46.88% 60.00
AM-LOC 65.31% 52.89% 58.45
AM-MNR 58.28% 51.16% 54.49
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 43.68% 33.04% 37.62
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.38% 66.70% 72.07
R-A0 81.70% 85.71% 83.66
R-A1 77.62% 71.15% 74.25
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 83.33% 47.62% 60.61
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 77.27% 65.38% 70.83
V 98.71% 98.71% 98.71
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
sentences training set with 1,000 iterations and more
than 1.5 million samples and 2 million features.
The predicting time is about 160 seconds on 1,346-
sentences development set.
4 Conclusions
We have described a maximum entropy classifier
is our semantic role labeling system, which takes
syntactic constituents as the labeling units. The
fast training speed of the maximum entropy clas-
sifier allows us just use one stage of arguments
identification and classification to build the system.
Some useful features and their combinations are
evaluated. Only the constituents with the largest
probability among embedding ones are kept. Af-
ter predicting all arguments which have matching
constituents in full parsing trees, a simple rule-
based post-processing is applied to correct the ar-
guments which have no matching constituents. The
constituent-based method depends much on the syn-
tactic parsing performance. The comparison be-
tween WSJ and Brown test sets results fully demon-
strates the point of view.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the conll-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL-2004, pages 89?97, Boston,
MA, USA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Pennsyl-
vania University.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. Framenet-based semantic parsing using maxi-
mum entropy models. In Proc. Coling 2004.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Vladamir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proc. EMNLP
2004.
Nianwen Xue and Martha Palmer. 2005. Automatic se-
mantic role labeling for chinese verbs. In Proc. IJCAI
2005.
192
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 211?215, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing Based on Dynamic Local Optimization
Ting Liu Jinshan Ma Huijia Zhu Sheng Li
Information Retrieval Lab
Harbin Institute of Technology
Harbin, 150001, China
{tliu,mjs,hjzhu,ls}@ir.hit.edu.cn
Abstract
This paper presents a deterministic pars-
ing algorithm for projective dependency
grammar. In a bottom-up way the al-
gorithm finds the local optimum dynam-
ically. A constraint procedure is made
to use more structure information. The
algorithm parses sentences in linear time
and labeling is integrated with the parsing.
This parser achieves 63.29% labeled at-
tachment score on the average in CoNLL-
X Shared Task.
1 Introduction
Recently, dependency grammar has gained renewed
attention in the parsing community. Good results
have been achieved in some dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al, 2004).
With the availability of many dependency treebanks
(van der Beek et al, 2002; Hajic? et al, 2004;
Bo?hmova? et al, 2003; Kromann, 2003; Dz?eroski et
al., 2006) and more other treebanks which can be
converted to dependency annotation (Brants et al,
2002; Nilsson et al, 2005; Chen et al, 2003; Kawata
and Bartels, 2000), multi-lingual dependency pars-
ing is proposed in CoNLL shared task (Buchholz et
al., 2006).
Many previous works focus on unlabeled parsing,
in which exhaustive methods are often used (Eis-
ner, 1996). Their global searching performs well
in the unlabeled dependency parsing. But with the
increase of parameters, efficiency has to be consid-
ered in labeled dependency parsing. Thus determin-
istic parsing was proposed as a robust and efficient
method in recent years. Such method breaks the
construction of dependency tree into a series of ac-
tions. A classifier is often used to choose the most
probable action to assemble the dependency tree.
(Yamada and Matsumoto, 2003) defined three ac-
tions and used a SVM classifier to choose one of
them in a bottom-up way. The algorithm in (Nivre
et al, 2004) is a blend of bottom-up and top-down
processing. Its classifier is trained by memory-based
learning.
Deterministic parsing derives an analysis without
redundancy or backtracking, and linear time can be
achieved. But when searching the local optimum in
the order of left-to-right, some wrong reduce may
prevent next analysis with more possibility. (Jin et
al., 2005) used a two-phase shift-reduce to decrease
such errors, and improved the accuracy of long dis-
tance dependencies.
In this paper a deterministic parsing based on dy-
namic local optimization is proposed. According to
the probabilities of dependency arcs, the algorithm
dynamically finds the one with the highest probabil-
ities instead of dealing with the sentence in order.
A procedure of constraint which can integrate more
structure information is made to check the rational-
ity of the reduce. Finally our results and error anal-
ysis are presented.
2 Dependency Probabilities
An example of Chinese dependency tree is showed
in Figure1. The tree can be represented as a directed
graph with nodes representing word tokens and arcs
211
Figure 1: A Chinese dependency tree
representing dependency relations. The assumption
that the arcs are independent on each other often is
made so that parsing can be handled easily. On the
other side the independence assumption will result
in the loss of information because dependencies are
interrelated on each other actually. Therefore, two
kinds of probabilities are used in our parser. One is
arc probabilities which are the possibility that two
nodes form an arc, and the other is structure proba-
bilities which are used to describe some specific syn-
tactic structures.
2.1 Arc Probabilities
A dependency arc A
i
can be expressed as a 4-tuple
A
i
= <Node
i
, Node
j
, D, R>. Node
i
and Node
j
are
nodes that constitute the directed arc. D is the direc-
tion of the arc, which can be left or right. R is rela-
tion type labeled on the arc. Under the independence
assumption that an arc depends on its two nodes we
can calculate arc probability given two nodes. In our
paper the arc probabilities are calculated as follows:
P
1
= P(R,D|CTag
i
, CTag
j
, Dist)
P
2
= P(R,D|FTag
i
, FTag
j
)
P
3
= P(R,D|CTag
i
, Word
j
)
P
4
= P(R,D|Word
i
, CTag
j
)
P
5
= P(R,D|Word
i
,CTag
i
, Word
j
,CTag
j
)
P
6
= P(R,D|CTag
i?1
, CTag
i
, CTag
j
, CTag
j+1
)
Where CTag is coarse-grained part of speech tag
and FTag is fine-grained tag. As to Word we choose
its lemma if it exists. Dist is the distance between
Node
i
and Node
j
. It is divided into four parts:
Dist = 1 if j-i = 1
Dist = 2 if j-i = 2
Dist = 3 if 3?j-i?6
Dist = 4 if j-i > 6
All the probabilities are obtained by maximum
likelihood estimation from the training data. Then
interpolation smoothing is made to get the final arc
probabilities.
2.2 Structure Probabilities
Structure information plays the critical role in syn-
tactic analysis. Nevertheless the flexibility of syn-
tactic structures and data sparseness pose obstacles
to us. Especially some structures are related to spe-
cific language and cannot be employed in multi-
lingual parsing. We have to find those language-
independent features.
In valency theory ?valence? represents the num-
ber of arguments that a verb is able to govern. In
this paper we extend the range of verbs and argu-
ments to all the words. We call the new ?valence?
Governing Degree (GD), which means the ability of
one node governing other nodes. In Figure1, the GD
of node ???? is 2 and the GDs of two other nodes
are 0. The governing degree of nodes in dependency
tree often shows directionality. For example, Chi-
nese token ??? always governs one left node. Fur-
thermore, we subdivide the GD into Left Governing
Degree (LGD) and Right Governing Degree (RGD),
which are the ability of words governing their left
children or right children. In Figure 1 the LGD and
RGD of verb ???? are both 1.
In the paper we use the probabilities of GD
over the fine-grained tags. The probabilities of
P(LDG|FTag) and P(RGD|FTag) are calculated
from training data. Then we only reserve the FTags
with large probability because their GDs are stable
and helpful to syntactic analysis. Other FTags with
small probabilities are unstable in GDs and cannot
provide efficient information for syntactic analysis.
If their probabilities are less than 0.65 they will be
ignored in our dependency parsing.
3 Dynamic local optimization
Many previous methods are based on history-based
models. Despite many obvious advantages, these
methods can be awkward to encode some constrains
within their framework (Collins, 2000). Classifiers
are good at encoding more features in the determin-
istic parsing (Yamada and Matsumoto, 2003; Nivre
et al, 2004). However, such algorithm often make
more probable dependencies be prevented by pre-
ceding errors. An example is showed in Figure 2.
Arc a is a frequent dependency and b is an arc with
more probability. Arc b will be prevented by a if the
reduce is carried out in order.
212
Figure 2: A common error in deterministic parsing
3.1 Our algorithm
Our deterministic parsing is based on dynamic local
optimization. The algorithm calculates the arc prob-
abilities of two continuous nodes, and then reduces
the most probable arc. The construction of depen-
dency tree includes four actions: Check, Reduce,
Delete, and Insert. Before a node is reduced, the
Check procedure is made to validate its correctness.
Only if the arc passes the Check procedure it can
be reduced. Otherwise the Reduce will be delayed.
Delete and Insert are then carried out to adjust the
changed arcs. The complete algorithm is depicted
as follows:
Input Sentence: S = (w
1
, w
2
,l, w
n
)
Initialize:
for i = 1 to n
R
i
= GetArcProb(w
i
,w
i+1
);
Push(R
i
) onto Stack;
Sort(Stack);
Start:
i = 0;
While Stack.empty = false
R = Stack.top+i;
if Check(R) = true
Reduce(R);
Delete(R?);
Insert(R?);
i = 0;
else
i++;
The algorithm has following advantages:
? Projectivity can be guaranteed. The node is
only reduced with its neighboring node. If a
node is reduced as a leaf it will be removed
from the sentence and doesn?t take part in next
Reduce. So no cross arc will occur.
? After n-1 pass a projective dependency tree is
complete. Algorithm is finished in linear time.
? The algorithm always reduces the node with the
Figure 3: Adjustment
highest probability if it passes the Check. No
any limitation on order thus the spread of errors
can be mitigated effectively.
? Check is an open process. Various constrains
can be encoded in this process. Structural con-
strains, partial parsed information or language-
dependent knowledge can be added.
Adjustment is illustrated in Figure 3, where ??
?? is reduced and arc R? is deleted. Then the algo-
rithm computes the arc probability of R? and inserts
it to the Stack.
3.2 Checking
The information in parsing falls into two kinds:
static and dynamic. The arc probabilities in 2.1 de-
scribe the static information which is not changed in
parsing. They are obtained from the training data in
advance. The structure probabilities in 2.2 describe
the dynamic information which varies in the process
of parsing. The use of dynamic information often
depends on what current dependency tree is.
Besides the governing degree, Check procedure
also uses another dynamic information?Sequential
Dependency. Whether current arc can be reduced is
relating to previous arc. In Figure 3 the reduce of the
arc R depends on the arc R?. If R? has been delayed
or its probability is little less than that of R, arc R
will be delayed.
If the arc doesn?t pass the Check it will be de-
layed. The delayed time ranges from 1 to Length
which is the length of sentence. If the arc is delayed
Length times it will be blocked. The Reduce will be
delayed in the following cases:
?
?
GD(Node
i
) > 0 and its probability is P. If
GD(Node
i
) = 0 and Node
i
is made as child
in the Reduce, the Node
i
will be delayed
Length*P times.
?
?
GD(Node
i
) ? m (m > 0) and its probability
is P. If GD(Node
i
) = m and Node
i
is made as
parent in the Reduce, the Node
i
will be delayed
Length*P times.
213
Figure 4: Token score with size of training data
Figure 5: Token score with sentence length
? P(R?) > ?P(R), the current arc R will be de-
layed Length*(P(R?)/P(R)) times. R? is the pre-
ceding arc and ? = 0.60.
? If arc R? is blocking, the arc R will be delayed.
?
GD is empirical value and GD is current value.
4 Experiments and analysis
Our parsing results and average results are listed
in the Table 1. It can be seen that the attachment
scores vary greatly with different languages. A gen-
eral analysis and a specific analysis are made respec-
tively in this section.
4.1 General analysis
We try to find the properties that make the differ-
ence to parsing results in multi-lingual parsing. The
properties of all the training data can be found in
(Buchholz et al, 2006). Intuitively the size of train-
ing data and average length of per sentence would
be influential on dependency parsing. The relation
of these properties and scores are showed in the Fig-
ure 4 and 5.
From the charts we cannot assuredly find the
properties that are proportional to score. Whether
Czech language with the largest size of training data
or Chinese with the shortest sentence length, don?t
achieve the best results. It seems that no any factor is
determining to parsing results but all the properties
exert influence on the dependency parsing together.
Another factor that maybe explain the difference
of scores in multi-lingual parsing is the characteris-
tics of language. For example, the number of tokens
with HEAD=0 in a sentence is not one for some lan-
guages. Table 1 shows the range of governing de-
gree of head. This statistics is somewhat different
with that from organizers because we don?t distin-
guish the scoring tokens and non-scoring tokens.
Another characteristic is the directionality of de-
pendency relations. As Table 1 showed, many rela-
tions in treebanks are bi-directional, which increases
the number of the relation actually. Furthermore, the
flexibility of some grammatical structures poses dif-
ficulties to language model. For instance, subject
can appear in both sides of the predicates in some
treebanks which tends to cause the confusion with
the object (Kromann, 2003; Afonso et al, 2002;
Civit Torruella and Mart?? Anton??n, 2002; Oflazer et
al., 2003; Atalay et al, 2003).
As to our parsing results, which are lower than all
the average results except for Danish. That can be
explained from the following aspects:
(1) Our parser uses a projective parsing algorithm
and cannot deal with the non-projective tokens,
which exist in all the languages except for Chinese.
(2) The information provided by training data is not
fully employed. Only POS and lemma are used. The
morphological and syntactic features may be helpful
to parsing.
(3) We haven?t explored syntactic structures in depth
for multi-lingual parsing and more structural fea-
tures need to be used in the Check procedure.
4.2 Specific analysis
Specifically we make error analysis to Chinese and
Turkish. In Chinese result we found many errors
occurred near the auxiliary word ???(DE). We call
the noun phrases with ??? DE Structure. The word
??? appears 355 times in the all 4970 dependencies
of the test data. In Table 2 the second row shows the
frequencies of ?DE? as the parent of dependencies.
The third row shows the frequencies while it is as
child. Its error rate is 33.1% and 43.4% in our re-
sults respectively. Furthermore, each head error will
result in more than one errors, so the errors from DE
Structures are nearly 9% in our results.
214
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu
our 50.74 75.29 58.52 77.70 59.36 68.11 70.84 71.13 57.21 65.08 63.83 41.72
ave 59.94 78.32 67.17 76.16 70.73 78.58 85.86 80.63 65.16 73.52 76.44 55.95
NH 17 1 28 4 9 1 14 1 11 1 1 5
BD 27/24 78/55 82/72 54/24 26/17 46/40 7/2 55/40 26/23 21/19 64/54 26/23
Table 1: The second and third rows are our scores and average scores. The fourth row lists the maximal
number of tokens with HEAD=0 in a sentence. The last row lists the number of relations/the number of
bi-directional relations of them (Our statistics are slightly different from that of organizers).
gold system error headerr
parent 320 354 106 106
child 355 355 154 74
Table 2: Chinese DE Structure Errors
The high error rate is due to the flexibility of DE
Structure. The children of DE can be nouns and
verbs, thus the ambiguities will occur. For example,
the sequence ?V N1 DE N2? is a common ambigu-
ious structure in Chinese. It needs to be solved with
semantic knowledge to some extent. The errors of
DE being child are mostly from noun compounds.
For example, the string ????????? results
in the error: ?DE? as the child of ????. It will be
better that noun compounds are processed specially.
Our results and average results achieve the low-
est score on Turkish. We try to find some reasons
through the following analysis. Turkish is a typi-
cal head-final language and 81.1% of dependencies
are right-headed. The monotone of directionality in-
creases the difficulties of identification. Another dif-
ficulty is the diversity of the same pair. Taking noun
and pronoun as example, which only achieve the ac-
curacy of 25% and 28% in our results, there are 14
relations in the noun-verb pairs and 11 relations in
the pronoun-verb pairs. Table 3 illustrates the distri-
bution of some common relations in the test data.
The similarity of these dependencies makes our
parser only recognize 23.3% noun-verb structures
and 21.8% pronoun-verb structures. The syntactic
or semantic knowledge maybe helpful to distinguish
these similar structures.
5 Conclusion
This paper has applied a deterministic algorithm
based on dynamic local optimization to multi-
total obj sub mod D.A L.A
Noun-V 1300 494 319 156 102 78
Pron-V 215 91 60 9 37 3
Table 3: The distribution of some common relations
lingual dependency parsing. Through the error
analysis for some languages, we think that the dif-
ference between languages is a main obstacle posed
on multi-lingual dependency parsing. Adopting
different learners according to the type of languages
may be helpful to multi-lingual dependency parsing.
Acknowledgement This work was supported
by the National Natural Science Foundation of
China under Grant No. 60435020?60575042 and
60503072.
References
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
M.X. Jin, M.Y. Kim, and J.H. Lee. 2005. Two-phase
shift-reduce deterministic dependency parser of chi-
nese. In Proc. of IJCNLP: Companion Volume includ-
ing Posters/Demos and tutorial abstracts.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of the Eighth Conf. on
Computational Natural Language Learning (CoNLL),
pages 49?56.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of the 8th Intern. Workshop on Parsing Technologies
(IWPT).
215
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT: Web based Scoring Method for English Lexical Substitution 
Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng Li 
Information Retrieval Laboratory, School of Computer Science and Technology, 
Box 321, Harbin Institute of Technology 
Harbin, P.R. China, 150001 
{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cn 
 
 
Abstract 
This paper describes the HIT system and its 
participation in SemEval-2007 English 
Lexical Substitution Task. Two main steps 
are included in our method: candidate sub-
stitute extraction and candidate scoring. In 
the first step, candidate substitutes for each 
target word in a given sentence are ex-
tracted from WordNet. In the second step, 
the extracted candidates are scored and 
ranked using a web-based scoring method. 
The substitute ranked first is selected as the 
best substitute. For the multiword subtask, 
a simple WordNet-based approach is em-
ployed. 
1 Introduction 
Lexical substitution aims to find alternative words 
that can occur in given contexts. It is important in 
many applications, such as query reformulation in 
question answering, sentence generation, and 
paraphrasing. There are two key problems in the 
lexical substitution task, the first of which is 
candidate substitute extraction. Generally speaking, 
synonyms can be regarded as candidate substitutes 
of words. However, some looser lexical 
relationships can also be considered, such as 
Hypernyms and Hyponyms defined in WordNet 
(Fellbaum, 1998). In addition, since lexical 
substitution is context dependent, some words 
which do not have similar meanings in general 
may also be substituted in some certain contexts 
(Zhao et al, 2007). As a result, finding a lexical 
knowledge base for substitute extraction is a 
challenging task. 
The other problem is candidate scoring and 
ranking according to given contexts. In the lexical 
substitution task of SemEval-2007, context is con-
strained as a sentence. The system therefore has to 
score the candidate substitutes of each target word 
using the given sentence. The following questions 
should be considered here: (1) What words in the 
given sentence are ?useful? context? (2) How to 
combine the context words and use them in rank-
ing candidate substitutes? For the first question, we 
can use all words of the sentence, words in a win-
dow, or words having syntactic relations with the 
target word. For the second question, we can re-
gard the context words as ?bag of words?, n-grams, 
or syntactic structures. 
In HIT, we extract candidate substitutes from 
WordNet, in which both synonyms and hypernyms 
are investigated (Section 3.1). After that, we score 
the candidates using a web-based scoring method 
(Section 3.2). In this method, we first select frag-
ments containing the target word from the given 
sentence. Then we construct queries by replacing 
the target word in the fragments with the candidate 
substitute. Finally, we search Google using the 
constructed queries and score each candidate based 
on the counts of retrieved snippets. 
The rest of this paper is organized as follows: 
Section 2 reviews some related work on lexical 
substitution. Section 3 describes our system, espe-
cially the web-based scoring method. Section 4 
presents the results and analysis. 
2 Related Work 
Synonyms defined in WordNet have been widely 
used in lexical substitution and expansion (Smea-
ton et al, 1994; Langkilde and Knight, 1998; Bol-
173
shakov and Gelbukh, 2004). In addition, a lot of 
methods have been proposed to automatically con-
struct thesauri of synonyms. For example, Lin 
(1998) clustered words with similar meanings by 
calculating the dependency similarity. Barzilay and 
McKeown (2001) extracted paraphrases using mul-
tiple translations of literature works. Wu and Zhou 
(2003) extracted synonyms with multiple resources, 
including a monolingual dictionary, a bilingual 
corpus, and a monolingual corpus. Besides the 
handcrafted and automatic synonym resources, the 
web has been exploited as a resource for lexical 
substitute extraction (Zhao et al, 2007). 
As for substitute scoring, various methods have 
been investigated, among which the classification 
method is the most widely used (Dagan et al, 2006; 
Kauchak and Barzilay, 2006). In detail, a binary 
classifier is trained for each candidate substitute, 
using the contexts of the substitute as features. 
Then a new contextual sentence containing the tar-
get word can be classified as 1 (the candidate is a 
correct substitute in the given sentence) or 0 (oth-
erwise). The features used in the classification are 
usually similar with that in word sense disam-
biguation (WSD), including bag of word lemmas 
in the sentence, n-grams and parts of speech (POS) 
in a window, etc. There are other models presented 
for candidate substitute scoring. Glickman et al 
(2006) proposed a Bayesian model and a Neural 
Network model, which estimate the probability of 
a word may occur in a given context. 
3 HIT System 
3.1 Candidate Substitute Extraction 
In HIT, candidate substitutes are extracted from 
WordNet. Both synonyms and hypernyms defined 
in WordNet are investigated. Let w be a target 
word, pos the specified POS of w. n the number of 
w?s synsets defined in WordNet. Then the system 
extracts w?s candidate substitutes as follows: 
z Extracts all the synonyms in each synset 
under pos1 as candidate substitutes. 
z If w has no synonym for the i-th synset 
(1?i?n), then extracts the synonyms of its 
nearest hypernym. 
z If pos is r (or a), and no candidate substi-
tute can be extracted as described above, 
                                                 
1 In this task, four kinds of POS are specified: n - noun, v - 
verb, a - adjective, r - adverb.  
then extracts candidate substitutes under the 
POS a (or r). 
3.2 Candidate Substitute Scoring 
As mentioned above, all words in the given sen-
tence can be used as contextual information in the 
scoring of candidate substitutes. However, it is ob-
vious that not all context words are really useful 
when determining a word?s substitutes. An exam-
ple can be seen from Figure 1. 
 
 
She turns eyes <head>bright</head> with 
excitement towards Fiona , still tugging on the 
string of the minitiature airship-cum-dance 
card she has just received at the door . 
Figure 1. An example of a context sentence. 
 
In the example above, words turns, eyes, with, 
and excitement are useful context words, while the 
others are not. The useless contexts may even be 
noise if they are used in the scoring. As a result, it 
is important to select context words carefully. 
In HIT, we select context words based on the 
following assumption: useful context words for 
lexical substitute are those near the target word in 
the given sentence. In other words, the words that 
are far from the target word are not taken into con-
sideration. Obviously, this assumption is not al-
ways true. However, considering only the 
neighboring words can reduce the risk of bringing 
in noise. Besides, Edmonds (1997) has also dem-
onstrated in his paper that short-distance colloca-
tions with neighboring words are more useful in 
lexical choice than long ones. 
Let w be the target word, t a candidate substitute, 
S the context sentence. Our basic idea is that: One 
can substitute w in S with t, which generates a new 
sentence S?. If S? can be found on the web, then the 
substitute is admissible. The more times S? occurs 
on the web, the more probable the substitute is. In 
practice, however, it is difficult to find a whole 
sentence S? on the web due to sparseness. Instead, 
we use fragments of S? which contains t and sev-
eral neighboring context words (based on the as-
sumption above). Then the question is how to ob-
tain one (or more) fragment of S?. 
A window with fixed size can be used here. Su-
ppose p is the position of t in S?, for instance, we 
can construct a fragment using words from posi-
tion p-r to p+r, where r is the radius of window. 
174
However, a fixed r is difficult to set, since it may 
be too large for some sentences, which makes the 
fragments too specific, while too small for some 
other sentences, which makes the fragments too 
loose. An example can be seen in Table 1. 
 
1(a) But when Daniel turned <head>blue</head> 
one time and he totally stopped breathing. 
1(b) Daniel turned t one time 
2(a) We recommend that you <head>check</head> 
with us beforehand. 
2(b) that you t with us 
Table 1. Examples of fragments with fixed size. 
 
In Table1, 1(a) and 2(a) are two sentences from 
the test data of SemEval-2007Task10. 1(b) and 2(b) 
are fragments constructed according to 1(a) and 
2(a), where the window radius is 2 and t denotes 
any candidate substitute of the target word. It is 
obvious that 1(b) is a rather strict fragment, which 
makes it difficult to find sentences containing it on 
the web, while 2(b) is quite loose, which can 
hardly constrain the semantics of t. 
Having considered the problem above, we pro-
pose a rule-based method that constructs fragments 
with varied lengths. Let Ft be a fragment contain-
ing t, the construction rules are as follows: 
Rule-1: Ft must contain at least two words be-
sides t, at least one of which is non-stop word. 
Rule-2: Ft does not cross sub-sentence boundary 
(?,?). 
Rule-3: Ft should be the shortest fragment that 
satisfies Rule-1 and Rule-2. 
According to the rules above, we construct at 
most three fragments for each S?: (1) t occurs at the 
beginning of Ft, (2) t occurs in the middle of Ft, 
and (3) t occurs at the end of Ft. Here we have an-
other constraint: if one constructed fragment F1 is 
the substring of F2, then F2 is removed. Please 
note that the morphology is not taken into account 
when we construct queries. 
For the sentence 1(a) and 2(a) in Table 1, the 
constructed fragments are as follows: 
 
For 1(a): Daniel turned t; t one time; turned t 
one 
For 2(a): recommend that you t; t with us be-
forehand 
Table 2. Examples of the constructed fragments 
 
To score a candidate substitute, we replace ?t? in 
the fragments with each candidate substitute and 
use them as queries, which are then fed to Google. 
The score of t is computed according to the counts 
of retrieved snippets: 
?
=
=
n
i
tWebMining iFSnippetcountn
tScore
1
))((
1
)(     (1) 
where n is the number of constructed fragments, 
Fti is the i-th fragment (query) corresponding to t, 
and count(Snippet(Fti)) is the count of snippets 
retrieved by Fti. 
All candidate substitutes with scores larger than 
0 are ranked and the first 10 substitutes are re-
tained for the oot subtask. If the number of candi-
dates whose scores are larger than 0 is less than 10, 
the system ranks the rest of the candidates by their 
frequencies using a word frequency list. The spare 
capacity is filled with those candidates with largest 
frequencies. For the best subtask, we simply output 
the substitute that ranks first in oot. 
3.3 Detection of Multiwords 
The method used to detect multiword in the HIT 
system is quite similar to that employed in the 
baseline system. We also use WordNet to detect if 
a multiword that includes the target word occurs 
within a window of 2 words before and 2 words 
after the target word.  
A difference from the baseline system lies in 
that our system looks up WordNet using longer 
multiword candidates first. If a longer one is found 
in WordNet, then its substrings will be ignored. 
For example, if we find ?get alng with? in Word-
Net, we will output it as a multiword and will not 
check ?get alng? any more. 
4 Results 
Our system is the only one that participates all the 
three subtasks of Task10, i.e., best, oot, and mw. 
The evaluation results of our system can be found 
in Table 3 to Table 5. Our system ranks the fourth 
in the best subtask and seventh in the oot subtask. 
We have analyzed the results from two aspects, 
i.e., the ability of the system to extract candidate 
substitutes and the ability to rank the correct sub-
stitutes in front. There are a total of 6,873 manual 
substitutes for all the 1,710 items in the gold stan-
dard, only 2,168 (31.54%) of which have been ex-
tracted as candidate substitutes by our system. This 
result suggests that WordNet is not an appropriate 
175
source for lexical substitute extraction. In the fu-
ture work, we will try some other lexical resources, 
such as the Oxford American Writer Thesaurus 
and Encarta. In addition, we will also try the 
method that automatically constructs lexical re-
sources, such as the automatic clustering method. 
Further analysis shows that, 1,388 (64.02%) out 
of the 2,168 extracted correct candidates are 
ranked in the first 10 in the oot output of our sys-
tem. This suggests that there is a big space for our 
system to improve the candidate scoring method. 
In the future work, we will consider more and 
richer features, such as the syntactic features, in 
candidate substitute scoring. Furthermore, A dis-
advantage of this method is that the web mining 
process is quite inefficient. Therefore, we will try 
to use the Web 1T 5-gram Version 1 from Google 
(LDC2006T13) in the future. 
 
 P R ModeP ModeR
OVERALL 11.35 11.35 18.86 18.86 
Further Analysis 
NMWT 11.97 11.97 19.81 19.81 
NMWS 12.55 12.38 19.93 19.65 
RAND 11.81 11.81 20.03 20.03 
MAN 10.81 10.81 17.53 17.53 
Baselines 
WORDNET 9.95 9.95 15.58 15.58 
LIN 8.84 8.53 14.69 14.23 
Table 3. best results. 
 
 P R ModeP ModeR
OVERALL 33.88 33.88 46.91 46.91 
Further Analysis 
NMWT 35.60 35.60 48.48 48.48 
NMWS 36.63 36.63 49.33 49.33 
RAND 33.95 33.95 47.25 47.25 
MAN 33.81 33.81 46.53 46.53 
Baselines 
WORDNET 29.70 29.35 40.57 40.57 
LIN 27.70 26.72 40.47 39.19 
Table 4. oot results. 
 
 Our System WordNet BL 
 P R P R 
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 5. mw results. 
 
Acknowledgements 
This research was supported by National Natural 
Science Foundation of China (60575042, 
60503072, 60675034). 
References 
Barzilay Regina and McKeown Kathleen R. 2001. Ex-
tracting paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL/EACL. 
Bolshakov Igor A. and Gelbukh Alexander. 2004. Syn-
onymous Paraphrasing Using WordNet and Internet. 
In Proceedings of NLDB. 
Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-
shtein Efrat, Strapparava Carlo. 2006. Direct Word 
Sense Matching for Lexical Substitution. In Proceed-
ings of ACL. 
Edmonds Philip. 1997. Choosing the Word Most Typi-
cal in Context Using a Lexical Co-occurrence Net-
work. In Proceedings of ACL. 
Fellbaum Christiane. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Cambridge, MA. 
Glickman Oren, Dagan Ido, Keller Mikaela, Bengio 
Samy. 2006. Investigating Lexical Substitution Scor-
ing for Subtitle Generation. In Proceedings of 
CoNLL. 
Kauchak David and Barzilay Regina. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of 
HLT-NAACL. 
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of the COLING-ACL. 
Lin Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING-ACL. 
Smeaton Alan F., Kelledy Fergus, and O?Donell Ruari. 
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding Posting Lists, Query Expansion 
with WordNet and POS Tagging of Spanish. In Pro-
ceedings of TREC-4. 
Wu Hua and Zhou Ming. 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proceedings of IWP. 
Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and 
Zhang Yu. 2007. Automatic Acquisition of Context-
Specific Lexical Paraphrases. In Proceedings of 
IJCAI-07. 
176
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
References Extension for the Automatic Evaluation of MT by             Syntactic Hybridization   Bo Wang, Tiejun Zhao, Muyun Yang, Sheng Li School of Computer Science and Technology Harbin Institute of Technology Harbin, China {bowang,tjzhao,ymy,sl}@mtlab.hit.edu.cn       Abstract Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the au-tomatic evaluation only based on multiple manual references and their syntactic struc-tures. In our approach, the syntactic equiva-lents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long sub-segments of reference sentences. The experi-mental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments. 1 Introduction While human evaluation of machine translation output remains the most reliable method to assess translation quality, it is a costly and time consum-ing process. The development of automatic ma-chine translation evaluation metrics enables the rapid assessment of system output. By providing immediate feedback on the effectiveness of various techniques, these metrics have guided machine translation research and have facilitated rapid ad-vances in the state of the art. In addition, automatic evaluation metrics are useful in comparing the per-formance of multiple machine translation systems 
on a given translation task. Since automatic evalua-tion metrics are meant to serve as a surrogate for human judgments, their quality is determined by how well they correlate with assessors? preferences and how accurately they predicts human judg-ments. Although current methods for automatically evaluating machine translation output do not re-quire humans to assess individual system output, humans are nevertheless needed to generate a number of reference translations. The quality of machine-generated translations is determined by automatically comparing system output with these references. All current automatic evaluation met-rics are based on the various measures of the gen-eral similarity between the system translation and manual references. This kind of method has an ob-vious drawback: it does not account for combina-tions of lexical and syntactic differences that might occur between a perfectly fluent and accurately-translated machine output and a human reference translation (beyond variations already captured by the different reference translations themselves). Moreover, the set of human reference translations is unlikely to be an exhaustive inventory of ?good translations? for any given foreign language sen-tence. Therefore, it would be highly desirable to extend the coverage of the references for the simi-larity based evaluation methods. To match the system translation with various presentation of the same meaning, many work ha-ven been proposed to extend the references by generating lexical variations. The first strategy fo-cuses on the extension based on paraphrase identi-
37
fication (Lepage and Denoual, 2005; Lassner et al 2005; Zhou et al 2006; Kauchak and Barzilay, 2006; Owczarzak et al 2006; Owczarzak et al 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of sub-strings they may share. In short, all paraphrases of human-generated references should be considered ?good? translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al 2005). This is an alter-nation to obtain lexical variations with synonymy dictionaries instead of the paraphrase. In this kind of method, the reference is matched against to the system translation with the pack of the synonymies of the reference words instead of the exact match-ing. Both two strategies can successfully capture the lexical variations and greatly extend the coverage of the references. But they still have two common deficiencies. The first is the demand of the external knowledge. Paraphrase based method need a mass of external corpus to extract paraphrases and syn-onymy based method need manually constructed semantic dictionaries. These demands seriously limit the application on various languages for which the external knowledge is absent. Another deficiency is that the two strategies cannot capture the equivalents of long sub-segments such as a clause. Synonymy based me-thod can only capture the equivalents of single words. Paraphrase based method can capture the equivalents of longer units but the length is still very narrow. In many cases, some long sub-segments can be varied with an entirely different presentation which cannot be decomposed into the variations of words or phrases. To address these problems we propose a novel strategy to generate variations presentation only using existing multiple manual references without any external knowledge. We identify the syntactic components on different level as the replaceable units and determine the syntactic equivalents of the components in the corresponding references. Then the equivalents of the syntactic components are hybridized into new references. The rest of the paper is organized as follows. Section 2 introduces the concept and identification of the syntactic equivalents. Section 3 proposes a process to hybridize the syntactic equivalents effi-
ciently. Experimental results are illustrated in sec-tion 4. We also include some related discussion in Section 5. Finally this work is concluded in Sec-tion 6. 2 Syntactic Equivalents  In our approach, we propose a novel method to obtain the equivalents of the sub-segments from the corresponding references to a single source sentence. A sub-segment can be a word, a phrase or longer unit such as a clause. As we know, the variations of the sentences to the same meaning can be distinguished into two categories. The first is the structural variations. In this case, presenta-tions employ the same words but arrange them in different structure. The second is lexical variations. In this case, presentations have the same structure but employ the different words. In practice, one reference sentence often has both of the two kinds of variations comparing with other corresponding reference sentences. As the previous works, we also focus on the lexical variations. The approach is that the equiva-lents of the words are not obtained by external knowledge. In our strategy, generally speaking, the equivalents of a sub-segment S in a reference sen-tence are identified as the sub-segments which play the same syntactic role in the same structure in the other corresponding references. The equivalents obtained in this way are called syntactic equiva-lents.  Suppose R1 and R2 is a corresponding reference sentence pair. T1 and T2 are the consecutive syntac-tic trees of R1 and R2 respectively. We formally define a syntactic equivalent pair between R1 and R2 with a 4-tuple:  <N1, N2, S1, S2>  where Ni is a non-terminal node in Ti and Si is the sub-segment which is covered by Ni. Then, all the syntactic equivalent pair R1 and R2 can be recur-sively identified using following process:  ?  The first syntactic equivalent pair <N1, N2, S1, S2> is identified where Ni is the root of Ti and Si= Ri. ?  Suppose <N1, N2, S1, S2> is a syntactic equivalent pair. {N11, N12, ?N1m} and { N21, N22, ?N2n} are the child nodes sequences of 
38
N1 and N2 respectively. If n=m and N1i= N2i (i.e. the child nodes sequence of N1 and N2 are exactly the same), for each node pair N1i and N2i a syntactic equivalent pair is identi-fied as < N1i, N2i, S1i, S2i>.  With this process, all equivalent pairs on differ-ent syntactic level can be identified by synchro-nously traveling the two trees from top to bottom. The following is an example of the identification of the equivalent pairs. Figure 1 gives out a refer-ence sentence pair and their syntactic trees. The nodes which are included in certain equivalent pair are surrounded by a rectangle.   (a)   (b)  Figure 1 An example of the identification of the syn-tactic equivalent pairs.    In this example, five equivalent pairs can be identified:  ?  <S, S, ?Machine translation develops con-stantly?, ?MT progresses persistently?> ?  <NP, NP, ?Machine translation?, ?MT?> ?  <VP, VP, ?develops constantly?, ?progresses persistently?> ?  <VV, VV, ?develops?, ?progresses?> ?  <ADV, ADV, ?constantly?, ?persistently?> 3 Hybridization of Syntactic Equivalents  The indentified syntactic equivalents pairs include the sub-segments which sharing the same role in the same syntactic structure. Because of this, we 
can obtain a variation of a reference sentence by switching the two sub-segments of an equivalent pair in this sentence. This operation did not change the structure of the sentence but only replace a sub-segment in the structure with its equivalent.  Consequently, two new references can be gener-ated by switching the two sub-segments of an equivalent pair between two reference sentences. Furthermore when we switch the sub-segments of all equivalent pairs between the two references, multiple new references are generated with various combinations of the switches. This operation is called the syntactic hybridization of the references which can be illustrated by following steps: Suppose R={ri}i=1?n is a reference set containing n reference sentences to a single source sentence. R? is the new reference set containing the original reference sentences and the hybridized reference sentences. R? can be obtained by formula (1):    where rooti is the root node of the syntactic tree of ri. Equ(nt) returns the set of all equivalent of the sub-segments covered by the tree node nt. The de-tailed process of Equ(nt) is:  Equ(nt):  Define set  equ = ? Add Seg(nt) to equ If nt is included in an equivalent pair <nt, nt?, s, s?> Add p? to equ Define childi=1?m is the m children of nt Define hybr = Equ(child1)?Equ(child2)??Equ(childm) Merge hybr into equ Return equ  where Seg(nt) is the sub-segment covered by the tree node nt. Operation S1? S2 generates the Carte-sian product of the sub-segment set S1 and S2, i.e. for each arbitrary sub-segment pair s1 and s2 se-lected from S1 and S respectively, we concatenate s1 and s2. Finally, the reduplicate references in R? are removed.   For the example in Section 2, eight hybridized references can be generated including the original two sentences:  
39
?  Machine Translation develops constantly ?  Machine Translation develops persistently ?  Machine Translation progresses constantly ?  Machine Translation progresses persistently  ?  MT develops constantly ?  MT develops persistently ?  MT progresses constantly ?  MT progresses persistently 4 Experiments  We will show experimental results in this section to verify the effectiveness of the extended set of hybridized reference sentences. In the experiments, multiple translations of the source language sen-tences are evaluated with several popular auto-matic evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson?s correlations between the human assessments and evaluation scores using two reference set are calculated and compared.   The multiple translations and human assess-ments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics in-clude BLEU (Papieni et al, 2002), GTM (Me-lamed et al, 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are ob-tained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43.  Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-bigram with a window of nine words. And METEOR is run in ?exact? mode.   Release Year 2006 Genre Newswire Number of segments 919 Source Language Chinese 
Target Language English Number of system transla-tions 7 Number of reference trans-lations 4 Human assessment scores Score 1-5, ade-quacy & fluency Table 1 Description of LDC2006T04  Release Year 2008 Genre Newswire Number of segments 249 Source Language Arabic Target Language English Number of system transla-tions 8 Number of reference trans-lations 4 Human assessment scores Score 1-7, ade-quacy  Table 2 Description of LDC2008E43  After the hybridization, each source sentence in LDC2006T04 has 31 corresponding reference sen-tences in average and each source sentence in LDC2008E43 has 66 corresponding reference sen-tences in average. The number of the references is greatly increased. And as shown in the results, the usage of the extended reference set improves the correlations with human assessments for all the metrics in most cases except the ROUGE on LDC 2008E43.  Metric Original Extended BLEU 0.3488 0.3564 GTM 0.3671 0.3681 ROUGE 0.4252 0.4325 METEOR 0.4686         0.4723 Table 3 Pearson?s correlations with human assess-ments on sentence level on LDC2006T04  Metric Original Extended BLEU 0.6092 0.6109 GTM 0.5434 0.5438 ROUGE 0.6628 0.6582 METEOR 0.7053         0.7089 Table 4 Pearson?s correlations with human assess-ments on sentence level on LDC2008E43  The following is a real instance in the experi-ments from LDC2008E43:  Four original references: 
40
 ?  Ten churches burned down in 10 days in the American state of Alabama ?  Burning of ten churches in ten days in the American state of Alabama ?  Ten churches set on fire in ten days in American state of Alabama ?  Torching of ten churches within ten days in American state of Alabama  Six additional references:  ?  Torching of ten churches in ten days in the American state of Alabama ?  Torching of ten churches within ten days in the American state of Alabama ?  Torching of ten churches in ten days in American state of Alabama ?  Burning of ten churches within ten days in American state of Alabama ?  Burning of ten churches within ten days in the American state of Alabama ?  Burning of ten churches in ten days in American state of Alabama  The syntactic structure of the original references:  ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VBN Burned) (PP (IN Down) (PP (IN in) (NP (NPB (CD 10) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Ala-bama))))))))))) ?  (TOP (NP (NPB (NN Burning)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Alabama))))))))))) ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VB Set) (PP (IN on) (NPB (NN Fire))) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))) ?  (TOP (NP (NPB (NNP Torching)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN within) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-
can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))))  To investigate the distribution of the equivalents we also perform several statistics about the count and the length of the syntactic nodes. In table 5, we list the information about the count of the nodes. The first row is the average words count per refer-ence sentence. The second and third row is the count of all tree nodes and equivalent nodes in all references respectively. The fourth and fifth row is the average count of tree nodes and equivalent nodes per reference sentence respectively.   2006T04 2008E43 Average length of  reference 31.52 34.43 Total tree nodes 211231 62569 Total equivalent nodes 21807 10073 Average tree nodes 57.46 62.82 Average equivalent nodes 5.93 10.11 Table 5 Counts of the tree nodes and equivalent nodes in references.  We also investigate the distribution of the length (count of covered words) of the nodes. First, we count the tree nodes and equivalent nodes whose length is from 1 word to 50 words. Then we calcu-late the pro-portion of equivalent nodes and tree nodes for each length. Figure 2 and 3 illustrate the distribution of absolute count of the equivalent nodes. The X-axis is the length of the nodes and the Y-axis is the count. Figure 4 and 5 illustrate the distribution of the proportions on two datasets re-spectively. The X-axis is the length of the nodes and the Y-axis is the proportion. The investigation reveals four main messages. First, the absolute counts of the short equivalents are much more than those of long equivalents as expected. Second, the proportion of the long equivalents is greater than those of short equiva-lents, this clarify that the reason of large amount of short equivalents is the large amount of short tree nodes. Third, also from the proportion of view we can see that the new method comparably bias to the long equivalents. This happens because the method adopts a top-down survey of the tree. Forth, the multiple references in Arabic-English data seem to match each other better than the references 
41
in Chinese-English data. Arabic-English references have much more equivalents than Chinese-English data and bias to long equivalents more significant.  
 Figure 2 Distribution of absolute length of equivalent node on LDC2006T04  
  Figure 3 Distribution of absolute length of equivalent node on LDC2008E43  
  Figure 4 Distribution of length proportion of equiva-lent nodes on LDC2006T04 
 Figure 5 Distribution of length proportion of equiva-lent nodes on LDC2008E43 5 Discussion  The experimental results verify the positive effect of the hybridized reference for the automatic eval-
evaluation in most cases. Though the improvement of the correlations is not very significant it is stable across the metrics in various styles. Compared with the previous works based on pa-raphrase and synonym the new method has three important advantages. The first is that the hybrid-ized reference can switch the long span sub-segments beyond the words and phrases.  The second is that the switch can be per-formed in multiple levels, i.e. a sub-segment can not only be replaced as a single unit but also can be varied by replacing some child sub-segments of it. It?s noticeable that the multiple level switches also make it possible to present some structural varia-tions by means of the lexical variations. In hybridi-zation, we can realize some structural variation between syntactic nodes by switch their parent node instead of reordering them directly.  The third advantage is that the new method needs no external knowledge which greatly facili-tates the application. But this advantage also re-sults in the main deficiency of this approach: the hybridization references cannot adopt any novel equivalents which are absent in existing references. This deficiency can be overcome by introducing the paraphrase and synonym into the syntactic hy-bridization. It should be indicated that though the hybridiza-tion process generate many new references not all of the new references are reasonable.  In table 6 we compare the effect of hybridized references and manual references with more details on LDC2006T04. In the table, the first column is the contents of the references for each source sen-tence. ?Manual? means the manual references and the number in front of it indicates how many man-ual references are provided. ?Hybr? means the hy-bridized references generated from the manual references in front of the ?+?. The second column is the Pearson?s correlations between human as-sessments and the BLEU scores using the corre-sponding reference set. Besides the set containing 4 references the other correlations are the average of the correlations based on all possible subset con-taining certain number of references. For example correlation of ?2 Manual? is the average of the cor-relations based on 6 possible subset containing 2 references.  Reference Set Correlation 1 Manual 0.2565 
42
2 Manual 0.3057 2 Manual+ Hybr 0.3082 3 Manual 0.3316 3 Manual + Hybr 0.3369 4 Manual 0.3488 4 Manual+ Hybr 0.3564 Table 6 Pearson?s correlations based on incremental reference set  As shown in the Table 6 hybridized references can improve the correlations with human assess-ments on different sizes of manual references set. But it also indicated that though hybridization can generate a mass of novel references the new refer-ences is always not more effective than even one additional manual references. This tells us that the quality of the hybridized references still need to be further refined. Another message revealed by the table is that with the increase of the number of manual refer-ences the improvement of correlation made by ad-ditional manual references is decreasing. However, the improvement made by the hybridized is in-creasing. This happens because the number of hy-bridized references increases much faster than the number of manual references. There are still several noticeable deficiencies of this work. First, it only works when there are more than two existing references. This make it cannot be used to extend the single reference in mass bi-lingual corpus. Second, which is also the most im-portant one is that this method strongly focuses on the precision at the cost of recall. Though we have recognized many equivalents for each sentence but there are still many equivalents that share different context cannot be recognized. This will be our main future work. The last deficiency is the bias to the long equivalents. This problem is caused by the same reason with the second deficiency: this method define the equivalent with the same syntac-tic context. If two sub-nodes do not share the same parent it often have different brothers. 6 Conclusions and Future Work  In this work we present a novel method to extend the coverage of the reference set for the automatic evaluation of machine translation. The new method decomposes the existing references into sub-segments according to the syntactic structure. And then generate new reference sentences by hybridiz-
ing the equivalents of the segments which play the same syntactic role in corresponding references. In this way the new method can not only capture the equivalents of words and phrases like the other methods but also capture the equivalents of long sub-segments which are out of the capability of the other methods. Another important advantage of the new method is the no use of the external knowl-edge which greatly facilitates the application. Experimental results show that with the ex-tended reference set the state-of-the-arts automatic evaluation metrics achieve better correlation with the human assessments. In the future work, we will relax the restriction of the equivalent definition and try to recognize more equivalents. We will also introduce the para-phrase and synonyms into our method to see fur-ther improvement. Another interesting challenge is to hybridize the equivalents in the different order and present the structural variations directly. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 60773066 and 60736014, the National High Tech-nology Development 863 Program of China under Grant No. 2006AA010108. References  Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Trans-lation and/or Summarization. M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, Uni-versity of Pennsylvania. I. Dan Melamed, Ryan Green, Joseph P. Turian, 2003, Precision and recall of machine translation, In Pro-ceedings of HLT/NAACL 2003. David Kauchak, Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation, In Proceedings of the NAACL 2006. Dan Klein, Christopher Manning. 2003. Accurate Un-lexicalized Parsing. In Proceedings of the 41th Meet-ing of the ACL, pp. 423-430. Yves Lepage, Etienne Denoual. 2005. Automatic gen-eration of paraphrases to be used as translation refer-
43
ences in objective evaluation measures of ma-chine translation, In Proceedings of the IWP 2005. Karolina Owczarzak, Declan Groves, Josef Van Ge-nabith ,Andy Way. 2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation, In Pro-ceedings of the Workshop on Statistical Ma-chine Translation. Karolina Owczarzak, Josef Van Genabith, Andy Way. 2007. Dependency-Based Automatic Evaluation for Machine Translation, In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation. Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-tion of machine translation, In Proceedings of the 40th Meeting of the ACL. Grazia Russo-Lassner, Jimmy Lin, Philip Resnik. 2005. Re-evaluating Machine Translation Results with Pa-raphrase Support, Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, MD. Chin-Yew Lin, Franz Josef Och. 2004. Automatic evaluation of machine translation quality using long-est common subsequence and skip-bigram sta-tistics. In Proceedings of the 42th  Meeting of the ACL. Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-evaluating Machine Translation Results with Pa-raphrase Support, In Proceedings of the EMNLP 2006. 
44
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 45?50,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study of Translation Rule Classification for Syntax-based Statistical
Machine Translation
Hongfei Jiang, Sheng Li, Muyun Yang and Tiejun Zhao
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,lisheng,ymy,tjzhao}@mtlab.hit.edu.cn
Abstract
Recently, numerous statistical machine trans-
lation models which can utilize various kinds
of translation rules are proposed. In these
models, not only the conventional syntactic
rules but also the non-syntactic rules can be
applied. Even the pure phrase rules are in-
cludes in some of these models. Although the
better performances are reported over the con-
ventional phrase model and syntax model, the
mixture of diversified rules still leaves much
room for study. In this paper, we present a
refined rule classification system. Based on
this classification system, the rules are classi-
fied according to different standards, such as
lexicalization level and generalization. Espe-
cially, we refresh the concepts of the structure
reordering rules and the discontiguous phrase
rules. This novel classification system may
supports the SMT research community with
some helpful references.
1 Introduction
Phrase-based statistical machine translation mod-
els (Marcu and Wong, 2002; Koehn et al, 2003; Och
and Ney, 2004; Koehn, 2004; Koehn et al, 2007)
have achieved significant improvements in trans-
lation accuracy over the original IBM word-based
model. However, there are still many limitations in
phrase based models. The most frequently pointed
limitation is its inefficacy to modeling the struc-
ture reordering and the discontiguous correspond-
ing. To overcome these limitations, many syntax-
based SMT models have been proposed (Wu, 1997;
Chiang, 2007; Ding et al, 2005; Eisner, 2003; Quirk
et al, 2005; Liu et al, 2007; Zhang et al, 2007;
Zhang et al, 2008a; Zhang et al, 2008b; Gildea,
2003; Galley et al, 2004; Marcu et al, 2006; Bod,
2007). The basic motivation behind syntax-based
model is that the syntax information has the poten-
tial to model the structure reordering and discontigu-
ous corresponding by the intrinsic structural gener-
alization ability. Although remarkable progresses
have been reported, the strict syntactic constraint
(the both sides of the rules should strictly be a sub-
tree of the whole syntax parse) greatly hinders the
utilization of the non-syntactic translation equiva-
lents. To alleviate this constraint, a few works have
attempted to make full use of the non-syntactic rules
by extending their syntax-based models to more
general frameworks. For example, forest-to-string
transformation rules have been integrated into the
tree-to-string translation framework by (Liu et al,
2006; Liu et al, 2007). Zhang et al (2008a) made
it possible to utilize the non-syntactic rules and even
the phrases which are used in phrase based model
by advancing a general tree sequence to tree se-
quence framework based on the tree-to-tree model
presented in (Zhang et al, 2007). In these mod-
els, various kinds of rules can be employed. For
example, as shown in Figure 1 and Figure 2, Fig-
ure 1 shows a Chinese-to-English sentence pair with
syntax parses on both sides and the word alignments
(dotted lines). Figure 2 lists some of the rules which
can be extracted from the sentence pair in Figure 1
by the system used in (Zhang et al, 2008a). These
rules includes not only conventional syntax rules but
also the tree sequence rules (the multi-headed syn-
tax rules ). Even the phrase rules are adopted by
45
the system. Although the better performances are
reported over the conventional phrase-based model
and syntax-based model, the mixture of diversified
rules still leaves much room for study. Given such a
hybrid rule set, we must want to know what kinds of
rules can make more important contributions to the
overall system performance and what kinds of rules
are redundant compared with the others. From en-
gineering point of view, the developers may concern
about which kinds of rules should be preferred and
which kinds of rules could be discard without too
much decline in translation quality. However, one of
the precondition for the investigations of these issues
is what are the ?rule categories?? In other words,
some comprehensive rule classifications are neces-
sary to make the rule analyses feasible. The motiva-
tion of this paper is to present such a rule classifica-
tion.
2 Related Works
A few researches have made some exploratory in-
vestigations towards the effects of different rules by
classifying the translation rules into different sub-
categories (Liu et al, 2007; Zhang et al, 2008a;
DeNeefe et al, 2007). Liu et al (2007) differenti-
ated the rules in their tree-to-string model which in-
tegrated with forest1-to-string into fully lexicalized
rules, non-lexicalized rules and partial lexicalized
rules according to the lexicalization levels. As an
extension, Zhang et al (2008a) proposed two more
categories: Structure Reordering Rules (SRR) and
Discontiguous Phrase Rules (DPR). The SRR stands
for the rules which have at least two non-terminal
leaf nodes with inverted order in the source and tar-
get side. And DPR refers to the rules having at
least one non-terminal leaf node between two termi-
nal leaf nodes. (DeNeefe et al, 2007) made an illu-
minating breakdown of the different kinds of rules.
Firstly, they classify all the GHKM2 rules (Galley et
al., 2004; Galley et al, 2006) into two categories:
lexical rules and non-lexical rules. The former are
the rules whose source side has no source words.
In other words, a non-lexical rule is a purely ab-
1A ?forest? means a sub-tree sequence derived from a given
parse tree
2One reviewer asked about the acronym GHKM. We guess
it is an acronym for the authors of (Galley et al, 2004): Michel
Galley, Mark Hopkins, Kevin Knight and Daniel Marcu.
? ? ???
Figure 1: A syntax tree pair example. Dotted lines stands
for the word alignments.
stract rule. The latter is the complementary set of
the former. And then lexical rules are classified fur-
ther into phrasal rules and non-phrasal rules. The
phrasal rules refer to the rules whose source side
and the yield of the target side contain exactly one
contiguous phrase each. And the one or more non-
terminals can be placed on either side of the phrase.
In other words, each phrasal rule can be simulated
by the conjunction of two more phrase rules. (De-
Neefe et al, 2007) classifies non-phrasal rules fur-
ther into structural rules, re-ordering rules, and non-
contiguous phrase rules. However, these categories
are not explicitly defined in (DeNeefe et al, 2007)
since out of its focus. Our proposed rule classifica-
tion is inspired by these works.
3 Rules Classifications
Currently, there have been several classifications
in SMT research community. Generally, the rules
can be classified into two main groups according to
whether syntax information is involved: bilingual
phrases (Phrase) and syntax rules (Syntax). Fur-
ther, the syntax rules can be divided into three cat-
egories according to the lexicalization levels (Liu et
al., 2007; Zhang et al, 2008a):
1) Fully lexicalized (FLex): all leaf nodes in both
the source and target sides are lexicons (termi-
nals)
2) Unlexicalized (ULex): all leaf nodes in both the
46
??
? ?
??? ?
???
? ?
Figure 2: Some rules can be extracted by the system used in (Zhang et al, 2008a) from the sentence pair in Figure 1.
source and target sides are non-lexicons (non-
terminals)
3) Partially lexicalized (PLex): otherwise.
In Figure 2, R1-R3 are FLex rules, and R5-R8 are
PLex rules.
Following (Zhang et al, 2008b), a syntax rule r
can be formalized into a tuple
< ?s, ?t, AT , ANT >
, where ?s and ?t are tree sequences of source side
and target side respectively, AT is a many-to-many
correspondence set which includes the alignments
between the terminal leaf nodes from source and tar-
get side, and ANT is a one-to-one correspondence
set which includes the synchronizing relations be-
tween the non-terminal leaf nodes from source and
target side.
Then, the syntax rules can also fall into two cat-
egories according to whether equipping with gen-
eralization capability (Chiang, 2007; Zhang et al,
2008a):
1) Initial rules (Initial): all leaf nodes of this rule are
terminals.
2) Abstract rules (Abstract): otherwise, i.e. at least
one leaf node is a non-terminal.
A non-terminal leaf node in a rule is named an ab-
stract node since it has the generalization capabil-
ity. Comparing these two classifications for syntax
rules, we can find that a FLex rule is a initial rule
when ULex rules and PLex rules belong to abstract
rules.
These classifications are clear and easy for un-
derstanding. However, we argue that they need
further refinement for in-depth study. Specially,
more refined differentiations are needed for the ab-
stract rules (ULex rules and PLex rules) since they
play important roles for the characteristic capabil-
ities which are deemed to be the advantages over
the phrase-based model. For instance, the potentials
to model the structure reordering and the discon-
tiguous correspondence. The Structure Reordering
Rules (SRR) and Discontiguous Phrase Rules (DPR)
mentioned by (Zhang et al, 2008a) can be regarded
as more in-depth classification of the syntax rules.
In (Zhang et al, 2008a), they are described as fol-
lows:
Definition 1: The Structure Reordering Rule
(SRR) refers to the structure reordering rule that has
at least two non-terminal leaf nodes with inverted
order in the source and target side.
Definition 2: The Discontiguous Phrase Rule
(DPR) refers to the rule having at least one non-
terminal leaf node between two lexicalized leaf
nodes.
47
Based on these descriptions, R7, R8 in Figure 2
belong to the category of SRR and R6, R7 fall into
the category of DPR. Although these two definitions
are easy implemented in practice, we argue that the
definition of SRR is not complete. The reordering
rules involving the reordering between content word
terminals and non-terminal (such as R5 in Figure
2) also can model the useful structure reorderings.
Moreover, it is not uncommon that a rule demon-
strates the reorderings between two non-terminals
as well as the reorderings between one non-terminal
and one content word terminal. The reason for our
emphasis of content word terminal is that the re-
orderings between the non-terminals and function
word are less meaningful.
One of the theoretical problems with phrase based
SMT models is that they can not effectively model
the discontiguous translations and numerous at-
tempts have been made on this issue (Simard et al,
2005; Quirk and Menezes, 2006; Wellington et al,
2006; Bod, 2007; Zhang et al, 2007). What seems
to be lacking, however, is a explicit definition to the
discontiguous translation. The definition of DPR
in (Zhang et al, 2008a) is explicit but somewhat
rough and not very accurate. For example, in Fig-
ure 3(a), non-terminal node pair ([0,???], [0,?love?]
) is surrounded by lexical terminals. According to
Definition 2, it is a DPR. However, obviously it is
not a discontiguous phrase actually. This rule can be
simulated by conjunctions of three phrases (???, ?I?;
???, ?love?; ???,?you?). In contrast, the translation
rule in Figure 3(b) is an actual discontiguous phrase
rule. The English correspondences of the Chinese
word ??? is dispersed in the English side in which
the correspondence of Chinese word ??? is inserted.
This rule can not be simulated by any conjunctions
of the sub phrases. It must be noted that the dis-
contiguous phrase (???-?switch . . . off?) can not
be abstracted under the existing synchronous gram-
mar frameworks. The fundamental reason is that
the corresponding parts should be abstracted in the
same time and lexicalized in the same time. In other
words, the discontiguous phrase can not be modeled
by the permutation between non-terminals (abstract
nodes). Another point to notice is that our focus in
this paper is the ability demonstrated by the abstract
rules. Thus, we do not pay much attentions to the re-
orderings and discontiguous phrases involved in the
? ?? ? ?
Figure 3: Examples for demonstrating the actual discon-
tiguous phrase. (a) is a negative example for the definition
of DPR in (Zhang et al, 2008a), (b) is a actual discon-
tiguous phrase rule.
2
Figure 4: The rule classifications used in this paper. (a)
shows that the rules can be divided into phrase rules and
syntax rules according to whether a rule includes the syn-
tactic information. (b) illustrates that the syntax rules can
be classified into three kinds according to the lexicaliza-
tion level. (c) shows that the abstract rules can be classi-
fied into more refined sub-categories.
phrase rules (e.g. ?? ??-?switch the light off?)
since they lack the generalization capability. There-
fore, the discontiguous phrase is limited to the rela-
tion between non-terminals and terminals.
On the basis of the above analyses, we present
a novel classification system for the abstract rules
based on the crossings between the leaf node
alignment links. Given an abstract rule r =<
?s, ?t, AT , ANT >, it is
1) a Structure Reordering Rule (SRR), if ? a link
l ? ANT is crossed with a link l? ? {AT ?ANT }
a) a SRR NT2 rule, if the link l? ? ANT
b) a SRR NT-T rule, if the link l? ? AT
2) not a Structure Reordering Rule (N-SRR), other-
wise.
48
??
?
Figure 5: The patterns to show the characteristics of dis-
contiguous phrase rules.
Note that the intersection of SRR NT2 and SRR NT-
T is not necessary an empty set, i.e. a rule can be
both SRR NT2 and SRR NT-T rule.
The basic characteristic of the discontiguous
translation is that the correspondence of one non-
terminal NT is inserted among the correspondences
of one phrase X . Figure 5 (a) illustrates this sit-
uation. However, this characteristic can not sup-
port necessary and sufficient condition. For exam-
ple, if the phrase X can be divided like Figure 5
(b), then the rule in Figure 5 (a) is actually a re-
ordering rule rather than a discontiguous phrase rule.
For sufficient condition, we constrain that the phrase
X = wi . . . wj need to satisfy the requirement: wi
should be connected with wj through word align-
ment links (A word is connected with itself). In Fig-
ure 5(c), f1 is connected with f2 when NT ? is in-
serted between e1 and e2. Thus, the rule in Figure
5(c) is a discontiguous phrase rule.
Definition 3: Given an abstract rule r =<
?s, ?t, AT , ANT >, it is a Discontiguous Phrase iff
? two links lt1, lt2 from AT and a link lnt from ANT ,
satisfy: lt1, lt2 are emitted from the same word and
lt1 is crossed with lnt when lt2 is not crossed with
lnt.
Through Definition 3, we know that the DPR is a
sub-set of the SRR NT-T.
4 Conclusions and Future Works
In this paper, we present a refined rule classifica-
tion system. Based on this classification system, the
rules are classified according to different standards,
such as lexicalization level and generalization. Es-
pecially, we refresh the concepts of the structure re-
ordering rules and the discontiguous phrase rules.
This novel classification system may supports the
SMT research community with some helpful refer-
ences.
In the future works, aiming to analyze the rule
contributions and the redundances issues using the
presented rule classification based on some real
translation systems, we plan to implement some syn-
chronous grammar based syntax translation models
such as the one presented in (Liu et al, 2007) or
in (Zhang et al, 2008a). Taking such a system as
the experimental platform, we can perform compre-
hensive statistics about distributions of different rule
categories. What is more important, the contribu-
tion of each rule category can be evaluated seriatim.
Furthermore, which kinds of rules are preferentially
applied in the 1-best decoding can be studied. All
these investigations could reveal very useful infor-
mation for the optimization of rule extraction and the
improvement of the computational models for syn-
chronous grammar based machine translation.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
Rens Bod. 2007. Unsupervised syntax-based machine
translation: The contribution of discontiguous phrases.
In Proceedings of Machine Translation Summit XI
2007,Copenhagen, Denmark.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. In computational linguistics, 33(2).
Ding, Y. and Palmer, M. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars In Proceedings of ACL.
DeNeefe, S. and Knight, K. and Wang, W. and Marcu, D.
2007. What can syntax-based MT learn from phrase-
based MT? In Proceedings of EMNLP/CONLL.
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-HLT 2004, pages 273-280.
49
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-rich
syntactic translation models In Proceedings of ACL-
COLING
Daniel Gildea 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of ACL 2003,
pages 80-87.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL 2003, pages 127-133, Edmonton,
Canada, May.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115-124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. ACL 2007,
demonstration session, Prague, Czech Republic, June
2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation.
In Proceedings of ACL-COLING.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL 2007, pages 704-711.
Daniel Marcu and William Wong. 2002. A phrase based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language Phrases. In
Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440-447.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417-449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages 271-
279, Ann Arbor, Michigan, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proceedings of
HLT/NAACL
Simard, M. and Cancedda, N. and Cavestro, B. and
Dymetman, M. and Gaussier, E. and Goutte, C. and
Yamada, K. and Langlais, P. and Mauser, A. 2005.
Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP, volume 2, pages 901-904.
Benjamin Wellington, Sonjia Waxmonsky and I. Dan
Melamed. 2006. Empirical Lower Bounds on the
Complexity of Translational Equivalence. In Proceed-
ings of ACL-COLING 2006, pages 977-984.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. In
Proceedings of ACL 1997. Computational Linguistics,
23(3):377-403.
Min Zhang, Hongfei Jiang, Ai Ti AW, Jun Sun, Sheng
Li, and Chew Lim Tan. 2007. A tree-to-tree
alignment-based model for statistical machine trans-
lation. In Proceedings of Machine Translation Summit
XI 2007,Copenhagen, Denmark.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008a. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT
Min Zhang, Hongfei Jiang, Haizhou Li, Ai Ti AW,
and Sheng Li. 2008b. Grammar Comparison Study
for Translational Equivalence Modeling and Statistical
Machine Translation. In Proceedings of Coling
50
Coling 2010: Poster Volume, pages 701?709,
Beijing, August 2010
Reexamination on Potential for Personalization in Web Search 
Daren Li1  Muyun Yang1  Haoliang Qi2  Sheng Li1  Tiejun Zhao1 
 
1School of Computer Science 
Harbin Institute of Technology 
{drli|ymy|tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
 
2School of Computer Science 
Heilongjiang Institute of Technology 
haoliang.qi@gmail.com 
 
Abstract 
Various strategies have been proposed 
to enhance web search through utiliz-
ing individual user information. How-
ever, considering the well acknowl-
edged recurring queries and repetitive 
clicks among users, it is still an open 
issue whether using individual user in-
formation is a proper direction of ef-
forts in improving the web search. In 
this paper, we first quantitatively dem-
onstrate that individual user informa-
tion is more beneficial than common 
user information. Then we statistically 
compare the benefit of individual and 
common user information through 
Kappa statistic. Finally, we calculate 
potential for personalization to present 
an overview of what queries can bene-
fit more from individual user informa-
tion. All these analyses are conducted 
on both English AOL log and Chinese 
Sogou log, and a bilingual perspective 
statistics consistently confirms our 
findings. 
1 Introduction 
Most of traditional search engines are designed 
to return identical result to the same query 
even for different users. However, it has been 
found that majority of queries are quite ambi-
guous (Cronen-Townsend et al, 2002) as well 
as too short (Silverstein et al, 1999) to de-
scribe the exact informational needs of users. 
Different users may have completely different 
information needs under the same query (Jan-
sen et al, 2000). For example, when users is-
sue a query ?Java? to a search engine, their 
needs can be something ranging from a pro-
gramming language to a kind of coffee. 
In order to solve this problem, personalized 
search is proposed, which is a typical strategy 
of utilizing individual user information. Pitkow 
et al (2002) describe personalized search as 
the contextual computing approach which fo-
cuses on understanding the information con-
sumption patterns of each user, the various 
information foraging strategies and applica-
tions they employ, and the nature of the infor-
mation itself. After that, personalized search 
has gradually developed into one of the hot 
topics in information retrieval. As for various 
personalization models proposed recently, Dou 
et al (2007), however, reveal that they actually 
harms the results for certain queries while im-
proving others. This result based on a large-
scale experiment challenges not only the cur-
rent personalization methods but also the mo-
tivation to improve web search by the persona-
lized strategies. 
In addition, the studies on query logs rec-
orded by search engines consistently report the 
prevailing repeated query submissions by large 
number of users (Silverstein et al, 1999; Spink 
et al, 2001). It is reported that the 25 most fre-
quent queries from the AltaVista cover 1.5% 
of the total query submissions, despite being 
only 0.00000016% of unique queries (Silvers-
tein et al, 1999). As a result, the previous us-
ers? activities may serve as valuable informa-
tion, and technologies focusing on common 
701
user information, such as collaborative filter-
ing (or recommendation) may be a better reso-
lution to web search. Therefore, the justifica-
tion of utilizing individual user information 
deserves further discussion. 
To address this issue, this paper conducts a 
bilingual perspective of survey on two large-
scale query logs publically available: the AOL 
in English and the Sogou1 in Chinese. First we 
quantitatively investigate the evidences for 
exploiting common user information and indi-
vidual user information in these two logs. Af-
ter that we introduce Kappa statistic to meas-
ure the consistency of users? implicit relevance 
judgment inferred from clicks. It is tentatively 
revealed that using individual user information 
is what requires web search to face with after 
common user information is well exploited. 
Finally, we study the distribution of potential 
for personalization over the whole logs to gen-
erally disclose what kind of query deserves for 
individual user information. 
The remainder of this paper is structured as 
follows. Section 2 introduces previous me-
thods employing individual and common user 
information. In Section 3, we quantitatively 
compare the evidences for exploiting common 
user information and individual user informa-
tion. In Section 4, we introduce Kappa statistic 
to measure the consistency of users? clicks on 
the same query and try to statistically present 
the development direction of current web 
search. Section 5 figures out utilizing individu-
al user information as a research issue after 
well exploiting common user information. Sec-
tion 6 presents the potential for personalization 
curve, trying to outline which kind of queries 
benefit the most from individual user informa-
tion. Conclusions and future work are detailed 
in Section 7. 
2 Related Work 
With the rapid expansion of World Wide Web, 
it becomes more and more difficult to find re-
levant information through one-size-fits-all 
information retrieval service provided by clas-
sical search engines. Two kinds of user infor-
mation are mainly used to enhance search en-
                                                 
1 A famous Chinese search engine with a large number of 
Chinese web search users. 
gines: common user information and individu-
al user information. We separately review the 
previous works focusing on using these two 
kinds of information. 
Among various attempts to improve the per-
formance of search engine, collaborative web 
search is the one to take advantage of the repe-
tition of users? behaviors, which we call com-
mon user information. Since there is no unified 
definition on collaborative web search, in this 
paper, we believe that the collaborative web 
search assumes that community search activi-
ties can provide valuable search knowledge, 
and sharing this knowledge facilitates improv-
ing traditional search engine results (Smyth, 
2007). An important technique of collaborative 
web search is Collaborative Filtering (CF, also 
known as collaborative recommendation), in 
which, items are recommended to an active 
user based on historical co-occurrence data 
between users and items (Herlocker et al, 
1999). A number of researchers have explored 
algorithms for collaborative filtering and the 
algorithms can be categorized into two classes: 
memory-based CF and model-based CF. 
Memory-based CF methods apply a nearest-
neighbor-like scheme to predict a user?s rat-
ings based on the ratings given by like-minded 
users (Yu et al, 2004). The model-based ap-
proaches expand memory-based CF to build a 
descriptive model of group-based user prefe-
rences and use the model to predict the ratings. 
Examples of model-based approaches include 
clustering models (Kohrs et al, 1999) and as-
pect models (J. Canny, 2002). 
The other way to improve web search is per-
sonalized web search, focusing on learning the 
individual preferences instead of others? beha-
viors, which is called individual user informa-
tion. Early works learn user profiles from the 
explicit description of users to filter search re-
sults (Chirita et al, 2005). However, most of 
users are not willing to provide explicit feed-
back on search results and describe their inter-
ests (Carroll et al, 1987). Therefore, recent 
researches on the personalized search focus on 
modeling user preference from different types 
of implicit data, such as query history (Speretta 
et al, 2005), browsing history (Sugiyama et al, 
2004), clickthrough data (Sun et al, 2005), 
immediate search context (Shen et al, 2005) 
and other personal information (Teevan et al, 
702
2005). So far, there is still no proper compari-
son between the two solutions. It is still an 
open question which kind of information is 
more effective to build the web search model. 
Considering the difficulty in collecting pri-
vate information, using individual user infor-
mation seems less promising as the cost-
effective solution to web search. To address 
this issue, some researches about the value of 
personalization have been conducted. Teevan 
et al (2007) have done a ground breaking job 
to quantify the benefit for the search engines if 
search results were tailored to satisfy each user. 
The possible improvement by the personalized 
search, named potential for personalization, is 
measured by a gap between the relevance of 
individualized rankings and group ranking 
based on NDCG. However, it is less touched 
for the position of individual user information 
in contrast with common user information in 
large scale query log and how to balance the 
usage of common and individual information 
in information retrieval model. 
This paper tentatively examines individual 
user information against common user infor-
mation on two large-scale search engine logs 
in following aspects: the evidence from clicks 
on the same query, Kappa statistic for the 
whole queries, and overall distribution of que-
ries in terms of number of submissions and 
Kappa value. The bilingual statistics consis-
tently reveals the tendency of using individual 
user information as an equally important issue 
as (if not more than) using common user in-
formation) issue for researches on web search. 
3 Quantitative Evidences for Using 
Common or Individual User Infor-
mation 
To quantitatively investigate the value of 
common user information and individual user 
information in query log, we discriminate the 
evidence for using the two different types of 
user information as follows: 
(1) Evidence for using common user infor-
mation: if there were multiple users who have 
exactly the same click sets on one query, we 
suppose those clicks sets, together with the 
query, as the evidence for exploiting common 
user information. It is clear that such queries 
are able to be better responded with other?s 
search results. Note that common user infor-
mation is hard to be clearly defined, in order to 
simplify the quantitative statistics we give a 
strict definition. Further analysis will be shown 
in following sections. 
(2) Evidence for using individual user in-
formation: if a user?s click set on a query was 
not the same as any other?s, for that query, the 
search intent of the user who issue that query 
can be better inferred from his/her individual 
information than common user information. 
We suppose this kind of clicks, together with 
the related queries, as the evidence for exploit-
ing individual user information. 
Since users may have different search in-
tents when they issue the same query, a query 
can be an evidence for using both common and 
individual user information. In our statistics, if 
a query has both duplicate click sets and 
unique click set, the query is not only counted 
by the first category but also the second cate-
gory.  
The statistics of the two categories are con-
ducted in the query log of both English and 
Chinese search engines. We use a subset of 
AOL Query Log from March 1, 2006 to May 
31, 2006 and Sogou Query Log from March 1, 
2007 to March 31, 2007. The basic statistics of 
AOL and Sogou log are shown in Table 1. No-
tice that the queries in raw AOL and Sogou log 
without clicks are removed in this study. 
 
Item AOL Sogou 
#days 92 31 
#users 6,614,960 7,488,754 
#queries 7,840,348 8,019,229 
#unique queries 4,811,649 4,580,836 
#clicks 12,984,610 17,607,808 
Table 1: Basic statistics of AOL & Sogou log 
 
Table 2 summarizes the statistics of differ-
ent evidence categories over AOL and Sogou 
log. Note that click set refers to the set of 
clicks related to a query submission instead of 
a unique query. As for evidence for using 
common and individual user information, there 
is no clear distinction in terms of number of 
records, number of users in two logs. However, 
in terms of unique query and distinct click set, 
one can?t fail to find that evidence for using 
individual user information clearly exceeds  
703
Log 
The Condition Number 
Repeated queries Click Records User Unique Query 
Distinct 
Click Set 
AOL 
3,745,088 
(47.77% of total 
query submissions) 
Same 2,438,284 277,416 382,267 461,460 
Different 2,563,245 343,846 542,593 1,349,892 
Sogou 
4,252,167 
(53.02% of total 
query submissions) 
Same 2,469,363 1,380,951 228,315 358,346 
Different 5,481,832 1,545,817 752,047 2,171,872 
 
Table 2: Different click behaviors on repeated queries 
that for using common user information, espe-
cially in Sogou log. Therefore, though making 
use of common and individual user informa-
tion can address equally well for half users and 
half visits to the search engine, the fact that  
much more unique queries and click sets ac-
tually claims the significance of needing indi-
vidual user information to personalize web 
results. And methods exploiting individual us-
er information provide a much more challeng-
ing task in terms of problem space, though one 
may argue utilizing common user information 
is much easier to attack. 
4 Kappa Statistics for Individual and 
Common user information 
Section 3 has shown the evidence for using 
individual user information is prevailing than 
common user information in quantity for the 
unique queries in search engines. However, 
these counts deserve a further statistical cha-
racterization. In this section, we introduce 
Kappa statistic to depict the overall consisten-
cy of users? clicks in query logs. 
4.1 Kappa 
Kappa is a statistical measure introduced to 
access the agreement among different raters. 
There are two types of Kappa. One is Cohen?s 
Kappa (Cohen, 1960), which measures only 
the degree of agreement between two raters. 
The other is Fleiss?s Kappa (Fleiss, 1971), 
which generalizes Cohen?s Kappa to measure 
agreement among more than two raters, de-
noted as: 
e
e
P
PP
?
?=
1
?  
where, P is the probability that a randomly 
selected rater agree with another on a random-
ly selected subject. eP is the expected probabil-
ity of agreement if all raters made ratings by 
chance. If we use Kappa to measure the consis-
tency of relevance judgment by different raters, 
P can be interpreted as the probability that 
two random selected raters consistently rate a 
random selected search result as relevant or 
non-relevant one. Similarly, eP can also be 
construed as the expected probability of iden-
tical relevance judgment rated by different ra-
ters all by chance.  
Teevan et al (2008) used Fleiss?s Kappa to 
measure the inter-rater reliability of different 
raters? explicit relevance judgments. We ex-
pand their work and employ Fleiss?s Kappa to 
measure the consistency of implicit relevance 
judgments by users on the same query2. Here 
clicks are treated as a proxy for relevance: 
documents clicked by a user are judged as re-
levant and those not clicked as non-relevant 
(Teevan et al, 2008). As we all know that the 
result set of one query may change over time, 
so we select the longest time span to calculate 
Kappa value of a query, during which the re-
sult set of it preserves unchanged. From Kappa 
value of each query, we can statistically interp-
ret to which extent users share consistent intent 
on the same query according to Table 3 (Lan-
dis and Koch, 1977). Though the interpretation 
in Table 3 is not accepted with no doubt, it can 
give us an intuition about what extent of 
agreement consistency is. In other words, 
Kappa is a measure with statistical sense. 
Meanwhile, Kappa values of queries with  
                                                 
2 There may be more than two users who submitted the 
same query. 
704
  
                                       (a). AOL                                                                  (b). Sogou 
 
Figure 1: Number of unique queries and query submissions as a function of Kappa value. 
 
? Interpretation 
< 0 No agreement 
0.0 ? 0.20 Slight agreement 
0.21 ? 0.40 Fair agreement 
0.41 ? 0.60 Moderate agreement 
0.61 ? 0.80 Substantial agreement 
0.81 ? 1.00 Almost perfect agreement 
Table 3:  Kappa Interpretation 
 
various sizes of click sets are also comparable. 
That is also the reason we choose Kappa to 
measure consistency. 
4.2 Distribution of Kappa 
As introduced in Section 2, common user in-
formation is supposed to be the repetition of 
users? behaviors. We consider that the amount 
of repetition of users? clicks on one query is 
quantified by the consistency of its clicks. To 
statistically present the scale of repetition in 
current query log, we try to give an overview 
of consistency level of two commercial query 
logs. 
Figure 1 plots distribution of Kappa value of 
the two logs in the coordinate with logarithmic 
Y-axis. About 34.5% unique queries (44.0% 
query submissions) in AOL log and only 
13.9% unique queries (15.2% query submis-
sions) in Sogou log have high Kappa values 
above 0.6. According to Table 3, click sets of 
these queries can be regarded as somewhat 
consistent. These queries can be roughly re-
solved by using common user information. On 
the other hand, for the rest of queries which 
constitute majority of the logs, users? click sets 
are rather diversified, which are hard to be sa-
tisfied by returning the same result list to them. 
As a whole, the queries in both AOL and So-
gou can be characterized as less consistently in 
the clicks according to Kappa value, which is a 
statistical support for exploiting individual user 
information. 
5 Individual or Common user infor-
mation: A Tendency View 
The above analyses quantitative analyses have 
shown that the repetition of search is not the 
statistically dominant factor, with the impres-
sion that employing individual user informa-
tion is equally, if not more, important than 
common user information. This section tries to 
further reveal this issue so as to balance the 
position of individual user information and 
common user information from a research 
point. 
Intuitively, a query can be characterized by 
the number of people issuing it, i.e. query fre-
quency if we remove the resubmissions of one 
query by the same people. We try to depict the 
above mentioned query submissions and Kap-
pa values as a function of number of people 
who issue the queries in Figure 2. In Figure 2, 
different numbers of users who issue the same 
query are shown on the x-axis, and the y-axis 
represents the number of different entities (left 
scale) and the average Kappa value (right scale) 
of the queries. We find that the number of que-
ries becomes very small when the number of 
users in a group grows over 10, so we set a 
variant step length for them: with the length 
step of the group size falling between 2 and 10 
set as 1, between 11 and 100 as 10, between 
101 and 1000 as 100 and above 1000 as 1000. 
705
   
 
                                     (a). AOL                                                                      (b). Sogou 
 
Figure 2: Average Kappa value of queries as a function of number of people in a group who issue 
the same query (line) and the number of submissions of the queries issued by the same size of 
group (dark columns). 
 
According to Figure 2(a), Kappa values of 
the queries in AOL log with more than 20 us-
ers are above 0.6, which indicates rather con-
sistent clicks for them, accounting for about  
29.4% of all query submissions. While for 
those queries visited by less than 20 users, the 
Kappa value declines gradually from 0.6 with 
the drop of users. For these queries occupying 
majority of query submissions, exploiting in-
dividual user information is supposed to be a 
better solution since the clicks on them are ra-
ther individualized. 
According to Figure 2(b), though Kappa 
values of queries increase similarly with 
people submitting them in AOL, the overall 
consistency of the queries in Sogou log is 
much lower: with a Kappa value below 0.6 
even for the queries visited by a large number 
of users. This fact indicates that Chinese users 
may be less consistent in their search intents, 
or partially reflects that the Chinese as a non-
inflection language has more ambiguity, which 
can also be implied from Table 2. Therefore, 
individual user information may be more ef-
fective than common user information in So-
gou log. 
Summarized from Figure 2, it is sensible 
that common user information is appropriate 
for the queries in the right-most of X-axis. 
With most number of visiting people, such 
queries bear rather consistent clicks though 
covering only a small proportion of the distinct 
query set. Moving from the right to the left, we 
can find the majority of queries yield a less 
Kappa value, for which the individualized 
clicks require individual user information to 
meet the needs of each user. In this sense, how 
to exploit individual user information is pre-
destined as the next issue of information re-
trieval if common user information was to be 
well utilized. 
6 Queries for Personalization 
Since using individual user information is a 
non-negligible issue in IR research, a subse-
quent issue is what queries can benefit in what 
extent from individual user information. In this 
section, we try to give an overview for this 
issue via a measure named potential for perso-
nalization. 
6.1 Potential for Personalization 
Potential for personalization proposed by Tee-
van et al (2007) is used to measure the norma-
lized Discounted Cumulative Gain (NDCG) 
improvement between the best ranking of the 
results to a group and individuals. NDCG is a 
well-known measure of the quality of a search 
result (J?rvelin and Kek?l?inen, 2000). 
The best ranking of the results to a group is 
the ranking with highest NDCG based on re-
levance judgments of the users in the group. 
For the queries with explicit judgments, the 
best ranking can be generated as follows: re-
sults that all raters thought were relevant are 
ranked first, followed by those that most 
people thought were relevant but a few people 
thought were irrelevant, until the  results most 
people though were irrelevant. In other word,  
706
  
 
(a)  AOL                                                                    (b)   Sogou 
 
Figure 3: Number of unique queries and query submissions as a function of potential for 
personalization 
 
   
 
(a)  AOL                                                                       (b) Sogou 
 
Figure 4: The average NDCG of group best ranking as a function of number of people in group 
(solid line), combining with the distribution of  the number of unique queries issued by the same 
size of group (dark columns) 
 
the best ranking always tries to put the results 
that have the highest collective gain first to get 
the highest NDCG. 
The previous work has shown that the im-
plicit click-based potential for personalization 
is strongly related to variation in explicit 
judgments (J. Teevan et al, 2008). In this pa-
per, we continue using click-based potential 
for personalization to measure the variation. 
Assuming the clicked results as relevant, we 
can calculate the potential for personalization 
of each query over the web search query log to 
present what kind of query can benefit more 
from personalization. 
6.2 Potential for Personalization Distribu-
tion over Query Logs 
Teevan et al (2007) have depicted a potential 
for personalization curve based on explicit 
judgment to characterize the benefit that could 
be obtained by personalizing search results for 
each user. We continue using potential for per-
sonalization based on click-through to roughly 
reveal what kind of query can benefit more 
from personalization. 
First we investigate the number of unique 
queries with different potential for personaliza-
tion, which is shown in Figure 3. We find that 
there are about 53.9% unique queries in AOL 
log and 32.4% unique queries in Sogou log, 
whose potential for personalization is 0. For 
these queries, current web search is able to re-
turn perfect results to all users. However, for 
the rest of queries, even the best group ranking 
of results can?t satisfy everyone who issues the 
query. So these queries should be better served 
by individual user information, covering 
707
46.1% unique queries in AOL and 67.6% in 
Sogou. 
Then, in order to further interpret what kind 
of query individual user information is needed 
most, we further relate potential for personali-
zation to the number of users who submit the 
queries over AOL and Sogou query log as 
shown in Figure 4. For clarity?s sake, we also 
set the same step length as in Figure 2. 
According to Figure 4, the curve of potential 
for personalization is approximately U-shaped 
in both AOL log and Sogou Log. As the num-
ber of users in one group increases, perfor-
mance of the best non-personalized rankings 
first declines, then flattens out and finally 
promotes3. Note that the left part of the curve 
is very similar to what Teevan et al (2007) 
showed in their work. 
Again in Figure 4, the queries which have 
the most potential for personalization are the 
ones which are issued by more than 6 and less 
than 20 users in AOL log. While in Sogou log, 
the queries issued by more than 6 and less than 
4000 users have the most potential for persona-
lization. Such different findings are probably 
caused by the content of query. There are 
many recommended queries in the homepage 
of Sogou search engine, most of which are in-
formational query and clicked by a large num-
ber of users. Even when the size of group who 
issue the same query becomes very big, the 
query still has a wide variation of users? beha-
viors. So the consistency level of queries in 
Sogou log is much lower than the queries in 
AOL log at the same size of group.  
7 Conclusion and Future Work 
In this paper, we try to justify the position of 
individual user information comparing with 
common user information. It is shown that ex-
ploiting individual user information is a non-
trivial issue challenging the IR community 
through the analysis of both English and Chi-
nese large scale search logs. 
We first classify the repetitive queries into 2 
categories according to whether the corres-
ponding clicks are unique among different us-
ers. We find that quantitatively the queries and 
                                                 
3 Note that the different step length dims the actual U-
shape in the figure. 
clicks deserving for individual user informa-
tion is much bigger than those deserving for 
common user information. 
After that we use Kappa statistic to present 
that the overall consistency of query clicks re-
coded in search logs is pretty low, which statis-
tically reveals that the repetition is not the do-
minant factor and individual user information 
is more desired to enhance most queries in cur-
rent query log. 
We also explore the distribution of Kappa 
values over different numbers of users in the 
group who issue the same query, concluding 
that how to utilize individual user information 
to improve the performance of web search en-
gine is the next research issue confronted by 
the IR community when the repeated search of 
users are properly exploited.  
Finally, potential for personalization is cal-
culated over the two query logs to present an 
overview of what kind of queries that the op-
timal group-based retrieval model fails, which 
is supposed to benefit most from individual 
user information. 
One possible enrichment to this work may 
come from the employment of content analysis 
based on text processing techniques. The dif-
ferent clicks, which are the basis of our exami-
nation, may have similar or even exact content 
in their web pages. Though the manual check 
for a small scale sampling from the Sogou log 
yields less than 1% probability for such case, 
the content based examination will be definite-
ly more convincing than simple click counts. 
In addition, the queries for the two types of 
user information are not examined for their 
contents or the related information needs. Con-
tent analysis or linguistic view to these queries 
would be more informative. Both of these is-
sues are to be addressed in our future work. 
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project 
(Grant No.2006AA010108). The authors are 
grateful for the anonymous reviewers for their 
valuable comments. 
 
 
708
References 
Canny John. 2002. Collaborative filtering with pri-
vacy via factor analysis. In Proceedings of SI-
GIR? 02, pages 45-57. 
Carroll M. John and Mary B. Rosson. 1987. Para-
dox of the active user. Interfacing thought: cog-
nitive aspect of human-computer interaction, 
pages 80-111. 
Chirita A. Paul, Wofgang Nejdl, Raluca Paiu, and 
Christian Kohlschutter. 2005. Using odp metada-
ta to personalize search. In Proceedings of SI-
GIR ?05, pages 178-185. 
Cohen Jacob. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20: 37-46 
Dou Zhicheng, Ruihua Song, and Ju-Rong Wen. 
2007. A Large-scale Evaluation and Analysis of 
Personalized Search Strategies. In Proceedings 
of WWW ?07, pages 581-590. 
Fleiss L. Joseph. 1971. Measuring nominal scale 
agreement among many raters. Psychological 
Bulletin, 76(5): 378-382. 
Herlocker L. Jonathan, Joseph A. Konstan, Al 
Borchers, and John Riedl. 1999. An algorithmic 
framework for performing collaborative filtering. 
In Proceedings of SIGIR ?99, pages 230-237. 
Jansen J. Bernard, Amanda Spink, and Tefko Sara-
cevic. 2000. Real life, real users, and real needs: 
a study and analysis of user queries on the web. 
Information Processing and Management, pages 
207-227. 
J?rvelin Kalervo and Jaana Kek?l?inen. 2000. IR 
evaluation methods for retrieving highly relevant 
documents. In Proceedings of SIGIR ?00, pages 
41-48. 
Kohrs Arnd and Bernard Merialdo. 1999. Cluster-
ing for collaborative filtering applications. In 
Proceedings of CIMCA ?99, pages 199-204. 
Landis J. Richard and Gary. G. Koch. 1977. The 
mea-surement of observer agreement for cate-
gorical data. Biometrics 33: 159-174. 
Pitkow James, Hinrich Schutze, Todd Cass, Rob 
Cooley, Don Turnbull, Andy Edmonds, Eytan 
Adar and Thomas Breuel. 2002. Personalized 
search. ACM, 45(9):50-55. 
Shen Xuehua, Bin Tan and ChengXiang Zhai. 2005 
Implicit user modeling for personalized search. 
In Proceedings of CIKM ?05, pages 824-831. 
 
Silverstein Craig, Monika Henzinger, Hannes Ma-
rais and Michael Moricz. 1999. Analysis of a 
very large web search engine query log. SIGIR 
Forum, 33(1):6-12. 
Smyth Barry. 2007. A Community-Based Approach 
to Personalizing Web Search. IEEE Computer, 
40(8): 42-50. 
Speretta Mirco and Susan Gauch. Personalized 
Search based on user search histories. 2005. In 
Proceedings of WI ?05, pages 622-628. 
Spink Amanda, Dietmar Wolfram, Major Jansen, 
Tefko Saracevic. 2001. Searching the web: The 
public and their queries. Journal of the American 
Society for Information Science and Technology, 
52(3), 226-234 
Sugiyama Kazunari, Kenji Hatano, and Masatoshi 
Yoshikawa. 2004. Adaptive web search based on 
user profile constructed without any effort from 
users. In Proceedings of WWW ?04, pages 675-
684. 
Sun Jian-Tao, Hua-Jun Zeng, Huan Liu, Yuchang 
Lu and Zheng Chen. 2005. CubeSVD: a novel 
approach to personalized web search. In Pro-
ceedings of WWW?05, pages 382-390. 
Teevan Jaime, Susan T. Dumais, and Eric Horvitz. 
2005. Personalizing search via automated analy-
sis of interests and activities. In Proceedings of 
SIGIR ?05, pages 449-456. 
Teevan Jaime, Susan T. Dumais and Eric Horvitz. 
2007. Characterizing the value of personalizing 
search. In Proceedings of SIGIR ?07, pages 757-
758. 
Teevan Jaime, Susan T. Dumais and Daniel J. 
Liebling. 2008. To personalize or Not to Perso-
nalize: Modeling Queries with Variation in User 
Intent. In Proceedings of SIGIR ?08, pages 163-
170. 
Townsend Steve Cronen and W. Bruce Croft. 2002. 
Quantifying query ambiguity. In Proceedings of 
HLT ?02, pages 613-622. 
Yu Kai, Anton Schwaighofer, Volker Tresp, Xiao-
wei Xu, Hans-Peter Kriegel. 2004. Probabilistic 
Memory-based Collaborative Filtering. In IEEE 
Transactions on Knowledge and Data Engineer-
ing, pages 56-59. 
 
709
Coling 2010: Poster Volume, pages 748?756,
Beijing, August 2010
Head-modifier Relation based Non-lexical Reordering Model 
for Phrase-Based Translation  
Shui Liu1, Sheng Li1, Tiejun Zhao1, Min Zhang2, Pengyuan Liu3 
1School of Computer Science and Technology, Habin Institute of Technology 
{liushui,lisheng,tjzhao}@mtlab.hit.edu.cn 
2Institute for Infocomm Research 
mzhang@i2r.a-star.edu.sg 
3Institute of Computational Linguistics, Peking University  
liupengyuan@pku.edu.cn 
 
Abstract 
Phrase-based statistical MT (SMT) is a 
milestone in MT. However, the transla-
tion model in the phrase based SMT is 
structure free which greatly limits its 
reordering capacity. To address this is-
sue, we propose a non-lexical head-
modifier based reordering model on 
word level by utilizing constituent based 
parse tree in source side. Our experi-
mental results on the NIST Chinese-
English benchmarking data show that, 
with a very small size model, our me-
thod significantly outperforms the base-
line by 1.48% bleu score. 
1 Introduction 
Syntax has been successfully applied to SMT to 
improve translation performance. Research in 
applying syntax information to SMT has been 
carried out in two aspects. On the one hand, the 
syntax knowledge is employed by directly inte-
grating the syntactic structure into the transla-
tion rules i.e. syntactic translation rules. On this 
perspective, the word order of the target transla-
tion is modeled by the syntax structure explicit-
ly.  Chiang (2005), Wu (1997) and Xiong (2006) 
learn the syntax rules using the formal gram-
mars. While more research is conducted to learn 
syntax rules with the help of linguistic analysis 
(Yamada and Knight, 2001; Graehl and Knight, 
2004). However, there are some challenges to 
these models. Firstly, the linguistic analysis is 
far from perfect. Most of these methods require 
an off-the-shelf parser to generate syntactic 
structure, which makes the translation results 
sensitive to the parsing errors to some extent. 
To tackle this problem, n-best parse trees and 
parsing forest (Mi and Huang, 2008; Zhang, 
2009) are proposed to relieve the error propaga-
tion brought by linguistic analysis. Secondly, 
some phrases which violate the boundary of 
linguistic analysis are also useful in these mod-
els ( DeNeefe et al, 2007; Cowan et al 2006). 
Thus, a tradeoff needs to be found between lin-
guistic sense and formal sense. 
On the other hand, instead of using syntactic 
translation rules, some previous work attempts 
to learn the syntax knowledge separately and 
then integrated those knowledge to the original 
constraint. Marton and Resnik (2008) utilize the 
language linguistic analysis that is derived from 
parse tree to constrain the translation in a soft 
way. By doing so, this approach addresses the 
challenges brought by linguistic analysis 
through the log-linear model in a soft way.  
Starting from the state-of-the-art phrase based 
model Moses ( Koehn e.t. al, 2007), we propose 
a head-modifier relation based reordering model 
and use the proposed model as  a soft syntax 
constraint in the phrase-based translation 
framework. Compared with most of previous 
soft constraint models, we study the way to util-
ize the constituent based parse tree structure by 
mapping the parse tree to sets of head-modifier 
for phrase reordering. In this way, we build a 
word level reordering model instead of phras-
al/constituent level model.  In our model, with 
the help of the alignment and the head-modifier 
dependency based relationship in the source 
side, the reordering type of each target word 
with alignment in source side is identified as 
one of pre-defined reordering types. With these 
reordering types, the reordering of phrase in 
translation is estimated on word level.   
 
748
 
Fig 1. An Constituent based Parse Tree 
 
 
2 Baseline  
Moses, a state-of-the-art phrase based SMT sys-
tem is used as our baseline system. In Moses, 
given the source language f and target language 
e, the decoder is to find: 
ebest = argmaxe p ( e | f ) pLM ( e ) ?
length(e)        (1)    
where p(e|f) can be computed using phrase 
translation model, distortion model and lexical 
reordering model. pLM(e) can be computed us-
ing the language model. ?length(e) is word penalty 
model.  
Among the above models, there are three 
reordering-related components: language model, 
lexical reordering model and distortion model. 
The language model can reorder the local target 
words within a fixed window in an implied way. 
The lexical reordering model and distortion 
reordering model tackle the reordering problem 
between adjacent phrase on lexical level and 
alignment level. Besides these reordering model, 
the decoder induces distortion pruning con-
straints to encourage the decoder translate the 
leftmost uncovered word in the source side 
firstly and to limit the reordering within a cer-
tain range. 
3 Model  
In this paper, we utilize the constituent parse 
tree of source language to enhance the  reorder- 
 
 
ing capacity of the translation model. Instead of 
directly employing the parse tree fragments 
(Bod, 1992; Johnson, 1998) in reordering rules 
(Huang and Knight, 2006; Liu 2006; Zhang and 
Jiang 2008), we make a mapping from trees to 
sets of head-modifier dependency relations 
(Collins 1996 ) which  can be obtained  from the 
constituent based parse tree with the help of 
head rules ( Bikel, 2004 ). 
3.1 Head-modifier Relation  
According to Klein and Manning (2003) and 
Collins (1999), there are two shortcomings in n-
ary Treebank grammar.  Firstly, the grammar is 
too coarse for parsing. The rules in different 
context always have different distributions. Se-
condly, the rules learned from training corpus 
cannot cover the rules in testing set. 
Currently, the state-of-the-art parsing algo-
rithms (Klein and Manning, 2003; Collins 1999) 
decompose the n-ary Treebank grammar into 
sets of head-modifier relationships. The parsing 
rules in these algorithms are constructed in the 
form of finer-grained binary head-modifier de-
pendency relationships. Fig.2 presents an exam-
ple of head-modifier based dependency tree 
mapped from the constituent parse tree in Fig.1.  
 
749
 
Fig. 2. Head-modifier Relationships with Aligned Translation 
 
Moreover, there are several reasons for which 
we adopt the head-modifier structured tree as 
the main frame of our reordering model. Firstly, 
the dependency relationships can reflect some 
underlying binary long distance dependency 
relations in the source side. Thus, binary depen-
dency structure will suffer less from the long 
distance reordering constraint. Secondly, in 
head-modifier relation, we not only can utilize 
the context of dependency relation in reordering 
model, but also can utilize some well-known 
and proved helpful context (Johnson, 1998) of 
constituent base parse tree in reordering model. 
Finally, head-modifier relationship is mature 
and widely adopted method in full parsing.   
3.2 Head-modifier Relation Based Reor-
dering Model  
Before elaborating the model, we define some 
notions further easy understanding. S=<f1, f 
2?fn> is the source sentence; T=<e1,e2,?,em> is 
the target sentence; AS={as(i) | 1? as(i) ? n } 
where as(i) represents that the ith word in source 
sentence  aligned to the as(i)th word in target 
sentence; AT={aT(i) | 1? aT (i) ? n } where aT(i) 
represents that the ith word in target sentence  
aligned to the aT(i)th word in source sentence; 
D= {( d(i), r(i) )| 0? d(i) ?n} is the head-
modifier relation set of  the words in S where 
d(i) represents that the ith word in source sen-
tence is the modifier of d(i)th  word in source 
sentence under relationship r(i); O= < o1, o2,?, 
om > is the sequence of the reordering type of 
every word in target language. The reordering 
model probability is P(O| S, T, D, A).  
Relationship: in this paper, we not only use the 
label of the constituent label as Collins (1996), 
but also use some well-known context in pars-
ing to define the head-modifier relationship r(.), 
including the POS of the modifier m,  the POS 
of the head h, the dependency direction d, the 
parent label of the dependency label l, the 
grandfather label of the dependency relation p, 
the POS of adjacent siblings of the modifier s. 
Thus, the head-modifier relationship can be 
represented as a 6-tuple <m, h, d, l, p, s>. 
 
r(.) relationship 
r(1) <VV, - , -, -, -, - > 
r(2) <NN, NN, right, NP, IP, - > 
r(3) <NN,VV, right, IP, CP, - > 
r(4) <VV, DEC, right, CP, NP, - > 
r(5) <NN,VV, left, VP, CP, - > 
r(6) <DEC, NP, right, NP, VP, - > 
r(7) <NN, VV, left, VP,  TOP, - > 
Table 1. Relations Extracted from Fig 2.  
 
In Table 1, there are 7 relationships extracted 
from the source head-modifier based dependen-
cy tree as shown in Fig.2. Please notice that, in 
this paper, each source word has a correspond-
ing relation.  
Reordering type: there are 4 reordering types 
for target words with linked word in the source 
side in our model: R= {rm1, rm2, rm3 , rm4}. The 
reordering type of target word as(i) is defined  as 
follows: 
? rm1: if the position number of the ith 
word?s head is less than i ( d(i) < i ) in 
source language, while the position num-
ber of the word aligned to i is less than 
750
as(d(i)) (as(i)  < as(d(i)) ) in target lan-
guage;  
? rm2: if the position number of the ith 
word?s head is less than i ( d(i) < i ) in 
source language, while the position num-
ber of the word aligned to i is larger than 
as(d(i)) (as(i) > as(d(i)) ) in target lan-
guage. 
? rm3: if the position number of the ith 
word?s head is larger than i ( d(i) > i ) in 
source language, while the position num-
ber of the word aligned to i is larger than 
as(d(i)) (as(i) > as(d(i))) in target language. 
? rm4: if the position number of the ith 
word?s head is larger than i ( d(i) > i) in 
source language, while the position num-
ber of the word aligned to i is less than 
as(d(i)) (as(i) < as(d(i)) ) in target lan-
guage. 
 
 
Fig. 3.  An example of the reordering types in 
Fig. 2. 
Fig. 3 shows examples of all the reordering 
types. In Fig. 3, the reordering type is labeled at 
the target word aligned to the modifier: for ex-
ample, the reordering type of rm1 belongs to the 
target word ?scale?. Please note that, in general, 
these four types of reordering can be divided 
into 2 categories: the target words order of rm2 
and rm4 is identical with source word order, 
while rm1 and rm3 is the swapped order of 
source. In practice, there are some special cases 
that can?t be classified into any of the defined 
reordering types: the head and modifier in 
source link to the same word in target. In such 
cases, rather than define new reordering types, 
we classify these special cases into these four 
defined reordering types: if the head is right to 
the modifier in source, we classify the reorder-
ing type into rm2; otherwise, we classify the 
reordering type into rm4. 
Probability estimation: we adopt maximum 
likelihood (ML) based estimation in this paper. 
In ML estimation, in order to avoid the data 
sparse problem brought by lexicalization, we 
discard the lexical information in source and 
target language: 
?
?
?
m
1i
Ti (i)))r(a-,-, |P(o
A) D, T, S, |P(O
                                  (2) 
where oi?{rm1,rm2,rm3,rm4} is the reorder-
ing type of ith word in  target language. 
To get a non-zero probability, additive smooth 
ing( Chen and Goodman, 1998) is used: 
?
?
?
?
??
?
?
??
?
?
?
?
?
?
||),,,,,(
),,,,,,(
||)))(((
)))((,(
) )))(((-,-,|P(o
)()()()()()(
)()()()()()(
i
OspldhmF
spldhmoF
OiarF
iaroF
iarF
iaiaiaiaiaia
Ro
iaiaiaiaiaiai
t
Ro
Ti
t
TTTTTT
i
TTTTTT
i
                                                                                 
(3) 
where F(. ) is the frequency of the statistic event 
in training corpus. For a given set of dependen-
cy relationships mapping from constituent tree, 
the reordering type of ith word is confined to 
two types: it is whether one of rm1 and rm2 or 
rm3 and rm4. Therefore, |O|=2 instead of |O|=4 
in (2). The parameter ? is an additive factor to 
prevent zero probability. It is computed as:   
                                        
),,,,,(
1
)()()()()()( iaiaiaiaiaia
Ro
TTTTTT
i
spldhmFC ?
?
???
       
(4) 
where c is a constant parameter(c=5 in this pa-
per). 
   In above, the additive parameter ? is an adap-
tive parameter decreasing with the size of the 
statistic space. By doing this, the data sparse 
problem can be relieved. 
4 Apply the Model to Decoder 
Our decoding algorithm is exactly the same as 
(Kohn, 2004). In the translation procedure, the 
decoder keeps on extending new phrases with-
out overlapping, until all source words are trans-
lated. In the procedure, the order of the target 
751
words in decoding procedure is fixed.  That is, 
once a hypothesis is generated, the order of tar-
get words cannot be changed in the future. Tak-
ing advantage of this feature, instead of compu-
ting a totally new reordering score for a newly 
generated hypothesis, we merely calculate the 
reordering score of newly extended part of the 
hypothesis in decoding. Thus, in decoding, to 
compute the reordering score, the reordering 
types of each target word in the newly extended 
phrase need to be identified.  
The method to identify the reordering types 
in decoding is proposed in Fig.4. According to 
the definition of reordering, the reordering type 
of the target word is identified by the direction 
of head-modifier dependency on the source side, 
the alignment between the source side and tar-
get side, and the relative translated order of 
word pair under the head-modifier relationship. 
The direction of dependency and the alignment 
can be obtained in input sentence and phrase 
table. While the relative translation order needs 
to record during decoding. A word index is em-
ployed to record the order. The index is con-
structed in the form of true/false array: the index 
of the source word is set with true when the 
word has been translated. With the help of this 
index, reordering type of every word in the 
phrase can be identified. 
 
1: Input: alignment array AT; the Start is the 
start position of the phrase in the source side; 
head-modifier relation d(.); source word in-
dex C, where C[i]=true  indicates that the 
ith word in source has been translated.   
2: Output: reordering type array O which re-
serves the reordering types of each word in 
the target phrase 
3: for i = 1, |AT| do 
4:    P  ? aT(i) + Start 
5:    if (d (P)<P) then 
6:      if C [d(p)] = false then 
7:         O[i] ? rm1 
8:      else 
9:         O[i] ? rm2 
10:        end if 
11:  else   
12:     if  C[d(p)] = true then 
13:        O[i] ? rm3 
14:       else 
15:          O[i] ? rm4 
16:       end if 
17:    end if 
18: C[p] ?true //update word index 
19: end for 
Fig. 4.  Identify the Reordering Types of  Newly 
Extended Phrase 
After all the reordering types in the newly ex-
tended phrase are identified, the reordering 
scores of the phrase can be computed by using 
equation (3). 
5 Preprocess the Alignment 
In Fig. 4, the word index is to identify the reor-
dering type of the target translated words. Ac-
tually, in order to use the word index without 
ambiguity, the alignment in the proposed algo-
rithm needs to satisfy some constraints.  
Firstly, every word in the source must have 
alignment word in the target side. Because, in 
the decoding procedure, if the head word is not 
covered by the word index, the algorithm cannot 
distinguish between the head word will not be 
translated in the future and the head word is not 
translated yet. Furthermore, in decoding, as 
shown in Fig.4, the index of source would be set 
with true only when there is word in target 
linked to it. Thus, the index of the source word 
without alignment in target is never set with true.  
 
Fig. 5.  A complicated Example of Alignment in 
Head-modifier based Reordering Model 
Secondly, if the head word has more than one 
alignment words in target, different alignment 
possibly result in different reordering type. For 
example, in Fig. 5, the reordering type of e2 is 
different when f2 select to link word e1 and e3   
in the source side.  
To solve this problem, we modify the align-
ment to satisfy following conditions: a) each 
word in source just has only one alignment 
word in target, and b) each word in target has at 
most one word aligned in source as its anchor 
word which decides the reordering type of the 
target word.  
To make the alignment satisfy above con-
straints, we modify the alignment in corpus. In 
752
order to explain the alignment preprocessing, 
the following notions are defined: if there is a 
link between the source word f j  and target word 
ei, let  l(ei ,fj) = 1 , otherwise l(ei ,fj) = 0; the 
source word fj?F1-to-N , iff  ?i l(ei,fj) >1, such 
as the source word f2 in Fig. 5; the source word 
fj?FNULL, iff ?i l(ei,fj) = 0, such as the source 
word f4 in Fig. 5; the target word ei?E1-to-N  , iff 
?j l(ei,fj) > 1, such as the target word e1 in Fig. 
5.  
In preprocessing, there are 3 types of opera-
tion, including DiscardLink(fj) , BorrowLink( f j )  
and FindAnchor(ei ) : 
DiscardLink( fj ) : if the word fj in source with 
more than one words aligned in target, i.e.  fj?
F1-to-N ; We set the target word en with l(en, fj) = 
1, where en= argmaxi p(ei | fj) and   p(ei | fj) is 
estimated by ( Koehn e.t. al, 2003), while set  
rest of words linked to fj with l (en, fj) = 0.    
BorrowLink( fj ): if the word fj in source with-
out a alignment word in target, i.e.  fj?FNULL ; 
let l(ei,fj)=1 where ei  aligned to the word fj , 
which is the nearest word to  fj  in the source 
side; when there are two words nearest to fj with 
alignment words in the target side at the same 
time, we select the alignment of  the left word 
firstly .  
FindAnchor( ): for the word ei  in target with 
more than one words aligned in source , i.e.  ei
?E1-to-N ; we select the word  fm  aligned to ei as 
its anchor word to decide the reordering type of 
ei  ,  where fm= argmaxj p(ei | fj) and  p(fj | ei) is 
estimated by ( Koehn et al 2003); For the rest 
of words aligned to  ei , we would set their word 
indexes with true in the update procedure of 
decoding  in the 18th line of Fig.4.    
With these operations, the required alignment 
can be obtained by preprocessing the origin 
alignment as shown in Fig. 6. 
1: Input: set of alignment A between target lan-
guage e and source language f  
2: Output: the 1-to-1 alignment required by the 
model 
3:  foreach fi?F1-to-N do 
4:    DiscardLink( fi ) 
5:  end for 
6:  foreach fi  ?FNULL  do 
7:    BorrowLink( fi ) 
8:  end for 
9:  foreach  ei?E1-to-N do  
10:   FindAnchor(ei ) 
11:endfor           
Fig. 6. Alignment Pre-Processing algorithm 
 
 
 
Fig. 7. An Example of Alignment Preprocessing. 
   An example of  the preprocess the alignment 
in Fig. 5 is shown in Fig. 7 : firstly, Discar-
dLink(f2) operation discards the link between f2 
and e1  in (a); then the link between f4 and e3 is 
established by operation BorrowLink(f4 )  in (b); 
at last, FindAnchor(e3) select f2 as the anchor 
word of e3  in source in (c). After the prepro-
cessing, the reordering type of e3   can be identi-
fied. Furthermore, in decoding, when the de-
coder scans over e2, the word index sets the 
word index of f3 and f4 with true. In this way, 
the never-true word indexes in decoding are 
avoided.  
6 Training the Reordering Model 
Before training, we get the required alignment 
by alignment preprocessing as indicated above. 
Then we train the reordering model with this 
alignment: from the first word to the last word 
in the target side, the reordering type of each 
word is identified. In this procedure, we skip the 
words without alignment in source. Finally, all 
the statistic events required in equation (3) are 
added to the model.   
In our model, there are 20,338 kinds of rela-
tions with reordering probabilities which are 
much smaller than most phrase level reordering 
models on the training corpus FBIS.   
Table 1 is the distribution of different reor-
dering types in training model.  
753
Type of Reordering   Percentage   %    
           
rm1 
rm2 
rm3 
3.69 
27.61 
20.94 
rm4 47.75 
Table 1: Percentage of different reordering 
types in model 
From Table 1, we can conclude that the reor-
dering type rm2 and rm4 are preferable in reor-
dering which take over nearly 3/4 of total num-
ber of reordering type and are identical with 
word order of the source. The statistic data indi-
cate that most of the words order doesn?t change 
in our head-modifier reordering view.  This 
maybe can explain why the models (Wu, 1997; 
Xiong, 2006; Koehn, et., 2003) with limited 
capacity of reordering can reach certain perfor-
mance. 
7 Experiment and Discussion  
7.1 Experiment Settings 
We perform Chinese-to-English translation task 
on NIST MT-05 test set, and use NIST MT-02 
as our tuning set. FBIS corpus is selected as our 
training corpus, which contains 7.06M Chinese 
words and 9.15M English words. We use GI-
ZA++(Och and Ney, 2000) to make the corpus 
aligned. A 4-gram language model is trained 
using Xinhua portion of the English Gigaword 
corpus (181M words). All models are tuned on 
BLEU, and evaluated on both BLEU and NIST 
score. 
To map from the constituent trees to sets of 
head-modifier relationships, firstly we use the 
Stanford parser (Klein, 2003) to parse the 
source of corpus FBIS, then we use the head-
finding rules in (Bikel, 2004) to get the head-
modifier dependency sets. 
In our system, there are 7 groups of features. 
They are: 
1. Language model score (1 feature) 
2. word penalty score (1 feature) 
3. phrase model scores (5 features) 
4. distortion score (1 feature) 
5. lexical RM scores (6 features) 
6. Number of each reordering type (4 fea-
tures) 
7. Scores of each reordering type (4 fea-
tures, computed by equation (3)) 
In these feature groups, the top 5 groups of 
features are the baseline model, the left two 
group scores are related with our model.  
In decoding, we drop all the OOV words and 
use default setting in Moses: set the distortion 
limitation with 6, beam-width with 1/100000, 
stack size with 200 and max number of phrases 
for each span with 50.  
7.2 Results and Discussion 
We take the replicated Moses system as our 
baseline. Table 2 shows the results of our model.  
In the table, Baseline model is the model includ-
ing feature group 1, 2, 3 and 4. Baselinerm mod-
el is the Baseline model with feature group 5. H-
M model is the Baseline model with feature 
group 6 and 7. H-Mrm model is the Baselinerm 
model with feature group 6 and 7.  
Model BLEU% NIST 
Baseline 27.06 7.7898 
Baselinerm  27.58     7.8477 
H-M  28.47     8.1491 
H-Mrm 29.06 8.0875 
Table 2: Performance of  the Systems on NIST-
05(bleu4 case-insensitive). 
From table 2, we can conclude that our reor-
dering model is very effective. After adding 
feature group 6 and 7, the performance is im-
proved by 1.41% and 1.48% in bleu score sepa-
rately. Our reordering model is more effective 
than the lexical reordering model in Moses:  
1.41% in bleu score is improved by adding our 
reordering model to Baseline model, while 0.48 
is improved by adding the lexical reordering to 
Baseline model.   
threshold KOR BLEU NIST 
?1 20,338  29.06  8.0875 
?2      13,447 28.83   8.3658 
?3      10,885 28.64 8.0350 
?4        9,518 28.94 8.1002 
?5        8,577       29.18   8.1213 
Table 3: Performance on NIST-05 with Differ-
ent Relation Frequency Threshold (bleu4 case-
insensitive). 
Although our model is lexical free, the data 
sparse problem affects the performance of the 
model. In the reordering model, nearly half 
numbers of the relations in our model occur less 
than three times. To investigate this, we statistic 
754
the frequency of the relationships in our model, 
and expertise our H-M full model with different 
frequency threshold.  
In Table 3, when the frequency of relation is 
not less than the threshold, the relation is added 
into the reordering model; KOR is the number 
of relation type in the reordering model.  
Table 3 shows that, in our model, many rela-
tions occur only once. However, these low-
frequency relations can improve the perfor-
mance of the model according to the experimen-
tal results. Although low frequency statistic 
events always do harm to the parameter estima-
tion in ML, the model can estimate more events 
in the test corpus with the help of low frequency 
event. These two factors affect the experiment 
results on opposite directions: we consider that 
is the reason the result don?t increase or de-
crease with the increasing of frequency thre-
shold in the model. According to the results, the 
model without frequency threshold achieves the 
highest bleu score. Then, the performance drops 
quickly, when the frequency threshold is set 
with 2. It is because there are many events can?t 
be estimated by the smaller model. Although, in 
the model without frequency threshold, there 
are some probabilities overestimated by these 
events which occur only once, the size of the 
model affects the performance to a larger extent. 
When the frequency threshold increases above 3, 
the size of model reduces slowly which makes 
the overestimating problem become the impor-
tant factor affecting performance. From these 
results, we can see the potential ability of our 
model: if our model suffer less from data spars 
problem, the performance should be further im-
proved, which is to be verified in the future.   
8 Related Work and Motivation 
There are several researches on adding linguis-
tic analysis to MT in a ?soft constraint? way. 
Most of them are based on constituents in parse 
tree. Chiang(2005), Marton and Resnik(2008) 
explored the constituent match/violation in hie-
ro; Xiong (2009 a) added constituent parse tree  
based linguistic analysis into BTG model; 
Xiong (2009 b) added source dependency struc-
ture to BTG; Zhang(2009) added tree-kernel to 
BTG model.  All these studies show promising 
results. Making soft constrain is an easy and 
efficient way in adding linguistic analysis into 
formal sense SMT model.   
In modeling the reordering, most of previous 
studies are on phrase level. In Moses, the lexical 
reordering is modeled on adjacent phrases. In 
(Wu, 1996; Xiong, 2006), the reordering is also 
modeled on adjacent translated phrases. In hiero, 
the reordering is modeled on the segments of 
the unmotivated translation rules. The tree-to-
string models (Yamada et al 2001; Liu et 
al.2006) are model on phrases with syntax re-
presentations. All these studies show excellent 
performance, while there are few studies on 
word level model in recent years. It is because, 
we consider, the alignment in word level model 
is complex which limits the reordering capacity 
of word level models.  
However, our work exploits a new direction 
in reordering that, by utilizing the decomposed 
dependency relations mapped from parse tree as 
a soft constraint, we proposed a novel head-
modifier relation based word level reordering 
model. The word level reordering model is 
based on a phrase based SMT framework. Thus, 
the task to find the proper position of translated 
words converts to score the reordering of the 
translated words, which relax the tension be-
tween complex alignment and word level reor-
dering in MT.  
9 Conclusion and Future Work 
Experimental results show our head-modifier 
relationship base model is effective to the base-
line (enhance by 1.48% bleu score), even with 
limited size of model and simple parameter es-
timation. In the future, we will try more compli-
cated smooth methods or use maximum entropy 
based reordering model. We will study the per-
formance with larger distortion constraint, such 
as the performances of   the distortion constraint 
over 15, or even the performance without distor-
tion model.  
10 Acknowledgement 
The work of this paper is funded by National 
Natural Science Foundation of China (grant no. 
60736014), National High Technology Re-
search and Development Program of China (863 
Program) (grant no. 2006AA010108), and Mi-
crosoft Research Asia IFP (grant no. FY09-
RES-THEME-158). 
755
References  
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpo-
ra. Computational Lingustics,23(3):377-403. 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05.263-270. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201-
228. 
Kenji Yamada and K. Knight. 2001. A syntax-based 
statistical translation model. ACL-01.523-530. 
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic Constraints for Hierarchical Phrased-based 
Translation. ACL-08. 1003-1011. 
Libin  shen, Jinxi Xu and Ralph Weischedel. 2008. A 
New String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. ACL-08. 577-585. 
J. Graehl and K. Knight.2004.Train ing Tree trans-
ducers. In proceedings of the 2004 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computation-
al Linguistics. 
Dekai Wu. 1996. A Polynomial-Time Algorithm for 
Statistical Machine Translation. In proceedings of 
ACL-1996 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Ma x-
imum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In  proceed-
ings of COLING-ACL 2006 
Deyi Xiong, Min Zhang, Aiti AW and Haizhou Li.  
2009a. A Syntax-Driven Bracket Model for 
Phrase-Based Translation. ACL-09.315-323. 
Deyi Xiong, Min Zhang, Aiti AW and Haizhou Li. 
2009b. A Source Dependency Model for Statistic 
Machine translation. MT-Summit 2009. 
Och, F.J. and Ney, H. 2000. Improved statistical 
alignment models. In Proceedings of ACL 38. 
Philipp Koehn, et al Moses: Open Source Toolkit 
for Statistical Machine Translation, ACL 2007. 
Philipp Koehn, Franz Joseph Och, and Daniel Mar-
cu.2003. Statistical Phrase-based Translation. In 
Proceedings of HLT-NAACL. 
Philipp Koehn. 2004. A Beam Search Decoder for 
Phrase-Based Translation model. In : Proceeding 
of AMTA-2004,Washington 
Rens Bod. 1992. Data oriented Parsing( DOP ). In  
Proceedings of COLING-92. 
Mark Johnson. 1998. PCFG models of linguistic tree 
representations. Computational Linguistics, 
24:613-632. 
Liang Huang, Kevin Knight, and Aravind Joshi. Sta-
tistical Syntax-Directed Translation with Ex-
tended Domain of Locality. 2006. In Proceedings 
of the 7th AMTA. 
Yang Liu, Qun Liu, and Shouxun Lin. Tree-to-String 
Alignment Template for Statistical Machine 
Translation. 2006.In Proceedings of the ACL 2006.  
Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. ACL-HLT-08. 559-567. 
Dan Klein, Christopher D. Manning. Accurate Un-
lexicalized Parsing. 2003. In Proceedings of  
ACL-03. 423-430. 
M. Collins. 1996. A new statistical parser based on 
bigram lexical dependencies. In Proceedings of  
ACL-96. 184-191. 
M. Collins. 1999. Head-Driven Statistical Models for 
Natural Language Parsing. Ph.D. thesis, Univ. of 
Pennsylvania. 
Andreas Zollmann. 2005. A Consistent and Efficient 
Estimator for the Data-Oriented Parsing Model. 
Journal of Automata, Languages  and Combinator-
ics. 2005(10):367-388 
Mark Johnson. 2002. The DOP estimation method is 
biased and inconsistent. Computational Linguis-
tics 28, 71-76. 
Daniel M. Bikel. 2004. On the Parameter Space of  
Generative Lexicalized Statistical Parsing Models. 
Ph.D. thesis. Univ. of Pennsylvania. 
S. F. Chen, J. Goodman. An Empirical Study of 
Smoothing Techniques for Language Modeling. 
In Proceedings of the 34th annual meeting on As-
sociation for Computational Linguistics, 
1996.310-318. 
Haitao Mi and Liang Huang.  2008. Forest-based 
translation Rule Extract ion. ENMLP-08. 2006-214. 
Hui Zhang, Min Zhang , Haizhou Li, A iti Aw and 
Chew Lim Tan. Forest-based Tree Sequence to 
String Translation Model. ACL-09: 172-180 
S DeNeefe, K. Knight, W. Wang, and D. Marcu. 
2007. What can syntax-based MT learn from 
phrase-based MT ? In Proc. EMNLP-CoNULL. 
Brooke Cowan, Ivona Kucerova, and Michael 
Collins.2006. A discriminative model for tree-to-
tree translation. In Proc. EMNLP. 
756
Coling 2010: Poster Volume, pages 1167?1175,
Beijing, August 2010
Bridging Topic Modeling and Personalized Search
Wei Song Yu Zhang Ting Liu Sheng Li
School of Computer Science
Harbin Institute of Technology
{wsong, yzhang, tliu, lisheng}@ir.hit.edu.cn
Abstract
This work presents a study to bridge topic
modeling and personalized search. A
probabilistic topic model is used to extract
topics from user search history. These
topics can be seen as a roughly summary
of user preferences and further treated as
feedback within the KL-Divergence re-
trieval model to estimate a more accurate
query model. The topics more relevant
to current query contribute more in updat-
ing the query model which helps to dis-
tinguish between relevant and irrelevant
parts and filter out noise in user search
history. We designed task oriented user
study and the results show that: (1) The
extracted topics can be used to cluster
queries according to topics. (2) The pro-
posed approach improves ranking qual-
ity consistently for queries matching user
past interests and is robust for queries not
matching past interests.
1 Introduction
The majority of queries submitted to search en-
gines are short and ambiguous and the users of
search engines often have different search intents
even when they submit the same query (Janse and
Saracevic, 2000)(Silverstein and Moricz, 1999).
The ?one size fits all? approach fails to optimize
each individual?s specific information need. Per-
sonalized search has be viewed as a promising
direction to solve the ?data overload? problem,
and aims to provide different search results ac-
cording to the specific preference of an individ-
ual(Pitkow and Breuel, 2002). Information re-
trieval (IR) communities have developed models
for context sensitive search and related applica-
tions (Shen and Zhai, 2005a)(White and Chen,
2009).
The search context includes a broad range of in-
formation types such as a user?s background, his
personal desktop index, browser history and even
the context information of a group of similar users
(Teevan, 2009). In this paper, we exploit the user
search history of an individual which contains the
past submitted queries, results returned and the
click through information. As described in (Tan
and Zhai, 2006), search history is one of the most
important forms of search context. When dealing
with search history, distinguishing between rele-
vant and irrelevant parts is important. The search
history may contain a lot of noisy information
which can harm the performance of personaliza-
tion (Dou and Wen, 2007). Hence, we need to
sort out relevant and irrelevant parts to optimize
search personalization.
In this paper, we propose a topic model based
approach to study users? preferences. The main
contribution of this work is modeling user search
history with topics for personalized search. Our
approach mainly consists of two steps: topic ex-
traction and relevance feedback. We assume that
a user?s search history is governed by the underly-
ing hidden properties and apply probabilistic La-
tent Semantic Indexing (pLSI) (Hofmann, 1999)
to extract topics from user search history. Each
topic indexes a unigram language model. We
model these extracted topics as feedback in the
KL-Divergence retrieval framework. The task is
to estimate a more accurate query model based
on the evidence from user feedback. We distin-
1167
guish relevant parts from irrelevant parts in search
history by focusing on the relevance between top-
ics and query. The closer a topic is to the cur-
rent query, the more it contributes in updating the
query model, which in turn is used to rerank the
documents in results set.
2 Related Work
2.1 Personalized IR
Personalized search is an active ongoing research
direction. Based on different representations of
user profile, we classify approaches as follows:
Taxonomy based methods: this approach
maps user interests to an existing taxonomy.
ODP1 is widely used for this purpose. For
example, by exploiting the user search history,
(Speretta and Gauch, 2005) modeled user interest
as a weighted concept hierarchy created from the
top 3 level of ODP. (Havelivala, 2002) proposed
the ?topic sensitive pagerank? algorithm by cal-
culating a set of PageRanks for each web page on
the top 16 ODP categories. (Qiu and Cho, 2006)
further improved this approach by building user
models from user click history. In recent stud-
ies, (Xu S. and Yu, 2008) used ODP categories
for exploring folksonomy for personalized search.
(Dou and Wen, 2007) proposed a method that rep-
resent user profile as a weighting vector of 67 pre-
defined topic categories provided by KDD Cup-
2005. Taxonomy based methods rely on a pre-
defined taxonomy and may suffer from the granu-
larity problem.
Content based methods: this category of
methods use traditional text presentation model
such as vector space model and language model
to express user preference. Rich content infor-
mation such as user search history, browser his-
tory and indexes of desktop documents are ex-
plored. The user profiles are built in the forms of
term vectors or term probability distributions. For
example, (Sugiyama and M., 2004) represented
user profiles as vectors of distinct terms and ac-
cumulated past preferences. (Teevan and Horvitz,
2005) constructed a rich user model based on both
search-related information, such as previously is-
sued queries, and other information such as doc-
1Open Directory Project, http://dmoz.org/
uments and emails a user had read and created.
(Shen and Zhai, 2005b) used browsing histories
and query sessions to construct short term indi-
vidual models for personalized search.
Learning to rank methods: (Eugene and Su-
san, 2005) and (Eugene and Zheng, 2006) incor-
porated user feedback into the ranking process in a
learning to rank framework. They leveraged mil-
lions of past user interaction with web search en-
gine to construct implicit feedback features. How-
ever, this approach aims to satisfy majority of
users rather than individuals.
2.2 Probabilistic Topic Models
Probabilistic topic models have become popular
tools for unsupervised analysis of document col-
lection. Topic models are based upon the idea
that documents are mixtures of topics, where
a topic is a probability distribution over words
(Steyvers and Griffiths, 2007). These topics are
interpretable to a certain degree. In fact, one of
the most important applications of topic models
is to find out semantic lexicons from a corpus.
One of the most popular topic models, the prob-
abilistic Latent Semantic Indexing Model (pLSI),
was introduced by Hofmann (Hofmann, 1999)
and quickly gained acceptance in a number of text
modeling applications. In this study, pLSI is used
to discover the underlying topics in user search
history. Though pLSI is argued that it is not a
complete generative model, we used it because it
does not need to generate unseen documents in
our case and the model is much easier to be es-
timated compared with sophisticated models such
as LDA(David M. Blei and Jordan, 2003).
2.3 Model based Relevance Feedback
Our work is also related to language model based
(pseudo) relevance feedback (Zhai and Lafferty,
2001b) and shares the similar idea with (Tan B.
and Zhai, 2007). The differences are: (1) The
feedback source is user search history rather than
top ranked documents for a query. (2) We make
use of user implicit feedback rather than explicit
feedback. (3) The topics in search history could
be extracted offline and updated periodically. Ad-
ditionally, these topics provide an informative pic-
ture of user search history.
1168
Table 1: An illustration of topics extracted from a
user?s search history. Terms with highest proba-
bilities are listed below each topic.
Topic 2 Topic 3 Topic 9 Topic 16
climb movie swim cup
0.032 0.091 0.044 0.027
setup download ticket world
0.022 0.078 0.032 0.022
equipment dvd notice team
0.020 0.061 0.019 0.016
practice watch travel brazil
0.009 0.060 0.016 0.011
player cinema hotel storm
0.006 0.038 0.008 0.007
3 Proposed Approach
3.1 Main Idea
A user?s search history usually covers multiple
topics. It is crucial to distinguish between rele-
vant and irrelevant parts for optimizing personal-
ization. We propose a topic model based method
to achieve that goal. First, we construct a doc-
ument collection revealing user intents according
to the user?s past activities. A probabilistic topic
model is applied on this collection to extract la-
tent topics. Then the extracted topics are used as
feedback. The query model is updated by high-
lighting the topics highly relevant to current query.
Finally, the search results are reranked according
to the relevance to the updated query model. Ta-
ble 1 shows 4 topics extracted from a user?s search
history. Each topic is a unigram language model.
The terms with higher probabilities belonging to
each topic are listed. We can predict that the user
has interests in both movie and football. However,
when the user submits a query about world cup,
the topic 16 is given higher preference for esti-
mating a more accurate query model.
3.2 Topic Extraction from Search History
Individual?s search history consists of all the past
query units. Each query unit includes query text,
returned search results (with title, snippets and
URLs) and click through information. Here, we
concatenated the title and snippet of each search
result to form a document being considered as a
whole. The whole search history can be seen as a
collection of documents. Obviously, many doc-
uments in the collection may fail to satisfy the
user?s information need and are uncertain for dis-
covering the user?s preferences. Therefore, the
first task is to select proper documents in search
history as the preference collection for topic dis-
covery.
3.2.1 Preference Collection
An intuitive solution is to use the documents
that are clicked by the user. The assumption is
that a user clicks on a result only if he is interested
in the document. However, user click is sparse in
real search environments and the documents not
clicked by the user may also be relevant to the
user?s information need. We assumed that the user
had only one search intent for a submitted query.
To enhance this coherence within a query unit, we
created only one super-document for a query unit
as follows: if a query unit had clicked documents,
then we concatenated these document to form a
preferred document. Otherwise, we selected the
top n documents from the search results and con-
catenated them as a preferred document. That is
motivated by the idea of pseudo relevance feed-
back (Lavrenko and Croft, 2001) and used here for
alleviating data sparsity. Pseudo relevance feed-
back is sensitive to the number of feedback docu-
ments. In this work, n is set to 3, because the aver-
age clicks for a query is not more than 3. By this
way, we got a preference collection whose size is
the same as the number of past queries.
3.2.2 Topic Extraction
Given the collection of preferred documents,
we applied pLSI on this collection to extract
underlying topics. We define the collection as
C={d1,d2,. . . ,dM}, where di corresponds to the
ith query unit, and M is the size of the collection.
Each query unit is viewed as a mixture of differ-
ent topics. It is reasonable in reality. For exam-
ple, a news document about ?play basketball with
obama? might be seen as a mixture of topics ?pol-
itics? and ?sports?.
Modeling: The basic idea of pLSI is to treat
the words in each document as being generated
from a mixture model where the component mod-
els are topic word distributions. Let k be the num-
1169
ber of topics which is assumed known and fixed.
?j is the word distribution for topic j. We extract
topics from collection C using a simple proba-
bilistic mixture model as described in (Zhai and
Yu, 2004). A word w within document d can be
viewed as generated from a mixture model:
pd(w) = ?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)
(1)
where ?B is the background model for all the doc-
uments. The background model is used to draw
common words across all the documents and lead
to more discriminative and informative topic mod-
els, since ?B gives high weights to non-topical
words. ?B is the probability that a term is gen-
erated from the background model which is set to
be a constant. To draw more discriminative topic
models, we set ?B to 0.95. Parameter pid,j indi-
cates the probability that topic j is assigned to the
specific document d, where?kj=1 pid,j=1.Parameter estimation: The parameters we
have to estimate including the background model
?B , {?j} and {pid,j}. ?B is maximum likelihood
estimated (MLE) using all available text in our
data set so that it is a fixed distribution. The other
parameters to be estimated are {?j} and {pid,j}.
The log-likelihood of document d is:
log p(d) =
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(2)
The log-likelihood of the whole collection C is:
log(C) =
?
d?C
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(3)
The Expectation-Maximization (EM) algorithm
(Dempster and Rubin, 1977) is used to find a
group of parameters maximizing equation (3).
The updating formulas are:
,
,
1
( ) ( )
,
,
( ) ( )
, ' '
' 1
, ,( 1)
,
, ,
' 1
(
E-Step:
( | )
( )
( | ) (1 ) ( | )
( | )
( )
( | )
M-Step:
( , )(1 ( )) ( )
( , )(1 ( )) ( ')
B B
d w k
B B B d j j
j
m m
d j j
d w k
m m
d j j
j
d w d wm w V
d j k
d w d ww V
j
p w
p z B
p w p w
p w
p z j
p w
c w d p z B p z j
c w d p z B p z j
p
? ?
? ? ? ? ?
? ?
? ?
?
=
=
+ ?
?=
= =
+ ?
= =
? = ==
? = =
?
?
?
??
, ,
1)
, ' , '
'
( , )(1 ( )) ( )
( | )
( ', )(1 ( )) ( )
d w d w
m d C
j
d w d w
d C w V
c w d p z B p z j
w
c w d p z B p z j
?+ ?
? ?
? = =
= ? = =
?
? ?
 
where c(w, d) denotes the number of times w
occurs in d. A hidden variable zd,w is introduced
for the identity of each word. p(zd,w = B) is
the probability that the word w in document d is
generated by the background model. p(zd,w = j)
denotes the probability that the word w in docu-
ment d is generated using topic j given that w is
not generated from the background model. Infor-
mally, the EM algorithm starts with randomly as-
signing values to the parameters to be estimated
and then alternates between E-Step and M-Step
iteratively until it yields a local maximum of the
log likelihood.
Interpretation: As shown in equation (1), a
word can be viewed as a mixture of topics. From
the updating formulas, we can see that the domi-
nant topic of a word depends on both itself and the
context. The word tends to have the same topic
with the document containing it. While the prob-
ability of assigning topic j to document d is es-
timated by aggregating all the fractions of words
generated by topic j in document d. We can ex-
plain it in a more intuitive way with in our applica-
tion. As we know, the queries are usually ambigu-
ous. A classic example is ?apple? which may re-
fer to a kind of fruit, apple Inc, apple electric prod-
ucts, etc. Therefore, it is reasonable to assume
that each word belongs to multiple latent seman-
tic properties. If a returned result contains ?ap-
ple? and other words like ?computer?, ?ipod? ,
etc. The word ?apple? in this result tends to have
the same topic distributions with ?computer? and
1170
?ipod?. If the user clicks the result, we can predict
that the user?s real preference about query ?ap-
ple? is related to electric products having a high
probability. Further, if ?apple? occurs frequently
in many documents related to electric products,
it obtains a higher probability in this topic. As
a result, we not only know user?s interest in elec-
tric products, but also find a preference to ?apple?
brand.
Since a document?s topic depends on the words
it contains, two documents with similar word dis-
tributions have similar topic distributions. In other
words, each topic is like a bridge connecting
queries with similar intents. In summary, the topic
extraction process plays a role in our application
for finding user preference, highlighting discrimi-
native words and connecting queries with similar
intents.
3.3 Topics as Feedback
The topics extracted from search history are con-
sidered as a kind of feedback. Since topic mod-
els actually are extensions of language models,
we use such feedback within the KL-Divergence
retrieval model (Xu and Croft, 1999)(Zhai and
Lafferty, 2001b) that is a principled framework
to model feedback in the language modeling ap-
proach. In this framework, feedback is treated as
updating the query language model based on extra
evidence obtained from the feedback sources. The
information retrieval task is to rank documents ac-
cording to the KL divergence D(?q||?d) between
a query language model ?q and a document lan-
guage model ?d. The KL divergence is defined as:
D(?q||?d) =
?
w?V
p(w|?q) log
p(w|?q)
p(w|?d)
(4)
where V denotes the vocabulary. We estimate
the document model ?d using Dirichlet estimation
(Zhai and Lafferty, 2001a):
p(w|?d) =
c(w, d) + ?p(w|?C)
|d| + ? (5)
where |d| is document length, p(w|?C) is collec-
tion language model which is estimated using the
whole data collection. ? is the Dirichlet prior that
is set to 20 in this work. The updated query model
is defined as:
p(w|?q) = ?pml(w|?q)
+(1 ? ?)
k?
j=1
p(w|?j)p(z = j|q)
(6)
where pml(w|?q) is the MLE query model. {?j}
represents a set of extracted topics each of which
is a unigram language model. ? is used to bal-
ance the two components. z is a hidden variable
over topics. The task is to estimate the multino-
mial topic distribution p(z|q) for query q. Since
pLSI does not properly provide a prior, we esti-
mate p(z = j|q) as:
p(z = j|q) = p(q, z = j)?k
j?=1 p(q, z = j?)
? sim(?q, ?j)?k
j?=1 sim(?q, ?j?)
(7)
Since the query text is usually very short, it is
not easy to make a decision based on query text
alone. Instead, we concatenate all the available
documents in returned result set to form a super-
document. A language model is estimated for it.
We convert both the document language model
and topic models into weighted term vectors and
use cosine similarity as the sim function. p(z|q)
plays an import role here as it determines the con-
tribution of topics. The topics with higher similar-
ity with current query contributes more in updat-
ing query model. This scheme helps to filter out
noisy information in search history.
4 Evaluation and Discussion
4.1 Data Collection
To the best of our knowledge, there is no public
collection with enough content information and
user implicit feedback. We decided to carry out
a data collection. Due to the difficulty to de-
scribe and evaluate user interests implicitly, we
predefined some user interests and implemented
a search system to collect user interactions.
The predefined interests belong to 5 big cate-
gories namely Entertainment, Computer & Inter-
net, Sports, Health and Social life. Each inter-
est is a kind of user preference such as ?movies?
1171
Table 2: An example of predefined user interests
and tasks
category Enterntainment
interest movies
task1 search for a brief introductionof your favorite movie
task2 search for an introduction ofan actor or actress you like
task3 search for movies about?artificial intelligence?
Table 3: Statistics of the data collection
user 1 2 3 4 5
#queries 218 256 177 206 311
#big category 5 5 5 5 5
#interest 25 25 25 25 25
#tasks 100 100 100 100 100
avg.#relevant 4.17 4.22 3.89 4.12 3.24results
avg.#clicked 2.37 2.21 2.71 1.98 2.42results
and ?outdoor sports?. For each interest, we de-
signed several tasks each of which had a goal. Ta-
ble 2 illustrates an example of a predefined user
interest and related tasks. The volunteers were
asked to find out the information need according
to the tasks. Though we defined these interests
and tasks, we did not impose any constraint on
the queries. The volunteers could choose and re-
formulate any query they thought good for find-
ing the desired information. But we did try to in-
crease the possibility that a user might issue am-
biguous queries by designing tasks like ?search
for movies about artificial intelligence? which was
categorized to interest ?movies?, but also related
to computer science.
To collect the user interaction with search en-
gine, we implemented a Lucene based search sys-
tem on Tianwang terabyte corpus(Yan and Peng,
2005). Five volunteers were asked to submit
queries to this system to find information satisfy-
ing the tasks of each interest. The system recorded
users? activities including submitted queries, re-
turned search results (with title, snippet and URL)
and users? click through information. When the
user finished a task, he clicked a button to tell the
system termination of the session containing all
the queries and activities related to this task. After
finishing all the tasks, the volunteers were asked to
judge the top 20 results? relevance (relevant or not
relevant) for each query according to the search
target. Each volunteer submitted 233 queries on
average. Table 3 presents some statistics of this
collection.
4.2 Evaluating Topic Extraction
It is not easy to assess the quality of topics, be-
cause topic extraction is an unsupervised process
and difficult to give a standard answer. Therefore,
we view the topic extraction as a clustering prob-
lem that is to organize queries into clusters. To
group queries into clusters through extracted top-
ics, we use j? = argmax
j
pid,j to assign a query to
the j?th topic. Each topic corresponds to a cluster.
All the queries are divided into k clusters. Based
on the data collection, we setup the golden an-
swers according to the predefined interests. We
view all the queries belonging to a predefined in-
terest(which includes multiple tasks) form a clus-
ter which helps us to build a golden answer with
25 clusters in tatal.
One purpose of making use of topics in search
history is to find more relevant parts and reduce
the noise. We hope that the extracted topics are
coherent. That is, a cluster should contain as many
queries as possible belonging to a single inter-
est. To evaluate coherence, we adopt purity (Zhao
and Karypis, 2001), a commonly used metric for
evaluating clustering. The higher the purity is,
the better the system performs. We compare our
method (denoted as PLSI) against the k-means al-
gorithm(denoted as K-Means) on the preference
collection.
Figure 1 shows the overall purity with differ-
ent number of topics. Our method gained better
performance than k-means algorithm consistently.
It is effective to discover and organize user inter-
ests. Besides, as illustrated in Table 1, our method
is able to give higher probability to discriminative
words of each topic that provides a clear picture
of user search history. This leads to an emergence
of novel approaches for personalized browsing.
1172
0.2
0.3
0.4
0.5
0.6
0.7
0.8
10 20 30 40 50 60 70 80 90 100
Number of topics
Pu
ri
ty
PLSI K-Means
 
Figure 1: Average purity over 5 users gained by
both PLSI and K-Means with different number of
topics(clusters).
4.3 Evaluating Result Reranking
4.3.1 Metric
To quantify the ranking quality, the Dis-
counted Cumulative Gain (DCG) (Jarvelin and
Kekakainen, 2000) is used. DCG is a metric that
gives higher weights to highly ranked documents
and incorporates different relevance levels by giv-
ing them different gain values.
DCG(i) =
{
G(1), if i = 1
DCG(i? 1) + G(i)log(i) , otherwise
In our work, we use G(i) = 1 for the results la-
beled as relevant by a user and G(i) = 0 for the
results that are not relevant. The average normal-
ized DCG (NDCG)over all the test queries is se-
lected to show the performance.
4.3.2 Systems
We evaluated the performance of following sys-
tems:
PLSI: The proposed method. The history model
was a weighted interpolation over topics extracted
from the preference collection described in ses-
sion 3.2.1.
PSEUDO: From each query unit, we selected
top n documents as pseudo feedback. The lan-
guage history model was estimated on all these
documents.
PLSI-PSEUDO: Top n documents from each
query unit were concatenated to form a preferred
document. The history model was constructed
based on topics extracted from these preferred
documents.
HISTORY: The history language model was es-
timated based on all the documents in search his-
tory.
TB: It was based on(Tan and Zhai, 2006)which
built a unit language model for every past query
and the history model was a weighted interpola-
tion of past unit language models.
ORIGINAL: The default search system.
The first 5 systems provided schemes to smooth
the query model. They estimated the query mod-
els by utilizing different types of feedback (im-
plicit feedback or pseudo feedback) and weight-
ing methods (topic modeling or simple language
modeling). The updated query model was an in-
terpolation between MLE query model and his-
tory language model. The interpolation parameter
was set to 0.5, and n was set to 3.
4.3.3 Performance Comparison
To evaluate the performance on a test query, we
focus on two conditions:
1. the test query matches some past interests.
We want to check the ability of systems to
find relevant information from noisy data.
2. the test query does not match any of past in-
terests. We are interested in the robustness of
the systems.
For the first case, the users were asked to se-
lect at most 2 queries they submitted for each
task. These queries were used as test queries.
The other queries were used to simulate the users?
search history. In total we got 400 queries for
testing. Figure 2 demonstrates the performance
of these systems over all test queries. PLSI
outperformed all other systems consistently that
shows topic model based methods help to esti-
mate a more accurate query model and the user
implicit feedback is better evidence. The PLSI-
PSEUDO also performed well that indicates the
top documents is useful for revealing the topic
of queries, even though they do not satisfy user
need on occasion. TB also gained better perfor-
mance than PSEUDO and HISTORY. It indicates
1173
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 2: The overall average performance of sys-
tems, when each test query matches some user
past interests
highlighting relevant parts in search history helps
to improve the retrieval performance, when the
query matches some of user past interests. Com-
pared with default system, both HISTORY and
PSEUDO improved a lot which proves that the
context in search history is reliable feedback.
For the second case, each user was asked to
hold out 5 interests from his collection for test-
ing and the other interests were used as search
history. The users selected queries from the held
out interests as test queries. These queries did
not match each user?s past interests. We got 244
test queries. As figure 3 shows, though systems
still performed better against ORIGINAL, the im-
provements were not significant. PLSI still gained
the best performance. It has better ability to al-
leviate the effect of noise. HISTORY and PLSI
are more robust than PLSI-PSEUDOwhich seems
sensitive to the number of topics in this case.
In both cases, HISTORY gained moderate per-
formance but quite robust. It is still a very strong
baseline, though noisy information is not filtered
out. PLSI performed best in both cases. PLSI-
PSEUDO outperformed PSEUDO when the test
queries matched user past interests and gained
comparable results in second case. It shows that
modeling user search history as a mixture of top-
ics and weighting topics according to relevance
between topics and query help to update a better
query model. However, it is necessary to deter-
mine if a query matches past interests that helps
to optimize personalized search strategies.
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 3: The overall average performance of sys-
tems, when each test query does not match any
user past interest.
5 Conclusion and Future Work
In this paper, we have proposed a topic model
based method for personalized search. This ap-
proach has some advantages: first, it provides a
principled way to combine topic modeling and
personalized search; second, it is able to find user
preferences in an unsupervised way and gives an
informative summary of user search history; third,
it explores the underlying relationship between
different query units via topics that helps to filter
out the noise and improve ranking quality.
In future, we plan to do a large scale study by
leveraging the already built search system or busi-
ness search engines. Also, we will try to add more
information to extend the existing model. Besides,
it is necessary to design methods for determin-
ing whether a submitted query matches the user
past interests that is crucial to apply our algorithm
adaptively and selectively.
Acknowledgements
This research is supported by the National Nat-
ural Science Foundation of China under Grant
No. 60736044, by the National High Technol-
ogy Research and Development Program of China
No. 2008AA01Z144, by Key Laboratory Opening
Funding of MOE-Microsoft Key Laboratory of
Natural Language Processing and Speech, Harbin
Institute of Technology, HIT.KLOF.2009020. We
thank the anonymous reviewers and Fikadu
Gemechu for their useful comments and help.
1174
References
David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Dempster, A.P., Laird N.M. and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of Royal Statist. Soc. B,
39:1?38.
Dou, Z., Su R. and J. Wen. 2007. A large-scale evalu-
ation and analysis of personalized search strategies.
Proc. WWW, pages 581?590.
Eugene, A., Eric B. and D. Susan. 2005. Improving
web search ranking by incorporating user behavior
information. Proc.SIGIR, pages 19?26.
Eugene, A. and Zijian Zheng. 2006. Identifying best
bet web search results by mining past user behavior.
Proc.SIGKDD, pages 902?908.
Havelivala, T.H. 2002. Topic-sensitive pagerank.
Proc. WWW, pages 517?526.
Hofmann, T. 1999. Probabilistic latent semantic in-
dexing. Proc.SIGIR, pages 50?57.
Janse, B.J., Spink A. Bateman J. and T. Saracevic.
2000. Real life, real users, and real needs: a study
and analysis of user queries on the web. Information
Processing and Management, 26(2):207?222.
Jarvelin, K. and J. Kekakainen. 2000. Ir evaluation
methods for retrieving highly relevant documents.
Proc.SIGIR, pages 41?48.
Lavrenko, V. and W. Croft. 2001. Relevance based
language models. Proc.SIGIR, pages 120?127.
Pitkow, J., Schutze H. Cass T. Cooley R. Turnbull D.
Edmonds A. Adar E. and T. Breuel. 2002. Person-
alized search. Commun,ACM, 45(9):50?55.
Qiu, F. and J. Cho. 2006. Automatic identification of
user interest for personalized search. Proc.WWW,
pages 727?736.
Shen, X., Tan B. and C. Zhai. 2005a. Context-
sensitive information retrieval using implicit feed-
back. Proc. SIGIR, pages 43?50.
Shen, X., Tan B. and C. Zhai. 2005b. Implicit user
modeling for personalized search. Proc. CIKM,
pages 824?831.
Silverstein, C., Marais H. Henzinger M. and
M. Moricz. 1999. Analysis of a very large web
search engine query log. SIGIR Forum, 33(1):6?12.
Speretta, M. and S. Gauch. 2005. Personalized search
based on user search histories. Proc. WI?05, pages
622?628.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. Handbook of Latent Semantic Analysis.
Erlbaum, Hillsdale, NJ.
Sugiyama, K., Hatano K. and Yoshkawa. M. 2004.
Personalized search based on user search histories.
Proc. WWW, pages 675?684.
Tan, B., Shen X. and C. Zhai. 2006. Mining long-
term search history to improve search accuracy.
Proc.SIGKDD, pages 718?723.
Tan B., Atulya Velivelli, Fang H. and C. Zhai. 2007.
Term feedback for information retrieval with lan-
guage models. Proc.SIGIR, pages 263?270.
Teevan, J., Dumais S.T. and E. Horvitz. 2005. Per-
sonalizing search via automated analysis of interests
and activities. Proc.SIGKDD, pages 449?456.
Teevan, J., Morris M.R. Bush S. 2009. Discover-
ing and using groups to improve personalization.
Proc.WSDM, pages 15?24.
White, R.W., Bailey P. and L. Chen. 2009. Pre-
dicting user interest from contextual information.
Proc.SIGIR, pages 363?370.
Xu, Jinxi and W. Croft. 1999. Cluster-based language
models for distributed retrieval. Proc.SIGIR, pages
254?261.
Xu S., Bao, S. Fei B. Su Z. and Y. Yu. 2008. Exploring
folksonomy for personalized search. Proc.SIGIR,
pages 155?162.
Yan, H., Li J. Zhu j. and B. Peng. 2005. Tian-
wang search engine at trec 2005: Terabyte track.
Proc.TREC.
Zhai, C. and J. Lafferty. 2001a. A study of smooth-
ing methods for language models applied to ad hoc
information retrieval. Proc.SIGIR, pages 334?342.
Zhai, Chengxiang and John Lafferty. 2001b. Model-
based feedback in the language modeling approach
to information retrieval. Proc.CIKM, pages 403?
410.
Zhai, C., Velivelli A. and B. Yu. 2004. A cross-
collection mixture model for comparative text min-
ing. Proc.SIGKDD, pages 743?748.
Zhao, Y. and G. Karypis. 2001. Criterion functions
for document clustering: Experiments and analysis.
Technical Report TR #01?40, Department of Com-
puter Science, University of Minnesota, Minneapo-
lis, MN.
1175
Coling 2010: Poster Volume, pages 1203?1210,
Beijing, August 2010
Utilizing Variability of Time and Term Content, within and across 
Users in Session Detection 
Shuqi Sun1, Sheng Li1, Muyun Yang1, Haoliang Qi2, Tiejun Zhao1 
1Harbin Institute of Technology, 2Heilongjiang Institute of Technology 
{sqsun, ymy, tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
haoliang.qi@gmail.com 
Abstract 
In this paper, we describe a SVM classi-
fication framework of session detection 
task on both Chinese and English query 
logs. With eight features on the aspects 
of temporal and content information ex-
tracted from pairs of successive queries, 
the classification models achieve signifi-
cantly superior performance than the stat-
of-the-art method. Additionally, we find 
through ROC analysis that there exists 
great discrimination power variability 
among different features and within the 
same feature across different users. To 
fully utilize this variability, we build lo-
cal models for individual users and com-
bine their predictions with those from the 
global model. Experiments show that the 
local models do make significant im-
provements to the global model, although 
the amount is small. 
1 Introduction 
To provide users better experiences of search 
engines, inspecting users? activities and inferring 
users? interests are indispensible. Query logs rec-
orded by search engines serves well for these 
purposes. Query log conveys the user interest 
information in the form of slices of the query 
stream. Thus the task of session detection con-
sists in distinguishing slice that corresponds to a 
user interest from other ones, and thus this paper, 
we adopt the definition of a session following 
(Jansen et al, 2007): 
(A session is) a series of interactions by the us-
er toward addressing a single information need. 
This definition is equivalent to that of the 
?search goal? proposed by Jones and Klinkner 
(2008), which corresponds to an atomic infor-
mation need, resulting in one or more queries.  
This paper adopts a classification point of 
view to the task of session detection (Jones and 
Klinkner, 2008). Given a pair of successive que-
ries in a query log, we examine it in various 
viewpoints (i.e. features) such as time proximity 
and similarity of the content of the two queries to 
determine whether these two queries cross a bor-
der of a search session. In other words, we classi-
fy the gap between the two queries into two clas-
ses: session shift and session continuation. In 
practice, search goals in a search mission and 
different search missions could be intermingled, 
and increase the difficulty of correctly identify-
ing them. In this paper, we do not take this issue 
into account and simply treat all boundaries be-
tween intermingled search goals as session shifts. 
The chief advantage in this choice is that we will 
have the opportunity to make classification mod-
el working online without caching user?s queries 
that are pending to be assigned to a session. 
Various studies built accurate models in pre-
dicting session boundaries and in distinguishing 
intermingled sessions, and they are summarized 
in Section 2. However, none of these works ana-
lyzed the contribution of individual features from 
a user-oriented viewpoint, or evaluated a fea-
ture?s discrimination power in a general scenario 
independent of its usage, as this paper does by 
conducting ROC analyses. During these analyses, 
we found that the discrimination power of fea-
tures varies dramatically, and for different users, 
the discrimination power of a particular feature 
also does not remain constant.  
Thus, it is appealing to build local models for 
users with have sufficient size of training exam-
ples, and combine the local models? predictions 
with those made by the global model trained by 
the whole training data. However, few of previ-
1203
ous works build user-specific models for the sake 
of characterizing the variability in user?s search 
activities, except that of Murray et al (2006). To 
fully make use of these two aspects of variability, 
inspired by Murray et al, we build users? local 
models based on a much broader range of evi-
dences, and show that different local models vary 
to a great extent, and experiments show that the 
local models do make significant improvements 
to the global model, although the amount is small. 
The remainder of this paper is organized as 
follows: Section 2 summarizes the related work 
of the session detection task. In Section 3, we 
first describe our classification framework as 
well as the features utilized. Then we conduct 
various evaluations on both English and Chinese 
query logs. Section 4 introduces the approaches 
to building local models based on an analysis of 
the variability of the discrimination power of 
features, and combine predictions of local mod-
els with those of the global model. Section 5 dis-
cusses the experimental results and concludes 
this paper. 
2 Related Work 
The simplest method in session detection is 
defining a timeout threshold and marking any 
time gaps of successive queries that exceed the 
threshold as session shifts. The thresholds 
adopted in different studies were significantly 
different, ranging from 5 minutes to 30 minutes 
(Silverstein et al, 1999; He and G?ker, 2000; 
Radlinski and Joachims, 2005; Downey et al, 
2007). Other study suggested adopting a dynamic 
timeout threshold. Murray et al (2006) proposed 
a user-centered hierarchical agglomerative 
clustering algorithm to determine timeout 
threshold for each user dynamically, other than 
setting a fixed threshold. However, Jones and 
Klinkner (2008) pointed out that single timeout 
criterion is always of limited utility, whatever its 
length is, and incorporating timeout features with 
other various features achieved satisfactory 
classification accuracy.  
An effective approach to combining the time 
out features with various evidences for session 
detection is machine learning. He et al (2002) 
collected statistical information from human an-
notated query logs to predict the probability a 
?New? pattern indicates a session shift according 
to the time gap between successive queries. 
?zmutlu and colleagues re-examined He et al?s 
work, and explored other machine learning tech-
niques such as neural networks, multiple linear 
regression, Monte Carlo simulation, conditional 
probabilities (Gayo-Avello, 2009), and HMMs 
(?zmutlu, 2009). 
In recent studies, Jones and Klinkner (2008) 
built logistic regression models to identify search 
goals and missions, and tackled the intermingled 
search goal/mission issue by examining arbitrary 
pairs of queries in the query log. Another contri-
bution of Jones and Klinkner is that they made a 
thorough analysis of contributions of individual 
features. However, they explored the features? 
contributions from a feature selection point of 
view rather than from a user-oriented one, and 
thus failed to characterize the variability of the 
discrimination power of the features when ap-
plied to different users. 
3 Learning to Detect Session Shifts 
3.1 Feature Extraction 
We adopt eight features covering both the tem-
poral and the content aspect of pairs of succes-
sive queries. Most these features are commonly 
used by previous studies (He and G?ker, 2000; 
?zmutlu, 2006; Jones and Klinkner, 2008). 
However, in this paper, we will analyze their 
contributions to the resulted model in a quite dif-
ferent way from that in previous works. 
Let Q = (q1, q2, ? , qn) denote a query log.  
The features are extracted from every successive 
pair of queries (qi, qi+1). Table 1 summarizes the 
features we adopt. The normalization described 
in Table1 is done according to the type of the 
feature. Features describing characters are nor-
malized by the average length of the two queries, 
while those describing character-n-grams are 
normalized by the average size of the n-gram sets 
of the two queries. Character-n-grams (e.g. bi-
grams ?ca? and ?at? in ?cat?) are robust to dif-
ferent representations of the same topic (e.g. ?IR? 
as Information Retrieval) and typos (e.g. 
?speling? as ?spelling?), and serve as a simple 
stemming method. In practice, character-n-grams 
are accumulative, which means they consist of 
all m-grams with m ? n. 
The feature ?avg_ngram_distance?, a variant 
of the ?lexical distance? in (Gayo-Avello, 2009), 
is more complicated than to be described briefly. 
1204
Here we first define n-gram distance (ND) from 
qi to qj, which is formalized as follows: 
j
ji
ji n
n
ND
qin   gram--char. of #
qin occur   qin   gram--char. of #
1)qq( ?=?  
Note that character-n-grams are accumulative 
and there could be multiple occurrences of a 
character-n-gram in a query, so the number of a 
character-n-gram is the sum of that of all m-
grams with m ? n, and multiple occurrences are 
all considered. At last, the average of character-
n-gram distance (ACD) of the pair (qi, qi+1) is:  
2
)qq()qq(
)q,q( 111
iiii
ii
NDND
ACD
?+?
=
++
+
 
There are seven features describing the content 
aspect of a query pair, and they are more or less 
overlapped (e.g. edit_distance vs. common_char). 
However, we show in the next subsection that all 
these features are beneficial to the final perfor-
mance.  
Feature Description 
time_interval time interval between 
successive queries 
avg_ngram_ 
distance 
avg. of character-n-gram 
distances 
edit_disance normalized Levenshtein 
edit distance 
common_prefix normalized length of pre-
fix shared 
common_suffix normalized length of suf-
fix shared 
common_char normalized number of 
characters shared 
common_ngram normalized number of 
character-n-grams shared 
Jaccard_ngram Jaccard distance between 
character-n-gram sets 
Table 1. Features used in classification models 
3.2 Data Preparation 
The query logs we explored include an English 
search log tracked by AOL from Mar 1, 2006 to 
May, 31 2006 (Pass et al, 2006), and a Chinese 
search log tracked by Sogou.com, which is one 
of the major Chinese Search Engines, from Mar 
1, 2007 to Mar 31, 20071. We applied systematic 
sampling over the user space on the two logs, 
which yielded 223 users and 2809 users, corre-
sponding to 6407 and 6917 query instances re-
                                                 
1 http://www.sogou.com/labs/resources.html 
spectively2. Sampling over the user space instead 
of over the query space avoids the bias to the 
most active users who submit much more queries 
than average users. 
For each sampled dataset, we invited annota-
tors who are familiar with IR and search process 
to determine each pair of successive queries of 
interest is across the border of a session. We 
made trivial pre-split process under two rules: 
 Queries from different users are not in the 
same session. 
 Queries from different days are not in the 
same session.  
Table 2 shows some basic statistics of the an-
notated data set. During the annotation process, 
the annotators were guided to identify the user?s 
information need at the finest granularity ever 
possible, because we focus on the atomic infor-
mation needs as described in Section 1. Conse-
quently, the average numbers of queries in a ses-
sion in both query logs are lower than previous 
studies. 
 AOL log Sogou log 
Queries 6407 6917 
Sessions 4571 5726 
Queries per session 1.40 1.21 
Longest session 21 12 
Table 2. Summary of the annotation results in 
both query logs 
3.3 Learning Framework 
In this section we seek to build accurate global 
classification model based on the whole training 
data obtained in the previous sub-subsection for 
both the query logs. We built the models within 
SVM framework. The implementation of SVM 
we used is libSVM (Chang and Lin, 2001). For 
the sake of evaluations and of model integration 
in the next section, we set the prediction of SVM 
to be probability estimation of the test example 
being positive. All features were pre-scaled into 
[0, 1] interval. We adopted the polynomial kernel, 
and for both datasets, we exhaustively tried each 
of the subset of the eight features using 5-fold 
cross validation. We found that using all the 
eight features yielded the best classification ac-
curacy. Thus in the experiments in rest of this 
                                                 
2 The sampling schema and sample size was deter-
mined following (Gayo-Avello, 2009). 
1205
section and the next section, we adopt the entire 
feature set to build global classification models. 
There is one parameter to be determined for 
feature extraction: the length of character-n-
grams. The proper lengths on AOL log and 
Sogou log are different. We tried the length from 
1 to 9, and according to cross validation accuracy, 
we found the best lengths for the two logs as 6 
and 3 respectively. 
3.4 Experimental Results 
3.4.1 Baseline Methods 
We provide two base line methods for compari-
sons. The first method is the commonly used 
timeout methods. We tried different timeout 
thresholds from 5 minutes to 30 minutes with a 
step of 5 minutes, and found that for both query 
logs the 5 minutes? threshold yield the best over-
all performance.  
The second method achieved the best perfor-
mance on the AOL log (Gayo-Avello, 2009), 
which addresses the session detection problem 
using a geometric interpolation method, in com-
parison to previous studies on this query log. We 
re-implemented this method and evaluated it on 
both the datasets. Similarly, the best parameters 
for the two query logs are different, such as the 
length of a character-n-gram. We only report the 
performance with the best parameter settings. 
3.4.2 Analyzing the Performance  
We analyze the performance of the SVM models 
according to precision, recall, F1-mean and F1.5-
mean of predictions on session shift and continu-
ation against human annotation data. 
The F

-mean is defined as: 
RP
PR
+
+
=
2
2)1(
mean-F ?
?
?  
where P denotes precision and R denotes recall. 
He et al (2002) regards recall more important 
than precision, and set the value of   in F

-mean 
to 1.5. We also report performance under this 
measure. 
In addition to traditional precision / recall 
based measures, we also perform ROC (Receiver 
Operating Characteristic) analysis to determine 
the discrimination power of different methods. 
The best merit of ROC analysis is that given a 
reference set, which is usually the human annota-
tion results, it evaluates a set of indicator?s dis-
crimination power for arbitrary binary classifica-
tion problem independent of the critical value 
with which the class predictions are made.  
Specifically, in the context session detection, 
regardless of the critical value that splits the clas-
sifier outputs into positive ones and negative 
ones (e.g. the 5-minutes? timeout threshold and 
50% probability in SVM?s output), the ROC 
analysis provides the overall discrimination pow-
er evaluation of the output set of a certain meth-
od (by trying to set each output value as the criti-
cal value). For the baseline method by Gayo-
Avello, the core of the decision heuristics also 
had a critical value to be determined. For details, 
readers could refer to (Gayo-Avello, 2009).  
3.4.3 Precision, Recall, and F-means 
Before we examine the discrimination power of 
each session detection method?s output independ-
ent of the threshold value selected. In this sub-
subsection, we begin with a more traditional eval-
uation schema: setting a proper threshold to pro-
duce binary predictions. It is straightforward to set 
the threshold for SVM method to 50%, and as 
described in sub-subsection 3.1.1, the threshold 
for timeout method is 5 minutes. The threshold of 
Gayo-Avello?s method is implied in its heuristics. 
Table 3 and Table 4 show the experimental re-
sults on AOL log and Sogou log respectively. 
For each dataset, we performed 1000-times boot-
strap resampling, generating 1000 bootstrapped 
datasets with the same size as the original dataset. 
To test the statistical significance of performance 
differences, we adopted Wilcoxon signed-rank 
test on the performance measures computed from 
the 1000 bootstrapped dataset, and found com-
parisons between each pair of methods were all 
significant at 95% level. 
The results show that SVM method clearly 
outperforms the baseline methods, and timeout 
method performs poorly. It may be argued that 
the poor performance of timeout method is due 
to the improper threshold value chosen. In this 
case, the ROC analysis, which assesses the dis-
crimination power of a method?s output set inde-
pendent of the threshold value chosen, is more 
suitable for performance evaluation. 
Gayo-Avello method significantly outperforms 
the timeout method. But due to its heuristic na-
ture, it is less likely to do better than the super-
vised-learning methods, although it avoids the 
over fitting issue. The Gayo-Avello method?s 
unstable performance in predicting session con-
1206
tinuations implies that its heuristics did not gen-
eralize well to Chinese query logs. 
 Timeout Gayo-Avello SVM 
P 
shift 75.92 89.35 90.96 
cont. 63.05 85.32 92.06 
R 
shift 64.49 87.85 93.82 
cont. 74.77 87.08 88.50 
F1 
shift 69.74 88.60 92.37 
cont. 68.41 86.19 90.25 
F1.5 
shift 67.62 88.31 92.92 
cont. 70.72 86.53 89.57 
Table 3. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on AOL dataset.  
 Timeout Gayo-Avello SVM 
P 
shift 67.75 75.10 87.53 
cont. 52.82 83.51 81.62 
R 
shift 59.52 91.44 86.17 
cont. 61.53 58.84 83.33 
F1 
shift 63.37 82.47 86.85 
cont. 56.84 69.04 82.47 
F1.5 
shift 61.83 85.71 86.59 
cont. 58.56 64.72 82.80 
Table 4. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on Sogou dataset. 
3.4.4 ROC Analysis 
By setting certain threshold value, we analyzed 
the three method?s performance using precision / 
recall based measures. In this sub-subsection, we 
try to set each value in an output set as the 
threshold value, and evaluate the discrimination 
power of methods by the area under the ROC 
curve. 
Figure 1 shows the ROC curves of the SVM 
method and the two baseline methods: timeout 
and Gayo-Avello, for predicting session shifts. 
ROC curves for predicting session continuations 
are symmetric with respect to the reference line, 
so we omit them in the rest of this paper for the 
sake of space limit.  
The results show that SVM method clearly 
outperforms the baseline methods in the prospec-
tive of discrimination power, with ROC area 
0.9562 on AOL dataset and 0.9154 on Sogou 
dataset. The curves of the two baseline methods 
are clearly under that of SVM method. This 
means baseline methods can never achieve accu-
racy as high as SVM method w.r.t. a fixed false 
alarm (classification error) rate, nor false alarm 
rate as low as SVM method w.r.t. a fixed accura-
cy rate. Again, Gayo-Avello method significantly 
outperforms timeout method, while underper-
forms the SVM method. For the question in the 
previous sub-subsection, coinciding with previ-
ous studies (Murray et al, 2006; Jones and 
Klinkner, 2008), applying single timeout thresh-
old always yields limited discrimination power, 
wherever the operating point on ROC curve (i.e. 
threshold value) is set. 
4 Making Use of the Variability of Dis-
crimination Power 
In this section, we first analyze the amount of 
contribution that each feature makes and show 
that the contribution, i.e. the discrimination pow-
er of each feature varies dramatically across dif-
ferent users. Then, we propose an approach to 
making use of this variability. Finally through 
experimental results, we show that the proposed 
approach makes small, yet significant improve-
ments to the SVM method in Section 3. 
4.1 Variability of Discrimination Power 
The ROC analysis of individual feature provides 
adequate characterizations of the discrimination 
power of the feature. Another advantage of 
adopting ROC analysis is that the results are in-
dependent not only of the critical value, but also 
of the scale of the feature values.  
Figure 2 shows the ROC curves of all the eight 
features in both datasets. Note that some features 
are with a higher value indicating session contin-
uation rather than session shift, so their ROC 
curves are below the reference line. The feature 
?time_interval? behaves exactly the same as the 
timeout method in Figure 1. For the rest of the 
features, ?avg_ngram_distance?, ?common_ngram? 
and ?Jaccard_ngram? achieve the best discrimi-
nation powers, showing the character-n-gram 
representation is effective. The feature ?com-
mon_char? performs significantly better in 
Sogou dataset than in AOL dataset, because Chi-
nese characters convey much more information 
than English characters do. ?common_suffix? 
performing worse than ?common_prefix? reflects 
the custom of users. Users tend to add terms at 
the end of the query in a searching iteration, thus 
predicting session continuations by examining 
the common suffixes is problematic. 
1207
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.7707
Gayo-Avello ROC area: 0.9130
SVM ROC area: 0.9562
Reference
AOL
    
0.
00
0.
25
0.
50
1.
00
0.
75
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.6365
Gayo-Avello ROC area: 0.8463
SVM ROC area: 0.9154
Reference
Sogou
 
Figure 1. ROC analysis of SVM method and two baseline methods for predicting session shifts on 
both AOL and Sogou dataset. All comparisons between ROC areas within the same dataset are at 
least 95% statistically significant, because the corresponding confidence intervals do not overlap. 
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.7707
avg_ngram_distance ROC area: 0.9560
edit_disance ROC area: 0.8848
common_prefix ROC area: 0.2177
common_suffix ROC area: 0.2985
common_char ROC area: 0.1360
common_ngram ROC area: 0.0480
Jaccard_ngram ROC area: 0.0464
Reference
AOL
  
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.6365
avg_ngram_distance ROC area: 0.9108
edit_disance ROC area: 0.8333
common_prefix ROC area: 0.2449
common_suffix ROC area: 0.3745
common_char ROC area: 0.0922
common_ngram ROC area: 0.1018
Jaccard_ngram ROC area: 0.0965
Reference
Sogou
 
Figure 2. ROC analysis of individual features for predicting session shifts on both AOL and Sogou 
dataset. Note that some curves with similar ROC area values overlap each other. 
In spite of the discrimination power a feature 
has, its behavior on different users is worth-
while to be examined. For selecting users that 
have sufficient data to draw stable conclusions, 
we consider only users who issued more than 50 
queries in the datasets. Unfortunately, there are 
too few users (6 users) qualified in Sogou da-
taset, so we show only the statistics of ROC 
area values of each of the features in Table 5 
based on 37 users in AOL dataset. 
The statistics in Table 5 show that for differ-
ent users. Recall that in sub-subsection 3.3.2, a 
0.04 difference of ROC area make the perfor-
mance of the SVM method significantly better 
1208
than that of the Gayo-Avello?s method. Thus, 
the discrimination power of a feature is likely to 
vary significantly, because all the standard de-
viations are at 0.03 or even higher level. Espe-
cially, the minimum and maximum values show 
that for these users, some of the findings above 
from the whole dataset do not hold. This implies 
that it is likely more feasible to build specific 
local models for these users to make full use of 
the variability within the same feature. 
Feature avg. sdev. min. max. 
time_interval 0.780 0.088 0.476 0.912
avg_ngram_ 
distance 
0.954 0.034 0.861 1.000
edit_disance 0.883 0.056 0.733 0.990
common_prefix 0.224 0.069 0.099 0.327
common_suffix 0.299 0.113 0.064 0.578
common_char 0.143 0.082 0.037 0.493
common_ngram 0.051 0.037 0.000 0.187
Jaccard_ngram 0.049 0.036 0.000 0.173
Table 5. Average, standard deviation, minimum, 
and maximum ROC areas of individual features 
4.2 Building Local Models 
We built individual local models for each user 
that issued more than 50 queries in AOL dataset. 
We also performed 5-fold cross validations and 
set the prediction to be the probability estima-
tion of a test example being positive. The fea-
ture selection process showed again that all the 
eight features are beneficial, and none of them 
should be excluded. 
In each fold of cross validation, we per-
formed 90%-bagging on the training set 10 
times to get the variance estimations of the local 
model. For each example in the test set, we set 
the final output on it to be the average of the 10 
outputs, and recorded the standard deviation of 
the outputs on this example which is used dur-
ing the model combination. We also conducted 
the same process for the global model for the 
sake of combination process described below. 
4.3 Combing with the Global Model 
Since the predictions of both the local and the 
global models are probability estimations, it is 
reasonable to combine them using linear combi-
nation. For each example, there are two outputs 
Ol and Og coming from local and global models 
accordingly. For each example e of a user?s sub 
dataset U, we have the outputs Ol(e) and Og(e) 
as well as the normalized deviations Dl(e) and 
Dg(e) (by the largest deviation in U of the corre-
sponding models). The final output O(e) is de-
fined as: 
)()(
)()()()(
)(
eDeD
eOeDeOeD
eO
gl
glgl
+
?+?
=
 
 Global Local Combine 
P 
shift 90.48 88.53 90.43 
cont. 91.75 92.12 92.52 
R 
shift 93.94 94.44 94.56 
cont. 87.20 84.16 87.04 
F1 
shift 92.18 91.39 92.45 
cont. 89.41 87.96 89.69 
F1.5 
shift 92.85 92.54 93.25 
cont. 88.55 86.46 88.65 
Table 6. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of global model (bagging), 
local model (bagging) and combined model  
This combination process is similar to (Osl et 
al., 2008). Note that the more the deviation of a 
model is, the less feasible the corresponding 
model is. We compared the performance of 
three models: global model, local model, and 
combined model. The results are summarized in 
Table 6. All comparisons between different 
models are statistically significant at 95% level, 
based on the same bootstrapping settings in sub-
subsection 3.4.3. The combined model shows 
slight (may due to the inferior performance of 
the local model), yet significant improvement to 
the global model. In spite of the amount of the 
improvement, the local model did correct some 
errors of the global model. It may be not ac-
ceptable to build such an expensive combined 
model for a limited improvement. Nevertheless, 
the results do show that the variability across 
different users is exploitable. 
5 Discussion and Conclusion 
In this paper, we built a learning framework of 
detecting sessions which corresponds to user?s 
interest in a query log. We considered two as-
pect of a pair of successive queries: temporal 
aspect and content aspect, and designed eight 
features based on these two aspects, and the 
SVM models built with these features achieved 
satisfactory performance (92.37% F1-mean on 
session shift, 90.25% F1-mean on session con-
tinuation), significantly better than the best-ever 
approach on AOL query log. 
1209
The analysis of the features? discrimination 
power was conducted not only among different 
features, but also within the same feature when 
applied to different users in the query log. By 
analyzing the statistics of ROC area values of 
each of the features based on 37 users in AOL 
dataset, experimental results showed that there 
is considerable variability in both these aspects. 
To make full use of this variability, we built 
local models for individual user and combine 
the yielded predictions with those yielded by the 
global model. Experiments showed that the lo-
cal model did make significant improvements to 
the global model, although the amount was 
small (92.45% vs. 92.18% F1-mean on session 
shift, 89.69% vs. 89.41% F1-mean on session 
continuation). 
In future studies, we will explore other learn-
ing frameworks which better integrate the local 
model and the global model, and will try to ac-
quire more data to build local models. We will 
also analyze more deeply the characteristics of 
ROC analysis in the feature selection process.  
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project (Grant 
No.2006AA010108). The authors are grateful 
for the anonymous reviewers for their valuable 
comments. 
References 
Chang Chih-Chung and Chih-Jen Lin. 2001. 
LIBSVM : a library for support vector machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Downey Doug, Susan Dumais, and Eric Horvitz. 
2007. Models of searching and brows-
ing: languages, studies, and applications. In Pro-
ceedings of the 20th international joint conference 
on Artificial intelligence, pages 2740-2747, Hy-
derabad, India. 
Gayo-Avello Daniel. 2009. A survey on session de-
tection methods in query logs and a proposal for 
future evaluation, Information Science 
179(12):1822-1843. 
He Daqing and Ayse G?ker. 2000. Detecting Session 
Boundaries from Web User Logs. In BCS/IRSG 
22nd Annual Colloqui-um on Information Re-
trieval Research, pages 57-66.  
He Daqing, Ayse G?ke, and David J. Harper. 2002. 
Combining evidence for automatic web session 
identification. Information Processing and Man-
agement: an International Journal, 38(5):727-742. 
Jansen Bernard J., Amanda Spink, Chris Blakely, 
and Sherry Koshman. 2007. Defining a session on 
Web search engines: Research Articles. Journal of 
the American Society for Information Science and 
Technology, 58(6):862-871 
Jones Rosie and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical 
segmentation of search topics in query logs. In 
Proceedings of the 17th ACM conference on In-
formation and knowledge management, pages 
699-708, Napa Valley, California, USA. 
Murray G. Craig, Jimmy Lin, and Abdur Chowdhury. 
2007. Identification of user sessions with hierar-
chical agglomerative clustering. American Society 
for Information Science and Technology, 43(1):1-
9. 
Osl Melanie, Christian Baumgartner, Bernhard Tilg, 
and Stephan Dreiseitl. 2008. On the combination 
of logistic regression and local probability esti-
mates. In Proceedings of Third International Con-
ference on Broadband Communications, Infor-
mation Technology & Biomedical Applications, 
pages 124-128. 
?zmutlu Seda. 2006. Automatic new topic identifi-
cation using multiple linear regression. Infor-
mation Processing and Management: an Interna-
tional Journal, 42(4):934-950. 
?zmutlu Huseyin C. 2009. Markovian analysis for 
automatic new topic identification in search en-
gine transaction logs. Applied Stochastic Models 
in Business and Industry, 25(6):737-768. 
Pass Greg, Abdur Chowdhury, and Cayley Torgeson. 
2006. A picture of search. In Proceedings of the 
1st international conference on Scalable infor-
mation systems, Hong Kong. 
Radlinski Filip and Thorsten Joachims. 2005. Query 
chains: learning to rank from implicit feedback. In 
Proceedings of the eleventh ACM SIGKDD inter-
national conference on Knowledge discovery in 
data mining, pages 239-248, Chicago, Illinois, 
USA. 
Silverstein Craig, Hannes Marais, Monika Henzinger, 
and Michael Moricz. 1999. Analysis of a very 
large web search engine query log. ACM SIGIR 
Forum, 33(1):6-12. 
1210
Coling 2010: Poster Volume, pages 1533?1540,
Beijing, August 2010
All in Strings: a Powerful String-based Automatic MT  
Evaluation Metric with Multiple Granularities 
 
Junguo Zhu1, Muyun Yang1, Bo Wang2, Sheng Li1, Tiejun Zhao1 
 
1 School of Computer Science and Technology, Harbin Institute of Technology 
{jgzhu; ymy; tjzhao; lish}@mtlab.hit.edu.cn 
2 School of Computer Science and Technology, Tianjin University  
bo.wang.1979@gmail.com 
 
 
Abstract 
String-based metrics of automatic ma-
chine translation (MT) evaluation are 
widely applied in MT research. Mean-
while, some linguistic motivated me-
trics have been suggested to improve 
the string-based metrics in sentence-
level evaluation. In this work, we at-
tempt to change their original calcula-
tion units (granularities) of string-based 
metrics to generate new features. We 
then propose a powerful string-based 
automatic MT evaluation metric, com-
bining all the features with various 
granularities based on SVM rank and 
regression models. The experimental 
results show that i) the new features 
with various granularities can contri-
bute to the automatic evaluation of 
translation quality; ii) our proposed 
string-based metrics with multiple gra-
nularities based on SVM regression 
model can achieve higher correlations 
with human assessments than the state-
of-art  automatic metrics. 
1 Introduction 
The automatic machine translation (MT) eval-
uation has aroused much attention from MT 
researchers in the recent years, since the auto-
matic MT evaluation metrics can be applied to 
optimize MT systems in place of the expensive 
and time-consuming human assessments. The 
state-of-art strategy to automatic MT evalua-
tion metrics estimates the system output quali-
ty according to its similarity to human refer-
ences. To capture the language variability ex-
hibited by different reference translations, a 
tendency is to include deeper linguistic infor-
mation into machine learning based automatic 
MT evaluation metrics, such as syntactic and 
semantic information (Amig? et al, 2005; Al-
brecht and Hwa, 2007; Gim?nez and M?rquez, 
2008). Generally, such efforts may achieve 
higher correlation with human assessments by 
including more linguistic features. Neverthe-
less, the complex and variously presented lin-
guistic features often prevents the wide appli-
cation of the linguistic motivated metrics. 
Essentially, linguistic motivated metrics in-
troduce additional restrictions for accepting the 
outputs of translations (Amig? et al, 2009).  
With more linguistic features attributed, the 
model is actually capturing the sentence simi-
larity in a finer granularity. In this sense, the 
practical effect of employing various linguistic 
knowledge is changing the calculation units of 
the matching in the process of the automatic 
evaluation. 
Similarly, the classical string-based metrics 
can be changed in their calculation units direct-
ly. For example, the calculation granularity in 
BLEU (Papineni et al, 2002) metric is word: 
n-grams are extracted on the basis of single 
word as well as adjacent multiple words. And 
the calculation granularity in PosBLEU 
(Popovi? and Ney, 2009) metric is Pos tag, 
which correlate well with the human assess-
ments. Therefore, it is straight forward to apply 
the popular string-based automatic evaluation 
metrics, such as BLEU, to compute the scores 
of the systems outputs in the surface or linguis-
1533
tic tag sequences on various granularities le-
vels. 
In this paper, we attempt to change the orig-
inal calculation units (granularities) of string-
based metrics to generate new features. After 
that, we propose a powerful string-based au-
tomatic MT evaluation metric, combining all 
the features with various granularities based on 
SVM rank (Joachims, 2002) and regression 
(Drucker et al, 1996) models. Our analysis 
indicates that: i) the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality; ii) our pro-
posed string-based metrics with multiple gra-
nularities based on SVM regression model can 
achieve higher correlations with human as-
sessments than the state-of-art automatic me-
trics . 
The remainder of this paper is organized as 
follows: Section 2 reviews the related re-
searches on automatic MT evaluation. Section 
3 describes some new calculation granularities 
of string-based metrics on sentence level. In 
Section 4, we propose string-based metrics 
with multiple granularities based on SVM rank 
and regression models. In Section 5, we 
present our experimental results on different 
sets of data. And conclusions are drawn in the 
Section 6. 
2 Related Work on Automatic Ma-
chine Translation Evaluation 
The research on automatic string-based ma-
chine translation (MT) evaluation is targeted at 
a widely applicable metric of high consistency 
to the human assessments. WER (Nie?en et al, 
2000), PER (Tillmann et al, 1997), and TER 
(Snover et al, 2006) focuses on word error rate 
of translation output. GTM (Melamed et al, 
2003) and the variants of ROUGE (Lin and 
Och, 2004) concentrate on matched longest 
common substring and discontinuous substring 
of translation output according to the human 
references. BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) are both based on 
the number of common n-grams between the 
translation hypothesis and human reference 
translations of the same sentence. BLEU and 
NIST are widely adopted in the open MT eval-
uation campaigns; however, the NIST MT 
evaluation in 2005 indicates that they can even 
error in the system level (Le and Przybocki, 
2005). Callison-Burch et al (2006) detailed the 
deficits of the BLEU and other similar metrics, 
arguing that the simple surface similarity cal-
culation between the machines translations and 
the human translations suffers from morpho-
logical issues and fails to capture what are im-
portant for human assessments.  
In order to attack these problems, some me-
trics have been proposed to include more lin-
guistic information into the process of match-
ing, e.g., Meteor (Banerjee and Lavie, 2005) 
metric and MaxSim (Chan nad Ng, 2008) me-
trics, which improve the lexical level by the 
synonym dictionary or stemming technique. 
There are also substantial studies focusing on 
including deeper linguistic information in the 
metrics (Liu and Gildea, 2005; Owczarzak et 
al., 2006; Amig? et al, 2006; Mehay and Brew, 
2007; Gim?nez and M?rquez, 2007; Owczar-
zak et al, 2007; Popovic and Ney, 2007; 
Gim?nez and M?rquez, 2008b). 
A notable trend improving the string-based 
metric is to combine various deeper linguistic 
information via machine learning techniques in 
the metrics (Amig? et al, 2005; Albrecht and 
Hwa, 2007; Gim?nez and M?rquez, 2008). 
Such efforts are practically amount of intro-
ducing additional linguistic restrictions into the 
automatic evaluation metrics (Amig? et al 
2009), achiving a higher performance at the 
cost of lower adaptability to other languages 
owing to the language dependent linguistics 
features. 
Previous work shows that including the new 
features into the evaluation metrics may bene-
fit to describe nature language accurately. In 
this sense, the string-based metrics will be im-
proved, if the finer calculation granularities are 
introduced into the metrics.  
Our study analyzes the role of the calcula-
tion granularities in the performance of metrics. 
We find that the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality. Also we pro-
pose a powerful string based automatic MT 
evaluation metric with multiple granularities 
combined by SVM. Finally, we seek a finer 
feature set of metrics with multiple calculation 
granularities. 
1534
3 The New Calculation Granularities 
of String-based Metrics on Sentence 
Level  
The string-based metrics of automatic machine 
translation evaluation on sentence level adopt a 
common strategy: taking the sentences of the 
documents as plain strings. Therefore, when 
changing the calculation granularities of the 
string-based metrics we can simplify the in-
formation of new granularity with plain strings.  
In this work, five kinds of available calculation 
granularities are defined: ?Lexicon?, ?Letter?, 
?Pos?, ?Constitute? and ?Dependency?.  
Lexicon: The calculation granularity is 
common word in the sentences of the docu-
ments, which is popular practice at present. 
Letter: Split the granularities of ?Lexical? 
into letters. Each letter is taken as a matching 
unit. 
Pos: The Pos tag of each ?Lexicon? is taken 
as a matching unit in this calculation granulari-
ty. 
Constitute: Syntactic Constitutes in a tree 
structure are available through the parser tools. 
We use Stanford Parser (Klein and Manning, 
2003a; Klein and Manning, 2003b) in this 
work.  The Constitute tree is changed into 
plain string, travelling by BFS (Breadth-first 
search traversal) 1.  
Dependency: Dependency relations in a de-
pendency structure are also available through 
the parser tools. The dependency structure can 
also be formed in a tree, and the same 
processing of being changed into plain string is 
adopted as ?Constitute?. 
The following serves as an example:  
Sentence:  
I have a dog 
Pos tag:  
I/PRON have/V a/ART dog/N 
Constitute tree:  
 
                                                 
1 We also attempt some other traversal algorithms, in-
cluding preorder, inorder and postorder traversal, the 
performance are proved to be similar.  
Dependency tree:  
 
Then, we can change the sentence into the 
plain string in multiple calculation granulari-
ties as follows: 
Lexicon string:  
I have a dog 
Letter string:  
I h a v e a d o g 
Pos string: 
PRON V ART N 
Constitute string:  
PRON V ART N NP NP VP S 
Dependency string: 
 a I dog have 
The translation hypothesis and human refer-
ence translations are both changed into those 
strings of various calculation granularities. The 
strings are taken as inputs of the string-based 
automatic MT evaluation metrics. The outputs 
of each metric are calculated on different 
matching units. 
4 String-based Metrics with Multiple 
Granularities Combined by SVM 
Introducing machine learning methods to es-
tablished MT evaluation metric is a popular 
trend. Our study chooses rank and regression 
support vector machine (SVM) as the learning 
model. Features are important for the SVM 
models. 
Plenty of scores can be generated from the 
proposed metrics. In fact, not all these features 
are needed. Therefore, feature selection should 
be a necessary step to find a proper feature set 
and alleviate the language dependency by us-
ing fewer linguistic features. 
Feature selection is an NP-Complete prob-
lem; therefore, we adopt a greedy selection 
algorithm called ?Best One In? to find a local 
optimal feature set. Firstly, we select the fea-
ture among all the features which best corre-
lates with the human assessments.  Secondly, a 
feature among the rest features is added in to 
the feature set, if the correlation with the hu-
man assessments of the metric using new set is 
1535
the highest among all new metrics and higher 
than the previous metric in cross training cor-
pus. The cross training corpus is prepared by 
dividing the training corpus into five parts. 
Each four parts of the five are for training and 
the rest one for testing; then, we integrate 
scores of the five tests as scores of cross train-
ing corpus.  The five-fold cross training can 
help to overcome the overfitting. At the end, 
the feature selection stops, if adding any of the 
rest features cannot lead to higher correlation 
with human assessments than the current me-
tric.  
5 Experiments 
5.1 The Impact of the Calculation Granu-
larities on String-based Metrics 
In this section, we use the data from NIST 
Open MT 2006 evaluation (LDC2008E43), 
which is described in Table 1.  It consists of 
249 source sentences that were translated by 
four human translators as well as 8 MT sys-
tems. Each machine translated sentence was 
evaluated by human judges for their adequacy 
on a 7-point scale. 
 
 NIST 2002  
NIST 
2003  
NIST 
Open 
MT 2006
LDC 
corpus 
LDC2003
T17 
LDC2006
T04 
LDC2008
E43 
Type Newswire Newswire Newswire
Source Chinese Chinese Arabic 
Target English English English 
# of  
sentences 878 919 249 
# of 
systems 3 7 8 
#  of 
references 4 4 4 
Score 
1-5, 
adequacy 
& fluency
1-5, 
adequacy 
& fluency 
1-7 
adequacy
Table 1: Description of LDC2006T04, 
LDC2003T17 and LDC2008E43 
 
To judge the quality of a metric, we com-
pute Spearman rank-correlation coefficient, 
which is a real number ranging from -1 (indi-
cating perfect negative correlations) to +1 (in-
dicating perfect positive correlations), between 
the metric?s scores and the averaged human 
assessments on test sentences. 
We select 21 features in ?lexicon? calcula-
tion granularity and 11?4 in the other calcula-
tion granularities. We analyze the correlation 
with human assessments of the metrics in mul-
tiple calculation granularities.  Table 2 lists the 
optimal calculation granularity of the multiple 
metrics on sentence level in the data 
(LDC2008E43).  
 
Metric Granularity 
BLEU-opt Letter 
NIST-opt Letter 
GTM(e=1) Dependency 
TER Letter 
PER Lexicon 
WER Dependency 
ROUGE-opt Letter 
Table 2 The optimal calculation granularity of the 
multiple metrics 
 
The most remarkable aspect is that not all 
the best metrics are based on the ?lexicon? cal-
culation granularities, such as the ?letter? and 
?dependency?. In other words, the granulari-
ties-shifted string-based metrics are promising 
to contribute to the automatic evaluation of 
translation quality. 
5.2 Correlation with Human Assessments 
of String-based Metrics with Multiple 
Granularities Based on SVM Frame 
We firstly train the SVM rank and regression 
models on LDC2008E43 using all the features 
(21+11? 4 species), without any selection. 
Secondly, the other two SVM rank and regres-
sion models are trained on the same data using 
the feature set via feature selection, which are 
described in Table 3. We have four string-
based evaluation metrics with multiple granu-
larities on rank and regression SVM frame 
?Rank_All, Regression_All, Rank_Select and 
Regression_Select?.  Then we apply the four 
metrics to evaluate the sentences of the test 
data (LDC2006T04 and LDC2003T17). The 
results of Spearman correlation with human 
assessments is summarized in Table 3. For 
comparison, the results from some state-of-art 
metrics (Papineni et al, 2002; Doddington, 
1536
2002; Melamed et al, 2003; Banerjee and La-
vie, 2005; Snover et al, 2006; Liu and Gildea, 
2005) and two machine learning methods (Al-
brecht and Hwa, 2007; Ding Liu and Gildea, 
2007) are also included in Table 3. Of the two 
machine learning methods, both trained on the 
data LDC2006T04. The ?Albrecht, 2007? 
score reported a result of Spearman correlation 
with human assessments on the data 
LDC2003T17 using 53 features, while the 
?Ding Liu, 2007? score reported that under 
five-fold cross validation on the data 
LDC2006T04 using 31 features. 
 
 Feature number 
LDC
2003
T17 
LDC
2006
T04 
Rank_All 65 0.323 0.495
Regression_All 65 0.345 0.507
Rank_Select 16 0.338 0.491
Regression_Select 8 0.341 0.510
Albrecht, 2007 53 0.309 -- 
Ding Liu, 2007 31 -- 0.369
BLEU-opt2 -- 0.301 0.453
NIST-opt -- 0.219 0.417
GTM(e=1) -- 0.270 0.375
METEOR3 -- 0.277 0.463
TER -- -0.250 -0.302
STM-opt -- 0.205 0.226
HWCM-opt -- 0.304 0.377
 
Table 3: Comparison of Spearman correlations with 
human assessments of our proposed metrics and 
some start-of-art metrics and two machine learning 
methods 
?-opt? stands for the optimum values of the pa-
rameters on the metrics 
 
Table 3 shows that the string-based meta-
evaluation metrics with multiple granularities 
based on SVM frame gains the much higher 
Spearman correlation than other start-of-art 
metrics on the two test data and, furthermore, 
our proposed metrics also are higher than the 
machine learning metrics (Albrecht and Hwa, 
2007; Ding Liu and Gildea, 2007).  
The underlining is that our proposed metrics 
are more robust than the aforementioned two 
                                                 
2 The result is computed by mteval11b.pl.  
3 The result is computed by meteor-v0.7. 
machine learning metrics. As shown in Table 1 
the heterogeneity between the training and test 
data in our method is much more significant 
than that of the other two machine learning 
based methods.  
In addition, the ?Regression_Select? metric 
using only 8 features can achieve a high corre-
lation rate which is close to the metric pro-
posed in ?Albrecht, 2007? using 53 features, 
?Ding Liu, 2007? using 31 features, ?Regres-
sion_All? and ?Rank_All? metrics using  65 
features and ?Rank_Select? metric using 16 
features. What is more, ?Regression_Select? 
metric is better than ?Albrecht, 2007?, and 
slightly lower than ?Regression_All? on the 
data LDC2003T17; and better than both ?Re-
gression_All? and ?Rank_All? metrics on the 
data LDC2006T04. That confirms that a small 
cardinal of feature set can also result in a me-
tric having a high correlation with human as-
sessments, since some of the features represent 
the redundant information in different forms. 
Eliminating the redundant information is bene-
fit to reduce complexity of the parameter 
searching and thus improve the metrics per-
formance based on SVM models. Meanwhile, 
fewer features can relieve the language depen-
dency of the machine learning metrics. At last, 
our experimental results show that regression 
models perform better than rank models in the 
string-based metrics with multiple granularities 
based on SVM frame, since ?Regres-
sion_Select? and ?Regression_All? achieve 
higher correlations with human assessments 
than the others. 
5.3 Reliability of Feature Selection  
The motivation of feature selection is keeping 
the validity of the feature set and alleviating 
the language dependency. We also look for-
ward to the higher Spearman correlation on the 
test data with a small and proper feature set.  
We use SVM-Light (Joachims, 1999) to 
train our learning models using NIST Open 
MT 2006 evaluation data (LDC2008E43), and 
test on the two sets of data, NIST?s 2002 and 
2003 Chinese MT evaluations. All the data are 
described in Table 1. To avoid the bias in the 
distributions of the two judges? assessments in 
NIST?s 2002 and 2003 Chinese MT evalua-
tions, we normalize the scores following (Blatz 
et al, 2003). 
1537
We trace the process of the feature selection. 
The selected feature set of the metric based on 
SVM rank includes 16 features and that of the 
metric based on SVM regression includes 8 
features. The selected features are listed in Ta-
ble 4. The values in Table 4 are absolute 
Spearman correlations with human assess-
ments of each single feature score.  The prefix-
es ?C_?, ?D_?, ?L_?, ?P_?, and ?W_? 
represent ?Constitute?, ?Dependency?, ?Let-
ter?, ?Pos? and ?Lexicon? respectively. 
 
Rank spear-man Regression
spear-
man
C_PER .331 C_PER .331
C_ROUGE-W .562 C_ROUGE-W .562
D_NIST9 .479 D_NIST9 .479
D_ROUGE-W .679 D_ROUGE-L .667
L_BLEU6 .702 L_BLEU6 .702
L_NIST9 .691 L_NIST9 .691
L_ROUGE-W .634 L_ROUGE-W .634
P_PER .370 P_ROUGE-W .683
P_ROUGE-W .616  
W_BLEU1_ind .551  
W_BLEU2 .659  
W_GTM .360  
W_METEOR .693  
W_NIST5 .468  
W_ROUGE1 .642  
W_ROUGE-W .683  
 
Table 4: Feature sets of SVM rank and regression 
 
Table 4 shows that 8 features are selected 
from 65 features in the process of feature se-
lection based on SVM regression while 16 fea-
tures based on SVM rank. Fewer features 
based on SVM regression are selected than 
SVM rank. Only one feature in feature set 
based on SVM regression does not occur in 
that based on SVM rank. The reason is that 
there are more complementary advantages be-
tween the common selected features.  
Next, we will verify the reliability of our 
feature selection algorithm. Figure 1 and Fig-
ure 2 show the Spearman correlation values 
between our SVM-based metrics (regression 
and rank) and the human assessments on both 
training data (LDC2008E43) and test data 
(LDC2006T04 and LDC2003T17).  
 
 
 
Figure 1: The Spearman correlation values between 
our SVM rank metrics and the human assessments 
on both training data and test data with the exten-
sion of the feature sets 
 
 
 
Figure 2: The Spearman correlation values between 
our SVM regression metrics and the human as-
sessments on both training data and test data with 
the extension of the feature sets 
 
From Figure 1 and Figure 2, with the exten-
sion of the feature sets, we can find that the 
tendency of correlation obtained by each me-
tric based on SVM rank or regression roughly 
the same on both the training data and test data. 
Therefore, the two feature sets of SVM rank 
and regression models are reliable. 
6 Conclusion 
In this paper we propose an integrated platform 
for automatic MT evaluation by improving the 
string based metrics with multiple granularities. 
Our proposed metrics construct a novel inte-
grated platform for automatic MT evaluation 
based on multiple features. Our  key contribu-
tion consists of two parts: i) we suggest a strat-
egy  of changing the various complex features 
into plain string form. According to the strate-
gy, the automatic MT evaluation frame are 
1538
much more clarified, and the computation of 
the similarity is much more simple, since the 
various linguistic features may express in the 
uniform strings with multiple calculation gra-
nularities. The new features have the same 
form and are dimensionally homogeneous; 
therefore, the consistency of the features is 
enhanced strongly. ii) We integrate the features 
with machine learning and proposed an effec-
tive approach of feature selection. As a result, 
we can use fewer features but obtain the better 
performance. 
In this framework, on the one hand, string-
based metrics with multiple granularities may 
introduce more potential features into automat-
ic evaluation, with no necessarily of new simi-
larity measuring method, compared with the 
other metrics. On the other hand, we succeed 
in finding a finer and small feature set among 
the combinations of plentiful features, keeping 
or improving the performance. Finally, we 
proposed a simple, effective and robust string-
based automatic MT evaluation metric with 
multiple granularities. 
Our proposed metrics improve the flexibility 
and performance of the metrics based on the 
multiple features; however, it still has some 
drawbacks: i) some potential features are not 
yet considered, e.g. the semantic roles; and ii) 
the loss of information exists in the process of 
changing linguistic information into plain 
strings. For example, the dependency label in 
the calculation granularity ?Dependency? is 
lost when changing information into string 
form. Though the final results obtain the better 
performance than the other linguistic metrics, 
the performance is promising to be further im-
proved if the loss of information can be prop-
erly dealt with. 
Acknowledgement 
This work is supported by Natural Science 
foundation China (Grant No.60773066 & 
60736014) and National Hi-tech Program 
(Project No.2006AA010108), and the Natural 
Scientific Reserach Innovation Foundation in 
Harbin Institute of Technology (Grant No. 
HIT.NSFIR.20009070). 
 
References 
Albrecht S. Joshua and Rebecca Hwa. 2007. A 
Reexamination of Machine Learning Approaches 
for Sentence-Level MT Evaluation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 880-
887. 
Amig? Enrique, Julio Gonzalo, Anselmo P?nas, 
and Felisa Verdejo. 2005. QARLA: a Framework 
for the Evaluation of Automatic Summarization. 
In Proceedings of the 43th Annual Meeting of 
the Association for Computational Linguistics. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo,  
Felisa Verdejo. 2009. The Contribution of Lin-
guistic Features to Automatic Machine Transla-
tion Evaluation. In proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo, and 
Llu?s M?rquez. 2006. MT Evaluation: Human- 
Like vs. Human Acceptable. In Proceedings of 
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Lin-
guistic, pages 17?24. 
Banerjee Satanjeev and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation 
with improved correlation with human judg-
ments. In Proceedings of the ACL Workshop on 
Intrinsic and Extrinsic Evaluation Measures. 
Blatz John, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto 
Sanchis, and Nicola Ueffing. 2003. Confidence 
estimation for machine translation. In Technical 
Report Natural Language Engineering Workshop 
Final Report, pages 97-100. 
 Callison-Burch Chris, Miles Osborne, and Philipp 
Koehn. 2006. Re-evaluating the Role of BLEU in 
Machine Translation Research. In Proceedings 
of 11th Conference of the European Chapter of 
the Association for Computational Linguistics  
Chan S. Yee and Hwee T. Ng. 2008. MAXSIM: A 
maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL-08: HLT, 
pages 55?62. 
Doddington George. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram Co- 
Occurrence Statistics. In Proceedings of the 2nd 
International Conference on Human Language 
Technology, pages 138?145. 
1539
Drucker Harris, Chris J. C. Burges, Linda Kaufman, 
Alex Smola, Vladimir Vapnik. 1996. Support 
vector regression machines.  In NIPS. 
Gim?nez Jes?s and Llu?s M?rquez. 2007. Linguistic 
Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL 
Workshop on Statistical Machine Translation. 
Gim?nez Jes?s and Llu?s M?rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through 
Non-Parametric Metric Combinations. In Pro-
ceedings of IJCNLP, pages 319?326. 
Gim?nez Jes?s and Llu?s M?rquez. 2008b. On the 
Robustness of Linguistic Features for Automatic 
MT Evaluation. 
Joachims Thorsten. 2002. Optimizing search en-
gines using clickthrough data. In KDD. 
Klein Dan and Christopher D. Manning. 2003a. 
Fast Exact Inference with a Factored Model for 
Natural Language Parsing. In Advances in 
Neural Information Processing Systems 15, pp. 
3-10.  
Klein Dan and Christopher D. Manning. 2003b. 
Accurate Unlexicalized Parsing. Proceedings of 
the 41st Meeting of the Association for Compu-
tational Linguistics, pp. 423-430. 
Le Audrey and Mark Przybocki. 2005. NIST 2005 
machine translation evaluation official results. 
In Official release of automatic evaluation scores 
for all submission. 
Lin Chin-Yew and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quali-
ty Using Longest Common Subsequence and 
Skip-Bigram Statistics. Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics, pp. 605-612. 
Liu Ding and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In 
Proceedings of ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization, pages 25?32. 
Liu Ding and Daniel Gildea. 2007. Source Lan-
guage Features and Maximum Correlation 
Training for Machine Translation Evaluation. In 
proceedings of NAACL HLT 2007, pages 41?48 
Mehay Dennis and Chris Brew. 2007. BLEUATRE: 
Flattening Syntactic Dependencies for MT Eval-
uation. In Proceedings of the 11th Conference on 
Theoretical and Methodological Issues in Ma-
chine Translation. 
Melamed Dan I., Ryan Green, and Joseph P. Turian. 
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on 
Human Language Technology and the North 
American Chapter of the Association for Com-
putational Linguistics. 
Nie?en Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. An Evaluation Tool for 
Machine  Translation: Fast Evaluation for MT 
Research. In Proceedings of the 2nd Internation-
al Conference on Language Resources and Eval-
uation . 
Owczarzak Karolina, Declan Groves, Josef Van 
Genabith, and Andy Way. 2006. Contextual Bi-
text- Derived Paraphrases in Automatic MT 
Evaluation. In Proceedings of the 7th Confe-
rence of the Association for Machine Translation 
in the Americas, pages 148?155. 
Owczarzak Karolina, Josef van Genabith, and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation. In Proceedings of the 
ACL Workshop on Statistical Machine Transla-
tion, pages 104?111. 
Papineni Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In 
Proceedings of 40th Annual Meeting of the As-
sociation for Computational Linguistics. 
Popovi? Maja and Hermann Ney. 2007. Word Er-
ror Rates: Decomposition over POS classes and 
Applications for Error Analysis. In Proceedings 
of the Second Workshop on Statistical Machine 
Translation, pages 48?55. 
Popovi? Maja and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine trans-
lation output. In Proceedings of the 4th EACL 
Workshop on Statistical Machine Translation, 
pages 29?32. 
Snover Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of AMTA, pag-
es 223?231. 
Tillmann Christoph, Stefan Vogel, Hermann Ney, 
A. Zubiaga, and H. Sawaf. 1997. Accelerated 
DP based Search for Statistical Translation. In 
Proceedings of European Conference on Speech 
Communication and Technology. 
1540
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 863?868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Microblog Entity Linking by Leveraging Extra Posts
Yuhang Guo, Bing Qin?, Ting Liu , Sheng Li
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{yhguo, bqin?, tliu, sli}@ir.hit.edu.cn
Abstract
Linking name mentions in microblog posts to
a knowledge base, namely microblog entity
linking, is useful for text mining tasks on mi-
croblog. Entity linking in long text has been
well studied in previous works. However few
work has focused on short text such as mi-
croblog post. Microblog posts are short and
noisy. Previous method can extract few fea-
tures from the post context. In this paper we
propose to use extra posts for the microblog
entity linking task. Experimental results show
that our proposed method significantly im-
proves the linking accuracy over traditional
methods by 8.3% and 7.5% respectively.
1 Introduction
Microblogging services (e.g. Twitter) are attracting
millions of users to share and exchange their ideas
and opinions. Millions of new microblog posts are
generated on such open broadcasting platforms ev-
ery day 1. Microblog provides a fruitful and instant
channel of global information publication and acqui-
sition.
A necessary step for the information acquisition
on microblog is to identify which entities a post is
about. Such identification can be challenging be-
cause the entity mention may be ambiguous. Let?s
begin with a real post from Twitter.
(1) No excuse for floods tax, says Abbott
URL
?Corresponding author
1See http://blog.twitter.com/2011/06/ 200-million-tweets-
per-day.html.
This post is about an Australia political lead-
er, Tony Abbot, and his opinion on flood tax
policy. To understand that this post mentions
Tony Abbot is not trivial because the name Ab-
bot can refer to many people and organization-
s. In the Wikipedia page of Abbott, there list-
s more than 20 Abbotts, such as baseball player
Jim Abbott, actor Bud Abbott and company
Abbott Laboratories, etc..
Given a knowledge base (KB) (e.g. Wikipedia),
entity linking is the task to identify the referent KB
entity of a target name mention in plain text. Most
current entity linking techniques are designed for
long text such as news/blog articles (Mihalcea and
Csomai, 2007; Cucerzan, 2007; Milne and Witten,
2008; Han and Sun, 2011; Zhang et al, 2011; Shen
et al, 2012; Kulkarni et al, 2009; Ratinov et al,
2011). Entity linking for microblog posts has not
been well studied.
Comparing with news/blog articles, microblog
posts are:
short each post contains no more than 140 charac-
ters;
fresh the new entity-related content may have not
been included in the knowledge base;
informal acronyms and spoken language writing
style are common.
Due to these properties, few feature can be ex-
tracted from a post. Without enough features, pre-
vious entity linking methods may fail. In order to
overcome the feature sparseness, we turn to another
property of microblog:
863
redundancy For each day, over 340M short mes-
sages are posted in twitter. Similar information
may be posted in different expressions.
For example, we find the following post,
(2) Julia Gillard and Tony Abbott on
the flood levy just after 8.30am on
@612brisbane!
The content of post (2) is highly related to post
(1). In contrast to the confusing post (1), the text
in post (2) explicitly indicates that the Abbott here
refers to the Australian political leader. This inspires
us to bridge the confusing post and the knowledge
base with other posts.
In this paper, we approach the microblog entity
linking by leveraging extra posts. A straightforward
method is to expand the post context with similar
posts, which we call Context-Expansion-based Mi-
croblog Entity Linking (CEMEL). In this method,
we first construct a query with the given post and
then search for it in a collection of posts. From the
search result, we select the most similar posts for the
context expansion. The disambiguation will benefit
from the extra posts if, hopefully, they are related
to the given post in content and include explicit fea-
tures for the disambiguation.
Furthermore, we propose a Graph-based Mi-
croblog Entity Linking (GMEL) method. In contrast
to CEMEL, the extra posts in GMEL are not directly
added into the context. Instead, they are represented
as nodes in a graph, and weighted by their similarity
with the target post. We use an iterative algorithm
in this graph to propagate the entity weights through
the edges between the post nodes.
We conduct experiments on real microblog da-
ta which we harvested from Twitter. Current enti-
ty linking corpus, such as the TAC-KBP data (M-
cNamee and Dang, 2009), mainly focuses on long
text. And few microblog entity linking corpus is
publicly available. In this work, we manually anno-
tated a microblog entity linking corpus. This corpus
inherit the target names from TAC-KBP2009. So it
is comparable with the TAC-KBP2009 corpus.
Experimental results show that the performance
of previous methods drops on microblog posts com-
paring with on long text. Both of CEMEL and
GMEL can significantly improve the performance
over baselines, which means that entity linking sys-
tem on microblog can be improved by leveraging ex-
tra posts. The results also show that GMEL outper-
forms CEMEL significantly.
We summarize our contributions as follows.
? We propose a context-expansion-based and a
graph-based method for microblog entity link-
ing by leveraging extra posts.
? We annotate a microblog entity linking corpus
which is comparable to an existing long text
corpus.
? We show the inefficiency of previous method
on the microblog corpus and our method can
significantly improve the results.
2 Task defination
The microblog entity linking task is that, for a name
mention in a microblog post, the system is to find the
referent entity of the name in a knowledge base, or
return a NIL mark if the entity is absence from the
knowledge base. This definition is close to the en-
tity linking task in the TAC-KBP evaluation (Ji and
Grishman, 2011) except for the context of the target
name is microblog post whereas in TAC-KBP the
context is news article or web log.
Several related tasks have been studied on mi-
croblog posts. In Meij et al (2012)?s work, they
link a post, rather than a name mention in the post,
to relevant Wikipedia concepts. Guo et al (2013a)
and Liu et al (2013) define entity linking as to first
detect all the mentions in a post and then link the
mentions to the knowledge base. In contrast, our
definition (as well as the TAC-KBP definition) fo-
cuses on a concerned name mention across different
posts/documents.
3 Method
A typical entity linking system can be broken down
into two steps:
candidate generation This step narrows down the
candidate entity range from any entity in the
world to a limited set.
candidate ranking This step ranks the candidates
and output the top ranked entity as the result.
864
Figure 1: An example of the GMEL graph. p1 . . . p4 are
post nodes and c1 . . . c3 are candidate entity nodes. Each
post node is connected to the corresponding candidate n-
odes from the knowledge base. The edges between the
nodes are weighted by the similarity between them.
In this paper, we use the candidate generation
method described in Guo et al(2013). For the candi-
date ranking, we use a Vector Space Model (VSM)
and a Learning to Rank (LTR) as baselines. VSM
is an unsupervised method and LTR is a supervised
method. Both of them have achieved the state-of-
the-art performances in the TAC-KBP evaluations.
The major challenge in microblog entity linking
is the lack of context in the post. An ideal solu-
tion is to expand the context with the posts which
contain the same entity. However, automatically
judging whether a name mention in two documents
refers to the same entity, namely cross document co-
reference, is not trivial. Here our solution is to rank
the posts by their possibility of co-reference to the
target one and select the most possible co-referent
posts for the expansion.
CEMEL is based on the assumption that, given a
name and two posts where the name is mentioned,
the higher similarity between the posts the high-
er possibility of their co-reference and that the co-
referent posts may contains useful features for the
disambiguation. However, two literally similar posts
may not be co-referent. If such non co-referent post
is expanded to the context, noises may be included.
Take the following post as an example.
(3) AG Abbott says that bullets have
crossed the border from Mexico to
Texas at least four times. URL
This post is similar to post (1) because they both
contains ?says? and ?URL?. But the Abbott in post
(3) refers to the Texas Attorney General Greg Ab-
bott. In this mean, the expanded context in post (3)
could mislead the disambiguation for post (1). Such
noise can be controlled by setting a strict number of
posts to expand the context or weighting the contri-
bution of this post to the target one.
Our CEMEL method consists of the following
steps: First we construct a query with the terms from
the target post. Second we search for the query in a
microblog post collection using a common informa-
tion retrieval model such as the vector space model.
Note that here we limit the searched posts must con-
tain the target name mention. Then we expand the
target post with top N similar posts and use a typical
entity linking method (such as VSM and LTR) with
the expanded context.
Figure 1 illustrates the graph of GMEL. Each n-
ode of this graph represents an candidate entity (e.g.
c1 . . . c3) or a post of the given target name (e.g.
p1 . . . p4) In this graph, each node represents an en-
tity or a post of the given target name. Between each
pair of post nodes, each pair of entity nodes and each
post node and its candidate entity nodes, there is an
edge. The edge is weighted by the similarity be-
tween the two linked nodes. Entity nodes are labeled
by themselves and candidate nodes are initialized as
unlabeled nodes. For the edges between post node
pairs and entity node pairs, we use cosine similari-
ty. For the edges between a post node and its can-
didate entity nodes, we use the score given by tra-
ditional entity linking methods. We use an iterative
algorithm on this graph to propagate the labels from
the entity nodes to the post nodes. We adapt Label
Propagation (LP) (Zhu and Ghahramani, 2002) and
Modified Adsorption (MAD) (Talukdar and Pereira,
2010) for the iteration over the graph.
4 Experiment
4.1 Data Annotation
Till now, few microblog entity linking data is pub-
licly available. In this work, we manually annotate
a data set on microblog posts2. We collect 15.6 mil-
lion microblog posts in Twitter dated from January
23 to February 8, 2011. In order to compare with ex-
isting entity linking on long text, we select a subset
of target names from TAC-KBP2009 and inherit the
knowledge base in the TAC-KBP evaluation. The
2We published this data so that researchers can reproduce
our results.
865
Figure 2: Percentage of the co-reference posts in the top
N similar posts
Figure 3: Impact of expansion post number in CEMEL
TAC-KBP2009 data set includes 513 target names.
We search for all the target names in the post col-
lection and get 26,643 matches. We randomly sam-
ple 120 posts for each of the top 30 most frequently
matched target names and filter out non-English and
overly short (i.e. less than 3 words) posts. Then
we get 2,258 posts for 25 target names and manual-
ly link the target name mentions in the posts to the
TAC-KBP knowledge base.
In order to evaluate the assumption in CEMEL:
similar posts tend to co-reference, we randomly s-
elect 10 posts for 5 target names respectively and
search for the posts in the post collection. From
the search result of each of the 50 posts, we select
the top 20 posts and manually annotate if they co-
reference with the query post.
4.2 Settings
We generate candidates with the method described
in (Guo et al, 2013b) and use Vector Space Mod-
el (VSM) (Varma et al, 2009) and Learning to Rank
(LTR) (Zheng et al, 2010) as the ranking model. We
Figure 4: Accuracy of GMEL with different rate of extra
post nodes
use Lucene and ListNet with default settings for the
VSM and LTR implementation respectively. We use
bigram feature for VSM and the feature set of (Chen
et al, 2011) for LTR. LTR is evaluated with 10-fold
cross validation. Given a target name, the GMEL
graph includes all the evaluation posts as well as a
set of extra post nodes searched from the post collec-
tion with the query of the target name. We filter out
determiners, interjections, punctuations, emoticon-
s, discourse markers and URLs in the posts with a
twitter part-of-speech tagger (Owoputi et al, 2013).
The similarity between a post and its candidate en-
tities is set with the score given by VSM or LTR
and the similarity between other nodes is set with the
corresponding cosine similarity. We employ junto3
with default settings for the iterative algorithm im-
plementation .
4.3 Results
Figure 2 shows the relationship between similari-
ty and co-reference. From this figure we can see
that the percentage decreases with the growth of N.
When the N is up to 10, about 60% of the similar
posts co-reference with the query post and the de-
crease speed slows down. The Pearson correlation
coefficient between the percentage and the number
of top N is -0.843, which shows a significant corre-
lation between the two variables (with p-value 0.01
under t-test).
Figure 3 shows the impact of the extra post num-
ber for the context expansion in CEMEL. We can see
that the accuracies of VSM and LTR are improved
3See https://github.com/parthatalukdar/junto
866
Figure 5: Label entropy of GMEL with different rate of
extra post nodes
Figure 6: Accuracy of the systems
by CEMEL. The improvements peak with 5-10 ex-
tra posts. Then more extra posts will pull down the
accuracy.
Figure 4 shows the accuracy of GMEL. The x-axis
is the rate of the extra post number over the evalu-
ation post number. We can see that the accuracy of
MAD increases with the number of extra post nodes
at first and then turns to be stable. The accuracy of
LP increases at first and drops when more extra posts
are added into the graph.
Figure 5 shows the information entropy of the la-
bels in LP and MAD. The curves show that the pre-
diction of LP tends to converge into a small number
of labels. This is because LP prefers smoothing la-
belings over the graph (Talukdar and Pereira, 2010).
We also evaluate our baselines on TAC-KBP2009
data set (LTR is trained on TAC-KBP2010 data set).
The accuracy of VSM and LTR are 0.8338 and
0.8372 respectively, which are comparable with the
state-of-the-art result (Hachey et al, 2013).
Figure 6 shows the performances of the systems
on the microblog data. We set the optimal expansion
post number of CEMEL and use MAD algorithm for
GMEL with all searched extra post nodes. From this
figure we can see that the results of VSM and LTR
baselines are comparable and both of them are sig-
nificantly lower than that on TAC-KBP2009 data.
CEMEL improves the VSM and LTR baselines by
4.3% and 2.7% respectively. GMEL improves VSM
and LTR by 8.3% and 7.5% respectively. The results
of GMEL are also significantly better than CEMEL.
All of the improvements are significant under Z-test
with p < 0.05.
5 Conclusion
In this paper we approach microblog entity linking
by leveraging extra posts. We propose a context-
expansion-based and a graph-based method. Exper-
imental results on our data set show that the per-
formance of traditional method drops on the mi-
croblog data. The graph-based method outperform-
s the context-expansion-based method and both of
them significantly improve the accuracy of tradition-
al methods. In the graph-based method the modified
adsorption algorithm performs better than the label
propagation algorithm.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61273321, 61073126, 61133012 and the National
863 Leading Technology Research Project via grant
2012AA011102. We would like to thank to Wanx-
iang Che, Ruiji Fu, Yanyan Zhao, Wei Song and
several anonymous reviewers for their constructive
comments and suggestions.
References
Zheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji.
2011. A toolkit for knowledge base population. In
SIGIR, pages 1267?1268.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Method-
s in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
867
Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013a. To link or not to link? a study on end-to-
end tweet entity linking. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1020?1030, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and Sheng
Li. 2013b. Improving candidate generation for entity
linking. In Elisabeth Mtais, Farid Meziane, Mohamad
Saraee, Vijayan Sugumaran, and Sunil Vadera, edi-
tors, Natural Language Processing and Information
Systems, volume 7934 of Lecture Notes in Computer
Science, pages 225?236. Springer Berlin Heidelberg.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating en-
tity linking with wikipedia. Artificial Intelligence,
194(0):130 ? 150. ?ce:title?Artificial Intelligence,
Wikipedia and Semi-Structured Resources?/ce:title?.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Techologies, pages 945?954, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148?1158, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu
Wei, and Yi Lu. 2013. Entity linking for tweets. In
Proceedings of the 51th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics.
P. McNamee and H.T. Dang. 2009. Overview of
the tac 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference
(TAC2009).
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 563?
572, New York, NY, USA. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
?07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233?242, New York, NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In CIKM ?08: Proceeding of the 17th
ACM conference on Information and knowledge man-
agement, pages 509?518, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In NAACL2013,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computation-
al Linguistics: Human Language Technologies, pages
1375?1384, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-
edge base via semantic knowledge. In Proceedings of
the 21st international conference on World Wide We-
b, WWW ?12, pages 449?458, New York, NY, USA.
ACM.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473?1481, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi,
Praveen Bysani, Santosh GSK, Kiran Kumar N, Kran-
thi Reddy, Karuna Kumar, and Nitin Maganti. 2009.
Iiit hyderabad at tac 2009. In Proceedings of the Sec-
ond Text Analysis Conference (TAC 2009), Gaithers-
burg, Maryland, USA, November.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011. Entity linking with effective acronym expan-
sion, instance selection, and topic modeling. In Toby
Walsh, editor, IJCAI 2011, pages 1909?1914.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In NAACL2010, pages 483?491, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
868
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825?833,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving Statistical Machine Translation with 
Monolingual Collocation 
 
Zhanyi Liu1, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu.com Inc., Beijing, China 
zhanyiliu@gmail.com 
{wanghaifeng, wu_hua}@baidu.com 
lisheng@hit.edu.cn 
 
Abstract? 
This paper proposes to use monolingual 
collocations to improve Statistical Ma-
chine Translation (SMT). We make use 
of the collocation probabilities, which are 
estimated from monolingual corpora, in 
two aspects, namely improving word 
alignment for various kinds of SMT sys-
tems and improving phrase table for 
phrase-based SMT. The experimental re-
sults show that our method improves the 
performance of both word alignment and 
translation quality significantly. As com-
pared to baseline systems, we achieve ab-
solute improvements of 2.40 BLEU score 
on a phrase-based SMT system and 1.76 
BLEU score on a parsing-based SMT 
system. 
1 Introduction 
Statistical bilingual word alignment (Brown et al 
1993) is the base of most SMT systems. As com-
pared to single-word alignment, multi-word 
alignment is more difficult to be identified. Al-
though many methods were proposed to improve 
the quality of word alignments (Wu, 1997; Och 
and Ney, 2000; Marcu and Wong, 2002; Cherry 
and Lin, 2003; Liu et al, 2005; Huang, 2009), 
the correlation of the words in multi-word 
alignments is not fully considered. 
In phrase-based SMT (Koehn et al, 2003), the 
phrase boundary is usually determined based on 
the bi-directional word alignments. But as far as 
we know, few previous studies exploit the collo-
cation relations of the words in a phrase. Some 
                                                 
This work was partially done at Toshiba (China) Research 
and Development Center. 
researches used soft syntactic constraints to pre-
dict whether source phrase can be translated to-
gether (Marton and Resnik, 2008; Xiong et al, 
2009). However, the constraints were learned 
from the parsed corpus, which is not available 
for many languages.  
In this paper, we propose to use monolingual 
collocations to improve SMT. We first identify 
potentially collocated words and estimate collo-
cation probabilities from monolingual corpora 
using a Monolingual Word Alignment (MWA) 
method (Liu et al, 2009), which does not need 
any additional resource or linguistic preprocess-
ing, and which outperforms previous methods on 
the same experimental data. Then the collocation 
information is employed to improve Bilingual 
Word Alignment (BWA) for various kinds of 
SMT systems and to improve phrase table for 
phrase-based SMT. 
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. A cept is the 
set of source words that are connected to the 
same target word (Brown et al, 1993). An 
alignment between a source multi-word cept and 
a target word is a many-to-one multi-word 
alignment. 
To improve phrase table, we calculate phrase 
collocation probabilities based on word colloca-
tion probabilities. Then the phrase collocation 
probabilities are used as additional features in 
phrase-based SMT systems. 
The evaluation results show that the proposed 
method in this paper significantly improves mul-
ti-word alignment, achieving an absolute error 
rate reduction of 29%. The alignment improve-
ment results in an improvement of 2.16 BLEU 
score on phrase-based SMT system and an im-
provement of 1.76 BLEU score on parsing-based 
SMT system. If we use phrase collocation proba-
bilities as additional features, the phrase-based 
825
SMT performance is further improved by 0.24 
BLEU score. 
The paper is organized as follows: In section 2, 
we introduce the collocation model based on the 
MWA method. In section 3 and 4, we show how 
to improve the BWA method and the phrase ta-
ble using collocation models respectively. We 
describe the experimental results in section 5, 6 
and 7. Lastly, we conclude in section 8. 
2 Collocation Model 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). A colloca-
tion is composed of two words occurring as ei-
ther a consecutive word sequence or an inter-
rupted word sequence in sentences, such as "by 
accident" or "take ... advice". In this paper, we 
use the MWA method (Liu et al, 2009) for col-
location extraction. This method adapts the bi-
lingual word alignment algorithm to monolingual 
scenario to extract collocations only from mono-
lingual corpora. And the experimental results in 
(Liu et al, 2009) showed that this method 
achieved higher precision and recall than pre-
vious methods on the same experimental data. 
2.1 Monolingual word alignment 
The monolingual corpus is first replicated to 
generate a parallel corpus, where each sentence 
pair consists of two identical sentences in the 
same language. Then the monolingual word 
alignment algorithm is employed to align the 
potentially collocated words in the monolingual 
sentences. 
According to Liu et al (2009), we employ the 
MWA Model 3 (corresponding to IBM Model 3) 
to calculate the probability of the monolingual 
word alignment sequence, as shown in Eq. (1). 
? ?
???
?
?
l
j
jaj
l
i
ii
lajdwwt
wnSASp
j1
1
3 ModelMWA 
),|()|(
)|()|,( ?    (1) 
Where lwS 1?  is a monolingual sentence, i?  
denotes the number of words that are aligned 
with 
iw . Since a word never collocates with itself, 
the alignment set is denoted as 
}&],1[|),{( ialiaiA ii ??? . Three kinds of prob-
abilities are involved in this model: word collo-
cation probability 
)|( jaj wwt
, position colloca-
tion probability ),|( lajd j  and fertility probabili-
ty )|( ii wn ? . 
In the MWA method, the similar algorithm to 
bilingual word alignment is used to estimate the 
parameters of the models, except that a word 
cannot be aligned to itself.  
Figure 1 shows an example of the potentially 
collocated word pairs aligned by the MWA me-
thod. 
 
Figure 1. MWA Example 
2.2 Collocation probability 
Given the monolingual word aligned corpus, we 
calculate the frequency of two words aligned in 
the corpus, denoted as ),( ji wwfreq . We filtered 
the aligned words occurring only once. Then the 
probability for each aligned word pair is esti-
mated as follows: 
? ??
?w j
ji
ji wwfreq
wwfreqwwp ),(
),()|(
                 (2) 
? ??
?w i
ji
ij wwfreq
wwfreqwwp ),(
),()|(
                  (3) 
In this paper, the words of collocation are 
symmetric and we do not determine which word 
is the head and which word is the modifier. Thus, 
the collocation probability of two words is de-
fined as the average of both probabilities, as in 
Eq. (4). 
2
)|()|(),( ijjiji wwpwwpwwr ??
      (4) 
If we have multiple monolingual corpora to 
estimate the collocation probabilities, we interpo-
late the probabilities as shown in Eq. (5). 
),(),( jik kkji wwrwwr ?? ?
          (5) 
k?  denotes the interpolation coefficient for 
the probabilities estimated on the kth corpus. 
3 Improving Statistical Bilingual Word 
Alignment 
We use the collocation information to improve 
both one-directional and bi-directional bilingual 
word alignments. The alignment probabilities are 
re-estimated by using the collocation probabili-
ties of words in the same cept. 
The team leader plays a key role in the project undertaking. 
The team leader plays a key role in the project undertaking. 
 
826
3.1 Improving one-directional bilingual 
word alignment 
According to the BWA method, given a bilingual 
sentence pair leE 1?  and mfF 1? , the optimal 
alignment sequence A  between E and F can be 
obtained as in Eq. (6). 
)|,(maxarg* EAFpA A?
                   (6) 
The method is implemented in a series of five 
models (IBM Models). IBM Model 1 only em-
ploys the word translation model to calculate the 
probabilities of alignments. In IBM Model 2, 
both the word translation model and position dis-
tribution model are used. IBM Model 3, 4 and 5 
consider the fertility model in addition to the 
word translation model and position distribution 
model. And these three models are similar, ex-
cept for the word distortion models. 
One-to-one and many-to-one alignments could 
be produced by using IBM models. Although the 
fertility model is used to restrict the number of 
source words in a cept and the position distortion 
model is used to describe the correlation of the 
positions of the source words, the quality of 
many-to-one alignments is lower than that of 
one-to-one alignments. 
Intuitively, the probability of the source words 
aligned to a target word is not only related to the 
fertility ability and their relative positions, but 
also related to lexical tokens of words, such as 
common phrase or idiom. In this paper, we use 
the collocation probability of the source words in 
a cept to measure their correlation strength. Giv-
en source words }|{ iaf jj ?  aligned to ie , their 
collocation probability is calculated as in Eq. (7). 
)1(*
),(2
})|({
1
1 1
][][
?
? ?
??
?
? ??
ii
k kg
giki
jj
i i ffr
iafr ??
? ?
     (7) 
Here, 
kif ][
and 
gif ][
denote the thk  word and 
thg  word in }|{ iaf jj ? ; ),( ][][ giki ffr
 denotes 
the collocation probability of 
kif ][
and 
gif ][
, as 
shown in Eq. (4).  
Thus, the collocation probability of the align-
ment sequence of a sentence pair can be calcu-
lated according to Eq. (8). 
? ?? ?
l
i jj iafrEAFr 1 })|({)|,(
           (8) 
Based on maximum entropy framework, we 
combine the collocation model and the BWA 
model to calculate the word alignment probabili-
ty of a sentence pair, as shown in Eq. (9). 
? ? ?
?
?
'
)),,(exp(
)),,(exp(
)|,(
A i
ii
i
ii
r AEFh
AEFh
EAFp ?
?     (9) 
Here, ),,( AEFhi and i?  denote features and 
feature weights, respectively. We use two fea-
tures in this paper, namely alignment probabili-
ties and collocation probabilities. 
Thus, we obtain the decision rule: 
}),,({maxarg* ?? i iiA AEFhA ?
          (10) 
Based on the GIZA++ package 1 , we imple-
mented a tool for the improved BWA method. 
We first train IBM Model 4 and collocation 
model on bilingual corpus and monolingual cor-
pus respectively. Then we employ the hill-
climbing algorithm (Al-Onaizan et al, 1999) to 
search for the optimal alignment sequence of a 
given sentence pair, where the score of an align-
ment sequence is calculated as in Eq. (10). 
We note that Eq. (8) only deals with many-to-
one alignments, but the alignment sequence of a 
sentence pair also includes one-to-one align-
ments. To calculate the collocation probability of 
the alignment sequence, we should also consider 
the collocation probabilities of such one-to-one 
alignments. To solve this problem, we use the 
collocation probability of the whole source sen-
tence, )(Fr , as the collocation probability of 
one-word cept. 
3.2 Improving bi-directional bilingual word 
alignments 
In word alignment models implemented in GI-
ZA++, only one-to-one and many-to-one word 
alignment links can be found. Thus, some multi-
word units cannot be correctly aligned. The 
symmetrization method is used to effectively 
overcome this deficiency (Och and Ney, 2003). 
Bi-directional alignments are generally obtained 
from source-to-target algnments 
tsA 2  and target-
to-source alignments 
stA 2 , using some heuristic 
rules (Koehn et al, 2005). This method ignores 
the correlation of the words in the same align-
ment unit, so an alignment may include many 
unrelated words2 , which influences the perfor-
mances of SMT systems. 
                                                 
1 http://www.fjoch.com/GIZA++.html 
2 In our experiments, a multi-word unit may include up to 
40 words. 
827
In order to solve the above problem, we incor-
porate the collocation probabilities into the bi-
directional word alignment process. 
Given alignment sets 
tsA 2  and stA 2 . We can 
obtain the union 
sttsts AAA 22 ??? . The source 
sentence mf1  can be segmented into m?  cepts 
mf ?1 . The target sentence le1  can also be seg-
mented into l ?  cepts le ?1 . The words in the same 
cept can be a consecutive word sequence or an 
interrupted word sequence. 
Finally, the optimal alignments A  between 
mf ?1  and le ?1  can be obtained from tsA ?  using the 
following decision rule. 
})()(),({maxarg
),,(
321
),(
*'
1
'
1
????
??
???
? Afe
jiji
AA
ml
jits
frerfep
Afe     (11) 
Here, )( jfr  and )( ier  denote the collocation 
probabilities of the words in the source language 
and target language respectively, which are cal-
culated by using Eq. (7). ),( ji fep  denotes the 
word translation probability that is calculated 
according to Eq. (12). 
i?  denotes the weights of 
these probabilities. 
||*||
2/))|()|((
),(
ji
ee ff
ji fe
efpfep
fep i j
? ? ?
? ? ?
    (12) 
)|( fep  and )|( efp  are the source-to-target 
and target-to-source translation probabilities 
trained from the word aligned bilingual corpus. 
4 Improving Phrase Table 
Phrase-based SMT system automatically extracts 
bilingual phrase pairs from the word aligned bi-
lingual corpus. In such a system, an idiomatic 
expression may be split into several fragments, 
and the phrases may include irrelevant words. In 
this paper, we use the collocation probability to 
measure the possibility of words composing a 
phrase. 
For each bilingual phrase pair automatically 
extracted from word aligned corpus, we calculate 
the collocation probabilities of source phrase and 
target phrase respectively, according to Eq. (13). 
)1(*
),(2
)(
1
1 1
1 ?
? ?
?
?
? ??
nn
wwr
wr
n
i
n
ij
ji
n                  (13) 
Here, nw1  denotes a phrase with n words; 
),( ji wwr
 denotes the collocation probability of a 
Corpora 
Chinese 
words 
English 
words 
Bilingual corpus 6.3M 8.5M 
Additional monolingual 
corpora 
312M 203M 
Table 1. Statistics of training data 
word pair calculated according to Eq. (4). For the 
phrase only including one word, we set a fixed 
collocation probability that is the average of the 
collocation probabilities of the sentences on a 
development set. These collocation probabilities 
are incorporated into the phrase-based SMT sys-
tem as features.  
5 Experiments on Word Alignment 
5.1 Experimental settings 
We use a bilingual corpus, FBIS (LDC2003E14), 
to train the IBM models. To train the collocation 
models, besides the monolingual parts of FBIS, 
we also employ some other larger Chinese and 
English monolingual corpora, namely, Chinese 
Gigaword (LDC2007T38), English Gigaword 
(LDC2007T07), UN corpus (LDC2004E12), Si-
norama corpus (LDC2005T10), as shown in Ta-
ble 1. 
Using these corpora, we got three kinds of col-
location models: 
CM-1: the training data is the additional mo-
nolingual corpora; 
CM-2: the training data is either side of the bi-
lingual corpus; 
CM-3: the interpolation of CM-1 and CM-2. 
To investigate the quality of the generated 
word alignments, we randomly selected a subset 
from the bilingual corpus as test set, including 
500 sentence pairs. Then word alignments in the 
subset were manually labeled, referring to the 
guideline of the Chinese-to-English alignment 
(LDC2006E93), but we made some modifica-
tions for the guideline. For example, if a preposi-
tion appears after a verb as a phrase aligned to 
one single word in the corresponding sentence, 
then they are glued together. 
There are several different evaluation metrics 
for word alignment (Ahrenberg et al, 2000). We 
use precision (P), recall (R) and alignment error 
ratio (AER), which are similar to those in Och 
and Ney (2000), except that we consider each 
alignment as a sure link. 
828
Experiments 
Single word alignments Multi-word alignments 
P R AER P R AER 
Baseline 0.77 0.45 0.43 0.23 0.71 0.65 
Improved BWA methods 
CM-1 0.70 0.50 0.42 0.35 0.86 0.50 
CM-2 0.73 0.48 0.42 0.36 0.89 0.49 
CM-3 0.73 0.48 0.41 0.39 0.78 0.47 
Table 2. English-to-Chinese word alignment results 
 
Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and 
the improved BWA method using CM-3. " " denotes the alignments of our method; " " denotes 
the alignments of the baseline method. 
||
||
g
rg
S
SSP ??
                      (14) 
||
||
r
rg
S
SSR ??
                     (15) 
||||
||*21
rg
rg
SS
SSAER ???
?              (16) 
Where, 
gS
 and 
rS  denote the automatically 
generated alignments and the reference align-
ments. 
In order to tune the interpolation coefficients 
in Eq. (5) and the weights of the probabilities in 
Eq. (11), we also manually labeled a develop-
ment set including 100 sentence pairs, in the 
same manner as the test set. By minimizing the 
AER on the development set, the interpolation 
coefficients of the collocation probabilities on 
CM-1 and CM-2 were set to 0.1 and 0.9. And the 
weights of probabilities were set as 6.01 ?? , 
2.02 ?? and 2.03 ?? . 
5.2 Evaluation results 
One-directional alignment results 
To train a Chinese-to-English SMT system, 
we need to perform both Chinese-to-English and 
English-to-Chinese word alignment. We only 
evaluate the English-to-Chinese word alignment 
here. GIZA++ with the default settings is used as 
the baseline method. The evaluation results in 
Table 2 indicate that the performances of our 
methods on single word alignments are close to 
that of the baseline method. For multi-word 
alignments, our methods significantly outper-
form the baseline method in terms of both preci-
sion and recall, achieving up to 18% absolute 
error rate reduction. 
Although the size of the bilingual corpus is 
much smaller than that of additional monolingual 
corpora, our methods using CM-1 and CM-2 
achieve comparable performances. It is because 
CM-2 and the BWA model are derived from the 
same resource. By interpolating CM1 and CM2, 
i.e. CM-3, the error rate of multi-word alignment 
results is further reduced. 
Figure 2 shows an example of word alignment 
results generated by the baseline method and the 
improved method using CM-3. In this example, 
our method successfully identifies many-to-one 
alignments such as "the people of the world  
??". In our collocation model, the collocation 
probability of "the people of the world" is much 
higher than that of "people world". And our me-
thod is also effective to prevent the unrelated 
?? ? ???? ?? ?? ? ?? ? ?? ?? ? ?? ? 
China's science and technology research has made achievements which have gained the attention of the people of the world . 
??  ? ???? ?? ?? ? ?? ? ?? ?? ? ?? ? 
zhong-guo  de     ke-xue-ji-shu      yan-jiu      qu-de       le      xu-duo   ling   shi-ren     zhu-mu     de     cheng-jiu . 
china        DE    science and         research   obtain      LE      many     let    common    attract     DE  achievement . 
                             technology                                                                            people    attention   
829
Experiments 
Single word alignments Multi-word alignments All alignments 
P R AER P R AER P R AER 
Baseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51 
Our methods 
WA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45 
WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44 
WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40 
Table 3. Bi-directional word alignment results 
words from being aligned. For example, in the 
baseline alignment "has made ... have ??", 
"have" and "has" are unrelated to the target word, 
while our method only generated "made  ?
?", this is because that the collocation probabili-
ties of "has/have" and "made" are much lower 
than that of the whole source sentence. 
Bi-directional alignment results 
We build a bi-directional alignment baseline 
in two steps: (1) GIZA++ is used to obtain the 
source-to-target and target-to-source alignments; 
(2) the bi-directional alignments are generated by 
using "grow-diag-final". We use the methods 
proposed in section 3 to replace the correspond-
ing steps in the baseline method. We evaluate 
three methods:  
WA-1: one-directional alignment method pro-
posed in section 3.1 and grow-diag-final; 
WA-2: GIZA++ and the bi-directional bilin-
gual word alignments method proposed in 
section 3.2; 
WA-3: both methods proposed in section 3. 
Here, CM-3 is used in our methods. The re-
sults are shown in Table 3. 
We can see that WA-1 achieves lower align-
ment error rate as compared to the baseline me-
thod, since the performance of the improved one-
directional alignment method is better than that 
of GIZA++. This result indicates that improving 
one-directional word alignment results in bi-
directional word alignment improvement. 
The results also show that the AER of WA-2 
is lower than that of the baseline. This is because 
the proposed bi-directional alignment method 
can effectively recognize the correct alignments 
from the alignment union, by leveraging colloca-
tion probabilities of the words in the same cept. 
Our method using both methods proposed in 
section 3 produces the best alignment perfor-
mance, achieving 11% absolute error rate reduc-
tion. 
Experiments BLEU (%) 
Baseline 29.62 
Our methods 
WA-1 
CM-1 30.85 
CM-2 31.28 
CM-3 31.48 
WA-2 
CM-1 31.00 
CM-2 31.33 
CM-3 31.51 
WA-3 
CM-1 31.43 
CM-2 31.62 
CM-3 31.78 
Table 4. Performances of Moses using the dif-
ferent bi-directional word alignments (Signifi-
cantly better than baseline with p < 0.01) 
6 Experiments on Phrase-Based SMT 
6.1 Experimental settings 
We use FBIS corpus to train the Chinese-to-
English SMT systems. Moses (Koehn et al, 2007) 
is used as the baseline phrase-based SMT system. 
We use SRI language modeling toolkit (Stolcke, 
2002) to train a 5-gram language model on the 
English sentences of FBIS corpus. We used the 
NIST MT-2002 set as the development set and 
the NIST MT-2004 test set as the test set. And 
Koehn's implementation of minimum error rate 
training (Och, 2003) is used to tune the feature 
weights on the development set. 
We use BLEU (Papineni et al, 2002) as eval-
uation metrics. We also calculate the statistical 
significance differences between our methods 
and the baseline method by using paired boot-
strap re-sample method (Koehn, 2004). 
6.2 Effect of improved word alignment on 
phrase-based SMT 
We investigate the effectiveness of the improved 
word alignments on the phrase-based SMT sys-
tem. The bi-directional alignments are obtained 
830
 Figure 3. Example of the translations generated by the baseline system and the system where the 
phrase collocation probabilities are added 
Experiments BLEU (%) 
Moses 29.62 
+ Phrase collocation probability 30.47 
+ Improved word alignments 
+ Phrase collocation probability 
32.02 
Table 5. Performances of Moses employing 
our proposed methods (Significantly better than 
baseline with p < 0.01) 
using the same methods as those shown in Table 
3. Here, we investigate three different collocation 
models for translation quality improvement. The 
results are shown in Table 4. 
From the results of Table 4, it can be seen that 
the systems using the improved bi-directional 
alignments achieve higher quality of translation 
than the baseline system. If the same alignment 
method is used, the systems using CM-3 got the 
highest BLEU scores. And if the same colloca-
tion model is used, the systems using WA-3 
achieved the higher scores. These results are 
consistent with the evaluations of word align-
ments as shown in Tables 2 and 3. 
6.3 Effect of phrase collocation probabili-
ties 
To investigate the effectiveness of the method 
proposed in section 4, we only use the colloca-
tion model CM-3 as described in section 5.1. The 
results are shown in Table 5. When the phrase 
collocation probabilities are incorporated into the 
SMT system, the translation quality is improved, 
achieving an absolute improvement of 0.85 
BLEU score. This result indicates that the collo-
cation probabilities of phrases are useful in de-
termining the boundary of phrase and predicting 
whether phrases should be translated together, 
which helps to improve the phrase-based SMT 
performance. 
Figure 3 shows an example: T1 is generated 
by the system where the phrase collocation prob-
abilities are used and T2 is generated by the 
baseline system. In this example, since the collo-
cation probability of "? ??" is much higher 
than that of "?? ?", our method tends to split 
"? ?? ?" into "(? ??) (?)", rather than 
"(?) (?? ?)". For the phrase "?? ??" in 
the source sentence, the collocation probability 
of the translation "in order to avoid" is higher 
than that of the translation "can we avoid". Thus, 
our method selects the former as the translation. 
Although the phrase "?? ?? ?? ?? ?
?" in the source sentence has the same transla-
tion "We must adopt effective measures", our 
method splits this phrase into two parts "?? ?
?" and "?? ?? ??", because two parts 
have higher collocation probabilities than the 
whole phrase. 
We also investigate the performance of the 
system employing both the word alignment im-
provement and phrase table improvement me-
thods. From the results in Table 5, it can be seen 
that the quality of translation is future improved. 
As compared with the baseline system, an abso-
lute improvement of 2.40 BLEU score is 
achieved. And this result is also better than  the 
results shown in Table 4. 
7 Experiments on Parsing-Based SMT 
We also investigate the effectiveness of the im-
proved word alignments on the parsing-based 
SMT system, Joshua (Li et al, 2009). In this sys-
tem, the Hiero-style SCFG model is used 
(Chiang, 2007), without syntactic information. 
The rules are extracted only based on the FBIS 
corpus, where words are aligned by "MW-3 & 
CM-3". And the language model is the same as 
that in Moses. The feature weights are tuned on 
the development set using the minimum error 
??  ??  ??  ??  ??  ??  ??  ?  ??  ? 
wo-men bi-xu      cai-qu   you-xiao  cuo-shi   cai-neng  bi-mian  chu      wen-ti      . 
we          must        use      effective   measure    can        avoid    out      problem  . 
We must  adopt effective measures  in order to avoid  problems  . 
 
 
We must adopt effective measures  can we avoid  out of the  question . 
T1: 
T2: 
831
Experiments BLEU (%) 
Joshua 30.05 
+ Improved word alignments 31.81 
Table 6. Performances of Joshua using the dif-
ferent word alignments (Significantly better than 
baseline with p < 0.01) 
rate training method. We use the same evaluation 
measure as described in section 6.1. 
The translation results on Joshua are shown in 
Table 6. The system using the improved word 
alignments achieves an absolute improvement of 
1.76 BLEU score, which indicates that the im-
provements of word alignments are also effective 
to improve the performance of the parsing-based 
SMT systems. 
8 Conclusion 
We presented a novel method to use monolingual 
collocations to improve SMT. We first used the 
MWA method to identify potentially collocated 
words and estimate collocation probabilities only 
from monolingual corpora, no additional re-
source or linguistic preprocessing is needed. 
Then the collocation information was employed 
to improve BWA for various kinds of SMT sys-
tems and to improve phrase table for phrase-
based SMT. 
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. To improve 
phrase table, we calculate phrase collocation 
probabilities based on word collocation probabil-
ities. Then the phrase collocation probabilities 
are used as additional features in phrase-based 
SMT systems. 
The evaluation results showed that the pro-
posed method significantly improved word 
alignment, achieving an absolute error rate re-
duction of 29% on multi-word alignment. The 
improved word alignment results in an improve-
ment of 2.16 BLEU score on a phrase-based 
SMT system and an improvement of 1.76 BLEU 
score on a parsing-based SMT system. When we 
also used phrase collocation probabilities as ad-
ditional features, the phrase-based SMT perfor-
mance is finally improved by 2.40 BLEU score 
as compared with the baseline system. 
Reference 
Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, 
and Jorg Tiedemann. 2000. Evaluation of Word 
Alignment Systems. In Proceedings of the Second 
International Conference on Language Resources 
and Evaluation, pp. 1255-1261. 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David Ya-
rowsky. 1999. Statistical Machine Translation. Fi-
nal Report. In Johns Hopkins University Workshop. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert. L. Mercer. 1993. The Ma-
thematics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics, pp. 88-95. 
David Chiang. 2007. Hierarchical Phrase-Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
Fei Huang. 2009. Confidence Measure for Word 
Alignment. In Proceedings of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP, pp. 932-
940. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in 
Natural Language Processing, pp. 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evalua-
tion. In Processings of the International Workshop 
on Spoken Language Translation 2005. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In Proceed-
ings of the Human Language Technology Confe-
rence and the North American Association for 
Computational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
ACL, Poster and Demonstration Sessions, pp. 177-
180. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-
nitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren Thornton, Jonathan Weese, and Omar Zaidan. 
2009. Demonstration of Joshua: An Open Source 
Toolkit for Parsing-based Machine Translation. In 
Proceedings of the 47th Annual Meeting of the As-
832
sociation for Computational Linguistics, Software 
Demonstrations, pp. 25-28. 
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear 
Models for Word Alignment. 2005. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 459-466. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pp. 487-495. 
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Lan-
guage Processing,  pp. 133-139. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics, pp. 
1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of 
the 38th Annual Meeting of the Association for 
Computational Linguistics, pp. 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1): 19-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Weijing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th annual meeting of the Association 
for Computational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403. 
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 
2009. A Syntax-Driven Bracketing Model for 
Phrase-Based Translation. In Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP, pp. 315-323. 
 
833
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036?1044,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering with Source Language Collocations 
 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Ting Liu1, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu Inc., Beijing, China 
{liuzhanyi, wanghaifeng, wu_hua}@baidu.com  
{tliu, lisheng}@hit.edu.cn 
 
 
 
Abstract 
This paper proposes a novel reordering model 
for statistical machine translation (SMT) by 
means of modeling the translation orders of 
the source language collocations. The model 
is learned from a word-aligned bilingual cor-
pus where the collocated words in source sen-
tences are automatically detected. During 
decoding, the model is employed to softly 
constrain the translation orders of the source 
language collocations, so as to constrain the 
translation orders of those source phrases con-
taining these collocated words. The experi-
mental results show that the proposed method 
significantly improves the translation quality, 
achieving the absolute improvements of 
1.1~1.4 BLEU score over the baseline me-
thods. 
1 Introduction 
Reordering for SMT is first proposed in IBM mod-
els (Brown et al, 1993), usually called IBM con-
straint model, where the movement of words 
during translation is modeled. Soon after, Wu 
(1997) proposed an ITG (Inversion Transduction 
Grammar) model for SMT, called ITG constraint 
model, where the reordering of words or phrases is 
constrained to two kinds: straight and inverted. In 
order to further improve the reordering perfor-
mance, many structure-based methods are pro-
posed, including the reordering model in 
hierarchical phrase-based SMT systems (Chiang, 
2005) and syntax-based SMT systems (Zhang et al, 
2007; Marton and Resnik, 2008; Ge, 2010; Vis-
weswariah et al, 2010). Although the sentence 
structure has been taken into consideration, these 
methods don?t explicitly make use of the strong 
correlations between words, such as collocations, 
which can effectively indicate reordering in the 
target language. 
In this paper, we propose a novel method to im-
prove the reordering for SMT by estimating the 
reordering score of the source-language colloca-
tions (source collocations for short in this paper). 
Given a bilingual corpus, the collocations in the 
source sentence are first detected automatically 
using a monolingual word alignment (MWA) me-
thod without employing additional resources (Liu 
et al, 2009), and then the reordering model based 
on the detected collocations is learned from the 
word-aligned bilingual corpus. The source colloca-
tion based reordering model is integrated into SMT 
systems as an additional feature to softly constrain 
the translation orders of the source collocations in 
the sentence to be translated, so as to constrain the 
translation orders of those source phrases contain-
ing these collocated words. 
This method has two advantages: (1) it can au-
tomatically detect and leverage collocated words in 
a sentence, including long-distance collocated 
words; (2) such a reordering model can be inte-
grated into any SMT systems without resorting to 
any additional resources. 
We implemented the proposed reordering mod-
el in a phrase-based SMT system, and the evalua-
tion results show that our method significantly 
improves translation quality. As compared to the 
baseline systems, an absolute improvement of 
1.1~1.4 BLEU score is achieved.  
1036
The paper is organized as follows: In section 2, 
we describe the motivation to use source colloca-
tions for reordering, and briefly introduces the col-
location extraction method. In section 3, we 
present our reordering model. And then we de-
scribe the experimental results in section 4 and 5. 
In section 6, we describe the related work.  Lastly, 
we conclude in section 7. 
2 Collocation 
A collocation is generally composed of a group of 
words that occur together more often than by 
chance. Collocations effectively reveal the strong 
association among words in a sentence and are 
widely employed in a variety of NLP tasks 
(Mckeown and Radey, 2000).   
Given two words in a collocation, they can be 
translated in the same order as in the source lan-
guage, or in the inverted order. We name the first 
case as straight, and the second inverted. Based on 
the observation that some collocations tend to have 
fixed translation orders such as ??? jin-rong ?fi-
nancial? ??  wei-ji ?crisis?? (financial crisis) 
whose English translation order is usually straight, 
and  ???  fa-lv ?law? ??  fan-wei ?scope?? 
(scope of law) whose English translation order is 
generally inverted, some methods have been pro-
posed to improve the reordering model for SMT 
based on the collocated words crossing the neigh-
boring components (Xiong et al, 2006). We fur-
ther notice that some words are translated in 
different orders when they are collocated with dif-
ferent words. For instance, when ??? chao-liu 
?trend?? is collocated with ??? shi-dai ?times??, 
they are often translated into the ?trend of times?; 
when collocated with ??? li-shi ?history??, the 
translation usually becomes the ?historical trend?. 
Thus, if we can automatically detect the colloca-
tions in the sentence to be translated and their or-
ders in the target language, the reordering 
information of the collocations could be used to 
constrain the reordering of phrases during decod-
ing. Therefore, in this paper, we propose to im-
prove the reordering model for SMT by estimating 
the reordering score based on the translation orders 
of the source collocations. 
In general, the collocations can be automatically 
identified based on syntactic information such as 
dependency trees (Lin, 1998). However these me-
thods may suffer from parsing errors. Moreover, 
for many languages, no valid dependency parser 
exists. Liu et al (2009) proposed to automatically 
detect the collocated words in a sentence with the 
MWA method. The advantage of this method lies 
in that it can identify the collocated words in a sen-
tence without additional resources. In this paper, 
we employ MWA Model l~3 described in Liu et al 
(2009) to detect collocations in sentences, which 
are shown in Eq. (1)~(3). 
?
?
?
l
j
cj jwwtSAp 11 ModelMWA 
)|()|(
 (1) 
?
?
??
l
j
jcj lcjdwwtSAp j12 ModelMWA 
),|()|()|(
 (2) 
?
?
?
?
?
???
l
j
jcj
l
i
ii
lcjdwwt
wnSAp
j
1
1
3 ModelMWA 
),|()|(
)|()|(
 (3) 
Where lwS 1?  is a monolingual sentence; i?  de-
notes the number of words collocating with 
iw ; 
}&],1[|),{( icliciA ii ???  denotes the potentially 
collocated words in S. 
The MWA models measure the collocated 
words under different constraints. MWA Model 1 
only models word collocation probabilities 
)|( jcj wwt
. MWA Model 2 additionally employs 
position collocation probabilities 
),|( lcjd j
. Be-
sides the features in MWA Model 2, MWA Model 
3 also considers fertility probabilities )|( ii wn ? . 
Given a sentence, the optimal collocated words 
can be obtained according to Eq. (4). 
)|(maxarg*  ModelMWA SApA iA?
           (4) 
Given a monolingual word aligned corpus, the 
collocation probabilities can be estimated as fol-
lows. 
2
)|()|(),( ijjiji wwpwwpwwr ??           
(5) 
Where, 
?
?
??
w
j
ji
ji wwcount
wwcountwwp ),(
),()|(
; 
),( ji ww  
denotes the collocated words in the corpus and 
),( ji wwcount
 denotes the co-occurrence frequency. 
1037
3 Reordering Model with Source Lan-
guage Collocations 
In this section, we first describe how to estimate 
the orientation probabilities for a given collocation, 
and then describe the estimation of the reordering 
score during translation. Finally, we describe the 
integration of the reordering model into the SMT 
system. 
3.1 Reordering probability estimation 
Given a source collocation ),( ji ff
 and its corres-
ponding translations 
),( ji aa ee
 in a bilingual sen-
tence pair, the reordering orientation of the 
collocation can be defined as in Eq. (6).  
??
?
????
?????
jiji
jiji
aaji aajiaaji
aajiaajio ji &or& ifinverted
or ifstraight
,,,
(6) 
In our method, only those collocated words in 
source language that are aligned to different target 
words, are taken into consideration, and those be-
ing aligned to the same target word are ignored. 
Given a word-aligned bilingual corpus where 
the collocations in source sentences are detected, 
the probabilities of the translation orientation of 
collocations in the source language can be esti-
mated, as follows: 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,straight(),|straight(
   (7) 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,inverted(),|inverted(
   
(8) 
Here, ),,( ji ffocount
 is collected according to 
the algorithm in Figure 1. 
3.2 Reordering model 
Given a sentence lfF 1?  to be translated, the col-
locations are first detected using the algorithm de-
scribed in Eq. (4). Then the reordering score is 
estimated according to the reordering probability 
weighted by the collocation probability of the col-
located words. Formally, for a generated transla-
tion candidate T , the reordering score is calculated 
as follows. 
),|(log),(),( ,,,),( iiciii i ciaacici ciO ffopffrTFP ??
    (9) 
Input: A word-aligned bilingual corpus where 
the source collocations are detected 
Initialization: 
),,( ji ffocount
=0 
for each sentence pair <F, E> in the corpus do 
for each collocated word pair 
),( ici ff
in F do 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffstraightocount
 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffinvertedocount  
Output: ),,( ji ffocount
 
Figure 1. Algorithm of estimating  
reordering frequency 
Here, 
),( ici ffr
 denotes the collocation probabil-
ity of 
if  and icf
 as shown in Eq. (5). 
In addition to the detected collocated words in 
the sentence, we also consider other possible word 
pairs whose collocation probabilities are higher 
than a given threshold.  Thus, the reordering score 
is further improved according to Eq. (10). 
?
?
??
?
??
????
),(&
)},{(),(
,,,
,,,
),(
)},|(log),(
),|(log),(),(
ji
i
ji
iicii
i
i
ffr
ciji
jiaajiji
ciaaci
ci
ciO
ffopffr
ffopffrTFP
 
(10) 
Where ? and ?  are two interpolation weights. 
?  is the threshold of collocation probability. The 
weights and the threshold can be tuned using a de-
velopment set. 
3.3 Integrated into SMT system 
The SMT systems generally employ the log-linear 
model to integrate various features (Chiang, 2005; 
Koehn et al, 2007). Given an input sentence F, the 
final translation E* with the highest score is chosen 
from candidates, as in Eq. (11). 
}),({maxarg*
1
?
?
? M
m
mmE
FEhE ?
 (11) 
Where hm(E, F) (m=1,...,M) denotes fea-
tures.
m?  is a feature weight. 
Our reordering model can be integrated into the 
system as one feature as shown in (10). 
1038
 Figure 2. An example for reordering 
4 Evaluation of Our Method 
4.1 Implementation 
We implemented our method in a phrase-based 
SMT system (Koehn et al, 2007). Based on the 
GIZA++ package (Och and Ney, 2003), we im-
plemented a MWA tool for collocation detection. 
Thus, given a sentence to be translated, we first 
identify the collocations in the sentence, and then 
estimate the reordering score according to the 
translation hypothesis. For a translation option to 
be expanded, the reordering score inside this 
source phrase is calculated according to their trans-
lation orders of the collocations in the correspond-
ing target phrase. The reordering score crossing the 
current translation option and the covered parts can 
be calculated according to the relative position of 
the collocated words. If the source phrase matched 
by the current translation option is behind the cov-
ered parts in the source sentence, then 
...)|staight(log ?op  is used, otherwise 
...)|inverted(log ?op . For example, in Figure 2, the 
current translation option is (
4332 eeff ? ). The 
collocations related to this translation option are 
),( 31 ff , ),( 32 ff , ),( 53 ff . The reordering scores 
can be estimated as follows: 
),|straight(log),( 3131 ffopffr ? 
),|inverted(log),( 3232 ffopffr ? 
),|inverted(log),( 5353 ffopffr ? 
In order to improve the performance of the de-
coder, we design a heuristic function to estimate 
the future score, as shown in Figure 3. For any un-
covered word and its collocates in the input sen-
tence, if the collocate is uncovered, then the higher 
reordering probability is used. If the collocate has 
been covered, then the reordering orientation can 
Input: Input sentence LfF 1?  
Initialization: Score = 0 
for each uncovered word 
if  do 
for each word
jf
(
icj ?  
or 
??)( , ji ffr
) do 
if 
jf
 is covered then 
if i > j then 
Score+=
),|straight(log)( , jiji ffopffr ?
 
else 
Score+=
),|inverted(log)( , jiji ffopffr ? 
else 
 Score +=
),|(log)(maxarg , jijio ffopffr
 
Output: Score 
Figure 3. Heuristic function for estimating future 
score 
be determined according to the relative positions of 
the words and the corresponding reordering proba-
bility is employed. 
4.2 Settings 
We use the FBIS corpus (LDC2003E14) to train a 
Chinese-to-English phrase-based translation model. 
And the SRI language modeling toolkit (Stolcke, 
2002) is used to train a 5-gram language model on 
the English sentences of FBIS corpus.  
We used the NIST evaluation set of 2002 as the 
development set to tune the feature weights of the 
SMT system and the interpolation parameters, 
based on the minimum error rate training method 
(Och, 2003), and the NIST evaluation sets of 2004 
and 2008 (MT04 and MT08) as the test sets. 
We use BLEU (Papineni et al, 2002) as evalua-
tion metrics. We also calculate the statistical signi-
ficance differences between our methods and the 
baseline method by using the paired bootstrap re-
sample method (Koehn, 2004). 
4.3 Translation results 
We compare the proposed method with various 
reordering methods in previous work. 
Monotone model: no reordering model is used. 
Distortion based reordering (DBR) model: a 
distortion based reordering method (Al-
Onaizan & Papineni, 2006). In this method, the 
distortion cost is defined in terms of words, ra-
ther than phrases. This method considers out-
bound, inbound, and pairwise distortions that  
f1    f2     f3     f4      f5 
e4 
e3 
e2 
e1 
1039
Reorder models MT04 MT08 
Monotone model 26.99 18.30 
DBR model 26.64 17.83 
MSDR model (Baseline) 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
SCBR Model 1 29.21 19.28 
SCBR Model 2 29.44 19.36 
SCBR Model 3 29.50 19.44 
SCBR models (1+2) 29.65 19.57 
SCBR models (1+2+3) 29.75 19.61 
Table 1. Translation results on various reordering models 
 
T1: The two sides are also the basic stand of not relaxed. 
T2: The basic stance of the two sides have not relaxed. 
Reference: The basic stances of both sides did not move. 
Figure 4. Translation example.  (*/*) denotes (pstraight / pinverted)
 are directly estimated by simple counting over 
alignments in the word-aligned bilingual cor-
pus. This method is similar to our proposed 
method. But our method considers the transla-
tion order of the collocated words. 
msd-bidirectional-fe reordering (MSDR or 
Baseline) model: it is one of the reordering 
models in Moses. It considers three different 
orientation types (monotone, swap, and discon-
tinuous) on both source phrases and target 
phrases. And the translation orders of both the 
next phrase and the previous phrase in respect 
to the current phrase are modeled. 
Source collocation based reordering (SCBR) 
model: our proposed method. We investigate 
three reordering models based on the corres-
ponding MWA models and their combinations. 
In SCBR Model i (i=1~3), we use MWA Mod-
el i as described in section 2 to obtain the col-
located words and estimate the reordering 
probabilities according to section 3. 
The experiential results are shown in Table 1. 
The DBR model suffers from serious data sparse-
ness. For example, the reordering cases in the 
trained pairwise distortion model only covered 
32~38% of those in the test sets. So its perfor-
mance is worse than that of the monotone model. 
The MSDR model achieves higher BLEU scores 
than the monotone model and the DBR model. Our 
models further improve the translation quality, 
achieving better performance than the combination 
of MSDR model and DBR model. The results in 
Table 1 show that ?MSDR + SCBR Model 3? per-
forms the best among the SCBR models. This is 
because, as compared to MWA Model 1 and 2, 
MWA Model 3 takes more information into con-
sideration, including not only the co-occurrence 
information of lexical tokens and the position of 
words, but also the fertility of words in a sentence. 
And when the three SCBR models are combined, 
the performance of the SMT system is further im-
proved. As compared to other reordering models, 
our models achieve an absolute improvement of 
0.98~1.19 BLEU score on the test sets, which are 
statistically significant (p < 0.05).  
Figure 4 shows an example: T1 is generated by 
the baseline system and T2 is generated by the sys-
tem where the SCBR models (1+2+3)1 are used.  
                                                          
1 In the remainder of this paper, ?SCBR models? means the 
combination of the SCBR models (1+2+3) unless it is explicit-
ly explained.  
Input:  ??     ?   ??      ??  ?    ?  ??  ??   ? 
shuang-fang    DE    ji-ben       li-chang   ye      dou mei-you song-dong . 
(0.99/0.01) 
both-side       DE     basic          stance  also    both    not        loose     . 
(0.21/0.79) 
(0.95/0.05) 
1040
Reordering models MT04 MT08 
MSDR model 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
CBR model 28.96 18.77 
WCBR model 29.15 19.10 
WCBR+SCBR 
models 
29.87 19.83 
Table 2. Translation results of co-occurrence 
based reordering models 
 CBR model 
SCBR 
Model3 
Consecutive words 77.9% 73.5% 
Interrupted words 74.1% 87.8% 
Total 74.3% 84.9% 
Table 3. Precisions of the reordering models on 
the development set 
The input sentence contains three collocations. The 
collocation (??, ??) is included in the same 
phrase and translated together as a whole. Thus its 
translation is correct in both translations. For the 
other two long-distance collocations (??, ??) 
and (??, ??), their translation orders are not 
correctly handled by the reordering model in the 
baseline system. For the collocation (??, ??), 
since the SCBR models indicate p(o=straight|??, 
??) < p(o=inverted|??, ??), the system fi-
nally generates the translation T2 by constraining 
their translation order with the proposed model. 
5 Collocations vs. Co-occurring Words 
We compared our method with the method that 
models the reordering orientations based on co-
occurring words in the source sentences, rather 
than the collocations.  
5.1 Co-occurrence based reordering model 
We use the similar algorithm described in section 3 
to train the co-occurrence based reordering (CBR) 
model, except that the probability of the reordering 
orientation is estimated on the co-occurring words 
and the relative distance. Given an input sentence 
and a translation candidate, the reordering score is 
estimated as shown in Eq. (12). 
? ??? ),( ,,, ),,|(log),( ji jijiaajiO ffopTFP ji
        (12) 
Here, 
ji??
 is the relative distance of two words 
in the source sentence.  
We also construct the weighted co-occurrence 
based reordering (WCBR) model. In this model, 
the probability of the reordering orientation is ad-
ditionally weighted by the pointwise mutual infor-
mation 2  score of the two words (Manning and 
Sch?tze, 1999), which is estimated as shown in Eq. 
(13). 
? ???
),(
,,,MI ),,|(log),(
),(
ji
jijiaajiji
O
ffopffs
TFP
ji
   (13) 
5.2 Translation results 
Table 2 shows the translation results. It can be seen 
that the performance of the SMT system is im-
proved by integrating the CBR model. The perfor-
mance of the CBR model is also better than that of 
the DBR model. It is because the former is trained 
based on all co-occurring aligned words, while the 
latter only considers the adjacent aligned words. 
When the WCBR model is used, the translation 
quality is further improved. However, its perfor-
mance is still inferior to that of the SCBR models, 
indicating that our method (SCBR models) of 
modeling the translation orders of source colloca-
tions is more effective. Furthermore, we combine 
the weighted co-occurrence based model and our 
method, which outperform all the other models. 
5.3 Result analysis 
Precision of prediction 
First of all, we investigate the performance of 
the reordering models by calculating precisions of 
the translation orders predicted by the reordering 
models. Based on the source sentences and refer-
ence translations of the development set, where the 
source words and target words are automatically 
aligned by the bilingual word alignment method, 
we construct the reference translation orders for 
two words. Against the references, we calculate 
three kinds of precisions as follows: 
|}1|||{|
|}&1{|
,
,,,,
CW ??
???? jio
ooj||iP
ji
aajiji ji
 (14) 
                                                          
2 For occurring words extraction, the window size is set to [-6, 
+6]. 
1041
|}1|||{|
|}&1{|
,
,,,,
IW ??
???? jio
ooj||iP
ji
aajiji ji
 (15) 
 |}{|
|}{|
,
,,,,
total
ji
aajiji
o
ooP ji??
 (16) 
Here, 
jio ,
 denotes the translation order of (
ji ff ,
) 
predicted by the reordering models. If 
)|straight( , ji ffop ?
>
),inverted( ji f|fop ?
, then 
straight, ?jio
, else if 
)|straight( , ji ffop ?
< 
),inverted( ji f|fop ?
, then
inverted, ?jio
. 
ji aajio ,,,
 
denotes the translation order derived from the word 
alignments. If 
ji aajiji oo ,,,, ?
, then the predicted 
translation order is correct, otherwise wrong. 
CWP  
and 
IWP  denote the precisions calculated on the 
consecutive words and the interrupted words in the 
source sentences, respectively. 
totalP  denotes the 
precision on both cases. Here, the CBR model and 
SCBR Model 3 are compared. The results are 
shown in Table 3.  
From the results in Table 3, it can be seen that 
the CBR model has a higher precision on the con-
secutive words than the SCBR model, but lower 
precisions on the interrupted words. It is mainly 
because the CBR model introduces more noise 
when the relative distance of words is set to a large 
number, while the MWA method can effectively 
detect the long-distance collocations in sentences 
(Liu et al, 2009). This explains why the combina-
tion of the two models can obtain the highest 
BLEU score as shown in Table 2. On the whole, 
the SCBR Model 3 achieves higher precision than 
the CBR model. 
Effect of the reordering model 
Then we evaluate the reordering results of the 
generated translations in the test sets. Using the 
above method, we construct the reference transla-
tion orders of collocations in the test sets. For a 
given word pair in a source sentence, if the transla-
tion order in the generated translation is the same 
as that in the reference translations, then it is cor-
rect, otherwise wrong. 
We compare the translations of the baseline me-
thod, the co-occurrence based method, and our me-
thod (SCBR models). The precisions calculated on 
both kinds of words are shown in Table 4. From 
Test sets 
Baseline 
(MSDR) 
MSDR+ 
WCBR 
MSDR+ 
SCBR 
MT04 78.9% 80.8% 82.5% 
MT08 80.7% 83.8% 85.0% 
Table 4. Precisions (total) of the reordering 
models on the test sets 
the results, it can be seen that our method achieves 
higher precisions than both the baseline and the 
method modeling the translation orders of the co-
occurring words. It indicates that the proposed me-
thod effectively constrains the reordering of source 
words during decoding and improves the transla-
tion quality. 
6 Related Work 
Reordering was first proposed in the IBM models 
(Brown et al, 1993), later was named IBM con-
straint by Berger et al (1996). This model treats 
the source word sequence as a coverage set that is 
processed sequentially and a source token is cov-
ered when it is translated into a new target token. 
In 1997, another model called ITG constraint was 
presented, in which the reordering order can be 
hierarchically modeled as straight or inverted for 
two nodes in a binary branching structure (Wu, 
1997). Although the ITG constraint allows more 
flexible reordering during decoding, Zens and Ney 
(2003) showed that the IBM constraint results in 
higher BLEU scores. Our method models the reor-
dering of collocated words in sentences instead of 
all words in IBM models or two neighboring 
blocks in ITG models. 
For phrase-based SMT models, Koehn et al 
(2003) linearly modeled the distance of phrase 
movements, which results in poor global reorder-
ing. More methods are proposed to explicitly mod-
el the movements of phrases (Tillmann, 2004; 
Koehn et al, 2005) or to directly predict the orien-
tations of phrases (Tillmann and Zhang, 2005; 
Zens and Ney, 2006), conditioned on current 
source phrase or target phrase. Hierarchical phrase-
based SMT methods employ SCFG bilingual trans-
lation model and allow flexible reordering (Chiang, 
2005). However, these methods ignored the corre-
lations among words in the source language or in 
the target language. In our method, we automati-
cally detect the collocated words in sentences and 
1042
their translation orders in the target languages, 
which are used to constrain the ordering models 
with the estimated reordering (straight or inverted) 
score. Moreover, our method allows flexible reor-
dering by considering both consecutive words and 
interrupted words. 
In order to further improve translation results, 
many researchers employed syntax-based reorder-
ing methods (Zhang et al, 2007; Marton and Res-
nik, 2008; Ge, 2010; Visweswariah et al, 2010). 
However these methods are subject to parsing er-
rors to a large extent. Our method directly obtains 
collocation information without resorting to any 
linguistic knowledge or tools, therefore is suitable 
for any language pairs. 
In addition, a few models employed the collo-
cation information to improve the performance of 
the ITG constraints (Xiong et al, 2006). Xiong et 
al. used the consecutive co-occurring words as col-
location information to constrain the reordering, 
which did not lead to higher translation quality in 
their experiments. In our method, we first detect 
both consecutive and interrupted collocated words 
in the source sentence, and then estimated the 
reordering score of these collocated words, which 
are used to softly constrain the reordering of source 
phrases. 
7 Conclusions 
We presented a novel model to improve SMT by 
means of modeling the translation orders of source 
collocations. The model was learned from a word-
aligned bilingual corpus where the potentially col-
located words in source sentences were automati-
cally detected by the MWA method. During 
decoding, the model is employed to softly con-
strain the translation orders of the source language 
collocations. Since we only model the reordering 
of collocated words, our methods can partially al-
leviate the data sparseness encountered by other 
methods directly modeling the reordering based on 
source phrases or target phrases. In addition, this 
kind of reordering information can be integrated 
into any SMT systems without resorting to any 
additional resources. 
The experimental results show that the pro-
posed method significantly improves the transla-
tion quality of a phrase based SMT system, 
achieving an absolute improvement of 1.1~1.4 
BLEU score over the baseline methods. 
References 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In 
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL, pp. 529-536. 
Adam L. Berger, Peter F. Brown, Stephen A. Della Pie-
tra, Vincent J. Della Pietra, Andrew S. Kehler, and 
Robert L. Mercer. 1996. Language Translation Appa-
ratus and Method of Using Context-Based Transla-
tion Models. United States Patent, Patent Number 
5510981, April.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Pietra, and Robert. L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parameter 
estimation. Computational Linguistics, 19(2): 263-
311. 
David Chiang. 2005. A Hierarchical Phrase-based Mod-
el for Statistical Machine Translation. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 263-270. 
Niyu Ge. 2010. A Direct Syntax-Driven Reordering 
Model for Phrase-Based Machine Translation. In 
Proceedings of Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the ACL, pp. 849-857. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 388-395. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. In 
Proceedings of the 45th Annual Meeting of the ACL, 
Poster and Demonstration Sessions, pp. 177-180. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of International Workshop on Spoken 
Language Translation. 
1043
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 2009 
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 487-495. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language 
Processing, Cambridge, MA; London, U.K.: Brad-
ford Book & MIT Press. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrased-based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics: Hu-
man Language Technologies, pp. 1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1) : 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Weij-
ing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Christoph Tillmann. 2004. A Unigram Orientation 
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 101-104. 
Christoph Tillmann and Tong Zhang. 2005. A Localized 
Prediction Model for Statistical Machine Translation. 
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pp. 557-564. 
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, 
Vijil Chenthamarakshan, and Nanda Kambhatla. 
2010. Syntax Based Reordering with Automatically 
Derived Rules for Improved Statistical Machine 
Translation. In Proceedings of the 23rd International 
Conference on Computational Linguistics, pp. 1119-
1127. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
the 21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 521-528. 
Richard Zens and Herman Ney. 2003. A Comparative 
Study on Reordering Constraints in Statistical Ma-
chine Translation. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, pp. 192-202. 
Richard Zens and Herman Ney. 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In Proceedings of the Workshop on Statistical 
Machine Translation, pp. 55-63. 
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 
2007. Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT. In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning, pp. 533-540. 
 
 
 
1044
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407?410,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
HIT-CIR: An Unsupervised WSD System Based on Domain Most
Frequent Sense Estimation
Yuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng Li
Harbin Institute of Technolgy
Harbin, Heilongjiang, PRC
yhguo@ir.hit.edu.cn
Abstract
This paper presents an unsupervised sys-
tem for all-word domain specific word
sense disambiguation task. This system
tags target word with the most frequent
sense which is estimated using a thesaurus
and the word distribution information in
the domain. The thesaurus is automati-
cally constructed from bilingual parallel
corpus using paraphrase technique. The
recall of this system is 43.5% on SemEval-
2 task 17 English data set.
1 Introduction
Tagging polysemous word with its most frequent
sense (MFS) is a popular back-off heuristic in
word sense disambiguation (WSD) systems when
the training data is inadequate. In past evalua-
tions, MFS from WordNet performed even bet-
ter than most of the unsupervised systems (Snyder
and Palmer, 2004; Navigli et al, 2007).
MFS is usually obtained from a large scale
sense tagged corpus, such as SemCor (Miller et al,
1994). However, some polysemous words have
different MFS in different domains. For example,
in the Koeling et al (2005) corpus, target word
coach means ?manager? mostly in the SPORTS
domain but means ?bus? mostly in the FINANCE
domain. So when the MFS is applied to specific
domains, it needs to be re-estimated.
McCarthy et al (2007) proposed an unsuper-
vised predominant word sense acquisition method
which obtains domain specific MFS without sense
tagged corpus. In their method, a thesaurus, in
which words are connected with their distribu-
tional similarity, is constructed from the domain
raw text. Word senses are ranked by their preva-
lence score which is calculated using the thesaurus
and the sense inventory.
In this paper, we propose another way to con-
struct the thesaurus. We use statistical machine
Figure 1: The architecture of HIT-CIR
translation (SMT) techniques to extract paraphrase
pairs from bilingual parallel text. In this way, we
avoid calculating similarities between every pair
of words and could find semantic similar words or
compounds which have dissimilar distributions.
Our system is comprised of two parts: the word
sense ranking part and the word sense tagging part.
Senses are ranked according to their prevalence
score in the target domain, and the predominant
sense is used to tag the occurrences of the target
word in the test data. The architecture of this sys-
tem is shown in Figure 1.
The word sense ranking part includes following
steps.
1. Tag the POS of the background text, count
the word frequency in each POS, and get the
polysemous word list of the POS.
2. Using SMT techniques to extract phrase table
407
Figure 2: Word sense ranking for the noun backbone
from the bilingual corpus. Extract the para-
phrases (called as neighbor words) with the
phrase table for each word in the polysemous
word list.
3. Calculate the prevalence score of each sense
of the target words, rank the senses with the
score and obtain the predominant sense.
We applied our system on the English data set
of SemEval-2 specific domain WSD task. This
task is an all word WSD task in the environ-
mental domain. We employed the domain back-
ground raw text provided by the task organizer as
well as the English WordNet 3.0 (Fellbaum, 1998)
and the English-Spanish parallel corpus from Eu-
roparl (Koehn, 2005).
This paper is organized as follows. Section 2
introduces how to rank word senses. Section 3
presents how to obtain the most related words of
the target words. We describe the system settings
in Section 4 and offer some discussions in Sec-
tion 5.
2 Word Sense Ranking
In our method, word senses are ranked according
to their prevalence score in the specific domain.
According to the assumption of McCarthy et al
(2007), the prevalence score is affected by the fol-
lowing two factors: (1) The relatedness score be-
tween a given sense of the target word and the
target word?s neighbor word. (2) The similarity
between the target word and its neighbor word.
In addition, we add another factor, (3) the impor-
tance of the neighbor word in the specific domain.
In this paper, ?neighbor words? means the words
which are most semantically similar to the target
word.
Figure 2 illustrates the word sense ranking pro-
cess of noun backbone. The contribution of a
neighbor word to a given word sense is measured
by the similarity between them and weighted by
the importance of the neighbor word in the tar-
get domain and the relatedness between the neigh-
bor word and the target word. Sum up the con-
tributions of each neighbor words, and we get the
prevalence score of the word sense.
Formally, the prevalence score of sense s
i
of a
target word w is assigned as follows:
ps(w, s
i
) =
?
n
j
?N
w
rs(w, n
j
) ? ns(s
i
, n
j
) ? dw(n
j
)
(1)
where
ns(s
i
, n
j
) =
sss(s
i
, n
j
)
?
s
i
?
?senses(w)
sss(s
i
?
, n
j
)
, (2)
sss(s
i
, n
j
) = max
s
x
?senses(n
j
)
sss
?
(s
i
, s
x
). (3)
rs(w, n
j
) is the relatedness score between w and
a neighbor word n
j
. N
w
= {n
1
, n
2
, . . . , n
k
}
is the top k relatedness score neighbor word set.
ns(s
i
, n
j
) is the normalized form of the sense sim-
ilarity score between sense s
i
and the neighbor
word n
j
(i.e. sss(s
i
, n
j
)). We define this score
with the maximum WordNet similarity score be-
tween s
i
and the senses of n
j
(i.e. sss
?
(s
i
, n
j
)).
In our system, lesk algorithm is used to measure
the sense similarity score between word senses.
408
Figure 3: Finding the neighbor words of noun backbone
The similarity of this algorithm is the count of
the number of overlap words in the gloss or the
definition of the senses (Banerjee and Pedersen,
2002). The domain importance weight dw(n
j
) is
assigned with the count of n
j
in the domain back-
ground corpus. For the neighbor word that does
not occur in the domain background text, we use
the add-one strategy. We will describe how to ob-
tain n
j
and rs in Section 3.
3 Thesaurus Construction
The neighbor words of the target word as well as
the relatedness score are obtained by extracting
paraphrases from bilingual parallel texts. When
a word is translated from source language to tar-
get language and then translated back to the source
language, the final translation may have the same
meaning to the original word but with different ex-
pressions (e.g. different word or compound). The
translation in the same language could be viewed
as a paraphrase term or, at least, related term of the
original word.
For example, in Figure 3, English noun back-
bone can be translated to columna, columna verte-
bral, pilar and convicciones etc. in Spanish, and
these words also have other relevant translations
in English, such as vertebral column, column, pil-
lar and convictions etc., which are semantically re-
lated to the target word backbone.
We use a statistical machine translation sys-
tem to calculate the translation probability from
English to another language (called as pivot lan-
guage) as well as the translation probability from
that language to English. By multiplying these
two probabilities, we get a paraphrase probabil-
ity. This method was defined in (Bannard and
Callison-Burch, 2005).
In our system, we choose the top k paraphrases
as the neighbor words of the target word, which
have the highest paraphrase probability. Note that
there are two directions of the paraphrase, from
target word to its neighbor word and from the
neighbor word to the target word. We choose
the paraphrase score of the former direction as
the relatedness score (rs). Because the higher
of the score in this direction, the target word is
more likely paraphrased to that neighbor word,
and hence the prevalence of the relevant target
word sense will be higher than other senses. For-
mally, the relatedness score is given by
rs(w, n
j
) =
?
f
p(f |w)p(n
j
|f), (4)
where f is the pivot language word.
We use the English-Spanish parallel text from
Europarl (Koehn, 2005). We choose Spanish as
the pivot language because in the both directions
the BLEU score of the translation between English
and Spanish is relatively higher than other English
and other languages (Koehn, 2005).
4 Data set and System Settings
The organizers of the SemEval-2 specific domain
WSD task provide no training data but raw back-
ground data in the environmental domain. The En-
glish background data is obtained from the offi-
cial web site of World Wide Fund (WWF), Euro-
pean Centre for Nature Conservation (ECNC), Eu-
ropean Commission and the United Nations Eco-
nomic Commission for Europe (UNECE). The
size of the raw text is around 15.5MB after sim-
ple text cleaning. The test data is from WWF and
ECNC, and contains 1398 occurrence of 436 tar-
get words.
For the implementation, we used bpos (Shen et
al., 2007) for the POS tagging. The maximum
409
number of the neighbor word of each target word k
was set to 50. We employed Giza++
1
and Moses
2
to get the phrase table from the bilingual paral-
lel corpus. TheWordNet::Similarity package
3
was
applied for the implement of the lesk word sense
similarity algorithm.
For the target word that is not in the polysemous
word list, we use the MFS from WordNet as the
back-off method.
5 Discussion and Future Work
The recall of our system is 43.5%, which is lower
than that of the MFS baseline, 50.5% (Agirre et
al., 2010). The baseline uses the most frequent
sense from the SemCor corpus (i.e. the MFS of
WordNet). This means that for some target words,
the MFS from SemCor is better than the domain
MFS we estimated in the environmental domain.
In the future, we will analysis errors in detail to
find the effects of the domain on the MFS.
For the domain specific task, it is better to use
parallel text in the domain of the test data in our
method. However, we didn?t find any available
parallel text in the environmental domain yet. In
the future, we will try some parallel corpus acqui-
sition techniques to obtain relevant corpus for en-
vironmental domain for our method.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
1
http://www.fjoch.com/GIZA++.html
2
http://www.statmt.org/moses/
3
http://wn-similarity.sourceforge.net/
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 597?
604, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 419?426, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590, December.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1994. A semantic concordance. In Proc. ARPA
Human Language Technology Workshop ?93, pages
303?308, Princeton, NJ, March. distributed as Hu-
man Language Technology by San Mateo, CA: Mor-
gan Kaufmann Publishers.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30?35, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
410
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242
Manchester, August 2008
A Cascaded Syntactic and Semantic Dependency Parsing System
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
Abstract
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
1 System Architecture
Our CoNLL 2008 Shared Task (Surdeanu et al,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
2 Syntactic Dependency Parsing
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
2.1 Features
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
score(h, c, l) = w ? f(h, c, l) (1)
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)?s:
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word?s
prefix.
2) We add two new features: ?bet-pos-h-same-
num? and ?bet-pos-c-same-num?. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
2.2 Relabeling
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
?I read books in the room.? and ?I read books in
the afternoon.?. It is hard to correctly label the arc
238
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
Table 1: Error Analysis of Each Label
between ?read? and ?in? unless we know the object
of ?in?.
We count the errors of each label, and show the
top ones in Table 1. ?Total? refers to the total num-
ber of the corresponding label in the development
data. The column of ?Mislabeled as? lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
2.3 Relabeling using Maximum Entropy
Classifier
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
?+ dir dist? means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c?s children. ?word c c? rep-
resents form or lemma of one child of the node
c. ?dir c? and ?dist c? represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
Table 2: Relabeling Feature Set (+ dir dist)
3 Semantic Dependency Parsing
3.1 Architecture
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al, 2004).
3.2 Predicate Identification
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it?s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding ?constituent? for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A ?POS pattern? is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
239
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either ?be? or
?get?, or else the relation type is ?APPO?, then the
verb is considered passive, otherwise active.
Also we used some ?combined? features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
3.3 Predicate Classification
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
?Bag of Words?. And an ?ordered? version is in-
troduced where each word is prefixed with a letter
?L?, ?R? or ?T? indicating it?s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with ?L?, ?R? or
?T? indicating the word position joined together,
namely ?Bag of POS (Ordered)?. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely ?Bag of POS
(Numbered)?.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a ?window?, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
3.4 Semantic Role Classification
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al, 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The ?POS
Path? feature consists of POS tags of the words
along the path from a word to the predicate. Other
than ?Up? and ?Down?, the ?Left? and ?Right? di-
rection of the path is added. Similarly, the ?Re-
lation Path? feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): ?Up-
stream paths? are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): ?Familyship rela-
tion? between a word and the predicate, being one
of ?self?, ?child?, ?descendant?, ?parent?, ?ances-
tor?, ?sibling?, and ?not-relative?.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ?p?. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
240
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
3.5 ILP-based Post Inference
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
?NULL? is also added to R, representing ?none of
the roles is assigned?.
For each word w ? W and semantic role label
r ? R we create a binary variable v
wr
? (0, 1),
whose value indicates whether or not the word w
is labeled as label r. p
wr
denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f =
?
w,r
log(p
wr
? v
wr
)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
v
wr
. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
?NULL?), i.e.:
?
r
v
wr
= 1
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
v
wr
= 0, if p
wr
< 0.3 and r 6= ?NULL?
C3: Statistics shows that the most roles (ex-
cept for the virtual role ?NULL?) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles? duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
?
r
v
wr
? 1,
if < p, r > /? {< p, r > |p ? P, r ? R;
d
pr
c
pr
?d
pr
> 0.3 ? d
pr
> 10}
(2)
where P is the set of predicates; c
pr
denotes the
count of words in the training corpus, which are
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
labeled as r ? R for predicate p ? P ; while d
pr
denotes something similar to c
pr
, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
4 Experiments
The original MSTParser
1
is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit
2
. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.5
3
is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
4.1 Syntactic Dependency Parsing
The experiments on development data show that
relabeling process is helpful, which improves the
1
http://sourceforge.net/projects/mstparser
2
http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3
http://sourceforge.net/projects/lpsolve
241
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
Table 4: The performance of predicate identifica-
tion and classification
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
Table 5: Comparison between different post infer-
ence strategies
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
4.2 Semantic Dependency Parsing
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label ?NULL?)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
Table 6: Semantic dependency parsing perfor-
mances
4.3 Overall Performance
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
5 Conclusion and Future Work
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
Acknowledgments
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
?863? National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
References
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
242
 Selecting Optimal Feature Template Subset for CRFs 
Xingjun Xu1 and Guanglu Sun2 and Yi Guan1 and  
Xishuang Dong1 and Sheng Li1 
1: School of Computer Science and Technology,  
Harbin Institute of Technology,  
150001, Harbin, China 
2: School of Computer Science and Technology,  
Harbin University of Science and Technology 
150080, Harbin, China 
 xxjroom@163.com; guanglu.sun@gmail.com 
guanyi@hit.edu.cn; dongxishuang@gmail.com 
lisheng@hit.edu.cn 
 
 
 
Abstract 
Conditional Random Fields (CRFs) are the 
state-of-the-art models for sequential labe-
ling problems. A critical step is to select 
optimal feature template subset before em-
ploying CRFs, which is a tedious task. To 
improve the efficiency of this step, we pro-
pose a new method that adopts the maxi-
mum entropy (ME) model and maximum 
entropy Markov models (MEMMs) instead 
of CRFs considering the homology be-
tween ME, MEMMs, and CRFs. Moreover, 
empirical studies on the efficiency and ef-
fectiveness of the method are conducted in 
the field of Chinese text chunking, whose 
performance is ranked the first place in 
task two of CIPS-ParsEval-2009. 
1 Introduction 
Conditional Random Fields (CRFs) are the state-
of-the-art models for sequential labeling problem. 
In natural language processing, two aspects of 
CRFs have been investigated sufficiently: one is to 
apply it to new tasks, such as named entity recog-
nition (McCallum and Li, 2003; Li and McCallum, 
2003; Settles, 2004), part-of-speech tagging (Laf-
ferty et al, 2001), shallow parsing (Sha and Perei-
ra, 2003), and language modeling (Roark et al, 
2004); the other is to exploit new training methods 
for CRFs, such as improved iterative scaling (Laf-
ferty et al, 2001), L-BFGS (McCallum, 2003) and 
gradient tree boosting (Dietterich et al, 2004). 
One of the critical steps is to select optimal fea-
ture subset before employing CRFs. McCallum 
(2003) suggested an efficient method of feature 
induction by iteratively increasing conditional log-
likelihood for discrete features. However, since 
there are millions of features and feature selection 
is an NP problem, this is intractable when search-
ing optimal feature subset. Therefore, it is neces-
sary that selects feature at feature template level, 
which reduces input scale from millions of fea-
tures to tens or hundreds of candidate templates. 
In this paper, we propose a new method that 
adopts ME and MEMMs instead of CRFs to im-
prove the efficiency of selecting optimal feature 
template subset considering the homology between 
ME, MEMMs, and CRFs, which reduces the train-
ing time from hours to minutes without loss of 
performance. 
The rest of this paper is organized as follows. 
Section 2 presents an overview of previous work 
for feature template selection. We propose our op-
timal method for feature template selection in Sec-
tion 3. Section 4 presents our experiments and re-
sults. Finally, we end this paper with some con-
cluding remarks. 
2 Related Work 
Feature selection can be carried out from two le-
vels: feature level (feature selection, or FS), or 
feature template level (feature template selection, 
or FTS). FS has been sufficiently investigated and 
 share most concepts with FTS. For example, the 
target of FS is to select a subset from original fea-
ture set, whose optimality is measured by an eval-
uation criterion (Liu and Yu, 2005). Similarly, the 
target of FTS is to select a subset from original 
feature template set. To achieve optimal feature 
subset, two problems in original set must be elimi-
nated: irrelevance and redundancy (Yu and Liu, 
2004). The only difference between FS and FTS is 
that the number of elements in feature template set 
is much less than that in feature set. 
Liu and Yu (2005) classified FS models into 
three categories: the filter model, the wrapper 
model, and the hybrid model. The filter model 
(Hall 2000; Liu and Setiono, 1996; Yu and Liu, 
2004) relies on general characteristics of the data 
to evaluate and select feature subsets without any 
machine learning model. The wrapper model (Dy 
and Brodley, 2000; Kim et al, 2000; Kohavi and 
John, 1997) requires one predetermined machine 
learning model and uses its performance as the 
evaluation criterion. The hybrid model (Das, 2001) 
attempts to take advantage of the two models by 
exploiting their different evaluation criteria in dif-
ferent search stages. 
There are two reasons to employ the wrapper 
model to accomplish FTS: (1) The wrapper model 
tends to achieve better effectiveness than that of 
the filter model with respect of a more direct eval-
uation criterion; (2) The computational cost is trac-
table because it can reduce the number of subsets 
sharply by heuristic algorithm according to the 
human knowledge. And our method belongs to 
this type. 
Lafferty (2001) noticed the homology between 
MEMMs and CRFs, and chose optimal MEMMs 
parameter vector as a starting point for training the 
corresponding CRFs. And the training process of 
CRFs converges faster than that with all zero pa-
rameter vectors. 
On the other hand, the general framework that 
processes sequential labeling with CRFs has also 
been investigated well, which can be described as 
follows: 
1. Converting the new problem to sequential 
labeling problem; 
2. Selecting optimal feature template subset for 
CRFs; 
3. Parameter estimation for CRFs; 
4. Inference for new data. 
In the field of English text chunking (Sha and 
Pereira, 2003), the step 1, 3, and 4 have been stu-
died sufficiently, whereas the step 2, how to select 
optimal feature template subset efficiently, will be 
the main topic of this paper.  
3 Feature Template Selection 
3.1 The Wrapper Model for FTS 
The framework of FTS based on the wrapper 
model for CRFs can be described as: 
1. Generating the new feature template subset; 
2. Training a CRFs model; 
3. Updating optimal feature template subset if the 
new subset is better; 
4. Repeating step 1, 2, 3 until there are no new 
feature template subsets. 
Let N denote the number of feature templates, 
the number of non-empty feature template subsets 
will be (2N-1). And the wrapper model is unable to 
deal with such case without heuristic methods, 
which contains: 
1. Atomic feature templates are firstly added to 
feature template subset, which is carried out by: 
Given the position i, the current word Wi and the 
current part-of-speech Pi are firstly added to cur-
rent feature template subset, and then Wi-1 and Pi-1, 
or Wi+1 and Pi+1, and so on, until the effectiveness 
is of no improvement. Taking the Chinese text 
chunking as example, optimal atomic feature tem-
plate subset is {Wi-3~Wi+3, Pi-3~Pi+3}; 
2. Adding combined feature templates properly 
to feature template set will be helpful to improve 
the performance, however, too many combined 
feature templates will result in severe data sparse-
ness problem. Therefore, we present three restric-
tions for combined feature templates: (1) A com-
bined feature template that contains more than 
three atomic templates are not allowable; (2) If a 
combined feature template contains three atomic 
feature template, it can only contain at most one 
atomic word template; (3) In a combined template, 
at most one word is allowable between the two 
most adjacent atomic templates; For example, the 
combined feature templates, such as {Pi-1, Pi, Pi+1, 
Pi+2}, {Wi, Wi+1, Pi},  and {Pi-1, Pi+2}, are not al-
lowable, whereas the combined templates, such as 
{Pi, Pi+1, Pi+2}, {Pi-1, Wi, Pi+1}, and {Pi-1, Pi+1}, are 
allowable. 
3. After atomic templates have been added, {Wi-
1, Wi}, or {Wi, Wi+1}, or {Pi-1, Pi}, or {Pi, Pi+1} are 
firstly added to feature template subset. The tem-
plate window is moved forward, and then back-
ward. Such process will repeat with expanding 
template window, until the effectiveness is of no 
improvement. 
 Tens or hundreds of training processes are still 
needed even if the heuristic method is introduced. 
People usually employ CRFs model to estimate the 
effectiveness of template subset However, this is 
more tedious than that we use ME or MEMMs 
instead. The idea behind this lie in three aspects: 
first, in one iteration, the Forward-Backward Al-
gorithm adopted in CRFs training is time-
consuming; second, CRFs need more iterations 
than that of ME or MEMMs to converge because 
of larger parameter space; third, ME, MEMMs, 
and CRFs, are of the same type (log-linear models) 
and based on the same principle, as will be dis-
cussed in detail as follows. 
3.2 Homology of ME, MEMMs and CRFs 
ME, MEMMs, and CRFs are all based on the Prin-
ciple of Maximum Entropy (Jaynes, 1957). The 
mathematical expression for ME model is as for-
mula (1): 
1
1( | ) exp( ( , ))( )
m
i ii
P y x x yZ x f??? ?
    (1) 
, and Z(x) is the normalization factor. 
MEMMs can be considered as a sequential ex-
tension to the ME model. In MEMMs, the HMM 
transition and observation functions are replaced 
by a single function P(Yi|Yi-1, Xi). There are three 
kinds of implementations of MEMMs (McCallum 
et al, 2000) in which we realized the second type 
for its abundant expressiveness. In implementation 
two, which is denoted as MEMMs_2 in this paper, 
a distributed representation for the previous state 
Yi-1 is taken as a collection of features with 
weights set by maximum entropy, just as we have 
done for the observations Xi. However, label bias 
problem (Lafferty et al, 2001) exists in MEMMs, 
since it makes a local normalization of random 
field models. CRFs overcome the label bias prob-
lem by global normalization. 
Considering the homology between CRFs and 
MEMMs_2 (or ME), it is reasonable to suppose 
that a useful template for MEMMs_2 (or ME) is 
also useful for CRFs, and vice versa. And this is a 
necessary condition to replace CRFs with ME or 
MEMMs for FTS. 
3.3 A New Framework for FTS 
Besides the homology of these models, the other 
necessary condition to replace CRFs with ME or 
MEMMs for FTS is that all kinds of feature tem-
plates in CRFs can also be expressed by ME or 
MEMMs. There are two kinds of feature templates 
for CRFs: one is related to Yi-1, which is denoted 
as g(Yi-1, Yi, Xi); the other is not related to Yi-1, 
which is denoted as f(Yi, Xi). Both of them can be 
expressed by MEMMs_2. If there is only the 
second kind of feature templates in the subset, it 
can also be expressed by ME. For example, the 
feature function f(Yi, Pi) in CRFs can be expressed 
by feature template {Pi} in MEMMs_2 or ME; and 
g(Yi-1, Yi, Pi) can be expressed by feature template 
{Yi-1, Pi} in MEMM_2.  
Therefore, MEMMs_2 or ME can be employed 
to replace CRFs as machine learning model for 
improving the efficiency of   FTS. 
Then the new framework for FTS will be: 
1. Generating the new feature template subset; 
2. Training an MEMMs_2 or ME model; 
3. Updating optimal feature template subset 
if the new subset is better; 
4. Repeating step 1, 2, 3 until there are no 
new feature template subsets. 
The wrapper model evaluates the effectiveness 
of feature template subset by evaluating the model 
on testing data. However, there is a serious effi-
ciency problem when decoding a sequence by 
MEMMs_2. Given N as the length of a sentence, 
C as the number of candidate labels, the time 
complexity based on MEMMs_2 is O(NC2) when 
decoding by viterbi algorithm. Considering the C 
different Yi-1 for every word in a sentence, we 
need compute P(Yi|Yi-1, Xi) (N.C) times for 
MEMMs_2. 
Reducing the average number of candidate label 
C can help to improve the decoding efficiency. 
And in most cases, the Yi-1 in P(Yi|Yi-1, Xi) is not 
necessary (Koeling, 2000; Osbome, 2000). There-
fore, to reduce the average number of candidate 
labels C, it is reasonable to use an ME model to 
filter the candidate label. Given a threshold T (0 
<= T <= 1), the candidate label filtering algorithm 
is as follows: 
1. CP = 0; 
2. While CP <= T 
a) Add the most probable candidate label Y? 
to viterbi algorithm; 
b) Delete Y? from the candidate label set; 
c) CP = P(Y?|Xi) + CP. 
If the probability of the most probable candidate 
label has surpassed T, other labels are discarded. 
Otherwise, more labels need be added to viterbi 
algorithm. 
4 Evaluation and Result 
4.1 Evaluation 
We evaluate the effectiveness and efficiency of the 
new framework by the data set in the task two of 
 CIPS-ParsEval-2009 (Zhou and Li, 2010). The 
effectiveness is supported by high F-1 measure in 
the task two of CIPS-ParsEval-2009 (see Figure 1), 
which shows that optimal feature template subset 
driven by ME or MEMMs is also optimal for 
CRFs. The efficiency is shown by significant de-
cline in training time (see Figure 3), where the 
baseline is CRFs, and comparative methods are 
ME or MEMMs. 
We design six subsets of feature template set 
and six experiments to show the effectiveness and 
efficiency of the new framework. As shown in 
Table 1 and Table 2, the 1~3 experiments shows 
the influence of the feature templates, which are 
unrelated to Yi-1, for both ME and CRFs. And the 
4~6 experiments show the influence of the feature 
templates, which are related to Yi-1, for both 
MEMMs_2 and CRFs. In table 1, six template 
subsets can be divided into two sets by relevance 
of previous label: 1, 2, 3 and 4, 5, 6. Moreover, the 
first set can be divided into 1, 2, and 3 by distances 
between features with headwords;  the second set 
can be divided into 4, 5 and 6 by relevance of ob-
served value. In order to ensure the objectivity of 
comparative experiments, candidate label filtering 
algorithm is not adopted. 
 
Figure 1: the result in the task two of CIPS-
ParsEval-2009 
 
 
 
 
1 Wi, Wi-1, Wi-2, Wi+1, Wi+2, Pi, Pi-1, Pi-2, Pi+1, 
Pi+2, Wi-1_Wi, Wi_Wi+1, Wi-1_Wi+1, Pi-1_Pi, 
Pi-2_Pi-1, Pi_Pi+1, Pi-1_Pi+1, Pi-1_Pi_Pi+1, Pi-
2_Pi-1_Pi,     Pi_Pi+1_Pi+2, Wi_Pi+1, Wi_Pi+2, 
Pi_Wi-1, Wi-2_Pi-1_Pi, Pi_Wi+1_Pi+1, Pi-
1_Wi_Pi, Pi_Wi+1 
2 Wi-3, Wi+3, Pi-3, Pi+3, Wi-3_Wi-2, Wi+2_Wi+3, 
Pi-3_Pi-2, Pi+2_Pi+3 
3 Wi-4, Wi+4, Pi-4, Pi+4, Wi-4_Wi-3, Wi+3_Wi+4, 
Pi-4_Pi-3, Pi+3_Pi+4 
4 Yi-1 
5 Yi-1_Pi_Pi+1, Yi-1_Pi, Yi-1_Pi-1_Pi 
6 Yi-1_Pi-4, Yi-1_Pi+4 
Table 1: six subsets of feature template set 
 
id Model FT subset 
1 ME vs. CRFs 1 
2 ME vs. CRFs 1, 2 
3 ME vs. CRFs 1, 2, 3 
4 MEMMs vs. CRFs 1, 2, 4 
5 MEMMs vs. CRFs 1, 2, 4, 5 
6 MEMMs vs. CRFs 1, 2, 4, 5, 6 
Table 2: six experiments 
4.2 Empirical Results 
The F-measure curve is shown in Figure 2. For the 
same and optimal feature template subset, the F-1 
measure of CRFs is superior to that of ME because 
of global normalization; and it is superior to that of 
MEMMs since it overcomes the label bias. 
 
Figure 2: the F-measure curve 
 
 
Figure 3: the training time curve 
 
The significant decline in training time of the 
new framework is shown in Figure 3, while the 
testing time curve in Figure 4 and the total time 
curve in Figure 5. The testing time of ME is more 
 than that of CRFs because of local normalization; 
and the testing time of MEMMs_2 is much more 
than that of CRFs because of N.C times of P(Yi|Yi-
1, Xi) computation. 
 
 
Figure 4: the testing time curve 
 
Figure 5: the total time curve 
All results of ME and MEMMs in figures are 
represented by the same line because perfor-
mances of these two models are the same when 
features are only related to observed values. 
5 Conclusions 
In this paper, we propose a new optimal feature 
template selection method for CRFs, which is car-
ried out by replacing the CRFs with MEMM_2 
(ME) as the machine learning model to address the 
efficiency problem according to the homology of 
these models. Heuristic method and candidate la-
bel filtering algorithm, which can improve the ef-
ficiency of FTS further, are also introduced. The 
effectiveness and efficiency of the new method is 
confirmed by the experiments on Chinese text 
chunking.  
Two problems deserve further study: one is to 
prove the homology of ME, MEMMs, and CRFs 
theoretically; the other is to expand the method to 
other fields. 
For any statistical machine learning model, fea-
ture selection or feature template selection is a 
computation-intensive step. This work can be ade-
quately reduced by means of analyzing the homol-
ogy between models and using the model with less 
computation amount. Our research proves to be a 
successful attempt. 
References 
Das Sanmay. 2001. Filters, wrappers and a boosting-
based hybrid for feature selection. In Proceedings of 
the Eighteenth International Conference on Machine 
Learning, pages 74?81. 
Dietterich Thomas G., Adam Ashenfelter, Yaroslav 
Bulatov. 2004. Training Conditional Random Fields 
via Gradient Tree Boosting. In Proc. of the 21th In-
ternational Conference on Machine Learning 
(ICML). 
Dy Jennifer G., and Carla E. Brodley. 2000. Feature 
subset selection and order identification for unsuper-
vised learning. In Proceedings of the Seventeenth In-
ternational Conference on Machine Learning, pages 
247?254. 
Hall Mark A.. 2000. Correlation-based feature selection 
for discrete and numeric class machine learning. In 
Proceedings of the Seventeenth International Confe-
rence on Machine Learning, pages 359?366. 
Jaynes, Edwin T.. 1957. Information Theory and Statis-
tical Mechanics. Physical Review 106(1957), May. 
No.4, pp. 620-630. 
Kim YongSeog, W. Nick Street and Filippo Menczer. 
2000. Feature Selection in Unsupervised Learning 
via Evolutionary Search. In Proceedings of the Sixth 
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining, pages 365?369. 
Koeling Rob. 2000. Chunking with Maximum Entropy 
Models. In Proceeding of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 2000, pp. 139-141. 
Kohavi Ron, and George H. John. 1997. Wrappers for 
feature subset selection. Artificial Intelligence, 97(1-
2):273?324. 
Lafferty John, Andrew McCallum, and Fernando Perei-
ra. 2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of the Eighteenth International Confe-
rence on Machine Learning. 
Li Wei, and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition using 
Conditional Random Fields and Feature Induction. 
ACM Transactions on Asian Language Information 
Processing (TALIP).  
Liu Huan, and Lei Yu. 2005. Toward Integrating Fea-
ture Selection Algorithms for Classification and 
Clustering. IEEE Transactions on knowledge and 
Data Engineering, v.17 n.4, p.491-502. 
Liu Huan, and Rudy Setiono. 1996. A probabilistic ap-
proach to feature selection - a filter solution. In Pro-
 ceedings of the Thirteenth International Conference 
on Machine Learning, pages 319?327. 
McCallum Andrew. 2003. Efficiently Inducing Features 
of Conditional Random Fields. In Proceedings of the 
Nineteenth Conference on Uncertainty in Artificial 
Intelligence. 
McCallum Andrew, DAyne Freitag, Fernando Pereira. 
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proceedings 
of ICML'2000, Stanford, CA, USA, 2000, pp. 591-
598. 
McCallum Andrew, and Wei Li. 2003. Early Results for 
Named Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced Lex-
icons. In Proceedings of The Seventh Conference on 
Natural Language Learning (CoNLL-2003), Edmon-
ton, Canada.  
Osbome Miles. 2000. Shallow Parsing as Part-of-
speech Tagging. In Proceeding of CoNLL-2000 and 
LLL-2000, Lisbon, Portugal, 2000,pp. 145-147. 
Roark Brian, Murat Saraclar, Michael Collins, and 
Mark Johnson. 2004. Discriminative language mod-
eling with conditional random fields and the percep-
tron algorithm. Proceedings of the 42nd Annual 
Meeting of the Association for Computational Lin-
guistics. 
Settles Burr. 2004. Biomedical Named Entity Recogni-
tion Using Conditional Random Fields and Rich Fea-
ture Sets. COLING 2004 International Joint Work-
shop on Natural Language Processing in Biomedi-
cine and its Applications (NLPBA). 
Sha Fei, and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. Proceedings of the 
2003 conference of the North American Chapter of 
the Association for Computational Linguistics on 
Human Language Technology, Edmonton, Canada. 
Yu Lei, and Huan Liu. 2004. Feature selection for high-
dimensional data: a fast correlation-based filter solu-
tion. In Proceedings of the twentieth International 
Conference on Machine Learning, pages 856?863. 
Zhou Qiang, and Yumei Li. 2010. Chinese Chunk Pars-
ing Evaluation Tasks. Journal of Chinese Informa-
tion Processing. 
 Complete Syntactic Analysis Based on Multi-level Chunking 
ZhiPeng Jiang and Yu Zhao and Yi Guan and  
Chao Li and Sheng Li 
School of Computer Science and Technology,  
Harbin Institute of Technology,  
150001, Harbin, China 
 xyf-3456@163.com; woshizhaoy@gmail.com 
guanyi@hit.edu.cn; beyondlee2008@yahoo.cn 
lisheng@hit.edu.cn 
 
 
 
Abstract 
This paper describes a complete syntactic 
analysis system based on multi-level 
chunking. On the basis of the correct se-
quences of Chinese words provided by 
CLP2010, the system firstly has a Part-of-
speech (POS) tagging with Conditional 
Random Fields (CRFs), and then does the 
base chunking and complex chunking with 
Maximum Entropy (ME), and finally gene-
rates a complete syntactic analysis tree. 
The system took part in the Complete Sen-
tence Parsing Track of the Task 2 Chinese 
Parsing in CLP2010, achieved the F-1 
measure of 63.25% on the overall analysis, 
ranked the sixth; POS accuracy rate of 
89.62%, ranked the third. 
1 Introduction 
Chunk is a group of adjacent words which belong 
to the same s-projection set in a sentence, whose 
syntactic structure is actually a tree (Abney, 1991), 
but apart from the root node, all other nodes are 
leaf nodes. Complete syntactic analysis requires a 
series of analyzing processes, eventually to get a 
full parsing tree. Parsing by chunks is proved to be 
feasible (Abney, 1994). 
The concept of chunking was first proposed by 
Abney in 1991, who defined chunks in terms of 
major heads, and parsed by chunks in 1994 (Ab-
ney, 1994). An additional chunk tag set {B, I, O} 
was added to chunking (Ramshaw and  Marcus, 
1995), which limited dependencies between ele-
ments in a chunk, changed chunking into a ques-
tion of sequenced tags, to promote the develop-
ment of chunking. Chunking algorithm was ex-
tended to the bottom-up parser, which is trained 
and tested on the Wall Street Journal (WSJ) part of 
the Penn Treebank (Marcus, Santorini and Mar-
cinkiewicz 1993), and achieved a performance of 
80.49% F-measure, the results show that it per-
formed better than a standard probabilistic con-
text-free grammar, and can improve performance 
by adding the information of parent node (Sang, 
2000). 
On Chinese parsing, Maximum Entropy Model 
was first used to have a POS tagging and chunking, 
and then a full parsing tree was generated (Fung, 
2004), training and testing in the Penn Chinese 
Treebank, which achieved 79.56% F-measure. The 
parsing process was divided into POS tagging, 
base chunking and complex chunking, having a 
POS tagging and chunking on a given sentence, 
and then looping the process of complex chunking 
up to identify the root node (Li and Zhou, 2009). 
This parsing method is the basis of this paper. In 
addition, we have the existing Chinese chunking 
system in laboratory, which ranked first in Task 2: 
Chinese Base Chunking of CIPS-ParsEval-2009, 
so we try to apply chunking to complete syntactic 
analysis in CLP2010, to achieve better results. 
We will describe the POS tagging based on 
CRFs in Section 2, including CRFs, feature tem-
plate selection and empirical results. Multi-level 
chunking based on ME will be expounded in Sec-
tion 3, including ME, MEMM, base chunking and 
complex chunking. Finally, we will summarize our 
work in Section 4. 
 2 POS Tagging Based on CRFs 
2.1 Conditional Random Fields 
X is a random variable over data sequences to be 
labeled, and Y is a random variable over corres-
ponding label sequences. All components Yi of Y 
are assumed to range over a finite label alphabet. 
For example, X might range over natural language 
sentences and Y range over part-of-speech tags of 
those sentences, a finite label alphabet is the set of 
possible part-of-speech tags (Lafferty and McCal-
lum and Pereira, 2001). CRFs is represented by the 
local feature vector f and the corresponding weight 
vector, f is divided into the state feature s (y, x, i) 
and transfer feature t (y, y', x, i), where y and y' are 
possible POS tags, x is the current input sentence, i 
is the position of current term (Jiang and Guan and 
Wang, 2006). Formalized as follows: 
s (y, x, i) = s (yi, x, i)                      (1) 
? ?
? ?1, , , 1
, ,
0 1
i it y y x i i
t y x i
i
?? ??? ?
? ??
      (2) 
By the local feature of the formula (1) and (2), 
the global features of x and y: 
? ? ? ?, , ,iF y x f y x i??              (3) 
At this point of (X, Y), the conditional probabil-
ity distribution of CRFs: 
? ? ? ?? ?? ?
exp ,| F Y Xp Y X Z X? ?
?
?
?       (4) 
where ? ? ? ?? ?exp ,yZ x F y x? ?? ??  is a fac-
tor for normalizing. For the input sentence x, the 
best sequence of POS tagging: 
? ?arg max |
y
y p y x?? ?
 
2.2 Feature Template Selection 
We use the template as a baseline which is taken 
by Yang (2009) in CIPS-ParsEval-2009, directly 
testing the performance, whose accuracy was 
93.52%. On this basis, we adjust the feature tem-
plate through the experiment, and improve the 
tagging accuracy of unknown words by introduc-
ing rules, in the same corpus for training and test-
ing, accuracy is to 93.89%. Adjusted feature tem-
plate is shown in Table 1, in which the term pre is 
the first character in current word, suf is the last 
character of current word, num is the number of 
characters of current word, pos-1 is the tagging re-
sults of the previous word. 
 
Table 1: feature template 
feature template 
w2,w1,w0,w-1,w-2,w+1w0,w0w-1,pre0, pre0w0,suf0, 
w0suf0,num,pos-1 
2.3 Empirical Results and Analysis 
We divide the training data provided by CLP2010 
into five folds, the first four of which are train cor-
pus, the last one is test corpus, on which we use 
the CRF++ toolkit for training and testing. Tag-
ging results with different features are shown in 
table 2. 
Table 2: tagging results with different features 
Model Explain Accuracy 
CRF baseline 93.52% 
CRF1 add w-1, pos-1 93.58% 
CRF2 add num 93.66% 
CRF3 add num, w-1, pos-1 93.68% 
CRF4 add num, rules 93.80% 
CRF5 add num, w-1, pos-1, rules 93.89% 
Tagging results show that the number of charac-
ter and POS information can be added to improve 
the accuracy of tagging, but in CLP2010, the tag-
ging accuracy is only 89.62%, on the one hand it 
may be caused by differences of corpus, on the 
other hand it may be due to that we don?t use all 
the features of CRFs but remove the features 
which appear one time in order to reduce the train-
ing time. 
3 Multi-level Chunking Based on ME 
3.1 Maximum Entropy Models and Maxi-
mum Entropy Markov Models 
Maximum entropy model is mainly used to esti-
mate the unknown probability distribution whose 
entropy is the maximum under some existing con-
ditions. Suppose h is the observations of context, t 
is tag, the conditional probability p (t | h) can be 
expressed as: 
exp( ( , ))( | ) ( )
ii i t hP t h Z h
f?? ?
 
where fi is the feature of model,  
( ) exp( ( , ))i it iZ h t hf??? ? 
 is a factor for nor-
malizing. 
i? is weigh of feature fi, training is the 
process of seeking the value of 
i? .  
Maximum entropy Markov model is the seria-
lized form of Maximum entropy model (McCal-
lum and Freitag and Pereira, 2000), for example, 
transition probabilities and emission probabilities 
are merged into a single conditional probability 
 function 
1( | , )i iP t t h?  in binary Maximum entropy 
Markov model, 
1( | , )i iP t t h?  is turned to ( | )p t h  to 
be solved by adding features which can express 
previously tagging information (Li and Sun and 
Guan, 2009). 
3.2 Base Chunking 
Following the method of multi-level chunking, we 
first do the base chunking on the sentences which 
are through the POS tagging, then loop the process 
of complex chunking until they can?t be merged. 
We use the existing Chinese base chunking system 
to do base chunking in laboratory, which marks 
boundaries and composition information of chunk 
with MEMM, and achieved 93.196% F-measure in 
Task 2: Chinese Base Chunking of CIPS-ParsEval 
-2009. The input and output of base chunking are 
as follows: 
Input???/nS ??/a  ??/n ?/v ??/nR  ?
?/n ?/p ??/n ?/uJDE ??/n ?/wD ??/n 
??/vN ?/f ?/wP ??/d  ??/v ?/wP ??/d  
??/v ?/c ??/d  ??/v ?/uJDE ??/v ??
/a  ??/n ??/n ?/uJDE ??/n  ??/n ?/wE 
Output???/nS [np ??/a  ??/n ] ?/v [np ?
?/nR  ??/n ] ?/p ??/n ?/uJDE [np ??
/n  ?/wD ??/n ] ??/vN ?/f ?/wP [vp ??
/d  ??/v ] ?/wP [vp ??/d  ??/v ] ?/c [vp 
??/d  ??/v ] ?/uJDE ??/v [np ??/a  ??
/n ] ??/n ?/uJDE [np ??/n  ??/n ] ?/wE 
3.3 Complex Chunking 
We take the sentences which are through POS tag-
ging and base chunking as input, using Li?s tag-
ging method and feature template. Categories of 
complex chunk include xx_Start, xx_Middle, 
xx_End and Other, where xx is a category of arbi-
trary chunk. The process of complex chunking is 
shown as follows: 
Step 1: input the sentences which are through POS 
tagging and base chunking, for example: 
??/nS  [np ??/a  ??/n  ] ?/uJDE  [np ??
/vN  ??/vN  ] ?/c  [np ??/n  ??/n  ]  
Step 2: if there are some category tags in the sen-
tence, then turn a series of tags to brackets, for 
instance, if continuous cells are marked as 
xx_Start, xx_Middle, ..., xx_Middle, xx_End, then 
the combination of continuous cells is a complex 
chunk xx; 
Step 3: determine the head words with the set of 
rules, and compress the sentence: 
??/nS  [np ??/n  ] ?/uJDE  [np ??/vN  ] ?
/c  [np ??/n  ] 
Step 4: if the sentence can be merged, mark the 
sentence with ME, then return step 2, else the 
analysis process ends: 
?? /nS@np_Start  [np ?? /n  ]@np_End ?
/uJDE@Other  [np ?? /vN  ]@np_Start ?
/c@np_Middle  [np ??/n  ]@np_End 
At last, the output is: 
[np [np ??/nS  [np ??/a  ??/n  ] ] ?/uJDE  
[np [np ??/vN  ??/vN  ] ?/c  [np ??/n  ?
?/n  ] ] ] 
Following the above method, we first use the 
Viterbi decoding, but in the decoding process we 
encountered two problems:  
1. Similar to the label xx_Start, whose back is only 
xx_Middle or xx_End, so the probability of 
xx_Start label turning to Other is 0, But, if only 
using ME to predict, the probability may not be 0.  
2. Viterbi decoding can?t solve that all the labels of 
predicted results are Other, if all labels are Other, 
they can?t be merged, this result doesn?t make 
sense. 
Solution:  
For the first question, we add the initial transfer 
matrix and the end transfer matrix in decoding 
process, that is, the corresponding xx_Middle or 
xx_End of xx_Start is seted to 1 in the transfer 
matrix, the others are marked as 0, matrix multip-
lication is taken during the state transition. It can 
effectively avoid errors caused by probability to 
improve accuracy.  
To rule out the second question, we use heuris-
tic search approach to decode, and exclude all 
Other labels with the above matrix. In addition, we 
defined another ME classifier to do some pruning 
in the decoding process, the features of ME clas-
sifier are POS, the head word, the POS of head 
word. The pseudo-code of Heuristic search is: 
While searching priority queue is not empty 
Take the node with the greatest priority in the 
queue; 
If the node?s depth = length of the chunking 
results 
Searching is over, reverse the search-
ing path to get searching results; 
Else 
Compute the probability of all candi-
date children nodes according to 
the current probability; 
Record searching path; 
Press it into the priority queue; 
In addition, we found that some punctuation at 
the end of a sentence can?t be merged, probably 
due to sparseness of data, according to that the 
tone punctuation (period, exclamation mark, ques-
 tion mark) at the end of the sentence can be added 
to implement a complete sentence (zj) (Zhou, 
2004), we carried out a separate deal with this sit-
uation, directly add punctuation at the end of the 
sentence, to form a sentence. 
In training data provided by CLP2010 in sub-
task: Complete Sentence Parsing, the head words 
aren?t marked. We can?t use the statistical method 
to determine the head words, but only by rules. We 
take Li?s rule set as baseline, but the rule set was 
used to supplement the statistical methods, so 
some head words don?t appear in the rule set, re-
sulting in many head words are marked as NULL, 
for this situation, we add some rules through expe-
riment, Table 3 lists some additional rules. 
Table 3: increasing part of rules 
parent  head words 
vp vp, vB, vSB, vM, vJY, vC, v 
ap a, b, d 
mp qN, qV, qC, q 
dj vp, dj, ap, v, fj 
dlc vp 
mbar m, mp 
3.4 Empirical Results and Analysis 
We take the corpus which are through correct POS 
tagging and base chunking for training and testing, 
it is divided into five folds, the first four as train-
ing corpus, the last one as testing corpus, using the 
existing ME toolkit to train and test model in la-
boratory. Table 4 shows the results on Viterbi de-
coding and Heuristic Search method, where head 
words are determined by rules. 
Table 4: results with different decoding 
Decoding Accuracy Recall Fmeasure 
Viterbi  84.87% 84.47% 84.67% 
Heuristic 
Search 
85.62% 85.19% 85.40% 
The system participated in the Complete Sen-
tence Parsing of CLP2010, results are shown in 
Table 5 below. Because we can?t determine the 
head words by statistical method on the corpus 
provided by CLP2010, resulting in the accuracy 
decreasing, creating a great impact on results. 
Table 5: the track results 
Training 
mode 
Model use F-measure POS 
Accuracy 
Closed Single 63.25% 89.62% 
4 Conclusions 
In this paper, we use CRFs to have a POS tagging, 
and increase the tagging accuracy by adjusting the 
feature template; multi-level chunking is applied 
to complete syntactic analysis, we do the base 
chunking with MEMM to recognize boundaries 
and components, and make the complex chunking 
with ME to generate a full parsing tree; on decod-
ing, we add transfer matrix to improve perfor-
mance, and remove some features with a ME clas-
sifier to reduce training time.  
As the training data are temporarily changed, 
our system?s training on the Event Description 
Sub-sentence Analysis of CLP2010 isn?t com-
pleted, and head words are marked in the training 
corpus of this task, so our next step will be to 
complete training and testing of this task, compare 
the existing evaluation results, and use ME clas-
sifier to determine head words, analyze impact of 
head words on system. On the POS tagging, we 
will retain all features to train and compare tag-
ging results. 
Acknowledgement 
We would like to thank XingJun Xu and BenYang 
Li for their valuable advice to our work in Com-
plete Sentence Parsing of CLP2010. We also thank 
JunHui Li, XiaoRui Yang and HaiLong Cao for 
paving the way for our work. 
References 
S. Abney (1991) Parsing by Chunks. Kluwer Academic 
Publishers, Dordrecht, 257-278 
Lance A. Ramshaw, Mitchell P. Marcus (1995) Text 
Chunking Using Transformation-Based Learning. In 
Proceeding of the Third ACL Workshop on Very 
Large Corpora, USA, 87-88 
Erik F. Tjong Kim Sang (2001) Transforming a Chunk-
er to a Parser. Computational Linguistics in the 
Netherlands 2000, 6-8 
YongSheng Yang, BenFeng Chen (2004) A Maximum-
Entropy Chinese Parser Augmented by Transforma-
tion-Based Learning. ACM Transactions on Asian 
Language Information Processing, 4-8 
John Lafferty, Andrew McCallum, and Fernando Perei-
ra (2001) Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of the Eighteenth International Confe-
rence on Machine Learning, 282-289 
Junhui Li, Guodong Zhou (2009) Soochow University 
Report for the 1st China Workshop on Syntactic 
Parsing. CIPS-ParsEval-2009, 5-8 
Wei Jiang, Yi Guan, and Xiaolong Wang (2006) Condi-
tional Random Fields Based POS Tagging.Computer 
Engineering and Applications, 14-15 
 Xiaorui Yang, Bingquan Liu, Chengjie Sun, and Lei 
Lin (2009) InsunPOS: a CRF-based POS Tagging 
System. CIPS-ParsEval-2009, 4-6 
A. McCallum, D. Freitag, and F. Pereira (2000) Maxi-
mum Entropy Markov Models for Information Ex-
traction and Segmentation. Proceedings of ICML-
2000, Stanford University, USA, 591-598 
Chao Li, Jian Sun, Yi Guan, Xingjun Xu, Lei Hou, and 
Sheng Li (2009) Chinese Chunking With Maximum 
Entropy Models. CIPS-ParsEval-2009, 2-4 
Qiang Zhou (2004) Annotation Scheme for Chinese 
Treebank. Journal of Chinese Information Processing,  
4-5 
