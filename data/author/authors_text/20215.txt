Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 259?264,
Dublin, Ireland, August 23-24, 2014.
ECNU: Expression- and Message-level Sentiment Orientation
Classification in Twitter Using Multiple Effective Features
Jiang Zhao
?
, Man Lan
?
, Tian Tian Zhu
?
Department of Computer Science and Technology
East China Normal University
?
51121201042,51111201046@ecnu.cn;
?
mlan@cs.ecnu.edu.cn
Abstract
Microblogging websites (such as Twitter,
Facebook) are rich sources of data for
opinion mining and sentiment analysis. In
this paper, we describe our approaches
used for sentiment analysis in twitter (task
9) organized in SemEval 2014. This task
tries to determine whether the sentiment
orientations conveyed by the whole tweets
or pieces of tweets are positive, negative
or neutral. To solve this problem, we ex-
tracted several simple and basic features
considering the following aspects: surface
text, syntax, sentiment score and twitter
characteristic. Then we exploited these
features to build a classifier using SVM
algorithm. Despite the simplicity of fea-
tures, our systems rank above the average.
1 Introduction
Microblogging services such as Twitter
1
, Face-
book
2
today play an important role in expressing
opinions on a variety of topics, discussing current
issues or sharing one?s feelings about different ob-
jects in our daily life (Agarwal and Sabharwal,
2012). Therefore, Twitter (and other platforms)
has become a valuable source of users? sentiments
and opinions and with the continuous and rapid
growth of the number of tweets, analyzing the sen-
timents expressed in twitter has attracted more and
more researchers and communities, for example,
the sentiment analysis task in twitter was held in
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
2
http://facebook.com/
SemEval 2013 (Nakov et al., 2013). It will bene-
fit lots of real applications such as simultaneously
businesses, media outlets, and help investors to
discover product trends, identify customer pref-
erences and categorize users by analyzing these
tweets (Becker et al., 2013).
The task of sentiment analysis in twitter in Se-
mEval 2014 (Sara et al., 2014) aims to classify
whether a tweet?s sentiment is positive, negative or
neutral at expression level or message level. The
expression-level subtask (i.e., subtask A) is to de-
termine the sentiment of a marked instance of a
word or phrase in the context of a given message,
while the message-level subtask (i.e., subtask B)
aims to determine the sentiment of a whole mes-
sage. Previous work (Nakov et al., 2013) showed
that message-level sentiment classification is more
difficult than that of expression-level (i.e., 0.690 vs
0.889 in terms of F-measure) since a message may
be composed of inconsistent sentiments.
To date, lots of approaches have been proposed
for conventional blogging sentiment analysis and
a very broad overview is presented in (Pang and
Lee, 2008). Inspired by that, many features used
in microblogging mining are adopted from tradi-
tional blogging sentiment analysis task. For ex-
ample, n-grams at the character or word level,
part-of-speech tags, negations, sentiment lexicons
were used in most of current work (Agarwal et
al., 2011; Barbosa and Feng, 2010; Zhu et al.,
2013; Mohammad et al., 2013; K?okciyan et al.,
2013). They found that n-grams are still effective
in spite of the short length nature of microblog-
ging and the distributions of different POS tags
in tweets with different polarities are highly dif-
ferent (Pak and Paroubek, 2010). Compared with
formal blog texts, tweets often contain many in-
formal writings including slangs, emoticons, cre-
259
ative spellings, abbreviations and special marks
(i.e., mentions @ and hashtags #), and thus many
twitter-specific features are proposed to character-
ize this phenomena. For example, features record
the number of emoticons, elongated words and
hashtags were used in (Mohammad et al., 2013;
Zhu et al., 2013; K?okciyan et al., 2013). In this
work, we adopted many features from previous
work and then these features were fed to SVM to
perform classification.
The remainder of this paper is organized as fol-
lows. Section 2 describes our systems including
preprocessing, feature representations, data sets,
etc. Results of two subtasks and discussions are
reported in Section 3. Finally, we conclude this
paper in Section 4.
2 Our Systems
We extracted eight types of features and the first
six types were used in subtask A and all features
were used in subtask B. Then, several classifica-
tion algorithms were examined on the develop-
ment data set and the algorithm with the best per-
formance was chosen in our final submitted sys-
tems.
2.1 Preprocessing
In order to remedy as many informal texts as
possible, we recovered the elongated words to
their normal forms, e.g., ?goooooood? to ?good?
and collected about five thousand slangs or ab-
breviations from Internet to convert each slang
to its complete form, e.g., ?1dering? to ?won-
dering?, ?2g2b4g? to ?to good to be forgotten?.
Then these preprocessed texts were used to extract
non twitter-specific features (i.e., POS, lexicon, n-
grams, word cluster and indicator feature).
2.2 Feature Representations
2.2.1 POS Features
(Pak and Paroubek, 2010) found that POS tags
help to identify the sentiments of tweets and they
pointed out that objective tweets often contain
more nouns than subjective tweets and subjec-
tive tweets may carry more adjectives and adverbs
than objective tweets. Therefore, we used Stan-
ford POS Tagger
3
and recorded the number of
four different tags for each tweet: noun (the cor-
responding POS tags are ?NN?, ?NNP?, ?NNS?
and ?NNPS?), verb (the corresponding POS tags
3
http://nlp.stanford.edu/software/tagger.shtml
are ?VB?, ?VBD?, ?VBG?, ?VBN?, ?VBP? and
?VBZ?), adjective (the corresponding POS tags
are ?JJ?, ?JJR? and ?JJS?) and adverb (the corre-
sponding POS tags are ?RB?, ?RBR? and ?RBS?).
Then we normalized them by the length of given
instance or message.
2.2.2 Sentiment Lexicon-based Features
Sentiment lexicons are widely used to calculate
the sentiment scores of phrases or messages in pre-
vious work (Nakov et al., 2013; Mohammad et al.,
2013) and they are proved to be very helpful to
detect the sentiment orientation. Given a phrase
or message, we calculated the following six fea-
ture values: (1) the ratio of positive words to all
words, i.e., the number of positive words divided
by the number of total words; (2) the ratio of neg-
ative words to all words; (3) the ratio of objective
words to all words; (4) the ratio of positive senti-
ment score to the total score (i.e., the sum of the
positive and negative score); (5) the ratio of nega-
tive sentiment score to the total score; (6) the ratio
of positive score to negative score, if the negative
score is zero, which means this phrase or message
has a very strong positive sentiment orientation,
we set ten times of positive score as its value.
During the calculation, we also considered the
effects of negation words since they may reverse
the sentiment orientation in most cases. To do so,
we defined the negation context as a snippet of a
tweet that starts with a negation word and ends
with punctuation marks. If a non-negation word
is in a negation context and also in the sentiment
lexicon, we reverse its polarity. For example, the
word ?bad? in phrase ?not bad? originally has a
negative score of 0.625, after reversal, this phrase
has a positive score of 0.625. A manually made
list containing 29 negation words (e.g., no, hardly,
never, etc) was used in our experiment.
Four sentiment lexicons were used to decide
whether a word is subjective or objective and ob-
tain its sentiment score.
MPQA (Wilson et al., 2009). This subjectiv-
ity lexicon contains about 8000 subjective words
and each word has two types of sentiment strength:
strong subjective and weak subjective, and four
kinds of sentiment polarities: positive, negative,
both (positive and negative) and neutral. We used
this lexicon to determine whether a word is posi-
tive, negative or objective and assign a value of 0.5
or 1 if it is weak or strong subjective (i.e., positive
or negative) respectively.
260
SentiWordNet(SWN) (Baccianella et al.,
2010). This sentiment lexicon contains about
117 thousand items and each item corresponds
to a synset of WordNet. Three sentiment scores:
positivity, negativity, objectivity are provided and
the sum of these three scores is always 1, for
example, living#a#3, positivity: 0.5, negativity:
0.125, objectivity: 0.375. In experiment we used
the most common sense of a word.
NRC (Mohammad et al., 2013). Mohammad et
al. collected two sets of tweets and each tweet con-
tains the seed hashtags or emoticons and then they
labeled the sentiment orientation for each tweet
according to its hashtags or emoticons. They used
pointwise mutual information (PMI) to calculate
the sentiment score for each word and obtained
two sentiment lexicons (i.e., hashtag lexicon and
emoticon lexicon).
IMDB. We generated an unigram lexicon by
ourselves from a large movie review data set from
IMDB website (Maas et al., 2011) which con-
tains 25,000 positive and 25,000 negative movie
reviews by calculating their PMI scores.
2.2.3 Word n-Gram
Words in themselves in tweets usually carry out
the original sentiment orientation, so we con-
sider word n-grams as one feature. We removed
URLs, mentions, hashtags, stopwords from tweet
and then all words were stemmed using the nltk
4
toolkit. For subtask A, only unigram was used and
we used word frequency as feature values. For
subtask B, both unigram and bigram were used.
Besides, weighted unigram was also used where
we replaced word frequency with their sentiment
scores using the hashtag lexicon and emoticon lex-
icon in NRC.
2.2.4 Twitter-specific Features
Punctuation Generally, punctuation may express
users? sentiment in a certain extent. Therefore we
recorded the frequency of the following four types
of punctuation: exclamation (!), question (?), dou-
ble (?) and single marks (?). In addition, we also
recorded the number of contiguous sequences of
exclamation marks, question marks, and both of
themwhich appeared at the end of a phrase or mes-
sage.
Emoticon Emoticons are widely used to directly
express the sentiment of users and thus we counted
4
http://nltk.org/
the number of positive emoticons, negative emoti-
cons and the sum of positive and negative emoti-
cons. To identify the polarities of emoticons, we
collected 36 positive emoticons and 33 negative
emoticons from the Internet.
Hashtag A hashtag is a short phrase that con-
catenates more than one words together without
white spaces and users usually use hashtags to
label the subject topic of a tweet, e.g., #toobad,
#ihateschool, #NewGlee. Since a hashtag may
contain a strong sentiment orientation, we first
used the Viterbi algorithm (Berardi et al., 2011)
to split hashtags and then calculated the sentiment
scores of hashtags using the hashtag and emoticon
lexicon in NRC.
2.2.5 Word Cluster
Apart from n-gram, we presented another word
representations based on word clusters to explore
shallow semantic meanings and reduced the spar-
sity of the word space. 1000 word clusters pro-
vided by CMU pos-tagging tool
5
were used to rep-
resent tweet contents. For each tweet we recorded
the number of words from each cluster, resulting
in 1000 features.
2.2.6 Indicator Features
We observed that the polarity of a message some-
times is revealed by some special individual posi-
tive or negative words in a certain degree. How-
ever the sentiment lexicon based features where
a synthetical sentiment score of a message is cal-
culated may hide this information. Therefore, we
directly used several individual sentiment scores
as features. Specifically, we created the following
sixteen features for each message where the hash-
tag and emoticon lexicons were used to obtain sen-
timent scores: the sentiment scores of the first and
last sentiment-bearing words, the three highest and
lowest sentiment scores.
2.3 Data sets and Evaluation Metric
The organizers provide tweet ids and a script for
all participants to collect data. Table 1 shows the
statistics of the data set used in our experiments.
To examine the generalization of models trained
on tweets, the test data provided by the organiz-
ers consists of instances from different domains
for both subtasks. Specifically, five corpora are in-
cluded: LiveJournal(2014) is a collection of com-
ments from LiveJournal blogs, SMS2013 is a SMS
5
http://www.ark.cs.cmu.edu/TweetNLP/
261
data set directly from last year, Twitter2013 is a
twitter data set directly from last year, Twitter2014
is a new twitter data set and Twitter2014Sarcasm
is a collection of tweets that contain sarcasm. No-
tice that the data set SMS2013 and Twitter2013
were also used as our development set. Form Ta-
ble 1, we find that (1) the class distributions of test
data sets almost agree with training data sets for
both subtasks, (2) the percentages of class neutral
in two subtasks are significantly different (4.7%
vs 45.5%), which reflects that a sentence which is
composed of different sentiment expressions may
act neutrality, (3) Twitter2014Sarcasm data set is
very small. According to the guideline, we did not
use any development data for training in the eval-
uation period.
data set Positive Negative Neutral Total
subtask A:
train 3,609(61%) 2,023(34%) 265(5%) 5,897
dev 2,734(62%) 1,541(35%) 160(3%) 4,435
test
LiveJournal 660(50%) 511(39%) 144(11%) 1,315
SMS2013 1,071(46%) 1,104(47%) 159( 7%) 2,334
Twitter2013 2,734(62%) 1,541(35%) 160(3%) 4,435
Twitter2014 1,807(73%) 578(23%) 88( 4%) 2,473
Twitter2014S 82(66%) 37(30%) 5(4%) 124
all 6,354(59%) 3,771(35%) 556(6%) 10,681
subtask B:
train 3,069(36%) 1,313(15%) 4,089(49%) 8,471
dev 1,572(41%) 601(16%) 1,640(43%) 3,813
test
LiveJournal 427(37%) 304(27%) 411(36%) 1,142
SMS2013 492(24%) 394(19%) 1,207(57%) 2,093
Twitter2013 1,572(41%) 601(16%) 1,640(43%) 3,813
Twitter2014 982(53%) 202(11%) 669(36%) 1,853
Twitter2014S 33(38%) 40(47%) 13(15%) 86
all 3,506(39%) 1,541(17%) 3,940(44%) 8,987
Table 1: Statistics of data sets in training (train),
development (dev), test (test) set. Twitter2014S
stands for Twitter2014Sarcasm.
We used macro-averaged F-measure of positive
and negative classes (without neutral since it is
margin in training data) to evaluate the perfor-
mance of our systems and the averaged F-measure
of five corpora was used to rank the final results.
2.4 Submitted System Configurations
For each subtask, each team can submit two runs:
(1) constrained: only the provided data set can be
used for training and no additional annotated data
is allowed for training, however other resources
such as lexicons are allowed; (2) unconstrained:
any additional data can be used for training. We
explored several classification algorithms on the
development set and configured our final systems
as follows. For constrained system, we used SVM
and logistic regression algorithm implemented in
scikit-learn toolkit (Pedregosa et al., 2011) to ad-
dress two subtasks respectively and used self-
training strategy to conduct unconstrained system.
Self-training is a semi-supervised learning method
where a classifier is first trained with a small
amount of labeled data and then we repeat the fol-
lowing procedure: the most confident predictions
by the current classifier are added to training pool
and then the classifier is retrained(Zhu, 2005). The
parameters in constrained models and the growth
size k and iteration number T in self-training are
listed in Table 2 according to the results of prelim-
inary experiments.
task constrained unconstrained
subtask A SVM, kernel=rbf, c=500 k=100, T=40
subtask B LogisticRegression, c=1 k=90, T=40
Table 2: System configurations for the constrained
and unconstrained runs in two subtasks.
3 Results and Discussion
3.1 Results
We submitted four systems as described above and
their final results are shown in Table 3, as well as
the top-ranked systems released by the organizers.
From the table, we observe the following findings.
Firstly, we find that the results of message-level
polarity classification are much worse than the re-
sults of expression-level polarity disambiguation
(82.93 vs 61.22) on both constrained and uncon-
strained systems, which is consistent with the pre-
vious work (Nakov et al., 2013). The low per-
formance of message-level task may result from
two possible reasons: (1) a message may con-
tain mixed sentiments and (2) the strength of
sentiments is different. In contrast, the texts in
expression-level task are usually short and contain
a single sentiment orientation, which leads to bet-
ter performance.
Secondly, whether on constrained or uncon-
strained systems, the performance on Twit-
ter2014Sarcasm data set is much worse than the
performance on the other four data sets. This is
because that sarcasm often expresses the opposite
meaning of what it seems to say, that means the
actual sentiment orientation of a word is opposite
to its original orientation. Moreover, even for our
human it is a challenge to identify whether it is a
sarcasm or not.
Thirdly, the results on LiveJournal and SMS
are comparable to the results on Twitter2013 and
Twitter2014 in both subtasks, which indicates that
262
online comments and SMS share some common
characteristics with tweets (e.g., emoticons and
punctuation). Therefore, in case of lack of labeled
online comments or SMS data, we can use the ex-
isting tweets as training data instead.
Fourthly, our unconstrained systems exploit the
test data of year 2014 in training stage and perform
a worse result in subtask B. We speculate that the
failure of using self-training on message-level data
set is because that the performance of initial clas-
sifier was low and thus in the following iterations
more and more noisy instances were selected to
add the training pool, which eventually resulted in
a final weak classifier.
In summary, we adopted some simple and ba-
sic features to classify the polarities of expressions
and messages and they were promising. For sub-
task A, our systems rank 5th out of 19 submissions
under the constrained setting and rank 2nd out of 6
submissions under the unconstrained setting. For
subtask B, our systems rank 16th out of 42 submis-
sions under the constrained setting and rank 5th
out of 8 submissions under the unconstrained set-
ting.
3.2 Feature Combination Experiments
To explore the effectiveness of different feature
types, we conducted a series of feature combina-
tion experiments using the constrained setting as
shown in Table 2 for both subtasks. For each time
we repeated to add one feature type to current fea-
ture set and then selected the best one until all the
feature types were processed. Table 4 shows the
results of different feature combinations and the
best results are shown in bold font.
From Table 4, we find that (1) MPQA, n-gram
and Word cluster are the most effective feature
types to identify the polarities; (2) The POS tags
make margin contribution to improve the perfor-
mance since Stanford parser is designed for for-
mal texts and in the future we may use specific
parser instead; (3) The lexicon IMDB extracted
from movie reviews has negative effects to clas-
sify twitter data, which indicates that there exist
differences in the way of expressing sentiments
between these two domains; (4) Twitter-specific
features, i.e., hashtag and emoticon, are not as ef-
fective as expected. This is because they are sparse
in the data sets. In subtask Awith 16578 instances,
only 292 instances (1.76%) have hashtags and 419
instances (2.52%) have emoticons. In subtask B
with 17458 messages, more instances have hash-
tags (16.72%) and emoticons (26.70%). (5) For
subtask A MPQA, n-gram, NRC and punctuation
features achieve the best performance and for sub-
task B the best performance is achieved by using
almost all features.
In summary, we find that n-gram and some lex-
icons such as MPQA are the most effective while
twitter-specific features (i.e., hashtag and emoti-
con) are not as discriminating as expected and the
main reason for this is that they are sparse in the
data sets.
Feature Subtask A Feature Subtask B
MPQA 77.49 Word cluster 53.50
.+n-gram 80.08(2.59) .+MPQA 58.35(4.85)
.+NRC 82.42(2.34) .+W1Gram 60.22(1.87)
.+Pun. 83.83(1.41) .+Pun. 60.99(0.77)
.+POS 83.83(0) .+Indicator 61.38(0.39)
.+Emoticon 83.49(-0.34) .+SWN 61.51(0.13)
.+Hashtag 83.54(0.05) .+Hashtag 61.54(0.03)
.+IMDB 83.51(-0.03) .+n-gram 61.56(0.02)
.+SWN 82.92(-0.59) .+Emoticon 61.69(0.13)
- - .+POS 61.71(0.02)
- - .+IMDB 61.11(-0.6)
- - .+NRC 61.23(0.12)
Table 4: The results of feature combination exper-
iments. The numbers in the brackets are the per-
formance increments compared with the previous
results. ?.+? means to add current feature to the
previous feature set.
4 Conclusion
In this paper we used several basic feature types to
identify the sentiment polarity at expression level
or message level and these feature types include
n-gram, sentiment lexicon and twitter-specific fea-
tures, etc. Although they are simple, our systems
are still promising and rank above average (e.g.,
rank 5th out of 19 and 16th out of 42 in subtask A
and B respectively under the constrained setting).
For the future work, we would like to analyze the
distributions of different sentiments in sentences.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Apoorv Agarwal and Jasneet Sabharwal. 2012. End-
to-end sentiment analysis of twitter data. In Pro-
263
Systems LiveJournal SMS2013 Twitter2013 Twitter2014 Twitter2014S Average
A-constrained (expression-level) 81.67 89.31 87.28 82.67 73.71 82.93
A-unconstrained 81.69 89.26 87.29 82.93 73.71 82.98
NRC-Canada-A-constrained
?
85.49 88.03 90.14 86.63 77.13 85.48
Think Positive-A-unconstrained
?
80.90 87.65 88.06 82.05 76.74 83.08
B-constrained(message-level) 69.44 59.75 62.31 63.17 51.43 61.22
B-unconstrained 64.08 56.73 63.72 63.04 49.33 59.38
NRC-Canada-B-constrained
?
74.84 70.28 70.75 69.85 58.16 68.78
Think Positive-B-unconstrained
?
66.96 63.20 68.15 67.04 47.85 62.64
Table 3: Performance of our systems and the top-ranked systems (marked with asterisk).
ceedings of the Workshop on Information Extraction
and Entity Analytics on Social Media Data, pages
39?44, Mumbai, India, December. The COLING
2012 Organizing Committee.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ?11, pages
30?38. Association for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 36?44. Association for Computational Lin-
guistics.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. Avaya: Sentiment analysis on
twitter with self-training and polarity lexicon expan-
sion. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 333?340. Association for Computational Lin-
guistics, June.
Giacomo Berardi, Andrea Esuli, Diego Marcheggiani,
and Fabrizio Sebastiani. 2011. Isti@ trec microblog
track 2011: Exploring the use of hashtag segmenta-
tion and text quality ranking. In TREC.
Nadin K?okciyan, Arda C?elebi, Arzucan
?
Ozg?ur, and
Suzan
?
Usk?udarli. 2013. Bounce: Sentiment classifi-
cation in twitter using rich feature sets. In Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 554?561.
Association for Computational Linguistics, June.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142?150. As-
sociation for Computational Linguistics.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321?327. Asso-
ciation for Computational Linguistics, June.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 312?320. Association for Computational Lin-
guistics, June.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Rosenthal Sara, Ritter Alan, Veselin Stoyanov, and
Nakov Preslav. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14). Association for Computational
Linguistics, August.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, pages 399?433.
Tian Tian Zhu, Fang Xi Zhang, and Man Lan. 2013.
Ecnucs: A surface information based system de-
scription of sentiment analysis in twitter in the
semeval-2013 (task 2). In Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), page 408.
Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
264
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 265?270,
Dublin, Ireland, August 23-24, 2014.
ECNU: Leveraging on Ensemble of Heterogeneous Features and
Information Enrichment for Cross Level Semantic Similarity Estimation
Tian Tian Zhu
Department of Computer Science and
Technology
East China Normal University
51111201046@ecnu.cn
Man Lan
?
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
?
Abstract
This paper reports our submissions to the
Cross Level Semantic Similarity (CLSS)
task in SemEval 2014. We submitted
one Random Forest regression system on
each cross level text pair, i.e., Paragraph
to Sentence (P-S), Sentence to Phrase (S-
Ph), Phrase to Word (Ph-W) and Word
to Sense (W-Se). For text pairs on P-S
level and S-Ph level, we consider them as
sentences and extract heterogeneous types
of similarity features, i.e., string features,
knowledge based features, corpus based
features, syntactic features, machine trans-
lation based features, multi-level text fea-
tures, etc. For text pairs on Ph-W level
and W-Se level, due to lack of informa-
tion, most of these features are not ap-
plicable or available. To overcome this
problem, we propose several information
enrichment methods using WordNet syn-
onym and definition. Our systems rank the
2nd out of 18 teams both on Pearson cor-
relation (official rank) and Spearman rank
correlation. Specifically, our systems take
the second place on P-S level, S-Ph level
and Ph-W level and the 4th place on W-Se
level in terms of Pearson correlation.
1 Introduction
Semantic similarity is an essential component of
many applications in Natural Language Process-
ing (NLP). Previous works often focus on text se-
mantic similarity on the same level, i.e., paragraph
to paragraph or sentence to sentence, and many ef-
fective text semantic measurements have been pro-
posed (Islam and Inkpen, 2008), (B?ar et al., 2012),
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
(Heilman and Madnani, 2012). However, in many
real world cases, the two texts may not always
be on the same level. The Cross Level Semantic
Similarity (CLSS) task in SemEval 2014 provides
a universal platform to measure the degree of se-
mantic equivalence between two texts across dif-
ferent levels. For each text pair on four cross lev-
els, i.e., Paragraph to Sentence (P-S), Sentence to
Phrase (S-Ph), Phrase to Word (Ph-W) and Word
to Sense (W-Se), participants are required to re-
turn a similarity score which ranges from 0 (no
relation) to 4 (semantic equivalence). We partici-
pate in all the four cross levels and take the second
place out of all 18 teams both on Pearson correla-
tion (official) and Spearman correlation ranks.
In this work, we present a supervised regres-
sion system for each cross level separately. For
P-S level and S-Ph level, we regard the paragraph
of P-S as a long sentence, and the phrase of S-
Ph as a short sentence. Then we use various types
of text similarity features including string features,
knowledge based features, corpus based features,
syntactic features, machine translation based fea-
tures, multi-level text features and so on, to cap-
ture the semantic similarity between two texts.
Some of these features are borrowed from our pre-
vious system in the Semantic Textual Similarity
(STS) task in
?
SEM Shared Task 2013 (Zhu and
Lan, 2013). Others followed the previous work
in (
?
Saric et al., 2012) and (Pilehvar et al., 2013).
For Ph-W level and W-Se level, since the text pairs
lack contextual information, for example, word or
sense alone no longer shares the property of sen-
tence, most features used in P-S level and S-Ph
level are not applicable or available. To overcome
the problem of insufficient information in word
and sense level, we propose several information
enrichment methods to extend information with
the aid of WordNet (Miller, 1995), which signif-
icantly improved the system performance.
The rest of this paper is organized as follows.
265
Section 2 describes the similarity features used on
four cross levels in detail. Section 3 presents ex-
periments and the results of four cross levels on
training data and test data. Conclusions and future
work are given in Section 4.
2 Text Similarity Measurements
To estimate the semantic similarity on P-S level
and S-Ph level, we treat the text pairs on both lev-
els as traditional semantic similarity computation
on sentence level and adopt 7 types of features,
i.e., string features, knowledge based features, cor-
pus based features, syntactic features, machine
translation based features, multi-level text features
and other features. All of them are borrowed
from previous work due to their superior perfor-
mance reported. For Ph-W level and W-Se level,
since word and sense alone cannot be treated as
sentence, we propose an information enrichment
method to extend original text with the help of
WordNet. Once the word or sense is enriched with
its synonym and its definition description, we can
thus adopt the previous features as well.
2.1 Preprocessing
For P-S level and S-Ph level, we perform text pre-
processing before we extract semantic similarity
features. Firstly, the Stanford parser
1
is used for
sentence tokenization and parsing. Specifically,
the tokens n?t and ?m are replaced with not and
am. Secondly, the Stanford POS Tagger
2
is used
for POS tagging. Thirdly, we use Natural Lan-
guage Toolkit
3
for WordNet based Lemmatiza-
tion, which lemmatizes the word to its nearest base
form that appears in WordNet, for example, was
is lemmatized as is rather than be.
2.2 Features on P-S Level and S-Ph Level
We treat all text pairs of P-S level and S-Ph level
as sentences and then extract 7 types of similar-
ity features as below. Totally we get 52 similarity
features. Generally, these similarity features are
represented as numerical values.
String features. Intuitively, if two texts share
more strings, they are considered to be more se-
mantic similar. We extract 13 string based features
in consideration of the common sequence shared
1
http://nlp.stanford.edu/software/lex-parser.shtml
2
http://nlp.stanford.edu/software/tagger.shtml
3
http://nltk.org/
by two texts. We chose the Longest Common Se-
quence (LCS) feature (Zhu and Lan, 2013), the N-
gram Overlap feature (n=1,2,3) and the Weighted
Word Overlap feature (
?
Saric et al., 2012). All
these features are computed from original text
and from the processed text after lemmatization
as well. Besides, we also computed the N-gram
Overlap on character level, named Character N-
gram (n=2,3,4).
Knowledge based features. Knowledge based
similarity estimation relies on the semantic net-
work of words. In this work we used the knowl-
edge based features in our previous work (Zhu and
Lan, 2013), which include four word similarity
metrics based onWordNet: Path similarity (Banea
et al., 2012), WUP similarity (Wu and Palmer,
1994), LCH similarity (Leacock and Chodorow,
1998) and Lin similarity (Lin, 1998). Then two
strategies, i.e., the best alignment strategy and the
aggregation strategy, are employed to propagate
the word similarity to the text similarity. Totally
we get 8 knowledge based features.
Corpus based features. Latent Semantic Analy-
sis (LSA) (Landauer et al., 1997) is a widely used
corpus based measure when evaluating text simi-
larity. In this work we use the Vector Space Sen-
tence Similarity proposed by (
?
Saric et al., 2012),
which represents each sentence as a single distri-
butional vector by summing up the LSA vector of
each word in the sentence. Two corpora are used
to compute the LSA vector of words: New York
Times Annotated Corpus (NYT) and Wikipedia.
Besides, in consideration of different weights for
different words, they also calculated the weighted
LSA vector for each word. In addition, we use
the Co-occurrence Retrieval Model (CRM) feature
from our previous work (Zhu and Lan, 2013) as
another corpus-based feature. The CRM is calcu-
lated based on a notion of substitutability, that is,
the more appropriate it is to substitute word w
1
in place of word w
2
in a suitable natural language
task, the more semantically similar they are. At
last, 6 corpus based features are extracted.
Syntactic features. Dependency relations of sen-
tences often contain semantic information. In this
work we follow two syntactic dependency similar-
ity features presented in our previous work (Zhu
and Lan, 2013), i.e., Simple Dependency Overlap
and Special Dependency Overlap. The Simple De-
pendency Overlap measures all dependency rela-
tions while the Special Dependency Overlap fea-
266
ture only focuses on the primary roles extracted
from several special dependency relations, i.e.,
subject, object and predict.
Machine Translation based features. Machine
translation (MT) evaluation metrics are designed
to assess whether the output of a MT system is
semantically equivalent to a set of reference trans-
lations. This type of feature has been proved to
be effective in our previous work (Zhu and Lan,
2013). As a result, we extend the original 6 lexical
level MT metrics to 10 metrics, i.e., WER, TER,
PER, BLEU, NIST, ROUGE-L, GTM-1,GTM-2,
GTM-3 and METEOR-ex. All these metrics are
calculated using the Asiya Open Toolkit for Auto-
matic Machine Translation (Meta-) Evaluation
4
.
Multi-level text Features. (Pilehvar et al., 2013)
presented a unified approach to semantic similar-
ity at multiple levels from word senses to text
documents through the semantic signature repre-
sentation of texts (e.g., sense, word or sentence).
Given initial nodes (senses), they performed ran-
dom walks on semantic network like WordNet,
then the resulting frequency distribution over all
nodes in WordNet served as semantic signature of
the text. By doing so the similarity of two texts
can be computed as the similarity of two seman-
tic signatures. In this work, we borrowed their
semantic signature method and adopted 3 similar-
ity measures to estimate two semantic signatures,
i.e., Cosine similarity, Weighted Overlap and Top-
k Jaccard (k=250, 500).
Other Features. Besides, other simple surface
features from texts, such as numbers, symbols and
length of texts, are extracted. Following (
?
Saric et
al., 2012) we adopt relative length difference, rela-
tive information content difference, numbers over-
lap, case match and stocks match.
2.3 Features on Ph-W Level
For Ph-W level, since word and phrase no longer
share the property of sentence, most features used
for sentence similarity estimation are not applica-
ble for this level. Therefore, we adopt the follow-
ing features as the basic feature set for Ph-W level.
String features. This type contains two fea-
tures. The first is a boolean feature which records
whether the word appears in the phrase. The sec-
ond is the Weighted Word Overlap feature men-
tioned in Section 2.2.
Knowledge based features. As described in Sec-
4
http://nlp.lsi.upc.edu/asiya/
tion 2.2, we compute the averaged score and the
maximal score between word and phrase using the
four word similarity measures based on WordNet,
i.e., Path, WUP, LCH and Lin.
Corpus based features. We adopt the Vector
Space Similarity described in Section 2.2. Specif-
ically, for word the single distributional vector is
the LSA vector of itself.
Multi-level text Features. As described in Sec-
tion 2.2, since the semantic signatures are pro-
posed for various kinds of texts (e.g., sense, word
or sentence), they serve as one basic feature.
Obviously, the above features extracted from
the phrase-word pair is significantly less than the
features used in P-S level and S-Ph level. This is
because the information contained in phrase-word
pair is much less than that in sentences and para-
graphs. To overcome this information insufficient
problem, we propose an information enrichment
method based on WordNet to extend the initial
word in Ph-W level as below.
Word Expansion with Definition. For the word
part in Ph-W level, we extract its definition in
terms of its most common concept inWordNet and
then replace the initial word with this definition.
This gives a much richer set of initial single word.
Since a word may have many senses, not all of
this word definition expansion are correct. But we
show below empirically that using this expanded
set improves performance. By doing so we treat
the phrase and the definition of the original word
as two sentences, and thus, all features described
in Section 2.2 are calculated.
2.4 Features on W-Se Level
For W-Se level, the information that a word and
a sense carry is less than other levels. Hence, the
basic features that can be extracted from the origi-
nal word-sense pair are even less than Ph-W level.
Therefore the basic features we use for W-Se level
are as follows.
String features. Two boolean string features
are used. One records whether the word-sense
pair shares the same POS tag and another records
whether the word-sense pair share the same word.
Knowledge based features. As described in Sec-
tion 2.2, four knowledge-based word similarity
measures based on WordNet are calculated.
Multi-level text Features. The multi-level text
features are the same as Ph-W level.
In consideration of the lack of contextual infor-
267
mation between word-sense pair, we also propose
three information enrichment methods in order to
generate more effective information for word and
sense with the aid of WordNet.
Word Expansion with Synonyms. For the word
part in W-Se level, we extract its synonyms with
the help of WordNet, then update the values
of above basic features if its synonyms achieve
higher feature value than the original word itself.
Sense Expansion with Definition. For the sense
in W-Se level, we directly use its definition in
WordNet to enrich its information. By doing so
the similarity estimation of W-Se level can be con-
verted to that of word-phrase level, therefore we
use all basic features for Ph-W level described in
Section2.3.
Word-Sense Expansion with Definition. Un-
like the above two expansion methods which focus
only on one part of W-Se level, the third method is
to enrich information for word and sense together
by using their definitions in WordNet. As before
we extract the word definition in terms of its most
common concept in WordNet and then replace the
initial word with this definition. Then we use all
features in Section 2.2.
3 Experiment and Results
We adopt supervised regression model for each
cross level. In order to compare the performance
of different regression algorithms, we perform 5-
fold cross validation on training data for each cross
level. We used several regression algorithms in-
cluding Support Vector Regression (SVR) with
3 different kernels (i.e., linear, polynomial and
rbf), Random Forest, Stochastic Gradient Descent
(SGD) and Decision Tree implemented in the
scikit-learn toolkit (Pedregosa et al., 2011). The
system performance is evaluated in Pearson corre-
lation (r) (official measure) and Spearman?s rank
correlation (?).
3.1 Results on Training Data
Table 1 and Table 2 show the averaged perfor-
mance of different regression algorithms in terms
of Pearson correlation (r) and Spearman?s rank
correlation (?) on the training data of P-S level and
S-Ph level using 5-fold cross validation, where the
standard deviation is given in brackets. The re-
sults show that Random Forest performs the best
both on P-S level and S-Ph level whether in (r) or
(?). We also find that the results of P-S level are
better than that of S-Ph level, and the reason may
be that paragraph and sentence pair contain more
information than the sentence and phrase pair.
Regression Algorithm r (%) ? (%)
SVR, ker=rbf 80.70 (?1.47) 79.90 (?1.66)
SVR, ker=poly 73.78 (?1.57) 74.41 (?1.89)
SVR, ker=linear 80.43 (?1.13) 79.46 (?1.51)
Random Forest 80.92 (?1.40) 80.20 (?2.00)
SGD 77.61 (?0.76) 77.14 (?1.49)
Decision Tree 73.23 (?2.14) 71.84 (?2.55)
Table 1: Results of different algorithms using 5-
fold cross validation on training data of P-S level
Regression Algorithm r (%) ? (%)
SVR, ker=rbf 66.14 (?5.14) 65.76 (?5.93)
SVR, ker=poly 58.93 (?2.29) 63.62 (?4.15)
SVR, ker=linear 66.78 (?4.51) 66.34 (?4.90)
Random Forest 73.18 (?5.23) 70.30 (?5.51)
SGD 63.18 (?3.61) 64.80 (?4.21)
Decision Tree 67.66 (?6.76) 66.03 (?6.64)
Table 2: Results of different algorithms using 5-
fold cross validation on training data of S-Ph level
Table 3 shows the results of different regression
algorithms and different feature sets in terms of
r and ? on the training data of Ph-W level us-
ing 5-fold cross validation, where the basic fea-
tures are denoted as Feature Set A and their com-
bination with word definition expansion features
are denoted as Feature Set B. The results show
that almost all algorithms performance have been
improved by using word definition expansion fea-
ture except Decision Tree. This proves the effec-
tiveness of the information enrichment method we
proposed in this level. Besides, Random Forest
achieves the best performance again with r=44%
and ?=41%. However, in comparison with P-S
level and S-Ph level, all scores in Table 3 drop a
lot even with information enrichment method. The
possible reason may be two: the reduction of in-
formation on Ph-W level and our information en-
richment method brings in a certain noise as well.
For W-Se level, in order to examine the perfor-
mance of different information enrichment meth-
ods, we perform experiments on 4 different fea-
ture sets from A to D, where feature set A con-
tains the basic features, feature set B, C and D
add one information enrichment method based on
former feature set. Table 4 and 5 present the r
and ? results of 4 feature sets using different re-
gression algorithms. From Table 4 and 5 we see
that most correlation scores are below 40% and
268
Regression Algorithm r (%) ? (%)
Feature Set A
1
Feature Set B
2
Feature Set A Feature Set B
SVR, ker=rbf 34.67 (?4.34) 42.62 (?6.36) 33.26 (?4.24) 40.87 (?6.24)
SVR, ker=poly 19.00 (?4.26) 24.06 (?5.55) 21.13 (?4.86) 28.35 (?6.11)
SVR, ker=linear 34.87 (?4.65) 41.91 (?2.05) 35.42 (?5.05) 42.69 (?0.55)
Random Forest 43.17 (?7.72) 44.00 (?6.88) 40.34 (?5.71) 41.80 (?6.76)
SGD 26.20 (?3.37) 38.69 (?4.60) 23.55 (?5.01) 38.00 (?2.64)
Decision Tree 39.22 (?7.54) 32.22 (?12.74) 38.90 (?6.03) 31.64 (?10.47)
1
Feature Set A = basic feature set
2
Feature Set B = Feature Set A + Word Definition Expansion Features
Table 3: Results of different algorithms using 5-fold cross validation on training data of Ph-W level
the performance of W-Se level is the worst among
all these four levels. This illustrates that the less
information the texts contain, the worse perfor-
mance the model achieves. Again the Random
Forest algorithm performs the best among all algo-
rithms. Again almost all information enrichment
features perform better than Feature set A. This il-
lustrates that these information enrichment meth-
ods do help to improve performance. When we ob-
serve the three information enrichment methods,
we find that feature set C performs the best. In
comparison with feature set C, feature set B only
used word synonyms to expand information and
this expansion is quite limited. Feature set D per-
forms better than B but still worse than C. The rea-
son may be that when we extend sense with its def-
inition, the definition is accurate and exactly repre-
sents the meaning of sense. However since a word
often contains more than one concepts, and when
we use the definition of the most common concept
to extend word, such extension may not be correct
and the generated information may contain more
noise and/or change the original meaning of word.
3.2 Results on Test Data
According to the experiments on training data, we
select Random Forest as the final regression algo-
rithm. The number of trees in Random Forest n is
optimized to 50 and the rest parameters are set to
be default. All features in Section 2.2 are used on
P-S level, S-Ph level and Ph-W level. For W-Se
level, we take all features except word-sense def-
inition expansion feature which has been shown
to impair the system performance. For each level,
all training examples are used to learn the corre-
sponding regression model. According to the offi-
cial results released by organizers, Table 6 and Ta-
ble 7 list the top 3 systems in terms of r (official)
and ?. Our final systems rank the second both in
terms of r and ? and also achieve the second place
on P-S level, S-Ph level and Ph-W level, as well
as the 4th place on W-Se level in terms of official
Pearson correlation.
Team P-S S-Ph Ph-W W-Se r Rank
SimCompass 0.811 0.742 0.415 0.356 1
ECNU 0.834 0.771 0.315 0.269 2
UNAL-NLP 0.837 0.738 0.274 0.256 3
Table 6: Pearson Correlation (official) on test data
Team P-S S-Ph Ph-W W-Se ? Rank
SimCompass 0.801 0.728 0.424 0.344 1
ECNU 0.821 0.757 0.306 0.263 2
UNAL-NLP 0.820 0.710 0.249 0.236 6
Table 7: Spearman Correlation on test data
4 Conclusion
We build a supervised Random Forest regression
model for each cross level. For P-S and S-Ph level,
we adopt the ensemble of heterogeneous similar-
ity features, i.e., string features, knowledge based
features, corpus based features, syntactic features,
machine translation based features, multi-level
text features and other features to capture the se-
mantic similarity between two texts with distinc-
tively different lengths. For Ph-W and W-Se level,
we propose information enrichment methods to
lengthen original texts in order to generate more
semantic features, which has been proved to be ef-
fective. Our submitted final systems rank the 2nd
out of 18 teams both on Pearson Rank (official
rank) and Spearman Rank, and also rank the sec-
ond place on P-S level, S-Ph level and Ph-W level,
as well as the 4th place on W-Se level in terms of
Pearson correlation. In future work we will focus
on information enrichment methods which bring
in more accurate information and less noises.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
269
Regression Algorithm Feature Set A
1
Feature Set B
2
Feature Set C
3
Feature Set D
4
SVR, ker=rbf 29.85 (?7.29) 34.49 (?5.55) 36.80 (?6.46) 22.19 (?6.49)
SVR, ker=poly 24.62 (?3.63) 29.27 (?3.53) 26.55 (?1.27) 25.89 (?5.63)
SVR, ker=linear 29.58 (?5.88) 34.87 (?3.97) 35.96 (?1.75) 34.57 (?3.75)
Random Forest 22.87 (?5.59) 33.97 (?1.78) 40.43 (?3.00) 37.54 (?3.20)
SGD 26.32 (?7.31) 27.36 (?6.44) 32.50 (?6.02) 18.00 (?6.13)
Decision Tree 23.40 (?5.65) 26.33 (?3.86) 33.64 (?6.97) 31.86 (?3.95)
1
Feature Set A = basic feature set
2
Feature Set B = Feature Set A + Synonym Expansion
3
Feature Set C = Feature Set B + Sense Definition Expansion Features
4
Feature Set D = Feature Set C + Word-Sense Definition Expansion Features
Table 4: Results of different algorithms using 5-fold CV on training data of W-Se level (r (%))
Regression Algorithm Feature Set A Feature Set B Feature Set C Feature Set D
SVR, ker=rbf 28.41 (?8.99) 29.61 (?6.23) 34.18 (?6.36) 22.90 (?6.78)
SVR, ker=poly 23.05 (?7.53) 22.47 (?4.47) 21.63 (?4.37) 25.37 (?7.25)
SVR, ker=linear 27.29 (?7.02) 31.79 (?4.00) 34.75 (?3.55) 34.19 (?3.06)
Random Forest 19.66 (?6.75) 31.98 (?3.21) 38.57 (?3.60) 37.56 (?3.15)
SGD 24.12 (?7.98) 24.62 (?6.36) 29.27 (?5.86) 23.05 (?11.23)
Decision Tree 22.30 (?5.25) 25.09 (?3.64) 31.99 (?7.81) 30.51 (?5.27)
Table 5: Results of different algorithms using 5-fold CV on training data of W-Se level (? (%))
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergis-
tic approach to semantic text similarity. pages 635?
642. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. pages 435?440. First Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Michael Heilman and Nitin Madnani. 2012. Ets:
Discriminative edit models for paraphrase scoring.
pages 529?535. First Joint Conference on Lexical
and Computational Semantics (*SEM).
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a
comparison of latent semantic analysis and humans.
In Proceedings of the 19th annual meeting of the
Cognitive Science Society, pages 412?417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th in-
ternational conference on Machine Learning, vol-
ume 1, pages 296?304. San Francisco.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2013).
Frane
?
Saric, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?sic. 2012. Takelab: Systems
for measuring semantic text similarity. pages 441?
448. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138. Association for Com-
putational Linguistics.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. Atlanta, Georgia, USA,
page 124.
270
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271?277,
Dublin, Ireland, August 23-24, 2014.
ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for
Semantic Relatedness and Textual Entailment
Jiang Zhao, Tian Tian Zhu, Man Lan
?
Department of Computer Science and Technology
East China Normal University
51121201042,51111201046@ecnu.cn; mlan@cs.ecnu.edu.cn
?
Abstract
This paper presents our approach to se-
mantic relatedness and textual entailment
subtasks organized as task 1 in SemEval
2014. Specifically, we address two ques-
tions: (1) Can we solve these two sub-
tasks together? (2) Are features proposed
for textual entailment task still effective
for semantic relatedness task? To address
them, we extracted seven types of features
including text difference measures pro-
posed in entailment judgement subtask, as
well as common text similarity measures
used in both subtasks. Then we exploited
the same feature set to solve the both sub-
tasks by considering them as a regression
and a classification task respectively and
performed a study of influence of differ-
ent features. We achieved the first and the
second rank for relatedness and entailment
task respectively.
1 Introduction
Distributional Semantic Models (DSMs)(surveyed
in (Turney et al., 2010)) exploit the co-occurrences
of other words with the word being modeled to
compute the semantic meaning of the word un-
der the distributional hypothesis: ?similar words
share similar contexts? (Harris, 1954). Despite
their success, DSMs are severely limited to model
the semantic of long phrases or sentences since
they ignore grammatical structures and logical
words. Compositional Distributional Semantic
Models (CDSMs)(Zanzotto et al., 2010; Socher et
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
al., 2012) extend DSMs to sentence level to cap-
ture the compositionality in the semantic vector
space, which has seen a rapidly growing interest
in recent years. Although several CDSMs have
been proposed, benchmarks are lagging behind.
Previous work (Grefenstette and Sadrzadeh, 2011;
Socher et al., 2012) performed experiments on
their own datasets or on the same datasets which
are limited to a few hundred instances of very short
sentences with a fixed structure.
To provide a benchmark so as to compare dif-
ferent CDSMs, the sentences involving composi-
tional knowledge task in SemEval 2014 (Marelli et
al., 2014) develops a large dataset which is full of
lexical, syntactic and semantic phenomena. It con-
sists of two subtasks: semantic relatedness task,
which measures the degree of semantic relatedness
of a sentence pair by assigning a relatedness score
ranging from 1 (completely unrelated) to 5 (very
related); and textual entailment (TE) task, which
determines whether one of the following three re-
lationships holds between two given sentences A
and B: (1) entailment: the meaning of B can be
inferred from A; (2) contradiction: A contradicts
B; (3) neutral: the truth of B cannot be inferred on
the basis of A.
Semantic textual similarity (STS) (Lintean and
Rus, 2012) and semantic relatedness are closely
related and interchangeably used in many liter-
atures except that the concept of semantic simi-
larity is more specific than semantic relatedness
and the latter includes concepts as antonymy and
meronymy. In this paper we regard the semantic
relatedness task as a STS task. Besides, regardless
of the original intention of this task, we adopted
the mainstream machine learning methods instead
of CDSMs to solve these two tasks by extracting
heterogenous features.
271
Like semantic relatedness, TE task (surveyed
in (Androutsopoulos and Malakasiotis, 2009)) is
also closely related to STS task since in TE task
lots of similarity measures at different levels are
exploited to boost classification. For example,
(Malakasiotis and Androutsopoulos, 2007) used
ten string similarity measures such as cosine sim-
ilarity at the word and the character level. There-
fore, the first fundamental question arises, i.e.,
?Can we solve both of these two tasks together??
At the same time, since high similarity does not
mean entailment holds, the TE task also utilizes
other features besides similarity measures. For ex-
ample, in our previous work (Zhao et al., 2014)
text difference features were proposed and proved
to be effective. Therefore, the second question sur-
faces here, i.e., ?Are features proposed for TE task
still effective for STS task?? To answer the first
question, we extracted seven types of features in-
cluding text similarity and text difference and then
fed them to classifiers and regressors to solve TE
and STS task respectively. Regarding the second
question, we conducted a series of experiments
to study the performance of different features for
these two tasks.
The rest of the paper is organized as follows.
Section 2 briefly describes the related work on
STS and TE tasks. Section 3 presents our systems
including features, learning methods, etc. Section
4 shows the experimental results on training data
and Section 5 reports the results of our submitted
systems on test data and gives a detailed analysis.
Finally, Section 6 concludes this paper with future
work.
2 Related Work
Existing work on STS can be divided into 4
categories according to the similarity measures
used (Gomaa and Fahmy, 2013): (1) string-based
method (B?ar et al., 2012; Malakasiotis and An-
droutsopoulos, 2007) which calculates similarities
using surface strings at either character level or
word level; (2) corpus-based method (Li et al.,
2006) which measures word or sentence similar-
ities using the information gained from large cor-
pora, including Latent Semantic Analysis (LSA),
pointwise mutual information (PMI), etc. (3)
knowledge-based method (Mihalcea et al., 2006)
which estimates similarities with the aid of ex-
ternal resources, such as WordNet
1
; (4) hybrid
1
http://wordnet.princeton.edu/
method (Zhu and Lan, 2013; Croce et al., 2013)
which integrates multiple similarity measures and
adopts supervised machine learning algorithms to
learn the different contributions of different fea-
tures.
The approaches to the task of TE can be roughly
divided into two groups: (1) logic inference
method (Bos and Markert, 2005) where automatic
reasoning tools are used to check the logical repre-
sentations derived from sentences and (2) machine
learning method (Zhao et al., 2013; Gomaa and
Fahmy, 2013) where a supervised model is built
using a variety of similarity scores.
Unlike previous work which separately ad-
dressed these two closely related tasks by using
simple feature types, in this paper we endeavor to
simultaneously solve these two tasks by using het-
erogenous features.
3 Our Systems
We consider the two tasks as one by exploiting the
same set of features but using different learning
methods, i.e., classification and regression. Seven
types of features are extracted and most of them
are based on our previous work on TE (Zhao et
al., 2014) and STS (Zhu and Lan, 2013). Many
learning algorithms and parameters are examined
and the final submitted systems are configured ac-
cording to the preliminary results on training data.
3.1 Preprocessing
Three text preprocessing operations were per-
formed before we extracted features, which in-
cluded: (1) we converted the contractions to their
formal writings, for example, doesn?t is rewrit-
ten as does not. (2) the WordNet-based Lemma-
tizer implemented in Natural Language Toolkit
2
was used to lemmatize all words to their nearest
base forms in WordNet, for example, was is lem-
matized to be. (3) we replaced a word from one
sentence with another word from the other sen-
tence if the two words share the same meaning,
where WordNet was used to look up synonyms.
No word sense disambiguation was performed and
all synsets for a particular lemma were considered.
3.2 Feature Representations
3.2.1 Length Features (len)
Given two sentences A and B, this feature type
records the length information using the follow-
2
http://nltk.org/
272
ing eight measure functions:
|A|, |B|, |A?B|, |B ?A|, |A ?B|, |A ?B|,
(|A|?|B|)
|B|
,
(|B|?|A|)
|A|
where |A| stands for the number of non-repeated
words in sentence A , |A?B| means the number of
unmatched words found in A but not in B , |A ?B|
stands for the set size of non-repeated words found
in either A or B and |A ? B| means the set size of
shared words found in both A and B .
Moreover, in consideration of different types of
words make different contributions to text similar-
ity, we also recorded the number of words in set
A?B and B ?A whose POS tags are noun, verb,
adjective and adverb respectively. We used Stan-
ford POS Tagger
3
for POS tagging. Finally, we
collected a total of sixteen features.
3.2.2 Surface Text Similarity (st)
As shown in Table 1, we adopted six commonly
used functions to calculate the similarity between
sentence A and B based on their surface forms,
where
??
x and
??
y are vectorial representations of
sentences A and B in tf ? idf schema.
Measure Definition
Jaccard S
jacc
= |A ? B|/|A ? B|
Dice S
dice
= 2 ? |A ? B|/(|A|+ |B|)
Overlap S
over
= |A ? B|/|A| and |A ? B|/|B|
Cosine S
cos
=
??
x ?
??
y /(?
??
x ? ? ?
??
y ?)
Manhattan M(
??
x ,
??
y ) =
n
?
i=1
|x
i
? y
i
|
Euclidean E(
??
x ,
??
y ) =
?
n
?
i=1
(x
i
? y
i
)
2
Table 1: Surface text similarity measures and their
definitions used in our experiments.
We also used three statistical correlation coef-
ficients (i.e., Pearson, Spearmanr, Kendalltau) to
measure similarity by regarding the vectorial rep-
resentations as different variables. Thus we got ten
features at last.
3.2.3 Semantic Similarity (ss)
The above surface text similarity features only
consider the surface words rather than their ac-
tual meanings in sentences. In order to build the
semantic representations of sentences, we used a
latent model to capture the contextual meanings
of words. Specifically, we adopted the weighted
textual matrix factorization (WTMF) (Guo and
Diab, 2012) to model the semantics of sentences
due to its reported good ability to model short
texts. This model first factorizes the original term-
sentence matrix X into two matrices such that
3
http://nlp.stanford.edu/software/tagger.shtml
X
i,j
? P
T
?,i
.Q
?,j
, where P
?,i
is a latent seman-
tic vector profile for word w
i
and Q
?,j
is a vector
profile that represents the sentence s
j
. Then we
employed the new representations of sentences,
i.e., Q, to calculate the semantic similarity be-
tween sentences using Cosine, Manhattan, Eu-
clidean, Pearson, Spearmanr, Kendalltau measures
respectively, which results in six features.
3.2.4 Grammatical Relationship (gr)
The grammatical relationship feature measures
the semantic similarity between two sentences
at the grammar level and this feature type was
also explored in our previous work (Zhao et al.,
2013; Zhu and Lan, 2013). We used Stanford
Parser
4
to acquire the dependency information
from sentences and the grammatical information
are represented in the form of relation unit, e.g.
nsubj(example, this), where nsubj stands for a de-
pendency relationship between example and this.
We obtained a sequence of relation units for each
sentence and then used them to estimate similarity
by adopting eight measure functions described in
Section 3.2.1, resulting in eight features.
3.2.5 Text Difference Measures (td)
There are two types of text difference measures.
The first feature type is specially designed for
the contradiction entailment relationship, which
is based on the following observation: there ex-
ist antonyms between two sentences or the nega-
tion status is not consistent (i.e., one sentence has
a negation word while the other does not have) if
contradiction holds. Therefore we examined each
sentence pair and set this feature as 1 if at least one
of these conditions is met, otherwise -1. WordNet
was used to look up antonyms and a negation list
with 28 words was used.
The second feature type is extracted from two
word sets A?B and B?A as follows: we first cal-
culated the similarities between every word from
A ? B and every word from B ? A , then took the
maximum, minimum and average value of them as
features. In our experiments, four WordNet-based
similarity measures (i.e., path, lch, wup, jcn (Go-
maa and Fahmy, 2013)) were used to calculate the
similarity between two words.
Totally, we got 13 text difference features.
4
http://nlp.stanford.edu/software/lex-parser.shtml
273
3.2.6 String Features (str)
This set of features is taken from our previous
work (Zhu and Lan, 2013) due to its superior per-
formance.
Longest common sequence (LCS) We computed
the LCS similarity on the original and lemmatized
sentences. It was calculated by finding the maxi-
mum length of a common contiguous subsequence
of two strings and then dividing it by the smaller
length of two strings to eliminate the impacts of
length imbalance.
Jaccard similarity using n-grams We obtained
n-grams at three different levels, i.e., the origi-
nal word level, the lemmatized word level and the
character level. Then these n-grams were used for
calculating Jaccard similarity defined in Table 1.
In our experiments, n = {1, 2, 3} were used for
the word level and n = {2, 3, 4} were used for the
character level.
Weighted word overlap (WWO) Since not all
words are equally important, the traditional Over-
lap similarity may not be always reasonable. Thus
we used the information content of word w to es-
timate the importance of word w as follows:
ic(w) = ln
?
w
?
?C
freq(w
?
)
freq(w)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the
corpus. To compute ic(w), we used the Web 1T
5-gram Corpus
5
. Then the WWO similarity of
two sentence s
1
and s
2
was calculated as follows:
Sim
wwo
(s
1
, s
2
) =
?
w?s
1
?s
2
ic(w)
?
w
?
?s
2
ic(w
?
)
Due to its asymmetry, we used the harmonic mean
of Sim
wwo
(s
1
, s
2
) and Sim
wwo
(s
2
, s
1
) as the fi-
nal WWO similarity. The WWO similarity is cal-
culated on the original and lemmatized strings re-
spectively.
Finally, we got two LCS features, nine Jaccard
n-gram features and two WWO features.
3.2.7 Corpus-based Features (cps)
Two types of corpus-based feature are also bor-
rowed from our previous work (Zhu and Lan,
2013), i.e., vector space sentence similarity and
co-occurrence retrieval model (CRM), which re-
sults in six features.
5
https://catalog.ldc.upenn.edu/LDC2006T13
Co-occurrence retrieval model (CRM) The
CRM word similarity is calculated as follows:
Sim
CRM
(w
1
, w
2
) =
2 ? |c(w
1
) ? c(w
2
)|
|c(w
1
)|+ |c(w
2
)|
where c(w) is the set of words that co-occur with
word w. We used the 5-gram part of the Web 1T
5-gram Corpus to obtain c(w). We only consid-
ered the word w with |c(w)| > T and then took
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). In our experi-
ment, we set T = {50, 200}. To propagate the
similarity from words to sentences, we adopted
the best alignment strategy used in (Banea et al.,
2012) to align two sentences.
Vector space sentence similarity This feature set
is taken from (
?
Sari?c et al., 2012), which is based
on distributional vectors of words. First we per-
formed latent semantic analysis (LSA) over two
corpora, i.e., the New York Times Annotated Cor-
pus (NYT) (Sandhaus, 2008) andWikipedia, to es-
timate the distributions of words. Then we used
two strategies to convert the distributional mean-
ings of words to sentence level: (i) simply sum-
ming up the distributional vector of each word w
in the sentence, (ii) using the information content
ic(w) to weigh the LSA vector of each wordw and
summing them up. Then we used cosine similarity
to measure the similarity of two sentences.
3.3 Learning Algorithms
We explored several classification algorithms to
classify entailment relationships and regression
algorithms to predict similarity scores using the
above 72 features after performing max-min stan-
dardization procedure by scaling them to [-1,1].
Five supervised learning methods were explored:
Support Vector Machine (SVM) which makes the
decisions according to the hyperplanes, Random
Forest (RF) which constructs a multitude of de-
cision trees at training time and selects the mode
of the classes output by individual trees, Gradient
Boosting (GB) that produces a prediction model
in the form of an ensemble of weak prediction
models, k-nearest neighbors (kNN) that decides
the class labels with the aid of the classes of k
nearest neighbors, and Stochastic Gradient De-
scent (SGD) which uses SGD technique to min-
imize loss functions. These supervised learning
methods are implemented in scikit-learn toolkit
(Pedregosa et al., 2011). Besides, we also used
a semi-supervised learning strategy for both tasks
274
in order to make full use of unlabeled test data.
Specifically, the co-training algorithm was used to
address TE task according to (Zhao et al., 2014).
Its strategy is to train two classifiers with two data
views and to add the top confident predicted in-
stances by one classifier to expand the training set
of another classifier and then to re-train the two
classifiers on the expanded training sets. For STS
task, we utilized CoReg algorithm (Zhou and Li,
2005) which uses two kNN regressors to perform
co-training paradigm.
3.4 Evaluation Measures
In order to evaluate the performance of differ-
ent algorithms, we adopted the official evaluation
measures, i.e., Pearson correlation coefficient for
STS task and accuracy for TE task.
4 Experiments on Training Data
To make a reasonable comparison between differ-
ent algorithms, we performed 5-fold cross valida-
tion on training data with 5000 sentence pairs. The
parameters tuned in different algorithms are listed
below: the trade-off parameter c in SVM, the num-
ber of trees n in RF, the number of boosting stages
n in GB, the number of nearest neighbors k in kNN
and the number of passes over the training data n
in SGD. The rest parameters are set to be default.
Algorithm
STS task TE task
Pearson para. Accuracy para.
SVM .807?.058 c=10 83.46?2.09 c=100
RF .805?.052 n=40 83.16?2.64 n=30
GB .806?.055 n=210 83.22?2.48 n=140
kNN .797?.062 k=25 82.54?2.45 k=17
SGD .765?.064 n=29 78.88?1.99 n=15
Table 2: The 5-fold cross validation results on
training data with mean and standard deviation for
each algorithm.
Table 2 reports the experimental results of 5-
fold cross validation with mean and standard devi-
ation and the optimal parameters on training data.
The results of semi-supervised learning methods
are not listed because only a few parameters are
tried due to the limit of time. From this table we
see that SVM, RF and GB perform comparable re-
sults to each other.
5 Results on Test Data
5.1 Submitted System Configurations
According to the above preliminary experimental
results, we configured five final systems for each
task. Table 3 presents the classification and regres-
sion algorithms with their parameters used in the
five systems for each task.
System STS task TE task
1 SVR, c=10 SVC, c=100
2 GB, n=210 GB, n=140
3 RF, n=40 RF, n=30
4 CoReg, k=13 co-training, k=40
5 majority voting majority voting
Table 3: Five system configurations for test data
for two tasks.
Among them, System 1 acts as our primary
and baseline system that employs SVM algorithm
and as comparison System 2 and System 3 exploit
GB and RF algorithm respectively. Unlike super-
vised settings in the aforementioned systems, Sys-
tem 4 employs a semi-supervised learning strategy
to make use of unlabeled test data. For CoReg,
the number of iteration and the number of near-
est neighbors are set as 100 and 13 respectively,
and for each iteration in co-training, the number
of confident predictions is set as 40. To further
improve performance, System 5 combines the re-
sults of 5 different algorithms (i.e. MaxEnt, SVM,
kNN, GB, RF) through majority voting. We used
the averaged values of the outputs from different
regressors as final similarity scores for semantic
similarity measurement task and chose the major
class label for entailment judgement task.
5.2 Results and Discussion
Table 4 lists the final results officially released by
the organizers in terms of Pearson and accuracy.
The best performance among these five systems is
shown in bold font. All participants can submit a
maximum of five runs for each task and only one
primary system is involved in official ranking. The
lower part of Table 4 presents the top 3 results and
the results with ? are achieved by our systems.
System STS task TE task(%)
1 0.8279 83.641
2 0.8389 84.128
3 0.8414 83.945
4 0.8210 81.165
5 0.8349 83.986
rank 1st 0.8279* 84.575
rank 2nd 0.8272 83.641*
rank 3rd 0.8268 83.053
Table 4: The results of our five systems for two
tasks and the officially top-ranked systems.
From this table, we found that (1) System 3 (us-
275
ing GB algorithm) and System 2 (using RF algo-
rithm) achieve the best performance among three
supervised systems in STS and TE task respec-
tively. However, there is no significant difference
among these systems. (2) Surprisingly, the semi-
supervised system (i.e., System 4) that employs
the co-training strategy to make use of test data
performs the worst, which is beyond our expecta-
tion. Based on our further observation in TE task,
the possible reason is that a lot of misclassified ex-
amples are added into the training pool in the ini-
tial iteration, which results in worse models built
in the subsequent iterations. And we speculate that
the weak learner kNN employed in CoReg may
lead to poor performance as well. (3) The major-
ity voting strategy fails to boost the performance
since GB and RF algorithm obtain the best perfor-
mance among these algorithms. (4) Our systems
obtain very good results on both STS and TE task,
i.e., we rank 1st out of 17 participants in STS task
and rank 2nd out of 18 participants in TE task ac-
cording to the results of primary systems and as
shown in Table 4 our primary system (i.e., System
1) do not achieve the best performance.
In a nutshell, our systems rank first and second
in STS and TE task respectively. Therefore the
answer to the first question raised in Section 1 is
yes. For two tasks, i.e., STS and TE, which are
very closely related but slightly different, we can
use the same features to solve them together.
5.3 Feature Combination Experiments
To answer the second question and explore the in-
fluences of different feature types, we performed
a series of experiments under the best system set-
ting. Table 5 shows the results of different feature
combinations where for each time we selected and
added one best feature type. From this table, we
find that for STS the most effective feature is cps
and for TE task is td. Almost all feature types have
positive effects on performance. Specifically, td
alone achieves 81.063% in TE task which is quite
close to the best performance (84.128%) and cps
alone achieves 0.7544 in STS task. Moreover, the
td feature proposed for TE task is quite effective
in STS task as well, which suggests that text se-
mantic difference measures are also crucial when
measuring sentence similarity.
Therefore the answer to the second question is
yes. It is clear that the features proposed for TE are
also effective for STS and heterogenous features
yield better performance than a single feature type.
len st ss gr td str cps result
+ 0.7544 (STS)
+ + 0.8057(+5.13)
+ + + 0.8280(+2.23)
+ + + + 0.8365(+0.85)
+ + + + + 0.8426(+0.61)
+ + + + + + 0.8432(+0.06)
+ + + + + + + 0.8429(-0.03)
+ 81.063 (TE)
+ + 82.484(+1.421)
+ + + 82.992(+0.508)
+ + + + 83.844(+0.852)
+ + + + + 83.925(+0.081)
+ + + + + + 84.067(+0.142)
+ + + + + + + 84.128(+0.061)
Table 5: Results of feature combinations, the num-
bers in the brackets are the performance incre-
ments compared with the previous results.
6 Conclusion
We set up five state-of-the-art systems and each
system employs different classifiers or regressors
using the same feature set. Our submitted systems
rank the 1st out of 17 teams in STS task with the
best performance of 0.8414 in terms of Pearson
coefficient and rank the 2nd out of 18 teams in
TE task with 84.128% in terms of accuracy. This
result indicates that (1) we can use the same fea-
ture set to solve these two tasks together, (2) the
features proposed for TE task are also effective
for STS task and (3) heterogenous features out-
perform a single feature. For future work, we may
explore the underlying relationships between these
two tasks to boost their performance by each other.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entail-
ment methods. arXiv preprint arXiv:0912.3747.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt:a supervised synergistic
approach to semantictext similarity. In First Joint
Conference on Lexical and Computational Seman-
tics (*SEM.
276
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
Joint Conference on Lexical and Computational Se-
mantics, pages 435?440. Association for Computa-
tional Linguistics.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 628?635. Association for Compu-
tational Linguistics.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. Unitor-core typed: Combining text similarity
and semantic filters through sv regression. In Pro-
ceedings of the 2nd Joint Conference on Lexical and
Computational Semantics, page 59.
Wael H Gomaa and Aly A Fahmy. 2013. A survey of
text similarity approaches. International Journal of
Computer Applications, 68(13):13?18.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics.
Zellig S Harris. 1954. Distributional structure. The
Philosophy of Linguistics,.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138?1150.
Mihai C. Lintean and Vasile Rus. 2012. Measuring se-
mantic similarity in short texts through greedy pair-
ing and word semantics. In FLAIRS Conference.
AAAI Press.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42?47. Association for Com-
putational Linguistics.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Evan Sandhaus. 2008. The new york times annotated
corpus ldc2008t19. Philadelphia: Linguistic Data
Consortium.
Socher, Richard, Huval Brody, Manning Christopher,
and Ng Andrew. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, Jeju Island, Korea.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 441?448, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
Jiang Zhao, Man Lan, and Zheng-Yu Niu. 2013. Ec-
nucs: Recognizing cross-lingual textual entailment
using multiple text similarity and text difference
measures. In Proceedings of the Seventh Interna-
tional Workshop on Semantic Evaluation (SemEval
2013), pages 118?123, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Jiang Zhao, Man Lan, Zheng-Yu Niu, and Donghong
Ji. 2014. Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In The 2014 International Joint Conference
on Neural Networks (IJCNN2014). IEEE.
Zhi-Hua Zhou and Ming Li. 2005. Semi-supervised
regression with co-training. In IJCAI, pages 908?
916.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, page 124.
277
