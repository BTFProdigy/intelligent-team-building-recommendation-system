Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1119?1127,
Beijing, August 2010
Syntax Based Reordering with Automatically Derived Rules for
Improved Statistical Machine Translation
Karthik Visweswariah
IBM Research
v-karthik@in.ibm.com
Jiri Navratil
IBM Research
jiri@us.ibm.com
Jeffrey Sorensen
Google, Inc.
sorenj@google.com
Vijil Chenthamarakshan
IBM Research
vijil.e.c@in.ibm.com
Nanda Kambhatla
IBM Research
kambhatla@in.ibm.com
Abstract
Syntax based reordering has been shown
to be an effective way of handling word
order differences between source and
target languages in Statistical Machine
Translation (SMT) systems. We present
a simple, automatic method to learn rules
that reorder source sentences to more
closely match the target language word or-
der using only a source side parse tree and
automatically generated alignments. The
resulting rules are applied to source lan-
guage inputs as a pre-processing step and
demonstrate significant improvements in
SMT systems across a variety of lan-
guages pairs including English to Hindi,
English to Spanish and English to French
as measured on a variety of internal test
sets as well as a public test set.
1 Introduction
Different languages arrange words in different or-
ders, whether due to grammatical constraints or
other conventions. Dealing with these word order
permutations is one of the fundamental challenges
of machine translation. Given an exceptionally
large training corpus, a phrase-based system can
learn these reordering on a case by case basis.
But, if our systems are to generalize to phrases not
seen in the training data, they must explicitly cap-
ture and model these reorderings. However, per-
mutations are difficult to model and impractical to
search.
Presently, approaches that handle reorderings
typically model word and phrase movements via
a distortion model and rely on the target language
model to produce words in the right order. Early
distortion models simply penalized longer jumps
more than shorter jumps (Koehn et al, 2003)
independent of the source or target phrases
in question. Other models (Tillman, 2004),
(Al-Onaizan and Papineni, 2006) generalize this
to include lexical dependencies on the source.
Another approach is to incorporate features,
based on the target syntax, during modeling and
decoding, and this is shown to be effective for var-
ious language pairs (Yamada and Knight, 2001),
(Zollmann and Venugopal, 2006). Hierarchical
phrase-based decoding (Chiang, 2005) also al-
lows for long range reordering without explic-
itly modeling syntax. While these approaches
have been shown to improve machine translation
performance (Zollmann et al, 2008) they usually
combine chart parsing with the decoding process,
and are significantly more computationally inten-
sive than phrase-based systems.
A third approach, one that has proved to be
useful for phrase-based SMT systems, is to re-
order each source-side sentence using a set of
rules applied to a parse tree of the source sen-
tence. The goal of these rules is to make the
word order of the source sentence more sim-
ilar to the expected target sentence word or-
der. With this approach, the reordering rules
are applied before training and testing with an
SMT system. The efficacy of these methods has
been shown on various language pairs including:
French to English (Xia and McCord, 2004), Ger-
man to English (Collins et al, 2005), English to
1119
Chinese, (Wang et al, 2007) and Hindi to English
(Ramanathan et al, 2008).
In this paper, we propose a simple model for re-
ordering conditioned on the source side parse tree.
The model is learned using a parallel corpus of
source-target sentence pairs, machine generated
word alignments, and source side parses. We ap-
ply the reordering model to both training and test
data, for four different language pairs: English
? Spanish, English ? French, English ? Hindi,
and English ? German. We show improvements
in machine translation performance for all of the
language pairs we consider except for English ?
German. We use this negative result to propose
extensions to our reordering model. We note that
the syntax based reordering we propose can be
combined with other approaches to handling re-
ordering and does not have to be followed by an
assumption of monotonicity. In fact, our phrase-
based model, trained upon reordered data, retains
its reordering models and search, but we expect
that these facilities are employed much more spar-
ingly with reordered inputs.
2 Related work
There is a significant quantity of work in syntax
based reordering employed to improve machine
translation systems. We summarize our contribu-
tions to be:
? Learning the reordering rules based on train-
ing data (without relying on linguistic knowl-
edge of the language pair)
? Requiring only source side parse trees
? Experimental results showing the efficacy for
multiple language pairs
? Using a lexicalized distortion model for our
baseline decoder
There have been several studies that have
demonstrated improvements with syntax
based reordering based upon hand-written
rules. There have also been studies inves-
tigating the sources of these improvements
(Zwarts and Dras, 2007). Hand-written rules
depend upon expert knowledge of the linguis-
tic properties of the particular language pair.
Initial efforts (Niessen and Ney, 2001) were
made at improving German-English translation
by handling two phenomena: question inver-
sion and detachable verb prefixes in German.
In (Collins et al, 2005), (Wang et al, 2007),
(Ramanathan et al, 2008), (Badr et al, 2009)
rules are developed for translation from Ger-
man to English, Chinese to English, English
to Hindi, and English to Arabic respectively.
(Xu et al, 2009) develop reordering rules based
upon a linguistic analysis of English and Korean
sentences and then apply those rules to trans-
lation from English into Korean and four other
languages: Japanese, Hindi, Urdu and Turkish.
Unlike this body of work, we automatically learn
the rules from the training data and show efficacy
on multiple language pairs.
There have been some studies that try to learn
rules from the data. (Habash, 2007) learns re-
ordering rules based on a dependency parse and
they report a negative result for Arabic to En-
glish translation. (Zhang et al, 2007) learn re-
ordering rules on chunks and part of speech
tags, but the rules they learn are not hierarchi-
cal and would require large amounts of training
data to learn rules for long sentences. Addition-
ally, we only keep a single best reordering (in-
stead of a lattice with possible reorderings) which
makes the decoding significantly more efficient.
(Xia and McCord, 2004) uses source and target
side parse trees to automatically learn rules to re-
order French sentences to match English order.
The requirement to have both source and target
side parse trees makes this method inapplicable
to any language that does not have adequate tree
bank resources. In addition, this work reports re-
sults using monotone decoding, since their exper-
iments using non-monotone decoding without a
distortion model were actually worse.
3 Reordering issues in specific languages
In this section we discuss the reordering issues
typical of translating between English and Hindi,
French, Spanish and German which are the four
language pairs we experiment on in this paper.
3.1 Spanish and French
Typical word ordering patterns common to these
two European languages relate to noun phrases in-
cluding groups of nouns and adjectives. In con-
1120
trast to English, French and Spanish adjectives
and adjunct nouns follow the main noun, i.e. we
typically observe a reversal of word order in noun
phrases, e.g., ?A beautiful red car? translates
into French as ?Une voiture rouge beau?, and as
?Un coche rojo bonito? into Spanish. Phrase-
based MT systems are capable of capturing these
patterns provided they occur with sufficient fre-
quency for each example in the training data. For
rare noun phrases, however, the MT may pro-
duce erroneous word order that can lead to seri-
ous distortions in the meaning. Particularly dif-
ficult are nominal phrases from specialized do-
mains that involve challenging terminology, for
example: ?group reference attribute? and ?valida-
tion checking code?. In both instances, the base-
line MT system generated translations with an in-
correct word order and, consequently, possibly a
different meaning. We will return to these two ex-
amples in Section 5.1 to compare the output of a
MT system with and without reordering.
3.2 German
Unlike French and Spanish, German poses a con-
siderably different challenge with respect to word
ordering. The most frequent reordering in German
relates to verbs, particularly verb groups consist-
ing of auxiliary and main verbs, as well as verbs
in relative clauses. Moreover, reordering patterns
between German and English tend to span large
portions of the sentence. We included German in
our investigations to determine whether our auto-
mated rule extraction procedure can capture such
long distance patterns.
3.3 Hindi
Hindi word order is significantly different than
English word order; the typical order followed
is Subject Object Verb (although Object Subject
Verb order can be used if nouns are followed by
appropriate case markers). This is in contrast to
English which has a Subject Verb Object order.
This can result in words that are close in English
moving arbitrarily far apart in Hindi depending on
the length of the noun phrase representing the ob-
ject and the length of the verb phrase. These long
range reorderings are generally hard for a phrase
based system to capture. Another way Hindi and
English differ is that prepositions in English be-
come postpositions in Hindi and appear after the
noun phrase. Again, this reordering can lead to
long distance movements of words. We include
Hindi in our investigation since it has significantly
different structure as compared to English.
4 Learning reordering rules
In this section we describe how we learn rules that
transform source parse trees so the leaf word order
is more like the target language. We restrict our-
selves to reorderings that can be obtained by per-
muting child nodes at various interior nodes in a
parse tree. With many reordering phenomena dis-
cussed in Section 3 this is a fairly strong assump-
tion about pairs of languages, and there are exam-
ples in English?Hindi where such an assumption
will not allow us to generate the right reordering.
As an example consider the English sentence ?I
do not want to play?. The sentence has a parse:
S
NP
PRP
I
VP
VBP
do
RB
not
VP
VB
want
S
VP
TO
to
VP
VB
play
The correct word order of the translation in Hindi
is ?I to play not want? In this case, the word not
breaks up the verb phrase want to play and hence
the right Hindi word order cannot be obtained by
the reordering allowed by our model. We found
such examples to be rare in English?Hindi, and
we impose this restriction for the simplicity of the
model. Experimental results on several languages
show benefits of reordering in spite of this simpli-
fying assumption.
Consider a source sentence s and its corre-
sponding constituency parse tree S1. We set up
the problem in a probabilistic framework, i.e. we
would like to build a probabilistic model P (T |S)
that assigns probabilities to trees such that the
1In this paper we work with constituency parse trees. Ini-
tial experiments, applying similar techniques to dependency
parse trees did not yield improvements.
1121
word order in trees T which are assigned higher
probability match the order of words in the target
language. A parse tree, S is a set of nodes. Inte-
rior nodes have an ordered list of children. Leaf
nodes in the tree are the words in the sentence
s, and interior nodes are labeled by the linguis-
tic constituent that they represent. Each word has
a parent node (with only one child) labeled by the
part-of-speech tag of the word.
Our model assigns non-zero probabilities to
trees that can be obtained by permuting the child
nodes at various interior nodes of the tree S. We
assume that children of a node are ordered inde-
pendently of all other nodes in the tree. Thus
P (T |S) =
?
n?I(S)
P (?(cn)|S, n, cn),
where I(S) is the set of interior nodes in the tree
S, cn is the list of children of node n and ? is a
permutation. We further assume that the reorder-
ing at a particular node is dependent only on the
labels of its children:
P (T |S) =
?
n?I(S)
P (?(cn)|cn).
We parameterize our model using a log-linear
model:
P (?(cn)|cn) =
1
Z(cn)
exp(?T f(?, cn)). (1)
We choose the simplest possible set of feature
functions: for each observed sequence of non-
terminals we have one boolean feature per per-
mutation of the sequence of non-terminals, with
the feature firing iff that particular sequence is ob-
served. Assuming, we have a training corpus C of
(T, S) tree pairs, we could optimize the parame-
ters of our model to maximize :
?
S?C P (T |S).
With the simple choice of feature functions de-
scribed above, this amounts to:
P (?(cn)|cn) =
count(?(cn))
count(cn)
,
where count(cn) is the number of times the se-
quences of nodes cn is observed in the training
data and count(?(cn)) is the number of times
that cn in S is permuted to ?(cn) in T . In Sec-
tion 6, we show considering more general fea-
ture functions and relaxing some of the indepen-
dence might yield improvements on certain lan-
guage pairs.
For each source sentence s with parse S we find
the tree T that makes the given alignment for that
sentence pair most monotone. For each node n in
the source tree S let Dn be the set of words that
are descendants of n. Let us denote by tpos(n) the
average position of words in the target sentence
that are aligned to words in Dn. Then
tpos(n) = 1|Dn|
?
w?Dn
a(w),
where a(w) is the index of the word on the target
side that w is aligned with. If a word w is not
aligned to any target word, we leave it out from
the mean position calculation above. If a word w
is aligned to many words we let a(w) be the mean
position of the words that w is aligned to. For each
node n in the tree we transform the tree by sorting
the list of children of n according to tpos. The
pairs of parse trees that we obtain (S, T ) in this
manner form our training corpus to estimate our
parameters.
In using our model, we once again go for the
simplest choice, we simply reorder the source side
sentences by choosing arg maxT P (T |S) both in
training and in testing; this amounts to reordering
each interior node based on the most frequent re-
ordering of the constituents seen in training. To
reduce the effect of noise in training alignments
we apply the reordering, only if we have seen the
constituent sequence often enough in our training
data (a count threshold parameter) and if the most
frequent reordering is sufficiently more frequent
than the next most frequent reordering (a signifi-
cance threshold).
5 Experiments
5.1 Results for French, Spanish, and German
In each language, the rule extraction was
performed using approximately 1.2M sen-
tence pairs aligned using a maxent aligner
(Ittycheriah and Roukos, 2005) trained using a
variety of domains (Europarl, computer manuals)
1122
and a maximum entropy parser for English
(Ratnaparkhi, 1999). With a significance thresh-
old of 1.2, we obtain about 1000 rules in the
eventual reordering process.
Phrase-based systems were trained for each lan-
guage pair using 11M sentence pairs spanning a
variety of publicly available (e.g. Europarl, UN
speeches) and internal corpora (IT technical and
news domains). The system phrase blocks were
extracted based on a union of HMM and max-
ent alignments with corpus-selective count prun-
ing. The lexicalized distortion model was used
as described in (Al-Onaizan and Papineni, 2006)
with a window width of up to 5 and a maximum
number of skipped (not covered) words during de-
coding of 2. The distortion model assigns a prob-
ability to a particular word to be observed with
a specific jump. The decoder uses a 5-gram in-
terpolated language model spanning the various
domains mentioned above. The baseline system
without reordering and a system with reordering
was trained and evaluated in contrastive experi-
ments. The evaluation was performed utilizing the
following (single-reference) test sets:
? News: 541 sentences from the news domain.
? TechA: 600 sentences from a computer-
related technical domain, this has been used
as a dev set.
? TechB: 1038 sentences from a similar do-
main as TechA used as a blind test.
? Dev09: 1026 sentences defined as the news-
dev2009b development set of the Workshop
on Statistical Machine Translation 2009 2.
This set provides a reference measurement
using a public data set. Previously published
results on this set can be found, for example,
in (Popovic et al, 2009).
In order to assess changes in word ordering pat-
terns prior to and after an application of the re-
ordering, we created histograms of word jumps
in the alignments obtained in the baseline as well
as in the reordered system. Given a source word
si at index i and the target word tj it is aligned
to at index j, a jump of 1 would correspond to
si+1 aligning to target word tj+1, while an align-
ment to tj?1 corresponds to a jump of -1, etc. A
2http://statmt.org/wmt09/
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1
?0.5
0
0.5
1
1.5
2
x 105
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?ES)
?8 ?6 ?4 ?2 0 2 4 6 8 10
?5000
0
5000
10000
15000
20000
Distance to next position
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?FR)
Figure 1: Difference-histogram of word order
distortions for English?Spanish (upper), and
English?French (lower).
histogram over the jump values gives us a sum-
mary of word order distortion. If all of the jumps
were one, then there is no reordering between the
two languages. To gain insight into changes in-
troduced by our reordering we look at differences
of the two histograms i.e., counts after reordering
minus counts before reordering. We would hope
that after reordering most of the jumps are small
and concentrated around one. Figure 1 shows
such difference-histograms for the language pairs
English?Spanish and English?French, respec-
tively, on a sample of about 15k sentence pairs
held out of the system training data. Here, a pos-
itive difference value indicates an increased num-
ber after reordering. In both cases a consistent
trend toward monotonicity is observed, i.e more
jumps of size one and two, and fewer large jumps.
This confirms the intended reordering effect and
indicates that the reordering rules extracted gen-
eralize well.
Table 1 shows the resulting uncased BLEU
scores for English-Spanish and English-French.
In both cases the reordering has a consistent
positive effect on the BLEU scores across test sets.
In examining the sources of improvement, we no-
ticed that word order in several noun phrases that
1123
System News TechA TechB Dev09
Baseline 0.3849 0.3371 0.3483 0.2244
Sp
an
ish
Reordered 0.4031 0.3582 0.3605 0.2320
Baseline 0.5140 0.2971 0.3035 0.2014
Fr
en
ch
Reordered 0.5242 0.3152 0.3154 0.2092
Baseline 0.2580 0.1582 0.1697 0.1281
G
er
m
an
Reordered 0.2544 0.1606 0.1682 0.1271
Baseline 20.0
H
in
di
Reordered 21.7
Table 1: Uncased BLEU scores for phrase-based
machine translation.
were not common in the training data were fixed
by use of the reordering rules.
Table 1 shows the BLEU scores for the
English?German language pair, for which a
mixed result is observed. The difference-
histogram for English?German, shown in Figure
2, differs from those of the other languages with
several increases in jumps of large magnitude, in-
dicating failure of the extracted rules to general-
ize.
The failure of our simple method to gain con-
sistent improvements comparable to Spanish and
French, along with our preliminary finding that a
relatively few manually crafted reordering rules
(we describe these in Section 6.4) tend to outper-
form our method, leads us to believe that a more
refined approach is needed in this case and will be
subject of further discussion below.
5.2 Results for Hindi
Our Hindi-English experiments were run with
an internal parallel corpus of roughly 250k sen-
tence pairs (5.5M words) consisting of various
domains (including news). To learn reordering
rules we used HMM alignments and a maxent
parser (Ratnaparkhi, 1999), with a count thresh-
old of 100, and a significance threshold of 1.7
(these settings gave us roughly 200 rules). We also
experimented with other values of these thresh-
olds and found that the performance of our sys-
tems were not very sensitive to these thresholds.
We trained Direct Translation Model 2 (DTM)
?10 ?5 0 5 10
?600
?400
?200
0
200
400
600
800
1000
1200
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?DE)
Figure 2: Difference-histogram of word order dis-
tortions for English?German.
systems (Ittycheriah and Roukos, 2007) with and
without source reordering and evaluated on a test
set of 357 sentences from the News domain.
We note that the DTM baseline includes features
(functions of target words and jump size) that al-
low it to model lexicalized reordering phenomena.
The reordering window size was set to +/- 8 words
for the baseline and system with reordered in-
puts. Table 1 shows the uncased BLEU scores for
English-Hindi, showing a gain from using the re-
ordering rules. For the reordered case, the HMM
alignments are rederived, but the accuracy of these
were no better than those of the unreordered in-
put and experiments showed that the gains in per-
formance were not due to the effect on the align-
ments.
Figure 3 shows difference-histograms for the
language pair English?Hindi, on a sample of
about 10k sentence pairs held out of the system
training data. The histogram indicates that our
reordering rules generalize and that the reordered
English is far more monotonic with respect to the
Hindi.
6 Analysis of errors and future
directions
In this section, we analyze some of the sources of
errors in reordering rules learned via our model, to
better understand directions for further improve-
ment.
1124
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1.5
?1
?0.5
0
0.5
1
1.5
x 104
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?HI)
Figure 3: Difference-histogram of word order dis-
tortions for English?Hindi.
6.1 Model weakness
In our initial experiments, we noticed that for the
most frequent reordering rules in English?Hindi
(e.g that IN NP or NP PP flips in Hindi) the prob-
ability of a reordering was roughly 65%. This
was concerning since it meant that on 35% of the
data we would be making wrong reordering deci-
sions by choosing the most likely reordering. To
get a better feel for whether we needed a stronger
model (e.g by lexicalization or by looking at larger
context in the tree rather than just the children),
we analyzed some of the cases in our training data
where (IN,NP), (NP, PP) pairs were left unaltered
in Hindi. In doing that analysis, we noticed exam-
ples involving negatives that our model does not
currently handle. The first issue was mentioned
in Section 4, where the assumption that we can
achieve the right word order by reordering con-
stituent phrases, is incorrect. The second issue
is illustrated by the following sentences: I have
some/no books, which have similar parse struc-
tures, the only difference being the determiner
some vs the determiner no. In Hindi, the order
of the fragments some books and the fragment
no books are different (in the first case the words
stay in order, in the second the flip). Handling
this example would need our model to be lexical-
ized. These issue of negatives requiring special
handling also came up in our analysis of German
(Section 6.4). Other than the negatives (which re-
quire a lexicalized model), the major reason for
the lack of sharpness of the reordering rule proba-
bility was alignment errors and parser issues. We
Aligner
Number of
Sentences fMeasure BLEU score
HMM 250k 62.4 21.7
MaxEnt 250k 76.6 21.4
Manual 5k - 21.3
Table 2: Using different alignments
look at these topics next.
6.2 Alignment accuracy
Since we rely on automatically generated align-
ments to learn the rules, low accuracy of
the alignments could impact the quality of
the rules learned. This is especially a con-
cern for English?Hindi since the quality of
HMM alignments are fairly low. To quan-
tify this effect, we learn reordering rules us-
ing three sets of alignments: HMM alignments,
alignments from a supervised MaxEnt aligner
(Ittycheriah and Roukos, 2005), and hand align-
ments. Table 2 summarizes our results using
aligners with differing alignment qualities for our
English?Hindi task and shows that quality of
alignments in learning the rules is not the driving
factor in affecting rule quality.
6.3 Parser accuracy
Accuracy of the parser in the source language is
a key requirement for our reordering method, be-
cause we choose the single best reordering based
on the most likely parse of the source sentence.
This would especially be an issue in translat-
ing from languages other than English, where the
parser would not be of quality comparable to the
English parser.
In examining some of the errors in reordering
we did observe a fair fraction attributable to
issues in parsing, as seen in the example sentence:
The rich of this country , corner almost 90% of
the wealth .
The second half of the sentence is parsed by the
Berkeley parser (Petrov et al, 2006) as:
FRAG
NP-SBJ
NN
corner
ADVP
RB
almost
NP-SBJ
NP
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
1125
and by IBM?s maximum entropy
parser parser (Ratnaparkhi, 1999) as:
VP
VB
corner
NP
NP
QP
RB
almost
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
With the first parse, we get the right Hindi order
for the second part of the sentence which is: the
wealth of almost 90% corner . To investigate the
effect of choice of parser we compared using the
Berkeley parser and the IBM parser for reorder-
ing, and we found the BLEU score essentially
unchanged: 21.6 for the Berkeley parser and
21.7 for the IBM parser. A potential source of
improvements might be to use alternative parses
(via different parsers or n-best parses) to generate
n-best reorderings both in training and at test.
6.4 Remarks on German reordering
Despite a common heritage, German word order is
distinct from English, particularly regarding verb
placement. This difference can be dramatic, if an
auxiliary (e.g. modal) verb is used in conjunction
with a full verb, or the sentence contains a subor-
dinate clause. In addition to our experiments with
automatically learned rules, a small set of hand-
crafted reordering rules was created and evalu-
ated. Our preliminary results indicate that the lat-
ter rules tend to outperform the automatically de-
rived ones by 0.5-1.0 BLEU points on average.
These rules are summarized as follows:
1. In a VP immediately following an NP, move
the negation particle to main verb.
2. Move a verb group away from a modal verb;
to the end the of a VP. Negation also moves
along with verb.
3. Move verb group to end of an embed-
ded/relative clause.
4. In a VP following a subject, move negation
to the end of VP (handling residual cases)
The above hand written rules show several weak-
nesses of our automatically learned rules for re-
ordering. Since our model is not lexicalized, nega-
tions are not handled properly as they are tagged
RB (along with other adverbs). Another limitation
apparent from the first rule above (the movement
of verbs in a verb phrase depends on the previous
phrase being a noun phrase) is that the automatic
reordering rule for a node?s children depends only
on the children of that node and not a larger con-
text. For instance, a full verb following a modal
verb is typically parsed as a VP child node of the
modal VP node, hence the automatic rule, as cur-
rently considered, will not take the modal verb
(being a sibling of the full-verb VP node) into ac-
count. We are currently investigating extensions
of the automatic rule extraction alorithm to ad-
dress these shortcomings.
6.5 Future directions
Based on our analysis of the errors and on the
hand designed German rules we would like to ex-
tend our model with more general feature func-
tions in Equation 1 by allowing features: that
are dependent on the constituent words (or head-
words), that examine a large context than just a
nodes children (see the first German rule above)
and that fire for all permutations when the con-
stituent X is moved to the end (or start). This
would allow us to generalize more easily to learn
rules of the type ?move X to the end of the
phrase?. Another direction that we feel should be
explored, is the use of multiple parses to obtain
multiple reorderings and combine these at a later
stage.
7 Conclusions
In this paper we presented a simple method to
automatically derive rules for reordering source
sentences to make it look more like target
language sentences. Experiments (on inter-
nal and public test sets) indicate performance
gains for English?French, English?Spanish,
and English?Hindi. For English?German we
did not see improvements with automatically
learned rules while a few hand designed rules did
give improvements, which motivated a few direc-
tions to explore.
1126
References
[Al-Onaizan and Papineni2006] Al-Onaizan, Yaser and
Kishore Papineni. 2006. Distortion models for sta-
tistical machine translation. In Proceedings of ACL.
[Badr et al2009] Badr, Ibrahim, Rabih Zbib, and
James Glass. 2009. Syntactic phrase reordering for
english-to-arabic statistical machine translation. In
Proceedings of EACL.
[Chiang2005] Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine transla-
tion. In Proceedings of ACL.
[Collins et al2005] Collins, Michael, Philipp Koehn,
and Ivona Kucerova. 2005. Clause restructuring
for statistical machine translation. In Proceedings
of ACL.
[Habash2007] Habash, Nizar. 2007. Syntactic prepro-
cessing for statistical machine translation. In MT
Summit.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In Proceedings of HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Proceedings of HLT-NAACL, pages 57?64.
[Koehn et al2003] Koehn, Philipp, Franz Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of HLT-NAACL.
[Niessen and Ney2001] Niessen, Sonja and Hermann
Ney. 2001. Morpho-syntactic analysis for reorder-
ing in statistical machine translation. In Proc. MT
Summit VIII.
[Petrov et al2006] Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning accu-
rate, compact, and interpretable tree annotation. In
COLING-ACL.
[Popovic et al2009] Popovic, Maja, David Vilar,
Daniel Stein, Evgeny Matusov, and Hermann Ney.
2009. The RWTH machine translation system for
WMT 2009. In Proceedings of WMT 2009.
[Ramanathan et al2008] Ramanathan, A., P. Bhat-
tacharyya, J. Hegde, R. M. Shah, and M. Sasikumar.
2008. Simple syntactic and morphological process-
ing can help english-hindi statistical machine trans-
lation. In Proceedings of International Joint Con-
ference on Natural Language Processing.
[Ratnaparkhi1999] Ratnaparkhi, Adwait. 1999. Learn-
ing to parse natural language with maximum en-
tropy models. Machine Learning, 34(1-3).
[Tillman2004] Tillman, Christoph. 2004. A unigram
orientation model for statistical machine translation.
In Proceedings of HLT-NAACL.
[Wang et al2007] Wang, Chao, Michael Collins, and
Philipp Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In Proceedings
of EMNLP-CoNLL.
[Xia and McCord2004] Xia, Fei and Michael McCord.
2004. Improving a statistical mt system with auto-
matically learned rewrite patterns. In Proceedings
of Coling.
[Xu et al2009] Xu, Peng, Jaeho Kang, Michael Ring-
gaard, and Franz Och. 2009. Using a dependency
parser to improve SMT for Subject-Object-Verb lan-
guages. In Proceedings of NAACL-HLT.
[Yamada and Knight2001] Yamada, Kenji and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL.
[Zhang et al2007] Zhang, Yuqi, Richard Zens, and
Hermann Ney. 2007. Chunk-level reordering
of source language sentences with automatically
learned rules for statistical machine translation. In
NAACL-HLT AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
[Zollmann and Venugopal2006] Zollmann, Andreas
and Ashish Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Pro-
ceedings on the Workshop on Statistical Machine
Translation.
[Zollmann et al2008] Zollmann, Andreas, Ashish
Venugopal, Franz Och, and Jay Ponte. 2008. A
systematic comparison of phrase-based, hierar-
chical and syntax-augmented statistical MT. In
Proceedings of COLING.
[Zwarts and Dras2007] Zwarts, Simon and Mark Dras.
2007. Syntax-based word reordering in phrase-
based statistical machine translation: why does it
work? In Proc. MT Summit.
1127
Coling 2010: Poster Volume, pages 1283?1291,
Beijing, August 2010
Urdu and Hindi: Translation and sharing of linguistic resources
Karthik Visweswariah, Vijil Chenthamarakshan, Nandakishore Kambhatla
IBM Research India
{v-karthik,vijil.e.c,kambhatla}@in.ibm.com
Abstract
Hindi and Urdu share a common phonol-
ogy, morphology and grammar but are
written in different scripts. In addition,
the vocabularies have also diverged signif-
icantly especially in the written form. In
this paper we show that we can get rea-
sonable quality translations (we estimated
the Translation Error rate at 18%) between
the two languages even in absence of a
parallel corpus. Linguistic resources such
as treebanks, part of speech tagged data
and parallel corpora with English are lim-
ited for both these languages. We use the
translation system to share linguistic re-
sources between the two languages. We
demonstrate improvements on three tasks
and show: statistical machine translation
from Urdu to English is improved (0.8
in BLEU score) by using a Hindi-English
parallel corpus, Hindi part of speech tag-
ging is improved (upto 6% absolute) by
using an Urdu part of speech corpus and
a Hindi-English word aligner is improved
by using a manually word aligned Urdu-
English corpus (upto 9% absolute in F-
Measure).
1 Introduction
Hindi and Urdu are official languages of India
and Urdu is also the national language of Pak-
istan. Hindi is spoken by around 853 million peo-
ple and Urdu by around 164 million people (Malik
et al, 2008). Although native speakers of Hindi
can comprehend most of spoken Urdu and vice
versa, these languages have diverged a bit since
independence of India and Pakistan ? with Hindi
deriving a lot of words from Sanskrit and Urdu
from Persian. One clear difference between Hindi
and Urdu is the script: Hindi is written in a left-
to-right Devanagari script while Urdu is written
in Nastaliq calligraphy style of the right-to-left
Perso-Arabic script. Hence, despite the similari-
ties, it is impossible for an Urdu speaker to read
Hindi text and vice versa. The first problem we
address is the translation between Hindi and Urdu
in the absence of a Hindi-Urdu parallel corpus.
Though these languages together are spoken by
around a billion people they are not very rich in
linguistic resources. A treebank for Hindi is still
under development1 and part of speech taggers for
Hindi and Urdu are trained on very small amounts
of data. For translation between Hindi/Urdu and
English there are no large corpora, the available
corpora are an order of magnitude smaller than
those available for European languages or Arabic-
English. Given the lack of linguistic resources
in each of the languages and the similarities be-
tween these languages, we explore whether each
language can benefit from resources available in
the other language.
1.1 Urdu-Hindi script conversion/translation
Sharing resources between Hindi and Urdu re-
quires us to be able to convert from one written
form to the other. Given that the languages share a
good fraction of their spoken vocabularies, the ob-
vious approach to convert between the two scripts
would be to transliterate between them. While this
approach has recently been attempted (Malik et
al., 2009), (Malik et al, 2008) there are two main
problems with this approach.
Challenges in Hindi-Urdu transliteration:
Urdu uses diacritical marks that were taken from
the Arabic script which serve various purposes.
Urdu has short and long vowels. Short vowels
are indicated by placing a diacritic with the con-
1https://verbs.colorado.edu/hindi
wiki/index.php/Hindi Treebank Data
1283
Figure 1: An Urdu sentence transliterated and
translated to Hindi
sonant that precedes it in the syllable. The diacrit-
ical marks are also used for gemination (doubling
of a consonant), which in Hindi is handled using a
conjunct form where the consonant is essentially
repeated twice. Yet another function of diacritical
marks is to mark the absence of a vowel follow-
ing a base consonant. Though diacritical marks
are critical for correct pronunciation and some-
times even for disambiguation of certain words,
they are sparingly used in written material in-
tended for native speakers of the language. Miss-
ing diacritical marks create substantial difficulties
for transliteration systems. Another difficulty is
created by the fact that Urdu words cannot have
a short vowel at the end of a word, whereas the
corresponding Hindi word can sometimes have a
short vowel. This cannot be resolved deterministi-
cally and results ambiguity in transliteration from
Urdu to Hindi. A third issue is the presence of
certain sounds (and their corresponding letters)
that have no equivalent in Urdu. These letters
are approximated in Urdu with phonetic equiva-
lents. Transliteration from Urdu to Hindi suffers
in the presence of words with these letters. Re-
cent work on Urdu-Hindi transliteration (Malik et
al., 2009) report transliteration word error rates
of 16.4% and 23.1% for Urdu sentences with and
without diacritical marks respectively. This prob-
lem is illustrated in Figure 1. The figure shows an
Urdu sentence that is transliterated to Hindi using
the Hindi Urdu Machine Transliteration (HUMT)
system 2 and translated using our Statistical Ma-
chine Translation System. The words which are
in red are transliteration errors (mainly because of
missing diacritical marks).
Difference in Word Frequency Distribu-
tions: Even if we could transliterate perfectly be-
tween Urdu and Hindi it might not be desirable to
2http://www.puran.info/HUMT/HUMT.aspx
do so from the point of view of human understand-
ing or for machine consumption. This is because
word frequencies of shared words would be dif-
ferent in Hindi and Urdu. At the extreme, there
are several Urdu words that a fluent Hindi speaker
would not understand and vice versa. More com-
monly, native speakers of Hindi and Urdu would
use different words to refer to the same concept,
even though both these words are technically cor-
rect in either of these languages. In initial experi-
ments to quantify this issue on our corpus, which
is mainly from the news domain, we estimated
that around 28% of the word tokens in Urdu would
not be natural in Hindi. This estimate assumes
perfect transliteration, and we estimated the total
error rate including transliteration at around 55%
for the publicly available HUMT system. In Fig-
ure 1, the words that have been underlined have
been replaced using a different word by our SMT
system, even though the original word might be
technically correct. Our preliminary experiments
exploring this issue convinced us that to be able
to convert from Urdu into natural Hindi (and vice
versa) we would need to go beyond transliteration
to translation to deal with the divergence of the
vocabularies in the written forms of the two lan-
guages.
Importance of Context We would like to point
out that in addition to word for word fidelity,
there are more subtle issues in translating from
Urdu-Hindi. One issue is that words in Hindi are
drawn from different source languages, and with
word to word translations, we might end up with
phrases that are unnatural. For example, consider
different ways of writing the English phrase Na-
tional and News in Hindi. The word National
in Hindi could possibly be written as rashtriya,
kaumi or national which have origins in Sanskrit,
Persian/Arabic and English respectively. Simi-
larly the word News could be written as samachar,
khabaren or news (once again with origins in San-
skrit, Persian/Arabic and English). The natural
ways for writing the phrase national news are:
rashtriya samachar, kaumi khabaren or national
news, any of the other six combinations would be
quite rare.
Another issue is that corresponding words in
Hindi and Urdu might have different genders. An
1284
example from (Sinha, 2009) are the words vajah
(Urdu, feminine) and karan (Hindi, masculine),
which would mean that the phrase because of him
would be written as us ke karan in Hindi and as us
ki vajah se in Urdu. We note that the ke in Hindi
and ki in Urdu are different because of the differ-
ence in genders of the word following them. This
suggests we would need to go beyond word for
word translation and would need to use a higher
order n-gram language model to translate with fi-
delity between Hindi and English.
We have established the need for going beyond
transliteration, but a key challenge is to achieve
good translation accuracy in the absence of a
Hindi-Urdu parallel corpus. In Section 3 we de-
scribe a multi-pronged approach to translate be-
tween Hindi and Urdu in the absence of a parallel
corpus that exploits the similarities between the
languages.
1.2 Applications: sharing linguistic resources
We next outline the three tasks for which we con-
sider sharing resources between Hindi and Urdu
which serve as a test of the efficacy of our sys-
tems.
Statistical machine translation
In recent years, there is a lot of interest in Statis-
tical Machine Translation (SMT) Systems (Brown
et al, 1993). Modern SMT systems (Koehn et al,
2003; Ittycheriah and Roukos, 2007) learn trans-
lation models based on large amounts of paral-
lel data. The quality of an SMT system is de-
pendent on the amount of parallel data on which
the system is trained. Unfortunately, for the pairs
Urdu-English and Hindi-English, parallel data are
not available in large quantities, thereby limiting
the quality of these SMT systems. In this pa-
per we show that we can improve the accuracy of
an Urdu?English SMT system by using a Hindi-
English parallel corpus.
Part of Speech tagging
Part of Speech (POS) tagging involves marking
the part of speech of a word based on its defini-
tion and surrounding context in a sentence. Se-
quential modeling techniques like HiddenMarkov
Models (Rabiner, 1990) and Conditional Random
Fields (Lafferty et al, 2001) are commonly used
to build Part of Speech taggers. These models are
typically trained using a manually tagged part of
speech corpus. Manual tagging of data requires
lot of human effort and hence large corpora are not
readily available for many languages. We improve
a Hindi POS tagger by using a manually tagged
Urdu POS corpus.
Supervised bitext alignment
Machine generated word alignments between
pairs of languages have many applications: build-
ing statistical machine translation systems, build-
ing dictionaries, projection of syntactic informa-
tion to resource poor languages (Yarowsky and
Ngai, 2001). Most of the early work on generat-
ing word alignments has been unsupervised, e.g.
IBM Models 1-5 (Brown et al, 1993), recent im-
provements on the IBM Models (Moore, 2004),
and the HMM algorithm described in (Vogel et al,
1996). Recently, significant improvements in per-
formance of aligners have been achieved by the
use of human annotated word alignments (Itty-
cheriah and Roukos, 2007; Lacoste-Julien et al,
2006). We describe a method to transfer man-
ual word alignments from Urdu-English to Hindi-
English to improve Hindi-English word align-
ments.
1.3 Contributions
Our main contributions are summarized below:
We present a hybrid technique to translate be-
tween Hindi and Urdu in the absence of a Hindi-
Urdu parallel corpus that significantly improves
upon past efforts to convert between Hindi and
Urdu via transliteration. We validate the efficacy
of the translation systems we present, by using it
to share linguistic resources between Hindi and
Urdu for three important tasks:
1. We improve a part of speech tagger for Hindi
using an Urdu part of speech corpus.
2. We use manual Urdu-English word align-
ments to improve the task of Hindi-English
bitext alignments.
3. We use a Hindi-English parallel corpus to
improve translation from Urdu to English.
1285
2 Related work
Converting between the scripts of Hindi and Urdu
is non-trivial and has been a recent focus (Ma-
lik et al, 2008; Malik et al, 2009). (Malik et
al., 2008) uses hand designed rules encoded us-
ing finite state transducers to transliterate between
Hindi and Urdu. As reported in (Malik et al,
2009) these hand designed rules achieve accu-
racies of only about 50% in the absence of di-
acritical marks. (Malik et al, 2009) improves
Urdu?Urdu transliteration performance to 79%
by post processing the output of the transducer
with a statistical language model. In contrast to
(Malik et al, 2009) we use a statistical model
for character transliteration. As discussed in Sec-
tion 1.1, due to the divergence of vocabularies
in written Hindi and Urdu, transliteration is not
sufficient to convert from written Urdu to written
Hindi. We also use a more flexible model that
allows for more natural translations by allowing
Urdu words to translate into Hindi words that do
not sound the same.
(Sinha, 2009) builds an English-Urdu machine
translation system using an English-Hindi ma-
chine translation system and a Hindi-Urdu word
mapping table, suitably adjusted for part of speech
and gender. Their system is not statistical, and
is largely based on manual creation of a large
database of Hindi-Urdu correspondences. Addi-
tionally, as mentioned in the conclusion, their sys-
tem cannot be used for direct translation from
Hindi to Urdu, since a grammatical analysis of
the English provides information necessary for the
Hindi to Urdu mapping. In contrast to this work,
our techniques are largely statistical, require min-
imal manual effort and can directly translate be-
tween Hindi and Urdu without the associated En-
glish.
3 Approach to translating between Hindi
and Urdu
As discussed in Section 1, transliteration between
Hindi and Urdu is not a straightforward task and
current efforts result in fairly high error rates. We
would like to combine the approaches of translit-
eration and translation since our goal is to use the
translation for sharing linguistic resources rather
than for direct consumption.
We use a fairly standard phrase based transla-
tion system to translate between Hindi and Urdu.
The key challenge that we overcome is being able
to develop such a system with acceptable accu-
racy in the absence of Hindi-Urdu resources (we
have neither a parallel corpus nor a dictionary with
sufficient coverage). In spite of the absence of re-
sources, translation between this language pair is
made feasible by the fact that word order is largely
maintained and translation can be done maintain-
ing a word to word correspondence. There are
some exceptions to the monotonicity in the two
languages. Consider the English phrase Govern-
ment of Sindh which in Urdu would be hukumat
e sindh in the same word order as in English,
while in Hindi it would be sindhi sarkar with the
word order flipped (with respect to English and
Urdu). This example also shows that sometimes
we do not have a word for word translation be-
tween Hindi and Urdu, the word sindhi in Hindi
corresponding to the Urdu words e sindh. In spite
of these exceptions, Hindi-Urdu translation can
largely be done with the monotonicity assumption
and with the assumption of word to word corre-
spondences. Thus the central issue in translating
between Hindi and Urdu is the creation of a word
to word conditional probability table. We explain
our technique assuming we are translating from
Urdu to Hindi. We take a hybrid approach to cre-
ating this table, using three different approaches.
The first approach is the pivot language ap-
proach (Wu and Wang, 2007), with English as a
pivot language. We get probabilities of a Urdu
word u being generated by a Hindi word h, con-
sidering intermediate English phrases e as:
Pp(u|h) =
?
e
P (u|e)P (e|h)
The translation probabilities P (u|e) and P (e|h)
are obtained using an Urdu-English and an
English-Hindi parallel corpus respectively.
This approach works reasonably well, but suf-
fers from a couple of drawbacks. There are sev-
eral common Hindi and Urdu words for which the
translation is unsatisfactory. This is because the
alignments for these words are not precise, they
often do not align to any English word, or align to
1286
an English words in combination with other Hindi
words. A common example of this is with verbs,
consider for example the English sentence
He works
which would translate into Hindi/Urdu as:
vah kaam karta hai
with word alignments He? vah, works? kaam
karta hai . Automatic aligners often make mis-
takes on these multi-word alignments, and this
create problems for words like karta and hai
which often do not have direct equivalents in En-
glish. To deal with this issue we manually build a
small phrase table for the most frequent Hindi and
Urdu words by a consulting an online Hindi-Urdu-
English dictionary (Platts, 1884). We also man-
ually handle the frequent examples we observed
of cases where we need to handle differences in
tokenization between Hindi and Urdu (e.g keliye
written as one word in Urdu and as ke liye in
Hindi).
The other issue with the pivot language ap-
proach is that for word pairs which are rare in
one of the languages,?e P (u|e)P (e|h) can eas-
ily work out to zero. This is exacerbated by align-
ment errors for rarer words. Thus, to strengthen
our phrase table especially for infrequent words,
we use a transliteration approach to build a phrase
table. Note that for rare words like names of peo-
ple and places, the words in Hindi and Urdu are
transliterations of each other.
In light of the issues in transliterating between
Hindi and Urdu (Malik et al, 2008; Malik et
al., 2009) we take a statistical approach (Abdul-
Jaleel and Larkey, 2003) to building a translitera-
tion based phrase table.
We assume a generative model for producing
Urdu words from Hindi words based on a charac-
ter transliteration probability table Pc. The prob-
ability Pt(u|h) of generating a Urdu word u from
a Hindi word h is given by:
Pt(u|h) =
?
a
?
i
Pc(ui|ha(i))P (ai|ai?1),
where a represents the alignment between the
Hindi and Urdu characters, a(i) is the the index
of the Hindi character that the ith Urdu charac-
ter is aligned to, Pc(uc|hc) is the probability of
an Urdu character uc being generated by a Hindi
character hc and P (ai|ai?1) represents a distor-
tion probability. Since transliteration is mono-
tonic and we want to encourage small jumps we
set: P (ai|ai?1) = c?(ai?ai?1) for ai > ai?1 and
0 otherwise. To obtain Pc we use the EM algo-
rithm and we can reuse standard machinery that
is used to obtain HMM word alignments in Statis-
tical Machine Translation (with the constraint of
Monotone alignments). To calculate a translitera-
tion based phrase table, for each Hindi word h we
search over a large vocabulary of Urdu words and
retain words u for which Pt(u|h) is sufficiently
high as possible transliterations of h. We set the
probabilities in the transliteration based phrase ta-
ble to be proportional to Pt(u|h). Finding this ta-
ble requires calculating Pt(u|h) for every pair of
words in the Urdu and Hindi vocabulary, we use
the Forward-Backward algorithm for efficiency
and parallelize the calculations over several ma-
chines.
The only remaining issue is how we get train-
ing data to train our transliteration model. To ob-
tain such training data we use a table of consonant
character conversions between Hindi and Urdu as
given in (Malik et al, 2008). We look for words in
our pivot language based translation table, where
there are at least three consonants and at least 50%
of the consonants are shared. We observed that
this yields pairs of words that are transliterations
of one another with high precision. These word
pairs are used as training data to build our charac-
ter transliteration model Pc.
Final word translation table is obtained by com-
bining our three approaches as follows: If the
word is present in our dictionary, we use the trans-
lation given in the dictionary and exclude all oth-
ers, if not we linearly interpolate between the
probability table we get based on using English
as a pivot language and probability table we get
based on transliteration.
4 Experimental results
In this section we report on experiments to eval-
uate the quality of our translation method de-
scribed in Section 3 and report on the application
of Hindi?Urdu translation to the sharing of lin-
guistic resources between the two languages.
1287
Algorithm 1 Create Urdu-Hindi Phrase Table
for all u such that u is very frequent Urdu word
do
h? Hindi word for u from dictionary
Pd(u|h)? 1
end for
U ? Urdu vocabulary
H ? Hindi vocabulary vocabulary
for all u ? U , h ? H do
Pp(u|h) ?
?
e P (u|e)P (e|h) {Create an
Urdu-Hindi translation table using English as
the pivot}
end for
for all u ? U , h ? H such that Pp(u|h) > ?
and ConsonantOverlap(u, h) > ? do
Add (u, h) to training set T
end for
Pc ?
argmax
Q
?
(u,h)?T
?
a
?
i
Q(ui|hai))P (ai|ai?1)
{Maximize using EM}
for all u ? U , h ? H do
Pt(u|h) ? c
?
a
?
i
Pc(ui|ha(i))P (ai|ai?1)
{Use Forward-Backward Algorithm}
end for
for all u ? U , h ? H do
if Pd(u|h)? 1 then
Pfinal(u|h)? 1
else
Pfinal(u|h)? ?pPp(u|h) + ?tPt(u|h)
end if
end for
4.1 Evaluation of Hindi-Urdu translation
We built a Hindi-Urdu transliteration system as
explained in Section 3. For building a pivot
language based translation table we used 70k
sentences from the NIST MT-08 corpus train-
ing corpus for Urdu-English. For Hindi-English
we used an internal corpus of 230k sentences.
We built our statistical transliteration model on
roughly 3k word pairs that we obtained as de-
scribed in Section 3. For Urdu?Hindi translation,
we used a five gram language model built from
a crawl of archives from Hindi news web sites
(the corpus size was about 60 million words). For
Hindi?Urdu translation we use the MT-08 Urdu
corpus (about 1.5 million words) to build a trigram
LM.
We evaluated the translation system in translat-
ing from Urdu to Hindi. We asked an annotator to
evaluate 100 sentences ( 2700 words), by marking
an error on a word if it was a wrong translation or
unnatural in Hindi. We compared our translation
system against the Hindi Urdu Machine Translit-
eration (HUMT) system3. We found an error rate
of 18% for our system as against 46% for the
HUMT system.
4.2 Word alignments
In this section we describe experiments at im-
proving a Hindi-English word aligner using hand
alignments for an Urdu-English corpus. For the
Urdu-English corpus we use a manually word
aligned corpus of roughly 10k sentences, while
for the Hindi-English corpus we had roughly 3k
sentences out of which we set aside 300 sentences
( 5300 words) for a test set. In addition to these
(relatively) small supervised corpora we also use
a sentence parallel Hindi-English corpus (without
manual word alignments) of roughly 250k sen-
tences.
For word alignments we use the Maximum
Entropy aligner described in (Ittycheriah and
Roukos, 2005) that is trained using hand aligned
training data. We first translate the Urdu sentences
in the Urdu-English word aligned corpus to Hindi,
and then transfer the alignments by simply replac-
ing the alignment links to a Urdu word by links
to the corresponding decoded Hindi word. The
above procedure covers bulk of the cases since
Urdu-Hindi translation is largely a word to word
translation. The special case of a phrase of multi-
ple Urdu words decoded to multiple Hindi words
is handled as follows: we align each of the words
in the Hindi phrase to the union of the sets of
English words that each word in the Urdu phrase
aligns to. Once we convert the Urdu-English man-
ual alignments to an additional corpus we build
two Hindi-English alignment models, one on the
original corpus, the other on the (Urdu?Hindi)-
English corpus. The MaxEnt aligner (Ittycheriah
and Roukos, 2005) models the probability of a
3http://www.puran.info/HUMT/HUMT.aspx
1288
nTrain Hindi data + Urdu
5 60.8 69.8
50 64.1 70.5
800 71.4 73.0
2800 75.1 75.7
Table 1: Word alignment F-Measure as a func-
tion of the number of manually aligned Hindi-
English sentences used for training. The third col-
umn shows improvements obtained by adding 10k
Urdu-English word alignments sentences.
particular set of links in the alignment L given the
source sentence S and the target sentence T as:
P (L|S, T ) = ?Mi=1 p(li|tM1 , sK1 , li?11 ). Let us de-
note by Ph and Pu the alignment models trained
on the Hindi-English and the (Urdu?Hindi)-
English corpora respectively. We combine these
models log-linearly to obtain our final model for
alignment:
P (L|S, T ) = P?h (L|S, T )P 1??u (L|S, T ).
To find the most likely alignment we use the same
algorithm as in (Ittycheriah and Roukos, 2005)
since the structure of the model is unchanged.
We report on the performance (Table 1) of a
baseline Hindi-English word aligner built with
varying amounts of Hindi-English manually word
aligned training data compared against an aligner
that combines in a model trained on the 10k
(Urdu?Hindi)-English sentences. We observe
large gains with small amounts of labelled Hindi-
English alignment data, and even when we have
2800 sentences of Hindi-English data we see a
gain in performance adding in the Urdu data.
We note that the MaxEnt aligner we use (Itty-
cheriah and Roukos, 2005) defaults to (roughly)
doing an HMM alignment using a word trans-
lation matrix obtained via unsupervised training.
Thus the aligners reported on in Table 1 use a
large amount of unsupervised data in addition to
the small amounts of labelled data mentioned in
the Table.
4.3 POS tagging
Unlike English for which there is an abundance
of POS training data for Hindi and Urdu data is
quite limited. For our experiments, we use the
num. words f(wi, ti), g(ti?1, ti) + h(tui , ti)
5k 76.5 82.5
10k 81.7 84.7
20k 84.5 86.7
47k 90.6 91.0
Table 2: POS tagging accuracy as a function of
the amount of Hindi POS tagged data used to
build the model. The third column indicates the
use of the Urdu data via a feature type.
CRULP corpus (Hussain, 2008) for Urdu and a
corpus from IITB (Dalal et al, 2007) for Hindi.
The CRULP POS corpus has 150k words and
uses a tagset of size 46 to tag the corpus. The
IITB corpus has 50k words and uses a tagset of
size 26. We set a side a test set of size 5k words
from the IITB corpus. For part of speech tagging
we use CRFs (Lafferty et al, 2001) with two types
of features, f(ti, wi) and g(ti, ti?1). With the
small amounts of training data we have, adding
additional feature templates degraded the perfor-
mance.
In our POS tagging experiments we consider
using the Urdu corpus to help POS tagging in
Hindi. We first translate all of the CRULP Urdu
data to Hindi. We cannot simply add in this data
to the training data because of differences in the
tagsets used in the data sets for the two languages.
In order to make use of the additional Urdu POS
tagged data (translated to Hindi), we build a sep-
arate POS tagger on this data, and use predictions
from this model as a feature in training the Hindi
POS tagger. We use these predictions via a fea-
ture template h(ti, tui ) where tui denotes the tag
assigned to the ith word by the POS tagger built
from the CRULP Urdu data set translated into
Hindi.
We present results in Table 2 with varying
amounts of Hindi data used for training, in each
case we present results with and without use of
the Urdu resources. We see a small gain even
when we use all of the available Hindi training
data and as expected we see larger gains when
smaller amounts of Hindi data are used.
We analyzed the type of errors and the er-
ror reduction when using the Urdu data for the
case where we used only 5k words of Hindi data.
1289
We find that the two frequent error types that
were greatly reduced were noun being tagged
as main verb (reduction of 65% relative) and
main verb tagged as auxiliary verb (reduction of
71%). Reduction in confusion between nouns and
main verbs is expected since these are open word
classes that can most benefit from additional data.
This also causes the reduction in errors of tag-
ging main verbs as auxiliary verbs, since in Hindi,
verbs are multi word groups with a main verb fol-
lowed by one or more auxiliary verbs. Reduction
of error rate in most of the other error types were
close to the overall error rate reduction.
4.4 Sharing parallel corpora for machine
translation
We experimented with using our internal Hindi-
English parallel corpus ( 230k) sentences to obtain
better translation for Urdu-English. The Urdu-
English corpus we use is the NIST MT-08 training
data set ( 70k sentences). We use the Direct Trans-
lation Model 2 (DTM) described in (Ittycheriah
and Roukos, 2007) for all our translation experi-
ments.
We build our baseline Urdu?English system
using the NIST MT-08 training data. In training
our DTM model we use HMM alignments, align-
ments with the MaxEnt aligner, and hand align-
ments for 10k sentences (the hand alignments
were used to train the MaxEnt aligner).
We translated the Hindi in our Hindi-English
corpus to Urdu, creating an additional Urdu-
English corpus. We then use a MaxEnt aligner
to align the Urdu-English words in this corpus.
Since we expect this corpus to be relatively noisy
due to incorrect translation from Urdu to Hindi we
do not include this corpus while generating HMM
alignments. We add the synthetic Urdu-English
data with MaxEnt alignments to our baseline data
and train a DTM model. Results comparing to the
baseline are given Table 3, which shows an im-
provement of 0.8 in BLEU score over the baseline
system by using data from the Hindi-English cor-
pus.
This improvement is not due to unknown
words being covered (the vocabulary covered is
the same). Also note that in the bridge language
approach we cannot get alernative translations
Corpus MT08 Eval
Urdu 23.1
+Hindi 23.9
Table 3: Improvement in Urdu-English machine
translation using Hindi-English data .
for single words that were not already present in
the Urdu-English phrase table. Thus, we believe
that the improvement is due to longer phrases
being seen more often in training. An example
improved translation is shown below:
Ref: just as long as its there they feel safe
Baseline: as long as this they just think there are safe
Improved: just as long as they are there they feel safe
5 Conclusions
In this paper, we showed that we can translate be-
tween Hindi and English without a parallel corpus
and improve upon previous efforts at transliterat-
ing between the two languages. We also showed
that Hindi-Urdu translation can be useful to the
sharing of linguistic resources between the two
languages. We believe this approach to sharing
linguistic resources will be of immense value es-
pecially with resources like treebanks which re-
quire a large effort to develop.
Acknowledgments
We thank Salim Roukos and Abe Ittycheriah for
discussions that helped guide our efforts.
References
[AbdulJaleel and Larkey2003] AbdulJaleel, Nasreen
and Leah S. Larkey. 2003. Statistical transliteration
for english-arabic cross language information
retrieval. In CIKM.
[Brown et al1993] Brown, Peter F., Vincent J.Della
Pietra, Stephen A. Della Pietra, and Robert. L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19:263?311.
[Dalal et al2007] Dalal, Aniket, Kumara Nagaraj, Uma
Sawant, Sandeep Shelke, and Pushpak Bhat-
tacharyya. 2007. Building feature rich pos tagger
for morphologically rich languages. In Proceed-
ings of the Fifth International Conference on Nat-
ural Language Processing, Hyderabad, India, Jan-
uary.
1290
[Hussain2008] Hussain, Sarmad. 2008. Resources for
urdu language processing. In Proceedings of the 6th
workshop on Asian Language Resources.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Sidner, Candace L., Tanja Schultz, Matthew
Stone, and ChengXiang Zhai, editors, HLT-NAACL,
pages 57?64. The Association for Computational
Linguistics.
[Koehn et al2003] Koehn, Philipp, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Lacoste-Julien et al2006] Lacoste-Julien, Simon,
Benjamin Taskar, Dan Klein, and Michael I. Jordan.
2006. Word alignment via quadratic assignment. In
HLT-NAACL.
[Lafferty et al2001] Lafferty, J., A. McCallum, , and
F. Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning.
[Malik et al2008] Malik, M. G. Abbas, Christian
Boitet, and Pushpak Bhattacharyya. 2008. Hindi
urdu machine transliteration using finite-state trans-
ducers. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 537?544, Manchester, UK, August.
Coling 2008 Organizing Committee.
[Malik et al2009] Malik, Abbas, Laurent Besacier,
Christian Boitet, and Pushpak Bhattacharyya. 2009.
A hybrid model for urdu hindi transliteration. In
Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration (NEWS 2009), pages
177?185, Suntec, Singapore, August. Association
for Computational Linguistics.
[Moore2004] Moore, Robert C. 2004. Improving
ibm word alignment model 1. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
518?525, Barcelona, Spain, July.
[Platts1884] Platts, John T. 1884. A dictionary of
Urdu, classical Hindi and English. W. H. Allen and
Co.
[Rabiner1990] Rabiner, Lawrence R. 1990. A tutorial
on hidden markov models and selected applications
in speech recognition. pages 267?296.
[Sinha2009] Sinha, R. Mahesh K. 2009. Developing
english-urdu machine translation via hindi. In Third
Workshop on Computational Approaches to Arabic-
Script-based Languages.
[Vogel et al1996] Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of
the 16th conference on Computational linguistics,
pages 836?841, Morristown, NJ, USA. Association
for Computational Linguistics.
[Wu and Wang2007] Wu, Hua and Haifeng Wang.
2007. Pivot language approach for phrase-based
statistical machine translation. In ACL.
[Yarowsky and Ngai2001] Yarowsky, David and Grace
Ngai. 2001. Inducing multilingual pos taggers and
np bracketers via robust projection across aligned
corpora. In NAACL.
1291
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486?496,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Word Reordering Model for Improved Machine Translation
Karthik Visweswariah
IBM Research India
Bangalore, India
v-karthik@in.ibm.com
Rajakrishnan Rajkumar
Dept. of Linguistics
Ohio State University
raja@ling.osu.edu
Ankur Gandhe
IBM Research India
Bangalore, India
ankugand@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
Bangalore, India
aramana2@in.ibm.com
Jiri Navratil
IBM T.J. Watson Research Center
Yorktown Heights, New York
jiri@us.ibm.com
Abstract
Preordering of source side sentences has
proved to be useful in improving statistical
machine translation. Most work has used a
parser in the source language along with rules
to map the source language word order into
the target language word order. The require-
ment to have a source language parser is a ma-
jor drawback, which we seek to overcome in
this paper. Instead of using a parser and then
using rules to order the source side sentence
we learn a model that can directly reorder
source side sentences to match target word or-
der using a small parallel corpus with high-
quality word alignments. Our model learns
pairwise costs of a word immediately preced-
ing another word. We use the Lin-Kernighan
heuristic to find the best source reordering ef-
ficiently during training and testing and show
that it suffices to provide good quality reorder-
ing.
We show gains in translation performance
based on our reordering model for translating
from Hindi to English, Urdu to English (with
a public dataset), and English to Hindi. For
English to Hindi we show that our technique
achieves better performance than a method
that uses rules applied to the source side En-
glish parse.
1 Introduction
Languages differ in the way they order words to pro-
duce sentences representing the same meaning. Ma-
chine translation systems need to reorder words in
the source sentence to produce fluent output in the
target language that preserves the meaning of the
source sentence.
Current phrase based machine translation systems
can capture short range reorderings via the phrase
table. Even the capturing of these local reordering
phenomena is constrained by the amount of training
data available. For example, if adjectives precede
nouns in the source language and follow nouns in the
target language we still need to see a particular ad-
jective noun pair in the parallel corpus to handle the
reordering via the phrase table. Phrase based sys-
tems also rely on the target side language model to
produce the right target side order. This is known
to be inadequate (Al-Onaizan and Papineni, 2006),
and this inadequacy has spurred various attempts to
overcome the problem of handling differing word
order in languages.
One approach is through distortion models, that
try to model which reorderings are more likely
than others. The simplest models just penalize
long jumps in the source sentence when producing
the target sentence. These models have also been
generalized (Al-Onaizan and Papineni, 2006; Till-
man, 2004) to allow for lexical dependencies on the
source. While these models are simple, and can
be integrated with the decoder they are insufficient
to capture long-range reordering phenomena espe-
cially for language pairs that differ significantly.
The weakness of these simple distortion models
has been overcome using syntax of either the source
or target sentence (Yamada and Knight, 2002; Gal-
ley et al, 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006). While these methods have shown to
be useful in improving machine translation perfor-
486
mance they generally involve joint parsing of the
source and target language which is significantly
more computationally expensive when compared to
phrase based translation systems. Another approach
that overcomes this weakness, is to to reorder the
source sentence based on rules applied on the source
parse (either hand written or learned from data) both
when training and testing (Collins et al, 2005; Gen-
zel, 2010; Visweswariah et al, 2010).
In this paper we propose a novel method for deal-
ing with the word order problem that is efficient and
does not rely on a source or target side parse being
available. We cast the word ordering problem as a
Traveling Salesman Problem (TSP) based on previ-
ous work on word-based and phrased-based statis-
tical machine translation (Tillmann and Ney, 2003;
Zaslavskiy et al, 2009). Words are the cities in the
TSP and the objective is to learn the distance be-
tween words so that the shortest tour corresponds to
the ordering of the words in the source sentence in
the target language. We show that the TSP distances
for reordering can be learned from a small amount
of high-quality word alignment data by means of
pairwise word comparisons and an informative fea-
ture set involving words and part-of-speech (POS)
tags adapted and extended from prior work on de-
pendency parsing (McDonald et al, 2005b). Ob-
taining high-quality word alignments that we require
for training is fairly easy compared with obtaining a
treebank required to obtain parses for use in syntax
based methods.
We show experimentally that our reordering
model, even when used to reorder sentences for
training and testing (rather than being used as an
additional score in the decoder) improves machine
translation performance for: Hindi ? English, En-
glish?Hindi, and Urdu? English. Although Urdu
is similar to Hindi from the point of reordering phe-
nomena we include it in our experiments since there
are publicly available datasets for Urdu-English. For
English ? Hindi we obtained better machine trans-
lation performance with our reordering model as
compared to a method that uses reordering rules ap-
plied to the source side parse.
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work and places our work in
context. Section 3 outlines reordering issues due
to syntactic differences between Hindi and English.
Section 4 presents our reordering model, Section 5
presents experimental results and Section 6 presents
our conclusions and possible future work.
2 Related work
There have been several studies demonstrating im-
proved machine translation performance by reorder-
ing source side sentences based on rules applied to
the source side parse during training and decoding.
Much of this work has used hand written rules and
several language pairs have been studied e.g German
to English (Collins et al, 2005), Chinese to English
(Wang et al, 2007), English to Hindi (Ramanathan
et al, 2009), English to Arabic (Badr et al, 2009)
and Japanese to English (Lee et al, 2010). There
have also been some studies where the rules are
learned from the data (Genzel, 2010; Visweswariah
et al, 2010; Xia and McCord, 2004). In addition
there has been work (Yamada and Knight, 2002;
Zollmann and Venugopal, 2006; Galley et al, 2006;
Liu et al, 2006) which uses source and/or target
side syntax in a Context Free Grammar framework
which results in machine translation decoding being
considered as a parsing problem. In this paper we
propose a model that does not require either source
or target side syntax while also preserving the effi-
ciency of reordering techniques based on rules ap-
plied to the source side parse.
In work that is closely related to ours, (Tromble
and Eisner, 2009) formulated word reordering as a
Linear Ordering Problem (LOP), an NP-hard permu-
tation problem. They learned LOP model weights
capable of assigning a score to every possible per-
mutation of the source language sentence from an
aligned corpus by using a averaged perceptron learn-
ing model. The key difference between our model
and the model in (Tromble and Eisner, 2009) is that
while they learn costs of a word wi appearing any-
where before wj , we learn costs of wi immediately
preceding wj . This results in more compact models
and (as we show in Section 5) better models.
Our model results in us having to solve a TSP
instance. The relation between the TSP and ma-
chine translation decoding has been explored before.
(Knight, 1999) showed that TSP is a sub-class ofMT
decoding and thus established that the latter is NP-
hard. (Zaslavskiy et al, 2009) casts phrase-based
487
decoding as a TSP and they show favorable speed
performance trade-offs compared with Moses, an
existing state-of-the-art decoder. In (Tillmann and
Ney, 2003), a beam-search algorithm used for TSP
is adapted to work with an IBM-4 word-based model
and phrase-based model respectively. As opposed
to calculating TSP distances from existing machine
translation components ( viz. the translation, dis-
tortion and language model probabilities) we learn
model weights to reorder source sentences to match
target word order using an informative feature set
adapted from graph-based dependency parsing (Mc-
Donald et al, 2005a).
3 Hindi-English reordering issues
This section provides a brief survey of constructions
that the two languages in question differ as well as
have in common. (Ramanathan et al, 2009) notes
the following divergences:
? English follows SVO order while Hindi follows
SOV order
? English uses prepositions while Hindi uses
post-positions
? Hindi allows greater word order freedom
? Hindi has a relatively richer case-marking sys-
tem
In addition to these differences, (Visweswariah et
al., 2010) mention the similarity in word order in
the case of adjective noun sequences (some books
vs. kuch kitab).
4 Reordering model
Consider a source sentence w consisting of a se-
quence of n words w1, w2, ... wn that we would
like to reorder into the target language order. Given
a permutation pi of the indices 1..n, let the candi-
date reordering be wpi1 , wpi2 , ..., wpin . Thus, pii de-
notes the index of the word in the source sentence
that maps to position i in the candidate reordering.
Clearly there are n! such permutations. Our reorder-
ing model assigns costs to candidate permutations
as:
C(pi|w) =
?
i
c(pii?1, pii).
The cost c(m,n) can be thought of as the cost of the
word at index m immediately preceding the word
with index n in the candidate reordering. In this pa-
per, we parametrize the costs as:
c(m,n) = ?T?(w,m, n),
where ? is a learned vector of weights and ? is a
vector of feature functions.
Given a source sentence w we reorder it accord-
ing to the permutation pi that minimizes the cost
C(pi|w). Thus, we would like our cost function
C(pi|w) to be such that the correct reordering pi? has
the lowest cost of all possible reorderings pi. In Sec-
tion 4.1 we describe the features ? that we use, and
in Section 4.2 we describe how we train the weights
? to obtain a good reordering model.
Given our model structure, the minimization
problem that we need to solve is identical to solving
a Asymmetric Traveling Salesman Problem (ATSP)
with each word corresponding to a city, and the costs
c(m,n) representing the pairwise distances between
the cities. Consider the following example:
English input: John eats apples
Hindi: John seba(apples) khaataa hai(eats)
Desired reordered English: John apples eats
The ATSP that we need to solve is represented
pictorially in Figure 1 with sample costs. Note that
we have one extra node numbered 0. We start and
end the tour at node 0, and this determines the first
word in the reordered sentence. In this example the
minimum cost tour is:
Start ? John ? apple ? eats
recovering the right reordering for translation into
Hindi.
Solving the ATSP (which is a well known NP hard
problem) efficiently is crucial for the efficiency of
our reordering model. To solve the ATSP, we first
convert the ATSP to a symmetric TSP and then use
the Lin-Kernighan heuristic as implemented in Con-
corde, a state-of-the-art TSP solver (Applegate et al,
2005). We also experimented with using the exact
TSP solver in Concorde but since it was slower and
did not improve performance we preferred using the
Lin-Kernighan heuristic. To convert the ATSP to
a symmetric TSP we double the size of the orig-
inal problem creating a node N ? for every node
N in the original graph. Following (Hornik and
488
3apples
2eats1John
0Start
c(2,3)=3c(3,2)=1
c(0,3)=5c(3,0)=2
c(0,1)=-1c(1,0)= 5 c(1,3)=0
c(0,2)=5
c(1,2)=3c(2,1)=5
c(3,1)=5
c(2,0)=-2
Figure 1: Example of an ATSP for reordering the sen-
tence: John eats apples.
Hahsler, 2009), we then set new costs as follows:
c?(A,B) = ?, c?(A,B?) = c?(B? , A) = c(A,B)
and C(A,A?) = ??. Even with this doubling of
the number of nodes, we observed that solving the
TSPs with the Lin-Kernighan heuristic is very fast,
taking roughly 10 milliseconds per sentence on av-
erage. Overall, this means that our reordering model
is as fast as parsing and hence our model is compara-
ble in performance to techniques based on applying
rules to the parse tree.
4.1 Features
Since we would like to model reordering phenomena
which are largely related to analyzing the syntax of
the source sentence, we chose to use features based
on those that have in the past been used for parsing
(McDonald et al, 2005a). A subset of the features
we use was also used for reordering in (Tromble and
Eisner, 2009).
To be able to generalize from relatively small
amounts of data, we use features that in addition to
depending on the words in the input sentence w de-
pend on the part-of-speech (POS) tags of the words
in the input sentence. All features ?(w, i, j) we use
are binary features, that fire based on the identities
of the words and POS tags at or surrounding posi-
tions i and j in the source sentence. The first set of
feature templates we use are given in Table 1. These
features depend only on the identities of the word
and POS tag of the two positions i and j and we call
wi pi wj pj
? ? ? ?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ? ?
Table 1: Bigram feature templates used to calculate the
cost that word at position i immediately precedes word at
position j in the target word order. wi (pi) denotes the
word (POS tag) at position i in the source sentence. Each
of the templates is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
these Bigram features.
The second set of feature templates we use are
given in Table 2. These features, in addition to ex-
amining positions i and j examine the surround-
ing positions. We instantiate these feature templates
separately for the POS tag sequence and for the
word sequence. We call these two feature sets Con-
textPOS and ContextWord respectively. When in-
stantiated with POS tags, the first row of Table 2
looks at all POS tags between positions i and j.
(Tromble and Eisner, 2009) use Bigram and Con-
textPOS features, while we extend their feature set
with the use of ContextWord features. Since Hindi
is verb final, in Hindi sentences with multiple verb
groups it is rare for words with a verb in between
to be placed together in the reordering to match En-
glish. Looking at the POS tags of words between
positions i and j allows us to penalize such reorder-
ings.
Each of the templates described in Table 1 and
Table 2 is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
The values of i-j between 5 and 10, and greater than
10 are quantized (negative values are similarly quan-
tized).
In Section 5.2 we report on experiments showing
the relative performance of these different feature
489
oi?1 oi oi+1 ob oj?1 oj oj+1
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
Table 2: Context feature templates used to calculate the
cost that word at position i immediately precedes word
at position j in the target word order. oi denotes the ob-
servation at position i in the source sentence and ob de-
notes an observation at a position between i and j (i.e
i + 1 ? b ? j ? 1). Each of the templates is instan-
tiated with the observation sequence o taken to be the
word sequence w and the POS tag sequence p. Each of
the templates is also conjoined with i-j the signed dis-
tance between the two positions in the source sentence.
types for the task of reordering Hindi sentences to
be in English word order.
4.2 Training
To train the weights ? in our model, we need a
collection of sentences, where we have the desired
reference reordering pi?(x) for each input sentence
x. To obtain these reference reorderings we use
word aligned source-target sentence pairs. The qual-
ity and consistency of these reference reorderings
will depend on the quality of the word alignments
that we use. Given word aligned source and tar-
get sentences, we drop the source words that are not
aligned. Let mi be the mean of the target word po-
sitions that the source word at index i is aligned to.
We then sort the source indices in increasing order
of mi. If mi = mj (for example, because wi and wj
are aligned to the same set of words) we keep them
in the same order that they occurred in the source
sentence. Obtaining the target ordering in this man-
ner, is certainly not the only possible way and we
would like to explore better treatment of this in fu-
ture work.
We used the single best Margin Infused Re-
laxed Algorithm (MIRA) ((McDonald et al, 2005b),
(Crammer and Singer, 2003)) with the online up-
dates to our parameters being given by
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w)? L(pi?, p?i).
In the equation above,
p?i = argmin
pi
C(pi|x)
is the best reordering based on the current parameter
value and L is a loss function. We take the loss of
a reordering to be the number of words for which
the preceding word is wrong relative to the reference
target order.
We also experimented with the averaged percep-
tron algorithm (Collins, 2002), but found single best
MIRA to work slightly better and hence used MIRA
for all our experiments.
5 Experiments
In this section we report on experiments to evalu-
ate our reordering model. The first method we use
for evaluation (monolingual BLEU) is by generat-
ing the desired reordering of the source sentence (as
described in Section 4.2) and compare the reordered
output to this desired reordered sentence using the
BLEU metric. In addition, to these monolingual
BLEU results, we also evaluate (in Section 5.5) the
reordering by its effect on eventual machine transla-
tion performance.
We note that our reordering techniques uses POS
information for the input sentence. The POS taggers
used in this paper are Maximum Entropy Markov
models trained using manually annotated POS cor-
pora. For Hindi, we used roughly fifty thousand
words with twenty six tags from the corpus de-
scribed in (Dalal et al, 2007). For Urdu we used
roughly fifty thousand words and forty six tags from
the CRULP corpus (Hussain, 2008) and for English
we used the Wall Street Journal section of the Penn
Treebank.
490
5.1 Reordering model training data and
alignment quality
To train our reordering models we need training data
where we have the input source language sentence
and the desired reordering in the target language.
As described in Section 4.2 we derive the refer-
ence reordered sentence using word alignments. Ta-
ble 3 presents our monolingual BLEU results for
Hindi to English reordering as the source of the
word alignments is varied. All results in Table 3
are with Bigram and ContextPOS features. We have
word alignments from three sources: A small set
of hand aligned sentences, HMM alignments (Vo-
gel et al, 1996) and alignments obtained using a su-
pervised Maximum Entropy aligner (Ittycheriah and
Roukos, 2005) trained on the hand alignments. The
F-measure for the HMM alignments were 65% and
78% for the Maximum Entropy model alignments.
We see that the quality of the alignments is an im-
portant determiner of reordering performance. Row
1 shows the BLEU for unreordered (baseline) Hindi
compared with the Hindi sentences reordered in En-
glish Order. Using just HMM alignments to train
our model we do worse than unreordered Hindi. Al-
though using the Maximum Entropy alignments is
better than using HMM alignments, we do not im-
prove upon a small number of hand alignments by
using all the Maximum Entropy alignments.
To improve upon the model trained with only
hand alignments we selected a small number of snip-
pets of sentences from our Maximum Entropy align-
ments. The goal was to pick parts of sentences
where the alignment is reliable enough to use for
training. The heuristic we used in the selection of
snippets was to pick maximal snippets of at least
7 consecutive Hindi words with all Hindi words
aligned to a consecutive span of English words,
with no unaligned English words in the span and no
English words aligned to Hindi words outside the
span. Adding snippets selected with this heuristic
improves the reordering performance of our model
as seen in the last row of Table 3.
5.2 Feature set comparison
In this section we report on experiments to deter-
mine the performance of the different classes of fea-
tures (Bigram, ContextPos and ContextWord) dis-
HMM MaxEnt Hand BLEU
- - - 35.9
220K - - 35.4
- 220K - 47.0
- 220K 6K 48.4
- - 6K 49.0
- Good 17K 6K 51.3
Table 3: Monolingual BLEU scores for Hindi to English
reordering using models trained on different alignment
types and tested on a development set of 280 Hindi sen-
tences (5590 tokens).
Feature template
Bigram ContextPOS ContextWord BLEU
- - - 35.9
? - - 43.8
? ? - 49.0
? ? ? 51.3
Table 4: Monolingual BLEU scores for Hindi to En-
glish reordering using models trained with different fea-
ture sets and tested on a development set of 280 Hindi
sentences (5590 tokens).
cussed in Section 4.1. Table 4 shows monolingual
BLEU results for training with different features sets
for Hindi to English reordering. In all cases, we
use a set of 6000 sentence pairs which were hand
aligned to generate the training data. It is clear that
all three sets of features contribute to performance of
the reordering model, however the number of Con-
textWord features is larger than the number of Bi-
gram and ContextPOS features put together, and it
may be desirable to select from this set of features
especially when training on large amounts of data.
5.3 Monolingual reordering comparisons
Table 5 compares our reordering model with a reim-
plementation of the reordering model proposed in
(Tromble and Eisner, 2009). Both the models use
exactly the same features (bigram features and Con-
textPOS features) and are trained on the same data.
To generate our training data, for Hindi to English
and English to Hindi we use a set of 6000 hand
aligned sentences, for Urdu to English we use a set
of 8500 hand aligned sentences and for English to
French we use a set of 10000 hand aligned sentences
(a subset of Europarl and Hansards corpus). Our
491
Language pair Monolingual BLEU
Source Target Unreordered LOP TSP
Hindi English 35.9 36.6 49.0
English Hindi 34.4 48.4 56.7
Urdu English 35.6 39.5 49.9
English French 64.4 78.2 81.2
Table 5: Monolingual BLEU scores comparing the orig-
inal source order with desired target reorder without re-
ordering, and reordering using our model (TSP) and the
model proposed in (Tromble and Eisner, 2009) (LOP).
test data consisted of 280 sentences for Hindi to En-
glish and 400 sentences for all other language pairs
generated from hand aligned sentences. We include
English-French here to compare on a fairly similar
language pair with local reordering phenomena (the
main difference being that in French adjectives gen-
erally follow nouns). We note that our model outper-
forms the model proposed in (Tromble and Eisner,
2009) in all cases.
5.4 Analysis of reordering performance
To get a feel for the qualitative performance of our
reordering algorithm and the kind of phenomena it
is able to capture, we analyze the reordering per-
formance in terms of (i) whether the clause restruc-
turing is done correctly ? these can be thought of
as medium-to-long range reorderings, (ii) whether
clause boundaries are respected, and (iii) whether lo-
cal (short range) reordering is performed correctly.
The following analysis is for Hindi to English re-
ordering with the best model (this is also the model
used for Machine Translation experiments reported
on in Section 5.5).
? Clause structure: As discussed in Section 3,
the canonical clause order in Hindi is SOV,
while in English it is SVO. However, variations
on this structure are possible and quite frequent
(e.g., clauses with two objects). To evaluate
clause restructuring, we compared sequences
of subjects, objects and verbs in the output and
reference reorderings.
We had a set of 70 sentences annotated with
subject, direct object, indirect object and verb
information ? these annotations were made on
the head word of each phrase, and the compar-
isons were on sequences of these words alone
and not the entire constituent phrase. 52 sen-
tences were reordered by the model to match
the order of the corresponding reference. Eight
sentences were ordered correctly but differently
from the reference, because the reference was
expressed in non-canonical fashion (e.g., in the
passive) ? note that these cases negatively im-
pact the monolingual BLEU score. The follow-
ing example shows a sentence being reordered
correctly, where, however, the reference is ex-
pressed differently (note the position of the
subject ?policy? (niiti) in the reference and the
reordered output) 1:
Input: aba1 (now) taka2 (till) aisii3 (this) niiti4
(policy) kabhii5 (ever) nahii6 (not) rahii7 (has)
hai8 (been)
Reordered: taka2 (till) aba1 (now) aisii3 (this)
niiti4 (policy) hai8 (been) kabhii5 (ever) nahii6
(not) rahii7 (has)
Reference: taka2 (till) aba1 (now) aisii3 (this)
kabhii5 (ever) nahii6 (not) rahii7 (has) hai8
(been) niiti4 (policy)
English: Till now this never has been the policy
The remaining ten sentences were reordered in-
correctly. These errors are largely in clauses
which deviate from the SVO order in some
way ? clauses with multiple subjects or objects,
clauses with no object, etc.. For example, the
following sentence with two subjects and ob-
jects corresponding to the verb wearing has not
been reordered correctly.
Input: sabhii1 (all) purusha2 (men) safeda3
(white) evama4 (and) mahilaaen5 (women)
kesariyaa6 (saffron) vastra7 (clothes) dhaarana8
(wear) kiye9 hue10 (-ing) thiin11 (were)
Reordered: sabhii1 (all) purusha2 (men)
safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron) vastra7 (clothes)
dhaarana8 (wear) thiin11 (were) kiye9 hue10 (-
ing)
Reference: sabhii1 (all) purusha2 (men)
thiin11 (were) dhaarana8 (wear) kiye9 hue10 (-
1The numeric subscripts in the examples indicate word po-
sitions in the input.
492
ing) safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron)
English: All men were wearing white and the
women saffron
The model possibly needs more data with pat-
terns that deviate from the standard SOV order
to learn to reorder them correctly. We could
also add to the model, features pertaining to
subject, object, etc.
? Clause boundaries: Measured on a set of
844 sentences which were marked with clause
boundaries, 37 sentences (4.4 %) had reorder-
ings that violated these boundaries. An exam-
ple of such a clause-boundary violation is be-
low:
Input: main1 (I) sarakaara2 (government) kaa3
(of) dhyaana4 (attention) maananiiya5 (hon-
ourable) pradhaana6 (prime) mantri7 (min-
ister) dvaaraa8 (by) isa9 (this) sabhaa10
(house) me11 (in) kiye12 gaye13 (made) isa14
(this) vaade15 (promise) ki16 ora17 (towards)
dilaanaa18 (to bring) chaahuungaa19 (would
like)
Reordered: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) kii16 ora17 (to-
wards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) dhyaana4 (attention) kaa3 (of)
sarakaara2 (government) men11 (in) isa14 (this)
sabhaa10 (house)
Reference: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) dhyaana4 (attention)
kaa3 (of) sarakaara2 (government) kii16 ora17
(towards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) men11 (in) isa9 (this) sabhaa10 (house)
English I would like to bring the attention of
the government towards this promise made by
the honourable prime minister in this house.
Note how the italicized clause, which is kept
together in the reference, is split up incorrectly
in the reordered output. The proportion of such
boundary violations is, however, quite low, be-
cause Hindi being a verb-final language, most
clauses end with a verb and it is probably quite
straightforward for the model to keep clauses
separate. A clause boundary detection program
should make it possible to eliminate the re-
maining errors.
? Local reordering: To estimate the short range
reordering performance, we consider how of-
ten different POS bigrams in the input are re-
ordered correctly. Here, we expect the model
to reorder prepositions correctly, and to avoid
any reordering that moves apart nouns and their
adjectival pre-modifiers or components of com-
pound nouns (see Section 3). Table 6 sum-
marizes the reordering performance for these
categories for a set of 280 sentences (same as
the test set used in Section 5.1). Each row
in Table 6 indicates the total number of cor-
rect instances for the pair, i.e., the number of
instances of the pair in the reference (column
titled Total), the number of instances that al-
ready appear in the correct order in the input
(column Input), and the number that are or-
dered correctly by the reordering model (col-
umn Reordered). The first two rows show that
adjective-noun and noun-noun (compounds)
are in most cases correctly retained in the orig-
inal order by the model. The final row shows
that while many prepositions have been moved
into their correct positions, there are still quite a
few mismatches with the reference. An impor-
tant reason why this happens is that nouns mod-
ified by prepositional phrases can often also be
expressed as noun compounds. For example,
vidyuta (electricity) kii (of) aavashyakataaen
(requirements) in Hindi can be expressed either
as ?requirements of electricity? or ?electricity
requirements?. The latter expression results in
a match with the input (explaining many of the
104 correct orders in the input) and a mismatch
with the model?s reordering. The same problem
in the training data would also adversely impact
the learning of the preposition reordering rule.
493
POS pair Total Input Reordered
adj-noun 234 192 196
noun-noun 46 44 42
prep-noun 436 104 250
Table 6: An analyis of reordering for a few POS bigrams
5.5 Machine translation results
We now present experiments in incorporating the re-
ordering model in machine translation systems. For
all results presented here, we reorder the training and
test data using the single best reordering based on
our reordering model for each sentence. For each of
the language pairs we evaluated, we trained Direct
Translation Model 2 (DTM) systems (Ittycheriah
and Roukos, 2007) with and without reordering and
compared performance on test data. We note that the
DTM system includes features that allow it to model
lexicalized reordering phenomena. The reordering
window size was set to +/-8 words for both the base-
line and our reordered input. In our experiments, we
left the word alignments fixed, i.e we reordered the
existing word alignments rather than realigning the
sentences after reordering. Redoing the word align-
ments with the reordered data could potentially give
further small improvements. We note that we ob-
tained better baseline performance using DTM sys-
tems than the standard Moses/Giza++ pipeline (e.g
we obtained a BLEU of 14.9 for English to Hindi
with a standard Moses/Giza++ pipeline). For all of
our systems we used a combination of HMM (Vo-
gel et al, 1996) and MaxEnt alignments (Ittycheriah
and Roukos, 2005).
For our Hindi-English experiments we use a train-
ing set of roughly 250k sentences (5.5Mwords) con-
sisting of the Darpa-TIDES dataset (Bojar et al,
2010) and an internal dataset from several domains
but dominated by news. Our test set was roughly
1.2K sentences from the news domain with a sin-
gle reference. To train our reordering model, we
used roughly 6K alignments plus 17K snippets se-
lected from MaxEnt alignments as described in Sec-
tion 5.1 with bigram, ContextPOS and ContextWord
features. The monolingual reordering BLEU (on the
same data reported on in Section 5.3) was 54.0 for
Hindi to English and 60.8 for English to Hindi.
For our Urdu-English experiments we used 70k
Language pair BLEU
Source Target Unreordered Reordered
Hindi English 14.7 16.7
Urdu English 23.3 24.8
English Hindi 20.7 22.5
Table 7: Translation performance without reordering
(baseline) compared with performance after preordering
with our reordering model.
sentences from the NIST MT-08 training corpus
and used the MT-08 eval set for testing. We note
that the MT-08 eval set has four references as com-
pared to one reference for our Hindi-English test
set. This largely explains the improved baseline per-
formance for Urdu-English as compared to Hindi-
English. We present averaged results for the Web
and News part of the test sets. To train the reorder-
ing model we used 9K hand alignments and 11K
snippets extracted from MaxEnt alignments as de-
scribed in Section 5.1 with bigram, ContextPOS and
ContextWord context feature. The monolingual re-
ordering BLEU for the reordering model thus ob-
tained (on the same data reported on in Section 5.3)
was 52.7.
Table 7 shows that for Hindi to English, English
to Hindi and for Urdu to English we see a gain
of 1.5 - 2 BLEU points. For English ? Hindi
we also experimented with a system that uses rules
(learned from the data using the methods described
in (Visweswariah et al, 2010)) applied to a parse to
reorder source side English sentences. This system
had a BLEU score of 21.2, which is an improvement
over the baseline, but our reordering model is better
by 1.3 BLEU points.
An added benefit of our reordering model is that
the decoder can be run with a smaller search space
exploring only a small amount of reordering with-
out losing accuracy but running substantially faster.
Table 8 shows the variation in machine Hindi to En-
glish translation performance with varying skip size
(this parameter sets the maximum number of words
skipped during decoding, lower values are associ-
ated with a restricted decoder search space and in-
creased speed).
494
skip Unreordered Reordered
2 12.2 16.7
4 13.4 16.7
8 14.7 16.4
Table 8: Translation performance with/without reorder-
ing with varying decoder search space.
6 Conclusion and future work
In this paper we presented a reordering model to
reorder source language data to make it resemble
the target language word order without using either
a source or target parser. We showed consistent
gains of up to 2 BLEU points in machine transla-
tion performance using this model to preorder train-
ing and test data. We show better performance com-
pared to syntax based reordering rules for English
to Hindi translation. Our model used only a part of
speech tagger (sometimes trained with fairly small
amounts of data) and a small corpus of word align-
ments. Considering the fact that treebanks required
to build high quality parsers are costly to obtain, we
think that our reordering model is a viable alterna-
tive to using syntax for reordering. We also note,
that with the preordering based on our reordering
model we can achieve the best BLEU scores with
a much tighter search space in the decoder. Even ac-
counting for the cost of finding the best reordering
according to our model, this usually results in faster
processing than if we did not have the reordering in
place.
In future work we plan to explore using more data
from automatic alignments, perhaps by considering
a joint model for aligning and reordering. We would
also like to explore doing away with the requirement
of having a POS tagger, using completely unsuper-
vised methods to class words. We currently only
look at word pairs in calculating the loss function
used in MIRA updates. We would like to investigate
the use of other loss functions and their effect on re-
ordering performance. We also would like to explore
whether the use of scores from our reordering model
directly in machine translation systems can improve
performance relative to using just the single best re-
ordering.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David L. Applegate, Robert E. Bixby, Vasek Chvatal, and
William J. Cook. 2005. Concorde tsp solver. In
http://www.tsp.gatech.edu/.
Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syn-
tactic phrase reordering for English-to-Arabic statisti-
cal machine translation. In Proceedings of EACL.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In LREC.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research.
Aniket Dalal, Kumar Nagaraj, Uma Sawant, Sandeep
Shelke, and Pushpak Bhattacharyya. 2007. Building
feature rich pos tagger for morphologically rich lan-
guages: Experiences in Hindi. In Proceedings of In-
ternational Conference on Natural Language Process-
ing.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proceedings of ACL.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Kurt Hornik and Michael Hahsler. 2009. TSP?
infrastructure for the traveling salesperson problem.
Journal of Statistical Software, 23(i02).
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
495
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proceedings of HLT-NAACL,
pages 57?64.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25:607?615, December.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In COL-
ING.
Y. Liu, Q. Liu, and S. Lin. 2006. Tree-to-String align-
ment template for statistical machine translation. In
Proceedings of ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of ACL-IJCNLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
496
Proceedings of NAACL-HLT 2013, pages 315?324,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving reordering performance using higher order and structural
features
Mitesh M. Khapra
IBM Research India
mikhapra@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
anandr42@gmail.com
Karthik Visweswariah
IBM Research India
v-karthik@in.ibm.com
Abstract
Recent work has shown that word aligned data
can be used to learn a model for reordering
source sentences to match the target order.
This model learns the cost of putting a word
immediately before another word and finds the
best reordering by solving an instance of the
Traveling Salesman Problem (TSP). However,
for efficiently solving the TSP, the model is
restricted to pairwise features which examine
only a pair of words and their neighborhood.
In this work, we go beyond these pairwise fea-
tures and learn a model to rerank the n-best
reorderings produced by the TSP model us-
ing higher order and structural features which
help in capturing longer range dependencies.
In addition to using a more informative set
of source side features, we also capture target
side features indirectly by using the transla-
tion score assigned to a reordering. Our exper-
iments, involving Urdu-English, show that the
proposed approach outperforms a state-of-the-
art PBSMT system which uses the TSP model
for reordering by 1.3 BLEU points, and a pub-
licly available state-of-the-art MT system, Hi-
ero, by 3 BLEU points.
1 Introduction
Handling the differences in word orders between
pairs of languages is crucial in producing good ma-
chine translation. This is especially true for lan-
guage pairs such as Urdu-English which have sig-
nificantly different sentence structures. For exam-
ple, the typical word order in Urdu is Subject Object
Verb whereas the typical word order in English is
Subject Verb Object. Phrase based systems (Koehn
et al, 2003) rely on a lexicalized distortion model
(Al-Onaizan and Papineni, 2006; Tillman, 2004)
and the target language model to produce output
words in the correct order. This is known to be in-
adequate when the languages are very different in
terms of word order (refer to Table 3 in Section 3).
Pre-ordering source sentences while training and
testing has become a popular approach in overcom-
ing the word ordering challenge. Most techniques
for pre-ordering (Collins et al, 2005; Wang et al,
2007; Ramanathan et al, 2009) depend on a high
quality source language parser, which means these
methods work only if the source language has a
parser (this rules out many languages). Recent work
(Visweswariah et al, 2011) has shown that it is pos-
sible to learn a reordering model from a relatively
small number of hand aligned sentences . This elim-
inates the need of a source or target parser.
In this work, we build upon the work of
Visweswariah et al (2011) which solves the reorder-
ing problem by treating it as an instance of the
Traveling Salesman Problem (TSP). They learn a
model which assigns costs to all pairs of words in
a sentence, where the cost represents the penalty of
putting a word immediately preceding another word.
The best permutation is found via the chained Lin-
Kernighan heuristic for solving a TSP. Since this
model relies on solving a TSP efficiently, it cannot
capture features other than pairwise features that ex-
amine the words and neighborhood for each pair of
words in the source sentence. In the remainder of
this paper we refer to this model as the TSP model.
Our aim is to go beyond this limitation of the TSP
model and use a richer set of features instead of us-
ing pairwise features only. In particular, we are in-
terested in features that allow us to examine triples
of words/POS tags in the candidate reordering per-
315
mutation (this is akin to going from bigram to tri-
gram language models), and also structural features
that allow us to examine the properties of the seg-
mentation induced by the candidate permutation. To
go beyond the set of features incorporated by the
TSP model, we do not solve the search problem
which would be NP-hard. Instead, we restrict our-
selves to an n-best list produced by the base TSP
model and then search in that list. Using a richer
set of features, we learn a model to rerank these n-
best reorderings. The parameters of the model are
learned using the averaged perceptron algorithm. In
addition to using a richer set of source side features
we also indirectly capture target side features by in-
terpolating the score assigned by our model with the
score assigned by the decoder of a MT system.
To justify the use of these informative features,
we point to the example in Table 1. Here, the head
(driver) of the underlined English Noun Phrase (The
driver of the car) appears to the left of the Noun
Phrase whereas the head (chaalak {driver}) of the
corresponding Urdu Noun Phrase (gaadi {car} ka
{of} chaalak {driver}) appears to the right of the
Noun Phrase. To produce the correct reordering of
the source Urdu sentence the model has to make an
unusual choice of putting gaadi {car} before bola
{said}. We say this is an unusual choice because the
model examines only pairwise features and it is un-
likely that it would have seen sentences having the
bigram ?car said?. If the exact segmentation of the
source sentence was known, then the model could
have used the information that the word gaadi {car}
appears in a segment whose head is the noun chaalak
{driver} and hence its not unusual to put gaadi {car}
before bola {said} (because the construct ?NP said?
is not unusual). However, since the segmentation
of the source sentence is not known in advance, we
use a heuristic (explained later) to find the segmen-
tation induced by a reordering. We then extract
features (such as first word current segment,
end word current segment) to approximate these
long range dependencies.
Using this richer set of features with Urdu-
English as the source language pair, our approach
outperforms the following state of the art systems:
(i) a PBSMT system which uses TSP model for re-
ordering (by 1.3 BLEU points), (ii) a hierarchical
PBSMT system (by 3 BLEU points). The overall
Input Urdu: fir gaadi ka chaalak kuch bola
Gloss: then car of driver said something
English: Then the driver of the car said something.
Ref. reordering: fir chaalak ka gaadi bola kuch
Table 1: Example motivating the use of structural features
gain is 6.3 BLEU points when compared to a stan-
dard PBSMT system which uses a lexicalized distor-
tion model (Al-Onaizan and Papineni, 2006).
The rest of this paper is organized as follows. In
Section 2 we discuss our approach of re-ranking the
n-best reorderings produced by the TSP model. This
includes a discussion of the model used, the features
used and the algorithm used for learning the parame-
ters of the model. It also includes a discussion on the
modification to the Chained Lin-Kernighan heuris-
tic to produce n-best reorderings. Next, in Section
3 we describe our experimental setup and report the
results of our experiments. In Section 4 we present
some discussions based on our study. In section 5 we
briefly describe some prior related work. Finally, in
Section 6, we present some concluding remarks and
highlight possible directions for future work.
2 Re-ranking using higher order and
structural features
As mentioned earlier, the TSP model (Visweswariah
et al, 2011) looks only at local features for a word
pair (wi, wj). We believe that for better reorder-
ing it is essential to look at higher order and struc-
tural features (i.e., features which look at the overall
structure of a sentence). The primary reason why
Visweswariah et al (2011) consider only pairwise
bigram features is that with higher order features the
reordering problem can no longer be cast as a TSP
and hence cannot be solved using existing efficient
heuristic solvers. However, we do not have to deal
with an NP-Hard search problem because instead of
considering all possible reorderings we restrict our
search space to only the n-best reorderings produced
by the base TSP model. Formally, given a set of
reorderings, ? = [pi1, pi2, pi3, ...., pin], for a source
sentence s, we are interesting in assigning a score,
score(pi), to each of these reorderings and pick the
reordering which has the highest score. In this paper,
we parametrize this score as:
score(pi) = ?T?(pi) (1)
316
where, ? is the weight vector and ?(pi) is a vector
of features extracted from the reordering pi. The aim
then is to find,
pi? = arg max
pi??
score(pi) (2)
In the following sub-sections, we first briefly
describe our overall approach towards finding pi?.
Next, we describe our modification to the Lin-
Kernighan heuristic for producing n-best outputs
for TSP instead of the 1-best output used by
(Visweswariah et al, 2011). We then discuss the fea-
tures used for re-ranking these n-best outputs, fol-
lowed by a discussion on the learning algorithm used
for estimating the parameters of the model. Finally,
we describe how we interpolate the score assigned
by our model with the score assigned by the decoder
of a SMT engine to indirectly capture target side fea-
tures.
2.1 Overall approach
The training stage of our approach involves two
phases : (i) Training a TSP model which will be
used to generate n-best reorderings and (ii) Training
a re-ranking model using these n-best reorderings.
For training both the models we need a collection
of sentences where the desired reordering pi?(x) for
each input sentence x is known. These reference or-
derings are derived from word aligned source-target
sentence pairs (see first 4 rows of Figure 1). We first
divide this word aligned data into N parts and use
A?i to denote the alignments leaving out the i-th
part. We then train a TSP model M?i using refer-
ence reorderings derived from A?i as described in
(Visweswariah et al, 2011). Next, we produce n-
best reorderings for the source sentences using the
algorithm getNBestReorderings(sentence) de-
scribed later. Dividing the data into N parts is nec-
essary to ensure that the re-ranking model is trained
using a realistic n-best list rather than a very opti-
mistic n-best list (which would be the case if part i
is reordered using a model which has already seen
part i during training).
Each of the n-best reorderings is then repre-
sented as a feature vector comprising of higher
order and structural features. The weights
of these features are then estimated using the
averaged perceptron method. At test time,
getNBestReorderings(sentence) is used to gen-
erate the n-best reorderings for the test sentence us-
ing the trained TSP model. These reorderings are
then represented using higher order and structural
features and re-ranked using the weights learned ear-
lier. We now describe the different stages of our al-
gorithm.
2.2 Generating n-best reorderings for the TSP
model
The first stage of our approach is to train a TSP
model and generate n-best reorderings using it. The
decoder used by Visweswariah et al (2011) relies
on the Chained Lin-Kernighan heuristic (Lin and
Kernighan, 1973) to produce the 1-best permutation
for the TSP problem. Since our algorithm aims at
re-ranking an n-best list of permutations (reorder-
ings), we made a modification to the Chained Lin-
Kernighan heuristic to produce this n-best list as
shown in Algorithm 1 .
Algorithm 1 getNBestReorderings(sentence)
NbestSet = ?
pi? = Identity permutation
pi? = linkernighan(pi?)
insert(NbestSet, pi?)
for i = 1? nIter do
pi
?
= perturb(pi?)
pi
?
= linkernighan(pi
?
)
if C(pi
?
) < maxpi?NbestSetC(pi) then
InsertOrReplace(NbestSet, pi
?
)
end if
if C(pi
?
) < C(pi?) then
pi? = pi
?
end if
end for
In Algorithm 1 perturb() is a four-edge pertur-
bation described in (Applegate et al, 2003), and
linkernighan() is the Lin-Kernighan heuristic that
applies a sequence of flips that potentially returns
a lower cost permutation as described in (Lin and
Kernighan, 1973). The cost C(pi) is calculated us-
ing a trained TSP model.
2.3 Features
We represent each of the n-best reorderings obtained
above as a vector of features which can be divided
into two sets : (i) higher order features and (ii) struc-
317
Segmentation Based Features
(extracted for every segment in
the induced segmentation)
Features fired for the seg-
ment [mere(PRP) ghar(NN)]
in Figure1
end lex current segment ghar
end lex prev segment Shyam
end pos current segment NN
end pos prev segment NN
length of current segment 2
first lex current segment mere
first lex next segment aaye
first pos current segment PRP
first pos next segment V RB
Higher order features Features fired for the triplet
Shyam(NN) the(Vaux)
aaye(VRB) in Figure1
lex triplet jumps lex triplet = ?Shyam the
aaye? && jumps = [4,?1]
pos triplet jumps pos triplet = ?NN Vaux
VRB? && jumps = [4,?1]
Table 2: Features used in our model.
tural features. The higher order features are es-
sentially trigram lexical and pos features whereas
the structural features are derived from the sentence
structure induced by a reordering (explained later).
2.3.1 Higher Order Features
Since deriving a good reordering would essen-
tially require analyzing the syntactic structure of the
source sentence, the tasks of reordering and parsing
are often considered to be related. The main motiva-
tion for using higher order features thus comes from
a related work on parsing (Koo and Collins, 2010)
where the performance of a state of the art parser
was improved by considering higher order depen-
dencies. In our model we use trigram features (see
Table 2) of the following form:
?(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2))
where rui =word at position i in the reordered
source sentence and J(x, y) = difference between
the positions of x and y in the original source
sentence.
Figure 1 shows an example of jumps between dif-
ferent word pairs in an Urdu sentence. Since such
higher order features will typically be sparse, we
also use some back-off features. For example, in-
stead of using the absolute values of jumps we di-
vide the jumps into 3 buckets, viz., high, low and
medium and use these buckets in conjunction with
the triplets as back-off features.
Figure 1: Segmentation induced on the Urdu sentence
when it is reordered according to its English translation.
Note that the words Shyam and mere are adjacent to each
other in the original Urdu sentence but not in the re-
ordered Urdu sentence. Hence, the word mere marks the
beginning of a new segment.
2.3.2 Structural Features
The second set of features is based on the hy-
pothesis that any reordering of the source sentence
induces a segmentation on the sentence. This seg-
mentation is based on the following heuristic: if wi
and wi+1 appear next to each other in the original
sentence but do not appear next to each other in the
reordered sentence then wi marks the end of a seg-
ment and wi+1 marks the beginning of the next seg-
ment. To understand this better please refer to Fig-
ure 1 which shows the correct reordering of an Urdu
sentence based on its English translation and the cor-
responding segmentation induced on the Urdu sen-
tence. If the correct segmentation of a sentence is
known in advance then one could use a hierarchical
model where the goal would be to reorder segments
instead of reordering words individually (basically,
instead of words, treat segments as units of reorder-
ing. In principle, this is similar to what is done by
parser based reordering methods). Since the TSP
model does not explicitly use segmentation based
features it often produces wrong reorderings (refer
to the motivating example in Section 1).
Reordering such sentences correctly requires
some knowledge about the hierarchical structure of
the sentence. To capture such hierarchical informa-
tion, we use features which look at the elements
318
(words, pos tags) of a segment and its neighboring
segments. These features along with examples are
listed in Table 2. These features should help us in
selecting a reordering which induces a segmentation
which is closest to the correct segmentation induced
by the reference reordering. Note that every feature
listed in Table 2 is a binary feature which takes on
the value 1 if it fires for the given reordering and
value 0 if it does not fire for the given reordering. In
addition to the features listed in Table 2 we also use
the score assigned by the TSP model as a feature.
2.4 Estimating model parameters
We use perceptron as the learning algorithm for es-
timating the parameters of our model described in
Equation 1. To begin with, all parameters are ini-
tialized to 0 and the learning algorithm is run for N
iterations. During each iteration the parameters are
updated after every training instance is seen. For ex-
ample, during the i-th iteration, after seeing the j-th
training sentence, we update the k-th parameter ?k
using the following update rule:
?(i,j)k = ?
(i,j?1)
k + ?k(pi
gold
j )? ?k(pi
?
j ) (3)
where, ?(i,j)k = value of the k-th parameter after
seeing sentence j in iteration i
?k = k-th feature
pigoldj = gold reordering for the j-th sentence
pi?j = arg max
pi??j
?(i,j?1)
T
?(pi)
where ?j is the set of n-best reorderings for the j-
th sentence. pi?j is thus the highest-scoring reorder-
ing for the j-th sentence under the current parame-
ter vector. Since the averaged perceptron method is
known to perform better than the perceptron method,
we used the averaged values of the parameters at the
end of N iterations, calculated as:
?avgk =
1
N ? t
N?
i=1
t?
j=1
?(i,j)k (4)
where, N = Number of iterations
t = Number of training instances
We observed that in most cases the reference re-
ordering in not a part of the n-best list produced
by the TSP model. In such cases instead of using
?k(pi
gold
j ) for updating the weights in Equation 3 we
use ?k(pi
closest to gold
j ) as this is known to be a better
strategy for learning a re-ranking model (Arun and
Koehn, 2007). piclosest to goldj is given by:
arg max
piij??j
# of common bigram pairs in piij and pi
gold
j
len(pigoldj )
where, ?j = set of n-best reorderings for j
th sentence
piclosest to goldj is thus the reordering which has the
maximum overlap with pigoldj in terms of the number
of word pairs (wm, wn) where wn is put next to wm.
2.5 Interpolating with MT score
The approach described above aims at producing a
better reordering by extracting richer features from
the source sentence. Since the final aim is to im-
prove the performance of an MT system, it would
potentially be beneficial to interpolate the scores as-
signed by Equation 1 to a given reordering with the
score assigned by the decoder of an MT system to
the translation of the source sentence under this re-
ordering. Intuitively, the MT score would allow us
to capture features from the target sentence which
are obviously not available to our model. With this
motivation, we use the following interpolated score
(scoreI ) to select the best translation.
scoreI(ti) = ??score?(pii) + (1? ?) ? scoreMT (ti)
where, ti =translation produced under the i-th
reordering of the source sentence
score?(pii) =score assigned by our model to the
i-th reordering
scoreMT (ti) =score assigned by the MT system to ti
The weight ? is used to ensure that score?(pii) and
scoreMT (pii) are in the same range (it just serves as
a normalization constant). We acknowledge that the
above process is expensive because it requires the
MT system to decode n reorderings for every source
sentence. However, the aim of this work is to show
that interpolating with the MT score which implic-
itly captures features from the target sentence helps
in improving the performance. Ideally, this interpo-
lation should (and can) be done at decode time with-
out having to decode n reorderings for every source
319
sentence (for example by expressing the n reorder-
ings as a lattice), but, we leave this as future work.
3 Empirical evaluation
We evaluated our reordering approach on Urdu-
English. We use two types of evaluation, one in-
trinsic and one extrinsic. For intrinsic evaluation,
we compare the reordered source sentence in Urdu
with a reference reordering obtained from the hand
alignments using BLEU (referred to as monolingual
BLEU or mBLEU by (Visweswariah et al, 2011) ).
Additionally, we evaluate the effect of reordering on
MT performance using BLEU (extrinsic evaluation).
As mentioned earlier, our training process in-
volves two phases : (i) Generating n-best reorder-
ings for the training data and (ii) using these n-best
reorderings to train a perceptron model. We use the
same data for training the reordering model as well
as our perceptron model. This data contains 180K
words of manual alignments (part of the NIST MT-
08 training data) and 3.9M words of automatically
generated machine alignments (1.7M words from
the NIST MT-08 training data1 and 2.2M words ex-
tracted from sources on the web2). The machine
alignments were generated using a supervised maxi-
mum entropy model (Ittycheriah and Roukos, 2005)
and then corrected using an improved correction
model (McCarley et al, 2011). We first divide the
training data into 10 folds. The n-best reorder-
ings for each fold are then generated using a model
trained on the remaining 9 folds. This division into
10 folds is done for reasons explained earlier in Sec-
tion 2.1. These n-best reorderings are then used to
train the perceptron model as described in Section
2.4. Note that Visweswariah et al (2011) used only
manually aligned data for training the TSP model.
However, we use machine aligned data in addition
to manually aligned data for training the TSP model
as it leads to better performance. We used this im-
provised TSP model as the state of the art baseline
(rows 2 and 3 in Tables 3 and 4 respectively) for
comparing with our approach.
We observed that the perceptron algorithm con-
verges after 5 iterations beyond which there is very
little (<1%) improvement in the bigram precision on
1http://www.ldc.upenn.edu
2http://centralasiaonline.com
the training data itself (bigram precision is the frac-
tion of word pairs which are correctly put next to
each other). Hence, for all the numbers reported in
this paper, we used 5 iterations of perceptron train-
ing. Similarly, while generating the n-best reorder-
ings, we experimented with following values of n :
10, 25, 50, 100 and 200. We observed that, by re-
stricting the search space to the top-50 reorderings
we get the best reordering performance (mBLEU)
on a development set. Hence, we used n=50 for our
MT experiments.
For intrinsic evaluation we use a development set
of 8017 Urdu tokens reordered manually. Table 3
compares the performance of the top-1 reordering
output by our algorithm with the top-1 reordering
generated by the improved TSP model in terms of
mBLEU. We see a gain of 1.8 mBLEU points with
our approach.
Next, we see the impact of the better reorderings
produced by our system on the performance of
a state-of-the-art MT system. For this, we used
a standard phrase based system (Al-Onaizan and
Papineni, 2006) with a lexicalized distortion model
with a window size of +/-4 words (Tillmann and
Ney, 2003). As mentioned earlier, our training data
consisted of 3.9M words including the NIST MT-08
training data. We use HMM alignments along with
higher quality alignments from a supervised aligner
(McCarley et al, 2011). The Gigaword English
corpus was used for building the English language
model. We report results on the NIST MT-08
evaluation set, averaging BLEU scores from the
News and Web conditions to provide a single BLEU
score. Table 4 compares the MT performance
obtained by reordering the training and test data
using the following approaches:
1. No pre-ordering: A baseline system which
does not use any source side reordering as a pre-
processing step
2. HIERO : A state of the art hierarchical phrase
based translation system (Chiang, 2007)
3. TSP: A system which uses the 1-best reordering
produced by the TSP model
4. Higher order & structural features: A system
320
Approach mBLEU
Unreordered 31.2
TSP 56.6
Higher order & structural features 58.4
Table 3: mBLEU scores for Urdu to English reordering
using different models.
Approach BLEU
No pre-ordering 21.9
HIERO 25.2
TSP 26.9
Higher order & structural features 27.5
Interpolating with MT score 28.2
Table 4: MT performance for Urdu to English without re-
ordering and with reordering using different approaches.
which reranks n-best reorderings produced by TSP
using higher order and structural features
5. Interpolating with MT score : A system which
interpolates the score assigned to a reordering by
our model with the score assigned by a MT system
We used Joshua 4.0 (Ganitkevitch et al, 2012)
which provides an open source implementation of
HIERO. For training, tuning and testing HIERO
we used the same experimental setup as described
above. As seen in Table 4, we get an overall gain of
6.2 BLEU points with our approach as compared to
a baseline system which does not use any reordering.
More importantly, we outperform (i) a PBSMT sys-
tem which uses the TSP model by 1.3 BLEU points
and (ii) a state of the art hierarchical phrase based
translation system by 3 points.
4 Discussions
We now discuss some error corrections and ablation
tests.
4.1 Example of error correction
We first give an example where the proposed ap-
proach performed better than the TSP model. In the
example below, I = input sentence, E= gold English
translation, T = incorrect reordering produced by
TSP and O = correct reordering produced by our
approach. Note that the words roman catholic aur
protestant in the input sentence get translated as
Sentence length mBLEU
Unreordered TSP Our
approach
1-14 words (small) 29.7 58.7 57.8
15-22 words (med.) 28.2 56.8 59.2
23+ words (long) 33.4 55.8 58.2
All 31.2 56.6 58.4
Table 5: mBLEU improvements on sentences of different
lengths
a continuous phrase in English (Roman Catholic
and Protestant) and hence should be treated as a
single unit by the reordering model. The TSP model
fails to keep this segment intact whereas our model
(which uses segmentation based features) does so
and matches the reference reordering.
I: ab roman catholic aur protestant ke darmiyaan
ikhtilafat khatam ho chuke hai
E: The differences between Roman Catholics and
Protestants have now ended
T: ab roman ikhtilafat ke darmiyaan catholic aur
protestant hai khatam ho chuke
O: ab ikhtilafat ke darmiyaan roman catholic aur
protestant hai khatam ho chuke
4.2 Performance based on sentence length
We split the test data into roughly three equal parts
based on length, and calculated the mBLEU im-
provements on each of these parts as reported in
Table 5. These results show that the model works
much better for medium-to-long sentences. In fact,
we see a drop in performance for small sentences. A
possible reason for this could be that the structural
features that we use are derived through a heuristic
that is error-prone, and in shorter sentences, where
there would be fewer reordering problems, these er-
rors hurt more than they help. While this needs to be
analyzed further, we could meanwhile combine the
two models fruitfully by using the base TSP model
for small sentences and the new model for longer
sentences.
321
Disabled feature mBLEU
end lex current segment 57.6
end lex prev segment 57.6
end pos current segment 57.8
end pos prev segment 57.4
length 57.6
lex triplet jumps 58.0
pos triplet jumps 56.1
first lex current segment 58.2
first lex next segment 58.2
first pos current segment 57.6
first pos next segment 57.6
NONE 58.4
Table 6: Ablation test indicating the contribution of each
feature to the reordering performance.
4.3 Ablation test
To study the contribution of each feature to the
reordering performance, we did an ablation test
wherein we disabled one feature at a time and mea-
sured the change in the mBLEU scores. Table 6
summarizes the results of our ablation test. The
maximum drop in performance is obtained when the
pos triplet jumps feature is disabled. This obser-
vation supports our claim that higher order features
(more than bigrams) are essential for better reorder-
ing. The lex triplet jumps feature has the least
impact on the performance mainly because it is a
lexicalized feature and hence very sparse. Also note
that there is a high correlation between the perfor-
mances obtained by dropping one feature from each
of the following pairs :
i) first lex current segment, first lex next segment
ii) first pos current segment, first pos next segment
iii) end lex current segment, end lex next segment.
This is because these pairs of features are
highly dependent features. Note that similar to
the pos triplet jumps feature we also tried a
pos quadruplet jumps feature but it did not help
(mainly due to overfitting and sparsity).
5 Related Work
There are several studies which have shown that re-
ordering the source side sentence to match the target
side order leads to improvements in Machine Trans-
lation. These approaches can be broadly classified
into three types. First, approaches which reorder
source sentences by applying rules to the source side
parse; the rules are either hand-written (Collins et
al., 2005; Wang et al, 2007; Ramanathan et al,
2009) or learned from data (Xia and McCord, 2004;
Genzel, 2010; Visweswariah et al, 2010). These
approaches require a source side parser which is
not available for many languages. The second type
of approaches treat machine translation decoding
as a parsing problem by using source and/or tar-
get side syntax in a Context Free Grammar frame-
work. These include Hierarchical models (Chi-
ang, 2007) and syntax based models (Yamada and
Knight, 2002; Galley et al, 2006; Liu et al, 2006;
Zollmann and Venugopal, 2006). The third type of
approaches, avoid the use of a parser (as required
by syntax based models) and instead train a reorder-
ing model using reference reorderings derived from
aligned data. These approaches (Tromble and Eis-
ner, 2009; Visweswariah et al, 2011; DeNero and
Uszkoreit, 2011; Neubig et al, 2012) have a low de-
code time complexity as reordering is done as a pre-
processing step and not integrated with the decoder.
Our work falls under the third category, as it im-
proves upon the work of (Visweswariah et al, 2011)
which is closely related to the work of (Tromble
and Eisner, 2009) but performs better. The focus
of our work is to use higher order and structural
features (based on segmentation of the source sen-
tence) which are not captured by their model. Some
other works have used collocation based segmenta-
tion (Henr??quez Q. et al, 2010) and Multiword Ex-
pressions as segments (Bouamor et al, 2012) to im-
prove the performance of SMT but without much
success. The idea of improving performance by re-
ranking a n-best list of outputs has been used re-
cently for the related task of parsing (Katz-Brown et
al., 2011) using targeted self-training for improving
the performance of reordering. However, in contrast,
in our work we directly aim at improving the perfor-
mance of a reordering model.
6 Conclusion
In this work, we proposed a model for re-ranking
the n-best reorderings produced by a state of the
art reordering model (TSP model) which is limited
to pair wise features. Our model uses a more in-
formative set of features consisting of higher order
features, structural features and target side features
322
(captured indirectly using translation scores). The
problem of intractability is solved by restricting the
search space to the n-best reorderings produced by
the TSP model. A detailed ablation test shows that
of all the features used, the pos triplet features are
most informative for reordering. A gain of 1.3 and 3
BLEU points over a state of the art phrase based and
hierarchical machine translation system respectively
provides good extrinsic validation of our claim that
such long range features are useful.
As future work, we would like to evaluate our al-
gorithm on other language pairs. We also plan to
integrate the score assigned by our model into the
decoder to avoid having to do n decodings for ev-
ery source sentence. Also, it would be interesting
to model the segmentation explicitly, where the aim
would be to first segment the sentence and then use
a two level hierarchical reordering model which first
reorders these segments and then reorders the words
within the segment.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David Applegate, William Cook, and Andre Rohe. 2003.
Chained lin-kernighan for large traveling salesman
problems. In INFORMS Journal On Computing.
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In In Proceed-
ings of MT Summit.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2012. Identifying bilingual multi-
word expressions for statistical machine translation.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Uur Doan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228, June.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 193?203, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 961?968, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, pro, and paraphrases. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 283?291, Montre?al, Canada, June. Association
for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Carlos Henr??quez Q., R. Marta Costa-jussa`, Vidas
Daudaravicius, E. Rafael Banchs, and B. Jose? Marin?o.
2010. Using collocation segmentation to augment the
phrase table. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, WMT ?10, pages 98?102, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for
machine translation reordering. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 183?192,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
323
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1?11, Stroudsburg, PA,
USA. Association for Computational Linguistics.
S. Lin and B. W. Kernighan. 1973. An effective heuristic
algorithm for the travelling-salesman problem. Oper-
ations Research, pages 498?516.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 609?616, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos,
Bing Xiang, and Jian-ming Xu. 2011. A correc-
tion model for word alignments. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 889?898,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 843?853, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 486?496,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
324
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1275?1284,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cut the noise: Mutually reinforcing reordering and alignments for
improved machine translation
Karthik Visweswariah
IBM Research India
v-karthik@in.ibm.com
Mitesh M. Khapra
IBM Research India
mikhapra@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
anandr42@gmail.com
Abstract
Preordering of a source language sentence
to match target word order has proved to
be useful for improving machine transla-
tion systems. Previous work has shown
that a reordering model can be learned
from high quality manual word alignments
to improve machine translation perfor-
mance. In this paper, we focus on further
improving the performance of the reorder-
ing model (and thereby machine transla-
tion) by using a larger corpus of sentence
aligned data for which manual word align-
ments are not available but automatic ma-
chine generated alignments are available.
The main challenge we tackle is to gen-
erate quality data for training the reorder-
ing model in spite of the machine align-
ments being noisy. To mitigate the effect
of noisy machine alignments, we propose
a novel approach that improves reorder-
ings produced given noisy alignments and
also improves word alignments using in-
formation from the reordering model. This
approach generates alignments that are 2.6
f-Measure points better than a baseline su-
pervised aligner. The data generated al-
lows us to train a reordering model that
gives an improvement of 1.8 BLEU points
on the NIST MT-08 Urdu-English eval-
uation set over a reordering model that
only uses manual word alignments, and a
gain of 5.2 BLEU points over a standard
phrase-based baseline.
1 Introduction
Dealing with word order differences between
source and target languages presents a significant
challenge for machine translation systems. Failing
to produce target words in the correct order results
in machine translation output that is not fluent and
is often very hard to understand. These problems
are particularly severe when translating between
languages which have very different structure.
Phrase based systems (Koehn et al, 2003) use
lexicalized distortion models (Al-Onaizan and Pa-
pineni, 2006; Tillman, 2004) and scores from the
target language model to produce words in the cor-
rect order in the target language. These systems
typically are only able to capture short range re-
orderings and the amount of data required to po-
tentially capture longer range reordering phenom-
ena is prohibitively large.
There has been a large body of work showing
the efficacy of preordering source sentences using
a source parser and applying hand written or auto-
matically learned rules (Collins et al, 2005; Wang
et al, 2007; Ramanathan et al, 2009; Xia and Mc-
Cord, 2004; Genzel, 2010; Visweswariah et al,
2010). Recently, approaches that address the prob-
lem of word order differences between the source
and target language without requiring a high qual-
ity source or target parser have been proposed
(DeNero and Uszkoreit, 2011; Visweswariah et
al., 2011; Neubig et al, 2012). These methods
use a small corpus of manual word alignments
(where the words in the source sentence are man-
ually aligned to the words in the target sentence)
to learn a model to preorder the source sentence to
match target order.
In this paper, we build upon the approach in
(Visweswariah et al, 2011) which uses manual
word alignments for learning a reordering model.
Specifically, we show that we can significantly
improve reordering performance by using a large
number of sentence pairs for which manual word
alignments are not available. The motivation for
going beyond manual word alignments is clear:
the reordering model can have millions of features
and estimating weights for the features on thou-
sands of sentences of manual word alignments is
1275
likely to be inadequate. One approach to deal with
this problem would be to use only part-of-speech
tags as features for all but the most frequent words.
This will cut down on the number of features and
perhaps the model would be learnable with a small
set of manual word alignments. Unfortunately, as
we will see in the experimental section, leaving
out lexical information from the models hurts per-
formance even with a relatively small set of man-
ual word alignments. Another option would be to
collect more manual word alignments but this is
undesirable because it is time consuming and ex-
pensive.
The challenge in going beyond manual word
alignments and using machine alignments is the
noise in the machine alignments which affects the
performance of the reordering model (see Section
5). We illustrate this with the help of a motivating
example. Consider the example English sentence
and its translation shown in Figure 1.
He went to the stadium to play
vaha khelne keliye stadium ko gaya
Figure 1: An example English sentence with
its Urdu translation with alignment links. Red
(dotted) links are incorrect links while the blue
(dashed) links are the corresponding correct links.
A standard word alignment algorithm that we
used (McCarley et al, 2011) made the mistake of
mis-aligning the Urdu ko and keliye (it switched
the two). Deriving reference reorderings from
these wrong alignments would give us an incor-
rect reordering. A reordering model trained on
such incorrect reorderings would obviously per-
form poorly. Our task is thus two-fold (i) im-
prove the quality of machine alignments (ii) use
these less noisy alignments to derive cleaner train-
ing data for a reordering model.
Before proceeding, we first point out that the
two tasks, viz., reordering and word alignment
are related: Having perfect reordering makes the
alignment task easier while having perfect align-
ments in turn makes the task of finding reorder-
ings trivial. Motivated by this fact, we introduce
models that allow us to connect the source/target
reordering and the word alignments and show
that these models help in mutually improving the
performance of word alignments and reordering.
Specifically, we build two models: the first scores
reorderings given the source sentence and noisy
alignments, the second scores alignments given
the noisy source and target reorderings and the
source and target sentences themselves. The sec-
ond model helps produce better alignments, while
we use the first model to help generate better ref-
erence reordering given noisy alignments. These
improved reference reorderings will then be used
to train a reordering model.
Our experiments show that reordering models
trained using these improved machine alignments
perform significantly better than models trained
only on manual word alignments. This results in
a 1.8 BLEU point gain in machine translation per-
formance on an Urdu-English machine translation
task over a preordering model trained using only
manual word alignments. In all, this increases
the gain in performance by using the preordering
model to 5.2 BLEU points over a standard phrase-
based system with no preordering.
The rest of this paper is structured as follows.
Section 2 describes the main reordering issues in
Urdu-English translation. Section 3 introduces the
reordering modeling framework that forms the ba-
sis for our work. Section 4 describes the two mod-
els we use to tie together reordering and align-
ments and how we use these models to generate
training data for training our reordering model.
Section 5 presents the experimental setup used for
evaluating the models proposed in this paper on
an Urdu-English machine translation task. Sec-
tion 6 presents the results of our experiments.
We describe related work in Section 7 and finally
present some concluding remarks and potential fu-
ture work in Section 8.
2 Reordering issues in Urdu-English
translation
In this section we describe the main sources of
word order differences between Urdu and English
since this is the language pair we experiment with
in this paper.
The typical word order in Urdu is Subject-
Object-Verb unlike English in which the order is
Subject-Verb-Object. Urdu has case markers that
sometimes (but not always) mark the subject and
the object of a sentence. This difference in the
placement of verbs can often lead to movements of
verbs over long distances (depending on the num-
ber of words in the object). Phrase based systems
do not capture such long distance movements well.
1276
Another difference is that Urdu uses post-
positions unlike English which uses prepositions.
This can also lead to long range movements de-
pending on the length of the noun phrase that the
post-position follows. The order of noun phrases
and prepositional phrases is also swapped in Urdu
as compared with English.
3 Reordering model
In this section we briefly describe the reordering
model (Visweswariah et al, 2011) that forms the
basis of our work. We also describe an approx-
imation we make in the training process that sig-
nificantly speeds up the training without much loss
of accuracy which enables training on much larger
data sets. Consider a source sentence w that we
would like to reorder to match the target order. Let
pi represent a candidate permutation of the source
sentence w. pii denotes the index of the word in the
source sentence that maps to position i in the can-
didate reordering, thus reordering with this candi-
date permutation pi we will reorder the sentence
w to wpi1 , wpi2 , ..., wpin . The reordering model we
use assigns costs to candidate permutations as:
C(pi|w) =
?
i
c(pii?1, pii).
The costs c(m,n) are pairwise costs of putting
wm immediately before wn in the reordering. We
reorder the sentence w according to the permu-
tation pi that minimizes the cost C(pi|w). We
find the minimal cost permutation by converting
the problem into a symmetric Travelling Salesman
Problem (TSP) and then using an implementation
of the chained Lin-Kernighan heuristic (Applegate
et al, 2003). The costs in the reordering model
c(m,n) are parameterized by a linear model:
c(m,n) = ?T?(w,m, n)
where ? is a learned vector of weights and ? is a
vector of binary feature functions that inspect the
words and POS tags of the source sentence at and
around positions m and n. We use the features
(?) described in Visweswariah et al (2011) that
were based on features used in dependency pars-
ing (McDonald et al, 2005a).
To learn the weight vector ? we require a cor-
pus of sentences w with their desired reorderings
pi?. Past work Visweswariah et al (2011) used
high quality manual word alignments to derive the
desired reorderings pi? as follows. Given word
aligned source and target sentences, we drop the
source words that are not aligned1. Let mi be the
mean of the target word positions that the source
word at index i is aligned to. We then sort the
source indices in increasing order ofmi (this order
defines pi?). If mi = mj (for example, because wi
and wj are aligned to the same set of words) we
keep them in the same order that they occurred in
the source sentence.
We used the single best Margin Infused Relaxed
Algorithm (MIRA) (McDonald et al (2005b),
Crammer and Singer (2003)) with online updates
to our parameters given by:
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w) ? L(pi?, p?i).
In the equation above, p?i = argminpi C(pi|w) is
the best reordering based on the current parameter
value ?i and L is a loss function. We take L to be
the number of words for which the hypothesized
permutation p?i has a different preceding word as
compared with the reference permutation pi?.
In this paper we focus on the case where in ad-
dition to using a relatively small number of man-
ual word aligned sentences to derive the refer-
ence permutations pi? used to train our model,
we would like to use more abundant but nois-
ier machine aligned sentence pairs. To handle
the larger amount of training data we obtain from
machine alignments, we make an approximation
in training that we found empirically to not af-
fect performance but that makes training faster
by more than a factor of five. This allows us
to train the reordering model with roughly 150K
sentences in about two hours. The approximation
we make is that instead of using the chained Lin-
Kernighan heuristic to solve the TSP problem to
find p?i = argminpi C(pi|w), we select greedily
for each word the preceding word that has the low-
est cost2. Using ?i to denote argminj c(j, i) and
letting
C(?|w) =
?
i
c(?i, i),
1Note that the unaligned source words are dropped only at
the time of training. At the time of testing all source words are
retained as the alignment information is obviously not avail-
able at test time.
2It should be noted that this approximation was done only
at the time of training. At the time of testing we still use the
chained Lin-Kernighan heuristic to solve the TSP problem.
1277
we do the update according to:
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(?|w) ? L(pi?,?).
Again the loss L(pi?,?) is the number of positions
i for which pi?i?1 is different from ?i?1.
4 Generating reference reordering from
parallel sentences
The main aim of our work is to improve the re-
ordering model by using parallel sentences for
which manual word alignments are not avail-
able. In other words, we want to generate rel-
atively clean reference reorderings from parallel
sentences and use them for training a reordering
model. A straightforward approach for this is to
use a supervised aligner to align the words in the
sentences and then derive the reference reordering
as we do for manual word alignments. However,
as we will see in the experimental results, the qual-
ity of a reordering model trained from automatic
alignments is very sensitive to the quality of align-
ments. This motivated us to explore if we can fur-
ther improve our aligner and the method for gen-
erating reference reorderings given alignments.
We improve upon the above mentioned ba-
sic approach by coupling the tasks of reorder-
ing and word alignment. We do this by build-
ing a reordering model (C(pis|ws,wt,a)) that
scores reorderings pis given the source sentence
ws, target sentence wt and machine alignments
a. Complementing this model, we build an align-
ment model (P (a|ws,wt,pis,pit)) that scores
alignments a given the source and target sen-
tences and their predicted reorderings according to
source and target reordering models. The model
(C(pis|ws,wt,a)) helps to produce better refer-
ence reorderings for training our final reordering
model given fixed machine alignments and the
alignment model (P (a|ws,wt,pis,pit)) helps im-
prove the machine alignments taking into account
information from reordering models. In the fol-
lowing sections, we describe our overall approach
followed by a description of the two models.
4.1 Overall approach to generating training
data
We first describe our overall approach to gen-
erating training data for the reordering model
given a small corpus of sentences with manual
C(pis|ws) C(pit|wt)
Step 1: Train reordering models
using manual word alignments
P (a|ws,wt, pis, pit)
C(pis|ws,a) C(pit|wt,a)
Step 2: Feed predictions
of the reordering models
to the alignment model
Step 3: Feed predictions
of the alignment model
to the reordering models
Figure 2: Overall approach: Building a sequence
of reordering and alignment models.
word alignments (H) and a much larger corpus of
parallel sentences (U ) that are not word aligned.
The basic idea is to chain together the two models,
viz., reordering model and alignment model, as
illustrated in Figure 2. The steps involved are as
described below:
Step 1: First, we use manual word alignments
(H) to train source and target reordering models
as described in (Visweswariah et al, 2011).
Step 2: Next, we use the hand alignments to train
an alignment model P (a|ws,wt,pis,pit). In
addition to the original source and target sentence,
we also feed the predictions of the reordering
model trained in Step 1 to this alignment model
(see section 4.2 for details of the model itself).
Step 3: Finally, we use the predictions of the
alignment model trained in Step 2 to train reorder-
ing models C(pis|ws,wt,a) (see section 4.3 for
details on the reordering model itself).
After building the sequence of models shown in
Figure 2, we apply them in sequence on the un-
aligned parallel data U , starting with the reorder-
ing models C(pis|ws) and C(pit|wt). The re-
orderings obtained for the source side in U (after
applying the final model C(pis|ws,a)) are used
along with reference reorderings obtained from
the manual word alignments to train our reorder-
ing model. Note that, in theory, we could iterate
over steps 2 and 3 several times but, in practice
we did not see a benefit of going beyond one iter-
1278
ation in our experiments. Also, since we are inter-
ested only in the source side reorderings produced
by the model C(pis|ws,a), the target reordering
model C(pit|wt,a) is needed only if we iterate
over steps 2 and 3.
We now point to some practical considerations
of our approach. Consider the case when we are
training an alignment model conditioned on re-
orderings (P (a|ws,wt,pis,pit)). If the reorder-
ing model that generated these reorderings pis,pit
were trained on the same data that we are using
to train the alignment model, then the reorder-
ings would be much better than we would ex-
pect on unseen test data, and hence the align-
ment model (P (a|ws,wt,pis,pit)) may learn to
make the alignment overly consistent with the re-
orderings pis and pit. To counter this problem,
we divide the training data H into K parts and
at each stage we apply a model (reordering or
alignment) on part i that had not seen part i in
training. This ensures that the alignment model
does not see very optimistic reorderings and vice
versa. We now describe the individual models,
viz., P (a|ws,wt,pis,pit) and C(pis|ws,a).
4.2 Modeling alignments given reordering
In this section we describe how we fuse informa-
tion from source and target reordering models to
improve word alignments.
As a base model we use the correction model
for word alignments proposed by McCarley et
al. (2011). This model was significantly better
than the MaxEnt aligner (Ittycheriah and Roukos,
2005) and is also flexible in the sense that it allows
for arbitrary features to be introduced while still
keeping training and decoding tractable by using a
greedy decoding algorithm that explores potential
alignments in a small neighborhood of the current
alignment. The model thus needs a reasonably
good initial alignment to start with for which we
use the MaxEnt aligner (Ittycheriah and Roukos,
2005) as in McCarley et al (2011).
The correction model is a log-linear model:
P (a|ws,wt) = exp(?
T?(a,ws,wt))
Z(ws,wt) .
The ?s are trained using the LBFGS algorithm
(Liu et al, 1989) to maximize the log-likelihood
smoothed with L2 regularization. The feature
functions ? we start with are those used in Mc-
Carley et al (2011) and include features encoding
the Model 1 probabilities between pairs of words
linked in the alignment a, features that inspect
source and target POS tags and parses (if avail-
able) and features that inspect the alignments of
adjacent words in the source and target sentence.
To incorporate information from the reorder-
ing model, we add features that use the predicted
source pis and target permutations pit. We intro-
duce some notation to describe these features. Let
Sm and Sn be the set of indices of target words
thatwsm andwsn are aligned to respectively. We de-
fine the minimum signed distance (msd) between
these two sets as:
msd(Sm, Sn) = i? ? j?
where, (i?, j?) = arg min
(i,j)?Sm?Sn
|i? j|
We quantize and encode with binary features
the minimum signed distance between the sets of
the indices of the target words that source words
adjacent in the reordering pis (wspisi and wspisi+1) are
aligned to. We instantiate similar features with the
roles of source and target sentences reversed. With
this addition of features we use the same training
and testing procedure as in McCarley et al (2011).
If the reorderings pis were perfect we would learn
to only allow alignments where wspisi and w
s
pisi+1
were aligned to adjacent words in the target sen-
tence. Although the reordering model is not per-
fect, preferring alignments consistent with the re-
ordering models improves the aligner.
4.3 Modeling reordering given alignments
To model source permutations given source (ws)
and target (wt) sentences, and alignments (a) we
reuse the reordering model framework described
in Section 3 adding additional features capturing
the relation between a hypothesized permutation
pi and alignments a. To allow for searching via
the same TSP formulation we once again assign
costs to candidate permutations as:
C(pis|ws,wt,a) =
?
i
c(pii?1, pii|ws,a).
Note that we introduce a dependence on the target
sentence wt only through the alignment a. Once
again we parameterize the costs by a linear model:
c(m,n) = ?T?(ws,a,m, n).
For the feature functions ?, in addition to the
features that only depend on ws,m, n (that we
1279
use in our standard reordering model) we add
binary indicator features based on msd(Sm, Sn)
and msd(Sm, Sn) conjoined with POS(wsm) and
POS(wsn).
Here, Sm and Sn are the set of indices of tar-
get words that wsm and wsn are aligned to respec-
tively. We conjoin the msd (minimum signed dis-
tance) with the POS tags to allow the model to cap-
ture the fact that the alignment error rate maybe
higher for some POS tags than others (e.g., we
have observed verbs have a higher error rate in
Urdu-English alignments).
Given these features we train the parameters ?
using the MIRA algorithm as described in Sec-
tion 3. Using this model, we can find the low-
est cost permutation C(pis|ws,a) using the Lin-
Kernighan heuristic as described in Section 3.
This model allows us to combine features from
the original reordering model along with informa-
tion coming from the alignments to find source re-
orderings given a parallel corpus and alignments.
We will see in the experimental section that this
improves upon the simple heuristic for deriving re-
orderings described in Section 3.
5 Experimental setup
In this section we describe the experimental setup
that we used to evaluate the models proposed in
this paper. All experiments were done on Urdu-
English and we evaluate reordering in two ways:
Firstly, we evaluate reordering performance di-
rectly by comparing the reordered source sentence
in Urdu with a reference reordering obtained from
the manual word alignments using BLEU (Pap-
ineni et al, 2002) (we call this measure monolin-
gual BLEU or mBLEU). All mBLEU results are
reported on a small test set of about 400 sentences
set aside from our set of sentences with manual
word alignments. Additionally, we evaluate the ef-
fect of reordering on our final systems for machine
translation measured using BLEU.
We use about 10K sentences (180K words) of
manual word alignments which were created in
house using part of the NIST MT-08 training data3
to train our baseline reordering model and to train
our supervised machine aligners. We use a parallel
corpus of 3.9M words consisting of 1.7M words
from the NIST MT-08 training data set and 2.2M
words extracted from parallel news stories on the
3http://www.ldc.upenn.edu
web4. The parallel corpus is used for building our
phrased based machine translation system and to
add training data for our reordering model. For
our English language model, we use the Gigaword
English corpus in addition to the English side of
our parallel corpus. Our Part-of-Speech tagger is
a Maximum Entropy Markov model tagger trained
on roughly fifty thousand words from the CRULP
corpus (Hussain, 2008).
For our machine translation experiments, we
used a standard phrase based system (Al-Onaizan
and Papineni, 2006) with a lexicalized distortion
model with a window size of +/-4 words5. To
extract phrases we use HMM alignments along
with higher quality alignments from a supervised
aligner (McCarley et al, 2011). We report results
on the (four reference) NIST MT-08 evaluation set
in Table 4 for the News and Web conditions. The
News and Web conditions each contain roughly
20K words in the test set, with the Web condition
containing more informal text from the web.
6 Results and Discussions
We now discuss the results of our experiments.
Need for additional data: We first show the need
for additional data in Urdu-English reordering.
Column 2 of Table 1 shows mBLEU as a function
of the number of sentences with manual word
alignments that are used to train the reordering
model. We see a roughly 3 mBLEU points drop
in performance per halving of data indicating a
potential for improvement by adding more data.
Using fewer features: We compare the perfor-
mance of a model trained using lexical features
for all words (Column 2 of Table 1) with a model
trained using lexical features only for the 1000
most frequent words (Column 3 of Table 1). The
motivation for this is to explore if a good model
can be learned even from a small amount of data if
we restrict the number of features in a reasonable
manner. However, we see that even with only
2.4K sentences with manual word alignments our
model benefits from lexical identities of more
than the 1000 most frequent words.
Effect of quality of machine alignments: We
next look at the use of automatically generated
4http://centralasiaonline.com
5Note that the same window size of +/-4 words was used
for all the systems, i.e., the baseline system as well as the
systems using different preordering techniques.
1280
Data size All features Frequent lex only
10K 52.5 50.8
5K 49.6 49.0
2.5K 46.6 46.2
Table 1: mBLEU scores for Urdu to English re-
ordering using different number of sentences of
manually word aligned training data with all fea-
tures and with lexical features instantiated only for
the 1000 most frequent words.
machine alignments to train the reordering model
and see the effect of aligner quality on the re-
ordering model generated using this data. These
experiments also form the baseline for the mod-
els we propose in this paper to clean up align-
ments. We experimented with two different super-
vised aligners : a maximum entropy aligner (Itty-
cheriah and Roukos, 2005) and an improved cor-
rection model that corrects the maximum entropy
alignments (McCarley et al, 2011).
Aligner Train size mBLEU
Type f-Measure (words)
None - 35.5
Manual 180K 52.5
MaxEnt 70.0 3.9M 49.5
Correction model 78.1 3.9M 55.1
Table 2: mBLEU scores for Urdu to English re-
ordering using models trained on different data
sources and tested on a development set of 8017
Urdu tokens.
Table 2 shows mBLEU scores when the re-
ordering model is trained on reordering references
created from aligners with different quality. We
see that the quality of the alignments matter a
great deal to the reordering model; using MaxEnt
alignments cause a degradation in performance
over just using a small set of manual word align-
ments. The alignments obtained using the aligner
of McCarley et al (2011) are of much better
quality and hence give higher reordering perfor-
mance. Note that this reordering performance
is much better than that obtained using manual
word alignments because the size of machine
alignments is much larger (3.9M v/s 180K words).
Improvements in reordering performance us-
ing the proposed models: Table 3 shows im-
provements in the reordering model when using
the models proposed in this paper. We useH to re-
fer to the manually word aligned data and U to re-
fer to the additional sentence pairs for which man-
ual word alignments are not available. We report
the following numbers :
1. Base correction model: This is the baseline
where we use the correction model of McCar-
ley et al (2011) for generating word alignments.
The f-Measure of this aligner is 78.1% (see row
1, column 2). Corresponding to this, we also re-
port the baseline for our reordering experiments
in the third column. Here, we first generate word
alignments for U using the aligner of McCarley et
al. (2011) and then extract reference reorderings
from these alignments. We then combine these
reference reorderings with the reference reorder-
ings derived fromH and use this combined data to
train a reordering model which serves as the base-
line (mBLEU = 55.1).
2. Correction model, C(pi|a): Here, once again
we generate alignments for U using the correc-
tion model of McCarley et al (2011). However,
instead of using the basic approach of extracting
reference reorderings, we use our improved model
C(pi|a) to generate reference reorderings from U .
These reference reorderings are again combined
with the reference reorderings derived fromH and
used to train a reordering model (mBLEU = 56.4).
3. P (a|pi), C(pi|a): Here, we build the entire se-
quence of models shown in Figure 2. The align-
ment model P (a|pi) is first improved by using pre-
dictions from the reordering model. These im-
proved alignments are then used to extract better
reference reorderings from U using C(pi|a).
We see substantial improvements over simply
adding in the data from the machine alignments.
Improvements come roughly in equal parts from
the two techniques we proposed in this paper : (i)
using a model to generate reference reorderings
from noisy alignments and (ii) using reordering in-
formation to improve the aligner.
Method f-Measure mBLEU
Base Correction model 78.1 55.1
Correction model, C(pi|a) 78.1 56.4
P (a|pi), C(pi|a) 80.7 57.6
Table 3: mBLEU with different methods to gener-
ate reordering model training data from a machine
aligned parallel corpus in addition to manual word
alignments.
Improvements in MT performance using the
proposed models: We report results for a phrase
based system with different preordering tech-
niques. For results including a reordering model,
we simply reorder the source side Urdu data both
while training and at test time. In addition to
1281
phrase based systems with different preordering
methods, we also report on a hierarchical phrase
based system for which we used Joshua 4.0 (Gan-
itkevitch et al, 2012). We see a significant gain of
1.8 BLEU points in machine translation by going
beyond manual word alignments using the best re-
ordering model reported in Table 3. We also note a
gain of 2.0 BLEU points over a hierarchical phrase
based system.
System type MT-08 evalWeb News All
Baseline (no preordering) 18.4 25.6 22.2
Hierarchical phrase based 19.6 30.7 25.4
Reordering: Manual alignments 20.7 30.0 25.6
+ Machine alignments simple 21.3 30.9 26.4
+ machine alignments, model based 22.1 32.2 27.4
Table 4: MT performance without preordering
(phrase based and hierarchical phrase based),
and with reordering models using different data
sources (phrase based).
7 Related work
Dealing with the problem of handling word order
differences in machine translation has recently re-
ceived much attention. The approaches proposed
for solving this problem can be broadly divided
into 3 sets as discussed below.
The first set of approaches handle the reorder-
ing problem as part of the decoding process. Hier-
archical models (Chiang, 2007) and syntax based
models (Yamada and Knight, 2002; Galley et
al., 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006) improve upon the simpler phrase
based models but with significant additional com-
putational cost (compared with phrase based sys-
tems) due to the inclusion of chart based parsing in
the decoding process. Syntax based models also
require a high quality source or target language
parser.
The second set of approaches rely on a source
language parser and treat reordering as a separate
process that is applied on the source language sen-
tence at training and test time before using a stan-
dard approach to machine translation. Preordering
the source data with hand written or automatically
learned rules is effective and efficient (Collins
et al, 2005; Wang et al, 2007; Ramanathan et
al., 2009; Xia and McCord, 2004; Genzel, 2010;
Visweswariah et al, 2010) but requires a source
language parser.
Recent approaches that avoid the need for a
source or target language parser and retain the ef-
ficiency of preordering models were proposed in
(Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al, 2011; Neubig
et al, 2012). (DeNero and Uszkoreit, 2011;
Visweswariah et al, 2011; Neubig et al, 2012) fo-
cus on the use of manual word alignments to learn
preordering models and in both cases no benefit
was obtained by using the parallel corpus in ad-
dition to manual word alignments. Our work is
an extension of Visweswariah et al (2011) and
we focus on being able to incorporate relatively
noisy machine alignments to improve the reorder-
ing model.
In addition to being related to work in reorder-
ing, our work is also more broadly related to sev-
eral other efforts which we now outline. Seti-
awan et al (2010) proposed the use of function
word reordering to improve alignments. While
this work is similar to one of our models (model
of alignments given reordering) we differ in us-
ing a reordering model of all words (not just func-
tion words) and both source and target sentences
(not just the source sentence). The task of directly
learning a reordering model for language pairs that
are very different is closely related to the task of
parsing and hence work on semi-supervised pars-
ing (Koo et al, 2008; McClosky et al, 2006;
Suzuki et al, 2009) is broadly related to our work.
Our work coupling reordering and alignments is
also similar in spirit to approaches where parsing
and alignment are coupled (Wu, 1997).
8 Conclusion
In the paper we showed that a reordering model
can benefit from data beyond a relatively small
corpus of manual word alignments. We proposed
a model that scores reorderings given alignments
and the source sentence that we use to gener-
ate cleaner training data from noisy alignments.
We also proposed a model that scores alignments
given source and target sentence reorderings that
improves a supervised alignment model by 2.6
points in f-Measure. While the improvement in
alignment performance is modest, the improve-
ment does result in improved reordering models.
Cumulatively, we see a gain of 1.8 BLEU points
over a baseline reordering model that only uses
manual word alignments, a gain of 2.0 BLEU
points over a hierarchical phrase based system,
and a gain of 5.2 BLEU points over a phrase based
1282
system that uses no source preordering on a pub-
licly available Urdu-English test set.
As future work we would like to evaluate our
models on other language pairs. Another avenue
of future work we would like to explore is the use
of monolingual source and target data to further
assist the reordering model. We hope to be able to
learn lexical information such as how many argu-
ments a verb takes, what nouns are potential sub-
jects for a given verb by gathering statistics from
an English parser and projecting to the source lan-
guage via our word/phrase translation table.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
David Applegate, William Cook, and Andre Rohe.
2003. Chained lin-kernighan for large traveling
salesman problems. In INFORMS Journal On Com-
puting.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228, June.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
John DeNero and Jakob Uszkoreit. 2011. Inducing
sentence structure from parallel corpora for reorder-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 193?203, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961?968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283?291, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL, pages 595?603.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On
the limited memory bfgs method for large scale op-
timization. Mathematical Programming, 45:503?
528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 609?616,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim
Roukos, Bing Xiang, and Jian-ming Xu. 2011. A
correction model for word alignments. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 889?
898, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
1283
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In Pro-
ceedings of ACL-IJCNLP.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 534?544, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 551?560, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 486?496,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403, September.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation.
1284
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 155?164,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Solution Post Identification from Discussion Forums
Deepak P
IBM Research - India
Bangalore, India
deepak.s.p@in.ibm.com
Karthik Visweswariah
IBM Research - India
Bangalore, India
v-karthik@in.ibm.com
Abstract
Discussion forums have evolved into a de-
pendable source of knowledge to solve
common problems. However, only a mi-
nority of the posts in discussion forums
are solution posts. Identifying solution
posts from discussion forums, hence, is an
important research problem. In this pa-
per, we present a technique for unsuper-
vised solution post identification leverag-
ing a so far unexplored textual feature, that
of lexical correlations between problems
and solutions. We use translation mod-
els and language models to exploit lex-
ical correlations and solution post char-
acter respectively. Our technique is de-
signed to not rely much on structural fea-
tures such as post metadata since such
features are often not uniformly available
across forums. Our clustering-based itera-
tive solution identification approach based
on the EM-formulation performs favor-
ably in an empirical evaluation, beating
the only unsupervised solution identifica-
tion technique from literature by a very
large margin. We also show that our unsu-
pervised technique is competitive against
methods that require supervision, outper-
forming one such technique comfortably.
1 Introduction
Discussion forums have become a popular knowl-
edge source for finding solutions to common prob-
lems. StackOverflow
1
, a popular discussion forum
for programmers is among the top-100 most vis-
ited sites globally
2
. Now, there are discussion fo-
rums for almost every major product ranging from
1
http://www.stackoverflow.com
2
http://www.alexa.com/siteinfo/stackoverflow.com
automobiles
3
to gadgets such as those of Mac
4
or
Samsung
5
. These typically start with a registered
user posting a question/problem
6
to which other
users respond. Typical response posts include so-
lutions or clarification requests, whereas feedback
posts form another major category of forum posts.
As is the case with any community of humans,
discussion forums have their share of inflamma-
tory remarks too. Mining problem-solution pairs
from discussion forums has attracted much atten-
tion from the scholarly community in the recent
past. Since the first post most usually contains
the problem description, identifying its solutions
from among the other posts in the thread has been
the focus of many recent efforts (e.g., (Gandhe et
al., 2012; Hong and Davison, 2009)). Extract-
ing problem-solution pairs from forums enables
the usage of such knowledge in knowledge reuse
frameworks such as case-based reasoning (Kolod-
ner, 1992) that use problem-solution pairs as raw
material. In this paper, we address the problem
of unsupervised solution post identification
7
from
discussion forums.
Among the first papers to address the solution
identification problem was the unsupervised ap-
proach proposed by (Cong et al, 2008). It em-
ploys a graph propagation method that prioritizes
posts that are (a) more similar to the problem post,
(b) more similar to other posts, and (c) authored
by a more authoritative user, to be labeled as so-
lution posts. Though seen to be effective in iden-
tifying solutions from travel forums, the first two
assumptions, (a) and (b), were seen to be not very
3
http://www.cadillacforums.com/
4
https://discussions.apple.com/
5
http://www.galaxyforums.net/
6
We use problem and question, as well as solution and
answer interchangeably in this paper.
7
This problem has been referred to as answer extraction
by some papers earlier. However, we use solution identifica-
tion to refer to the problem since answer and extraction have
other connotations in the Question-Answering and Informa-
tion Extraction communities respectively.
155
reliable in solution identification in other kinds of
discussion boards. (Catherine et al, 2012) reports
a study that illustrates that non-solution posts are,
on an average, as similar to the problem as solution
posts in technical forums. The second assump-
tion (i.e., (b) above) was also not seen to be use-
ful in discussion forums since posts that are highly
similar to other posts were seen to be complaints,
repetitive content being more pervasive among
complaint posts than solutions (Catherine et al,
2013). Having exhausted the two obvious textual
features for solution identification, subsequent ap-
proaches have largely used the presence of lexi-
cal cues signifying solution-like narrative (e.g., in-
structive narratives such as ?check the router for
any connection issues?) as the primary content-
based feature for solution identification.
All solution identification approaches
since (Cong et al, 2008) have used super-
vised methods that require training data in the
form of labeled solution and non-solution posts.
The techniques differ from one another mostly
in the non-textual features that are employed in
representing posts. A variety of high precision as-
sumptions such as solution post typically follows
a problem post (Qu and Liu, 2011), solution posts
are likely to be within the first few posts, solution
posts are likely to have been acknowledged by
the problem post author (Catherine et al, 2012),
users with high authoritativeness are likely to
author solutions (Hong and Davison, 2009), and
so on have been seen to be useful in solution
identification. Being supervised methods, the
above assumptions are implicitly factored in
by including the appropriate feature (e.g., post
position in thread) in the feature space so that the
learner may learn the correlation (e.g., solution
posts typically are among the first few posts)
using the training data. Though such assumptions
on structural features, if generic enough, may be
built into unsupervised techniques to aid solution
identification, the variation in availability of
such features across forums limits the usage of
models that rely heavily on structural features.
For example, some forums employ chronological
order based flattening of threads (Seo et al, 2009)
making reply-to information unavailable; models
that harness reply-to features would then have
limited utility on identifying solutions within
such flattened threads. On medical forums,
privacy considerations may force forum data to
be dumped without author information, making a
host of author-id based features unavailable. On
datasets that contain data from across forums,
the model may have to be aware of the absence
of certain features in subsets of the data, or be
modeled using features that are available on all
threads.
Our Contribution: We propose an unsuper-
vised method for solution identification. The cor-
nerstone of our technique is the usage of a hith-
erto unexplored textual feature, lexical correla-
tions between problems and solutions, that is ex-
ploited along with language model based charac-
terization of solution posts. We model the lexical
correlation and solution post character using reg-
ularized translation models and unigram language
models respectively. To keep our technique appli-
cable across a large variety of forums with vary-
ing availability of non-textual features, we design
it to be able to work with minimal availability of
non-textual features. In particular, we show that
by using post position as the only non-textual fea-
ture, we are able to achieve accuracies compara-
ble to supervision-based approaches that use many
structural features (Catherine et al, 2013).
2 Related Work
In this section, we provide a brief overview of pre-
vious work related to our problem. Though most
of the answer/solution identification approaches
proposed so far in literature are supervised meth-
ods that require a labeled training corpus, there are
a few that require limited or no supervision. Ta-
ble 1 provides an overview of some of the more
recent solution identification techniques from lit-
erature, with a focus on some features that we wish
to highlight. The common observation that most
problem-solving discussion threads have a prob-
lem description in the first post has been explic-
itly factored into many techniques; knowing the
problem/question is important for solution iden-
tification since author relations between problem
and other posts provide valuable cues for solution
identification. Most techniques use a variety of
such features as noted in Section 1. SVMs have
been the most popular method for supervised and
semi-supervised learning for the task of solution
identification.
Of particular interest to us are approaches that
use limited or no supervision, since we focus on
unsupervised solution identification in this paper.
156
Paper Reference Supervision Assumptions on Features other than Learning
Problem Position Post Content Used Technique
(Qu and Liu, 2011) Supervised First Post likely HMM assumes Naive Bayes
to be problem solution follows problem & HMM
(Ding et al, 2008) Supervised First Post Post Position, Author, CRFs
Context Posts
(Kim et al, 2010) Supervised None Post Position, Author, MaxEnt,
Previous Posts, Profile etc. SVM, CRF
(Hong and Davison, 2009) Supervised First Post Post Position, Author, SVM
Author Authority
(Catherine et al, 2012) Supervised First Post Post Position, Author, Problem SVM
Author?s activities wrt Post
(Catherine et al, 2013) Limited First Post Post Position/Rating, Author, SVMs &
Supervision Author Rating, Post Ack Co-Training
(Cong et al, 2008) Unsupervised None Author, Author Authority, Graph
Relation to Problem Author Propagation
Our Method Unsupervised First Post Post Position Translation
Models & LM
Table 1: Summary of Some Solution Identification Techniquess
The only unsupervised approach for the task, that
from (Cong et al, 2008), uses a graph propaga-
tion method on a graph modeled using posts as
vertices, and relies on the assumptions that posts
that bear high similarity to the problem and other
posts and those authored by authoritative users are
more likely to be solution posts. Some of those as-
sumptions, as mentioned in Section 1, were later
found to be not generalizable to beyond travel fo-
rums. The semi-supervised approach presented
in (Catherine et al, 2013) uses a few labeled
threads to bootstrap SVM based learners which are
then co-trained in an iterative fashion. In addition
to various features explored in literature, they use
acknowledgement modeling so that posts that have
been acknowledged positively may be favored for
being labeled as solutions.
We will use translation and language models
in our method for solution identification. Usage
of translation models for modeling the correlation
between textual problems and solutions have been
explored earlier starting from the answer retrieval
work in (Xue et al, 2008) where new queries were
conceptually expanded using the translation model
to improve retrieval. Translation models were also
seen to be useful in segmenting incident reports
into the problem and solution parts (Deepak et al,
2012); we will use an adaptation of the generative
model presented therein, for our solution extrac-
tion formulation. Entity-level translation models
were recently shown to be useful in modeling cor-
relations in QA archives (Singh, 2012).
3 Problem Definition
Let a thread T from a discussion forum be made
up of t posts. Since we assume, much like
many other earlier papers, that the first post is
the problem post, the task is to identify which
among the remaining t ? 1 posts are solutions.
There could be multiple (most likely, different)
solutions within the same thread. We may now
model the thread T as t ? 1 post pairs, each
pair having the problem post as the first element,
and one of the t ? 1 remaining posts (i.e., re-
ply posts in T ) as the second element. Let C =
{(p
1
, r
1
), (p
2
, r
2
), . . . , (p
n
, r
n
)} be the set of such
problem-reply pairs from across threads in the dis-
cussion forum. We are interested in finding a sub-
set C
?
of C such that most of the pairs in C
?
are
problem-solution pairs, and most of those in C?C
?
are not so. In short, we would like to find problem-
solution pairs from C such that the F-measure
8
for
solution identification is maximized.
4 Our Approach
4.1 The Correlation Assumption
Central to our approach is the assumption of lex-
ical correlation between the problem and solution
8
http://en.wikipedia.org/wiki/F1 score
157
texts. At the word level, this translates to assum-
ing that there exist word pairs such that the pres-
ence of the first word in the problem part pre-
dicts the presence/absence of the second word in
the solution part well. Though not yet harnessed
for solution identification, the correlation assump-
tion is not at all novel. Infact, the assumption
that similar problems have similar solutions (of
which the correlation assumption is an offshoot)
forms the foundation of case-based reasoning sys-
tems (Kolodner, 1992), a kind of knowledge reuse
systems that could be the natural consumers of
problem-solution pairs mined from forums. The
usage of translation models in QA retrieval (Xue et
al., 2008; Singh, 2012) and segmentation (Deepak
et al, 2012) were also motivated by the correlation
assumption. We use an IBM Model 1 translation
model (Brown et al, 1990) in our technique; sim-
plistically, such a model m may be thought of as
a 2-d associative array where the value m[w
1
][w
2
]
is directly related to the probability ofw
1
occuring
in the problem when w
2
occurs in the solution.
4.2 Generative model for Solution Posts
Consider a unigram language model S
S
that mod-
els the lexical characteristics of solution posts, and
a translation model T
S
that models the lexical cor-
relation between problems and solutions. Our gen-
erative model models the reply part of a (p, r) pair
(in which r is a solution) as being generated from
the statistical models in {S
S
, T
S
} as follows.
? For each word w
s
occuring in r,
1. Choose z ? U(0, 1)
2. If z ? ?, Choose w ? Mult(S
S
)
3. Else, Choose w ? Mult(T
p
S
)
where T
p
S
denotes the multionomial distribu-
tion obtained from T
S
conditioned over the words
in the post p; this is obtained by assigning each
candidate solution word w a weight equal to
avg{T
S
[w
?
][w]|w
?
? p}, and normalizing such
weights across all solution words. In short, each
solution word is assumed to be generated from
the language model or the translation model (con-
ditioned on the problem words) with a probabil-
ity of ? and 1 ? ? respectively, thus accounting
for the correlation assumption. The generative
model above is similar to the proposal in (Deepak
et al, 2012), adapted suitably for our scenario. We
model non-solution posts similarly with the sole
difference being that they would be sampled from
the analogous models S
N
and T
N
that characterize
behavior of non-solution posts.
Example: Consider the following illustrative
example of a problem and solution post:
? Problem: I am unable to surf the web on the
BT public wifi.
? Solution: Maybe, you should try disconnect-
ing and rejoining the network.
Of the solution words above, generic words
such as try and should could probably be ex-
plained by (i.e., sampled from) the solution lan-
guage model, whereas disconnect and rejoin could
be correlated well with surf and wifi and hence are
more likely to be supported better by the transla-
tion model.
4.3 Clustering-based Approach
We propose a clustering based approach so as to
cluster each of the (p, r) pairs into either the so-
lution cluster or the non-solution cluster. The ob-
jective function that we seek to maximize is the
following:
?
(p,r)?C
{
F ((p, r),S
S
, T
S
) if label((p,r))=S
F ((p, r),S
N
, T
N
) if label((p,r))=N
(1)
F ((p, r),S, T ) indicates the conformance of
the (p, r) pair (details in Section 4.3.1) with the
generative model that uses the S and T models as
the language and translation models respectively.
The clustering based approach labels each (p, r)
pair as either solution (i.e., S) or non-solution (i.e.,
N ). Since we do not know the models or the la-
belings to start with, we use an iterative approach
modeled on the EM meta-algorithm (Dempster et
al., 1977) involving iterations, each comprising of
an E-step followed by the M-step. For simplicity
and brevity, instead of deriving the EM formula-
tion, we illustrate our approach by making an anal-
ogy with the popular K-Means clustering (Mac-
Queen, 1967) algorithm that also uses the EM for-
mulation and crisp assignments of data points like
we do. K-Means is a clustering algorithm that
clusters objects represented as multi-dimensional
points into k clusters where each cluster is rep-
resented by the centroid of all its members. Each
iteration in K-Means starts off with assigning each
158
In K-Means In Our Approach
Data Multi-dimensional Points (p, r) pairs
Cluster Model Respective Centroid Vector Respective S and T Models for each cluster
Initialization Random Choice of Centroids Models learnt using (p, r) pairs labeled
using the Post Position of r
E-Step label(d) = label((p, r)) = argmax
i
F ((p, r),S
i
, T
i
)
argmin
i
dist(d, centroid
i
) (Sec 4.3.1), and learn solution word
source probabilities (Sec 4.3.2)
M-Step centroid
i
= avg{d|label(d) = i} Re-learn S
S
and T
S
using pairs labeled S
S
N
and T
N
using pairs labeled N (Sec 4.3.3)
Output The clustering of points (p, r) pairs labeled as S
Table 2: Illustrating Our Approach wrt K-Means Clustering
data object to its nearest centroid, followed by re-
computing the centroid vector based on the assign-
ments made. The analogy with K-Means is illus-
trated in Table 2.
Though the analogy in Table 2 serves to provide
a high-level picture of our approach, the details re-
quire further exposition. In short, our approach is
a 2-way clustering algorithm that uses two pairs of
models, [S
S
, T
S
] and [S
N
, T
N
], to model solution
pairs and non-solution pairs respectively. At each
iteration, the post-pairs are labeled as either solu-
tion (S) or non-solution (N ) based on which pair
of models they better conform to. Within the same
iteration, the four models are then re-learnt using
the labels and other side information. At the end
of the iterations, the pairs labeled S are output as
solution pairs. We describe the various details in
separate subsections herein.
4.3.1 E-Step: Estimating Labels
As outlined in Table 2, each (p, r) pair would
be assigned to one of the classes, solution or
non-solution, based on whether it conforms better
with the solution models (i.e., S
S
& T
S
) or non-
solution models (S
N
& T
N
), as determined using
the F ((p, r),S, T ) function, i.e.,
label((p, r)) = argmax
i?{S,N}
F ((p, r),S
i
, T
i
)
F (.) falls out of the generative model:
F ((p, r),S, T ) =
?
w?r
??S[w]+(1??)?T
p
[w]
where S[w] denotes the probability of w from
S and T
p
[w] denotes the probability of w from
the multinomial distribution derived from T con-
ditioned over the words in p, as in Section 4.2.
4.3.2 E-Step: Estimating Reply Word Source
Since the language and translation models operate
at the word level, the objective function entails that
we let the models learn based on their fractional
contribution of the words from the language and
translation models. Thus, we estimate the propor-
tional contribution of each word from the language
and translation models too, in the E-step. The frac-
tional contributions of the word w ? r in the (p, r)
pair labeled as solution (i.e., S) is as follows:
f
(p,r)
S
S
(w) =
S
S
[w]
S
S
[w] + T
p
S
[w]
f
(p,r)
T
S
(w) =
T
p
S
[w]
S
S
[w] + T
p
S
[w]
The fractional contributions are just the actual
supports for the word w, normalized by the to-
tal contribution for the word from across the two
models. Similar estimates, f
(p,r)
S
N
(.) and f
(p,r)
S
N
(.)
are made for reply words from pairs labeled N .
In our example from Section 4.2, words such as
rejoin are likely to get higher f
(p,r)
T
S
(.) scores due
to being better correlated with problem words and
consequently better supported by the translation
model; those such as try may get higher f
(p,r)
S
S
(.)
scores.
4.3.3 M-Step: Learning Models
We use the labels and reply-word source estimates
from the E-step to re-learn the language and trans-
lation models in this step. As may be obvious
from the ensuing discussion, those pairs labeled
as solution pairs are used to learn the S
S
and T
S
models and those labeled as non-solution pairs are
159
used to learn the models with subscript N . We let
each reply word contribute as much to the respec-
tive language and translation models according to
the estimates in Section 4.3.2. In our example, if
the word disconnect is assigned a source proba-
bility of 0.9 and 0.1 for the translation and lan-
guage models respectively, the virtual document-
pair from (p, r) that goes into the training of the
respective T model would assume that disconnect
occurs in r with a frequency of 0.9; similarly, the
respective S would account for disconnect with a
frequency of 0.1. Though fractional word frequen-
cies are not possible in real documents, statistical
models can accomodate such fractional frequen-
cies in a straightforward manner. The language
models are learnt only over the r parts of the (p, r)
pairs since they are meant to characterize reply be-
havior; on the other hand, translation models learn
over both p and r parts to model correlation.
Regularizing the T models: In our formula-
tion, the language and translation models may be
seen as competing for ?ownership? of reply words.
Consider the post and reply vocabularies to be
of sizes A and B respectively; then, the transla-
tion model would have A ? B variables, whereas
the unigram language model has only B variables.
This gives the translation model an implicit edge
due to having more parameters to tune to the data,
putting the language models at a disadvantage.
To level off the playing field, we use a regular-
ization
9
operation in the learning of the transla-
tion models. The IBM Model 1 learning pro-
cess uses an internal EM approach where the E-
step estimates the alignment vector for each prob-
lem word; this vector indicates the distribution of
alignments of the problem word across the solu-
tion words. In our example, an example alignment
vector for wifi could be: {rejoin : 0.4, network :
0.4, disconnect : 0.1, . . .}. Our regularization
method uses a parameter ? to discard the long tail
in the alignment vector by resetting entries hav-
ing a value ? ? to 0.0 followed by re-normalizing
the alignment vector to add up to 1.0. Such prun-
ing is performed at each iteration in the learn-
ing of the translation model, so that the following
M-steps learn the probability matrix according to
such modified alignment vectors.
The semantics of the ? parameter may be in-
9
We use the word regularization in a generic sense to
mean adapting models to avoid overfitting; in particular, it
may be noted that we are not using popular regularization
methods such as L1-regularization.
Alg. 1 Clustering-based Solution Identification
Input. C, a set of (p, r) pairs
Output. C
?
, the set of identified solution pairs
Initialization
1. ?(p, r) ? C
2. if(r.postpos = 2) label((p, r)) = S
3. else label((p, r)) = N
4. Learn S
S
& T
S
using pairs labeled S
5. Learn S
N
& T
N
using pairs labeled N
EM Iterations
6. while(not converged ?#Iterations < 10)
E-Step:
7. ?(p, r) ? C
8. label((p, r)) = argmax
i
F ((p, r),S
i
, T
i
)
9. ?w ? r
10. Estimate f
(p,r)
S
label(p,r)
(w) , f
(p,r)
T
label(p,r)
(w)
M-Step:
11. Learn S
S
& T
S
from pairs labeled S
using the f
(p,r)
S
S
(.) f
(p,r)
T
S
(.) estimates
12. Learn S
N
& T
N
from pairs labeled N
using the f
(p,r)
S
N
(.) f
(p,r)
T
N
(.) estimates
Output
13. Output (p, r) pairs from C with
label((p, r)) = S as C
?
tuitively outlined. If we would like to allow align-
ment vectors to allow a problem word to align with
upto two reply words, we would need to set ? to
a value close to 0.5(=
1
2
); ideally though, to al-
low for the mass consumed by an almost inevitable
long tail of very low values in the alignment vec-
tor, we would need to set it to slightly lower than
0.5, say 0.4.
4.3.4 Initialization
K-Means clustering mostly initializes centroid
vectors randomly; however, it is non-trivial to ini-
tialize the complex translation and language mod-
els randomly. Moreover, an initialization such that
the S
S
and T
S
models favor the solution pairs
more than the non-solution pairs is critical so that
they may progressively lean towards modeling so-
lution behaviour better across iterations. Towards
this, we make use of a structural feature; in partic-
ular, adapting the hypothesis that solutions occur
in the first N posts (Ref. (Catherine et al, 2012)),
we label the pairs that have the the reply from the
second post (note that the first post is assumed to
be the problem post) in the thread as a solution
160
post, and all others as non-solution posts. Such
an initialization along with uniform reply word
source probabilities is used to learn the initial es-
timates of the S
S
, T
S
, S
N
and T
N
models to be
used in the E-step for the first iteration. We will
show that we are able to effectively perform solu-
tion identification using our approach by exploit-
ing just one structural feature, the post position,
as above. However, we will also show that we can
exploit other features as and when available, to de-
liver higher accuracy clusterings.
4.3.5 Method Summary
The overall method comprising the steps that
have been described is presented in Algorithm 1.
The initialization using the post position (Ref.
Sec 4.3.4) is illustrated in Lines 1-5, whereas the
EM-iterations form Steps 6 through 12. Of these,
the E-step incorporates labeling (Line 8) as de-
scribed in Sec 4.3.1 and reply-word source estima-
tion (Line 10) detailed in Sec 4.3.2. The models
are then re-learnt in the M-Step (Lines 11-12) as
outlined in Sec 4.3.3. At the end of the iterations
that may run up to 10 times if the labelings do not
stabilize earlier, the pairs labeled S are output as
identified solutions (Line 13).
Time Complexity: Let n denote |C|, and the
number of unique words in each problem and re-
ply post be a and b respectively. We will de-
note the vocabulary size of problem posts as A
and that of reply posts as B. Learning of the
language and translation models in each iteration
costs O(nb + B) and O(k
?
(nab + AB)) respec-
tively (assuming the translation model learning
runs for k
?
iterations). The E-step labeling and
source estimation cost O(nab) each. For k iter-
ations of our algorithm, this leads to an overall
complexity of O(kk
?
(nab+AB)).
5 Experimental Evaluation
We use a crawl of 140k threads from Apple Dis-
cussion forums
10
. Out of these, 300 threads (com-
prising 1440 posts) were randomly chosen and
each post was manually tagged as either solution
or non-solution by the authors of (Catherine et al,
2013) (who were kind enough to share the data
with us) with an inter-annotator agreement
11
of
0.71. On an average, 40% of replies in each thread
and 77% of first replies were seen to be solutions,
10
http://discussions.apple.com
11
http://en.wikipedia.org/wiki/Cohen?s kappa
Figure 1: F% (Y) vs. #Iterations (X)
T
S
ProblemWord, SolutionWord T
S
[p][s]
network, guest 0.0754
connect, adaptor 0.0526
wireless, adaptor 0.0526
translat, shortcut 0.0492
updat, rebuilt 0.0405
S
S
SolutionWord S
S
[s]
your 0.0115
try 0.0033
router 0.0033
see 0.0033
password 0.0023
Table 4: Sample T
S
and S
S
Estimates
leading to an F-measure of 53% for our initializa-
tion heuristic. We use the F-measure
12
for solu-
tion identification, as the primary evaluation mea-
sure. While we vary the various parameters sep-
arately in order to evaluate the trends, we use a
dataset of 800 threads (containing the 300 labeled
threads) and set ? = 0.5 and ? = 0.4 unless other-
wise mentioned. Since we have only 300 labeled
threads, accuracy measures are reported on those
(like in (Catherine et al, 2013)). We pre-process
the post data by stemming words (Porter, 1980).
5.1 Quality Evaluation
In this study, we compare the performance of our
method under varying settings of ? against the
only unsupervised approach for solution identi-
fication from literature, that from (Cong et al,
2008). We use an independent implementation
of the technique using Kullback-Leibler Diver-
gence (Kullback, 1997) as the similarity measure
between posts; KL-Divergence was seen to per-
form best in the experiments reported in (Cong et
al., 2008).
Table 3 illustrates the comparative performance
12
http://en.wikipedia.org/wiki/F1 score
161
Technique Precision Recall F-Measure
Unsupervised Graph Propagation (Cong et al, 2008) 29.7 % 55.6 % 38.7 %
Our Method with only Translation Models (? = 0.0) 41.8 % 86.8 % 56.5 %
Our Method with only Language Models (? = 1.0) 63.2 % 62.1 % 62.6 %
Our Method with Both Models (? = 0.5) 61.3 % 66.9 % 64.0 %
Methods using Supervision (Catherine et al, 2013)
ANS CT 40.6 % 88.0 % 55.6 %
ANS-ACK PCT 56.8 % 84.1 % 67.8%
Table 3: Quality Evaluation
Figure 2: F% (Y) vs. ? (X) Figure 3: F% (Y) vs. ? (X) Figure 4: F% (Y) vs. #Threads (X)
on various quality metrics, of which F-Measure is
typically considered most important. Our pure-
LM
13
setting (i.e., ? = 1) was seen to perform up
to 6 F-Measure points better than the pure-TM
14
setting (i.e., ? = 0), whereas the uniform mix is
seen to be able to harness both to give a 1.4 point
(i.e., 2.2%) improvement over the pure-LM case.
The comparison with the approach from (Cong et
al., 2008) illustrates that our method is very clearly
the superior method for solution identification out-
performing the former by large margins on all the
evaluation measures, with the improvement on F-
measure being more than 25 points.
Comparison wrt Methods from (Catherine et
al., 2013): Table 3 also lists the performance of
SVM-based methods from (Catherine et al, 2013)
that use supervised information for solution iden-
tification, to help put the performance of our tech-
nique in perspective. Of the two methods therein,
ANS CT is a more general method that uses two
views (structural and lexical) of solutions which
are then co-trained. ANS-ACK PCT is an en-
hanced method that requires author-id informa-
tion and a means of classifying posts as acknowl-
edgements (which is done using additional super-
vision); a post being acknowledged by the prob-
lem author is then used as a signal to enhance
the solution-ness of a post. In the absence of
author information (such as may be common in
13
Language Model
14
Translation Model
privacy-constrained domains such as medical fo-
rums) and extrinsic information to enable identify
acknowledgements, ANS CT is the only technique
available. Our technique is seen to outperform
ANS CT by a respectable margin (8.6 F-measure
points) while trailing behind the enhanced ANS-
ACK PCT method with a reasonably narrow 3.8
F-measure point margin. Thus, our unsupervised
method is seen to be a strong competitor even for
techniques using supervision outlined in (Cather-
ine et al, 2013), illustrating the effectiveness of
LM and TM modeling of reply posts.
Across Iterations: For scenarios where com-
putation is at a premium, it is useful to know how
quickly the quality of solution identification sta-
bilizes, so that the results can be collected after
fewer iterations. Figure 1 plots the F-measure
across iterations for the run with ? = 0.5, ? = 0.4
setting, where the F-measure is seen to stabilize in
as few as 4-5 iterations. Similar trends were ob-
served for other runs as well, confirming that the
run may be stopped as early as after the fourth it-
eration without considerable loss in quality.
Example Estimates from LMs and TMs: In
order to understand the behavior of the statistical
models, we took the highest 100 entries from both
S
S
and T
S
and attempted to qualitatively evalu-
ate semantics of the words (or word pairs) corre-
sponding to those. Though the stemming made it
hard to make sense of some entries, we present
some of the understandable entries from among
162
the top-100 in Table 4. The first three entries from
T
S
deal with connection issues for which adaptor
or guest account related solutions are proposed,
whereas the remaining have something to do with
the mac translator app and rebuilding libraries af-
ter an update. The top words from S
S
include im-
perative words and words from solutions to com-
mon issues that include actions pertaining to the
router or password.
5.2 Varying Parameter Settings
We now analyse the performance of our approach
against varying parameter settings. In particular,
we vary ? and ? values and the dataset size, and
experiment with some initialization variations.
Varying ?: ? is the weighting parameter that
indicates the fraction of weight assigned to LMs
(vis-a-vis TMs). As may be seen from Figure 2,
the quality of the results as measured by the F-
measure is seen to peak around the middle (i.e.,
? = 0.5), and decline slowly towards either ex-
treme, with a sharp decline at ? = 0 (i.e., pure-
TM setting). This indicates that a uniform mix is
favorable; however, if one were to choose only one
type of model, usage of LMs is seen to be prefer-
able than TMs.
Varying ? : ? is directly related to the extent of
pruning of TMs, in the regularization operation;
all values in the alignment vector ? ? are pruned.
Thus, each problem word is roughly allowed to be
aligned with at most ?
1
?
solution words. The
trends from Figure 3 suggests that allowing a prob-
lem word to be aligned to up to 2.5 solution words
(i.e., ? = 0.4) is seen to yield the best performance
though the quality decline is graceful towards ei-
ther side of the [0.1, 0.5] range.
Varying Data Size: Though more data always
tends to be beneficial since statistical models ben-
efit from redundancy, the marginal utility of ad-
ditional data drops to very small levels beyond
a point; we are interested in the amount of data
beyond which the quality of solution identifica-
tion flattens out. Figure 4 suggests that there is
a sharp improvement in quality while increasing
the amount of data from 300 threads (i.e., 1440
(p, r) pairs) to 550 (2454 pairs), whereas the in-
crement is smaller when adding another 250 pairs
(total of 3400 pairs). Beyond 800 threads, the F-
measure was seen to flatten out rapidly and stabi-
lize at ? 64%.
Initialization: In Apple discussion forums,
posts by Apple employees that are labeled with
the Apple employees tag (approximately ? 7% of
posts in our dataset) tend to be solutions. So are
posts that are marked Helpful (? 3% of posts) by
other users. Being specific to Apple forums, we
did not use them for initialization in experiments
so far with the intent of keeping the technique
generic. However, when such posts are initial-
ized as solutions (in addition to first replies as we
did earlier), the F-score for solution identification
for our technique was seen to improve slightly, to
64.5% (from 64%). Thus, our technique is able
to exploit any extra solution identifying structural
features that are available.
6 Conclusions and Future Work
We considered the problem of unsupervised so-
lution post identification from discussion forum
threads. Towards identifying solutions to the prob-
lem posed in the initial post, we proposed the us-
age of a hitherto unexplored textual feature for
the solution identification problem; that of lexical
correlations between problems and solutions. We
model and harness lexical correlations using trans-
lation models, in the company of unigram lan-
guage models that are used to characterize reply
posts, and formulate a clustering-based EM ap-
proach for solution identification. We show that
our technique is able to effectively identify solu-
tions using just one non-content based feature, the
post position, whereas previous techniques in liter-
ature have depended heavily on structural features
(that are not always available in many forums) and
supervised information. Our technique is seen to
outperform the sole unsupervised solution identi-
fication technique in literature, by a large margin;
further, our method is even seen to be competi-
tive to recent methods that use supervision, beat-
ing one of them comfortably, and trailing another
by a narrow margin. In short, our empirical analy-
sis illustrates the superior performance and estab-
lishes our method as the method of choice for un-
supervised solution identification.
Exploration into the usage of translation models
to aid other operations in discussion forums such
as proactive word suggestions for solution author-
ing would be interesting direction for follow-up
work. Discovery of problem-solution pairs in
cases where the problem post is not known before-
hand, would be a challenging problem to address.
163
References
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Rose Catherine, Amit Singh, Rashmi Gangadharaiah,
Dinesh Raghu, and Karthik Visweswariah. 2012.
Does similarity matter? the case of answer extrac-
tion from technical discussion forums. In COLING
(Posters), pages 175?184.
Rose Catherine, Rashmi Gangadharaiah, Karthik
Visweswariah, and Dinesh Raghu. 2013. Semi-
supervised answer extraction from discussion fo-
rums. In IJCNLP.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 467?474. ACM.
P. Deepak, Karthik Visweswariah, Nirmalie Wiratunga,
and Sadiq Sani. 2012. Two-part segmentation of
text documents. In CIKM, pages 793?802.
Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), pages 1?
38.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xianyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In ACL.
Ankur Gandhe, Dinesh Raghu, and Rose Catherine.
2012. Domain adaptive answer extraction for dis-
cussion boards. In Proceedings of the 21st interna-
tional conference companion on World Wide Web,
pages 501?502. ACM.
Liangjie Hong and Brian D Davison. 2009. A
classification-based approach to question answering
in discussion boards. In Proceedings of the 32nd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 171?
178. ACM.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010.
Tagging and linking web forum posts. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 192?202. Asso-
ciation for Computational Linguistics.
Janet L Kolodner. 1992. An introduction to case-based
reasoning. Artificial Intelligence Review, 6(1):3?34.
Solomon Kullback. 1997. Information theory and
statistics. Courier Dover Publications.
James MacQueen. 1967. Some methods for classi-
fication and analysis of multivariate observations.
In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability, volume 1,
page 14. California, USA.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program: electronic library and information
systems, 14(3):130?137.
Zhonghua Qu and Yang Liu. 2011. Finding problem
solving threads in online forum. In IJCNLP, pages
1413?1417.
Jangwon Seo, W Bruce Croft, and David A Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM conference
on Information and knowledge management, pages
1907?1910. ACM.
Amit Singh. 2012. Entity based q&a retrieval. In
EMNLP-CoNLL, pages 1266?1277.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives.
In SIGIR, pages 475?482.
164
