Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 41?45,
Uppsala, July 2010.
Levels of Certainty in Knowledge-Intensive Corpora:
An Initial Annotation Study
Aron Henriksson
DSV/KTH-Stockholm University
Sweden
aronhen@dsv.su.se
Sumithra Velupillai
DSV/KTH-Stockholm University
Sweden
sumithra@dsv.su.se
Abstract
In this initial annotation study, we sug-
gest an appropriate approach for determin-
ing the level of certainty in text, including
classification into multiple levels of cer-
tainty, types of statement and indicators of
amplified certainty. A primary evaluation,
based on pairwise inter-annotator agree-
ment (IAA) using F
1
-score, is performed
on a small corpus comprising documents
from the World Bank. While IAA results
are low, the analysis will allow further re-
finement of the created guidelines.
1 Introduction
Despite ongoing efforts to codify knowledge, it is
often communicated in an informal manner. In
our choice of words and expressions, we implicitly
or explicitly judge the certainty of the knowledge
we wish to convey. This fact makes it possible
to gauge the reliability of knowledge based on the
subjective perspective of the author.
As knowledge is often difficult to ascertain, it
seems reasonable to regard knowledge on a contin-
uum of varying degrees of certainty, as opposed to
a binary (mis)conception. This corresponds to the
notion of epistemic modality: the degree of confi-
dence in, or commitment to, the truth of proposi-
tions (Hyland, 1998). Hedging is a means of af-
fecting epistemic modality by qualifying proposi-
tions, realized through tentative words and expres-
sions such as possibly and tends to.
A holistic perspective on certainty?in which
not only speculation is considered, but also signs
of increased certainty?requires a classification
into various levels. Applying such an approach
to knowledge-intensive corpora, it may in due
course be possible to evaluate unstructured, infor-
mal knowledge. This would not least be valuable
to organizational knowledge management prac-
tices, where it could provide a rough indicator of
reliability in internal knowledge audits.
2 Related Research
The hedging concept was first introduced by
Lakoff (1973) but has only really come into the
spotlight in more recent years. Studies have
mainly taken place in the biomedical domain, with
Hyland?s (1998) influential work investigating the
phenomenon in scientific research articles. Spec-
ulative keywords and negations, along with their
linguistic scopes, are annotated in the BioScope
corpus by Vincze et al (2008), which contains a
large collection of medical and biological text (sci-
entific articles and abstracts, as well as radiology
reports). After several iterations of refining their
guidelines, they report IAA values ranging from
77.6 to 92.37 F
1
-score for speculative keywords
(62.5 and 95.54 F
1
-score for full scope). This cor-
pus is freely available and has been used for train-
ing and evaluation of automatic classifiers, see e.g.
Morante and Daelemans (2009). One of the main
findings is that hedge cues are highly domain-
dependent. Automatic identification of other pri-
vate states, including opinions, represents a sim-
ilar task, see e.g. Wiebe et al (2005). Diab et
al. (2009) study annotation of committed and non-
committed belief and show that automatic tagging
of such classes is feasible. A different annotation
approach is proposed by Rubin et al (2006), in
which certainty in newspaper articles is catego-
rized along four dimensions: level, perspective, fo-
cus and time. Similarly, five dimensions are used
in Wilbur et al (2006) for the creation of an an-
notated corpus of biomedical text: focus, polarity,
certainty, evidence and directionality.
3 Method
Based on previous approaches and an extensive lit-
erature review, we propose a set of guidelines that
41
(1) incorporates some new features and (2) shifts
the perspective to suit knowledge-intensive cor-
pora, e.g. comprising organizational knowledge
documents. Besides categorization into levels of
certainty, this approach distinguishes between two
types of statement and underscores the need to
take into account words and expressions that add
certainty to a proposition.
A small corpus of 10 World Bank documents?
a publicly available resource known as Viewpoints
(The World Bank Group, 2010)?is subsequently
annotated in two sets by different annotators. The
corpus is from a slightly different domain to those
previously targeted and represents an adequate al-
ternative to knowledge documents internal to an
organization by fulfilling the criterion of knowl-
edge intensity. The process is carried out in a
Prote?ge? plugin: Knowtator (Ogren, 2006). Pair-
wise IAA, measured as F
1
-score, is calculated to
evaluate the feasibility of the approach.
Statements are annotated at the clause level, as
sentences often contain subparts subject to differ-
ent levels of certainty. These are not predefined
and the span of classes is determined by the an-
notator. Furthermore, a distinction is made be-
tween different types of statement: statements that
give an account of something, typically a report
of past events, and statements that express con-
crete knowledge claims. The rationale behind this
distinction is that text comprises statements that
make more or less claims of constituting knowl-
edge. Thus, knowledge claims?often less preva-
lent than accounts?should be given more weight
in the overall assessment, as the application lies
in automatically evaluating the reliability of infor-
mal knowledge. Assuming the view of knowledge
and certainty as continuous, it is necessary to dis-
cretize that into a number of intervals, albeit more
than two. Hence, accounts and claims are cate-
gorized according to four levels of certainty: very
certain, quite certain, quite uncertain and very un-
certain. In addition to the statement classes, four
indicators make up the total of twelve. We in-
troduce certainty amplifiers, which have received
little attention in previous work. These are lin-
guistic features that add certainty to a statement,
e.g. words like definitely and expressions like
without a shadow of a doubt. Hedging indica-
tors, on the other hand, have gained much atten-
tion recently and signify uncertainty. The source
hedge class is applicable to instances where the
source of epistemic judgement is stated explicitly,
yet only when it provides a hedging function (e.g.
some say). Modality strengtheners are features
that strengthen the effect of epistemic modality
when used in conjunction with other (un)certainty
indicators?but alone do not signify any polarity
orientation?and may be in the form of vagueness
(e.g. <could be> around that number) or quantity
gradations (e.g. very <sure>).
4 Results
The corpus contains a total of 772 sentences,
which are annotated twice: set #1 by one anno-
tator and set #2 by five annotators, annotating two
documents each. The statistics in Table 1 show
a discrepancy over the two sets in the number of
classified statements, which is likely due to diffi-
culties in determining the scope of clauses. There
are likewise significant differences in the propor-
tion between accounts and claims, as had been an-
ticipated.
Accounts Claims
Set #1 Set #2 Set #1 Set #2
726 574 395 393
Table 1: Frequencies of accounts and claims.
Despite the problem of discriminating between ac-
counts and claims, they seem to be susceptible to
varying levels of certainty. The average distribu-
tion of certainty for account statements is depicted
in Figure 1. As expected, an overwhelming ma-
jority (87%) of such statements are quite certain,
merely relating past events and established facts.
Figure 1: Average distribution of certainty in ac-
count statements.
By comparison, knowledge claims are more
commonly hedged (23%), although the majority
is still quite certain. Interestingly, claims are also
expressed with added confidence more often than
accounts?around one in every ten claims.
42
Figure 2: Average distribution of certainty in
knowledge claims.
As expected, the most common indicator is of
hedging. Common cues include may, can, might,
could, indicate(s), generally and typically. Many
of these cues are also among the most common in
the biomedical sub-corpus of BioScope (Vincze et
al., 2008). It is interesting to note the fairly com-
mon phenomenon of certainty amplifiers. These
are especially interesting, as they have not been
studied much before, although Wiebe et al (2005)
incorporate intensity ratings in their annotation
scheme. There is agreement on words like clearly,
strongly and especially.
Indicator Set #1 Set #2
Certainty amplifier 61 29
Hedging indicator 151 133
Source hedge 0 40
Modality strengthener 9 122
Table 2: Frequency of indicators
To evaluate the approach, we calculate IAA by
pairwise F
1
-score, considering set #1 as the gold
standard, i.e. as correctly classified, in relation to
which the other subsets are evaluated. We do this
for exact matches and partial matches1. For exact
matches in a single document, the F
1
-score val-
ues range from an extremely low 0.09 to a some-
what higher?although still poor?0.52, yielding
an overall average of 0.28. These results clearly
reflect the difficulty of the task, although one has
to keep in mind the impact of the discrepancy in
the number of annotations. This is partly reflected
in the higher overall average for partial matches:
0.41.
Certainty amplifiers and hedging indicators
have F
1
-scores that range up to 0.53 and 0.55 re-
spectively (ditto for partial matches) in a single
document. Over the entire corpus, however, the
1Partial matches are calculated on a character level while
exact matches are calculated on a token level.
averages come down to 0.27 for certainty ampli-
fiers (0.30 for partial matches) and 0.33 for hedg-
ing indicators (0.35 for partial matches).
Given the poor results, we want to find out
whether the main difficulty is presented by having
to judge certainty according to four levels of cer-
tainty, or whether it lies in having to distinguish
between types of statement. We therefore general-
ize the eight statement-related classes into a single
division between accounts and claims. Naturally,
the agreement is higher than for any single class,
with 0.44 for the former and 0.41 for the latter.
A substantial increase is seen in partial matches,
with 0.70 for accounts and 0.55 for claims. The
results are, however, sufficiently low to conclude
that there were real difficulties in distinguishing
between the two.
Statement Type Exact F
1
Partial F
1
Account 0.44 0.70
Claim 0.41 0.55
Table 3: Pairwise IAA per statement type, F
1
-
scores for exact and partial matches.
We subsequently generalize the eight classes into
four, according to their level of certainty alone.
The results are again low: quite certain yields
the highest agreement at 0.47 (0.76 for partial
matches), followed by quite uncertain at 0.24
(0.35 for partial matches). These numbers suggest
that this part of the task is likewise difficult. The
rise in F
1
-scores for partial matches is noteworthy,
as it highlights the problem of different interpreta-
tions of clause spans.
Certainty Level Exact F
1
Partial F
1
Very certain 0.15 0.15
Quite certain 0.47 0.76
Quite uncertain 0.24 0.35
Very uncertain 0.08 0.08
Table 4: Pairwise IAA per certainty level, F
1
-
scores for exact and partial matches
5 Discussion
In the guidelines, it is suggested that the level of
certainty can typically be gauged by identifying
the number of indicators. There is, however, a se-
rious drawback to this approach. Hedging indica-
tors, in particular, are inherently uncertain to dif-
ferent degrees. Consider the words possibly and
43
probably. According to the guidelines, a single
occurrence of either of these hedging indicators
would normally render a statement quite uncer-
tain. Giving freer hands to the annotator might
be a way to evade this problem; however, it is not
likely to lead to any more consistent annotations.
Kilicoglu and Bergler (2008) address this by as-
signing weights to hedging cues.
A constantly recurring bone of contention is
presented by the relationship between certainty
and precision. One of the hardest judgements to
make is whether imprecision, or vagueness, is a
sign of uncertainty. Consider the following exam-
ple from the corpus:
Cape Verde had virtually no private sec-
tor.
Clearly, this statement would be more certain if it
had said: Cape Verde had no private sector. How-
ever, virtually no could be substituted with, say,
a very small, in which case the statement would
surely not be deemed uncertain. Perhaps precision
is a dimension of knowledge that should be ana-
lyzed in conjunction with certainty, but be anno-
tated separately.
6 Conclusion
There are, of course, a number of ways one can
go about annotating the level of certainty from
a knowledge perspective. Some modifications to
the approach described here are essential?which
the low IAA values are testament to?while oth-
ers may be worth exploring. Below is a selection
of five key changes to the approach that may lead
to improved results:
1. Explicate statement types. Although there
seems to be a useful difference between the
two types, the distinction needs to be further
explicated in the guidelines.
2. Focus on indicators. It is clear that indicators
cannot be judged in an identical fashion only
because they have been identified as signify-
ing either certainty or uncertainty. It is not
simply the number of occurrences of indica-
tors that determines the level of certainty but
rather how strong those indicators are. A pos-
sible solution is to classify indicators accord-
ing to the level of certainty they affect.
3. Discard rare classes. Very rare phenomena
that do not have a significant impact on the
overall assessment can be sacrificed without
affecting the results negatively, which may
also make the task a little less difficult.
4. Clarify guidelines. A more general remedy
is to clarify further the guidelines, including
instructions on how to determine the scope of
clauses; alternatively, predefine them.
5. Instruct annotators. Exposing annotators
to the task would surely result in increased
agreement, in particular if they agree be-
forehand on the distinctions described in the
guidelines. At the same time, you do not
want to steer the process too much. Perhaps
the task is inherently difficult to define in de-
tail. Studies on how to exploit subjective an-
notations might be interesting to explore, see
e.g. Reidsma and op den Akker (2008).
In the attempt to gauge the reliability of knowl-
edge, incorporating multiple levels of certainty
becomes necessary, as does indicators of in-
creased certainty. Given the similar rates of
agreement on hedging indicators and certainty
amplifiers (0.33 and 0.27 respectively; 0.30 and
0.35 for partial matches), the latter class seem
to be confirmed. It is an existing and impor-
tant phenomenon, although?like hedging indica-
tors?difficult to judge. Moreover, a differentia-
tion between types of statement is important due
to their?to different degrees?varying claims of
constituting knowledge. An automatic classifier
built on such an approach could be employed with
significant benefit to organizations actively man-
aging their collective knowledge. The advantage
of being aware of the reliability of knowledge are
conceivably manifold: it could, for instance, be
(1) provided as an attribute to end-users brows-
ing documents, (2) used as metadata by search
engines, (3) used in knowledge audits and knowl-
edge gap analyses, enabling organizations to learn
when knowledge in a particular area needs to be
consolidated. It is, of course, also applicable in a
more general information extraction sense: infor-
mation that is extracted from text needs to have a
certainty indicator attached to it.
A dimension other than certainty that has a clear
impact on knowledge is precision. It would be in-
teresting to evaluate the reliability of knowledge
based on a combination of certainty and precision.
The annotated World Bank corpus will be made
available for further research on the Web.
44
References
Mona T. Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaram, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In Proceedings of the Third Linguistic Annota-
tion Workshop, ACL-IJCNLP, pages 68?73, Suntec,
Singapore, August. ACL and AFNLP.
Ken Hyland. 1998. Hedging in Scientific Research Ar-
ticles. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9.
George Lakoff. 1973. Hedges: A study in meaning
criteria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2:458?508.
Roser Morante and Walter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Philip V. Ogren. 2006. Knowtator: a prote?ge? plug-in
for annotated corpus construction. In Proceedings of
the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 273?275, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ?subjective? annotations. In HumanJudge
?08: Proceedings of the Workshop on Human Judge-
ments in Computational Linguistics, pages 8?16,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2006. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
Computing Affect and Attitutde in Text: Theory and
Applications. Springer.
The World Bank Group. 2010. Documents
& Reports. http://go.worldbank.org/
3BU2Z3YZ40, Accessed May 13, 2010.
Veronika Vincze, Gyo?rgy Szaarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39:165?210.
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC Bioinfor-
matics, 7:356+, July.
45
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 36?44,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Corpus-Driven Terminology Development: Populating Swedish
SNOMED CT with Synonyms Extracted from Electronic Health Records
Aron Henriksson1, Maria Skeppstedt1, Maria Kvist1,2, Martin Duneld1, Mike Conway3
1Department of Computer and Systems Sciences (DSV), Stockholm University, Sweden
2Department of Learning, Informatics, Management and Ethics (LIME), Karolinska Institute, Sweden
3Division of Biomedical Informatics, University of California San Diego, USA
Abstract
The various ways in which one can re-
fer to the same clinical concept needs to
be accounted for in a semantic resource
such as SNOMED CT. Developing termi-
nological resources manually is, however,
prohibitively expensive and likely to re-
sult in low coverage, especially given the
high variability of language use in clinical
text. To support this process, distributional
methods can be employed in conjunction
with a large corpus of electronic health
records to extract synonym candidates for
clinical terms. In this paper, we exem-
plify the potential of our proposed method
using the Swedish version of SNOMED
CT, which currently lacks synonyms. A
medical expert inspects two thousand term
pairs generated by two semantic spaces ?
one of which models multiword terms in
addition to single words ? for one hundred
preferred terms of the semantic types dis-
order and finding.
1 Introduction
In recent years, the adoption of standardized ter-
minologies for the representation of clinical con-
cepts ? and their textual instantiations ? has en-
abled meaning-based retrieval of information from
electronic health records (EHRs). By identify-
ing and linking key facts in health records, the
ever-growing stores of clinical documentation now
available to us can more readily be processed
and, ultimately, leveraged to improve the qual-
ity of care. SNOMED CT1 has emerged as the
de facto international terminology for represent-
ing clinical concepts in EHRs and is today used
in more than fifty countries, despite only being
1http://www.ihtsdo.org/snomed-ct/
available in a handful of languages2. Translations
into several other languages are, however, under
way3. This translation effort is essential for more
widespread integration of SNOMED CT in EHR
systems globally.
Translating a comprehensive4 terminology such
as SNOMED CT to an additional language is,
however, a massive and expensive undertaking. A
substantial part of this process involves enrich-
ing the terminology with synonyms in the tar-
get language. SNOMED CT has, for instance,
recently been translated into Swedish; however,
the Swedish version does not as yet contain syn-
onyms. Methods and tools that can accelerate the
language porting process in general and the syn-
onym identification task in particular are clearly
needed, not only to lower costs but also to in-
crease the coverage of SNOMED CT in clinical
text. Methods that can account for real-world lan-
guage use in the clinical setting, then, as well as to
changes over time, are particularly valuable.
This paper evaluates a semi-automatic method
for the extraction of synonyms of SNOMED CT
preferred terms using models of distributional se-
mantics to induce semantic spaces from a large
corpus of clinical text. In contrast to most ap-
proaches that exploit the notion of distributional
similarity for synonym extraction, this method ad-
dresses the key problem of identifying synonymy
between terms of varying length: a simple solution
is proposed that effectively incorporates the notion
of paraphrasing in a distributional framework. The
semantic spaces ? and, by extension, the method ?
are evaluated for their ability to extract synonyms
of SNOMED CT terms of the semantic types dis-
order and finding in Swedish.
2SNOMED CT is currently available in US English, UK
English, Spanish, Danish and Swedish.
3http://www.ihtsdo.org/snomed-ct/
snomed-ct0/different-languages/
4SNOMED CT contains more than 300,000 active con-
cepts and over a million relations.
36
2 Background
Synonymy is an aspect of semantics that con-
cerns the fact that concepts can be instantiated
using multiple linguistic expressions, or, viewed
conversely, that multiple linguistic expressions
can refer to the same concept. As synonymous
expressions do not necessarily consist of single
words, we sometimes speak of paraphrasing rather
than synonymy (Androutsopoulos and Malakasio-
tis, 2010). This variability of language use needs
to be accounted for in order to build high-quality
natural language processing (NLP) and text min-
ing systems. This is typically achieved by us-
ing thesauri or encoding textual instantiations of
concepts in a semantic resource, e.g. an ontol-
ogy. Creating such resources manually is, how-
ever, prohibitively expensive and likely to lead
to low coverage, especially in the clinical genre
where language use variability is exceptionally
high (Meystre et al, 2008).
2.1 Synonym Extraction
As a result, the task of extracting synonyms ?
and other semantic relations ? has long been a
central challenge in the NLP research commu-
nity, not least in the biomedical (Cohen and Hersh,
2005) and clinical (Meystre et al, 2008) do-
mains. A wide range of techniques has been pro-
posed for relation extraction in general and syn-
onym extraction in particular ? lexico-syntactic
patterns (Hearst, 1992), distributional semantics
(Dumais and Landauer, 1997) and graph-based
models (Blondel et al, 2004) ? from a variety
of sources, including dictionaries (Blondel et al,
2004), linked data such as Wikipedia (Nakayama
et al, 2007), as well as both monolingual (Hindle,
1990) and multilingual (van der Plas and Tiede-
mann, 2006) corpora. In recent years, ensemble
methods have been applied to obtain better perfor-
mance on the synonym extraction task, combin-
ing models from different families (Peirsman and
Geeraerts, 2009), with different parameter settings
(Henriksson et al, 2012) and induced from differ-
ent data sources (Wu and Zhou, 2003).
In the context of biomedicine, the goal has of-
ten been to extract synonyms of gene and pro-
tein names from the biomedical literature (Yu and
Agichtein, 2003; Cohen et al, 2005; McCrae and
Collier, 2008). In the clinical domain, Conway
and Chapman (2012) used a rule-based approach
to generate potential synonyms from the BioPor-
tal ontology web service, verifying candidate syn-
onyms against a large clinical corpus. Zeng et
al. (2012) used three query expansion methods
for information retrieval of clinical documents and
found that a model of distributional semantics ?
LDA-based topic modeling ? generated the best
synonyms. Henriksson et al (2012) combined
models of distributional semantics ? random in-
dexing and random permutation ? to extract syn-
onym candidates for Swedish MeSH5 terms and
possible abbreviation-definition pairs. In the con-
text of SNOMED CT, distributional methods have
been applied to capture synonymous relations be-
tween terms of varying length: 16-24% of English
SNOMED CT synonyms present in a large clini-
cal corpus were successfully identified in a list of
twenty suggestions (Henriksson et al, 2013).
2.2 Distributional Semantics
Models of distributional semantics (see Cohen and
Widdows (2009) for an overview of methods and
their application in the biomedical domain) were
initially motivated by the inability of the vector
space model to account for synonymy, which had
a negative impact on recall in information retrieval
systems (Deerwester et al, 1990). The theoretical
foundation underpinning such models of seman-
tics is the distributional hypothesis (Harris, 1954),
according to which words with similar meanings
tend to appear in similar contexts. By exploiting
the availability of large corpora, the meaning of
terms can be modeled based on their distribution
in different contexts. An estimate of the semantic
relatedness between terms can then be quantified,
thereby, in some sense, rendering semantics com-
putable.
An obvious application of distributional seman-
tics is the extraction of semantic relations between
terms, such as synonymy, hyp(o/er)nymy and co-
hyponymy (Panchenko, 2013). As synonyms are
interchangeable in some contexts ? and thus have
similar distributional profiles ? synonymy is cer-
tainly a semantic relation that should be captured.
However, since hyp(o/er)nyms and co-hyponyms
? in fact, even antonyms ? are also likely to have
similar distributional profiles, such semantic rela-
tions will be extracted too.
Many models of distributional semantics dif-
fer in how context vectors, representing term
5Medical Subject Headings (http://www.nlm.nih.
gov/mesh).
37
meaning, are constructed. They are typically de-
rived from a term-context matrix that contains
the (weighted, normalized) frequency with which
terms occur in different contexts. Partly due
to the intractability of working with such high-
dimensional data, it is projected into a lower-
dimensional (semantic) space, while approxi-
mately preserving the relative distances between
data points. Methods that rely on computa-
tionally expensive dimensionality reduction tech-
niques suffer from scalability issues.
Random Indexing
Random indexing (RI) (Kanerva et al, 2000) is
a scalable and computationally efficient alterna-
tive in which explicit dimensionality reduction is
avoided: a lower dimensionality d is instead cho-
sen a priori as a model parameter and the d-
dimensional context vectors are then constructed
incrementally. Each unique term in the corpus is
assigned a static index vector, consisting of ze-
ros and a small number of randomly placed 1s
and -1s6. Each term is also assigned an initially
empty context vector, which is incrementally up-
dated by adding the index vectors of the surround-
ing words within a sliding window, weighted by
their distance to the target term. The semantic re-
latedness between two terms is then estimated by
calculating, for instance, the cosine similarity be-
tween their context vectors.
Random Permutation
Random permutation (RP) (Sahlgren et al, 2008)
is a modification of RI that attempts to take into
account term order information by simply permut-
ing (i.e. shifting) the index vectors according to
their direction and distance from the target term
before they are added to the context vector. RP
has been shown to outperform RI on the synonym
part of the TOEFL7 test.
Model Parameters
The model parameters need to be configured for
the task that the semantic space is to be used
for. For instance, with a document-level con-
text definition, syntagmatic relations are mod-
eled, i.e. terms that belong to the same topic
(<car, motor, race>), whereas, with a sliding win-
dow context definition, paradigmatic relations are
6By generating sparse vectors of a sufficiently high di-
mensionality in this way, the context representations will be
nearly orthogonal.
7Test Of English as a Foreign Language
modeled (<car, automobile, vehicle>) (Sahlgren,
2006). Synonymy is an instance of a paradigmatic
relation.
The dimensionality has also been shown to be
potentially very important, especially when the
size of the vocabulary and the number of contexts8
are large (Henriksson and Hassel, 2013).
3 Materials and Methods
The task of semi-automatically identifying syn-
onyms of SNOMED CT preferred terms is here
approached by, first, statistically identifying mul-
tiword terms in the data and treating them as com-
pounds; then, performing a distributional analysis
of a preprocessed clinical corpus to induce a se-
mantic term space; and, finally, extracting the se-
mantically most similar terms for each preferred
term of interest.
The experimental setup can be broken down
into the following steps: (1) data preparation, (2)
term recognition, (3) model parameter tuning and
(4) evaluation. Semantic spaces are induced with
different parameter configurations on two dataset
variants: one with unigram terms only and one that
also includes multiword terms. The model param-
eters are tuned using MeSH, which contains syn-
onyms for Swedish. The best parameter settings
for each of the two dataset variants are then em-
ployed in the final evaluation, where a medical ex-
pert inspects one hundred term lists extracted for
SNOMED CT preferred terms belonging to the se-
mantic types disorder and finding.
3.1 Data Preparation
The data used to induce the semantic spaces is ex-
tracted from the Stockholm EPR Corpus (Dalia-
nis et al, 2009), which contains Swedish health
records from the Karolinska University Hospital
in Stockholm9. The subset (?33 million tokens)
used in these experiments comprises all forms of
text-based records ? i.e., clinical notes ? from
a large variety of clinical practices. The docu-
ments in the corpus are initially preprocessed by
simply lowercasing tokens and removing punctu-
ation and digits. Lemmatization is not performed,
as we want to be able to capture morphological
8The vocabulary size and the number of contexts are
equivalent when employing a window context definition.
9This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpro?vningsna?mnden i Stock-
holm), permission number 2012/834-31/5.
38
variants of terms; stop-word filtering is not per-
formed, as traditional stop words ? for instance,
high-frequency function words ? could potentially
be constituents of multiword terms.
3.2 Term Recognition
Multiword terms are extracted statistically from
the corpus using the C-value statistic (Frantzi and
Ananiadou, 1996; Frantzi et al, 2000). This tech-
nique has been used successfully for term recog-
nition in the biomedical domain, largely due to
its ability to handle nested terms (Zhang et al,
2008). Using the C-value statistic for term recog-
nition first requires a list of candidate terms, for
which the C-value can then be calculated. Here,
this is simply produced by extracting n-grams ?
unigrams, bigrams and trigrams ? from the corpus
with TEXT-NSP (Banerjee and Pedersen, 2003).
The statistic is based on term frequency and term
length (number of words); if a candidate term is
part of a longer candidate term (as will be the case
for practically all unigram and bigram terms), the
number and frequency of those longer terms are
also taken into account (Figure 1).
In order to improve the quality of the extracted
terms, a number of filtering rules is applied to the
generated term list: terms that begin and/or end
with certain words, e.g. prepositions and articles,
are removed. The term list ? ranked according to
C-value ? is further modified by giving priority to
terms of particular interest, e.g. SNOMED CT dis-
order and finding preferred terms: these are moved
to the top of the list, regardless of their C-value.
As a result, the statistical foundation on which the
distributional method bases its semantic represen-
tation will effectively be strengthened.
The term list is then used to perform exact string
matching on the entire corpus: multiword terms
with a higher C-value than their constituents are
concatenated. We thereby treat multiword terms as
separate (term) types with distinct distributions in
the data, different from those of their constituents.
3.3 Model Parameter Tuning
Term spaces with different parameter configu-
rations are induced from the two dataset vari-
ants: one containing only unigram terms (Uni-
gram Word Spaces) and one containing also mul-
tiword terms (Multiword Term Spaces). The fol-
lowing model parameters are tuned:
? Distributional Model: Random indexing (RI)
vs. Random permutation (RP)
? Context Window Size: 2+2, 4+4, 8+8 sur-
rounding terms (left+right of the target term)
? Dimensionality: 1000, 2000, 3000
As the Swedish version of SNOMED CT cur-
rently does not contain synonyms, it cannot be
used to perform the parameter tuning automat-
ically. This is instead done with the Swedish
version of MeSH, which is one of the very few
standard terminologies that contains synonyms for
medical terms in Swedish. However, as the op-
timal parameter configurations for capturing syn-
onymy are not necessarily identical for all seman-
tic types, the parameter tuning is performed by
evaluating the semantic spaces for their ability to
identify synonyms of MeSH terms that belong to
the categories Disease or Syndrome and Sign or
Symptom. These particular categories are simply
chosen as they, to a reasonable extent, seem to
correspond to the SNOMED CT semantic types
studied in this paper, namely Disorder and Find-
ing. Only synonym pairs that appear at least fifty
times in each of the dataset variants are included
(155 for Unigram Word Spaces and 123 for Mul-
tiword Term Spaces), as the statistical foundation
for terms that only occur rarely in the data may
not be sufficiently solid. In these Multiword Term
Spaces, the MeSH terms ? but not the synonyms
? are given precedence in the term list. A term
is provided as input to a semantic space and the
twenty semantically most similar terms are out-
put, provided that they also appear at least fifty
times in the data. Recall Top 20 is calculated for
each input term: what proportion of the MeSH
synonyms are identified in a list of twenty sugges-
tions? Since each synonym pair must appear at
least fifty times in the corresponding dataset vari-
ant, it should be duly noted that the optimization
sets will not be identical, which in turn means that
the results of the Unigram Word Spaces and the
Multiword Term Spaces are not directly compara-
ble. The optimal parameter configuration, then,
may be different when also multiword terms are
modeled.
3.4 Evaluation
The optimal parameter configuration for each
dataset variant is employed in the final evaluation.
In this Multiword Term Space, the SNOMED CT
39
C-value(a) =
{
log2 |a| ? f(a) if a is not nested
log2 |a| ? (f(a)?
1
P (Ta)
?
bTa f(b)) otherwise
a = candidate term Ta = set of extracted candidate terms that contain a
b = longer candidate terms P (Ta) = number of candidate terms in Ta
f(a) = term frequency of a f(b) = term frequency of longer candidate term b
|a| = length of candidate term (number of words)
Figure 1: C-Value Formula. The formula for calculating C-value of candidate terms.
preferred terms of interest, rather than the MeSH
terms, are prioritized in the term list. The seman-
tic spaces ? and, in effect, the method ? are pri-
marily evaluated for their ability to identify syn-
onyms of SNOMED CT preferred terms, in this
case of concepts that belong to the semantic types
disorder and finding. The need to identify syn-
onyms for these semantic types is clear, as it has
been shown that the coverage of SNOMED CT
for mentions of disorders (38%) and, in particu-
lar, findings (23%) in Swedish clinical text is low
(Skeppstedt et al, 2012). Since the Swedish ver-
sion of SNOMED CT currently lacks synonyms,
the evaluation reasonably needs to be manual, as
there is no reference standard. One option, then,
could be to choose a random sample of preferred
terms to use in the evaluation. A potential draw-
back of such a(n) (unguided) selection is that many
concepts in the English version of SNOMED CT
do not have any synonymous terms, which might
lead to evaluators spending valuable time looking
for something which does not exist. An alterna-
tive approach, which is assumed here, is to inspect
concepts that have many synonyms in the English
version of SNOMED CT. The fact that some con-
cepts have many textual instantiations in one lan-
guage does not necessarily imply that they also
have many textual instantiations in another lan-
guage. This, however, seems to be the case when
comparing the English and Swedish versions of
MeSH: terms10 that have the most synonyms in the
English version tend to have at least one synonym
in the Swedish version to a larger extent than a ran-
dom selection of terms (60% and 62% of the terms
in the Swedish version have at least one synonym
when looking at the top 100 and top 50 terms with
the most synonyms in the English version, com-
pared to 41% overall in the Swedish version).
For the two dataset variants, we thus select 25
SNOMED CT preferred terms for each semantic
10These calculations are based on MeSH terms that belong
to the categories Disease or Syndrome and Sign or Symptom.
type ? disorder and finding ? that (1) have the most
synonyms in the English version and (2) occur at
least fifty times in the data. In total, fifty terms
are input to the Unigram Word Space and another
fifty terms (potentially with some overlap) are in-
put to the Multiword Term Space. A medical ex-
pert inspects the twenty semantically most simi-
lar terms for each input term. Synonymy is here
the primary semantic relation of interest, but the
semantic spaces are also evaluated for their abil-
ity, or tendency, to extract other semantic term re-
lations: hypernyms or hyponyms, co-hyponyms,
antonyms, as well as disorder-finding relations.
4 Results
The term recognition and concatenation of mul-
tiword terms naturally affect some properties of
the dataset variants, such as the vocabulary size
(number of types) and the type-token ratio. The
Unigram Word Space contains 381,553 types and
an average of 86.54 tokens/type, while the Mul-
tiword Term Space contains 2,223,953 types and
an average of 9.72 tokens/type. This, in turn, may
have an effect on which parameter configuration is
?optimal? for the synonym extraction task. In fact,
this seems to be the case when tuning the parame-
ters for the two dataset variants. For the Unigram
Word Spaces, random indexing with a sliding con-
text window of 8+8 terms and a dimensionality of
2000 seems to work best, whereas for the Mul-
tiword Term Spaces, random permutation with a
sliding window context of 4+4 terms and a dimen-
sionality of 3000 works better (Table 1).
When these parameter configurations are ap-
plied to the SNOMED CT terms, a total of 40 syn-
onyms are extracted by the Unigram Word Space
and 33 synonyms by the Multiword Term Space
(Table 2). On average, 0.80 and 0.66 synonyms
are extracted per preferred term, respectively. The
number of identified synonyms per input term
varies significantly: for some, none; for others, up
to ten. Other semantic relations are also extracted
40
Unigram Word Spaces Multiword Term Spaces
RI RP RI RP
Sliding Window? 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8
1000 dimensions 0.43 0.47 0.48 0.41 0.45 0.42 0.21 0.25 0.26 0.25 0.26 0.24
2000 dimensions 0.43 0.48 0.49 0.48 0.48 0.43 0.21 0.24 0.25 0.25 0.25 0.24
3000 dimensions 0.44 0.47 0.48 0.46 0.45 0.43 0.22 0.24 0.24 0.23 0.27 0.25
Table 1: Model Parameter Tuning. Results, reported as recall top 20, for MeSH synonyms that appear
at least 50 times in each of the dataset variants (unigram vs. multiword). Random indexing (RI) and
Random permutation (RP) term spaces were built with different context window sizes (2+2, 4+4, 8+8
surrounding terms) and dimensionality (1000, 2000, 3000).
by the semantic spaces: mainly co-hyponyms,
but also hypernyms and hyponyms, antonyms and
disorder-finding relations. The Unigram Word
Space extracts, on average, 0.52 hypernyms or hy-
ponyms, 1.8 co-hyponyms, 0.1 antonyms and 0.34
disorder-finding relations. The Multiword Term
Space extracts, on average, 0.16 hypernyms or
hyponyms, 1.1 co-hyponyms, 0.14 antonyms and
0.66 disorder-finding relations. In general, more
of the above semantic relations are extracted by
the Unigram Word Space than by the Multiword
Term Space (178 vs. 136). It is, however, inter-
esting to note that almost twice as many disorder-
finding relations are extracted by the latter com-
pared to the former. Of course, none of the re-
lations extracted by the Unigram Word Space in-
volve a multiword term; on the other hand, more
than half (around 57%) of the relations extracted
by the Multiword Term Space involve at least one
multiword term.
Both semantic spaces identify more synonyms
of preferred terms that belong to the semantic type
finding than disorder (in total 56 vs. 39). The same
holds true for hyp(er/o)nyms and co-hypnoyms;
however, the converse is true for antonyms and
disorder-finding relations.
5 Discussion
The results demonstrate that it is indeed possible
to extract synonyms of medical terms by perform-
ing a distributional analysis of a large corpus of
clinical text ? unigram-unigram relations, as well
as unigram-multiword and multiword-unigram re-
lations. It is also clear, however, that other se-
mantically related terms share distributional pro-
files to a similar degree as synonymous terms. The
predominance of the other semantic relations, ex-
cept for antonymy, in the term lists can reason-
ably be explained by the simple fact that there
exist more hypernyms, hyponyms, co-hyponyms
and disorder-finding relations than synonyms (or
antonyms).
It is also evident that more semantic relations,
and indeed more synonyms, are extracted by the
Unigram Word Space than the Multiword Term
Space. Again, it is important to underline that the
results cannot be compared without due qualifica-
tion since the evaluation sets are not identical: the
Unigram Word Space does not contain any mul-
tiword terms, for instance. The ability to model
multiword terms in a distributional framework and
to handle semantic composition ? i.e., how mean-
ing is, and sometimes is not, composed by the
meaning of its constituents ? has long been an en-
deavor in the NLP research community (Sag et al,
2002; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Mitchell, 2011). Treating
multiword terms as compound tokens is a simple
and rather straightforward approach, which also
makes intuitive sense: rather than treat individ-
ual words as clearly delineated bearers of mean-
ing, identify semantic units ? regardless of term
length ? and model their distributional profiles.
Unfortunately, there are problems with this ap-
proach. First, the attendant increase in vocabu-
lary size entails a lower tokens-type ratio, which
in turn means that the statistical foundation for
terms will weaken. In this case, the average token-
type ratio decreased from 86.54 to 9.72. This ap-
proach therefore requires access to a sufficiently
large corpus. Second, the inflation in vocabulary
size entails a corresponding increase in the num-
ber of vectors in the semantic space. This not only
requires more memory; to ensure that the crucial
near-orthogonality property11 of RI-based models
is maintained, the dimensionality has to be suffi-
11Random indexing assumes that the index vectors ? rep-
resenting distinct contexts ? are nearly orthogonal.
41
Unigram Word Space Multiword Term Space
DISORDER FINDING DISORDER FINDING
Synonyms
sum 18 22 16 17
average 0.72 0.88 0.64 0.68
? 1 / preferred term 12 12 8 6
involves mwe - - 10 13
Hyp(er/o)nyms
sum 12 14 4 4
average 0.48 0.56 0.16 0.16
? 1 / preferred term 6 8 4 3
involves mwe - - 3 3
Co-hyponyms
sum 34 56 22 33
average 1.36 2.24 0.88 1.32
? 1 / preferred term 14 17 10 13
involves mwe - - 19 15
Antonyms
sum 3 2 4 3
average 0.12 0.08 0.16 0.12
? 1 / preferred term 3 2 3 3
involves mwe - - 0 1
Disorder-Finding
sum 11 6 28 5
average 0.44 0.24 1.12 0.2
? 1 / preferred term 6 5 12 5
involves mwe - - 11 2
Table 2: Evaluation Results. The types of semantic relations extracted among the twenty most se-
mantically similar terms of 25 DISORDER and 25 FINDING SNOMED CT preferred terms from each
semantic space. Sum is the total number of identified relevant terms. Average is the average number of
relevant terms per preferred term. ? 1 / preferred term is the number of preferred terms for which at
least one relevant term is identified. Involves mwe is the number of relevant relations where either the
preferred term or the relevant term is a multiword expression.
ciently large in relation to the number of contexts
(represented by index vectors). In the Multiword
Term Space the vocabulary size is over two million
(compared to less than 400,000 in the Unigram
Word Space). A dimensionality of 3000 is likely
insufficient to ensure that each term type has an
initial distinct and uncorrelated representation. In
the evaluation, there were several examples where
two groups of terms ? semantically homogenous
within each group, but semantically heterogenous
across groups ? co-existed in the same term list:
these ?topics? had seemingly collapsed into the
same subspace. Despite these problems, it should
be recognized that the Multiword Term Space is, in
fact, able to retrieve 23 synonymous relations that
involve at least one multiword term. The Unigram
Word Space cannot retrieve any such relations.
The ability to extract high-quality terms would
seem to be an important prerequisite for this ap-
proach to modeling multiword terms in a distribu-
tional framework. However, despite employing a
rather simple means of extracting terms ? without
using any syntactic information ? the terms that
actually appeared in the lists of semantically re-
lated terms were mostly reasonable. This perhaps
indicates that the term recognition task does not
need to be perfect: terms of interest, of course,
need to be identified, but some noise in the form
of bad terms might be acceptable. A weakness
of the term recognition part is, however, that too
many terms were identified, which in turn led to
the aforementioned inflation in vocabulary size.
42
Limiting the number of multiword terms in the ini-
tial term list ? for instance by extracting syntactic
phrases as candidate terms ? could provide a pos-
sible solution to this problem.
Overall, more synonyms were identified for the
semantic type finding than for disorder. One pos-
sible explanation for this could be that there are
more ways of describing a finding than a disorder
? not all semantic types can be assumed to have
the same number of synonyms. The same holds
true for all other semantic relations except for dis-
order-finding, where disorders generated a much
larger number of distributionally similar findings
than vice versa. This could perhaps also be ex-
plained by the possible higher number of syn-
onyms for finding than disorder.
When this method was evaluated using the
English version of SNOMED CT, 16-24% of
known synonyms were identified (Henriksson et
al., 2013). In this case, however, we extracted
synonym candidates for terms that may or may
not have synonyms. This is thus a scenario that
more closely resembles how this method would
actually be used in a real-life setting to populate
a terminology with synonyms. Although the com-
parison with MeSH showed that terms with many
synonyms in English also tend to have at least one
synonym in Swedish, approximately 40% of them
did not have any synonyms. It is thus not certain
that the terms used in this evaluation all have at
least one synonym, which was also noted by the
evaluator in this study.
6 Conclusions
In this study, we have demonstrated a method
that could potentially be used to expedite the lan-
guage porting process of terminologies such as
SNOMED CT. With access to a large corpus of
clinical text in the target language and an initial
set of terms, this language-independent method is
able to extract and present candidate synonyms to
the lexicographer, thereby providing valuable sup-
port for semi-automatic terminology development.
A means to model multiword terms in a distri-
butional framework is an important feature of the
method and is crucial for the synonym extraction
task.
Acknowledgments
This work was partly supported by the Swedish
Foundation for Strategic Research through the
project High-Performance Data Mining for Drug
Effect Detection (ref. no. IIS11-0053) at Stock-
holm University, Sweden, and partly funded by
the Stockholm University Academic Initiative
through the Interlock project. Finally, we would
like to thank the reviewers for their constructive
feedback.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation, and Use of the Ngram Statis-
tic Package. In Proceedings of CICLing, pages 370?
381.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Representing
Adjective-Noun Constructions in Semantic Space.
In Proceedings of EMNLP, pages 1183?1193.
Vincent D. Blondel, Anah?? Gajardo, Maureen Hey-
mans, Pierre Senellart, and Paul Van Dooren.
2004. A Measure of Similarity between Graph Ver-
tices: Applications to Synonym Extraction and Web
Searching. SIAM Review, 46(4):647?666.
Aaron M. Cohen and William R. Hersh. 2005. A Sur-
vey of Current Work in Biomedical Text Mining.
Briefings in Bioinformatics, 6(1):57?71.
Trevor Cohen and Dominic Widdows. 2009. Empirical
Distributional Semantics: Methods and Biomedical
Applications. J Biomed Inform, 42(2):390?405.
AM Cohen, WR Hersh, C Dubay, and K Spackman.
2005. Using co-occurrence network structure to
extract synonymous gene and protein names from
medline abstracts. BMC Bioinformatics, 6(1):103.
Mike Conway and Wendy W. Chapman. 2012. Dis-
covering Lexical Instantiations of Clinical Con-
cepts using Web Services, WordNet and Corpus Re-
sources. In AMIA Fall Symposium, page 1604.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus:
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR, pages 243?249.
Scott Deerwester, Susan T. Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Susan T. Dumais and Thomas K. Landauer. 1997. A
Solution to Plato?s Problem: The Latent Semantic
43
Analysis Theory of Acquisition, Induction and Rep-
resentation of Knowledge. Psychological Review,
104(2):211?240.
Katerina Frantzi and Sophia Ananiadou. 1996. Ex-
tracting Nested Collocations. In Proceedings of
COLING, pages 41?46.
Katerina Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic Recognition of Multi-
Word Terms: The C-value/NC-value Method. In-
ternational Journal on Digital Libraries, 3(2):115?
130.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of EMNLP, pages 1394?1404.
Zellig S. Harris. 1954. Distributional Structure. Word,
10:146?162.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings
of COLING, pages 539?545.
Aron Henriksson and Martin Hassel. 2013. Optimiz-
ing the Dimensionality of Clinical Term Spaces for
Improved Diagnosis Coding Support. In Proceed-
ings of Louhi.
Aron Henriksson, Hans Moen, Maria Skeppstedt, Ann-
Marie Eklund, and Vidas Daudaravicius. 2012.
Synonym Extraction of Medical Terms from Clini-
cal Text Using Combinations of Word Space Mod-
els. In Proceedings of SMBM, pages 10?17.
Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying Syn-
onymy between SNOMED Clinical Terms of Vary-
ing Length Using Distributional Analysis of Elec-
tronic Health Records. In AMIA Annual Symposium
(submitted).
Donald Hindle. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings of
ACL, pages 268?275.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random Indexing of Text Samples for Latent
Semantic Analysis. In Proceedings CogSci, page
1036.
John McCrae and Nigel Collier. 2008. Synonym
Set Extraction from the Biomedical Literature by
Lexical Pattern Discovery. BMC Bioinformatics,
9(1):159.
Ste?phane M. Meystre, Guergana K. Savova, Karin C.
Kipper-Schuler, John F. Hurdle, et al 2008. Ex-
tracting Information from Textual Documents in the
Electronic Health Record: A Review of Recent Re-
search. Yearb Med Inform, 35:128?44.
Jeffrey Mitchell. 2011. Composition in Distributional
Models of Semantics. Ph.D. thesis, University of
Edinburgh.
Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2007. Wikipedia Mining for an Association Web
Thesaurus Construction. In Proceedings of WISE,
pages 322?334.
Alexander Panchenko. 2013. Similarity Measures for
Semantic Relation Extraction. Ph.D. thesis, PhD
thesis, Universite? catholique de Louvain & Bauman
Moscow State Technical University.
Yves Peirsman and Dirk Geeraerts. 2009. Predicting
Strong Associations on the Basis of Corpus Data. In
Proceedings of EACL, pages 648?656.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of CICLing, pages 1?15.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a Means to Encode Order
in Word Space. In Proceedings of CogSci, pages
1300?1305.
Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words in
High-Dimensional Vector Spaces. Ph.D. thesis, PhD
thesis, Stockholm University.
Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based Entity Recognition and Coverage
of SNOMED CT in Swedish Clinical Text. In Pro-
ceedings of LREC, pages 1250?1257.
Lonneke van der Plas and Jo?rg Tiedemann. 2006.
Finding Synonyms Using Automatic Word Align-
ment and Measures of Distributional Similarity. In
Proceedings of COLING/ACL, pages 866?873.
Hua Wu and Ming Zhou. 2003. Optimizing Syn-
onym Extraction Using Monolingual and Bilingual
Resources. In Proceedings of the Second Interna-
tional Workshop on Paraphrasing, pages 72?79.
Hong Yu and Eugene Agichtein. 2003. Extracting
Synonymous Gene and Protein Terms from Biolog-
ical Literature. Bioinformatics, 19(suppl 1):i340?
i349.
Qing T Zeng, Doug Redd, Thomas Rindflesch, and
Jonathan Nebeker. 2012. Synonym, Topic Model
and Predicate-Based Query Expansion for Retriev-
ing Clinical Documents. In Proceedings AMIA An-
nual Symposium, pages 1050?9.
Ziqi Zhang, Jose? Iria, Christopher Brewster, and Fabio
Ciravegna. 2008. A Comparative Evaluation of
Term Recognition Algorithms. In Proceedings of
LREC.
44
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94?103,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
EACL - Expansion of Abbreviations in CLinical text
Lisa Tengstrand*, Be
?
ata Megyesi*, Aron Henriksson
+
, Martin Duneld
+
and Maria Kvist
+
*Department of Linguistics and Philology,
Uppsala University, Sweden
tengstrand@ling.su.se, beata.megyesi@lingfil.uu.se
+
Department of Computer and System Sciences,
Stockholm University, Sweden
aronhen@dsv.su.se, xmartin@dsv.su.se, maria.kvist@karolinska.se
Abstract
In the medical domain, especially in clin-
ical texts, non-standard abbreviations are
prevalent, which impairs readability for
patients. To ease the understanding of the
physicians? notes, abbreviations need to be
identified and expanded to their original
forms. We present a distributional seman-
tic approach to find candidates of the origi-
nal form of the abbreviation, and combine
this with Levenshtein distance to choose
the correct candidate among the semanti-
cally related words. We apply the method
to radiology reports and medical journal
texts, and compare the results to general
Swedish. The results show that the cor-
rect expansion of the abbreviation can be
found in 40% of the cases, an improve-
ment by 24 percentage points compared to
the baseline (0.16), and an increase by 22
percentage points compared to using word
space models alone (0.18).
1 Introduction
Abbreviations are prevalent in text, especially in
certain text types where the author has either lim-
ited space or time to write the written message and
therefore shortens some words or phrases. This
might, however, make it difficult for the reader
to understand the meaning of the actual abbre-
viation. Although some abbreviations are well-
known, and frequently used by most of us (e.g.,
i.e., pm, etc.), most of the abbreviations used in
specialized domains are often less known to the
public. Interpreting them is not an easy task, as ab-
breviations are often ambiguous and their correct
meaning depends on the context in which they ap-
pear. For example, military and governmental staff
would naturally read EACL as Emergency Action
Checklist, people in the food and beverage busi-
ness might think of the company name EACL, lin-
guists would probably interpret it as the European
Chapter of Chinese Linguistics, while computa-
tional linguists would generally claim that EACL
stands for the European Chapter of the Associa-
tion for Computational Linguistics. However, the
readers of this particular article know, as the title
suggests, that the intended meaning here is the Ex-
pansion of Abbreviations in CLinical text.
It has been shown that abbreviations are fre-
quently occurring in various domains and genres,
such as in historical documents, messages in so-
cial media, as well as in different registers used
by specialists within a particular field of exper-
tise. Clinical texts produced by health care per-
sonnel is an example of the latter. The clinical
texts are communication artifacts, and the clini-
cal setting requires that information is expressed
in an efficient way, resulting in short telegraphic
messages. Physicians and nurses need to docu-
ment their work to describe findings, treatments
and procedures precisely and compactly, often un-
der time pressure.
In recent years, governments and health care ac-
tors have started making electronic health records
accessible, not only to other caretakers, but also
to patients in order to enable them to participate
actively in their own health care processes. How-
ever, several studies have shown that patients have
difficulties to comprehend their own health care
reports and other medical texts due to the different
linguistic features that characterize these, aswell
as to medical jargon and technical terminology
(Elhadad, 2006; Rudd et al., 1999; Keselman et
al., 2007). It has also been shown that physicians
rarely adapt their writing style in order to produce
documents that are accessible to lay readers (Al-
lvin, 2010). Besides the use of different termi-
nologies and technical terms, an important obsta-
cle for patients to comprehend medical texts is the
frequent use of ? for the patients unknown ? ab-
94
breviations (Keselman et al., 2007; Adnan et al.,
2010).
In health records, abbreviations, which consti-
tute linguistic units that are inherently difficult to
decode, are commonly used and often non stan-
dard (Skeppstedt, 2012). An important step in
order to increase readability for lay readers is to
translate abbreviated words into their correspond-
ing full length words.
The aim of this study is to explore a distri-
butional semantic approach combined with word
normalization, measured by Levenshtein distance,
to abbreviation expansion. Using distributional
semantic models, which can be applied to large
amounts of data, has been shown to be a viable
approach to extracting candidates for the underly-
ing, original word of an abbreviation. In order to
find the correct expansion among the semantically
related candidates, we apply the Levenshtein dis-
tance measure. We report on experiments on com-
parative studies of various text types in Swedish,
including radiology reports, medical journals and
texts taken from a corpus of general Swedish.
2 Background
An abbreviation is a shorter ? abbreviated ? form
of a word or phrase, often originating from a tech-
nical term or a named entity. Abbreviations are
typically formed in one of three ways: by (i) clip-
ping the last character sequence of the word (e.g.,
pat for patient or pathology), (ii) merging the ini-
tial letter(s) of the words to form an acronym (e.g.,
UU for Uppsala University), or (iii) merging some
of the letters ? often the initial letter of the sylla-
bles ? in the word (e.g., msg for message). Abbre-
viations can also be formed as a combination of
these three categories (e.g., EACL for Expansion
of Abbreviations in CLinical text).
Automatically expanding abbreviations to their
original form has been of interest to computational
linguists as a means to improve text-to-speech, in-
formation retrieval and information extraction sys-
tems. Rule-based systems as well as statistical and
machine learning methods have been proposed to
detect and expand abbreviations. A common com-
ponent of most solutions is their reliance on the as-
sumption that an abbreviation and its correspond-
ing definition will appear in the same text.
Taghva and Gilbreth (1999) present a method
for automatic acronym-definition extraction in
technical literature, where acronym detection is
based on case and token length constraints. The
surrounding text is subsequently searched for pos-
sible definitions corresponding to the detected
acronym using an inexact pattern-matching algo-
rithm. The resulting set of candidate definitions
is then narrowed down by applying the Longest
Common Subsequence (LCS) algorithm (Nakatsu
et al., 1982) to the candidate pairs. They report
98% precision and 93% recall when excluding
acronyms of two or fewer characters.
Park and Byrd (2001), along somewhat similar
lines, propose a hybrid text mining approach for
abbreviation expansion in technical literature. Or-
thographic constraints and stop lists are first used
to detect abbreviations; candidate definitions are
then extracted from the adjacent text based on a set
of pre-specified conditions. The abbreviations and
definitions are converted into patterns, for which
transformation rules are constructed. An initial
rule-base comprising the most frequent rules is
subsequently employed for automatic abbreviation
expansion. They report 98% precision and 94%
recall as an average over three document types.
In the medical domain, most approaches to
abbreviation resolution also rely on the co-
occurrence of abbreviations and definitions in a
text, typically by exploiting the fact that abbrevi-
ations are sometimes defined on their first men-
tion. These studies extract candidate abbreviation-
definition pairs by assuming that either the defi-
nition or the abbreviation is written in parenthe-
ses (Schwartz and Hearst, 2003). The process of
determining which of the extracted abbreviation-
definition pairs are likely to be correct is then
performed either by rule-based (Ao and Takagi,
2005) or machine learning (Chang et al., 2002;
Movshovitz-Attias and Cohen, 2012) methods.
Most of these studies have been conducted on
English corpora; however, there is one study on
Swedish medical text (Dann?ells, 2006). There are
problems with this popular approach to abbrevia-
tion expansion: Yu et al. (2002) found that around
75% of all abbreviations in the biomedical litera-
ture are never defined.
The application of this method to clinical text
is even more problematic, as it seems highly un-
likely that abbreviations would be defined in this
way. The telegraphic style of clinical narrative,
with its many non-standard abbreviations, is rea-
sonably explained by time constraints in the clin-
ical setting. There has been some work on iden-
95
tifying such undefined abbreviations in clinical
text (Isenius et al., 2012), as well as on finding
the intended abbreviation expansion among candi-
dates in an abbreviation dictionary (Gaudan et al.,
2005).
Henriksson et al. (2012; 2014) present a method
for expanding abbreviations in clinical text that
does not require abbreviations to be defined, or
even co-occur, in the text. The method is based
on distributional semantic models by effectively
treating abbreviations and their corresponding def-
inition as synonymous, at least in the sense of shar-
ing distributional properties. Distributional se-
mantics (see Cohen and Widdows (2009) for an
overview) is based on the observation that words
that occur in similar contexts tend to be semanti-
cally related (Harris, 1954). These relationships
are captured in a Random Indexing (RI) word
space model (Kanerva et al., 2000), where se-
mantic similarity between words is represented as
proximity in high-dimensional vector space. The
RI word space representation of a corpus is ob-
tained by assigning to each unique word an ini-
tially empty, n-dimensional context vector, as well
as a static, n-dimensional index vector, which con-
tains a small number of randomly distributed non-
zero elements (-1s and 1s), with the rest of the
elements set to zero
1
. For each occurrence of a
word in the corpus, the index vectors of the sur-
rounding words are added to the target word?s con-
text vector. The semantic similarity between two
words can then be estimated by calculating, for in-
stance, the cosine similarity between their context
vectors. A set of word space models are induced
from unstructured clinical data and subsequently
combined in various ways with different parame-
ter settings (i.e., sliding window size for extracting
word contexts). The models and their combina-
tions are evaluated for their ability to map a given
abbreviation to its corresponding definition. The
best model achieves 42% recall. Improvement of
the post-processing of candidate definitions is sug-
gested in order to obtain enhanced performance on
this task.
The estimate of word relatedness that is ob-
tained from a word space model is purely statis-
tical and has no linguistic knowledge. When word
pairs should not only share distributional proper-
ties, but also have similar orthographic represen-
1
Generating sparse vectors of a sufficiently high dimen-
sionality in this manner ensures that the index vectors will be
nearly orthogonal.
tations ? as is the case for abbreviation-definition
pairs ? normalization procedures could be ap-
plied. Given a set of candidate definitions for a
given abbreviation, the task of identifying plausi-
ble candidates can be viewed as a normalization
problem. Petterson et al. (2013) utilize a string
distance measure, Levenshtein distance (Leven-
shtein, 1966), in order to normalize historical
spelling of words into modern spelling. Adjusting
parameters, i.e., the maximum allowed distance
between source and target, according to observed
distances between known word pairs of historical
and modern spelling, gives a normalization accu-
racy of 77%. In addition to using a Levenshtein
distance weighting factor of 1, they experiment
with context free and context-sensitive weights for
frequently occurring edits between word pairs in a
training corpus. The context-free weights are cal-
culated on the basis of one-to-one standard edits
involving two characters; in this setting the nor-
malization accuracy is increased to 78.7%. Fre-
quently occurring edits that involve more than two
characters, e.g., substituting two characters for
one, serve as the basis for calculating context-
sensitive weights and gives a normalization accu-
racy of 79.1%. Similar ideas are here applied to
abbreviation expansion by utilizing a normaliza-
tion procedure for candidate expansion selection.
3 Method
The current study aims to replicate and extend
a subset of the experiments conducted by Hen-
riksson et al. (2012), namely those that concern
the abbreviation expansion task. This includes
the various word space combinations and the pa-
rameter optimization. The evaluation procedure
is similar to the one described in (Henriksson et
al., 2012). The current study, however, focuses on
post-processing of the semantically related words
by introducing a filter and a normalization proce-
dure in an attempt to improve performance. An
overview of the approach is depicted in Figure 1.
Abbreviation expansion can be viewed as a two-
step procedure, where the first step involves de-
tection, or extraction, of abbreviations, and the
second step involves identifying plausible expan-
sions. Here, the first step is achieved by extracting
abbreviations from a clinical corpus with clinical
abbreviation detection software and using a list of
known medical abbreviations. The second step is
performed by first extracting a set of semantically
96
clinical text
abbreviation
extraction
abbreviations
baseline corpus
word space
induction
expansion
word
extraction
clinical word space
expansion
word
filtering
Levenshtein
distance
normal-
ization
abbreviation-candidate expansions
evaluation
Figure 1: The abbreviation expansion process of
the current study.
similar words for each abbreviation and treating
these as initial expansions. More plausible expan-
sions of each abbreviation are then obtained by fil-
tering the expansion words and applying a normal-
ization procedure.
3.1 Data
3.1.1 Corpora
Four corpora are used in the experiments: two
clinical corpora, a medical (non-clinical) corpus
and a general Swedish corpus (Table 1).
The clinical corpora are subsets of the Stock-
holm EPR Corpus (Dalianis et al., 2009), com-
prising health records for over one million pa-
tients from 512 clinical units in the Stockholm re-
gion over a five-year period (2006-2010)
2
. One
of the clinical corpora contains records from vari-
ous clinical units, for the first five months of 2008,
henceforth referred to as SEPR, and the other con-
tains radiology examination reports, produced in
2009 and 2010, the Stockholm EPR X-ray Corpus
(Kvist and Velupillai, 2013) henceforth referred to
as SEPR-X. The clinical corpora were lemmatized
2
This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpr?ovningsnamnden i Stock-
holm), permission number 2012/2028-31/5
using Granska (Knutsson et al., 2003).
The experiments in the current study also in-
clude a medical corpus. The electronic editions of
L?akartidningen (Journal of the Swedish Medical
Association), with issues from 1996 to 2010, have
been compiled into a corpus (Kokkinakis, 2012),
here referred to as LTK.
To compare the medical texts to general
Swedish, the third version of the Stockholm Ume?a
Corpus (SUC 3.0) (K?allgren, 1998) is used. It is
a balanced corpus and consists of written Swedish
texts from the early 1990?s from various genres.
Corpus #Tokens #Types #Lemmas
SEPR 109,663,052 853,341 431,932
SEPR-X 20,290,064 200,703 162,387
LTK 24,406,549 551,456 498,811
SUC 1,166,593 97,124 65,268
Table 1: Statistical descriptions of the corpora
3.1.2 Reference standards
A list of medical abbreviation-definition pairs is
used as test data and treated as the reference stan-
dard in the evaluation. The list is derived from
Cederblom (2005) and comprises 6384 unique ab-
breviations from patient records, referrals and sci-
entific articles. To increase the size of the test
data, the 40 most frequent abbreviations are ex-
tracted by a heuristics-based clinical abbreviation
detection tool called SCAN (Isenius et al., 2012).
A domain expert validated these abbreviations and
manually provided the correct expansion(s).
An inherent property of word space models is
that they model semantic relationships between
unigrams. There are, however, abbreviations that
expand into multiword expressions. Ongoing re-
search on modeling semantic composition with
word space models exists, but, in the current study
abbreviations that expanded to multiword defini-
tions were simply removed from the test data set.
The two sets of abbreviation-expansion pairs were
merged into a single test set, containing 1231
unique entries in total.
In order to obtain statistically reliable seman-
tic relations in the word space, the terms of inter-
est must be sufficiently frequent in the data. As a
result, only abbreviation-expansion pairs with fre-
quencies over 50 in SEPR and SEPR-X, respec-
tively, were included in each test set. The SEPR
test set contains 328 entries and the SEPR-X test
97
set contains 211 entries. Each of the two test data
sets is split into a development set (80%) for model
selection, and a test set (20%) for final perfor-
mance estimation.
3.2 Expansion word extraction
For the experiments where semantically related
words were used for extraction of expansion
words, the top 100 most correlated words for each
of the abbreviations were retrieved from each of
the word space model configurations that achieved
the best results in the parameter optimization ex-
periments.
The optimal parameter settings of a word space
vary with the task and data at hand. It has been
shown that when modeling paradigmatic (e.g.,
synonymous) relations in word spaces, a fairly
small context window size is preferable (Sahlgren,
2006). Following the best results of Henriksson et
al. (2012), we experiment with window sizes of
1+1, 2+2, and 4+4.
Two word space algorithms are explored: Ran-
dom Indexing (RI), to retrieve the words that occur
in a similar context as the query term, and Random
Permutation (RP), which also incorporates word
order information when accumulating the context
vectors (Sahlgren et al., 2008). In order to exploit
the advantages of both algorithms, and to combine
models with different parameter settings, RI and
RP model combinations are also evaluated. The
models and their combinations are:
? Random Indexing (RI): words with a contextually high
similarity are returned; word order within the context
window is ignored.
? Random Permutation (RP): words that are contextu-
ally similar and used in the same relative positions are
returned; these are more likely to share grammatical
properties.
? RP-filtered RI candidates (RI RP): returns the top ten
terms in the RI model that are among the top thirty
terms in the RP model.
? RI-filtered RP candidates (RP RI): returns the top ten
terms in the RP model that are among the top thirty
terms in the RI model.
? RI and RP combination of similarity scores (RI+RP):
sums the cosine similarity scores from the two models
for each candidate term and returns the candidates with
the highest aggregate score.
All models are induced with three different con-
text window sizes for the two clinical corpora,
SEPR and SEPR-X. For each corpus, two variants
are used for word space induction, one where stop
words are removed and one where stop words are
retained. All word spaces are induced with a di-
mensionality of 1000.
For parameter optimization and model selec-
tion, the models and model combinations are
queried for semantically similar words. For each
of the abbreviations in the development set, the ten
most similar words are retrieved. Recall is com-
puted with regard to this list of candidate words,
whether the correct expansion is among these ten
candidates. Since the size of the test data is rather
limited, 3-fold cross validation is performed on
the development set for the parameter optimiza-
tion experiments. For both SEPR and SEPR-X de-
velopment sets, a combination of a RI model with
a context window size of 4+4 and a RP model with
4+4 context window size in the summing similar-
ity scores setting were among the most successful
with recall scores of 0.25 for SEPR and 0.17 for
SEPR-X.
3.3 Filtering expansion words
Given the expansion words, extracted from clini-
cal word spaces or baseline corpora (the baselines
are more thoroughly accounted for in 3.5), a filter
was applied in order to generate candidate expan-
sions. The filter was defined as a set of require-
ments, which had to be met in order for the expan-
sion word to be extracted as a candidate expansion.
The requirements were that the intitial letter of the
abbreviation and expansion word had to be iden-
tical. All the letters of the abbreviation also had
to be present in the expansion word in the same
order.
String length difference was also a part of the
requirements: the expansion word had to be at
least one character longer than the abbreviation.
In order to define an upper bound for expansion to-
ken length, string length differences of the SEPR
and SEPR-X development sets were obtained.
The distribution of string length differences for
abbreviation-expansion pairs in the SEPR devel-
opment set ranged from 1 to 21 characters. If a
maximum string length difference of 14 was al-
lowed, 95.2% of the abbreviation-expansion pairs
were covered. As for the string length differences
in the SEPR-X development set, the distribution
ranged from 1 to 21 characters. If a string length
difference of up to and including 14 characters
was allowed, 96.3% of the abbreviation-expansion
pairs were covered. Thus, a maximum difference
98
in string length of 14 was also required for the ex-
pansion word to be extracted as a candidate expan-
sion.
3.4 Levenshtein distance normalization
Given the set of filtered candidate expansions for
the abbreviations, choosing the correct one can be
seen as a normalization problem. The goal is to
map a source word to a target word, similarly to
for instance methods for spelling correction. The
target word is chosen from a list of words, and the
choice is based on the distance between the source
and the target where a small distance implies high
plausibility. However, we cannot adopt the same
assumptions as for the problem of spelling correc-
tion, where the most common distance between a
source word and the correct target word is 1 (Ku-
kich, 1992). Intuitively, we can expect that there
are abbreviations that expand to words within a
larger distance than 1. It would seem somewhat
useless to abbreviate words by one character only,
although it is not entirely improbable.
Similarly to measuring the string length differ-
ence in order to define an upper bound for filtering
candidate expansions, the Levenshtein distances
for abbreviation-expansion pairs in the develop-
ment sets were obtained.
For the SEPR and SEPR-X development sets,
allowing a Levenshtein distance up to and in-
cluding 14 covers 97.8% and 96.6% of the
abbreviation-expansion pairs, as shown in Table 2.
Given the filtered candidate expansions, the
Levenshtein distance for the abbreviation and each
of the candidate expansions were computed. For
each one of the candidate expansions, the Leven-
shtein distance beween the entry and the abbrevi-
ation was associated with the entry. The result-
ing list was sorted in ascending order according to
Levenshtein distance.
Going through the candidate expansion list, if
the Levenshtein distance was less than or identical
to the upper bound for Levenshtein distance (14),
the candidate expansion was added to the expan-
sion list that was subsequently used in the evalu-
ation. In the Levenshtein distance normalization
experiments, a combination of semantically re-
lated words and words from LTK was used. When
compiling the expansion list, semantically related
words were prioritized. This implied that word
space candidate expansion would occupy the top
positions in the expansion list, in ascending order
SEPR SEPR SEPR-X SEPR-X
LD Avg % SDev Avg % SDev
1 1 0.3 0.4 0.2
2 4.6 0.4 5 0.6
3 13 1.2 14.7 1.3
4 12.2 1 15.1 0.6
5 12.7 1.3 14.5 2.2
6 12.7 0.8 12.9 0.9
7 8.4 0.7 7.8 0.3
8 10.4 1.5 9.8 2
9 5.7 0.7 4.9 0.5
10 4.1 0.7 2.9 0.3
11 3 0.5 2.6 0.4
12 3 0.6 2.6 0.4
13 3.8 5.5 1.3 0.5
14 3.5 1.1 2.2 0.8
15 1.3 0.5 1.3 0.5
16 1.6 0.4 0.4 0.2
17 0.2 0.1
18 0.8 0.3 1 0.1
20 0.2 0.1
21 0.2 0.1 0.5 0
Table 2: Levenshtein distance distribution for
abbreviation-expansion pairs. Average proportion
over 5 folds at each Levensthein distance with
standard deviation (SDev) in SEPR and SEPR-X
development sets.
according to Levenshtein distance. The size of the
list was restricted to ten, and the remaining posi-
tions, if there were any, were populated by LTK
candidate expansions in ascending order accord-
ing to Levenshtein distance to the abbreviation. If
there were more than one candidate expansion at
a specific Levenshtein distance, ranking of these
was randomized.
3.5 Evaluation
The evaluation procedure of the abbreviation ex-
pansion implied assessing the ability of finding the
correct expansions for abbreviations. In order to
evaluate the performance gain of using semantic
similarity to produce the list of candidate expan-
sions over using the filtering and normalization
procedure alone, a baseline was created. For the
baseline, expansion words were instead extracted
from the baseline corpora, the corpus of general
Swedish SUC 3.0 and the medical corpus LTK.
A list of all the lemma forms from each baseline
99
corpus (separately) was provided for each abbre-
viation as initial expansion words. The filter and
normalization procedure was then applied to these
expansion words.
The reference standard contained abbreviation-
expansion pairs, as described in 3.1.2. If any of the
correct expansions (some of the abbreviations had
multiple correct expansions) was present in the ex-
pansion list provided for each abbreviation in the
test set, this was regarded as a true positive. Preci-
sion was computed with regard to the position of
the correct expansion in the list and the number of
expansions in the expansion list, as suggested in
Henriksson (2013). For an abbreviation that ex-
panded to one word only, this implied that the ex-
pansion list besides holding the correct expansion,
also contained nine incorrect expansions, which
was taken into account when computing precision.
The list size was static: ten expansions were pro-
vided for each abbreviation, and this resulted in
an overall low precision. Few of the abbreviations
in the development set expanded to more than one
word, giving a precision of 0.17-0.18 for all exper-
iments.
Results of baseline abbreviation expansion in
the development sets are given in table 3. Recall
is given as an average of 5 folds, as cross valida-
tion was performed. The baseline achieves over-
all low recall, with the lowest score of 0.08 for the
SEPR-X development set using SUC for candidate
expansion extraction. The rest of the recall results
are around 0.11.
Corpus SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
SUC 0.10 0.05 0.08 0.06
LTK 0.11 0.06 0.11 0.11
Table 3: Baseline average recall for SEPR and
SEPR-X development sets.
Results from abbreviation expansion using se-
mantically related words with filtering and nor-
malization to refine the selection of expansions on
SEPR and SEPR-X development sets are shown in
Table 4. Recall is given as an average of 5 folds,
as cross validation was performed. The seman-
tically related words are extracted from the word
space model configuration that had the top recall
scores in the parameter optimization experiments
described in 3.2, namely the combination of an
RI model and an RP model both with 4+4 context
window sizes. Recall is increased by 14 percent-
age points for SEPR and 20 percentage points for
SEPR-X when applying filtering and normaliza-
tion to the semantically related words.
SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
0.39 0.05 0.37 0.1
Table 4: Abbreviation expansion results for SEPR
and SEPR-X development sets using the best
model from parameter optimization experiments
(RI.4+4+RP.4+4).
4 Results
4.1 Expansion word extraction
The models and model combinations that had the
best recall scores in the word space parameter op-
timization were also evaluated on the test set. The
models that had top recall scores in 3.2 achieved
0.2 and 0.18 for SEPR and SEPR-X test sets re-
spectively, compared to 0.25 and 0.17 in the word
space parameter optimization.
4.2 Filtering expansion words and
Levenshtein normalization
Abbreviation expansion with filtering and normal-
ization was evaluated on the SEPR and SEPR-X
test sets. The results are summarized in Table 5.
SEPR SEPR-X
SUC 0.09 0.16
LTK 0.08 0.14
Expansion word extraction 0.20 0.18
Filtering and normalization 0.38 0.40
Table 5: SEPR and SEPR-X test set results in ab-
breviation expansion.
Baseline recall scores were 0.09 and 0.08 for
SUC and LTK respectively, showing a lower score
for LTK compared to the results on the SEPR de-
velopment set. For abbreviation expansion (with
filtering and normalization) using semantically re-
lated words in combination with LTK, the best re-
call score was 0.38 for the SEPR test set, com-
pared to 0.39 for the same model evaluated on the
SEPR development set. Compared to the results of
using semantically related words only (expansion
word extraction), recall increased by 18 percent-
100
age points for the same model when filtering and
normalization was applied.
Evaluation on the SEPR-X test set gave higher
recall scores for both baseline corpora compared
to the baseline results for the SEPR-X develop-
ment set: the SUC result increased by 8 percentage
points for recall. For LTK, there was an increase in
recall of 3 percentage points. For the SEPR-X test
set, recall increased by 22 percentage points when
filtering and normalization was applied to seman-
tically related words extracted from the best model
configuration.
In comparison to the results of Henriksson et
al (2012), where recall of the best model is 0.31
without and 0.42 with post-processing of the ex-
pansion words for word spaces induced from the
data set (i.e., an increase in recall by 11 percentage
points), the filtering and normalization procedure
for expansion words of the current study yielded
an increase by 18 percentage points.
5 Discussion
The filter combined with the Levenshtein normali-
sation procedure to refine candidate expansion se-
lection showed a slight improvement compared to
using post-processing, although the normalization
procedure should be elaborated in order to be able
to confidently claim that Levenshtein distance nor-
malization is a better approach to expansion candi-
date selection. A suggestion for future work is to
introduce weights based on frequently occurring
edits between abbreviations and expansions and to
apply these in abbreviation normalization.
The approach presented in this study is limited
to abbreviations that translate into one full length
word. Future research should include handling
multiword expressions, not only unigrams, in or-
der to process acronyms and initialisms.
Recall of the development sets in the word
space parameter optimization experiments showed
higher scores for SEPR (0.25) compared to SEPR-
X (0.17). An explanation to this could be that the
amount of data preprocessing done prior to word
space induction might have varied, in terms of ex-
cluding sentences with little or no clinical con-
tent. This will of course affect word space co-
occurrence information, as word context is accu-
mulated without taking sentence boundaries into
account.
The lemmatization of the clinical text used for
word space induction left some words in their
original form, causing test data and semantically
related words to be morphologically discrepant.
Lemmatization adapted to clinical text might have
improved results. Spelling errors were also fre-
quent in the clinical text, and abbreviations were
sometimes normalized into a misspelled variant of
the correct expansion. In the future, spelling cor-
rection could be added and combined with abbre-
viation expansion.
The impact that this apporach to abbreviation
expansion might have on readability of clinical
texts should also be assessed by means of an ex-
trinsic evaluation, a matter to be pursued in future
research.
6 Conclusions
We presented automatic expansion of abbrevia-
tions consisting of unigram full-length words in
clinical texts. We applied a distributional semantic
approach by using word space models and com-
bined this with Levenshtein distance measures to
choose the correct candidate among the semanti-
cally related words. The results show that the cor-
rect expansion of the abbreviation can be found
in 40% of the cases, an improvement by 24 per-
centage points compared to the baseline (0.16) and
an increase by 22 percentage points compared to
using word space models alone (0.18). Applying
Levenshtein distance to refine the selection of se-
mantically related candidate expansions yields a
total recall of 0.38 and 0.40 for radiology reports
and medical health records, respectively.
Acknowledgments
The study was partly funded by the V?ardal Fun-
dation and supported by the Swedish Foundation
for Strategic Research through the project High-
Performance Data Mining for Drug Effect Detec-
tion (ref. no. IIS11-0053) at Stockholm Univer-
sity, Sweden. The authors would also like to direct
thanks to the reviewers for valuable comments.
References
M. Adnan, J. Warren, and M. Orr. 2010. Assess-
ing text characteristics of electronic discharge sum-
maries and their implications for patient readability.
In Proceedings of the Fourth Australasian Workshop
on Health Informatics and Knowledge Management-
Volume 108, pages 77?84. Australian Computer So-
ciety, Inc.
101
H. Allvin. 2010. Patientjournalen som genre: En text-
och genreanalys om patientjournalers relation till pa-
tientdatalagen. Master?s thesis, Stockholm Univer-
sity.
H. Ao and T. Takagi. 2005. ALICE: an algorithm
to extract abbreviations from MEDLINE. Journal
of the American Medical Informatics Association,
12(5):576?586.
S. Cederblom. 2005. Medicinska f?orkortningar och
akronymer (In Swedish). Studentlitteratur.
J.T. Chang, H. Sch?utze, and R.B. Altman. 2002. Creat-
ing an online dictionary of abbreviations from med-
line. Journal of the American Medical Informatics
Association, 9:612?620.
T. Cohen and D. Widdows. 2009. Empirical dis-
tributional semantics: Methods and biomedical ap-
plications. Journal of Biomedical Informatics,
42(2):390?405.
H. Dalianis, M. Hassel, and S. Velupillai. 2009. The
Stockholm EPR Corpus ? Characteristics and some
initial findings. In Proceedings of the 14th Interna-
tional Symposium on Health Information Manage-
ment Research, pages 243?249.
D. Dann?ells. 2006. Automatic acronym recognition.
In Proceedings of the 11th conference on European
chapter of the Association for Computational Lin-
guistics (EACL), pages 167?170.
N. Elhadad. 2006. User-sensitive text summarization:
Application to the medical domain. Ph.D. thesis,
Columbia University.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in
MEDLINE. Bioinformatics, 21(18):3658?3664,
September.
Z.S. Harris. 1954. Distributional structure. Word,
10:146?162.
A. Henriksson, H. Moen, M. Skeppstedt, A. Eklund,
V. Daudaravicius, and M. Hassel. 2012. Syn-
onym Extraction of Medical Terms from Clinical
Text Using Combinations of Word Space Models.
In Proceedings of Semantic Mining in Biomedicine
(SMBM 2012), pages 10?17.
A. Henriksson, H. Moen, M. Skeppstedt, V. Daudar-
avicius, and M. Duneld. 2014. Synonym extrac-
tion and abbreviation expansion with ensembles of
semantic spaces. Journal of Biomedical Semantics,
5(6).
A. Henriksson. 2013. Semantic Spaces of Clini-
cal Text: Leveraging Distributional Semantics for
Natural Language Processing of Electronic Health
Records. Licentiate thesis, Department of Computer
and Systems Sciences, Stockholm University.
N. Isenius, S. Velupillai, and M. Kvist. 2012.
Initial Results in the Development of SCAN: a
Swedish Clinical Abbreviation Normalizer. In Pro-
ceedings of the CLEF 2012 Workshop on Cross-
Language Evaluation of Methods, Applications, and
Resources for eHealth Document Analysis (CLEFe-
Health2012).
G. K?allgren. 1998. Documentation of the Stockholm-
Ume?a corpus. Department of Linguistics, Stockholm
University.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In Proceedings of the 22nd annual con-
ference of the cognitive science society, page 1036.
A. Keselman, L. Slaughter, C. Arnott-Smith, H. Kim,
G. Divita, A. Browne, C. Tsai, and Q. Zeng-Treitler.
2007. Towards consumer-friendly PHRs: patients
experience with reviewing their health records.
In AMIA Annual Symposium Proceedings, volume
2007, pages 399?403.
O. Knutsson, J. Bigert, and V. Kann. 2003. A ro-
bust shallow parser for Swedish. In Proceedings of
Nodalida.
D. Kokkinakis. 2012. The Journal of the
Swedish Medical Association-a Corpus Resource
for Biomedical Text Mining in Swedish. In Pro-
ceedings of Third Workshop on Building and Eval-
uating Resources for Biomedical Text Mining Work-
shop Programme, page 40.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys
(CSUR), 24(4):377?439.
M. Kvist and S. Velupillai. 2013. Professional Lan-
guage in Swedish Radiology Reports ? Charac-
terization for Patient-Adapted Text Simplification.
In Scandinavian Conference on Health Informatics
2013, pages 55?59.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
D. Movshovitz-Attias and W.W. Cohen. 2012.
Alignment-HMM-based Extraction of Abbrevia-
tions from Biomedical Text. In Proceedings of the
2012 Workshop on Biomedical Natural Language
Processing (BioNLP 2012), pages 47?55.
N. Nakatsu, Y. Kambayashi, and S. Yajima. 1982. A
longest common subsequence algorithm suitable for
similar text strings. Acta Informatica, 18(2):171?
179.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for
finding abbreviations and their definitions. In Pro-
ceedings of the 2001 conference on empirical meth-
ods in natural language processing, pages 126?133.
102
E. Pettersson, B. Megyesi, and J. Nivre. 2013. Nor-
malisation of historical text using context-sensitive
weighted levenshtein distance and compound split-
ting. In Proceedings of the 19th Nordic Conference
of Computational Linguistics (NODALIDA 2013),
pages 163?179.
R.E. Rudd, B.A. Moeykens, and T.C. Colton. 1999.
Health and literacy: a review of medical and pub-
lic health literature. Office of Educational Research
and Improvement.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society, pages 1300?1305.
M. Sahlgren. 2006. The Word-space model. Ph.D.
thesis, Stockholm University.
A.S. Schwartz and M.A. Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of Pacific Sympo-
sium on Biocomputing, pages 451?462.
M. Skeppstedt. 2012. From Disorder to Order: Ex-
tracting clinical findings from unstructured text. Li-
centiate thesis, Department of Computer and Sys-
tems Sciences, Stockholm University.
K. Taghva and J. Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition,
1(4):191?198.
H. Yu, G. Hripcsak, and C. Friedman. 2002. Map-
ping abbreviations to full forms in biomedical arti-
cles. Journal of the American Medical Informatics
Association, 9(3):262?272.
103
