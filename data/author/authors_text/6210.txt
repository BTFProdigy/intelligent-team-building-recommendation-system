Information structure and pauses in a corpus of spoken Danish
Patrizia Paggio
Centre for Language Technology
University of Copehagen
Denmark
patrizia@cst.dk
Abstract
This paper describes a study in which a
corpus of spoken Danish annotated with
focus and topic tags was used to inves-
tigate the relation between information
structure and pauses. The results show
that intra-clausal pauses in the focus do-
main, tend to precede those words that
express the property or semantic type
whereby the object in focus is distin-
guished from other ones in the domain.
1 Introduction
The interest for corpora annotated with infor-
mation structure has been raised recently by
several authors. Kruijff-Korbayova? and Kruijff
(2004) describe a method where a rich discourse-
level annotation is used to investigate informa-
tion structure, while both Postolache (2005) and
Diderichsen and Elming (2005) study the appli-
cation of machine learning to the problem of au-
tomatic identification of topic and focus. In this
study, on the contrary, information structure is
annotated manually, and the annotation is used
to investigate the correlation between informa-
tion structure tags and intra-clausal pauses.
2 Annotating information structure
The starting point for this study was the corpus
of spoken Danish ?DanPass? (Gr?nnum, 2005),
a collection of 54 monologues produced by 18
different subjects dealing with three well-defined
tasks, following the methodology established in
Terken (1985). In the first task, the subjects de-
scribe a geometrical network, in the second the
process of assembling the drawing of a house
out of existing pieces, and in the third they solve
a map task. The corpus has been annotated
with several annotation tiers, including orthogra-
phy, phonetic transcription, pauses and PoS-tags.
Two independent annotators added then tags for
focus and topic based on a set of simple guide-
lines, and using the Praat tool to carry out the
annotation.
The annotation reflects the assumption that a
sentence can be divided into an obligatory fo-
cus part, which expresses the non-presupposed
information, and a presupposed background part.
A referent in the background part may function
as the sentence topic in the sense of Lambrecht
(1994). For each sentence in the corpus, the an-
notators were asked to identify what they intu-
itively considered non-presupposed information
and annotate it as belonging to the focus. Techni-
cally, each word belonging to the focus is added
a focus tag. The annotators were also asked to
test whether they could single out a sentence ref-
erent by means of the ?What about X? test (Rein-
hart, 1981). If they could, they were asked to add
topic tags to all the words making up the corre-
sponding expression. Words not bearing any tag
are considered part of the background.
The guidelines did not contain any reference
to pausing, nor did the annotators know that their
work would be used to study the correlation be-
191
Focus Topic No tag Total
Network C1 1608 268 2526 4402
Network C2 1889 287 2226 4402
House C1 4025 386 4151 8562
House C2 4193 377 3992 8562
Table 1: Tags in two corpus sections
tween pauses and information structure. In fact,
that was not the purpose of the annotation work,
which is of more general interest. It should also
be noted that the annotators were not explicitly
instructed to code phrases, since we did not want
to make the assumption that topic or focus nec-
essarily correspond to syntactic phrases. Ap-
proximately two person months were spent an-
notating two sections of the corpus. The kappa
score varied between 0.7 to 0.8 depending on
the corpus section, showing an acceptable inter-
annotator agreement. Most disagreements relate
to the identification of the focus left-hand bound-
ary, where one of the annotators sometimes iden-
tified wider focus domains than the other. These
differences have not been inspected yet, but will
be used to revise the guidelines to produce a
unique consistent annotation. Table (1) shows
the number of tags assigned by the two coders
(C1 and C2) in the two sections of the corpus
coded so far.
Below, an example of an annotated tier is
shown in a linearised format (the textgrids out-
put by Praat also contain time intervals that link
the transcription to the sound file):
(1) + ovenover + er der en/F + gr?n/F cirkel/F
= og oven over den/T gr?nne/T cirkel/T er
der en/F + lilla/F trekant/F +
?PAUSE above PAUSE there is [F a PAUSE
green circle] PAUSE and above [T the green
circle] there is [F a PAUSE purple triangle]?
The example consists of two sentences. In the
first, the annotator has tagged ?en gr?n cirkel? (a
green circle) as the focus; in the second, ?den
gr?nne cirkel? (the green circle) has been tagged
as the topic, while ?en lilla trekant? (a purple tri-
angle) is tagged as the focus. Pauses are indi-
cated by ?+? and ?=?. The former is a silent pause,
and the latter a pause accompanied by a sound,
like ?hmm?. Pauses were already available in the
orthographic transcription of the corpus, which
was produced earlier by different annotators.
3 Pauses in earlier studies
The material annotated so far already gives us
the possibility to investigate whether there is a
significant relation between pauses and informa-
tion structure. Earlier studies (Jensen, 2005)
(Hansen et al, 1993) investigated the effect of
syntactic boundaries (clausal as well as phrasal)
on the placing of pauses in spoken Danish. In
the first study, it is found that more than 55%
of the pauses co-occur with clause boundaries,
12% with phrase boundaries, and the remain-
ing 33% occur within phrases or in conjunction
with repairs, interjections and enumerations. It
is also noted that pauses falling within a syntac-
tic phrase tend to be placed in the final part of
the sentence. The second study confirms this ob-
servation by showing that 60% of the pauses that
do not co-occur with syntactic boundaries occur
within the last 40% of the sentence (measured in
number of syllables). The authors of both inves-
tigations make the hypothesis that information
structure may have an effect on the occurrence
of pauses within clauses. However, the empiri-
cal material used in those works is not annotated
with respect to information structure, and there-
fore, no conclusive claim could be made. In ad-
dition, the data used in Hansen et al(1993) come
from news reading, and are thus essentially writ-
ten language although delivered orally.
4 Pauses and focusing in Danish
The purpose of this pilot study is, on the basis
of the annotated DanPass corpus, to verify i. to
what degree pauses tend to be associated with fo-
cus and topic, and ii. where in the focus domain
pauses tend to occur, particularly whether pauses
are used to mark the left-hand focus boundary.
Since we already know from the studies cited
above that there is a strong tendency for pauses
to coincide with clause boundaries, we decided
192
F word T word No tag Total
Pause 20.29 7.59 39.70 28.34
No pause 79.71 92.41 60.30 71.66
Total 100 100 100 100
Table 2: Distribution of pauses over information
structure categories (%)
to exclude those from the study, and only look
at pauses that occur within clauses. So far, the
investigation has been carried out for the network
description part of the corpus, and only for the
data produced by one of the coders.
The first question ? whether pauses relate to
words coded as either focus or topic ? was inves-
tigated by counting, out of a total 3659 words,
how many words tagged as either F or T, or bear-
ing no tag, are preceded by a pause (silent or non
silent). The results, shown in Tables (2), seem
to disconfirm the hypothesis that there should
be a correlation between pauses and information
structure categories, or at least that a correlation,
if it exists, can be expressed by looking at the fre-
quency with which pauses precede focus or topic
words. In fact, over 65% of the intra-clausal
pauses in the material precede untagged words,
and the observed frequency of a pause before a
focus or a topic word is lower than the average
28.34% (baseline).
Since we know that topics often occur
sentence-initially, the results in the tables are
misleading in that only intra-clausal pauses are
taken into consideration. Therefore we also
looked at what percentage of topic words are
succeeded rather than preceded by a pause, and
found that 33.50% are. This figure is interesting,
but needs further investigations.
Now we zoom in on the focus domain. First
of all, we look at pause distribution across dif-
ferent part-of-speech categories, again by in-
specting the pauses preceding words. Table (3)
shows the frequency with which different part-
of-speech categories occurring in the focus do-
main (i.e. tagged ?F?) are preceded by a pause.
The total no. of words considered is 1661.
The interesting fact that emerges is that adjec-
tives have a remarkably higher probability to be
preceded by a pause than any of the other cate-
gory, and also a clearly higher probability than
the average 28.34%.
We then looked at the first pause in the fo-
cus domain. The first pause falls before the first
focus word in only 30% of the cases. In other
words, it does not seem to mark the left-hand
boundary of the focus domain. By running a de-
cision tree generator (Witten and Eibe, 2005) on
the data, we found that the strongest rule learnt
by the system was one that places the first pause
in the focus domain between a determiner and an
adjective (2). Another rule predicts that a pause
will fall between an adjective and a noun (3).
(2) tilbage er der... en/F + r?d/F firkant/F
?left there is... [F a PAUSE red square]?
(3) til venstre... l?gger du en/F r?d/F +
firkant/F
?to the left... you put [F a red PAUSE
square]?
The two rules reflect a strong characteristic of
the monologues under investigation, where the
speakers have to draw the listener?s attention to
the various geometrical figures in the network
they are describing. To tell them apart from each
other, they either use the colour of the figure or
its shape. In other words, the pauses occurring
in the focus domain tend to precede the word
that expresses what Dik (1989) calls selecting fo-
cus, here an adjective that, by defining a selecting
property or type, helps distinguishing the object
in focus from other similar ones. From the point
of view of accentuation, however, the adjective is
not more prominent than the noun, and is there-
fore not annotated as the only word in focus.
5 Conclusions and further research
In conclusion, the pilot study shows that words
making up the topic or the focus of a sentence
do not show a general tendency to be preceded
by pauses. However, preliminary results indicate
that topics tend to be followed by pauses. Fur-
thermore, words belonging to specific syntatic
193
Adj Adv Conj Det N Prep Part Pro Verb Other Total
Pause 36.34 6.94 16.67 18.97 17.11 19.83 25.00 4.76 6.33 20.00 20.29
No pause 63.66 93.06 83.33 81.03 82.89 80.17 75.00 95.24 93.67 80.00 79.71
Total 100 100 100 100 100 100 100 100 100 100 100
Table 3: Distribution of pauses over part-of-speech categories in the focus domain (%)
categories may have a significantly higher proba-
bility to be preceded by a pause than a randomly
chosen word. In the corpus we have worked
with, these words express the property or seman-
tic type whereby the object in focus can be dis-
tinguished from other similar objects. In other
words, the system by which Danish speakers use
pauses seems sensitive to information structure
in a subtle way that, at least as far as focus is
concerned, creates boundaries that do not neces-
sarily correspond to those between syntactic con-
stituents.
An interesting issue we haven?t yet ad-
dressed is whether intra-clausal pauses relate to
prosodic phrases, which according to Steedman
(2001) correspond to information structural con-
stituents. Since the DanPass annotation also
foresees a tier for prosodic phrases, this inves-
tigation is possible. Furthermore, we want to
test whether there are differences in the way in
which different users relate pauses to topic es-
tablishment and focusing. We know already now
that the percentage of pauses per word varies
across speakers, and that speakers? individual
pause rates do not vary much depending on the
task. The corpus provides a very nice means of
studying whether they use pauses for different
purposes.
Acknowledgements
This work was supported by the Carlsberg Foun-
dation.
References
Philip Diderichsen and Jakob Elming. 2005. A
corpus-based approach to topic in Danish dialog.
In Proceedings of the ACL Student Research Work-
shop, pages 119?114. Ann Arbor Michigan, June.
Simon Dik. 1989. The Theory of Functional Gram-
mar. Functional Grammar Series. Dordrecht:
Foris Publications.
Nina Gr?nnum. 2005. DanPASS - Danish phoneti-
cally annotated spontaneous speech. Talk given at
FONETIK 2005 in Gothemburg, May.
Peter Molb?k Hansen, Niels Reinholt Petersen, and
Ebbe Spang-Hanssen. 1993. Syntactic boundaries
and pauses in read-aloud Danish prose. In Bjo?rn
Granstro?m and Lennart Nord, editors, Nordic
Prosody VI. Papers from a symposium, pages 159?
172. Stockholm: Almqvist and Wiksell Interna-
tional.
Anne Jensen. 2005. Clause Linkage in Spoken Dan-
ish. Ph.D. thesis, Department of General and Ap-
plied Linguistics, University of Copenhagen, July.
Ivana Kruijff-Korbayova? and Geert-Jan M. Kruijff.
2004. Discourse-level annotation for investigating
information structure. In Proceedings of the ACL
Workshop on Discourse Annotation.
Knud Lambrecht. 1994. Information Structure and
Sentence Form. Cambridge Studies in Linguistics.
Cambridge: Cambridge University Press.
Oana Postolache. 2005. Learning information struc-
ture in the Prague treebank. In Proceedings of
the ACL Student Research Workshop. Ann Arbor,
Michigan, June.
Tanya Reinhart. 1981. Pragmatics and linguis-
tics: an analysis of sentence topics. Philosophica,
27(1):53?94.
Mark Steedman. 2001. Information-structural se-
mantics for English intonation. In Proceedings of
LSA Summer Institute Workshop on Topic and Fo-
cus. Santa Barbara, July.
Jacques M. B. Terken. 1985. Use and Function of
Accentuation: Some Experiments. Ph.D. thesis,
Leiden University, September.
Ian H. Witten and Frank Eibe. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann: San Francisco, 2nd edition.
194
Ontological resources and question answering 
 
Roberto Basili (*), Dorte H. Hansen (**),Patrizia Paggio (**),   
Maria Teresa Pazienza (*), Fabio Massimo Zanzotto (*) 
 
 
(*) Dip. di Informatica Sistemi e Produzione 
University of Rome ?Tor Vergata? 
{basili,pazienza,zanzotto} 
@info.uniroma2.it  
 
 
(**) Centre for Language Technol-
ogy 
University of Copenhagen 
{patrizia,dorte}@cst.dk 
 
 
Abstract 
This paper discusses the possibility of build-
ing an ontology-based question answering 
system in the context of the Semantic Web 
presenting a proof-of-concept system. The 
system is under development in the MOSES 
European Project.  
Introduction  
Question Answering (QA) systems (as QA track of 
the Text Retrieval Conference (TREC-QA) competi-
tions (Voorhees 2001)), are able both to understand 
questions in natural language and to produce answers in 
the form of selected paragraphs extracted from very 
large collections of text. Generally, they are open-
domain systems, and do not rely on specialised concep-
tual knowledge as they use a mixture of statistical tech-
niques and shallow linguistic analysis. Ontological 
Question Answering systems, e.g. (Woods et al 1972, 
Zajac 2000) propose to attack the problem by means of 
an internal unambiguous knowledge representation. As 
any knowledge intensive application, ontological QA 
systems have as intrinsic limitation related to the small 
scale of the underlying syntactic-semantic models of 
natural language. 
 
While limitations are well-known, we are still ques-
tioning if any improvement has occurred  since the de-
velopment of the first ontological QA system LUNAR. 
Several important facts have emerged that could influ-
ence related research approaches: 
 
 a growing availability of lexical knowledge bases 
that model and structure words: WordNet (Miller 
1995) and EuroWordNet (Vossen 1998) among 
others; some open-domain QA systems have proven 
the usefulness of these resources, e.g. WordNet in 
the system described in (Harabagiu et al 2001). 
 
 the vision of a Web populated by ?ontologically? 
tagged documents which the semantic Web initia-
tive has promoted; in case this vision becomes a re-
ality, it will require a world-wide collaborative 
work for building interrelated ?conceptualisations? 
of domain specific knowledge 
 
 the trend in building shallow, modular, and robust 
natural language processing systems (Abney 1996,  
Hobbs et al 1996, Ait-Moktar&Chanod 1997, 
Basili&Zanzotto 2002) which is making them ap-
pealing in the context of ontological QA systems, 
both for text interpretation (Andreasen et al 2002) 
and for database access (Popescu et al 2003). 
 Given this background, we  are investigating a new 
approach to ontology-based QA in which users ask 
questions in natural language to knowledge bases of 
facts extracted from a federation of Web sites and or-
ganised in topic map repositories (Garshol 2003). Our 
approach is being investigated in the context of EU pro-
ject MOSES1, with the explicit objective of developing 
an ontology-based methodology to search, create, main-
tain and adapt semantically structured Web contents 
according to the vision of the Semantic Web. MOSES is 
taking advantage of expertise coming from several 
fields: software agent technology, NLP, graph theory 
                                                           
1
 MOSES is a cooperative project under the 5th Frame-
work Programme. The project partners are FINSA Consult-
ing, MONDECA, Centre for Language Technology, 
University of Copenhagen, University of Roma Tre, Univer-
sity of Roma Tor Vergata and ParaBotS. 
and text mining. The test-bed chosen in the project is 
related to the development of an ontology-based knowl-
edge management system and an ontology-based search 
engine that will both accept questions and produce an-
swers in natural language for the Web sites of two 
European universities. The challenges of the project are:  
 
 building an ontological QA system;  
 
 developing a multilingual environment which im-
plies the ability to treat several languages, and, im-
portantly, several conceptualisations.    
 
In this paper, after briefly describing how the project 
is trying to comply with the semantic Web vision, we 
will focus on question processing, and in particular on 
the way in which NLP techniques and ontological 
knowledge interact in order to support questions to spe-
cific sites or to site federations.  
 
An ontology-based approach to question 
answering 
 
In our ontological QA system, both questions and 
domain knowledge are represented by the same onto-
logical language. It is foreseen to develop the QA sys-
tem in two steps. First a prototypical implementation is 
planned to answer questions related to the current 
?state-of-affairs? of the site to which the question is 
posed. In a second step, given a ?federation? of sites 
within the same domain, we will  investigate whether 
and how an ontological approach could support QA 
across the sites. Answering a question can then be seen 
as a collaborative task between ontological nodes be-
longing to the same QA system. Since each node has its 
own version of the domain ontology, the task of passing 
a question from node to node may be reduced to a map-
ping task between (similar) conceptual representations. 
To make such an approach feasible, a number of diffi-
cult problems must still be solved. In this paper, we will 
provide details on how: 
 
 to build on existing ontologies and interface be-
tween them and language resources;  
 
 to interpret questions wrt the ontological language;  
 
 to model the mapping task for federated questions. 
 
Building on off-the-shelf semantic Web on-
tologies 
One of the results of the Semantic Web initiative 
will be the production of many interrelated domain-
specific ontologies that provide the formal language for 
describing the content of Web documents. In spite of the 
freedom allowed in the production of new conceptuali-
sations, it is reasonable to expect that a first knowledge 
representation jungle will leave room to a more orderly 
place where only the more appreciated conceptualisa-
tions have survived. This is a prerequisite for achieving 
interoperability among software agents. In view of this, 
and since publicly available non-toy ontology examples 
are already available, the effort of adapting an existing 
ontology to a specific application is both useful and 
possible. This experiment is being conducted in MOSES 
to treat the university domain.   
 
Ontologies for the Semantic Web are written in for-
mal languages (OWL, DAML+OIL, SHOE) that are 
generalisations/restrictions of Description Logics 
(Baader et al 2003). TBox assertions describe concepts 
and relations. A typical entry for a concept is: 
 
ID Course 
Label Course 
Subclassof Work 
 
Table 1 A concept 
 
where ID is the concept unique identifier, label is 
the readable name of the concept,  subclassof indicates 
the relation to another class. As the label has the only 
purpose of highlighting the concept to human readers, 
alternative linguistic expressions are not represented. 
On the contrary, this piece of information is recorded in 
a lexical data base like WordNet. The problem is even 
more obvious when considering relationships.  
 
ID teacherOf 
Label Teaches 
Domain #Faculty 
Range #Course 
 
Table 2 A relationship 
 
In Table 2, domain and range contain the two con-
cepts related to the described binary relation. The label 
teacherOf does not mention alternative linguistic ex-
pressions like: #Faculty gives #Course or #Faculty de-
livers #Course, etc. 
  
For the ontology producers, only one concept or re-
lation name is sufficient. Synonymy is not a relevant 
phenomenon in ontological representations. In fact, it is 
considered a possible generator of unnecessary concept 
name clashes, i.e. concept name ambiguity.  Conceptu-
alisations (as in tables 1,2) are inherently weak when-
ever used to define linguistic models for NLP 
applications. Interpreting questions like: 
 
(1) Who gives/teaches the database class/course 
this year?  
 
with respect to a university domain ontology means 
in fact mapping all the questions onto the concepts and 
relations in Table 2. There is a gap to be filled between 
linguistic and ontological ways of expressing the do-
main knowledge.  
Linguistic interfaces to ontologies 
 In developing an ontological QA system, the main 
problem is to build what we call the ?linguistic inter-
face? to the ontology which consists of all the  linguistic 
expressions used to convey concepts and relationships. 
To make this attempt viable, we are currently studying 
methods to automatically relate lexical knowledge bases 
like WordNet (Miller 1995) to domain ontologies 
(Basili et al2003a) and to induce syntactic-semantic 
patterns for relationships (Basili et al2003b). 
 
The linguistic interface constitutes the basis on 
which to build the semantic model of the natural lan-
guage processing sub-system. One way of conceiving 
such a model is in terms of syntactic-semantic mapping 
rules that apply to alternative expressions of the same 
conceptual knowledge.  The amount of syntactic analy-
sis such rules  foresee will vary according to the ap-
proach chosen.   
 
Classifying questions  
To facilitate recognition of what are the relevant ex-
pressions to be encoded in the linguistic interface, we 
have introduced a classification of the possible ques-
tions that the system is expected to support. A classifi-
cation often quoted is that in Lauer, Peacocok  and 
Graesser (1992), which mainly builds on speech act 
theory. Another influential, more syntactically-oriented 
approach is that in Moldovan et al (1999) where to each 
syntactic category correspond one or several possible 
answer types, or focuses (a person, a date, a name, etc.).   
 
Several dimensions have been identified as relevant 
for MOSES 
1. the number of sites and pages in which the an-
swer is to be found. Thus, a first distinction is 
done between site-specific and federated ques-
tions. In the first case, analysis involves only 
one language and one knowledge domain. In 
the second, the interpretation of a question 
produced by a local linguistic analyser is 
matched against the knowledge domain of 
other sites; 
2. sub-domain coverage  (e.g. people, courses, re-
search).  
3. format of the answer: which in MOSES is not 
only a text paragraph as in standard QA, but 
could also be composed of one or more in-
stances of semantic concepts (professors, 
courses) or relations (courses being taught by 
specific professors), whole Web pages, tables, 
etc. due to the heterogeneity of information 
sources  
These dimensions have been explored in ?question 
cards? defined by the project?s user groups2.  
 
FORM 1 
Input Hvem underviser i filmhistorie  
(Who teaches film history) 
Syntactic 
type 
Who (Hvem) 
Syntactic 
subtype 
V ? copula 
CONTENT  
Focus 
constraint 
Teacher 
Concepts  Faculty 
Course.Name: history of film 
Relations TeacherOf(Faculty, Course) 
Answer 
count  
List 
 
Table 3: Example of question classification 
 
From the point of view of the linguistic analysis, 
however, syntactic category and content are the central 
dimensions of sentence classification. Syntactic catego-
ries are e.g. yes/no question, what-question, who-
question, etc. Subtypes  relate to the position inside the 
question where the focus is expressed, e.g. depending 
on whether the wh-pronoun is a determiner, or the main 
verb is a copula. The content consists of concepts and 
relations from the ontology, the focus constraint3 (the 
ontological type being questioned), and a count feature 
indicating the number of instances to be retrieved. Table 
3 shows an example of linguistic classification. For each 
sentence type, several paraphrases are described.  
 
Ontology Mapping in a Multilingual Envi-
ronment: challenges  
The conceptualisation of the university world  as it 
appears in the DAML+OIL ontology library is an inter-
esting representation for the application scenarios tar-
geted in MOSES (i.e. People/Course/Research).  
Described classes and relations cover in fact, at least at 
a high level, most of the relevant concepts of the  ana-
lysed scenarios.  Such an ontology has been adapted to 
develop conceptualisations for each of the two national 
                                                           
2
 The University of Roma III and the Faculty of Hu-
manities at the University of Copenhagen. 
3
 In the sense of Rooth (1992). 
university sub-systems (i.e. Italian and Danish) while 
providing additional information required for answering 
the input questions. This is temporal information or 
other kind of information at a border line with the do-
main, (e.g. concepts related to the job market). A first 
important matter we have dealt with is  the language. 
Whereas concept and relation labels in the Italian ontol-
ogy are expressed either in English (for concepts di-
rectly taken from the original source) or in Italian, in the 
Danish counterpart all labels are in Danish. This means 
that a mapping algorithm making use of string similarity 
measures applied to concept labels will have to work 
with translation, either directly between the two lan-
guages involved, or via a pivot language like English. 
The goal would be to establish correspondences such as 
?Lektor?    (?AssociateProfessor?)    ?ProfessoreAsso-
ciato?. 
Another problem is related to structural differences: 
not all the nodes in  an ontology are represented also in 
the other and vice-versa,  moreover nodes that are 
somehow equivalent, may have different structural 
placements. This is the case for the ?Lek-
tor?/?ProfessoreAssociato? pair just mentioned: in the 
Danish system, ?Lektor? is not a subclass of ?Professor?, 
although ?associate professor? is considered a correct 
translation.  
Question analysis  
 
Question analysis is carried out in the MOSES lin-
guistic module associated with each system node. To 
adhere to the semantic Web approach, MOSES poses no 
specific constraints on how the conceptual representa-
tion should be produced, nor on the format of the output 
of each linguistic module. The agent that passes this 
output to the content matcher (an ontology-based search 
engine) maps the linguistic representation onto a com-
mon MOSES interchange formalism (still in an early 
development phase). Two independent modules have 
been developed for Danish and Italian language analy-
sis. They have a similar architecture  (both use preproc-
essing, i.e. POS-tagging and lemmatising, prior to 
syntactic and semantic analyses), but specific parsers. 
Whereas the Danish parser, an adapted version of PET 
(Callmeier 2000) produces typed feature structures 
(Copestake 2002), the Italian one outputs quasi-logical 
forms. Both representation types have proven adequate 
to express the desired conceptual content. As an exam-
ple, the Italian analysis module is described below. 
Analysis of Italian questions 
Analysis of Italian questions is carried out by using 
two different linguistic interpretation levels. The syntac-
tic interpretation is built by a general purpose robust 
syntactic analyser, i.e. Chaos (Basili&Zanzotto 2002). 
This will produce a Question Quasi-Logical Form (Q-
QLF) of an input question based on the extended de-
pendency graph formalism (XDG) introduced in 
(Basili&Zanzotto 2002).  In this formalism, the syntac-
tic model of the sentence is represented via a planar 
graph  where nodes represent constituents and arcs the 
relationships between them. Constituents produced are 
chunks, i.e. kernels of verb phrases (VPK), noun 
phrases (NPK), prepositional phrases (PPK) and adjec-
tival phrases (ADJK). Relations among the constituents 
represent their grammatical functions: logical subjects 
(lsubj), logical objects (lobj), and prepositional modifi-
ers. For example, the Q-QLF of the question 
 
(2) Chi insegna il corso di Database? 
 (Who teaches the database course?) 
 
is shown in Figure 1.  
 
 lsubj lobj di 
NPK NPK VPK PPK 
[Chi] [insegna] [il corso] [di Database][?] 
 
Figure 1 A Q-QLF within the XDG formalism 
Then a robust semantic analyser, namely the Dis-
course Interpreter from LaSIE (Humphreys et al 1996) 
is applied. An internal world model  has been used to 
represent the way in which the relevant concepts (i.e. 
objects) and relationships (i.e. events) are associated 
with linguistic forms (see Figure 2).  Under the object 
node, concepts from the domain concept hierarchy are 
mapped onto synsets (sets of synonyms) in the linguistic 
hierarchy EWN (i.e. the EuroWordNet.base concepts). 
This is to guarantee that linguistic reasoning analysis is 
made using general linguistic knowledge. 
 
Events 
Objects 
Domain 
Concept  
Hierarchy 
WN1.6:EWN 
Base Concepts 
 
Figure 2 The world model taxonomy 
 TEACH_EVENT ==> teach_course. 
teach_course ==> tenere v insegnare v fare. 
 
props(teach_course(E),[ 
 (consequence(E, 
 [relation(E,teacherOf),r_arg1(E,X),r_arg2(E,Z)] ):- 
  nodeprop(E,lsubj(E,X)),  
X <- ewn4123(_),   /* human_1 */ 
  nodeprop(E,lobj(E,Z)),  
Z <- ewn567704(_)  /* education_1 */ 
 ) 
]). 
 
Figure 3 Example of syntactic-semantic inter-
pretation rule 
 
The association of objects and events with linguistic 
forms is used in matching rules as shown in Figure 3. 
The rule expresses the fact that, if any word like tenere, 
insegnare or fare is encountered in relation with a hu-
man_1 (represented by the base concept ewn4123) and 
the word education_1 (ewn567704),  the relation teach-
erOf can be induced.  
 
The analysis resulting for sentence (2) is then: 
 
focus(e2), 
relation(e1,teacherOf), 
r_arg1(e1, person_dch(e2)), 
r_arg2(e1,course_dch(e3)), 
relation(e4,hasSubject), 
r_arg1(e4, course_dch(e3)), 
r_arg2(e4,topic_dch("Database")). 
 
 
This means that the user is interested in a person, the 
entity e2 of the class person_dch, that is in a relation 
teacherOf with the entity e4 (instance of the class 
course_dch), that is in turn related by hasSubject 
with the topic (i.e. topic_dch) "Database". This result 
can be passed on to the content matcher. 
 
Treating federated questions  
Now we want to extend this approach to question 
analysis in order to manage federated questions. A pos-
sible solution would be sending the natural language 
question to several nodes and let each node interpret it 
against its own domain knowledge. This is unfeasible in 
a multilingual environment. The solution we are inves-
tigating is based on the notion of ontology mapping. Let 
us consider the case of a student questioning   not only 
the Danish but also the Italian site (by selecting specific 
modalities for entering questions):  
 
(3) Hvem er lektor i fransk? 
(Who is associate professor of French?) 
 
As the question is in Danish, it has to be analysed by 
the Danish analysis component, which will produce a 
semantic interpretation roughly corresponding to the 
following term: 
 
all(x) (lektor(x) & CourseOffer(x,y) & 
Course(y) & Name(y, French))4 
 
Since all concepts and relations come from the Dan-
ish ontology, it is not a problem to query the Danish 
knowledge base for all relevant examples. In order to 
query the Italian knowledge base, however, equivalent 
concepts and relations must be substituted for those in 
the ?Danish? interpretation. The corresponding Italian 
representation is: 
 
all(x) (ProfessoreAssociato(x) & 
TeacherOf(x,y) & Course(y) &  
Subject(y, French)) 
 
The first problem is establishing a correspondence 
between ?lektor? and ?ProfessoreAssociato?, which as 
shown in the ontology fragments below are not structur-
ally equivalent.  
As suggested in (Pazienza&Vindigni 2003, Med-
che&Staab 2001), equivalence relations must be estab-
lished by considering is-a structures and lexical concept 
labels together. In the example under discussion, an 
initial equivalence can be posited between the top nodes 
of the two ontology fragments, since they both refer 
explicitly to the original DAML+OIL ontology via a 
sameAs relation. However, none of the concept labels 
under ?Faculty? in the Italian ontology are acceptable 
translations of ?Lektor?, nor do any of the nodes refer to 
common nodes in a common reference ontology. Thus, 
the matching algorithm must search further down for en 
equivalent concept by considering possible translations 
of concept labels and testing the relations that equiva-
lence candidates participate in. Thus, distance from a 
common starting node, lexical equivalence and occur-
rence in similar relations are all constraints to be con-
sidered. 
 
                                                           
4
 All concepts and relations will in fact be expressed in 
Danish. Here,to facilitate non-Danish readers, we are using 
English equivalents with the exception of the concept ?Lek-
tor?  under discussion. 
 L?rersta
 
Professorat 
(Professorship) 
Lektor 
(Associate 
Professor) 
Adjunkt 
(Assistant 
 Professor) 
 
? 
Professor 
(FullProfessor) 
G?steProfessor 
(GuestProfessor) 
 
Faculty 
Professore 
(Tenured 
Professor) 
TitolareCorso 
(Teaching 
Assistant) 
Ricercatore 
(Research 
Assistant) 
 
? 
ProfessoreAssociato 
(Associated 
Professor) 
Ordinario 
(FullProfessor) 
 
 
Figure 4: The ?Faculty? Danish and Italian sub-ontologies 
 
The same problem of finding a correct mapping ap-
pears for the relations. In this case, we must be able to 
discover that CourseOffer and TeacherOf  represent the 
same relation. For instance we can rely on the fact that 
they have both two roles, and the concepts filling these 
roles, Faculty and Course (or rather the Danish and Ital-
ian equivalent concepts) correspond. Discovering simi-
larities between relations, however, may be a much 
more complex task than shown in this example. In gen-
eral, it presupposes the ability to map between concepts. 
 
Conclusion  
Our focus in this paper has been, in the context of 
ontology-based QA, to discuss how to interface between 
ontology and linguistic resources on the one hand, and 
ontology and natural language questions on the other 
while remaining within a unique framework. An inter-
esting issue in a multilingual environment is how to 
support questions to federation of sites organised around 
local ontologies.  We have begun to address this issue in 
terms of ontology mapping.  Specific algorithms for 
machine learning and information extraction have also 
been identified and are under development. 
 
References 
Steven Abney (1996) Part-of-speech tagging and 
partial parsing. In G.Bloothooft K.Church, S.Young, 
editor, Corpus-based methods in language and speech. 
Kluwer academic publishers, Dordrecht.  
 
Salah Ait-Mokhtar and Jean-Pierre Chanod. (1997) 
Incremental Finite-state parsing. In Proceedings of 
ANLP97, Washington.  
 
Andreasen, Troels,  Per Anker Jensen, J?rgen F. 
Nilsson, Patrizia Paggio, Bolette Sandford Pedersen and 
Hanne Erdman Thomsen (2002) Ontological Extraction 
of Content for Text Querying, in Natural Language 
Processing and Information Systems, Revised Papers of 
NLDB 2002. Springer-Verlag, pp. 123?136. 
 Baader, F., D. Calvanese, D. McGuinness, D. Nardi, 
P.F. Patel-Schneinder, eds. (2003) The Description Lo-
gics Handbook: Theory, Implementation, and Applica-
tions, Cambridge University Press 
 
Basili, Roberto, Michele Vindigni, Fabio Massimo 
Zanzotto (2003a) Integrating ontological and linguistic 
knowledge for Conceptual Information Extraction, Web 
Intelligence Conference, Halifax, Canada, September 
2003 
 
Basili, Roberto, Maria Teresa Pazienza, and Fabio 
Massimo Zanzotto (2003b) Exploiting the feature vector 
model for learning linguistic representations of rela-
tional concepts Workshop on Adaptive Text Extraction 
and Mining (ATEM 2003) held in conjuction with Eu-
ropena Conference on Machine Learning (ECML 2003) 
Cavtat (Croatia), September 2003 
 
Basili, Roberto and Fabio Massimo Zanzotto (2002) 
Parsing Engineering and Empirical Robustness Journal 
of Natural Language Engineering 8/2-3 June 2002  
 
Burger, John et al(2002) Issues, tasks and program 
structures to roadmap research in question & answer-
ing (Q&A). NIST DUC Vision and Roadmap Docu-
ments, http://www-nlpir.nist.gov 
/projects/duc/roadmapping.html. 
 
Callmeier, Ulrich (2000) PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. In Flickinger, D., Oepen, S., Tsujii, J. and 
Uszkoreit, H. (eds.) Natural Language Engineering. 
Special Issue on Efficient Processing with HPSG. Vol. 
6, Part 1, March 2000, 99?107. 
 
Copestake, Ann (2002) Implementing Typed Feature 
Structure Grammars. CSLI Publications. Stanford Uni-
versity. 
 
Garshol, Lars Marius (2003) Living with Topic 
Maps and RDF. Technical report. 
http://www.ontopia.net/topicmaps/materials/tmrdf.html. 
 
Harabagiu, Sanda,  Dan Moldovan, Marius Paca, 
Rada Mihalcea, Mihai Surdeanu, Rhzvan Bunescu, 
Roxana Girju, Vasile Rus, and Paul Morrescu (2001) 
The role of lexico-semantic feedback in open-domain 
textual question-answering. In Proceedings of the Asso-
ciation for Computational Linguistics, July 2001. 
 
Hobbs, Jerry R., Douglas E. Appelt, John Bear, 
David Israel, Megumi Kameyama, Mark Stickel, and 
Mabry Tyson (1996). FASTUS: A cascaded finite-state 
transducer for extracting information from natural-
language text. In Finite State Devices for Natural Lan-
guage Processing. MIT Press, Cambridge, MA. 
 
Humphreys, K., R. Gaizauskas, S. Azzam, C. 
Huyck, B. Mitchell, H. Cunningham, and Y. Wilks 
(1998) University of sheffield: Description of the 
LASIE-II system as used for MUC-7. In Proceedings of 
the Seventh Message Understanding Conferences 
(MUC-7). Morgan Kaufman, 1998. 
 
Meadche, Alexander and Steffen Staab (2001) Com-
paring Ontologies-Similarity Measures and Comparison 
Study, Internal Report No. 408, Institute AIFB, Univer-
sity of Karlsruhe, Germany, 2001 
 
Miller, George A. (1995) WordNet: A lexical data-
base for English. Communications of the ACM, 
38(11):39--41, 1995. 
 
Pazienza, Maria Teresa and Michele Vindigni 
(2003) Agent-based Ontological Mediation in IE sys-
tems in M.T. Pazienza ed. Information Extraction in the 
Web Era, LNAI 2700, Sprinter Berlin 2003 
 
Rooth, M. (1992) A Theory of Focus Interpretation. 
In Natural Language Semantics, Vol. 1, No. 1, pp. 75-
116. 
 
Voorhees, Ellen M. (2001) The TREC question an-
swering track. Natural Language Engineering 7(4), pp. 
361?378. 
 
Vossen, Piek (1998) EuroWordNet: A Multilingual 
Database with Lexical Semantic Networks Kluwer Aca-
demic Publishers, Dordrecht, October 1998 
 
Woods, W., R. Kaplan, and B. Nash-Weber (1972) 
The Lunar Sciences Natural Language Information Sys-
tem: Final Report. Technical Report, Bolt Beranek and 
Newman, Number 2378, June 1972. 
 
Zajac, Remi (2001) Towards Ontological Question 
Answering, ACL-2001 Workshop on Open-Domain 
Question Answering, Toulose, France, 2001 
 
Spell ing and Grammar  Correct ion for Danish in SCARRIE  
Pat r i z ia  Pagg io  
Center for Sprogteknologi  
Copenhagen (DK) 
patrizia@cst, ku. dk 
Abst rac t  
This paper reports on work carried out to de- 
velop a spelling and grammar corrector for Dan- 
ish, addressing in particular the issue of how a 
form of shallow parsing is combined with er- 
ror detection and correction for the treatment 
of context-dependent spelling errors. The syn- 
tactic grammar for Danish used by the system 
has been developed with the aim of dealing with 
the most frequent error types found in a parallel 
corpus of unedited and proofread texts specif- 
ically collected by the project's end users. By 
focussing on certain grammatical constructions 
and certain error types, it has been possible 
to exploit the linguistic 'intelligence' provided 
by syntactic parsing and yet keep the system 
robust and efficient. The system described is 
thus superior to other existing spelling checkers 
for Danish in its ability to deal with context- 
dependent errors. 
1 In t roduct ion  
In her much-quoted and still relevant review of  
technologies for automatic word correction (Ku- 
kich, 1992), Kukich observes that "research in 
context-dependent spelling correction is in its 
infancy" (p. 429), and that the task of treating 
context-dependent rrors is still an elusive one 
due to the complexity of the linguistic knowl- 
edge often necessary to analyse the context in 
sufficient depth to find and correct such er- 
rors. But progress in parsing technology and 
the growing speed of computers eem to have 
made the task less of a chimera. The '90s 
have in fact seen a renewed interest in gram- 
mar checking, and proposals have been made 
for systems covering English (Bernth, 1997) and 
other languages uch as Italian (Bolioli et al, 
1992), Spanish and Greek (Bustamante and 
Ldon, 1996), Czech (Holan et al, 1997) and 
Swedish (Hein, 1998). 
This paper describes the prototype of a 
spelling and grammar corrector for Danish 
which combines traditional spelling checking 
functionalities with the ability to carry out com- 
pound analysis and to detect and correct cer- 
tain types of context-dependent spelling errors 
(hereafter simply "grammar errors"). Gram- 
mar correction is carried out by parsing the 
text, making use of feature overriding and error 
weights to accommodate he errors. Although 
a full parse of each sentence is attempted, the 
grammar has been developed with the aim of 
dealing only with the most frequent error types 
found in a parallel corpus of unedited and proof- 
read texts specifically collected by the project's 
end users. By focussing on certain grammati- 
cal constructions and certain error types, it has 
been possible to exploit the linguistic 'intelli- 
gence' provided by syntactic parsing and yet 
keep the system robust and efficient. The sys- 
tem described is thus superior to other existing 
spelling checkers for Danish in its ability to deal 
with certain types of grammar errors. 
We begin by giving an overview of the sys- 
tem's components in Section 2. In Section 3 we 
describe the error types we want to deal with: 
Section 4 gives an overview of the grammar: 
in particular, the methods adopted for treating 
feature mismatches and structural errors are ex- 
plained. Finally, in Section 5 evaluation results 
are presented and a conclusion is drawn. 
2 The  proto type  
The prototype is a system for high-quality 
proofreading for Danish which has been de- 
veloped in the context of a collaborative EU- 
project 1. Together with the Danish prototype, 
1Main contractors in the consortium were: 
WordFinder Software AB (Sweden), Center for 
255 
the project has also produced similar systems 
for Swedish and Norwegian, all of them tailored 
to meet the specific needs of the Scandinavian 
publishing industry. They all provide writing 
support in the form of word and grammar check- 
ing. 
The Danish version of the system 2 constitutes 
a further development of the CORRie prototype 
(Vosse, 1992) (Vosse, 1994), adapted to deal 
with the Danish language, and to the needs of 
the project's end users. The system processes 
text in batch mode and produces an annotated 
output text where errors are flagged and re- 
placements suggested where possible. Text cor- 
rection is performed in two steps: first the sys- 
tem deals with spelling errors and typos result- 
ing in invalid words, and then with grammar 
errors. 
Invalid words are identified on the basis of 
dictionary lookup. The dictionary presently 
consists of 251,000 domain-relevant word forms 
extracted from a collection of 68,000 newspa- 
per articles. A separate idiom list allowing for 
the identification of multi-word expressions is 
also available. Among the words not found in 
the dictionary or the idiom list, those occurring 
most frequently in the text (where frequency is 
assessed relative to the length of the text) are 
taken to be new words or proper names 3. The 
remaining unknown words are passed on to the 
compound analysis grammar, which is a set of 
regular expressions covering the most common 
types of compound nominals in Danish. This is 
an important feature, as in Danish compound- 
ing is very productive, and compounds are writ- 
ten as single words. 
Words still unknown at this point are taken 
to be spelling errors. The System flags them as 
Sprogteknologi (Denmark), Department of Linguistics 
at Uppsala University (Sweden), Institutt for lingvistikk 
og litteraturvitenskab at the University of Bergen (Nor- 
way), and Svenska Dagbladet (Sweden). A number of 
subcontractors also contributed to the project. Subcon- 
tractors in Denmark were: Munksgaard International 
Publishers, Berlingske Tidende, Det Danske Sprog- og 
Litteraturselskab, and Institut for Almen og Anvendt 
Sprogvidenskab t the University of Copenhagen. 
2In addition to the author of the present paper, 
tlle Danish SCARRIE team at CST consisted of Claus 
Povlsen, Bart Kongejan and Bradley Music. 
3The system also checks whether a closely matching 
alternative can be found in the dictionary, to avoid mis- 
taking a consistently misspelt word for a new word. 
such and tries to suggest a replacement. The 
algorithm used is based on trigram and tri- 
phone analysis (van Berkel and Smedt, 1988), 
and takes into account the orthographic strings 
corresponding to the invalid word under con- 
sideration and its possible replacement, as well 
as the phonetic representations of the same two 
words. Phonetic representations are generated 
by a set of grapheme-to-phoneme rules (Hansen,  
1999) the aim of which is to assign phonetically 
motivated misspellings and their correct coun- 
terparts identical or similar phonetic represen- 
tations. 
Then the system tries to identify context- 
dependent spelling errors. This is done by pars- 
ing the text. Parsing results are passed on to 
a corrector to find replacements for the errors 
found. The parser is an implementation of the 
Tomita algorithm with a component for error 
recognition whose job is to keep track of error 
weights and feature mismatches as described in 
(Vosse, 1991). Each input sentence is assigned 
the analysis with the lowest error weight. If the 
error is due to a feature mismatch, the offending 
feature is overridden, and if a dictionary entry 
satisfying the grammar constraints expressed by 
the context is found in the dictionary, it is of- 
fered as a replacement. If the structure is in- 
complete, on the other hand, an error message 
is generated. Finally, if the system identifies an 
error as a split-up or a run-on, it will suggest 
either a possible concatenation, or a sequence 
of valid words into which the misspelt word can 
be split up. 
3 The  er rors  
To ensure the coverage of relevant error types, 
a set of parallel unedited and proofread texts 
provided by the Danish end users has been col- 
lected. This text collection consists of newspa- 
per and magazine articles published in 1997 for 
a total of 270,805 running words. The articles 
have been collected in their raw version, as well 
as in the edited version provided by the pub- 
lisher's own proofreaders. Although not very 
large in number of words, th@ corpus consists 
of excerpts from 450 different articles to en- 
sure a good spread of lexical domains and error 
types. The corpus has been used to construct 
test suites for progress evaluation, and also to 
guide grammar development. The aim set for 
256 
Error type No. 
Context independent errors 386 
Context dependent errors 308 
Punctuation problems 212 
Style problems 89 
Graphical problems 24 
Total 1019 
% 
38 
30 
21 
9 
2 
100 
Figure 1: Error distribution in the Danish cor- 
pus 
grammar development was then to enable the 
system to identify and analyse the grammati- 
cal constructions in which errors typically occur, 
whilst to some extent disregarding the remain- 
der of the text. 
The errors occurring in the corlbus have been 
analysed according to the taxonomy in (Ram- 
bell, 1997). Figure 1 shows the distribution of 
the various error types into the five top-level 
categories of the taxonomy. As can be seen, 
grammar errors account for 30~0 of the errors. 
Of these, 70% fall into one of the following cat- 
egories (Povlsen, 1998): 
? Too many finite verbal forms or missing fi- 
nite verb 
? Errors in nominal phrases: 
- agreement errors, 
- wrong determination, 
- genitive errors, 
- errors concerning pronouns; 
? Split-ups and run-ons. 
Another way of grouping the errors is by the 
kind of parsing failure they generate: they can 
then be viewed as either feature mismatches, 
or as structural errors. Agreement errors are 
typical examples of feature mismatches. In the 
following nominal phrase, for example: 
(1) de *interessant projekter 
(the interesting projects) 
_the error can be formalised as a mismatch be- 
tween the definiteness of the determiner de (the) 
and the indefiniteness of the adjective interes- 
sant (interesting). Adjectives have in fact both 
an indefinite and a definite form in Danish. 
The sentence below, on the other hand, is an 
example of structural error. 
(2) i sin tid *skabet han skulpturer over 
atomkraften 
(during his time wardrobe/created he 
sculptures about nuclear power) 
Since the finite verb skabte (created) has been 
misspelt as skabet (the wardrobe), the syntactic 
structure corresponding to the sentence is miss- 
ing a verbal head. 
Run-ons and split-ups are structural errors of 
a particular kind, having to do with leaves in the 
syntactic tree. In some cases they can only be 
detected on the basis of the context, because the 
misspelt word has the wrong category or bears 
some other grammatical feature that is incorrect 
in the context. Examples are given in (3) and 
(4) below, which like the preceding examples are 
taken from the project's corpus. In both cases, 
the error would be a valid word in a different 
context. More specifically, rigtignok (indeed) is 
an adverb, whilst rigtig nok (actually correct) is 
a modified adjective; and inden .for (inside) is a 
preposition, whilst indenfor (indoors) is an ad- 
verb. In both examples the correct alternative 
is indicated in parentheses. 
(3) ... studerede rain gruppe *rigtig nok 
(rigtignok) under temaoverskrifter 
(studied my group indeed on the basis 
of topic headings) 
(4) *indenfor (inden for) de gule mute 
(inside the yellow walls) 
Although the system has a facility for identi- 
fying and correcting split-ups and run-ons based 
on a complex interaction between the dictio- 
nary, the idiom list, the compound grammar 
and the syntactic grammar, this facility has not 
been fully developed yet, and will therefore not 
be described any further here. More details can 
be found in (Paggio, 1999). 
4 The  grammar  
The grammar is an augmented context-free 
grammar consisting of rewrite rules where sym- 
bols are associated with features. Error weights 
and error messages can also be attached to ei- 
ther rules or single features. The rules are ap- 
plied by unification, but in cases where one or 
more features do not unify, the offending fea- 
tures will be overridden. 
257 
In the current version of the grammar~ only 
the structures relevant to the error types we 
want the system to deal with - in other words 
nominal phrases and verbal groups - are ac- 
counted for in detail. The analysis produced is 
thus a kind of shallow syntactic analysis where 
the various sentence constituents are attached 
under the topmost S node as fragments. 
For example, adjective phrases can be anal- 
ysed as fragments, as shown in the following 
rule: 
Fragment -> 
AP "?Fragment AP rule":2 
To indicate that the fragment analysis is not 
optimal, it is associated with an error weight, 
as well as an error message to be used for de- 
bugging purposes (the message is not visible to 
the end user). The weight penalises parse trees 
built by applying the rule. The rule is used e.g. 
to analyse an AP following a copula verb as in: 
(5) De projekter er ikke interessante. 
(Those projects are not interesting) 
The main motivation for implementing a 
grammar based on the idea of fragments was 
efficiency. Furthermore, the fragment strategy 
could be implemented very quickly. However, 
as will be clarified in Section 5, this strategy is 
sometimes responsible for bad flags. 
4.1 Feature  mismatches  
As an alternative to the fragment analysis, APs 
can be attached as daughters in NPs. This is of 
course necessary for the treatment of agreement 
in NPs, one of the error types targeted in our 
application. This is shown in the following rule: 
NP(def Gender PersNumber) -> 
Det (def Gender PersNumber) 
AP(def _ _) 
N(indef Gender:9- PersNumber) 
The rule will parse a correct definite NP such 
as :  
(6) 
but also 
(7) 
(S) 
de interessante projekter 
(the interesting projects) 
de *interessant projekter 
de interessante *projekterne 
The  feature overriding mechanism makes it 
possible for the system to suggest interessante 
as the correct replacement in (7), and projekter 
in (8). Let us see how this is done in more de- 
tail for example (7). The parser tries to apply 
the NP rule to the input string. The rule states 
that the adjective phrase must be definite (AP 
(def _ _)). But the dictionary entry correspond- 
ing to interessant bears the feature 'indef'. The 
parser will override this feature and build an 
NP according to the constraints expressed by 
the rule. At this point, a new dictionary lookup 
is performed, and the definite form of the ad- 
jective can be suggested as a replacement. 
Weights are used to control rule interaction 
as well as to establish priorities among features 
that may have to be overridden. For example 
in our NP rule, a weight has been attached to 
the Gender feature in the N node. The weight 
expresses the fact that it costs more to over- 
ride gender on the head noun than on the de- 
terminer or adjective. The rationale behind this 
is the fact that if there is a gender mismatch, 
the parser should not try to find an alternative 
? form of the noun (which does not exist), but if 
necessary override the gender feature either on 
the adjective or the determiner. 
4.2. Captur ing  s t ruc tura l  e r ro rs  in 
grammar  ru les  
To capture structural errors, the formalism al- 
lows the grammar writer to write so-called error 
rules. The syntax of error rules is very similar 
to that used in 'normal' rules, the only differ- 
ence being that an error rule must have an er- 
? ror weight and an error message attached to it. 
The purpose of the weight is to ensure that er-  
ror rules are applied only if 'normal' rules are 
not applicable. The error message can serve two 
purposes. Depending on whether it is stated as 
an implicit or an explicit message (i.e. whether 
it is preceded by a question mark or not), it will 
appear in the log file where it can be used for 
debugging purposes, or in the output text as a 
message to the end user. 
The following is an error rule example. 
VGroup(_ finite Tense) -> 
V(_ finite:4 Tense) 
V(_ finite:4 _) 
"Sequence of two finite verbs":4 
, ' j r -  O 258
Error type Total Flagged Hits Misses No suggs 
Errors in single words 81 69 (85.2%) 35 (50.8%) 11 (15.9%) 23 (33.3%) 
Errors in compounds 42 28 (66.7%) 8 (28.6%) 8 (28.6%) 12 (42.8%) 
NP agreement errors 18 15 (83.3%) 15 (100.0%) 0 (0.0%) 0 (0.0%) 
VP errors 13 8 (61.6%) 8 (100.0%) 0 (0 .0%)0  (0.0%) 
Total 154 120 (78.0%)66 (55.0%) 19 (15.8%) 35 (29.2%) 
Figure 2: Error coverage and suggestion adequacy evaluated on test suites 
A weight of 4 is attached to the rule as a 
whole, but there are also weights attached to the 
'finiteness' feature on the daughters: their func- 
tion is to make it costly for the system to apply 
the rule to non-finite forms. In other words, the 
feature specification 'finite' is made difficult to 
override to ensure that it is indeed a sequence of 
finite verbal forms the rule applies to and flags. 
The rule will for example parse the verbal se- 
quence in the following sentence: 
(9) Jeg vil *bevarer (berate) rain frihed. 
(*I want keep my freedom) 
As a result of parsing, the system in this case 
will not attempt to correct the wrong verbal 
form, but issue the error message "Sequence of 
two finite verbs". 
Error rules can thus be used to explicitly 
describe an error and to issue error messages. 
However, so far we have made very limited use 
of them, as controlling their interaction with 
'normal' rules and with the feature overriding 
mechanism is not entirely easy. In fact, they are 
consistently used only to identify incorrect se- 
quences of finite verbal forms or sentences miss- 
ing a finite verb. To this sparse use of error rules 
corresponds, on the other hand, an extensive x- 
ploitation of the feature overriding mechanism. 
This strategy allows us to keep the number of 
rules in the  grammar elatively low, but relies 
on a careful manual adjustment of the weights 
attached to the various features in the rules. 
5 Eva luat ion  and  Conc lus ion  
The project's access to a set of parallel unedited 
and proofread texts has made it possible to au- 
tomate the evaluation of the system's linguis- 
tic functionality. A tool has been implemented 
to compare the results obtained by the sys- 
tem with the corrections suggested by the pub- 
lisher's human proofreaders in order to derive 
measures telling us how well the system per- 
formed on recall (lexical coverage as well as cov- 
erage of errors), precision (percentage of cor- 
rect flaggings), as well as suggestion adequacy 
(hits, misses and no suggestions offered). The 
reader is referred to (Paggio and Music, 1998) 
for more details on the evaluation methodology. 
The automatic procedure was used to evaluate 
the system during development, and in connec- 
tion with the user validation. Testing was done 
on constructed test suites displaying examples 
of the errors targeted in the project and with 
text excerpts from the parallel corpora~ 
Figure 2 shows error recall and suggestion ad- 
equacy figures for the various error types repre- 
sented in the test suites. These figures are very 
positive, especially with regard to the treatment 
of grammar errors. To make a comparison with 
a commercial product, the Danish version of the 
spelling and grammar checker provided by Mi- 
crosoft Word does not flag any of the grammar 
errors. 
Figure 3 shows how the system performed on 
one of the test corpora. The corpus was assem- 
bled by mixing short excerpts containing rele- 
vant grammar errors and randomly chosen text. 
Since unlike test suites, the corpus also contains 
correct ext, the figure this time also shows lex- 
ical coverage and precision figures. The corpus 
consists of 278 sentences, with an average length 
of 15.18 words per sentence. It may be sur- 
prising to see that it contains a limited number 
of errors, but it must be remembered that the 
texts targeted in the project are written by ex- 
perienced journalists. 
The corpus was processed in 58 cpu-seconds 
on an HP 9000/B160. As expected, the system 
performs less well than on the test suites, and in 
general precision is clearly too low. However, we 
still consider these results encouraging given the 
relatively small resources the project has been 
able to spend on grammar development, and we 
259 
Recall 
4220 total words 
4157 valid words 
3996 (96.1%) valid words accepted 
161 (3.9%) valid words rejected 
63 invalid words (real errors) 
36 (57.1%) real errors potted (good flags) 
27 (42.9%) real errors missed 
Precision 
175 flaggings 
36 (20.6%) good flags 
139 (79.4%) bad flags (false positives) 
Suggestion adequacy 
36 good flags 
15 (41.7%) hits (initial suggestion correct ) 
8 (22.2%) misses (suggestions offered, none correct) 
13 (36.1%) with no suggestions offered 
Figure 3: Test corpus evaluation 
believe they can be improved. 
We regard error coverage as quite satisfac- 
tory for a research prototype. In a comparative 
test made on a similar (slightly smaller) corpus, 
SCARR/E obtained 58.1% error coverage, and 
Word 53.5%. To quote a figure from another 
recently published test (Martins et al, 1998), 
the ReGra system is reported to miss 48.1% 
real errors. It is worth noting that ReGra has 
much more extensive linguistic resources avail- 
able than SCARRIE, i.e. a dictionary of 1.5 
million words and a grammar of 600 production 
rules. Most of the errors not found by SCAR- 
RIE in the test have to do with punctuation 
and other stylistic matters not treated in the 
project. There are also, however, agreement er- 
rors which go unnoticed. These failures are due 
to one of two reasons: either that no parse has 
been produced for the sentence in question, or 
that the grammar has produced a wrong analy- 
sis. 
The precision obtained is at least at first sight 
much too low. On the same test corpus, how- 
ever, Word only reached 15.9% precision. On 
closer inspection, 72 of the bad flags produced 
by SCARRIE turned out to be due to unrecog- 
nised proper names. Disregarding those, preci- 
sion goes up to 34.9%. As was mentioned early, 
SCARRIE has a facility for guessing unknown 
proper names on the basis of their frequency 
of occurrence in the text. But since the test 
corpus consists of Short unrelated excerpts, a 
large number of proper names only occur once 
or twice. To get an impression of how the sys- 
tem would perform in a situation where the 
same proper names and unknown words had a 
higher frequency of occurrence, we doubled the 
test corpus by simply repeating the same text 
twice. As expected, precision increased. The 
system produced 178 flags, 60 of which were 
correct (39.7%). This compares well with the 
40% precision reported for instance for ReGra. 
In addition to the problem of unkown proper 
names, false flags are related to unrecognised 
acronyms and compounds (typically forms con- 
taining acronyms or dashes), and a not very pre- 
cise treatment of capitalisation. Only 13 false 
flags are due to wrong grammar analyses caused 
either by the fragment approach or by the gram- 
mar's limited coverage. In particular, genitive 
phrases, which are not treated at the moment, 
are responsible for most of these false alarms. 
In conclusion, we consider the results ob- 
tained so far promising, and the problems re- 
vealed by the evaluation tractable within the 
current system design. In particular, future 
development should focus on treating stylistic 
matters uch as capitalisation and punctuation 
which have not been in focus in the current 
prototype. The coverage of the grammar, in 
particular the treatment of genitive phrases, 
should also be further developed. The data pro- 
260 
vided by the evaluation reported on in this pa- 
per, however, are much too limited to base fur- 
ther development on. Therefore, more exten- 
sive testing and debugging should also be car- 
ried out. 
In addition, two aspects of the system that 
have only be touched on in this paper would 
be worth further attention: one is the mecha- 
nism for the treatment of split-ups and run-ons, 
which as mentioned earlier is not well-integrated 
at the moment; the other is the weight adjust- 
ment process, which is done manually at the 
moment, and for which the adoption of a semi- 
automatic tool could be considered. 
Re ferences  
A. Bernth. 1997. EasyEnglish: a tool for im- 
proving document quality. In Proceedings of 
? the Fifth Conference on Applied Natural Lan- 
guage Processing. 
A. Bolioli, L. Dini, and G. Malnati. 1992. 
JDII: Parsing Italian with a robust constraint 
grammar. In Proceedings of COLING:92, 
pages 1003-1007. 
Flora Ram~rez Bustamante and Fer- 
nando S?nchez L@on. 1996. GramCheck: A 
grammar and style checker. In Proceedings 
of COLING-96, pages 175-181, Copenhagen, 
Denmark. 
Peter Molb~ek Hansen. 1999. Grapheme-to- 
phoneme rules for the Danish component 
of the SCARRIE project. In Hanne E. 
Thomsen and Sabine'Kirchmeier-Andersen, 
editors, Datalingvistisk Forenings drsmcde 
1998 i Kcbehavn, Proceedings, number 25 in 
LAMBDA, pages 79-91. Institut for datal- 
ingvistik, Handelshcjskolen i Kcbenhaven. 
Anna S?gvall Hein. 1998. A chart-based frame- 
work for grammar checking: Initial studies. 
In Proceedings of Nodalida-98. 
Tom~ Holan, Vladislav Kubofi, and Mar- 
tin Pl?tek. 1997. A prototype of a gram- 
mar checker for Czech. In Proceedings of 
ANLP'97. 
Karen Kukich. 1992. Techniques for automati- 
cally correcting words in text. A CM Comput- 
_ ing Surveys, 24(4):377-439. 
Ronaldo Teixeira Martins, Ricardo Hasegawa, 
Maria Volpe Nunes, Gisele Monthila, and Os- 
valdo Novais De Oliveira Jr. 1998. Linguistic 
issues in the development ofReGra: a gram- 
mar Checker for Brazilian Portuguese. Natu- 
ral Language Engineering, 4(4):287-307, De- 
cember. 
Patrizia' Paggio and Bradley Music. 1998. Eval- 
uation in the SCARRIE project. In Pro- 
ceedings of the First International Conference 
on Language Resources ~ Evaluation, pages 
277-282. Granada, Spain. 
Patrizia Paggio. 1999. Treatment of grammat- 
ical errors and evaluation in SCARRIE. In 
Hanne E. Thomsen and Sabine Kirchmeier- 
Andersen, editors, Datalingvistisk Forenings 
drsmCde 1998 i KCbehavn, Proceedings, num- 
ber 25 in LAMBDA, pages 65-78. Insti- 
tut for datalingvistik, Handelshcjskolen i 
Kcbenhaven. 
Claus Povlsen. 1998. Three types of gram- 
matical errors in Danish. Technical report, 
Copenhagen: Center for Sprogteknologi. 
Olga Rambell. 1997. Error typology for auto- 
matic proof-reading purposes. Technical re- 
port, Uppsala: Uppsala University. : 
Brigitte van Berkel and Koenra~d De Smedt. 
1988. Triphone analysis: a combined method 
for the correction of orthographical nd ty- 
pographical errors. In Proceedings of the 2nd 
conference on Applied Natural Language Pro- 
cessing, pages 77-83. ACL, Austin. 
Theo  Vosse. 1991. Detection and correction 
of morpho-syntactic errors in shift-reduce 
parsing. In R. Heemels, A. Nijholt, and 
K. Sikkel, editors, Tomita's Algorithm: Ex- 
tensions and Applications, number 91-68 in 
Memoranda Informatica, pages 69-78. Uni- 
versity of Twente/ 
Theo Vosse. 1992. Detecting and correcting 
morpho-syntactic errors in real texts. In Pro- 
ceedings of the Third Conference on Applied 
Natural Language Processing, pages 111-118, 
Trento, Italy. 
Theo G. Vosse. 1994. The Word Connection - 
Grammar-based Spelling Error Correction in 
Dutch. Ph.D. thesis, Rijksuniversiteit a Lei- 
den: the Netherlands. ISBN 90-75296-01-0. 
261 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2007?2017, Dublin, Ireland, August 23-29 2014.
Learning when to point: A data-driven approach
Albert Gatt
Institute of Linguistics
University of Malta
albert.gatt@um.edu.mt
Patrizia Paggio
Institute of Linguistics, Uni of Malta
Centre for Language Technology, Uni of Copenhagen
patrizia.paggio@um.edu.mt
Abstract
The relationship between how people describe objects and when they choose to point is complex
and likely to be influenced by factors related to both perceptual and discourse context. In this
paper, we explore these interactions using machine-learning on a dialogue corpus, to identify
multimodal referential strategies that can be used in automatic multimodal generation. We show
that the decision to use a pointing gesture depends on features of the accompanying description
(especially whether it contains spatial information), and on visual properties, especially distance
or separation of a referent from its previous referent.
1 Introduction
The automatic generation of multimodal referring actions is a relatively under-studied phenomenon in
Natural Language Generation (NLG). While there has been extensive research on Referring Expression
Generation (REG) focusing on the choice of content in expressions such as (1) below (Dale, 1989; Dale
and Reiter, 1995; Krahmer and van Deemter, 2012), their multimodal counterpart ? exemplified in (2) ?
raises questions that go beyond these choices.
(1) the group of five large red circles
(2) there?s a group of five large red ones [+pointing gesture with arm extended]
One important question concerns the appropriateness of a pointing gesture under different conditions.
The relevant conditions here include both the physical or perceptual common ground shared by interlocu-
tors (for example, what other objects are in the vicinity of the target referent, and therefore potentially
confusable with it), the discursive common ground (for example, whether this object has been referred
to before) and the content of the interlocutor?s speech act, that is, what she chooses to say in addition to
pointing. For example in (2), the speaker, who is engaged in a dialogue in which she needs to guide her
interlocutor through a route on an abstract map (see Section 3 below), has chosen to use the cardinality
of the referent (it is a group made up of five circles), its size, and its colour. Her choice of properties
may be sufficient to distinguish it from all its distractors in the current context. However, unlike (1), (2)
is a composite utterance consisting of two communicative modalities, each of which contributes to the
communicative intention (Enfield, 2009).
This paper addresses the question of when a pointing gesture is appropriate as part of a composite,
multimodal referring action. This is an important component of many multimodal generation systems,
including those that communicate through embodied agents. We address this question in a data-driven
manner, using a corpus of dialogues in which references have been annotated at both the level of speech
and gesture. Our aim is to learn strategies for combinations of pointing and describing, as a function of
perceptual and discursive features. We first summarise some relevant psycholinguistic and computational
work (Section 2), before describing our corpus data (Section 3) and reporting on the machine-learning
experiments conducted (Section 4). Section 5 concludes with some remarks on future work.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2007
2 Pointing and reference
The idea that gesture and speech are planned separately, incorporated in early work on multimodal gen-
eration (Andr?e and Rist, 1996) is contradicted by more recent psycholinguistic research, in which gesture
and language are increasingly viewed as tightly coupled (Kita and
?
Ozy?urek, 2003; McNeill, 1985; Mc-
Neill and Duncan, 2000), contributing jointly to the composite utterances (Enfield, 2009). This view
has also influenced recent work in multimodal NLG. For example, Kopp et al. (2008) use ?multimodal
concepts?, combining propositional and gestural or perceptual information.
In the case of referring expressions, pointing has been treated as a property, on a par with an object?s
colour or size. Thus, van der Sluis and Krahmer (2007) propose an algorithm in a graph-based framework
(Krahmer et al., 2003) which selects pointing gestures of varying degrees of precision based on their cost
when compared to other linguistically realisable features. Similarly, Kranstedt and Wachsmuth (2005)
propose an extension of Dale and Reiter?s (1995) Incremental Algorithm, which initially considers the
possibility of producing an unambiguous pointing gesture. If this fails, a pointing gesture that is less
precise may be generated, together with descriptive features of an object.
Both of these approaches assume that the choice of modality in a referring action ultimately hinges on a
trade-off between what can be said and what is easiest to produce, a view that has some empirical support
(Beun and Cremers, 1998; Bangerter, 2004; Piwek, 2007). On the other hand de Ruiter et al. (2012)
found that likelihood of pointing was unaffected by the difficulty of using descriptive features. From
a computational perspective, our earlier work (Gatt and Paggio, 2013) also found evidence, based on a
machine-learning study on dialogue data, for the co-occurrence of pointing with descriptive (especially
spatial) features, suggesting that pointing gestures may be planned in tandem (and not in competition)
with these features.
The present paper uses the same corpus data as Gatt and Paggio (2013); however, that paper focused
on the relationship between descriptive features (in the spoken part of the utterance) and pointing. In
contrast, here we take a much broader view, also addressing the impact of the physical/perceptual features
of the objects under discussion, and aspects of the discourse history.
3 Data used in this study
Matcher's mapwithoutitinerary
Director's mapwith itinerary
Director Follower
Low screensto hide maps
(a) Experiment setup
START
1/5
16
9
2/1817
4/ 1514
12 7/11
8/13
10
16
16
16
16
14 141414
4/ 154
/ 15 4/ 154/ 15
12 12
12
12
1010
10 1
0
7/117/11
7/11 7/11
8/138/1
3 8/138/13
1/5
1/51/5
1/5
9
99 9
17 1717
17
2/18
2/18 2/18 2/183/63/63/63/63/6
(b) Group circles map (numbers indicate the order in
which landmarks are visited along the itinerary)
Figure 1: MREDI dialogue setup (reproduced from Gatt and Paggio (2013)).
We use the MREDI (Multimodal REference in DIalogue) corpus (van der Sluis et al., 2008; Gatt and
Paggio, 2013), a collection of MapTask-like dialogues (Anderson et al., 1991). Dialogues in MREDI
were conducted by dyads consisting of a Director and a Follower. The Director?s task was to guide the
Follower along a route through a visually shared ?map?, located approximately one metre away, directly
in front of them, blown up to roughly A0 size. The Director also had a private map on which the route
was indicated, while the Follower?s private map was used to mark the route as it unfolded in the course
of the conversation. Figure 1a displays the basic setup.
2008
There were no restrictions on what interlocutors could say. Participants in the study were told in
advance that they could use both speech and gestures, but were not explicitly instructed to point. The
maps consisted of collections of shapes of different colours and sizes and were very densely populated
(see Figure 1b). Four maps were used in the study: in two of these, landmarks consisted of individual
circles or squares, while in the other two they consisted of groups or clusters of five circles or squares
(Figure 1b is a group circle map). In the group maps, all elements of a group of five were of the same
colour and size.
On each map, there were 18 ?landmarks?; these were the milestones along the itinerary and were
marked on the Director?s private map, but not visible on the large map that constituted the common
ground. For example, the landmarks (groups of 5 circles) in Figure 1b are numbered from 1 to 18. Each
dyad did all four maps; the order was randomised for each pair of participants. Participants switched roles
between one map and another. In addition to the difference between group and individual landmarks, the
maps were designed to manipulate a number of independent variables:
1. Distinguishing Properties (DistProps): Landmarks on the itinerary differed from their distractors
? the objects in their immediate vicinity (the focus area) ? in colour, or in size, or in both colour
and size. The focus area was defined as the set of objects immediately surrounding a target. This
means that different landmarks required different combinations of properties to ensure that they
could be unambiguously identified by a description. For example, in Figure 1b, the group marked
17 consists of a landmark where size is the distinguishing feature, since all five circles in the group
are small, and the objects in their immediate vicinity are either large or medium-sized. There were
equal numbers of landmarks on each map that could be distinguished by colour only, size only, or
both.
2. Prior reference (Discourse): Some of the landmarks were visited twice in the itinerary; these are
indicated using two numbers in Figure 1b. Thus, landmark 8 in this map was also visited later as
landmark 13. There were 6 landmarks on each map that were revisited in this way. This is the
primary manipulation related to discourse history.
3. Shift of domain focus (Distance): Landmarks were located either near to or far away from the
previous target. For example, in Figure 1b, landmark 17 and landmark 18 are adjacent (?near?
condition), but landmark 17 is far from the preceding landmark 16.
In what follows, we use data from 8 dyads. Similar to Gatt and Paggio (2013), we only consider
utterances by Directors. These were transcribed and split up according to the landmark to which they
corresponded. In case a landmark was described over multiple turns in the dialogue, each turn was
annotated as a separate utterance. Our dataset consists of a total of 2255 such utterances, of which 370
(16.4%) contain a pointing gesture. This is a relatively low proportion of such gestures, compared to
some previous studies, such as Beun and Cremers (1998), who found that 48% of referential acts in
their task-oriented dialogue corpus included a pointing gesture. However, Beun and Cremers focussed
exclusively on first-mention referring expressions. Furthermore, the low proportion of pointing gestures
in MREDI may be due to the fact that under our definition, the identification of a landmark may be spread
over several turns, with possible interruptions by the Follower. Each such turn constitutes a separate
utterance. This raises the likelihood that certain features of the composite utterance, including pointing,
will only occur on some of the turns.
3.1 Features
Utterances in MREDI were annotated with the features displayed in Table 1. These codify aspects of the
descriptive content of a referential act, as well as the presence or absence of a pointing gesture.
The features originally encoded in the MREDI corpus had frequency values; Gatt and Paggio (2013)
used these frequencies in their study. However, for our experiments, we collapsed the features related
to descriptive content ? hereafter referred to as descriptive features ? into boolean features. This signif-
icantly reduces the feature set and makes the rules acquired in our machine-learning experiments easier
2009
Feature Name Definition Example
Visual
S Size mention of the target size the group of small circles
Sh Shape mention of the target shape the circles at the bottom
C Colour mention of the target colour The blue square near the red square
Deictic/anaphoric
ID Identity Statement of identity between
the current and a previous or later target
the red square,
the same one we saw at number 5
D Deixis Use of a deictic reference those squares
Locative
RP Relative position Position of the target landmark relative
to another object on the map
the blue square
just below the red square
AP Absolute position Target position based on absolute
frame of reference
The blue circle down at the bottom
FP Path references References to non-targets on the
path leading to the target.
go east to the first tiny square,
past the blue one
DIR Directions Direction-giving. take a right, go across
and straight down
Action
GZ Gaze Gaze at the shared map (boolean).
Point Pointing Use of a pointing gesture (boolean).
1
Table 1: Features annotated in the dialogues. All features have frequency values, except for the Action
features, which are boolean.
to interpret. Further, it enables us to test our hypothesis that the presence or absence of a type of feature
(descriptive, physical or discursive) impacts the decision to point. The boolean descriptive features are
as follows:
1. Deixis: this has the value true if the utterance contains a demonstrative pronoun (such as that), or
a reference to the landmark that identifies it with the previous landmark. Thus, this feature is true
if ID > 0 or D > 0 in Table 1;
2. Locative: this has the value true if the utterance contains any of the spatial properties in Table 1.
Thus, the feature is true if AP > 0 or RP > 0 or FP > 0 or DIR > 0.;
3. Visual: this has the value true if the utterance contains at least one mention of the landmark?s
visual properties. This, the feature is true if C > 0 or Sh > 0 or S > 0.
In addition to these features, our experiments also made use of the physical features (Distance and
DistProps) manipulated as part of the MREDI data collection study (see above), as well as the feature
Discourse, which encodes prior reference.
Finally, we added a new feature to the dataset, MapConfl, which indicates the type of map on which
utterances were produced, namely, individual or group circles or squares. This feature was included
because the larger size of group landmarks, compared to individuals, may have influenced the decision
to point, since groups are more visually salient.
The feature Gaze is present whenever a pointing gesture is made; hence, it is not used in the machine
learning experiments reported below.
4 Experiments
In our earlier study on the MREDI corpus (Gatt and Paggio, 2013), investigating the relationship between
pointing and descriptive features, we found that the latter could indeed be used as predictors of pointing
gestures with an accuracy of 0.833 (F-score). The study also concluded that among the descriptive
features it was locative properties that were most useful in guiding the decision of whether or not to
point, compared to features describing visual characteristics of the objects.
However, in much of the work reviewed in Section 2, especially work arguing in favour of a trade-off
in cost between pointing and describing, the occurrence of pointing is made to depend on the physical
properties of referents. Therefore, in the present study we want to test whether the occurrence of pointing
2010
gestures can be predicted more accurately as a function of (i) the descriptive features that speakers use
to refer to landmarks; (ii) the physical/perceptual context in which they are found and (iii) whether
or not they have been referred to earlier in the discourse. Furthermore, we want to investigate which
combinations of physical and descriptive features provide the best results.
Two sets of experiments were conducted on different versions of the MREDI dataset. The first dataset
(referred to as the complete dataset) is the same one used in the Gatt and Paggio (2013) study. It includes
all of the 2255 Director?s utterances from the eight dyads in the corpus, including those that did not
contain any references at all, linguistic or gestural. Such utterances might, for example, be confirmations
or feedback produced in the course of the dialogue.
We also report results on a second dataset (referred to as the referential dataset), consisting of all
utterances that contain a reference, either using descriptive features, pointing, or both. This dataset
consisted of 1542 utterances. Note that the number of utterances with a pointing gesture is still 370 in
the pruned dataset.
The task in the experiments was to classify the binary feature Point. As mentioned earlier, 370 of these
utterances contain a pointing gesture. In other words, there are 370 occurrences of Point=1.
All the experiments were run using the Weka tool (Witten and Frank, 2005), which gives access
to many different algorithms, using 10-fold cross-validation throughout. In the experiments with the
complete dataset, the ZeroR and OneR classifiers were first run on the data to establish a baseline.
ZeroR always chooses the most frequent value of the class that is being predicted; in the present case,
it consistently classifies all utterances as Point = 0, since the majority of utterances do not containg
pointing gestures. OneR identifies a single feature, on the basis of which all classifications are made.
On the MREDI data, OneR always assigned Point = 0 to all utterances, based on a single rule using the
MapConfl feature (i.e. the type of map or domain in which the dialogue was being carried out). Note
that both baseline classifiers were trained using all features.
Various combinations of descriptive and physical features were then tested using different classifiers
in Weka, including NaiveBayes, Support Vector Machines, Maximum Entropy (Logistic in Weka) and
the J48 Decision Tree classifier. The present paper will report results for the last two of these, for the fol-
lowing reasons. First, these were the ones which performed best. In addition, the decision trees built by
J48 provide an analysis tool to understand how the various features interact, given their transparency; on
the other hand, MaxEnt sometimes outperforms J48 and provides a ?ceiling?, in addition to the baselines
described above.
The strategy used in testing feature combinations was essentially ablative. We tested first using all
features, and then compared the performance of the classifiers when they use only descriptive features
(Visual, Locative and Deixis), or only Discourse together with the physical features (DistProps and
Distance). Omitting descriptive features and using only physical features with Discourse invariably
performed near or below baseline (see below). Thus, we experimented with combinations of descriptive
features and each physical feature, as well as Discourse, individually.
4.1 Results on the complete dataset
The results for the complete dataset are shown in Table 2 in terms of Precision, Recall and F-measure for
each of the classifiers. The top rows display the results using all features, while the baseline results are
in the bottom rows. The remaining results for different combinations of features are in descending order
of F-score.
Interestingly, using all features ? i.e. MapConfl, DistProps, Discourse, Distance, Visual, Locative and
Deictic ? with or without MapConfl, results in worse overall performance than using a combination of de-
scriptive features (Locative, Deictic and Visual) with Distance. This combination is closely matched for
accuracy by the combination involving descriptive features, Distance and DistProps. However, dropping
Distance (using only descriptive features and DistProps) results in worse performance.
The addition of Distance and/or DistProps clearly improves the predictive accuracy of a classifier that
uses descriptive features. However, the worst combination is found when the descriptive features are
excluded. This is in line with the results reported by Gatt and Paggio (2013), who found that features of
2011
Classifier P R F Features
J48 0.827 0.847 0.832 All
Logistic 0.831 0.854 0.828 All
J48 0.833 0.851 0.838 All - MapConfl
Logistic 0.832 0.851 0.837 All - MapConfl
Logistic 0.839 0.853 0.844 Descriptive + Distance
J48 0.839 0.853 0.844 Descriptive minus Deictic + Distance
Logistic 0.839 0.853 0.844 Descriptive minus Deictic + Distance
J48 0.836 0.851 0.84 Descriptive+DistProps + Distance
J48 0.839 0.853 0.84 Descriptive+Distance
Logistic 0.833 0.851 0.838 Descriptive+DistProps + Distance
J48 0.821 0.847 0.824 Descriptive+DistProps
Logistic 0.809 0.842 0.794 Only Descriptive
Logistic 0.809 0.842 0.794 Descriptive + DistProps
Logistic 0.809 0.842 0.793 Descriptive + Discourse
J48 0.803 0.84 0.787 Only Descriptive
J48 0.795 0.838 0.781 Descriptive + Discourse
J48 0.699 0.836 0.761 Physical + Discourse
Logistic 0.699 0.836 0.761 Physical + and Discourse
ZeroR 0.699 0.836 0.761 All
OneR 0.699 0.836 0.761 All
Table 2: Predicting pointing gestures with different feature combinations in the complete MREDI dataset.
the descriptions produced by speakers were good predictors of pointing.
Adding only DistProps to the descriptive features improved the accuracy of the Logistic classifier
somewhat, though it had a greater impact on J48. However, Distance seems to have the greatest impact
of the two physical features. Discourse does not appear to play an important role: incorporating this
feature does not result in much improvement over using only descriptive features; indeed, in the case of
J48, it decreases accuracy.
We also tested one of the best combinations involving descriptive features and Distance but excluding
the Deictic feature from the set of descriptive features. This was done because pointing in referential
acts is frequently viewed on a par with deictic expressions, insofar as they are both indexical (Bangerter,
2004). This raises the question whether, out of all the descriptive features, Deixis could be considered
a somewhat redundant predictor. The results suggest that removing Deixis from the descriptive features
does not alter the accuracy of the classifier. We return to the role of Deixis in the discussion in Section
4.3.
4.2 Results on the referential dataset
Exactly the same combinations of features were tested, using 10-fold cross-validation, in separate ex-
periments on the referential dataset. This was done in order to compare the results on a dataset which
contains less ?noise?, that is, fewer utterances which were non-referential. Such utterances may compro-
mise the predictive validity of certain features, as they inflate the number of utterances in which Point=0.
Table 3 contains the results obtained on the reduced dataset. The accuracy is in general lower due to
the fact that predicting the absence of pointing is easier in the complete dataset, where many utterances
contain no reference at all, descriptive or gestural.
Contrary to the findings on the complete dataset, using the complete set of features as predictors of
pointing gives slightly better results than using either descriptive or physical features alone, at least in
2012
Classifier P R F Features
J48 0.783 0.799 0.785 All - MapConfl
Logistic 0.726 0.764 0.679 All - MapConfl
J48 0.774 0.793 0.776 All
Logistic 0.704 0.760 0.681 All
J48 0.781 0.797 0.784 Descriptive + DistProps + distance
J48 0.766 0.785 0.770 Descriptive + DistProps
J48 0.748 0.777 0.745 Descriptive + Distance
J48 0.758 0.783 0.744 Only descriptive
J48 0.774 0.788 0.740 Descriptive + Discourse
Logistic 0.720 0.762 0.675 Descriptive + DistProps + distance
Logistic 0.688 0.759 0.662 Descriptive + Discourse
Logistic 0.699 0.760 0.661 Descriptive + DistProps
Logistic 0.759 0.761 0.660 Descriptive + Distance
J48 0.577 0.760 0.656 Only physical + Discourse
Logistic 0.577 0.760 0.656 Only descriptive
Logistic 0.577 0.760 0.656 Only physical + Discourse
ZeroR 0.577 0.76 0.656 All
ONeR 0.577 0.76 0.656 All
Table 3: Predicting pointing gestures with different feature combinations in the referential MREDI
dataset.
the case of the decision tree classifier. This combination also exceeds the combination of descriptives,
DistProps and Distance, though only marginally. However, this does remain the next best combination
for J48, consistent with the results on the complete dataset. However, this combination performs quite
badly in the case of the Logistic classifier.
The fact that using all features performs better this time is probably due to the fact that there are fewer
non-referential utterances in this dataset. Once again, the role of Discourse seems marginal.
4.3 Analysis and discussion
Figure 2 shows the decision trees built by J48 for the two datasets when descriptive features are used
together with DistProps and Distance.
Visual
0 (1556/117)
Locative
0 (310/57)
Distance
0 (116/40) 1 (273/117)
=TRUE
=FALSE
=FALSE
=TRUE
=CLOSE =FAR
(a) Complete MREDI dataset
Visual
Locative
0 (310/57)
Deictic
0 (290/28)
Locative
0 (519/56) 1 (33)
Deictic
1 (68/27) DistProps
1 (109/50)
0 (118/48)
Distance
0 (23/8) 1 (71/31)
=TRUE =FALSE
=TRUE
=FALSE =TRUE
=FALSE
=TRUE =FALSE=TRUE
=FALSE
=COLOUR
=SIZE
=BOTH
=CLOSE =FAR
(b) Referential MREDI dataset
Figure 2: J48 Decision trees from the complete and referential datasets
Our main findings can be summarised as follows. First, descriptive features play an important role in
2013
the prediction of pointing; this replicates previous observations (Gatt and Paggio, 2013). Second, and
more importantly, the prediction accuracy improves when physical features, representing aspects of the
visual/perceptual context, are taken into account. This is especially true of Distance, suggesting that
a sizeable shift of perceptual focus, from one landmark to another further away, motivates a pointing
gesture, as shown in both trees in Figure 2. Once again, it is worth comparing this to the results of Beun
and Cremers (1998), who find that shifts of perceptual focus play a role in increasing the amount of
(descrpitive) information speakers include in a referring expression. However, they find no impact of
focus shifts on pointing gestures; our results, by contrast, suggest that such shifts do play a role.
There are a number of striking features in the trees in the figure. First, the descriptive feature Locative
plays a crucial role. All cases of pointing involve the presence of a Locative, with one exception: on the
referential dataset (Figure 2b), in case no Visual, Deictic or Locative features are used, the tree predicts
a pointing gesture. However, this case covers a very small number of instances (33), with 0% error rate.
All of these turn out to be utterances where there is no descriptive reference at all and speakers rely
exclusively on pointing. Example (3) below is typical of these.
(3) D: And a slightly bigger green to the right of that
M: M-hm
D: In the center of those like pack
M: Yeah
D: is number 9. [+pointing]
Clearly, these are cases in which the pointing gesture occurs as part of an extended sequence of ut-
terances which jointly identify a landmark. Descriptive features have already been uttered; the pointing
comes at the very end. In summary, the one case where Locatives don?t feature in predicting a pointing
gesture turns out to be a rather special case.
A second striking aspect of the trees is that while Deixis plays a predictive role in the tree based
on the referential dataset, it doesn?t in the case of the complete dataset. This is interesting in view of
the relationship that has often been noted between referential pointing gestures and deictic expressions
(Bangerter, 2004). Note, however, that there is no inconsistency between the two trees: the single path
through the tree in Figure 2a that results in pointing is subsumed by the path in Figure 2b which specifies
in addition that Deixis should be false, and DistProps should have the value colour. This still leaves
open the question why Deixis plays no role in the full dataset, despite being included as part of the
descriptive features that resulted in this tree. Indeed, we have already shown that, among the descriptive
features, Deixis doesn?t contribute much predictive power on the full dataset (see Section 4.1).
One possibility is that Deixis is generally under-represented in the corpus. However, there are propor-
tionately fewer utterances in the full dataset containing Deixis (20%), compared to the referential dataset
(30%). Furthermore, it may be partially dependent on the Locative features. There may be a priori
reasons to assume this as a working hypothesis: Deixis anchors parts of the speech signal to physical
properties of the common ground, potentially making it redundant with respect to location (which has
already specified the relevant physical/spatial features of the common ground).
Complete Dataset Referential Dataset
Locative Deictic Deictic
false true false true
false 74 26 42 58
true 88 12 88 12
overall 80 20 70 30
Table 4: Deictic features (D and ID) relative to Locatives. All figures are percentages.
Table 4 displays the distribution of Deictic expressions with respect to Locatives, that is, the proportion
of utterances containing a Deictic expression as a function of whether the utterances also contain a
Locative expression. The tables shows proportions both for the full and the referential dataset.
2014
Note that when Deictics are used, it is mostly in the absence of a Locative expression. A chi-square
test of independence suggests that the frequency of use of Locative and Deictic expressions are not
independent (complete dataset: ?
2
1
= 63.044, p < .001; referential: ?
2
= 358.21, p < .001). However,
there is a higher proportion of Deictic expressions in the referential dataset (30% overall); this may
account for the use of this feature in the decision tree for this dataset (it is more informative). Crucially,
the trend in the use of deictic expressions is reversed in the two datasets: when Locative is false on
the referential dataset, most utterances involve a deictic expression; the reverse is true on the complete
dataset.
There is one path through the tree in Figure 2b which seems to contradict the hypothesis that deictic
expressions are used in the absence of locatives. There are 68 cases where pointing is used when both
Deictic and Locative are true. One possibility is that this is caused by our having defined the Deictic
feature as true whenever there is an actual deictic expression (variable D in Table 1; e.g. those squares)
or an identity expression (variable ID; e.g. the same one we saw). To investigate this further, Table 5
shows a breakdown of the frequencies of the presence or absence of a locative expression, as a function of
whether a true deictic (D) or an identity expression (ID) is used in an utterance. Once again, proportions
are displayed for both datasets.
Complete dataset Referential dataset
True Deictic Locative Locative
false true false true
false 54 46 26 74
true 78 22 76 24
(a) True deictic expressions (D)
Complete dataset Referential dataset
Identity Locative Locative
false true false true
false 56 44 31 69
true 68 32 67 33
(b) Identity expressions (ID)
Table 5: Identity (ID) and actual Deictic (D) expressions relative to Locatives. All figures are percent-
ages.
There are two observations that stem from these proportions: First, in line with our earlier observa-
tions, there is a greater proportion of true deictic (D) expressions in utterances that contain no locative
expression. For example, 78% of utterances in the complete dataset that have no locatives contain a
deictic; the corresponding figure in the referential dataset is 76%. The same pattern holds for identity
(ID) expressions. Second, out of the utterances that do not contain a locative, the proportion containing
a true deictic (D) is greater than the proportion containing an identity expression (ID). This may explain
the apparent exception ? represented by the path in Figure 2b ? to our generalisation that locatives and
deictics are redundant with respect to each other, and locatives tend to be avoided if deictics are used.
The explanation may lie in the conflation, in the boolean Deictic feature used in our experiments, of true
deictics and identity expressions. The path in the decision tree where both Locative and Deictic are true
may be accounting for utterances in which an identity expression is used, rather than a true deictic.
5 Conclusions and future work
This paper addressed the question of when pointing gestures should be generated, as a function of the
features a speaker uses to identify a referent, as well as the features of the context in which an utterance
is being produced. The best predictors of pointing are descriptive features, especially locatives, and
features of the perceptual context, especially distance from the last referent. The latter is a marker of
a shift of perceptual focus, akin to the focus shifts identified by Beun and Cremers (1998). Our study
also sheds light on the relationship between pointing and the use of deictic expressions, suggesting that,
while the two are often used together, deictics tend to be used more in the absence of locative features.
We also note some limitations of our methodology. Inspection of the results in Tables 2 and 3 shows
that the best performing classifiers, though they exceed baselines, do not do so by a wide margin. We
believe that one of the main reasons for this is the relative scarcity of pointing gestures in our dataset (as
discussed in Section 3), which may have resulted in a sizeable subset of utterances where pointing was
relatively straightforward to predict (e.g. based on one feature, as in the OneR baseline classifier). This
is a limitation we intend to investigate in future work, through a more diverse dataset where pointing
2015
features more strongly. In addition, it is worth noting that the ablative testing reported here does suggest
that certain features play a greater role in determining when speakers choose to point.
Our work addresses an important question in Natural Language Generation systems that seek to gen-
erate multimodal referring acts, namely, how pointing and describing should be combined and when. In
future work, we intend to extend this research in two ways: first, by extending our focus to incorporate
the interactive features of a dialogue and their impact on referential success; and second, by focusing on
other domains with a view to testing the generalisability of the results.
Acknowledgements
Thanks to Ielka van der Sluis, Adrian Bangerter and Paul Piwek, who collaborated on the development
of the MREDI corpus. Thanks to the anonymous reviewers of COLING 2014 for helpful comments.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller,
C. Sotillo, H. S. Thompson, and R. Weinert. 1991. The HCRC Map Task corpus. Language and Speech,
34:351?366.
E. Andr?e and T. Rist. 1996. Coping with temporal constraints in multimedia presentation planning. In Proceedings
of the 13th National Conference on Artificial Intelligence (AAAI?96).
A. Bangerter. 2004. Using pointing and describing to achieve joint focus of attention in dialogue. Psychological
Science, 15(6):415?419.
R.J. Beun and A. Cremers. 1998. Object reference in a shared domain of conversation. Pragmatics and Cognition,
6(1-2):121?152.
R. Dale and E. Reiter. 1995. Computational interpretation of the Gricean maxims in the generation of referring
expressions. Cognitive Science, 19(8):233?263.
R. Dale. 1989. Cooking up referring expressions. In Proceedings of the 27th annual meeting of the Association
for Computational Linguistics (ACL?89), pages 68?75.
J.P. de Ruiter, A. Bangerter, and P. Dings. 2012. The interplay between gesture and speech in the production of
referring expressions: Investigating the tradeoff hypothesis. Topics in Cognitive Science, 4:232?248.
N.J. Enfield. 2009. The Anatomy of Meaning: Speech, Gesture and Composite Utterances. Cambridge University
Press, Cambridge.
A. Gatt and P. Paggio. 2013. What and where: An empirical investigation of pointing gestures and descriptions in
multimodal referring actions. In Proceedings of the 14th European Workshop on Natural Language Generation
(ENLG?13).
S. Kita and A.
?
Ozy?urek. 2003. What does cross-linguistic variation in semantic coordination of speech and
gesture reveal?: Evidence for an interface representation of spatial thinking and speaking. Journal of Memory
and Language, 48:16?32.
S. Kopp, K. Bergmann, and I. Wachsmuth. 2008. Multimodal communication from multimodal thinking: Towards
an integrated model of speech and gesture production. International Journal of Semantic Computing, 2(1):115?
136.
E. Krahmer and K. van Deemter. 2012. Computational generation of referring expressions: A survey. Computa-
tional Linguistics, 38(1):173?218.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-based generation of referring expressions. Computational
Linguistics, 29(1):53?72.
A. Kranstedt and I. Wachsmuth. 2005. Incremental generation of multimodal deixis referring to objects. In
Proceedings of the 10th European Workshop on Natural Language Generation (ENLG?05).
D. McNeill and S.D. Duncan. 2000. Growth points in thinking for speaking. In D. McNeill, editor, Language and
Gesture, pages 141?161. Cambridge University Press.
2016
D. McNeill. 1985. So you think gestures are nonverbal? Psychological Review, 92(3):350?371.
P. Piwek. 2007. Modality choice for generation of referring acts: Pointing vs describing. In Proceedings of the
Workshop on Multimodal Output Generation (MOG?07)., pages 129?139.
I. van der Sluis and E. Krahmer. 2007. Generating multimodal referring expressions. Discourse Processes,
44(3):145?174.
I. van der Sluis, P. Piwek, A. Gatt, and A. Bangerter. 2008. Towards a balanced corpus of multimodal referring
expressions in dialogue. In Proceedings of the Symposium on Multimodal Output Generation (MOG?08).
I.H. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, second edition.
2017
Proceedings of the ACL 2010 Conference Short Papers, pages 318?324,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Classification of Feedback Expressions in Multimodal Data
Costanza Navarretta
University of Copenhagen
Centre for Language Technology (CST)
Njalsgade 140, 2300-DK Copenhagen
costanza@hum.ku.dk
Patrizia Paggio
University of Copenhagen
Centre for Language Technology (CST)
Njalsgade 140, 2300-DK Copenhagen
paggio@hum.ku.dk
Abstract
This paper addresses the issue of how lin-
guistic feedback expressions, prosody and
head gestures, i.e. head movements and
face expressions, relate to one another in
a collection of eight video-recorded Dan-
ish map-task dialogues. The study shows
that in these data, prosodic features and
head gestures significantly improve auto-
matic classification of dialogue act labels
for linguistic expressions of feedback.
1 Introduction
Several authors in communication studies have
pointed out that head movements are relevant to
feedback phenomena (see McClave (2000) for an
overview). Others have looked at the application
of machine learning algorithms to annotated mul-
timodal corpora. For example, Jokinen and Ragni
(2007) and Jokinen et al (2008) find that machine
learning algorithms can be trained to recognise
some of the functions of head movements, while
Reidsma et al (2009) show that there is a depen-
dence between focus of attention and assignment
of dialogue act labels. Related are also the stud-
ies by Rieks op den Akker and Schulz (2008) and
Murray and Renals (2008): both achieve promis-
ing results in the automatic segmentation of dia-
logue acts using the annotations in a large multi-
modal corpus.
Work has also been done on prosody and ges-
tures in the specific domain of map-task dialogues,
also targeted in this paper. Sridhar et al (2009)
obtain promising results in dialogue act tagging
of the Switchboard-DAMSL corpus using lexical,
syntactic and prosodic cues, while Gravano and
Hirschberg (2009) examine the relation between
particular acoustic and prosodic turn-yielding cues
and turn taking in a large corpus of task-oriented
dialogues. Louwerse et al (2006) and Louwerse
et al (2007) study the relation between eye gaze,
facial expression, pauses and dialogue structure
in annotated English map-task dialogues (Ander-
son et al, 1991) and find correlations between the
various modalities both within and across speak-
ers. Finally, feedback expressions (head nods and
shakes) are successfully predicted from speech,
prosody and eye gaze in interaction with Embod-
ied Communication Agents as well as human com-
munication (Fujie et al, 2004; Morency et al,
2005; Morency et al, 2007; Morency et al, 2009).
Our work is in line with these studies, all of
which focus on the relation between linguistic
expressions, prosody, dialogue content and ges-
tures. In this paper, we investigate how feedback
expressions can be classified into different dia-
logue act categories based on prosodic and ges-
ture features. Our data are made up by a collec-
tion of eight video-recorded map-task dialogues in
Danish, which were annotated with phonetic and
prosodic information. We find that prosodic fea-
tures improve the classification of dialogue acts
and that head gestures, where they occur, con-
tribute to the semantic interpretation of feedback
expressions. The results, which partly confirm
those obtained on a smaller dataset in Paggio and
Navarretta (2010), must be seen in light of the
fact that our gesture annotation scheme comprises
more fine-grained categories than most of the stud-
ies mentioned earlier for both head movements
and face expressions. The classification results
improve, however, if similar categories such as
head nods and jerks are collapsed into a more gen-
eral category.
In Section 2 we describe the multimodal Dan-
ish corpus. In Section 3, we describe how the
prosody of feedback expressions is annotated, how
their content is coded in terms of dialogue act, turn
and agreement labels, and we provide inter-coder
agreement measures. In Section 4 we account for
the annotation of head gestures, including inter-
318
coder agreements results. Section 5 contains a de-
scription of the resulting datasets and a discussion
of the results obtained in the classification experi-
ments. Section 6 is the conclusion.
2 The multimodal corpus
The Danish map-task dialogues from the Dan-
PASS corpus (Gr?nnum, 2006) are a collection
of dialogues in which 11 speaker pairs cooper-
ate on a map task. The dialogue participants
are seated in different rooms and cannot see each
other. They talk through headsets, and one of them
is recorded with a video camera. Each pair goes
through four different sets of maps, and changes
roles each time, with one subject giving instruc-
tions and the other following them. The material
is transcribed orthographically with an indication
of stress, articulatory hesitations and pauses. In
addition to this, the acoustic signals are segmented
into words, syllables and prosodic phrases, and an-
notated with POS-tags, phonological and phonetic
transcriptions, pitch and intonation contours.
Phonetic and prosodic segmentation and anno-
tation were performed independently and in paral-
lel by two annotators and then an agreed upon ver-
sion was produced with the supervision of an ex-
pert annotator, for more information see Gr?nnum
(2006). The Praat tool was used (Boersma and
Weenink, 2009).
The feedback expressions we analyse here are
Yes and No expressions, i.e. in Danish words like
ja (yes), jo (yes in a negative context), jamen (yes
but, well), nej (no), n?h (no). They can be single
words or multi-word expressions.
Yes and No feedback expressions represent
about 9% of the approximately 47,000 running
words in the corpus. This is a rather high pro-
portion compared to other corpora, both spoken
and written, and a reason why we decided to use
the DanPASS videos in spite of the fact that the
gesture behaviour is relatively limited given the
fact that the two dialogue participants cannot see
each other. Furthermore, the restricted contexts
in which feedback expressions occur in these di-
alogues allow for a very fine-grained analysis of
the relation of these expressions with prosody and
gestures. Feedback behaviour, both in speech and
gestures, can be observed especially in the person
who is receiving the instructions (the follower).
Therefore, we decided to focus our analysis only
on the follower?s part of the interaction. Because
of time restrictions, we limited the study to four
different subject pairs and two interactions per
pair, for a total of about an hour of video-recorded
interaction.
3 Annotation of feedback expressions
As already mentioned, all words in DanPASS are
phonetically and prosodically annotated. In the
subset of the corpus considered here, 82% of the
feedback expressions bear stress or tone informa-
tion, and 12% are unstressed; 7% of them are
marked with onset or offset hesitation, or both.
For this study, we added semantic labels ? includ-
ing dialogue acts ? and gesture annotation. Both
kinds of annotation were carried out using ANVIL
(Kipp, 2004). To distinguish among the various
functions that feedback expressions have in the di-
alogues, we selected a subset of the categories de-
fined in the emerging ISO 24617-2 standard for
semantic annotation of language resources. This
subset comprises the categories Accept, Decline,
RepeatRephrase and Answer. Moreover, all feed-
back expressions were annotated with an agree-
ment feature (Agree, NonAgree) where relevant.
Finally, the two turn management categories Turn-
Take and TurnElicit were also coded.
It should be noted that the same expression may
be annotated with a label for each of the three se-
mantic dimensions. For example, a yes can be an
Answer to a question, an Agree and a TurnElicit at
the same time, thus making the semantic classifi-
cation very fine-grained. Table 1 shows how the
various types are distributed across the 466 feed-
back expressions in our data.
Dialogue Act
Answer 70 15%
RepeatRephrase 57 12%
Accept 127 27%
None 212 46%
Agreement
Agree 166 36%
NonAgree 14 3%
None 286 61%
Turn Management
TurnTake 113 24%
TurnElicit 85 18%
None 268 58%
Table 1: Distribution of semantic categories
319
3.1 Inter-coder agreement on feedback
expression annotation
In general, dialogue act, agreement and turn anno-
tations were coded by an expert annotator and the
annotations were subsequently checked by a sec-
ond expert annotator. However, one dialogue was
coded independently and in parallel by two expert
annotators to measure inter-coder agreement. A
measure was derived for each annotated feature
using the agreement analysis facility provided in
ANVIL. Agreement between two annotation sets
is calculated here in terms of Cohen?s kappa (Co-
hen, 1960)1 and corrected kappa (Brennan and
Prediger, 1981)2. Anvil divides the annotations in
slices and compares each slice. We used slices of
0.04 seconds. The inter-coder agreement figures
obtained for the three types of annotation are given
in Table 2.
feature Cohen?s k corrected k
agreement 73.59 98.74
dial act 84.53 98.87
turn 73.52 99.16
Table 2: Inter-coder agreement on feedback ex-
pression annotation
Although researchers do not totally agree on
how to measure agreement in various types of an-
notated data and on how to interpret the resulting
figures, see Artstein and Poesio (2008), it is usu-
ally assumed that Cohen?s kappa figures over 60
are good while those over 75 are excellent (Fleiss,
1971). Looking at the cases of disagreement we
could see that many of these are due to the fact
that the annotators had forgotten to remove some
of the features automatically proposed by ANVIL
from the latest annotated element.
4 Gesture annotation
All communicative head gestures in the videos
were found and annotated with ANVIL using a
subset of the attributes defined in the MUMIN an-
notation scheme (Allwood et al, 2007). The MU-
MIN scheme is a general framework for the study
of gestures in interpersonal communication. In
this study, we do not deal with functional classi-
fication of the gestures in themselves, but rather
1(Pa? Pe)/(1? Pe).
2(Po ? 1/c)/(1 ? 1/c) where c is the number of cate-
gories.
with how gestures contribute to the semantic in-
terpretations of linguistic expressions. Therefore,
only a subset of the MUMIN attributes has been
used, i.e. Smile, Laughter, Scowl, FaceOther for
facial expressions, and Nod, Jerk, Tilt, SideTurn,
Shake, Waggle, Other for head movements.
A link was also established in ANVIL between
the gesture under consideration and the relevant
speech sequence where appropriate. The link was
then used to extract gesture information together
with the relevant linguistic annotations on which
to apply machine learning.
The total number of head gestures annotated is
264. Of these, 114 (43%) co-occur with feedback
expressions, with Nod as by far the most frequent
type (70 occurrences) followed by FaceOther as
the second most frequent (16). The other tokens
are distributed more or less evenly, with a few oc-
currences (2-8) per type. The remaining 150 ges-
tures, linked to different linguistic expressions or
to no expression at all, comprise many face ex-
pressions and a number of tilts. A rough prelim-
inary analysis shows that their main functions are
related to focusing or to different emotional atti-
tudes. They will be ignored in what follows.
4.1 Measuring inter-coder agreement on
gesture annotation
The head gestures in the DanPASS data have been
coded by non expert annotators (one annotator
per video) and subsequently controlled by a sec-
ond annotator, with the exception of one video
which was annotated independently and in parallel
by two annotators. The annotations of this video
were then used to measure inter-coder agreement
in ANVIL as it was the case for the annotations
on feedback expressions. In the case of gestures
we also measured agreement on gesture segmen-
tation. The figures obtained are given in Table 3.
feature Cohen?s k corrected k
face segment 69.89 91.37
face annotate 71.53 94.25
head mov segment 71.21 91.75
head mov annotate 71.65 95.14
Table 3: Inter-coder agreement on head gesture
annotation
These results are slightly worse than those ob-
tained in previous studies using the same annota-
tion scheme (Jokinen et al, 2008), but are still sat-
320
isfactory given the high number of categories pro-
vided by the scheme.
A distinction that seemed particularly difficult
was that between nods and jerks: although the
direction of the two movement types is different
(down-up and up-down, respectively), the move-
ment quality is very similar, and makes it difficult
to see the direction clearly. We return to this point
below, in connection with our data analysis.
5 Analysis of the data
The multimodal data we obtained by combining
the linguistic annotations from DanPASS with the
gesture annotation created in ANVIL, resulted into
two different groups of data, one containing all Yes
and No expressions, and the other the subset of
those that are accompanied by a face expression
or a head movement, as shown in Table 4.
Expression Count %
Yes 420 90
No 46 10
Total 466 100
Yes with gestures 102 90
No with gestures 12 10
Total with gestures 114 100
Table 4: Yes and No datasets
These two sets of data were used for automatic
dialogue act classification, which was run in the
Weka system (Witten and Frank, 2005). We exper-
imented with various Weka classifiers, compris-
ing Hidden Naive Bayes, SMO, ID3, LADTree
and Decision Table. The best results on most of
our data were obtained using Hidden Naive Bayes
(HNB) (Zhang et al, 2005). Therefore, here we
show the results of this classifier. Ten-folds cross-
validation was applied throughout.
In the first group of experiments we took into
consideration all the Yes and No expressions (420
Yes and 46 No) without, however, considering ges-
ture information. The purpose was to see how
prosodic information contributes to the classifica-
tion of dialogue acts. We started by totally leav-
ing out prosody, i.e. only the orthographic tran-
scription (Yes and No expressions) was consid-
ered; then we included information about stress
(stressed or unstressed); in the third run we added
tone attributes, and in the fourth information on
hesitation. Agreement and turn attributes were
used in all experiments, while Dialogue act anno-
tation was only used in the training phase. The
baseline for the evaluation are the results provided
by Weka?s ZeroR classifier, which always selects
the most frequent nominal class.
In Table 5 we provide results in terms of preci-
sion (P), recall (R) and F-measure (F). These are
calculated in Weka as weighted averages of the re-
sults obtained for each class.
dataset Algor P R F
YesNo ZeroR 27.8 52.8 36.5
HNB 47.2 53 46.4
+stress HNB 47.5 54.1 47.1
+stress+tone HNB 47.8 54.3 47.4
+stress+tone+hes HNB 47.7 54.5 47.3
Table 5: Classification results with prosodic fea-
tures
The results indicate that prosodic information
improves the classification of dialogue acts with
respect to the baseline in all four experiments with
improvements of 10, 10.6, 10.9 and 10.8%, re-
spectively. The best results are obtained using
information on stress and tone, although the de-
crease in accuracy when hesitations are introduced
is not significant. The confusion matrices show
that the classifier is best at identifying Accept,
while it is very bad at identifying RepeatRephrase.
This result if not surprising since the former type
is much more frequent in the data than the latter,
and since prosodic information does not correlate
with RepeatRephrase in any systematic way.
The second group of experiments was con-
ducted on the dataset where feedback expressions
are accompanied by gestures (102 Yes and 12 No).
The purpose this time was to see whether ges-
ture information improves dialogue act classifica-
tion. We believe it makes sense to perform the
test based on this restricted dataset, rather than the
entire material, because the portion of data where
gestures do accompany feedback expressions is
rather small (about 20%). In a different domain,
where subjects are less constrained by the techni-
cal setting, we expect gestures would make for a
stronger and more widespread effect.
The Precision, Recall and F-measure of the Ze-
roR classifier on these data are 31.5, 56.1 and 40.4,
respectively. For these experiments, however, we
used as a baseline the results obtained based on
stress, tone and hesitation information, the com-
bination that gave the best results on the larger
321
dataset. Together with the prosodic information,
Agreement and turn attributes were included just
as earlier, while the dialogue act annotation was
only used in the training phase. Face expression
and head movement attributes were disregarded
in the baseline. We then added face expression
alone, head movement alone, and finally both ges-
ture types together. The results are shown in Ta-
ble 6.
dataset Algor P R F
YesNo HNB 43.1 56.1 46.4
+face HNB 43.7 56.1 46.9
+headm HNB 44.7 55.3 48.2
+face+headm HNB 49.9 57 50.3
Table 6: Classification results with head gesture
features
These results indicate that adding head ges-
ture information improves the classification of di-
alogue acts in this reduced dataset, although the
improvement is not impressive. The best results
are achieved when both face expressions and head
movements are taken into consideration.
The confusion matrices show that although the
recognition of both Answer and None improve, it
is only the None class which is recognised quite
reliably. We already explained that in our annota-
tion a large number of feedback utterances have an
agreement or turn label without necessarily having
been assigned to one of our task-related dialogue
act categories. This means that head gestures
help distinguishing utterances with an agreement
or turn function from other kinds. Looking closer
at these utterances, we can see that nods and jerks
often occur together with TurnElicit, while tilts,
side turns and smiles tend to occur with Agree.
An issue that worries us is the granularity of
the annotation categories. To investigate this, in
a third group of experiments we collapsed Nod
and Jerk into a more general category: the distinc-
tion had proven difficult for the annotators, and we
don?t have many jerks in the data. The results, dis-
played in Table 7, show as expected an improve-
ment. The class which is recognised best is still
None.
6 Conclusion
In this study we have experimented with the au-
tomatic classification of feedback expressions into
different dialogue acts in a multimodal corpus of
dataset Algor P R F
YesNo HNB 43.1 56.1 46.4
+face HNB 43.7 56.1 46.9
+headm HNB 47 57.9 51
+face+headm HNB 51.6 57.9 53.9
Table 7: Classification results with fewer head
movements
Danish. We have conducted three sets of experi-
ments, first looking at how prosodic features con-
tribute to the classification, then testing whether
the use of head gesture information improved the
accuracy of the classifier, finally running the clas-
sification on a dataset in which the head move-
ment types were slightly more general. The re-
sults indicate that prosodic features improve the
classification, and that in those cases where feed-
back expressions are accompanied by head ges-
tures, gesture information is also useful. The re-
sults also show that using a more coarse-grained
distinction of head movements improves classifi-
cation in these data.
Slightly more than half of the head gestures in
our data co-occur with other linguistic utterances
than those targeted in this study. Extending our in-
vestigation to those, as we plan to do, will provide
us with a larger dataset and therefore presumably
with even more interesting and reliable results.
The occurrence of gestures in the data stud-
ied here is undoubtedly limited by the technical
setup, since the two speakers do not see each other.
Therefore, we want to investigate the role played
by head gestures in other types of video and larger
materials. Extending the analysis to larger datasets
will also shed more light on whether our gesture
annotation categories are too fine-grained for au-
tomatic classification.
Acknowledgements
This research has been done under the project
VKK (Verbal and Bodily Communication) funded
by the Danish Council for Independent Research
in the Humanities, and the NOMCO project, a
collaborative Nordic project with participating re-
search groups at the universities of Gothenburg,
Copenhagen and Helsinki which is funded by the
NOS-HS NORDCORP programme. We would
also like to thank Nina Gr?nnum for allowing us to
use the DanPASS corpus, and our gesture annota-
tors Josephine B?dker Arrild and Sara Andersen.
322
References
Jens Allwood, Loredana Cerrato, Kristiina Jokinen,
Costanza Navarretta, and Patrizia Paggio. 2007.
The MUMIN Coding Scheme for the Annotation of
Feedback, Turn Management and Sequencing. Mul-
timodal Corpora for Modelling Human Multimodal
Behaviour. Special Issue of the International Jour-
nal of Language Resources and Evaluation, 41(3?
4):273?287.
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The HCRC Map Task
Corpus. Language and Speech, 34:351?366.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
Paul Boersma and David Weenink, 2009. Praat: do-
ing phonetics by computer. Retrieved May 1, 2009,
from http://www.praat.org/.
Robert L. Brennan and Dale J. Prediger. 1981. Co-
efficient Kappa: Some uses, misuses, and alterna-
tives. Educational and Psychological Measurement,
41:687?699.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
lettin, 76(5):378?382.
Shinya Fujie, Y. Ejiri, K. Nakajima, Y Matsusaka, and
Tetsunor Kobayashi. 2004. A conversation robot
using head gesture recognition as para-linguistic in-
formation. In Proceedings of the 13th IEEE Inter-
national Workshop on Robot and Human Interactive
Communication, pages 159 ? 164, september.
Agustin Gravano and Julia Hirschberg. 2009. Turn-
yielding cues in task-oriented dialogue. In Pro-
ceedings of SIGDIAL 2009: the 10th Annual Meet-
ing of the Special Interest Group in Discourse and
Dialogue, September 2009, pages 253?261, Queen
Mary University of London.
Nina Gr?nnum. 2006. DanPASS - a Danish pho-
netically annotated spontaneous speech corpus. In
N. Calzolari, K. Choukri, A. Gangemi, B. Maegaard,
J. Mariani, J. Odijk, and D. Tapias, editors, Pro-
ceedings of the 5th LREC, pages 1578?1583, Genoa,
May.
Kristiina Jokinen and Anton Ragni. 2007. Cluster-
ing experiments on the communicative prop- erties
of gaze and gestures. In Proceeding of the 3rd.
Baltic Conference on Human Language Technolo-
gies, Kaunas, Lithuania, October.
Kristiina Jokinen, Costanza Navarretta, and Patrizia
Paggio. 2008. Distinguishing the communica-
tive functions of gestures. In Proceedings of the
5th MLMI, LNCS 5237, pages 38?49, Utrecht, The
Netherlands, September. Springer.
Michael Kipp. 2004. Gesture Generation by Imita-
tion - From Human Behavior to Computer Charac-
ter Animation. Ph.D. thesis, Saarland University,
Saarbruecken, Germany, Boca Raton, Florida, dis-
sertation.com.
Max M. Louwerse, Patrick Jeuniaux, Mohammed E.
Hoque, Jie Wu, and Gwineth Lewis. 2006. Mul-
timodal communication in computer-mediated map
task scenarios. In R. Sun and N. Miyake, editors,
Proceedings of the 28th Annual Conference of the
Cognitive Science Society, pages 1717?1722, Mah-
wah, NJ: Erlbaum.
Max M. Louwerse, Nick Benesh, Mohammed E.
Hoque, Patrick Jeuniaux, Gwineth Lewis, Jie Wu,
and Megan Zirnstein. 2007. Multimodal communi-
cation in face-to-face conversations. In R. Sun and
N. Miyake, editors, Proceedings of the 29th Annual
Conference of the Cognitive Science Society, pages
1235?1240, Mahwah, NJ: Erlbaum.
Evelyn McClave. 2000. Linguistic functions of head
movements in the context of speech. Journal of
Pragmatics, 32:855?878.
Louis-Philippe Morency, Candace Sidner, Christopher
Lee, and Trevor Darrell. 2005. Contextual Recog-
nition of Head Gestures. In Proceedings of the In-
ternational Conference on Multi-modal Interfaces.
Louis-Philippe Morency, Candace Sidner, Christopher
Lee, and Trevor Darrell. 2007. Head gestures for
perceptual interfaces: The role of context in im-
proving recognition. Artificial Intelligence, 171(8?
9):568?585.
Louis-Philippe Morency, Iwan de Kok, and Jonathan
Gratch. 2009. A probabilistic multimodal ap-
proach for predicting listener backchannels. Au-
tonomous Agents and Multi-Agent Systems, 20:70?
84, Springer.
Gabriel Murray and Steve Renals. 2008. Detecting
Action Meetings in Meetings. In Proceedings of
the 5th MLMI, LNCS 5237, pages 208?213, Utrecht,
The Netherlands, September. Springer.
Harm Rieks op den Akker and Christian Schulz. 2008.
Exploring features and classifiers for dialogue act
segmentation. In Proceedings of the 5th MLMI,
pages 196?207.
Patrizia Paggio and Costanza Navarretta. 2010. Feed-
back in Head Gesture and Speech. To appear in Pro-
ceedings of 7th Conference on Language Resources
and Evaluation (LREC-2010), Malta, May.
323
Dennis Reidsma, Dirk Heylen, and Harm Rieks op den
Akker. 2009. On the Contextual Analysis of Agree-
ment Scores. In Michael Kipp, Jean-Claude Mar-
tin, Patrizia Paggio, and Dirk Heylen, editors, Multi-
modal Corpora From Models of Natural Interaction
to Systems and Applications, number 5509 in Lec-
ture Notes in Artificial Intelligence, pages 122?137.
Springer.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangaloreb,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
dialog act tagging. Computer Speech & Language,
23(4):407?422.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, second edition.
Harry Zhang, Liangxiao Jiang, and Jiang Su. 2005.
Hidden Naive Bayes. In Proceedings of the Twen-
tieth National Conference on Artificial Intelligence,
pages 919?924.
324
Proceedings of the 14th European Workshop on Natural Language Generation, pages 82?91,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
What and where: An empirical investigation of pointing gestures and
descriptions in multimodal referring actions
Albert Gatt
Institute of Linguistics
University of Malta
albert.gatt@um.edu.mt
Patrizia Paggio
Institute of Linguistics
University of Malta
patrizia.paggio@um.edu.mt
Abstract
Pointing gestures are pervasive in human
referring actions, and are often combined
with spoken descriptions. Combining ges-
ture and speech naturally to refer to objects
is an essential task in multimodal NLG
systems. However, the way gesture and
speech should be combined in a referring
act remains an open question. In particu-
lar, it is not clear whether, in planning a
pointing gesture in conjunction with a de-
scription, an NLG system should seek to
minimise the redundancy between them,
e.g. by letting the pointing gesture indi-
cate locative information, with other, non-
locative properties of a referent included
in the description. This question has a
bearing on whether the gestural and spo-
ken parts of referring acts are planned sep-
arately or arise from a common underly-
ing computational mechanism. This paper
investigates this question empirically, us-
ing machine-learning techniques on a new
corpus of dialogues involving multimodal
references to objects. Our results indi-
cate that human pointing strategies inter-
act with descriptive strategies. In partic-
ular, pointing gestures are strongly asso-
ciated with the use of locative features in
referring expressions.
1 Introduction
Referring Expression Generation (REG) is consid-
ered a core task in many NLG systems (Krahmer
and van Deemter, 2012). Typically, the REG task is
defined in terms of identification: a referent needs
to be unambiguously identified in a discourse, en-
abling the reader or listener to pick it out from
among its potential distractors. Most work in this
area has focused on algorithms that select the con-
tent for definite descriptions (Dale, 1989; Dale and
Reiter, 1995), or on the best form for a referring
expression given the discourse context, for exam-
ple, whether it should be a full definite description,
a reduced one, or a pronoun (McCoy and Strube,
1999; Callaway and Lester, 2002; Krahmer and
Theune, 2002).
Less attention has been payed to the role of
gestures in referring actions and the way these
can be coupled with discursive strategies for ref-
erent identification. This question becomes partic-
ularly important in the context of multimodal sys-
tems, for example, those involving embodied con-
versational agents, where the ?naturalness? of an
interaction hinges in part on the appropriate use
of embodied actions, including referring actions.
Multimodal strategies can also make communica-
tion more efficient. For example, Louwerse and
Bangerter (2010) found that the use of pointing
gestures resulted in significantly faster resolution
of ambiguous referring expressions; crucially, this
result was replicated when the pointing gesture
was artificially generated, rather than made by a
human.
Like human communicators, embodied agents
need the ability to plan multimodal referring acts,
combining both linguistic reference and pointing.
An important question is whether these two com-
ponents of a referring act should be planned in or-
der to minimise redundancy between them or not.
For example, given that a pointing gesture can ef-
ficiently locate a target referent in a visual do-
main, should an accompanying description avoid
mentioning locative properties, thereby minimis-
ing redundancy? This question is the main focus
of this paper. However, it bears on a deeper is-
sue, of relevance to the architecture of multimodal
systems (and the cognitive architectures whose be-
haviours such systems seek to emulate): Should
gestural and descriptive strategies be viewed as
separate (implying that a REG module can plan
its linguistic referring expressions more or less in-
82
dependently of whether a pointing gesture is also
used) or should they be viewed as tightly coupled?
If they are indeed coupled, are there any features
of a linguistic description (for example, an object?s
location) which are excluded when a pointing ges-
ture is used, or are linguistic features always re-
dundant with pointing?
The present paper addresses these questions in a
data-driven fashion, using a multimodal corpus of
dialogues collected specifically to study referring
actions at both the linguistic and gestural levels.
We focus on pointing (that is, deictic) gestures di-
rected at an intended referent (as opposed to, say,
iconic gestures) and investigate the extent to which
pointing interacts with linguistic means for refer-
ent identification. Following an overview of pre-
vious work on pointing and reference (Section 2)
and a description of the corpus (Section 3), we de-
scribe a number of machine-learning experiments
that address the main empirical question (Section
4), concluding with a discussion.
2 Background: Pointing and describing
There is a growing consensus in the psycholin-
guistic literature, especially following the work
of McNeill (McNeill, 1985), that gesture and
language share a number of underlying mental
processes and are therefore coupled to a signif-
icant degree. This view is in part based on the
observation that gestures are temporally coupled
with speech and contribute meaningfully to the
achievement of a communicative intention (Mc-
Neill and Duncan, 2000). For instance, in the ex-
ample below, extracted from our corpus (see Sec-
tion 3), a speaker identifies a landmark (composed
of a collection of five circles) on a map through a
combination of a pointing gesture and the mention
of the size and colour of the elements making up
the landmark.
(1) there?s a group of five large red ones [points]
In this case, the pointing gesture further con-
tributes to the communicative aim of identifying
the cluster of five objects, in tandem with the vi-
sual features mentioned in the description. Mc-
Neill?s proposal (McNeill and Duncan, 2000) is
that speech and gesture should be considered as
the joint outcome of the language production pro-
cess, rather than as outcomes of separate pro-
cesses. Various models have been proposed which
are more or less congruent with this view. For
example, de Ruiter (2000) proposes that the two
modalities are planned together at early stages of
conceptualisation during speech production, while
Kita and O?zyu?rek (2003) suggest that gestures are
planned by spatio-motoric processes which differ
from the planning of speech production, but inter-
act with it at particular points.
Recent computational work has also taken these
ideas on board. For example, Kopp et. al. (2008)
describe a system for the concurrent planning and
generation of gesture and speech, whose archi-
tecture is inspired by Kita and O?zyu?rek (2003)
and which makes use of ?multimodal concepts?
(inspired by McNeill?s ?growth points?) combin-
ing both propositional and visuo-spatial proper-
ties. This contrasts with earlier architectures, such
as that proposed by Andre? and Rist (1996), where
generation of text and gesture is undertaken by
separate modules communicating with a central
planner.
The idea that the planning of language is tightly
coupled with that of gesture raises the possibility
that the two modalities may overlap to different
degrees. Gesture may be completely redundant
with speech, or may encode aspects of the com-
municative intention that are not included in the
linguistic message itself. This raises an interesting
question for multimodal REG: are there features of
objects that tend to be mentioned in tandem with
a pointing gesture; if so, which are they? For ex-
ample, the reference in (1) mentions the size and
colour of the landmark, but not its location, pos-
sibly suggesting that the speaker relied on point-
ing to convey the ?where? of the target referent, as
opposed to the ?what?, which is conveyed by the
description. This, however, is not the case in the
example below, where pointing is accompanied by
a mention of the referent?s location.
(2) [...] the red ones directly to the left [...]
[points]
There are at least two views on the relationship
between pointing and describing (de Ruiter et al,
2012). On the one hand, the trade-off hypothesis
holds that the decision to use a pointing gesture de-
pends on the effort or ?cost? involved (the further
away from the speaker and the smaller a referent
is, the more costly it would be to point at it), com-
pared to the effort involved in describing a referent
linguistically.
On the other hand, pointing and (some aspects
of) describing might proceed hand in hand, so that
83
there is some degree of redundancy between the
two modalities. Under this view, pointing may be
chosen not based on (low) cost assessment but as
part of a specifically multimodal cognitive strat-
egy.
Evidence for the trade-off hypothesis is reported
by Bangerter (2004), who found that, as pointing
became easier in a task-oriented dialogue (because
the distance between the speaker and the referent
was shorter), there was a decrease in verbal effort,
as measured by the number of words produced, as
well as a decrease in the use of locative and visual
features such as colour. Piwek (2007) also found
that referring acts accompanied by pointing tended
to include descriptions containing fewer properties
than those which were not. These results are com-
patible with a view of the speaker/generator as es-
sentially seeking to minimise effort in the commu-
nicative act, adopting the easiest available strategy
that will not compromise communicative success
(Beun and Cremers, 1998).
Similar results are reported by van der Sluis and
Krahmer (2007), who model the trade-off hypoth-
esis in a multimodal REG algorithm based on the
graph-based framework of Krahmer et. al. (2003).
The algorithm chooses to use pointing gestures,
with various degrees of precision, depending on
their cost relative to that of features that can be
used in a linguistic description.
There is also evidence against the trade-off
model. Recent experimental work by de Ruiter
et. al. (2012) showed that the tendency for speak-
ers to point was unaffected by the difficulty of re-
ferring to an object using linguistic features, al-
though pointing did decrease with repeated refer-
ence to the same entities. Interestingly, the authors
observed a correlation between the rate of pointing
and the use of locative properties of objects. This
would appear to favour a model in which the lin-
guistically describable features of objects are dif-
ferentiated: speakers may be using locative prop-
erties and pointing together as part of a strategy to
identify the ?where? of an object. This is in line
with the observation by Louwerse and Bangerter
(2010) that, in visual domains, using pointing ges-
tures with locative expressions increases the speed
with which references are resolved.
The evidence from de Ruiter et. al would seem
to contradict the assumptions underlying current
multimodal REG models. As we have seen, van der
Sluis and Krahmer (van der Sluis and Krahmer,
2007) assume a trade-off between speech and ges-
ture. A similar assumption is made by Kranstedt
and Wachsmuth (2005), who view pointing ges-
turs as mainly concerned with the ?where? of an
object. Their algorithm, which underlies the plan-
ning of multimodal references by a virtual agent,
extends the Incremental Algorithm (Dale and Re-
iter, 1995) as follows. Given an object in a 3D
space, the algorithm first considers the possibil-
ity of producing an unambiguous pointing ges-
ture; failing this, a pointing gesture covering the
intended referent and some of its surrounding dis-
tractors may be planned. In the latter case, the al-
gorithm then integrates other features of the ob-
ject (e.g. its colour), in an effort to exclude the
distractors that remain within the scope of the am-
biguous point. One of the claims underlying this
model is that ?absolute? location, which is covered
by pointing, is given first preference after pointing
itself, with other features of a referent being con-
sidered afterwards, in a preference order that will
only use relative location if all other options (such
as colour) are exhausted.
In summary, the empirical evidence for the
relationship between pointing and describing is
mixed. While the view that the planning of lan-
guage in different modalities should be tightly
coupled has proven useful and productive, the pre-
cise way in which the two interact in a referring act
is still an open question, especially where the re-
lationship between location and the other features
of a target referent is concerned. In the remain-
der of this paper, we report on an empirical study
that used machine learning methods with a view to
establishing the relationship between descriptive
features and pointing in multimodal references.
Our study is not committed to a specific architec-
ture for multimodal reference planning; rather, our
aim is to establish whether pointing and describ-
ing can partly overlap in the information that they
convey about a referent. Specifically, we are in-
terested in whether the use of a description that
includes spatial or locative information excludes a
pointing gesture.
3 Corpus and data
The data used in this study comes from the MREDI
(Multimodal REference in DIalogue) corpus (van
der Sluis et al, 2008)2, a new collection of dia-
2We intend to make this corpus publicly available in the
near future.
84
Matcher's mapwithoutitinerary
Director's mapwith itinerary
Director Follower
Low screensto hide maps
(a) Experiment setup
START
1/5
16
9
2/1817
4/ 1514
12 7/11
8/13
10
16
16
16
16
14 141414
4/ 154
/ 15 4/ 154/ 15
12 12
12
12
1010
10 1
0
7/117/11
7/11 7/11
8/138/1
3 8/138/13
1/5
1/51/5
1/5
9
99 9
17 1717
17
2/18
2/18 2/18 2/183/63/63/63/63/6
(b) Group circles map (numbers indicate the order
in which landmarks are visited along the itinerary)
Figure 1: MREDI dialogue setup
Feature Name Definition Example
Visual
S Size mention of the target size the group of small circles
Sh Shape mention of the target shape the circles at the bottom
C Colour mention of the target colour The blue square near the red square
Deictic/anaphoric
I Identity Statement of identity between
the current and a previous or later target
the red square,
the same one we saw at number 5
D Deixis Use of a deictic reference those squares
Locative
RP Relative position Position of the target landmark relative
to another object on the map
the blue square
just below the red square
AP Absolute position Target position based on absolute
frame of reference
The blue circle down at the bottom
FP Path references References to non-targets on the
path leading to the target.
go east to the first tiny square,
past the blue one
DIR Directions Direction-giving. take a right, go across
and straight down
Action GZ Gaze Gaze at the shared map (boolean).
Point Pointing Use of a pointing gesture (boolean).1
Table 1: Features annotated in the dialogues. All features have frequency values, except for the Action
features, which are boolean.
logues elicited using a task similar to the Map-
Task (Anderson et al, 1991), in which a director
and a follower talked about a map displayed on a
wall in front of them, approximately 1 metre away.
Each also had a private copy of the map; the di-
rector?s map had an itinerary on it, and her task
was to communicate the itinerary to the follower,
who marked it on his own private map. Partici-
pants were free to interact using speech and ges-
ture, without touching the shared map or standing
up. They could see each other, but could not see
each other?s private maps. Figure 1(a) displays the
basic experimental setup.
The maps consisted of shapes (squares or cir-
cles), with a sequence of landmarks constituting
the itinerary (initially known only to the director).
The maps were designed to manipulate a number
of independent variables, in a balanced design:
? Cardinality The target destinations in the
itineraries were either individual landmarks
(in 2 of the maps) or sets of 5 landmarks with
the same attributes (e.g., all green squares);
? Visual Attributes: Targets on the itinerary
differed from their distractors ? the objects
in their immediate vicinity (the focus area)
? in colour, or in size, or in both colour and
size. The focus area was defined as the set of
objects immediately surrounding a target;
? Prior reference: Some of the targets were
visited twice in the itinerary;
? Shift of domain focus: Targets were located
near to or far away from the previous target.
Note that if two targets t1 and t2 were in the
near condition, then t1 is one of the distrac-
tors of t2 and vice versa.
Each participant dyad did all four maps (single-
ton squares and circles; group squares and circles),
85
in a pseudo-random order, alternating in the di-
rector/matcher role so that each was director for
two of the maps. Figure 1(b) displays the direc-
tor?s map consisting of group circles. Note that the
itinerary is marked by numbering the target land-
marks. Landmarks with two numbers are visited
twice (for example, the first landmark is marked
1, but is also marked 5, meaning that it is the first
and the fifth landmark in the itinerary). During the
experiment, the map was mounted on a wall and
blown up to A0 size; this significantly reduced the
impression of visual clutter.
Data was collected from 8 pairs of participants3.
In the present study, we focus exclusively on the
directors? utterances. These were transcribed and
split up according to the landmark to which they
corresponded. In case a landmark was described
over multiple turns in the dialogue, each turn was
annotated as a separate utterance. Utterances were
annotated with the features displayed in Table 1.
Broadly, features are divided into four types: (a)
Deictic/Anaphoric, pertaining to the use of de-
ictic demonstratives, and/or references to previ-
ously identified entities; (ii) Visual, that is, cor-
responding to a landmark?s perceptual properties;
(iii) Locative, involving a description of the ob-
ject?s location; and (iv) Action, pertaining to ges-
ture and gaze. All features are frequencies per
utterance, except for Action features, which are
boolean.
Feature Frequency Mean SD
S 510 0.23 0.48
Sh 252 0.10 0.40
C 603 0.30 0.50
I 249 0.10 0.40
D 375 0.17 0.43
RP 529 0.13 0.40
AP 293 0.13 0.40
FP 989 0.40 0.70
DIR 251 0.11 0.37
GZ 836
Point 370
Table 2: Descriptive statistics for features in the
corpus
The corpus consists of a total of 2255 director?s
3A number of other dialogues were recorded, but were not
included in the corpus because participants focused on their
own private maps and never used pointing gestures, making it
impossible to study the conditions under which such gestures
are produced.
utterances. The frequency of each feature in the
corpus, as well as the per-utterance mean and stan-
dard deviation (where relevant), are indicated in
Table 2; note that, with the exception of Action
features, all feature values are frequencies per ut-
terance.
Type No point (#) Point (#) Total
Group 907 201 1108
Singleton 978 169 1147
Total 1885 370 2255
Table 3: Frequency of occurrence of pointing ges-
tures relative to different object types.
As expected, linguistic features are much more
frequent than pointing gestures. In fact only
16.4% of the utterances in the corpus are accompa-
nied by pointing gestures. Previous studies, such
as that by Beun and Cremers (Beun and Cremers,
1998) report a higher incidence of pointing (48%
overall). Note, however, that Beun and Cremers
focussed exclusively on first mention descriptions
(which numbered 145 in all), while our corpus in-
cludes subsequent mentions, as well as multiple
consecutive references to the same object divided
over several utterances (which are counted sepa-
rately in our totals).
Table 3 shows frequency figures for the pointing
gestures in the corpus relative to the type of object
they refer to (group vs. singleton): in accordance
with the trade-off theory, which predicts that larger
objects should be easier to point at, we see a sig-
nificant difference (?2(1) = 4.769, p = 0.028)
between the two types, with more pointing occur-
ring with group objects (that is, in group maps).
4 Experiments
In much of the work discussed in Section 2, the
generation of pointing gestures is viewed as de-
pendent on physical characteristics of the refer-
ents, in other words on their being suitable for
pointing. This is especially true of work related
to the trade-off hypothesis, in which the costs of
pointing gestures are calculated as a function of
the referent object?s size and its distance from the
speaker. In the present paper, by contrast, we
are interested in investigating the relation between
pointing and linguistic means of referent identi-
fication. More specifically, we address the ques-
tion to what degree the different linguistic expres-
sions used by the speaker to refer to objects in
86
the MREDI dialogues, can be used to predict the
occurrence of pointing gestures. Note that this
question addresses the correlation between prop-
erties in a description and the occurrence of point-
ing, rather than the issue of how pointing and de-
scribing should be planned. Nevertheless, as we
have emphasised in Section 2, the question of co-
occurrence of the two referential strategies does
have a bearing on architectural issues.
A first set of experiments were run in order to
test the general trade-off hypothesis. We tested
a number of classifiers on the task of classifying
the binary feature point, given all the linguistic
features in the corpus. More specifically, the at-
tributes used for the classification were MapConfl,
DIR, RP, AP, FP, S, Sh, C, D, I, Point. They are
all explained and exemplified in Table 1 with the
exception of MapConfl, which indicates whether a
specific case in the data comes from a group or a
singleton map. This feature was included because,
as noted in the previous section, whether a target
landmark was a singleton or a group made a dif-
ference, presumably because groups are larger and
more visually salient. Note further that one of the
Action features, GZ (gaze), is ignored in the ex-
periments because it is an almost univocal predic-
tor of pointing. Indeed, gazing is involved roughly
every time Point has the value y (yes) (but not the
other way round).
The experiments were run using the Weka (Wit-
ten and Frank, 2005) tool, which gives access
to many different algorithms, and 10-fold cross-
validation was used throughout. The results are
shown in Table (4) in terms of Precision, Recall
and F-measure for each of the classifiers.
Classifier P R F
Baseline 1 (ZeroR) 0.699 0.836 0.761
Baseline 2 (OneR) 0.762 0.834 0.765
SMO 0.699 0.836 0.761
NaiveBayes 0.795 0.811 0.802
Logistic 0.806 0.84 0.808
J48 0.829 0.85 0.833
Table 4: Predicting pointing gestures given all the
linguistic features in the corpus: classification re-
sults.
Two baselines were created to evaluate the re-
sults. The first one is provided by the ZeroR clas-
sifier, which always chooses the most frequent
class, in this case n (no pointing gesture). The
F-measure obtained by this method is somewhat
high at 0.761, because there are relatively few
pointing gestures in the data. The second base-
line, which provides a slightly more interesting re-
sult against which to evaluate the other classifiers,
is provided by OneR. It achieves an F-measure of
0.765 by predicting a pointing gesture if DIR >=
2.5, in other words if there are at least 2.5 occur-
rences of direction expressions in the utterance.
Using this rule has the effect of predicting a few
of the pointing gestures, with an F-measure on the
y class (occurrence of pointing gestures) of 0.031.
The other four sets of results were obtained
by running four different classification algorithms
with the same set of attributes. Apart from SMO
(an algorithm using support vector machines), all
the classifiers perform better than the baseline.
The best results are produced by the decision tree
classifier J48, which obtains an overall F-measure
of 0.833, and an F-measure of 0.421 on the y class.
The confusion matrix generated by J48 on this
data-set is shown in Table (5)
a b ? classified as
1794 91 a = n
247 123 b = y
Table 5: Predicting pointing given all the linguistic
features in the corpus: confusion matrix.
The model created by the decision tree classi-
fier (J48) is quite complex (size=57 and no. of
leaves=29). The first branching, which corre-
sponds to no AP (Absolute Position) and no C
(Colour), assigns n to as many as 1571 instances
(with 115 errors). The tree is shown in Fig-
ure (2). The tree also shows that certain combina-
tions of features are more likely to be associated
with pointing gestures. These are predominantly
combinations including occurrences of AP, or, in
the absence of absolute position, combinations in-
cluding positive values for FP (Frequency of ref-
erence on Path) and DIR (Direction).
The maximum entropy model, built by the lo-
gistic regression algorithm (Logistic), shows sim-
ilar tendencies in that the attributes that are as-
signed the highest weights are AP, C and DIR.
These results confirm the general hypothesis
that there is a strong relationship between linguis-
tic features used in a description and pointing ges-
tures. Indeed, it is possible to predict pointing ges-
tures on the basis of the linguistic features used.
87
Figure 2: J48 decision tree
Classifier P R F Features
Exp1: J48 0.829 0.85 0.833 All features
Exp3: Logistic 0.806 0.84 0.808 Loc+D+I
Exp2: J48 0.835 0.851 0.806 MapConfl+Loc+D+I
Exp6: NaiveBayes 0.793 0.825 0.802 Loc
Exp4: NaiveBayes 0.764 0.804 0.779 MapConfl+Visual+D+I
Exp5: J48 0.761 0.808 0.777 MapConfl+Visual
Exp8: NaiveBayes 0.761 0.808 0.777 Visual
Exp9: NaiveBayes 0.761 0.801 0.775 Visual+D+I
Baseline 2: OneR 0.762 0.834 0.765 Dir
Exp7: F48 0.699 0.836 0.761 MapConfl+D+I
Baseline 1: ZeroR 0.699 0.836 0.761 Most freq class
Table 6: Predicting pointing gestures with different feature combinations: classification results.
In particular, the results suggest a difference be-
tween features that express locative properties and
those having to do with the visual description of
the same object (its colour, size and shape). More
specifically, it would seem that locative features
are more useful to the classifiers than visual prop-
erties.
To test this second hypothesis, we ran a series
of experiments where the task was still to predict
pointing gestures, but different subsets of the lin-
guistic features were tested one at the time. For
each feature combination, we run the classification
using J48, Naive Bayes and the Logistic regression
algorithm. In Table (6), we show the best result
obtained for each feature combination. The classi-
fiers are ordered from the most accurate to the least
accurate, and the combination of features used by
each of them is listed in the last column. The best
results and the two baselines from the previous set
of experiments are included for the sake of com-
parison. Note that the term Loc is used to refer to
all the locative attributes AP, DIR, RP, AP and FP,
88
while Visual refers to S, Sh and C.
The best results are those obtained when the
complete feature set is used in the training. How-
ever, the next best results are achieved by the clas-
sifiers using the locative features, either alone or
together with features concerning the map type,
identity with a previously mentioned object and
deictic reference, with an F-measure in the range
0.802?0.808. If visual features are used instead,
the F-measure is in the range 0.775?0.779. The
worst results are obtained if neither location nor
visual description are used. Thus, although the dif-
ferences between the best and the worst classifiers
are not dramatic, in this data we see a tendency for
the locative features to be slightly better predictors
of pointing gestures than features corresponding to
visual descriptions.
5 Discussion and conclusions
The automatic classification experiments de-
scribed above show that to a certain extent, the
pointing gestures occurring in the MREDI corpus
can be predicted based on the linguistic expres-
sions used by the speaker in conjunction with
pointing. More precisely, linguistic descriptions
can be used to predict about one third of the point-
ing gestures that speakers have produced in the
corpus. This is an interesting and novel result,
which not only supports the general notion that
gestures and speech should be seen as tightly cou-
pled, but also suggests that this coupling does not
result in a minimisation of redundancy between
the two modalities. Rather, it appears that a num-
ber of pointing gestures accompanied descriptions
containing locative properties, something that con-
tradicts the predictions of models based on the
trade-off hypothesis (Kranstedt and Wachsmuth,
2005; van der Sluis and Krahmer, 2007).
There are a number of limitations of the present
study, which we plan to address in future work.
First, pointing gestures in our corpus were rela-
tively scarce (16.4% of utterances were accompa-
nied by pointing). This in part explains the relative
accuracy of our baselines: predicting the major-
ity class (that is, no pointing) in every case will
clearly yield reasonable results given that the size
of the class is so large. On the other hand, the
relative scarcity of pointing may also indicate that
pointing is somewhat more costly than linguistic
description, in cognitive and physical terms. In
fact, the difference we see in the number of point-
ing gestures between singleton and group maps
also seems to confirm this assumption: in the
group maps, where objects are larger, and thus
more easily pointed at according to the trade-off
model, there are in fact significantly more pointing
gestures. The incidence of pointing may also have
been affected by the nature of the domains used:
although the shared maps in the experiments were
large and quite close to the interlocutors, the pres-
ence of objects of the same shape may have added
to the general visual clutter of the maps, making
pointing less likely.
Another aspect of the data that we have not
investigated is the presence of individual strate-
gies. We know that speakers differ a lot in their
use of gesturing as regards e.g. frequency, type
of gesture and representation techniques. Recent
models of gesture production for embodied agents
are taking such differences into account (Neff et
al., 2008; Bergmann and Kopp, 2009). Similarly,
some speakers might have a greater preference for
pointing than others. For example, Beun and Cre-
mers (1998) note that certain speakers in their cor-
pus explicitly stated that they had attempted to per-
form the task in their dialogues without pointing,
in spite of their having been told that they could
point. Recent data-driven experiments on referen-
tial descriptions by Dale and Viethen (Dale and Vi-
ethen, 2010), In a domain quite similar to the one
used here, suggest that speakers do indeed clus-
ter according to their preferred referential strat-
egy. Similar assumptions have informed REG al-
gorithms trained on the TUNA Corpus, in the con-
text of the Generation Challenges (Gatt and Belz,
2010) (Bohnet, 2008; Di Fabbrizio et al, 2008).
In future work, we plan to address this question
in a multimodal context, where results by Piwek
(2007) have already suggested that such individ-
ual strategies may play an important role.
The hypothesis that specific combinations of
pointing and linguistic descriptions (for example,
an object?s colour or size) can be excluded, is
clearly not borne out by the data. There is, how-
ever, a tendency for locative features to act as
stronger predictors of pointing gestures. Although
the trend is not very strong, it is an interesting
one since it confirms the experimental results by
de Ruiter et. al. reviewed earlier (de Ruiter et al,
2012). This may suggest that a pointing gesture
may ultimately be planned within the same system
as locative features (i.e. the decision of whether or
89
not to point is not dependent on the decision of
whether or not to describe inherent, visual proper-
ties of the object, but on whether the object?s lo-
cation is to be indicated). Another feature that is
worth exploring further is deixis, specifically the
difference between proximal and distal deictic ex-
pressions and their interaction with pointing ges-
tures. For example, Piwek et al (2007) found that
proximal deictic expressions tend to be associated
with a more intensive attentional focusing mecha-
nism, while Bangerter (2004) also observes an as-
sociation between pointing and the use of deictic
expressions.
From an NLG perspective, our results suggest
that decisions to generate a pointing gesture and
those to select visual attributes might take place
independently (perhaps in parallel, perhaps in dif-
ferent modules). From a cognitive perspective, it
suggests two types of interaction between atten-
tion/vision and language/gesture, related to the de-
scription of the ?what? of an object and its ?where?
(Landau and Jackendoff, 1993).
Finally, our study focused on the relationship
between the two modalities involved in a referen-
tial act, addressing the question of redundancy be-
tween them. We have not addressed the impact of
the visual properties of a target referent in relation
to its surrounding objects, on the choices speakers
make in these two modalities. This is a priority for
future work, given that the corpus was designed to
balance the presence or absence of various visual
properties of an object (see Section 3). Taking this
even further, it remains to be investigated, for ex-
ample, whether there would be interesting differ-
ences in the relationship betwene pointing and de-
scribing between 2D scenes of the kind used here,
and 3D environments of the sort used by Kranst-
edt and Wachsmuth (2005). Another priority is
to take into account the interactive nature of the
dialogues, with particular focus on the follower?s
feedback to the director, as an indicator of the suc-
cess of referential expressions. This is another as-
pect of the dialogue situation that may have an im-
pact on planning multimodal referential acts.
Acknowledgements
Special thanks are due to Ielka van der Sluis,
Adrian Bangerter and Paul Piwek, who were in-
volved in every step of the design, collection and
annotation of the MREDI corpus, and who also
commented on preliminary drafts of this paper.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H. S. Thompson, and R. Wein-
ert. 1991. The HCRC Map Task corpus. Language
and Speech, 34:351?366.
E. Andre? and T. Rist. 1996. Coping with temporal
constraints in multimedia presentation planning. In
Proceedings of the 13th National Conference on Ar-
tificial Intelligence (AAAI?96).
A. Bangerter. 2004. Using pointing and describing to
achieve joint focus of attention in dialogue. Psycho-
logical Science, 15(6):415?419.
K. Bergmann and S. Kopp. 2009. GNetIc - using
bayesian decision networks for iconic gesture gen-
eration. In A. Nijholt and H. Vilhja?lmsson, editors,
Proceedings of the 9th International Conference on
Intelligent Virtual Agents (LNAI 5773), pages 76?89.
Springer.
R.J. Beun and A. Cremers. 1998. Object reference in
a shared domain of conversation. Pragmatics and
Cognition, 6(1-2):121?152.
B. Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion (INLG?08).
C. Callaway and J. C. Lester. 2002. Pronominalization
in generated discourse and dialogue. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02).
R. Dale and E. Reiter. 1995. Computational interpre-
tation of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(8):233?
263.
R. Dale and J. Viethen. 2010. Attribute-centric re-
ferring expression generation. In E. Krahmer and
M. Theune, editors, Empirical Methods in Natu-
ral Language Generation, volume 5790 of LNAI.
Springer, Berlin and Heidelberg.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th annual meeting of the As-
sociation for Computational Linguistics (ACL?89),
pages 68?75.
J.P. de Ruiter, A. Bangerter, and P. Dings. 2012. The
interplay between gesture and speech in the produc-
tion of referring expressions: Investigating the trade-
off hypothesis. Topics in Cognitive Science, 4:232?
248.
J.P. de Ruiter. 2000. The production of gesture and
speech. In D. McNeill, editor, Language and Ges-
ture, pages 284?311. Cambridge University Press.
90
G. Di Fabbrizio, A. J. Stent, and S. Bangalore.
2008. Trainable speaker-based referring expression
generation. In Proceedings of the 12th Confer-
ence on Computational Natural Language Learning
(CONLL?08), pages 151?158.
A. Gatt and A. Belz. 2010. Introducing shared task
evaluation to nlg: The TUNA shared task evaluation
challenges. In E. Krahmer and M. Theune, editors,
Empirical Methods in Natural Language Genera-
tion. Springer.
S. Kita and A. O?zyu?rek. 2003. What does cross-
linguistic variation in semantic coordination of
speech and gesture reveal?: Evidence for an inter-
face representation of spatial thinking and speaking.
Journal of Memory and Language, 48:16?32.
S. Kopp, K. Bergmann, and I. Wachsmuth. 2008. Mul-
timodal communication from multimodal thinking:
Towards an integrated model of speech and gesture
production. International Journal of Semantic Com-
puting, 2(1):115?136.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Information
Sharing: Reference and Presupposition in Language
Generation and Interpretation. CSLI Publications,
Stanford.
E. Krahmer and K. van Deemter. 2012. Computational
generation of referring expressions: A survey. Com-
putational Linguistics, 38(1):173?218.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
A. Kranstedt and I. Wachsmuth. 2005. Incremental
generation of multimodal deixis referring to objects.
In Proceedings of the 10th European Workshop on
Natural Language Generation (ENLG?05).
B. Landau and R. Jackendoff. 1993. what and where in
spatial language and spatial cognition. Behavioral
and Brain Sciences, 16:217?238.
M. Louwerse and A. Bangerter. 2010. Effects of am-
biguous gestures and language on the time-course of
reference resolution. Cognitive Science, 34:1517?
1529.
K.F. McCoy and M. Strube. 1999. Generating
anaphoric expressions: Pronoun or definite descrip-
tion? In Proceedings of the Workshop on the Rela-
tion of Discourse/Dialogue Structure and Reference.
D. McNeill and S.D. Duncan. 2000. Growth points in
thinking for speaking. In D. McNeill, editor, Lan-
guage and Gesture, pages 141?161. Cambridge Uni-
versity Press.
D. McNeill. 1985. So you think gestures are nonver-
bal? Psychological Review, 92(3):350?371.
M. Neff, M. Kipp, I. Albrecht, and H.-P. Seidel. 2008.
Gesture modeling and animation based on a proba-
bilistic recreation of speaker style. ACM Transac-
tions on Graphics, 27(1):1?24.
P. Piwek, R-J. Beun, and A. Cremers. 2007. proximal
and distal in language and cognition: Evidence from
deictic demonstratives in dutch. Journal of Prag-
matics, 40(4):694?718.
P. Piwek. 2007. Modality choice for generation of re-
ferring acts: Pointing vs describing. In Proceedings
of the Workshop on Multimodal Output Generation
(MOG?07)., pages 129?139.
I. van der Sluis and E. Krahmer. 2007. Generating
multimodal referring expressions. Discourse Pro-
cesses, 44(3):145?174.
I. van der Sluis, P. Piwek, A. Gatt, and A. Bangerter.
2008. Towards a balanced corpus of multimodal re-
ferring expressions in dialogue. In Proceedings of
the Symposium on Multimodal Output Generation
(MOG?08).
I.H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, second edition.
91
