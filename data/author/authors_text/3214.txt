Formalising Multi-layer Corpora in OWL DL ?
Lexicon Modelling, Querying and Consistency Control
Aljoscha Burchardt1, Sebastian Pad?2?, Dennis Spohr3?, Anette Frank4?and Ulrich Heid3
1Dept. of Comp. Ling. 2Dept. of Linguistics 3Inst. for NLP 4Dept. of Comp. Ling.
Saarland University Stanford University University of Stuttgart University of Heidelberg
Saarbr?cken, Germany Stanford, CA Stuttgart, Germany Heidelberg, Germany
albu@coli.uni-sb.de pado@stanford.edu spohrds,heid@ims.uni-stuttgart.de frank@cl.uni-heidelberg.de
Abstract
We present a general approach to formally
modelling corpora with multi-layered anno-
tation, thereby inducing a lexicon model in a
typed logical representation language, OWL
DL. This model can be interpreted as a graph
structure that offers flexible querying func-
tionality beyond current XML-based query
languages and powerful methods for consis-
tency control. We illustrate our approach by
applying it to the syntactically and semanti-
cally annotated SALSA/TIGER corpus.
1 Introduction
Over the years, much effort has gone into the creation
of large corpora with multiple layers of linguistic an-
notation, such as morphology, syntax, semantics, and
discourse structure. Such corpora offer the possibility
to empirically investigate the interactions between
different levels of linguistic analysis.
Currently, the most common use of such corpora
is the acquisition of statistical models that make use
of the ?more shallow? levels to predict the ?deeper?
levels of annotation (Gildea and Jurafsky, 2002; Milt-
sakaki et al, 2005). While these models fill an im-
portant need for practical applications, they fall short
of the general task of lexicon modelling, i.e., creat-
ing an abstracted and compact representation of the
corpus information that lends itself to ?linguistically
informed? usages such as human interpretation or
integration with other knowledge sources (e.g., deep
grammar resources or ontologies). In practice, this
task faces three major problems:
?At the time of writing, Sebastian Pad? and Dennis Spohr
were affiliated with Saarland University, and Anette Frank with
DFKI Saarbr?cken and Saarland University.
Ensuring consistency. Annotation reliability and
consistency are key prerequisites for the extraction of
generalised linguistic knowledge. However, with the
increasing complexity of annotations for ?deeper? (in
particular, semantic) linguistic analysis, it becomes
more difficult to ensure that all annotation instances
are consistent with the annotation scheme.
Querying multiple layers of linguistic annotation.
A recent survey (Lai and Bird, 2004) found that cur-
rently available XML-based corpus query tools sup-
port queries operating on multiple linguistic levels
only in very restricted ways. Particularly problematic
are intersecting hierarchies, i.e., tree-shaped analyses
on multiple linguistic levels.
Abstractions and application interfaces. A per-
vasive problem in annotation is granularity: The gran-
ularity offered by a given annotation layer may di-
verge considerably from the granularity that is needed
for the integration of corpus-derived data in large
symbolic processing architectures or general lexical
resources. This problem is multiplied when more
than one layer of annotation is considered, for exam-
ple in the characterisation of interface phenomena.
While it may be possible to obtain coarser-grained
representations procedurally by collapsing categories,
such procedures are not flexibly configurable.
Figure 1 illustrates these difficulties with a sentence
from the SALSA/TIGER corpus (Burchardt et al,
2006), a manually annotated German newspaper cor-
pus which contains role-semantic analyses in the
FrameNet paradigm (Fillmore et al, 2003) on top
of syntactic structure (Brants et al, 2002).1 The se-
1While FrameNet was originally developed for English, the
majority of frames has been found to generalise well to other
389
which the official Croatia but in significant international-law difficulties bring would
Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (?which would bring
official Croatia into significant difficulties with international law?)
mantic structure consists of frames, semantic classes
assigned to predicating expressions, and the semantic
roles introduced by these classes. The verb bringen
(?to bring?) is used metaphorically and is thus analy-
sed as introducing one frame for the ?literal? reading
(PLACING) and one for the ?understood? reading
(CAUSATION), both with their own role sets.
The high complexity of the semantic structure even
on its own shows the necessity of a device for con-
sistency checking. In conjunction with syntax, it
presents exactly the case of intersecting hierarchies
which is difficult to query. With respect to the issue of
abstraction, note that semantic roles are realised vari-
ously as individual words (was (?which?) ) and con-
stituents (NPs, PPs), a well-known problem in deriv-
ing syntax-semantics mappings from corpora (Frank,
2004; Babko-Malaya et al, 2006).
Our proposal. We propose that the problems in-
troduced above can be addressed by formalising cor-
pora in an integrated, multi-layered corpus and lexi-
con model in a declarative logical framework, more
specifically, the description logics-based OWL DL
formalism. The major benefits of this approach are
that all relevant properties of the annotation and the
underlying model are captured in a uniform represen-
tation and, moreover, that the formal semantics of the
model makes it possible to use general and efficient
knowledge representation techniques for consistency
control. Finally, we can extract specific subsets from
a corpus by defining task-specific views on the graph.
After a short discussion of related approaches in
languages (Burchardt et al, 2006; Boas, 2005).
Section 2, Section 3 provides details on our method-
ology. Sections 4 and 5 demonstrate the benefits of
our strategy on a model of the SALSA/TIGER data.
Section 6 concludes.
2 Related Work
One recent approach to lexical resource modelling
is the Lexical Systems framework (Polgu?re, 2006),
which aims at providing a highly general represen-
tation for arbitrary kinds of lexica. While this is
desirable from a representational point of view, the
resulting models are arguably too generic to support
strong consistency checks on the encoded data.
A further proposal is the currently evolving Lex-
ical Markup Framework (LMF; Francopoulo et
al. (2006)), an ISO standard for lexical resource mod-
elling, and an LMF version of FrameNet exists. How-
ever, we believe that our usage of a typed formalism
takes advantage of a strong logical foundation and
the notions of inheritance and entailment (cf. Schef-
fczyk et al (2006)) and is a crucial step beyond the
representational means provided by LMF.
Finally, the closest neighbour to our proposal is
the ATLAS project (Laprun et al, 2002), which
combines annotations with a descriptive meta-model.
However, to our knowledge, ATLAS only models
basic consistency constraints, and does not capture
dependencies between different layers of annotation.
390
3 Modelling Multilevel Corpora in OWL DL
3.1 A formal graph-based Lexicon
This section demonstrates how OWL DL, a strongly
typed representation language, can serve to transpar-
ently formalise corpora with multi-level annotation.
OWL DL is a logical language that combines the
expressivity of OWL2 with the favourable computa-
tional properties of Description Logics (DL), notably
decidability and monotonicity (Baader et al, 2003).
The strongly typed, well-defined model-theoretic se-
mantics distinguishes OWL DL from recent alterna-
tive approaches to lexicon modelling.
Due to the fact that OWL DL has been defined
in the Resource Description Framework (RDF3), the
first central benefit of using OWL DL is the possibil-
ity to conceive of the lexicon as a graph ? a net-like
entity with a high degree of interaction between lay-
ers of linguistic description, with an associated class
hierarchy. Although OWL DL itself does not have a
graph model but a model-theoretic semantics based
on First Order Logic, we will illustrate our ideas with
reference to a graph-like representation, since this is
what we obtain by transforming our OWL DL files
into an RDFS database.
Each node in the graph instantiates one or more
classes that determine the properties of the node. In
a straightforward sense, properties correspond to la-
belled edges between nodes. They are, however, also
represented as nodes in the graph which instantiate
(meta-)classes themselves.
The model is kept compact by OWL?s support for
multiple instantiation, i.e., the ability of instances
to realise more than one class. For example, in a
syntactically and semantically annotated corpus, all
syntactic units (constituents, words, or even parts
of words) can instantiate ? in addition to a syntac-
tic class ? one or more semantic classes. Multiple
instantiation enables the representation of informa-
tion about several annotation layers within single
instances.
As we have argued in Section 2, we believe that
having one generic model that can represent all cor-
pora is problematic. Instead, we propose to construct
lexicon models for specific types of corpora. The
2http://www.w3.org/2004/OWL/
3http://www.w3.org/RDF/
design of such models faces two central design ques-
tions: (a) Which properties of the annotated instances
should be represented?; (b) How are different types
of these annotation properties modelled in the graph?
Implicit features in annotations. Linguistic anno-
tation guidelines often concentrate on specifying the
linguistic data categories to be annotated. However,
a lot of linguistically relevant information often re-
mains implicit in the annotation scheme. Examples
from the SALSA corpus include, e.g., the fact that
the annotation in Figure 1 is metaphorical. This in-
formation has to be inferred from the configuration
that one predicate evokes two frames. As such infor-
mation about different annotation types is useful in
final lexicon resources, e.g. to define clean generali-
sations over the data (singling out ?special cases?), to
extract information about special data categories, and
to define formally grounded consistency constraints,
we include it in the lexicon model.
Form of representation. All relevant information
has to be represented either as assertional statements
in the model graph (i.e., nodes connected by edges),
or as definitional axioms in the class hierarchy.4
This decision involves a fundamental trade-off be-
tween expressivity and flexibility. Modelling features
as axioms in the class hierarchy imposes definitional
constraints on all instances of these classes and is
arguably more attractive from a cognitive perspec-
tive. However, modelling features as entities in the
graph leads to a smaller class hierarchy, increased
querying flexibility, and more robustness in the face
of variation and noise in the data.
3.2 Modelling SALSA/TIGER Data
We now illustrate these decisions concretely by de-
signing a model for a corpus with syntactic and
frame-semantic annotation, more concretely the
SALSA/TIGER corpus. However, the general points
we make are valid beyond this particular setting.
As concerns implicit annotation features, we have
designed a hierarchy of annotation types which now
explicitly expresses different classes of annotation
phenomena and which allows for the definition of
annotation class-specific properties. For example,
frame targets are marked as a multi-word target if
4This choice corresponds to the DL distinction between TBox
(?intensional knowledge?) and ABox (?extensional knowledge?).
391
L
in
gu
is
ti
c
m
od
el
? Frames
w Intentionally_affect
w Placing
w Motion, . . .
? Roles
w Intentionally_affect.Act
w Placing.Means
? TIGER edge labels and POS
w SB, OA, PPER, ADJA, . . .
? Generalised functions and categories
w subj, obj, NounP, AdjP, . . .
A
nn
ot
at
io
n
ty
pe
s
? Frame Annotations
w Simple
w Metaphoric
w Underspecified
? Role Annotations
w Simple
w Underspecified
? Target Annotations
w Single-word targets
w Multi-word targets
? Sentences, syntactic units, . . .
Figure 2: Schema of the OWL DL model?s class hierarchy (?TBox?)
their span contains at least two terminal nodes. The
hierarchy is shown on the right of Figure 2, which
shows parts of the bipartite class hierarchy.
The left-hand side of Figure 2 illustrates the lin-
guistic model, in which frames and roles are organ-
ised according to FrameNet?s inheritance relation.
Although this design seems to be straightforward, it
is the result of careful considerations concerning the
second design decision. Since FrameNet is a hierar-
chically structured resource with built-in inheritance
relations, one important question is whether to model
individual frames, such as SELF_MOTION or LEAD-
ERSHIP, and their relations either as instances of a
general class Frame and as links between these in-
stances, or as hierarchically structured classes with
richer axiomatisation. In line with our focus on con-
sistency checking, we adopt the latter option, which
allows us to use built-in reasoning mechanisms of
OWL DL to ensure consistency.
Annotation instances from the corpus instantiate
multiple classes in both hierarchies (cf. Figure 2): On
the annotation side according to their types of phe-
nomena; on the linguistic side based on their frames,
roles, syntactic functions, and categories.
Flexible abstraction. Section 1 introduced granu-
larity as a pervasive problem in the use of multi-level
corpora. Figure 2 indicates that the class hierarchy
of the OWL DL model offers a very elegant way
of defining generalised data categories that provide
abstractions over model classes, both for linguistic
categories and annotation types. Moreover, proper-
ties can be added to each abstracting class and then
be used, e.g., for consistency checking. In our case,
Figure 2 shows (functional) edge labels and part-of-
speech tags provided by TIGER, as well as sets of
(largely theory-neutral) grammatical functions and
categories that subsume these fine-grained categories
and support the extraction of generalised valence in-
formation from the lexicon.
An annotated corpus sentence. To substantiate
the above discussion, Figure 3 shows a partial lexicon
representation of the example in Figure 1. The boxes
represent instance nodes, with classes listed above
the horizontal line, and datatype properties below
it.5 The links between these instances indicate OWL
object properties which have been defined for the
instantiated classes. For example, the metaphorical
PLACING frame is shown as a grey box in the middle.
Multiple inheritance is indicated by instances
carrying more than one class, such as the in-
stance in the left centre, which instantiates the
classes SyntacticUnit, NP, OA, NounP and
obj. Multi-class instances inherit the properties
of each of these classes, so that e.g., the meta-
phoric frame annotation of the PLACING frame
in the middle has both the properties defined for
frames (hasCoreRole) and for frame annotations
(hasTarget). The generalised syntactic categories
discussed above are given in italics (e.g., NounP).
The figure highlights the model?s graph-based
structure with a high degree of interrelation between
the lexicon entities. For example, the grey PLAC-
ING frame instance is directly related to its roles
(left, bottom), its lexical anchor (right), the surround-
ing sentence (top), and a flag (top left) indicating
metaphorical use.
5For the sake of simplicity, we excluded explicit ?is-a? links.
392
MetaphoricFrameAnnotationUspFrameAnnotationCausation
SyntacticUnitPRELSSB
hasTigerIDhasContent
NounPsubj
SyntacticUnitNENKhasTigerIDhasContent s2910_17"Kroatien"
Source
SyntacticUnitNPOA
hasTigerIDhasContent
SimpleRoleAnnotationPlacing.ThemehasContent
UspFrameAnnotationSupport LemmahasLemma
LexicalUnitrdf:ID bringen.Placing
SingleWordTargethasContent
SyntacticUnitVVINFHDhasTigerIDhasContent
SyntacticUnitNNNKhasTigerIDhasContent
NounPobj
s2910_15
"das"
s2910_502
"das offizielle Kroatien"
s2910_14
"was"SyntacticUnitARTNKhasTigerIDhasContent
"bringen"
"bringen"
SimpleRoleAnnotationPlacing.CausehasContent "was"
SentenceAnnotationhasSentenceIDhasContent s2910"Die Ausrufung des ..."
MetaphoricFrameAnnotationPlacing
"das offizielle Kroatien"
SimpleRoleAnnotationPlacing.GoalhasContent "in betr?chtliche v?lker..."
"bringen"s2910_23
"Schwierigkeiten"s2910_22
consistsOf
isAssignedTo hasFlag
hasFrameAnnotation hasFrameAnnotation
isUspWith
hasFrameAnnotation
hasCoreRole
hasCoreRoleisAssignedTo
hasCoreRole
hasTargetisTargetOf isAssignedTo hasHead
hasAnnotation?Instance
hasReadingisReadingOf
isAnnotationInstanceOf
isAssignedToSyntacticUnitADJANKhasTigerIDhasContent s2910_16"offizielle"
consistsOf consistsOf
...
Figure 3: Partial lexicon representation of an annotated corpus sentence
4 Querying the Model
We now address the second desideratum introduced
in Section 1, namely a flexible and powerful query
mechanism. For OWL DL models, such a mecha-
nism is available in the form of the Sesame (Broekstra
et al, 2002) SeRQL query language. Since SeRQL
makes it possible to extract and view arbitrary sub-
graphs of the model, querying of intersective hierar-
chies is possible in an intuitive manner.
An interesting application for this querying mecha-
nism is to extract genuine lexicon views on the corpus
annotations, e.g., to extract syntax-semantics map-
ping information for particular senses of lemmas, by
correlating role assignments with deep syntactic in-
formation. These can serve both for inspection and
for interfacing the annotation data with deep gram-
matical resources or general lexica. Applied to our
complete corpus, this ?lexicon? contains on average
8.5 role sets per lemma, and 5.6 role sets per frame.
The result of such a query is illustrated in Table 1 for
the lemma senken (?to lower?).
From such view, frame- or lemma-specific role
sets, i.e., patterns of role-category-function assign-
ments can easily be retrieved. A typical example is
given in Table 2, with additional frequency counts.
The first row indicates that the AGENT role has been
realised as a (deep) subject noun phrase and the ITEM
as (deep) object noun phrase.
We found that generalisations over corpus cate-
gories encoded in the class hierarchies are central
Role Cat Func Freq
Item NounP obj 26
Agent NounP subj 15
Difference PrepP mod-um 6
Cause NounP subj 4
Value_2 PrepP mod-auf 3
Value_2 PrepP pobj-auf 2
Value_1 PrepP mod-von 1
Table 1: Role-category-function assignments for
senken / CAUSE_CHANGE_OF_SCALAR_POSITION (CCSP)
Role set for senken / CCSP Freq
Agent Item 11
subj obj
NounP NounP
Cause Item 4
subj obj
NounP NounP
Item 4
obj
NounP
Agent Item Difference 2
subj obj mod-um
NounP NounP PrepP
Table 2: Sample of role sets for senken / CCSP
to the usefulness of the resulting patterns. For ex-
ample, the number of unique mappings between se-
mantic roles and syntactic categories in our corpus
is 5,065 for specific corpus categories, and 2,289 for
abstracted categories. Thus, the definition of an ab-
straction layer, in conjunction with a flexible query
mechanism, allows us to induce lexical characterisa-
tions of the syntax-semantics mapping ? aggregated
393
and generalised from disparate corpus annotations.
Incremental refinements. Querying, and the re-
sulting lexical views, can serve yet another purpose:
Such aggregates make it possible to conduct a data-
driven search for linguistic generalisations which
might not be obvious from a theoretical perspective,
and allow quick inspection of the data for counterex-
amples to plausible regularities.
In the case of semantic roles, for example, such
a regularity would be that semantic roles are not
assigned to conflicting grammatical functions (e.g.,
deep subject and object) within a given lemma. How-
ever, some of the role sets we extracted contained
exactly such configurations. Further inspection re-
vealed that these irregularities resulted from either
noise introduced by errors in the automatic assign-
ment of grammatical functions, or instances with
syntactically non-local role assignments.
Starting from such observations, our approach sup-
ported a semi-automatic, incremental refinement of
the linguistic and annotation models, in this case in-
troducing a distinction between local and non-local
role realisations.
Size of the lexicon. Using a series of SeRQL
queries, we have computed the size of the cor-
pus/lexicon model for the SALSA/TIGER data (see
Table 3). The lexicon model architecture as described
in Section 3 results in a total of more than 304,000
instances in the lexicon, instantiating 581 different
frame classes and 1,494 role classes.
5 Consistency Control
The first problem pointed out in Section 1 was the
need for efficient consistency control mechanisms.
Our OWL DL-based model in fact offers two mech-
anisms for consistency checking: axiom-based and
query-based checking.
Axiom-based checking. Once some constraint has
been determined to be universally applicable, it can
be formulated in Description Logics in the form of
axiomatic expressions on the respective class in the
model. Although the general interpretation of these
axioms in DL is that they allow for inference of new
statements, they can still be used as a kind of well-
formedness ?constraint?. For example, if an individ-
ual is asserted as an instance of a particular class, the
Type No. of instances
Lemmas 523
Lemma-frame pairs (LUs) 1,176
Sentences 13,353
Syntactic units 223,302
Single-word targets 16,268
Multi-word targets 258
Frame annotations 16,526
Simple 14,700
Underspecified 995
Metaphoric 785
Elliptic 107
Role annotations 31,704
Simple 31,112
Underspecified 592
Table 3: Instance count based on the first SALSA
release
reasoner will detect an inconsistency if this instance
does not adhere to the axiomatic class definition. For
semantic role annotations, axioms can e.g. define the
admissible relations between a particular frame and
its roles. This is illustrated in the DL statements be-
low, which express that an instance of PLACING may
at most have the roles GOAL, PATH, etc.
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Relations between roles can be formalised in a
similar way. An example is the excludes relation in
FrameNet, which prohibits the co-occurrence of roles
like CAUSE and AGENT of the PLACING frame. This
can be expressed by the following statement.
Placing v ?((?.hasRole Placing.Cause)u
(?.hasRole Placing.Agent))
The restrictions are used in checking the consistency
of the semantic annotation; violations of these con-
straints lead to inconsistencies that can be identified
by theorem provers. Although current state-of-the-art
reasoners do not yet scale to the size of entire cor-
pora, axiom-based checking still works well for our
data due to SALSA?s policy of dividing the original
TIGER corpus into separate subcorpora, each deal-
ing with one particular lemma (cf. Scheffczyk et al
(2006)).
394
Query-based checking. Due to the nature of our
graph representation, constraints can combine dif-
ferent types of information to control adherence to
annotation guidelines. Examples are the assignment
of the SUPPORTED role of support verb constructions,
which ought to be assigned to the maximal syntactic
constituent projected by the supported noun, or the
exclusion of reflexive pronouns from the span of the
target verb. However, the consistency of multi-level
annotation is often difficult to check: Not only are
some types of classification (e.g. assignment of se-
mantic classes) inherently difficult; the annotations
also need to be considered in context. For such cases,
axiom-based checking is too strict. In practice, it is
important that manual effort can be reduced by auto-
matically extracting subsets of ?suspicious? data for
inspection. This can be done using SeRQL queries
which ? in contrast to the general remarks on the
scalability of reasoners ? are processed and evaluated
very quickly on the entire annotated corpus data.
Example queries that we formulated examine sus-
picious configurations of annotation types, such as
target words evoking two or more frame annota-
tions which are neither marked as underspecified nor
tagged as a pair of (non-)literal metaphorical frame
annotations. Here, we identified 8 cases of omitted
annotation markup, namely 4 missing metaphor flags
and 4 omitted underspecification links.
On the semantic level, we extracted annotation
instances (in context) for metaphorical vs. non-
metaphorical readings, or frames that are involved
in underspecification in certain sentences, but not in
others. While the result sets thus obtained still re-
quire manual inspection, they clearly illustrate how
the detection of inconsistencies can be enhanced by
a declarative formalisation of the annotation scheme.
Another strategy could be to concentrate on frames
or lemmas exhibiting proportionally high variation
in annotation (Dickinson and Meurers, 2003).
6 Conclusion
In this paper, we have constructed a Description
Logics-based lexicon model directly from multi-layer
linguistic corpus annotations. We have shown how
such a model allows for explicit data modelling, and
for flexible and fine-grained definition of various de-
grees of abstractions over corpus annotations.
Furthermore, we have demonstrated that a pow-
erful logical formalisation which integrates an un-
derlying annotation scheme can be used to directly
control consistency of the annotations using general
KR techniques. It can also overcome limitations
of current XML-based search tools by supporting
queries which are able to connect multiple levels of
linguistic analysis. These queries can be used vari-
ously as an additional means of consistency control,
to derive quantitative tendencies from the data, to
extract lexicon views tailored to specific purposes,
and finally as a general tool for linguistic research.
Acknowledgements
This work has been partly funded by the German
Research Foundation DFG (grant PI 154/9-2). We
also thank the two anonymous reviewers for their
valuable comments and suggestions.
References
Franz Baader, Diego Calvanese, Deborah L. McGuinness,
Daniele Nardi, and Peter F. Patel-Schneider. 2003.
The Description Logic Handbook: Theory, Implemen-
tation and Applications. CUP.
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in Synchronizing the English
Treebank and PropBank. In Proceedings of the COL-
ING/ACL Workshop on Frontiers in Linguistically An-
notated Corpora, Sydney.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases. In-
ternational Journal of Lexicography, 18(4):445?478.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Jeen Broekstra, Arjohn Kampman, and Frank van Herme-
len. 2002. Sesame: A generic architecture for storing
and querying RDF and RDF Schema. In Proceedings
of the 1st ISWC, Sardinia.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th LREC,
Genoa.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th EACL, Budapest.
395
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Gil Francopoulo, Monte George, Nicoletta Calzolari,
Monica Monachini, Nuria Bel, Mandy Pet, and Clau-
dia Soria. 2006. LMF for multilingual, specialized
lexicons. In Proceedings of the 5th LREC, Genoa.
Anette Frank. 2004. Generalisations over corpus-
induced frame assignment rules. In Proceedings of the
LREC Workshop on Building Lexical Resources From
Semantically Annotated Corpora, Lisbon.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Catherine Lai and Steven Bird. 2004. Querying and up-
dating treebanks: A critical survey and requirements
analysis. In Proceedings of the Australasian Language
Technology Workshop, Sydney.
Christophe Laprun, Jonathan Fiscus, John Garofolo, and
Sylvain Pajot. 2002. Recent Improvements to the AT-
LAS Architecture. In Proceedings of HLT 2002, San
Diego.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Exper-
iments on sense annotations and sense disambigua-
tion of discourse connectives. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, Barcelona, Spain.
Alain Polgu?re. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. In Proceedings of the COLING/ACL Workshop
on Multilingual Language Resources and Interoper-
ability, Sydney.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.
2006. Ontology-based reasoning about lexical re-
sources. In Proceedings of the 5th OntoLex, Genoa.
396
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Modeling Monolingual and Bilingual Collocation Dictionaries in
Description Logics
Dennis Spohr and Ulrich Heid
Institute for Natural Language Processing
University of Stuttgart
Azenbergstr. 12, D-70174 Stuttgart, Germany
{spohrds,heid}@ims.uni-stuttgart.de
Abstract
This paper discusses an approach to mod-
eling monolingual and bilingual dictio-
naries in the description logic species of
the OWL Web Ontology Language (OWL
DL). The central idea is that the model
of a bilingual dictionary is a combination
of the models of two monolingual dictio-
naries, in addition to an abstract transla-
tion model. The paper addresses the ad-
vantages of using OWL DL for the design
of monolingual and bilingual dictionaries
and proposes a generalized architecture for
that purpose. Moreover, mechanisms for
querying and checking the consistency of
such models are presented, and it is con-
cluded that DL provides means which are
capable of adequately meeting the require-
ments on the design of multilingual dictio-
naries.
1 Introduction
We discuss the modeling of linguistic knowledge
about collocations, for monolingual and bilingual
electronic dictionaries, for multiple uses in NLP
and for humans.
Our notion of collocation is a lexicographic
one, inspired by (Bartsch, 2004); we start from
her working definition: ?collocations are lexically
and/or pragmatically constrained recurrent cooc-
currences of at least two lexical items which are
in a direct relation with each other.? The fact of
being lexically and/or pragmatically constrained
leads to translation problems, as such constraints
are language specific. With Hausmann (2004), we
assume that collocations have a base and a col-
locate, where the base is autosemantic and thus
translatable without reference to the collocation,
whereas the collocate is synsemantic, i.e. its read-
ing is selected within a given collocation. Ex-
amples of collocations according to this defini-
tion include adjective+noun-combinations (heavy
smoker, strong tea, etc.), verb+subject- (question
arises, question comes up) and verb+complement-
groups (give+talk, take+walk) etc. The definition
excludes however named entities (Rio de Janeiro)
and frequent compositional groups (e.g. the police
said...). Our data have been semi-automatically
extracted from 200 million words of German
newspaper text of the 1990s (cf. Ritz (2005)).
We claim that a detailed monolingual descrip-
tion of the linguistic properties of collocations pro-
vides a solid basis for bilingual collocation dictio-
naries. The types of linguistic information needed
for NLP and those required for human use, e.g. in
text production or translation into a foreign lan-
guage, overlap to a large extent. Thus it is rea-
sonable to define comprehensive monolingual data
models and to relate these with a view to transla-
tion.
In section 2, we briefly list the most impor-
tant phenomena to be captured (see also Heid and
Gouws (2006)); section 3 introduces OWL DL,
motivates its choice as a representation format and
describes our monolingual modeling. In section 4,
we discuss and illustrate the bilingual dictionary
architecture.
2 Collocation Data
Properties of collocations. A mere list of word
pairs or sequences (give a talk, lose one?s patience)
is not a collocation dictionary. For use in NLP, lin-
guistic properties of the collocations and of their
components must be provided: these include the
category of the components (giveV + talkN ), the
65
distribution of base (talk) and collocate (give), as
well as morphosyntactic preferences, e.g. with re-
spect to the number of an element (e.g. have high
hopes), the use of a determiner (lose one?sposs|{}
patience, cf. Evert et al (2004)).
For collocations to be identifiable in the context
of a sentence (e.g. to avoid attachment ambiguity
in parsing) and, conversely, in generation, to be
correctly inserted into a sentence, the syntagmatic
behavior of collocations must be described. This
includes their function within a sentence (e.g in
the case of adverbial NPs) and the subcategoriza-
tion of their components, e.g. with support verb
constructions (make the proposal to + INF). As
subcategorization is not fully predictable from
the subcategorization of the noun (how to explain
the preposition choice in Unterstu?tzung finden
bei jmdm, ?find support in so.?, be supported?),
we prefer to encode the respective data in the
monolingual dictionary. To support translation
mapping at the complement level, the representa-
tion of each complement contains its grammatical
category (NP, AP, etc.), its grammatical func-
tion (subject, object, etc.) and a semantic role
inspired by FrameNet1. This allows us to cater
for divergence cases: jmdSubj/SPEAKER bringt
jmdmInd.Obj/ADDRESSEE etw.Obj/TOPIC in
Erinnerung vs. someoneSubj/SPEAKER reminds
someoneObj/ADDRESSEE of sth.Prep.Obj/TOPIC .
Relations involving collocations. For language
generation, paraphrasing or for summarization,
paradigmatic relations of collocations must also
be modeled. These include synonymy, antonymy
and taxonomic relations, but also morphological
ones (word formation) and combinations of col-
locations. Synonymy and antonymy should re-
late collocations with other collocations, but also
with single words and with idioms: all three types
should have the same status. Next to strict syn-
onymy, there may be ?quasi-synonymy?.
Transparent noun compounds tend to share col-
locates with their heads (Pause einlegen, Rauch-
pause einlegen, Kaffeepause einlegen): if the re-
lation between compound and head (Kaffeepause
? Pause) and between the respective collocations
is made explicit, this knowledge can be exploited
in translation, when a compositional equivalent
is chosen (have a (smoking/coffee) break). Para-
phrasing and its applications also profit from an
explicit representation of morphological relations
1Cf. http://framenet.icsi.berkeley.edu/
between collocates: submit + proposal, submis-
sion of + proposal and submitter of + proposal all
refer to the same collocational pattern.
A formal model for a collocation dictionary,
monolingual and/or bilingual, has to keep track
of the above mentioned properties and relations of
collocations; both should be queriable, alone and
in arbitrary combinations.
Other collocation dictionaries and dictionary
architectures. Most of the above mentioned
properties and relations have been discussed in the
descriptive literature, but to our knowledge, they
have never been modeled all in an electronic dic-
tionary. The Danish STO dictionary (Braasch and
Olsen, 2000) and Krenn?s (2000) database of Ger-
man support verb+PP-constructions both empha-
size morphosyntactic preferences, but do not in-
clude relations. The electronic learners? dictionar-
ies DAFLES and DICE2 focus on semantic expla-
nations of collocations, but do not contain details
about most of the properties and relations men-
tioned above. The implementation of Mel?c?uk?s
Meaning?Text-Theory in the DiCo/LAF model3
comes closest to our requirements, insofar as it
is highly relational and includes some though not
all of the morphological relations we described
above.
The Papillon project (Se?rasset and Mangeot-
Lerebours, 2001) proposes a general architecture
for the interlingual linking of monolingual dictio-
naries; as it is inspired by the DiCo formalizar-
ion, it foresees links between readings, e.g. to ac-
count for morphological relations. This mecha-
nism could in principle be extended to syntag-
matic phenomena; we are, however, not aware of
a Papillon-based collocation dictionary.
3 Modeling in OWL DL
In this section, we present the main features of
OWL DL and their relevance to the modeling of
lexical data. Section 3.2 addresses the design of
a monolingual collocation dictionary using OWL
DL (Spohr, 2005).
3.1 Main Features of OWL
OWL DL is the description logic sublanguage
of the OWL Web Ontology Language (Bech-
2Cf. http://www.kuleuven.ac.be/dafles/
and DICE: http://www.dicesp.com/
3Cf. http://olst.ling.umontreal.ca/
dicouebe/
66
hofer et al, 2004), combining the expressivity of
OWL with the computational completeness and
decidability of Description Logics (Baader et al,
2003)4. Properties of OWL DL relevant for lexical
modeling are listed and discussed in the following.
Classes. An OWL DL data model consists of
a subsumption hierarchy of classes, i.e. a class
X subsumes all its subclasses X1 to Xn. While
classes represent concepts, their instances (called
OWL individuals) represent concrete manifesta-
tions in the model. Classes and their instances can
be constrained by stating assertions in the model
definition, e.g. a class can be defined as being
disjoint with other classes, which means that in-
stances of a certain class cannot at the same time
be instances of the disjoints of this particular class.
Properties. Classes are described by properties.
These can be used either to specify XML Schema
Datatypes (datatype properties) or to relate in-
stances of one class to instances of (probably)
other classes (object properties). These classes are
then defined as the domain and range of a property,
i.e. a particular property may only relate instances
of classes in its domain to instances of classes in
its range. In addition to this, a property may be
assigned several distinct formal attributes, such as
symmetric, transitive or functional, and can be de-
fined as the inverse of another property. Similar
to classes, properties can be structured hierarchi-
cally as well, which, among others, facilitates the
use of underspecified information in queries (see
section 3.2).
Inferences. The possibility to infer explicit
knowledge from implicit statements is a core fea-
ture of OWL DL and can be performed by using
DL reasoners (such as FaCT5, Pellet6 or Racer-
Pro7). The most basic inference is achieved via
the subsumption relation among classes or prop-
erties in the respective hierarchy (see above), but
also more sophisticated inferences are possible.
Among others, these may involve the formal at-
tributes of properties just mentioned. For example,
4As the emphasis in our work is on morphology, syntax
and lexical combinatorics, we profit from the formal prop-
erties of DL without feeling the need for non-monotonicity
as implemented, for example, in DATR (Evans and Gazdar,
1996).
5http://www.cs.man.ac.uk/?horrocks/
FaCT/
6http://www.mindswap.org/2003/pellet/
7http://www.racer-systems.com
stating that instance A is linked to B via a sym-
metric property P leads a reasoner to infer that B
is also linked to A via P. In conjunction with tran-
sitivity, a relatively small set of explicit statements
may suffice to interrelate several instances implic-
itly (i.e. all instances in a particular equivalence
class created by P).
Consistency. In addition to inferences, DL rea-
soners can further be used to check the consistency
of an OWL DL model. One of the primary ob-
jectives is to check whether the assertions made
about classes and their instances (see above) are
logically consistent or whether there are contradic-
tions. This consistency checking is based on the
open-world assumption, which states that ?what
cannot be proven to be true is not believed to be
false? (Haarslev and Mo?ller, 2005). Since lexi-
cal data occasionally demand a closed world, other
checking formalisms are required, which are men-
tioned in section 3.2 below.
3.2 Monolingual Collocation Dictionary
A data model for a monolingual collocation dic-
tionary based on OWL DL has been presented
in (Spohr, 2005). It was designed using the
Prote?ge? OWL Plugin (Knublauch et al, 2004) and
makes use of the advantages of OWL DL men-
tioned above.
Lexical vs. descriptive entities. On the class
level, the model distinguishes between lexical en-
tities (e.g. single-word and multi-word entities,
such as collocations or idioms) and descriptive en-
tities (e.g. gender, part-of-speech, or subcategori-
sation frames), with lexical entities being linked
to descriptive entities via properties. More than 40
of these descriptive properties have been modeled.
In order to reflect the distinction between metalan-
guage vocabulary and object language vocabulary,
the two types of entities can be separated such that
they are part of different models. In other words,
the classes and instances of descriptive entities
constitute a model of descriptions, which is im-
ported by a lexicon model containing classes and
instances of lexical entities (see also section 4.1
below).
Lexical relations. In addition to descrip-
tive properties, the data model also contains
a number of lexical relations linking lexical
entities, such as morphological or semantic
relations. These relations have been structured
67
hierarchically and contain several subproperties,
such as hasCompound or isSynonymOf,
which use the formal attributes mentioned in
section 3.1. For instance, isSynonymOf
has been defined as a symmetric and transi-
tive property (as opposed to the non-transitive
isQuasiSynonymOf; see section 2), while
hasCompound has been defined as the inverse of
a property isCompoundOf. A small sample of
descriptive and lexical relations of the collocation
Kritik u?ben is illustrated in Figure 1 below.
Property Value
hasLemma ?Kritik u?ben?
hasCompound Selbstkritik ueben
isSynonymOf kritisieren VV 1
hasCollocationType V-Nobj acc
hasComplementation SubcatFrame 12
hasExampleSentence Example 84
isInCorpus HGC-STZ
Figure 1: Sample of the properties of Kritik u?ben
Semantic relations link lexical entities on
the conceptual (i.e. word sense) level. There-
fore, the synonym of Kritik u?ben is not some
general single-word entity kritisieren VV,
but a particular word sense of kritisieren,
kritisieren VV 1 in this case (see
Spohr (2005) for more detail).
Queries. The data model can be queried very
efficiently using the Sesame framework (Broek-
stra et al, 2002; Broekstra, 2005) and its associ-
ated query language SeRQL. An example query
retrieving all collocations and their types is given
below, along with a sample of the results8.
SELECT *
FROM {A} rdf:type {lex:Collocation},
{A} lex:hasCollocationType {B}
A B
in_Frage_kommen V-PPpobj
Kritik_ueben V-Nobj_acc
Lob_aussprechen V-Nobj_acc
zu_Last_legen V-PPpobj
Figure 2: Query for retrieving collocations and
their types, along with results
Due to the fact that the relations in the data
8In these examples, lex: is the namespace prefix for re-
sources defined in the data model.
model have been structured hierarchically, it is
possible to state underspecified queries. Figure 3
illustrates an underspecified query for semanti-
cally related entities, regardless of the precise na-
ture of this relation. Hence, the first two rows
in the result table below contain synonym pairs,
while the last two rows contain antonym pairs.
SELECT *
FROM {A} lex:hasSemanticRelationTo {B}
A B
Kritik_ueben kritisieren_VV_1
kritisieren_VV_1 Kritik_ueben
Kritik_ueben Lob_aussprechen
Lob_aussprechen Kritik_ueben
Figure 3: Underspecified query for semantically
related entities, along with results
As is indicated in Figure 3, the results appear
twice, i.e. they contain every combination of those
entities between which the relation holds. This is
due to the fact that the respective semantic rela-
tions have been defined as symmetric properties
(see above).
Consistency and data integrity. Section 3.1
mentioned the distinction between the open-world
assumption and the closed-world assumption.
While the consistency checking performed by DL
reasoners is generally based on an open world, it
is vital especially for lexical data to simulate a
closed world in order to check data integrity. Con-
sider, for instance, the assertion that every collo-
cation has to have a base and a collocate. Due to
the open-world assumption, a DL reasoner would
never render a collocation violating this constraint
inconsistent, simply because it cannot prove that
this collocation has either no base or no collo-
cate. In order for this to happen, the simulation
of a closed world is needed. In our approach, this
is achieved by stating consistency constraints in
SeRQL. Figure 4 below illustrates a constraint for
the purpose just mentioned.
This query retrieves all collocations and sub-
tracts those who have a path to both a base and
a collocate. The result set then contains exactly
those instances which have either no base or no
collocate.
68
SELECT Coll
FROM {Coll} rdf:type {lex:Collocation}
MINUS
SELECT Coll
FROM {Coll} lex:hasBase {};
lex:hasCollocate {}
Figure 4: Constraint checking: does every collo-
cation have a base and a collocate?
4 Bilingual Model Architecture
Based on the definition of a monolingual colloca-
tion dictionary described above, the architecture of
a bilingual dictionary model can be designed such
that it is made up of several components (i.e. OWL
models). These are introduced in the following.
4.1 Components of a Bilingual Dictionary
The components of a bilingual dictionary are illus-
trated in Figure 5.
Translation model
Bilingual dictionary model
createdFrom
Monolingual dictionary model
Model of descriptions
Lexicon model
imports
createdFromcreatedFrom
imports imports
Monolingual dictionary model
Figure 5: Architecture of a bilingual dictionary
model
Model of descriptions. The most basic com-
ponent of a bilingual dictionary model is a
model of descriptions, which contains language-
independent classes and instances of descriptive
entities, as well as the relations among them (see
section 3.2).
Lexicon model. The model of descriptions is
imported by an abstract lexicon model via the
owl:imports statement (see (Bechhofer et al,
2004)). The effect of using the import statement
is that the lexicon model can access the classes,
instances and properties defined in the description
model without being able to alter the data therein.
In addition to the thus available classes, the lexi-
con model further provides classes of lexical enti-
ties and relations among them, as well as relations
linking lexical and descriptive entities.
Monolingual dictionary model. The lexicon
model serves as input for the creation of a mono-
lingual dictionary model, i.e. the lexicon model is
not imported by the dictionary model, rather the
dictionary model is an instantiation of it. There are
practical reasons for doing so, the most important
one being that the class of lexical entities (defined
in the lexicon model) and its instances (defined
in the monolingual dictionary) thus have the same
namespace prefix, which would not be the case if
the lexicon model was imported by the monolin-
gual dictionary. The advantages are most obvious
in the context of the mapping between monolin-
gual dictionary models (see section 4.2). Finally,
a monolingual dictionary may further introduce its
own instances (or even classes) of descriptive en-
tities, i.e. descriptions which are language-specific
and which are hence not part of the language-
independent model of descriptions (see above).
Translation model. The translation model is an
abstract model containing only relations between
monolingual dictionary models, i.e. it does not
contain class definitions. Since the model is re-
quired to be generic, these relations do not have
a specified domain and range, as otherwise the
translation model would be restricted to a single
language pair. The specification of the domain
and range of the relations is performed in the fi-
nal model of the bilingual dictionary.
Bilingual dictionary model. The bilingual dic-
tionary model is an instantiation of the translation
model. It further imports two monolingual dictio-
69
nary models and specifies the domain and range of
the abstract relations in the translation model (see
section 4.2 below).
4.2 Mapping between Models
By importing the monolingual dictionaries, each
of these models is assigned a unique namespace
prefix, e.g. english: or german:. Thus, in
an English-German dictionary, for instance, a rela-
tion called hasTranslationmay be defined as
a symmetric property linking lexical entities of the
English monolingual dictionary model (i.e. its do-
main is defined for instances with the english:
prefix) to lexical entities of the German model
(i.e. instances with german:). This translation
mapping is illustrated in Figure 6 for the colloca-
tion Kritik u?ben.
express criticism
MWE: Collocation Single?Word Entity
criticize
Monolingual English Dictionary Model
Paraphrase
"to criticize very fiercely"
Kritik ?ben kritisieren
MWE: Collocation Single?Word Entity
MWE: Idiom
in der Luft zerrei?en
Monolingual German Dictionary Model
Figure 6: Translation mapping between monolin-
gual dictionaries
As is indicated there, multi-word entities can be
translated as single-word entities and vice versa.
Moreover, since hasTranslation has been
defined as a symmetric property, the translation
mapping is bidirectional. However, since some in-
stance in one language model might not have an
equivalent instance in the other model, a further
property can be defined which links the respective
entity to a new instance created in the bilingual
model (see Paraphrase in the figure above). As
this instance is only required for the modeling of
this particular bilingual dictionary, it is not part of
the ?original? monolingual models, and hence the
relation between the respective entities is not bidi-
rectional.
In addition to the translation mapping of lexi-
cal entities, it may further be necessary to map
instances of descriptive entities of one model
onto instances in the other model. As was men-
tioned in section 4.1, the model of descriptions
contains language-independent descriptive enti-
ties. Since both monolingual dictionaries import
the model of descriptions (via the lexicon model),
the two ?versions? of it are unified in the bilin-
gual model. However, it is certainly conceivable
to have two languages which both avail them-
selves of a descriptive entity that is not language-
independent, but which is the same for the two
languages in question. For example, not all lan-
guages have the gender neuter. English and
German, however, do have it, and therefore an
English-German bilingual dictionary has to ex-
press that english:neuter is the same as
german:neuter. In OWL, this can be achieved
by using the owl:sameAs statement, which ex-
presses exactly the circumstances just mentioned.
4.3 Example Query
A query retrieving the situation depicted in
Figure 6 is given below. It extracts the
(quasi-)synonyms of Kritik u?ben (which Kritik
u?ben itself is a part of) and their respective transla-
tions and/or paraphrases. The latter is achieved by
restricting the properties that Rel2 may stand for
to those having the prefix bdm:, i.e. the prefix de-
fined for the bilingual dictionary model. In other
words, the query leaves the exact relation between
B and C underspecified and simply restricts it to
being defined in the bilingual dictionary, which
only contains relations linking instances belonging
to different monolingual dictionaries. The results
are shown in the table below.
5 Conclusion
We have described a model for monolingual and
bilingual collocation dictionaries in OWL DL.
This formalism is well suited for the intended
modularization of linguistic resources, be they
language- or language-pair- specific (our dictio-
70
SELECT DISTINCT B, C
FROM {} german:hasLemma {A};
Rel1 {B} Rel2 {C}
WHERE A LIKE "Kritik u?ben"
AND (Rel1 = german:isSynonymOf
OR Rel1 = german:isQuasiSynonymOf)
AND namespace(Rel2) = bdm:
B C
kritisieren_VV_1 express_criticism
kritisieren_VV_1 criticize_VV_1
Kritik_ueben express_criticism
Kritik_ueben criticize_VV_1
in_Luft_zerreissen ?to criticize very fiercely?
Figure 7: Query for retrieving the
(quasi-)synonyms of Kritik u?ben and their
translations and paraphrases, along with results
nary models), generalized over one or more lan-
guages (our lexicon model), or more abstract, in
the sense of a meta-model or an inventory of the
descriptive devices shared by the linguistic de-
scriptions of several languages (our model of de-
scriptions, see figure 5 above). This model of
descriptions will be larger for related languages
(e.g. the indo-european ones), and smaller for ty-
pologically very diverse languages; it is however
by no means meant to have any interlingual, let
alone universal function, but is rather understood
in the sense of PARGRAM?s shared inventory of
descriptive devices9.
We have modelled so far about 1000 colloca-
tions, their components, preferences and relations
(also with single words); we intend to consider-
ably enlarge the collocation dictionary, using the
possibilities to combine OWL DL models with
databases, offered by the Sesame framework. The
formalism also supports experiments with credu-
lous inferencing at the level of translation equiv-
alence, e.g. by following not only explicit equiv-
alence relations, but also synonymy relations: in
line with the query discussed in section 4.3 above
(cf. Figure 7), one could also start from the English
express criticism and retrieve the equivalent collo-
cation Kritik u?ben as well as its (quasi-)synonyms
kritisieren (single word) and in der Luft zerrei?en
(idiom), which may thus be proposed as equivalent
candidates for express criticism.
More such investigations into the data collec-
9Cf. http://www2.parc.com/istl/groups/
nltt/pargram/gram.html
tion are planned; they may require non-standard
access to the dictionary, i.e. access via paths in-
volving other properties and relations than just
lemmas and equivalents. The relational nature of
the dictionary supports this kind of exploration;
we intend to specify and implement a ?linguist-
friendly? query overlay to SeRQL and a Graphi-
cal User Interface to make such explorations more
easy.
References
Franz Baader, Diego Calvanese, Deborah L. McGuin-
ness, Daniele Nardi, and Peter F. Patel-Schneider.
2003. The Description Logic Handbook: Theory,
Implementation and Applications. Cambridge Uni-
versity Press, Cambridge, UK.
Sabine Bartsch. 2004. Structural and Functional
Properties of Collocations in English. A Corpus
Study of Lexical and Pragmatic Constraints on Lex-
ical Cooccurrence. Narr, Tu?bingen, Germany.
Sean Bechhofer, Frank van Harmelen, Jim Hendler, Ian
Horrocks, Deborah L. McGuinness, Peter F. Patel-
Schneider, and Lynn Andrea Stein. 2004. OWL
Web Ontology Language Reference. Technical re-
port.
Anna Braasch and Sussi Olsen. 2000. Formalised
representation of collocations in a Danish compu-
tational lexicon. In Proceedings of the EURALEX
International Congress 2000, Stuttgart, Germany.
Jeen Broekstra, Arjohn Kampman, and Frank van Her-
melen. 2002. Sesame: A Generic Architecture for
Storing and Querying RDF and RDF Schema. In
Proceedings of the First International Semantic Web
Conference (ISWC 2002), pages 54?68, Sardinia,
Italy.
Jeen Broekstra. 2005. Storage, Querying and Infer-
encing for Semantic Web Languages. Ph.D. thesis,
Vrije Universiteit Amsterdam, The Netherlands.
Roger Evans and Gerald Gazdar. 1996. DATR: A lan-
guage for lexical knowledge representation. Com-
putational Linguistics, 22(2):167?216.
Stefan Evert, Ulrich Heid, and Kristina Spranger.
2004. Identifying Morphosyntactic Preferences in
Collocations. In Proceedings of LREC-2004, Lis-
bon, Portugal.
Volker Haarslev and Ralf Mo?ller, 2005. RacerPro
User?s Guide and Reference Manual, Version 1.8.1.
Franz Josef Hausmann. 2004. Was sind eigentlich Kol-
lokationen? In Karin Steyer, editor, Wortverbindun-
gen - mehr oder weniger fest, pages 309?334. Insti-
tut fu?r Deutsche Sprache: Jahrbuch 2003.
71
Ulrich Heid and Rufus H. Gouws. 2006. A model
for a multifunctional electronic dictionary of collo-
cations. Draft of a paper submitted to EURALEX
2006.
Holger Knublauch, Mark A. Musen, and Alan L. Rec-
tor. 2004. Editing description logic ontologies
with the Prote?ge? OWL plugin. In Proceedings of
the International Workshop in Description Logics -
DL2004, Whistler, BC, Canada.
Brigitte Krenn. 2000. The Usual Suspects: Data-
Oriented Models for Identification and Representa-
tion of Lexical Collocations. Ph.D. thesis, DFKI
Universita?t des Saarlandes, Saarbru?cken, Germany.
Julia Ritz. 2005. Entwicklung eines Systems
zur Extraktion von Kollokationen mittels mor-
phosyntaktischer Features. Diploma thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart, Germany.
Gilles Se?rasset and Mathieu Mangeot-Lerebours.
2001. Papillon lexical database project: Monolin-
gual dictionaries & interlingual links. In Proceed-
ings of the Sixth Natural Language Processing Pa-
cific Rim Symposium: NLPRS-2001, pages 119?125,
Tokyo, Japan.
Dennis Spohr. 2005. A Description Logic Approach
to Modelling Collocations. Diploma thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart, Germany.
72
Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 38?45,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Part-of-Speech tagging of Northern Sotho:
Disambiguating polysemous function words
Gertrud Faa??? Ulrich Heid? Elsabe? Taljard? Danie Prinsloo?
? Institut fu?r Maschinelle Sprachverarbeitung ? University of Pretoria
Universita?t Stuttgart South Africa
Germany
faaszgd@ims.uni-stuttgart.de elsabe.taljard@up.ac.za
heid@ims.uni-stuttgart.de danie.prinsloo@up.ac.za
Abstract
A major obstacle to part-of-speech
(=POS) tagging of Northern Sotho
(Bantu, S 32) are ambiguous function
words. Many are highly polysemous and
very frequent in texts, and their local
context is not always distinctive.
With certain taggers, this issue leads to
comparatively poor results (between 88
and 92 % accuracy), especially when
sizeable tagsets (over 100 tags) are used.
We use the RF-tagger (Schmid and Laws,
2008), which is particularly designed for
the annotation of fine-grained tagsets (e.g.
including agreement information), and
we restructure the 141 tags of the tagset
proposed by Taljard et al (2008) in a way
to fit the RF tagger. This leads to over
94 % accuracy. Error analysis in addition
shows which types of phenomena cause
trouble in the POS-tagging of Northern
Sotho.
1 Introduction
In this paper, we discuss issues of the part-of-
speech (POS) tagging of Northern Sotho, one of
the eleven official languages of South Africa, spo-
ken in the North-east of the country. Northern
Sotho is a Bantu language belonging to the Sotho
family (Guthrie, 1967: S32). It is written disjunc-
tively (contrary to e.g. Zulu), i.e. certain mor-
phemes appear as character strings separated by
blank spaces. It makes use of 18 noun classes (1,
1a, 2, 2b, 3 to 10, 14, 15, and the locative classes
16, 17, 18, N-, which may be summarized as LOC
for their identical syntactic features). A concor-
dial system helps to verify agreement or resolve
ambiguities.
We address questions of the ambiguity of func-
tion words, in the framework of an attempt to use
?standard? European-style statistical POS taggers
on Northern Sotho texts.
In the remainder of this section, we briefly dis-
cuss our objectives (section 1.1) and situate our
work within the state of the art (section 1.2). Sec-
tion 2 deals with the main issues at stake, the han-
dling of unknown open class words, and the pol-
ysemy of Northern Sotho function words. In sec-
tion 3, we discuss our methodology, summarizing
the tagset and the tagging technique used, and re-
porting results from other taggers. Section 4 is
devoted to details of our own results, the effects
of the size of training material (4.2), the effects
of polysemy and reading frequency (4.3), and it
includes a discussion of proposals for quality im-
provement (Spoustova? et al, 2007). We conclude
in section 5.
1.1 Objectives
The long term perspective of our work is to sup-
port information extraction, lexicography, as well
as grammar development of Northern Sotho with
POS-tagged and possibly parsed corpus data. We
currently use the 5.5 million word University of
Pretoria Sepedi Corpus (PSC, cf. de Schryver
and Prinsloo (2000)), as well as a 45,000 words
training corpus. We aim at high accuracy in the
POS-tagging, and at minimizing the amount of un-
known word forms in arbitrary unseen corpora, by
using guessers for the open word classes.
1.2 Recent work
A few publications, so far, deal with POS-tagging
of Northern Sotho; most prominently, de Schryver
and de Pauw (2007) have presented the MaxTag
method, a tagger based on Maximum Entropy
38
Learning (Berger et al, 1996) as implemented in
the machine learning package Maxent (Le, 2004).
When trained on manually annotated text, it ex-
tracts features such as the first and last letter of
each word, or the first two and last two letters or
the first three and last three letters of each word;
it takes the word and the tag preceding and fol-
lowing the item to be tagged, etc., to decide about
word/tag probabilities. De Schryver and de Pauw
report an accuracy of 93.5 % on unseen data, using
a small training corpus of only ca. 10,000 word
forms.
Other work is only partly engaged in POS-
tagging, e.g. Kotze??s (2008) finite state analysis of
the verb complex of Northern Sotho. This study
does not cover all parts of speech and can thus not
be directly compared with our work. Taljard et al
(2008) and Van Rooy and Pretorius (2003) present
tagsets for Northern Sotho and the closely related
language Setswana, but they focus on the defini-
tion of the tagsets without discussing their auto-
matic application in detail. In (Prinsloo and Heid,
2005), POS-tagging is mentioned as a step in a
corpus processing pipeline for Northern Sotho, but
no experimental results are reported.
2 Challenges in tagging Northern Sotho
POS-tagging of Northern Sotho and of any dis-
junctively written Bantu language has to deal es-
pecially with two major issues which are con-
sequences of their morphology and their syntax.
One is the presence, in any unseen text, of a num-
ber of lexical items which are not covered by the
lexicon of the tagger (?unknown words?), and the
other is an extraordinarily high number of ambigu-
ous function words.
2.1 Unknown words
In Northern Sotho, nouns, verbs and adverbs are
open class items; all other categories are closed
word classes: their items can be listed. The open
classes are characterized in particular by a rich
morphology: nouns can form derivations to ex-
press diminutives and augmentatives, as well as
locative forms, to name just a few. Adding the
suffix -ng to toropo ?town?, for example, forms
toropong, ?in/at/to town?. For verbs, tense, voice,
mood and many other dimensions, as well as nom-
inalization, lead to an even larger number of de-
rived items. Prinsloo (1994) distinguishes 18 clus-
ters of verbal suffixes which give rise to over 260
individual derivation types per verb. Only a few
of these derivations are highly frequent in corpus
text; however, due to productivity, a large number
of verbal derivation types can potentially appear in
any unseen text.
For tagging, noun and verb derivations show up
as unknown items, and an attempt to cover them
within a large lexicon will partly fail due to pro-
ductivity and recursive applicability of certain af-
fixes. The impact of the unknown material on
tagging quality is evident: de Schryver and de
Pauw (2007) report 95.1 % accuracy on known
items, but only 78.9 % on unknowns; this leads
to a total accuracy of 93.5 % on their test corpus.
We have carried out experiments with a version
of the memory-based tagger, MBT (Daelemans et
al., 2007), which arrives at 90.67 % for the known
items of our own test corpus (see section 3.2), as
opposed to only 59.68 % for unknowns.
To counterbalance the effect of unknown items,
we use rule-based and partly heuristic guessers for
noun and verb forms (cf. Prinsloo et al (2008) and
(Heid et al, 2008)) and add their results to the tag-
ger lexicon before applying the statistical tagger:
the possible annotations for all words contained in
the text are thus part of the knowledge available to
the tagger.
Adverbs are also an open word class in North-
ern Sotho; so far, we have no tools for identifying
them. In high quality tagging, the suggestions of
our guessers are examined manually, before they
are added to the tagger lexicon.
2.2 Polysemous function words and
ambiguity
Function words of Northern Sotho are highly am-
biguous, and because of the disjunctive writing
system of the language, a number of bound mor-
phemes are written separately from other words.
A single form can have several functions. For
example, the token -a- is nine-ways ambiguous:
it can be a subject concord of noun class 1 or 6,
an object concord of class 6, a possessive concord
of class 6, a demonstrative of class 6, a hortative
or a question particle or a verbal morpheme in-
dicating present tense or past tense (Appendix A
illustrates the ambiguity of -a- with example sen-
tences). Furthermore, the most polysemous func-
tion words are also the most frequent word types in
corpora. The highly ambiguous item go1 alone ac-
111 different functions of go may be distinguished: object
39
counts for over 5 % of all occurrences in our train-
ing corpus, where 88 types of function words, with
an average frequency of well over 200, make up
for about 40 % of all occurrences.
The different readings of the function words are
not evenly distributed: some are highly frequent,
others are rare. Furthermore, many ambiguous
function words appear in the context of other func-
tion words; thus the local context does not nec-
essarily disambiguate individual function words.
This issue is particularly significant with ambigu-
ities between concords which can have the same
function (e.g. object) in different noun classes. As
mentioned, -a- can be a subject concord of either
noun class 1 or 6: though there are some clearcut
cases, like the appearance of a noun of class 6 (in-
dicating class 6), or an auxiliary or the conjunction
ge in the left context (both rather indicating class
1) there still remain a number of occurrences of -a-
in the corpora only where a broader context, some-
times even information from preceding sentences,
may help to disambiguate this item.
Consequently, comparing tagging performance
across different tagsets does not give very clear re-
sults: if a tagset, like the one used by de Schryver
and de Pauw (2007), does not distinguish noun
classes, obviously a large number of difficult dis-
ambiguation cases does not appear at all (their
tagset distinguishes, for example, subject and ob-
ject concord, but gives no information on noun
class numbers). For the lexicographic applica-
tion we are interested in, and more generally as a
preparatory step to chunking or parsing of North-
ern Sotho texts, an annotation providing informa-
tion on noun classes is however highly desirable.
3 Methodology
3.1 Tagset
There are several proposals for tagsets to be used
for Northern Sotho and related languages. Van
Rooy and Pretorius (2003) propose a detailed
tagset for Setswana, which is fully in line with
the guidelines stated by the EAGLES project, cf.
Leech and Wilson (1999). This tagset encodes a
considerable number of semantic distinctions in
its nominal and verbal tags. In Kotze??s work on
concord of class 15, object concord of the locative classes,
object concord of the 2nd person singular, subject concord
of class 15, indefinite subject concord, subject concord of
the locative classes, class prefix of class 15, locative particle,
copulative indicating either an indefinite subject, or a subject
of class 15 or a locative subject.
the Northern Sotho verb complex, (Kotze?, 2008),
a number of POS tags are utilized to distinguish
the elements of the verb, however, due to Kotze??s
objectives, her classification does not cover other
items. De Schryver and de Pauw (2007) use a
tagset of only 56 different tags, whereas the pro-
posal by Van Rooy and Pretorius leads to over 100
tags. Finally, Taljard et al (2008) propose a rather
detailed tagset: contrary to the other authors men-
tioned, they do encode noun classes in all relevant
tags, which leads to a total of 141 tags. Further-
more, they encode a number of additional mor-
phosyntactic distinctions on a second level of their
tagset, which leads to a total of 262 different clas-
sifications of Northern Sotho morphemes.
Our current tagset is inspired by Taljard et al
(2008). However, we disregard some of their sec-
ond level information for the moment (which in
many cases encodes lexical properties of the items,
e.g. the subdivision of particles: hortative, ques-
tion, instrumental, locative, connective, etc.). We
use the RF-tagger (Schmid and Laws, 2008) (cf.
section 3.3), which is geared towards the annota-
tion of structured tagsets, by separating informa-
tion which partitions the inventory of forms (e.g.
broad word classes) from feature-like information
possibly shared by several classes, such as the
Sotho noun classes. With this method, we are able
to account for Taljard et al?s (2008) 141 tags by
means of only 25 toplevel tags, plus a number of
feature-like labels of lower levels. We summarize
the properties of the tagsets considered in table 1.
3.2 Training corpus
Our training corpus consists of ca. 45.000 manu-
ally annotated word forms, from two text types.
Over 20.000 word forms come from a novel of
the South African author Oliver K. Matsepe (Mat-
sepe, 1974); over 10.000 forms come from a Ph.D.
dissertation by Raphehli M. Thobakgale (Thobak-
gale, 2005), and another 10.000 from a second
Ph.D. dissertation, by Ramalau R. Maila (Maila,
2006). Obviously, this is not a balanced corpus; it
was indeed chosen because of its easy accessibil-
ity. We use this corpus to train our taggers and to
test them; in a ten-fold cross validation, we split
the text into ten slices of roughly equal size, train
on 9 of them and test on the tenth. In this article,
we give figures for the median of these results.
40
Authors No. of tags ? noun class tool?
(van Rooy and Pretorius, 2003) 106 - noun class no
(De Schryver and De Pauw, 2007) 56 - noun class yes
(Kotze?, 2008) partial N.R. yes
(Taljard et al, 2008) 141/262 + noun class no
This paper 25/141 + noun class yes
Table 1: Tagsets for N. Sotho: authors, # of tags, consideration of the noun class system, use in tools
3.3 Tagging techniques: the RF-tagger
We opt for the RF-tagger (Schmid and Laws,
2008), because it is a Hidden-Markov-Model
(HMM) tagger which was developed especially
for POS tagsets with a large number of (fine-
grained) tags. Tests with our training corpus have
shown that this tagger outperforms the Tree-tagger
((Schmid, 1994) and (Schmid, 1995)), as shown in
figure 1. An additional external lexicon may serve
as input, too. The development of the RF-tagger
was based on the widely shared opinion that for
languages like German or Czech, agreement in-
formation (e.g. case, number or person) should
preferably appear as part of all appropriate part
of speech tags. However, as tagsets increase im-
mensely in size when such information is part of
the tags, the data are decomposed, i.e. split into
several levels of processing. The probability of
each level is then calculated separately (the joint
probability of all levels is afterwards calculated as
their product). With such methodology, a tag of
the German determiner das may contain five levels
of information, e.g. ART.Def.Nom.Sg.Neut to de-
fine a definite, nominative singular, neutral deter-
miner (article) that appears in the nominative case.
This approach makes sense for the Bantu-
languages as well, since information on noun class
numbers should be part of the noun tags, too, as in
Taljard et als (2008) tagset. A noun here is not
only tagged ?N?, but Nclass, e.g. mohumagadi
?(married) woman? as N01. All concords, pro-
nouns or other types that concordially agree with
nouns are also labelled with a class number, e.g.
o, the subject concord of class 1, is labelled CS01.
This approach makes sense, especially in the view
of chunking/parsing and reference resolution, be-
cause any of those elements can acquire a pronom-
inal function when the noun that they refer to is
deleted (Louwrens, 1991).
To utilize the RF-tagger, we split all tags con-
taining noun class numbers into several levels (e.g.
the tag N01 becomes N.01). Emphatic and posses-
sive pronouns are represented on three levels (e.g.
PROPOSSPERS becomes PRO.POSS.PERS)2.
4 Results
In a preliminary experiment, we compared several
taggers3 on our manually annotated data. Apart
from the RF-tagger (Schmid and Laws, 2008),
we also used the Tree-Tagger (Schmid, 1994), the
TnT tagger (Brants, 2000) and MBT (Daelemans
et al, 2007).
4.1 Comparing taggers
The results give a relatively homogenous picture,
with the RF-tagger achieving a median of 94.16 %
when utilising a lexicon containing several thou-
sand nouns and verbs. It leads to 91 % accuracy
without this lexicon (to simulate similar condi-
tions as for TnT or MBT where no external lex-
icon was offered). TnT achieves 91.01 %, and
MBT 87.68 %. Data from the Tree-Tagger were
not comparable for they had been obtained at an
earlier stage using the lexicon (92.46 %).
4.2 Effects of the size of the training corpus
on the tagging results
All probabilistic taggers are in need of training
data the size of which depends on the size of the
tagset and on the frequencies of occurrence of
each context. De Schryver and de Pauw (2007)
demonstrated that when utilizing a tagset that con-
tains only about a third of the tags (56) contained
in Taljard et al?s (2008) tagset (141), their Max-
Tag POS-tagger reaches a 93.5 % accuracy with a
training corpus of only about 10,000 tokens.
Figure 1 depicts the effects of the size of the
training corpus on the accuracy figures of the Tree-
tagger and the RF-tagger. Tests with training cor-
pora of the sizes 15,000, 30,000 and 45,000 tokens
2Tests have shown that the quantitative pronouns should
be treated separately, their tags are thus only split into two
levels.
3Check the References section for the availability of these
taggers.
41
Figure 1: Effects of the size of the training corpus
on tagger results.
showed that the results might not improve much if
more data is added. The RF-tagger already reaches
more than 94 % correctness when utilizing the cur-
rent 45,000 token training corpus.
4.3 Effects of the highly polysemous function
words of Northern Sotho
The less frequently a token-label pair appears in
the corpus, the lower is its probability (leading to
the sparse data problem, when probability guesses
become unreliable because of low numbers of oc-
currences). This issue poses a problem for North-
ern Sotho function words: if they occur very fre-
quently with a certain label, the chances of them
being detected with another label are fairly low.
This effect is demonstrated in table 2, which de-
scribes the detection of the parts of speech of the
highly ambiguous function word -a-. The word -
a- as PART(icle) occurs only 45 times while -a-
as CS.01 occurs 1,182 times. More than 50 % of
the particle occurrences (23) are wrongly labelled
CS.01 by the tagger. In table 2, we list the cor-
rect tags of all occurrences of -a-, as well as the
assigned tags to each of them by our tagger. Each
block of table 2 is ordered by decreasing numbers
of occurrence of each tag in the output of the RF-
tagger. For easier reference, the correct tags as-
signed by the RF-tagger are printed in bold. Table
2 also clearly shows the effect of ambiguous local
context on the tagging result: the accuracy of the
CS.06-annotation (subject concord of class 6) is
considerably lower than that of the more frequent
CS.01 (96.45 % vs. 63.08 %), and CS.01 is the
most frequent error in the CS.06 assignment pro-
a as freq RF-tagger sums %
CS.01 1182 CS.01 1140 96.4
CS.06 19 1.6
MORPH 14 1.2
CDEM.06 3 0.3
PART 2 0.2
CPOSS.06 2 0.2
CO.06 2 0.2
CS.06 176 CS.06 111 63.1
CS.01 43 24.4
CPOSS.06 10 5.7
CDEM.06 5 2.8
MORPH 3 1.7
PART 3 1.7
CO.06 1 0.6
CO.06 18 MORPH 7 38.9
CS.01 6 33.3
CO.06 3 16.7
CS.06 2 11.11
PART 45 CS.01 23 51.1
MORPH 11 24.4
CDEM.06 5 11.1
PART 5 11.1
CPOSS.06 1 2.2
CDEM.06 97 CDEM.06 89 91.8
CPOSS.06 4 4.1
CS.06 2 2.1
CS.01 1 1.0
PART 1 1.0
CPOSS.06 209 CPOSS.06 186 89.0
CDEM.06 12 5.7
CS.06 6 2.9
PART 4 1.9
CS.01 1 0.5
MORPH 89 MORPH 44 49.4
CO.06 26 29.2
CS.01 15 16.9
CPOSS.06 4 4.5
sums 1816 1816
Table 2: RF-tagger results for -a-
cess.
4.4 Suggestions for increasing correctness
percentages
Spoustova? et al (2007) describe a significant in-
crease of accuracy in statistical tagging when uti-
lizing rule-based macros as a preprocessing, for
Czech. We have contemplated, in an earlier stage
of our work (Prinsloo and Heid, 2005) to adopt a
42
similar strategy, i.e. to design rule-based macros
for the (partial) disambiguation of high-frequency
function words. However, the fact that the local
context of many function words is similar (i.e. the
ambiguity of this local context (see above)), is a
major obstacle to a disambiguation of single func-
tion words by means of rules. Rules would interact
in many ways, be dependent on the application or-
der, or disambiguate only partially (i.e. leave sev-
eral tag options). An alternative would be to de-
sign rules for the disambiguation of word or mor-
pheme sequences. This would however amount to
partial parsing. The status of such rules within a
tagging architecture would then be unclear.
4.5 Effects of tagset size and structure
While a preprocessing with rule-based disam-
biguation does not seem to be promising, there
are other methods of improving accuracy, such as,
e.g., the adaptation of the tagset. Obviously, types
appearing in different contexts should have differ-
ent labels. For example, in the tagset of Taljard et
al. (2008), auxiliary verbs are a sub-class of verbs
(V aux). In typical Northern Sotho contexts, how-
ever, auxiliaries are surrounded by subject con-
cords, while verbs are only preceded by them.
When ?promoting? the auxiliaries to the first level
by labelling them VAUX, the RF-tagger result in-
creases by 0.13 % to 94.16 % accuracy. We still
see room for further improvement here. For exam-
ple, ga as PART (either locative particle PART loc
or hortative particle PART hort) is identified cor-
rectly in only 29.2 % of all cases at the moment.
The hortative particle usually appears at the be-
ginning of a verbal segment, while the locative in
most cases follows the segment. Results may in-
crease to an even higher accuracy when ?promot-
ing? these second level annotations, hort(ative) and
loc(ative) to the first annotation level.
5 Conclusions and future work
This article gives an overview of work on POS-
tagging for Northern Sotho. Depending on the
place of tagging in an overall NLP chain for this
language, different choices with respect to the
tagset and to the tagging technology may prove
adequate.
In our work, which is part of a detailed lin-
guistic annotation of Northern Sotho corpora for
linguistic exploration with a view to lexicons and
grammars, it is vital to provide a solid basis for
chunking and/or parsing, by including informa-
tion on noun class numbers in the annotation.
We found that the RF-tagger (Schmid and Laws,
2008) performs well on this task, partly because
it allows us to structure the tagset into layers, and
to deal with noun class information in the same
way as with agreement features for European lan-
guages. We reach over 94 % correctness, which
indicates that at least a first attempt at covering the
PSC corpus may now be in order.
Our error analysis, however, also highlights a
few more general aspects of the POS annotation
of Northern Sotho and related languages: obvi-
ously, frequent items and items in distinctive lo-
cal contexts are tagged quite well. When noun
class information is part of the distinctions under-
lying the tagset, function words usable for more
than one noun class tend however, to appear in
non-distinctive local contexts and thus to lead to
a considerable error rate. Furthermore, we found a
few cases of uses of, e.g., subject concords that are
anaphoric, with antecedents far away and thus not
accessible to tagging procedures based on the lo-
cal context. These facts raise the question whether,
to achieve the highest quality of lexical classifica-
tion of the words and morphemes of a text, chunk-
ing/parsing might be required altogether, rather
than tagging.
Our experiments also showed that several pa-
rameters are involved in fine-tuning a Sotho tag-
ger. The size and structure of the tagset is one
such a prominent parameter. Tendencies towards
simpler and smaller tagsets obviously conflict with
the needs of advanced processing of the texts and
of linguistically demanding applications. It seems
that tagset design and tool development go hand in
hand.
We intend to apply the current version of the
RF-tagger to the PSC corpus and to evaluate the
results carefully. We expect a substantial gain
from the use of the guessers for nouns and verbs,
cf. (Prinsloo et. al, 2008) and (Heid et al, 2008).
Detailed error analysis should allow us to also
design specific rules to correct the output of the
tagger. Instead of preprocessing (as proposed by
Spoustova? et al (2007)), a partial postprocess-
ing may contribute to further improving the overall
quality. Rules would then probably have to be ap-
plied to particular sequences of words and/or mor-
phemes which cause difficulties in the statistical
process.
43
References
Adam L. Berger, Stephen Della Peitra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics 22(1): pp. 39 ? 71.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference ANLP-
2000, Seattle, WA.
Walter Daelemans, Jakub Zavrel, Antal van den Bosch.
2007. MBT: Memory-Base Tagger, version 3.1. Ref-
erence Guide. ILK Technical Report Series 07?08
[online]. Available : http://ilk.uvt.nl/
mbt (10th Jan, 2009).
Gilles-Maurice de Schryver and Guy de Pauw. 2007.
Dictionary Writing System (DWS) + Corpus Query
Package (CQP): The Case of TshwaneLex. Lexikos
17 AFRILEX-reeks/series 17:2007: pp. 226 ?
246. [Online tagger:] http://aflat.org/?q=
node/177. (10th Feb, 2009)
Gilles-Maurice de Schryver and Daan J. Prinsloo.
2000. The compilation of electronic corpora with
special reference to the African languages. South-
ern African Linguistics and Applied Language Stud-
ies 18(1-4): pp. 89 ? 106.
Malcolm Guthrie. 1971. Comparative Bantu: an in-
troduction to the comparative linguistics and prehis-
tory of the Bantu languages, vol 2, Farnborough:
Gregg International.
Ulrich Heid, Daan J. Prinsloo, Gertrud Faa?, and
Elsabe? Taljard. 2008 Designing a noun guesser for
part of speech tagging in Northern Sotho (33 pp).
ms: University of Pretoria.
Petronella M. Kotze?. 2008. Northern Sotho grammat-
ical descriptions: the design of a tokeniser for the
verbal segment. Southern African Linguistics and
Applied Language Studies 26(2): pp. 197 ? 208.
Zhang Le. 2004. Maximum Entropy Modeling Toolkit
for Python and C++ (Technical Report) [online].
Available: http://homepages.inf.ed.
ac.uk/s0450736/maxent_toolkit.html
(10th Jan, 2009).
Geoffrey Leech, Andrew Wilson. 1999. Standards for
Tagsets. in van Halteren (Ed.) Syntactic world-class
tagging: pp. 55 ? 80 Dordrecht/Boston/London:
Kluwer Academic Publishers.
Louis J. Louwrens. 1991. Aspects of the Northern
Sotho Grammar p. 154. Pretoria:Via Afrika.
Ramalau A. Maila. 2006. Kgolo ya tiragatso ya Se-
pedi. [=?Development of the Sepedi Drama?]. Doc-
toral thesis. University of Pretoria, South Africa.
Oliver K. Matsepe. 1974. Ts?a Ka Mafuri. [=?From the
homestead?]. Pretoria: Van Schaik.
Daan J. Prinsloo. 1994. Lemmatization of verbs in
Northern Sotho. SA Journal of African Languages
14(2): pp. 93 ? 102.
Daan J. Prinsloo and Ulrich Heid. 2005. Creating
word class tagged corpora for Northern Sotho by lin-
guistically informed bootstrapping. in: Isabella Ties
(Ed.): LULCL, Lesser used languages and compu-
tational linguistics, 27/28-10-2005, Bozen/Bolzano,
(Bozen: Eurac) 2006: pp. 97 ? 113.
Daan J. Prinsloo, Gertrud Faa?, Elsabe? Taljard, and
Ulrich Heid. 2008. Designing a verb guesser for
part of speech tagging in Northern Sotho. Southern
African Linguistics and Applied Languages Studies
(SALALS) 26(2).
Helmut Schmid and Florian Laws. 2008. Es-
timation of Conditional Probabilities with
Decision Trees and an Application to Fine-
Grained POS Tagging [online]. COLING
2008. Manchester, Great Britain. Available:
http://www.ims.uni-stuttgart.de/
projekte/corplex/RFTagger/ (10th Jan,
2009).
Helmut Schmid. September 1994. Probabilistic
Part-of-Speech Tagging Using Decision Trees. Pro-
ceedings of the International Conference on New
Methods in Language Processing[online]. Avail-
able: http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
(10th Jan, 2009).
Helmut Schmid. March 1995. Improvements in Part-
of-Speech Tagging with an Application to German.
Proceedings of the ACL SIGDAT-Workshop.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan
Votrubec, Pavel Krbec, and Pavel Kve?ton?. Jun
29, 2007. The best of two worlds: Coop-
eration of Statistical and Rule-based Taggers
for Czech. Balto-Slavonic Natural Language
Processing: pp. 67 ? 74 [online]. Available:
http://langtech.jrc.it/BSNLP2007/
m/BSNLP-2007-proceedings.pdf (10th
Jan, 2009).
Elsabe? Taljard, Gertrud Faa?, Ulrich Heid and Daan J.
Prinsloo. 2008. On the development of a tagset for
Northern Sotho with special reference to standardi-
sation. Literator 29(1) 2008. Potchefstroom, South
Africa.
Raphehli M. Thobakgale. Khuets?o ya OK Matsepe go
bangwadi ba Sepedi [=?Influence of OK Matsepe
on the writers of Sepedi?]. Doctoral thesis. Univer-
sity of Pretoria, South Africa.
Bertus van Rooy and Rigardt Pretorius. 2003. A word-
class tagset for Setswana. Southern African Linguis-
tics and Applied Language Studies 21(4): pp. 203 ?
222.
44
Appendix A. The polysemy of -a-
Description Example
1 Subject ge monna a fihla
concord of conjunctive + noun cl. 1 + subject concord cl. 1 + verb stem
if/when + man + subj-cl1 + arrive
?when the man arrives?
2 Subject masogana a thus?a basadi
concord of noun cl. 6 + subject concord cl. 6 + verb stem + noun cl.2
nominal cl. 6 young men + subj-cl6 + help women
?the young men help the women?
3 Possessive maoto a gagwe
concord of noun cl. 6 + possessive concord cl. 6 + possessive pronoun cl. 1
nominal cl. 6 feet + of + his
?his feet?
4 Present tense morutis?i o a bits?a
morpheme noun cl. 1 + subject concord cl.1 + present tense marker + verb stem
teacher + subj-cl1 + pres + call
?the teacher is calling?
5 Past tense morutis?i ga o a bits?a masogana
morpheme noun cl. 1 + negation morpheme + subject concord cl.1 +
past tense marker + verb stem + noun cl. 6
teacher + neg + subj-cl1 + past + call + young men
?the teacher did not call the young men?
6 Demonstrative ba nyaka masogana a
concord of subject concord cl. 2 + verb stem + noun cl. 6 + demonstrative concord
nominal cl. 6 they + look for + young men + these
?they are looking for these young men?
7 Hortative a ba tsene
particle hortative particle + subject concord cl. 2 + verb stem
let + subj-cl2 + come in
?let them come in?
8 Interrogative a o tseba Sepedi
particle interrogative particle + subject concord 2nd pers sg. + verb stem + noun cl. 7
ques + subj-2nd-pers-sg + know + Sepedi
?do you know Sepedi?
9 Object moruti o a bidits?e
concord of noun cl. 1 + subject concord cl. 1 + object concord cl. 6 + verb stem
teacher + subj-cl1 + obj-cl6 + called
?the teacher called them?
45
Coling 2010: Poster Volume, pages 614?622,
Beijing, August 2010
A Linguistically Grounded Graph Model for Bilingual Lexicon
Extraction
Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,
Ulrich Heid, Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de
Abstract
We present a new method, based on
graph theory, for bilingual lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent linguis-
tic relations between words such as ad-
jectival modification. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.
1 Introduction
Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .
In this paper, we present a new graph-theoretic
method for bilingual lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations
(such as adjectival modification) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one final similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to find a
translation.
Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.
The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.
2 Related Work
Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.
Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine
614
a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.
Garera et al (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.
Haghighi et al (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to find matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.
Most previous work has used vector spaces and
(except for Garera et al (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modification and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.
3 Translation Model
Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.
We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modified version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).
SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same
house
magazine
book
thought
buy
read
Haus
Zeitschrift
Buch
Gedanke
kaufen
lesen
Figure 1: Similarity through seed translations
objects in the two languages; one of these (maga-
zine ? Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy ?
kaufen. Furthermore, book and Buch are similar
(because of read ? lesen) and this similarity will
be added to buy ? kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.
To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modification. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a fixed set of nodes; a node
is not necessarily part of every relation.
The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.
Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will benefit from three different sources.
Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modified by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and
615
relation entities description example
used in this paper
amod a, n adjectival modification a fast car
dobj v, n object subcategorization drive a car
ncrd n, n noun coordination cars and busses
other possible relations
vsub v, n subject subcategorization a man sleeps
poss n, n possessive the child?s toy
acrd a, a adjective coordination red or blue car
Table 1: Relations used in this paper (top) and
possible extensions (bottom).
dobj
amod
ncrd
verb
adjective
noun
like promote
idea
article book
magazine
interesting political
Figure 2: Graph snippet with typed edges
promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modification and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to fil-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a filter on
pair frequency (? 3). We process noun pairs by
applying a frequency threshold on words (? 100)
and pairs (? 3). Verb-object pairs (the smallest
data set) were not frequency-filtered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (significance at ? = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76). As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.
We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to ?semantically weak? high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.
nodes n a v
de 34,545 10,067 2,828
en 22,257 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,906
en 288,889 686,073 510,351
Table 2: Node and edge statistics
4 SimRank
Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al, 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now briefly review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al, 2009).
The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed
616
as the normalized sum of the pairwise similarities
of their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl.
where N(i) and N(j) are the sets of i?s and j?s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.
For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predefined set of node-node
equivalences (seed translation pairs).
Sij =
c
|NA(i)| |NB(j)|
?
k?NA(i),l?NB(j)
Skl.
Multi-edge Extraction (MEE) Algorithm To
combine different information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :
Sij =
c
|T |
?
t?T
1
f(|NA,t(i)|)f(|NB,t(j)|)
?
k?NA,t(i),
l?NB,t(j)
Skl.
f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).
While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs. (See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.
Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
influence to words with few neighbors. We there-
fore modified the normalization function g(n) =
n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N(i)|, otherwise the similarity computation
does not converge. We use the function h(n) =?n?
?
maxk(|N(k)|). h grows quickly for small
node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less influ-
ence on final similarity scores. In all experiments
reported in this paper, the matrices A?, B? are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.
Threshold Sieving For larger experiments,
there is a limit to scalability, as the similarity ma-
trix fills up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the final result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter ? that specifies maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.
We verified that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.
5 Benchmark Data Set
Rapp?s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set
617
n a v
training set .61 .31 .08
TS100 .65 .28 .07
TS1000 .66 .14 .20
Table 3: Percentages of POS in test and training
(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.
Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then find the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.
We went to great lengths to implement Rapp?s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.
Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp?s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)?(iv) can be used in-
dependently of (i) ? but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph
6 Results
In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we verified using dict.cc and leo.org. We
report the results separately for the original test set
(?strict?) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.
The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.
MRR is Mean Reciprocal Rank of correct trans-
lations: 1n
?n
i
1
ranki (Voorhees and Tice, 1999).
MRR is a more fine-grained measure than acc@n,
618
TS100, strict TS100, extended TS1000
acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRR
baseline .50 .67 .56 .54 .70 .60 .33 .56 .41
single .44 .67 .52 .49 .68 .56 .40? .70? .50
MEE .52 .79? .62 .58 .82? .68 .48? .76? .58
Table 4: Results compared to baseline?
e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.
The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline ? only slightly
on TS100, but significantly on TS1000. Using the
various relations separately (single) already yields
a significant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for finding the correct translation.
MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
confirm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.
On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically significant.
6.1 Relation Comparison
Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.
Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-
?We indicate statistical significance at the ? = 0.05 (?)
and 0.01 level (?) when compared to the baseline. We did
not calculate significance for MRR.
forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.
Similarly, many semantically non-restrictive
adjectives such as first and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.
MEE performance for nouns (.45) is signifi-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun
noun adj verb all
TS100 baseline .55 .43 .29 .50
amod .15 .71 - .30
ncrd .34 - - .22
dobj .02 - .43 .04
MEE .45 .71 .43 .52
TS1000 baseline .42 .26 .18 .33
MEE .53 .55 .27 .48
Table 5: Relation comparison, acc@1
619
source acc@1 acc@10
dobj .02 .10
amod .15 .37
amod+dobj .22 .43
ncrd+dobj .32 .65
ncrd .34 .60
ncrd+amod .49 .74
MEE .45 .77
Table 6: Accuracy of sources for nouns
node type, the ?pivot? node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.
We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.
6.2 Comparison MEE vs. All-in-one
An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.
Since AIO requires more memory, we had to
filter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun
MEEsmall AIOsmall
acc@1 .51 .52
acc@10 .72 .75
MRR .62 .59
Table 7: MEE vs. AIO
pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with ? = 10?10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
significant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.
7 Analysis and Discussion
Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.
Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic field). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
field of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al, 2010).
Similarly, the method sometimes puts the
antonym of the correct translation in first place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difficult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.
Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.
620
bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed?s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the financial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the financial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difficult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users? confidence
in the machine translation system. For example,
at some point Google translated ?sarkozy sarkozy
sarkozy? into ?Blair defends Bush?. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.
As we discussed earlier, parallel text is not
available in sufficient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufficient for
bilingual lexicon extraction.
In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.
In this paper, we have attempted to reimplement
Rapp?s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most confident predictions of
nouns. But since the evaluation data sets are not
publicly available it is difficult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.
For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.
8 Summary
We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.
9 Acknowledgement
This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.
621
References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut fu?r maschinelle Sprachverarbeitung (IMS),
Universita?t Stuttgart.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In COLING-ACL, pages 414?
420.
Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ?09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129?137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In KDD
?02, pages 538?543.
Koehn, Philipp and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422?433.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, may.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Schmid, Helmut. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ?04, page 162.
Voorhees, Ellen M. and Dawn M. Tice. 1999. The
TREC-8 question answering track evaluation. In
Proceedings of the 8th Text Retrieval Conference.
622
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards a Tool for Interactive Concept Building for Large Scale Analysis
in the Humanities
Andre Blessing1 Jonathan Sonntag2 Fritz Kliche3
Ulrich Heid3 Jonas Kuhn1 Manfred Stede2
1Institute for Natural Language Processing
Universitaet Stuttgart, Germany
2Institute for Applied Computational Linguistics
University of Potsdam, Germany
3Institute for Information Science and Natural Language Processing
University of Hildesheim, Germany
Abstract
We develop a pipeline consisting of var-
ious text processing tools which is de-
signed to assist political scientists in find-
ing specific, complex concepts within
large amounts of text. Our main focus is
the interaction between the political scien-
tists and the natural language processing
groups to ensure a beneficial assistance for
the political scientists and new application
challenges for NLP. It is of particular im-
portance to find a ?common language? be-
tween the different disciplines. Therefore,
we use an interactive web-interface which
is easily usable by non-experts. It inter-
faces an active learning algorithm which
is complemented by the NLP pipeline to
provide a rich feature selection. Political
scientists are thus enabled to use their own
intuitions to find custom concepts.
1 Introduction
In this paper, we give examples of how NLP meth-
ods and tools can be used to provide support for
complex tasks in political sciences. Many con-
cepts of political science are complex and faceted;
they tend to come in different linguistic realiza-
tions, often in complex ones; many concepts are
not directly identifiable by means of (a small set
of) individual lexical items, but require some in-
terpretation.
Many researchers in political sciences either
work qualitatively on small amounts of data which
they interpret instance-wise, or, if they are in-
terested in quantitative trends, they use compara-
tively simple tools, such as keyword-based search
in corpora or text classification on the basis of
terms only; this latter approach may lead to im-
precise results due to a rather unspecific search as
well as semantically invalid or ambigious search
words. On the other hand, large amounts of e.g.
news texts are available, also over longer periods
of time, such that e.g. tendencies over time can
be derived. The corpora we are currently working
on contain ca. 700,000 articles from British, Irish,
German and Austrian newspapers, as well as (yet
unexplored) material in French.
Figure 1 depicts a simple example of a quantita-
tive analysis.1 The example shows how often two
terms, Friedensmission(?peace operation?), and
Auslandseinsatz(?foreign intervention?) are used
in the last two decades in newspaper texts about
interventions and wars. The long-term goal of the
project is to provide similar analysis for complex
concepts. An example of a complex concept is
the evocation of collective identities in political
contexts, as indirect in the news. Examples for
such collective identities are: the Europeans, the
French, the Catholics.
The objective of the work we are going to dis-
cuss in this paper is to provide NLP methods and
tools for assisting political scientists in the ex-
ploration of large data sets, with a view to both,
a detailed qualitative analysis of text instances,
and a quantitative overview of trends over time,
at the level of corpora. The examples discussed
here have to do with (possibly multiple) collective
identities. Typical context of such identities tend
to report communication, as direct or as indirect
speech. Examples of such contexts are given in 1.
(1) Die
The
Europa?er
Europeans
wu?rden
would
die
the
Lu?cke
gap
fu?llen,
fill,
1The figure shows a screenshot of our web-based
prototype.
55
Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency
of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was
predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention)
being now frequently used.
sagte
said
Ru?he.
Ru?he.
,,The Europeans would fill the gap, Ru?he said.?
The tool support is meant to be semi-automatic,
as the automatic tools propose candidates that
need to be validated or refused by the political sci-
entists.
We combine a chain of corpus processing tools
with classifier-based tools, e.g. for topic clas-
sifiers, commentary/report classifiers, etc., make
the tools interoperable to ensure flexible data ex-
change and multiple usage scenarios, and we em-
bed the tool collection under a web (service) -
based user interface.
The remainder of this paper is structured as fol-
lows. In section 2, we present an outline of the ar-
chitecture of our tool collection, and we motivate
the architecture. Section 3 presents examples of
implemented modules, both from corpus process-
ing and search and retrieval of instances of com-
plex concepts. We also show how our tools are re-
lated to the infrastructural standards in use in the
CLARIN community. In section 4, we exemplify
the intended use of the methods with case studies
about steps necessary for identifying evocation:
being able to separate reports from comments, and
strategies for identifying indirect speech. Section
6 is devoted to a conclusion and to the discussion
of future work.
2 Project Goals
A collaboration between political scientists and
computational linguists necessarily involves find-
ing a common language in order to agree on
the precise objectives of a project. For exam-
ple, social scientists use the term codebook for
manual annotations of text, similar to annotation
schemes or guidelines in NLP. Both disciplines
share methodologies of interactive text analysis
which combine term based search, manual an-
notation and learning-based annotation of large
amounts of data. In this section, we give a brief
56
summary of the goals from the perspective of each
of the two disciplines, and then describe the text
corpus that is used in the project. Section 3 will
describe our approach to devising a system archi-
tecture that serves to realize the goals.
2.1 Social Science Research Issue
Given the complexity of the underlying research
issues (cf. Section 1) and the methodological tra-
dition of manual text coding by very well-trained
annotators in the social science and particular in
political science, our project does not aim at any
fully-automatic solution for empirical issues in po-
litical science. Instead, the goal is to provide as
much assistance to the human text analyst as possi-
ble, by means of a workbench that integrates many
tasks that otherwise would have to be carried out
with different software tools (e.g., corpus prepro-
cessing, KWIC searches, statistics). In our project,
the human analyst is concerned specifically with
manifestations of collective identities in newspa-
per texts on issues of war and military interven-
tions: who are the actors in political crisis man-
agement or conflict? How is this perspective of
responsible actors characterized in different news-
papers (with different political orientation; in dif-
ferent countries)? The analyst wants to find doc-
uments that contain facets of such constellations,
which requires search techniques involving con-
cepts on different levels of abstraction, ranging
from specific words or named entities (which may
appear with different names in different texts) to
event types (which may be realized with different
verb-argument configurations). Thus the text cor-
pus should be enriched with information relevant
to such queries, and the workbench shall provide
a comfortable interface for building such queries.
Moreover, various types and (possibly concurrent)
layers of human annotations have to complement
the automatic analysis, and the manual annota-
tion would benefit from automatic control of code-
book2 compliance and the convergence of coding
decisions.
2.2 Natural Language Processing Research
Issue
Large collections of text provide an excellent op-
portunity for computational linguists to scale their
methods. In the scenario of a project like ours, this
becomes especially challenging, because standard
2or, in NLP terms: annotation scheme.
automatic analysis components have to be com-
bined with manual annotation or interactive inter-
vention of the human analyst.
In addition to this principled challenge, there
may be more mundane issues resulting from pro-
cessing corpora whose origin stretches over many
years. In our case, the data collection phase coin-
cided with a spelling reform in German-speaking
countries. Many aspects of spelling changed twice
(in 1996 and in 2006), and thus it is the responsi-
bility of the NLP branch of the project to provide
an abstraction over such changes and to enable to-
day?s users to run a homogeneous search over the
texts using only the current spelling. While this
might be less important for generic web search ap-
plications, it is of great importance for our project,
where the overall objective is a combination of
quantitative and qualitative text analysis.
In our processing chain, we first need to harmo-
nize the data formats so that the processing tools
operate on a common format. Rather than defin-
ing these from scratch, we aim at compatibility
with the standardization efforts of CLARIN3 and
DARIAH4, two large language technology infras-
tructure projects in Europe that in particular target
eHumanities applications. One of the objectives
is to provide advanced tools to discover, explore,
exploit, annotate, analyse or combine textual re-
sources. In the next section we give more details
about how we interact which the CLARIN-D in-
frastructure (Boehlke et al, 2013).
3 Architecture
The main goal is to provide a web-based user-
interface to the social scientist to avoid any soft-
ware installation. Figure 2 presents the workflow
of the different processing steps in this project.
The first part considers format issues that occur
if documents from different sources are used. The
main challenge is to recognize metadata correctly.
Date and source name are two types of metadata
which are required for analyses in the social sci-
ences. But also the separation of document con-
tent (text) and metadata is important to ensure that
only real content is processed with the NLP meth-
ods. The results are stored in a repository which
uses a relational database as a back-end. All fur-
ther modules are used to add more annotations to
the textual data. First a complex linguistic pro-
3http://www.clarin.eu/
4http://www.dariah.eu/
57
cessing chain is used to provide state-of-the-art
corpus linguistic annotations (see Section 3.2 for
details). Then, to ensure that statistics over oc-
currence counts of words, word combinations and
constructions are valid and not blurred by the mul-
tiple presence of texts or text passages in the cor-
pus, we filter duplicates. Duplicates can occur
if our document set contains the same document
twice or if two documents are very similar, e.g.
they differ in only one sentence.
Raw documents
Repository:MetadataStructural dataTextual data Topic filter
Duplicate filter
Linguistic analysisSentence splitter Tokenizer
Web-basedUserinterface
Tagger ParserCoref NER
ImportExploration Workbench
Concept detection
Complex Concept Builder
Figure 2: Overview of the complete processing
chain.
We split the workflow for the user into two
parts: The first part is only used if the user im-
ports new data into the repository. For that he
can use the exploration workbench (Section 3.1).
Secondly, all steps for analyzing the data are done
with the Complex Concept Builder (Section 3.2).
3.1 Exploration Workbench
Formal corpus inhomogeneity (e.g. various data
formats and inconsistent data structures) are a ma-
jor issue for researchers working on text corpora.
The web-based ?Exploration Workbench? allows
for the creation of a consistent corpus from vari-
ous types of data and prepares data for further pro-
cessing with computational linguistic tools. The
workbench can interact with to existing computa-
tional linguistic infrastructure (e.g. CLARIN) and
provides input for the repository also used by the
Complex Concept Builder.
The workbench converts several input formats
(TXT, RTF, HTML) to a consistent XML repre-
sentation. The conversion tools account for differ-
ent file encodings and convert input files to Uni-
code (UTF-8). We currently work on newspa-
per articles wrapped with metadata. Text mining
components read out those metadata and identify
text content in the documents. Metadata appear
at varying positions and in diverse notations, e.g.
for dates, indications of authors or newspaper sec-
tions. The components account for these varia-
tions and convert them to a consistent machine
readable format. The extracted metadata are ap-
pended to the XML representation. The result-
ing XML is the starting point for further compu-
tational linguistic processing of the source docu-
ments.
The workbench contains a tool to identify text
duplicates and semi-duplicates via similarity mea-
sures of pairs of articles (Kantner et al, 2011).
The method is based on a comparison of 5-grams,
weighted by significance (tf-idf measure (Salton
and Buckley, 1988)). For a pair of documents it
yields a value on a ?similarity scale? ranging from
0 to 1. Values at medium range (0.4 to 0.8) are
considered semi-duplicates.
Data cleaning is important for the data-driven
studies. Not only duplicate articles have a nega-
tive impact, also articles which are not of interest
for the given topic have to be filtered out. There
are different approaches to classify articles into a
range of predefined topics. In the last years LDA
(Blei et al, 2003; Niekler and Ja?hnichen, 2012)
is one of the most successful methods to find top-
ics in articles. But for social scientists the cate-
gories typically used in LDA are not sufficient. We
follow the idea of Dualist (Settles, 2011; Settles
and Zhu, 2012) which is an interactive method for
classification. The architecture of Dualist is based
on MALLET (McCallum, 2002) which is easily
integrable into our architecture. Our goal is to
design the correct feature to find relevant articles
for a given topic. Word features are not sufficient
since we have to model more complex features (cf.
Section 2.1).
The workbench is not exclusively geared to the
data of the current project. We chose a modular
set-up of the tools of the workbench and provide
user-modifiable templates for the extraction of var-
ious kinds of metadata, in order to keep the work-
bench adaptable to new data and to develop tools
suitable for data beyond the scope of the current
corpus.
58
3.2 Complex Concept Builder
A central problem for political scientists who in-
tend to work on large corpora is the linguistic va-
riety in the expression of technical terms and com-
plex concepts. An editorial or a politician cited
in a news item can mobilize a collective identity
which can be construed from e.g. regional or so-
cial affiliation, nationality or religion. A reason-
able goal in the context of the search for collec-
tive identity evocation contexts is therefore to find
all texts which (possibly) contain collective iden-
tities. Moreover, while we are training our inter-
active tools on a corpus on wars and military in-
terventions the same collective identities might be
expressed in different ways in a corpus i.e. on the
Eurocrisis.
From a computational point of view, many dif-
ferent tools need to be joined to detect interest-
ing texts. An example application could be a case
where a political scientist intends to extract news-
paper articles that cite a politician who tries to
rally support for his political party. In order to
detect such text, we need a system to identify di-
rect and indirect speech and a sentiment system to
determine the orientation of the statement. These
systems in turn need various kinds of preprocess-
ing starting from tokenization over syntactic pars-
ing up to coreference resolution. The Complex
Concept Builder is the collection of all these sys-
tems with the goal to assist the political scientists.
So far, the Complex Concept Builder imple-
ments tokenization (Schmid, 2009), lemmatisation
(Schmid, 1995), part-of-speech tagging (Schmid
and Laws, 2008), named entity detection (Faruqui
and Pado?, 2010), syntactical parsing (Bohnet,
2010), coreference analysis for German (Lappin
and Leass, 1994; Stuckardt, 2001), relation extrac-
tion (Blessing et al, 2012) and sentiment analysis
for English (Taboada et al, 2011).
It is important for a researcher of the humanities
to be able to adapt existing classification systems
according to his own needs. A common procedure
in both, NLP and political sciences, is to annotate
data. Therefore, one major goal of the project and
the Complex Concept Builder is to provide ma-
chine learning systems with a wide range of pos-
sible features ? including high level information
like sentiment, text type, relations to other texts,
etc. ? that can be used by non-experts for semi-
automatic annotation and text selection. Active
learning is used to provide immediate results that
can then be improved continuously. This aspect
of the Complex Concept Builder is especially im-
portant because new or adapted concepts that may
be looked for can be found without further help of
natural language processing experts.
3.3 Implementation
We decided to use a web-based platform for our
system since the social scientist needs no software
installation and we are independent of the used
operating system. Only a state-of-the-art web-
browser is needed. On the server side, we use a
tomcat installation that interacts with our UIMA
pipeline (Ferrucci and Lally, 2004). A HTML-
rendering component designed in the project (and
parametrizable) allows for a flexible presentation
of the data. A major issue of our work is interac-
tion. To solve this, we use JQuery and AJAX to
dynamically interact between client- and server-
side.
4 Case Study
In this section we explore the interaction between
various sub-systems and how they collaborate to
find complex political concepts. The following
Section 4.1 describes the detection of direct and
indirect speech and its evaluation follows in Sec-
tion 4.2. Section 4.3 is a general exploration of a
few selected sub-systems which require, or benefit
from direct and indirect speech. Finally, Section
4.4 discusses a specific usage scenario for indirect
speech.
4.1 Identifying Indirect Speech
The Complex Concept Builder provides analy-
ses on different linguistic levels (currently mor-
phosyntax, dependency syntax, named entities) of
annotation. We exploit this knowledge to identify
indirect speech along with a mentioned speaker.
Our indirect speech recognizer is based on three
conditions: i) Consider all sentences that contain
at least one word which is tagged as subjunctive
(i.e. ?*.SUBJ?) by the RFTagger. ii) This verb
has to be a direct successor of another verb in the
dependency tree. iii) This verb needs to have a
subject.
Figure 3 depicts the dependency parse tree of
sentence 2.
(2) Der Einsatz werde wegen der Risiken fu?r die
unbewaffneten Beobachter ausgesetzt, teilte
59
Einsatzmission
theDer
,,
ausgesetztstopped
werde
wegenbecause of
Risikorisks
teilteinformed
will be Missionschefhead of mission
MoodMood
RobertRobert
mitam
SaturdaySamstag
on
..
SBOC
VFIN.Aux.3.Sg.Pres.Subj
VFIN.Full.3.Sg.Past.IndRFTags
Figure 3: Dependency parse of a sentence that
contains indirect speech (see Sentence 2).
Missionschef Robert Mood am Samstag mit.
The mission will be stopped because of the risks to the
unarmed observers, informed Head of Mission Robert
Mood on Saturday.
The speaker of the indirect speech in Sentence
2 is correctly identified as Missionschef (Head of
Mission) and the corresponding verb is teilte mit
(from mitteilen) (to inform).
The parsing-based analysis helps to identify the
speaker of the citation which is a necessary in-
formation for the later interpretation of the cita-
tion. As a further advantage, such an approach
helps to minimize the need of lexical knowledge
for the identification of indirect speech. Our er-
ror analysis below will show that in some cases
a lexicon can help to avoid false positives. A lexi-
con of verbs of communication can easily be boot-
strapped by using our approach to identify candi-
dates for the list of verbs which then restrict the
classifier in order to achieve a higher precision.
4.2 Indirect Speech Evaluation
For a first impression, we present a list of sen-
tences which were automatically annotated as pos-
itive instances by our indirect speech detector.
The sentences were rated by political scientists.
Additionally, for each sentence we extracted the
speaker and the used verb of speech. We man-
ually evaluated 200 extracted triples (sentence,
speaker, verb of speech): The precision of our
system is: 92.5%
Examples 2, 3 and 4 present good candidates
which are helpful for further investigations on col-
lective identities. In example 3 Cardinal Lehmann
is a representative speaker of the Catholic commu-
nity which is a collective identity. Our extracted
sentences accelerate the search for such candidates
which amounts to looking manually for needles in
a haystack.
example speaker verb of speech
(2) Robert Mood teilte (told)
(3) Kardinal Karl Lehmann sagte (said)
(4) Sergej Ordzhonikidse sagte (said)
(5) Bild (picture) tru?ben (tarnish)
(6) sein (be) sein (be)
Examples 5 and 6 show problems of our first
approach. In this case, the speaker is not a person
or an organisation, and the verb is not a verb of
speech.
(3) Ein Angriffskrieg jeder Art sei ? sit-
tlich verwerflich ?, sagte der Vorsitzende
der Bischoffskonferenz, Kardinal Karl
Lehmann.
Any kind of war of aggression is ?morally reprehen-
sible,? said the chairman of the Bishops? Conference,
Cardinal Karl Lehmann.
(4) Derartige Erkla?rungen eines Staatschefs
seien im Rahmen der internationalen
Beziehungen inakzeptabel, sagte der UN-
Generaldirektor Sergej Ordzhonikidse
gestern in Genf.
Such statements of heads of states are unacceptable in
the context of international relations, said UN General
Director Sergei Ordzhonikidse in Geneva yesterday.
(5) Wu?rden die Wahlen verschoben, tru?bte sich
das gescho?nte Bild.
Would the elections be postponed, the embellished im-
age would tarnish.
(6) Dies sei alles andere als einfach, ist aus Of-
fizierskreisen zu ho?ren.
This is anything but simple, is to hear from military
circles.
60
EinsatzEimosheDrhsatzs,uDigao
psgasatzEdidsowshgbcdsatzhsu hdo
pgddsgDsatzEcihsoEbchsgwsatzfhgdso
ciwsatzciRsofshksatzfgDDo
wsd asatzspuciEglsoshlrcDsatzdsDDo
MSMMy.SMMy BMSMMyB.SMMyOMSMMyO.SMMyCMSMMyC.SMMyVMSMMy
p EdtFEsktEussbctRshwE
Figure 4: 10 most used verbs (lemma) in indirect
speech.
4.3 Using Indirect Speech
Other modules benefit from the identification of
indirect speech, as can be seen from Sentence 7.
The sentiment system assigns a negative polarity
of ?2.15 to the sentence. The nested sentiment
sources, as described by (Wiebe et al, 2005), of
this sentence require a) a direct speech with the
speaker ?Mazower? and b) an indirect speech with
the speaker ?no one? to be found.5
(7) ?There were serious arguments about what
should happen to the Slavs and Poles in east-
ern Europe,? says Mazower, ?and how many
of them should be sent to the camps and what
proportion could be Germanised . . . No one
ever came out and directly said Hitler had got
it wrong, but there was plenty of implied crit-
icism through comparisons with the Roman
empire. [...]?6
A collective identity evoked in Sentence 7 is
?the Germans?? although the term is not explic-
itly mentioned. This collective identity is de-
scribed as non-homogeneous in the citation and
can be further explored manually by the political
scientists.
The following are further applications of the
identified indirect speeches a) using the frequency
of speeches per text as a feature for classifica-
tion; e.g. a classification system for news re-
ports/commentaries as described in Section 4.4 b)
a project-goal is to find texts in which collective
5The reported sentiment value for the whole sentence is
applicable only to the direct speech. The indirect speech (i.e.
?Hitler had got it wrong?) needs a more fine-grained polarity
score. Since our Complex Concept Builder is very flexible, it
is trivial to score each component separately.
6http://www.guardian.co.uk/education/2008/jul
/01/academicexperts.highereducationprofile
identities are mobilised by entities of political de-
bate (i.e. persons, organisations, etc.); the detec-
tion of indirect speech is mandatory for any such
analysis.
4.4 Commentary/Report Classification
A useful distinction for political scientists dealing
with newspaper articles is the distinction between
articles that report objectively on events or back-
grounds and editorials or press commentaries.
We first extracted opinionated and objective
texts from DeReKo corpus (Stede, 2004; Kupietz
et al, 2010). Some texts were removed in order to
balance the corpus. The balanced corpus contains
2848 documents and has been split into a develop-
ment and a training and test set. 570 documents
were used for the manual creation of features. The
remaining 2278 documents were used to train and
evaluate classifiers using 10-fold cross-validation
with the WEKA machine learning toolkit (Hall et
al., 2009) and various classifiers (cf. Table 1).
The challenge is that the newspaper articles
from the training and evaluation corpus come from
different newspapers and, of course, from differ-
ent authors. Commentaries in the yellow press
tend to have a very different style and vocabulary
than commentaries from broadsheet press. There-
fore, special attention needs to be paid to the in-
dependence of the classifier from different authors
and different newspapers. For this reason, we use
hand-crafted features tailored to this problem. In
return, this means omitting surface-form features
(i.e. words themselves).
The support vector machine used the SMO al-
gorithm (Platt and others, 1998) with a polynomial
kernel K(x, y) =< x, y > e with e = 2. All other
algorithms were used with default settings.
precision recall f-score
SVM 0.819 0.814 0.813
Naive Bayes 0.79 0.768 0.764
Multilayer Percep-
tron
0.796 0.795 0.794
Table 1: Results of a 10-fold cross-validation for
various machine learning algorithms.
A qualitative evaluation shows that direct and
indirect speech is a problem for the classifier.
Opinions voiced via indirect speech should not
lead to a classification as ?Commentary?, but
should be ignored. Additionally, the number of
61
uses of direct and indirect speech by the author can
provide insight into the intention of the author. A
common way to voice one?s own opinion, without
having to do so explicitly, is to use indirect speech
that the author agrees with. Therefore, the number
of direct and indirect speech uses will be added
to the classifier. First experiments indicate that the
inclusion of direct and indirect speech increase the
performance of the classifier.
5 Related Work
Many approaches exist to assist social scientists in
dealing with large scale data. We discuss some
well-known ones and highlight differences to the
approach described above.
The Europe Media Monitor (EMM) (Stein-
berger et al, 2009) analyses large amounts of
newspaper articles and assists anyone interested in
news. It allows its users to search for specific top-
ics and automatically clusters articles from differ-
ent sources. This is a key concept of the EMM,
because it collects about 100, 000 articles in ap-
proximately 50 languages per day and it is impos-
sible to scan through these by hand. EMM users
are EU institutions, national institutions of the EU
member states, international organisations and the
public (Steinberger et al, 2009).
The topic clusters provide insight into ?hot?
topics by simply counting the amount of articles
per cluster or by measuring the amount of news on
a specific topic with regards to its normal amount
of news. Articles are also data-mined for geo-
graphical information, e.g. to update in which
geographical region the article was written and
where the topic is located. Social network infor-
mation is gathered and visualised as well.
Major differences between the EMM and our
approach are the user group and the domain of
the corpus. The complex concepts political sci-
entists are interested in are much more nuanced
than the concepts relevant for topic detection and
the construction of social networks. Additionally,
the EMM does not allow its users to look for their
own concepts and issues, while this interactivity
is a central contribution of our approach (cf. Sec-
tions 1, 2.1 and 3.2).
The CLARIN-D project also provides a web-
based platform to create NLP-chains. It is called
WebLicht (Hinrichs et al, 2010), but in its cur-
rent form, the tool is not immediately usable for
social scientists as the separation of metadata and
textual data and the encoding of the data is hard
for non-experts. Furthermore, WebLicht does not
yet support the combination of manual and au-
tomatic annotation needed for text exploration in
the social science. Our approach is based on the
webservices used by WebLicht. But in contrast to
WebLicht, we provide two additional components
that simplify the integration (exploration work-
bench) and the interpretation (complex concept
builder) of the research data. The former is in-
tended, in the medium term, to be made available
in the CLARIN framework.
6 Conclusion and Outlook
We developed and implemented a pipeline of var-
ious text processing tools which is designed to as-
sist political scientists in finding specific, complex
concepts within large amounts of text. Our case
studies showed that our approach can provide ben-
eficial assistance for the research of political sci-
entists as well as researcher from other social sci-
ences and the humanities. A future aspect will be
to find metrics to evaluate our pipeline. In recently
started annotation experiments on topic classifica-
tion Cohen?s kappa coefficient (Carletta, 1996) is
mediocre. It may very well be possible that the
complex concepts, like multiple collective identi-
ties, are intrinsically hard to detect, and the anno-
tations cannot be improved substantially.
The extension of the NLP pipeline will be an-
other major working area in the future. Examples
are sentiment analysis for German, adding world
knowledge about named entities (e.g. persons and
events), identification of relations between enti-
ties.
Finally, all these systems need to be evaluated
not only in terms of f-score, precision and recall,
but also in terms of usability for the political scien-
tists. This also includes a detailed investigation of
various political science concepts and if they can
be detected automatically or if natural language
processing can help the political scientists to de-
tect their concepts semi-automatically. The defini-
tion of such evaluation is an open research topic in
itself.
Acknowledgements
The research leading to these results has been
done in the project eIdentity which is funded from
the Federal Ministry of Education and Research
(BMBF) under grant agreement 01UG1234.
62
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andre Blessing, Jens Stegmann, and Jonas Kuhn.
2012. SOA meets relation extraction: Less may be
more in interaction. In Proceedings of the Work-
shop on Service-oriented Architectures (SOAs) for
the Humanities: Solutions and Impacts, Digital Hu-
manities, pages 6?11.
Volker Boehlke, Gerhard Heyer, and Peter Wittenburg.
2013. IT-based research infrastructures for the hu-
manities and social sciences - developments, exam-
ples, standards, and technology. it - Information
Technology, 55(1):26?33, February.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional, pages 89?97.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Manaal Faruqui and Sebastian Pado?. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Proceedings
of KONVENS 2010, Saarbru?cken, Germany.
D. Ferrucci and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information process-
ing in the corporate research environment. Natural
Language Engineering, 10(3-4):327?348.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Erhard W. Hinrichs, Marie Hinrichs, and Thomas Za-
strow. 2010. WebLicht: Web-Based LRT Services
for German. In Proceedings of the ACL 2010 System
Demonstrations, pages 25?29.
Cathleen Kantner, Amelie Kutter, Andreas Hilde-
brandt, and Mark Puettcher. 2011. How to get rid
of the noise in the corpus: Cleaning large samples
of digital newspaper texts. International Relations
Online Working Paper, 2, July.
Marc Kupietz, Cyril Belica, Holger Keibel, and An-
dreas Witt. 2010. The german reference corpus
dereko: a primordial sample for linguistic research.
In Proceedings of the 7th conference on interna-
tional language resources and evaluation (LREC
2010), pages 1848?1854.
Shalom Lappin and Herbert J Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational linguistics, 20(4):535?561.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Andreas Niekler and Patrick Ja?hnichen. 2012. Match-
ing results of latent dirichlet alocation for text.
In Proceedings of ICCM 2012, 11th International
Conference on Cognitive Modeling, pages 317?322.
Universita?tsverlag der TU Berlin.
John Platt et al 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector
machines. technical report msr-tr-98-14, Microsoft
Research.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513?
523.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Helmut Schmid, 2009. Corpus Linguistics: An In-
ternational Handbook, chapter Tokenizing and Part-
of-Speech Tagging. Handbooks of Linguistics and
Communication Science. Walter de Gruyter, Berlin.
Burr Settles and Xiaojin Zhu. 2012. Behavioral fac-
tors in interactive training of text classifiers. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
563?567. Association for Computational Linguis-
tics.
Burr Settles. 2011. Closing the loop: Fast, inter-
active semi-supervised annotation with queries on
features and instances. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1467?1478. Association for Com-
putational Linguistics.
Manfred Stede. 2004. The potsdam commentary
corpus. In Proceedings of the 2004 ACL Work-
shop on Discourse Annotation, DiscAnnotation ?04,
pages 96?102, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the europe me-
dia monitor family of applications. In Proceedings
of the Information Access in a Multilingual World-
Proceedings of the SIGIR 2009 Workshop, pages 1?
8.
63
Roland Stuckardt. 2001. Design and enhanced evalua-
tion of a robust anaphor resolution algorithm. Com-
putational Linguistics, 27(4):479?506.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
64
