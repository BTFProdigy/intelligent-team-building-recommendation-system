Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65?71,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Combining Morphosyntactic Enriched Representation with
n-best Reranking in Statistical Translation
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte and H. Schwenk
Spoken Language Processing Group
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{maynard,allauzen,dechelot,schwenk}@limsi.fr
Abstract
The purpose of this work is to explore
the integration of morphosyntactic infor-
mation into the translation model itself, by
enriching words with their morphosyntac-
tic categories. We investigate word dis-
ambiguation using morphosyntactic cate-
gories, n-best hypotheses reranking, and
the combination of both methods with
word or morphosyntactic n-gram lan-
guage model reranking. Experiments
are carried out on the English-to-Spanish
translation task. Using the morphosyn-
tactic language model alone does not
results in any improvement in perfor-
mance. However, combining morphosyn-
tactic word disambiguation with a word
based 4-gram language model results in a
relative improvement in the BLEU score
of 2.3% on the development set and 1.9%
on the test set.
1 Introduction
Recent works in statistical machine translation
(SMT) shows how phrase-based modeling (Och and
Ney, 2000a; Koehn et al, 2003) significantly out-
perform the historical word-based modeling (Brown
et al, 1993). Using phrases, i.e. sequences of
words, as translation units allows the system to pre-
serve local word order constraints and to improve
the consistency of phrases during the translation pro-
cess. Phrase-based models provide some sort of
context information as opposed to word-based mod-
els. Training a phrase-based model typically re-
quires aligning a parallel corpus, extracting phrases
and scoring them using word and phrase counts. The
derived statistics capture the structure of natural lan-
guage to some extent, including implicit syntactic
and semantic relations.
The output of a SMT system may be difficult to
understand by humans, requiring re-ordering words
to recover its syntactic structure. Modeling language
generation as a word-based Markovian source (an n-
gram language model) discards linguistic properties
such as long term word dependency and word-order
or phrase-order syntactic constraints. Therefore, ex-
plicit introduction of structure in the language mod-
els becomes a major and promising focus of atten-
tion.
However, as of today, it seems difficult to outper-
form a 4-gram word language model. Several stud-
ies have attempted to use morphosyntactic informa-
tion (also known as part-of-speech or POS informa-
tion) to improve translation. (Och et al, 2004) have
explored many different feature functions. Rerank-
ing n-best lists using POS has also been explored by
(Hasan et al, 2006). In (Kirchhoff and Yang, 2005),
a factored language model using POS information
showed similar performance to a 4-gram word lan-
guage model. Syntax-based language models have
also been investigated in (Charniak et al, 2003). All
these studies use word phrases as translation units
and POS information in just a post-processing step.
This paper explores the integration of morphosyn-
tactic information into the translation model itself
by enriching words with their morphosyntactic cat-
65
egories. The same idea has already been applied
in (Hwang et al, 2007) to the Basic Travel Ex-
pression Corpus (BTEC). To our knowledge, this
approach has not been evaluated on a large real-
word translation problem. We report results on
the TC-STAR task (public European Parliament Ple-
nary Sessions translation). Furthermore, we pro-
pose to combine this approach with classical n-best
list reranking. Experiments are carried out on the
English-to-Spanish task using a system based on the
publicly available Moses decoder.
This paper is organized as follows: In Section
2 we first describe the baseline statistical machine
translation systems. Section 3 presents the consid-
ered task and the processing of the corpora. The
experimental evaluation is summarized in section 4.
The paper concludes with a discussion of future re-
search directions.
2 System Description
The goal of statistical machine translation is to pro-
duce a target sentence e from a source sentence f .
Among all possible target language sentences the
one with the highest probability is chosen. The use
of a maximum entropy approach simplifies the intro-
duction of several additional models explaining the
translation process:
e? = argmaxPr(e|f)
= argmaxe {exp(
?
i
?ihi(e, f))} (1)
where the feature functions hi are the system
models characterizing the translation process, and
the coefficients ?i act as weights.
2.1 Moses decoder
Moses1 is an open-source, state-of-the-art phrase-
based decoder. It implements an efficient beam-
search algorithm. Scripts are also provided to train a
phrase-based model. The popular Giza++ (Och and
Ney, 2000b) tool is used to align the parallel corpora.
The baseline system uses 8 feature functions hi,
namely phrase translation probabilities in both di-
rections, lexical translation probabilities in both di-
rections, a distortion feature, a word and a phrase
1http://www.statmt.org/moses/
penalty and a trigram target language model. Ad-
ditional features can be added, as described in the
following sections. The weights ?i are typically op-
timized so as to maximize a scoring function on a
development set (Och and Ney, 2002).
The moses decoder can output n-best lists, pro-
ducing either distinct target sentences or not (as
different segmentations may lead to the same sen-
tence). In this work, distinct sentences were always
used.
These n-best lists can be rescored using higher
order language models (word- or syntactic-based).
There are two ways to carry out the rescoring: one,
by replacing the language model score or by adding
a new feature function; two, by performing a log-
linear interpolation of the language model used for
decoding and the new language model. This latter
approach was used in all the experiments described
in this paper. The set of weights is systematically
re-optimized using the algorithm presented below.
2.2 Weight optimization
A common criterion to optimize the coefficients of
the log-linear combination of feature functions is to
maximize the BLEU score (Papineni et al, 2002)
on a development set (Och and Ney, 2002). For
this purpose, the public numerical optimization tool
Condor (Berghen and Bersini, 2005) is integrated in
the following iterative algorithm:
0. Using good general purpose weights, the
Moses decoder is used to generate 1000-best
lists.
1. The 1000-best lists are reranked using the cur-
rent set of weights.
2. The current hypothesis is extracted and scored.
3. This BLEU score is passed to Condor, which
either computes a new set of weights (the al-
gorithm then proceeds to step 1) or detects that
a local maxima has been reached and the algo-
rithm stops iterating.
The solution is usually found after about 100 itera-
tions. It is stressed that the n-best lists are generated
only once and that the whole tuning operates only
on the n-best lists.
66
English: IPP declareV V P resumedV V D theDT sessionNN ofIN theDT EuropeanNP ParliamentNP
Spanish: declaroV Lfin reanudadoV Ladj elART perodoNC dePREP sesionesNC
delPDEL ParlamentoNC EuropeoADJ
Figure 1: Example of POS-tag enriched bi-text used to train the translation models
2.3 POS disambiguation
It is well-known that syntactic structures vary
greatly across languages. Spanish, for example,
can be considered as a highly inflectional language,
whereas inflection plays only a marginal role in En-
glish.
POS language models can be used to rerank the
translation hypothesis, but this requires tagging the
n-best lists generated by the SMT system. This can
be difficult since POS taggers are not well suited for
ill-formed or incorrect sentences. Finding a method
in which morphosyntactic information is used di-
rectly in the translation model could help overcome
this drawback but also takes account for the syntac-
tic specificities of both source and target languages.
It seems likely that the morphosyntactic informa-
tion of each word will be useful to encode linguis-
tic characteristics, resulting in a sort of word disam-
biguation by considering its morphosyntactic cate-
gory. Therefore, in this work we investigate a trans-
lation model which enriches every word with its syn-
tactic category. The enriched translation units are a
combination of the original word and the POS tag, as
shown in Figure 1. The translation system takes a se-
quence of enriched units as inputs and outputs. This
implies that the test data must be POS tagged before
translation. Likewise, the POS tags in the enriched
output are removed at the end of the process to pro-
vide the final translation hypothesis which contain
only a word sequence. This approach also allows
to carry out a n-best reranking step using either a
word-based or a POS-based language model.
3 Task, corpus and tools
The experimental results reported in this article were
obtained in the framework of an international evalu-
ation organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
2http://www.tc-star.org/
long-term effort to advance research in all core tech-
nologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the sum-
mary edited by the European Parliament in several
languages, which is also known as the Final Text
Editions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used to
train the statistical translation models (see Table 1
for some statistics).
Spanish English
Whole parallel corpus
Sentence Pairs 1.2M
Total # Words 34.1M 32.7M
Vocabulary size 129k 74k
Sentence length ? 40
Sentence Pairs 0.91M
Total # Words 18.5M 18.0M
Word vocabulary 104k 71k
POS vocabulary 69 59
Enriched units vocab. 115k 77.6k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
Three different conditions are considered in the
TC-STAR evaluation: translation of the Final Text
Edition (text), translation of the transcriptions of the
acoustic development data (verbatim) and transla-
tion of speech recognizer output (ASR). Here we
only consider the verbatim condition, translating
from English to Spanish. For this task, the develop-
ment and test data consists of about 30k words. The
test data is partially collected in the Spanish parlia-
ment. This results in a small mismatch between de-
velopment and test data. Two reference translations
are provided. The scoring is case sensitive and in-
cludes punctuation symbols.
67
3.1 Text normalization
The training data used for normalization differs sig-
nificantly from the development and test data. The
Final Text Edition corpus follows common ortho-
graphic rules (for instance, the first letter of the word
following a full stop or a column is capitalized) and
represents most of the dates, quantities, article refer-
ences and other numbers in digits. Thus the text had
to be ?true-cased? and all numbers were verbalized
using in-house language-specific tools. Numbers are
not tagged as such at this stage; this is entirely left
to the POS tagger.
3.2 Translation model training corpus
Long sentences (more than 40 words) greatly slow
down the training process, especially at the align-
ment step with Giza++. As shown in Figure 2, the
histogram of the length of Spanish sentences in the
training corpus decreases steadily after a length of
20 to 25 words, and English sentences exhibit a sim-
ilar behavior. Suppressing long sentences from the
corpus reduces the number of aligned sentences by
roughly 25% (see Table 1) but speeds the whole
training procedure by a factor of 3. The impact on
performance is discussed in the next section.
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 0  10  20  30  40  50  60  70  80  90  100
Histogram of Spanish sentences? lengths (training set)
Figure 2: Histogram of the sentence length (Spanish
part of the parallel corpus).
3.3 Language model training corpus
In the experiments reported below, a trigram word
language model is used during decoding. This
model is trained on the Spanish part of the parallel
corpus using only sentences shorter than 40 words
(total of 18.5M of language model training data).
Second pass language models were trained on all
available monolingual data (34.1M words).
3.4 Tools
POS tagging was performed with the TreeTagger
(Schmid, 1994). This software provides resources
for both of the considered languages and it is freely
available. TreeTagger is a Markovian tagger that
uses decision trees to estimate trigram transition
probabilities. The English version is trained on the
PENN treebank corpus3 and the Spanish version on
the CRATER corpus.4
Language models are built using the SRI-LM
toolkit (Stolcke, 2002). Modified Knesser-Ney dis-
counting was used for all models. In (Goodman,
2001), a systematic description and comparison of
the usual smoothing methods is reported. Modified
Knesser-Ney discounting appears to be the most ef-
ficient method.
4 Experiments and Results
Two baseline English-to-Spanish translation mod-
els were created with Moses. The first model was
trained on the whole parallel text ? note that sen-
tences with more than 100 words are excluded by
Giza++. The second model was trained on the cor-
pus using only sentences with at most 40 words. The
BLEU score on the development set using good gen-
eral purpose weights is 48.0 for the first model and
47.0 for the second. Because training on the whole
bi-text is much slower, we decided to perform our
experiments on the bi-texts restricted to the ?short?
sentences.
4.1 Language model generation
The reranking experiments presented below use the
following language models trained on the Spanish
part of the whole training corpus:
? word language models,
? POS language model,
? POS language model, with a stop list used to
remove the 100 most frequent words (POS-
stop100 LM),
? language model of enriched units.
3http://www.cis.upenn.edu/ treebank
4http://www.comp.lancs.ac.uk/linguistics/crater/corpus.html
68
English : you will be aware President that over the last few sessions in Strasbourg. ..
Baseline: usted sabe que el Presidente durante los u?ltimos sesiones en Estrasburgo ...
Enriched units: usted sabe que el Presidente en los u?ltimos per??odos de sesiones en Estrasburgo ...
English : ... in this house there might be some recognition ...
Baseline: ... en esta asamblea no puede ser un cierto reconocimiento ...
Enriched units: ... en esta asamblea existe un cierto reconocimiento ...
Figure 3: Comparative translations using the baseline word system and the enriched unit system.
For each of these four models, various orders
were tested (n = 3, 4, 5), but in this paper we only
report those orders that yielded the greatest improve-
ments. POS language models were obtained by first
extracting POS sequences from the previously POS-
tagged training corpus and then by estimating stan-
dard back-off language models.
As shown in Table 1, the vocabulary size of the
word language model is 104k for Spanish and 74k
for English. The number of POS is small: 69 for
Spanish and 59 for English. We emphasize that
the tagset provided by TreeTagger does include nei-
ther gender nor number distinction. The vocabulary
size of the enriched-unit language model is 115k for
Spanish and 77.6k for English. The syntactical am-
biguity of words is low: the mean ambiguity ratio is
1.14 for Spanish and 1.12 for English.
4.2 Reranking the word n-best lists
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on words as units are summarized in Table 2. The
baseline result, with trigram word LM reranking,
gives a BLEU score of 47.0 (1rst row). From the
n-best lists provided by this translation model, we
compared reranking performances with different tar-
get language models. As observed in the literature,
an improvement can be obtained by reranking with
a 4-gram word language model (47.0 ? 47.5, 2d
row). By post-tagging this n-best list, a POS lan-
guage model reranking can be performed. However,
reranking with a 5-gram POS language model alone
does not give any improvement from the baseline
(BLEU score of 46.9, 3rd row). This result corre-
sponds to known work in the literature (Kirchhoff
and Yang, 2005; Hasan et al, 2006), when using
POS only as a post-processing step during rerank-
ing. As suggested in section 2.3, this lack of per-
formance can be due to the fact that the tagger is
not able to provide a usefull tagging of sentences
included in the n-best lists. This observation is
also available when reranking of the word n-best is
done with a language model based on enriched units
(BLEU score of 47.6, not reported in Table 2).
4.3 POS disambiguation and reranking
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on enriched units are summarized in Table 3. Us-
ing a trigram language model of enriched transla-
tion units leads to a BLEU score of 47.4, a 0.4 in-
crease over the baseline presented in section 4.2.
Figure 3 shows comparative translation examples
from the baseline and the enriched translation sys-
tems. In the first example, the baseline system out-
puts ?durante los u?ltimos sesiones? where the en-
riched translation system produces ?en los u?ltimos
per??odos de sesiones?, a better translation that may
be attributed to the introduction of the masculine
word ?per??odos?, allowing the system to build a
syntactically correct sentence. In the second exam-
ple, the syntactical error ?no puede ser un cierto re-
conocimiento? produced by the baseline system in-
duces an incorrect meaning of the sentence, whereas
the enriched translation system hypothesis ?existe un
cierto reconocimiento? is both syntactically and se-
mantically correct.
Reranking the enriched n-best with POS language
models (either with or without a stop list) does not
seem to be efficient (0.3 BLEU increasing with the
POS-stop100 language model).
A better improvement is obtained when reranking
is performed with the 4-gram word language model.
This results in a BLEU score of 47.9, correspond-
ing to a 0.9 improvement over the word baseline. It
is interesting to observe that reranking a n-best list
69
Dev. Test
3g word LM baseline 47.0 46.0
4g word LM reranking 47.5 46.5
5g POS reranking 46.9 46.1
Table 2: BLEU scores using words as translation
units.
obtained with a translation model based on enriched
units with a word LM results in better performances
than a enriched units LM reranking of a n-best list
obtained with a translation model based on words.
The last two rows of Table 3 give results when
combining word and POS language models to rerank
the enriched n-best lists. In both cases, 10 features
are used for reranking (8 Moses features + word
language model probability + POS language model
probability). The best result is obtained by com-
bining the 5-gram word language model with the 5-
gram POS-stop100 language model. In that case,
the best BLEU score is observed (48.1), with a 2.3%
relative increase over the trigram word baseline.
4.4 Results on the test set
The results on the test set are given in the second
column of Tables 2 and 3. Although the enriched
translation system is only 0.1 BLEU over the base-
line system (46.0 ? 46.1) when using a trigram lan-
guage model, the best condition observed on the de-
velopment set (word and POS-stop100 LMs rerank-
ing) results in a 46.8 BLEU score, corresponding to
a 0.8 increasing.
It can be observed that rescoring with a 4-gram
word language model leads to same score resulting
in a 1.9% relative increase over the trigram word
baseline.
5 Conclusion and future work
Combining word language model reranking of n-
best lists based on syntactically enriched units seems
to produce more consistent hypotheses. Using en-
riched translation units results in a relative 2.3%
improvement in BLEU on the development set and
1.9% on the test over the trigram baseline. Over a
standard translation model with 4-gram rescoring,
the enriched unit translation model leads to an abso-
lute increase in BLEU score of 0.4 both on the devel-
opment and the test sets. These first results are en-
Dev. Test
3g enriched units LM baseline 47.4 46.1
4g enriched units LM reranking 47.8 46.8
4g word LM reranking 47.9 46.9
5g POS LM reranking 47.5 46.2
5g POS-stop100 LM reranking 47.7 46.3
word + POS LMs reranking 47.9 46.9
word + POS-stop100 LMs rerank. 48.1 46.8
Table 3: BLEU scores using enriched translation
units.
couraging enough to further investigate the integra-
tion of syntactic information in the translation model
itself, rather than to restrict it to the post-processing
pass. As follow-up experiments, it is planned to in-
clude gender and number information in the tagset,
as well as the word stems to the enriched units.
This work should be considered as preliminary
experiments for the investigation of factored trans-
lation models, which Moses is able to handle. POS
factorization is indeed a way to add some general-
ization capability to the enriched translation models.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), and by the French Government
under the project INSTAR (ANR JCJC06 143038).
We would like to thanks Marc Ferras for his help
concerning the Spanish language.
References
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of powell?s
UOBYQA algorithm: Experimental results and com-
parison with the DFO algorithm. Journal of Computa-
tional and Applied Mathematics, 181:157?175.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proceedings of MT Summit IX.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and ?H.
Ney. 2005. Cross domain automatic transcription on
70
the TC-STAR epps corpus. In Proceedings of ICASSP
2005.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403?434, October.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypothesis using structural properties. In Pro-
ceedings of EACL 2006.
Y.S. Hwang, A. Finch, and Y. Sasaki. 2007. Improving
statistical machine translation using shallow linguistic
knoledge. to be published in Computer, Speech and
Language.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation. In
Proceedings of ACL ?05 workshop on Building and Us-
ing Parallel Text, pages 125?128.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
2003 (HLT-NAACL 2003), Edmonton, Canada, May.
Franz Josef Och and Hermann Ney. 2000a. Improved
statistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pages 440?447, Hongkong, China, Octo-
ber.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 440?447, Hong Kong, China,
October.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In NAACL,
pages 161?168.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, University of Pennsylva-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, September.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages II:
901?904.
71
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 100?104,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?09
Alexandre Allauzen, Josep Crego, Aur?lien Max and Fran?ois Yvon
LIMSI/CNRS and Universit? Paris-Sud 11, France
BP 133, 91403 Orsay C?dex
firstname.lastname@limsi.fr
Abstract
This paper describes our Statistical Ma-
chine Translation systems for the WMT09
(en:fr) shared task. For this evaluation, we
have developed four systems, using two
different MT Toolkits: our primary sub-
mission, in both directions, is based on
Moses, boosted with contextual informa-
tion on phrases, and is contrasted with a
conventional Moses-based system. Addi-
tional contrasts are based on the Ncode
toolkit, one of which uses (part of) the En-
glish/French GigaWord parallel corpus.
1 Introduction
This paper describes our Statistical Machine
Translation systems for the WMT09 (en:fr) shared
task. For this evaluation, we have developed four
systems, using two different MT toolkits: our
primary submission, in both direction, is based
on Moses, boosted with contextual information
on phrases; we also provided a contrast with a
vanilla Moses-based system. Additional contrasts
are based on the N-code decoder, one of which
takes advantage of (part of) the English/French Gi-
gaWord parallel corpus.
2 System architecture and resources
In this section, we describe the main characteris-
tics of the baseline phrase-based systems used in
this evaluation and the resources that were used to
train our models.
2.1 Pre- and post-processing tools
All the available textual corpora were processed
and normalized using in-house text processing
tools. Our last year experiments (D?chelotte et
al., 2008) revealed that using better normalization
tools provides a significant reward in BLEU, a fact
that we could observe again this year. The down-
side is the need to post-process our outputs so as
to ?detokenize? them for scoring purposes, which
is unfortunately an error-prone process.
Based again on last year?s experiments, our sys-
tems are built in ?true case?: the first letter of each
sentence is lowercased when it should be, and the
remaining tokens are left as is.
Finally, the N-code (see 2.5) and the context-
aware (see 3) systems require the source to be
morpho-syntactically analysed. This was per-
formed using the TreeTagger1 for both languages.
2.2 Alignment and translation models
Our baseline translation models (see 2.4 and 2.5)
use all the parallel corpora distributed for this eval-
uation: Europarl V4, news commentary (2006-
2009) and the additional news data, totalling 1.5M
sentences. Our preliminary attempts with larger
translation models using the GigaWord corpus are
reported in section 3.2. All these corpora were
aligned with GIZA++2 using default settings.
2.3 Language Models
To train our language models (LMs), we took ad-
vantage of the a priori information that the test
set would be of newspaper/newswire genre. We
1http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger.
2http://www.fjoch.com/GIZA++.html.
100
Source Period M. words
News texts 1994-06 3 317
En BN transcripts 2000-07 341
WMT 86
Newswires 1994-07 723
Newspapers 1987-06 486
Fr WEB 2008 23
WMT 46
News-train08 167
Table 1: Corpora used to train the target language
models in English and French.
thus built much larger LMs for translating both to
French and to English, and optimized their combi-
nation on the first part of the official development
data (dev2009a).
Corpora and vocabulary Statistics regarding
the training material are summarized in table 1 in
terms of source, time period, and millions of oc-
currences. ?WMT? stands for all text provided
for the evaluation. Development sets and the large
training corpora (news-train08 and the GigaWord
corpus) were not included. Altogether, these data
contain a total number of 3.7 billion tokens for En-
glish and 1.4 billion tokens for French.
To estimate such large LMs, a vocabulary was
first defined for both languages by including all to-
kens in the WMT parallel data. This initial vocab-
ulary of 130K words was then extended by adding
the most frequent words observed in the additional
training data. This procedure yielded a vocabulary
of one million words in both languages.
Language model training The training data
were divided into several sets based on dates on
genres (resp. 7 and 9 sets for English and French).
On each set, a standard 4-gram LM was estimated
from the 1M word vocabulary with in-house tools
using absolute discounting interpolated with lower
order models. The resulting LMs were then lin-
early interpolated using interpolation coefficients
chosen so as to minimise perplexity of the devel-
opment set (dev2009a). Due to memory limita-
tions, the final LMs were pruned using perplexity
as pruning criterion.
Out of vocabulary word and perplexity To
evaluate our vocabulary and LMs, we used the of-
ficial devtest and test sets. The out-of-vocabulary
(OOV) rate was drastically reduced by increasing
the vocabulary size, the mean OOV rate decreas-
ing from 2.5% to 0.7%, a trend observed in both
languages.
For French, using a small LM trained on the
"WMT" data only resulted in a perplexity of 301
on the devtest corpus and 299 on the test set. Us-
ing all additional data yielded a large decrease in
perplexity (106 on the devtest and 108 on the test);
again the same trend was observed for English.
2.4 A Moses baseline
Our baseline system was a vanilla phrase-based
system built with Moses (Koehn et al, 2007) us-
ing default settings. Phrases were extracted using
the ?grow-diag-final-and? heuristics, using a max-
imum phrase length of 7; non-contextual phrase
scores contain the 4 translation model scores, plus
a fixed phrase penalty; 6 additional scores param-
eterize the lexicalized reordering model. Default
decoding options were used (20 alternatives per
phrase, maximum distortion distance of 7, etc.)
2.5 A N-code baseline
N-code implements the n-gram-based approach
to Statistical Machine Translation (Mari?o et al,
2006). In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training
such a model requires to reorder source sentences
so as to match the target word order. This is also
performed via a stochastic finite-state reordering
model, which uses part-of-speech information to
generalise reordering patterns beyond lexical reg-
ularities. The reordering model is trained on a ver-
sion of the parallel corpora where the source sen-
tences have been reordered via the unfold heuris-
tics (Crego and Mari?o, 2007). A conventional n-
gram language model of the target language pro-
vides the third component of the system.
In all our experiments, we used 4-gram reorder-
ing models and bilingual tuple models built using
Kneser-Ney backoff (Chen and Goodman, 1996).
The maximum tuple size was also set to 7.
2.6 Tuning procedure
The Moses-based systems were tuned using the
implementation of minimum error rate train-
ing (MERT) (Och, 2003) distributed with the
Moses decoder, using the development corpus
(dev2009a). For the context-less systems, tun-
ing concerned the 14 usual weights; tuning the
101
22 weights of the context-aware systems (see 3.1)
proved to be much more challenging, and the
weights used in our submissions are probably far
from optimal. The N-code systems only rely on
9 weights, since they dispense with the lexical re-
ordering model; these weights were tuned on the
same dataset, using an in-house implementation of
the simplex algorithm.
3 Extensions
3.1 A context-aware system
In phrase-based translation, source phrases are
translated irrespective of their (source) context.
This is often not perceived as a limitation as
(i) typical text domains usually contain only few
senses for polysemous words, thus limiting the
use of word sense disambiguation (WSD); and (ii)
using long-span target language models (4-grams
and more) often capture sufficient context to se-
lect the more appropriate translation for a source
phrase based on the target context. In fact, at-
tempts at using source contexts in phrase-based
SMT have to date failed to show important gains
on standard evaluation test sets (Carpuat and Wu,
2007; Stroppa et al, 2007; Gimpel and Smith,
2008; Max et al, 2008). Importantly, in all con-
ditions where gains have been obtained, the tar-
get language was the ?morphologically-poor? En-
glish.
Nonetheless, there seems to be a clear consen-
sus on the importance of better exploiting source
contexts in SMT, so as to improve phrase disam-
biguation. The following sentence extract from
the devtest corpus is a typical example where the
lack of context in our phrase-based system yields
an incorrect translation:
Source: the long weekend comes with a price . . .
Target: Le long week-end vient avec un prix . . .
(the long weekend comes accompanied by a price)
While grammatically correct, the French trans-
lation sounds unnatural, and getting the correct
meaning requires knowledge of the idiom in the
source language. In such a situation, the right con-
text of the phrase comes with can be successfully
used to propose a better translation.3
From an engineering perspective, integrating
context into phrase-based SMT systems can be
performed by (i) transforming source words into
unique tokens, so as to record the original context
3Our context-aware phrase-based system indeed proposes
the appropriate translation: Le long week-end a un prix.
of each entry of the phrase table; and by (ii) adding
one or several contextual scores to the phrase ta-
ble. Using standard MERT, the corresponding
weights can be optimized on development data.
A typical contextual score corresponds to
p(e|f , C(f)), where C(f) is some contextual in-
formation about the source phrase f . An exter-
nal disambiguation system can be used to pro-
vide one global context score (Stroppa et al, 2007;
Carpuat and Wu, 2007; Max et al, 2008)); alter-
natively, several scores based on single features
can be estimated using relative frequencies (Gim-
pel and Smith, 2008):
p(e|f , C(f)) =
count(e, f , C(f))
?
e? count(e?, f , C(f))
For these experiments, we followed the latter ap-
proach, restricting ourselves to features represent-
ing the local context up to a fixed distance d (using
the values 1 and 2 in our experiments) from the
source phrase f endstart:
? lexical context features:
? left context: p(e|f , f start?1start?d )
? right context: p(e|f , f end+dend+1 )
? shallow syntactic features (denoting tF1 the
sequence of POS tags for the source sen-
tence):
? left context: p(e|f , tstart?1start?d)
? right context: p(e|f , tend+dend+1)
As in (Gimpel and Smith, 2008), we filtered out
all translations for which p(e|f) < 0.0002. This
was necessary to make score computation practi-
cal given our available hardware resources.
Results on the devtest corpus for
English?French were similar for the context-
aware phrase-based and the baseline phrase-based
system; small gains were achieved in the reverse
direction (see Table 2). The same trend was
observed on the test data.
Manual inspection of the output of the base-
line and context-aware systems on the devtest
corpus for English?French translation confirmed
two facts: (1) performing phrase translation dis-
ambiguation is only useful if a more appropriate
translation has been seen during training ; and (2)
phrase translation disambiguation can capture im-
portant source dependencies that the target lan-
guage model can not recover. The following ex-
102
ample, involving an unseen sense4 (ball in the se-
mantic field of dance rather than sports), illus-
trates our first remark:
Source: about 500 people attended the ball .
Baseline : Environ 500 personnes ont assist? ? la
balle.
+Context: Environ 500 personnes ont particip? ?
la balle.
The next example is a case where contextual in-
formation helped selecting an appropriate transla-
tion, in constrast to the baseline system.
Source: . . . the new method for calculating pen-
sions due to begin next year . . .
Baseline : . . . le nouveau mode de calcul des pen-
sions due ? commencer l?ann?e prochaine . . .
+Context: . . . la nouvelle m?thode de calcul des
pensions qui va d?buter l?ann?e prochaine . . .
3.2 Preliminary experiments with the
GigaWord parallel corpus
One exciting novelty of this year?s campaign was
the availability of a very large parallel corpus for
the en:fr pair, containing about 20M aligned sen-
tences.
Our preliminary work consisted in selecting the
most useful pairs of sentences, based on their av-
erage perplexity, as computed on our develop-
ment language models. The top ranking sen-
tences (about 8M sentences) were then fed into the
usual system development procedure: alignment,
reordering (for the N-code system), phrase pair
extraction, model estimation. Given the unusual
size of this corpus, each of these steps proved
extremely resource intensive, and, for some sys-
tems, actually failed to complete. Contrarily, the
N-code systems, conceptually simpler, proved to
scale nicely.
Given the very late availability of this cor-
pus, our experiments were very limited and we
eventually failed to deliver the test submissions
of our ?GigaWord? system. Preliminary exper-
iments using the N-code systems (see Table 2),
however, showed a clear improvement of perfor-
mance. There is no reason to doubt that similar
gains would be observed with the Moses systems.
3.3 Experiments
The various systems presented above were all de-
veloped according to the same procedure: train-
ing used all the available parallel text; tuning was
4This was confirmed after careful inspection of the phrase
tables of the baseline system.
en ? fr fr ? en
Moses Ncode Moses Ncode
small LM 20.06 18.98 21.14 20.41
Large LM 22.93 21.95 22.20 22.28
+context 23.06 22.69
+giga 23.21 23.14
Table 2: Results on the devtest set
performed on dev2009a (1000 sentences), and our
internal tests were performed on dev2009b (1000
sentences). Results are reported in table 2.
Our primary submission corresponds to
the +context entry, our first contrast to
Moses+LargeLM, and our second contrast to
Ncode+largeLM. Due to lack of time, no official
submission was submitted for the +giga variant.
For the record, the score we eventually obtained
on the test corpus was 26.81, slightly better than
our primary submission which obtained a score of
25.74 (all these numbers were computed on the
complete test set).
4 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT?09 shared task. We
used last year experiments to build competitive
systems, which greatly benefited from in-house
normalisation and language modeling tools.
One motivation for taking part in this campaign
was to use the GigaWord corpus. Even if time did
not allow us to submit a system based on this data,
it was a interesting opportunity to confront our-
selves with the technical challenge of scaling up
our system development tools to very large paral-
lel corpora. Our preliminary results indicate that
this new resource can actually help improve our
systems.
Naturally, future work includes adapting our
systems so that they can use models learnt from
corpora of the size of the GigaWord corpus. In
parallel, we intend to keep on working on context-
aware systems to study the impact of more types
of scores, e.g. based on grammatical dependencies
as in (Max et al, 2008). Given the difficulties we
had tuning our systems, we feel that a preliminary
task should be improving our tuning tools before
addressing these developments.
103
Acknowledgments
This work was partly realised as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
References
M. Carpuat and D. Wu. 2007. Context-Dependent
Phrasal Translation Lexicons for Statistical Machine
Translation. In Proceedings of Machine Translation
Summit XI, pages 73?80, Copenhagen, Denmark.
F. Casacuberta and E. Vidal. 2004. Machine transla-
tion with inferred stochastic finite-state transducers.
Computational Linguistics, 30(3):205?225.
S. F. Chen and J. T. Goodman. 1996. An empirical
study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 310?318, Santa Cruz, NM.
J. M. Crego and J. B. Mari?o. 2007. Improving
SMT by coupling reordering and decoding. Ma-
chine Translation, 20(3):199?215.
D. D?chelotte, G. Adda, A. Allauzen, O. Galibert, J.-L.
Gauvain, H. Meynard, and F. Yvon. 2008. Limsi?s
statistical translation systems for WMT?08. In Pro-
ceedings of the NAACL-HTL Statistical Machine
Translation Workshop, pages 107-100, Columbus,
Ohio.
K. Gimpel and N. A. Smith. 2008. Rich Source-Side
Context for Statistical Machine Translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 9?17, Columbus, Ohio.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session, Prague, Czech Republic.
A. Max, R. Makhloufi, and P. Langlais. 2008. Explo-
rations in using grammatical dependencies for con-
textual phrase translation disambiguation. In Pro-
ceedings of EAMT, poster session, Hamburg, Ger-
many.
J. B. Mari?o, R. E. Banchs R, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M. R. Costa-
Juss?. 2006. N-gram-based machine translation.
Computational Linguistics, 32(4):527?549.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proceedings of the 11th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI?07),
pages 231?240, Sk?vde, Sweden.
104
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778?788,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Training continuous space language models:
some practical issues
Le Hai Son and Alexandre Allauzen and Guillaume Wisniewski and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
BP 133, 91403 Orsay Cedex
Firstname.Lastname@limsi.fr
Abstract
Using multi-layer neural networks to esti-
mate the probabilities of word sequences is
a promising research area in statistical lan-
guage modeling, with applications in speech
recognition and statistical machine transla-
tion. However, training such models for large
vocabulary tasks is computationally challeng-
ing which does not scale easily to the huge
corpora that are nowadays available. In this
work, we study the performance and behav-
ior of two neural statistical language models
so as to highlight some important caveats of
the classical training algorithms. The induced
word embeddings for extreme cases are also
analysed, thus providing insight into the con-
vergence issues. A new initialization scheme
and new training techniques are then intro-
duced. These methods are shown to greatly re-
duce the training time and to significantly im-
prove performance, both in terms of perplexity
and on a large-scale translation task.
1 Introduction
Statistical language models play an important role in
many practical applications, such as machine trans-
lation and automatic speech recognition. Let V be
a finite vocabulary, statistical language models de-
fine distributions over sequences of words wL1 in V
?
usually factorized as:
P (wL1 ) = P (w1)
L?
l=1
P (wl|w
l?1
1 )
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in real-world Natural Language
Processing applications where V typically contains
dozens of thousands words.
Many approaches to this problem have been pro-
posed over the last decades, the most widely used
being back-off n-gram language models. n-gram
models rely on a Markovian assumption, and de-
spite this simplification, the maximum likelihood es-
timate (MLE) remains unreliable and tends to under-
estimate the probability of very rare n-grams, which
are hardly observed even in huge corpora. Con-
ventional smoothing techniques, such as Kneser-
Ney and Witten-Bell back-off schemes (see (Chen
and Goodman, 1996) for an empirical overview,
and (Teh, 2006) for a Bayesian interpretation), per-
form back-off on lower order distributions to pro-
vide an estimate for the probability of these unseen
events. n-gram language models rely on a discrete
space representation of the vocabulary, where each
word is associated with a discrete index. In this
model, the morphological, syntactic and semantic
relationships which structure the lexicon are com-
pletely ignored, which negatively impact the gen-
eralization performance of the model. Various ap-
proaches have proposed to overcome this limita-
tion, notably the use of word-classes (Brown et al,
1992; Niesler, 1997), of generalized back-off strate-
gies (Bilmes et al, 1997) or the explicit integration
of morphological information in the random-forest
model (Xu and Jelinek, 2004; Oparin et al, 2008).
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
778
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated probability estimates are jointly
computed in a multi-layer neural network architec-
ture. This approach has showed significant and
consistent improvements when applied to automatic
speech recognition (Schwenk, 2007; Emami and
Mangu, 2007; Kuo et al, 2010) and machine trans-
lation tasks (Schwenk et al, 2006). Hence, contin-
uous space language models are becoming increas-
ingly used. These successes have revitalized the re-
search on neuronal architectures for language mod-
els, and given rise to several new proposals (see, for
instance, (Mnih and Hinton, 2007; Mnih and Hinton,
2008; Collobert and Weston, 2008)). A major diffi-
culty with these approaches remains the complexity
of training, which does not scale well to the mas-
sive corpora that are nowadays available. Practical
solutions to this problem are discussed in (Schwenk,
2007), which introduces a number of optimization
and tricks to make training doable. Even then, train-
ing a neuronal language model typically takes days.
In this paper, we empirically study the conver-
gence behavior of two multi-layer neural networks
for statistical language modeling, comparing the
standard model of (Bengio et al, 2003) with the log-
bilinear (LBL) model of (Mnih and Hinton, 2007).
Our contributions are the following: we first pro-
pose a reformulation of Mnih and Hinton?s model,
which reveals its similarity with extant models, and
allows a direct and fair comparison with the stan-
dard model. For the standard model, these results
highlight the impact of parameter initialization. We
first investigate a re-initialization method which al-
lows to escape from the local extremum the standard
model converges to. While this method yields a sig-
nificative improvement, the underlying assumption
about the structure of the model does not meet the
requirement of very large-scale tasks. We therefore
introduce a different initialization strategy, called
one vector initialization. Experimental results show
that these novel training strategies drastically reduce
the total training time, while delivering significant
improvements both in terms of perplexity and in a
large-scale translation task.
The rest of this paper is organized as follows. We
first describe, in Section 2, the standard and the LBL
language models. By reformulating the latter, we
show that both models are very similar and empha-
size the remaining differences. Section 2.4 discusses
complexity issues and possible solutions to reduce
the training time. We then report, in Section 3, pre-
liminary experimental results that enlighten some
caveats of the standard approach. Based on these
observations, we introduce in Section 4 novel and
more efficient training schemes, yielding improved
performance and a reduced training time both on
small and large scale experiments.
2 Continuous space language models
Learning a language model amounts to estimate the
parameters of the discrete conditional distribution
over words given each possible history, where the
history corresponds to some function of the preced-
ing words. For an n-gram model, the history con-
tains the n ? 1 preceding words, and the model
parameters correspond to P (wl|w
l?1
l?n+1). Continu-
ous space language models aim at computing these
estimates based on a distributed representation of
words (Bengio et al, 2003), thereby reducing the
sparsity issues that plague conventional maximum
likelihood estimation. In this approach, each word
in the vocabulary is mapped into a real-valued vec-
tor and the conditional probability distributions are
then expressed as a (parameterized) smooth func-
tion of these feature vectors. The formalism of neu-
ral networks allows to express these two steps in a
well-known framework, where, crucially, the map-
ping and the model parameters can be learned in
conjunction. In the next paragraphs, we describe the
two continuous space language models considered
in our study and present the various issues associ-
ated with the training of such models, as well as their
most common remedies.
2.1 The standard model
In the following, we will consider words as indices
in a finite dictionary of size V ; depending on the
context, w will either refer to the word or to its in-
dex in the dictionary. A word w can also be repre-
sented by a 1-of-V coding vector v of RV in which
all elements are null except the wth. In the standard
approach of (Bengio et al, 2003), the feed-forward
network takes as input the n?1 word history and de-
livers an estimate of the probability P (wl|w
l?1
l?n+1)
779
as its output. It consists of three layers.
The first layer builds a continuous representation
of the history by mapping each word into its real-
valued representation. This mapping is defined by
RTv, where R ? RV?m is a projection matrix
and m is the dimension of the continuous projection
word space. The output of this layer is a vector i of
(n ? 1)m real numbers obtained by concatenating
the representations of the context words. The pro-
jection matrix R is shared along all positions in the
history vector and is learned automatically.
The second layer introduces a non-linear trans-
form, where the output layer activation values are
defined by h = tanh (Wihi + bih) , where i is the
input vector, Wih ? RH?(n?1)m and bih ? RH are
the parameters of this layer. The vector h ? RH can
be considered as an higher (more abstract) represen-
tation of the context than i.
The third layer is an output layer that estimates the
desired probability, thanks to the softmax function:
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
(1)
o = Whoh + bho, (2)
where Who ? RV?H and bho ? RV are respec-
tively the projection matrix and the bias term associ-
ated with this layer. The wth component in P corre-
sponds to the estimated probability of the wth word
of the vocabulary given the input history vector.
The standard model has two hyper-parameters
(the dimension of projection space m and the size of
hidden layer, H) that define the architecture of the
neural network and a set of free parameters ? that
need to be learned from data: the projection matrix
R, the weight matrix Wih, the bias vector bih, the
weight matrix Who and the bias vector bho.
In this model, the projection matrices R and Who
play similar roles as they define maps between the
vocabulary and the hidden representation. The fact
that R assigns similar representations to history
words w1 and w2 implies that these words can be
exchanged with little impact on the resulting prob-
ability distribution. Likewise, the similarity of two
lines in Who is an indication that the corresponding
words tend to have a similar behavior, i.e. tend to
have a similar probabilities of occurrence in all con-
texts. In the remainder, we will therefore refer to R
as the matrix representing the context space, and to
Who as the matrix for the prediction space.
2.2 The log-bilinear model
The work reported (Mnih and Hinton, 2007) de-
scribes another parameterization of the architecture
introduced in the previous section. This parameter-
ization is based on Factored Restricted Boltzmann
Machine. According to (Mnih and Hinton, 2007),
this model, termed the log-bilinear language model
(LBL), achieves, for large vocabulary tasks, bet-
ter results in terms of perplexity than the standard
model, even if the reasons beyond this improvement
remain unclear.
In this section, we will describe this model and
show how it relates to the standard model. The LBL
model estimates the n-gram parameters by:
P (wl|w
l?1
l?n+1) =
exp(?E(wl;w
l?1
l?n+1))
?
w exp(?E(w;w
l?1
l?n+1))
(3)
In this equation, E is an energy function defined as:
E(wl;w
l?1
1 ) = ?
(
l?1?
k=l?n+1
vk
TRCTk
)
RTvl
(4)
? brTRTvl ? bv
Tvl
= ?vTl R
(
l?1?
k=l?n+1
CkR
Tvk + br
)
? vTl bv (5)
where R is the projection matrix introduced above,
(vk)l?n+1?k?l?1 are the 1-of-V coding vectors for
the history words and vl is the coding vector for wl;
Ck ? Rm?m is a combination matrix and br and bv
denote bias vectors. All these parameters need to be
learned during training.
Equation (4) can be rewritten using the notations
introduced for the standard model. We then rename
br and bv respectively bih and bho. We also denote
i the concatenation of the (n ? 1) vectors RTvk;
likewise Wih denotes the H ? (n? 1)m matrix ob-
tained by concatenating row-wise the (n ? 1) ma-
trices Ck. With these new notations, equations (4)
780
and (3) can be rewritten as:
h = Wihi + bih
o = Rh + bho
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
This formulation allows to highlight the similarity of
the LBL model and the standard model. These two
models differ only by the activation function of their
hidden layer (linear for the LBL model and tangent
hyperbolic for the standard model) and by their def-
inition of the prediction space: for the LBL model,
the context space and the prediction space are the
same (R = Who, and thus H = m), while in the
standard model, the prediction space is defined in-
dependently from the context space. This restriction
drastically reduces the number of free parameters of
the LBL model.
It is finally noteworthy to outline the similarity
of this model with standard maximum entropy lan-
guage models (Lau et al, 1993; Rosenfeld, 1996).
Let x denote the binary vector formed by stacking
the (n-1) 1-of-V encodings of the history words;
then the conditional probability distributions esti-
mated in the model are proportional to expF (x),
where F is an affine transform of x. The main dif-
ference with MaxEnt language models are thus the
restricted form of the feature functions, which only
test one history word, and the particular representa-
tion of F , which is defined as:
F (x) = RWihR
?Tv + Rbih + bho
where, as before, R? is formed by concatenating
(n? 1) copies of the projection matrix R.
2.3 Training and inference
Training the two models introduced above can be
achieved by maximizing the log-likelihood L of the
parameters ?. This optimization is usually per-
formed by stochastic back-propagation as in (Ben-
gio et al, 2003). For all our experiments, the learn-
ing rate is fixed at 5?10?3. The learning weight de-
cay and the the weight decay (respectively 1? 10?9
and 0) seem to have a minor impact on the results.
Learning starts with a random initialization of the
parameters under the uniform distribution and con-
verges to a local maximum of the log-likelihood
function. Moreover, to prevent overfitting, an early
stopping strategy is adopted: after each epoch, train-
ing is stopped when the likelihood of a validation set
stops increasing.
2.4 Complexity issues
The main problem with neural language models is
their computational complexity. For the two mod-
els presented in this section, the number of floating
point operations needed to predict the label of a sin-
gle example is1:
((n? 1) ?m + 1)?H + (H + 1)? V (6)
where the first term of the sum corresponds to the
computation of the hidden layer and the second one
to the computation of the output layer. The projec-
tion of the context words amounts to select one row
of the projection matrix R, as the words are repre-
sented with a 1-of-V coding vector. We can there-
fore assume that the computation complexity of the
first layer is negligible. Most of the computation
time is thus spent in the output layer, which implies
that the computing time grows linearly with the vo-
cabulary size. Training these models for large scale
tasks is therefore challenging, and a number of tricks
have been introduced to make training and inference
tractable (Schwenk and Gauvain, 2002; Schwenk,
2007).
Short list A simple method to reduce the com-
plexity in inference and in learning is to reduce
the size of the output vocabulary (Schwenk, 2007):
rather than estimating the probability P (wl =
w|wl?1l?n+1) for all words in the vocabulary, we only
estimate it for the N most frequent words of the
training set (the so-called short-list). In this case,
two vocabularies need to be considered, correspond-
ing respectively to the context vocabulary Vc used to
define the history; and the prediction vocabulary Vp.
However, this method fails to deliver any probability
estimate for words outside of the prediction vocab-
ulary, meaning that a fall-back strategy needs to be
defined for those words. In practice, neural network
1Recall that learning requires to repeatedly predict the label
for all the examples in the training set.
781
language models are combined with a conventional
n-gram model as described in (Schwenk, 2007).
Batch mode and resampling Additional speed-
ups can be obtained by propagating several exam-
ples at once through the network (Bilmes et al,
1997). This ?batch mode? allows to factorize the
matrix operations and cut down both inference and
training time. In all our experiments, we used a
batch size of 64. Moreover, the training time is lin-
ear in the number of examples in the training data2.
Training on very large corpora, which, nowadays,
comprise billions of word tokens, cannot be per-
formed exhaustively and requires to adopt resam-
pling strategies, whereby, at each epoch, the system
is trained with only a small random subset of the
training data. This approach enables to effectively
estimate neural language models on very large cor-
pora; it has also been observed empirically that sam-
pling the training data can increase the generaliza-
tion performance (Schwenk, 2007).
3 A head-to-head comparison
In this section, we analyze a first experimental
study of the two neural network language models
introduced in Section 2 in order to better under-
stand the differences between these models espe-
cially in terms of the word representations they in-
duce. Based on this study, we will propose, in the
next section, improvements of both the speed and
the prediction capacity of the models. In all our ex-
periments, 4-gram language models are used.
3.1 Corpus
The data we use for training is a large monolingual
corpus, containing all the English texts in the par-
allel data of the Arabic to English NIST 2009 con-
strained task3. It consists of 176 millions word to-
kens with 532, 557 different word types as the size
of vocabulary. The perplexity is computed with re-
spect to the 2006 NIST test data, which is used here
as our development data.
2Equation (6) gives the complexity of inference for a single
example.
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3.2 Convergence study
In a first experiment, we trained the two models in
the same setting: we choose to consider a small
vocabulary comprising the 10, 000 most frequent
words. The same vocabulary is used to constrain
the words occurring in the history and the words
to be predicted. The size of hidden layer is set to
m = H = 200, the history contains the 3 preceding
words, we use a batch size of 64, a resampling rate
of 5% and no weight decay.
Figure 1 displays the perplexity convergence
curve measured on the development data for the
standard and the LBL models4. The convergence
perplexities after the combination with the standard
back-off model are also provided for all the mod-
els in table 2 (see section 4.3). We can observe
that the LBL model converges faster than the stan-
dard model: the latter needs 13 epochs to reach
the stopping criteria, while the former only needs
6 epochs. However, upon convergence, the stan-
dard model reaches a lower perplexity than the LBL
model.
0 2 4 6 8 10 12 14120
130
140
150
160
170
180
epochs
per
plex
ity
Perplexity
standardlog bilinear
Figure 1: Convergence rate of the standard and the LBL
models evaluated by the evolution of the perplexity on a
development set
As described in Section 2.2, the main difference
between the standard and the LBL model is the way
the context and the prediction spaces are defined: in
the standard model, the two spaces are distinct; in
4The use of a back-off 4-model estimated with the modified
Knesser-Ney smoothing on the same training data achieves a
perplexity of 141 on the development data.
782
the LBL model, they are bound to be the same. With
a smaller number of parameters, the LBL model can
not capture as many characteristics of the data as the
standard model, but it converges faster5. This differ-
ence in convergence can be explained by the scarcity
of the updates in the projection matrix R in the
standard model: during backpropagation, only those
weights that are associated with words in the history
are updated. By contrast, each training sample up-
dates all the weights in the prediction matrix Who.
3.3 An analysis of the continuous word space
To deepen our understanding, we propose to further
analyze the induced word embeddings by finding,
for some randomly selected words, the five nearest
neighbors (according to the Euclidian distance) in
the context space and in the prediction space of the
two models. Results are presented in Table 1.
If we look first at the standard model, the global
picture is that for frequent words (is, are, and, to
a lesser extend, have), both spaces seem to define
meaningful neighborhood, corresponding to seman-
tic and syntactic similarities; this is less true for rarer
words, where we see a greater discrepancy between
the context and prediction spaces. For instance, the
date 1947 seems to be randomly associated in the
context space, while the 5 nearest words in the pre-
diction space form a consistent set of dates. The
same trend is also observed for the word Castro. Our
interpretation is that for less frequent words, the pro-
jection vectors are hardly ever updated and remain
close to their original random initialization.
By contrast, the similarities in the (unique) pro-
jection space of the LBL remain consistent for all
frequency ranges, and are very similar to the predic-
tion space of the standard model. This seems to val-
idate our hypothesis that in the standard model, the
prediction space is learned much faster than the con-
text space and corroborates our interpretation of the
impact of the scarce updates of rare words. Another
possible explanation is that there is no clear relation
5We could increase the number of parameters of the LBL
model for a fairer comparison with the standard model. How-
ever, this would also increase the size of the vocabulary and
cause two new issues: on one hand, the time complexity would
drastically increase for the LBL model, and on the other hand,
both models would not be comparable in terms of perplexity as
their vocabulary would be different.
between the context space and the target function:
the context space is learned only indirectly by back-
propagation. As a result, due to the random initial-
ization of the parameters and to data sparsity, many
vectors of R might be blocked in some local max-
ima, meaning that similar vectors cannot be grouped
in a consistent way and that the induced similarity is
more ?loose?.
4 Improving the standard model
In Section 3.2, we observed that slightly better re-
sults can be obtained with the standard rather than
with the LBL model. The latter is however much
faster to train, and seems to induce better projection
matrices. Both effects can be attributed to the partic-
ular parameterization of this model, which uses the
same projection matrix both for the context and for
the prediction spaces. In this section, we propose
several new learning regimes that allowed us to im-
prove the standard model in terms of both speed and
prediction capacity. All these improvements rely on
the idea of sharing word representations. While this
idea is not new (see for instance (Collobert and We-
ston, 2008)), our analysis enables to better under-
stand its impact on the convergence rate. Finally, the
improvements we propose are evaluated on a real-
word machine translation task.
4.1 Improving performances with
re-initialization
The experiments reported in the previous section
suggest that it is possible to improve the perfor-
mances of the standard model by building a better
context space. Thus, we introduce a new learning
regime, called re-initialization which aims to im-
prove the context space by re-injecting the informa-
tion on word neighborhoods that emerges in the pre-
diction space. One possible implementation of this
idea is as follows:
1. train a standard model until convergence;
2. use the prediction space of this model to ini-
tialize the context space of a new model; the
prediction space is chosen randomly;
3. train this new model.
783
Table 1: The 5 closest words in the representation spaces of the standard and LBL language models.
word (frequency) model space 5 most closest words
is standard context was are were be been
900, 350 standard prediction was has would had will
LBL both was reveals proves are ON
are standard context were is was be been
478, 440 standard prediction were could will have can
LBL both were is was FOR ON
have standard context had has of also the
465, 417 standard prediction are were provide remain will
LBL both had has Have were embrace
meeting standard context meetings conference them 10 talks
150, 317 standard prediction undertaking seminar meetings gathering project
LBL both meetings summit gathering festival hearing
Imam standard context PCN rebellion 116. Cuba 49
787 standard prediction Castro Sen Nacional Al- Ross
LBL both Salah Khaled Al- Muhammad Khalid
1947 standard context 36 Mercosur definite 2002-2003 era
774 standard prediction 1965 1945 1968 1964 1975
LBL both 1965 1976 1964 1968 1975
Castro standard context exclusively 12. Boucher Zeng Kelly
768 standard prediction Singh Clark da Obasanjo Ross
LBL both Clark Singh Sabri Rafsanjani Sen
Figure 2: Evolution of the perplexity on a development
set for various initialization regimes.
The evolution of the perplexity with respect to train-
ing epochs for this new method is plotted on Fig-
ure 2, where we only represent the evolution of the
perplexity during the third training step. As can be
seen, at convergence, the perplexity the model esti-
mated with this technique is about 10% smaller than
the perplexity of the standard model.
This result can be explained by considering the re-
initialization as a form of annealing technique: re-
initializing the context space allows to escape from
the local extrema the standard model converges to.
The fact that the prediction space provides a good
initialization of the context space also confirms our
analysis that one difficulty with the standard model
is the estimation of the context space parameters.
4.2 Iterative re-initialization
The re-initialization policy introduced in the previ-
ous section significantly reduces the perplexity, at
the expense of a longer training time, as it requires
to successively train two models. As we now know
that the parameters of the prediction space are faster
to converge, we introduce a second training regime
called iterative re-initialization which aims to take
advantage of this property. We summarize this new
training regime as follows:
1. Train the model for one epoch.
2. Use the prediction space parameters to reini-
tialize the context space.
3. Iterate steps (1) and (2) until convergence.
784
Figure 3: Evolution of the perplexity on the training data
for various initialization regimes.
This regimes yields a model that is somewhat in-
between the standard and LBL models as it adds a
relationship between the two representation spaces,
which lacks in the former model. This relationship is
however not expressed through the tying of the cor-
responding parameters; instead we let the prediction
space guide the convergence of the context space.
As a consequence, we hope that it can achieve a con-
vergence speed as fast as the one of the LBL model
without degrading its prediction capacity.
The result plotted on Figure 2 shows that this in-
deed the case: using this training regime, we ob-
tained a perplexity similar to the one of the stan-
dard model, while at the same time reducing the
total training time by more than a half, which is
of great practical interest (each epoch lasts approxi-
mately 8 hours on a 3GHz Xeon processor).
Figure 3 displays the perplexity convergence
curve measured on the training data for the standard
learning regime as well as for the re-initialization
and iterative re-initialization. These results show
the same trend as for the perplexity measured on
the development data, and suggest a regularization
effect of the re-initialization schemes rather than al-
lowing the models to escape local optima.
4.3 One vector initialization
Principle The new training regimes introduced
above outperform the standard training regime both
in terms of perplexity and of training time. However,
exchanging information between the context and
prediction spaces is only possible when the same
vocabulary is used in both spaces. As discussed
in Section 2.4, this configuration is not realistic for
very large-scale tasks. This is because increasing the
number of predicted word types is much more com-
putationally demanding than increasing the number
of types in the context vocabulary. Thus, the former
vocabulary is typically order of magnitudes larger
than the latter, which means that the re-initialization
strategies can no longer be directly used.
It is nonetheless possible to continue drawing in-
spirations from the observations made in Section 3,
and, crucially, to question the random initialization
strategy. As discussed above, this strategy may ex-
plain why the neighborhoods in the induced con-
text space for the less frequent types were diffi-
cult to interpret. As a straightforward alternative,
we consider a different initialization strategy where
all the words in the context vocabulary are initially
projected onto the same (random) point in the con-
text space. The intuition is that it will be easier to
build meaningful neighborhoods, especially for rare
types, if all words are initially considered similar
and only diverge if there is sufficient evidence in the
training data to suggest that they should. This model
is termed the one vector initialization model.
Experimental evaluation To validate this ap-
proach, we compare the convergence of a standard
model trained (with the standard learning regime)
with the one vector initialization regime. The con-
text vocabulary is defined by the 532, 557 words oc-
curring in the training data and the prediction vo-
cabulary by the 10, 000 most frequent words6. All
other parameters are the same as in the previous
experiments. Based on the curves displayed on
Figure 4, we can observe that the model obtained
with the one vector initialization regime outperforms
the model trained with a completely random ini-
tialization. Moreover, the latter reaches conver-
gence in only 14 epochs, while the learning regime
we propose only needs 9 epochs. Convergence is
even faster than when we used the standard training
regime and a small context vocabulary.
6In this case, the distinction between the context and the pre-
diction vocabulary rules out the possibility of a relevant compar-
ison based on perplexity between the continuous space language
model and a standard back-off language model.
785
0 5 10 15100
110
120
130
140
150
160
170
180
epochs
perp
lexit
y
Perplexity
standardone vector initialization
Figure 4: Perplexity with all-10, 000, 200? 200 models
Table 2: Summary of the perplexity (PPX) results mea-
sured on the same development set with the different con-
tinuous space language models. For all of them, the prob-
abilities are combined with the back-off n-gram model
Vc size Model # epochs PPX
10000 log bilinear 6 239
standard 13 227
iterative reinit. 6 223
reinit. 11 211
all standard 14 276
one vector init. 9 260
To illustrate the impact of our initialization
scheme, we also used a principal component anal-
ysis to represent the induced word representations
in a two dimensional space. Figure 5 represents the
vectors associated with numbers7 in red, while all
other words are represented in blue. Two different
models are used: the standard model on the left, and
the one vector initialization model on the right. We
can observe that, for the standard model, most of
the red points are scattered all over a large portion
of the representation space. On the opposite, for
the one vector initialization model, points associated
with numbers are much more concentrated: this is
simply because all the points are originally identi-
cal, and the training aim to spread the point around
this starting point. We also created the closest word
list reported in Table 3, in a manner similar to Ta-
ble 1. Clearly, the new method seems to yield more
7Number are all the words consisting only of digits, with an
optional sign, point or comma such as: 1947; 0,001; -8,2.
(a) with the standard model (b) with the one vector initial-
ization model
Figure 5: Comparison of the word embedding in the con-
text space for numbers (red points).
meaningful neighborhoods in the context space.
It is finally noteworthy to mention that when used
with a small context vocabulary (as in the experi-
mental setting of Section 4.1) this initialization strat-
egy underperforms the standard initialization. This
is simply due to the much greater data sparsity in
the large context vocabulary experiments, where the
rarer word types are really rare (they typically occur
once or twice). By contrast, the rarer words in the
small vocabulary tasks occurred more than several
hundreds times in the training corpus, which was
more than sufficient to guide the model towards sat-
isfactory projection matrices. This finally suggests
that there still exists room for improvement if we
can find more efficient initialization strategies than
starting from one or several random points.
4.4 Statistical machine translation experiments
As a last experiment, we compare the various mod-
els on a large scale machine translation task. Sta-
tistical language models are key component of cur-
rent statistical machine translation systems (Koehn,
2010), where they both help disambiguate lexical
choices in the target language and influence the
choice of the right word ordering. The integration of
a neural network language model in such a system is
far from easy, given the computational cost of com-
puting word probabilities, a task that is performed
repeatedly during the search of the best translation.
We then had to resort to a two pass decoding ap-
proach: the first pass uses a conventional back-off
language model to produce a n-best list (the n most
likely translations and their associated scores); in the
second pass, the probability of the neural language
model is computed for each hypothesis and the n-
786
Table 3: The 5 closest words in the context space of the standard and one vector initialization language models
word (freq.) model 5 closest words
is standard was are were been remains
900, 350 1 vector init. was are be were been
conducted standard undertaken launched $270,900 Mufamadi 6.44-km-long
18, 388 1 vector init. pursued conducts commissioned initiated executed
Cambodian standard Shyorongi $3,192,700 Zairian depreciations teachers
2, 381 1 vector init. Danish Latvian Estonian Belarussian Bangladeshi
automatically standard MSSD Sarvodaya $676,603,059 Kissana 2,652,627
1, 528 1 vector init. routinely occasionally invariably inadvertently seldom
Tosevski standard $12.3 Action,3 Kassouma 3536 Applique
34 1 vector init. Shafei Garvalov Dostiev Bourloyannis-Vrailas Grandi
October-12 standard 39,572 anti-Hutu $12,852,200 non-contracting Party?s
8 1 vector init. March-26 April-11 October-1 June-30 August4
3727th standard Raqu Tatsei Ayatallah Mesyats Langlois
1 1 vector init. 4160th 3651st 3487th 3378th 3558th
best list is accordingly reordered to produce the final
translations.
The different language models discussed in this
article are evaluated on the Arabic to English
NIST 2009 constrained task. For the continuous
space language model, the training data consists
in the parallel corpus used to train the translation
model (previously described in section 3.1). The de-
velopment data is again the 2006 NIST test set and
the test data is the official 2008 NIST test set. Our
system is built using the open-source Moses toolkit
(Koehn et al, 2007) with default settings. To set
up our baseline results, we used an extensively op-
timized standard back-off 4-grams language model
using Kneser-Ney smoothing described in (Allauzen
et al, 2009). The weights used during the reranking
are tuned using the Minimum Error Rate Training
algorithm (Och, 2003). Performance is measured
based on the BLEU (Papineni et al, 2002) scores,
which are reported in Table 4.
Table 4: BLEU scores on the NIST MT08 test set with
different language models.
Vc size Model # epochs BLEU
all baseline - 37.8
10000 log bilinear 6 38.2
standard 13 38.3
iterative reinit. 6 38.4
reinit. 11 38.4
all standard 14 38.6
one vector init. 9 38.7
All the experimented neural language models
yield to a significant BLEU increase. The best re-
sult is obtained by the one vector initialization stan-
dard model which achieves a 0.9 BLEU improve-
ment. While this results is similar to the one ob-
tained with the standard model, the training time is
reduced here by a third.
5 Conclusion
In this work, we proposed three new methods
for training neural network language models and
showed their efficiency both in terms of computa-
tional complexity and generalization performance in
a real-word machine translation task. These meth-
ods rely on conclusions drawn from a careful study
of the convergence rate of two state-of-the-art mod-
els and are based on the idea of sharing the dis-
tributed word representations during training.
Our work highlights the impact of the initializa-
tion and the training scheme for neural network lan-
guage models. Both our experimental results and
our new training methods can be closely related to
the pre-training techniques introduced by (Hinton
and Salakhutdinov, 2006). Our future work will thus
aim at studying the connections between our empir-
ical observations and the deep learning framework.
Acknowledgments
This work was partly realized as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
787
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 100?104, Athens, Greece, March. Association
for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
J. Bilmes, K. Asanovic, C. Chin, and J. Demmel. 1997.
Using phipac to speed error back-propagation learn-
ing. Acoustics, Speech, and Signal Processing, IEEE
International Conference on, 5:4153.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ahmed Emami and Lidia Mangu. 2007. Empirical study
of neural network language models for Arabic speech
recognition. In Proc. ASRU?07, pages 147?152, Ky-
oto. IEEE.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proc. ICML ?07, pages 641?648, New York, NY, USA.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Thomas R. Niesler. 1997. Category-based statistical
language models. Ph.D. thesis, University of Cam-
bridge.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL?03, pages
160?167, Sapporo, Japan.
Ilya Oparin, Ondr?ej Glembek, Luka?s? Burget, and Jan
C?ernocky?. 2008. Morphological random forests for
language modeling of inflectional languages. In Proc.
SLT?08, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. ACL?02, pages
311?318, Philadelphia.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10:187?228.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Peng Xu and Frederik Jelinek. 2004. Random forests in
language modeling. In Proceedings of EMNLP?2004,
pages 325?332, Barcelona, Spain.
788
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933?943,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Assessing Phrase-Based Translation Models with Oracle Decoding
Guillaume Wisniewski and Alexandre Allauzen and Fran?cois Yvon
Univ. Paris Sud ; LIMSI?CNRS
91403 ORSAY CEDEX
France
{wisniews,allauzen,yvon}@limsi.fr
Abstract
Extant Statistical Machine Translation (SMT) sys-
tems are very complex softwares, which embed mul-
tiple layers of heuristics and embark very large num-
bers of numerical parameters. As a result, it is diffi-
cult to analyze output translations and there is a real
need for tools that could help developers to better
understand the various causes of errors.
In this study, we make a step in that direction and
present an attempt to evaluate the quality of the
phrase-based translation model. In order to identify
those translation errors that stem from deficiencies
in the phrase table (PT), we propose to compute the
oracle BLEU-4 score, that is the best score that a
system based on this PT can achieve on a reference
corpus. By casting the computation of the oracle
BLEU-1 as an Integer Linear Programming (ILP)
problem, we show that it is possible to efficiently
compute accurate lower-bounds of this score, and re-
port measures performed on several standard bench-
marks. Various other applications of these oracle de-
coding techniques are also reported and discussed.
1 Phrase-Based Machine Translation
1.1 Principle
A Phrase-Based Translation System (PBTS) consists of a
ruleset and a scoring function (Lopez, 2009). The ruleset,
represented in the phrase table, is a set of phrase1pairs
{(f, e)}, each pair expressing that the source phrase f
can be rewritten (translated) into a target phrase e. Trans-
lation hypotheses are generated by iteratively rewriting
portions of the source sentence as prescribed by the rule-
set, until each source word has been consumed by exactly
one rule. The order of target words in an hypothesis is
uniquely determined by the order in which the rewrite op-
eration are performed. The search space of the translation
model corresponds to the set of all possible sequences of
1Following the usage in statistical machine translation literature, we
use ?phrase? to denote a subsequence of consecutive words.
rules applications. The scoring function aims to rank all
possible translation hypotheses in such a way that the best
one has the highest score.
A PBTS is learned from a parallel corpus in two inde-
pendent steps. In a first step, the corpus is aligned at the
word level, by using alignment tools such as Giza++
(Och and Ney, 2003) and some symmetrisation heuris-
tics; phrases are then extracted by other heuristics (Koehn
et al, 2003) and assigned numerical weights. In the
second step, the parameters of the scoring function are
estimated, typically through Minimum Error Rate train-
ing (Och, 2003).
Translating a sentence amounts to finding the best scor-
ing translation hypothesis in the search space. Because
of the combinatorial nature of this problem, translation
has to rely on heuristic search techniques such as greedy
hill-climbing (Germann, 2003) or variants of best-first
search like multi-stack decoding (Koehn, 2004). More-
over, to reduce the overall complexity of decoding, the
search space is typically pruned using simple heuristics.
For instance, the state-of-the-art phrase-based decoder
Moses (Koehn et al, 2007) considers only a restricted
number of translations for each source sequence2 and en-
forces a distortion limit3 over which phrases can be re-
ordered. As a consequence, the best translation hypothe-
sis returned by the decoder is not always the one with the
highest score.
1.2 Typology of PBTS Errors
Analyzing the errors of a SMT system is not an easy task,
because of the number of models that are combined, the
size of these models, and the high complexity of the vari-
ous decision making processes. For a SMT system, three
different kinds of errors can be distinguished (Germann
et al, 2004; Auli et al, 2009): search errors, induction
errors and model errors. The former corresponds to cases
where the hypothesis with the best score is missed by
the search procedure, either because of the use of an ap-
2the ttl option of Moses, defaulting to 20.
3the dl option of Moses, whose default value is 7.
933
proximate search method or because of the restrictions of
the search space. Induction errors correspond to cases
where, given the model, the search space does not contain
the reference. Finally, model errors correspond to cases
where the hypothesis with the highest score is not the best
translation according to the evaluation metric.
Model errors encompass several types of errors that oc-
cur during learning (Bottou and Bousquet, 2008)4. Ap-
proximation errors are errors caused by the use of a re-
stricted and oversimplistic class of functions (here, finite-
state transducers to model the generation of hypotheses
and a linear scoring function to discriminate them) to
model the translation process. Estimation errors corre-
spond to the use of sub-optimal values for both the phrase
pairs weights and the parameters of the scoring function.
The reasons behind these errors are twofold: first, train-
ing only considers a finite sample of data; second, it re-
lies on error prone alignments. As a result, some ?good?
phrases are extracted with a small weight, or, in the limit,
are not extracted at all; and conversely that some ?poor?
phrases are inserted into the phrase table, sometimes with
a really optimistic score.
Sorting out and assessing the impact of these various
causes of errors is of primary interest for SMT system
developers: for lack of such diagnoses, it is difficult to
figure out which components of the system require the
most urgent attention. Diagnoses are however, given the
tight intertwining among the various component of a sys-
tem, very difficult to obtain: most evaluations are limited
to the computation of global scores and usually do not
imply any kind of failure analysis.
1.3 Contribution and organization
To systematically assess the impact of the multiple
heuristic decisions made during training and decoding,
we propose, following (Dreyer et al, 2007; Auli et al,
2009), to work out oracle scores, that is to evaluate the
best achievable performances of a PBTS. We aim at both
studying the expressive power of PBTS and at providing
tools for identifying and quantifying causes of failure.
Under standard metrics such as BLEU (Papineni et al,
2002), oracle scores are difficult (if not impossible) to
compute, but, by casting the computation of the oracle
unigram recall and precision as an Integer Linear Pro-
gramming (ILP) problem, we show that it is possible to
efficiently compute accurate lower-bounds of the oracle
BLEU-4 scores and report measurements performed on
several standard benchmarks.
The main contributions of this paper are twofold. We
first introduce an ILP program able to efficiently find
the best hypothesis a PBTS can achieve. This program
can be easily extended to test various improvements to
4We omit here optimization errors.
phrase-base systems or to evaluate the impact of differ-
ent parameter settings. Second, we present a number of
complementary results illustrating the usage of our or-
acle decoder for identifying and analyzing PBTS errors.
Our experimental results confirm the main conclusions of
(Turchi et al, 2008), showing that extant PBTs have the
potential to generate hypotheses having very high BLEU-
4 score and that their main bottleneck is their scoring
function.
The rest of this paper is organized as follows: in Sec-
tion 2, we introduce and formalize the oracle decoding
problem, and present a series of ILP problems of increas-
ing complexity designed so as to deliver accurate lower-
bounds of oracle score. This section closes with various
extensions allowing to model supplementary constraints,
most notably reordering constraints (Section 2.5). Our
experiments are reported in Section 3, where we first in-
troduce the training and test corpora, along with a de-
scription of our system building pipeline (Section 3.1).
We then discuss the baseline oracle BLEU scores (Sec-
tion 3.2), analyze the non-reachable parts of the reference
translations, and comment several complementary results
which allow to identify causes of failures. Section 4 dis-
cuss our approach and findings with respect to the exist-
ing literature on error analysis and oracle decoding. We
conclude and discuss further prospects in Section 5.
2 Oracle Decoder
2.1 The Oracle Decoding Problem
Definition To get some insights on the errors of phrase-
based systems and better understand their limits, we pro-
pose to consider the oracle decoding problem defined as
follows: given a source sentence, its reference transla-
tion5 and a phrase table, what is the ?best? translation
hypothesis a system can generate? As usual, the quality
of an hypothesis is evaluated by the similarity between
the reference and the hypothesis. Note that in the ora-
cle decoding problem, we are only assessing the ability
of PBT systems to generate good candidate translations,
irrespective of their ability to score them properly.
We believe that studying this problem is interesting for
various reasons. First, as described in Section 3.4, com-
paring the best hypothesis a system could have gener-
ated and the hypothesis it actually generates allows us to
carry on both quantitative and qualitative failure analysis.
The oracle decoding problem can also be used to assess
the expressive power of phrase-based systems (Auli et
al., 2009). Other applications include computing accept-
able pseudo-references for discriminative training (Till-
mann and Zhang, 2006; Liang et al, 2006; Arun and
5The oracle decoding problem can be extended to the case of multi-
ple references. For the sake of simplicity, we only describe the case of
a single reference.
934
Koehn, 2007) or combining machine translation systems
in a multi-source setting (Li and Khudanpur, 2009). We
have also used oracle decoding to identify erroneous or
difficult to translate references (Section 3.3).
Evaluation Measure To fully define the oracle de-
coding problem, a measure of the similarity between a
translation hypothesis and its reference translation has
to be chosen. The most obvious choice is the BLEU-4
score (Papineni et al, 2002) used in most machine trans-
lation evaluations.
However, using this metric in the oracle decoding
problem raises several issues. First, BLEU-4 is a met-
ric defined at the corpus level and is hard to interpret at
the sentence level. More importantly, BLEU-4 is not de-
composable6: as it relies on 4-grams statistics, the con-
tribution of each phrase pair to the global score depends
on the translation of the previous and following phrases
and can not be evaluated in isolation. Because of its non-
decomposability, maximizing BLEU-4 is hard; in partic-
ular, the phrase-level decomposability of the evaluation
metric is necessary in our approach.
To circumvent this difficulty, we propose to evaluate
the similarity between a translation hypothesis and a ref-
erence by the number of their common words. This
amounts to evaluating translation quality in terms of un-
igram precision and recall, which are highly correlated
with human judgements (Lavie et al, ). This measure
is closely related to the BLEU-1 evaluation metric and
the Meteor (Banerjee and Lavie, 2005) metric (when it is
evaluated without considering near-matches and the dis-
tortion penalty). We also believe that hypotheses that
maximize the unigram precision and recall at the sen-
tence level yield corpus level BLEU-4 scores close the
maximal achievable. Indeed, in the setting we will intro-
duce in the next section, BLEU-1 and BLEU-4 are highly
correlated: as all correct words of the hypothesis will be
compelled to be at their correct position, any hypothesis
with a high 1-gram precision is also bound to have a high
2-gram precision, etc.
2.2 Formalizing the Oracle Decoding Problem
The oracle decoding problem has already been consid-
ered in the case of word-based models, in which all trans-
lation units are bound to contain only one word. The
problem can then be solved by a bipartite graph matching
algorithm (Leusch et al, 2008): given a n?m binary ma-
trix describing possible translation links between source
words and target words7, this algorithm finds the subset
of links maximizing the number of words of the reference
that have been translated, while ensuring that each word
6Neither at the sentence (Chiang et al, 2008), nor at the phrase level.
7The (i, j) entry of the matrix is 1 if the ith word of the source can
be translated by the jth word of the reference, 0 otherwise.
is translated only once.
Generalizing this approach to phrase-based systems
amounts to solving the following problem: given a set
of possible translation links between potential phrases of
the source and of the target, find the subset of links so that
the unigram precision and recall are the highest possible.
The corresponding oracle hypothesis can then be easily
generated by selecting the target phrases that are aligned
with one source phrase, disregarding the others. In ad-
dition, to mimic the way OOVs are usually handled, we
match identical OOV tokens appearing both in the source
and target sentences. In this approach, the unigram pre-
cision is always one (every word generated in the oracle
hypothesis matches exactly one word in the reference).
As a consequence, to find the oracle hypothesis, we just
have to maximize the recall, that is the number of words
appearing both in the hypothesis and in the reference.
Considering phrases instead of isolated words has a
major impact on the computational complexity: in this
new setting, the optimal segmentations in phrases of both
the source and of the target have to be worked out in ad-
dition to links selection. Moreover, constraints have to
be taken into account so as to enforce a proper segmenta-
tion of the source and target sentences. These constraints
make it impossible to use the approach of (Leusch et al,
2008) and concur in making the oracle decoding prob-
lem for phrase-based models more complex than it is for
word-based models: it can be proven, using arguments
borrowed from (De Nero and Klein, 2008), that this prob-
lem is NP-hard even for the simple unigram precision
measure.
2.3 An Integer Program for Oracle Decoding
To solve the combinatorial problem introduced in the pre-
vious section, we propose to cast it into an Integer Lin-
ear Programming (ILP) problem, for which many generic
solvers exist. ILP has already been used in SMT to find
the optimal translation for word-based (Germann et al,
2001) and to study the complexity of learning phrase
alignments (De Nero and Klein, 2008) models. Follow-
ing the latter reference, we introduce the following vari-
ables: fi,j (resp. ek,l) is a binary indicator variable that
is true when the phrase contains all spans from between-
word position i to j (resp. k to l) of the source (resp.
target) sentence. We also introduce a binary variable, de-
noted ai,j,k,l, to describe a possible link between source
phrase fi,j and target phrase ek,l. These variables are
built from the entries of the phrase table according to se-
lection strategies introduced in Section 2.4. In the fol-
lowing, index variables are so that:
0 ? i < j ? n, in the source sentence and
0 ? k < l ? m, in the target sentence,
935
where n (resp. m) is the length of the source (resp. target)
sentence.
Solving the oracle decoding problem then amounts to
optimizing the following objective function:
max
i,j,k,l
?
i,j,k,l
ai,j,k,l ? (l ? k) , (1)
under the constraints:
?x ? J1,mK :
?
k,l s.t. k?x?l
ek,l ? 1 (2)
?y ? J1, nK :
?
i,j s.t. i?y?j
fi,j = 1 (3)
?k, l :
?
i,j
ai,j,k,l = fk,l (4)
?i, j :
?
k,l
ai,j,k,l = ei,j (5)
The objective function (1) corresponds to the number
of target words that are generated. The first set of con-
straints (2) ensures that each word in the reference e ap-
pears in no more than one phrase. Maximizing the objec-
tive under these constraints amounts to maximizing the
unigram recall. The second set of constraints (3) ensures
that each word in the source f is translated exactly once,
which guarantees that the search space of the ILP prob-
lem is the same as the search space of a phrase-based sys-
tem. Constraints (4) bind the fk,l and ai,j,k,l variables,
ensuring that whenever a link ai,j,k,l is active, the corre-
sponding phrase fk,l is also active. Constraints (5) play a
similar role for the reference.
The Relaxed Problem Even though it accurately
models the search space of a phrase-based decoder,
this programs is not really useful as is: due to out-of-
vocabulary words or missing entries in the phrase table,
the constraint that all source words should be translated
yields infeasible problems8. We propose to relax this
problem and allow some source words to remain untrans-
lated. This is done by replacing constraints (3) by:
?y ? J1, nK :
?
i,j s.t. i?y?j
fi,j ? 1
To better reflect the behavior of phrase-based decoders,
which attempt to translate all source words, we also need
to modify the objective function as follows:
?
i,j,k,l
ai,j,k,l ? (l ? k) +
?
i,j
fi,j ? (j ? i) (6)
The second term in this new objective ensures that opti-
mal solutions translate as many source words as possible.
8An ILP problem is said to be infeasible when every possible solu-
tion violates at least one constraint.
The Relaxed-Distortion Problem A last caveat
with the Relaxed optimization program is caused by
frequently occurring source tokens, such as function
words or punctuation signs, which can often align with
more than one target word. For lack of taking distor-
tion information into account in our objective function,
all these alignments are deemed equivalent, even if some
of them are clearly more satisfactory than others. This
situation is illustrated on Figure 1.
le chat et le chien
the cat and the dog
Figure 1: Equivalent alignments between ?le? and ?the?. The
dashed lines corresponds to a less interpretable solution.
To overcome this difficulty, we propose a last change
to the objective function:
?
i,j,k,l
ai,j,k,l ? (l ? k) +
?
i,j
fi,j ? (j ? i)
??
?
i,j,k,l
ai,j,k,l|k ? i| (7)
Compared to the objective function of the relaxed prob-
lem (6), we introduce here a supplementary penalty factor
which favors monotonous alignments. For each phrase
pair, the higher the difference between source and target
positions, the higher this penalty. If ? is small enough,
this extra term allows us to select, among all the opti-
mal alignments of the relaxed problem, the one with
the lowest distortion. In our experiments, we set ? to
min {n,m} to ensure that the penalty factor is always
smaller than the reward for aligning two single words.
2.4 Selecting Indicator Variables
In the approach introduced in the previous sections, the
oracle decoding problem is solved by selecting, among
a set of possible translation links, the ones that yield the
solution with the highest unigram recall.
We propose two strategies to build this set of possible
translation links. In the first one, denoted exact match,
an indicator ai,j,k,l is created if there is an entry (f, e) so
that f spans from word position i to j in the source and
e from word position k to l in the target. In this strat-
egy, the ILP program considers exactly the same ruleset
as conventional phrase-based decoders.
We also consider an alternative strategy, which could
help us to identify errors made during the phrase extrac-
tion process. In this strategy, denoted inside match, an
indicator ai,j,k,l is created when the following three cri-
teria are met: i) f spans from position i to j of the source;
ii) a substring of e, denoted e?, spans from position k to l
936
of the reference; iii) (f, e?) is not an entry of the phrase ta-
ble. The resulting set of indicator variables thus contains,
at least, all the variables used in the exact match strategy.
In addition, we license here the use of phrases containing
words that do not occur in the reference. In fact, using
such solutions can yield higher BLEU scores when the
reward for additional correct matches exceeds the cost
incurred by wrong predictions. These cases are symp-
toms of situations where the extraction heuristic failed to
extract potentially useful subphrases.
2.5 Oracle Decoding with Reordering Constraints
The ILP problem introduced in the previous section can
be extended in several ways to describe and test various
improvements to phrase-based systems or to evaluate the
impact of different parameter settings. This flexibility
mainly stems from the possibility offered by our frame-
work to express arbitrary constraints over variables. In
this section, we illustrate these possibilities by describing
how reordering constraints can easily be considered.
As a first example, the Moses decoder uses a distortion
limit to constrain the set of possible reorderings. This
constraint ?enforces (...) that the last word of a phrase
chosen for translation cannot be more than d9 words from
the leftmost untranslated word in the source? (Lopez,
2009) and is expressed as:
?aijkl, ai?j?k?l? s.t. k > k
?,
aijkl ? ai?j?k?l? ? |j ? i
? + 1| ? d,
The maximum distortion limit strategy (Lopez, 2009) is
also easily expressed and take the following form (assum-
ing this constraint is parameterized by d):
?l < m? 1,
ai,j,k,l?ai?,j?,l+1,l? ? |i
? ? j ? 1| < d
Implementing the ?local? or MJ-d (Kumar and Byrne,
2005) reordering strategy is also straightforward, and im-
plies using the following constraints:
?i, k,
?
?
?
?
?
?
?
i??i
ai?,j?,k?,l? ?
?
k??k
ai?,j?,k?,l?
?
?
?
?
?
?
? d
Similarly, It is possible to simulate decoding under the
so-called IBM(d) reordering constraints10 by considering
the following constraints:
?? ? m, max
i,k,l
j??
ai,j,k,l ? j ?
?
i,j,k,l
ai,j,k,l ? (j ? i) ? d
9This corresponds to the dl parameter of Moses
10Under IBM(d) constraints, the translation is done, phrase by phrase,
from the beginning of the sentence until the end and only one of the first
d untranslated phrase can be selected for translation.
In these constraints, the first factor corresponds to the
rightmost translated word of the source and the second
one to the number of translated source words. The con-
straints simply enforce that, at each step of the decoding,
there are no more than d source words that were skipped.
Note that the constraints introduced above are not all
linear in the problem variables; however they can eas-
ily be linearized using standard ILP techniques (Roth and
Yih, 2005).
3 Oracle Decoding for Failure Analysis
3.1 Experimental Setting
We propose to use our oracle decoder to study the ability
of a PBTS to translate from English to French and from
German to English. These two languages pairs present
different challenges: English to French translation is con-
sidered a relatively easy pair, notwithstanding the diffi-
culties of generating the right inflection marks in French.
Translating from German into English is more difficult,
notably due to the productivity of inflectional and com-
pounding processes in German, and also to significant
differences in word ordering between these languages.
Our experiments are based on the corpora distributed
for the WMT?09 constrained tasks (Callison-Burch et
al., 2009). All data are tokenized, cleaned and con-
verted to lowercase letters using the tools provided
by the organizers. We then used a standard training
pipeline to construct the translation model: the bitexts
were aligned using Giza++11, symmetrized using the
grow-diag-final-and heuristic; the phrase table
was extracted and scored using the tools distributed with
Moses.12 Finally, baseline systems were optimized using
WMT?08 test set as development using MERT. Note that,
for all these steps, we used the default value of the var-
ious parameters. The extracted phrase table is then used
to find the oracle alignment on the task test set. Recall
that oracle decoding do not use the scores estimated by
Moses in any way.
In the experiments reported below, two settings are
considered. In the first one, denoted NEWSCO, Moses
was trained only on a small data set taken from the News
Commentary corpus. Using a small sized corpus reduces
both training time and decoding time, which allows us to
quickly test different configurations of the decoder. In a
second setting, denoted EUROPARL, Moses was trained
on a larger corpora containing the entirety of the Europarl
Corpus, but no in-domain data, to provide results on more
realistic conditions. Statistics regarding the different cor-
pora used are reported in Table 1. These statistics are
computed on the lowercase cleaned corpora.
11http://www.fjoch.com/GIZA++.html
12http://statmt.org/moses
937
en ? fr de ? en
NEWSCO EUROPARL NEWSCO EUROPARL
#words 1, 023, 401 21, 616, 114 1, 530, 693 22, 898, 644
#sentences 51, 375 1, 050, 398 71, 691 1, 118, 399
#vocabulary 31, 416 78, 071 78, 140 242, 219
#phrase table 3, 061, 701 46, 003, 525 4, 133, 190 44, 402, 367
% OOV 5.3% 3.1% 8.0% 5.2%
Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and
percentage of test words not appearing in the train set (OOV).
Finding the oracle alignment amounts to solving the
ILP problems introduced above. Even though ILP prob-
lems are NP-hard in general, there exist several off-the-
shelf ILP solvers able to efficiently find an optimal solu-
tion or decide that the problem is infeasible. In our exper-
iments, we used the free solver SCIP (Achterberg, 2007).
An optimal solution was found for all problems we con-
sidered. Decoding the 3, 027 sentences of WMT?09 test
set takes about 10 minutes (wall time) for the NEWSCO
setting, and several hours for the EUROPARL setting13.
3.2 Oracle BLEU Score
Table 2 reports, for all considered settings, the BLEU-4
scores14 achieved by our oracle decoder, as well as the
number of source words used to generate the oracle hy-
pothesis and the number of target words that are reach-
able. In these experiments, two objective functions were
considered: first, we only consider the objective function
corresponding to the relaxed problem defined by Eq. (6);
second, we introduced an extra term in the objective to
penalize distortion, as described by Eq. (7). Unless ex-
plicitly stated otherwise, we always used the exact match
strategy.
The main result in 2 is that, for the two language pairs
considered, the expressive power of PBTS is not the lim-
iting factor to achieve high translation performance. In
fact, for most sentences in the test set, excellent oracle
hypotheses, which contain a very high proportion of ref-
erence words, are found. This remains true even when the
phrase table is extracted from a small corpus. Given that
the best BLEU-4 scores achieved during the WMT?09
evaluation are about 28 for the English to French task
and 24 for the German to English task ((Callison-Burch
et al, 2009), Tables 26 and 25), these results strongly
suggest that the main bottleneck of current phrase-based
translation systems is their scoring function rather than
their expressive power. As we will discuss in Section 4,
similar conclusions were drawn by (Auli et al, 2009) and
(Turchi et al, 2008).
Several additional comments on these numbers are in
13All our experiments are run on a 8 cores computer, each core being
a 2.2GHz Intel Processor; the decoder is multi-threaded.
14These are computed on lowercase with the default tokenization.
order. Despite these very high BLEU scores, in most
cases, the reference is only partly translated. In the most
favorable case, for the English to French EUROPARL set-
ting, only 26% of the references could be fully gener-
ated15. These numbers are consistent with the results re-
ported in (Auli et al, 2009). Similarly, only about 31%
of the source sentences are completely translated by the
oracle decoder, which supports our choice to consider a
relaxed version of the ILP problem. Finally, Table 2 also
shows that introducing the distortion penalty does not af-
fect the oracle performance of the decoder.
Considering the inside match strategy improves the
performance of the oracle decoder: for instance, for the
English to French NEWSCO setting, oracle decoder with
the inside match strategy achieves a BLEU-4 score of
70.15 (a 2.5 points improvement over the baseline). To
achieve this score, 21.45% of the phrases used during de-
coding were phrases that are not considered by the exact
match strategy. Similar results can be observed for other
settings, which highlights the significance of one kind of
failure of the extraction heuristic: useful ?subphrases? of
actual phrase pairs are not always extracted.
The numbers in Table 2, no matter how good they may
look, should be considered with caution: they only imply
that, for most test sentences, all the information necessary
to produce a good translation is available in the phrase ta-
ble. However, the alignment decisions underlying these
oracle hypotheses are sometimes hard to justify, and one
has to accept that part of these good hypotheses transla-
tions are due to a series of lucky alignment errors. This
is illustrated on Figure 2, which displays one such lucky
oracle alignment based on the misalignment, during train-
ing, of the French preposition ?des? (of the) with the En-
glish noun ?stock?. Such lucky errors are naturally also
observed in the outputs of conventional decoders, even
though phrase table filtering heuristics probably makes
them somewhat more rare.
3.3 Analyzing Non-Reachable Parts of a Reference
Table 3 contains typical examples of sentence pairs that
could not be fully generated by our oracle decoder. They
15Similar numbers were obtained, albeit much more slowly, with the
--constraint option of Moses.
938
training set objective function % source translated % target generated 4-BLEU
en ? fr
NEWSCO
RELAXED 86.04% 84.74% 67.65
RELAXED-DISTORTION 85.99% 84.77% 67.77
EUROPARL
RELAXED 93.66% 93.06% 85.05
RELAXED-DISTORTION 93.65% 93.06% 85.08
de ? en
NEWSCO
RELAXED 82.57% 82.33% 64.60
RELAXED-DISTORTION 82.59% 82.30% 64.65
EUROPARL
RELAXED 90.34% 91.16% 81.77
RELAXED-DISTORTION 90.36% 91.12% 81.77
Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1
stock fall in asia
chute des actions en asie
Figure 2: Example of alignment obtained by our oracle decoder
illustrate the three main reasons which cause some parts
of the reference to remain unreachable:
? phrases are missing from the phrase table, either
because they do not occur in the training corpus
(OOVs) or because they failed to be extracted. In
Table 3, OOV errors are mainly due to past tense
forms translated into verbs conjugated in passe? sim-
ple (?rejeta?, ?rencontre`rent?, ?renoua?) a French
literary tense, mostly used in formal writings.
? obvious errors (misspelled words, misinterpretation
or mistranslation, ...) in the reference. The refer-
ence of the fifth example contains one such error:
the state name ?Nevada? is translated to ?n?e?vadiez?
(literally ?have not escaped?), yielding a very poor
reference sentence.
? parts of the reference have no translation equiva-
lence in the source. This can be either because ref-
erences are produced in ?context? and some pieces
of information are moved across sentence bound-
aries or because these references are non-literal. The
fourth example, which seems to be the translation of
a title, falls into this category: the French part con-
tains a reference to the context (?les SA? is referring
to the bacteria the text is talking about) which is not
in the source text. Non-literal translation are illus-
trated by the third example, where English ?Mon-
day? is translated into French ?la veille? (the day
before).
While the first kind of errors is inherent to the use of
a statistical approach, the last two kinds result from the
quality of the data used in the evaluation and directly im-
pact both training and evaluation of automatic translation
systems: if they should not distort too much comparisons
of MT systems, these errors prevent us from assessing
the ?global? quality of automatic translation and, if sim-
ilar errors are found in the train set, they make learning
harder as some probability mass is wasted to model them.
To provide a more quantitative analysis, we manually
looked at all the non-aligned parts of some WMT?09 ref-
erences and found that out of 800 references, more than
133 contain either an obvious translation error or can not
be achieved by a PBTS16. Note that, while identifying
these errors could be done in many ways, our oracle de-
coder makes it far easier.
3.4 Identifying Causes of Failure
By comparing the hypotheses found by the oracle de-
coder and the ones found by the phrase-based decoder,
causes of failure can be easily identified. In this section,
we will present several measures that allow us to identify
and quantify several causes of failure.
Errors Caused by Search Space Pruning Recall from
Section 1.1 that Moses uses several heuristics to prune the
search space. In particular, there is a distortion limit and
a limit on the number of target phrases considered for one
source phrase. In this paragraph, we evaluate the impact
of these two heuristics on translation quality.
Table 4 presents the average distortion computed on
the oracle hypotheses, as well as the percentage of
phrases used that have a distortion strictly greater than
6 (the default distortion limit of Moses). All these num-
bers are obtained by solving the RELAXED-DISTORTION
problem. Surprisingly enough, the average distortion of
oracle hypotheses is quite small, even for the German to
English task, and the distortion constraint seems to be vi-
olated only in a few cases. It also appears that the distor-
tion of the hypotheses generated in the NEWSCO setting
is significantly larger than in the EUROPARL setting. This
can be explained by the extra degrees of freedom in the
16Annotation at a finer level is an on-going effort; the annotated
corpus is available from http://www.limsi.fr/Individu/
wisniews/oracle decoding.
939
? ? On Monday the American House of Representatives rejected the plan to support the financial
system, into which up to 700 billion dollars (nearly 12 billion Czech crowns) was to be invested.
? Lundi, la chambre des repre?sentants ame?ricaine rejeta le projet de soutient du syste`me financier,
auquel elle aurait du? consacrer jusqu?a` 700 milliards de dollars (pre`s de 12 bilions de kc?).
? ? Representatives of the legislators met with American Finance Minister Henry Paulson Saturday
night in order to give the government fund a final form.
? Dans la nuit de samedi a` dimanche, des repre?sentants des le?gislateurs rencontre`rent le ministre
des finances ame?ricain Henry Paulson, afin de donner au fond du gouvernement une forme finale.
? ? The Prague Stock Market immediately continued its fall from Monday at the beginning of
Tuesday?s trading , when it dropped by nearly six percent.
? Mardi, de`s le de?but des e?changes, la bourse de prague renoua avec sa chute de la veille,
lorsqu?elle perdait presque six pour cent.
? ? Antibiotic Resistance
? Les SA re?sistent aux antibiotiques.
? ? According to Nevada Democratic senator Harry Reid, that is how that legislators are trying to
have Congress to reach a definitive agreement as early as on Sunday.
? D?apre`s le se?nateur de`mocrate n?e?vadiez Harry Reid, les le?gislateurs font de sorte que le Congre`s
aboutisse a` un accord de?finitif de`s dimanche.
Table 3: Output examples of our oracle decoder on the English to French task. Words in bold are non-aligned words and words in
italic are non-aligned out-of-vocabulary words. For clarity the examples have been detokenized and recased.
training set avg.
distortion
%phrases
with a dist.
> 6
en ? fr
NEWSCO 4.57 22.02%
EUROPARL 3.21 13.32%
de ? en
NEWSCO 5.16 25.37%
EUROPARL 3.81 17.21%
Table 4: Average distortion and percentage of phrases with a
distortion greater that Moses default distortion limit.
alignment decisions enabled by the use of larger training
corpora and phrase table.
To evaluate the impact of the second heuristic, we com-
puted the number of phrases discarded by Moses (be-
cause of the default ttl limit) but used in the oracle hy-
potheses. In the English to French NEWSCO setting,
they account for 34.11% of the total number of phrases
used in the oracle hypotheses. When the oracle decoder
is constrained to use the same phrase table as Moses, its
BLEU-4 score drops to 42.78. This shows that filtering
the phrase table prior to decoding discards many useful
phrase pairs and is seriously limiting the best achievable
performance, a conclusion shared with (Auli et al, 2009).
Search Errors Search errors can be identified by com-
paring the score of the best hypothesis found by Moses
and the score of the oracle hypothesis. If the score of the
oracle hypothesis is higher, then there has been a search
error; on the contrary, there has been an estimation error
when the score of the oracle hypothesis is lower than the
score of the best hypothesis found by Moses.
Based on the comparison of the score of Moses hy-
potheses and of oracle hypotheses for the English to
French NEWSCO setting, our preliminary conclusion is
that the number of search errors is quite limited: only
about 5% of the hypotheses of our oracle decoder are ac-
tually getting a better score than Moses solutions. Again,
this shows that the scoring function (model error) is
one of the main bottleneck of current PBTS. Compar-
ing these hypotheses is nonetheless quite revealing: while
Moses mostly selects phrase pairs with high translation
scores and generates monotonous alignments, our ILP de-
coder uses larger reorderings and less probable phrases
to achieve better solutions: on average, the reordering
score of oracle solutions is ?5.74, compared to ?76.78
for Moses outputs. Given the weight assigned through
MERT training to the distortion score, no wonder that
these hypotheses are severely penalized.
The Impact of Phrase Length The observed outputs
do not only depend on decisions made during the search,
but also on decisions made during training. One such
decision is the specification of maximal length for the
source and target phrases. In our framework, evaluating
the impact of this decision is simple: it suffices to change
the definition of indicator variables so as to consider only
alignments between phrases of a given length.
In the English-French NEWSCO setting, the most re-
strictive choice, when only alignments between single
words are authorized, yields an oracle BLEU-4 of 48.68;
however, authorizing phrases up to length 2 allows to
achieve an oracle value of 66.57, very close to the score
achieved when considering all extracted phrases (67.77).
940
This is corroborated with a further analysis of our ora-
cle alignments, which use phrases whose average source
length is 1.21 words (respectively 1.31 for target words).
If many studies have already acknowledged the predomi-
nance of ?small? phrases in actual translations, our oracle
scores suggest that, for this language pair, increasing the
phrase length limit beyond 2 or 3 might be a waste of
computational resources.
4 Related Work
To the best of our knowledge, there are only a few works
that try to study the expressive power of phrase-based ma-
chine translation systems or to provide tools for analyzing
potential causes of failure.
The approach described in (Auli et al, 2009) is very
similar to ours: in this study, the authors propose to find
and analyze the limits of machine translation systems by
studying the reference reachability. A reference is reach-
able for a given system if it can be exactly generated
by this system. Reference reachability is assessed using
Moses in forced decoding mode: during search, all hy-
potheses that deviate from the reference are simply dis-
carded. Even though the main goal of this study was to
compare the search space of phrase-based and hierarchi-
cal systems, it also provides some insights on the impact
of various search parameters in Moses, delivering con-
clusions that are consistent with our main results. As de-
scribed in Section 1.2, these authors also propose a typol-
ogy of the errors of a statistical translation systems, but
do not attempt to provide methods for identifying them.
The authors of (Turchi et al, 2008) study the learn-
ing capabilities of Moses by extensively analyzing learn-
ing curves representing the translation performances as a
function of the number of examples, and by corrupting
the model parameters. Even though their focus is more
on assessing the scoring function, they reach conclusions
similar to ours: the current bottleneck of translation per-
formances is not the representation power of the PBTS
but rather in their scoring functions.
Oracle decoding is useful to compute reachable
pseudo-references in the context of discriminative train-
ing. This is the main motivation of (Tillmann and Zhang,
2006), where the authors compute high BLEU hypothe-
ses by running a conventional decoder so as to maximize
a per-sentence approximation of BLEU-4, under a simple
(local) reordering model.
Oracle decoding has also been used to assess the
limitations induced by various reordering constraints in
(Dreyer et al, 2007). To this end, the authors propose
to use a beam-search based oracle decoder, which com-
putes lower bounds of the best achievable BLEU-4 us-
ing dynamic programming techniques over finite-state
(for so-called local and IBM constraints) or hierarchically
structured (for ITG constraints) sets of hypotheses. Even
though the numbers reported in this study are not directly
comparable with ours17, it seems that our decoder is not
only conceptually much simpler, but also achieves much
more optimistic lower-bounds of the oracle BLEU score.
The approach described in (Li and Khudanpur, 2009) em-
ploys a similar technique, which is to guide a heuristic
search in an hypergraph representing possible translation
hypotheses with n-gram counts matches, which amounts
to decoding with a n-gram model trained on the sole ref-
erence translation. Additional tricks are presented in this
article to speed-up decoding.
Computing oracle BLEU scores is also the subject of
(Zens and Ney, 2005; Leusch et al, 2008), yet with a
different emphasis. These studies are concerned with
finding the best hypotheses in a word graph or in a con-
sensus network, a problem that has various implications
for multi-pass decoding and/or system combination tech-
niques. The former reference describes an exponential
approximate algorithm, while the latter proves the NP-
completeness of this problem and discuss various heuris-
tic approaches. Our problem is somewhat more complex
and using their techniques would require us to built word
graphs containing all the translations induced by arbitrary
segmentations and permutations of the source sentence.
5 Conclusions
In this paper, we have presented a methodology for ana-
lyzing the errors of PBTS, based on the computation of
an approximation of the BLEU-4 oracle score. We have
shown that this approximation could be computed fairly
accurately and efficiently using Integer Linear Program-
ming techniques. Our main result is a confirmation of
the fact that extant PBTS systems are expressive enough
to achieve very high translation performance with respect
to conventional quality measurements. The main efforts
should therefore strive to improve on the way phrases and
hypotheses are scored during training. This gives further
support to attempts aimed at designing context-dependent
scoring functions as in (Stroppa et al, 2007; Gimpel and
Smith, 2008), or at attempts to perform discriminative
training of feature-rich models. (Bangalore et al, 2007).
We have shown that the examination of difficult-to-
translate sentences was an effective way to detect errors
or inconsistencies in the reference translations, making
our approach a potential aid for controlling the quality or
assessing the difficulty of test data. Our experiments have
also highlighted the impact of various parameters.
Various extensions of the baseline ILP program have
been suggested and/or evaluated. In particular, the ILP
formalism lends itself well to expressing various con-
straints that are typically used in conventional PBTS. In
17The best BLEU-4 oracle they achieve on Europarl German to En-
glish is approximately 48; but they considered a smaller version of the
training corpus and the WMT?06 test set.
941
our future work, we aim at using this ILP framework to
systematically assess various search configurations. We
plan to explore how replacing non-reachable references
with high-score pseudo-references can improve discrim-
inative training of PBTS. We are also concerned by de-
termining how tight is our approximation of the BLEU-
4 score is: to this end, we intend to compute the best
BLEU-4 score within the n-best solutions of the oracle
decoding problem.
Acknowledgments
Warm thanks to Houda Bouamor for helping us with the
annotation tool. This work has been partly financed by
OSEO, the French State Agency for Innovation, under
the Quaero program.
References
Tobias Achterberg. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universita?t Berlin.
http://opus.kobv.de/tuberlin/volltexte/
2007/1611/.
Abhishek Arun and Philipp Koehn. 2007. Online learning
methods for discriminative training of phrase based statis-
tical machine translation. In Proc. of MT Summit XI, Copen-
hagen, Denmark.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn.
2009. A systematic analysis of translation model search
spaces. In Proc. of WMT, pages 224?232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved correla-
tion with human judgments. In Proc. of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak.
2007. Statistical machine translation through global lexi-
cal selection and sentence reconstruction. In Proc. of ACL,
pages 152?159, Prague, Czech Republic.
Le?on Bottou and Olivier Bousquet. 2008. The tradeoffs of large
scale learning. In Proc. of NIPS, pages 161?168, Vancouver,
B.C., Canada.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh
Schroeder. 2009. Findings of the 2009 Workshop on Sta-
tistical Machine Translation. In Proc. of WMT, pages 1?28,
Athens, Greece.
David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou
Ng. 2008. Decomposability of translation metrics for
improved evaluation and efficient algorithms. In Proc. of
ECML, pages 610?619, Honolulu, Hawaii.
John De Nero and Dan Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL: HLT, Short Papers,
pages 25?28, Columbus, Ohio.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007.
Comparing reordering constraints for smt using efficient bleu
oracle computation. In NAACL-HLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages 103?
110, Rochester, New York.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proc. of ACL, pages 228?235,
Toulouse, France.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2004. Fast and optimal decoding for
machine translation. Artificial Intelligence, 154(1-2):127?
143.
Ulrich Germann. 2003. Greedy decoding for statistical ma-
chine translation in almost linear time. In Proc. of NAACL,
pages 1?8, Edmonton, Canada.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-side
context for statistical machine translation. In Proc. of WMT,
pages 9?17, Columbus, Ohio.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of NAACL, pages
48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of ACL, demonstration session.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Proc.
of AMTA, pages 115?124, Washington DC.
Shankar Kumar and William Byrne. 2005. Local phrase re-
ordering models for statistical machine translation. In Proc.
of HLT, pages 161?168, Vancouver, Canada.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. The
significance of recall in automatic metrics for MT evaluation.
In In Proc. of AMTA, pages 134?143, Washington DC.
Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008.
Complexity of finding the BLEU-optimal hypothesis in a
confusion network. In Proc. of EMNLP, pages 839?847,
Honolulu, Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction
of oracle-best translations from hypergraphs. In Proc. of
NAACL, pages 9?12, Boulder, Colorado.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL, pages 761?768, Sydney,
Australia.
Adam Lopez. 2009. Translation as weighted deduction. In
Proc. of EACL, pages 532?540, Athens, Greece.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Comput.
Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, ToddWard, andWei-jing Zhu.
2002. Bleu: A method for automatic evaluation of machine
translation. Technical report, Philadelphia, Pennsylvania.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML, pages
737?744, Bonn, Germany.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for smt using context-informed
942
features. In Andy Way and Barbara Gawronska, editors,
Proc. of TMI, pages 231?240, Sko?vde, Sweden.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical mt. In Proc. of ACL,
pages 721?728, Sydney, Australia.
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learn-
ing performance of a machine translation system: a statistical
and computational analysis. In Proc. of WMT, pages 35?43,
Columbus, Ohio.
Richard Zens and Hermann Ney. 2005. Word graphs for sta-
tistical machine translation. In Proc. of the ACL Workshop
on Building and Using Parallel Texts, pages 191?198, Ann
Arbor, Michigan.
943
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39?48,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Continuous Space Translation Models with Neural Networks
Le Hai Son and Alexandre Allauzen and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
rue John von Neumann, 91403 Orsay cedex, France
Firstname.Lastname@limsi.fr
Abstract
The use of conventional maximum likelihood
estimates hinders the performance of existing
phrase-based translation models. For lack of
sufficient training data, most models only con-
sider a small amount of context. As a par-
tial remedy, we explore here several contin-
uous space translation models, where transla-
tion probabilities are estimated using a con-
tinuous representation of translation units in
lieu of standard discrete representations. In
order to handle a large set of translation units,
these representations and the associated esti-
mates are jointly computed using a multi-layer
neural network with a SOUL architecture. In
small scale and large scale English to French
experiments, we show that the resulting mod-
els can effectively be trained and used on top
of a n-gram translation system, delivering sig-
nificant improvements in performance.
1 Introduction
The phrase-based approach to statistical machine
translation (SMT) is based on the following infer-
ence rule, which, given a source sentence s, selects
the target sentence t and the underlying alignment a
maximizing the following term:
P (t,a|s) =
1
Z(s)
exp
( K?
k=1
?kfk(s, t,a)
)
, (1)
where K feature functions (fk) are weighted by a
set of coefficients (?k), and Z is a normalizing fac-
tor. The phrase-based approach differs from other
approaches by the hidden variables of the translation
process: the segmentation of a parallel sentence pair
into phrase pairs and the associated phrase align-
ments.
This formulation was introduced in (Zens et al,
2002) as an extension of the word based mod-
els (Brown et al, 1993), then later motivated within
a discriminative framework (Och and Ney, 2004).
One motivation for integrating more feature func-
tions was to improve the estimation of the translation
model P (t|s), which was initially based on relative
frequencies, thus yielding poor estimates.
This is because the units of phrase-based mod-
els are phrase pairs, made of a source and a tar-
get phrase; such units are viewed as the events of
discrete random variables. The resulting representa-
tions of phrases (or words) thus entirely ignore the
morphological, syntactic and semantic relationships
that exist among those units in both languages. This
lack of structure hinders the generalization power of
the model and reduces its ability to adapt to other
domains. Another consequence is that phrase-based
models usually consider a very restricted context1.
This is a general issue in statistical Natural Lan-
guage Processing (NLP) and many possible reme-
dies have been proposed in the literature, such as,
for instance, using smoothing techniques (Chen and
Goodman, 1996), or working with linguistically en-
riched, or more abstract, representations. In statisti-
cal language modeling, another line of research con-
siders numerical representations, trained automat-
ically through the use of neural network (see eg.
1typically a small number of preceding phrase pairs for the
n-gram based approach (Crego and Marin?o, 2006), or no con-
text at all, for the standard approach of (Koehn et al, 2007).
39
(Collobert et al, 2011)). An influential proposal,
in this respect, is the work of (Bengio et al, 2003)
on continuous space language models. In this ap-
proach, n-gram probabilities are estimated using a
continuous representation of words in lieu of stan-
dard discrete representations. Experimental results,
reported for instance in (Schwenk, 2007) show sig-
nificant improvements in speech recognition appli-
cations. Recently, this model has been extended in
several promising ways (Mikolov et al, 2011; Kuo
et al, 2010; Liu et al, 2011). In the context of SMT,
Schwenk et al (2007) is the first attempt to esti-
mate translation probabilities in a continuous space.
However, because of the proposed neural architec-
ture, the authors only consider a very restricted set
of translation units, and therefore report only a slight
impact on translation performance. The recent pro-
posal of (Le et al, 2011a) seems especially relevant,
as it is able, through the use of class-based models,
to handle arbitrarily large vocabularies and opens the
way to enhanced neural translation models.
In this paper, we explore various neural architec-
tures for translation models and consider three dif-
ferent ways to factor the joint probability P (s, t)
differing by the units (respectively phrase pairs,
phrases or words) that are projected in continuous
spaces. While these decompositions are theoreti-
cally straightforward, they were not considered in
the past because of data sparsity issues and of the
resulting weaknesses of conventional maximum like-
lihood estimates. Our main contribution is then to
show that such joint distributions can be efficiently
computed by neural networks, even for very large
context sizes; and that their use yields significant
performance improvements. These models are eval-
uated in a n-best rescoring step using the framework
of n-gram based systems, within which they inte-
grate easily. Note, however that they could be used
with any phrase-based system.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the n-gram based ap-
proach, and discuss various implementations of this
framework. We then describe, in Section 3, the neu-
ral architecture developed and explain how it can be
made to handle large vocabulary tasks as well as lan-
guage models over bilingual units. We finally re-
port, in Section 4, experimental results obtained on
a large-scale English to French translation task.
2 Variations on the n-gram approach
Even though n-gram translation models can be
integrated within standard phrase-based systems
(Niehues et al, 2011), the n-gram based frame-
work provides a more convenient way to introduce
our work and has also been used to build the base-
line systems used in our experiments. In the n-
gram based approach (Casacuberta and Vidal, 2004;
Marin?o et al, 2006; Crego and Marin?o, 2006), trans-
lation is divided in two steps: a source reordering
step and a translation step. Source reordering is
based on a set of learned rewrite rules that non-
deterministically reorder the input words so as to
match the target order thereby generating a lattice
of possible reorderings. Translation then amounts
to finding the most likely path in this lattice using a
n-gram translation model 2 of bilingual units.
2.1 The standard n-gram translation model
n-gram translation models (TMs) rely on a spe-
cific decomposition of the joint probability P (s, t),
where s is a sequence of I reordered source words
(s1, ..., sI ) and t contains J target words (t1, ..., tJ ).
This sentence pair is further assumed to be de-
composed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing, and ultimately derives from initial word and
phrase alignments. In this framework, the basic
translation units are tuples, which are the analogous
of phrase pairs, and represent a matching u = (s, t)
between a source s and a target t phrase (see Fig-
ure 1). Using the n-gram assumption, the joint prob-
ability of a segmented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
A first issue with this model is that the elementary
units are bilingual pairs, which means that the under-
lying vocabulary, hence the number of parameters,
can be quite large, even for small translation tasks.
Due to data sparsity issues, such models are bound
2Like in the standard phrase-based approach, the translation
process also involves additional feature functions that are pre-
sented below.
40
to face severe estimation problems. Another prob-
lem with (2) is that the source and target sides play
symmetric roles, whereas the source side is known,
and the target side must be predicted.
2.2 A factored n-gram translation model
To overcome some of these issues, the n-gram prob-
ability in equation (2) can be factored by decompos-
ing tuples in two (source and target) parts :
P (ui|ui?1, ..., ui?n+1) =
P (ti|si, si?1, ti?1, ..., si?n+1, ti?n+1)
? P (si|si?1, ti?1..., si?n+1, ti?n+1)
(3)
Decomposition (3) involves two models: the first
term represents a TM, the second term is best viewed
as a reordering model. In this formulation, the TM
only predicts the target phrase, given its source and
target contexts.
Another benefit of this formulation is that the el-
ementary events now correspond either to source or
to target phrases, but never to pairs of such phrases.
The underlying vocabulary is thus obtained as the
union, rather than the cross product, of phrase in-
ventories. Finally note that the n-gram probability
P (ui|ui?1, ..., ui?n+1) could also factor as:
P (si|ti, si?1, ti?1, ..., si?n+1, ti?n+1)
? P (ti|si?1, ti?1, ..., si?n+1, ti?n+1)
(4)
2.3 A word factored translation model
A more radical way to address the data sparsity is-
sues is to take (source and target) words as the basic
units of the n-gram TM. This may seem to be a step
backwards, since the transition from word (Brown et
al., 1993) to phrase-based models (Zens et al, 2002)
is considered as one of the main recent improvement
in MT. One important motivation for considering
phrases rather than words was to capture local con-
text in translation and reordering. It should then be
stressed that the decomposition of phrases in words
is only re-introduced here as a way to mitigate the
parameter estimation problems. Translation units
are still pairs of phrases, derived from a bilingual
segmentation in tuples synchronizing the source and
target n-gram streams, as defined by equation (3).
In fact, the estimation policy described in section 3
will actually allow us to design n-gram models with
longer contexts than is typically possible in the con-
ventional n-gram approach.
Let ski denote the k
th word of source tuple si.
Considering again the example of Figure 1, s111 is
to the source word nobel, s411 is to the source word
paix, and similarly t211 is the target word peace. We
finally denote hn?1(tki ) the sequence made of the
n ? 1 words preceding tki in the target sentence: in
Figure 1, h3(t211) thus refers to the three word con-
text receive the nobel associated with the target word
peace. Using these notations, equation (3) is rewrit-
ten as:
P (s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (5)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words in ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
Likewise, the contribution of the source phrase
s11 =nobel, de, la, paix is:
P
(
nobel|[receive, the], [recevoir,le]
)
? P
(
de|[receive, the], [le,nobel]
)
? P
(
la|[receive, the], [nobel, de]
)
? P
(
paix|[receive, the], [de,la]
)
.
A benefit of this new formulation is that the involved
vocabularies only contain words, and are thus much
smaller. These models are thus less bound to be af-
fected by data sparsity issues. While the TM defined
by equation (5) derives from equation (3), a variation
can be equivalently derived from equation (4).
41
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented in bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
3 The SOUL model
In the previous section, we defined three different
n-gram translation models, based respectively on
equations (2), (3) and (5). As discussed above, a
major issue with such models is to reliably estimate
their parameters, the numbers of which grow expo-
nentially with the order of the model. This problem
is aggravated in natural language processing, due to
well known data sparsity issues. In this work, we
take advantage of the recent proposal of (Le et al,
2011a): using a specific neural network architecture
(the Structured OUtput Layer model), it becomes
possible to handle large vocabulary language mod-
eling tasks, a solution that we adapt here to MT.
3.1 Language modeling in a continuous space
Let V be a finite vocabulary, n-gram language mod-
els (LMs) define distributions over finite sequences
of tokens (typically words) wL1 in V
+ as follows:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
i?n+1) (6)
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains dozens of thousands words.
In spite of the simplifying n-gram assump-
tion, maximum likelihood estimation remains un-
reliable and tends to underestimate the proba-
bility of very rare n-grams. Smoothing tech-
niques, such as Kneser-Ney and Witten-Bell back-
off schemes (see (Chen and Goodman, 1996) for an
empirical overview, and (Teh, 2006) for a Bayesian
interpretation), perform back-off to lower order dis-
tributions, thus providing an estimate for the proba-
bility of these unseen events.
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated estimates are jointly computed
in a multi-layer neural network architecture. Fig-
ure 2 provides a partial representation of this kind
of model and helps figuring out their principles. To
compute the probability P (wi|w
i?1
i?n+1), the n ? 1
context words are projected in the same continu-
ous space using a shared matrix R; these continuous
word representations are then concatenated to build
a single vector that represents the context; after a
non-linear transformation, the probability distribu-
tion is computed using a softmax layer.
The major difficulty with the neural network ap-
proach remains the complexity of inference and
training, which largely depends on the size of the
output vocabulary (i.e. the number of words that
have to be predicted). One practical solution is to re-
strict the output vocabulary to a short-list composed
of the most frequent words (Schwenk, 2007). How-
ever, the usual size of the short-list is under 20k,
which does not seem sufficient to faithfully repre-
sent the translation models of section 2.
3.2 Principles of SOUL
To circumvent this problem, Structured Output
Layer (SOUL) LMs are introduced in (Le et al,
2011a). Following Mnih and Hinton (2008), the
SOULmodel combines the neural network approach
with a class-based LM (Brown et al, 1992). Struc-
42
turing the output layer and using word class informa-
tion makes the estimation of distributions over the
entire vocabulary computationally feasible.
To meet this goal, the output vocabulary is struc-
tured as a clustering tree, where each word belongs
to only one class and its associated sub-classes. If
wi denotes the ith word in a sentence, the sequence
c1:D(wi) = c1, . . . , cD encodes the path for wordwi
in the clustering tree, with D being the depth of the
tree, cd(wi) a class or sub-class assigned to wi, and
cD(wi) being the leaf associated with wi (the word
itself). The probability of wi given its history h can
then be computed as:
P (wi|h) =P (c1(wi)|h)
?
D?
d=2
P (cd(wi)|h, c1:d?1).
(7)
There is a softmax function at each level of the tree
and each word ends up forming its own class (a leaf).
The SOULmodel, represented on Figure 2, is thus
the same as for the standard model up to the output
layer. The main difference lies in the output struc-
ture which involves several layers with a softmax ac-
tivation function. The first (class layer) estimates
the class probability P (c1(wi)|h), while other out-
put sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d?1). Finally, theword layers es-
timate the word probabilities P (cD(wi)|h, c1:D?1).
Words in the short-list remain special, since each of
them represents a (final) class.
Training a SOULmodel can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al, 2003),
this optimization is performed by stochastic back-
propagation. Details of the training procedure can
be found in (Le et al, 2011b).
Neural network architectures are also interesting
as they can easily handle larger contexts than typical
n-grammodels. In the SOUL architecture, enlarging
the context mainly consists in increasing the size of
the projection layer, which corresponds to a simple
look-up operation. Increasing the context length at
the input layer thus only causes a linear growth in
complexity in the worst case (Schwenk, 2007).
0...0100
10...000
0...0010
wi-1
wi-2
wi-3
R
R
R
shared input space
input layer
hidden layers
shortlist
sub-classlayers
wordlayers
classlayer
Figure 2: The architecture of a SOUL Neural Network
language model in the case of a 4-gram model.
3.3 Translation modeling with SOUL
The SOUL architecture was used successfully to
deliver (monolingual) LMs probabilities for speech
recognition (Le et al, 2011a) and machine transla-
tion (Allauzen et al, 2011) applications. In fact,
using this architecture, it is possible to estimate n-
gram distributions for any kind of discrete random
variables, such as a phrase or a tuple. The SOUL ar-
chitecture can thus be readily used as a replacement
for the standard n-gram TM described in section 2.1.
This is because all the random variables are events
over the same set of tuples.
Adopting this architecture for the other n-gram
TM respectively described by equations (3) and (5)
is more tricky, as they involve two different lan-
guages and thus two different vocabularies: the pre-
dicted unit is a target phrase (resp. word), whereas
the context is made of both source and target phrases
(resp. words). A subsequent modification of the
SOUL architecture was thus performed to make up
for ?mixed? contexts: rather than projecting all the
context words or phrases into the same continuous
space (using the matrix R, see Figure 2), we used
two different projection matrices, one for each lan-
guage. The input layer is thus composed of two vec-
tors in two different spaces; these two representa-
tions are then combined through the hidden layer,
the other layers remaining unchanged.
43
4 Experimental Results
We now turn to an experimental comparison of the
models introduced in Section 2. We first describe
the tasks and data that were used, before presenting
our n-gram based system and baseline set-up. Our
results are finally presented and discussed.
Let us first emphasize that the design and inte-
gration of a SOUL model for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two pass approach:
the first pass uses a conventional back-off n-gram
model to produce a k-best list (the k most likely
translations); in the second pass, the probability of
a m-gram SOUL model is computed for each hy-
pothesis, added as a new feature and the k-best list
is accordingly reordered3. In all the following ex-
periments, we used a fixed context size for SOUL of
m = 10, and used k = 300.
4.1 Tasks and corpora
The two tasks considered in our experiments
are adapted from the text translation track of
IWSLT 2011 from English to French (the ?TED?
talk task): a small data scenario where the only
training data is a small in-domain corpus; and a large
scale condition using all the available training data.
In this article, we only provide a short overview of
the task; all the necessary details regarding this eval-
uation campaign are on the official website4.
The in-domain training data consists of 107, 058
sentence pairs, whereas for the large scale task, all
the data available for the WMT 2011 evaluation5 are
added. For the latter task, the available parallel data
includes a large Web corpus, referred to as the Gi-
gaWord parallel corpus. This corpus is very noisy
and is accordingly filtered using a simple perplexity
criterion as explained in (Allauzen et al, 2011). The
total amount of training data is approximately 11.5
million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
As the provided development data was quite small,
3The probability estimated with the SOULmodel is added as
a new feature to the score of an hypothesis given by Equation 1.
The coefficients are retuned before the reranking step.
4iwslt2011.org
5www.statmt.org/wmt11/
Model Vocabulary size
Small task Large task
src trg src trg
Standard 317k 8847k
Phrase factored 96k 131k 4262k 3972k
Word factored 45k 53k 505k 492k
Table 1: Vocabulary sizes for the English to French tasks
obtained with various SOUL translation (TM) models.
For the factored models, sizes are indicated for both
source (src) and target (trg) sides.
development and test set were inverted, and we fi-
nally used a development set of 1,664 sentences, and
a test set of 934 sentences. The table 1 provides the
sizes of the different vocabularies. The n-gram TMs
are estimated over a training corpus composed of tu-
ple sequences. Tuples are extracted from the word-
aligned parallel data (using MGIZA++6 with default
settings) in such a way that a unique segmentation
of the bilingual corpus is achieved, allowing to di-
rectly estimate bilingual n-gram models (see (Crego
and Marin?o, 2006) for details).
4.2 n-gram based translation system
The n-gram based system used here is based on an
open source implementation described in (Crego et
al., 2011). In a nutshell, the TM is implemented as
a stochastic finite-state transducer trained using a n-
gram model of (source, target) pairs as described in
section 2.1. Training this model requires to reorder
source sentences so as to match the target word or-
der. This is performed by a non-deterministic finite-
state reordering model, which uses part-of-speech
information generated by the TreeTagger to gener-
alize reordering patterns beyond lexical regularities.
In addition to the TM, fourteen feature functions
are included: a target-language model; four lexi-
con models; six lexicalized reordering models (Till-
mann, 2004; Crego et al, 2011); a distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model. The four lexicon mod-
els are similar to the ones used in standard phrase-
based systems: two scores correspond to the rela-
tive frequencies of the tuples and two lexical weights
are estimated from the automatically generated word
6geek.kyloo.net/software
44
alignments. The weights associated to feature func-
tions are optimally combined using the Minimum
Error Rate Training (MERT) (Och, 2003). All the
results in BLEU are obtained as an average of 4 op-
timization runs7.
For the small task, the target LM is a standard
4-gram model estimated with the Kneser-Ney dis-
counting scheme interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1996), while for the large task, the target LM is ob-
tained by linear interpolation of several 4-grammod-
els (see (Lavergne et al, 2011) for details). As for
the TM, all the available parallel corpora were sim-
ply pooled together to train a 3-gram model. Results
obtained with this large-scale system were found to
be comparable to some of the best official submis-
sions.
4.3 Small task evaluation
Table 2 summarizes the results obtained with the
baseline and different SOUL models, TMs and a
target LM. The first comparison concerns the stan-
dard n-gram TM, defined by equation (2), when es-
timated conventionally or as a SOULmodel. Adding
the latter model yields a slight BLEU improvement
of 0.5 point over the baseline. When the SOUL TM
is phrased factored as defined in equation (3) the
gain is of 0.9 BLEU point instead. This difference
can be explained by the smaller vocabularies used
in the latter model, and its improved robustness to
data sparsity issues. Additional gains are obtained
with the word factored TM defined by equation (5):
a BLEU improvement of 0.8 point over the phrase
factored TM and of 1.7 point over the baseline are
respectively achieved. We assume that the observed
improvements can be explained by the joint effect of
a better smoothing and a longer context.
The comparison with the condition where we only
use a SOUL target LM is interesting as well. Here,
the use of the word factored TM still yields to a 0.6
BLEU improvement. This result shows that there
is an actual benefit in smoothing the TM estimates,
rather than only focus on the LM estimates.
Table 3 reports a comparison among the dif-
ferent components and variations of the word
7The standard deviations are below 0.1 and thus omitted in
the reported results.
Model BLEU
dev test
Baseline 31.4 25.8
Adding a SOUL model
Standard TM 32.0 26.3
Phrase factored TM 32.7 26.7
Word factored TM 33.6 27.5
Target LM 32.6 26.9
Table 2: Results for the small English to French task ob-
tained with the baseline system and with various SOUL
translation (TM) or target language (LM) models.
Model BLEU
dev test
Adding a SOUL model
+ P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
32.6 26.9
+ P
(
ski |h
n?1(t1i ), h
n?1(ski )
)
32.1 26.2
+ the combination of both 33.2 27.5
+ P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
31.7 26.1
+ P
(
tki |h
n?1(s1i ), h
n?1(tki )
)
32.7 26.8
+ the combination of both 33.4 27.2
Table 3: Comparison of the different components and
variations of the word factored translation model.
factored TM. In the upper part of the table,
the model defined by equation (5) is evaluated
component by component: first the translation
term P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
, then its distor-
tion counterpart P
(
ski |h
n?1(t1i ), h
n?1(ski )
)
and fi-
nally their combination, which yields the joint prob-
ability of the sentence pair. Here, we observe that
the best improvement is obtained with the transla-
tion term, which is 0.7 BLEU point better than the
latter term. Moreover, the use of just a translation
term only yields a BLEU score equal to the one ob-
tained with the SOUL target LM, and its combina-
tion with the distortion term is decisive to attain the
additional gain of 0.6 BLEU point. The lower part of
the table provides the same comparison, but for the
variation of the word factored TM. Besides a similar
trend, we observe that this variation delivers slightly
lower results. This can be explained by the restricted
context used by the translation term which no longer
includes the current source phrase or word.
45
Model BLEU
dev test
Baseline 33.7 28.2
Adding a word factored SOUL TM
+ in-domain TM 35.2 29.4
+ out-of-domain TM 34.8 29.1
+ out-of-domain adapted TM 35.5 29.8
Adding a SOUL LM
+ out-of-domain adapted LM 35.0 29.2
Table 4: Results for the large English to French trans-
lation task obtained by adding various SOUL translation
and language models (see text for details).
4.4 Large task evaluation
For the large-scale setting, the training material in-
creases drastically with the use of the additional out-
of-domain data for the baseline models. Results are
summarized in Table 4. The first observation is the
large increase of BLEU (+2.4 points) for the base-
line system over the small-scale baseline. For this
task, only the word factored TM is evaluated since
it significantly outperforms the others on the small
task (see section 4.3).
In a first scenario, we use a word factored TM,
trained only on the small in-domain corpus. Even
though the training corpus of the baseline TM is one
hundred times larger than this small in-domain data,
adding the SOUL TM still yields a BLEU increase
of 1.2 point8. In a second scenario, we increase the
training corpus for the SOUL, and include parts of
the out-of-domain data (the WMT part). The result-
ing BLEU score is here slightly worse than the one
obtained with just the in-domain TM, yet delivering
improved results with the respect to the baseline.
In a last attempt, we amended the training regime
of the neural network. In a fist step, we trained con-
ventionally a SOUL model using the same out-of-
domain parallel data as before. We then adapted this
model by running five additional epochs of the back-
propagation algorithm using the in-domain data. Us-
ing this adapted model yielded our best results to
date with a BLEU improvement of 1.6 points over
the baseline results. Moreover, the gains obtained
using this simple domain adaptation strategy are re-
8Note that the in-domain data was already included in the
training corpus of the baseline TM.
spectively of +0.4 and +0.8 BLEU, as compared
with the small in-domain model and the large out-
of-domain model. These results show that the SOUL
TM can scale efficiently and that its structure is well
suited for domain adaptation.
5 Related work
To the best of our knowledge, the first work on ma-
chine translation in continuous spaces is (Schwenk
et al, 2007), where the authors introduced the model
referred here to as the the standard n-gram trans-
lation model in Section 2.1. This model is an ex-
tension of the continuous space language model
of (Bengio et al, 2003), the basic unit is the tuple
(or equivalently the phrase pair). The resulting vo-
cabulary being too large to be handled by neural net-
works without a structured output layer, the authors
had thus to restrict the set of the predicted units to a
8k short-list . Moreover, in (Zamora-Martinez et al,
2010), the authors propose a tighter integration of a
continuous space model with a n-gram approach but
only for the target LM.
A different approach, described in (Sarikaya et
al., 2008), divides the problem in two parts: first the
continuous representation is obtained by an adapta-
tion of the Latent Semantic Analysis; then a Gaus-
sian mixture model is learned using this continu-
ous representation and included in a hidden Markov
model. One problem with this approach is the sep-
aration between the training of the continuous rep-
resentation on the one hand, and the training of the
translation model on the other hand. In comparison,
in our approach, the representation and the predic-
tion are learned in a joined fashion.
Other ways to address the data sparsity issues
faced by translation model were also proposed in the
literature. Smoothing is obviously one possibility
(Foster et al, 2006). Another is to use factored lan-
guage models, introduced in (Bilmes and Kirchhoff,
2003), then adapted for translation models in (Koehn
and Hoang, 2007; Crego and Yvon, 2010). Such ap-
proaches require to use external linguistic analysis
tools which are error prone; moreover, they did not
seem to bring clear improvements, even when trans-
lating into morphologically rich languages.
46
6 Conclusion
In this paper, we have presented possible ways to use
a neural network architecture as a translation model.
A first contribution was to produce the first large-
scale neural translation model, implemented here in
the framework of the n-gram based models, tak-
ing advantage of a specific hierarchical architecture
(SOUL). By considering several decompositions of
the joint probability of a sentence pair, several bilin-
gual translation models were presented and dis-
cussed. As it turned out, using a factorization which
clearly distinguishes the source and target sides, and
only involves word probabilities, proved an effec-
tive remedy to data sparsity issues and provided sig-
nificant improvements over the baseline. Moreover,
this approach was also experimented within the sys-
tems we submitted to the shared translation task of
the seventh workshop on statistical machine trans-
lation (WMT 2012). These experimentations in a
large scale setup and for different language pair cor-
roborate the improvements reported in this article.
We also investigated various training regimes for
these models in a cross domain adaptation setting.
Our results show that adapting an out-of-domain
SOUL TM is both an effective and very fast way to
perform bilingual model adaptation. Adding up all
these novelties finally brought us a 1.6 BLEU point
improvement over the baseline. Even though our
experiments were carried out only within the frame-
work of n-gram basedMT systems, using such mod-
els in other systems is straightforward. Future work
will thus aim at introducing them into conventional
phrase-based systems, such as Moses (Koehn et al,
2007). Given that Moses only implicitly uses n-
gram based information, adding SOUL translation
models is expected to be even more helpful.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
References
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL ?03: Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology, pages 4?6.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, pages 1?17.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrase-table smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53?61, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181?184, Detroit, Michigan.
47
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for Arabic speech recognition. In Proc. ICASSP
2010.
Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and
Franc?ois Yvon. 2011. LIMSI?s experiments in do-
main adaptation for IWSLT11. In Proceedings of
the eight International Workshop on Spoken Language
Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Ilya Oparin, Abdel Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857?2860.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Extensions
of recurrent neural network language model. In Proc.
of ICASSP?11, pages 5528?5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider context by using bilingual lan-
guage models in machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 198?206, Edinburgh, Scotland. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449, Decem-
ber.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Ruhi Sarikaya, Yonggang Deng, Mohamed Afify, Brian
Kingsbury, and Yuqing Gao. 2008. Machine trans-
lation in continuous space. In Proceedings of Inter-
speech, pages 2350?2353, Brisbane, Australia.
Holger Schwenk, Marta R. Costa-Jussa`, and Jose? A.R.
Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 430?438, Prague, Czech Re-
public.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104.
Francisco Zamora-Martinez, Maria Jose? Castro-Bleda,
and Holger Schwenk. 2010. N-gram-based Machine
Translation enhanced with Neural Networks for the
French-English BTEC-IWSLT?10 task. In Proceed-
ings of the seventh International Workshop on Spoken
Language Translation (IWSLT), pages 45?52.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
48
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54?59,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?10
Alexandre Allauzen, Josep M. Crego, ?Ilknur Durgar El-Kahlout and Franc?ois Yvon
LIMSI/CNRS and Universite? Paris-Sud 11, France
BP 133, 91403 Orsay Cedex
Firstname.Lastname@limsi.fr
Abstract
This paper describes our Statistical Ma-
chine Translation systems for the WMT10
evaluation, where LIMSI participated for
two language pairs (French-English and
German-English, in both directions). For
German-English, we concentrated on nor-
malizing the German side through a proper
preprocessing, aimed at reducing the lex-
ical redundancy and at splitting complex
compounds. For French-English, we stud-
ied two extensions of our in-house N -code
decoder: firstly, the effect of integrating a
new bilingual reordering model; second,
the use of adaptation techniques for the
translation model. For both set of exper-
iments, we report the improvements ob-
tained on the development and test data.
1 Introduction
LIMSI took part in the WMT 2010 evalua-
tion campaign and developed systems for two
languages pairs: French-English and German-
English in both directions. For German-English,
we focused on preprocessing issues and performed
a series of experiments aimed at normalizing the
German side by removing some of the lexical re-
dundancy and by splitting compounds. For this
pair, all the experiments were performed using the
Moses decoder (Koehn et al, 2007). For French-
English, we studied two extensions of our n-gram
based system: first, the effect of integrating a
new bilingual reordering model; second, the use
of adaptation techniques for the translation model.
Decoding is performed using our in-house N -code
(Marin?o et al, 2006) decoder.
2 System architecture and resources
In this section, we describe the main characteris-
tics of the phrase-based systems developed for this
evaluation and the resources that were used to train
our models. As far as resources go, we used all the
data supplied by the 2010 evaluation organizers.
Based on our previous experiments (De?chelotte et
al., 2008) which have demonstrated that better nor-
malization tools provide better BLEU scores (Pap-
ineni et al, 2002), we took advantage of our in-
house text processing tools for the tokenization
and detokenization steps. Only for German data
did we used the TreeTagger (Schmid, 1994) tok-
enizer. Similar to last year?s experiments, all of
our systems are built in ?true-case?.
3 German-English systems
As German is morphologically more complex than
English, the default policy which consists in treat-
ing each word form independently from the oth-
ers is plagued with data sparsity, which poses a
number of difficulties both at training and de-
coding time. When aligning parallel texts at
the word level, German compound words typi-
cally tend to align with more than one English
word; this, in turn, tends to increase the number
of possible translation counterparts for each En-
glish type, and to make the corresponding align-
ment scores less reliable. In decoding, new com-
pounds or unseen morphological variants of ex-
isting words artificially increase the number out-
of-vocabulary (OOV) forms, which severely hurts
the overall translation quality. Several researchers
have proposed normalization (Niessen and Ney,
2004; Corston-oliver and Gamon, 2004; Goldwa-
ter and McClosky, 2005) and compound splitting
(Koehn and Knight, 2003; Stymne, 2008; Stymne,
2009) methods. Our approach here is similar, yet
uses different implementations; we also studied
the joint effect of combining both techniques.
3.1 Reducing the lexical redundancy
In German, determiners, pronouns, nouns and ad-
jectives carry inflection marks (typically suffixes)
54
Input POS Lemma Analysis
In APPR in APPR.In
der* ART d ART.Def.Dat.Sg.Fem
Folge NN Folge N.Reg.Dat.Sg.Fem
befand VVFIN befinden VFIN.Full.3.Sg.Past.Ind
die* ART d ART.Def.Nom.Sg.Fem
derart ADV derart ADV
gesta?rkte* ADJA gesta?rkt ADJA.Pos.Nom.Sg.Fem
Justiz NN Justiz N.Reg.Nom.Sg.Fem
wiederholt ADJD wiederholt ADJD.Pos
gegen APPR gegen APPR.Acc
die* ART d ART.Def.Acc.Sg.Fem
Regierung NN Regierung N.Reg.Acc.Sg.Fem
und KON und CONJ.Coord.-2
insbesondere ADV insbesondere ADV
gegen APPR gegen APPR.Acc
deren* PDAT d PRO.Dem.Subst.-3.Gen.Sg.Fem
Geheimdienste* NN Geheimdienst N.Reg.Acc.Pl.Masc
. $. . SYM.Pun.Sent
Table 1: TreeTagger and RFTagger outputs. Starred word forms are modified during preprocessing.
so as to satisfy agreement constraints. Inflections
vary according to gender, case, and number infor-
mation. For instance, the German definite deter-
miner could be marked in sixteen different ways
according to the possible combinations of genders
(3), case (4) and number (2)1, which are fused
in six different tokens der, das, die, den, dem,
des. With the exception of the plural and gen-
itive cases, all these words translate to the same
English word: the. In order to reduce the size of
the German vocabulary and to improve the robust-
ness of the alignment probabilities, we considered
various normalization strategies for the different
word classes. In a nutshell, normalizing amounts
to collapsing several German forms of a given
lemma into a unique representative, using manu-
ally written normalization patterns. A pattern typ-
ically specifies which forms of a given morpho-
logical paradigm should be considered equivalent
when translating into English. These normaliza-
tion patterns use the lemma information computed
by the TreeTagger and the fine-grained POS infor-
mation computed by the RFTagger (Schmid and
Laws, 2008), which uses a tagset containing ap-
proximately 800 tags. Table 1 displays the analy-
sis of an example sentence. 2
In most cases, normalization patterns replace a
word form by its lemma; in order to partially pre-
1For the plural forms, gender distinctions are neutralized
and the same 4 forms are used for all genders .
2The English reference: Subsequently , the energized judi-
ciary continued ruling against government decisions , embar-
rassing the government ? especially its intelligence agencies
.
serve some inflection marks, we introduced two
generic suffixes, +s and +en which respectively
denote plural and genitive wherever needed. Typ-
ical normalization rules take the following form:
? For articles, adjectives, and pronouns (Indef-
inite , possessive, demonstrative, relative and
reflexive), if a token has;
? Genitive case: replace with lemma+en
(Ex. des, der, des, der ? d+en)
? Plural number: replace with lemma+s
(Ex. die, den ? d+s)
? All other gender, case and number: re-
place with lemma (Ex. der, die, das, die
? d)
? For nouns;
? Plural number: replace with lemma+s
(Ex. Bilder, Bildern, Bilder ? Bild+s))
? All other gender and case: replace with
lemma (Ex Bild, Bilde, Bildes ? Bild;
Using these tags, a normalized version of previ-
ous sentence is as follows: In d Folge befand d de-
rart gesta?rkt Justiz wiederholt gegen d Regierung
und insbesondere gegen d+en Geheimdienst+s.
Several experiments were carried out to assess the
effect of different normalization schemes. Remov-
ing all gender and case information, except for the
genitive for articles, adjectives and pronouns, al-
lowed to achieve the best BLEU scores.
3.2 Compound Splitting
Combining nouns, verbs and adjectives to forge
new words is a very common process in German.
55
It partly explains the difference between the num-
ber of types and tokens between English and Ger-
man in parallel texts. In most cases, compounds
are formed by a mere concatenation of existing
word forms, and can easily be split into simpler
units. As words are freely conjoined, the vocab-
ulary size increases vastly, yielding to sparse data
problems that turn into unreliable parameter esti-
mates. We used the frequency-based segmenta-
tion algorithm initially introduced in (Koehn and
Knight, 2003) to handle compounding. Our im-
plementation extends this technique to handle the
most common letter fillers at word junctions. In
our experiments, we investigated different split-
ting schemes in a manner similar to the work of
(Stymne, 2008).
4 French-English systems
4.1 Baseline N -coder systems
For this language pair, we used our in-house
N -code system, which implements the n-gram-
based approach to SMT. In a nutshell, the transla-
tion model is implemented as a stochastic finite-
state transducer trained using a n-gram model
of (source,target) pairs (Casacuberta and Vidal,
2004). Training this model requires to reorder
source sentences so as to match the target word
order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, our sys-
tem implements eight feature functions which are
optimally combined using a discriminative train-
ing framework (Och, 2003): a target-language
model; two lexicon models, which give comple-
mentary translation scores for each tuple; two
lexicalized reordering models aiming at predict-
ing the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. One novelty this year
are the introduction of lexicalized reordering mod-
els (Tillmann, 2004). Such models require to
estimate reordering probabilities for each phrase
pairs, typically distinguishing three case, depend-
ing whether the current phrase is translated mono-
tone, swapped or discontiguous with respect to the
3Part-of-speech information for English and French is
computed using the above mentioned TreeTagger.
previous (respectively next phrase pair).
In our implementation, we modified the three
orientation types originally introduced and con-
sider: a consecutive type, where the original
monotone and swap orientations are lumped to-
gether, a forward type, specifying a discontiguous
forward orientation, and a backward type, specify-
ing a discontiguous backward orientation. Empir-
ical results showed that in our case, the new orien-
tations slightly outperform the original ones. This
may be explained by the fact that the model is ap-
plied over tuples instead of phrases.
Counts of these three types are updated for
each unit collected during the training process.
Given these counts, we can learn probability dis-
tributions of the form pr(orientation|(st)) where
orientation ? {c, f, b} (consecutive, forward
and backward) and (st) is a translation unit.
Counts are typically smoothed for the estimation
of the probability distribution.
The overall search process is performed by our
in-house n-code decoder. It implements a beam-
search strategy on top of a dynamic programming
algorithm. Reordering hypotheses are computed
in a preprocessing step, making use of reordering
rules built from the word reorderings introduced
in the tuple extraction process. The resulting re-
ordering hypotheses are passed to the decoder in
the form of word lattices (Crego and no, 2006).
4.2 A bilingual POS-based reordering model
For this year evaluation, we also experimented
with an additional reordering model, which is esti-
mated as a standard n-gram language model, over
generalized translation units. In the experiments
reported below, we generalized tuples using POS
tags, instead of raw word forms. Figure 1 displays
the same sequence of tuples when built from sur-
face word forms (top), and from POS tags (bot-
tom).
Figure 1: Sequence of units built from surface
word forms (top) and POS-tags (bottom).
Generalizing units greatly reduces the number
of symbols in the model and enables to take larger
56
n-gram contexts into account: in the experiments
reported below, we used up to 6-grams. This new
model is thus helping to capture the mid-range
syntactic reorderings that are observed in the train-
ing corpus. This model can also be seen as a trans-
lation model of the sentence structure. It models
the adequacy of translating sequences of source
POS tags into target POS tags. Additional details
on these new reordering models can be found in
(Crego and Yvon, 2010).
4.3 Combining translation models
Our main translation model being a conventional
n-gram model over bilingual units, it can directly
take advantage of all the techniques that exist for
these models. To take the diversity of the available
parallel corpora into account, we independently
trained several translation models on subpart of
the training data. These translation models were
then linearly interpolated, where the interpolation
weights are chosen so as to minimize the perplex-
ity on the development set.
5 Language Models
The English and French language models (LMs)
are the same as for the last year?s French-English
task (Allauzen et al, 2009) and are heavily tuned
to the newspaper/newswire genre, using the first
part of the WMT09 official development data
(dev2009a). We used all the authorized news
corpora, including the French and English Gi-
gaword corpora, for translating both into French
(1.4 billion tokens) and English (3.7 billion to-
kens). To estimate such LMs, a vocabulary was
defined for both languages by including all to-
kens in the WMT parallel data. This initial vo-
cabulary of 130K words was then extended with
the most frequent words observed in the training
data, yielding a vocabulary of one million words
in both languages. The training data was divided
into several sets based on dates and genres (resp.
7 and 9 sets for English and French). On each
set, a standard 4-gram LM was estimated from
the 1M word vocabulary with in-house tools using
Kneser-Ney discounting interpolated with lower
order models (Kneser and Ney, 1995; Chen and
Goodman, 1998)4. The resulting LMs were then
linearly combined using interpolation coefficients
4Given the amount of training data, the use of the modi-
fied Kneser-Ney smoothing is prohibitive while previous ex-
periments did not show significant improvements.
chosen so as to minimize perplexity of the de-
velopment set (dev2009a). The final LMs were
finally pruned using perplexity as pruning crite-
rion (Stolcke, 1998).
For German, since we have less training
data, we only used the German monolingual
texts (Europarl-v5, News Commentary and News
Monolingual) provided by the organizers to train
a single n-gram language model, with modified
Kneser-Ney smoothing scheme (Chen and Good-
man, 1998), using the SRILM toolkit (Stolcke,
2002).
6 Tuning
Moses-based systems were tuned using the imple-
mentation of minimum error rate training (MERT)
(Och, 2003) distributed with the Moses decoder,
using the development corpus (news-test2008).
The N -code systems were also tuned by
the same implementation of MERT, which was
slightly modified to match the requirements of our
decoder. The BLEU score is used as objective
function for MERT and to evaluate test perfor-
mance. The interpolation experiment for French-
English was tuned on news-test2008a (first 1025
lines). Optimization was carried out over new-
stest2008b (last 1026 lines).
7 Experiments
For each system, we used all the available par-
allel corpora distributed for this evaluation. We
used Europarl and News commentary corpora for
German-English task and Europarl, News com-
mentary, United Nations and Gigaword corpora
for the French-English tasks. All corpora were
aligned with GIZA++ for word-to-word align-
ments with grow-diag-final-and and default set-
tings. For the German-English tasks, we applied
normalization and compound splitting as a pre-
processing step. For the French-English tasks, we
used new POS-based reordering model and inter-
polation.
7.1 German-English Tasks
We combined our two preprocessing schemes (see
Section 3) by applying compound splitting over
normalized data. Our experiments showed that for
German to English, using 4 characters as the mini-
mum split length and 8 characters as the minimum
compound candidate, and allowing the insertion of
-s -n -en -nen -e -es -er -ien) and the truncation of
57
-e -en -n yielded the best BLEU scores. On the
reverse direction, the best setting is different: 5
characters as minimum split length, 10 characters
as minimum compound candidate, no truncation.
These processes are performed before align-
ment, training, tuning and decoding. Before de-
coding, we also replaced all OOV words with their
lemma. We used the Moses (Koehn et al, 2007)
decoder, with default settings, to obtain the trans-
lations. For translating from English to German,
we used a two-level decoding. The first decoding
step translates English to ?preprocessed German?,
which is then turned into German by undoing the
effect of normalization. In this second step, we
thus aim at restoring inflection marks and at merg-
ing compounds. For this second ?translation? step,
we also use a Moses-based system. To point out
the error rate of the second step, we also translated
the preprocessed reference German text and com-
puted the BLEU score as 97.05. Our experiments
showed that this two-level decoding strategy was
not improving the direct baseline systems. Table 2
reports the BLEU scores5 on newstest2010 of our
official submissions.
System De ? En En ? De
Baseline 20.0 15.3
Norm+Split 21.3 15.0
Table 2: Results for German-English
7.2 French-English tasks
As explained above, in addition to the baseline
system (base), two contrast systems were built.
The first introduces an additional POS-based bilin-
gual 6-gram reordering model (bilrm), the second
implements the bilingual n-gram model after in-
terpolating 4 models trained respectively on the
news, epps, UNdoc and gigaword subparts of the
parallel corpus (interp). Optimization was carried
out over newstest2008b (last 1026 lines) and tested
over newstest2010 (2489 lines). Table 3 reports
translation accuracy for the three systems and for
both translation directions.
As can be seen, the system using the new
reordering model (base+bilrm) outperformed the
baseline system when translating into French,
while no difference was measured when translat-
ing into English. The interpolation experiments
5Scores are computed with the official script mteval-
v11b.pl
System Fr ? En En ? Fr
base 26.52 27.22
base+bilrm 26.50 27.84
base+bilrm+interp 26.84 27.62
Table 3: Results for French-English
did not show any clear impact on performance.
8 Conclusions
In this paper, we presented our statistical MT sys-
tems developed for the WMT?10 shared task, in-
cluding several novelties, namely the preprocess-
ing of German, and the integration of several new
techniques in our n-gram based decoder.
Acknowledgments
This work was partly realized as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, Aure?lien Max,
and Franc?ois Yvon. 2009. LIMSI?s statistical trans-
lation systems for WMT?09. In Proceedings of
WMT?09, Athens, Greece.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
Simon Corston-oliver and Michael Gamon. 2004.
Normalizing german and english inflectional mor-
phology to improve statistical word alignment. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas, pages 48?57.
Springer Verlag.
Josep M. Crego and Jose? B. Mari no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proceedings of Human Language Technology
58
Conference and Conference on Empirical Methods
in Natural Language Processing, pages 676?683,
Vancouver, British Columbia, Canada, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL ?03: Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics,
pages 187?193. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. Annual Meeting of the Association for Compu-
tational Linguistics (ACL), demonstration session,
Prague, Czech Republic.
Jose? B. Marin?o, Rafael E. Banchs R, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic information. Computational Lin-
guistics, 30(2):181?204.
Franz J. Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Langage Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In GoTAL ?08: Pro-
ceedings of the 6th international conference on Ad-
vances in Natural Language Processing, pages 464?
475, Berlin, Heidelberg. Springer-Verlag.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of german compounds. In EACL
?09: Proceedings of the 12th Conference of the
European Chapter of the Association for Compu-
tational Linguistics: Student Research Workshop,
pages 61?69, Morristown, NJ, USA. Association for
Computational Linguistics.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of the Human Language Technology con-
ference / North American chapter of the Association
for Computational Linguistics 2004, pages 101?104,
Boston, MA, USA.
59
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 542?553,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
From n-gram-based to CRF-based Translation Models
Thomas Lavergne Josep Maria Crego
LIMSI/CNRS
BP 133
F-91 403 Orsay Ce?dex
{lavergne,jmcrego}@limsi.fr
Alexandre Allauzen Franc?ois Yvon
LIMSI/CNRS & Uni. Paris Sud
BP 133
F-91 403 Orsay Ce?dex
{allauzen,yvon}@limsi.fr
Abstract
A major weakness of extant statistical ma-
chine translation (SMT) systems is their lack
of a proper training procedure. Phrase extrac-
tion and scoring processes rely on a chain of
crude heuristics, a situation judged problem-
atic by many. In this paper, we recast the ma-
chine translation problem in the familiar terms
of a sequence labeling task, thereby enabling
the use of enriched feature sets and exact train-
ing and inference procedures. The tractabil-
ity of the whole enterprise is achieved through
an efficient implementation of the conditional
random fields (CRFs) model using a weighted
finite-state transducers library. This approach
is experimentally contrasted with several con-
ventional phrase-based systems.
1 Introduction
A weakness of existing phrase-based SMT systems,
that has been repeatedly highlighted, is their lack
of a proper training procedure. Attempts to de-
sign probabilistic models of phrase-to-phrase align-
ments (e.g. (Marcu and Wong, 2002)) have thus far
failed to overcome the related combinatorial prob-
lems (DeNero and Klein, 2008) and/or to yield im-
proved training heuristics (DeNero et al, 2006).
Phrase extraction and scoring thus rely on a chain
of heuristics see (Koehn et al, 2003), which evolve
phrase alignments from ?symmetrized? word-to-
word alignments obtained with IBM models (Brown
et al, 1990) and the like (Liang et al, 2006b; Deng
and Byrne, 2006; Ganchev et al, 2008). Phrase
scoring is also mostly heuristic and relies on an op-
timized interpolation of several simple frequency-
based scores. Overall, the training procedure of
translation models within conventional phrase-based
(or hierarchical) systems is generally considered un-
satisfactory and the design of better estimation pro-
cedures remains an active research area (Wuebker et
al., 2010).
To overcome the NP-hard problems that derive
from the need to consider all possible permutations
of the source sentence, we make here a radical
simplification and consider training the translation
model given a fixed segmentation and reordering.
This idea is not new, and is one of the grounding
principle of n-gram-based approaches (Casacuberta
and Vidal, 2004; Marin?o et al, 2006) in SMT. The
novelty here is that we will use this assumption to re-
cast machine translation (MT) in the familiar terms
of a sequence labeling task.
This reformulation allows us to make use of the
efficient training and inference tools that exists for
such tasks, most notably linear CRFs (Lafferty et
al., 2001; Sutton and McCallum, 2006). It also en-
ables to easily integrate linguistically informed (de-
scribing morphological or morpho-syntactical prop-
erties of phrases) and/or contextual features into the
translation model. In return, in addition to having
a better trained model, we also expect (i) to make
estimation less sensible to data sparsity issues and
(ii) to improve the ability of our system to make
the correct lexical choices based on the neighbor-
ing source words. As explained in Section 2, this
reformulation borrows much from the general ar-
chitecture of n-gram MT systems and implies to
solve several computational challenges. In our ap-
542
proach, the tractability of the whole enterprise is
achieved through an efficient reimplementation of
CRFs using a public domain library for weighted
finite-state transducers (WFSTs) (see details in Sec-
tion 3). This approach is experimentally contrasted
with more conventional n-gram based and phrase-
based approaches on a standard benchmark in Sec-
tion 4, where we also evaluate the benefits of various
feature sets and training regimes. We finally relate
our new system with alternative proposals for train-
ing discriminatively SMT systems in Section 5, be-
fore drawing some lessons and discussing possible
extensions of this work.
The main contribution of this work are thus (i) a
detailed presentation of the CRF in translation in-
cluding all necessary implementation details and (ii)
an experimental study of various feature functions
and of various ways to integrate target side LM in-
formation.
2 MT as sequence labeling
In this section, we briefly review the n-gram based
approach to SMT, originally introduced in (Casacu-
berta and Vidal, 2004; Marin?o et al, 2006), which
constitutes our starting point. We then describe our
new proposal, which, in essence, consists in replac-
ing the modeling of compound source-target trans-
lation units by a conditional model where the prob-
ability of each target side phrase is conditioned on
the source sentence.
2.1 The n-gram based approach in SMT
The n-gram based approach of (Marin?o et al, 2006)
is a variation of the standard phrase-based model,
characterized by the peculiar form of the translation
model. In this approach, the translation model is
based on bilingual units called tuples. Tuples are
the analogous of phrase pairs, as they represent a
matching u = (e, f) between a source f and a tar-
get e word sequence. The probability of a sequence
of tuples is computed using a conventional n-gram
model as:
p(u1 . . . uI) =
I?
i=1
p(ul|ui?1 . . . ui?n+1).
The probability of a sentence pair (f , e) is then ei-
ther recovered by marginalization, or approximated
by maximization, over all possible joint segmenta-
tions of f and e into tuples.
As for any n-gram model, the parameters are es-
timated using statistics collected in a training corpus
made of sequences of tuples derived from the par-
allel sentences in a two step process. First, a word
alignment is computed using a standard alignment
pipeline1 based on the IBM models. Source words
are then reordered so as to disentangle the align-
ment links and to synchronize the source and tar-
get texts. Special care has to be paid to non-aligned
source words, which have to be collapsed with their
neighbor words. A byproduct of this process is a de-
terministic joint segmentation of parallel sentences
into minimal bilingual units, the tuples, that consti-
tute the basic elements in the model. This process is
illustrated on Figure 1, where the unfolding process
enables the extraction of tuples such as: (demanda,
said ) or (de nouveau, again).
f : demanda de nouveau la femme voile?e
e: the veiled dame said again
f? : la voile?e femme demanda de nouveau
Figure 1: The tuple extraction process
The original (top) and reordered (bottom) French
sentence aligned with its translation.
At test time, the source text is reordered so as
to match the reordering implied by the disentangle-
ment procedure. Various proposals has been made
to perform such source side reordering (Collins et
al., 2005; Xia and McCord, 2004), or even learn-
ing reordering rules based on syntactic or morpho-
syntactic information (Crego and Marin?o, 2007).
The latter approach amounts to accumulate reorder-
ing patterns during the training; test source sen-
tences are then non-deterministically reordered in
all possible ways yielding a word graph. This graph
is then monotonously decoded, where the score of
a translation hypothesis combines information from
the translation models as well as from other infor-
mation sources (lexicalized reordering model, target
1Here, using the MGIZA++ package (Gao and Vogel, 2008).
543
side language model (LM), word and phrase penal-
ties, etc).
2.2 Translating with CRFs
A discriminative version of the n-gram approach
consists in modeling P (e|f) instead of P (e, f),
which can be efficiently performed with CRFs (Laf-
ferty et al, 2001; Sutton and McCallum, 2006). As-
suming matched sequences of observations (x =
xL1 ) and labels (y = y
L
1 ), CRFs express the con-
ditional probability of labels as:
P (yL1 |x
L
1 ) =
1
Z(xL1 ; ?)
exp(?TG(xL1 , y
L
1 )),
where ? is a parameter vector and G denotes a vec-
tor of feature functions testing various properties of
x and y. In the linear-chain CRF, each compo-
nent Gk(xI1, y
I
1) of G is decomposed as a sum of
local features: Gk(xI1, y
I
1) =
?
i gk(x
I
1, yi?1, yi)
2.
CRFs are trained by maximizing the (penalized) log-
likelihood of a corpus containing observations and
their labels.
In principle, the data used to train n-gram trans-
lation models provide all the necessary information
required to train a CRF3. It suffices to consider that
the alphabet of possible observations ranges over all
possible source side fragments, and that each tar-
get side of a tuple is a potential label. The model
thus defines the probability of a segmented target
e? = e?I1 given the segmented and reordered source
sentence f? = f? I1 . To complete the model, one just
needs to define a distribution over source segmen-
tations P (f? |f). Given the deterministic relationship
between e and e? expressed by the ?unsegmentation?
function ? which maps e? with e = ?(e?), we then
have:
P (e|f) =
?
f? ,e|?(e)=e
P (e?, f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f? , f)P (f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f?)P (f? |f)
2Assuming first order dependencies.
3This is a significant difference with (Blunsom et al, 2008),
as we do not need to introduce latent variables during training.
In practice, we will only consider a restricted
number of possible segmentation/reorderings of the
source, denoted L(f), and compute the best transla-
tion e? as ?(e??), where:
e?? = arg max
e
P (e?|f)
? arg max
f??L(f),e
P (e?, |f? , f)P (f? |f) (1)
Even with these simplifying assumptions, this
approach raises several challenging computational
problems. First, training a CRF is quadratic in the
number of labels, of which we will have plenty (typ-
ically hundreds of thousands). A second issue is de-
coding: as we need to consider at test time a combi-
natorial number of possible source reorderings and
segmentations, we can no longer dispense with the
computation of the normalizer Z(f? ; ?) which is re-
quired to compute P (e?, f? |f) as P (f? |f)P (e?|f?) and to
compare hypotheses associated with different values
of f? . We discuss our solutions to these problems in
the next section.
3 Implementation issues
3.1 Training
Basic training The main difficulties in training are
caused by the unusually large number of labels, each
of which corresponds to a (small) sequence of target
words. Hopefully, each observation (source side tu-
ple) occurs with a very small number of different
labels. A first simplification is thus to consider that
the set of possible ?labels? e? for a source sequence
f? is limited to those that are seen in training: all
the other associations (f? , e?) are deemed impossible,
which amounts to setting the corresponding param-
eter value to ??.
A second speed-up is to enforce sparsity in the
model, through the use of a `1 regularization term
(Tibshirani, 1996): on the one hand, this greatly re-
duces the memory usage; furthermore, sparse mod-
els are also prone to various optimization of the
forward-backward computations (Lavergne et al,
2010). As discussed in (Ng, 2004; Turian et al,
2007), this feature selection strategy is well suited
to the task at hand, where the number of possible
features is extremely large. Optimization is per-
544
formed using the Rprop algorithm4 (Riedmiller and
Braun, 1993), which provides the memory efficiency
needed to cope with the very large feature sets con-
sidered here.
Training with a target language model One of
the main strength of the phrase-based ?log-linear?
models is their ability to make use of powerful
target side language models trained on very large
amounts of monolingual texts. This ability is crucial
to achieve good performance and has to be preserved
no matter the difficulties that occur when one moves
away from conventional phrase-based systems (Chi-
ang, 2005; Huang and Chiang, 2007; Blunsom and
Osborne, 2008; Ka?a?ria?inen, 2009). It thus seems
appropriate to include a LM feature function in our
model or alternatively to define:
P (e?|f?) =
1
Z(f? ; ?)
PLM (e?) exp(?
TG(f? , e?)),
where PLM is the target language model and
Z(f? ; ?) =
?
e PLM (e?) exp(?
TG(f? , e?)). Imple-
menting this approach implies to deal with the lack
of synchronization between the units of the trans-
lation models, which are variable-length (possibly
empty) tuples, and the units of the language models,
which are plain words.
In practice, this extension is implemented by per-
forming training and inference over a graph whose
nodes are not only indexed by their position and the
left target context, but also by the required n-gram
(target) history. In most cases, for small values of
n such as considered in this study, the n-gram his-
tory can be deduced from the left target tuple. The
most problematic case is when the left target tuple
is NULL, which require to copy the history from the
previous states. As a consequence, for the values of
n considered here, the impact of this extension on
the total training time is limited.
Reference reachability A recurring problem for
discriminative training approaches is reference un-
reachability (Liang et al, 2006a): this happens when
the model cannot predict the reference translation,
which means in our case that the probability of the
reference cannot be computed. In our implementa-
tion, this only happens when the reference involves
4Adapted to handle a locally non-differentiable objective.
a tuple (f? ,e?) that is too rare to be included in the
model. As a practical workaround, when this hap-
pens for a given training sentence, we make sure
to ?locally? augment the tuple dictionary with the
missing part of the reference, which is then removed
for processing the rest of the training corpus.
3.2 Inference
Our decoder is implemented as a cascade of
weighted finite-state transducers (WFSTs) using the
functionalities of the OpenFst library (Allauzen et
al., 2007). This library provides many basic opera-
tion for WFSTs, notably the left (pi1) and right (pi2)
projections as well as the composition operation (?).
The related notions and algorithms are presented in
detail in (Mohri, 2009), to which we refer the reader.
In essence, our decoder is implemented of a finite-
state cascade involoving the following steps: (i)
source reordering and segmentation (ii) application
of the translation model and (optionally) (iii) com-
position with a target side language model, an ar-
chitecture that is closely related to the proposal of
(Kumar et al, 2006). A more precise account of
these various steps is given below, where we de-
scribe the main finite-state transducers involved in
our decoder:
? S, the acceptor for the source sentence f ;
? R, which implements segmentation and re-
ordering rules;
? T , the tuple dictionary, associating source side
sequences with possible translations based on
the inventory of tuples;
? F , the feature matcher, mapping each feature
with the corresponding parameter value;
Source reordering The computation of R mainly
follows the approach of (Crego and Marin?o, 2007)
and uses a part-of-speech tagged version of the re-
ordered training data. Each reordering pattern seen
in training is generalized as a non-deterministic re-
ordering rule which expresses a possible rearrange-
ment of some subpart of the source sentence. Each
rule is implemented as an elementary finite-state
transducer, and the set of possible word reorderings
is computed as the composition of these transducers.
R is finally obtained by composing the result with a
545
transducer computing all the possible segmentations
of its input into sequences of source side tuples5.
The output of S ? R are sequences of source side
tuples f? ; each path in this transducer is addition-
ally weighted with a simplistic n-tuple segmentation
model, estimated using the source side of the paral-
lel training corpus. Note that these scores are nor-
malized, so that the weight of each path labelled f? in
S ?R is logP (f? |f).
The feature matcher F The feature matcher is
also implemented as a series of elementary weighted
transducers, each transducer being responsible for a
given class of feature functions. The simplest trans-
ducer in this family deals with the class of unigram
feature functions, ie. feature functions that only test
the current observation and label. It is represented
on the left part of Figure 3.2, where for the sake of
readability we only display one example for each
test pattern (here: an unconditional feature that al-
ways returns true for a given label, a test on the
source word, and a test on the source POS label).
As long as dependencies between source and/or tar-
get symbols remain local, they can be captured by
finite-state transducers such as the ones on the mid
and right part of Figure 3.2, which respectively com-
pute bigram target features, and joint bigram source
and target features.
The feature matcher F is computed as the com-
position of these elementary transducers, where we
only include source and target labels that can occur
given the current input sentence. Weights in F are
interpreted in the tropical semiring. exp(F ) is ob-
tained by replacing weights w in F with exp(w) in
the real semiring.
Decoding a word graph If the input segmentation
and reordering were deterministically set, meaning
that the automaton I = pi1(S ? R ? T ) would only
contain one path, decoding would amount to finding
the best path in S ?R ? T ?F . However, we need to
compute:
arg max
e
P (e?|f) = arg max
e
?
f?
P (e?, f? |f)
= arg max
e
?
f?
P (e?|f?)P (f? |f).
5When none is found, we also consider a maximal segmen-
tation into isolated words.
This requires to compare model scores for mul-
tiple source segmentations and reorderings f? , hence
to compute P (f? |f) and P (e?|f?), rather than just the
non-normalized value that is usually used in CRFs.
Computing the normalizer Z(f? ; ?) for all se-
quences in S ?R is performed efficiently using stan-
dard finite-state operations as :
D = det(pi1(pi2(S ?R) ? T ? exp(F ))).
In fact, determinization (in the real semiring) has the
effect of accumulating for each f? the corresponding
normalizer Z(f? ; ?). Replacing each weight w in D
by ? log(w) and using the log semiring enables to
compute? log(Z(f? ; ?)). The best translation is then
obtained as: bestpath(pi2(S?R)??log(D)?T ?F )
in the tropical semiring.
Decoding and Rescoring with a target language
model An alternative manner of using a (large)
target side language model is to use it for rescoring
purposes. The consistent use of finite-state machines
and operations makes it fairly easy to include one
during decoding : it suffices to perform the search in
pi2(S?R)?? log(D)?T ?F ?L, where L represents
a n-gram language model. When combining several
models, notably a source segmentation model and/or
a target language model for rescoring, we have made
sure to rescale the (log)probabilities so as to balance
the language model scores with the CRF scores, and
to use a fixed word bonus to make hypotheses of dif-
ferent length more comparable. All these parameters
are tuned as part of the decoder development pro-
cess. It is finally noteworthy that, in our architecture,
alternative decoding strategies, such as MBR (Ku-
mar and Byrne, 2004) are also readily implemented.
4 Experiments
4.1 Corpora and metrics
For these experiments, we have used a medium size
training corpus, extracted from the datasets made
available for WMT 20116 evaluation campaign, and
have focused on one translation direction, from
French to English7.
Translation model training uses the entire News-
Commentary subpart of the WMT?2011 training
6statmt.org/wmt11
7Results in the other direction suggest similar conclusions.
546
0le : the/?le,the
DET : the/?DET,the
? : the/?the 0 1
? : the/0
? : cat/?the,cat
0 1
? : the/0
chat : cat/?chat,cat
Figure 2: Feature matchers. The star symbol (*) matches any possible observation.
French English
sent? token types token types
train 115 K 3 339 K 60 K 2 816 K 58 K
test 2008 2.0 K 55 K 9 K 49 K 8 K
test 2009 2.5 K 72 K 11 K 65 K 10 K
test 2010 2.5 K 69 K 10 K 61 K 9 K
Table 1: Corpora used for the experiments
data; for language models, we have considered two
approaches (i) a ?large? bigram model highly opti-
mized using all the available monolingual data and
(ii) a ?small? trigram language model trained on
just the English side of the NewsCommentary cor-
pus. The regularization parameters used in training
are tuned using the WMT 2009 test set; the various
parameters implied in the decoding are tuned (for
BLEU) on WMT 2008 test set; the internal tests re-
ported below are performed on the 2010 test lines
(see Table 1) using the best parameters found during
tuning. Various statistics regarding these corpora are
reproduced on Table 1.
All the training corpora were aligned using
MGIZA++ with standard parameters8, and pro-
cessed in the standard tuple extraction pipeline. The
development and test corpora were also processed
analogously. For the sake of comparison, we also
trained a standard n-gram-based and a Moses sys-
tem (Koehn et al, 2007) with default parameters
and a 3-gram target LM trained using only the tar-
get side of our parallel corpus. The development set
(test 2009) was used to tune these two systems. All
performance are measured using BLEU (Papineni et
al., 2002).
8As part of a much larger batch of texts.
4.2 Features
The baseline system is composed only of transla-
tion features [trs] and target bigram features [t2g].
The former correspond to functions of the form
gus,t(f? , e?, i) = I(f?i = s ? e?i = t), where s
and t respectively denote source and target phrases
and I() is the indicator function. These are also
generalized to part-of-speech and also to any pos-
sible source phrase, giving rise to features such as
gu?,t = (f? , e?, i) = I(e?i = t). Target bigram features
correspond to functions of the form gbt,t?(f? , e?, i) =
I(e?i?1 = t? e?i = t?). The last baseline feature is the
copy feature, which fires whenever the source and
target segments are identical.
Supplementary groups of features are considered
in further stages:
? suffix/prefix features [ix]. These features allow
to generalize baseline features on the source
side to fixed length prefixes and suffixes, thus
smoothing the parameters.
? context features [ctx]. These features are sim-
ilar to unigram features, but also test the left
source tuple and the corresponding part-of-
speech.
? segmentation features [seg]. These features are
meant to express a preference for longer tuples
and to regulate the number of target words per
source word. We consider the following feature
functions (|e| denotes the length of e):
? target length features :
gl?,l(f? , e?, i) = I(|e?i| = l)
? source-target length features :
gll,l?(f? , e?, i) = I(|f?i| = l ? |e?i| = l
?)
? source-target length ratio :
gll(f? , e?, i) = I(round(
| efi|
|ei|
) = l)
547
Note that all these features are further condi-
tioned on the target label.
? reordering features [ord]. These features are
meant to model preferences for specific lo-
cal reordering patterns and take into account
neighbor source fragments in e? together with
the current label. Each source side segment
f?i is made of some source words that, prior
to source reordering, were located at indices
i1 . . . il, so that f?i = fi1 . . . fil . The high-
est (resp. lowest) index in this sequence is df?ie
(resp. bf?ic). The leftmost (resp. rightmost) in-
dex is [f?i[ (resp. ]f?i]).
Using these notations, our model includes the
following patterns:
? distortion features, measuring the gaps be-
tween consecutive source fragments :
gol,t(f? , e?, i)=I(?(f?i, e?i)= l ? e?i= t),
where ?(f?i, e?i) ={
bf?ic ? df?i?1e if (df?i?1e ? bf?ic)
df?ie ? bf?i?1c otherwise .
? lexicalized reordering, identifying mono-
tone, swap and discontinuous configura-
tions (Tillman, 2004). The monotonous
test is defined as: gom(f? , e?, i) =
I(]ei?1] = [ei[); the swap and discon-
tinuous configurations are defined analo-
gously.
? ?gappiness? test : this feature is activated
whenever the source indices i1...il contain
one or several gaps.
4.3 Experiments and lessons learned
Training time The first lesson learned is that
training can be performed efficiently. Our baseline
system, which only contains trs and trg contains ap-
proximately 87 million features, out of which a lit-
tle bit more than 600K are selected. Adding up all
supplementary features raises the number of param-
eters to about 130M features, out of which 1.5M are
found useful. All these systems require between 3
and 5 hours to train9. These numbers are obtained
with a `1 penalty term ? 1, which offers a good bal-
ance between accuracy and sparsity.
9All experiments run on a server with 64G of memory and
two Xeon processors with 4 cores at 2.27 Ghz.
Test conditions In order to better assess the
strengths and weaknesses of our approach, we com-
pare several test settings: the most favorable con-
siders only one possible segmentation/reordering f?
for each f , obtained through forced alignment with
the reference; we then consider the more challeng-
ing case where the reordering is fixed, but several
segmentations are considered; then the regular de-
coding task, where both segmentation and reorder-
ing are unknown and where the entire space of all
segmentations and reordering is searched. For each
condition, we also vary (i) the set of features used
and (ii) the target language model used, if any.
Wherever applicable, we also report contrasts with
n-gram-based systems subject to the same input and
comparable resources, varying the order of the tuple
language model, as well as with Moses. Results are
in Table 2.
dev test # feat.
decoding with optimal segmentation/reordering
CRF (trs,trg) 23.8 25.1 660K
CRF +ctx 24.1 25.4 1.5M
CRF +ix,ord,seg 24.3 25.6 1.5M
decoding with optimal reordering
n-gram (2g,3g) 20.6 24.1 755K
n-gram (3g,3g) 21.5 25.2 755K
CRF trs,trg - 22.8 660K
CRF +ctx - 23.1 1.5M
CRF +ix,ord,seg - 23.5 1.5M
regular decoding
Moses (3g) 21.2 20.5
n-gram (2g,3g) 20.6 20.2 755K
n-gram (3g,3g) 21.5 21.2 755K
CRF (trs,trg) - 18.3 660K
CRF +ctx - 18.8 1.5M
CRF +ix,ord,seg - 19.1 1.5M
CRF +ix,ord,seg+3g - 19.1 1.5M
Table 2: Translation performance
Extending the feature set As expected, the use
of increasingly complex feature sets seems benefi-
cial in all experimented conditions. It is noteworthy
that throwing in reordering and contextual features
is helping, even when decoding one single segmen-
tation and reordering. This is because these features
do not help to select the best input reordering, but
548
help choose the best target phrase.
Searching a larger space Going from the sim-
pler to the more difficult conditions yields signif-
icant degradations in the model, as our best score
drops down from 25.6 to 23.5 (with known reorder-
ing) then to 19.1 (regular decoding). This is a clear
indication that our current segmentation/reordering
model is not delivering very useful scores. A similar
loss is incurred by the n-gram system, which loses
4 bleu points between the two conditions.
LM rescoring Our results to date with target side
language models have proven inconclusive, which
might explain why our best results remain between
one and two BLEU points behind the n-gram based
system using comparable information. Note also
that preliminary experiments with incorporating a
large bigram during training have also failed to date
to provide us with improvements over the baseline.
Summary In sum, the results accumulated during
this first round of experiments tend to show that our
CRF model is still underperforming the more es-
tablished baseline by approximately 1 to 1.5 BLEU
point, when provided with comparable resources.
Sources of improvements that have been clearly
identified is the scoring of reordering and segmen-
tations, and the use of a target language model in
training and/or decoding.
5 Related work
Discriminative learning approaches have proven
successful for many NLP tasks, notably thanks to
their ability to cope with flexible linguistic repre-
sentations and to accommodate potentially redun-
dant descriptions. This is especially appealing for
machine translation, where the mapping between
a source word or phrase and its target correlate(s)
seems to involve an large array of factors, such as its
morphology, its syntactic role, its meaning, its lexi-
cal context, etc. (see eg. (Och et al, 2004; Gimpel
and Smith, 2008; Chiang et al, 2009), for inspira-
tion regarding potentially useful features in SMT).
Discriminative learning requires (i) a parameter-
ized scoring function and (ii) a training objective.
The scoring function is usually assumed to be linear
and ranks candidate outputs y for input x accord-
ing to ?TG(x, y), where ? is the parameter vector. ?
andG deterministically imply the input/output map-
ping as x ? arg maxy ?
TG(x, y). Given a set of
training pairs {xi, yi, i = 1 . . . N}, parameters are
learned by optimizing some regularized loss func-
tion of ?, so as to make the inferred input/output
mapping faithfully replicate the observed instances.
Machine translation, like most NLP tasks, does
not easily lend itself to that approach, due to the
complexity of the input/output objects (word or la-
bel strings, parse trees, dependency structures, etc).
This complexity makes inference and learning in-
tractable, as both steps imply the resolution of
the arg max problem over a combinatorially large
space of candidates y. Structured learning tech-
niques (Bakir et al, 2007), developed over the last
decade, rely on decompositions of these objects into
sub-parts as part of a derivation process, and use
conditional independence assumptions between sub-
parts to render the learning and inference problem
tractable. For machine translation, this only pro-
vides part of the solution, as the training data only
contain pairs of word aligned sentences (f , e), but
lack the explicit derivation h from f to e that is re-
quired to train the model in a fully supervised way.
The approach of (Liang et al, 2006a) circumvents
the issue by assuming that the hidden derivation h
can be approximated through forced decoding. As-
suming that h is in fact observed as the optimal
(Viterbi) derivation h? from f to e given the cur-
rent parameter value10, it is straightforward to re-
cast the training of a phrase-based system as a stan-
dard structured learning problem, thus amenable to
training algorithms such as the averaged perceptron
of (Collins, 2002). This approximation is however
not genuine, and the choice of the most appropriate
derivation seems to raises intriguing issues (Watan-
abe et al, 2007; Chiang et al, 2008).
The authors of (Blunsom et al, 2008; Blunsom
and Osborne, 2008) consider models for which it is
computationally possible to marginalize out all pos-
sible derivations of a given translation. As demon-
strated in these papers, this approach is tractable
even when the derivation process is a based on syn-
chronous context-free grammars, rather that finite-
state devices. However, the computational cost as-
10If one actually exists in the model, thus raising the issue of
reference reachability, see discussion in Section 3.
549
sociated with training and inference remains very
high, especially when using a target side language
model, which seems to preclude the application to
large-scale translation tasks11. The recent work of
(Dyer and Resnik, 2010) proceeds from a similar
vein: translation is however modeled as a two step
process, where a set of possible source reorderings,
represented as a parse forest, are associated with
possible target sentences, using, as we do, a finite-
state translation model. This translation model is
trained discriminatively by marginalizing out the
(unobserved) reordering variables; inference can be
performed effectively by intersecting the input parse
forest with a transducer representing translation op-
tions.
A third strategy is to consider a simpler class of
derivation process, which only partly describe the
mapping between f and e. This is, for instance,
the approach of (Bangalore et al, 2007), where a
simple bag-of-word representation of the target sen-
tence is computed using a battery of boolean clas-
sifiers (one for each target word). In this approach,
discriminative training is readily applicable, as the
required supervision is overtly present in example
source-target pairs (f , e); however, a complemen-
tary reshaping/reordering step is necessary to turn
the bag-of-word into a full-fledged translation. This
work was recently revisited in (Mauser et al, 2009),
where a conditional model predicting the presence
of each target phrase provides a supplementary score
for the standard ?log-linear? model.
This line of research has been continued notably
in (Ka?a?ria?inen, 2009), which introduces an exponen-
tial model of bag of phrases (allowing some over-
lap), that enables to capture localized dependencies
between target words, while preserving (to some ex-
tend) the efficiency of training and inference. Su-
pervision is here indirectly provided by word align-
ment and correlated phrase extraction processes
implemented in conventional phrase-based systems
(Koehn et al, 2003). If this model seems to deliver
state-of-the-art performance on large-scale tasks, it
does so at a very high computational cost. More-
over, for lack of an internal modeling of reordering
processes, this approach, like the bag-of-word ap-
11For instance, the experiments reported in (Blunsom and Os-
borne, 2008) use the English-Chinese BTEC, where the average
sentence length is lesser than 10.
proach, seems only appropriate for language pairs
with similar or related word ordering.
The approach developed in this paper fills a gap
between the hierarchical model of (Blunsom et
al., 2008) and the phrase-based model (Ka?a?ria?inen,
2009), with whom we share several important as-
sumptions, such as the use of alignment information
to provide supervision, and the resort to a an ?ex-
ternal?, albeit a more powerful, reordering compo-
nent. Using a finite-state model enables to process
reasonably large corpora, and gives some hopes as to
the scalability of the whole enterprise; it also makes
the integration of a target side language model much
easier than in hierarchical models.
6 Discussion and future work
In this paper, we have given detailed description of
an original phrase-based system implementing a dis-
criminative version of the n-gram model, where the
translation model probabilities are computed with
conditional random fields. We have showed how
to implement this approach using a memory effi-
cient implementation of the optimization algorithms
needed for training: in our approach, training a mid-
scale translation system with hundred of thousands
sentence pairs and millions of features only takes a
couple of hours on a standalone desktop machine.
Using `1 regularization has enabled to assess the
usefulness of various families of features.
We have also detailed a complete decoder im-
plemented as a pipeline of finite-state transducers,
which allows to efficiently combine several models,
to produce n-best lists and word lattices.
The results obtained in a series of preliminary ex-
periments show that our system is already deliver-
ing competitive translations, as acknowledged by a
comparison with two strong phrase-based baselines.
We have already started to implement various opti-
mizations and to experiment with somewhat larger
datasets (up to 500K sentence pairs) and larger fea-
ture sets, notably incorporating word sense disam-
biguation features: this work needs to be contin-
ued. In addition, we intend to explore a number
of extensions of this architecture, such as imple-
menting MBR decoding (Kumar and Byrne, 2004)
or adapting the translation model to new domains
and conditions, using, for instance, the proposal of
550
(Daume III, 2007)12.
One positive side effect of experimenting with
new translation models is that they help reevalu-
ate the performance of the whole translation system
pipeline: in particular, discriminative training seems
to be more sensible to alignments errors than the cor-
responding n-gram system, which suggests to pay
more attention to possible errors in the training data;
we have also seen that the current reordering model
defines a too narrow search space and delivers in-
sufficiently discriminant scores: we will investigate
various ways to further improve the computation and
scoring of hypothetical source reorderings.
Acknowledgements
The authors wish to thank the reviewers for com-
ments and suggestions. This work was achieved as
part of the Quaero Programme, funded by OSEO,
French State agency for innovation.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Go?khan Bakir, Thomas Hofmann, Bernhard Scho?lkopf,
Alexander J.Smola, Ben Taskar, and S.V.N. Vish-
wanathan. 2007. Predicting structured output. MIT
Press.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 152?159,
Prague, Czech Republic.
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 215?223, Honolulu,
Hawaii.
12In a nutshell, this proposal amounts to having three dif-
ferent parameters for each feature; one parameter is trained
as usual; the other two parameters are updated conditionally,
depending whether the training instance comes from the in-
domain or from the out-domain training dataset.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-08: HLT,
pages 200?208, Columbus, Ohio.
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218?226. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Josep M. Crego and Jose? B. Marin?o. 2007. Improving
SMT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. Association for Compu-
tational Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
08: HLT, Short Papers, pages 25?28, Columbus, Ohio.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
551
surface heuristics. In Proceedings of the ACL work-
shop on Statistical Machine Translation, pages 31?38,
New York City, NY.
Yonggang Deng and William Byrne. 2006. MTTK: An
alignment toolkit for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Demon-
strations, pages 265?268, New York City, USA.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 858?866, Los Angeles,
California. Association for Computational Linguistics.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations ? In Pro-
ceedings of ACL-08: HLT, pages 986?993, Columbus,
Ohio.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In SETQA-NLP ?08.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Matti Ka?a?ria?inen. 2009. Sinuhe ? statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1027?1036, Singapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistic, pages 127?133, Edmond-
ton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
Annual Meeting of the Association for Computational
Linguistics (ACL), demonstration session, pages 177?
180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages
169?176, Boston, Massachusetts, USA. Association
for Computational Linguistics.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the International Conference on Ma-
chine Learning, pages 282?289. Morgan Kaufmann,
San Francisco, CA.
Thomas Lavergne, Olivier Capp, and Franois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504?513, Uppsala,
Sweden.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006a. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia.
Percy Liang, Ben Taskar, and Dan Klein. 2006b. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 133?139.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer Verlag.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning, pages 78?86.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
552
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Martin Riedmiller and Heinrich Braun. 1993. A direct
adaptive method for faster backpropagation learning:
The RPROP algorithm. In Proceedings of the IEEE
International Conference on Neural Networks, pages
586?591, San Francisco, USA.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. J.R.Statist.Soc.B, 58(1):267?288.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Susan Du-
mais, Daniel Marcu, and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 101?104, Boston,
Massachusetts, USA.
J. Turian, B. Wellington, and I.D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In Proc. Neural Information Pro-
cessing Systems (NIPS), volume 19, pages 1409?1417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484, Uppsala, Sweden.
Fei Xia and Michael McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING), pages
508?514, Geneva, Switzerland.
553
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 1?10,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Measuring the Influence of Long Range Dependencies with Neural Network
Language Models
Le Hai Son and Alexandre Allauzen and Franc?ois Yvon
Univ. Paris-Sud and LIMSI/CNRS
rue John von Neumann, 91 403 Orsay cedex, France
Firstname.Lastname@limsi.fr
Abstract
In spite of their well known limitations,
most notably their use of very local con-
texts, n-gram language models remain an es-
sential component of many Natural Language
Processing applications, such as Automatic
Speech Recognition or Statistical Machine
Translation. This paper investigates the po-
tential of language models using larger con-
text windows comprising up to the 9 previ-
ous words. This study is made possible by
the development of several novel Neural Net-
work Language Model architectures, which
can easily fare with such large context win-
dows. We experimentally observed that ex-
tending the context size yields clear gains in
terms of perplexity and that the n-gram as-
sumption is statistically reasonable as long as
n is sufficiently high, and that efforts should
be focused on improving the estimation pro-
cedures for such large models.
1 Introduction
Conventional n-gram Language Models (LMs) are a
cornerstone of modern language modeling for Natu-
ral Language Processing (NLP) systems such as sta-
tistical machine translation (SMT) and Automatic
Speech Recognition (ASR). After more than two
decades of experimenting with these models in a
variety of languages, genres, datasets and appli-
cations, the vexing conclusion is that these mod-
els are very difficult to improve upon. Many vari-
ants of the simple n-gram model have been dis-
cussed in the literature; yet, very few of these vari-
ants have shown to deliver consistent performance
gains. Among these, smoothing techniques, such as
Good-Turing, Witten-Bell and Kneser-Ney smooth-
ing schemes (see (Chen and Goodman, 1996) for an
empirical overview and (Teh, 2006) for a Bayesian
interpretation) are used to compute estimates for the
probability of unseen events, which are needed to
achieve state-of-the-art performance in large-scale
settings. This is because, even when using the sim-
plifying n-gram assumption, maximum likelihood
estimates remain unreliable and tend to overeresti-
mate the probability of those rare n-grams that are
actually observed, while the remaining lots receive
a too small (null) probability.
One of the most successful alternative to date is
to use distributed word representations (Bengio et
al., 2003) to estimate the n-gram models. In this
approach, the discrete representation of the vocabu-
lary, where each word is associated with an arbitrary
index, is replaced with a continuous representation,
where words that are distributionally similar are rep-
resented as neighbors. This turns n-gram distribu-
tions into smooth functions of the word representa-
tion. These representations and the associated esti-
mates are jointly computed using a multi-layer neu-
ral network architecture. The use of neural-networks
language models was originally introduced in (Ben-
gio et al, 2003) and successfully applied to large-
scale speech recognition (Schwenk and Gauvain,
2002; Schwenk, 2007) and machine translation
tasks (Allauzen et al, 2011). Following these ini-
tial successes, the neural approach has recently been
extended in several promising ways (Mikolov et al,
2011a; Kuo et al, 2010; Liu et al, 2011).
Another difference between conventional and
1
neural network language models (NNLMs) that has
often been overlooked is the ability of the latter to
fare with extended contexts (Schwenk and Koehn,
2008; Emami et al, 2008); in comparison, standard
n-gram LMs rarely use values of n above n = 4
or 5, mainly because of data sparsity issues and
the lack of generalization of the standard estimates,
notwithstanding the complexity of the computations
incurred by the smoothing procedures (see however
(Brants et al, 2007) for an attempt to build very
large models with a simple smoothing scheme).
The recent attempts of Mikolov et al (2011b)
to resuscitate recurrent neural network architectures
goes one step further in that direction, as a recur-
rent network simulates an unbounded history size,
whereby the memory of all the previous words ac-
cumulates in the form of activation patterns on the
hidden layer. Significant improvements in ASR us-
ing these models were reported in (Mikolov et al,
2011b; Mikolov et al, 2011a). It must however be
emphasized that the use of a recurrent structure im-
plies an increased complexity of the training and in-
ference procedures, as compared to a standard feed-
forward network. This means that this approach can-
not handle large training corpora as easily as n-gram
models, which makes it difficult to perform a fair
comparison between these two architectures and to
assess the real benefits of using very large contexts.
The contribution is this paper is two-fold. We
first analyze the results of various NNLMs to assess
whether long range dependencies are efficient in lan-
guage modeling, considering history sizes ranging
from 3 words to an unbounded number of words (re-
current architecture). A by-product of this study is a
slightly modified version of n-gram SOUL model
(Le et al, 2011a) that aims at quantitatively esti-
mating the influence of context words both in terms
of their position and their part-of-speech informa-
tion. The experimental set-up is based on a large
scale machine translation task. We then propose a
head to head comparison between the feed-forward
and recurrent NNLMs. To make this comparison
fair, we introduce an extension of the SOUL model
that approximates the recurrent architecture with a
limited history. While this extension achieves per-
formance that are similar to the recurrent model on
small datasets, the associated training procedure can
benefit from all the speed-ups and tricks of standard
feedforward NNLM (mini-batch and resampling),
which make it able to handle large training corpora.
Furthermore, we show that this approximation can
also be effectively used to bootstrap the training of a
?true? recurrent architecture.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the basics of NNLMs ar-
chitectures. We then describe, in Section 3, a num-
ber of ways to speed up training for our ?pseudo-
recurrent? model. We finally report, in Section 4,
various experimental results aimed at measuring the
impact of large contexts, first in terms of perplexity,
then on a realistic English to French translation task.
2 Language modeling in a continuous
space
Let V be a finite vocabulary, language models de-
fine distributions over sequences1 of tokens (typi-
cally words) wL1 in V
+ as follows:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
1 ) (1)
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains hundreds of thousands words. In
the n-gram model, the context is limited to the n?1
previous words, yielding the following factorization:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
i?n+1) (2)
Neural network language models (Bengio et al,
2003) propose to represent words in a continuous
space and to estimate the probability distribution as
a smooth function of this representation. Figure 1
provides an overview of this approach. The context
words are first projected in a continuous space using
the shared matrix R. Denoting v the 1-of-V coding
vector of word v (all null except for the vth compo-
nent which is set to 1), its projection vector is the
vth line of R: RTv. The hidden layer h is then
computed as a non-linear function of these vectors.
Finally, the probability of all possible outcomes are
computed using one or several softmax layer(s).
1wji denotes a sequence of tokens wi . . . j when j ? i, or
the empty sequence otherwise.
2
  
0...0100
10...000
0...0010
v-3
v-2
v-1
R
R
R
shared input space
input layer
hidden layers
shortlist
sub-classlayers
wordlayers
classlayer
input part output part
W
Figure 1: 4-gram model with SOUL at the output layer.
This architecture can be divided in two parts, with
the hidden layer in the middle: the input part (on the
left hand side of the graph) which aims at represent-
ing the context of the prediction; and the output part
(on the right hand side) which computes the proba-
bility of all possible successor words given the con-
text. In the remaining of this section, we describe
these two parts in more detail.
2.1 Input Layer Structure
The input part computes a continuous representation
of the context in the form of a context vector h to be
processed through the hidden layer.
2.1.1 N -gram Input Layer
Using the standard n-gram assumption of equa-
tion (2), the context is made up of the sole n?1 pre-
vious words. In a n-gram NNLM, these words are
projected in the shared continuous space and their
representations are then concatenated to form a sin-
gle vector i, as illustrated in the left part of Figure 1:
i = {RTv?(n?1);R
Tv?(n?2); . . . ;R
Tv?1}, (3)
where v?k is the kth previous word. A non-linear
transformation is then applied to compute the first
hidden layer h as follows:
h = sigm (Wi+ b) , (4)
with sigm the sigmoid function. This kind of archi-
tecture will be referred to as a feed-forward NNLM.
Conventional n-gram LMs are usually limited to
small values of n, and using n greater that 4 or 5
does not seem to be of much use. Indeed, previ-
ous experiments using very large speech recognition
systems indicated that the gain obtained by increas-
ing the n-gram order from 4 to 5 is almost negli-
gible, whereas the model size increases drastically.
While using large context seems to be very imprac-
tical with back-off LMs, the situation is quite dif-
ferent for NNLMs due to their specific architecture.
In fact, increasing the context length for a NNLM
mainly implies to expend the projection layer with
one supplementary projection vector, which can fur-
thermore be computed very easily through a sim-
ple look-up operation. The overall complexity of
NNLMs thus only grows linearly with n in the worst
case (Schwenk, 2007).
In order to better investigate the impact of each
context position in the prediction, we introduce a
slight modification of this architecture in a man-
ner analog to the proposal of Collobert and Weston
(2008). In this variation, the computation of the hid-
den layer defined by equation (4) is replaced by:
h = sigm
(
max
k
[
WkR
Tv?k
]
+ b
)
, (5)
where Wk is the sub-matrix of W comprising the
columns related to the kth history word, and the max
is to be understood component-wise. The product
WkRT can then be considered as defining the pro-
jection matrix for the kth position. After the projec-
tion of all the context words, the max function se-
lects, for each dimension l, among the n ? 1 values
([WkRTv?k]l) the most active one, which we also
assume to be the most relevant for the prediction.
2.1.2 Recurrent Layer
Recurrent networks are based on a more complex
architecture designed to recursively handle an arbi-
trary number of context words. Recurrent NNLMs
are described in (Mikolov et al, 2010; Mikolov et
al., 2011b) and are experimentally shown to outper-
form both standard back-off LMs and feed-forward
NNLMs in terms of perplexity on a small task. The
key aspect of this architecture is that the input layer
for predicting the ith word wi in a text contains both
a numeric representation vi?1 of the previous word
and the hidden layer for the previous prediction.
3
The hidden layer thus acts as a representation of the
context history that iteratively accumulates an un-
bounded number of previous words representations.
Our reimplementation of recurrent NNLMs
slightly differs from the feed-forward architecture
mainly by its input part.We use the same deep archi-
tecture to model the relation between the input word
presentations and the input layer as in the recurrent
model. However, we explicitly restrict the context to
the n?1 previous words. Note that this architecture
is just a convenient intermediate model that is used
to efficiently train a recurrent model, as described in
Section 3. In the recurrent model, the input layer is
estimated as a recursive function of both the current
input word and the past input layer.
i = sigm(Wi?1 +RTv?1) (6)
As in the standard model, RTv?k associates each
context word v?k to one feature vector (the corre-
sponding row in R). This vector plays the role of
a bias at subsequent input layers. The input part is
thus structured in a series of layers, the relation be-
tween the input layer and the first previous word be-
ing at level 1, the second previous word is at level 2
and so on. In (Mikolov et al, 2010; Mikolov et al,
2011b), recurrent models make use of the entire con-
text, from the current word position all the way back
to the beginning of the document. This greatly in-
creases the complexity of training, as each document
must be considered as a whole and processed posi-
tion per position. By comparison, our reimplemen-
tation only considers a fixed context length, which
can be increased at will, thus simulating a true recur-
rent architecture; this enables us to take advantage
of several techniques during training that speed up
learning (see Section 3). Furthermore, as discussed
below, our preliminary results show that restricting
the context to the current sentence is sufficient to at-
tain optimal performance 2.
2.2 Structured Output Layer
A major difficulty with the neural network approach
is the complexity of inference and training, which
largely depends on the size of the output vocabu-
2The test sets used in MT experiments are made of various
News extracts. Their content is thus not homogeneous and us-
ing words from previous sentences doesn?t seem to be relevant.
lary ,i.e. of the number of words that have to be pre-
dicted. To overcome this problem, Le et al (2011a)
have proposed the structured Output Layer (SOUL)
architecture. Following (Mnih and Hinton, 2008),
the SOUL model combines the neural network ap-
proach with a class-based LM (Brown et al, 1992).
Structuring the output layer and using word class in-
formation makes the estimation of distribution over
large output vocabulary computationally feasible.
In the SOUL LM, the output vocabulary is struc-
tured in a clustering tree, where every word is asso-
ciated to a unique path from the root node to a leaf
node. Denoting wi the ith word in a sentence, the se-
quence c1:D(wi) = c1, . . . , cD encodes the path for
word wi in this tree, with D the tree depth, cd(wi)
the class or sub-class assigned to wi, and cD(wi) the
leaf associated with wi, comprising just the word it-
self. The probability of wi given its history h can
then be computed as:
P (wi|h) =P (c1(wi)|h)
?
D?
d=2
P (cd(wi)|h, c1:d?1).
(7)
There is a softmax function at each level of the
tree and each word ends up forming its own class
(a leaf). The SOUL architecture is represented in
the right part of Figure 1. The first (class layer)
estimates the class probability P (c1(wi)|h), while
sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d?1), d = 2 . . . (D ? 1). Fi-
nally, the word layer estimates the word probabili-
ties P (cD(wi)|h, c1:D?1). As in (Schwenk, 2007),
words in the short-list remain special, as each of
them represents a (final) class on its own right.
3 Efficiency issues
Training a SOUL model can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al, 2003),
this optimization is performed by Stochastic Back-
Propagation (SBP). Recurrent models are usually
trained using a variant of SBP called the Back-
Propagation Through Time (BPTT) (Rumelhart et
al., 1986; Mikolov et al, 2011a).
Following (Schwenk, 2007), it is possible to
greatly speed up the training of NNLMs using,
4
for instance, n-gram level resampling and bunch
mode training with parallelization (see below); these
methods can drastically reduce the overall training
time, from weeks to days. Adapting these meth-
ods to recurrent models are not straightforward. The
same goes with the SOUL extension: its training
scheme requires to first consider a restricted output
vocabulary (the shortlist), that is then extended to in-
clude the complete prediction vocabulary (Le et al,
2011b). This technique is too time consuming, in
practice, to be used when training recurrent mod-
els. By bounding the recurrence to a dozen or so
previous words, we obtain a recurrent-like n-gram
model that can benefit from a variety of speed-up
techniques, as explained in the next sections.
Note that the bounded-memory approximation is
only used for training: once training is complete, we
derive a true recurrent network using the parameters
trained on its approximation. This recurrent archi-
tecture is then used for inference.
3.1 Reducing the training data
Our usual approach for training large scale models
is based on n-gram level resampling a subset of the
training data at each epoch. This is not directly com-
patible with the recurrent model, which requires to
iterate over the training data sentence-by-sentence in
the same order as they occur in the document. How-
ever, by restricting the context to sentences, data re-
sampling can be carried out at the sentence level.
This means that the input layer is reinitialized at
the beginning of each sentence so as to ?forget?, as
it were, the memory of the previous sentences. A
similar proposal is made in (Mikolov et al, 2011b),
where the temporal dependencies are limited to the
level of paragraph. Another useful trick, which is
also adopted here, is to use different sampling rates
for the various subparts of the data, thus boosting the
use of in-domain versus out-of-domain data.
3.2 Bunch mode
Bunch mode training processes sentences by batches
of several examples, thus enabling matrix operation
that are performed very efficiently by the existing
BLAS library. After resampling, the training data is
divided into several sentence flows which are pro-
cessed simultaneously. While the number of exam-
ples per batch can be as high as 128 without any
visible loss of performance for n-gram NNLM, we
found, after some preliminary experiments, that the
value of 32 seems to yield a good tradeoff between
the computing time and the performance for recur-
rent models. Using such batches, the training time
can be speeded up by a factor of 8 at the price of a
slight loss (less than 2%) in perplexity.
3.3 SOUL training scheme
The SOUL training scheme integrates several steps
aimed at dealing with the fact that the output vocab-
ulary is split in two sub-parts: very frequent words
are in the so-called short-list and are treated differ-
ently from the less frequent ones. This setting can
not be easily reproduced with recurrent models. By
contrast, using the pseudo-recurrent n-gram NNLM,
the SOUL training scheme can be adopted; the re-
sulting parameter values are then plugged in into a
truly recurrent architecture. In the light of the results
reported below, we content ourselves with values of
n in the range 8-10.
4 Experimental Results
We now turn to the experimental part, starting with a
description of the experimental setup. We will then
present an attempt to quantify the relative impor-
tance of history words, followed by a head to head
comparison of the various NNLM architectures dis-
cussed in the previous sections.
4.1 Experimental setup
The tasks considered in our experiments are derived
from the shared translation track of WMT 2011
(translation from English to French). We only pro-
vide here a short overview of the task; all the neces-
sary details regarding this evaluation campaign are
available on the official Web site3 and our system
is described in (Allauzen et al, 2011). Simply note
that our parallel training data includes a large Web
corpus, referred to as the GigaWord parallel cor-
pus. After various preprocessing and filtering steps,
the total amount of training data is approximately
12 million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
To built the target language models, the mono-
lingual corpus was first split into several sub-parts
3http://www.statmt.org/wmt11
5
based on date and genre information. For each of
these sub-corpora, a standard 4-gram LM was then
estimated with interpolated Kneser-Ney smoothing
(Chen and Goodman, 1996). All models were cre-
ated without any pruning nor cutoff. The baseline
back-off n-gram LM was finally built as a linear
combination of several these models, where the in-
terpolation coefficients are chosen so as to minimize
the perplexity of a development set.
All NNLMs are trained following the prescrip-
tions of Le et al (2011b), and they all share the
same inner structure: the dimension of the projec-
tion word space is 500; the size of two hidden lay-
ers are respectively 1000 and 500; the short-list con-
tains 2000 words; and the non-linearity is introduced
with the sigmoid function. For the recurrent model,
the parameter that limits the back-propagation of er-
rors through time is set to 9 (see (Mikolov et al,
2010) for details). This parameter can be considered
to play a role that is similar to the history size in
our pseudo-recurrent n-gram model: a value of 9 in
the recurrent setting is equivalent to n = 10. All
NNLMs are trained with the following resampling
strategy: 75% of in-domain data (monolingual News
data 2008-2011) and 25% of the other data. At each
epoch, the parameters are updated using approxi-
mately 50 millions words for the last training step
and about 140 millions words for the previous ones.
4.2 The usefulness of remote words
In this section, we analyze the influence of each con-
text word with respect to their distance from the pre-
dicted word and to their POS tag. The quantitative
analysis relies on the variant of the n-gram architec-
ture based on (5) (see Section 2.1), which enables
us to keep track of the most important context word
for each prediction. Throughout this study, we will
consider 10-gram NNLMs.
Figure 2 represents the selection rate with respect
to the word position and displays the percentage of
coordinates in the input layer that are selected for
each position. As expected, close words are the most
important, with the previous word accounting for
more than 35% of the components. Remote words
(at a distance between 7 and 9) have almost the
same, weak, influence, with a selection rate close to
2.5%. This is consistent with the perplexity results
of n-gram NNLMs as a function of n, reported in
Tag Meaning Example
ABR abreviation etc FC FMI
ABK other abreviation ONG BCE CE
ADJ adjective officielles alimentaire mondial
ADV adverb contrairement assez alors
DET article; une les la
possessive pronoun ma ta
INT interjection oui adieu tic-tac
KON conjunction que et comme
NAM proper name Javier Mercure Pauline
NOM noun surprise inflation crise
NUM numeral deux cent premier
PRO pronoun cette il je
PRP preposition; de en dans
preposition plus article au du aux des
PUN punctuation; : , -
punctuation citation ?
SENT sentence tag ? . !
SYM symbol %
VER verb ont fasse parlent
<s> start of sentence
Table 1: List of grouped tags from TreeTagger.
Table 2: the difference between all orders from 4-
gram to 8-gram are significant, while the difference
between 8-gram and 10-gram is negligible.
POS tags were computed using the TreeTag-
ger (Schmid, 1994); sub-types of a main tag are
pooled to reduce the total number of categories. For
example, all the tags for verbs are merged into the
same VER class. Adding the token <s> (sentence
start), our tagset contains 17 tags (see Table 1).
The average selection rates for each tag are shown
in Figure 3: for each category, we display (in bars)
the average number of components that correspond
to a word in that category when this word is in pre-
vious position. Rare tags (INT, ABK , ABR and
SENT) seem to provide a very useful information
and have very high selection rates. Conversely, DET,
PUN and PRP words occur relatively frequently and
belong to the less selective group. The two most
frequent tags (NOM and VER ) have a medium se-
lection rate (approximately 0.5).
4.3 Translation experiments
The integration of NNLMs for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two-pass approach:
the first pass uses a conventional back-off n-gram
model to produce a list of the k most likely trans-
lations; in the second pass, the NNLMs probability
6
1 2 3 4 5 6 7 8 90.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Figure 2: Average selection rate per word position for the
max-based NNLM, computed on newstest2009-2011. On
x axis, the number k represents the kth previous word.
0 5 10 150.0
0.2
0.4
0.6
0.8
1.0
PUN DET SYM PRP NUM KON ADV SENT PRO VER <s> ADJ NOM ABR NAM ABK INT
Figure 3: Average selection rate of max function of the
first previous word in terms of word POS-tag information,
computed on newstest2009-2011. The green line repre-
sents the distribution of occurrences of each tag.
of each hypothesis is computed and the k-best list is
accordingly reordered. The NNLM weights are op-
timized as the other feature weights using Minimum
Error Rate Training (MERT) (Och, 2003). For all
our experiments, we used the value k = 300.
To clarify the impact of the language model or-
der in translation performance, we considered three
different ways to use NNLMs. In the first setting,
the NNLM is used alone and all the scores provided
by the MT system are ignored. In the second set-
ting (replace), the NNLM score replaces the score
of the standard back-off LM. Finally, the score of
the NNLM can be added in the linear combination
(add). In the last two settings, the weights used for
Model Perplexity BLEU
alone replace add
Baseline 90 29.4 31.3 -
4-gram 92 29.8 31.1 31.5
6-gram 82 30.2 31.6 31.8
8-gram 78 30.6 31.6 31.8
10-gram 77 30.5 31.7 31.8
recurrent 81 30.4 31.6 31.8
Table 2: Results for the English to French task obtained
with the baseline system and with various NNLMs. Per-
plexity is computed on newstest2009-2011 while BLEU is
on the test set (newstest2010).
n-best reranking are re-tuned with MERT.
Table 2 summarizes the BLEU scores obtained on
the newstest2010 test set. BLEU improvements are
observed with feed-forward NNLMs using a value
of n = 8 with respect to the baseline (n = 4).
Further increase from 8 to 10 only provides a very
small BLEU improvement. These results strengthen
the assumption made in Section 3.3: there seem to
be very little information in remote words (above
n = 7-8). It is also interesting to see that the 4-gram
NNLM achieves a comparable perplexity to the con-
ventional 4-gram model, yet delivers a small BLEU
increase in the alone condition.
Surprisingly4, on this task, recurrent models seem
to be comparable with 8-gram NNLMs. The rea-
son may be the deep architecture of recurrent model
that makes it hard to be trained in a large scale task.
With the recurrent-like n-gram model described in
Section 2.1.2, it is feasible to train a recurrent model
on a large task. With 10% of perplexity reduction as
compared to a backoff model, its yields comparable
performances as reported in (Mikolov et al, 2011a).
To the best of our knowledge, it is the first recurrent
NNLM trained on a such large dataset (2.5 billion
words) in a reasonable time (about 11 days).
5 Related work
There have been many attempts to increase the
context beyond a couple of history words (see eg.
(Rosenfeld, 2000)), for example: by modeling syn-
4Pers. com. with T. Mikolov: on the ?small? WSJ data
set, the recurrent model described in (Mikolov et al, 2011b)
outperforms the 10-gram NNLM.
7
tactic information, that better reflects the ?distance?
between words (Chelba and Jelinek, 2000; Collins
et al, 2005; Schwartz et al, 2011); with a unigram
model of the whole history (Kuhn and Mori, 1990);
by using trigger models (Lau et al, 1993); or by try-
ing to model document topics (Seymore and Rosen-
feld, 1997). One interesting proposal avoids the n-
gram assumption by estimating the probability of a
sentence (Rosenfeld et al, 2001). This approach
relies on a maximum entropy model which incor-
porates arbitrary features. No significant improve-
ments were however observed with this model, a fact
that can be attributed to two main causes: first, the
partition function can not be computed exactly as it
involves a sum over all the possible sentences; sec-
ond, it seems that data sparsity issues for this model
are also adversely affecting the performance.
The recurrent network architecture for LMs was
proposed in (Mikolov et al, 2010) and then ex-
tended in (Mikolov et al, 2011b). The authors pro-
pose a hierarchical architecture similar to the SOUL
model, based however on a simple unigram clus-
tering. For large scale tasks (? 400M training
words), advanced training strategies were investi-
gated in (Mikolov et al, 2011a). Instead of resam-
pling, the data was divided into paragraphs, filtered
and then sorted: the most in-domain data was thus
placed at the end of each epoch. On the other hand,
the hidden layer size was decreased by simulating a
maximum entropy model using a hash function on
n-grams. This part represents direct connections be-
tween input and output layers. By sharing the pre-
diction task, the work of the hidden layer is made
simpler, and can thus be handled with a smaller
number of hidden units. This approach reintroduces
into the model discrete features which are somehow
one main weakness of conventional backoff LMs as
compared to NNLMs. In fact, this strategy can be
viewed as an effort to directly combine the two ap-
proaches (backoff-model and neural network), in-
stead of using a traditional way, through interpola-
tion. Training simultaneously two different models
is computationally very demanding for large vocab-
ularies, even with help of hashing technique; in com-
parison, our approach keeps the model architecture
simple, making it possible to use the efficient tech-
niques developed for n-gram NNLMs.
The use the max, rather than a sum, on the hid-
den layer of neural network is not new. Within the
context of language modeling, it was first proposed
in (Collobert et al, 2011) with the goal to model a
variable number of input features. Our motivation
for using this variant was different, and was mostly
aimed at analyzing the influence of context words
based on the selection rates of this function.
6 Conclusion
In this paper, we have investigated several types
of NNLMs, along with conventional LMs, in or-
der to assess the influence of long range dependen-
cies within sentences in the language modeling task:
from recurrent models that can recursively handle
an arbitrary number of context words to n-gram
NNLMs with n varying between 4 and 10. Our con-
tribution is two-fold.
First, experimental results showed that the influ-
ence of word further than 9 can be neglected for the
statistical machine translation task 5. Therefore, the
n-gram assumption with n ? 10 appears to be well-
founded to handle most sentence internal dependen-
cies. Another interesting conclusion of this study
is that the main issue of the conventional n-gram
model is not its conditional independence assump-
tions, but the use of too small values for n.
Second, by restricting the context of recurrent net-
works, the model can benefit of the advanced train-
ing schemes and its training time can be divided by
a factor 8 without loss on the performances. To the
best of our knowledge, it is the first time that a re-
current NNLM is trained on a such large dataset in
a reasonable time. Finally, we compared these mod-
els within a large scale MT task, with monolingual
data that contains 2.5 billion words. Experimental
results showed that using long range dependencies
(n = 10) with a SOUL language model significantly
outperforms conventional LMs. In this setting, the
use of a recurrent architecture does not yield any im-
provements, both in terms of perplexity and BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, the French State agency
for innovation.
5The same trend is observed in speech recognition.
8
References
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland.
Y Bengio, R Ducharme, P Vincent, and C Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3(6):1137?1155.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 507?514, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Ahmad Emami, Imed Zitouni, and Lidia Mangu. 2008.
Rich morphology based n-gram language models for
arabic. In INTERSPEECH, pages 829?832.
R. Kuhn and R. De Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
12(6):570?583, june.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Ilya Oparin, Abdel. Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857?2860.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceedings
of the 11th Annual Conference of the International
Speech Communication Association (INTERSPEECH
2010), volume 2010, pages 1045?1048. International
Speech Communication Association.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for train-
ing large scale neural network language models. In
Proceedings of ASRU 2011, pages 196?201. IEEE Sig-
nal Processing Society.
Toma?s? Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model. In
Proc. of ICASSP?11, pages 5528?5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language models:
A vehicle for linguistic-statistical integration. Com-
puters, Speech and Language, 15:2001.
R. Rosenfeld. 2000. Two decades of statistical language
modeling: Where do we go from here ? Proceedings
of the IEEE, 88(8).
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986.
Parallel distributed processing: explorations in the mi-
crostructure of cognition, vol. 1. chapter Learning
9
internal representations by error propagation, pages
318?362. MIT Press, Cambridge, MA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Lane Schwartz, Chris Callison-Burch, William Schuler,
and Stephen Wu. 2011. Incremental syntactic lan-
guage models for phrase-based translation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 620?631, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
H. Schwenk and P. Koehn. 2008. Large and diverse lan-
guage models for statistical machine translation. In
International Joint Conference on Natural Language
Processing, pages 661?666, Janv 2008.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Kristie Seymore and Ronald Rosenfeld. 1997. Using
story topics for language model adaptation. In Proc. of
Eurospeech ?97, pages 1987?1990, Rhodes, Greece.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
10
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joint WMT 2012 Submission of the QUAERO Project
?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,
?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,
?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,
?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2012 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems and finally the RWTH system combi-
nation combined these translations in our final
submission. Experimental results show im-
provements of up to 1.7 points in BLEU and
3.4 points in TER compared to the best single
system.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
This paper is structured as follows. In Section
2, the different engines of all four groups are in-
troduced. In Section 3, the RWTH Aachen system
combination approach is presented. Experiments
with different system selections for system combi-
nation are described in Section 4. Finally in Section
5, we discuss the results.
2 Translation Systems
For WMT 2012 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned
on the newstest2009 or newstest2010 development
set. The newstest2011 dev set was used to train
the system combination parameters. Finally, the
newstest2008-newstest2010 dev sets were used to
compare the results of the different system combina-
tion settings. In this Section all four different system
engines are presented.
2.1 RWTH Aachen Single Systems
For the WMT 2012 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
parallel corpus, the translation probabilities are esti-
mated by relative frequencies. The standard feature
322
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. The model weights are optimized with standard
Mert (Och, 2003) on 200-best lists. The optimiza-
tion criterium is BLEU.
2.1.2 Hierarchical System
For the hierarchical setups (HPBT) described in
this paper, the open source Jane toolkit (Vilar et
al., 2010) is employed. Jane has been developed at
RWTH and implements the hierarchical approach as
introduced by Chiang (2007) with some state-of-the-
art extensions. In hierarchical phrase-based transla-
tion, a weighted synchronous context-free grammar
is induced from parallel text. In addition to contigu-
ous lexical phrases, hierarchical phrases with up to
two gaps are extracted. The search is typically car-
ried out using the cube pruning algorithm (Huang
and Chiang, 2007). The model weights are opti-
mized with standard Mert (Och, 2003) on 100-best
lists. The optimization criterium is 4BLEU ?TER.
2.1.3 Preprocessing
In order to reduce the source vocabulary size
translation, the German text was preprocessed
by splitting German compound words with the
frequency-based method described in (Koehn and
Knight, 2003a). To further reduce translation com-
plexity for the phrase-based approach, we performed
the long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.4 Language Model
For both decoders a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl, the
109 French-English, UN and LDC Gigaword Fourth
Edition corpora. For the 109 French-English, UN
and LDC Gigaword corpora RWTH applied the data
selection technique described in (Moore and Lewis,
2010).
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogenous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003b). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation and op-
timization with regard to the BLEU score is done us-
ing Minimum Error Rate Training as described in
Venugopal et al (2005).
2.2.3 Translation Models
The translation model is trained on the Europarl
and News Commentary Corpus and the phrase ta-
ble is based on a discriminative word alignment
(Niehues and Vogel, 2008).
In addition, the system applies a bilingual lan-
guage model (Niehues et al, 2011) to extend the
context of source language words available for trans-
lation.
Furthermore, we use a discriminative word lexi-
con as introduced in (Mauser et al, 2009). The lex-
icon was trained and integrated into our system as
described in (Mediani et al, 2011).
At last, we tried to find translations for
out-of-vocabulary (OOV) words by using quasi-
morphological operations as described in Niehues
and Waibel (2011). For each OOV word, we try to
find a related word that we can translate. We modify
the ending letters of the OOV word and learn quasi-
morphological operations to be performed on the
known translation of the related word to synthesize
a translation for the OOV word. By this approach
we were for example able to translate Kaminen into
chimneys using the known translation Kamin # chim-
ney.
2.2.4 Language Models
We use two 4-gram SRI language models, one
trained on the News Shuffle corpus and one trained
1http://hunspell.sourceforge.net/
323
on the Gigaword corpus. Furthermore, we use a 5-
gram cluster-based language model trained on the
News Shuffle corpus. The word clusters were cre-
ated using the MKCLS algorithm. We used 100
word clusters.
2.2.5 Reordering Model
Reordering is performed based on part-of-speech
tags obtained using the TreeTagger (Schmid, 1994).
Based on these tags we learn probabilistic continu-
ous (Rottmann and Vogel, 2007) and discontinuous
(Niehues and Kolss, 2009) rules to cover short and
long-range reorderings. The rules are learned from
the training corpus and the alignment. In addition,
we learned tree-based reordering rules. Therefore,
the training corpus was parsed by the Stanford parser
(Rafferty and Manning, 2008). The tree-based rules
consist of the head node of a subtree and all its
children as well as the new order and a probability.
These rules were applied recursively. The reordering
rules are applied to the source sentences and the re-
ordered sentence variants as well as the original se-
quence are encoded in a word lattice which is used
as input to the decoder. For the test sentences, the
reordering based on parts-of-speech and trees allows
us to change the word order in the source sentence
so that the sentence can be translated more easily.
In addition, we build reordering lattices for all train-
ing sentences and then extract phrase pairs from the
monotone source path as well as from the reordered
paths.
2.3 LIMSI-CNRS Single System
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine translation
system based on bilingual n-gram2. In this approach,
the translation model relies on a specific decomposi-
tion of the joint probability of a sentence pair P(s, t)
using the n-gram assumption: a sentence pair is de-
composed into a sequence of bilingual units called
tuples, defining a joint segmentation of the source
and target. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing which ultimately derives from initial word and
phrase alignments.
2http://ncode.limsi.fr/
2.3.1 An Overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using a
n-gram model of (source,target) pairs (Casacuberta
and Vidal, 2004). Training this model requires to
reorder source sentences so as to match the target
word order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a ?weak?
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
used in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 development set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2.3.2 Continuous Space Translation Models
One critical issue with standard n-gram transla-
tion models is that the elementary units are bilingual
pairs, which means that the underlying vocabulary
can be quite large. Unfortunately, the parallel data
available to train these models are typically smaller
than the corresponding monolingual corpora used to
train target language models. It is very likely then,
that such models should face severe estimation prob-
lems. In such setting, using neural network language
3Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
324
model techniques seem all the more appropriate. For
this study, we follow the recommendations of Le et
al. (2012), who propose to factor the joint proba-
bility of a sentence pair by decomposing tuples in
two (source and target) parts, and further each part
in words. This yields a word factored translation
model that can be estimated in a continuous space
using the SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the computa-
tional cost of computing n-gram probabilities. The
solution used here was to resort to a two pass ap-
proach: the first pass uses a conventional back-off
n-gram model to produce a k-best list; in the second
pass, the k-best list is reordered using the probabil-
ities of m-gram SOUL translation models. In the
following experiments, we used a fixed context size
for SOUL of m = 10, and used k = 300.
2.3.3 Corpora and Data Preprocessing
The parallel data is word-aligned using
MGIZA++4 with default settings. For the En-
glish monolingual training data, we used the same
setup as last year5 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we took advantage of our in-house
text processing tools for tokenization and detok-
enization steps (De?chelotte et al, 2008) and our sys-
tem was built in ?true-case?. As German is mor-
phologically more complex than English, the default
policy which consists in treating each word form
independently is plagued with data sparsity, which
is detrimental both at training and decoding time.
Thus, the German side was normalized using a spe-
cific pre-processing scheme (Allauzen et al, 2010;
Durgar El-Kahlout and Yvon, 2010), which notably
aims at reducing the lexical redundancy by (i) nor-
malizing the orthography, (ii) neutralizing most in-
flections and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
a system composed of the standard SYSTRAN MT
engine in combination with a statistical post editing
(SPE) component.
4http://geek.kyloo.net/software
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k -
800k entries per language pair).
The SYSTRAN phrase-based SPE component
views the output of the rule-based system as the
source language, and the (human) reference trans-
lation as the target language, see (L. Dugast and
Koehn, 2007). It performs corrections and adaptions
learned from the 5-gram language model trained on
the parallel target-to-target corpus. Moreover, the
following measures - limiting unwanted statistical
effects - were applied:
? Named entities, time and numeric expressions
are replaced by special tokens on both sides.
This usually improves word alignment, since
the vocabulary size is significantly reduced. In
addition, entity translation is handled more re-
liably by the rule-based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus to help to improve word
alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
The SPE language model was trained on 2M bilin-
gual phrases from the news/Europarl corpora, pro-
vided as training data for WMT 2012. An addi-
tional language model built from 15M phrases of
the English LDC Gigaword corpus using Kneser-
Ney (Kneser and Ney, 1995) smoothing was added.
Weights for these separate models were tuned by
the Mert algorithm provided in the Moses toolkit
(P. Koehn et al, 2007), using the provided news de-
velopment set.
325
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical models
is selected as consensus translation.
4 Experiments
This year, we tried different sets of single systems
for system combination. As RWTH has two dif-
ferent translation systems, we put the output of
both systems into system combination. Although
both systems have the same preprocessing and lan-
guage model, their hypotheses differ because of
their different decoding approach. Compared to
the other systems, the system by SYSTRAN has a
completely different approach (see section 2.4). It
is mainly based on a rule-based system. For the
German?English pair, SYSTRAN achieves a lower
BLEU score in each test set compared to the other
groups. However, since the SYSTRAN system is
very different to the others, we still obtain an im-
provement when we add it also to system combina-
tion.
We did experiments with different optimization
criteria for the system combination optimization.
All results are listed in Table 1 (unoptimized), Table
2 (optimized on BLEU) and Table 3 (optimized on
TER-BLEU). Further, we investigated, whether we
will loose performance, if a single system is dropped
from the system combination. The results show that
for each optimization criteria we need all systems to
achieve the best results.
For the BLEU optimized system combination, we
obtain an improvement compared to the best sin-
gle systems for all dev sets. For newstest2008, we
get an improvement of 1.5 points in BLEU and 1.5
points in TER compared to the best single system of
Karlsruhe Institute of Technology. For newstest2009
we get an improvement of 1.9 points in BLEU and
1.5 points in TER compared to the best single sys-
tem. The system combination of all systems outper-
forms the best single system with 1.9 points in BLEU
and 1.9 points in TER for newstest2010. For new-
stest2011 the improvement is 1.3 points in BLEU
and 2.9 points in TER.
For the TER-BLEU optimized system combina-
tion, we achieved more improvement in TER com-
pared to the BLEU optimized system combination.
For newstest2008, we get an improvement of 0.8
points in BLEU and 3.0 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. The system combinations performs better
on newstest2009 with 1.3 points in BLEU and 2.7
points in TER. For newstest2010, we get an im-
provement of 1.7 points in BLEU and 3.4 points in
TER and for newstest2011 we get an improvement
of 0.7 points in BLEU and 2.5 points in TER.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally, the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU and
a lower TER score compared to each single sys-
tem. For each optimization criteria the system com-
binations using all single systems outperforms the
system combinations using one less single system.
Although the single system of SYSTRAN has the
worst error scores and the RWTH single systems are
similar, we achieved the best result in using all single
systems. For the WMT 12 evaluation, we submitted
the system combination of all systems optimized on
BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
326
Table 1: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are unoptimized.
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
KIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9
RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7
Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2
RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4
SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8
sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5
sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6
sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6
sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0
sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1
sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3
Table 2: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6
sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1
sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5
sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2
sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5
sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2
Table 3: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on TER-BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6
sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9
sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0
sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3
sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5
sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4
327
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003a. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn and K. Knight. 2003b. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In NAACL ?12: Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology.
Jose? B. Marin?o, R. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4).
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT English-
French Translation Systems for IWSLT 2011. In Pro-
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. In Proc.
of Third ACL Workshop on Statistical Machine Trans-
lation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In Pro-
328
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Evelyne
Tzoukermann and SusanEditors Armstrong, editors,
Proceedings of the ACL SIGDATWorkshop, pages 47?
50. Kluwer Academic Publishers.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
329
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
